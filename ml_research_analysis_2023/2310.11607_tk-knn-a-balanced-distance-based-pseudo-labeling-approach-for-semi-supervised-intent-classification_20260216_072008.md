---
ver: rpa2
title: 'TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised
  Intent Classification'
arxiv_id: '2310.11607'
source_url: https://arxiv.org/abs/2310.11607
tags:
- top-k
- data
- labeled
- tk-knn
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TK-KNN, a balanced distance-based pseudo-labeling
  approach for semi-supervised intent classification. The method addresses the problem
  of model overconfidence and class imbalance in existing pseudo-labeling approaches.
---

# TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification

## Quick Facts
- **arXiv ID**: 2310.11607
- **Source URL**: https://arxiv.org/abs/2310.11607
- **Reference count**: 20
- **Primary result**: TK-KNN achieves up to 10.92% better accuracy than FlexMatch on CLINC150 with 1% labeled data

## Executive Summary
TK-KNN addresses overconfidence and class imbalance in pseudo-labeling for semi-supervised intent classification. The method combines top-k sampling per class with KNN-based similarity scoring in the embedding space to improve pseudo-label quality. Experiments on CLINC150, Banking77, and HWU64 show TK-KNN outperforms existing methods, particularly when labeled data is scarce. The approach achieves state-of-the-art results with 10.92% improvement over the second-best method (FlexMatch) on CLINC150 with only 1% labeled data.

## Method Summary
TK-KNN uses a top-k sampling strategy to enforce balanced class representation and incorporates KNN search in the embedding space to improve label quality. The method trains a BERT-base model with classification head using cross-entropy, supervised contrastive, and differential entropy losses. During iterative self-training, it generates pseudo-labels, computes cosine similarity between unlabeled examples and labeled examples in predicted classes, and selects top-k examples per class based on a weighted combination of model confidence and similarity. The method addresses class imbalance and model overconfidence by ensuring all classes are equally represented in pseudo-labeled data and prioritizing examples that are both confident and close to labeled data in the embedding space.

## Key Results
- TK-KNN achieves 10.92% better accuracy than FlexMatch on CLINC150 with 1% labeled data
- Consistent improvement across all three benchmark datasets (CLINC150, Banking77, HWU64) with varying labeled data percentages
- Ablation studies confirm both top-k sampling and KNN ranking contribute to performance gains
- Method shows robustness across different class distributions in benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Top-k sampling balances class representation and reduces overconfidence bias compared to threshold-based pseudo-labeling.
- **Mechanism**: Instead of selecting pseudo-labels based on a global confidence threshold, the method selects the top-k most confident predictions per class. This ensures that all classes are equally represented in the pseudo-labeled set, preventing the model from becoming biased toward easier-to-predict classes.
- **Core assumption**: Classes vary in difficulty; easier classes will dominate threshold-based selection early in training, leading to class imbalance and overfitting.
- **Evidence anchors**: [abstract] "uses a top-k sampling strategy to enforce balanced classes"; [section] "the model can become very biased towards the easy classes in the early iterations of learning"

### Mechanism 2
- **Claim**: KNN-based ranking improves pseudo-label quality by leveraging embedding similarity.
- **Mechanism**: For each unlabeled example, the method computes cosine similarity between its embedding and the embeddings of labeled examples in the predicted class. The score combines model confidence and similarity, prioritizing examples that are both confident and close to labeled data in the embedding space.
- **Core assumption**: In the embedding space, examples from the same class cluster together, so similarity to labeled examples indicates label reliability.
- **Evidence anchors**: [abstract] "incorporates a K-Nearest Neighbors search in the embedding space to improve label quality"; [section] "leverage the embedding space of the labeled and unlabeled examples to find those with similar embeddings"

### Mechanism 3
- **Claim**: Joint training with cross-entropy, supervised contrastive, and differential entropy losses improves embedding discriminability.
- **Mechanism**: Cross-entropy encourages correct predictions, supervised contrastive loss pulls embeddings of the same class together and pushes different classes apart, and differential entropy regularizer prevents collapse by spreading representations.
- **Core assumption**: Better embeddings lead to more accurate KNN-based ranking and improved model generalization.
- **Evidence anchors**: [section] "supplemented the cross-entropy loss with a supervised contrastive loss... and a differential entropy regularization loss"; [section] "This ensures that our model with learn good discriminative features in the latent space"

## Foundational Learning

- **Concept: Semi-supervised learning (SSL)**
  - Why needed here: The method uses unlabeled data to improve intent classification when labeled data is scarce.
  - Quick check question: What is the key difference between supervised and semi-supervised learning in terms of data usage?

- **Concept: Pseudo-labeling**
  - Why needed here: The method generates labels for unlabeled data using the model's predictions, then retrains on the combined set.
  - Quick check question: What is a potential risk of using pseudo-labels, and how does this method mitigate it?

- **Concept: K-Nearest Neighbors (KNN)**
  - Why needed here: KNN is used to rank unlabeled examples based on embedding similarity to labeled examples, improving pseudo-label selection.
  - Quick check question: How does KNN help ensure that selected pseudo-labels are more likely to be correct?

## Architecture Onboarding

- **Component map**: BERT-base encoder -> Embedding extractor -> KNN similarity calculator -> Top-k selector -> Loss functions (cross-entropy + supervised contrastive + differential entropy) -> Classification head

- **Critical path**:
  1. Train initial model on labeled data
  2. Generate pseudo-labels and compute KNN similarities
  3. Rank and select top-k per class
  4. Combine with labeled data and retrain
  5. Repeat until convergence or max cycles

- **Design tradeoffs**:
  - Top-k vs. threshold: Top-k ensures class balance but may include lower-confidence labels; threshold is simpler but risks class imbalance
  - KNN similarity: Improves label quality but adds computational cost
  - Joint loss functions: Improves embeddings but increases training complexity and hyperparameter tuning

- **Failure signatures**:
  - Poor performance on minority classes: Indicates class imbalance in pseudo-label selection
  - No improvement over cycles: Suggests pseudo-labels are noisy or KNN similarity is unreliable
  - Slow convergence: May indicate suboptimal hyperparameters (k, β) or loss weights

- **First 3 experiments**:
  1. Verify that top-k sampling balances class representation by counting pseudo-labels per class after each cycle
  2. Test KNN ranking by comparing pseudo-label accuracy with and without similarity scores
  3. Ablation study of loss components to confirm their individual contributions to performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the TK-KNN method perform on heavily imbalanced datasets compared to the benchmark datasets used in this study?
- **Basis in paper**: [inferred] The authors mention that they did not explore heavily imbalanced datasets and are unaware of how TK-KNN would perform under those scenarios.
- **Why unresolved**: The paper does not provide any experimental results or analysis on heavily imbalanced datasets, which could be a common scenario in real-world applications.
- **What evidence would resolve it**: Conducting experiments on heavily imbalanced datasets and comparing the performance of TK-KNN with other methods on these datasets would provide insights into the method's effectiveness in handling imbalanced data.

### Open Question 2
- **Question**: What is the impact of varying the hyperparameters k and β on the performance of TK-KNN, and how can we determine the optimal values for these parameters?
- **Basis in paper**: [explicit] The authors mention that TK-KNN relies on two hyperparameters k and β that can affect performance based on how they are configured, and they explore experiments to gauge their effect on learning by testing different values for these parameters.
- **Why unresolved**: While the authors provide some insights into the impact of these hyperparameters, they do not offer a clear method for determining the optimal values for k and β in different scenarios.
- **What evidence would resolve it**: Conducting a more extensive hyperparameter search and analyzing the performance of TK-KNN with different combinations of k and β values on various datasets would help in identifying the optimal values for these parameters.

### Open Question 3
- **Question**: How does the TK-KNN method compare to other state-of-the-art semi-supervised learning methods in terms of computational efficiency and scalability?
- **Basis in paper**: [inferred] The authors mention that TK-KNN requires more GPU usage due to the increased number of training cycles, especially if the value of k is small. However, they do not provide a direct comparison of the computational efficiency and scalability of TK-KNN with other methods.
- **Why unresolved**: The paper does not include a detailed analysis of the computational efficiency and scalability of TK-KNN compared to other state-of-the-art methods, which is crucial for understanding its practical applicability.
- **What evidence would resolve it**: Conducting experiments to measure the computational efficiency and scalability of TK-KNN and comparing these metrics with other state-of-the-art semi-supervised learning methods would provide insights into the method's practical applicability and limitations.

## Limitations
- Method's performance depends on well-separated class clusters in embedding space, which may not hold for all intent classification tasks
- Computational overhead from KNN calculations and iterative self-training limits scalability to large datasets
- Hyperparameter sensitivity to k and β values requires careful tuning for optimal performance

## Confidence
- **High Confidence**: Claims about TK-KNN's effectiveness on CLINC150, Banking77, and HWU64 datasets with 1-10% labeled data
- **Medium Confidence**: Claims about the specific mechanisms (top-k sampling and KNN ranking) improving label quality
- **Low Confidence**: Claims about the method's robustness to different embedding qualities or class distributions

## Next Checks
1. Evaluate TK-KNN on a dataset with highly overlapping intent classes to assess performance when KNN similarity becomes less reliable
2. Systematically vary k and β across a wider range to identify optimal values for different datasets and class distributions
3. Measure training time and memory usage on progressively larger datasets to quantify computational overhead and identify practical limits for real-world deployment