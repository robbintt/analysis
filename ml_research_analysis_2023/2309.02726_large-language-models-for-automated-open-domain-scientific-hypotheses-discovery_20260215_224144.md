---
ver: rpa2
title: Large Language Models for Automated Open-domain Scientific Hypotheses Discovery
arxiv_id: '2309.02726'
source_url: https://arxiv.org/abs/2309.02726
tags:
- hypothesis
- hypotheses
- social
- technology
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOMATO, the first task in NLP for automated
  open-domain scientific hypotheses discovery in social science and business research.
  Unlike prior work, TOMATO requires generating hypotheses from raw web corpus, rather
  than manually selected sentences, and aims to propose hypotheses that are novel
  to humanity.
---

# Large Language Models for Automated Open-domain Scientific Hypotheses Discovery

## Quick Facts
- **arXiv ID:** 2309.02726
- **Source URL:** https://arxiv.org/abs/2309.02726
- **Reference count:** 39
- **Primary result:** Introduces TOMATO task and MOOSE framework for automated open-domain scientific hypotheses discovery using LLMs

## Executive Summary
This paper introduces TOMATO, the first NLP task for automated open-domain scientific hypotheses discovery in social science and business research. Unlike prior work, TOMATO requires generating hypotheses from raw web corpus rather than manually selected sentences, aiming to propose hypotheses that are novel to humanity. The authors propose MOOSE, a multi-module framework leveraging large language models with three types of feedback mechanisms: present-feedback for iterative refinement, past-feedback to improve inspiration selection based on future hypothesis generation, and future-feedback to provide justifications or initial suggestions to future modules. Experiments using both GPT-4 and expert evaluation show that MOOSE significantly outperforms a baseline LLM in generating valid, novel, and helpful hypotheses, with each feedback mechanism contributing to performance gains.

## Method Summary
The MOOSE framework uses a multi-module architecture with five core generation modules: Background Finder, Inspiration Title Finder, Inspiration Finder, Hypothesis Proposer, and three feedback checkers (Clarity, Reality, Novelty). The framework processes raw web corpus to extract relevant backgrounds and inspirations, then generates hypotheses through iterative refinement using present-feedback loops. Past-feedback creates a backward loop from hypothesis generation to inspiration selection, while future-feedback provides justifications and suggestions to guide subsequent modules. The system uses GPT-3.5-turbo for all modules with 0.9 temperature and 0.9 top_p parameters, evaluating hypotheses across validness, novelty, and helpfulness metrics.

## Key Results
- MOOSE significantly outperforms baseline LLM in generating valid, novel, and helpful hypotheses
- Each feedback mechanism (present, past, future) contributes to performance gains
- Iterative present-feedback improves hypothesis quality across multiple refinement rounds
- Past-feedback enhances novelty by improving inspiration selection based on downstream hypothesis generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The multi-stage module design allows progressive refinement of hypotheses by separating background identification, inspiration selection, and hypothesis generation into distinct components.
- **Mechanism:** Each module specializes in one task: the Background Finder locates relevant research context from raw web corpus, the Inspiration Finder identifies supporting theories or variables, and the Hypothesis Proposer synthesizes these into novel hypotheses. This separation enables targeted improvements through feedback loops.
- **Core assumption:** Breaking the hypothesis generation process into modular steps improves quality over end-to-end generation because each component can be optimized independently.
- **Evidence anchors:**
  - [abstract] "A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance"
  - [section 4.1] "the base framework is developed based on the intuitive understanding of how social science researchers propose an initial research hypothesis from scratch"
- **Break condition:** If the Inspiration Finder consistently retrieves irrelevant content, the entire chain fails because the Hypothesis Proposer cannot synthesize meaningful connections.

### Mechanism 2
- **Claim:** Present-feedback loops enable iterative hypothesis refinement by providing immediate quality assessment and modification suggestions.
- **Mechanism:** After initial hypothesis generation, Clarity, Reality, and Novelty checkers evaluate the output and provide specific feedback. The Hypothesis Proposer then incorporates this feedback to generate improved versions, repeating for multiple iterations.
- **Core assumption:** Language models can effectively critique their own outputs and provide actionable feedback for improvement.
- **Evidence anchors:**
  - [section 4.2] "the LLMs for filtering also provide feedback to the hypothesis proposer module, so that the hypothesis proposer module can generate the hypothesis again, leveraging the feedback"
  - [table 4] Shows steady improvement in validness and novelty with additional present-feedback iterations
- **Break condition:** If feedback becomes too generic or contradictory across iterations, the refinement process may stall or degrade quality.

### Mechanism 3
- **Claim:** Past-feedback leverages future module outputs to improve earlier module performance, creating a form of backward credit assignment.
- **Mechanism:** The Inspiration Feedback module analyzes hypotheses generated by the Hypothesis Proposer to provide guidance to the Inspiration Title Finder on selecting more relevant inspiration sources. This allows earlier modules to benefit from downstream performance insights.
- **Core assumption:** Hypothesis quality is strongly influenced by the relevance of selected inspirations, and this relationship can be learned through feedback.
- **Evidence anchors:**
  - [section 4.3] "it is hard to evaluate selected inspirations unless we know what hypotheses these inspirations (combined with a given background) could lead to"
  - [table 3] Shows MOOSE with both future and past-feedback outperforms version with only future-feedback
- **Break condition:** If the feedback signal is too noisy or the relationship between inspirations and hypothesis quality is too complex to capture, past-feedback may provide misleading guidance.

## Foundational Learning

- **Concept: Abductive reasoning**
  - Why needed here: The TOMATO task requires generating novel hypotheses from observations, which is fundamentally abductive reasoning - inferring the best explanation for observed phenomena.
  - Quick check question: Can you explain how abductive reasoning differs from deductive and inductive reasoning in the context of scientific hypothesis generation?

- **Concept: Feedback loops in machine learning**
  - Why needed here: The framework relies on multiple feedback mechanisms (present, past, future) to iteratively improve hypothesis quality, requiring understanding of how feedback loops work in AI systems.
  - Quick check question: What is the key difference between present-feedback and past-feedback in terms of when the feedback signal is generated relative to the module being improved?

- **Concept: Module composition and data flow**
  - Why needed here: The framework consists of interconnected modules with specific data dependencies, requiring understanding of how information flows through the system and how modules interact.
  - Quick check question: Trace the data flow from raw web corpus through to final hypothesis generation, identifying which modules receive input from which other modules.

## Architecture Onboarding

- **Component map:** Background Finder → Inspiration Title Finder → Inspiration Finder → Hypothesis Suggestor → Hypothesis Proposer, with three feedback mechanisms (Clarity/Reality/Novelty checkers for present-feedback, Inspiration Feedback for past-feedback, and justifications/suggestions for future-feedback).

- **Critical path:** The main pipeline runs through all five generation modules in sequence. Present-feedback adds iterations around the Hypothesis Proposer, while past-feedback creates a loop from Hypothesis Proposer back to Inspiration Title Finder.

- **Design tradeoffs:** Modular design enables specialized optimization but increases complexity and latency. The framework trades computational efficiency for quality through multiple feedback iterations and module interactions.

- **Failure signatures:** If hypotheses are consistently invalid, check the Reality Checker feedback; if consistently not novel, examine the Novelty Checker and Inspiration Feedback; if generation is slow, profile the feedback loop iterations.

- **First 3 experiments:**
  1. Test Background Finder alone on sample corpus to verify it retrieves relevant research contexts
  2. Run full pipeline with single present-feedback iteration to establish baseline performance
  3. Enable past-feedback and compare novelty scores against baseline to validate the mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MOOSE change when using different large language models (e.g., GPT-3.5, GPT-4, LLaMA) for each module?
- Basis in paper: [inferred] The paper mentions using gpt-3.5-turbo for each module in MOOSE and gpt-4 for evaluation, but does not explore the impact of different LLMs on performance.
- Why unresolved: The paper does not provide a comparison of MOOSE's performance with different LLMs for each module.
- What evidence would resolve it: Experimental results comparing MOOSE's performance using different LLMs for each module.

### Open Question 2
- Question: Can the feedback mechanisms in MOOSE be further improved by incorporating domain-specific knowledge or external knowledge bases?
- Basis in paper: [inferred] The paper mentions that MOOSE uses three feedback mechanisms (present-feedback, past-feedback, and future-feedback) to improve the quality of generated hypotheses, but does not explore the potential of incorporating domain-specific or external knowledge.
- Why unresolved: The paper does not investigate the potential benefits of incorporating domain-specific or external knowledge into the feedback mechanisms.
- What evidence would resolve it: Experimental results comparing MOOSE's performance with and without the incorporation of domain-specific or external knowledge in the feedback mechanisms.

### Open Question 3
- Question: How does the complexity of the raw web corpus (e.g., length, domain specificity, language style) affect the performance of MOOSE in generating valid, novel, and helpful hypotheses?
- Basis in paper: [inferred] The paper mentions that the raw web corpus is mostly from common news, Wikipedia, and business reviews, but does not explore how different characteristics of the corpus might impact MOOSE's performance.
- Why unresolved: The paper does not provide a systematic analysis of how different corpus characteristics affect MOOSE's performance.
- What evidence would resolve it: Experimental results comparing MOOSE's performance using raw web corpora with varying characteristics (e.g., length, domain specificity, language style).

## Limitations

- The framework's performance relies heavily on the quality of LLM-generated feedback, which may vary with different model versions or prompts
- The human heuristics used for Inspiration Feedback when ChatGPT proved inadequate are not fully specified, creating uncertainty about reproducibility
- The evaluation depends on expert judgment for novelty assessment, which may not scale to larger hypothesis generation tasks

## Confidence

- **High confidence:** The modular architecture design and baseline performance improvements are well-supported by experimental results
- **Medium confidence:** The contribution of individual feedback mechanisms to performance gains, as the ablation studies show improvements but the exact magnitude may vary with implementation details
- **Medium confidence:** The novelty assessment methodology, as it relies on human expert judgment which may have subjective elements

## Next Checks

1. Implement a controlled ablation study testing each feedback mechanism independently to quantify their individual contributions to hypothesis quality
2. Evaluate the framework's performance across different LLM models (GPT-4, Claude, etc.) to assess generalizability of the modular approach
3. Conduct a scalability test by increasing the number of hypotheses generated per background to assess whether performance degrades with larger output volumes