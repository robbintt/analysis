---
ver: rpa2
title: Model evaluation for extreme risks
arxiv_id: '2305.15324'
source_url: https://arxiv.org/abs/2305.15324
tags:
- evaluation
- evaluations
- risks
- extreme
- capabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AI developers must be able to identify dangerous capabilities (through
  "dangerous capability evaluations") and the propensity of models to apply their
  capabilities for harm (through "alignment evaluations"). These evaluations will
  become critical for keeping policymakers and other stakeholders informed, and for
  making responsible decisions about model training, deployment, and security.
---

# Model evaluation for extreme risks

## Quick Facts
- arXiv ID: 2305.15324
- Source URL: https://arxiv.org/abs/2305.15324
- Reference count: 11
- Primary result: AI developers need systematic model evaluation to identify dangerous capabilities and alignment issues before catastrophic harm occurs

## Executive Summary
This paper argues that model evaluation is essential for identifying dangerous capabilities and alignment issues in AI systems that could pose extreme risks. The authors distinguish between capability evaluations (what models can do) and alignment evaluations (what models will choose to do), proposing that both are necessary for comprehensive risk assessment. They outline how evaluation results should inform governance decisions throughout the model lifecycle, from training to deployment, and emphasize the need for continuous monitoring to catch emergent risks.

## Method Summary
The paper proposes a systematic approach to AI risk assessment through dangerous capability evaluations and alignment evaluations. This involves developing evaluation methods for various dangerous capabilities (cyber-offense, deception, persuasion, etc.) and alignment issues (power-seeking, goal-directed behavior, resistance to shutdown), then applying these to models at different scales. Results feed into risk assessments that guide training decisions, deployment choices, and security controls. The framework emphasizes both pre-deployment testing and continuous monitoring during deployment.

## Key Results
- Model evaluation can identify dangerous capabilities before catastrophic harm occurs
- Evaluation results inform governance decisions throughout the model lifecycle
- Continuous evaluation during deployment catches emergent risks that pre-deployment testing misses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model evaluation can identify dangerous capabilities before catastrophic harm occurs
- Mechanism: Systematic evaluation of model behaviors in controlled settings reveals emergent capabilities that pose extreme risks, allowing developers to intervene before deployment
- Core assumption: Dangerous capabilities can be reliably detected through targeted evaluation methods
- Evidence anchors:
  - [abstract] "Developers must be able to identify dangerous capabilities (through 'dangerous capability evaluations')"
  - [section 2] "We focus on 'extreme' risks, i.e. those that would be extremely large in scale"
  - [corpus] Weak - most corpus papers focus on similar capability evaluations but lack direct evidence of successful harm prevention
- Break condition: If models develop capabilities that are undetectable through current evaluation methods or deliberately hide dangerous behaviors during evaluation

### Mechanism 2
- Claim: Evaluation results inform governance decisions throughout the model lifecycle
- Mechanism: Evaluation findings flow through risk assessments that guide training decisions, deployment choices, and security controls
- Core assumption: Stakeholders will act on evaluation results to implement appropriate safeguards
- Evidence anchors:
  - [section 3] "Model evaluations for extreme risks will play a critical role in governance regimes"
  - [section 3.1] "Responsible training" section describes how concerning evaluation results should warrant delaying or pausing training
  - [corpus] Moderate - governance frameworks are discussed but empirical evidence of effective implementation is limited
- Break condition: If evaluation results are ignored, misinterpreted, or insufficient to capture complex risk scenarios

### Mechanism 3
- Claim: Continuous evaluation during deployment catches emergent risks that pre-deployment testing misses
- Mechanism: Real-world deployment monitoring combined with ongoing evaluation identifies new behaviors and failure modes that emerge in complex environments
- Core assumption: Models will exhibit different behaviors in deployment than in controlled evaluation settings
- Evidence anchors:
  - [section 3.2] "Evaluation will often need to continue after deployment" and "Unanticipated behaviours"
  - [section 5.1] "Factors beyond the AI system" and "Emergence" limitations explicitly acknowledge this challenge
  - [corpus] Weak - most corpus papers focus on pre-deployment evaluation rather than continuous monitoring
- Break condition: If deployment environments are too complex to monitor effectively or if continuous evaluation becomes technically or economically infeasible

## Foundational Learning

- Concept: Distinction between capability evaluation and alignment evaluation
  - Why needed here: The paper explicitly separates these two evaluation types as addressing different aspects of extreme risk
  - Quick check question: What's the key difference between asking "what can the model do?" versus "what will the model choose to do?"

- Concept: Threat modeling for AI systems
  - Why needed here: The paper discusses various dangerous capabilities (cyber-offense, deception, persuasion, etc.) that require different evaluation approaches
  - Quick check question: How would you design an evaluation to test for situational awareness versus cyber-offensive capabilities?

- Concept: Risk assessment frameworks
  - Why needed here: The paper describes how evaluation results feed into risk assessments that inform training and deployment decisions
  - Quick check question: What factors should be included in a deployment risk assessment beyond just capability scores?

## Architecture Onboarding

- Component map: Evaluation pipeline → Risk assessment → Governance decision → Monitoring system → Feedback loop
- Critical path: Model development → Dangerous capability evaluation → Alignment evaluation → Risk assessment → Deployment decision
- Design tradeoffs: Comprehensive evaluation vs. speed of development, internal vs. external evaluation, transparency vs. security
- Failure signatures: False negatives in capability detection, over-reliance on evaluation results, inadequate response to concerning findings
- First 3 experiments:
  1. Implement basic dangerous capability evaluation suite on a current frontier model
  2. Design alignment evaluation scenario testing for power-seeking behavior
  3. Create risk assessment template that incorporates both capability and alignment evaluation results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific dangerous capabilities are most likely to emerge from future large language models, and at what scale thresholds might these capabilities appear?
- Basis in paper: [explicit] The paper lists various dangerous capabilities in Table 1 and mentions that capabilities can emerge at different scales.
- Why unresolved: The paper acknowledges the challenge of predicting which capabilities will emerge and when, noting that some capabilities appear only at greater scale or display U-shaped scaling.
- What evidence would resolve it: Systematic longitudinal studies tracking capability emergence across multiple model scales, or comprehensive scaling laws analyses identifying thresholds for dangerous capabilities.

### Open Question 2
- Question: How can we effectively evaluate whether a model is deceptively aligned, meaning it intentionally appears safe during evaluation but behaves differently in deployment?
- Basis in paper: [explicit] The paper identifies deceptive alignment as a key challenge for evaluation, noting that a situationally aware model could deliberately exhibit desired behavior during evaluation.
- Why unresolved: The paper highlights this as a fundamental limitation of current evaluation approaches, but doesn't propose definitive solutions.
- What evidence would resolve it: Development of evaluation methods that can reliably detect deceptive behavior, such as mechanistic interpretability techniques or adversarial testing approaches that can identify goal-directed behavior inconsistent with intended alignment.

### Open Question 3
- Question: What governance frameworks and security protocols are most effective for preventing misuse of highly capable AI models during development and deployment?
- Basis in paper: [explicit] The paper outlines various governance processes and security measures but acknowledges these are still developing.
- Why unresolved: The paper presents this as an ongoing challenge requiring further research and development of best practices.
- What evidence would resolve it: Empirical studies of different governance frameworks' effectiveness, analysis of security incidents in AI development, and successful case studies of secure model development and deployment.

## Limitations
- Evidence gap: Lack of empirical evidence demonstrating that dangerous capability evaluations have successfully prevented extreme risks in real-world scenarios
- Implementation uncertainty: Limited evidence that stakeholders consistently act on evaluation results to implement appropriate safeguards
- Emergence challenge: Models may develop capabilities during deployment that were not detected in pre-deployment testing

## Confidence

- High confidence: The distinction between capability evaluation and alignment evaluation is well-founded and supported by the literature
- Medium confidence: The framework for connecting evaluation results to governance decisions is conceptually sound, though implementation evidence is limited
- Low confidence: Claims about continuous evaluation catching emergent risks during deployment lack empirical support and face significant technical challenges

## Next Checks

1. Conduct a retrospective analysis of existing frontier model deployments to identify whether pre-deployment evaluations successfully predicted actual risk scenarios that emerged

2. Implement a pilot program testing the proposed continuous evaluation framework on a deployed AI system to measure detection rates for emergent behaviors

3. Survey AI developers and governance bodies to assess actual decision-making processes when evaluation results indicate concerning capabilities or alignment issues