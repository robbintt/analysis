---
ver: rpa2
title: The Quo Vadis of the Relationship between Language and Large Language Models
arxiv_id: '2310.11146'
source_url: https://arxiv.org/abs/2310.11146
tags:
- language
- llms
- human
- about
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large Language Models (LLMs) are increasingly being adopted as\
  \ scientific models of human language. This paper examines whether LLMs can provide\
  \ credible insights into the target system they seek to represent\u2014human language."
---

# The Quo Vadis of the Relationship between Language and Large Language Models

## Quick Facts
- arXiv ID: 2310.11146
- Source URL: https://arxiv.org/abs/2310.11146
- Reference count: 9
- Primary result: LLMs fall short as transparent and reliable scientific models of human language

## Executive Summary
Large Language Models are increasingly adopted as scientific models of human language, but this paper argues they fail to provide credible insights into language at their current stage of development. The authors analyze LLMs through the lens of scientific modeling components - object, medium, meaning, and user - and conclude that their opacity, inconsistency, and inability to capture hierarchical structure undermine their explanatory power. While LLMs may demonstrate some formal linguistic competence, they lack the transparency and consistency required for reliable scientific inference about human language.

## Method Summary
The paper analyzes Large Language Models as scientific models of human language by examining their transparency, reliability, and explanatory power. The authors review existing literature on LLM performance, training practices, and linguistic capabilities rather than conducting new experiments. They evaluate LLMs against criteria for scientific models, including transparency of training data, consistency of outputs, and ability to capture hierarchical linguistic structure. The analysis focuses on comparing LLM behavior with established linguistic theory and human language processing.

## Key Results
- LLMs lack transparency in training data and representation, making it impossible to trace model outputs to their source
- LLM outputs show inconsistent rule application that fails to match human linguistic competence
- LLMs process language as sequences rather than capturing the hierarchical, compositional structure of human language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs lack transparency in training data and representation, undermining their reliability as scientific models of language.
- Mechanism: Without knowing what data constitutes the training set or how it is represented internally, users cannot trace model outputs back to their source, making it impossible to evaluate or replicate findings.
- Core assumption: Scientific models require transparent, traceable components to be informative about their target system.
- Evidence anchors:
  - [abstract] "LLMs hardly offer any explanations for language... the precise origin of these claims is not clear."
  - [section] "the precise nature of the training data remains private... absence of transparency... entails that an accurate estimation of the risks may not be possible yet."
  - [corpus] Weak correlation; related papers focus on broader LLM adoption, not transparency issues.
- Break condition: If training data were made fully public and representation methods disclosed, this mechanism would no longer apply.

### Mechanism 2
- Claim: LLMs generate inconsistent outputs that fail to align with human linguistic competence.
- Mechanism: LLMs apply rules stochastically rather than consistently, producing errors outside the human error domain and failing to generalize from training data.
- Core assumption: Human language competence is characterized by consistent rule application and generalization.
- Evidence anchors:
  - [section] "LLMs seem to apply the rules selectively... they are stochastic parrots... failures to consistently detect attraction errors."
  - [section] "The output consists of sentences that pair tokens in ways that largely adhere to the rules... but from a qualitative point of view, how different is the synthetic text generated by LLMs from the standard linguistic productions of humans?"
  - [corpus] Moderate correlation; neighboring papers discuss LLM limitations but not consistency issues.
- Break condition: If LLMs could demonstrate consistent rule application matching human competence, this mechanism would fail.

### Mechanism 3
- Claim: LLMs fail to capture the hierarchical, compositional structure of human language.
- Mechanism: LLMs process language as sequences of tokens based on surface statistics rather than abstract hierarchical structures, missing core syntactic principles.
- Core assumption: Human language is fundamentally hierarchical and compositional, not linear.
- Evidence anchors:
  - [section] "language is not a linear system of strings. It is purely an abstract hierarchical organizational and re-formatting system..."
  - [section] "they generalized in a way more consistent with an incorrect linear rule than the correct hierarchical rule for English Yes/No questions."
  - [corpus] Weak correlation; neighboring papers focus on LLM applications, not structural linguistics.
- Break condition: If LLMs could reliably process hierarchical structure matching human syntactic competence, this mechanism would fail.

## Foundational Learning

- Concept: Scientific modeling requires transparency and replicability.
  - Why needed here: The paper critiques LLMs as scientific models of language, emphasizing that opacity undermines their explanatory value.
  - Quick check question: Can you identify what components of a scientific model must be transparent for it to be informative about its target system?

- Concept: Distinction between formal and functional linguistic competence.
  - Why needed here: The paper analyzes whether LLMs possess formal linguistic competence (syntax, morphology) versus functional competence (meaning, pragmatics).
  - Quick check question: How does the paper argue that LLMs succeed at formal competence but fail at functional competence?

- Concept: Hierarchical structure in human language versus sequence processing.
  - Why needed here: The paper emphasizes that human language is fundamentally hierarchical, while LLMs process sequences, leading to structural failures.
  - Quick check question: What evidence does the paper provide that LLMs fail to capture hierarchical structure in language?

## Architecture Onboarding

- Component map: Object (target system) -> Medium (representation) -> Meaning (content about target) -> User (agent using model)
- Critical path: Identify target system → Examine training data and representation → Analyze output consistency → Evaluate explanatory power → Formulate research questions
- Design tradeoffs: Transparency vs. commercial confidentiality; consistency vs. probabilistic generation; hierarchical structure vs. sequence processing efficiency
- Failure signatures: Inconsistent rule application; generation of non-human errors; inability to distinguish possible from impossible language; factual hallucinations
- First 3 experiments:
  1. Test LLM consistency on grammaticality judgments across multiple runs with identical inputs.
  2. Compare LLM next-word predictions with human predictions on semantically coherent but structurally complex sentences.
  3. Evaluate LLM ability to distinguish between possible and impossible syntactic constructions in controlled stimuli.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise threshold for determining when a model's inaccuracies become unacceptable for scientific use in linguistics?
- Basis in paper: [explicit] The paper discusses the need to identify risks and evaluate their impact, but notes that tools for adequate evaluation are often lacking.
- Why unresolved: The paper argues that there is no clear threshold for harmless misinformation caused by hallucinating models, and the degree to which challenges impact generated content needs to be measured.
- What evidence would resolve it: Systematic testing across different models and languages to establish quantifiable metrics for acceptable versus unacceptable error rates in linguistic outputs.

### Open Question 2
- Question: How can Large Language Models be modified to capture the hierarchical structure of language that is essential for human-like language processing?
- Basis in paper: [explicit] The paper highlights that LLMs struggle with core linguistic properties like compositionality, structure-dependence, and hierarchical organization, and suggests the need for syntactic priors and inductive biases.
- Why unresolved: Current LLMs rely on sequence processing and lack the built-in recursive syntactic composition operations that would enable them to generalize in a way consistent with human language acquisition and processing.
- What evidence would resolve it: Experimental results showing that LLMs with augmented syntactic structure-sensitivity biases can reliably distinguish between possible and impossible language, and make predictions about human parsing that align with actual human behavior.

### Open Question 3
- Question: What are the implications of the disconnect between formal linguistic competence and functional competence for the development of Large Language Models?
- Basis in paper: [explicit] The paper discusses the distinction between formal and functional competence, arguing that LLMs may excel at the former but fall short of the latter, and that this dissociation is problematic for modeling human language.
- Why unresolved: The paper suggests that the integration of formal and functional competence in neurotypical functioning is proof of the limitations of considering them as separate modules, but does not provide a clear path forward for addressing this in LLMs.
- What evidence would resolve it: Research demonstrating that LLMs with integrated formal and functional competence can perform tasks that require both abstract rule application and real-world knowledge, and that this integration leads to more human-like language outputs.

## Limitations

- Analysis relies heavily on existing literature rather than direct experimental validation
- Cannot account for potential future improvements in LLM architecture that might address current limitations
- The distinction between formal and functional competence may not map cleanly onto actual LLM capabilities

## Confidence

- High confidence: The claim that LLMs lack transparency in training data and representation is well-supported by industry practices and the absence of public documentation about training corpora.
- Medium confidence: The argument about inconsistent output generation is plausible given the stochastic nature of LLMs, but direct empirical validation across multiple model versions would strengthen this claim.
- Low confidence: The assertion that LLMs fundamentally fail to capture hierarchical structure is compelling but difficult to verify without access to model internals and controlled experiments.

## Next Checks

1. Conduct controlled experiments testing LLM consistency on grammaticality judgments across multiple runs with identical inputs, documenting variability patterns
2. Design a standardized framework for quantifying the "mismatch" between LLM outputs and human language, including hallucination detection and semantic frame analysis
3. Test whether fine-tuning on transparently sourced, linguistically curated data improves LLM performance on hierarchical structure tasks while maintaining functional competence