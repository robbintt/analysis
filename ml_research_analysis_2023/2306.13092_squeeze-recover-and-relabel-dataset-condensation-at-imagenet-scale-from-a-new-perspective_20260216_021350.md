---
ver: rpa2
title: 'Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From
  A New Perspective'
arxiv_id: '2306.13092'
source_url: https://arxiv.org/abs/2306.13092
tags:
- data
- training
- dataset
- synthetic
- imagenet-1k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SRe2L, a novel dataset condensation framework
  that decouples model training from synthetic data optimization to handle large-scale
  datasets. The approach involves three stages: squeezing (extracting information
  from original data), recovering (synthesizing data with BN statistics alignment),
  and relabeling (generating soft labels via multi-crop optimization).'
---

# Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective

## Quick Facts
- arXiv ID: 2306.13092
- Source URL: https://arxiv.org/abs/2306.13092
- Reference count: 40
- Key outcome: Achieves 60.8% validation accuracy on ImageNet-1K with 50 images per class, outperforming state-of-the-art by 32.9%

## Executive Summary
SRe2L introduces a novel dataset condensation framework that decouples model training from synthetic data optimization to handle large-scale datasets. The approach involves three sequential stages: squeezing (extracting information from original data), recovering (synthesizing data with BN statistics alignment), and relabeling (generating soft labels via multi-crop optimization). This decoupling strategy enables effective ImageNet-1K condensation at 224×224 resolution while maintaining state-of-the-art performance. The method demonstrates 16× speedup and 6.4× memory reduction compared to MTT, along with superior synthetic image quality featuring clearer semantic information.

## Method Summary
SRe2L is a three-stage dataset condensation framework that decouples the bilevel optimization process. First, a base model is trained on the full dataset (Squeeze stage). Second, this pretrained model is used to optimize synthetic data by aligning their BN statistics (Recover stage), using multi-crop augmentation to enhance informativeness. Finally, the synthetic data are relabeled with soft labels generated from the recovery model and trained to convergence (Relabel stage). This sequential approach eliminates costly unrolled iterations while maintaining performance, enabling effective condensation of ImageNet-1K at conventional resolution.

## Key Results
- Achieves 60.8% validation accuracy on ImageNet-1K with 50 images per class
- Outperforms state-of-the-art by 32.9% on the same task
- Demonstrates 16× faster runtime and 6.4× less memory usage compared to MTT
- Shows superior synthetic image quality with clearer semantic information than previous methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decoupling bilevel optimization into three sequential stages reduces computational overhead while maintaining performance.
- **Mechanism**: By first training a model on the full dataset, then optimizing synthetic data independently using this fixed model, and finally relabeling, SRe2L avoids costly simultaneous updates of synthetic data and model weights.
- **Core assumption**: A fixed pretrained model contains sufficient information to guide synthetic data synthesis without needing joint optimization.
- **Evidence anchors**: Abstract states decoupling handles varying scales effectively; paper shows state-of-the-art performance at 224×224 resolution.
- **Break condition**: If the pretrained model fails to capture essential dataset characteristics, synthetic data recovery will be poor.

### Mechanism 2
- **Claim**: BN statistics alignment provides a more stable and scalable recovery objective than full feature matching.
- **Mechanism**: Synthetic data are optimized to match the running mean and variance of BN layers from the pretrained model, providing a global dataset-level alignment rather than per-batch matching.
- **Core assumption**: BN statistics encode sufficient distributional information to guide synthetic data synthesis toward realistic and informative samples.
- **Evidence anchors**: Paper claims BN-matching significantly outperforms feature-matching; BN-based matching appears to be a novel contribution.
- **Break condition**: If the model lacks BN layers, special adaptations are needed; incorrect BN stats alignment leads to poor synthetic data quality.

### Mechanism 3
- **Claim**: Multi-crop optimization enhances synthetic image informativeness by focusing updates on cropped regions.
- **Mechanism**: Random crops are extracted during each iteration, resized to target resolution, and only the cropped region is updated, encouraging synthetic images to contain multiple informative patches.
- **Core assumption**: Multiple cropped views capture diverse aspects of class semantics, improving label fidelity.
- **Evidence anchors**: Paper reports notable validation accuracy improvement with multi-crop; this appears to be a novel contribution.
- **Break condition**: If cropped regions are too small or uninformative, optimization may fail to recover meaningful class features.

## Foundational Learning

- **Concept**: Batch Normalization (BN) statistics
  - **Why needed here**: BN running mean and variance serve as the alignment target during synthetic data recovery, replacing full feature matching.
  - **Quick check question**: What are the two statistics tracked by BN layers that SRe2L aligns during the recovery stage?

- **Concept**: Bilevel optimization
  - **Why needed here**: Understanding bilevel optimization is key to grasping why SRe2L decouples the process and how this reduces computational cost.
  - **Quick check question**: In conventional dataset condensation, what are the inner and outer loops of bilevel optimization?

- **Concept**: Knowledge distillation and soft labels
  - **Why needed here**: The relabeling stage uses soft labels generated by a teacher model to better align synthetic data with true class semantics.
  - **Quick check question**: How does temperature scaling affect the softness of the labels used for relabeling synthetic data?

## Architecture Onboarding

- **Component map**: Data Loader → Squeeze Model → Recover Synthetic Data (BN-Matching) → Relabel (Soft Labels) → Final Training

- **Critical path**: 
  1. Squeeze: Train base model on full dataset
  2. Recover: Optimize synthetic data to match BN statistics of squeezed model
  3. Relabel: Generate soft labels for synthetic data and train final model

- **Design tradeoffs**:
  - Squeeze budget vs. synthetic data quality: Longer squeeze training makes recovery harder
  - Recovery iterations vs. memory/time: More iterations improve accuracy but increase cost
  - Temperature for soft labels: Higher temperature improves training but may reduce label sharpness

- **Failure signatures**:
  - Synthetic images lack semantic clarity → likely insufficient recovery budget or poor BN alignment
  - Validation accuracy plateaus early → possible overfitting to recovery model or inadequate relabeling
  - Memory spikes → likely retaining full model states instead of only BN stats

- **First 3 experiments**:
  1. Run Squeeze stage on Tiny-ImageNet with ResNet-18; verify Top-1 accuracy meets baseline
  2. Execute Recover stage with BN matching only; check synthetic data diversity and label consistency
  3. Combine Squeeze + Recover + Relabel; compare validation accuracy to MTT baseline on Tiny-ImageNet IPC=50

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding scalability to larger datasets (ImageNet-21K, JFT-300M), the theoretical limits of condensation performance, and the relationship between dataset scale and architecture scaling. While the authors demonstrate successful ImageNet-1K condensation, they acknowledge that extending to datasets orders of magnitude larger remains an open challenge. The paper also notes the need for deeper theoretical understanding of why certain architectural choices (like BN statistics alignment) prove particularly effective for dataset condensation tasks.

## Limitations

- The method's scalability to datasets larger than ImageNet-1K (e.g., JFT-300M) remains unproven
- Performance gains depend heavily on the quality of the pretrained model from the Squeeze stage
- The approach requires BN layers, necessitating special adaptations for transformer-based architectures like ViT
- Computational advantages are specific to ImageNet-1K scale and may not translate proportionally to smaller or larger datasets

## Confidence

- **High confidence**: The three-stage framework architecture and specific performance numbers on ImageNet-1K with 50 IPC (60.8% accuracy, 32.9% improvement)
- **Medium confidence**: The claim about computational efficiency gains (16× faster, 6.4× less memory) relative to MTT
- **Medium confidence**: The qualitative assessment of synthetic image quality improvements

## Next Checks

1. Test the decoupling approach on a smaller-scale dataset (e.g., CIFAR-100) to verify the three-stage process maintains its advantage when computational burden is less severe
2. Evaluate synthetic data quality using established metrics like FID (Fréchet Inception Distance) to quantify qualitative improvements in semantic information
3. Conduct ablation studies on the BN statistics alignment component by comparing against feature matching on the same architecture to isolate the contribution of this specific innovation