---
ver: rpa2
title: 'Less is More: Understanding Word-level Textual Adversarial Attack via n-gram
  Frequency Descend'
arxiv_id: '2302.02568'
source_url: https://arxiv.org/abs/2302.02568
tags:
- examples
- adversarial
- training
- attacks
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why word-level textual adversarial attacks
  are effective by analyzing the n-gram frequency changes in adversarial examples.
  The authors find that in approximately 90% of cases, word-level attacks generate
  examples with lower n-gram frequencies (n-FD).
---

# Less is More: Understanding Word-level Textual Adversarial Attack via n-gram Frequency Descend

## Quick Facts
- arXiv ID: 2302.02568
- Source URL: https://arxiv.org/abs/2302.02568
- Reference count: 40
- Primary result: Word-level attacks consistently generate examples with lower n-gram frequencies, and training on such examples improves model robustness comparably to gradient-based adversarial training

## Executive Summary
This paper investigates why word-level textual adversarial attacks are effective by analyzing how they affect n-gram frequencies in generated examples. The authors discover that approximately 90% of word-level attacks produce examples with decreased n-gram frequencies (n-FD), with 2-grams showing the strongest tendency. They propose a novel adversarial training approach that generates examples by minimizing n-gram frequencies rather than maximizing loss, integrated into the convex hull framework. Experiments demonstrate that training on n-FD examples, particularly 2-FD, improves model robustness comparably to gradient-based adversarial training while providing a more interpretable perspective on textual attacks.

## Method Summary
The paper proposes a novel adversarial training approach based on n-gram frequency descent (n-FD). The method involves computing n-gram frequencies in the training corpus, then generating adversarial examples by iteratively replacing words to minimize n-gram frequency changes. This approach is integrated into the convex hull framework, where the model learns to handle less frequent n-gram patterns. The authors use a greedy algorithm to find suboptimal examples by minimizing frequency changes, and train models on augmented datasets containing both clean and n-FD examples. They evaluate performance on IMDb and AG-News datasets using CNN, LSTM, and BERT architectures, measuring both clean accuracy and robust accuracy against word-level attacks.

## Key Results
- Word-level attacks lead to n-gram frequency descent in approximately 90% of cases, with 2-grams showing the strongest tendency
- Training models on n-FD examples (particularly 2-FD) improves robustness comparably to gradient-based adversarial training
- 2-gram frequency minimization provides the best balance between attack interpretation and robustness improvement across different model architectures
- The n-FD approach successfully addresses the vulnerability of models to low-frequency n-grams in adversarial examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples generated by word-level attacks tend to have lower n-gram frequencies (n-FD), which exploits model instability on less frequent sequences.
- Mechanism: When attackers replace words, they often choose substitutes that create n-grams with lower frequency in the training corpus, making the model less confident in its predictions.
- Core assumption: NLP models are less stable when processing n-grams with low frequency in the training data, creating vulnerability to attacks.
- Evidence anchors:
  - [abstract]: "word-level attacks lead to the generation of examples where the frequency of n-grams decreases, a tendency we term as the n-gram Frequency Descend (n-FD)"
  - [section 2.3]: "Experimental results show that all attacks show a strong tendency toward generation of examples exhibiting n-gram frequency descend (n-FD)"
  - [corpus]: Weak - corpus contains papers about adversarial attacks but none specifically about n-gram frequency analysis
- Break condition: If models are trained on balanced n-gram distributions or become invariant to n-gram frequency, this mechanism would fail.

### Mechanism 2
- Claim: Training models on n-FD examples improves robustness by exposing the model to less frequent n-gram patterns.
- Mechanism: By generating adversarial examples that minimize n-gram frequencies instead of maximizing loss, the model learns to handle less frequent sequences better.
- Core assumption: Exposure to adversarial examples during training helps models generalize better to out-of-distribution inputs.
- Evidence anchors:
  - [abstract]: "training models using examples with n-FD... performs comparably with the gradient-based approach in improving model robustness"
  - [section 3.4]: "Both ADV-G and ADV-F can effectively improve the robustness of model. Additionally, ADV-F2 achieves competitive performance with ADV-G"
  - [corpus]: Weak - corpus contains general adversarial training papers but none specifically about n-gram frequency-based adversarial training
- Break condition: If the n-FD examples do not represent realistic adversarial patterns or if the model overfits to the specific frequency patterns, this approach would fail.

### Mechanism 3
- Claim: 2-gram frequency is the optimal choice for both interpreting attacks and improving robustness.
- Mechanism: 2-grams provide the best balance between capturing local context and avoiding the sparsity issues of higher-order n-grams.
- Core assumption: 2-grams capture meaningful linguistic patterns while remaining frequent enough to be useful for both analysis and training.
- Evidence anchors:
  - [abstract]: "when n = 2, n-FD has the best interpretation ability of textual attacks"
  - [section 2.3]: "2-FD examples achieve better coverage compared with other cases"
  - [section 3.4]: "ADV-F2 gets consistently better performance than ADV-F1 in robustness"
  - [corpus]: Weak - corpus contains papers about n-grams in general but none specifically comparing different n-gram orders for adversarial analysis
- Break condition: If the linguistic structure of the target domain favors different n-gram orders, or if the training corpus distribution changes significantly, 2-grams might not remain optimal.

## Foundational Learning

- Concept: n-gram frequency analysis
  - Why needed here: The paper's core insight is that adversarial examples tend to have lower n-gram frequencies, so understanding how to calculate and use n-gram frequencies is essential
  - Quick check question: How would you compute the frequency of the 2-gram "the cat" in a given corpus?

- Concept: Adversarial training
  - Why needed here: The paper proposes a novel adversarial training approach based on n-gram frequency minimization, which builds on standard adversarial training concepts
  - Quick check question: What is the difference between standard adversarial training and the n-FD adversarial training proposed in this paper?

- Concept: Convex hull framework for adversarial training
  - Why needed here: The paper integrates the n-FD approach into the convex hull framework, so understanding how this framework works is crucial
  - Quick check question: How does the convex hull framework differ from traditional adversarial training approaches?

## Architecture Onboarding

- Component map: Data pipeline -> n-gram frequency computation -> Adversarial example generation -> Model training -> Evaluation -> Analysis
- Critical path: Data → n-gram frequency computation → Adversarial example generation → Model training → Evaluation → Analysis
- Design tradeoffs:
  - n-gram order selection: Higher n captures more context but suffers from sparsity; 2-grams provide a good balance
  - Frequency source: Using training corpus frequencies vs. external corpus; training corpus is more relevant but may be smaller
  - Update frequency: Updating n-gram frequencies during training vs. using static frequencies; dynamic updates are more accurate but computationally expensive
- Failure signatures:
  - If n-FD examples don't improve robustness, check if the frequency computation is correct or if the model architecture is unsuitable
  - If 2-grams don't perform better than other n-gram orders, verify the corpus statistics and consider domain-specific linguistic patterns
  - If the approach is too slow, consider frequency approximation techniques or limiting the n-gram vocabulary size
- First 3 experiments:
  1. Verify that generated adversarial examples consistently have lower n-gram frequencies than clean examples
  2. Compare model performance on n-FD examples vs. clean examples to confirm the vulnerability
  3. Test n-FD adversarial training against standard adversarial training on a simple dataset to confirm the robustness improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of 2-grams make them uniquely effective at explaining adversarial examples compared to other n-gram values?
- Basis in paper: [explicit] The authors observe that 2-FD examples achieve better coverage and interpretation ability than 1-FD, 3-FD, or 4-FD examples, but do not provide a theoretical explanation for why n=2 is optimal.
- Why unresolved: The paper only presents empirical observations without investigating the linguistic or statistical properties that make 2-grams special for adversarial attack interpretation.
- What evidence would resolve it: Controlled experiments comparing linguistic patterns, frequency distributions, and model sensitivity across different n-gram values would help explain why 2-grams are uniquely effective.

### Open Question 2
- Question: How does the convex hull framework's use of fractional n-gram frequencies compare to discrete n-gram frequency counting in terms of adversarial example generation quality?
- Basis in paper: [inferred] The authors use fractional methods to calculate n-gram frequency changes in the convex hull framework, but do not compare this approach to traditional discrete counting methods.
- Why unresolved: The paper does not benchmark the fractional frequency approach against discrete counting, leaving uncertainty about whether this approximation affects the quality of generated adversarial examples.
- What evidence would resolve it: Direct comparisons of adversarial example quality metrics (attack success rate, perturbation stealth) between fractional and discrete n-gram frequency counting approaches would clarify the impact of this methodological choice.

### Open Question 3
- Question: What is the relationship between n-gram frequency distributions in adversarial examples and model robustness across different NLP architectures?
- Basis in paper: [explicit] The authors show that training on n-FD examples improves robustness, with 2-FD and 3-FD being most effective, but do not investigate how different model architectures respond to various n-gram frequency distributions.
- Why unresolved: The paper focuses on empirical results across three architectures without analyzing how the internal representations of different models interact with n-gram frequency patterns.
- What evidence would resolve it: Analysis of feature representations and attention patterns in different architectures when processing n-FD examples would reveal how model structure influences the relationship between n-gram frequencies and robustness.

## Limitations

- The paper does not fully explain why word-level attacks consistently produce n-FD examples, only observing this phenomenon empirically without investigating the underlying mechanism
- The optimal choice of 2-grams is demonstrated empirically but lacks theoretical justification for why this specific n-gram order is superior to others
- The connection between n-gram frequency and model robustness could be more thoroughly explored, particularly in terms of how frequency relates to the model's learned representations and decision boundaries

## Confidence

- **High confidence**: The empirical observation that word-level attacks produce n-FD examples (approximately 90% of cases) is well-supported by experiments across multiple attack methods, datasets, and model architectures. The effectiveness of n-FD adversarial training in improving robustness is also convincingly demonstrated.
- **Medium confidence**: The claim that 2-gram frequency provides the optimal balance for both attack interpretation and robustness improvement is supported by experimental results, but the paper does not provide theoretical justification for why 2-grams are superior to other n-gram orders.
- **Medium confidence**: The assertion that training on n-FD examples improves robustness by exposing models to less frequent n-gram patterns is plausible given the evidence, but the exact mechanism linking frequency exposure to improved generalization remains somewhat speculative.

## Next Checks

1. **Mechanism validation**: Conduct ablation studies to determine whether the n-FD phenomenon is inherent to word-level attacks or dependent on specific implementation details (e.g., vocabulary constraints, synonym selection strategies).
2. **Generalization testing**: Evaluate whether n-FD adversarial training improves robustness to attacks that specifically target high-frequency n-grams or use different perturbation strategies beyond word substitution.
3. **Linguistic analysis**: Perform a detailed linguistic analysis of n-FD examples to determine whether the frequency descent corresponds to meaningful semantic shifts or merely statistical artifacts of the substitution process.