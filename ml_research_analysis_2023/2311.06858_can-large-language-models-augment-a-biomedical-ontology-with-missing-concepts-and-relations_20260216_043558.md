---
ver: rpa2
title: Can Large Language Models Augment a Biomedical Ontology with missing Concepts
  and Relations?
arxiv_id: '2311.06858'
source_url: https://arxiv.org/abs/2311.06858
tags:
- mindfulness-basedstressreduction
- affects
- subclassof
- treats
- psychosocial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) to
  augment the SNOMED-CT biomedical ontology with missing concepts and relations. The
  researchers used conversational interactions with ChatGPT to analyze clinical practice
  guidelines and detect relationships among new medical concepts not present in SNOMED-CT.
---

# Can Large Language Models Augment a Biomedical Ontology with missing Concepts and Relations?

## Quick Facts
- arXiv ID: 2311.06858
- Source URL: https://arxiv.org/abs/2311.06858
- Reference count: 3
- Key outcome: LLM found 14 of 52 gold standard relations (precision 0.63, recall 0.58) while also identifying 3 new concepts and 39 new relations

## Executive Summary
This study investigates whether large language models can semi-automatically extend the SNOMED-CT biomedical ontology by identifying missing concepts and relations from clinical practice guidelines. Using ChatGPT to analyze a fatigue guideline, researchers prompted the model to extract concepts and generate triples that could extend the ontology. Domain experts evaluated the results against a manually created gold standard, finding that while the LLM captured some existing relationships, it also suggested novel additions that were subsequently incorporated into the gold standard.

## Method Summary
The researchers extracted four subsections from a fatigue guideline discussing non-pharmacological treatments and used ChatGPT to identify concepts and build triples (Concept1 relation Concept2) using UMLS semantic network relations. They prompted ChatGPT 10 times with the same prompt, accepting only concepts and triples appearing in at least 6 runs. The results were post-processed by reversing inverse relations, converted to OWL format, and evaluated by three domain experts against a manually generated gold standard. Precision and recall were calculated for four difficulty levels based on concept presence in SNOMED-CT and relation types.

## Key Results
- The LLM identified 14 of 52 relations present in the gold standard
- It also discovered 3 new concepts and 39 new relations that were added to the gold standard
- Overall precision was 0.63 and recall was 0.58 across all difficulty levels
- Performance varied significantly across the four difficulty levels, with Level 1 (all concepts in SNOMED-CT) achieving higher precision than more challenging levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can augment biomedical ontologies by identifying missing concepts and relations from clinical guidelines.
- Mechanism: LLMs process unstructured text from clinical guidelines to extract explicit and implicit concepts and relationships, which are then formalized into triples for ontology extension.
- Core assumption: LLMs can understand domain-specific context and generate semantically correct triples from clinical text.
- Evidence anchors:
  - [abstract]: "We explore the potential of large language models (LLM) to expand an existing ontology in a semi-automated fashion."
  - [section]: "We prompted ChatGPT 10 times... We accepted as results, only the concepts and the triples that appeared in at least 6 of the 10 runs."
  - [corpus]: Weak - no direct evidence from corpus; relies on manual prompting results.

### Mechanism 2
- Claim: Using conversational interactions with LLMs improves the quality of ontology extension by allowing iterative refinement.
- Mechanism: Multiple prompts with different formulations (including inverse relations and examples) help capture a wider range of relationships and reduce bias in relation selection.
- Core assumption: LLMs respond differently to prompt variations, and multiple attempts increase coverage of valid triples.
- Evidence anchors:
  - [abstract]: "Our initial experimentation with the conversational prompts yielded promising preliminary results."
  - [section]: "We prompted ChatGPT 10 times... Because ChatGPT returns different results when prompted with the same prompt several times..."
  - [corpus]: Weak - corpus does not directly support this mechanism; evidence is from experimental setup.

### Mechanism 3
- Claim: Partitioning results by difficulty levels helps identify LLM strengths and weaknesses in ontology extension.
- Mechanism: By categorizing triples into four levels based on concept presence in SNOMED-CT and relation type, researchers can pinpoint where LLMs perform best and target improvements.
- Core assumption: LLM performance varies systematically across different types of ontology extension tasks.
- Evidence anchors:
  - [section]: "We partitioned the triples found by ChatGPT into 4 levels of difficulty for the LLM and for a human expert..."
  - [section]: "Table 1 present the results when we partition the triples into the 4 levels of difficulty."
  - [corpus]: No direct evidence; mechanism derived from study design.

## Foundational Learning

- Concept: Biomedical ontologies and their role in healthcare
  - Why needed here: Understanding SNOMED-CT and UMLS semantic network is crucial for evaluating ontology extension tasks.
  - Quick check question: What is the difference between SNOMED-CT and UMLS semantic network in terms of relation coverage?

- Concept: Large language models and their capabilities
  - Why needed here: Knowing how LLMs process text and generate responses is essential for designing effective prompts and interpreting results.
  - Quick check question: How does temperature setting in LLMs affect the consistency of generated responses?

- Concept: Ontology evaluation metrics (precision, recall)
  - Why needed here: Assessing the quality of LLM-generated ontology extensions requires understanding these metrics and their implications.
  - Quick check question: If an LLM suggests 50 triples and 30 are correct, what are the precision and recall if the gold standard contains 40 triples?

## Architecture Onboarding

- Component map: Clinical guidelines -> Concept extraction -> Triple generation -> Post-processing -> OWL format -> Expert evaluation
- Critical path:
  1. Extract concepts from clinical guidelines
  2. Generate triples using LLM with appropriate prompts
  3. Post-process triples (reverse inverse relations)
  4. Convert to OWL format
  5. Expert evaluation and gold standard comparison
- Design tradeoffs:
  - Multiple LLM runs vs. consistency: Running 10 times increases coverage but introduces variability
  - Manual vs. automated concept extraction: Manual extraction ensures quality but limits scalability
  - Inverse relations inclusion: Captures more relationships but requires post-processing
- Failure signatures:
  - Low precision (<0.5) indicates LLM is suggesting too many invalid relationships
  - Low recall (<0.5) suggests LLM is missing many valid relationships
  - Inconsistent results across runs indicate temperature or model instability
- First 3 experiments:
  1. Test single vs. multiple LLM runs to quantify consistency improvement
  2. Compare different prompt formulations (with/without examples, inverse relations)
  3. Evaluate different clinical guidelines to assess domain transferability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs specifically trained on biomedical texts compare to general-purpose LLMs like ChatGPT for ontology extension tasks?
- Basis in paper: [explicit] The authors mention exploring "LLMs trained on biomedical texts, such as BioMedML" as an intriguing direction for future work.
- Why unresolved: The study only tested general-purpose LLMs, leaving the performance of biomedical-specific LLMs unknown.
- What evidence would resolve it: Direct comparison experiments using both general-purpose and biomedical-specific LLMs on the same ontology extension task with identical evaluation metrics.

### Open Question 2
- Question: What is the impact of prompt formulation strategies on LLM performance for ontology extension?
- Basis in paper: [explicit] The authors describe an iterative prompt formulation process involving trial and error with different prompt variations.
- Why unresolved: The study didn't systematically test different prompt strategies or provide a framework for optimal prompt design.
- What evidence would resolve it: Controlled experiments testing various prompt formulations (context inclusion, relation specification, examples provided) with systematic evaluation of their impact on precision and recall.

### Open Question 3
- Question: Can the ontology extension process be improved by integrating concept extraction within the LLM pipeline?
- Basis in paper: [inferred] The authors suggest "incorporating the concept extraction process within the suggestion pipeline, potentially utilizing named-entity-recognition models."
- Why unresolved: The current approach treats concept extraction and relation identification as separate steps without exploring integrated approaches.
- What evidence would resolve it: Comparative experiments testing integrated versus separate approaches for concept extraction and relation identification, measuring improvements in overall performance metrics.

## Limitations

- The evaluation by domain experts is subjective and could introduce bias in labeling true positives and false negatives
- The study relies on a single clinical guideline (fatigue management), limiting generalizability to other medical domains
- The exact prompts used with ChatGPT are not fully specified, making exact replication difficult
- The use of ChatGPT-3.5-turbo without specifying the model version raises concerns about result stability across model updates

## Confidence

- High confidence in the experimental methodology and evaluation framework
- Medium confidence in the quantitative results due to potential subjectivity in expert evaluation
- Low confidence in the generalizability of findings beyond the specific clinical domain tested

## Next Checks

1. Replicate the experiment with multiple clinical guidelines from different medical specialties to assess domain transferability
2. Conduct inter-rater reliability analysis by having multiple independent expert panels evaluate the same LLM outputs
3. Test the approach with different LLMs (e.g., Claude, GPT-4) and compare consistency of results across models