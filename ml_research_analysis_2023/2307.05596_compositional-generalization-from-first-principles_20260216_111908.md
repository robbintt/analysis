---
ver: rpa2
title: Compositional Generalization from First Principles
arxiv_id: '2307.05596'
source_url: https://arxiv.org/abs/2307.05596
tags:
- compositional
- component
- generalization
- support
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes and studies compositional generalization
  in the context of supervised learning. Drawing inspiration from identifiable representation
  learning, the authors define a broad class of compositional data-generating processes
  that are shown to enable compositional generalization.
---

# Compositional Generalization from First Principles

## Quick Facts
- arXiv ID: 2307.05596
- Source URL: https://arxiv.org/abs/2307.05596
- Reference count: 40
- Key outcome: This paper formalizes and studies compositional generalization in supervised learning, deriving sufficient conditions on training distribution support and model architecture for compositional generalization to occur.

## Executive Summary
This paper presents a formal framework for compositional generalization in supervised learning, drawing inspiration from identifiable representation learning. The authors establish sufficient conditions on the training distribution's support and model architecture that guarantee compositional generalization when the composition function is known. Through theoretical analysis and synthetic experiments, they demonstrate that compositional generalization can be achieved when the full Jacobian of each component function can be reconstructed from the training data, enabling exact recovery of component functions even for novel compositions.

## Method Summary
The paper investigates compositional generalization using a synthetic multi-sprite dataset where images are generated by composing individual sprite renderings through a known composition function. Two models are compared: a compositional model that learns separate component functions and combines them, and a monolithic baseline that learns the entire mapping directly. The compositional model uses a soft pixel-wise addition with sigmoid function for composition to ensure training stability. Both models are trained on 100k samples for 2000 epochs on an NVIDIA RTX 2080 Ti, with evaluation on both in-domain and entire latent space test sets using MSE and variance-weighted R² scores.

## Key Results
- Theoretical conditions on training distribution support and model architecture are sufficient for compositional generalization when the composition function is known
- Compositional models achieve perfect generalization to novel compositions of known components, while monolithic baselines fail
- Models can generalize compositionally even when sufficient support conditions are not strictly met, particularly with more complex composition functions like alpha compositing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A model can generalize compositionally if the composition function C and training support P are chosen such that the full Jacobian of each component function can be reconstructed everywhere.
- Mechanism: When C is known, the Jacobian of the composite function f can be decomposed into contributions from each component function's Jacobian. If the support P is broad enough, the Jacobian of the composite can be summed over multiple samples with fixed component latents, yielding full-rank Jacobians for each component function. This allows reconstruction of each component function's Jacobian everywhere, and by integration, the component functions themselves.
- Core assumption: The composition function C is known and differentiable, and the training support P has sufficient variability to ensure full-rank Jacobians when summed over subsets of P with fixed component latents.
- Evidence anchors:
  - [abstract] "sufficient conditions on only the support of the training distribution and the model architecture are derived, which are sufficient for compositional generalization."
  - [section] "The composition function C and the support P must be chosen such that the full Jacobian can be reconstructed for each component function for all component latents."
  - [corpus] Weak evidence: The corpus mentions similar concepts like "Compositional generalization" and "Principled Out-of-Distribution Generalization" but does not directly reference Jacobian reconstruction.
- Break condition: If the composition function C is unknown or non-differentiable, or if the support P is too narrow to ensure full-rank Jacobians, reconstruction fails and compositional generalization is not guaranteed.

### Mechanism 2
- Claim: A model generalizes compositionally if it can perfectly fit the ground-truth data-generating process on the training distribution P, under the conditions of sufficient support and compositional support.
- Mechanism: If a model matches the ground-truth f on P, and P has compositional and sufficient support, then the model's component functions must match the ground-truth's component functions on Q (the test distribution). This is because the component functions can be reconstructed from the Jacobian on P, and compositional support ensures that any test point can be constructed from components seen in P.
- Core assumption: The model can perfectly fit the ground-truth on P, and the conditions of compositional and sufficient support are met.
- Evidence anchors:
  - [abstract] "sufficient conditions on only the support of the training distribution and the model architecture, which are sufficient for compositional generalization."
  - [section] "Assume the following assumptions hold: (A1) C, φk, ˆφk are differentiable, C is Lipschitz in φ, and φ is continuous in z. (A2) P has compositional support w.r.t.Q. (A3) P has sufficient support w.r.t.f. (A4) There exists an initial point p0 ∈ supp P such that φ(p0) = ˆφ(p0). Then ˆf generalizes to Q."
  - [corpus] Weak evidence: The corpus mentions "Compositional generalization" but does not directly reference the conditions of perfect fitting on P.
- Break condition: If the model cannot perfectly fit the ground-truth on P, or if the conditions of compositional and sufficient support are violated, the guarantee of generalization to Q does not hold.

### Mechanism 3
- Claim: The sufficient support condition can be relaxed if the composition function C is more complex (e.g., using alpha compositing) but may require additional assumptions.
- Mechanism: In the case of alpha compositing, the sufficient support condition as defined may not be met due to the presence of transparent pixels. However, the model may still generalize compositionally if it can learn to reconstruct the component functions from the observations, possibly by leveraging the structure of the composition function or additional inductive biases.
- Core assumption: The model can learn to reconstruct the component functions even when the sufficient support condition is not met, possibly due to the structure of the composition function or additional inductive biases.
- Evidence anchors:
  - [section] "Changing the output of each component function from RGB to RGBa and implementing the composition as alpha compositing yields a model that is still compositional, but for which no support can satisfy the sufficient support condition since the derivative of transparent pixels will always be zero and the Jacobian matrix can therefore never have full rank... However, we observe that the model still generalizes to the entire latent space and achieves even lower reconstruction error than the original model."
  - [corpus] Weak evidence: The corpus mentions "Compositional generalization" but does not directly reference the relaxation of the sufficient support condition.
- Break condition: If the model cannot learn to reconstruct the component functions from the observations, even with a more complex composition function, compositional generalization is not guaranteed.

## Foundational Learning

- Concept: Compositional data-generating process
  - Why needed here: Understanding the compositional nature of the data-generating process is crucial for deriving the conditions under which compositional generalization can occur.
  - Quick check question: Can you describe a simple compositional data-generating process, such as the multi-sprite example, and identify the component functions and composition function?

- Concept: Jacobian matrix and its role in function reconstruction
  - Why needed here: The Jacobian matrix of a function contains information about how the function changes with respect to its inputs, and it plays a key role in reconstructing component functions from the composite function.
  - Quick check question: Given a composite function f = C(φ1(z1), φ2(z2)), can you write down the Jacobian of f with respect to z1 and explain how it relates to the Jacobians of C and φ1?

- Concept: Sufficient support and compositional support
  - Why needed here: These conditions define the requirements on the training distribution for compositional generalization to occur, and they are central to the theoretical framework.
  - Quick check question: Can you explain the difference between sufficient support and compositional support, and provide an example of a training distribution that satisfies both conditions?

## Architecture Onboarding

- Component map:
  - Component functions: φ1, φ2, ..., φK (e.g., sprite renderers)
  - Composition function: C (e.g., pixel-wise addition or alpha compositing)
  - Model architecture: ˆf (e.g., a neural network that learns to reconstruct images given component latents)
  - Training distribution: P (e.g., a distribution over component latents with compositional and sufficient support)
  - Test distribution: Q (e.g., a distribution over component latents that contains novel combinations of known components)

- Critical path:
  1. Define the compositional data-generating process (component functions and composition function).
  2. Ensure the training distribution P has compositional and sufficient support.
  3. Train the model ˆf to fit the ground-truth data-generating process f on P.
  4. Verify that the model generalizes to Q (test distribution with novel combinations of known components).

- Design tradeoffs:
  - Choosing a more complex composition function (e.g., alpha compositing) may relax the sufficient support condition but may require a more complex model architecture.
  - Using a broader training support P may ensure sufficient support but may require more data and computational resources.
  - Assuming a known composition function C may simplify the problem but may not always be realistic.

- Failure signatures:
  - If the model fails to generalize to Q, check if the training distribution P has compositional and sufficient support.
  - If the model cannot perfectly fit the ground-truth on P, compositional generalization is not guaranteed.
  - If the composition function C is unknown or non-differentiable, reconstruction of component functions may fail.

- First 3 experiments:
  1. Train a model on a simple compositional dataset (e.g., multi-sprite) with a known composition function and broad training support, and verify that it generalizes to novel combinations of known components.
  2. Train a model on a compositional dataset with a more complex composition function (e.g., alpha compositing) and verify that it still generalizes compositionally, even if the sufficient support condition is not met.
  3. Train a model on a compositional dataset with insufficient training support (e.g., narrow diagonal support) and verify that it fails to generalize to novel combinations of known components.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sufficient support condition be relaxed in the presence of certain inductive biases in the model architecture?
- Basis in paper: [inferred] The paper mentions that models with an inductive bias towards shift invariance might be able to cope with certain gaps in the training support, and assuming all component functions are identical would substantially simplify the problem.
- Why unresolved: The current theoretical framework does not account for inductive biases in the model architecture. It would be valuable to understand how different architectural choices affect the sufficient conditions for compositional generalization.
- What evidence would resolve it: Experiments showing compositional generalization on datasets with gaps in support when using models with shift invariance or other relevant inductive biases, compared to models without these biases.

### Open Question 2
- Question: Is it possible to extend the theoretical framework to the unsupervised setting where ground-truth latents are not available?
- Basis in paper: [explicit] The paper explicitly states that it only studied a supervised regression setting and that extending to the unsupervised setting is an important avenue for future work.
- Why unresolved: The unsupervised setting introduces inherent ambiguities that make generalization guarantees harder to derive. The current framework relies on having access to ground-truth latents during training.
- What evidence would resolve it: A theoretical extension of the framework to the unsupervised setting, along with empirical validation on unsupervised compositional generalization benchmarks.

### Open Question 3
- Question: Can the sufficient conditions for compositional generalization be relaxed or alternative proof strategies be found that avoid the need for a known composition function or an initial point?
- Basis in paper: [inferred] The paper mentions that the composition function is assumed to be known and that the initial point condition is a technicality of the proof that is not reflected in the experiments. It also notes that the fact that a model with alpha compositing still generalizes well suggests there might be alternative proof strategies.
- Why unresolved: The current theoretical framework requires the composition function to be known and relies on the existence of an initial point for the proof. Relaxing these assumptions would make the framework more widely applicable.
- What evidence would resolve it: A theoretical extension of the framework that relaxes the assumption of a known composition function and the need for an initial point, along with empirical validation on datasets where the composition function is unknown or the initial point is not accessible.

## Limitations

- The theoretical framework relies heavily on the assumption that the composition function C is known and differentiable, which may not hold in many real-world scenarios.
- The sufficient support condition, while sufficient, may be overly restrictive for practical applications, particularly when dealing with more complex composition functions like alpha compositing.
- The empirical validation is limited to synthetic multi-sprite datasets, raising questions about the framework's applicability to more complex real-world data distributions.

## Confidence

- High Confidence: The theoretical derivation of sufficient conditions for compositional generalization (Mechanisms 1 and 2) is mathematically rigorous and well-supported by the formal framework.
- Medium Confidence: The empirical validation on synthetic datasets demonstrates the theoretical claims, but the limited scope of experiments reduces confidence in broader applicability.
- Medium Confidence: The relaxation of sufficient support conditions for complex composition functions (Mechanism 3) is observed empirically but lacks rigorous theoretical justification.

## Next Checks

1. Evaluate the framework's performance when the composition function C is unknown or non-differentiable, testing the limits of the theoretical assumptions.
2. Apply the compositional generalization framework to real-world datasets (e.g., object detection with occlusion) to assess practical applicability beyond synthetic data.
3. Investigate how approximate fitting of the ground-truth function on P (rather than perfect fitting) affects compositional generalization guarantees, bridging the gap between theory and practice.