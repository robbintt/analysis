---
ver: rpa2
title: 'Good regularity creates large learning rate implicit biases: edge of stability,
  balancing, and catapult'
arxiv_id: '2310.17087'
source_url: https://arxiv.org/abs/2310.17087
tags:
- learning
- rate
- large
- regularity
- sharpness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the occurrence of large learning rate phenomena,
  including the edge of stability, balancing, and catapult, in nonconvex optimization.
  The authors propose a new family of functions with varying degrees of regularity,
  quantified by the degree of regularity (dor).
---

# Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult

## Quick Facts
- arXiv ID: 2310.17087
- Source URL: https://arxiv.org/abs/2310.17087
- Reference count: 40
- Primary result: Large learning rate phenomena (edge of stability, balancing, catapult) occur when objective functions have good regularity (low degree of regularity)

## Executive Summary
This paper studies large learning rate phenomena in nonconvex optimization, proposing a new family of functions with varying degrees of regularity (dor). The authors show that edge of stability, balancing, and catapult effects are more likely to occur when functions have good regularity (small dor), as this concentrates sharpness near minima. They develop a global convergence theory for large learning rate gradient descent on this function family, a first for nonconvex functions without globally Lipschitz continuous gradients. Experiments on neural networks demonstrate that loss/activation functions and batch normalization significantly affect regularity and training dynamics.

## Method Summary
The authors define a family of functions with parameter-controlled regularity, specifically functions of the form f(x,y) = (log(exp(xy-1)+1) + log(exp(1-xy)+1))^a for 0 < a ≤ 1 (good regularity) and f(x,y) = (1-(xy)^b)^2 for b = 2n+1, n ∈ Z (bad regularity). They apply gradient descent with learning rates inversely proportional to the squared norm of initial conditions and analyze convergence behavior. The analysis focuses on how the degree of regularity affects the concentration of sharpness near minima and the occurrence of large learning rate phenomena. Neural network experiments use CIFAR-10 and MNIST datasets with various loss functions, activation functions, and batch normalization configurations.

## Key Results
- Large learning rate phenomena (edge of stability, balancing, catapult) are more likely when objective functions have good regularity
- The sharpness of good regularity functions concentrates near the minimum and vanishes far away
- Batch normalization can enhance model regularity, inducing large learning rate phenomena in cases where they would otherwise not occur

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Good regularity (low dor) makes large learning rate phenomena more likely by concentrating sharpness near the minimum.
- **Mechanism:** Low dor causes sharpness to concentrate near the minimum and vanish far away. Large learning rate GD escapes sharp regions and enters flatter ones, leading to phenomena like EoS, balancing, and catapult.
- **Core assumption:** Sharpness profile is tightly linked to degree of regularity, and large learning rate GD prefers flatter regions.
- **Evidence anchors:** [abstract] "These phenomena are more likely to occur when the objective function has good regularity." [section 4] "The sharpness of good regularity function concentrates near the minimum...while the situation with bad regularity function is the opposite."
- **Break condition:** High regularity (large dor) spreads sharpness widely, causing GD to converge to sharp minima without exhibiting large learning rate phenomena.

### Mechanism 2
- **Claim:** Large learning rates induce a preference for flatter minima, driving the edge of stability phenomenon.
- **Mechanism:** Large learning rates prevent convergence in sharp regions due to stability constraints. GD "searches" for flatter regions with smaller local Lipschitz constants. Once in a flat region, GD converges and sharpness stabilizes near 2/h.
- **Core assumption:** Learning rate is large enough to prevent sharp region convergence but not so large as to cause divergence.
- **Evidence anchors:** [abstract] "Large learning rates...yield various implicit biases including the edge of stability." [section 4] "GD tries to escape from the attraction of sharp minima and enters a flat region."
- **Break condition:** If learning rate is too small or too large, GD either converges to sharp minima or diverges, respectively, without exhibiting EoS.

### Mechanism 3
- **Claim:** Batch normalization enhances model regularity, inducing large learning rate phenomena.
- **Mechanism:** Batch normalization smoothes the landscape, reducing the degree of regularity. This allows GD to exhibit large learning rate phenomena even in models with originally high regularity.
- **Core assumption:** Batch normalization reduces the degree of regularity of the objective function.
- **Evidence anchors:** [abstract] "different choices of loss and activation functions, as well as batch normalization, can all affect regularity and lead to varying training dynamics." [section 3.2] "batch normalization...helps reduce the degree of regularity of the model."
- **Break condition:** If batch normalization doesn't effectively reduce degree of regularity, large learning rate phenomena may not occur in high regularity models.

## Foundational Learning

- **Concept:** Degree of regularity (dor)
  - Why needed here: To quantify function regularity and its impact on large learning rate phenomena.
  - Quick check question: What is the degree of regularity of the function f(s) = (1 - s^2)^2?

- **Concept:** Edge of stability (EoS)
  - Why needed here: To understand the phenomenon where sharpness stabilizes near 2/h under large learning rates.
  - Quick check question: What is the limiting sharpness of a function with good regularity under large learning rates?

- **Concept:** Balancing phenomenon
  - Why needed here: To understand how large learning rates induce preference for more balanced weights.
  - Quick check question: How does the balancing phenomenon relate to the sharpness of the optimization landscape?

## Architecture Onboarding

- **Component map:** Objective function (with degree of regularity) -> Gradient descent (with large learning rate) -> Training dynamics (including EoS, balancing, catapult) -> Optional batch normalization

- **Critical path:**
  1. Define objective function with specific degree of regularity
  2. Apply GD with large learning rate to optimize the function
  3. Analyze resulting training dynamics for large learning rate phenomena
  4. Optionally apply batch normalization to enhance regularity and induce phenomena

- **Design tradeoffs:**
  - Regularity vs. simplicity: Lower dor functions more likely to exhibit phenomena but may be more complex to analyze
  - Learning rate vs. stability: Larger learning rates more likely to induce phenomena but may cause instability
  - Batch normalization vs. original model: Can induce phenomena but may alter model behavior

- **Failure signatures:**
  - No large learning rate phenomena: High regularity function or insufficient learning rate
  - Instability or divergence: Learning rate too large or function not well-behaved
  - No balancing: High regularity or no batch normalization applied

- **First 3 experiments:**
  1. Define f(x, y) = (log(exp(xy-1)+1) + log(exp(1-xy)+1))^a with a=1 (good regularity) and apply GD with large learning rate. Analyze for edge of stability phenomenon.
  2. Define f(x, y) = (1-(xy)^b)^2 with b=3 (bad regularity) and apply GD with large learning rate. Analyze for absence of large learning rate phenomena.
  3. Apply batch normalization to experiment 2 and repeat analysis. Observe restoration of large learning rate phenomena.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between degree of regularity and sharpness of converged minimum for different activation functions in deep neural networks?
- Basis in paper: [explicit] The paper states that bad regularity can eliminate EoS and balancing, and that limiting sharpness is well below the stability edge of 2/h for functions with large regularities.
- Why unresolved: The paper provides theoretical framework for a specific function family but doesn't fully characterize the relationship for all activation functions in deep neural networks.
- What evidence would resolve it: Comprehensive study analyzing degree of regularity and sharpness across a wide range of activation functions in various neural network architectures with both theoretical analysis and empirical validation.

### Open Question 2
- Question: How does the depth of a neural network affect large learning rate phenomena like EoS and balancing?
- Basis in paper: [inferred] The paper focuses on 2-layer networks and mentions the current framework lacks depth analysis due to non-multivariate nature.
- Why unresolved: Extending current techniques to multivariate cases is highly nontrivial, and there might be additional consequences of increasing depth not captured by the current framework.
- What evidence would resolve it: Theoretical extension of current framework to handle multivariate cases, along with empirical studies on deep neural networks to validate predictions and explore additional phenomena.

### Open Question 3
- Question: What are the additional factors beyond regularity and symmetry that contribute to large learning rate phenomena in neural network training?
- Basis in paper: [explicit] The paper mentions regularity is sufficient and necessary for three implicit biases discussed, but may only be necessary beyond these functions. Symmetry could also help create these biases.
- Why unresolved: The paper doesn't fully explore other potential factors like bias involvement, multiple data points, or more complicated network architectures.
- What evidence would resolve it: Comprehensive study identifying and quantifying various factors' impact on large learning rate phenomena, including symmetry, bias, data points, and network architecture, with both theoretical analysis and empirical validation.

## Limitations

- The theoretical framework relies on specific function families with analytically tractable regularity properties, which may not generalize to arbitrary neural network loss landscapes.
- The analysis assumes deterministic gradient descent without stochasticity, potentially missing practical optimization dynamics.
- Theoretical bounds rely on specific initial conditions within Lebesgue measure-zero excluded sets, raising questions about practical applicability across diverse initializations.

## Confidence

- **High confidence:** Mathematical framework for defining and analyzing degree of regularity is rigorous and well-supported.
- **Medium confidence:** Global convergence theory for large learning rate GD on proposed function family is technically correct but may not generalize to practical neural network optimization.
- **Low confidence:** Claim that batch normalization universally reduces dor and induces large learning rate phenomena across diverse architectures requires more extensive empirical validation.

## Next Checks

1. **Empirical Generalization Test:** Implement the proposed function family (2) with varying a and b parameters and systematically measure degree of regularity, sharpness distribution, and occurrence of EoS/balancing phenomena across the parameter space to validate theoretical predictions.

2. **Neural Network Landscape Analysis:** For practical neural network architectures (CNNs, ResNets) trained on CIFAR-10/MNIST, measure the empirical degree of regularity of the loss landscape and correlate it with the occurrence of large learning rate phenomena across different loss functions, activation functions, and batch normalization configurations.

3. **Stochastic vs Deterministic Comparison:** Compare the deterministic convergence theory with stochastic gradient descent (SGD) experiments on the proposed function family and practical networks to quantify the impact of gradient noise on theoretical predictions.