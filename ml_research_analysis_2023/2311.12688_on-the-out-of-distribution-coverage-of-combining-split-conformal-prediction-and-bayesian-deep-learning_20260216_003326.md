---
ver: rpa2
title: On the Out-of-Distribution Coverage of Combining Split Conformal Prediction
  and Bayesian Deep Learning
arxiv_id: '2311.12688'
source_url: https://arxiv.org/abs/2311.12688
tags:
- intensity
- corruption
- accuracy
- cred
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how combining split conformal prediction with
  Bayesian deep learning models affects out-of-distribution (OOD) coverage in image
  classification. It shows that if a model is overconfident on the calibration set,
  conformal prediction can improve OOD coverage by making prediction sets larger;
  if the model is underconfident, conformal prediction can harm OOD coverage by making
  sets smaller.
---

# On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning

## Quick Facts
- arXiv ID: 2311.12688
- Source URL: https://arxiv.org/abs/2311.12688
- Reference count: 40
- Primary result: Conformal prediction can improve or harm OOD coverage depending on whether the base model is overconfident or underconfident on calibration data

## Executive Summary
This paper investigates how combining split conformal prediction with Bayesian deep learning affects out-of-distribution (OOD) coverage in image classification. The key finding is that the impact of conformal prediction on OOD coverage depends critically on the base model's calibration characteristics. When models are overconfident on calibration data (as with SGD and deep ensembles), conformal prediction improves OOD coverage by expanding prediction sets. Conversely, when models are underconfident (as with mean-field variational inference), conformal prediction harms OOD coverage by shrinking sets. Empirical results on CIFAR10-Corrupted and MedMNIST demonstrate these contrasting effects across different Bayesian methods.

## Method Summary
The study trains AlexNet-inspired models on CIFAR10 and ResNet18 on MedMNIST using three Bayesian approaches: stochastic gradient descent (SGD), deep ensembles, and mean-field variational inference. Split conformal prediction is applied using threshold and adaptive prediction set methods with calibration sets derived from in-distribution test data. Coverage and set size are evaluated on both in-distribution and OOD test sets across different error tolerances (α). The paper examines average marginal coverage, set size, and accuracy to assess the trade-offs between different method combinations.

## Key Results
- Mean-field variational inference (generally underconfident) loses OOD coverage when conformal prediction is applied
- SGD and deep ensembles (generally overconfident) gain OOD coverage when conformal prediction is applied
- Performance varies significantly across different error tolerances, with stricter tolerances (0.01) showing different patterns than looser tolerances (0.05)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction improves OOD coverage when the base model is overconfident on calibration data
- Mechanism: Conformal prediction sets a threshold based on calibration scores. If the model is overconfident, this threshold forces larger prediction sets, which increases the chance of containing OOD labels.
- Core assumption: Calibration data is exchangeable with in-distribution test data
- Evidence anchors:
  - [abstract]: "if the model is overconfident on the calibration set, the use of conformal prediction may improve out-of-distribution coverage"
  - [section]: "conformal prediction creates a thresholdτ that ultimately makes the prediction sets larger than they would otherwise be without conformal prediction"
  - [corpus]: No direct evidence found
- Break condition: Calibration set is not representative of in-distribution data, or the overconfidence is not consistent across calibration examples

### Mechanism 2
- Claim: Conformal prediction harms OOD coverage when the base model is underconfident on calibration data
- Mechanism: If the model is underconfident, the conformal threshold makes prediction sets smaller to maintain coverage, reducing the chance of containing OOD labels
- Core assumption: Calibration data is exchangeable with in-distribution test data
- Evidence anchors:
  - [abstract]: "if the model is generally underconfident on the calibration set, then the resultant conformal sets may exhibit worse out-of-distribution coverage"
  - [section]: "conformal prediction creates a thresholdτ that makes the prediction setssmaller than they would otherwise be"
  - [corpus]: No direct evidence found
- Break condition: Calibration set is not representative of in-distribution data, or the underconfidence is not consistent across calibration examples

### Mechanism 3
- Claim: The impact of conformal prediction depends on the error tolerance α selected
- Mechanism: Different error tolerances require different thresholds. A stricter tolerance (e.g., 0.01) amplifies the effects of model confidence on OOD coverage compared to a looser tolerance (e.g., 0.05)
- Core assumption: The relationship between model confidence and prediction set size is monotonic with respect to α
- Evidence anchors:
  - [section]: "The performance of the combination of methods varies at different error tolerances"
  - [section]: "at the 0.01 error tolerance both the combination of stochastic gradient descent and conformal methods as well as deep ensembles with the conformal methods outperforms... However, at the0.05 error tolerance this is not the case"
  - [corpus]: No direct evidence found
- Break condition: The monotonic relationship between α and coverage effects does not hold for the specific model/threshold combination

## Foundational Learning

- Concept: Marginal coverage guarantee
  - Why needed here: The paper relies on understanding how conformal prediction guarantees coverage on average, and why this guarantee breaks down for OOD data
  - Quick check question: If a prediction set method has 95% marginal coverage, what fraction of the time should the true label be in the prediction set on average?

- Concept: Exchangeability assumption
  - Why needed here: Conformal prediction requires calibration and test data to be exchangeable. The paper examines what happens when this assumption is violated for OOD data
  - Quick check question: If calibration data comes from distribution P and test data from distribution Q, under what condition does the marginal coverage guarantee still hold?

- Concept: Model calibration vs. uncertainty quantification
  - Why needed here: The paper distinguishes between well-calibrated probabilities and faithful uncertainty representation, which affect how conformal prediction interacts with Bayesian methods
  - Quick check question: Can a model be well-calibrated on in-distribution data but still fail to represent uncertainty properly on OOD data?

## Architecture Onboarding

- Component map: Base model (trained via SGD, deep ensembles, or MFV) → Prediction probabilities → Conformal threshold (from calibration set) → Final prediction sets
- Critical path: Calibration set generation → Base model training → Score computation on calibration set → Quantile threshold calculation → OOD coverage evaluation
- Design tradeoffs: Larger calibration sets improve threshold stability but reduce training data; stricter α improves coverage guarantees but increases set size
- Failure signatures: Unexpected coverage drop on OOD data; prediction sets that are consistently too small/large; calibration coverage that doesn't match nominal level
- First 3 experiments:
  1. Verify marginal coverage on in-distribution data for each base model without conformal prediction
  2. Test coverage on OOD data for each base model without conformal prediction
  3. Compare coverage changes when adding conformal prediction to each base model on both in-distribution and OOD data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does combining split conformal prediction with Bayesian deep learning improve out-of-distribution coverage versus when it harms it?
- Basis in paper: Explicit - The paper explicitly states that if a model is overconfident on the calibration set, conformal prediction can improve OOD coverage by making prediction sets larger, while if the model is underconfident, conformal prediction can harm OOD coverage by making sets smaller.
- Why unresolved: While the paper provides empirical evidence from two experiments (CIFAR10-Corrupted and MedMNIST) demonstrating these effects, it does not provide a complete theoretical framework for predicting when conformal prediction will help or harm OOD coverage in general cases. The paper only examines three specific Bayesian methods (SGD, deep ensembles, and MFV) and two conformal methods (threshold and adaptive prediction sets).
- What evidence would resolve it: Additional experiments testing more combinations of Bayesian methods and conformal methods, along with a theoretical analysis that can predict the impact of conformal prediction on OOD coverage based on model calibration characteristics and the type of distribution shift.

### Open Question 2
- Question: How does the choice of error tolerance (α) affect the trade-off between in-distribution coverage guarantees and out-of-distribution coverage when combining conformal prediction with Bayesian deep learning?
- Basis in paper: Explicit - The paper notes that the performance of the combination of methods varies at different error tolerances, showing that at the 0.01 error tolerance both SGD with conformal methods and deep ensembles with conformal methods outperform MFV with conformal methods, while at the 0.05 error tolerance this is not the case.
- Why unresolved: The paper demonstrates that error tolerance selection impacts performance but doesn't provide guidance on how to choose the optimal tolerance that balances in-distribution guarantees with out-of-distribution coverage, especially in realistic scenarios where out-of-distribution data is expected.
- What evidence would resolve it: Systematic experiments varying the error tolerance across a wider range and developing guidelines for selecting tolerance levels based on expected distribution shift severity and application requirements.

### Open Question 3
- Question: Can we develop diagnostic tools to predict when applying conformal prediction to a Bayesian deep learning model will harm out-of-distribution coverage before deployment?
- Basis in paper: Inferred - The paper suggests that underconfidence on the calibration set predicts harm from conformal prediction, but this is based on post-hoc analysis of reliability diagrams rather than a practical diagnostic tool that could be applied during model development.
- Why unresolved: The paper identifies that underconfidence correlates with harm but doesn't provide actionable diagnostics that practitioners could use to assess whether their specific model-setup combination is likely to experience this issue before deploying on real data.
- What evidence would resolve it: Development and validation of simple metrics or visualization tools that can be computed during training to predict whether conformal prediction will harm OOD coverage for a given model, calibration set, and expected deployment scenario.

## Limitations
- The exchangeability assumption between calibration and OOD data is violated, making coverage guarantees unreliable for OOD examples
- The paper does not explore how calibration set size or composition affects the outcomes of combining conformal prediction with Bayesian methods
- Limited to three specific Bayesian methods and two conformal approaches, leaving open questions about generalizability to other model families

## Confidence
- **Mechanism 1** (Conformal improves OOD coverage for overconfident models): Medium confidence - supported by abstract claims but lacks direct empirical evidence in the corpus
- **Mechanism 2** (Conformal harms OOD coverage for underconfident models): Medium confidence - similar support level to Mechanism 1
- **Mechanism 3** (α tolerance affects coverage outcomes): Low confidence - only mentioned in passing with limited empirical validation

## Next Checks
1. **Exchangeability stress test**: Systematically vary the similarity between calibration and OOD test distributions to measure how coverage guarantees degrade as exchangeability breaks down
2. **Calibration set size sensitivity**: Evaluate how different calibration set sizes affect the stability of conformal thresholds and subsequent OOD coverage for both overconfident and underconfident models
3. **Dynamic threshold adaptation**: Implement a method that detects model confidence characteristics on calibration data and adjusts conformal prediction strategy accordingly to optimize OOD coverage