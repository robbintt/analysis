---
ver: rpa2
title: 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative
  Capability of LLMs'
arxiv_id: '2305.12392'
source_url: https://arxiv.org/abs/2305.12392
tags:
- veri
- graph
- iteration
- module
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PiVe, a framework to improve graph-based generation
  from large language models by using a small verifier model that provides iterative
  corrective feedback. The verifier is trained via a simple perturbation-based data
  augmentation method to detect missing or incorrect triples in the generated graphs.
---

# PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs

## Quick Facts
- arXiv ID: 2305.12392
- Source URL: https://arxiv.org/abs/2305.12392
- Reference count: 13
- Key outcome: Iterative prompting with verifier feedback achieves consistent 26% average improvement in graph generation quality

## Executive Summary
This paper introduces PiVe, a framework that improves large language models' ability to generate structured semantic graphs from text by using a small verifier model that provides iterative corrective feedback. The verifier is trained on perturbed graphs to detect missing or incorrect triples, and PiVe iteratively prompts the LLM with corrective instructions until the verifier signals correctness. The framework demonstrates consistent 26% average improvement across three graph datasets and can also operate in a cost-effective offline correction mode.

## Method Summary
PiVe addresses the challenge of LLMs struggling with structured graph generation by introducing a verifier module trained to detect missing triples. The verifier is trained using a simple perturbation-based data augmentation method where correct graphs are modified by removing triples or entities. During text-to-graph generation, the LLM generates an initial graph which the verifier evaluates. If errors are detected, corrective instructions are appended to the prompt and the process repeats until the verifier signals correctness or a threshold is reached. The framework also includes an offline correction mode where all corrections are applied after a single LLM call.

## Key Results
- PiVe achieves consistent 26% average improvement across three graph datasets
- Both iterative prompting and offline correction modes outperform baseline LLM generation
- Dataset-specific and unified (instruction-tuned) verifiers both demonstrate effectiveness
- PiVe successfully enhances automatically generated text-graph datasets, creating GenWiki-HIQ

## Why This Works (Mechanism)

### Mechanism 1
Iterative prompting with verifier feedback improves graph quality by explicitly correcting LLM omissions. The verifier detects missing triples in the LLM output and provides corrective instructions that are appended to the prompt, guiding the LLM to include them in the next iteration. Core assumption: LLMs systematically miss certain triples that can be reliably detected by a trained verifier model.

### Mechanism 2
Training verifier on perturbed graphs enables it to detect common LLM generation errors. The verifier is trained on a dataset created by systematically removing triples from correct graphs, learning to identify when a generated graph is missing expected information. Core assumption: Perturbation-based training data captures the distribution of errors LLMs make when generating graphs.

### Mechanism 3
Offline correction mode provides a cost-effective alternative to iterative prompting. After one LLM call to generate an initial graph, all corrections are applied offline by the verifier without additional LLM calls. Core assumption: The verifier is sufficiently capable to both detect errors and generate corrections without requiring the LLM's generation ability.

## Foundational Learning

- Concept: Graph representation and linearization
  - Why needed here: Understanding how semantic graphs are represented as sets of triples is essential for both generating and verifying graph outputs
  - Quick check question: Given a sentence "John is married to Mary," what would be the corresponding triple representation in a semantic graph?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The LLM generates graphs based on demonstration examples provided in the prompt, making understanding prompt design critical
  - Quick check question: How does changing the number of demonstration examples in a prompt typically affect LLM performance on structured generation tasks?

- Concept: Text-to-graph generation task definition
  - Why needed here: The entire framework addresses improving LLMs' ability to convert text into structured graph representations
  - Quick check question: What is the primary difference between generating a graph from text versus generating text from a graph?

## Architecture Onboarding

- Component map:
  LLM (ChatGPT/GPT-4) -> Verifier module (T5-large/Flan-T5-XXL) -> Prompt builder -> Evaluation metrics (T-F1, G-F1, G-BERTScore, GED) -> Data pipeline

- Critical path:
  1. Generate initial graph from LLM using demonstration prompt
  2. Verify graph with verifier module
  3. If incorrect, append detected missing triples to prompt
  4. Repeat steps 1-3 until verifier signals correctness or threshold reached

- Design tradeoffs:
  - Iterative prompting vs. offline correction: Better quality vs. lower cost
  - Single dataset-specific vs. unified verifier: Specialized performance vs. generalization
  - Number of iterations: More corrections vs. increased API costs
  - Perturbation strategy: Random vs. heuristic methods for training data

- Failure signatures:
  - Verifier consistently outputs "Correct" but graph quality remains low (verifier not sensitive enough)
  - No improvement across iterations (verifier not providing useful corrections)
  - LLM ignores verifier instructions (prompt format or demonstration issues)
  - High computational cost with diminishing returns (iteration threshold too low)

- First 3 experiments:
  1. Run PiVe with single iteration on a small dataset to verify basic functionality
  2. Compare iterative prompting vs. offline correction on a held-out validation set
  3. Test different iteration thresholds (0.9, 0.95, 0.97) to find optimal stopping point

## Open Questions the Paper Calls Out

### Open Question 1
How does PiVe's iterative prompting approach scale with increasing complexity of graphs, particularly those with more entities and relations? The paper demonstrates PiVe's effectiveness on graphs with up to 6 triples, but does not explore scalability beyond this.

### Open Question 2
What is the impact of using different pre-trained language models (LLMs) on the performance of PiVe, beyond ChatGPT? The paper uses ChatGPT (gpt-3.5-turbo) as the LLM, but does not explore the effects of using other models like GPT-4 or open-source alternatives.

### Open Question 3
How does the choice of perturbation method (random vs. heuristic) during verifier training affect the final performance of PiVe? The paper describes two perturbation methods but does not conduct a comparative analysis of their impact on verifier performance and downstream graph generation quality.

### Open Question 4
Can PiVe be extended to handle more complex graph structures, such as those with nested or hierarchical relationships? The current implementation focuses on flat triples, but the paper does not address the potential for extending PiVe to more complex graph structures.

## Limitations

- The perturbation-based training approach may not fully capture the error patterns of LLMs across different domains and text complexity levels
- The framework's effectiveness on graphs with significantly more entities and relations beyond the tested scope remains unproven
- The cost-benefit tradeoff between iterative prompting and offline correction lacks comprehensive quantification across different use cases

## Confidence

- **High Confidence**: The core claim that PiVe improves graph generation quality is well-supported by consistent 26% average improvements across three datasets
- **Medium Confidence**: The effectiveness of the unified (instruction-tuned) verifier compared to dataset-specific verifiers is demonstrated but could benefit from more extensive ablation studies
- **Medium Confidence**: The claim about offline correction being a cost-effective alternative is supported but lacks comprehensive cost analysis

## Next Checks

1. Test PiVe on graphs with varying structural complexity (e.g., graphs with more than 10 triples) to assess scalability and identify potential failure modes
2. Conduct ablation studies on the perturbation methods to determine which strategy (random vs. heuristic) contributes most to verifier performance across different text domains
3. Measure and compare the actual computational costs (API calls, processing time) of iterative prompting versus offline correction across different dataset sizes and graph complexities