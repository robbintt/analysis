---
ver: rpa2
title: 'Expanding Frozen Vision-Language Models without Retraining: Towards Improved
  Robot Perception'
arxiv_id: '2308.16493'
source_url: https://arxiv.org/abs/2308.16493
tags:
- data
- embeddings
- encoder
- language
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of extending frozen, pretrained
  vision-language models (VLMs) to understand additional modalities without retraining.
  The core method idea involves aligning the embedding spaces of different modalities,
  such as inertial measurement unit (IMU) data, to the vision embedding space through
  a combination of supervised and contrastive training.
---

# Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception

## Quick Facts
- arXiv ID: 2308.16493
- Source URL: https://arxiv.org/abs/2308.16493
- Reference count: 40
- One-line primary result: Using multiple modalities as input improves the VLM's scene understanding and enhances its overall performance in various tasks, as evidenced by improved human activity recognition accuracy when combining IMU and vision data embeddings.

## Executive Summary
This paper addresses the challenge of extending frozen, pretrained vision-language models (VLMs) to understand additional modalities without retraining. The proposed approach aligns the embedding spaces of different modalities, such as inertial measurement unit (IMU) data, to the vision embedding space through a combination of supervised and contrastive training. This allows the VLM to interpret and reason about these additional modalities without retraining. The method demonstrates improved scene understanding and task performance, particularly in human activity recognition when combining IMU and vision data embeddings.

## Method Summary
The method involves training an IMU encoder to produce embeddings that align with the frozen vision encoder's embedding space through contrastive and supervised learning. The IMU encoder uses a transformer architecture to process sequential IMU data, while the vision encoder is a frozen ViT-L/14 model. The aligned embeddings are linearly combined and passed through a Perceiver Resampler to produce fixed-length latent vectors compatible with the frozen VLM's input requirements. The VLM (Otter) then reasons about the multi-modal input to generate responses. Training uses infoNCE loss for contrastive alignment and supervised classification loss on the latent vectors.

## Key Results
- Improved human activity recognition accuracy when combining IMU and vision embeddings
- Successful alignment of IMU embeddings to vision embedding space through contrastive learning
- Demonstration that frozen VLMs can process IMU data when embeddings are properly aligned

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive alignment of IMU embeddings to the vision embedding space enables frozen VLMs to interpret IMU data without retraining.
- Mechanism: By optimizing the cosine similarity of temporally overlapping image-IMU pairs using infoNCE loss, the IMU encoder learns to produce embeddings that occupy the same manifold as the vision encoder's outputs. The frozen VLM, trained on vision embeddings, can then process IMU embeddings as if they were vision features.
- Core assumption: The vision encoder's embedding space is sufficiently rich and generalizable to represent the semantic content of IMU data when aligned through contrastive learning.
- Evidence anchors:
  - [abstract] "aligning the embedding spaces of different modalities (in this case, inertial measurement unit (IMU) data) to the vision embedding space through a combination of supervised and contrastive training"
  - [section] "This contrastive and supervised learning objective enables the IMU encoder to learn a meaningful mapping from raw sensor data to a representative embedding space that aligns with the image encoder."
  - [corpus] Weak - no direct evidence found in neighbors about contrastive alignment for frozen VLMs.
- Break condition: If the vision embedding space lacks the semantic dimensions needed to represent IMU features, alignment will produce embeddings that the VLM cannot meaningfully interpret.

### Mechanism 2
- Claim: Linear combination of vision and IMU embeddings creates a richer, more discriminative scene representation than either modality alone.
- Mechanism: The vision encoder captures spatial and visual context while the IMU encoder captures kinematic activity patterns. When combined, the resulting embedding preserves both types of information, allowing the VLM to reason about both what is happening and what is in the scene.
- Core assumption: The two modalities capture complementary, non-redundant information about the scene that can be linearly combined without destructive interference.
- Evidence anchors:
  - [abstract] "using multiple modalities as input improves the VLM's scene understanding and enhances its overall performance in various tasks"
  - [section] "This linear combination of encoded representations provides the VLM with a more holistic representation of the scene."
  - [corpus] Weak - no direct evidence found in neighbors about linear combination of multimodal embeddings for VLMs.
- Break condition: If the two modalities encode overlapping or conflicting information, linear combination could degrade rather than enhance representation quality.

### Mechanism 3
- Claim: The Perceiver Resampler module enables variable-length modality inputs to be mapped to the fixed-length token sequences expected by the frozen LLM.
- Mechanism: Both image and IMU encoders output sequences of variable length. The Perceiver Resampler transforms these into fixed-length latent vectors that the LLM's cross-attention layers can process consistently, regardless of the original modality.
- Core assumption: The Perceiver Resampler can learn to extract the most salient features from any modality's encoded representation while maintaining semantic consistency across modalities.
- Evidence anchors:
  - [abstract] "outputs a predefined set of latent vectors that are resampled representations of the input"
  - [section] "This allows for variably sized inputs to be mapped to the same length of tokens"
  - [corpus] Weak - no direct evidence found in neighbors about Perceiver Resampler usage for multimodal VLMs.
- Break condition: If the Perceiver Resampler cannot adequately capture modality-specific features in the fixed-length representation, information loss could prevent the LLM from understanding the input.

## Foundational Learning

- Concept: Contrastive learning and infoNCE loss
  - Why needed here: The method relies on aligning two embedding spaces through contrastive optimization, which requires understanding how positive and negative pairs are defined and how the loss function shapes the learned representation.
  - Quick check question: In the context of this paper, what constitutes a positive pair versus a negative pair when training the IMU encoder?

- Concept: Transformer-based modality encoders
  - Why needed here: The IMU data is processed through a transformer encoder before alignment, so understanding how transformers extract temporal features from sequential data is essential for grasping the method's effectiveness.
  - Quick check question: Why might a transformer encoder be preferred over a CNN for processing IMU time-series data in this context?

- Concept: In-context learning in LLMs
  - Why needed here: The VLM's ability to reason about multimodal inputs without retraining depends on its in-context learning capabilities, which determine how it interprets and responds to the concatenated prompt containing vision, IMU, and text.
  - Quick check question: How does the VLM's in-context learning ability enable it to understand IMU embeddings that were never part of its original training data?

## Architecture Onboarding

- Component map: IMU data → IMU transformer encoder → IMU embeddings → Perceiver resampler → Fixed-length latent vectors; Video frame → Frozen ViT-L/14 encoder → Vision embeddings → Perceiver resampler → Fixed-length latent vectors; Latent vectors + text prompt → Otter VLM → Generated response
- Critical path: IMU data → IMU encoder → Perceiver resampler → Otter VLM → Response generation
- Design tradeoffs:
  - Using frozen VLMs avoids expensive retraining but limits the ability to learn modality-specific processing
  - Linear combination of embeddings is simple but may not capture complex modality interactions
  - Small dataset (6,000 pairs) enables quick experimentation but may limit alignment quality
- Failure signatures:
  - VLM outputs nonsensical or repetitive responses when given IMU embeddings
  - Performance degrades when adding IMU embeddings to vision-only inputs
  - t-SNE visualization shows poor separation of activity classes in combined embedding space
- First 3 experiments:
  1. Train IMU encoder with contrastive loss only (no supervised term) and evaluate activity recognition performance
  2. Vary the linear combination weights (e.g., 70-30, 50-50, 30-70) and measure impact on scene understanding tasks
  3. Remove the Perceiver resampler and train with raw encoder outputs to assess its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for selecting or weighting different modalities in a multi-modal VLM to maximize scene understanding and task performance?
- Basis in paper: [inferred] The paper discusses linearly combining embeddings from different modalities (e.g., vision and IMU) and mentions the need for empirical tuning of weights. It also references ImageBind's use of multi-modal embedding space arithmetic for better video retrieval.
- Why unresolved: The paper does not provide a definitive answer on the best approach for modality selection and weighting. It suggests that different modalities have varying levels of generalizable information and that this warrants further investigation.
- What evidence would resolve it: Empirical studies comparing different methods for modality selection and weighting, such as learned attention mechanisms, gating functions, or adaptive weighting schemes, and their impact on VLM performance across various tasks and datasets.

### Open Question 2
- Question: How does the size of the VLM (e.g., number of parameters, model architecture) affect its ability to effectively utilize and reason about multiple modalities?
- Basis in paper: [inferred] The paper mentions that larger VLMs might have longer inference times, which could be problematic in some implementations. It also suggests exploring the effects of larger VLMs or VLMs with different architectures on multi-modal fine-tuning.
- Why unresolved: The paper does not provide a comprehensive analysis of the relationship between VLM size and multi-modal performance. It only hints at potential trade-offs between model size and inference speed.
- What evidence would resolve it: Systematic experiments comparing the performance of VLMs of different sizes on multi-modal tasks, along with analyses of their reasoning capabilities and inference times.

### Open Question 3
- Question: How can we mitigate the hallucination problem in VLMs when extending them to multiple modalities without retraining?
- Basis in paper: [explicit] The paper acknowledges that extending modalities without pretraining can lead to an increase in hallucinations because the learnable model parameters have not been trained for multi-modal processing.
- Why unresolved: The paper does not provide specific solutions for mitigating hallucinations in multi-modal VLMs. It only mentions that poor training and low-quality training data can affect the degree of hallucination.
- What evidence would resolve it: Development and evaluation of techniques to reduce hallucinations in multi-modal VLMs, such as improved training objectives, data augmentation strategies, or post-hoc correction methods.

## Limitations

- The core mechanism of contrastive alignment between IMU and vision embeddings assumes the vision encoder's embedding space can meaningfully represent IMU semantics, but this has not been empirically demonstrated beyond the specific MMAct dataset used.
- The linear combination approach for merging modalities, while intuitive, may not capture complex interactions between vision and IMU data that could be better modeled through more sophisticated fusion methods.
- The small training dataset (6,000 image-IMU pairs) raises concerns about whether the alignment generalizes to other activities or sensor configurations.

## Confidence

**High confidence**: The basic architecture design (frozen VLM + aligned IMU encoder + Perceiver resampler) is clearly specified and follows established patterns in multimodal learning. The use of infoNCE loss for contrastive alignment is well-grounded in prior work.

**Medium confidence**: The claim that linear combination of embeddings improves scene understanding is supported by the reported activity recognition results, but the mechanism by which this enhancement occurs is not fully explained. The choice of combination weights appears arbitrary without sensitivity analysis.

**Low confidence**: The assertion that this approach generalizes to arbitrary additional modalities beyond IMU data is speculative, as the paper only validates on one specific sensor type. The claim about improved "overall performance in various tasks" is limited by the single downstream task (activity recognition) presented.

## Next Checks

1. **Embedding Space Analysis**: Generate t-SNE or UMAP visualizations of the aligned IMU and vision embeddings to verify that semantically similar activities cluster together and that the two modalities occupy compatible regions of the embedding space. This would validate whether the contrastive alignment is producing meaningful representations.

2. **Cross-Dataset Generalization**: Test the aligned IMU encoder on a different activity recognition dataset (e.g., Opportunity or PAMAP2) to assess whether the learned alignment generalizes beyond the MMAct dataset. Performance degradation would indicate overfitting to the specific dataset characteristics.

3. **Ablation of Fusion Strategy**: Replace the linear combination of embeddings with alternative fusion methods (e.g., concatenation followed by MLP, or attention-based fusion) to determine whether the simple linear approach is optimal or merely sufficient. This would reveal whether the method's effectiveness depends critically on the fusion strategy chosen.