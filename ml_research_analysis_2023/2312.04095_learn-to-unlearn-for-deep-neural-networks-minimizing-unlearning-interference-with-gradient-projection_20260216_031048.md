---
ver: rpa2
title: 'Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning Interference
  with Gradient Projection'
arxiv_id: '2312.04095'
source_url: https://arxiv.org/abs/2312.04095
tags:
- dataset
- data
- unlearning
- training
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of machine unlearning, which aims
  to remove the effect of specific training samples from a trained model as if they
  were never present in the original training dataset. The core method, called Projected-Gradient
  Unlearning (PGU), leverages gradient projection to remove information related to
  the forgetting data while preserving knowledge about the retaining dataset.
---

# Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning Interference with Gradient Projection

## Quick Facts
- **arXiv ID:** 2312.04095
- **Source URL:** https://arxiv.org/abs/2312.04095
- **Reference count:** 40
- **Primary result:** Proposed method produces models that behave similarly to models retrained from scratch across multiple metrics, including test error, forgetting error, and membership inference attack AUC.

## Executive Summary
This paper addresses the challenge of machine unlearning in deep neural networks, where the goal is to remove the influence of specific training samples as if they were never part of the training process. The authors propose Projected-Gradient Unlearning (PGU), a method that leverages gradient projection to selectively remove information about forgetting data while preserving knowledge about the retaining dataset. The method is efficient, scalable, and can work even when the original training data is no longer accessible. Experimental results on various datasets and models demonstrate that PGU can achieve performance comparable to models retrained from scratch across multiple metrics.

## Method Summary
The core method, Projected-Gradient Unlearning (PGU), works by computing the Core Gradient Space (CGS) of the retaining dataset and updating model weights in the orthogonal direction to this space. This allows the model to discard information about the forgetting data while preserving knowledge learned from the retaining data. The method pre-computes and caches basis vectors and eigenvalues of the gradient space for the full training data, enabling efficient computation of the CGS for the retaining dataset when a deletion request is received. PGU supports incremental unlearning, allowing the model to adjust weights as more training data is removed. The approach is validated on CIFAR-10 and TinyImageNet datasets using various model architectures including AllCNN, SmallVGG, and ResNet-18.

## Key Results
- When unlearning 500 samples of CIFAR-10 class 0 using the AllCNN model, PGU achieves a test error of 9.86%, forgetting error of 9.92%, and MIA AUC of 0.534, close to retrained model performance
- PGU successfully removes the first 5 classes of TinyImageNet dataset using ResNet-18 while maintaining good performance on remaining classes
- The method supports incremental unlearning, effectively removing subsets of 1,000 training samples at a time on CIFAR-10 with SmallVGG model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method removes information related to forgetting data while preserving knowledge about the retaining dataset by updating weights in the orthogonal direction to the Core Gradient Space (CGS).
- Mechanism: CGS of the retaining dataset is computed, and gradient updates are projected onto the orthogonal complement of this space, discarding forgetting data information while preserving retaining data knowledge.
- Core assumption: CGS of retaining dataset is orthogonal to information related to forgetting data.
- Evidence anchors: Abstract and section explicitly describe this mechanism; related work on orthogonal gradient projection supports the approach.

### Mechanism 2
- Claim: The method efficiently computes CGS for retaining dataset from full training data gradient space when original data is inaccessible.
- Mechanism: Pre-computes and caches basis vectors and eigenvalues of full training data gradient space, then efficiently computes CGS for retaining data by subtracting forgetting data contribution.
- Core assumption: Basis vectors and eigenvalues can be pre-computed and cached efficiently.
- Evidence anchors: Section describes pre-computation strategy; common ML practice supports caching basis vectors.

### Mechanism 3
- Claim: The method supports incremental unlearning by maintaining consistent CGS across deletions.
- Mechanism: CGS of retaining dataset at any time remains consistent with original model's CGS, enabling easy re-computation when retaining dataset changes.
- Core assumption: CGS remains stable across incremental deletions.
- Evidence anchors: Section explains how weight updates preserve retaining dataset outputs; incremental unlearning is common ML practice.

## Foundational Learning

- **Concept:** Gradient descent and stochastic gradient descent (SGD)
  - Why needed here: Used to update model weights during unlearning process
  - Quick check question: What is the difference between gradient descent and stochastic gradient descent?

- **Concept:** Singular value decomposition (SVD)
  - Why needed here: Used to compute basis vectors and eigenvalues of gradient space
  - Quick check question: What is the purpose of using SVD in the method?

- **Concept:** Membership inference attack (MIA)
  - Why needed here: Used to evaluate effectiveness of unlearning in removing forgetting data information
  - Quick check question: What is a membership inference attack and how is it used to evaluate unlearning effectiveness?

## Architecture Onboarding

- **Component map:** Input layer -> CGS computation -> Gradient projection -> Unlearning process -> Evaluation
- **Critical path:** Input layer → CGS computation → Gradient projection → Unlearning process → Evaluation
- **Design tradeoffs:** Computational efficiency through pre-computation vs. effectiveness in preserving knowledge while removing forgetting data information
- **Failure signatures:** High test error (knowledge not preserved), high forgetting error (forgetting data not removed), high MIA AUC (information still recoverable)
- **First 3 experiments:**
  1. Data removal: Remove 500 samples of CIFAR-10 class 0 using AllCNN model
  2. Class removals: Remove first 5 classes of TinyImageNet using ResNet-18
  3. Incremental data removals: Remove 1,000 samples at a time using SmallVGG on CIFAR-10

## Open Questions the Paper Calls Out

1. **Optimal threshold determination:** What is the optimal threshold γl for balancing unlearning effectiveness and knowledge preservation across different model architectures and datasets? The paper provides empirical observations for specific model-dataset pairs but lacks a principled method for threshold determination.

2. **Scalability to large models:** How does the method scale to extremely large models (e.g., GPT-scale transformers) in terms of both computational efficiency and unlearning effectiveness? The paper only demonstrates results on CNN architectures and a ResNet-18 model.

3. **Theoretical guarantees:** What are the theoretical guarantees for the completeness of information removal from the forgetting dataset? The paper relies on empirical evaluation through membership inference attacks rather than formal guarantees.

4. **Realistic deletion scenarios:** How does the method perform under more realistic data deletion scenarios where the forgetting dataset distribution differs significantly from the training distribution? The experiments use synthetic forgetting datasets rather than real-world deletion requests.

## Limitations

- Method's effectiveness and scalability to extremely large models and datasets remains theoretical
- Orthogonal gradient projection mechanism relies on assumption that CGS of retaining data is independent from forgetting data information
- Limited empirical validation across diverse model types beyond CNNs and standard ResNets

## Confidence

- **High:** The core mathematical framework of gradient projection orthogonal to CGS
- **Medium:** Experimental results on CIFAR-10 and TinyImageNet with specific models
- **Low:** Scalability claims for extremely large models and datasets

## Next Checks

1. **Architecture generalization test:** Apply PGU to transformer-based models (BERT, ViT) to verify effectiveness across different neural architectures beyond CNNs and standard ResNets.

2. **Dataset diversity evaluation:** Test PGU on datasets with varying characteristics (text, tabular, medical imaging) to assess whether CGS orthogonality assumption holds across different data modalities and distributions.

3. **Large-scale scalability benchmark:** Implement PGU on ImageNet-scale datasets with state-of-the-art models (e.g., EfficientNet, ConvNeXt) to empirically validate computational efficiency and memory requirements against theoretical analysis.