---
ver: rpa2
title: Optimally Teaching a Linear Behavior Cloning Agent
arxiv_id: '2311.15399'
source_url: https://arxiv.org/abs/2311.15399
tags:
- teaching
- cone
- extreme
- optimal
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimally teaching a Linear
  Behavior Cloning (LBC) learner by minimizing the number of state demonstrations
  needed to teach a target policy. The authors present an algorithm called TIE (Teach
  using Iterative Elimination) that achieves instance-optimal teaching dimension.
---

# Optimally Teaching a Linear Behavior Cloning Agent

## Quick Facts
- arXiv ID: 2311.15399
- Source URL: https://arxiv.org/abs/2311.15399
- Reference count: 35
- Key outcome: Presents TIE algorithm achieving instance-optimal teaching dimension for linear behavior cloning by identifying extreme rays of feature difference cones and solving a set cover problem

## Executive Summary
This paper addresses the problem of optimally teaching a Linear Behavior Cloning (LBC) learner by minimizing the number of state demonstrations needed to teach a target policy. The authors present TIE (Teach using Iterative Elimination), an algorithm that achieves instance-optimal teaching dimension by first identifying extreme rays of the cone generated by feature difference vectors, then solving a set cover problem to find the minimal teaching set. The paper proves that finding the optimal teaching set is NP-hard and provides an approximation algorithm with a logarithmic approximation ratio. Experimental results validate the efficiency and effectiveness of TIE on two test cases: a "pick the right diamond" game and a polygon tower problem.

## Method Summary
The method works by building the set of feature difference vectors Ψ(DS) from all state-action pairs where the action differs from the target policy. TIE then identifies the extreme rays of the cone generated by these vectors using an iterative elimination procedure that tests each vector's uniqueness on its extreme ray via linear programming. Once extreme rays are identified, the problem reduces to a set cover problem where each state covers a subset of extreme rays, and the goal is to find the minimum number of states that cover all extreme rays. The algorithm achieves a logarithmic approximation ratio using the greedy set cover algorithm.

## Key Results
- TIE algorithm achieves instance-optimal teaching dimension for LBC learners
- Finding the optimal teaching set is proven to be NP-hard
- Greedy set cover algorithm provides logarithmic approximation ratio
- Experimental validation on "pick the right diamond" and polygon tower problems shows efficiency and effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teaching optimality is achieved by covering all extreme rays of the primal cone generated by feature difference vectors.
- Mechanism: The teacher demonstrates only the minimal set of states whose induced feature differences span all extreme rays, ensuring the learner's version space exactly matches the target policy's consistent weights.
- Core assumption: Realizability holds—there exists a weight vector that induces the target policy.
- Evidence anchors:
  - [abstract] "TIE works by first identifying the extreme rays of the cone generated by feature difference vectors, then solving a set cover problem..."
  - [section 3.2] "We first show that it is not necessary to teach with the full demonstration set DS... Instead, we just need a subset U ⊆ Ψ(DS) that covers the extreme rays of cone(Ψ(DS))."
- Break condition: If realizability fails, the learner's version space cannot be constrained to only weights inducing the target policy.

### Mechanism 2
- Claim: Finding the extreme rays is done by iteratively eliminating vectors that are conic combinations of others.
- Mechanism: For each candidate vector, solve a linear program that checks if it is the unique representative on its extreme ray; if not, eliminate it.
- Core assumption: The feature difference vectors form a pointed cone, so extreme rays are well-defined and can be identified via LP feasibility.
- Evidence anchors:
  - [section 3.3] "Given a set of vectors X ∈ Rd and x ∈ X, the following linear program determines if x is the only vector on an extreme ray of cone(X)."
- Break condition: Numerical instability or degenerate cones could cause incorrect elimination.

### Mechanism 3
- Claim: The minimal teaching set problem reduces to a finite set cover over extreme rays.
- Mechanism: Each state corresponds to a subset of extreme rays it can cover; solving the set cover yields the smallest teaching set.
- Core assumption: The number of extreme rays is finite and each state covers at most |A|−1 of them.
- Evidence anchors:
  - [section 3.2] "Each state s ∈ S induces a set of feature difference vectors {ψsπ†(s)b : b ∈ A, b ̸= π†(s)} which can cover at most |A| − 1 extreme rays."
- Break condition: If the extreme ray set is very large relative to |A|, the set cover becomes intractable despite approximation.

## Foundational Learning

- Concept: Duality between primal and dual cones in convex analysis.
  - Why needed here: The learner's version space is the dual cone of the primal cone of feature differences; understanding this duality is essential to see why covering extreme rays suffices.
  - Quick check question: If w is in the dual cone of X, what condition must it satisfy for all x in X?

- Concept: Set cover problem and its NP-hardness.
  - Why needed here: The teaching problem is reduced to a finite set cover over extreme rays; knowing its hardness explains why exact solutions are infeasible and approximations are used.
  - Quick check question: What is the approximation ratio of the greedy algorithm for set cover?

- Concept: Linear programming for feasibility and optimality in cones.
  - Why needed here: Identifying extreme rays relies on solving LPs that test whether a vector is uniquely on an extreme ray.
  - Quick check question: In the LP test, what does an unbounded objective value indicate about the vector's position relative to the cone?

## Architecture Onboarding

- Component map: Build Ψ(DS) -> Find extreme rays -> Solve set cover -> Return teaching set
- Critical path: Build Ψ(DS) → Find extreme rays → Solve set cover → Return teaching set
- Design tradeoffs:
  - Exact set cover: NP-hard, may be infeasible for large problems
  - Greedy approximation: Fast, logarithmic approximation ratio but not optimal
  - LP solving: Polynomial per vector but repeated many times; numerical precision matters
- Failure signatures:
  - Empty version space: Realizability assumption violated
  - Very large teaching sets: Approximation error or poor feature design
  - Long runtimes: Too many extreme rays or large |A|
- First 3 experiments:
  1. Run on the "pick the right diamond" game with small n to verify minimal set size = 2.
  2. Test on polygon tower with n=6 to check TIE matches the ground truth minimal set.
  3. Scale polygon tower to n=12 and measure runtime growth and approximation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the TIE algorithm's approximation ratio of log(|A|-1) be improved for specific classes of teaching problems?
- Basis in paper: [explicit] The paper states that the greedy set cover algorithm achieves an approximation ratio of log(largest subset cardinality), which for this problem is log(|A|-1).
- Why unresolved: The paper uses the standard greedy algorithm for set cover without exploring problem-specific optimizations that might yield better approximation ratios.
- What evidence would resolve it: Analysis showing improved approximation ratios for TIE on restricted classes of LBC teaching problems, such as those with specific feature function structures or target policies.

### Open Question 2
- Question: How does the teaching dimension scale with the size of the state space and action space in practical scenarios?
- Basis in paper: [inferred] The paper discusses theoretical bounds and presents experiments on specific examples, but does not provide a general characterization of teaching dimension scaling.
- Why unresolved: The relationship between teaching dimension and problem parameters (state space size, action space size, feature function complexity) is not fully explored.
- What evidence would resolve it: Empirical studies or theoretical analysis showing how teaching dimension varies with problem parameters across a range of LBC teaching problem instances.

### Open Question 3
- Question: Can the TIE algorithm be extended to handle non-linear feature functions or non-linear policy families?
- Basis in paper: [explicit] The paper focuses on linear behavior cloning learners with linear feature functions. The teaching algorithm relies on properties of linear cones and their extreme rays.
- Why unresolved: The current TIE algorithm is specifically designed for linear feature functions and policies. Extending it to non-linear cases would require different mathematical tools and may not have a clean geometric interpretation.
- What evidence would resolve it: Development of an algorithm for teaching non-linear behavior cloning learners, or proof that such an algorithm is computationally intractable in general.

## Limitations
- The NP-hardness of finding the optimal teaching set limits scalability to large problems
- The approach assumes realizability (existence of a weight vector inducing the target policy), which may not hold in practical scenarios
- Numerical precision in LP-based extreme ray identification could lead to incorrect elimination of vectors

## Confidence

- **High Confidence**: The theoretical framework connecting extreme rays of primal cones to dual cone constraints is mathematically sound and well-established in convex analysis literature
- **Medium Confidence**: The approximation algorithm's logarithmic bound is proven, but practical performance may vary significantly depending on problem structure and feature design
- **Medium Confidence**: The reduction from teaching to set cover is correct, but the practical impact of this reduction on teaching efficiency requires empirical validation across diverse domains

## Next Checks

1. **Numerical Robustness Test**: Implement the iterative elimination procedure with multiple LP solvers and tolerance levels to assess sensitivity to numerical precision, particularly on degenerate cone examples
2. **Realizability Stress Test**: Systematically evaluate TIE's performance when realizability is violated by introducing noise or misspecification in the feature vectors, measuring how quickly the version space becomes empty
3. **Scalability Benchmark**: Compare TIE's runtime and approximation quality against baseline random and heuristic teaching strategies on problems with increasing state space size (n > 20) and action space cardinality (|A| > 5)