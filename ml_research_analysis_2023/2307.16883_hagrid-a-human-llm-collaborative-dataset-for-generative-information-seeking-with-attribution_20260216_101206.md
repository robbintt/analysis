---
ver: rpa2
title: 'HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking
  with Attribution'
arxiv_id: '2307.16883'
source_url: https://arxiv.org/abs/2307.16883
tags:
- quotes
- answers
- linguistics
- association
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HAGRID, a human-LLM collaborative dataset
  for generative information-seeking with attribution. The dataset is built on top
  of MIRACL, a publicly available information retrieval dataset, and aims to address
  the lack of accessible datasets for building open-source generative search models.
---

# HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution

## Quick Facts
- arXiv ID: 2307.16883
- Source URL: https://arxiv.org/abs/2307.16883
- Reference count: 24
- Primary result: HAGRID is a dataset of 2,638 questions with LLM-generated attributed answers and human evaluations for informativeness and attributability.

## Executive Summary
This paper introduces HAGRID, a dataset created through human-LLM collaboration to support generative information-seeking with attribution. Built on the MIRACL retrieval dataset, HAGRID contains questions, relevant passages, and LLM-generated answers with IEEE-style citations. Human annotators evaluate each answer for informativeness and attributability, resulting in a dataset designed to train end-to-end generative retrieval models. The dataset addresses the lack of accessible, attributed generative search datasets for open-source development.

## Method Summary
The authors construct HAGRID by first extracting questions and relevant passages from MIRACL, then using GPT-3.5 to generate attributed answers in a zero-shot manner with IEEE-style citations. Human annotators with text annotation experience evaluate the generated answers for informativeness (whether the answer is useful) and attributability (whether claims are supported by cited passages). The dataset includes 1,922 training questions and 716 development questions, with around 3 answers per question on average.

## Key Results
- 84% of generated answers were evaluated for informativeness, with 60% deemed informative
- 24-88% of answers were evaluated for attributability, with 71-73% found to be attributable
- Generated answers tend to be extractive, covering 70% of cited text on average with 25% density
- Quotes are cited nearly evenly when the number of quotes is small

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM can generate structured, attributed answers when prompted with relevant passages and explicit citation instructions.
- Mechanism: GPT-3.5 receives a prompt containing a question and a set of relevant passages (quotes) formatted as contexts, with an instruction to answer using IEEE-style in-text citations. The model learns to associate answer fragments with source passages and formats them accordingly.
- Core assumption: The LLM's zero-shot generation capability is sufficient to produce human-like, well-cited explanations when provided with supporting text and explicit formatting guidance.
- Evidence anchors:
  - [section] "We instruct GPT-3.5, i.e., gpt-3.5-turbo-0301 (OpenAI, 2022), to generate an answer to a question in a zero-shot fashion. We do not prepare any demonstrations or instructions for prompting with GPT-3.5. We provide an instruction, and a list of quotes as contexts and ask the LLM to reference answers within brackets [] in the IEEE format."
  - [corpus] No strong corpus evidence; relies on LLM performance in other attribution tasks.
- Break condition: If the model fails to generate valid IEEE-style citations or omits attribution in its responses, or if generated answers are too abstract to be attributable to any passage.

### Mechanism 2
- Claim: Human annotators can reliably evaluate both informativeness and attributability of LLM-generated answers.
- Mechanism: Annotators assess each sentence of the generated answer for whether it directly answers the question (informativeness) and whether its claims are fully supported by the cited passages (attributability). Grouping sentences without citations with the following cited sentence simplifies the task.
- Core assumption: Human judgment is reliable and consistent for evaluating the correctness and source-grounding of generated explanations.
- Evidence anchors:
  - [section] "For human assessment, we hired 4 specialist annotators with 1+ year of experience with text data annotation... To minimize any potential biases and ensure consistency in the annotation process, our team implemented a carefully designed onboarding procedure with training sessions specifically tailored to this task."
  - [corpus] No strong corpus evidence; the claim is based on procedural description.
- Break condition: If annotator agreement is low or annotators cannot reliably determine attributability, especially in cases of multi-sentence or multi-quote citations.

### Mechanism 3
- Claim: Reusing an existing IR dataset (MIRACL) with human-judged relevance labels provides high-quality input passages for attribution generation.
- Mechanism: MIRACL's passages are already judged for relevance to each query, so using them as contexts for LLM answer generation ensures that the LLM is working from factually related material.
- Core assumption: Human relevance judgments in MIRACL are accurate and representative of true passage-question relationships.
- Evidence anchors:
  - [section] "MIRACL...split documents based on natural discourse units using two consecutive newlines...The dataset represents a standard ad hoc retrieval task, where passages have been marked relevant for each query."
  - [corpus] No strong corpus evidence; the quality is inferred from MIRACL's methodology.
- Break condition: If passages judged as relevant by MIRACL are not actually supportive of the answer or contain misleading information.

## Foundational Learning

- Concept: Information retrieval with relevance judgments
  - Why needed here: The dataset is built on top of MIRACL, which requires understanding of passage relevance and how judged passages are used as contexts for answer generation.
  - Quick check question: What is the difference between a relevant passage and a supporting passage for a generated answer?

- Concept: Zero-shot prompt engineering
  - Why needed here: The LLM is used in a zero-shot fashion with only a brief instruction and context, so understanding how to construct prompts that elicit structured, cited responses is critical.
  - Quick check question: What formatting instruction is given to GPT-3.5 to ensure answers include IEEE-style citations?

- Concept: Human-in-the-loop evaluation
  - Why needed here: The dataset is constructed through collaboration between LLM generation and human evaluation, requiring knowledge of evaluation criteria (informativeness and attributability) and how to apply them.
  - Quick check question: How is a sentence determined to be attributable in HAGRID's annotation process?

## Architecture Onboarding

- Component map: MIRACL query+passage extraction → LLM answer generation (GPT-3.5) → Post-processing (format validation) → Human annotation pipeline (informativeness + attributability) → Dataset assembly
- Critical path: Prompt construction → LLM generation → Annotation → Dataset release
- Design tradeoffs: Using LLM for answer generation saves human effort but introduces generation noise; using human evaluation ensures quality but is slow and expensive; building on MIRACL ensures high-quality passages but limits question diversity.
- Failure signatures: LLM outputs without proper citations, human annotators unable to judge attributability due to complex citation chains, passages not supporting generated claims, annotator disagreement on informativeness.
- First 3 experiments:
  1. Run LLM generation with a small sample of MIRACL queries and manually inspect citation format compliance.
  2. Have two annotators independently label a sample of generated answers for informativeness and compute agreement.
  3. Measure coverage and density between generated answers and cited quotes to assess extractiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of quotes to provide to LLMs for generating informative and attributable answers in information-seeking tasks?
- Basis in paper: [explicit] The paper discusses how the number of associated quotes impacts the quality of generated answers, noting that as the number of quotes grows, LLMs tend to become fallible in generating informative and attributable answers.
- Why unresolved: The paper observes a trend but does not provide a specific threshold or optimal number of quotes.
- What evidence would resolve it: Experiments varying the number of quotes and measuring the informativeness and attributability of generated answers could identify the optimal range.

### Open Question 2
- Question: How do different prompting strategies affect the quality of attributed explanations generated by LLMs?
- Basis in paper: [explicit] The paper mentions that they explored several instructions in the prompt to guide the LLM in generating both short and long answers, but found no significant differences between these generated answers.
- Why unresolved: The paper does not provide detailed analysis or comparison of different prompting strategies.
- What evidence would resolve it: A systematic comparison of various prompting strategies, including their impact on answer quality, length, and citation accuracy, could provide insights into optimal prompting approaches.

### Open Question 3
- Question: How well do LLMs generalize to information-seeking tasks in languages other than English?
- Basis in paper: [explicit] The paper notes that while the source dataset, MIRACL, is multilingual, they focused on the English subset and left non-English languages for future work.
- Why unresolved: The paper does not explore the performance of LLMs on information-seeking tasks in other languages.
- What evidence would resolve it: Experiments evaluating LLM performance on information-seeking tasks in multiple languages, including low-resource languages, could assess their generalizability.

## Limitations
- Reliance on LLM-generated answers introduces uncertainty about response diversity and quality, particularly for complex queries
- Human evaluation covers only a subset of generated answers (84% for informativeness, 24-88% for attributability), potentially missing systematic patterns
- The dataset may not generalize well to questions requiring complex reasoning or multi-hop inference

## Confidence
- **High confidence**: The dataset construction methodology and basic statistics (1,922 training questions, 716 dev questions, average 3 answers per question) are well-documented and verifiable.
- **Medium confidence**: The quality of human annotations is supported by specialist annotators with onboarding procedures, but inter-annotator agreement metrics are not reported.
- **Medium confidence**: The claim about extractiveness of generated answers (70% coverage, 25% density) is based on analysis of the dataset but may not generalize to all query types.

## Next Checks
1. Compute and report inter-annotator agreement scores (e.g., Cohen's kappa) for both informativeness and attributability annotations to quantify evaluation reliability.
2. Manually sample and verify the claim that generated answers cover 70% of cited text on average, checking for potential selection bias in the analyzed subset.
3. Evaluate the dataset's effectiveness by training a retrieval-augmented generation model on HAGRID and testing on an external attributed retrieval benchmark to assess real-world applicability.