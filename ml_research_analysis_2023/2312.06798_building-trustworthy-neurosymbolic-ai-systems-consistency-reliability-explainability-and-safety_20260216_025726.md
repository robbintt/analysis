---
ver: rpa2
title: 'Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability,
  and Safety'
arxiv_id: '2312.06798'
source_url: https://arxiv.org/abs/2312.06798
tags:
- llms
- arxiv
- language
- knowledge
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of ensuring trust in large language
  models (LLMs) for critical applications like healthcare by making them consistent,
  reliable, explainable, and safe. The core method is the CREST framework, which integrates
  knowledge graphs and domain-specific guidelines with LLMs through ensemble methods,
  reward functions, and instruction-following tuning.
---

# Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety

## Quick Facts
- arXiv ID: 2312.06798
- Source URL: https://arxiv.org/abs/2312.06798
- Authors: 
- Reference count: 18
- Primary result: CREST framework improves LLM consistency and reliability in healthcare applications through knowledge graph integration and ensemble methods

## Executive Summary
This paper presents the CREST framework to address trust issues in large language models for critical applications like healthcare. The framework combines multiple LLMs with knowledge graphs and domain-specific guidelines through ensemble methods, reward functions, and instruction-following tuning. In a mental health case study using the PRIMATE dataset, CREST demonstrated 6% improvement in PHQ-9 answerability and 21% BLEURT score gains over GPT 3.5, showing enhanced consistency and reliability in generating contextually relevant and safe responses.

## Method Summary
The CREST framework integrates knowledge graphs and clinical guidelines with multiple LLMs through a semi-deep ensemble approach. It uses external knowledge sources as evaluators that provide rewards during training, encouraging alignment with factual knowledge. The framework incorporates procedural knowledge as instructions during training to improve safety, and dynamically adjusts ensemble weights based on knowledge source assessments. This NeuroSymbolic AI approach grounds LLM outputs in structured knowledge rather than relying solely on statistical patterns from training data.

## Key Results
- 6% improvement in PHQ-9 answerability over GPT 3.5
- 21% improvement in BLEURT scores for response quality
- Enhanced consistency in responses to semantically similar queries
- Improved alignment with clinical practice guidelines

## Why This Works (Mechanism)

### Mechanism 1
Integrating knowledge graphs with LLMs through reward functions improves consistency by grounding outputs in factual knowledge. External knowledge sources act as evaluators that provide rewards during training, reducing variability in responses to similar queries. Core assumption: LLMs can be effectively guided by reward functions derived from structured knowledge sources. Break condition: Incomplete or biased knowledge sources may reinforce incorrect information.

### Mechanism 2
Ensembling multiple LLMs with knowledge infusion enhances reliability by compensating for individual model weaknesses. The ensemble combines outputs from diverse LLMs, adjusting contributions based on external knowledge assessments. Core assumption: Different LLMs can complement each other when guided by shared knowledge sources. Break condition: Poor accounting for individual model strengths may not improve reliability.

### Mechanism 3
Using procedural knowledge as instructions during training improves safety by aligning outputs with expert-defined guidelines. Clinical practice guidelines guide LLM responses, ensuring adherence to safety standards. Core assumption: Expert guidelines can be effectively translated into instructions for LLM training. Break condition: Outdated or incomplete guidelines may produce misaligned outputs.

## Foundational Learning

- Concept: Knowledge-infused Learning
  - Why needed here: To integrate domain-specific knowledge into LLMs, enhancing consistency, reliability, and safety
  - Quick check question: How does knowledge-infused learning differ from traditional fine-tuning methods?

- Concept: Ensemble Methods
  - Why needed here: To combine strengths of multiple LLMs, compensating for individual weaknesses
  - Quick check question: What are the advantages of using a semi-deep ensemble over a shallow ensemble?

- Concept: Reward Functions in Reinforcement Learning
  - Why needed here: To guide LLM training by providing feedback based on alignment with external knowledge sources
  - Quick check question: How do reward functions influence the training process of LLMs?

## Architecture Onboarding

- Component map: Knowledge Graphs -> Evaluators -> Reward Functions -> LLMs (Flan T5-XL, T5-XL) -> Ensemble Method -> Final Output
- Critical path:
  1. LLMs generate responses to user queries
  2. Evaluators assess responses using knowledge sources
  3. Reward functions provide feedback to LLMs
  4. LLMs adjust outputs based on rewards
  5. Ensemble method combines outputs from multiple LLMs
- Design tradeoffs: Balancing knowledge source complexity with computational efficiency; ensuring ensemble method combines outputs without introducing bias
- Failure signatures: Inconsistent responses to similar queries; outputs misaligned with factual knowledge; over-reliance on specific knowledge sources
- First 3 experiments:
  1. Evaluate consistency of individual LLMs with and without knowledge infusion
  2. Test reliability of ensemble method by comparing with and without knowledge integration
  3. Assess safety by measuring adherence to expert-defined guidelines

Assumption: Knowledge sources are comprehensive and unbiased. If violated, system may produce unreliable or unsafe outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge-infused ensembling of LLMs be optimized to achieve better reliability than current statistical ensembling methods?
- Basis in paper: The paper discusses knowledge-infused ensemble methods that integrate external knowledge sources, contrasting them with statistical ensembling methods
- Why unresolved: Lacks empirical results comparing knowledge-infused ensembling to traditional statistical methods
- What evidence would resolve it: Empirical studies comparing performance of both methods on tasks requiring consistency, reliability, explainability, and safety

### Open Question 2
- Question: What are the most effective ways to quantify and measure user-level explainability in LLMs for critical applications?
- Basis in paper: The paper highlights need for user-level explainability but notes current metrics are insufficient
- Why unresolved: Does not provide comprehensive framework for quantifying user-level explainability
- What evidence would resolve it: Development and validation of quantitative metrics capturing user-level explainability, tested on real-world applications

### Open Question 3
- Question: How can LLMs be made inherently safe, beyond relying on external guardrails or rules?
- Basis in paper: The paper argues current safety measures are insufficient and intrinsic approach is needed
- Why unresolved: Does not provide detailed methodology for achieving inherent safety or empirical evidence
- What evidence would resolve it: Empirical studies demonstrating effectiveness of grounding LLMs in domain knowledge compared to traditional guardrail approaches

## Limitations
- Single-dataset evaluation limits generalizability across medical domains
- Heavy dependence on knowledge source quality and completeness
- Underspecified implementation details for reward functions and ensemble weights
- Limited safety validation beyond guideline adherence

## Confidence
- High Confidence: General approach of combining knowledge graphs with LLMs through reward-based training is technically sound
- Medium Confidence: Reported performance improvements are plausible but single-dataset evaluation limits generalizability
- Low Confidence: Claims about handling complex safety scenarios and maintaining performance across diverse domains lack empirical support

## Next Checks
1. Test CREST framework on at least two additional medical specialties with different guideline sets to assess generalizability of improvements
2. Systematically introduce controlled errors into knowledge graphs to measure performance degradation and resilience
3. Design adversarial and edge case scenarios to test framework's ability to handle guideline conflicts and potentially harmful inputs