---
ver: rpa2
title: A Survey of AI Text-to-Image and AI Text-to-Video Generators
arxiv_id: '2311.06329'
source_url: https://arxiv.org/abs/2311.06329
tags:
- text-to-image
- text-to-video
- generation
- generators
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of AI text-to-image
  and text-to-video generators, analyzing their architectures, training data, image
  quality, computational requirements, and interpretability. The study examines popular
  models such as CogView2, DALL-E 2, Imagen, Make-A-Video, Phenaki, GODIVA, CogVideo,
  and NUWA.
---

# A Survey of AI Text-to-Image and AI Text-to-Video Generators

## Quick Facts
- arXiv ID: 2311.06329
- Source URL: https://arxiv.org/abs/2311.06329
- Reference count: 0
- One-line primary result: Comprehensive survey analyzing architectures, training data, image quality, computational requirements, and interpretability of text-to-image and text-to-video generators

## Executive Summary
This paper provides a comprehensive survey of AI text-to-image and text-to-video generation models, examining eight major systems including CogView2, DALL-E 2, Imagen, Make-A-Video, Phenaki, GODIVA, CogVideo, and NUWA. The study analyzes their architectures, training data requirements, image quality metrics, computational demands, and interpretability features. Key findings highlight the effectiveness of hierarchical transformer approaches for image generation, the benefits of leveraging frozen language models for text encoding, and the potential of spatiotemporal factorization for video generation without paired text-video data.

## Method Summary
The paper conducts a literature review and comparative analysis of existing text-to-image and text-to-video generation models. The methodology involves gathering technical documentation and research papers for each surveyed model, extracting key specifications including architecture details, training datasets, image quality metrics, computational requirements, and interpretability features. The analysis creates comparative tables and structured discussions following the survey format presented in the paper.

## Key Results
- CogView2 achieves 10x faster high-resolution image generation using hierarchical transformer architecture compared to sliding-window approaches
- Imagen demonstrates superior text-to-image alignment by leveraging frozen T5-XXL language models for text encoding
- Make-A-Video enables video generation from text without requiring paired text-video training data through spatiotemporal factorization

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical transformer-based approaches enable faster high-resolution image generation compared to non-hierarchical methods. CogView2 uses a hierarchical transformer architecture where low-resolution images are generated first, then refined through an iterative super-resolution module using local parallel autoregressive generation. This differs from sliding-window approaches used in earlier models like CogView, which process overlapping windows sequentially. The core assumption is that hierarchical decomposition allows computational efficiency gains while maintaining or improving image quality. Evidence shows CogView2 is 10 times faster than CogView for generating images of similar resolution and better quality. This could break if computational overhead of managing multiple resolution stages exceeds efficiency gains or if quality degradation occurs at refinement stages.

### Mechanism 2
Large-scale frozen language models (like T5-XXL) provide superior text encoding for image generation compared to training encoders from scratch. Imagen leverages a large frozen T5-XXL encoder to encode text into embeddings, which are then mapped to images through a conditional diffusion model. This approach exploits pre-trained language understanding capabilities rather than learning text encoding solely for image generation. The core assumption is that pre-trained language models have already learned rich semantic representations that transfer effectively to image generation tasks. Evidence indicates Imagen achieves state-of-the-art results in terms of FID score and image-text alignment using this approach. This could break if the frozen language model's understanding becomes misaligned with the specific visual domain or if domain-specific terminology requires specialized encoding not captured by general language models.

### Mechanism 3
Spatiotemporally factorized diffusion models enable video generation without requiring text-video paired training data. Make-A-Video extends text-to-image diffusion models to video generation by using a spatiotemporally factorized diffusion model that leverages joint text-image priors. This allows the model to learn video generation capabilities from unpaired text-image and video data. The core assumption is that spatial and temporal generation processes can be effectively separated and learned independently while maintaining coherent video output. Evidence shows this approach eliminates the need for paired text-video data and enables potential scaling to larger amounts of video data. This could break if factorizing spatial and temporal components leads to temporal inconsistencies or if the joint priors fail to capture complex spatiotemporal relationships in videos.

## Foundational Learning

- Concept: Diffusion models and their conditioning mechanisms
  - Why needed here: Multiple models (Imagen, Make-A-Video, Imagen Video) rely on diffusion models as their core generative framework, with text conditioning applied at various stages
  - Quick check question: How does a conditional diffusion model differ from a standard diffusion model in terms of the forward and reverse processes?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: All surveyed models use transformer-based architectures with various attention mechanisms (hierarchical, sparse, 3D attention) that are fundamental to their operation
  - Quick check question: What is the computational complexity of standard self-attention versus sparse attention mechanisms like those used in GODIVA?

- Concept: Multimodal representation learning and cross-modal alignment
  - Why needed here: These systems must align textual descriptions with visual content, requiring understanding of how different modalities can be meaningfully connected
  - Quick check question: What are the key challenges in aligning text embeddings with visual features when training on unpaired text-image datasets?

## Architecture Onboarding

- Component map: Text encoder (T5, CLIP, or custom transformer) -> Latent space representation (discrete tokens via VQ-VAE or continuous embeddings) -> Generative model (diffusion, GAN, or transformer-based decoder) -> Resolution enhancement modules (super-resolution diffusion models) -> Attention mechanisms (standard, sparse, hierarchical, 3D attention)

- Critical path: Text input → Text encoding → Latent space mapping → Initial generation → Resolution enhancement → Output

- Design tradeoffs:
  - Model size vs. computational efficiency (larger models generally perform better but require more resources)
  - Pre-training vs. fine-tuning (frozen encoders save compute but may limit adaptation)
  - Resolution vs. generation speed (higher resolution requires more computational steps)
  - Text-video alignment vs. data requirements (paired data improves alignment but is scarce)

- Failure signatures:
  - Text-video misalignment (generated content doesn't match textual description)
  - Temporal inconsistency in videos (objects change appearance or position unrealistically)
  - Mode collapse (generation becomes repetitive or lacks diversity)
  - Vanishing gradients in deep hierarchical models

- First 3 experiments:
  1. Implement a simple text-to-image diffusion model using a frozen T5 encoder to verify text encoding effectiveness
  2. Test different attention mechanisms (standard vs. sparse) on a small video generation task to measure computational vs. quality tradeoffs
  3. Evaluate the impact of pre-training scale by comparing models trained with different-sized frozen language models on image generation quality

## Open Questions the Paper Calls Out

### Open Question 1
How can we develop evaluation metrics that accurately assess the quality and diversity of generated images and videos while considering factors such as photorealism, text-image/video alignment, and social biases? This question remains unresolved because existing evaluation metrics like FID and IS may not fully capture the nuances of text-to-image and text-to-video generation, such as text-image/video alignment and social biases. Development and validation of new evaluation metrics that consider photorealism, text-image/video alignment, and social biases, along with comparative studies demonstrating their effectiveness over existing metrics, would resolve this question.

### Open Question 2
How can we improve the efficiency and scalability of text-to-image and text-to-video generation models while maintaining or enhancing their quality and diversity? This question remains unresolved because current models often require significant computational resources, limiting their scalability and accessibility. Development of novel architectures, training strategies, or hardware optimizations that improve the efficiency and scalability of text-to-image and text-to-video generation models without compromising quality and diversity would resolve this question.

### Open Question 3
How can we address the issue of social biases and stereotypes in the training data and generated outputs of text-to-image and text-to-video generation models? This question remains unresolved because existing models are often trained on biased datasets, leading to biased and stereotypical outputs. Development of techniques to identify, quantify, and mitigate social biases in the training data and generated outputs, along with studies demonstrating the effectiveness of these techniques in reducing biases and improving fairness, would resolve this question.

## Limitations

- Analysis is constrained by availability of public technical details for proprietary models like DALL-E 2, potentially leading to incomplete architectural understanding
- Computational requirements mentioned are often based on published claims rather than independently verified benchmarks, introducing uncertainty in resource comparisons
- The survey's comparative analysis may be limited by different evaluation protocols used across studies for image quality metrics

## Confidence

High confidence in architectural descriptions of open-source models (CogView2, Imagen, Make-A-Video) based on published papers with detailed technical specifications. Medium confidence in comparative analysis of image quality metrics, as these are often reported using different evaluation protocols across studies. Low confidence in claimed computational efficiency improvements, as many efficiency claims lack standardized benchmarking across different hardware configurations.

## Next Checks

1. Replicate the hierarchical generation approach from CogView2 on a standardized dataset to verify the claimed 10x speed improvement over sliding-window methods, measuring both generation time and image quality using consistent metrics.

2. Implement a controlled comparison of frozen T5 encoders versus trained-from-scratch encoders in diffusion models for text-to-image generation, using identical model architectures and training data to isolate the impact of encoder choice on generation quality.

3. Test the spatiotemporal factorization approach from Make-A-Video by implementing a minimal version that generates short videos from text using only unpaired text-image and video data, measuring temporal consistency and alignment with text prompts.