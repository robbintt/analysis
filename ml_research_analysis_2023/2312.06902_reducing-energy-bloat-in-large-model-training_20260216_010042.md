---
ver: rpa2
title: Reducing Energy Bloat in Large Model Training
arxiv_id: '2312.06902'
source_url: https://arxiv.org/abs/2312.06902
tags:
- energy
- time
- pipeline
- iteration
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Perseus tackles energy bloat in large-scale DNN training by identifying\
  \ and removing two types of inefficiency: intrinsic bloat (from pipeline stage imbalance)\
  \ and extrinsic bloat (from stragglers). It uses a graph-cut-based algorithm to\
  \ characterize the full iteration time\u2013energy Pareto frontier, enabling it\
  \ to minimize energy without throughput loss."
---

# Reducing Energy Bloat in Large Model Training

## Quick Facts
- arXiv ID: 2312.06902
- Source URL: https://arxiv.org/abs/2312.06902
- Authors: 
- Reference count: 40
- Per reduces energy consumption by up to 30% in large-scale DNN training by identifying and removing intrinsic and extrinsic energy bloat through graph-cut-based optimization.

## Executive Summary
Perseus tackles energy bloat in large-scale DNN training by identifying and removing two types of inefficiency: intrinsic bloat (from pipeline stage imbalance) and extrinsic bloat (from stragglers). It uses a graph-cut-based algorithm to characterize the full iteration time–energy Pareto frontier, enabling it to minimize energy without throughput loss. By precisely slowing down non-critical computations and adjusting for stragglers, Perseus reduces energy consumption by up to 30% in experiments on models like GPT-3 and Bloom, outperforming prior approaches. The method is scalable, low-overhead, and provides a principled solution for sustainable, large-scale AI training.

## Method Summary
Perseus identifies energy bloat in pipeline-parallel training through a two-phase approach. First, it profiles computation times and energies during training to construct a DAG representation. Then it uses a graph-cut-based algorithm to characterize the iteration time–energy Pareto frontier, starting from minimum energy and iteratively finding optimal schedules. When stragglers are detected, Perseus looks up the pre-characterized frontier to find the energy-optimal iteration time and adjusts frequencies accordingly. The system consists of a Perseus Server that runs the optimization algorithm and maintains a lookup table, and Perseus Client that profiles computations and controls GPU frequencies via an Asynchronous Frequency Controller.

## Key Results
- Reduces energy consumption by up to 30% in large-scale DNN training
- Outperforms prior approaches like ZeusGlobal in energy efficiency
- Maintains throughput while optimizing energy consumption
- Scalable to models with thousands of computations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perseus reduces intrinsic energy bloat by identifying non-critical computations in the pipeline DAG and slowing them down without affecting iteration time.
- Mechanism: It constructs a computation DAG, removes non-critical computations (those not on any critical path), and uses a graph cut algorithm to select computations to slow down by τ while minimizing energy increase.
- Core assumption: Stage imbalance creates non-critical computations that can be slowed without delaying the critical path.
- Evidence anchors:
  - [abstract] "Perseus obtains the time–energy Pareto frontier of any large model training job using an efficient graph cut-based algorithm, and schedules computation energy consumption across time to reduce both types of energy bloat."
  - [section 2.2] "Such intrinsic energy bloat opens up the opportunity to precisely slow down each non-critical computation in the pipeline such that the length of the critical path does not change."
- Break condition: If stage imbalance is eliminated (perfect balance), there would be no non-critical computations to slow down.

### Mechanism 2
- Claim: Perseus reduces extrinsic energy bloat by slowing down non-straggler pipelines to match the straggler's iteration time without exceeding the energy-optimal point T*.
- Mechanism: When a straggler is detected, it looks up the pre-characterized Pareto frontier to find the energy-optimal iteration time Topt = min(T*, T') and adjusts frequencies accordingly.
- Core assumption: The straggler's iteration time T' is known and can be used to determine how much to slow down non-stragglers.
- Evidence anchors:
  - [abstract] "By precisely slowing down non-critical computations and adjusting for stragglers, Perseus reduces energy consumption by up to 30%..."
  - [section 3.1] "Perseus finds the energy-optimal iteration time for the non-straggler pipeline...by precisely slowing down computations in the pipeline."
- Break condition: If T' is much larger than T*, slowing further would increase energy consumption, so Perseus caps at T*.

### Mechanism 3
- Claim: Perseus efficiently characterizes the entire iteration time–energy Pareto frontier through iterative graph cuts starting from minimum energy schedule.
- Mechanism: It starts with all computations at minimum energy, then iteratively reduces iteration time by τ while using min-cut to find the least energy-increasing modifications.
- Core assumption: The exponential function fit to time-energy measurements provides a good continuous approximation for the discrete optimization problem.
- Evidence anchors:
  - [section 4.2] "we iteratively reduce the pipeline's iteration time by unit time τ (e.g., 1 ms) while increasing total energy minimally, which gives us the next Pareto-optimal energy schedule."
  - [section 4.1] "We fit the exponential function (a·e^(bt) + c) to Pareto-optimal computation time and energy measurements to make the choices continuous."
- Break condition: If the discrete nature of GPU frequencies causes the continuous relaxation to deviate significantly from optimal.

## Foundational Learning

- Concept: Pipeline parallelism and pipeline bubbles
  - Why needed here: Perseus operates on pipeline-parallel training; understanding bubbles is key to why imbalance causes energy waste.
  - Quick check question: What causes pipeline bubbles in data-parallel training, and how do they relate to energy bloat?

- Concept: Directed Acyclic Graphs (DAGs) and critical paths
  - Why needed here: The core algorithm uses DAG representation where computations are edges; identifying critical paths is essential for energy optimization.
  - Quick check question: How do you identify critical path computations in a DAG, and why can non-critical computations be slowed without affecting iteration time?

- Concept: Graph cuts and max-flow min-cut theorem
  - Why needed here: The core optimization uses minimum s-t cuts to find which computations to modify; understanding this is crucial for the algorithm.
  - Quick check question: How does finding a minimum s-t cut in a graph relate to minimizing energy increase when speeding up computations?

## Architecture Onboarding

- Component map:
  Perseus Server -> Perseus Client -> Training Framework
  (Server runs optimization and maintains lookup table; Client profiles and controls frequencies)

- Critical path:
  1. Client profiles computation times/energies during training
  2. Server asynchronously builds Pareto frontier
  3. Client uses minimum-time schedule initially
  4. On straggler detection, server provides optimal schedule for current T'
  5. Client applies new frequency schedule

- Design tradeoffs:
  - Frequency scanning vs. profiling overhead: More frequencies = better optimization but longer profiling
  - τ granularity: Smaller τ = finer-grained frontier but more iterations
  - Online vs. offline profiling: Online is more accurate but adds training overhead

- Failure signatures:
  - Profiling shows energy increasing with lower frequencies (should decrease) → GPU power model incorrect
  - Optimization takes too long → DAG too large or τ too small
  - Energy savings minimal → Stage imbalance already low or frequencies poorly mapped

- First 3 experiments:
  1. Run Perseus on a simple 4-stage pipeline (like GPT-3 1.3B) and verify intrinsic bloat reduction matches expectations
  2. Introduce an artificial straggler (slow one pipeline) and verify extrinsic bloat reduction
  3. Compare Perseus Pareto frontier against ZeusGlobal baseline to verify domination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Perseus's graph-cut algorithm scale effectively to even larger models (e.g., trillion-parameter models) or more complex pipeline schedules (e.g., with more stages or microbatches)?
- Basis in paper: [explicit] The authors discuss the polynomial runtime of their algorithm and test it on models up to 176B parameters, but do not explore extreme-scale scenarios or highly complex pipeline schedules.
- Why unresolved: The paper focuses on models up to 176B parameters and standard pipeline parallelism configurations. Scaling to trillion-parameter models or more intricate schedules may introduce new challenges in terms of runtime efficiency and optimization accuracy.
- What evidence would resolve it: Empirical results showing Perseus's performance on trillion-parameter models or highly complex pipeline schedules, including runtime and energy savings, would provide clarity.

### Open Question 2
- Question: How does Perseus handle dynamic changes in workload characteristics (e.g., varying layer sizes or computation times) during training?
- Basis in paper: [inferred] The paper assumes static DAG representations of training iterations and does not address dynamic changes in workload characteristics that could occur during long training runs.
- Why unresolved: Large model training can span weeks or months, and dynamic changes in workload characteristics (e.g., due to data distribution shifts or model adaptation) could impact the accuracy of Perseus's energy scheduling.
- What evidence would resolve it: A study evaluating Perseus's performance under dynamic workload conditions, including its ability to adapt energy schedules in real-time, would provide insights.

### Open Question 3
- Question: What is the impact of Perseus's energy optimization on model quality or convergence, especially under extrinsic energy bloat reduction scenarios?
- Basis in paper: [explicit] The paper focuses on energy savings without throughput loss but does not investigate potential impacts on model quality or convergence, particularly when pipelines are slowed down to mitigate extrinsic energy bloat.
- Why unresolved: Slowing down pipelines to reduce extrinsic energy bloat could introduce delays or synchronization issues that might affect model convergence or quality, especially in large-scale distributed training.
- What evidence would resolve it: Empirical results comparing model quality and convergence metrics (e.g., loss, accuracy) between Perseus-optimized training and baseline training would clarify this impact.

### Open Question 4
- Question: How does Perseus's performance vary across different GPU architectures (e.g., NVIDIA H100 vs. A100) or non-NVIDIA GPUs?
- Basis in paper: [explicit] The paper evaluates Perseus on A100 and A40 GPUs but does not explore its performance on other GPU architectures, such as the newer H100 or GPUs from other vendors.
- Why unresolved: Different GPU architectures have varying frequency ranges, power management capabilities, and interconnect topologies, which could influence Perseus's effectiveness in reducing energy bloat.
- What evidence would resolve it: Comparative studies of Perseus's energy savings and performance across multiple GPU architectures would provide a clearer picture of its generalizability.

## Limitations

- Experimental scope limited to pipeline parallelism scenarios, not data parallelism where energy bloat might manifest differently
- Assumes consistent straggler patterns and predictable pipeline behavior that may not hold in shared cluster environments
- Frequency control and energy modeling are tailored to NVIDIA GPUs, limiting generalizability to other hardware platforms

## Confidence

**High Confidence**: The core algorithmic approach using graph cuts for Pareto frontier characterization is well-established in optimization theory. The energy savings claims (up to 30%) are supported by controlled experiments on representative large models.

**Medium Confidence**: The claims about reducing both intrinsic and extrinsic energy bloat are theoretically sound, but the relative contribution of each optimization mechanism to total savings isn't clearly separated in the experimental results.

**Low Confidence**: The scalability claims for models with thousands of computations rely on theoretical complexity analysis rather than extensive empirical validation on the largest model scales.

## Next Checks

1. **Cross-Platform Validation**: Test Perseus on AMD GPUs or cloud TPUs to verify that the graph-cut optimization approach generalizes beyond NVIDIA hardware and doesn't depend on vendor-specific power models.

2. **Dynamic Workload Testing**: Evaluate Perseus under realistic cluster conditions with variable resource contention and non-deterministic straggler patterns to assess robustness beyond the controlled single-straggler scenario.

3. **Energy-Saving Breakdown Analysis**: Instrument the system to measure the individual contributions of intrinsic vs. extrinsic bloat reduction mechanisms, and verify the 30% savings claim by decomposing it across different optimization components.