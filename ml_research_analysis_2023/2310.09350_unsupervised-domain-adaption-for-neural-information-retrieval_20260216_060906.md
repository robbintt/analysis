---
ver: rpa2
title: Unsupervised Domain Adaption for Neural Information Retrieval
arxiv_id: '2310.09350'
source_url: https://arxiv.org/abs/2310.09350
tags:
- query
- unsupervised
- cropping
- used
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares unsupervised domain adaptation methods for
  neural information retrieval, specifically focusing on the use of Large Language
  Models (LLMs) and rule-based string manipulation techniques for generating synthetic
  training data. The authors evaluate the effectiveness of these methods on the BEIR
  benchmark, which includes datasets from various domains.
---

# Unsupervised Domain Adaption for Neural Information Retrieval

## Quick Facts
- arXiv ID: 2310.09350
- Source URL: https://arxiv.org/abs/2310.09350
- Reference count: 20
- Primary result: LLM-based synthetic query generation significantly outperforms rule-based methods for unsupervised domain adaptation in neural information retrieval across BEIR benchmark datasets.

## Executive Summary
This paper evaluates unsupervised domain adaptation methods for neural information retrieval, comparing Large Language Models (LLMs) against rule-based string manipulation techniques for generating synthetic training data. The authors demonstrate that LLM-generated queries consistently outperform rule-based approaches across all BEIR benchmark datasets, with a medium-sized 2.7B parameter model providing optimal performance. The study shows that fine-tuning supervised retrievers on LLM-generated synthetic data from target domains yields better adaptation than zero-shot application, even without manual annotations.

## Method Summary
The approach uses an SBERT-based bi-encoder trained on MS-MARCO as the base retriever, which is then fine-tuned using synthetic query-document pairs generated from target domain documents. Query generation is performed using OPT LLM checkpoints of varying sizes (125M to 13B parameters) with top-p sampling (p=0.9), compared against Contriever's rule-based independent cropping method. The synthetic data is created by generating queries from documents in BEIR datasets, then using these pairs to fine-tune the base retriever. Evaluation uses nDCG@10 and Recall@100 metrics across 14 BEIR datasets in both zero-shot and unsupervised adaptation scenarios.

## Key Results
- LLM-based query generation outperforms rule-based methods by a large margin across all BEIR datasets
- Medium-sized LLM (2.7B parameters) provides optimal balance between performance and computational cost, with no significant gains from larger models
- Unsupervised domain adaptation via LLM-generated synthetic data significantly improves retrieval performance compared to zero-shot application of supervised models
- Sampling with top-p=0.9 provides good query diversity while maintaining semantic coherence

## Why This Works (Mechanism)

### Mechanism 1
LLM-based synthetic query generation produces higher-quality queries than rule-based cropping methods. LLMs generate queries that better capture semantic intent and natural language structure, while cropping methods produce fragmented or semantically weak queries. The synthetic query-document pairs are used directly as training labels without further filtering. If the prompt fails to constrain the LLM output, the generated queries may drift from document content, reducing their utility as training labels.

### Mechanism 2
Fine-tuning a supervised retriever on LLM-generated synthetic data from the target domain yields better adaptation than zero-shot application. The fine-tuning step adapts the model's embeddings to the distributional characteristics of the target domain, leveraging in-domain document structures even without in-domain manual annotations. The synthetic data must be sufficiently representative of true in-domain query-document relationships to enable effective adaptation. If the synthetic data is too noisy or unrepresentative, fine-tuning may diverge from the target domain's true retrieval patterns.

### Mechanism 3
A medium-sized LLM (2.7B parameters) suffices for effective query generation, with no further performance gains from larger models. Larger models may overfit to the prompt or produce more repetitive queries, while medium-sized models balance generation quality and diversity. Query quality plateaus beyond 2.7B parameters and computational cost increases outweigh marginal gains. If prompt design changes or if the target domain requires more complex query generation, larger models might become beneficial again.

## Foundational Learning

- **Concept: Cosine similarity for dense retrieval**
  - Why needed here: The SBERT-based bi-encoder uses cosine similarity to score query-document relevance; choosing the right similarity metric affects retrieval quality.
  - Quick check question: If you compute cosine similarity between two identical embeddings, what value do you get? (Answer: 1.0)

- **Concept: Contrastive learning with in-batch negatives**
  - Why needed here: The bi-encoder is trained using in-batch negatives to pull positive query-document pairs closer and push negative pairs apart in embedding space.
  - Quick check question: In a batch of size B, how many negative passages are available per positive example? (Answer: B - 1)

- **Concept: Unsupervised domain adaptation**
  - Why needed here: The system adapts a model trained on MS-MARCO to target domains without manual annotations, using synthetic data generated from target domain documents.
  - Quick check question: What is the difference between zero-shot and unsupervised domain adaptation? (Answer: Zero-shot applies a model trained on source data directly; adaptation fine-tunes it on synthetic in-domain data.)

## Architecture Onboarding

- **Component map:** Document retriever (SBERT bi-encoder) -> Query generator (OPT LLM) -> Synthetic data pipeline -> Fine-tuned retriever -> BEIR evaluation harness

- **Critical path:**
  1. Load and preprocess MS-MARCO for supervised training
  2. Generate synthetic queries for BEIR target documents using LLM
  3. Fine-tune supervised model on synthetic data
  4. Evaluate on BEIR test sets
  5. Log metrics and compare with baselines

- **Design tradeoffs:**
  - LLM size vs. generation time: 2.7B balances quality and speed
  - Query diversity vs. semantic fidelity: Sampling with p=0.9 allows diversity but may produce low-quality queries if p is too high
  - Fine-tuning vs. full retraining: Fine-tuning is faster but may overfit to synthetic data

- **Failure signatures:**
  - Low nDCG@10: LLM generation failed (empty queries, prompt mismatch) or synthetic data too noisy
  - Overfitting: High training but low validation performance; reduce synthetic data size or epochs
  - Out-of-memory: Reduce batch size or switch to smaller LLM checkpoint

- **First 3 experiments:**
  1. Run zero-shot evaluation on MS-MARCO-trained model across all BEIR datasets to establish baseline
  2. Generate synthetic queries for a single BEIR dataset (e.g., TREC-COVID) with 2.7B OPT, verify output quality
  3. Fine-tune model on synthetic TREC-COVID data and evaluate; compare with zero-shot result

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sampling technique and parameter settings for query generation using LLMs in the context of unsupervised domain adaptation for neural information retrieval?
- Basis in paper: The paper mentions using sampling with a top p of 0.9 for generating queries with LLMs, but also states that they plan to explore other sampling techniques and strategies in future work.
- Why unresolved: The current study only uses one sampling technique (top p of 0.9) and does not explore the impact of different sampling methods or parameter settings on the performance of the system.
- What evidence would resolve it: Conduct experiments using different sampling techniques (e.g., top k, temperature) and various parameter settings to determine the optimal combination for query generation in unsupervised domain adaptation scenarios.

### Open Question 2
- Question: How do different types of sentence similarities (e.g., Euclidean distance, Manhattan distance) impact the performance of neural IR models in unsupervised domain adaptation?
- Basis in paper: The paper mentions that they plan to study the impact of other types of sentence similarities on the performance of their models in the future.
- Why unresolved: The current study only uses cosine similarity for measuring the similarity between query and context embeddings. The impact of other similarity metrics on the performance of neural IR models in unsupervised domain adaptation is not explored.
- What evidence would resolve it: Conduct experiments using different similarity metrics (e.g., Euclidean distance, Manhattan distance) and compare their performance with cosine similarity in unsupervised domain adaptation scenarios.

### Open Question 3
- Question: What is the impact of using larger retrievers (e.g., ColBERT, SPLADE) compared to the SBERT model used in this study on the performance of neural IR systems in unsupervised domain adaptation?
- Basis in paper: The paper mentions that they plan to examine the use of retrievers with more parameters and the use of rerankers to improve performance in future work.
- Why unresolved: The current study only uses the SBERT model for the neural IR system. The impact of using larger, more complex retrievers on the performance of neural IR systems in unsupervised domain adaptation is not explored.
- What evidence would resolve it: Conduct experiments using larger retrievers (e.g., ColBERT, SPLADE) and compare their performance with the SBERT model in unsupervised domain adaptation scenarios.

## Limitations

- The study relies on a single prompt format described only as "vanilla version of prompts used by InPars," lacking specificity for reproducibility
- Only 14 of 18 BEIR datasets were used in experiments without clear justification for dataset exclusion
- The paper does not validate synthetic query quality against ground truth, assuming generated queries are semantically equivalent to real queries

## Confidence

- **High confidence**: The finding that medium-sized LLMs (2.7B parameters) suffice for effective query generation, supported by direct empirical comparison across multiple model sizes with clear performance plateaus
- **Medium confidence**: The superiority of LLM-based adaptation over zero-shot application, though this requires assuming the synthetic data adequately represents true query-document relationships without validation against ground truth
- **Medium confidence**: The claim that LLMs produce more semantically coherent queries than rule-based cropping, based on the stated mechanism but lacking direct quantitative comparison of query quality metrics

## Next Checks

1. Test prompt sensitivity by generating synthetic queries using 3-5 different prompt variants and measuring impact on retrieval performance across all BEIR datasets
2. Implement a query quality assessment pipeline that measures semantic similarity between generated queries and reference queries (where available) to validate the claim about LLM superiority
3. Conduct an ablation study comparing fine-tuning on synthetic data versus direct cross-domain application without adaptation to quantify the actual benefit of the unsupervised adaptation approach