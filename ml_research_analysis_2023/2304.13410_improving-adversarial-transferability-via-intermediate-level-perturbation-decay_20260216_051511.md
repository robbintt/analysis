---
ver: rpa2
title: Improving Adversarial Transferability via Intermediate-level Perturbation Decay
arxiv_id: '2304.13410'
source_url: https://arxiv.org/abs/2304.13410
tags:
- adversarial
- intermediate-level
- perturbation
- attacks
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method to improve the adversarial transferability
  of black-box attacks. The key idea is to encourage the intermediate-level perturbation
  in the feature space to possess a greater magnitude than a directional guide and
  to be in the same adversarial direction as that of the guide.
---

# Improving Adversarial Transferability via Intermediate-level Perturbation Decay

## Quick Facts
- arXiv ID: 2304.13410
- Source URL: https://arxiv.org/abs/2304.13410
- Reference count: 40
- Primary result: Improves adversarial transferability via intermediate-level perturbation decay, outperforming state-of-the-art methods by large margins on ImageNet (+10.07% on average) and CIFAR-10 (+3.88% on average)

## Executive Summary
This paper addresses the challenge of adversarial transferability in black-box attacks by introducing Intermediate-Level Perturbation Decay (ILPD). The method encourages intermediate-level perturbations in the feature space to possess greater magnitude while maintaining the same adversarial direction as a guide. Unlike two-stage approaches that can lead to suboptimal performance due to deviations from the guide, ILPD achieves this within a single stage of optimization. The authors demonstrate significant improvements in transferability across various victim models on both ImageNet and CIFAR-10 datasets.

## Method Summary
The method introduces ILPD to improve adversarial transferability by modifying the gradient computation during optimization. Specifically, ILPD replaces the gradient with respect to the current intermediate-level perturbation with the gradient with respect to a decayed perturbation (1/γ * z(j)_h + (1-1/γ) * z(0)_h). This encourages larger perturbation magnitudes while maintaining alignment with the adversarial direction. The approach is evaluated against state-of-the-art methods on CIFAR-10 and ImageNet, showing substantial improvements in attack success rates.

## Key Results
- Outperforms state-of-the-art methods by +10.07% on average for ImageNet attacks
- Achieves +3.88% improvement on average for CIFAR-10 attacks
- Demonstrates better gradient alignment between substitute and victim models
- Shows effectiveness across various victim model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ILPD improves adversarial transferability by encouraging larger perturbation magnitudes in the feature space while maintaining the same adversarial direction as the guide.
- Mechanism: ILPD modifies the gradient computation by replacing the gradient w.r.t. the current intermediate-level perturbation with the gradient w.r.t. a decayed perturbation (1/γ * z(j)_h + (1-1/γ) * z(0)_h). This encourages larger perturbation magnitudes while staying in the same adversarial direction as the guide.
- Core assumption: The gradient of the prediction loss w.r.t. a less adversarial intermediate-level perturbation (i.e., the decayed perturbation) is better aligned with the gradient of the victim model than the gradient w.r.t. a more adversarial perturbation.
- Evidence anchors:
  - [abstract] "The proposed ILPD method outperforms state-of-the-art methods by large margins in attacking various victim models on ImageNet (+10.07% on average) and CIFAR-10 (+3.88% on average)."
  - [section 4.2] "Figure 4 shows that ILPD leads to more aligned input gradient across models, especially after the first 20 iterations."
  - [corpus] Weak evidence - the corpus contains related papers but does not directly address the mechanism of ILPD.

### Mechanism 2
- Claim: ILPD improves gradient alignment between substitute and victim models, which is a key factor in adversarial transferability.
- Mechanism: By using a less adversarial intermediate-level perturbation (the decayed perturbation), ILPD leads to less incorrect predictions on the substitute model. This results in better alignment of the input gradients and prediction probabilities between the substitute and victim models.
- Core assumption: The difference in prediction probabilities and activation masks across models is a major obstacle to adversarial transferability, and reducing this difference improves transferability.
- Evidence anchors:
  - [section 4.1] "Replacing ∇z_g L(z_g, y) seems more effective at initial iterations while replacing the activation masks leads to slightly more improvement at the end."
  - [section 4.2] "Figure 4 shows that ILPD leads to more aligned input gradient across models, especially after the first 20 iterations."
  - [corpus] Weak evidence - the corpus contains related papers but does not directly address the mechanism of gradient alignment in ILPD.

### Mechanism 3
- Claim: ILPD overcomes the limitations of two-stage intermediate-level attacks by encouraging larger perturbation magnitudes in the feature space within a single stage of optimization.
- Mechanism: Traditional two-stage attacks first determine a directional guide and then maximize the scalar projection of the intermediate-level perturbation onto this guide. This can lead to deviations from the guide and sub-optimal attack performance. ILPD, on the other hand, encourages larger perturbation magnitudes in the feature space while staying in the same adversarial direction as the guide, all within a single stage of optimization.
- Core assumption: The deviation from the directional guide in two-stage attacks leads to sub-optimal attack performance, and a single-stage approach that encourages larger perturbation magnitudes in the feature space can overcome this limitation.
- Evidence anchors:
  - [section 3.1] "The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack."
  - [section 3.2] "Given Figure 1, one may consider that a straightforward improvement of the intermediate-level attacks is to instead maximize the magnitude of (intermediate-level) perturbation only along the direction of v."
  - [corpus] Weak evidence - the corpus contains related papers but does not directly address the mechanism of overcoming the limitations of two-stage attacks in ILPD.

## Foundational Learning

- Concept: Adversarial examples and transferability
  - Why needed here: Understanding the concept of adversarial examples and transferability is crucial for grasping the problem that ILPD aims to solve and the significance of its results.
  - Quick check question: What is the difference between white-box and black-box attacks, and how does transferability play a role in black-box attacks?

- Concept: Intermediate-level attacks and perturbation
  - Why needed here: ILPD is an intermediate-level attack method, so understanding the concept of intermediate-level attacks and perturbation is essential for understanding how ILPD works and how it differs from other attack methods.
  - Quick check question: What is the difference between input-level and intermediate-level attacks, and why might intermediate-level attacks be more effective in certain scenarios?

- Concept: Gradient alignment and its impact on transferability
  - Why needed here: ILPD improves gradient alignment between substitute and victim models, which is a key factor in adversarial transferability. Understanding the concept of gradient alignment and its impact on transferability is crucial for understanding the mechanism behind ILPD's effectiveness.
  - Quick check question: How does the diversity in input gradients across models affect adversarial transferability, and what are some factors that contribute to this diversity?

## Architecture Onboarding

- Component map: Input image -> Substitute model -> Intermediate layer -> ILPD decay -> Gradient computation -> Output adversarial example
- Critical path:
  1. Compute the decayed intermediate-level perturbation (1/γ * z(j)_h + (1-1/γ) * z(0)_h)
  2. Compute the gradient of the prediction loss w.r.t. the decayed perturbation
  3. Backpropagate the gradient to the input and update the adversarial example
  4. Repeat steps 1-3 for a fixed number of iterations or until a stopping criterion is met
- Design tradeoffs:
  - Choice of intermediate layer: Affects the effectiveness of the attack and the computational cost
  - Choice of decay factor (γ): Balances the magnitude of the perturbation and the adversarial direction
  - Number of iterations: Affects the strength of the attack and the computational cost
- Failure signatures:
  - Poor transferability: Adversarial examples do not transfer well to victim models
  - High computational cost: The attack takes too long to generate adversarial examples
  - Overfitting: The attack overfits to the substitute model and does not generalize well to other models
- First 3 experiments:
  1. Test the effect of different choices of intermediate layers on the attack performance
  2. Test the effect of different decay factors (γ) on the attack performance
  3. Compare the transferability of ILPD-generated adversarial examples to those generated by other state-of-the-art attack methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of γ in ILPD affect adversarial transferability across different model architectures and datasets?
- Basis in paper: [explicit] The paper evaluates ILPD with varying γ on ImageNet and CIFAR-10, showing optimal performance with 1/γ = 0.1 (γ = 10) for ImageNet and 1/γ = 0.3 (γ = 10/3) for CIFAR-10, but does not provide a comprehensive analysis across diverse architectures.
- Why unresolved: The optimal γ appears to be dataset and architecture-dependent, but the paper does not explore this systematically or provide guidelines for selecting γ in novel settings.
- What evidence would resolve it: Systematic experiments varying γ across multiple model architectures (e.g., CNNs, transformers, MLPs) and datasets, with analysis of the relationship between γ, model complexity, and transferability.

### Open Question 2
- Question: What is the theoretical foundation for why decaying intermediate-level perturbations improves gradient alignment between substitute and victim models?
- Basis in paper: [explicit] The paper analyzes ILPD from a gradient alignment perspective, showing it leads to more aligned input gradients, but does not provide a rigorous theoretical explanation for this phenomenon.
- Why unresolved: The empirical observation that ILPD improves gradient alignment is demonstrated, but the underlying mechanism is not fully explained through theoretical analysis or mathematical proofs.
- What evidence would resolve it: Formal mathematical proofs or theoretical models demonstrating how decaying intermediate-level perturbations affects gradient alignment, possibly leveraging concepts from optimization theory or information geometry.

### Open Question 3
- Question: How does ILPD perform when combined with query-based black-box attack methods, and what are the trade-offs between query efficiency and transferability?
- Basis in paper: [explicit] The paper combines ILPD with transfer-based methods like MI-FGSM, DI2-FGSM, and Admix, showing improved transferability, but does not explore combinations with query-based methods.
- Why unresolved: The paper focuses on transfer-based attacks and does not investigate how ILPD could enhance query-based black-box attacks, which could provide insights into balancing query efficiency and attack success.
- What evidence would resolve it: Experiments combining ILPD with query-based methods like HopSkipJumpAttack or AutoZoom, measuring both query efficiency and transferability success rates across different victim models and datasets.

## Limitations
- Limited theoretical analysis of why ILPD improves gradient alignment and transferability
- Choice of intermediate layer and decay factor appears to be empirical rather than principled
- Does not explore combinations with query-based black-box attack methods

## Confidence
- Mechanism 1 (ILPD improves transferability by encouraging larger perturbation magnitudes): Low
- Mechanism 2 (ILPD improves gradient alignment): Medium
- Mechanism 3 (ILPD overcomes limitations of two-stage attacks): Low

## Next Checks
1. Perform ablation studies to isolate the effect of ILPD on gradient alignment and perturbation magnitude, and to determine the optimal choice of intermediate layer and decay factor.
2. Conduct theoretical analysis to explain the relationship between perturbation decay, adversarial direction, and gradient alignment, and to provide a more principled understanding of ILPD's effectiveness.
3. Evaluate the transferability of ILPD-generated adversarial examples to a wider range of victim models and datasets to assess the generalizability of the results.