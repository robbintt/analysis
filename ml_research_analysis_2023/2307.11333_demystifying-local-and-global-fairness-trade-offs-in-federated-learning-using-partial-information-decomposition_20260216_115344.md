---
ver: rpa2
title: Demystifying Local and Global Fairness Trade-offs in Federated Learning Using
  Partial Information Decomposition
arxiv_id: '2307.11333'
source_url: https://arxiv.org/abs/2307.11333
tags:
- disparity
- local
- fairness
- global
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an information-theoretic framework to analyze\
  \ trade-offs between local and global fairness in federated learning (FL). Using\
  \ partial information decomposition (PID), the authors identify three sources of\
  \ unfairness\u2014unique, redundant, and masked disparity\u2014and decompose global\
  \ and local fairness into these components."
---

# Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition

## Quick Facts
- arXiv ID: 2307.11333
- Source URL: https://arxiv.org/abs/2307.11333
- Reference count: 40
- This paper introduces an information-theoretic framework to analyze trade-offs between local and global fairness in federated learning using partial information decomposition.

## Executive Summary
This paper addresses a fundamental challenge in federated learning: understanding the relationship between local and global fairness. Using partial information decomposition (PID), the authors identify three distinct sources of unfairness—unique, redundant, and masked disparity—that affect how well local fairness guarantees translate to global fairness. The framework reveals that achieving global fairness through local fairness is fundamentally limited when redundant disparity exists, and vice versa for masked disparity. The theoretical results establish conditions under which one form of fairness implies the other, providing a foundation for designing more effective fairness-aware federated learning systems.

## Method Summary
The method employs PID to decompose global and local fairness disparities in federated learning. Global disparity is measured as mutual information I(Z; Ŷ) between sensitive attribute Z and model predictions Ŷ, while local disparity is measured as I(Z; Ŷ|S) conditioned on client identity S. Using PID, these disparities are decomposed into unique, redundant, and masked components. The framework is validated experimentally on the Adult dataset using federated learning with FedAvg aggregation, where data heterogeneity is controlled through Dirichlet distribution parameters. The decomposition reveals which type of disparity dominates under different heterogeneity scenarios, providing insights for targeted fairness mitigation strategies.

## Key Results
- Global fairness cannot be achieved solely through local fairness when redundant disparity exists, as shown in Theorem 1
- Local fairness cannot be guaranteed from global fairness due to masked disparity, as shown in Theorem 2
- Under specific conditions (e.g., ˆY ⊥ S or zero redundant disparity), local fairness can imply global fairness and vice versa, as established in Theorems 3-5
- Experimental results on Adult dataset validate theoretical findings, demonstrating how different data heterogeneity scenarios lead to distinct types of disparities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial information decomposition (PID) can identify three distinct sources of unfairness in federated learning: unique, redundant, and masked disparity.
- Mechanism: PID decomposes mutual information I(Z; (A,B)) into non-negative terms (unique, redundant, synergistic) that isolate how sensitive attribute Z relates to model predictions Ŷ and client identity S. This decomposition reveals whether unfairness arises from model dependence on Z alone (unique), from client-sensitive attribute correlation (redundant), or from interactions between Ŷ and S that cancel out when viewed globally (masked).
- Core assumption: The PID framework from information theory accurately captures the sources of group fairness disparities in FL.
- Evidence anchors:
  - [abstract] "We leverage a body of work in information theory called partial information decomposition (PID) which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity."
  - [section 3.1] Proposition 1 formalizes the decomposition of global and local disparity into these PID terms.
  - [corpus] Weak evidence - only 1 neighbor paper directly mentions PID in fairness context; corpus is sparse on PID-specific claims.
- Break condition: If PID terms cannot be reliably computed or if the underlying assumptions about information structure don't hold (e.g., continuous variables poorly approximated by discrete binning).

### Mechanism 2
- Claim: Global fairness does not imply local fairness and vice versa due to the presence of masked and redundant disparities respectively.
- Mechanism: Theorem 1 shows that even with zero local disparity (unique + masked = 0), global disparity can be nonzero if redundant disparity exists. Theorem 2 shows that even with zero global disparity (unique + redundant = 0), local disparity can be nonzero if masked disparity exists. The difference I(Z; Ŷ) - I(Z; Ŷ|S) = I(Z; Ŷ; S) quantifies this tradeoff.
- Core assumption: The PID decomposition accurately represents the information-theoretic limits of fairness tradeoffs.
- Evidence anchors:
  - [abstract] "We show the limitations of achieving global fairness using local fairness due to the redundant disparity (see Theorem 1) and the limitations of achieving local fairness using global fairness due to the masked disparity (see Theorem 2)."
  - [section 3.2] Formal proofs of Theorems 1 and 2 using PID decomposition.
  - [corpus] Moderate evidence - neighbor papers discuss cost of local vs global fairness but don't provide PID-based proofs.
- Break condition: If the interaction information I(Z; Ŷ; S) is zero or if the PID decomposition fails to capture the true sources of disparity.

### Mechanism 3
- Claim: Under certain conditions, local fairness can imply global fairness and vice versa by eliminating specific disparities.
- Mechanism: Theorem 3 states that if local disparity goes to zero and redundant disparity is zero, global disparity is zero. Theorem 4 shows that if local disparity is zero and Ŷ ⊥ S, global disparity is zero. Theorem 5 shows that if masked disparity is zero, local disparity ≤ global disparity. These conditions can be achieved through data heterogeneity control or model design.
- Core assumption: The conditions identified (e.g., Ŷ ⊥ S) are practically achievable through model design or data preprocessing.
- Evidence anchors:
  - [abstract] "We identify the conditions under which one form of fairness (local or global) implies the other. Specifically, we have established conditions under which local fairness can result in global fairness (Theorem 4) and conditions under which global fairness can result in local fairness (Theorem 5)."
  - [section 3.3] Theorems 3, 4, and 5 with their proofs.
  - [corpus] Weak evidence - neighbor papers discuss fairness conditions but not through PID-based necessary/sufficient conditions.
- Break condition: If the independence conditions cannot be enforced (e.g., Ŷ ⊥ S is impossible due to dataset constraints) or if the PID terms cannot be reduced to zero through optimization.

## Foundational Learning

- Concept: Partial Information Decomposition (PID)
  - Why needed here: PID provides the mathematical framework to decompose global and local fairness into distinct sources of unfairness, enabling analysis of tradeoffs that cannot be captured by traditional mutual information alone.
  - Quick check question: What are the three non-negative terms that PID decomposes I(Z; (A,B)) into, and what does each represent?

- Concept: Mutual Information and Conditional Mutual Information
  - Why needed here: These information-theoretic measures quantify global disparity (I(Z; Ŷ)) and local disparity (I(Z; Ŷ|S)), providing the foundation for PID decomposition and fairness analysis.
  - Quick check question: How is local disparity defined in terms of conditional mutual information, and what does it represent in the FL context?

- Concept: Statistical Parity and its relationship to Mutual Information
  - Why needed here: The paper establishes connections between information-theoretic measures and standard fairness metrics, showing that zero mutual information corresponds to statistical parity.
  - Quick check question: What is the relationship between global statistical parity gap and I(Z; Ŷ) when they are non-zero, according to Lemma 1?

## Architecture Onboarding

- Component map: K federated clients → Central server → Model fθ → Predictions Ŷ; PID analysis operates on joint distribution of Z, Ŷ, and S
- Critical path: Data heterogeneity → Model training via FedAvg → Compute global/local disparity → PID decomposition → Identify disparity sources → Apply mitigation strategies based on identified disparities
- Design tradeoffs: Achieving global fairness may require sacrificing local fairness due to masked disparity, while achieving local fairness may fail to ensure global fairness due to redundant disparity. The tradeoff depends on data heterogeneity and model design choices.
- Failure signatures: Persistent global disparity despite local fairness mitigation indicates redundant disparity; persistent local disparity despite global fairness indicates masked disparity; failure to reduce either suggests unique disparity dominates.
- First 3 experiments:
  1. Implement FedAvg with varying data heterogeneity (α values) and measure PID decomposition to identify which disparity type dominates.
  2. Apply local fairness regularization and observe its effect on unique, redundant, and masked disparities across different heterogeneity scenarios.
  3. Design model architecture to enforce Ŷ ⊥ S and measure reduction in redundant disparity and improvement in global fairness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical bounds on the trade-off between global and local fairness under different data heterogeneity scenarios?
- Basis in paper: [explicit] The paper mentions deriving "fundamental information-theoretic limits and trade-offs between global and local disparity" and discusses limitations in achieving global fairness using local fairness due to redundant disparity, and vice versa due to masked disparity.
- Why unresolved: While the paper provides conditions under which one form of fairness implies the other (e.g., Theorems 1-5), it doesn't explicitly quantify the bounds on this trade-off for varying degrees of data heterogeneity.
- What evidence would resolve it: Experimental results showing how the trade-off between global and local fairness changes with different levels of data heterogeneity, or theoretical proofs establishing bounds on this trade-off under various conditions.

### Open Question 2
- Question: How does the choice of PID computation method affect the decomposition of global and local disparity into unique, redundant, and masked components?
- Basis in paper: [explicit] The paper uses a specific definition of unique information from Bertschinger et al. (2014) and mentions that "alternative PID computation techniques" exist (referencing works like Gurushankar et al., 2022; Venkatesh & Schamberg, 2022; Pakman et al., 2021).
- Why unresolved: The paper doesn't compare results using different PID computation methods, leaving open the question of how sensitive the decomposition is to the choice of method.
- What evidence would resolve it: Re-running the experiments with different PID computation methods and comparing the resulting decompositions and derived conclusions.

### Open Question 3
- Question: How do other fairness metrics (e.g., equalized odds) interact with the PID decomposition of global and local fairness in federated learning?
- Basis in paper: [explicit] The paper mentions that "Future studies could also investigate how this approach could be extended to other fairness metrics, such as equalized odds" in the conclusions section.
- Why unresolved: The current framework focuses on statistical parity, but other fairness metrics may capture different aspects of fairness and could interact differently with the unique, redundant, and masked disparities.
- What evidence would resolve it: Extending the PID framework to other fairness metrics and comparing how the decomposition changes, potentially leading to different insights about fairness trade-offs in FL.

## Limitations
- PID decomposition reliability for continuous variables may be compromised by discretization approximation errors
- Experimental validation limited to single dataset (Adult) and binary classification task
- Theoretical assumptions of iid local models and Bernoulli-distributed Ŷ may not hold in practical FL deployments

## Confidence
- High confidence: The theoretical framework connecting PID to fairness trade-offs is mathematically sound and the fundamental limits (Theorems 1-2) are well-established
- Medium confidence: The experimental results support the theoretical claims but are limited in scope and diversity of scenarios tested
- Low confidence: The practical achievability of conditions in Theorems 3-5 (e.g., enforcing Ŷ ⊥ S) requires further validation across different FL systems and datasets

## Next Checks
1. Validate PID decomposition accuracy by comparing continuous vs discretized variable implementations on synthetic datasets where ground truth disparities are known
2. Test the framework on additional datasets (e.g., COMPAS, German Credit) with different sensitive attributes and classification tasks to assess generalizability
3. Implement practical strategies to enforce Ŷ ⊥ S and measure their effectiveness in reducing redundant disparity across different data heterogeneity levels