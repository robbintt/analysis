---
ver: rpa2
title: Large Language Models for Propaganda Detection
arxiv_id: '2310.06422'
source_url: https://arxiv.org/abs/2310.06422
tags:
- propaganda
- gpt-4
- detection
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the application of Large Language Models
  (LLMs) like GPT-3 and GPT-4 for propaganda detection in text. Propaganda detection
  is challenging due to subtle manipulation techniques and contextual dependencies.
---

# Large Language Models for Propaganda Detection

## Quick Facts
- arXiv ID: 2310.06422
- Source URL: https://arxiv.org/abs/2310.06422
- Reference count: 15
- GPT-4 achieves comparable results to the current state-of-the-art RoBERTa model for propaganda detection

## Executive Summary
This paper investigates the application of Large Language Models (LLMs) like GPT-3 and GPT-4 for propaganda detection in text. Propaganda detection is challenging due to subtle manipulation techniques and contextual dependencies. The study uses the SemEval-2020 task 11 dataset featuring news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies. The models are evaluated using metrics such as F1 score, precision, and recall, and compared to the state-of-the-art approach using RoBERTa. The results demonstrate that GPT-4 achieves comparable results to the current state-of-the-art, highlighting the potential of LLMs for propaganda detection. However, challenges such as overfitting, hallucinations, and the need for sufficient training data are also discussed.

## Method Summary
The study employs five variations of GPT-3 and GPT-4 models using the OpenAI API to perform multi-label classification of propaganda techniques. These variations include base models with and without instructional prompts, and chain-of-thought prompting strategies. The models are evaluated on the SemEval-2020 task 11 dataset using micro-averaged F1 score, precision, and recall metrics. Temperature is set to 0 to ensure deterministic outputs. The paper compares LLM performance to a RoBERTa baseline and analyzes results across different propaganda techniques.

## Key Results
- GPT-4 achieves comparable results to the current state-of-the-art RoBERTa model for propaganda detection
- GPT-4 chain-of-thought model displayed higher precision, indicating a more discerning approach to prediction
- GPT-4 is superior to GPT-3 in understanding linguistic features beneficial for the subjective task of propaganda detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's zero-shot or few-shot prompting achieves performance comparable to fine-tuned transformer models on propaganda detection.
- Mechanism: GPT-4's pre-training on vast amounts of text captures nuanced linguistic features and contextual dependencies required to detect subtle propaganda techniques without explicit task-specific fine-tuning.
- Core assumption: The training data of GPT-4 includes diverse examples of rhetorical and psychological manipulation techniques present in propaganda.
- Evidence anchors:
  - [abstract]: "GPT-4 achieves comparable results to the current state-of-the-art" (comparing to RoBERTa).
  - [section]: "GPT-4 being superior in the understanding of linguistic features over GPT-3 proves beneficial for the subjective task of propaganda detection."
- Break condition: If the training data lacks sufficient examples of certain propaganda techniques, performance will degrade for those labels.

### Mechanism 2
- Claim: The 'chain of thought' prompting strategy improves precision by requiring the model to justify each prediction.
- Mechanism: Instructing the model to provide reasoning for each classification decision acts as an internal filter, reducing false positives by only labeling when confident reasoning exists.
- Core assumption: The reasoning process generated by the model is based on actual detection of propaganda features, not just hallucination.
- Evidence anchors:
  - [section]: "GPT-4 'chain of thought' model displayed a higher Precision, indicating a more discerning approach to prediction."
  - [section]: "While GPT-4 generated plausible reasoning for each classified label, GPT-3 was unable to do so."
- Break condition: If the reasoning is based on hallucination rather than genuine detection, precision gains are illusory and misleading.

### Mechanism 3
- Claim: Setting the temperature parameter to 0 increases reproducibility of results for text classification tasks.
- Mechanism: Temperature controls randomness in output generation; setting it to 0 makes the model deterministic, ensuring consistent outputs across inference calls.
- Core assumption: The underlying model's predictions are stable and not influenced by random variations when temperature is set to 0.
- Evidence anchors:
  - [section]: "To ensure reproducible outputs, we set the temperature parameter to 0, such that the output becomes more deterministic."
  - [section]: "We reduce the fear of non-deterministic results."
- Break condition: If the model's underlying predictions vary due to other sources of randomness or model instability, reproducibility is not guaranteed.

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: Propaganda detection involves identifying multiple techniques within a single text, requiring the model to output multiple labels per article.
  - Quick check question: How does multi-label classification differ from multi-class classification, and why is it important for propaganda detection?

- Concept: Prompt engineering patterns
  - Why needed here: Different prompting strategies (few-shot, chain of thought) significantly impact model performance on complex tasks like propaganda detection.
  - Quick check question: What are the key differences between few-shot and chain of thought prompting, and when might each be more effective?

- Concept: Propaganda techniques and their linguistic markers
  - Why needed here: Understanding the 14 propaganda techniques defined in the dataset is crucial for evaluating model performance and interpreting results.
  - Quick check question: Can you list three propaganda techniques and describe their linguistic characteristics?

## Architecture Onboarding

- Component map: News articles -> GPT-3/GPT-4 models with prompt engineering -> Multi-label propaganda technique classification -> Evaluation metrics
- Critical path:
  1. Load and preprocess dataset
  2. Construct prompts with appropriate engineering strategies
  3. Generate predictions using OpenAI API
  4. Evaluate results using standard metrics
  5. Analyze performance per propaganda technique

- Design tradeoffs:
  - GPT-4 vs. fine-tuned GPT-3: Higher performance but potentially higher cost
  - Few-shot vs. chain of thought: Trade-off between coverage and precision
  - Temperature setting: Determinism vs. potential exploration of diverse outputs

- Failure signatures:
  - Low precision: Model predicting propaganda techniques without sufficient evidence
  - Low recall: Model missing actual instances of propaganda techniques
  - Inconsistent outputs: Temperature not set to 0 or model instability
  - Overfitting: Fine-tuned models only predicting over-represented labels

- First 3 experiments:
  1. Compare GPT-4 'base' and 'chain of thought' prompts on a small subset of the test data to observe precision-recall trade-offs.
  2. Test GPT-3 models with and without instruction at training to verify the impact of instructional prompts.
  3. Evaluate the effect of different temperature settings on output consistency for a single article.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs for propaganda detection vary across different languages and cultural contexts?
- Basis in paper: [inferred] The paper mentions that propaganda detection was applied exclusively to English news articles and identifies language morphology and representation within training data as factors determining applicability to other languages.
- Why unresolved: The paper only tested LLMs on English news articles, leaving the question of performance in other languages and cultural contexts unanswered.
- What evidence would resolve it: Conducting experiments with LLMs for propaganda detection in various languages and cultural contexts would provide evidence for the performance variation across different languages and cultural contexts.

### Open Question 2
- Question: How does the fine-tuning of GPT-4 with the propaganda dataset affect its performance compared to the out-of-the-box model?
- Basis in paper: [explicit] The paper mentions the possibility of fine-tuning GPT-4 with the propaganda dataset as a strategy to address the issue of some propaganda techniques not being well-represented in the model's training data.
- Why unresolved: The paper does not explore the performance of fine-tuned GPT-4 for propaganda detection.
- What evidence would resolve it: Fine-tuning GPT-4 with the propaganda dataset and comparing its performance to the out-of-the-box model would provide evidence for the impact of fine-tuning on the model's performance.

### Open Question 3
- Question: How does the maximum token length limit affect the performance of LLMs for propaganda detection in longer news articles?
- Basis in paper: [explicit] The paper identifies the maximum token length as a potentially limiting factor for successful propaganda detection within newspaper articles.
- Why unresolved: The paper does not provide insights into the impact of the maximum token length limit on the performance of LLMs for propaganda detection in longer news articles.
- What evidence would resolve it: Conducting experiments with LLMs for propaganda detection in longer news articles with varying token lengths would provide evidence for the impact of the maximum token length limit on the model's performance.

## Limitations

- Lack of detailed implementation specifications, particularly regarding the exact prompt templates used for each model variation
- Absence of information on the distribution of propaganda techniques in the dataset, making it difficult to assess whether performance variations are due to model capabilities or class imbalance
- Comparison to RoBERTa lacks detailed methodology, limiting the ability to evaluate the true advancement

## Confidence

- High Confidence: The comparative performance between GPT-4 and GPT-3 models, particularly the observation that GPT-4 demonstrates superior understanding of linguistic features for propaganda detection.
- Medium Confidence: The claim that GPT-4 achieves performance comparable to state-of-the-art methods.
- Low Confidence: The assertion that chain-of-thought prompting significantly improves precision.

## Next Checks

1. **Prompt Template Verification**: Implement and test the exact prompt templates described in the paper (or obtain them from authors) to verify that the reported performance differences between model variations are reproducible.

2. **Per-Technique Analysis**: Conduct a detailed breakdown of model performance across all 14 propaganda techniques to identify which specific techniques are challenging and whether certain prompting strategies excel for particular technique types.

3. **Baseline Replication**: Replicate the RoBERTa baseline using the exact same data splits and evaluation methodology to ensure fair comparison and verify the claimed performance improvements of the LLM approaches.