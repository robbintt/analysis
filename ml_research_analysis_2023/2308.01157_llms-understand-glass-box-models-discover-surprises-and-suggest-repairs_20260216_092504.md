---
ver: rpa2
title: LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs
arxiv_id: '2308.01157'
source_url: https://arxiv.org/abs/2308.01157
tags:
- data
- risk
- llms
- surprising
- levels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper shows that large language models (LLMs) can effectively
  interpret and reason about glass-box interpretable models, specifically Generalized
  Additive Models (GAMs), which decompose complex outcomes into univariate graph-represented
  components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive
  model-level summaries without requiring the entire model to fit in context.
---

# LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs

## Quick Facts
- arXiv ID: 2308.01157
- Source URL: https://arxiv.org/abs/2308.01157
- Reference count: 30
- Primary result: Large language models can effectively interpret and reason about glass-box interpretable models (GAMs) by analyzing decomposed univariate components, enabling anomaly detection and model repair suggestions.

## Executive Summary
This paper demonstrates that large language models (LLMs) can effectively interpret and reason about glass-box interpretable models, specifically Generalized Additive Models (GAMs), by adopting a hierarchical approach to reasoning. By decomposing complex outcomes into univariate graph-represented components, LLMs can provide comprehensive model-level summaries without requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science, such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. The authors demonstrate the utility of these new capabilities using multiple examples in healthcare, with particular emphasis on Generalized Additive Models (GAMs).

## Method Summary
The approach uses Explainable Boosting Machines (EBMs) to train GAMs, which decompose complex outcomes into univariate component functions (graphs). These component graphs are encoded as JSON text and passed to GPT-4 with hierarchical chain-of-thought prompting. The LLM first analyzes individual graphs, then aggregates these component insights into model-level summaries, and finally identifies surprising effects that contradict prior knowledge. The TalkToEBM package provides an open-source LLM-GAM interface for this workflow.

## Key Results
- LLMs can provide comprehensive model-level summaries by analyzing individual GAM components sequentially
- The approach enables automated detection of anomalies and surprising effects in interpretable models
- LLMs can suggest repairs to remove identified anomalies, leveraging their embedded domain knowledge
- The method scales to large models through compact JSON encoding of piecewise-constant graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can effectively interpret and reason about glass-box interpretable models (GAMs) without requiring the entire model to fit in context.
- Mechanism: By decomposing complex outcomes into univariate graph-represented components, the LLM can analyze each component independently and then aggregate the summaries for model-level analysis.
- Core assumption: The GAM's additive structure allows the LLM to understand the model by reasoning about individual graphs sequentially.
- Evidence anchors:
  - [abstract] "By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context."
  - [section] "GAMs [3, 4] represent complex outcomes as sums of univariate component functions (graphs); thus, by analyzing each of these component functions in turn, the LLM does not need to understand the entire model at once."
  - [corpus] Weak - only 5 related papers found, average FMR=0.512, none directly address this specific mechanism.
- Break condition: If the model cannot be decomposed into separable components or if the LLM cannot reason about individual graphs effectively.

### Mechanism 2
- Claim: LLMs can detect anomalies and surprising effects in GAMs by leveraging their extensive background knowledge.
- Mechanism: The LLM is prompted to identify abnormal patterns and contradictions with prior knowledge, then ranks these surprises to highlight potential data or model issues.
- Core assumption: The LLM has sufficient domain knowledge embedded to recognize when model predictions contradict expected patterns.
- Evidence anchors:
  - [abstract] "This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge"
  - [section] "Finally, LLMs can apply their embedded domain knowledge to provide grounded interpretations of these graphs and models."
  - [corpus] Weak - only 5 related papers found, average FMR=0.512, none directly address anomaly detection in GAMs.
- Break condition: If the LLM lacks relevant domain knowledge or cannot distinguish between surprising and expected patterns.

### Mechanism 3
- Claim: The LLM-GAM interface can scale to large models by encoding individual graphs as compact text representations.
- Mechanism: Each graph is encoded as a JSON object with intervals and values, allowing the LLM to work with compact representations that fit within context windows.
- Core assumption: Piecewise-constant graphs can be exactly described as lists of regions and values without losing essential information.
- Evidence anchors:
  - [section] "By encoding piecewise-constant graphs as JSON objects of x-bins and y-values (Figure 2A), we pass an exact and efficient description of the graph to the LLM."
  - [section] "For example, the maximum description length of a single graph in the pneumonia GAM is 2,345 GPT-4 tokens. This easily fits within GPT-4's context window."
  - [corpus] Weak - only 5 related papers found, average FMR=0.512, none directly address this specific encoding mechanism.
- Break condition: If graph representations become too complex to encode compactly or if the LLM cannot reason about the encoded format.

## Foundational Learning

- Concept: Hierarchical reasoning
  - Why needed here: Enables the LLM to break down complex model analysis into manageable sub-tasks (individual graph analysis) before combining insights
  - Quick check question: Can you explain how analyzing individual GAM components separately helps the LLM understand the entire model?

- Concept: Chain-of-thought prompting
  - Why needed here: Guides the LLM through a structured analysis process, starting with component-level summaries before model-level conclusions
  - Quick check question: What is the difference between asking the LLM to analyze a single graph versus asking it to summarize the entire model at once?

- Concept: Glass-box interpretability
  - Why needed here: Provides transparent model structures that can be decomposed and analyzed, unlike black-box models
  - Quick check question: Why are GAMs more suitable for LLM interpretation than deep neural networks?

## Architecture Onboarding

- Component map:
  GAM model (trained using Explainable Boosting Machines) -> Graph encoding module (converts GAM components to JSON) -> LLM interface (GPT-4 for analysis and interpretation) -> Prompt engineering layer (system prompts and chain-of-thought instructions) -> Surprise detection module (analyzes LLM responses for anomalies) -> Output aggregation (combines component insights into model summaries)

- Critical path:
  1. Load GAM model
  2. Extract individual component graphs
  3. Encode each graph as JSON text
  4. Query LLM with graph analysis prompts
  5. Aggregate component summaries
  6. Query LLM for model-level insights
  7. Identify and rank surprises

- Design tradeoffs:
  - Accuracy vs. context window size (simplifying graphs vs. preserving detail)
  - Prompt specificity vs. LLM creativity (guided analysis vs. open-ended exploration)
  - Encoding completeness vs. compactness (detailed JSON vs. minimal representation)

- Failure signatures:
  - LLM returns generic or hallucinated responses
  - Analysis focuses on local details while missing global patterns
  - Surprise detection flags expected patterns as anomalies
  - Graph encoding loses critical information for analysis

- First 3 experiments:
  1. Test LLM interpretation of a single simple graph with known properties
  2. Evaluate LLM's ability to detect expected anomalies in a controlled dataset
  3. Measure context window usage when analyzing increasingly complex GAMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop standardized interfaces between LLMs and interpretable models to ensure consistent and reliable interpretations?
- Basis in paper: [explicit] The paper mentions that directly interfacing with LLMs can lead to unstructured and confusing results, highlighting the need for standardized graph-LLM interfaces and methods to verify LLM interpretations.
- Why unresolved: While the paper acknowledges the importance of standardized interfaces, it does not provide a concrete solution or framework for developing such interfaces. The authors mention that this is an area requiring further investigation.
- What evidence would resolve it: Development and evaluation of a standardized interface framework that enables consistent and verifiable interpretations of interpretable models by LLMs, demonstrated through empirical studies on various datasets and model types.

### Open Question 2
- Question: How can we effectively address the issue of data pollution and memorization in LLMs when interpreting models on sensitive or confidential datasets?
- Basis in paper: [explicit] The paper discusses the concern of data pollution, where LLMs may have been trained on data or papers related to the dataset being analyzed, leading to potential hallucinations or biased interpretations.
- Why unresolved: While the paper acknowledges the issue of data pollution, it does not provide a comprehensive solution or methodology to mitigate this problem. The authors suggest that this is an important area for future research.
- What evidence would resolve it: Development and evaluation of techniques to detect and mitigate data pollution in LLMs, such as using data obfuscation, fine-tuning on domain-specific data, or employing domain expert validation of LLM interpretations.

### Open Question 3
- Question: How can we extend the capabilities of LLMs to understand and explain more complex black-box models, beyond interpretable models like GAMs?
- Basis in paper: [explicit] The paper discusses the potential of using intermediate steps of model explanation, such as SHAP values, to enable LLMs to explain black-box models. However, it also acknowledges the limitations and potential instability of such explanations.
- Why unresolved: While the paper suggests a potential approach for explaining black-box models using LLMs, it does not provide a comprehensive solution or evaluate the effectiveness of this approach on various black-box models and datasets.
- What evidence would resolve it: Development and evaluation of techniques to enable LLMs to understand and explain complex black-box models, demonstrated through empirical studies comparing the quality and reliability of LLM explanations to human expert interpretations and established model explanation methods.

## Limitations
- Evaluation relies on a single dataset (MCHD pneumonia data) without systematic testing across diverse domains
- Limited empirical validation of the surprise detection capability - demonstrates the mechanism but doesn't quantify precision/recall
- Heavy dependence on GPT-4's capabilities without ablation studies using smaller or open-source LLMs

## Confidence
- High confidence: The core mechanism of decomposing GAMs into component graphs for LLM analysis is well-supported by the mathematical structure of GAMs and the demonstrated token efficiency
- Medium confidence: The LLM's ability to provide meaningful model summaries and detect anomalies is promising but requires more systematic evaluation across different domains and model complexities
- Low confidence: The generalizability of the approach to other interpretable model families beyond GAMs and the robustness to different prompt engineering approaches

## Next Checks
1. Conduct systematic evaluation across 3-5 diverse datasets (healthcare, finance, social sciences) to assess generalizability of the LLM-GAM interface
2. Implement controlled experiments comparing LLM-analyzed models with and without known anomalies to measure precision and recall of the surprise detection capability
3. Perform ablation studies using different LLM models (GPT-3.5, open-source alternatives) to quantify the impact of model size and reasoning capabilities on analysis quality