---
ver: rpa2
title: Rather a Nurse than a Physician -- Contrastive Explanations under Investigation
arxiv_id: '2310.11906'
source_url: https://arxiv.org/abs/2310.11906
tags:
- contrastive
- explanations
- human
- rationales
- non-contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares contrastive and non-contrastive explanations
  in text classification using human rationale annotations and model explanations.
  The authors collect human rationale annotations for both settings on a subset of
  the BIOS dataset and compare them with model-based explanations using LRP, GradientxInput,
  and Gradient Norm methods.
---

# Rather a Nurse than a Physician -- Contrastive Explanations under Investigation

## Quick Facts
- arXiv ID: 2310.11906
- Source URL: https://arxiv.org/abs/2310.11906
- Reference count: 15
- Key outcome: This paper compares contrastive and non-contrastive explanations in text classification using human rationale annotations and model explanations, finding that both settings align equally well with human rationales.

## Executive Summary
This study investigates whether contrastive explanations (where annotators select keywords relative to the other class) differ from non-contrastive explanations (where keywords are selected relative to the overall decision) in text classification tasks. The authors collect human rationale annotations for both settings on a subset of the BIOS dataset and compare them with model-based explanations using LRP, Gradient×Input, and Gradient Norm methods. Surprisingly, they find that human annotations in both settings agree on a similar level, and model-based explanations computed in both settings align equally well with human rationales, suggesting that contrastive explanations are not necessarily more class-specific than non-contrastive ones.

## Method Summary
The study uses three datasets (SST2, DynaSent, BIOS, DBpedia-Animals) and three model architectures (RoBERTa, GPT-2, T5 in small, base, and large sizes). For the BIOS dataset, human annotators provide rationale annotations in both contrastive and non-contrastive settings. The models are fine-tuned on each task, and explanations are computed using Layer-wise Relevance Propagation (LRP), Gradient×Input, and Gradient Norm methods. The agreement between human and model rationales is measured using Cohen's Kappa and Spearman correlation coefficients, with additional analysis of part-of-speech tags and entropy of rationales.

## Key Results
- Human rationale annotations in contrastive and non-contrastive settings agree on a similar level (Cohen's Kappa 0.4-0.73)
- Model-based explanations computed in both settings align equally well with human rationales
- Contrastive explanations show lower entropy than non-contrastive ones, indicating more selective token choice
- Models tend to give more importance to verbs compared to human annotators who primarily select nouns and adjectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive and non-contrastive model-based explanations align equally well with human rationales.
- Mechanism: The study compares human rationale annotations in both contrastive and non-contrastive settings with model-based explanations computed using Layer-wise Relevance Propagation (LRP), Gradient×Input, and Gradient Norm methods. The agreement between model explanations and human rationales is measured using Cohen's Kappa and Spearman correlation coefficients.
- Core assumption: Human rationale annotations serve as a reliable proxy for understanding how humans explain decisions, and model-based explanations can be effectively compared to these annotations.
- Evidence anchors:
  - [abstract]: "A cross-comparison between model-based rationales and human annotations, both in contrastive and non-contrastive settings, yields a high agreement between the two settings for models as well as for humans."
  - [section 5.3]: "Considering the agreement across all classes, i.e., all in Figure 5, we observe that in most cases contrastive and non-contrastive model-based rationales have a similar agreement rate with both contrastive and non-contrastive human rationales (maximum difference is 0.06)."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.349, average citations=0.0. Top related titles: An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection, Modifications of the Miller definition of contrastive (counterfactual) explanations, WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning.
- Break condition: If human rationale annotations are not reliable proxies for human explanations, or if model-based explanations cannot be effectively compared to human rationales, the mechanism would break down.

### Mechanism 2
- Claim: Human rationale annotations in contrastive and non-contrastive settings agree on a similar level.
- Mechanism: The study collects human rationale annotations for a subset of the BIOS dataset in both contrastive and non-contrastive settings. The agreement between these annotations is measured using Cohen's Kappa scores, both within and between the two settings.
- Core assumption: Human annotators can consistently select relevant tokens in both contrastive and non-contrastive settings, and the agreement between these annotations reflects the similarity in token selection.
- Evidence anchors:
  - [section 5.2]: "We observe similar results for all three scores per class with scores ranging from 0.4 to 0.73 for the comparison across contrastive and non-contrastive settings."
  - [section 5.2]: "This indicates that the agreement across settings is similar to the agreement within settings and thus the selection of tokens does not necessarily differ between contrastive and non-contrastive annotations."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.349, average citations=0.0. Top related titles: An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection, Modifications of the Miller definition of contrastive (counterfactual) explanations, WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning.
- Break condition: If human annotators cannot consistently select relevant tokens in both settings, or if the agreement between annotations does not reflect the similarity in token selection, the mechanism would break down.

### Mechanism 3
- Claim: Contrastive explanations are more class-specific, focusing on specific terms for classes that share a joint set of features.
- Mechanism: The study analyzes the part-of-speech (POS) tags of rationales in the BIOS data and finds that human and model-based rationales are mainly formed by nouns and adjectives. However, models tend to give more importance to verbs compared to annotators. Additionally, the study examines the degree of information in explanations using entropy, revealing that non-contrastive explanations are less sparse, indicating that humans choose relevant tokens more selectively in the contrastive setting.
- Core assumption: The POS tags of rationales can provide insights into the grammatical structure of explanations, and the degree of information in explanations can be quantified using entropy.
- Evidence anchors:
  - [section 5.3]: "While human and model-based rationales are mainly formed by nouns and adjectives, models tend to give more importance to verbs compared to annotators, who barely selected them as keywords in their explanations."
  - [section 5.4]: "Averaged sentence level entropy values on human rationales reveal that non-contrastive explanations are less sparse, i.e., their averaged entropy is significantly higher (1.72 vs 1.14, p<0.05)."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.349, average citations=0.0. Top related titles: An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection, Modifications of the Miller definition of contrastive (counterfactual) explanations, WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning.
- Break condition: If the POS tags of rationales do not provide meaningful insights into the grammatical structure of explanations, or if entropy cannot effectively quantify the degree of information in explanations, the mechanism would break down.

## Foundational Learning

- Concept: Contrastive explanations
  - Why needed here: Understanding the difference between contrastive and non-contrastive explanations is crucial for interpreting the study's findings and implications.
  - Quick check question: What is the main difference between contrastive and non-contrastive explanations?

- Concept: Human rationale annotations
  - Why needed here: Human rationale annotations serve as a reference point for evaluating model explanations and understanding how humans explain decisions.
  - Quick check question: How are human rationale annotations collected and used in the study?

- Concept: Model-based explanations
  - Why needed here: Model-based explanations provide insights into the inner workings of machine learning models and can be compared to human rationales to assess their plausibility and faithfulness.
  - Quick check question: What are the different methods used to compute model-based explanations in the study?

## Architecture Onboarding

- Component map: Datasets (BIOS, SST2, DynaSent, DBpedia-Animals) -> Models (RoBERTa, GPT-2, T5 small/base/large) -> Explainability methods (LRP, Gradient×Input, Gradient Norm) -> Evaluation metrics (Cohen's Kappa, Spearman correlation, entropy)

- Critical path:
  1. Collect human rationale annotations for a subset of the BIOS dataset in both contrastive and non-contrastive settings.
  2. Fine-tune and extract explanations from three different models (RoBERTa, GPT-2, T5) in three different sizes using three post-hoc explainability methods (LRP, Gradient×Input, Gradient Norm).
  3. Compare human rationale annotations with model-based explanations using agreement metrics (Cohen's Kappa, Spearman correlation coefficient).
  4. Analyze the agreement between human rationale annotations in contrastive and non-contrastive settings.
  5. Examine the part-of-speech tags and entropy of rationales to understand the grammatical structure and degree of information in explanations.

- Design tradeoffs:
  - Using human rationale annotations as a reference point for evaluating model explanations relies on the assumption that human annotations are reliable proxies for human explanations.
  - Comparing model-based explanations with human rationale annotations in a post-hoc manner may not capture the full complexity of human reasoning and decision-making processes.

- Failure signatures:
  - Low agreement between human rationale annotations and model-based explanations.
  - Significant differences in agreement between contrastive and non-contrastive settings for both human annotations and model explanations.
  - Inconsistent results across different models, sizes, and explainability methods.

- First 3 experiments:
  1. Compare human rationale annotations with model-based explanations for a small subset of the BIOS dataset using LRP, Gradient×Input, and Gradient Norm methods. Measure the agreement using Cohen's Kappa and Spearman correlation coefficient.
  2. Analyze the part-of-speech tags of rationales in the BIOS data to understand the grammatical structure of human and model-based explanations.
  3. Compute the entropy of rationales to quantify the degree of information in contrastive and non-contrastive explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do contrastive explanations perform on multi-class classification tasks with hundreds of labels?
- Basis in paper: [explicit] The paper notes that their analysis is limited to tasks with a small number of classes and suggests that experimenting with datasets including hundreds of labels could potentially lead to different results.
- Why unresolved: The paper only tests on datasets with 5-8 classes, which may not be representative of more complex tasks with many similar classes.
- What evidence would resolve it: Conducting experiments on datasets with hundreds of labels, such as legal or biomedical text, and comparing the performance of contrastive and non-contrastive explanations in these settings.

### Open Question 2
- Question: How does the timing of rationale annotation (pre-hoc vs. post-hoc) affect the quality and nature of human rationales?
- Basis in paper: [explicit] The paper mentions that human rationales are collected post-hoc, after a decision has been made, and notes that this could affect the results.
- Why unresolved: The paper does not explore how rationales collected before a decision (pre-hoc) might differ from those collected after (post-hoc).
- What evidence would resolve it: Collecting and comparing human rationales in both pre-hoc and post-hoc settings to analyze differences in the selection of tokens and the reasoning process.

### Open Question 3
- Question: How do human gaze patterns compare to human rationale annotations in evaluating model explanations?
- Basis in paper: [explicit] The paper includes human gaze data in its analysis and finds lower agreement with model explanations compared to human annotations.
- Why unresolved: The paper only briefly discusses the potential of human gaze as an alternative to human rationales and does not provide a detailed comparison.
- What evidence would resolve it: Conducting a more in-depth analysis of human gaze patterns and their correlation with model explanations, and comparing these findings to those from human rationale annotations.

## Limitations
- The study relies on human rationale annotations as ground truth, which can be subjective and may not fully capture human reasoning processes
- The findings are based primarily on the BIOS dataset with occupation classification, which may not generalize to other domains or tasks with different characteristics
- The equal alignment of contrastive and non-contrastive explanations across all methods suggests these approaches may not fully capture the intended distinction between explanation types

## Confidence
- **High confidence**: The finding that contrastive and non-contrastive model-based explanations align equally well with human rationales is well-supported by multiple agreement metrics (Cohen's Kappa, Spearman correlation) across different models and explainability methods. The statistical significance tests (p<0.05) for entropy comparisons also provide strong support.
- **Medium confidence**: The observation that human annotations in both settings agree on a similar level is supported by Cohen's Kappa scores, but the moderate agreement values (0.4-0.73) indicate room for variability. The interpretation that this means token selection doesn't differ significantly between settings requires careful consideration.
- **Low confidence**: The claim that contrastive explanations are more class-specific is based on indirect evidence (entropy differences, POS analysis) rather than direct measurement of class specificity. The entropy difference (1.72 vs 1.14) is statistically significant but may not fully capture the practical implications for explanation quality.

## Next Checks
1. **Annotator agreement analysis**: Conduct a detailed analysis of inter-annotator agreement within each setting separately, examining whether specific classes or types of examples show systematically different agreement patterns that might explain the overall similarity between settings.

2. **Cross-dataset validation**: Test whether the equal alignment of contrastive and non-contrastive explanations holds across more diverse datasets and tasks, particularly those with clearer class distinctions or different linguistic characteristics than BIOS.

3. **Explanation method comparison**: Compare the contrastive and non-contrastive explanations generated by different explanation methods (LRP, Gradient×Input, Gradient Norm) against each other to determine if the similarity observed with human rationales is also present in the explanations themselves.