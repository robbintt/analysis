---
ver: rpa2
title: 'InRank: Incremental Low-Rank Learning'
arxiv_id: '2306.11250'
source_url: https://arxiv.org/abs/2306.11250
tags:
- training
- low-rank
- rank
- learning
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper generalizes the theory of greedy low-rank learning (GLRL)
  to arbitrary orthogonal initialization by focusing on cumulative weight updates
  instead of weight matrices. The authors prove that cumulative weight updates follow
  an incremental low-rank trajectory in a three-layer linear network, where each singular
  vector is learned exponentially faster based on its strength.
---

# InRank: Incremental Low-Rank Learning

## Quick Facts
- arXiv ID: 2306.11250
- Source URL: https://arxiv.org/abs/2306.11250
- Reference count: 40
- Primary result: InRank achieves comparable prediction performance to full-rank models while requiring at most 33% of the total ranks

## Executive Summary
This paper proposes Incremental Low-Rank Learning (InRank), a method that generalizes greedy low-rank learning theory to arbitrary orthogonal initialization by focusing on cumulative weight updates rather than weight matrices. The authors prove that cumulative weight updates follow an incremental low-rank trajectory in three-layer linear networks, with each singular vector learned exponentially faster based on its strength. This behavior is empirically observed across various neural networks and training algorithms. InRank parameterizes cumulative weight updates as low-rank matrices and incrementally augments their ranks during training, achieving comparable prediction performance to full-rank models while significantly reducing memory usage and computational cost.

## Method Summary
InRank replaces standard weight matrices W with a factorization W = W₀ + UV, where W₀ is the initial weight and UV represents the cumulative updates as a low-rank matrix. During training, the algorithm monitors the explained ratio of singular values from the UV factorization to determine when additional rank is needed. When the explained ratio falls below a threshold α, the rank is expanded by adding new modes to U and V. An efficient variant freezes the rank after an initial determination phase, reducing computational overhead while maintaining performance. The method is evaluated on GPT-2 models trained on WikiText-103, demonstrating significant reductions in memory usage (37%) and training time (20%) while maintaining comparable perplexity to full-rank models.

## Key Results
- InRank achieves comparable prediction performance to full-rank models while requiring at most 33% of the total ranks
- The efficient variant of InRank reduces training time by 20% and memory usage by 37% when training GPT-medium on WikiText-103 from scratch
- Rank increments predominantly occur during early training stages, allowing rank freezing afterward for efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cumulative weight updates follow a greedy low-rank learning trajectory regardless of initialization magnitude
- Mechanism: By focusing on d_t = w_t - w₀, the algorithm removes initialization dependency and allows analysis of rank progression independent of initial weight scale
- Core assumption: Orthogonal inputs and orthogonal weight initialization enable independent evolution of each singular mode
- Break condition: Non-orthogonal inputs or initialization breaking orthogonality cause mode interference and prevent independent rank progression

### Mechanism 2
- Claim: The explained ratio metric accurately identifies when additional rank is needed
- Mechanism: The explained ratio g(M, r_l, b) = Σᵢ₌₁ʳˡ sᵢ / Σᵢ₌₁ʳˡ⁺ᵇ sᵢ quantifies how well current rank represents the spectrum
- Core assumption: The truncated spectrum contains the most informative modes for the current training state
- Break condition: Flat spectrum or important information in lower singular values may cause explained ratio to misidentify rank needs

### Mechanism 3
- Claim: Early training phase captures intrinsic rank, allowing rank freezing afterward for efficiency
- Mechanism: Rank increments predominantly occur in early training stages, allowing freezing after intrinsic rank determination
- Core assumption: Intrinsic rank is primarily determined by problem structure and early optimization dynamics
- Break condition: Complex interactions emerging later in training or significant learning rate schedule changes may require continued rank adaptation

## Foundational Learning

- Concept: Matrix factorization and SVD
  - Why needed here: Algorithm relies on factorizing cumulative weight updates into low-rank matrices and monitoring singular value progression
  - Quick check question: Given a matrix M, how do you compute its rank-k approximation using SVD?

- Concept: Gradient flow and differential equations
  - Why needed here: Theoretical analysis models weight evolution as continuous differential equations to understand rank progression dynamics
  - Quick check question: How does the gradient flow equation ∂w/∂t = -∇L(w) relate to discrete SGD updates?

- Concept: Implicit regularization in optimization
  - Why needed here: Paper builds on theory that gradient-based training implicitly biases networks toward low-rank solutions
  - Quick check question: What is the difference between explicit and implicit regularization in neural network training?

## Architecture Onboarding

- Component map: Weight parameterization layer -> Rank monitoring system -> Rank expansion module -> Efficient mode
- Critical path: 1) Initialize weights using standard method, 2) Set initial rank r=1 with small UV factors, 3) At each iteration, compute explained ratio on UV, 4) If ratio < α, expand rank by adding new modes, 5) Continue training with updated factorization
- Design tradeoffs:
  - Rank determination frequency vs. computational overhead
  - Buffer size b (wider spectrum vs. more computation)
  - Threshold α (earlier rank increase vs. better rank utilization)
  - InRank vs. InRank-Efficient (accuracy vs. speed/memory)
- Failure signatures:
  - Rank never increases: Threshold too high or initialization too large
  - Rank increases too rapidly: Threshold too low or buffer too small
  - Performance degradation: Rank insufficient for problem complexity
  - High computational cost: Too frequent rank checks or large buffer
- First 3 experiments:
  1. Train a 3-layer linear network on synthetic orthogonal data, monitor singular value progression to verify sigmoid learning curves
  2. Apply InRank to a small GPT-2 variant on WikiText-103, compare perplexity and rank usage against full-rank baseline
  3. Compare InRank vs InRank-Efficient on same task, measure trade-offs in accuracy, training time, and memory usage

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several emerge from the analysis and results presented.

## Limitations
- Theoretical extension to arbitrary orthogonal initialization relies on assumptions about orthogonal inputs that may not hold in practical settings
- Empirical validation is limited to a single dataset (WikiText-103) and model family (GPT-2), raising questions about generalizability
- Performance trade-offs depend heavily on hyperparameter choices (threshold α, buffer size b) that weren't extensively explored across different model scales

## Confidence

- Theoretical framework for cumulative weight updates: High - The mathematical analysis is rigorous and builds on established work
- Empirical effectiveness on GPT-2/WikiText-103: Medium - Results are promising but limited in scope
- Generalizability to other architectures/datasets: Low - No evidence beyond the specific experimental setup
- Rank monitoring methodology: Medium - The explained ratio concept is sound but hyperparameter sensitivity wasn't thoroughly examined

## Next Checks
1. Test InRank on non-orthogonal input distributions to verify the theoretical assumptions break down as predicted
2. Apply the algorithm to vision transformers and smaller language models to assess cross-domain effectiveness
3. Conduct ablation studies varying threshold α and buffer size b to understand their impact on rank progression and final performance