---
ver: rpa2
title: 'Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous
  Mixture of LMs'
arxiv_id: '2310.13961'
source_url: https://arxiv.org/abs/2310.13961
tags:
- instruction
- output
- data
- tasks
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating high-quality instruction-tuning
  data using smaller, open-source language models (10-40B parameters) instead of large
  proprietary models (175B+ parameters). The proposed method, Ensemble-Instruct, improves
  upon Self-Instruct by categorizing tasks and simplifying prompts to make few-shot
  learning easier for smaller models, and by ensembling outputs from multiple LMs
  to select high-quality synthetic examples.
---

# Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs

## Quick Facts
- arXiv ID: 2310.13961
- Source URL: https://arxiv.org/abs/2310.13961
- Reference count: 9
- Base models fine-tuned on synthetic data achieve Rouge-L scores of 45.4-50.4 on SuperNatural Instructions, outperforming GPT-3 (39.9)

## Executive Summary
Ensemble-Instruct presents a method for generating high-quality instruction-tuning data using smaller, open-source language models (10-40B parameters) instead of large proprietary models (175B+ parameters). The approach improves upon Self-Instruct by categorizing tasks and simplifying prompts to make few-shot learning easier for smaller models, and by ensembling outputs from multiple LMs to select high-quality synthetic examples. Using a heterogeneous mixture of LMs for instruction and instance generation, followed by output ensembling, the method produces around 45k synthetic instruction-tuning samples. Fine-tuning base models like MPT-7B and GPT-JT-6B on this data yields significant performance gains, with MPT-7B outperforming the much larger GPT-3 on the SuperNatural Instructions benchmark.

## Method Summary
Ensemble-Instruct generates synthetic instruction-tuning data by first categorizing tasks into those requiring input and those not requiring input, then using simplified few-shot learning prompts for each category. It employs a heterogeneous mixture of smaller LMs to generate instructions and instances, followed by an output ensembling step that uses additional LMs and consensus filtering to select high-quality examples. The final dataset of approximately 45k samples is used to fine-tune base models, resulting in improved performance on instruction-following tasks.

## Key Results
- MPT-7B fine-tuned on Ensemble-Instruct data achieves 50.4 Rouge-L on SuperNatural Instructions, outperforming GPT-3 (39.9)
- The approach significantly improves performance on user-oriented tasks, with GPT-JT-6B reaching 45.4 Rouge-L
- Ablation studies confirm the importance of task categorization, simplified prompts, and output ensembling
- The method scales to larger models, with GPT-JT-6B showing strong performance gains

## Why This Works (Mechanism)

### Mechanism 1
Smaller LMs (10-40B) struggle to generate high-quality synthetic instruction-tuning data using the original Self-Instruct approach because they have difficulty learning from the complex ICL prompts. Ensemble-Instruct simplifies the ICL prompts by categorizing tasks into those requiring input and those not requiring input, then using separate, simpler prompt templates for each category. This reduces the cognitive load on the smaller LM during few-shot learning. Core assumption: Smaller LMs benefit more from simplified, task-categorized prompts than from the complex, unified prompts used in Self-Instruct.

### Mechanism 2
Ensembling outputs from multiple heterogeneous LMs improves the accuracy and diversity of the generated instruction-tuning data. Ensemble-Instruct generates additional outputs for each instruction-input pair (or instruction alone) using a diverse set of LMs. It then applies a consensus filtering algorithm (minimum Bayesian Risk decoding with thresholding) to select the best output based on Rouge-L similarity scores. Core assumption: Outputs from a diverse set of LMs will contain complementary information, and a consensus approach can filter out low-quality or inconsistent outputs.

### Mechanism 3
Using instruction-tuned LMs for output generation in the ensembling step is more effective than using vanilla LMs. Ensemble-Instruct employs instruction-tuned LMs (e.g., FLAN-UL2, FLAN-T5-XXL) to generate additional outputs for ensembling. These models are better at generating high-quality outputs zero-shot compared to vanilla LMs, which require few-shot ICL. Core assumption: Instruction-tuned LMs have learned to follow instructions better and can generate higher-quality outputs without needing few-shot examples.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the core technique used by Ensemble-Instruct to generate synthetic instruction-tuning data. Understanding how ICL works and its limitations is crucial for understanding why Ensemble-Instruct's modifications are necessary and effective.
  - Quick check question: What are the key differences between few-shot learning and in-context learning, and why is ICL particularly relevant for instruction-tuning data generation?

- Concept: Prompt engineering and template design
  - Why needed here: Ensemble-Instruct relies heavily on carefully designed ICL prompts and templates. Understanding how to craft effective prompts for different task types is essential for implementing and extending the method.
  - Quick check question: How does the categorization of tasks into those requiring input and those not requiring input influence the design of ICL prompts in Ensemble-Instruct?

- Concept: Model ensembling and consensus filtering
  - Why needed here: The output ensembling step is a key innovation in Ensemble-Instruct. Understanding different ensembling techniques and consensus filtering algorithms is necessary for implementing and potentially improving this component.
  - Quick check question: How does the minimum Bayesian Risk decoding with thresholding used in Ensemble-Instruct compare to other consensus filtering approaches, and what are the trade-offs involved?

## Architecture Onboarding

- Component map:
  Task Categorization -> Instruction Generation -> Instance Generation -> Output Ensembling -> Data Collection -> Fine-tuning

- Critical path:
  Task categorization → Instruction generation → Instance generation → Output ensembling → Data collection → Fine-tuning

- Design tradeoffs:
  Simplicity vs. complexity of ICL prompts: Simpler prompts are easier for smaller LMs but may not capture all task nuances
  Diversity vs. consensus in ensembling: More diverse LMs can improve consensus but may also introduce more noise
  Zero-shot vs. few-shot generation: Zero-shot is faster but may be less accurate for some tasks

- Failure signatures:
  Poor instruction quality: Indicates issues with task categorization or instruction generation prompts
  Inconsistent outputs: Suggests problems with the ensembling or consensus filtering process
  Low diversity in generated data: May indicate insufficient heterogeneity in the LMs used for ensembling

- First 3 experiments:
  1. Implement task categorization and simplified ICL prompts, then evaluate instruction generation quality on a small set of seed tasks
  2. Add instance generation and test the full pipeline (instruction + instance generation) for a few task types
  3. Implement output ensembling with a small set of additional LMs and evaluate the consensus filtering performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of instruction-tuning data generated by Ensemble-Instruct scale with the number and diversity of language models used in the ensemble? The paper does not provide a detailed analysis of how the number and diversity of LMs in the ensemble affect the quality of the generated data. An empirical study comparing the quality of instruction-tuning data generated using different numbers and types of LMs in the ensemble would resolve this.

### Open Question 2
What is the optimal balance between the number of ICL demonstrations used for instruction generation and the number of ICL demonstrations used for instance generation in Ensemble-Instruct? The paper mentions using different numbers of ICL demonstrations for instruction generation (24 for type A tasks and 10 for type B tasks) and instance generation (18 for type A tasks and 15 for type B tasks) but does not provide a systematic analysis of how varying the number of ICL demonstrations affects the quality of the generated data. An ablation study comparing the performance of Ensemble-Instruct using different numbers of ICL demonstrations for instruction and instance generation would resolve this.

### Open Question 3
How does the performance of Ensemble-Instruct compare to other methods for generating instruction-tuning data, such as using proprietary models like GPT-3 or ChatGPT? The paper claims that Ensemble-Instruct outperforms Self-Instruct, which uses GPT-3, and mentions related work that uses ChatGPT or GPT-4, but does not provide a direct comparison between Ensemble-Instruct and these other methods. An empirical comparison of the performance of models fine-tuned on data generated by Ensemble-Instruct, Self-Instruct, and methods using proprietary models would resolve this.

## Limitations
- The paper does not provide exact ICL templates and prompt formats, creating uncertainty about whether improvements are solely due to categorization or also depend on specific prompt engineering details
- The consensus filtering mechanism relies on Rouge-L similarity scores without evidence that Rouge-L is the optimal metric or that chosen threshold values are universally applicable
- The claim that smaller LMs struggle with complex ICL prompts is inferred from experimental results rather than directly tested with controlled experiments

## Confidence
*High Confidence:* The claim that Ensemble-Instruct improves performance on the SuperNatural Instructions benchmark (Rouge-L scores of 45.4-50.4 vs. 39.9 for GPT-3) is supported by clear experimental results. The ablation studies showing that removing task categorization, simplified prompts, or ensembling degrades performance also have high confidence.

*Medium Confidence:* The claim that instruction-tuned LMs are superior to vanilla LMs for output generation is supported by experimental results (Table 4), but the comparison is limited to specific models (FLAN-UL2, FLAN-T5-XXL vs. UL2, FALCON) and may not generalize to all instruction-tuned or vanilla LMs.

*Low Confidence:* The paper asserts that smaller LMs struggle with complex ICL prompts, but this is inferred from the experimental results rather than directly tested. There is no corpus evidence or controlled experiment comparing smaller LMs' performance with complex vs. simplified prompts on the same tasks.

## Next Checks
1. **Prompt Template Validation:** Conduct a controlled experiment where the same set of smaller LMs generate instruction-tuning data using both the original Self-Instruct prompts and the simplified, task-categorized prompts from Ensemble-Instruct. Compare the quality and diversity of the generated data using multiple evaluation metrics (not just Rouge-L) to isolate the effect of prompt simplification.

2. **Consensus Filtering Robustness:** Test the minimum Bayesian Risk decoding with thresholding on datasets with known ground truth to evaluate its precision and recall in selecting high-quality outputs. Compare its performance against simpler ensembling methods (e.g., majority voting, median output) to determine if the complexity of the consensus algorithm is justified.

3. **Cross-Model Generalization:** Apply the Ensemble-Instruct methodology to a different set of base models (e.g., LLaMA, Bloom) and evaluate whether the performance gains observed with MPT-7B and GPT-JT-6B generalize to other architectures and parameter sizes. This would test the robustness of the approach across different model families.