---
ver: rpa2
title: Selective Pre-training for Private Fine-tuning
arxiv_id: '2305.13865'
source_url: https://arxiv.org/abs/2305.13865
tags:
- private
- data
- learning
- pre-training
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper tackles private fine-tuning of domain-specific language
  models under differential privacy and fixed-size constraints. The key idea is selective
  pre-training: using a privacy-preserving data selection algorithm to choose a subset
  of public data that better matches the target domain, followed by standard pre-training
  on that subset and DP fine-tuning on private data.'
---

# Selective Pre-training for Private Fine-tuning

## Quick Facts
- arXiv ID: 2305.13865
- Source URL: https://arxiv.org/abs/2305.13865
- Authors: 
- Reference count: 40
- Key outcome: Selective pre-training with privacy-preserving data selection consistently outperforms standard pre-training, enabling smaller models to match larger ones while preserving differential privacy.

## Executive Summary
This paper introduces selective pre-training as a method for improving private fine-tuning of domain-specific language models under differential privacy constraints. The approach uses a privacy-preserving data selection algorithm to identify public data that matches the target domain, then pre-trains on this subset before applying DP fine-tuning on private data. Experiments on Enron emails and GLUE benchmarks demonstrate that this method consistently outperforms standard pre-training baselines, both in perplexity and accuracy. Notably, smaller models trained with selective pre-training can match or surpass the performance of much larger models trained without it, demonstrating DP as an effective tool for model compression and efficiency.

## Method Summary
The framework consists of a privacy-preserving data selection algorithm that uses a domain classifier (trained with DP) to identify public data similar to the private target distribution. This selected subset is used for pre-training, followed by DP fine-tuning on the private data. The method addresses the challenge of private fine-tuning under fixed-size constraints by leveraging public data to reduce the distribution gap between pre-training and fine-tuning stages.

## Key Results
- Selective pre-training consistently outperforms random pre-training on both Enron and GLUE benchmarks
- Smaller models (41M, 82M) using selective pre-training match or exceed larger models (82M, 289M) using random pre-training
- The approach demonstrates differential privacy as an effective tool for model compression without performance loss
- Framework successfully deployed in real-world applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective pre-training on a subset of public data that matches the private distribution improves downstream performance.
- Mechanism: Domain classifier identifies target-like data, reducing distribution gap between pre-training and fine-tuning.
- Core assumption: Domain classifier can accurately distinguish target-like examples.
- Evidence anchors: Word cloud analysis shows improved overlap from 25 to 40 nouns; average neighbor FMR=0.429.

### Mechanism 2
- Claim: DP combined with selective pre-training enables model compression without performance loss.
- Mechanism: Domain-aligned pre-training provides better inductive biases for DP models, allowing smaller models to match larger ones.
- Core assumption: Pre-training data quality matters more for DP models than non-DP models.
- Evidence anchors: 41M model using selective pre-training outperforms 82M model using normal pre-training.

### Mechanism 3
- Claim: Privacy-preserving data selection improves transfer learning for domain-specific tasks.
- Mechanism: Algorithm selects data that reduces distribution shift, improving adaptation during fine-tuning.
- Core assumption: Reducing distribution shift improves transfer learning in private settings.
- Evidence anchors: Benefits of selective pre-training are greater for private deep learning vs non-private.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: The framework relies on DP to protect private data during fine-tuning and data selection
  - Quick check question: What is the privacy budget formula used in this paper for the complete framework?

- Concept: Transfer Learning
  - Why needed here: Core argument is that pre-training on aligned data improves transfer to private domain
  - Quick check question: How does the paper measure transfer learning performance (e.g., perplexity, accuracy)?

- Concept: Data Selection and Domain Adaptation
  - Why needed here: Selective pre-training requires understanding how to choose relevant data and measure distribution alignment
  - Quick check question: What metric does the paper use to evaluate whether selected data is closer to target distribution?

## Architecture Onboarding

- Component map: Domain Classifier (DP-trained) -> Data Selection Module -> Pre-training Pipeline -> DP Fine-tuning Pipeline -> Privacy Accounting
- Critical path: Data Selection → Pre-training → DP Fine-tuning → Evaluation
- Design tradeoffs: Data selection quality vs privacy cost of training classifier; number of pre-training tokens vs model size and performance; privacy parameters (ε, δ) vs model accuracy
- Failure signatures: Low F1-score on domain classifier → poor data selection → minimal performance gains; high perplexity on test set → distribution mismatch persists; privacy budget exceeded → framework unusable
- First 3 experiments: 1) Train domain classifier on Enron vs OpenWebText and measure F1-score; 2) Compare perplexity of models pre-trained on random vs selected subsets; 3) Test model compression by comparing small selective-pretrained vs large random-pretrained models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of pre-training tokens for a given model size in the selective pre-training framework?
- Basis in paper: The paper mentions that increasing pre-training tokens does not necessarily lead to better downstream accuracy and suggests there may be an optimal number.
- Why unresolved: The paper does not provide a clear answer and leaves it as an open direction for future research.
- What evidence would resolve it: Experimental results showing the relationship between model size, number of pre-training tokens, and downstream performance.

### Open Question 2
- Question: How does the quality of the public dataset affect the performance of the selective pre-training framework?
- Basis in paper: The paper mentions that the distribution of selected data is closer to target distribution, which improves performance.
- Why unresolved: The paper does not explore the relationship between public dataset quality and performance in detail.
- What evidence would resolve it: Experiments comparing performance using different public datasets with varying similarity to target dataset.

### Open Question 3
- Question: Can the selective pre-training framework be extended to other domains beyond language modeling, such as computer vision or speech recognition?
- Basis in paper: The paper focuses on language modeling tasks and does not explore applicability to other domains.
- Why unresolved: The paper provides no evidence or discussion on potential extension to other domains.
- What evidence would resolve it: Experiments applying the framework to tasks in other domains like image classification or speech recognition.

## Limitations

- Privacy accounting across the complete pipeline lacks full transparency, making verification of (ε, δ)-DP guarantees difficult
- Distribution alignment evidence relies primarily on qualitative word cloud comparisons rather than rigorous quantitative metrics
- Evaluation focuses primarily on Enron emails and GLUE benchmarks, leaving cross-domain generalization untested

## Confidence

- High Confidence: Selective pre-training improves DP fine-tuning performance on domain-specific tasks, supported by consistent improvements over random pre-training baselines
- Medium Confidence: Model compression through selective pre-training, as experimental evidence shows smaller models matching larger ones, but privacy accounting complexity makes verification challenging
- Medium Confidence: Privacy guarantees of the complete framework, as the paper claims (ε, δ)-DP but lacks detailed composition analysis

## Next Checks

1. **Privacy Budget Analysis**: Perform complete privacy accounting analysis separating costs of domain classifier training, pre-training on selected data, and DP fine-tuning, verifying total privacy budget remains within claimed bounds.

2. **Distribution Alignment Quantification**: Implement quantitative metrics (beyond word clouds) to measure distribution shift between selected public data and private target data, comparing against baseline random selection approaches.

3. **Cross-Domain Generalization**: Test selective pre-training approach on at least two additional domain-specific datasets (e.g., medical text and legal documents) to verify performance gains generalize beyond evaluated domains.