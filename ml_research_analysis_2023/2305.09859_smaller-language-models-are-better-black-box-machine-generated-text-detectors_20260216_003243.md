---
ver: rpa2
title: Smaller Language Models are Better Black-box Machine-Generated Text Detectors
arxiv_id: '2305.09859'
source_url: https://arxiv.org/abs/2305.09859
tags:
- text
- curvature
- detector
- machine-generated
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores cross-detection of machine-generated text using
  surrogate models, without access to the original generator. It finds that smaller
  and partially-trained models are better universal detectors, outperforming larger
  models in detecting text generated by both small and large models.
---

# Smaller Language Models are Better Black-box Machine-Generated Text Detectors

## Quick Facts
- arXiv ID: 2305.09859
- Source URL: https://arxiv.org/abs/2305.09859
- Reference count: 8
- Key outcome: Smaller and partially-trained models are better universal detectors of machine-generated text, with OPT-125M achieving 0.81 AUC in detecting ChatGPT generations while GPTJ-6B achieves only 0.45 AUC.

## Executive Summary
This paper challenges the conventional wisdom that larger language models are better at detecting machine-generated text. The authors demonstrate that smaller and partially-trained models are superior universal detectors, outperforming larger models in cross-detection tasks where the detector doesn't have access to the original generator. Using a curvature-based detection method that measures the local optimality of sequences under different likelihood functions, the research shows that smaller models assign higher likelihood to generations from larger models, making them more effective cross-detectors. The findings have significant implications for developing robust detection systems that can identify text generated by unknown models.

## Method Summary
The detection method uses curvature (local optimality) of the likelihood function to distinguish machine-generated text from human-written text. For each target sequence, the method calculates the difference in loss between the sequence and its perturbations generated by a T5-3B mask-filling model. The target pool consists of 300 human-written sequences from SQuAD and 300 machine-generated sequences from 15 different models. Twenty-three detector models ranging from OPT-125M to OPT-6.7B are evaluated. The detection performance is measured using AUC scores, with smaller models consistently outperforming larger ones in cross-detection scenarios.

## Key Results
- Smaller models like OPT-125M achieve significantly higher AUC scores (0.81) in detecting ChatGPT generations compared to larger models like GPTJ-6B (0.45 AUC).
- Partially trained models outperform fully trained models as universal detectors, with this gap being larger for bigger models.
- The curvature-based detection method successfully distinguishes between human-written and machine-generated text without requiring access to the original generator.
- Self-detection (where the detector model is the same as the generator) remains effective but smaller models can still outperform larger ones in cross-detection scenarios.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models detect cross-model generations better due to looser likelihood surfaces.
- Mechanism: Smaller models assign higher likelihood to machine-generated text from larger models because their loss surfaces are flatter, meaning they are less selective and thus more likely to accept out-of-distribution generations.
- Core assumption: Likelihood of generated text is inversely related to model size in a cross-detection setting.
- Evidence anchors:
  - [abstract]: "smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models."
  - [section]: "We hypothesize that the reason behind large models being poor detectors of text generated by other models... is that larger models have a more refined taste, therefore they don't attribute text generated by other models as their own generations."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.46, average citations=0.0. Top related titles: Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack, An Evaluation of Explanation Methods for Black-Box Detectors of Machine-Generated Text, M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection. (Weak corpus evidence for this specific mechanism)
- Break condition: If the cross-model likelihood assumption is inverted, i.e., larger models assign higher likelihood to smaller-model generations.

### Mechanism 2
- Claim: Partially trained models are better detectors because they haven't fully fit to their training data.
- Mechanism: Early checkpoints of larger models have broader, flatter loss surfaces, so they are more likely to accept text generated by other models as their own. Fully trained models have sharper loss surfaces and reject more out-of-distribution text.
- Core assumption: Loss surface sharpness correlates with model training progress.
- Evidence anchors:
  - [abstract]: "we also find that partially trained models are better detectors than the fully trained ones, and this gap is bigger for larger models."
  - [section]: "our hypothesis for this is similar to that of Section 4, where we believe that partially trained models have not yet fit to the training data tightly, so they over claim other models' generations as their own."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.46, average citations=0.0. (Weak corpus evidence for this specific mechanism)
- Break condition: If model training stage does not affect likelihood surface shape.

### Mechanism 3
- Claim: Curvature-based detection works because generated text is locally optimal under the generator's likelihood function.
- Mechanism: The detection method measures the difference in loss between a target sequence and its perturbations. Generated text has lower curvature (less change in loss under perturbations) than human-written text, making it distinguishable.
- Core assumption: Generated text is more locally optimal (lower curvature) than human-written text under the generator's likelihood function.
- Evidence anchors:
  - [abstract]: "The detection method uses curvature (local optimality) of the likelihood function, comparing the loss of a sequence to its perturbations."
  - [section]: "The intuition in both prior works is that this measure of curvature is larger around training samples/generations from the model, compared to unseen human written text and can therefore be used to determine if a given sequence is part of the training data or not... or a generation of the target model or not."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.46, average citations=0.0. (Weak corpus evidence for this specific mechanism)
- Break condition: If human-written text is equally or more locally optimal than generated text under the likelihood function.

## Foundational Learning

- Concept: Local optimality in likelihood functions
  - Why needed here: The detection method relies on comparing the loss of a sequence to its perturbations to measure local optimality (curvature).
  - Quick check question: How does the curvature of a sequence under a model's likelihood function relate to whether the sequence is likely generated by that model?

- Concept: Model size and generalization
  - Why needed here: The paper shows that smaller models are better cross-detectors, which relates to how model size affects generalization and likelihood assignment.
  - Quick check question: How might the size of a language model affect its ability to generalize to text generated by other models?

- Concept: Training progression and model capacity
  - Why needed here: The paper finds that partially trained models are better detectors, which relates to how model capacity and training progression affect loss surfaces.
  - Quick check question: How might the stage of training a model is in affect its likelihood surface and its ability to detect cross-model generations?

## Architecture Onboarding

- Component map:
  Generator Model -> Target Pool (Human-written + Machine-generated) -> Perturbation Model (T5-3B) -> Detector Models (23 models) -> Curvature Calculation -> Binary Classification

- Critical path:
  1. Generate a target pool of human-written and machine-generated text.
  2. Generate perturbations of each target sequence using the perturbation model.
  3. Calculate the loss of the target sequences and their perturbations under the detector model.
  4. Calculate curvature as the difference in loss between the target and its perturbations.
  5. Threshold curvature to classify sequences as human-written or machine-generated.

- Design tradeoffs:
  - Using smaller models as detectors vs. larger models: Smaller models are better cross-detectors but may have lower overall accuracy on their own generations.
  - Using partially trained models as detectors vs. fully trained models: Partially trained models are better cross-detectors but may have lower overall accuracy.
  - Using different perturbation models: The choice of perturbation model affects the quality of the perturbations and thus the accuracy of the curvature calculation.

- Failure signatures:
  - Low AUC: Indicates poor distinguishability between human-written and machine-generated text.
  - High curvature for both human-written and machine-generated text: Indicates that the perturbations are not good neighbors of the target sequences.

- First 3 experiments:
  1. Replicate the self-detection results from Mitchell et al. (2023) to verify the curvature-based detection method.
  2. Test cross-detection using a small model (e.g., OPT-125M) to detect text generated by a larger model (e.g., GPT3).
  3. Test cross-detection using a partially trained model (e.g., Pythia-160M at 5k steps) to detect text generated by a larger model (e.g., GPT3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do smaller models assign higher likelihood to generations from larger models compared to larger models?
- Basis in paper: [explicit] The paper states that "smaller models, however, assign higher likelihood to generations of models their size or larger" and hypothesizes this is due to "smaller models have a less specific taste and are looser fitting models"
- Why unresolved: The paper only provides a hypothesis without testing or confirming the underlying mechanisms
- What evidence would resolve it: Controlled experiments comparing loss landscapes and curvature distributions across different model sizes when evaluating generations from other models

### Open Question 2
- Question: How do different perturbation generation methods affect the accuracy of curvature-based detection?
- Basis in paper: [explicit] The paper conducts ablation studies on perturbation methods, testing different mask-filling model sizes and masking percentages
- Why unresolved: While the paper shows effects of these parameters, it doesn't determine optimal configurations or explain the underlying reasons for the observed patterns
- What evidence would resolve it: Systematic comparison of detection accuracy across a comprehensive range of perturbation methods and parameters

### Open Question 3
- Question: Can partially trained models be used as universal detectors across different model architectures and training datasets?
- Basis in paper: [explicit] The paper finds that "partially trained models are better detectors than the fully trained ones" but only tests this within the Pythia family
- Why unresolved: The experiments are limited to one model family, leaving open whether this finding generalizes
- What evidence would resolve it: Testing partially trained checkpoints from multiple model families (OPT, GPT, etc.) on cross-detection tasks

## Limitations

- Methodological concerns: The curvature-based detection method relies on perturbations generated by a separate T5-3B model, with limited validation of perturbation quality.
- Training data contamination: The paper doesn't address potential training data overlap between target models and the SQuAD dataset used for human-written text.
- Limited generalizability: The findings are based on a relatively narrow set of model architectures and training regimes, limiting universal claims.

## Confidence

- **High confidence**: The empirical observation that smaller models achieve higher AUC scores in cross-detection tasks is well-supported by the presented results.
- **Medium confidence**: The claim that partially-trained models outperform fully-trained models is supported by the data, but the magnitude of this effect varies across model sizes.
- **Low confidence**: The specific mechanisms proposed (flatter loss surfaces, looser likelihood assignments) are largely speculative and lack direct empirical validation.

## Next Checks

1. **Perturbation quality validation**: Systematically vary the perturbation model size, masking percentage, and perturbation strategy to quantify how perturbation quality affects detection performance.

2. **Training data overlap analysis**: Conduct experiments using human-written text from datasets with varying degrees of overlap with the target models' training corpora to isolate memorization vs. genuine detection capability.

3. **Loss surface analysis**: Directly measure and compare loss surface properties (sharpness, flatness) of different detector models on held-out data to empirically validate the proposed mechanism linking loss surface shape to cross-detection ability.