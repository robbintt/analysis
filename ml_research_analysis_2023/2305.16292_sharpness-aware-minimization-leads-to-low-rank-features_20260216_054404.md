---
ver: rpa2
title: Sharpness-Aware Minimization Leads to Low-Rank Features
arxiv_id: '2305.16292'
source_url: https://arxiv.org/abs/2305.16292
tags:
- block
- rank
- feature
- features
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper uncovers a new effect of Sharpness-Aware Minimization
  (SAM): reduction of feature rank across different network layers and architectures.
  The authors show empirically that SAM leads to lower rank features in ResNets, Vision
  Transformers, and MLP-Mixers on multiple classification tasks (CIFAR, Tiny ImageNet,
  ImageNet) and contrastive learning (MS-COCO).'
---

# Sharpness-Aware Minimization Leads to Low-Rank Features

## Quick Facts
- arXiv ID: 2305.16292
- Source URL: https://arxiv.org/abs/2305.16292
- Reference count: 40
- Key outcome: SAM reduces feature rank across architectures and tasks, improving generalization through pruning ReLU activations.

## Executive Summary
This paper reveals that Sharpness-Aware Minimization (SAM) not only improves generalization but also systematically reduces feature rank across multiple network architectures and datasets. Through empirical studies on CIFAR, Tiny ImageNet, ImageNet, and MS-COCO, the authors demonstrate that SAM induces low-rank representations by pruning ReLU activations during training. They provide theoretical justification using two-layer ReLU networks and show that directly enforcing low-rank features fails to match SAM's generalization performance, suggesting SAM discovers beneficial low-dimensional structures in a data-adaptive manner.

## Method Summary
The paper investigates SAM's effect on feature rank across diverse architectures (ResNets, ViTs, MLP-Mixers) and tasks (classification, contrastive learning). Using PCA to measure rank (components explaining 99% variance), they train models with SAM using various ρ parameters and compare against standard SGD. The method includes theoretical analysis of two-layer ReLU networks to explain the pruning mechanism, empirical validation across five datasets, and experiments showing direct low-rank enforcement doesn't replicate SAM's generalization benefits.

## Key Results
- SAM consistently reduces feature rank across all tested architectures and datasets
- Lower rank correlates with improved generalization on classification and contrastive tasks
- SAM prunes ReLU activations, directly contributing to rank reduction
- Direct low-rank enforcement fails to match SAM's generalization improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM reduces feature rank by pruning ReLU activations during training.
- Mechanism: The SAM update includes a regularization component that drives pre-activation values toward negative values, effectively deactivating ReLU units. This pruning directly reduces the rank of the feature matrix.
- Core assumption: The regularization term in SAM's update rule is strong enough to push activations below zero consistently.
- Evidence anchors:
  - [abstract] "We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction."
  - [section 4.1] "the update of wj for neuron j on each step of SAM...drives pre-activations ⟨wj, x⟩ to negative values"
  - [corpus] Weak - neighbors focus on SAM's generalization properties, not feature rank mechanisms.

### Mechanism 2
- Claim: SAM's sharpness minimization implicitly favors flatter minima with lower-rank representations.
- Mechanism: By minimizing the maximum loss in a neighborhood (through the SAM objective), the optimization naturally selects solutions where fewer dimensions are needed to represent the data well.
- Core assumption: Flatter minima correlate with lower-rank feature representations.
- Evidence anchors:
  - [abstract] "we provide a theoretical argument supporting this effect of SAM"
  - [section 4.2] "a minimum that corresponds to a network sparsely activated on the training data"
  - [corpus] Weak - neighbors discuss SAM's optimization properties but don't explicitly link flatness to rank.

### Mechanism 3
- Claim: SAM's perturbation-based gradient estimation enables rank reduction that standard SGD cannot achieve.
- Mechanism: The weight perturbation in SAM allows the optimizer to explore parameter space more effectively, finding configurations where redundant features can be pruned without harming performance.
- Core assumption: The perturbation step provides meaningful exploration that standard SGD lacks.
- Evidence anchors:
  - [section 2.2] "SAM modifies stochastic gradient descent (SGD) such that on every iteration of training, the gradient is taken not at the current iterate but rather at an approximate worst-case point"
  - [section 4.1] "The data fitting term is the same as normal gradient but with a larger effective learning rate"
  - [corpus] Weak - neighbors focus on SAM's efficiency but not its perturbation mechanism's role in rank reduction.

## Foundational Learning

- Concept: Sharpness-Aware Minimization (SAM)
  - Why needed here: Understanding SAM is essential to grasp how the low-rank effect emerges through its unique optimization objective
  - Quick check question: What distinguishes SAM's update rule from standard SGD?

- Concept: Feature rank and PCA-based dimensionality reduction
  - Why needed here: The paper's core finding is about feature rank reduction, so understanding how rank is measured is crucial
  - Quick check question: How does the paper measure feature rank differently from mathematical matrix rank?

- Concept: ReLU activation pruning and sparsity
  - Why needed here: The mechanistic explanation for rank reduction relies on understanding how ReLU units can be deactivated
  - Quick check question: Why doesn't sparse activation automatically imply low rank in all cases?

## Architecture Onboarding

- Component map: SAM optimizer → weight perturbation → gradient estimation → parameter update. Feature extraction layers → activation functions → rank measurement via PCA.
- Critical path: SAM perturbation step → regularization component in update → ReLU deactivation → feature rank reduction → generalization improvement.
- Design tradeoffs: Higher ρ values in SAM lead to stronger rank reduction but may harm convergence; balancing rank reduction with optimization stability is key.
- Failure signatures: If rank reduction doesn't occur despite using SAM, check if weight perturbation is properly implemented or if the architecture's skip connections interfere with pruning.
- First 3 experiments:
  1. Train a simple two-layer ReLU network with SAM and monitor both feature rank and number of active ReLUs over training epochs.
  2. Compare feature rank reduction between SAM and standard SGD on CIFAR-10 with ResNet-18, varying ρ.
  3. Test if directly enforcing low-rank features (via linear bottleneck layers) can match SAM's generalization improvements on MS-COCO.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific components of deep neural network architectures (beyond two-layer networks) are most responsible for inducing low-rank features when trained with SAM?
- Basis in paper: [explicit] The paper notes that "the overall rank reduction mechanism can be more complex, especially for deep networks with pre-activation skip connections and self-attention layers" and observes different mechanisms in post-activation ResNets versus pre-activation ViTs.
- Why unresolved: The paper only provides preliminary observations on different architectures (ResNets, ViTs, MLP-Mixers) but doesn't systematically analyze which architectural components are most influential in creating the low-rank effect.
- What evidence would resolve it: Controlled experiments ablating different architectural components (residual connections, attention mechanisms, activation functions) while keeping training method constant, to isolate which features are most critical for SAM-induced rank reduction.

### Open Question 2
- Question: How does the low-rank effect of SAM interact with different data distributions and what determines the adaptive nature of this rank reduction?
- Basis in paper: [explicit] The paper shows the low-rank effect occurs across diverse datasets (CIFAR, Tiny ImageNet, ImageNet, MS-COCO) but doesn't explain what determines the adaptive rank reduction or how it varies with data structure.
- Why unresolved: While the paper demonstrates the effect is data-adaptive, it doesn't characterize what aspects of the data distribution drive this adaptation or whether the mechanism differs for structured versus unstructured data.
- What evidence would resolve it: Comparative studies across synthetic datasets with known low-rank structure versus natural datasets, measuring how SAM's rank reduction correlates with intrinsic data dimensionality and structure.

### Open Question 3
- Question: Why does SAM achieve generalization improvements through low-rank features when directly enforcing low-rank features does not improve generalization?
- Basis in paper: [explicit] Section 6 shows "directly inducing low-rank features does not result in improved generalization for natural data such as MS-COCO" despite SAM achieving both low-rank features and generalization improvements.
- Why unresolved: The paper demonstrates this contrast but doesn't explain what makes SAM's low-rank effect uniquely beneficial for generalization compared to explicit low-rank constraints.
- What evidence would resolve it: Analysis comparing the distribution of low-rank features learned by SAM versus explicit constraints, examining whether SAM discovers more task-relevant low-dimensional subspaces or better preserves discriminative information within reduced dimensions.

## Limitations

- Theoretical analysis is limited to two-layer ReLU networks without full generalization guarantees for deeper architectures
- The paper demonstrates correlation between rank reduction and generalization but doesn't establish causation through controlled experiments
- Unclear characterization of exact conditions under which pruning occurs consistently across different architectures

## Confidence

- **High Confidence**: Empirical observations of rank reduction across CIFAR-10/100, Tiny ImageNet, and ImageNet-1k with multiple architectures (PreAct ResNet-18, ViTs, MLP-Mixers)
- **Medium Confidence**: Theoretical analysis for two-layer ReLU networks explaining the pruning mechanism
- **Medium Confidence**: The claim that low-rank features directly contribute to improved generalization

## Next Checks

1. **Architecture Ablation Study**: Test SAM's rank reduction effect across a broader range of architectures (DenseNet, EfficientNet) to determine if the effect is architecture-agnostic or specific to certain design patterns.

2. **Rank vs. Generalization Causation**: Design experiments where rank is artificially constrained at different levels during SAM training to quantify the precise relationship between rank reduction and generalization improvement.

3. **Dynamic Rank Analysis**: Monitor feature rank evolution throughout training (not just at initialization and final) to understand whether rank reduction is gradual, sudden, or occurs in phases during optimization.