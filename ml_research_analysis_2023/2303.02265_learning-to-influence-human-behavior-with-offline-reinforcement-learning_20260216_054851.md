---
ver: rpa2
title: Learning to Influence Human Behavior with Offline Reinforcement Learning
arxiv_id: '2303.02265'
source_url: https://arxiv.org/abs/2303.02265
tags:
- human
- agent
- behavior
- learning
- luence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of learning to influence human
  behavior in collaborative settings where humans exhibit suboptimal behavior. It
  proposes using offline reinforcement learning (RL) to learn influence strategies
  from human-human interaction data without requiring online interaction with humans
  or simulators.
---

# Learning to Influence Human Behavior with Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2303.02265
- Source URL: https://arxiv.org/abs/2303.02265
- Reference count: 8
- Primary result: Offline RL can learn influence strategies from human-human interaction data to improve real human performance in collaborative tasks.

## Executive Summary
This paper proposes using offline reinforcement learning to learn influence strategies that improve human behavior in collaborative settings. The approach learns from datasets of human-human interactions by relabeling suboptimal trajectories with desired rewards, enabling the discovery of influence strategies that combine components of observed behaviors. The method is evaluated in the Overcooked collaborative cooking environment, where learned policies successfully improve human performance on both original tasks and unseen modified objectives. The approach also extends to handle changing human strategies by learning latent representations of human behavior and conditioning the policy on these representations, enabling adaptive influence that responds to evolving human intentions.

## Method Summary
The method uses offline reinforcement learning (specifically CQL) to learn influence policies from relabeled human-human interaction data. Suboptimal human-human trajectories are relabeled with desired rewards, and the offline RL algorithm learns to optimize these rewards using only the static dataset. The approach is extended with a latent strategy modeling component that infers human behavioral patterns from interaction history, allowing the policy to adapt its influence strategy based on the inferred human intentions. The learned policies combine components of observed behaviors to create new influence strategies that improve human performance.

## Key Results
- Learned influence policies significantly outperform behavior cloning and filtered behavior cloning when paired with real human players in Overcooked
- The method successfully generalizes to modified reward functions where humans lack complete information about task objectives
- Latent strategy modeling enables adaptive influence that responds to changing human strategies within episodes

## Why This Works (Mechanism)

### Mechanism 1
Offline RL learns to effectively influence suboptimal humans by extending and combining elements of observed human-human behavior. The algorithm "stitches" components of suboptimal trajectories by propagating reward signals from successful trajectories through the Q-function, discovering new influence strategies that combine observed behaviors.

### Mechanism 2
Learning latent representations of human behavior enables adaptive influence that responds to changing human strategies within episodes. The encoder maps sequences of past states to a low-dimensional representation of the human's current strategy, and the policy conditions on this representation to adapt its influence strategy.

### Mechanism 3
Relabeling suboptimal human-human interaction data with desired rewards enables offline RL to learn influence without requiring online interaction or simulators. The CQL algorithm handles distributional shift by penalizing Q-values for out-of-distribution actions, allowing safe learning from the static dataset.

## Foundational Learning

- **Partially Observable MDPs (POMDPs) and Hidden MDPs (Hi-MDPs)**: Needed to model the human's strategy as a latent variable that influences environment dynamics. Quick check: In a Hi-MDP, what additional component beyond a standard POMDP is needed to model the human's changing strategy?
- **Offline Reinforcement Learning and Distributional Shift**: Required for learning from static human-human interaction data without online interaction. Quick check: How does CQL address the distributional shift problem that arises when learning from offline human-human interaction data?
- **Latent Variable Models and Encoder-Decoder Architectures**: Needed to infer the human's latent strategy from interaction history. Quick check: What objective ensures that the learned latent strategy representations are sequentially consistent across timesteps?

## Architecture Onboarding

- **Component map**: Human action → State history → Encoder LSTM → Strategy representation → Q-function MLP → Action selection → Environment state → Human action (loop)
- **Critical path**: The sequential flow from human action through the encoder to strategy representation, then through the Q-function to action selection, and back to the environment state
- **Design tradeoffs**: Hand-crafted features vs learned embeddings (trade-off between interpretability and expressiveness), fixed vs adaptive strategy representation dimension (simplicity vs flexibility), LSTM vs transformer for encoder (simplicity vs longer-range dependencies)
- **Failure signatures**: Policy fails to influence (check encoder learning meaningful representations), Q-values diverge (check CQL penalty term magnitude), human behavior not adapting (verify strategy representation changes correlate with human strategy changes)
- **First 3 experiments**: 1) Train with no strategy conditioning vs strategy-conditioned version to measure impact of latent strategy modeling, 2) Vary strategy representation dimension (4, 8, 16) to find optimal tradeoff, 3) Test on modified objectives where human lacks reward information to verify generalization

## Open Questions the Paper Calls Out

The paper identifies several open questions including how robust the learned influence strategy is to different types of human cognitive biases, whether the method can learn to influence humans toward more prosocial behaviors, how offline RL compares to methods that explicitly model human cognitive processes, what minimum data requirements exist for effective learning, and how the strategy performs against humans who actively resist influence.

## Limitations
- Relies heavily on diversity and quality of offline human-human interaction dataset
- Hand-crafted state features may miss subtle behavioral cues
- Fixed latent strategy representation dimension may not be optimal for all behavioral patterns

## Confidence
- High confidence in the core offline RL mechanism for combining observed behaviors
- Medium confidence in the latent strategy modeling component
- Medium confidence in the generalization claims

## Next Checks
1. Conduct ablation studies removing the latent strategy component to quantify its contribution to adaptive influence
2. Test the learned policies with diverse human populations to assess robustness and generalization
3. Implement online fine-tuning capability to evaluate whether modest online interaction can significantly improve the offline-learned influence strategies