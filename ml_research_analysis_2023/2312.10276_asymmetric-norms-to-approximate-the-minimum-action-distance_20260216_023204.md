---
ver: rpa2
title: Asymmetric Norms to Approximate the Minimum Action Distance
arxiv_id: '2312.10276'
source_url: https://arxiv.org/abs/2312.10276
tags:
- distance
- asymmetric
- states
- symmetric
- minimum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning a state representation
  for reward-free Markov decision processes (MDPs) where the goal is to approximate
  the minimum number of actions needed to transition between any two states (the Minimum
  Action Distance or MAD). Previous methods used symmetric norms which fail in asymmetric
  environments.
---

# Asymmetric Norms to Approximate the Minimum Action Distance

## Quick Facts
- arXiv ID: 2312.10276
- Source URL: https://arxiv.org/abs/2312.10276
- Reference count: 4
- Primary result: Asymmetric Wide Norms outperform symmetric norms in approximating Minimum Action Distance in asymmetric environments and improve downstream planning

## Executive Summary
This paper introduces Wide Norms, a parametric asymmetric distance metric, to approximate the Minimum Action Distance (MAD) in reward-free Markov decision processes. Unlike previous methods using symmetric norms, Wide Norms can capture directional dependencies in environments where actions are not reversible. The approach learns an embedding space where distances correspond to MAD, enabling goal-conditioned planning through a learned transition model. Empirical results demonstrate superior performance in asymmetric environments while maintaining competitive results in symmetric settings.

## Method Summary
The method learns state embeddings using Wide Norms, a parametric distance metric based on Mahalanobis norm with ReLU activation that allows asymmetric distances. The training uses trajectory data to constrain the learned distances to upper bounds derived from trajectory distances, which are looser bounds on MAD. After learning the embedding, a transition model is trained to predict next-state embeddings given actions, enabling goal-conditioned planning in the latent space by selecting actions that minimize distance to goal embeddings.

## Key Results
- Asymmetric Wide Norms accurately approximate MAD in environments with inherent asymmetry
- Performance matches symmetric norms in symmetric environments but significantly outperforms them in asymmetric environments
- The embedding plus transition model enables effective goal-conditioned planning
- Code is publicly available at https://github.com/lorenzosteccanella/SRL (branch "NIPS-GCRL-Workshop")

## Why This Works (Mechanism)

### Mechanism 1
Asymmetric Wide Norms represent true MAD by using a parametric distance function that preserves directional dependencies between states. The Wide Norm formulation ||x|W N = ||W relu(x :: −x)||2 allows asymmetric semi-norms to be combined, capturing the asymmetry in environments where actions are not reversible. This works because the true MAD in asymmetric environments cannot be accurately represented by symmetric norms.

### Mechanism 2
The method uses trajectory distance (TD) as an upper bound in the loss formulation, allowing MAD approximation without explicit state equality detection. The optimization constrains learned distances to not exceed TD values from trajectories, which are guaranteed upper bounds on MAD. This indirect supervision works because diverse and sufficiently long trajectories provide meaningful constraints for most state pairs.

### Mechanism 3
The learned embedding enables goal-conditioned planning by combining with a transition model. After learning an embedding where distances approximate MAD, a transition model ρζ predicts next state embeddings given current state and action. Planning proceeds by selecting actions that minimize distance to goal embeddings in the latent space, leveraging the correlation between embedding distance and actual action counts.

## Foundational Learning

- **Markov Decision Processes (MDPs) and state transition graphs**: Understanding MDP structure is essential for grasping why asymmetry matters in state transitions. *Quick check*: What is the difference between the transition function P(s'|s,a) in an MDP and the adjacency matrix of the state-transition graph?

- **Norms and distance metrics, especially asymmetric norms**: The paper contrasts symmetric norms with asymmetric Wide Norms. *Quick check*: What properties must a function satisfy to be considered a norm, and which of these properties are violated in asymmetric norms?

- **Self-supervised representation learning and constrained optimization**: The method learns state representations without rewards by optimizing distance metrics under trajectory-derived constraints. *Quick check*: How does converting hard constraints into penalty terms in the loss function affect the optimization landscape?

## Architecture Onboarding

- **Component map**: Trajectory dataset loader → Distance constraint generator → Wide Norm embedding network → Transition model network → Planning module

- **Critical path**: 
  1. Load trajectories and extract (state, state, TD) and (state, next_state, 1) tuples
  2. Train Wide Norm embedding to minimize distance to TD under upper-bound constraints
  3. Train transition model to predict next state embedding given action
  4. Use embedding + transition model for goal-conditioned planning

- **Design tradeoffs**:
  - Symmetric vs. asymmetric norms: symmetric norms are simpler but fail in asymmetric environments; asymmetric norms are more expressive but require more parameters and careful initialization
  - Penalty coefficient C in loss: too small and constraints are ignored; too large and optimization becomes unstable
  - Embedding dimension: too low and the model cannot capture the geometry; too high and overfitting or inefficient planning may occur

- **Failure signatures**:
  - Embedding distances converge to min(dMAD(s_i,s_j), dMAD(s_j,s_i)) instead of full MAD → symmetric norm used in asymmetric environment
  - Transition model predictions have high error → embedding space poorly aligned with true dynamics
  - Planning fails to reach goals → either embedding distances are wrong or transition model is inaccurate

- **First 3 experiments**:
  1. Verify that Wide Norms produce asymmetric distance matrices in a known asymmetric grid world (e.g., one-way passages)
  2. Compare planning success rate using embedding+transition model vs. ground truth MDP planner in a simple environment
  3. Ablation: replace Wide Norms with L1 norm and measure degradation in asymmetric environments only

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Wide Norms compare to other asymmetric distance metrics beyond symmetric norms, such as learned attention-based distance functions? The paper only compares Wide Norms to symmetric norms, leaving open whether other asymmetric approaches could perform better.

### Open Question 2
What is the theoretical relationship between the quality of trajectories in the dataset and the accuracy of the learned Minimum Action Distance approximation? The paper acknowledges this dependence but doesn't provide theoretical bounds on approximation accuracy based on trajectory characteristics.

### Open Question 3
How does the choice of norm (e.g., L1, L2, L∞) in the symmetric case affect performance in environments with varying degrees of asymmetry? The paper only tests L1 norm in symmetric cases, not exploring whether different symmetric norms perform better across different asymmetry levels.

### Open Question 4
Can the Wide Norms approach be extended to handle stochastic MDPs where transition probabilities are non-uniform? The paper focuses on deterministic MDPs and doesn't address how the approach would handle stochastic environments with probabilistic action outcomes.

## Limitations
- Assumes true MAD can be captured by Wide Norm parametrization, which may fail in environments requiring higher-order non-linearities
- Relies on trajectory-based upper bounds, requiring sufficient diversity and length in training data
- Transition model accuracy directly impacts planning performance, creating potential failure cascades from embedding to planning stages

## Confidence
- High confidence in the theoretical foundation of Wide Norms as asymmetric distance metrics
- Medium confidence in empirical performance claims, dependent on specific implementation details and hyperparameter choices
- Low confidence in scalability to very large state spaces without additional techniques (hashing, compression)

## Next Checks
1. Test Wide Norms on environments with known non-linear asymmetry (e.g., asymmetric friction fields) to verify parametrization limits
2. Evaluate performance degradation with decreasing trajectory length and diversity to quantify data requirements
3. Compare planning success rates using only the embedding (no transition model) versus full embedding+model approach to isolate contribution of each component