---
ver: rpa2
title: Cross Entropy in Deep Learning of Classifiers Is Unnecessary -- ISBE Error
  is All You Need
arxiv_id: '2311.16357'
source_url: https://arxiv.org/abs/2311.16357
tags:
- softmax
- function
- loss
- learning
- isbe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the necessity of using cross-entropy in deep
  learning classifiers, proposing instead a simpler "ISBE" approach. ISBE involves
  normalizing raw scores to soft scores during the forward pass and computing the
  error directly as the difference between the soft scores and target scores during
  the backward pass, bypassing the explicit calculation of cross-entropy loss.
---

# Cross Entropy in Deep Learning of Classifiers Is Unnecessary -- ISBE Error is All You Need

## Quick Facts
- arXiv ID: 2311.16357
- Source URL: https://arxiv.org/abs/2311.16357
- Reference count: 40
- One-line primary result: ISBE technique achieves comparable or better classification accuracy than cross-entropy while reducing backward pass computation time by up to 3%

## Executive Summary
This paper challenges the conventional wisdom that cross-entropy loss is necessary for training neural network classifiers. The authors propose the ISBE (Inference-Softmax-Backward-Error) approach, which bypasses explicit cross-entropy computation by directly calculating error as the difference between normalized scores and target scores during backpropagation. Through experiments on MNIST using various activation functions, the paper demonstrates that ISBE achieves classification accuracy above 99.5% while offering computational efficiency gains. The theoretical foundation shows that for softmax activation, ISBE gradients are mathematically equivalent to traditional cross-entropy gradients.

## Method Summary
The ISBE technique modifies the standard training pipeline by eliminating explicit cross-entropy loss calculation. During the forward pass, raw network outputs are normalized to soft scores using activation functions like softmax, sigmoid, or tanh. During the backward pass, error is computed directly as the difference between these soft scores and target scores, bypassing the Jacobian computation required for cross-entropy. The method is implemented by modifying the backward pass of standard neural networks and is tested across multiple architectures on the MNIST dataset with various activation functions including their hard variants.

## Key Results
- ISBE achieves classification accuracy above 99.5% on MNIST, matching or slightly exceeding traditional cross-entropy methods
- Computational time savings of up to 3% in the backward pass when using ISBE
- The technique works across multiple activation functions (SoftMax, Sigmoid, Tanh, and their hard variants) with comparable performance
- Theoretical proof shows ISBE gradients match cross-entropy gradients for softmax activation

## Why This Works (Mechanism)

### Mechanism 1
Omitting explicit cross-entropy computation still yields correct gradients for softmax-based classifiers. The Jacobian of softmax is of the form `diag(y) - yy^T`. When combined with the cross-entropy gradient `-y^*/y`, the product simplifies to `y - y*`, eliminating the need for explicit cross-entropy calculation. This relies on the softmax Jacobian having the specific form `diag(y) - yy^T`.

### Mechanism 2
ISBE works with other monotonic activation functions beyond softmax. The backward pass directly computes `error = soft_scores - target_scores` without requiring the specific softmax Jacobian property. This works when activation functions produce bounded outputs in a limited range (e.g., [0,1] or [-1,+1]) and are monotonic.

### Mechanism 3
ISBE reduces computational overhead by ~3% compared to traditional cross-entropy. By bypassing explicit cross-entropy calculation and its associated Jacobian computation, ISBE eliminates these operations from the backward pass, reducing overall computation time. This assumes the computational cost of cross-entropy calculation and its Jacobian is non-negligible in the backward pass.

## Foundational Learning

- **Concept:** Softmax normalization
  - Why needed here: Understanding how softmax transforms raw scores into probabilities is crucial for grasping why the ISBE gradient simplification works.
  - Quick check question: What mathematical property of the softmax function's Jacobian enables the gradient simplification?

- **Concept:** Cross-entropy loss
  - Why needed here: Understanding cross-entropy as a divergence measure between predicted and target distributions is essential for appreciating what ISBE replaces.
  - Quick check question: How does cross-entropy loss mathematically penalize incorrect classifications?

- **Concept:** Backpropagation algorithm
  - Why needed here: ISBE modifies the backward pass, so understanding how gradients flow backward through the network is critical.
  - Quick check question: What is the chain rule application that allows error gradients to flow from output to input layers?

## Architecture Onboarding

- **Component map:** Input layer → Network (CNN/MLP) → ISBE unit → Loss calculation (implicit in ISBE) → Parameter updates
- **Critical path:** Forward pass: Network output → ISBE normalization → Soft scores; Backward pass: Soft scores - Target scores → Error propagation to network parameters
- **Design tradeoffs:** ISBE: Simpler implementation, ~3% speedup, works with multiple activations; Traditional: More complex, no speedup, limited to softmax + cross-entropy
- **Failure signatures:** Classification accuracy drops below baseline; Gradient explosion or vanishing (if inappropriate activation chosen); Training instability (if activation doesn't meet Lipschitz condition)
- **First 3 experiments:** 1) Replace softmax + cross-entropy with ISBE using softmax activation on MNIST; verify accuracy matches baseline; 2) Try ISBE with sigmoid activation on MNIST; compare accuracy and training time to baseline; 3) Implement ISBE with tanh activation; test classification performance and check for gradient stability

## Open Questions the Paper Calls Out

### Open Question 1
Does the ISBE approach maintain its computational advantage when applied to larger, more complex datasets beyond MNIST, such as CIFAR-10 or ImageNet? The paper demonstrates a 3% computational saving on MNIST, but it's unclear if this advantage scales with dataset complexity.

### Open Question 2
What are the theoretical properties of activation functions that make them suitable for the ISBE approach? The paper mentions the Lipschitz condition as a potential property, but are there others? A comprehensive theoretical analysis is needed.

### Open Question 3
How does the choice of loss reduction method (none, mean, sum) impact the performance of ISBE with different activation functions? The paper shows some differences, but a more systematic investigation is needed across various architectures and datasets.

## Limitations

- Theoretical justification for ISBE is complete only for softmax activation, with limited experimental validation for other activation functions
- The 3% computational savings claim is based on specific architectures (N0, N1) that are not fully described
- The MNIST dataset, while standard, represents a limited scope for generalization claims

## Confidence

- **High confidence**: ISBE with softmax activation produces identical gradients to cross-entropy + softmax (theoretical proof provided)
- **Medium confidence**: ISBE maintains classification accuracy (>99.5%) across different activation functions (limited experimental validation)
- **Low confidence**: ISBE provides consistent 3% computational speedup across different architectures and implementations (sparse data, specific conditions)

## Next Checks

1. Verify gradient equivalence: Implement gradient checking to confirm that ISBE gradients match traditional cross-entropy gradients for all tested activation functions
2. Scale complexity testing: Test ISBE on larger datasets (CIFAR-10, ImageNet) to validate if computational savings scale with problem size
3. Architecture generalization: Implement ISBE across diverse network architectures (ResNets, Transformers) to assess if the 3% speedup holds beyond the specific N0/N1 architectures mentioned