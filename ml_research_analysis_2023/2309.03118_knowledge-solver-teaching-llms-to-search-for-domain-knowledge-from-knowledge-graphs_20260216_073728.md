---
ver: rpa2
title: 'Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge
  Graphs'
arxiv_id: '2309.03118'
source_url: https://arxiv.org/abs/2309.03118
tags:
- knowledge
- llms
- arxiv
- question
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knowledge Solver is a novel method to enhance LLMs by leveraging
  external knowledge graphs for tasks that require domain-specific knowledge. Unlike
  prior approaches that train additional modules, KSL teaches LLMs to search for knowledge
  by themselves using a multi-hop decision sequence encoded into text prompts.
---

# Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs

## Quick Facts
- arXiv ID: 2309.03118
- Source URL: https://arxiv.org/abs/2309.03118
- Reference count: 10
- Key outcome: Improves LLM accuracy on knowledge-intensive tasks by 2.8% to 38.5% through graph traversal encoded as text prompts

## Executive Summary
Knowledge Solver (KSL) is a novel method that enhances large language models' ability to reason with domain-specific knowledge by teaching them to search knowledge graphs through text-based multi-hop decision sequences. Unlike prior approaches that add specialized modules, KSL leverages the LLM's existing reasoning capabilities by encoding knowledge graph traversal paths as natural language prompts. The approach shows substantial improvements across three benchmark datasets - CommonsenseQA, OpenbookQA, and MedQA-USMLE - with accuracy gains ranging from 2.8% to 38.5% over baseline LLMs.

## Method Summary
KSL transforms knowledge graph traversal into a text-based multi-hop reasoning task that LLMs can perform natively without additional modules. The method works by encoding the local neighborhood around entities as text prompts that include the current entity, possible next entities, and their relations. LLMs then generate the next entity in the path purely through text generation. For zero-shot reasoning, KSL constructs prompts for each hop in the traversal path. For finetuning, KSL converts graph paths into instruction-response pairs and uses LoRA-based instruction tuning to inject domain-specific knowledge into the LLM's parameters.

## Key Results
- Zero-shot reasoning with KSL improves baseline LLM performance by 2.8% to 38.5% across three datasets
- Finetuned LLMs using KSL-constructed datasets outperform finetuned LLMs without KSL
- Performance gains are consistent across different model sizes (GPT-3.5, LLaMA-7B, LLaMA2-7B)
- Improvements are most substantial on CommonsenseQA (38.5%) and OpenbookQA (13.2%) compared to MedQA-USMLE (2.8%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge Solver transforms graph traversal into a text-based multi-hop reasoning task that LLMs can perform natively.
- Mechanism: Instead of training GNN modules, KSL encodes the local neighborhood around each entity as text (including relations and history) and uses the LLM's generative capability to select the next entity in the path.
- Core assumption: LLMs can interpret structured knowledge in natural language form and make coherent multi-step decisions.
- Evidence anchors:
  - [abstract] "we design a simple yet effective prompt to transform retrieval into a multi-hop decision sequence"
  - [section] "we put the current head entity vh and all linked head entities Vt... in the text prompt to inform LLMs of the existence of external knowledge"
- Break condition: If the LLM fails to interpret the relation semantics in the prompt, or if the path length exceeds the LLM's reasoning capacity.

### Mechanism 2
- Claim: Encoding knowledge graph edges as text prompts allows zero-shot reasoning without additional modules.
- Mechanism: For each hop, KSL constructs a prompt that includes the current entity, possible next entities, and their relations, letting the LLM choose the next step purely via text generation.
- Core assumption: The LLM's parameters already encode sufficient reasoning ability to navigate structured knowledge when provided in text form.
- Evidence anchors:
  - [abstract] "empowers LLMs with searching knowledge ability in zero-shot manner"
  - [section] "This teaches LLMs to interact with external knowledge to achieve final goals"
- Break condition: If the knowledge graph is too sparse or the relations are too ambiguous for the LLM to disambiguate in text form.

### Mechanism 3
- Claim: Finetuning on KSL-constructed datasets injects domain-specific knowledge into the LLM's parameters.
- Mechanism: By converting graph paths into instruction-response pairs, KSL creates a dataset that teaches the LLM to follow structured reasoning patterns.
- Core assumption: Instruction tuning with LoRA is effective at adapting LLMs to new reasoning tasks without full retraining.
- Evidence anchors:
  - [section] "we use a similar template in Alpaca... to encourage LLMs to learn domain-specific knowledge"
  - [section] "We use LoRA (Hu et al., 2021) to tune LLMs since it can help greatly reduce GPU memory burden"
- Break condition: If the finetuning dataset is too small or the knowledge graph coverage is insufficient for the target domain.

## Foundational Learning

- Concept: Knowledge Graphs as multi-relational graphs
  - Why needed here: Understanding how entities and relations form structured knowledge that can be traversed
  - Quick check question: In a KG, what do edges represent and how do they differ from nodes?

- Concept: Multi-hop reasoning
  - Why needed here: KSL relies on LLMs making sequential decisions across multiple steps to reach answers
  - Quick check question: Why might a single-hop retrieval be insufficient for complex question answering?

- Concept: Instruction tuning with LoRA
  - Why needed here: The finetuning approach uses parameter-efficient methods to adapt LLMs to KSL's reasoning patterns
  - Quick check question: What advantage does LoRA provide over full model finetuning in terms of memory and training time?

## Architecture Onboarding

- Component map: Knowledge Graph retrieval -> Text prompt encoder -> LLM inference -> Answer selection
- Critical path:
  1. Retrieve subgraph for question-answer pair
  2. Encode subgraph as text prompts for each hop
  3. LLM generates next entity selection
  4. Repeat until answer entity reached or max hops
  5. Evaluate generated answer against ground truth

- Design tradeoffs:
  - Zero-shot vs finetuned: Zero-shot requires no training but may be less accurate; finetuned improves performance but requires computational resources
  - Prompt complexity: More detailed prompts may improve accuracy but increase token usage and latency
  - Graph hop limit: Higher limits allow more complex reasoning but increase inference time and risk of incorrect paths

- Failure signatures:
  - LLM gets stuck in loops (repeated entity selections)
  - LLM selects irrelevant entities due to ambiguous relations
  - Performance degradation on datasets with sparse KG coverage
  - Input prompt exceeds context window limits

- First 3 experiments:
  1. Measure zero-shot accuracy on CommonsenseQA with varying hop limits (1-5)
  2. Compare finetuned vs zero-shot performance on OpenbookQA
  3. Test different prompt formats (minimal vs detailed relation descriptions) on MedQA-USMLE

## Open Questions the Paper Calls Out
- How does the choice of the initial question entity affect the performance of Knowledge Solver in zero-shot reasoning? (The paper mentions this is randomly chosen and leaves investigation for future research)
- How does the size and quality of the Knowledge Graph affect the performance of Knowledge Solver? (Performance improvement on MedQA-USMLE is less substantial, possibly due to insufficient KG size)
- How does the multi-hop decision sequence design of Knowledge Solver compare to other knowledge retrieval methods in terms of efficiency and effectiveness? (The paper introduces this as a key component but doesn't provide detailed comparison)

## Limitations
- The exact template and format of text prompts used to encode knowledge graph structure is not fully specified, making exact reproduction challenging
- Performance improvements rely heavily on the completeness and relevance of external knowledge graphs, with no detailed analysis of coverage limitations
- Computational costs and potential for overfitting in the finetuning approach are not thoroughly discussed, nor is scalability to larger models or more diverse domains

## Confidence
- High Confidence: The core mechanism of encoding graph traversal as text prompts is well-described and technically sound
- Medium Confidence: Reported accuracy improvements are substantial but lack of detailed prompt specifications introduces uncertainty about reproducibility
- Low Confidence: Long-term effectiveness of finetuning across diverse domains and potential for catastrophic forgetting are not addressed

## Next Checks
1. Implement multiple variations of the text prompt template to determine which specific elements are most critical for successful graph traversal and reasoning
2. Systematically measure the percentage of questions that can be answered using available knowledge graph paths versus those requiring external information
3. Apply KSL to a fourth, previously unseen dataset from a different knowledge domain to assess cross-domain generalization beyond the three studied datasets