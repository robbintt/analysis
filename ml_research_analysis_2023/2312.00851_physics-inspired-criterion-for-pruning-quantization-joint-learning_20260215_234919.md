---
ver: rpa2
title: Physics Inspired Criterion for Pruning-Quantization Joint Learning
arxiv_id: '2312.00851'
source_url: https://arxiv.org/abs/2312.00851
tags:
- compression
- learning
- quantization
- ieee
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a physics inspired criterion (PIC) for pruning-quantization
  joint learning (PIC-PQ) in deep neural networks (DNNs). The authors draw an analogy
  between elasticity dynamics (ED) and model compression (MC) to derive PIC from Hooke's
  law in ED.
---

# Physics Inspired Criterion for Pruning-Quantization Joint Learning

## Quick Facts
- arXiv ID: 2312.00851
- Source URL: https://arxiv.org/abs/2312.00851
- Reference count: 40
- This paper proposes a physics-inspired criterion for joint pruning and quantization that achieves state-of-the-art accuracy-BOPs trade-offs on image classification benchmarks.

## Executive Summary
This paper introduces a physics-inspired criterion (PIC) for joint pruning and quantization in deep neural networks. The authors draw an analogy between elasticity dynamics and model compression, deriving a criterion from Hooke's law that establishes a linear relationship between filter importance and filter properties using a learnable deformation scale. A relative shift variable enables global ranking across all layers. Experiments demonstrate superior accuracy-BOPs trade-offs compared to existing methods, achieving 54.96× BOPs compression on ResNet56 with only 0.10% accuracy drop on CIFAR10, and 53.24× BOPs compression on ResNet18 with 0.61% accuracy drop on ImageNet.

## Method Summary
The method establishes a physics-inspired criterion (PIC) for joint pruning-quantization by drawing an analogy between elasticity dynamics and model compression. The criterion uses a learnable deformation scale to create a linear relationship between filter importance and filter properties (defined as feature map ranks). A relative shift variable enables global ranking across layers. The method employs a regularized evolutionary algorithm to learn the optimal parameters for ranking filters, then assigns pruning masks and quantization bitwidths based on this global ranking. Mixed-precision quantization is applied with learnable bitwidth assignment, optimizing for both accuracy and computational efficiency.

## Key Results
- Achieves 54.96× BOPs compression ratio on ResNet56 with only 0.10% accuracy drop on CIFAR10
- Achieves 53.24× BOPs compression ratio on ResNet18 with 0.61% accuracy drop on ImageNet
- Outperforms existing pruning-quantization methods in accuracy-BOPs trade-off across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The linear relationship between filter importance and filter property (FP) via a learnable deformation scale (al(i)) enables interpretable pruning by ranking filters based on their contribution to feature richness.
- Mechanism: Drawing on Hooke's law, the method maps each convolutional layer to an elastomer where the deformation scale (al(i)) acts like the elasticity modulus (EM), controlling how much a filter's importance is influenced by its FP. The FP is defined as the average rank of the feature maps generated by the filter, which reflects information richness. Filters with lower FP (lower rank) are deemed less important and pruned.
- Core assumption: The rank of a filter's feature maps is a stable and reliable measure of its information richness, and this rank is not significantly affected by the input images.
- Evidence anchors:
  - [abstract] "we establish a linear relationship between the filters' importance distribution and the filter property (FP) by a learnable deformation scale"
  - [section] "we define the FP as: F P Θl(i) i = R ol(i) i (D), where ol(i) i (D) is the feature map of the i-th filter in the l(i)-th layer. R(·) is the rank of feature map for input images D."
- Break condition: If the feature map rank is not a stable measure of information richness, or if the linear relationship does not hold, the pruning will not be interpretable or effective.

### Mechanism 2
- Claim: The relative shift variable (bl(i)) enables global ranking of filters across all layers, moving beyond layer-local importance to a cross-layer perspective.
- Mechanism: The relative shift variable is added to the importance distribution equation (I l(i) i = al(i) · F P Θl(i) i + bl(i)), allowing filters from different layers to be compared directly. This is necessary because Hooke's law only provides a local ranking within each layer. The global ranking ensures that filters are pruned based on their importance relative to all other filters in the network, not just within their own layer.
- Core assumption: A global ranking of filters is necessary for effective pruning-quantization joint learning, as it ensures that the most important filters are preserved regardless of their layer.
- Evidence anchors:
  - [abstract] "we extend PIC with a relative shift variable for a global view"
  - [section] "While Hooke's law may possess theoretical appeal, its direct application is limited for us due to the absence of a global optimization concept. Hooke's law can only cover the importance ranking of a filter in its own layer, instead of its importance ranking among all filters. So we further introduce a relative shift variable to rank filters across different layers globally."
- Break condition: If the global ranking does not lead to better compression or accuracy, or if the relative shift variable is not effectively learned, the global ranking mechanism will not work.

### Mechanism 3
- Claim: The combination of pruning and quantization in a joint learning framework, guided by the physics-inspired criterion (PIC), leads to better accuracy-accuracy trade-offs than sequential pruning and quantization.
- Mechanism: The PIC provides a unified criterion for both pruning and quantization. The global ranking of filters informs both which filters to prune and how to assign quantization bitwidths to each layer. Layers with more important filters receive higher bitwidths, while less important layers can be quantized more aggressively. This joint optimization ensures that both pruning and quantization work together to maximize accuracy while minimizing computational complexity.
- Core assumption: Joint learning of pruning and quantization is more effective than sequential application, and the PIC provides a suitable criterion for this joint learning.
- Evidence anchors:
  - [abstract] "Experiments on benchmarks of image classification demonstrate that PIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs) compression ratio"
  - [section] "Compared to two-stage compression strategy, pruning-quantization joint learning is more likely to ensure mutual promotion."
- Break condition: If the joint learning does not lead to better results than sequential pruning and quantization, or if the PIC does not provide a suitable criterion, the joint learning mechanism will not work.

## Foundational Learning

- Concept: Elasticity Dynamics (ED) and Hooke's Law
  - Why needed here: The paper draws an analogy between ED and model compression (MC) to derive the physics-inspired criterion (PIC). Understanding ED and Hooke's law is crucial for understanding the motivation and design of PIC.
  - Quick check question: What is the relationship between force, deformation, and elasticity modulus in Hooke's law?

- Concept: Feature Map Rank as a Measure of Information Richness
  - Why needed here: The filter property (FP) is defined as the average rank of the feature maps generated by the filter. Understanding why feature map rank is a suitable measure of information richness is important for understanding the PIC.
  - Quick check question: Why is the rank of a feature map a reliable measure of the information richness it contains?

- Concept: Lipschitz Continuity
  - Why needed here: The paper uses Lipschitz continuity to provide an upper bound on the difference in loss between the baseline and compressed models. This is part of the mathematical justification for the PIC.
  - Quick check question: What is the Lipschitz condition, and how does it relate to the optimization of complex functions?

## Architecture Onboarding

- Component map: Sampled images (Dsample) -> Filter Property (FP) Generation -> Physics Inspired Criterion (PIC) Learning -> Global Filter Ranking -> Compression Policy Assignment -> Model Compression and Fine-tuning
- Critical path: FP Generation → PIC Learning → Global Filter Ranking → Compression Policy Assignment → Model Compression and Fine-tuning
- Design tradeoffs:
  - Using a global ranking of filters allows for more effective pruning and quantization, but it requires additional computation to learn the deformation scale and relative shift variable.
  - The choice of the penalty factor (p) for bitwidth assignment affects the trade-off between accuracy and compression ratio.
  - The number of fine-tuning iterations (τ) affects the accuracy of the compressed model.
- Failure signatures:
  - If the global ranking does not lead to better compression or accuracy, there may be an issue with the PIC or the learning algorithm.
  - If the compressed model has low accuracy, there may be an issue with the fine-tuning process or the choice of hyperparameters.
  - If the compression ratio is low, there may be an issue with the pruning or quantization policy assignment.
- First 3 experiments:
  1. Verify that the feature map rank is a stable measure of information richness by calculating the rank for different input images and checking for consistency.
  2. Test the PIC learning algorithm on a small network to ensure that it can learn the deformation scale and relative shift variable effectively.
  3. Evaluate the compressed model on a validation set to check for accuracy and compression ratio, and compare the results to a baseline model.

## Open Questions the Paper Calls Out
No open questions were explicitly identified in the provided content.

## Limitations
- The physics analogy, while novel, lacks direct empirical validation beyond the proposed method's performance.
- The claim that feature map rank is a stable measure of information richness across different input distributions remains an assumption rather than a proven property.
- The global ranking mechanism's effectiveness depends heavily on the quality of the learned deformation scale and relative shift variables, but the paper provides limited analysis of how these parameters behave during training.

## Confidence
- High Confidence: The reported accuracy-BOPs trade-offs on CIFAR10 and ImageNet benchmarks appear robust, with detailed experimental results and multiple comparison points to baseline methods.
- Medium Confidence: The mechanism by which the physics-inspired criterion guides joint pruning-quantization is logically sound but lacks ablation studies to isolate the contribution of each component.
- Low Confidence: The fundamental assumption that feature map rank correlates with information richness and that this relationship is stable across different inputs is not empirically validated in the paper.

## Next Checks
1. **Rank Stability Analysis**: Measure feature map rank consistency across different input distributions and network states (early vs late training) to validate the core assumption about information richness.

2. **Component Ablation Study**: Systematically disable the global ranking mechanism and deformation scale learning to quantify their individual contributions to the final performance.

3. **Physics Analogy Validation**: Design experiments to test whether the Hooke's law analogy provides meaningful insights beyond standard importance scoring methods, such as interpretability benefits or improved optimization dynamics.