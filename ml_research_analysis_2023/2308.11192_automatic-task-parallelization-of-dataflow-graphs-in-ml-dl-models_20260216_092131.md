---
ver: rpa2
title: Automatic Task Parallelization of Dataflow Graphs in ML/DL models
arxiv_id: '2308.11192'
source_url: https://arxiv.org/abs/2308.11192
tags:
- parallelism
- graph
- cluster
- clusters
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for automatic task parallelization
  of dataflow graphs in ML/DL models. It addresses the problem of optimizing inference
  performance for batch size 1 on CPUs or power-constrained edge devices.
---

# Automatic Task Parallelization of Dataflow Graphs in ML/DL models

## Quick Facts
- arXiv ID: 2308.11192
- Source URL: https://arxiv.org/abs/2308.11192
- Reference count: 26
- Key outcome: Method achieves up to 1.9x speedup over serial execution for batch size 1 ML inference on CPUs via critical-path-based linear clustering

## Executive Summary
This paper presents Ramiel, a method for automatic task parallelization of dataflow graphs in ML/DL models. The approach addresses inference optimization for batch size 1 on CPUs and power-constrained edge devices by exploiting inherent parallel paths in ML dataflow graphs. The method uses critical-path-based linear clustering augmented with cluster merging, cloning, and constant propagation/dead-code elimination to generate readable parallel Pytorch+Python code from ONNX models. Experimental results demonstrate significant performance improvements over serial execution while maintaining lightweight compilation suitable for resource-constrained environments.

## Method Summary
The method employs critical-path-based linear clustering to partition ML dataflow graphs into parallel clusters that can be executed simultaneously. The algorithm recursively finds the longest weighted path (critical path) in the graph, removes its nodes, and repeats until all nodes are assigned to clusters. This produces linear clusters that can be scheduled in parallel. The approach is enhanced with cluster merging to combine non-overlapping clusters, cloning to improve performance, and constant propagation/dead-code elimination to prune the graph before clustering. For batch sizes greater than 1, hyperclustering is applied to reduce slack. The entire pipeline generates parallel Pytorch+Python code from ONNX models through the Ramiel tool.

## Key Results
- Achieves up to 1.9x speedup over serial execution on Intel Xeon CPU for batch size 1 inference
- Outperforms some current mechanisms in both compile and runtimes while being lightweight
- Demonstrates effectiveness across multiple ML models including Squeezenet, Googlenet, Inception V3/V4, Yolo V5, BERT, Retinanet, and NASNet
- Generates readable and executable parallel Pytorch+Python code from ONNX models

## Why This Works (Mechanism)

### Mechanism 1
Critical-path-based linear clustering exploits inherent parallel paths in ML dataflow graphs to minimize makespan. The algorithm recursively finds the longest weighted path (critical path) in the graph, removes its nodes, and repeats until all nodes are assigned to clusters. This produces linear clusters that can be scheduled in parallel. Core assumption: The weighted length of the critical path approximates the optimal parallel execution time, and partitioning around it yields near-optimal scheduling.

### Mechanism 2
Cluster merging reduces the number of small, inefficient clusters by combining non-overlapping clusters. After initial linear clustering, clusters whose execution spans do not overlap are merged iteratively until no further merging is possible, reducing scheduling overhead. Core assumption: Merging non-overlapping clusters does not increase makespan and reduces the overhead of managing many small clusters.

### Mechanism 3
Constant propagation and dead-code elimination prune the graph by removing nodes that evaluate to constants or are unreachable. ONNX Runtime is used to transform the graph by evaluating constant sub-expressions and eliminating dead code, simplifying the structure before clustering. Core assumption: Removing constant and dead nodes does not affect the functional output but reduces graph complexity and cluster count.

## Foundational Learning

- Concept: Dataflow graph representation of ML models
  - Why needed here: The entire parallelization strategy operates on the graph structure of ML models; understanding nodes, edges, and tensor dependencies is essential.
  - Quick check question: What are the two types of edges typically found in a dataflow graph for ML models?

- Concept: Critical path in a weighted directed acyclic graph (DAG)
  - Why needed here: Linear clustering relies on finding the critical path to partition the graph; without this, the algorithm cannot proceed.
  - Quick check question: How is the weighted length of a path computed in this context?

- Concept: Graph pruning via constant propagation and dead-code elimination
  - Why needed here: These optimizations simplify the graph before clustering, improving cluster quality and reducing runtime.
  - Quick check question: What is the difference between constant propagation and dead-code elimination?

## Architecture Onboarding

- Component map: ONNX Model Ingestion → Model2Graph Convertor → (Optional: Constant Propagation/DCE) → Cloning Stage → Linear Clustering + Cluster Merging → Hyperclustering (if batch > 1) → ParallelCodegenerator → Runnable Pytorch+Python code
- Critical path: 1. Load ONNX model 2. Convert to internal graph 3. Apply optimizations (CP/DCE, cloning) 4. Linear clustering and merging 5. Generate parallel code
- Design tradeoffs:
  - Linear clustering is fast but may produce many small clusters; cluster merging trades a bit of scheduling flexibility for fewer clusters
  - Cloning adds redundant computation for potential speedup; must be applied sparingly to avoid exponential blow-up
  - Hyperclustering improves batch performance but adds complexity in code generation and may require switched hyperclustering for load balance
- Failure signatures:
  - Too many small clusters → high scheduling overhead, poor speedup
  - Overlapping merged clusters → increased makespan
  - Incorrect constant propagation → functional bugs in generated code
  - Hyperclustering with insufficient batch size → wasted work, no slack reduction
- First 3 experiments:
  1. Run Ramiel on a small ONNX model (e.g., Squeezenet) with batch size 1; verify sequential vs parallel runtime and inspect generated code
  2. Enable constant propagation on a model with constants (e.g., Yolo V5) and compare cluster count and runtime before/after
  3. Test hyperclustering with batch size 2 on Squeezenet; profile slack reduction and speedup vs. non-hyperclustered version

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed method compare to existing techniques when scaling to larger and more complex ML models beyond those tested in the paper? The paper focuses on a limited set of models and does not explore the performance of the method on a broader range of ML models, which could provide insights into its scalability and effectiveness.

### Open Question 2
How does the proposed method handle dynamic changes in the ML model or input data during runtime? The paper focuses on static compile-time optimizations and does not discuss how the method handles dynamic changes in the ML model or input data during runtime.

### Open Question 3
How does the proposed method compare to existing techniques in terms of energy efficiency and power consumption on resource-constrained edge devices? The paper mentions that the method is designed to be lightweight and fast enough for power and resource-constrained devices, but it does not provide a direct comparison with existing techniques in terms of energy efficiency and power consumption.

## Limitations

- Performance validation limited to Intel Xeon CPUs; unclear how approach generalizes to GPUs, ARM-based edge devices, or heterogeneous systems
- Scalability of hyperclustering unexplored, particularly regarding memory consumption and scheduling complexity at larger batch sizes
- No quantitative analysis of cloning overhead or conditions where performance gains are outweighed by additional computation and memory costs

## Confidence

**High Confidence**: The core mechanism of critical-path-based linear clustering is well-established in the literature [17] and the implementation follows standard graph algorithms. The approach of combining linear clustering with cluster merging is sound and addresses known limitations of pure linear clustering.

**Medium Confidence**: The performance claims (up to 1.9x speedup) are based on experiments with specific models and hardware. While the methodology appears sound, the generalization to other models, batch sizes, and hardware platforms requires further validation.

**Low Confidence**: The paper doesn't provide detailed analysis of edge cases, such as graphs with highly irregular structures, models with complex control flow, or scenarios where the graph pruning optimizations might remove nodes incorrectly.

## Next Checks

1. **Hardware portability test**: Run Ramiel-generated parallel code on both high-end CPUs (Intel Xeon) and power-constrained ARM devices (Raspberry Pi or similar) using the same set of ONNX models. Compare speedup ratios and resource utilization to validate hardware independence claims.

2. **Memory overhead characterization**: For models where cloning is applied, measure the additional memory consumption and compare it against the performance gains. Determine the break-even point where cloning overhead negates performance benefits.

3. **Edge case robustness**: Create synthetic ONNX graphs with known problematic structures (high fan-out, deep recursion, irregular branching) and test whether Ramiel can handle them without errors or significant performance degradation. Verify that constant propagation and dead-code elimination don't introduce functional bugs.