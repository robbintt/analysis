---
ver: rpa2
title: Can Large Language Models emulate an inductive Thematic Analysis of semi-structured
  interviews? An exploration and provocation on the limits of the approach and the
  model
arxiv_id: '2305.13014'
source_url: https://arxiv.org/abs/2305.13014
tags:
- data
- themes
- phase
- which
- some
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This exploratory paper investigates whether Large Language Models\
  \ (LLMs) can emulate aspects of an inductive Thematic Analysis (TA) on semi-structured\
  \ interview data. Using GPT 3.5-Turbo and two open datasets (gaming and teaching\
  \ interviews), the author attempted to reproduce the first five phases of Braun\
  \ & Clarke\u2019s (2006) TA framework through a series of carefully engineered prompts\
  \ and data chunking due to model limits."
---

# Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model

## Quick Facts
- arXiv ID: 2305.13014
- Source URL: https://arxiv.org/abs/2305.13014
- Reference count: 6
- LLMs can partially reproduce inductive thematic analysis patterns from qualitative interview data

## Executive Summary
This exploratory study investigates whether Large Language Models (LLMs) can emulate aspects of inductive Thematic Analysis (TA) on semi-structured interview data. Using GPT 3.5-Turbo and two open datasets (gaming and teaching interviews), the author attempts to reproduce phases 2-5 of Braun & Clarke's TA framework through prompt engineering and data chunking. Results show the model can infer a substantial proportion of themes found in original human analyses, particularly for the gaming dataset where 9 out of 13 themes matched. The study concludes that LLMs can partially support TA by identifying relevant patterns and codes, but significant methodological work is needed to ensure validity, handle hallucinations, and integrate human oversight.

## Method Summary
The study uses GPT-3.5-Turbo to emulate inductive Thematic Analysis on two open-access semi-structured interview datasets (gaming and teaching). The author employs prompt engineering to reproduce phases 2-5 of Braun & Clarke's framework, breaking data into ~2500-token chunks due to model limits. Phase 2 generates codes by prompting for themes per chunk, Phase 3 groups codes into higher-level themes, and Phase 5 renames themes with synthetic descriptions. Temperature parameter tuning tests theme consistency across runs. Results are qualitatively compared against original human-coded themes from the same datasets.

## Key Results
- GPT-3.5-Turbo inferred 9 out of 13 original themes from the gaming dataset
- Teaching dataset showed less consistent but still partially aligned themes
- Temperature variation testing revealed stable themes across runs, suggesting validity
- Data chunking strategy successfully managed token limits while maintaining thematic coherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can partially reproduce inductive Thematic Analysis by inferring codes and themes from qualitative interview data.
- Mechanism: The model processes text chunks, applies pattern recognition to identify recurring semantic structures, and groups them into higher-level themes without pre-existing codebooks.
- Core assumption: The semantic structure of interview data contains sufficient patterns for the LLM to detect and meaningfully group them.
- Evidence anchors:
  - [abstract]: "Results showed the model could infer a substantial proportion of themes found in the original human analyses, particularly for the gaming dataset where 9 out of 13 themes matched."
  - [section]: "The model can infer at least partially some of the main Themes" (Phase 2 discussion).
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.46, average citations=0.0. Top related titles: "Redefining Qualitative Analysis in the AI Era: Utilizing ChatGPT for Efficient Thematic Analysis".
- Break condition: The model fails when patterns are too subtle, context-dependent, or require deep cultural/interpretative understanding beyond surface text semantics.

### Mechanism 2
- Claim: Temperature parameter tuning enables theme validation through consistency checking across multiple runs.
- Mechanism: Running the same prompt with different temperature values produces varied outputs; recurring themes across runs indicate validity, while absent or inconsistent themes suggest model uncertainty.
- Core assumption: Themes that appear consistently across temperature variations represent stable patterns in the data rather than random model outputs.
- Evidence anchors:
  - [abstract]: "The study concludes that LLMs can partially support TA by identifying relevant patterns and codes, but significant methodological work is needed to ensure validity, handle hallucinations, and integrate human oversight."
  - [section]: "changing the temperature, we can see if different themes are produced or indeed reproduced or if there are significant differences" (Phase 4 discussion).
  - [corpus]: Weak - no direct corpus evidence for temperature-based validation methodology.
- Break condition: If all temperature variations produce wildly different themes, the model cannot identify stable patterns, indicating either data complexity or model limitations.

### Mechanism 3
- Claim: Chunking and prompt engineering mitigate token limits while maintaining thematic coherence across interview segments.
- Mechanism: Breaking interviews into smaller chunks allows processing within token limits; careful prompt engineering ensures each chunk's themes can be aggregated meaningfully across the full dataset.
- Core assumption: Thematic patterns are locally coherent within chunks and can be aggregated without losing the broader dataset context.
- Evidence anchors:
  - [abstract]: "The paper used two existing datasets of open access semi-structured interviews, previously analysed with Thematic Analysis by other researchers. It used the previously produced analysis (and the related themes) to compare with the results produced by the LLM."
  - [section]: "I simply wrote a script to divide datasets in chunks of roughly 2500 tokens" and "Due to the limits given by the maximum number of tokens... I found out it is useful to do some data preparation and data cleaning" (4.3 Data preparation).
  - [corpus]: Found 25 related papers - indicates active research community working on similar technical constraints.
- Break condition: If thematic patterns span across chunk boundaries in ways that chunking disrupts, the aggregated themes will be incomplete or misleading.

## Foundational Learning

- Concept: Thematic Analysis methodology (Braun & Clarke's 6 phases)
  - Why needed here: The LLM approach attempts to replicate these phases, so understanding the methodology is essential for evaluating what the model can and cannot do.
  - Quick check question: What are the key differences between inductive and deductive Thematic Analysis?

- Concept: Natural Language Processing and pattern recognition
  - Why needed here: LLMs use NLP techniques to identify patterns in text; understanding these techniques helps explain model behavior and limitations.
  - Quick check question: How do LLMs identify patterns differently from human analysts?

- Concept: Prompt engineering principles
  - Why needed here: The quality and structure of prompts directly impact LLM outputs; understanding prompt engineering is crucial for reproducible results.
  - Quick check question: What prompt elements most strongly influence LLM output consistency?

## Architecture Onboarding

- Component map: Data preparation -> Prompt engineering -> LLM execution -> Results aggregation -> Validation
- Critical path: Data preparation → Prompt engineering → LLM execution → Results aggregation → Validation
- Design tradeoffs:
  - Chunk size vs. thematic coherence: Smaller chunks reduce token usage but may fragment themes
  - Temperature settings vs. consistency: Higher temperatures increase creativity but reduce reproducibility
  - Prompt specificity vs. flexibility: Detailed prompts improve consistency but may miss unexpected patterns
- Failure signatures:
  - Inconsistent themes across temperature runs indicate unstable pattern recognition
  - Missing themes that were present in human analysis suggest pattern detection limitations
  - Hallucinations (inventing codes/themes) indicate prompt or model configuration issues
- First 3 experiments:
  1. Test different chunk sizes (1500, 2500, 3000 tokens) to find optimal balance between processing limits and thematic coherence
  2. Run identical prompts with temperature settings (0.0, 0.5, 1.0) to establish consistency baselines
  3. Compare LLM-generated themes against human-coded themes from the same dataset to identify matching and missing patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the effectiveness of LLMs in thematic analysis be reliably measured across different research contexts, and how might the thematic consistency vary with dataset characteristics?
- Basis in paper: [explicit] The paper discusses the need for methodological procedures to ensure quality and validity of LLM-based analysis and suggests that human-AI collaboration is likely the best approach.
- Why unresolved: The paper shows that LLMs can infer themes and codes but notes that validation is currently qualitative and observational. There is no established standard for measuring effectiveness across diverse research contexts or understanding how dataset characteristics influence thematic consistency.
- What evidence would resolve it: Comparative studies using consistent evaluation metrics across varied datasets, coupled with statistical analyses of thematic consistency, would provide concrete evidence of LLM effectiveness in thematic analysis.

### Open Question 2
- Question: How can the issue of LLM hallucination in qualitative analysis be systematically addressed to ensure methodological rigor?
- Basis in paper: [explicit] The paper identifies hallucination as a significant issue, noting that it can affect the validity of themes and codes generated by LLMs.
- Why unresolved: While the paper suggests that human oversight and collaboration can mitigate hallucinations, it does not provide a clear framework or method for systematically identifying and correcting hallucinations in qualitative analysis.
- What evidence would resolve it: Development and testing of specific protocols for detecting and correcting hallucinations, validated through user research and peer-reviewed studies, would help establish methodological rigor in LLM-based qualitative analysis.

### Open Question 3
- Question: What are the ethical implications of using LLMs for qualitative analysis, particularly concerning data privacy and the potential for bias in automated analysis?
- Basis in paper: [explicit] The paper highlights the need for ethical considerations, especially regarding the use of anonymized data and the potential for bias in LLM outputs.
- Why unresolved: The paper acknowledges these concerns but does not explore them in depth, nor does it propose solutions for ensuring ethical use of LLMs in qualitative research.
- What evidence would resolve it: Ethical guidelines and frameworks specific to LLM use in qualitative research, along with empirical studies on bias detection and mitigation, would provide clarity on ethical implications and best practices.

## Limitations
- Exact prompt templates were not fully documented, requiring reconstruction
- Datasets were referenced but not directly accessible for verification
- Token limits necessitated chunking that may have fragmented thematic patterns
- Temperature-based validation lacks robust corpus support for this specific application

## Confidence
- Medium confidence in LLM's ability to partially reproduce inductive TA patterns: Supported by matching themes (9/13 for gaming dataset) but limited by qualitative comparison methods
- Low confidence in temperature-based validation mechanism: Theoretically sound but lacks direct corpus evidence
- Medium confidence in chunking approach maintaining thematic coherence: Methodologically reasonable given token constraints

## Next Checks
1. Reconstruct and test the exact prompt templates with multiple temperature settings (0.0, 0.5, 1.0) on the same datasets to verify theme consistency patterns
2. Obtain and process the original datasets using the described chunking strategy (2500 tokens) and compare LLM-generated themes against human-coded themes using quantitative similarity metrics
3. Conduct cross-validation by running the complete pipeline on an additional open dataset of semi-structured interviews not used in the original study