---
ver: rpa2
title: A Study of Quantisation-aware Training on Time Series Transformer Models for
  Resource-constrained FPGAs
arxiv_id: '2310.02654'
source_url: https://arxiv.org/abs/2310.02654
tags:
- quantisation
- layer
- scheme
- linear
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores quantisation-aware training (QAT) for time
  series Transformer models, proposing an adaptive quantisation scheme that dynamically
  selects between symmetric and asymmetric schemes during training. The approach matches
  the quantisation scheme to the real data distribution, reducing computational overhead
  while maintaining acceptable precision.
---

# A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs

## Quick Facts
- arXiv ID: 2310.02654
- Source URL: https://arxiv.org/abs/2310.02654
- Reference count: 10
- Key outcome: Adaptive quantisation-aware training scheme dynamically selects symmetric/asymmetric quantisation per layer based on data distribution, reducing computational overhead while maintaining precision on resource-constrained FPGAs.

## Executive Summary
This study addresses the challenge of deploying time series Transformer models on resource-constrained FPGAs by proposing an adaptive quantisation-aware training (QAT) approach. The method dynamically selects between symmetric and asymmetric quantisation schemes during training based on the symmetry of data distributions, rather than using fixed schemes. Applied to air pollution forecasting with the AirU dataset, the approach achieves significant computational overhead reduction while maintaining acceptable precision, particularly when combined with mixed-precision strategies that allocate higher precision to sensitive layers.

## Method Summary
The method implements a Transformer architecture for time series forecasting with custom QLinear modules supporting 2-16 bit quantisation using symmetric (SQ), asymmetric (AQ), and adaptive (APQ) schemes. The APQ scheme evaluates data symmetry after each mini-batch and selects SQ for zero-symmetric distributions and AQ otherwise. Mixed-precision quantisation is achieved by identifying the most sensitive layer (L8) through ablation studies and applying 8-bit quantisation to it while using 4-bit for other layers. The AirU dataset with 19,380 observations is used, with MinMax normalisation and 831 test samples for evaluation.

## Key Results
- Adaptive scheme selection (APQ) reduces computational overhead by matching quantisation to data distribution symmetry
- Mixed-precision approach (4-bit for most layers, 8-bit for sensitive L8) balances model size and precision
- RMSE remains acceptable after quantisation, with the approach robust across different configurations
- The method effectively informs model quantisation and deployment decisions for resource-constrained FPGAs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matching the quantisation scheme to the data distribution reduces computational overhead while maintaining precision.
- Mechanism: The adaptive quantisation scheme evaluates the symmetry of the floating-point data distribution and selects symmetric quantisation (SQ) when data is zero-symmetric, and asymmetric quantisation (AQ) otherwise. This avoids unnecessary zero-point computations for symmetric data, reducing overhead.
- Core assumption: The data distribution within each quantisation object remains stable enough during training for the symmetry check to be meaningful.
- Evidence anchors:
  - [abstract] "Our approach demonstrates that matching the quantisation scheme to the real data distribution can reduce computational overhead while maintaining acceptable precision."
  - [section 4] "Consequently, many relevant studies [2,10] utilise the SQ scheme for layer parameters and the AQ scheme for layer feature vectors. However, this assumption may only hold for some model architectures and applications, as the data distribution of quantisation objects can vary."
- Break condition: If data distribution changes rapidly during training, the symmetry-based decision becomes outdated and may degrade performance.

### Mechanism 2
- Claim: Dynamic selection of quantisation schemes during QAT improves model precision compared to fixed schemes.
- Mechanism: The APQ scheme updates the quantisation scheme after each mini-batch based on the current data distribution, allowing the model to adapt to shifts in weight/bias/input distributions throughout training.
- Core assumption: The overhead of scheme selection and re-quantisation during training is outweighed by the precision gains from adaptive matching.
- Evidence anchors:
  - [section 5.2] "We incorporate quantisation scheme awareness into the training process... This approach allows us to update the quantisation scheme, similar to the standard QAT procedure for updating quantisation parameters."
  - [section 6.2] "Compared to the above configurations, we highlight the capability of the 'SQ+APQ' configuration by showing two models (1) aiming for higher precision and (2) aiming for lower overhead."
- Break condition: If the scheme-switching logic introduces instability or if the computational cost of re-evaluation per batch is too high.

### Mechanism 3
- Claim: Mixed-precision quantisation (4-bit for most layers, 8-bit for sensitive ones) balances model size and precision.
- Mechanism: By identifying layers that are highly sensitive to low-bit quantisation (e.g., the output layer L8), the method preserves precision where needed while aggressively compressing the rest.
- Core assumption: Sensitivity to quantisation error is layer-dependent and can be determined empirically through ablation studies.
- Evidence anchors:
  - [section 6.3] "The linear layer (L8) in the model output layer exhibits the highest sensitivity to 4-bit quantisation, contributing significantly to the degradation in RMSE when all linear layers were quantised to 4 bits."
  - [section 3] "With these settings, we can calculate the total number of parameters in the Transformer model... and the total number of parameters in all linear layers."
- Break condition: If the sensitivity analysis misidentifies critical layers, leading to precision loss in important parts of the model.

## Foundational Learning

- Concept: Quantisation-aware training (QAT)
  - Why needed here: QAT simulates the effects of quantisation during training, allowing the model to adapt its weights to the quantisation grid and reducing the accuracy drop post-quantisation.
  - Quick check question: What is the key difference between QAT and post-training quantisation (PTQ)?

- Concept: Symmetric vs asymmetric quantisation
  - Why needed here: Symmetric quantisation eliminates the zero-point term, reducing computation, but is only suitable for data symmetric around zero. Asymmetric quantisation handles general distributions but adds computational overhead.
  - Quick check question: Under what condition should symmetric quantisation be preferred over asymmetric?

- Concept: Mixed-precision quantisation
  - Why needed here: Different layers have different sensitivities to precision loss; allocating higher precision to sensitive layers while using lower precision elsewhere can achieve a better size-accuracy tradeoff.
  - Quick check question: How does mixed-precision quantisation differ from uniform quantisation in terms of deployment constraints?

## Architecture Onboarding

- Component map: Input layer (L1) → Positional Encoding → Multi-head Attention (L2-L5) → FFN (L6-L7) → Output layer (L8)
- Critical path: Input → L1 → PE → MHA (L2-L5) → FFN (L6-L7) → L8 → Output
- Design tradeoffs:
  - Precision vs. computational overhead: AQ offers better precision for asymmetric data but adds zero-point operations; SQ is faster but less precise for asymmetric data.
  - Model size vs. accuracy: Lower bit-width reduces size but increases RMSE; mixed-precision mitigates this.
  - Training complexity vs. deployment efficiency: APQ adds training overhead but yields better runtime performance.
- Failure signatures:
  - High RMSE increase after quantisation: Likely due to inappropriate scheme choice or overly aggressive bit-width reduction.
  - Model size not compressing as expected: May indicate quantisation not applied to all intended parameters or insufficient bit reduction.
  - Training instability: Could result from frequent scheme switching or poor symmetry threshold tuning.
- First 3 experiments:
  1. Apply uniform 8-bit quantisation (All-AQ vs All-SQ) to baseline model and measure RMSE and operation overhead.
  2. Implement APQ with threshold 0.1 and compare precision/ overhead against fixed schemes.
  3. Perform ablation study to identify most sensitive layer to 4-bit quantisation, then apply mixed-precision (4-bit for others, 8-bit for sensitive).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different threshold values for the APQ scheme affect model precision and computational overhead?
- Basis in paper: [explicit] The paper mentions that the threshold value in the APQ scheme can be tailored based on model architecture and application requirements, but does not explore different threshold values.
- Why unresolved: The paper only tests one threshold value (0.1) and shows two extreme cases, but does not systematically evaluate the impact of varying threshold values on model performance.
- What evidence would resolve it: A comprehensive study testing multiple threshold values and their effects on model precision and computational overhead would resolve this question.

### Open Question 2
- Question: How does the proposed adaptive quantisation scheme perform on other Transformer architectures or time series models?
- Basis in paper: [inferred] The paper only tests the adaptive quantisation scheme on a single Transformer architecture for time series forecasting, leaving open the question of its generalizability.
- Why unresolved: The study is limited to one specific model architecture, and it is unclear how the adaptive quantisation would perform on other Transformer variants or different time series models.
- What evidence would resolve it: Testing the adaptive quantisation scheme on multiple Transformer architectures and other time series models would provide evidence for its generalizability.

### Open Question 3
- Question: What is the optimal balance between precision and computational overhead for different IoT deployment scenarios?
- Basis in paper: [inferred] The paper discusses the trade-off between precision and computational overhead but does not provide guidance on how to choose the optimal balance for different deployment scenarios.
- Why unresolved: The study presents various quantisation configurations and their effects on precision and overhead, but does not offer recommendations for selecting the most appropriate configuration based on specific deployment requirements.
- What evidence would resolve it: A systematic evaluation of different quantisation configurations across various IoT deployment scenarios, considering factors such as available resources, latency requirements, and accuracy needs, would help determine the optimal balance.

## Limitations
- The approach assumes data distributions remain stable during training, which may not hold for all applications or datasets.
- Computational overhead of scheme selection and re-quantisation per mini-batch needs empirical validation to ensure net benefit.
- The sensitivity analysis for mixed-precision allocation was based on a limited ablation study that may not generalise beyond the tested configuration.

## Confidence
- **High confidence**: The fundamental approach of matching quantisation schemes to data symmetry is sound and the experimental methodology is appropriate for the stated objectives.
- **Medium confidence**: The APQ scheme's effectiveness in practice, particularly the threshold value of 0.1 for scheme selection, needs more extensive validation across different model architectures and data distributions.
- **Medium confidence**: The claim that mixed-precision quantisation achieves optimal size-accuracy tradeoffs is supported by specific ablation results but may not generalise beyond the tested configuration.

## Next Checks
1. **Distribution stability test**: Monitor the symmetry metric (S) across training epochs to verify that data distributions remain stable enough for the APQ decision to be meaningful. Plot S values over time for each layer to identify potential distribution shifts.

2. **Overhead measurement**: Instrument the training loop to measure the actual computational overhead introduced by scheme selection logic. Compare this against the claimed reduction in runtime operations to verify the net benefit.

3. **Generalisation study**: Apply the same mixed-precision strategy to a different time series dataset or Transformer variant to test whether layer L8 remains consistently the most sensitive to 4-bit quantisation, or if sensitivity patterns vary with architecture and data characteristics.