---
ver: rpa2
title: A Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous
  TTS
arxiv_id: '2303.02719'
source_url: https://arxiv.org/abs/2303.02719
tags:
- speech
- spontaneous
- ssls
- proc
- read
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the use of self-supervised speech representations
  (SSRs) as intermediate targets in two-stage TTS systems, comparing different SSR
  models and layers on both read and spontaneous speech. Using a consistent TTS architecture,
  the study finds that the 9th layer of wav2vec2.0-ASR outperforms other tested SSRs
  and mel-spectrograms in both settings.
---

# A Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous TTS

## Quick Facts
- arXiv ID: 2303.02719
- Source URL: https://arxiv.org/abs/2303.02719
- Reference count: 0
- Key outcome: The 9th layer of wav2vec2.0-ASR outperforms other tested SSLs and mel-spectrograms in both read and spontaneous TTS, with greater relative improvement for spontaneous speech.

## Executive Summary
This paper evaluates self-supervised speech representations (SSRs) as intermediate targets in two-stage TTS systems, comparing different SSR models and layers on both read and spontaneous speech. Using a consistent TTS architecture, the study finds that the 9th layer of wav2vec2.0-ASR outperforms other tested SSRs and mel-spectrograms in both settings. For spontaneous speech, SSRs show greater relative improvement over mel-spectrograms than for read speech. The results demonstrate that not all SSRs are equally effective for TTS, and that the best-performing SSR differs from the one with the highest resynthesis accuracy. This work highlights the potential of SSRs to improve TTS, especially for spontaneous speech synthesis, and suggests TTS performance as a useful metric for evaluating SSR models.

## Method Summary
The study uses FastPitch 1.1 acoustic models with added monotonic alignment and HiFi-GAN vocoders to evaluate SSRs as intermediate targets in two-stage TTS. The authors extract representations from four SSL models (wav2vec2.0-ASR, wav2vec1.0, HuBERT-base, and wav2vec2.0-w2v) at various layers from LJSpeech (read speech) and Trinity Speech-Gesture Dataset (spontaneous speech). Models are trained with L2 loss for SSRs and L1+GAN loss for mel-spectrograms, with CMOS listening tests and mel-spectrogram reconstruction error as evaluation metrics.

## Key Results
- The 9th layer of wav2vec2.0-ASR outperforms other tested SSLs and mel-spectrograms in both read and spontaneous TTS.
- For spontaneous speech, SSRs show greater relative improvement over mel-spectrograms than for read speech.
- Resynthesis error is not a good predictor of overall TTS quality afforded by using a given SSL.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 9th layer of wav2vec2.0-ASR captures more prosodic information useful for TTS than the final layer.
- Mechanism: Earlier layers in wav2vec2.0-ASR retain more fine-grained temporal and prosodic cues while later layers compress these into higher-level abstractions optimized for ASR. The 9th layer strikes a balance between preserving speech details and being compact enough for TTS modeling.
- Core assumption: Prosodic information is critical for high-quality speech synthesis and is not fully retained in the final ASR layer.
- Evidence anchors:
  - [abstract]: "The 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other tested SSLs and mel-spectrogram"
  - [section]: "Earlier layers of wav2vec2.0-ASR contain more prosodic information than later layers [11]"
  - [corpus]: Weak evidence - only one ASR-tuned model tested; assumption based on general SSL layer behavior
- Break condition: If the TTS model can extract prosody from higher layers or if the 9th layer is too fine-grained for efficient TTS modeling

### Mechanism 2
- Claim: Self-supervised speech representations (SSRs) provide more robust intermediate targets for spontaneous TTS than mel-spectrograms.
- Mechanism: SSRs capture richer acoustic and prosodic variations (breathing, disfluencies, discourse markers) present in spontaneous speech that mel-spectrograms miss, allowing the TTS model to better reconstruct these phenomena.
- Core assumption: Spontaneous speech contains acoustic patterns not well-represented in mel-spectrograms but captured by SSL models trained on diverse, untranscribed speech.
- Evidence anchors:
  - [abstract]: "For spontaneous speech, SSRs show greater relative improvement over mel-spectrograms than for read speech"
  - [section]: "Spontaneous speech has many unique verbal and nonverbal phenomena (e.g., breathing, disfluencies, discourse markers) often not represented in text"
  - [corpus]: Strong - TSGD corpus specifically designed to capture spontaneous speech with these phenomena
- Break condition: If the TTS model cannot effectively utilize the additional variability in SSRs, leading to unstable synthesis

### Mechanism 3
- Claim: SSRs outperform mel-spectrograms in TTS quality despite having higher resynthesis error, indicating TTS performance is a distinct evaluation metric from reconstruction accuracy.
- Mechanism: TTS quality depends on factors beyond acoustic fidelity, such as prosodic naturalness and speaker characteristics, which may not correlate with mel-spectrogram reconstruction error but are better captured by SSRs.
- Core assumption: The optimal representation for TTS is not the one that best reconstructs the original audio, but rather the one that enables the TTS model to generate natural-sounding speech.
- Evidence anchors:
  - [abstract]: "The resynthesis error is not a good predictor of the overall TTS quality afforded by using a given SSL"
  - [section]: "It is clear from Table 1 and 2 that resynthesis performance does not predict overall TTS performance"
  - [corpus]: Weak - only four SSL models compared; correlation with other tasks unclear
- Break condition: If future studies show that SSRs with better reconstruction accuracy consistently produce better TTS

## Foundational Learning

- Concept: Self-supervised learning (SSL) in speech
  - Why needed here: Understanding how SSL models learn speech representations without transcriptions is key to interpreting why certain layers work better for TTS
  - Quick check question: How do contrastive loss and masked prediction objectives differ in what aspects of speech they capture?

- Concept: Two-stage TTS architecture
  - Why needed here: The paper compares intermediate representations in a specific TTS framework; understanding this architecture is essential to grasp the experimental setup
  - Quick check question: What are the roles of the acoustic model and vocoder in two-stage TTS, and how does changing the intermediate representation affect each?

- Concept: Prosodic features in speech
  - Why needed here: The paper emphasizes prosodic information as a key differentiator between layers and representations; understanding what constitutes prosody is crucial
  - Quick check question: What acoustic correlates define prosody (e.g., pitch, duration, energy patterns), and why are they important for natural-sounding speech synthesis?

## Architecture Onboarding

- Component map: Input text → FastPitch acoustic model → SSR intermediate representation → HiFi-GAN vocoder → Output waveform. The SSR extraction and vocoder training are tied to specific representations.
- Critical path: Text encoding → acoustic feature prediction → SSR generation → waveform synthesis. The SSR layer selection directly impacts the acoustic model's output quality.
- Design tradeoffs: Using SSRs vs mel-spectrograms involves balancing representation richness against model complexity and training stability. Higher-dimensional SSRs may capture more information but require more computational resources.
- Failure signatures: Poor intelligibility suggests the SSR lacks sufficient phonetic information; robotic or unnatural prosody indicates missing prosodic cues; unstable synthesis may result from excessive variability in the SSR.
- First 3 experiments:
  1. Train the baseline FastPitch + HiFi-GAN system with mel-spectrograms on LJSpeech to establish a performance baseline
  2. Extract the 9th layer of wav2vec2.0-ASR from speech and train a vocoder to assess the resynthesis quality of this representation
  3. Replace mel-spectrograms with the 9th layer of wav2vec2.0-ASR in the two-stage TTS pipeline and evaluate subjective quality improvements over the baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific self-supervised learning (SSL) model and layer is optimal for text-to-speech (TTS) synthesis across different speech types?
- Basis in paper: [explicit] The paper tests various SSL models and layers, finding that the 9th layer of a 12-layer wav2vec2.0-ASR model outperforms others in both read and spontaneous TTS, but notes that resynthesis accuracy does not predict TTS quality.
- Why unresolved: The study suggests that resynthesis error is not a good predictor of TTS quality, indicating that other factors may influence the optimal choice of SSL model and layer for TTS.
- What evidence would resolve it: Comparative TTS performance evaluations across a wider range of SSL models and layers, including weighted or stacked layers, could help identify the optimal configuration for different speech types.

### Open Question 2
- Question: How does the performance of SSL-based TTS systems compare between read and spontaneous speech?
- Basis in paper: [explicit] The study finds that SSL features outperform mel-spectrograms more in spontaneous TTS than in read-speech TTS, suggesting SSLs are especially suitable for spontaneous speech synthesis.
- Why unresolved: While the paper shows a preference for SSLs in spontaneous TTS, it does not explore the underlying reasons for this difference or how it varies across different SSL models and layers.
- What evidence would resolve it: Detailed analyses of the characteristics of read and spontaneous speech that affect SSL-based TTS performance, along with comparative studies of different SSL configurations, could clarify these differences.

### Open Question 3
- Question: What factors beyond acoustic information influence the effectiveness of SSLs in TTS?
- Basis in paper: [explicit] The paper notes that resynthesis performance does not predict TTS quality and that the ranking of SSLs in TTS does not align with their resynthesis error, suggesting other factors are at play.
- Why unresolved: The study identifies a discrepancy between resynthesis and TTS performance but does not investigate which specific aspects of SSLs contribute to their effectiveness in TTS beyond acoustic information.
- What evidence would resolve it: Investigating the prosodic, linguistic, and contextual features captured by different SSL models and layers, and their impact on TTS quality, could identify the key factors influencing SSL effectiveness.

## Limitations

- The study focuses on only four SSL models and a single layer from wav2vec2.0-ASR, leaving open the possibility that other configurations might perform better.
- Evaluation relies on CMOS scores from crowdsourced listeners, which may not capture all aspects of speech quality and could be influenced by listener bias.
- The study uses only English datasets, limiting generalizability to other languages or multilingual settings.
- Resynthesis error may not fully capture the perceptual quality of representations for TTS.

## Confidence

- High Confidence: SSRs generally outperform mel-spectrograms for spontaneous speech TTS; the 9th layer of wav2vec2.0-ASR is optimal for both read and spontaneous speech.
- Medium Confidence: SSRs capture prosodic information more effectively than mel-spectrograms; resynthesis error does not predict TTS quality.
- Low Confidence: Generalization of findings to other SSL models or languages; specific mechanisms by which SSRs improve TTS.

## Next Checks

1. **Layer-by-Layer Validation**: Conduct a finer-grained evaluation of all 12 layers of wav2vec2.0-ASR and other SSL models to identify the optimal layer for TTS and confirm whether the 9th layer consistently performs best across different speech types and languages.

2. **Correlation Analysis**: Perform correlation analysis between resynthesis error, ASR performance, and subjective TTS quality metrics to determine if any objective measure reliably predicts TTS performance across different SSL models.

3. **Cross-Lingual Testing**: Evaluate the same SSR models and layers on non-English spontaneous speech datasets (e.g., Mandarin or Spanish) to assess the generalizability of the findings and identify any language-specific effects.