---
ver: rpa2
title: Extrapolating Large Language Models to Non-English by Aligning Languages
arxiv_id: '2308.04948'
source_url: https://arxiv.org/abs/2308.04948
tags:
- data
- translation
- non-english
- cross-lingual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to elicit large language models' non-English
  ability by building semantic alignment between English and non-English languages.
  Specifically, the authors perform cross-lingual instruction-tuning on pre-trained
  LLaMA with mixed translation task instruction data and cross-lingual general task
  instruction data.
---

# Extrapolating Large Language Models to Non-English by Aligning Languages

## Quick Facts
- arXiv ID: 2308.04948
- Source URL: https://arxiv.org/abs/2308.04948
- Authors: 
- Reference count: 7
- Primary result: x-LLaMA improves non-English performance through cross-lingual instruction-tuning

## Executive Summary
This paper addresses the challenge of extending large language models (LLMs) to non-English languages by building semantic alignment across languages through cross-lingual instruction-tuning. The authors propose x-LLaMA, a model obtained by instruction-tuning pre-trained LLaMA-7B with mixed cross-lingual general task instruction data and translation task instruction data. The approach demonstrates significant improvements on non-English question answering tasks (XQUAD, MLQA) and achieves competitive translation performance compared to multilingual translation systems.

## Method Summary
The method involves instruction-tuning pre-trained LLaMA-7B with a mixture of cross-lingual general task instruction data (translated ALPACA dataset) and translation task instruction data (parallel text from WIKI MATRIX and NEWS COMMENTARY paired with translation instructions). The model is trained for 3 epochs using learning rate 2e-5, batch size 128, and FSDP training strategy on 8×Tesla A100. The resulting x-LLaMA models are evaluated on non-English QA tasks (XQUAD, MLQA, C-EVAL) and translation tasks (FLORES-101).

## Key Results
- x-LLaMA achieves significant improvements on XQUAD, MLQA, and C-EVAL non-English QA datasets
- Translation performance follows a power-law relationship with translation task data scale
- x-LLaMA outperforms previous LLaMA-based models and surpasses M2M in half of evaluated translation directions
- Cross-lingual instruction-tuning is more efficient than continued pre-training for learning semantic alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic alignment between English and non-English languages improves non-English task performance
- Mechanism: Cross-lingual instruction-tuning with translation tasks creates stronger correspondence between language representations
- Core assumption: Semantic alignment in middle layers is sufficient for downstream task performance
- Evidence anchors: [abstract], [section 3.1], [corpus]
- Break condition: If alignment doesn't generalize from translation tasks to other non-English tasks

### Mechanism 2
- Claim: Scaling up translation task data improves bilingual translation performance following a power-law relationship
- Mechanism: More translation data strengthens semantic alignment, following Y = 100 - α · βlog10(γ·X)
- Core assumption: Relationship between data scale and performance is monotonic and follows proposed formulation
- Evidence anchors: [section 3.2], [section 5.2], [corpus]
- Break condition: If additional data beyond certain point doesn't improve performance or relationship breaks down

### Mechanism 3
- Claim: Putting non-English text on target side of translation data is more effective than source side
- Mechanism: Model learns to generate non-English text more effectively when trained to produce it as output
- Core assumption: Generation capabilities are more important than comprehension for improving non-English task performance
- Evidence anchors: [section 3.1], [section 6], [corpus]
- Break condition: If comprehension is more critical than generation for certain tasks

## Foundational Learning

- Concept: Cross-lingual semantic alignment
  - Why needed here: Core mechanism for improving non-English performance through instruction-tuning
  - Quick check question: What is the difference between cross-lingual alignment and monolingual pre-training in terms of semantic concept representation?

- Concept: Scaling laws in machine learning
  - Why needed here: Understanding power-law relationship between data scale and performance for resource optimization
  - Quick check question: How does the scaling law formulation Y = 100 - α · βlog10(γ·X) differ from typical linear scaling laws?

- Concept: Instruction-tuning vs. continued pre-training
  - Why needed here: Contrasting approaches to improving non-English ability with different resource requirements
  - Quick check question: What are key differences between instruction-tuning and continued pre-training in objectives and use cases?

## Architecture Onboarding

- Component map: Pre-trained LLaMA-7B -> Cross-lingual instruction data (ALPACA + translations) -> Translation task instruction data (WIKI MATRIX, NEWS COMMENTARY) -> x-LLaMA model -> Evaluation datasets (XQUAD, MLQA, C-EVAL, FLORES-101) -> Training infrastructure (8×Tesla A100, FSDP)

- Critical path: 1) Prepare cross-lingual instruction data, 2) Instruction-tune LLaMA-7B, 3) Evaluate on non-English QA and translation tasks, 4) Analyze scaling behavior and alignment strength

- Design tradeoffs: Open-source translation data vs. expert-annotated data, middle layer alignment vs. full model alignment, En→X translation vs. X→En or both directions

- Failure signatures: Performance plateaus despite increasing data, non-English task improvement without translation improvement, strong middle-layer alignment without downstream task generalization

- First 3 experiments: 1) Ablation study comparing different instruction data combinations, 2) Scaling study varying translation task data amounts, 3) Comparison between cross-lingual instruction-tuning and continued pre-training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the scaling law formulation for cross-lingual instruction-tuning be further optimized?
- Basis in paper: [inferred] Paper discusses scaling law but suggests exploring ways to lift it
- Why unresolved: Identifies potential for improvement but lacks specific optimization methods
- What evidence would resolve it: Experimental results showing improved performance with optimized scaling law

### Open Question 2
- Question: What are the challenges in universally boosting LLM's non-English performance on tasks of all subjects?
- Basis in paper: [explicit] Notes difficulty in universally boosting non-English performance across all subjects
- Why unresolved: Acknowledges difficulty without delving into specific challenges or solutions
- What evidence would resolve it: Analysis identifying specific challenges and proposing solutions for diverse subjects

### Open Question 3
- Question: How does efficiency of cross-lingual instruction-tuning compare to continued pre-training?
- Basis in paper: [explicit] Suggests cross-lingual instruction-tuning is more efficient but lacks detailed comparison
- Why unresolved: Suggests efficiency without detailed resource utilization and performance gain comparison
- What evidence would resolve it: Comparative analysis quantifying resource efficiency and performance gains

## Limitations

- Translation task data relies on publicly available parallel corpora rather than expert-annotated data
- Power-law scaling relationship based on limited data points may not generalize to other language pairs
- Efficiency claim for translation task data lacks comprehensive ablation studies across multiple scales
- Comparison with M2M baselines incomplete due to fundamental architectural differences

## Confidence

- High Confidence: Basic observation that cross-lingual instruction-tuning improves non-English performance compared to monolingual fine-tuning
- Medium Confidence: Power-law relationship between translation data scale and performance
- Low Confidence: Claim that translation task data is more efficient than monolingual data for semantic alignment

## Next Checks

1. Conduct experiments with 10 different data scales (1K to 10M instances) for multiple language pairs to verify power-law relationship and determine optimal data scale

2. Perform detailed error analysis on cross-lingual semantic alignment examining examples where cosine similarity is high but semantic equivalence is questionable

3. Compare x-LLaMA performance with fine-tuned M2M-100-7B on same non-English tasks to isolate effect of instruction-tuning vs multilingual pre-training