---
ver: rpa2
title: 'Reranking for Natural Language Generation from Logical Forms: A Study based
  on Large Language Models'
arxiv_id: '2309.12294'
source_url: https://arxiv.org/abs/2309.12294
tags:
- candidates
- film
- language
- natural
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating natural language
  text from logical forms using large language models, where generated outputs often
  suffer from inconsistencies, hallucinations, or missing semantics. The authors propose
  a generate-and-rerank approach that first generates multiple candidate outputs using
  an LLM (Codex) and then re-ranks them using a fine-tuned CodeBERT model.
---

# Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models

## Quick Facts
- arXiv ID: 2309.12294
- Source URL: https://arxiv.org/abs/2309.12294
- Authors: 
- Reference count: 17
- Key outcome: Reranking with a fine-tuned CodeBERT model significantly improves semantic consistency and fluency of LLM-generated text from logical forms, outperforming generator-only baselines across three datasets.

## Executive Summary
This paper addresses the challenge of generating natural language text from logical forms using large language models, where outputs often suffer from hallucinations and semantic inconsistencies. The authors propose a generate-and-rerank approach that first produces multiple candidate outputs using Codex, then re-ranks them using a fine-tuned CodeBERT model trained on metrics like BLEURT, PRISM, and parser probability. Experiments on GeoQuery, Jobs, and CFQ datasets demonstrate significant improvements over baselines, with the reranker achieving higher semantic consistency and fluency scores. The approach is particularly effective under fixed generation budgets and when using instruction-following LLMs like ChatGPT, though Codex remains superior.

## Method Summary
The approach generates multiple candidate outputs from logical forms using an LLM (Codex), then re-ranks them using a fine-tuned CodeBERT model. The reranker is trained using margin ranking loss on metric scores (BLEURT, PRISM, parser probability) computed from pairs of candidates for the same logical form. During inference, the candidate with the highest combined score is selected. The method was evaluated on three datasets (GeoQuery, Jobs, CFQ-MCD1) and compared against random selection, self-consistency, and generator-only baselines using automatic metrics and manual evaluation.

## Key Results
- Reranker achieves BLEURT scores of 81.0 vs 77.8 for generator baseline on GeoQuery
- Parser probability improves from 81.8% to 90.9% on GeoQuery with reranker
- Reranker significantly outperforms baselines on compositional generalization datasets (Jobs query split, CFQ-MCD1)
- Fixed generation budget experiments show variable candidate counts per LF outperform fixed counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reranking improves semantic consistency by filtering out candidates with missing or hallucinated information.
- Mechanism: The reranker scores candidates based on metrics that capture semantic alignment (parser probability) and fluency (BLEURT, PRISM), allowing it to prioritize outputs faithful to the logical form.
- Core assumption: High parser probability indicates semantic fidelity, and BLEURT/PRISM correlate with human judgments of fluency and correctness.
- Evidence anchors:
  - [abstract] "Our approach involves initially generating a set of candidate outputs by prompting an LLM and subsequently reranking them using a task-specific reranker model."
  - [section] "We conduct an in-depth analysis of various pre-trained metrics...identify and select the metrics that effectively produce rankings of natural language candidates, prioritizing fluency and semantic fidelity."
  - [corpus] Weak: No explicit semantic consistency claim in cited papers; inferred from general reranking literature.

### Mechanism 2
- Claim: Using diverse training candidates improves the reranker's ability to generalize across candidate sets.
- Mechanism: Training with larger candidate sets exposes the reranker to more variation in candidate quality and diversity, helping it learn to discriminate better even in test sets with higher diversity.
- Core assumption: Exposure to diverse candidate sets during training enables the reranker to handle varied test distributions.
- Evidence anchors:
  - [section] "While scores for all metrics improve as the train-time n-best list grows, the most significant gains are observed in parser probability."
  - [section] "Increasing the candidate set size also increases the diversity of candidate sets."
  - [corpus] Weak: No explicit candidate diversity evidence in corpus; based on general ML generalization principles.

### Mechanism 3
- Claim: Combining multiple complementary metrics yields stronger reranking than any single metric.
- Mechanism: BLEURT (semantic similarity), PRISM (paraphrasing similarity), and parser probability (semantic consistency) capture different aspects of quality; their combination provides a more holistic score.
- Core assumption: Metrics from different families (encoder-only, encoder-decoder, parser) are complementary and reduce blind spots.
- Evidence anchors:
  - [section] "Our findings support the conclusion by Freitag et al. (2022) that trained metrics outperform BLEU, and the suggestion by Amrhein et al. (2022) and Moghe et al. (2022) that a combination of different families of metrics is likely to be stronger than any one metric alone."
  - [section] "Each of the three metrics in our best-performing combination work in very different ways...We suspect that the differences between these models contribute to their strength in combination."
  - [corpus] Moderate: Cites prior work but no direct ablation on metric combinations in corpus.

## Foundational Learning

- Concept: Generate-and-rerank framework
  - Why needed here: Separates the difficult task of generation from the selection task, allowing a specialized model to optimize for quality.
  - Quick check question: What are the two main stages of the approach, and why is separation beneficial?

- Concept: Margin ranking loss for pairwise comparison
  - Why needed here: Enables the reranker to learn relative quality judgments without requiring absolute scores, which are harder to obtain.
  - Quick check question: How does the loss function encourage the model to rank better candidates higher than worse ones?

- Concept: Automatic metric alignment with human judgment
  - Why needed here: Without reliable automatic metrics, the reranker cannot be trained or evaluated effectively.
  - Quick check question: Why did the authors manually curate an evaluation set, and what did they measure?

## Architecture Onboarding

- Component map: Generator (Codex) → produces n candidates per LF → Reranker (CodeBERT + head) → scores candidates using combined metric scores → Evaluation pipeline (BLEURT, PRISM, parser probability)

- Critical path:
  1. Prompt Codex with exemplars + LF
  2. Collect n unique candidates
  3. Encode each (LF, candidate) pair with CodeBERT
  4. Apply regression head to produce score
  5. Select candidate with highest combined score

- Design tradeoffs:
  - Large generator (Codex 175B) vs. computational cost
  - Fixed candidate count vs. variable to maximize LFs covered under budget
  - Manual metric evaluation vs. scalability

- Failure signatures:
  - Reranker overfits to training metric distribution
  - Generator produces low-diversity candidates
  - Combined metric poorly aligned with human judgment

- First 3 experiments:
  1. Train reranker with n=8 candidates, evaluate on test set, compare to generator baseline
  2. Vary n at train and test time, observe impact on parser probability vs. BLEURT/PRISM
  3. Use fixed generation budget, compare fixed vs. variable candidate counts per LF

## Open Questions the Paper Calls Out

- Question: What is the impact of using smaller language models for candidate generation in terms of overall reranker performance and computational efficiency?
  - Basis in paper: Inferred from the limitations section, which mentions that Codex requires significant computational resources due to its size and suggests that smaller models could generate candidates more efficiently, though likely at lower quality.
  - Why unresolved: The paper only uses Codex as the generator and does not experiment with smaller models to quantify the trade-off between generation quality and computational cost.
  - What evidence would resolve it: Comparative experiments using smaller language models (e.g., GPT-2, GPT-Neo) for candidate generation, measuring both reranker performance and computational efficiency metrics like generation time and cost.

- Question: How well does the generate-and-rerank approach generalize to languages other than English, and what modifications would be necessary for effective cross-lingual application?
  - Basis in paper: Inferred from the limitations section, which states that the approach likely does not transfer well to other languages without modification due to Codex's English-centric pre-training data.
  - Why unresolved: The paper only evaluates on English datasets and does not explore multilingual applications or the necessary adaptations for non-English languages.
  - What evidence would resolve it: Experiments applying the approach to non-English datasets, testing different generator models with multilingual pre-training, and evaluating the effectiveness of various cross-lingual adaptation strategies.

- Question: What specific techniques from semantic parsing research could be applied to improve the compositional generalization abilities of the reranker model?
  - Basis in paper: Explicit from the limitations section, which suggests that the reranker may suffer from lack of compositional generalization and mentions potential improvements using synthetic data or supervised attention techniques from semantic parsing.
  - Why unresolved: The paper identifies this as a limitation but does not implement or test these suggested techniques to verify their effectiveness for the reranker model.
  - What evidence would resolve it: Experiments applying compositional generalization techniques such as data augmentation with synthetic examples or supervised attention mechanisms to the reranker, measuring improvements in performance on out-of-distribution test sets.

## Limitations

- High computational costs for Codex generation, limiting scalability
- Potential generalization issues for the reranker across domains and compositional structures
- Manual evaluation process doesn't scale to larger datasets or diverse linguistic phenomena

## Confidence

**High confidence:** The core finding that reranking improves output quality compared to generator-only baselines is well-supported by consistent improvements across all three datasets and multiple evaluation metrics. The generate-and-rerank framework itself is established and well-validated.

**Medium confidence:** The specific metric combination (BLEURT + PRISM + parser probability) appears optimal based on ablation studies, but the analysis is limited to the tested datasets. Different domains might benefit from different metric combinations, and the paper doesn't explore sensitivity to this choice.

**Low confidence:** Claims about the reranker's performance under fixed generation budgets are based on limited experiments. The paper suggests variable candidate counts are better than fixed counts, but the experimental evidence is sparse and doesn't account for different budget levels.

## Next Checks

1. **Cross-domain validation:** Test the reranker on at least two additional domains (e.g., database queries for medical data, or robotic command generation) to assess generalization beyond the three datasets studied. Compare performance to domain-specific reranking approaches.

2. **Metric robustness analysis:** Conduct controlled experiments where BLEURT, PRISM, and parser probability are systematically degraded (e.g., by training on mismatched domains) to quantify how sensitive the reranker is to metric quality. Include human evaluation on a subset to establish ground truth.

3. **Budget scaling study:** Perform systematic experiments varying the total generation budget (number of candidates × number of logical forms) across multiple orders of magnitude. Measure how performance scales and identify the point of diminishing returns for different candidate allocation strategies.