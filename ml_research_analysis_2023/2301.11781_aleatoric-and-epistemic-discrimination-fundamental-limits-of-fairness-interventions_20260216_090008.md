---
ver: rpa2
title: 'Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions'
arxiv_id: '2301.11781'
source_url: https://arxiv.org/abs/2301.11781
tags:
- fairness
- classi
- data
- discrimination
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new framework for quantifying discrimination
  in machine learning classification by separating it into two components: aleatoric
  discrimination (inherent bias in the data distribution) and epistemic discrimination
  (due to algorithmic choices during model development). The authors define the fairness
  Pareto frontier as the optimal accuracy achievable under fairness constraints, assuming
  perfect knowledge of the data distribution.'
---

# Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions

## Quick Facts
- arXiv ID: 2301.11781
- Source URL: https://arxiv.org/abs/2301.11781
- Reference count: 26
- Primary result: Introduces framework separating discrimination into aleatoric (data bias) and epistemic (algorithmic) components, showing state-of-the-art fairness interventions effectively reduce epistemic discrimination but are limited by data quality issues like disparate missing patterns

## Executive Summary
This paper develops a novel framework for understanding and quantifying discrimination in machine learning classification by separating it into two fundamental components: aleatoric discrimination (inherent bias in the data distribution) and epistemic discrimination (due to algorithmic choices during model development). The authors define the fairness Pareto frontier as the optimal accuracy achievable under fairness constraints, assuming perfect knowledge of the data distribution. They demonstrate how to compute this theoretical limit using Blackwell's conditions for comparing statistical experiments, then measure epistemic discrimination as the gap between a classifier's accuracy and this limit. Experiments on standard datasets show that existing fairness interventions effectively reduce epistemic discrimination, approaching the information-theoretic limit. However, when data contains missing values with disparate patterns across groups, aleatoric discrimination increases significantly, reducing the effectiveness of these interventions and highlighting the importance of addressing data quality issues.

## Method Summary
The authors introduce a framework to quantify and separate discrimination sources in ML classification. They characterize the fairness Pareto frontier using Blackwell's conditions, iteratively refining achievable conditional distributions through piecewise linear approximations. Epistemic discrimination is measured as the gap between actual classifier performance and this theoretical limit. The method involves computing the frontier via convex optimization with functional constraints, applying fairness interventions to datasets, and comparing intervention performance against the computed frontier. The approach is validated on UCI Adult, COMPAS, and German Credit datasets, with experiments examining both complete data and scenarios with disparate missing value patterns across groups.

## Key Results
- State-of-the-art fairness interventions effectively reduce epistemic discrimination, approaching the information-theoretic limit defined by the fairness Pareto frontier
- Disparate missing patterns across groups significantly increase aleatoric discrimination, sharply degrading the effectiveness of fairness interventions
- The fairness Pareto frontier can be approximated using a greedy improvement algorithm based on Blackwell's conditions with piecewise linear functions
- When data has missing values, there remains significant room for improvement in handling aleatoric discrimination through better data quality management

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fairness Pareto frontier is precisely characterized by Blackwell's conditions through iterative refinement of achievable conditional distributions P(Ŷ|S,Y).
- Mechanism: Blackwell's conditions establish when one statistical experiment is more informative than another. By iteratively tightening the feasible region of conditional distributions through piecewise linear approximations, the algorithm converges to the theoretical limit of achievable accuracy under fairness constraints.
- Core assumption: The conditional distribution P(Ŷ|S,Y) can be approximated arbitrarily well using piecewise linear functions of the form max over linear constraints.
- Evidence anchors: [abstract] "We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments."

### Mechanism 2
- Claim: Epistemic discrimination is quantified as the gap between actual classifier performance and the fairness Pareto frontier.
- Mechanism: By comparing the accuracy of deployed fairness interventions to the theoretical optimal given by FairFront, we can measure how much improvement is still possible through algorithmic changes.
- Core assumption: The fairness Pareto frontier represents the true information-theoretic limit achievable by any classifier given the data distribution and fairness constraints.
- Evidence anchors: [abstract] "We then quantify epistemic discrimination as the gap between a classifier's accuracy given fairness constraints and the limit posed by aleatoric discrimination."

### Mechanism 3
- Claim: Disparate missing patterns across groups significantly increase aleatoric discrimination by degrading the achievable fairness Pareto frontier.
- Mechanism: When different population groups have different missing value patterns, the effective information available for prediction becomes group-dependent, making it impossible to achieve both high accuracy and fairness simultaneously.
- Core assumption: Mode imputation is used to handle missing values, and the resulting imputed distributions accurately reflect the true underlying distributions.
- Evidence anchors: [abstract] "However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination."

## Foundational Learning

- Concept: Blackwell's conditions for comparing statistical experiments
  - Why needed here: They provide the mathematical foundation for characterizing the set of achievable conditional distributions P(Ŷ|S,Y) that respect fairness constraints.
  - Quick check question: What are the three equivalent conditions in Blackwell's theorem, and how do they relate to comparing information content?

- Concept: Convex optimization with functional constraints
  - Why needed here: The fairness Pareto frontier is characterized through a sequence of convex programs that approximate the boundary of the feasible region.
  - Quick check question: How does the algorithm handle the infinite number of constraints in the characterization of the feasible region?

- Concept: Information-theoretic limits and mutual information
  - Why needed here: The fairness Pareto frontier represents the maximum achievable accuracy under fairness constraints, which can be understood as an information-theoretic limit.
  - Quick check question: How does the data processing inequality relate to the relationship between the original distribution and the achievable conditional distribution?

## Architecture Onboarding

- Component map: FairFront computation -> Benchmark integration -> Missing value analysis -> Evaluation pipeline
- Critical path: 1. Compute FairFront using Algorithm 1 with piecewise linear approximation 2. Apply fairness interventions to datasets 3. Compare intervention performance to FairFront 4. Generate missing values with disparate patterns 5. Repeat steps 1-3 on datasets with missing values
- Design tradeoffs: Approximation accuracy vs. computational complexity in Algorithm 1; generality of the framework vs. specificity to particular fairness metrics; use of mode imputation vs. more sophisticated missing value handling methods
- Failure signatures: FairFront approximation too loose, leading to overestimation of achievable performance; fairness interventions perform significantly worse than predicted by the gap to FairFront; disparate missing patterns not captured accurately by the mode imputation
- First 3 experiments: 1. Reproduce FairFront computation on Adult dataset with varying fairness thresholds 2. Apply Reduction method and compare its performance curve to FairFront 3. Introduce missing values with disparate patterns and measure the degradation of FairFront and intervention performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed algorithm scale with increasing number of subgroups (A) and classes (C) in the classification task?
- Basis in paper: [explicit] The paper mentions that the algorithm's complexity depends on k (number of linear pieces) and T (number of iterations), but does not analyze scaling with respect to A and C
- Why unresolved: The paper only demonstrates results on binary classification with binary groups (A=2, C=2), but does not provide analysis for larger, more realistic settings
- What evidence would resolve it: Empirical results showing algorithm performance on datasets with varying numbers of subgroups and classes, or theoretical analysis of computational complexity as a function of A and C

### Open Question 2
- Question: What is the optimal trade-off between k (number of linear pieces) and T (number of iterations) for balancing approximation accuracy and computational efficiency?
- Basis in paper: [explicit] The paper uses k=6 and T=20 in experiments but does not provide systematic analysis of how these parameters affect the quality of the fairness Pareto frontier approximation
- Why unresolved: The choice of parameters appears somewhat arbitrary in the experiments, and the paper does not explore how different parameter settings impact the tightness of the approximation or computational requirements
- What evidence would resolve it: A systematic study varying k and T across multiple datasets, measuring both approximation error and computation time to identify optimal parameter settings

### Open Question 3
- Question: How robust is the fairness Pareto frontier to different types of missing data patterns beyond the simple probability-based erasure model used in experiments?
- Basis in paper: [inferred] The paper demonstrates that disparate missing patterns increase aleatoric discrimination but only tests a limited scenario with uniform missingness probability differences between groups
- Why unresolved: Real-world missing data patterns can be much more complex (e.g., feature-specific, dependent on observed values, or following MAR/MNAR mechanisms), which could affect the fairness Pareto frontier differently
- What evidence would resolve it: Experiments testing various realistic missing data mechanisms and patterns to determine which types of missingness most severely impact the fairness-accuracy trade-off

## Limitations
- Computational complexity grows exponentially with the number of subgroups, limiting practical applicability to datasets with many protected attributes
- Assumes access to the true data distribution, which is rarely available in practice
- Mode imputation for missing values may not capture true underlying patterns, potentially overestimating the impact of disparate missingness
- Evaluation focuses on tabular datasets and may not generalize to other data modalities

## Confidence
- High confidence: The theoretical framework connecting Blackwell's conditions to fairness constraints is mathematically sound and well-established in information theory
- Medium confidence: The approximation algorithm for computing the fairness Pareto frontier provides reasonable estimates in practice, but tightness depends on iteration parameters
- Low confidence: The impact of disparate missing value patterns on aleatoric discrimination is demonstrated but may be dataset-specific

## Next Checks
1. Implement the DC program solver for constraint verification and test convergence properties across different datasets and fairness thresholds
2. Compare the fairness Pareto frontier approximation accuracy using different numbers of linear pieces and maximum iterations to establish the trade-off between computational cost and approximation quality
3. Evaluate the impact of different missing value imputation strategies (mean imputation, multiple imputation, model-based imputation) on aleatoric discrimination to determine if the mode imputation results are robust