---
ver: rpa2
title: Interpretability Illusions in the Generalization of Simplified Models
arxiv_id: '2312.03656'
source_url: https://arxiv.org/abs/2312.03656
tags:
- attention
- depth
- generalization
- original
- simplified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated how simplified model representations affect
  interpretability of deep learning models, focusing on Transformers trained on controlled
  datasets with systematic generalization splits. The core method involved simplifying
  models using techniques like singular value decomposition (SVD) and clustering,
  then evaluating how well these simplified proxies matched the original model's behavior
  on both in-distribution and out-of-distribution test sets.
---

# Interpretability Illusions in the Generalization of Simplified Models

## Quick Facts
- arXiv ID: 2312.03656
- Source URL: https://arxiv.org/abs/2312.03656
- Reference count: 33
- Key outcome: Simplified proxy models accurately approximate original models on in-distribution data but fail to capture behavior on systematic generalization splits, leading to interpretability illusions.

## Executive Summary
This study investigates how simplified model representations affect interpretability of deep learning models, focusing on Transformers trained on controlled datasets with systematic generalization splits. The research reveals that while simplified proxies can accurately approximate original models on in-distribution evaluations, they consistently fail to capture behavior on various tests of systematic generalization. These findings raise important questions about the reliability of mechanistic interpretations derived from simplified representations and highlight the importance of evaluating model interpretations out of distribution.

## Method Summary
The study used dimensionality reduction techniques like SVD and clustering to simplify model representations, then evaluated how well these simplified proxies matched the original model's behavior on both in-distribution and out-of-distribution test sets. The approach involved training two-layer Transformers on controlled datasets including Dyck balanced-parenthesis languages and code completion tasks, then applying various simplification methods to the model's key and query embeddings. The approximation quality was measured using Jensen-Shannon Divergence and same prediction metrics across different types of distribution shifts.

## Key Results
- Simplified proxies showed high faithfulness to original models on in-distribution evaluations but exhibited significant generalization gaps on systematic generalization splits
- The magnitude of generalization gaps varied across tasks and even between different prediction types within the same task
- In some cases, simplified models actually outperformed the original models on certain generalization tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simplified proxy models can accurately approximate original models on in-distribution data but fail to capture behavior on systematic generalization splits.
- Mechanism: Dimensionality reduction and clustering methods preserve the most salient features for in-distribution tasks but discard low-variance components that encode systematic generalization capabilities.
- Core assumption: Low-variance components in representation space are crucial for systematic generalization but not for in-distribution accuracy.
- Evidence anchors: [abstract] "simplified proxies were more faithful to the original models on in-distribution evaluations but less faithful on systematic generalization splits"
- Break condition: If systematic generalization relies primarily on high-variance components, this mechanism fails.

### Mechanism 2
- Claim: Data-dependent simplifications (like SVD on training data) create models that are optimized for the training distribution but not for out-of-distribution generalization.
- Mechanism: SVD and clustering methods calculate principal components or clusters based on training data statistics, which may not capture the model's out-of-distribution behavior patterns.
- Core assumption: The training data distribution is not representative of the model's true generalization capabilities.
- Evidence anchors: [abstract] "even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model's behavior out of distribution"
- Break condition: If model behavior out-of-distribution is predictable from in-distribution statistics, this mechanism fails.

### Mechanism 3
- Claim: The relationship between model complexity and generalization is non-monotonic, allowing simplified models to sometimes outperform original models on certain generalization tasks.
- Mechanism: Data-independent simplifications (like one-hot attention) can remove noise or overfitting patterns present in the original model, leading to better generalization on specific tasks.
- Core assumption: Original models contain unnecessary complexity or overfitting that hinders generalization on certain distribution shifts.
- Evidence anchors: [abstract] "in some cases, simplifications even led to better generalization than the original model"
- Break condition: If original models are already optimally regularized, this mechanism fails.

## Foundational Learning

- Concept: Systematic generalization
  - Why needed here: Understanding how models handle unseen structures and depths is central to the paper's findings about interpretability illusions.
  - Quick check question: What distinguishes systematic generalization from standard generalization in neural networks?

- Concept: Dimensionality reduction (SVD/PCA)
  - Why needed here: These are the primary tools used for model simplification and their limitations drive the main findings.
  - Quick check question: How does SVD identify which dimensions to preserve versus discard?

- Concept: Attention mechanisms in Transformers
  - Why needed here: The paper focuses on interpreting individual attention heads through simplification techniques.
  - Quick check question: What role do key and query embeddings play in determining attention patterns?

## Architecture Onboarding

- Component map: Input tokens → embeddings → first attention layer → first MLP → second attention layer → second MLP → output predictions
- Critical path: Tokens flow through embedding layer, two attention-MLP blocks, with simplifications applied at attention layers through SVD/clustering or one-hot replacement
- Design tradeoffs: Simplicity vs. fidelity trade-off in model simplifications; data-dependent vs. data-independent simplification methods
- Failure signatures: Generalization gaps appear as reduced prediction similarity on out-of-distribution test sets while maintaining high similarity on in-distribution data
- First 3 experiments:
  1. Train a two-layer Transformer on Dyck-(20,10) and evaluate prediction similarity between original and rank-8 SVD approximation on seen vs. unseen structure splits
  2. Replace attention patterns with one-hot attention and measure accuracy changes on depth generalization split
  3. Train code completion models and evaluate approximation quality breakdown by prediction type (keywords, repeated words, etc.)

## Open Questions the Paper Calls Out

1. Do generalization gaps appear consistently across different types of distribution shifts, or are they task-specific? The paper demonstrates generalization gaps in both the Dyck and code completion tasks, but the magnitude and nature of the gaps differ between tasks and even between different prediction types within the same task.

2. How do different simplification techniques compare in terms of their faithfulness to the original model's behavior, especially out of distribution? The paper compares dimensionality reduction (SVD) and clustering as simplification methods, finding that both introduce generalization gaps. However, it doesn't explore other potential simplification techniques or provide a systematic comparison of their effectiveness.

3. What are the underlying mechanisms that lead to generalization gaps in simplified models? The paper suggests that simplified models might miss important features or representations that are crucial for out-of-distribution generalization, such as underestimating the model's ability to generalize to unseen depths in the Dyck task.

## Limitations
- The study focuses on relatively simple Transformer architectures rather than modern large language models
- The controlled datasets (Dyck languages, CodeSearchNet) may not fully capture the complexity of real-world tasks
- The paper doesn't investigate whether these interpretability illusions affect downstream applications or human understanding of model behavior

## Confidence
- **High** for the core observation that simplified proxies accurately approximate original models on IID data
- **Medium** for the systematic generalization gaps, as the paper demonstrates these effects but the underlying mechanisms require further validation
- **Low** for claims about simplified models sometimes outperforming originals, which is presented as an empirical observation without clear theoretical explanation

## Next Checks
1. Apply the same simplification and evaluation pipeline to larger Transformer variants (e.g., BERT-base) on natural language tasks to test if generalization gaps scale with model size.

2. Use probing techniques to identify which specific components (attention patterns, MLP activations, or residual connections) contribute most to systematic generalization failures in simplified proxies.

3. Conduct user studies where interpretability researchers analyze simplified versus original models to determine if these generalization gaps lead to incorrect mechanistic conclusions about model behavior.