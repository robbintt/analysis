---
ver: rpa2
title: Comparing Optimization Targets for Contrast-Consistent Search
arxiv_id: '2311.00488'
source_url: https://arxiv.org/abs/2311.00488
tags:
- loss
- function
- hyper-parameter
- functions
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the optimization target of Contrast-Consistent
  Search (CCS), which aims to recover internal representations of truth in large language
  models. The authors propose a new Midpoint-Displacement (MD) loss function as a
  proxy optimization target for CCS.
---

# Comparing Optimization Targets for Contrast-Consistent Search

## Quick Facts
- arXiv ID: 2311.00488
- Source URL: https://arxiv.org/abs/2311.00488
- Reference count: 18
- MD-Acc probers achieved an average test accuracy of 75.57% across four models, compared to 71.05% for CCS

## Executive Summary
This paper investigates the optimization target of Contrast-Consistent Search (CCS) by proposing a new Midpoint-Displacement (MD) loss function as a proxy. The authors demonstrate that for a specific hyper-parameter value, the MD loss function produces probers with similar weights to CCS. They show that this hyper-parameter is not optimal and that with a better hyper-parameter, the MD loss function achieves higher test accuracy than CCS (75.57% vs 71.05% on average across four models). The MD loss function shows a high cosine similarity (0.6336) with CCS probers, suggesting it captures a similar optimization target.

## Method Summary
The paper introduces the Midpoint-Displacement (MD) loss function as a proxy for CCS. The method involves training probers on contrast pairs from five datasets (IMDB, Amazon, BoolQ, AG News, RTE) using four different language models (UnifiedQA T5-Large, GPT-Neo, DeBERTa). Probers are trained for 1000 epochs with a learning rate of 0.01. The MD loss function includes a hyper-parameter λ that controls the trade-off between mean square value of midpoint activations (σ²m) and mean square displacement (σ²d). A grid search is performed to find the optimal λ value that maximizes test accuracy. The performance is evaluated using test accuracy and cosine similarity with CCS weight vectors.

## Key Results
- MD-Acc probers achieved an average test accuracy of 75.57% across four models, compared to 71.05% for CCS
- MD loss function showed a high cosine similarity (0.6336) with CCS probers when λ ≈ 0.99
- The optimal hyper-parameter for test accuracy differs from the one that maximizes similarity with CCS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MD loss function captures the same optimization target as CCS when λ is close to 1
- Mechanism: When λ approaches 1, the MD loss function prioritizes minimizing σ²m over maximizing σ²d, mirroring CCS's implicit trade-off caused by sigmoid saturation
- Core assumption: CCS's success depends on balancing σ²d and σ²m rather than simply maximizing σ²d
- Evidence anchors: [abstract], [section 4]
- Break condition: If CCS actually clusters activations by truth value, then MD loss would not be a good proxy

### Mechanism 2
- Claim: MD loss outperforms CCS when λ is optimized for test accuracy rather than cosine similarity with CCS
- Mechanism: By allowing λ to vary freely during hyper-parameter search, MD loss can find a balance between σ²d and σ²m that maximizes test accuracy
- Core assumption: The optimal trade-off between σ²d and σ²m for test accuracy differs from the trade-off that produces CCS-like weights
- Evidence anchors: [abstract], [section 4]
- Break condition: If optimal λ for test accuracy coincides with optimal λ for CCS similarity

### Mechanism 3
- Claim: CCS (and MD loss) success depends on contrast pair structure rather than exact loss formulation
- Mechanism: Contrast pairs provide unique displacement signal that enables unsupervised truth probing; different loss functions can exploit this signal
- Core assumption: Displacement between contrast pairs is key feature enabling unsupervised truth probing
- Evidence anchors: [section 5], [section 3.2]
- Break condition: If displacement between contrast pairs does not correlate with truth values

## Foundational Learning

- Concept: Contrast-Consistent Search (CCS) and its loss function
  - Why needed here: Understanding CCS is crucial to grasp why MD loss function is proposed as proxy optimization target
  - Quick check question: What is the key property of truth that CCS exploits to find a direction in activation space?

- Concept: Hyper-parameter optimization and its impact on loss function performance
  - Why needed here: The paper demonstrates that different hyper-parameter values for MD loss lead to different performance levels
  - Quick check question: How does the hyper-parameter λ in MD loss function control the trade-off between σ²d and σ²m?

- Concept: Cosine similarity as a measure of direction similarity in high-dimensional space
  - Why needed here: The paper uses cosine similarity to compare directions found by CCS and MD loss functions
  - Quick check question: What does a high cosine similarity between two weight vectors imply about their directions in activation space?

## Architecture Onboarding

- Component map: Contrast pairs (x+, x-) → Model activations → Normalized features → Loss function minimization → Prober weights → Test accuracy

- Critical path: Contrast pairs → Model activations → Normalized features → Loss function minimization → Prober weights → Test accuracy

- Design tradeoffs:
  - Using contrast pairs limits method to binary features but enables unsupervised learning
  - MD loss function introduces hyper-parameter requiring supervised labels for optimization
  - Higher-dimensional models may capture more nuanced truth representations but increase computational cost

- Failure signatures:
  - Low test accuracy despite high cosine similarity with CCS (or vice versa)
  - Unstable hyper-parameter selection across different datasets or models
  - Poor performance on datasets with less clear contrast pairs

- First 3 experiments:
  1. Replicate MD-CCS experiment with λ = 0.99 to verify high cosine similarity with CCS
  2. Perform grid search for MD-Acc hyper-parameter to find value that maximizes test accuracy
  3. Compare directions found by MD-CCS and MD-Acc using cosine similarity to confirm they differ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an unsupervised method be developed to determine the optimal hyper-parameter for MD loss function?
- Basis in paper: The paper discusses need for supervised labels to determine optimal hyper-parameter through grid search
- Why unresolved: Paper relies on supervised labels to find best hyper-parameter, no unsupervised method presented
- What evidence would resolve it: Demonstrating unsupervised method that consistently finds hyper-parameters yielding high test accuracy across various datasets without using supervised labels

### Open Question 2
- Question: How does behavior of probers trained with MD loss compare to CCS in terms of empirical pairwise agreement for contrast pairs?
- Basis in paper: Paper suggests comparing behavioral similarity between probers by measuring empirical pairwise agreement for contrast pairs
- Why unresolved: Paper only compares cosine similarity between probers, does not evaluate behavioral similarity through empirical pairwise agreement
- What evidence would resolve it: Conducting experiments to measure and compare empirical pairwise agreement of probers trained with MD loss and CCS across various datasets

### Open Question 3
- Question: Is there a hyper-parameter for MD loss function that shows high transferability between different datasets?
- Basis in paper: Paper mentions possibility of establishing hyper-parameter with high transferability between datasets
- Why unresolved: Paper uses grid search to find dataset-specific hyper-parameters, does not explore transferability of single hyper-parameter across multiple datasets
- What evidence would resolve it: Identifying hyper-parameter value that consistently yields high test accuracy across multiple datasets without requiring re-tuning for each dataset

## Limitations

- The optimal hyper-parameter for MD-Acc is determined using test labels, raising questions about the method's truly unsupervised nature
- The claim that CCS's success depends on contrast pair structure rather than loss function formulation is primarily inferential
- The mechanism by which MD loss captures CCS's optimization target when λ ≈ 1 is inferred rather than empirically verified

## Confidence

- **High confidence**: The empirical results showing MD-Acc achieves higher test accuracy (75.57% vs 71.05%) are well-supported by experimental data
- **Medium confidence**: The claim that MD loss is a good proxy for CCS when λ ≈ 1 is supported by cosine similarity results but relies on assumptions about mechanism
- **Low confidence**: The inference that CCS's success fundamentally depends on contrast pair structure rather than specific loss formulation is speculative

## Next Checks

1. Design an ablation study that systematically varies the trade-off between σ²d and σ²m in CCS to determine whether the sigmoid saturation mechanism is truly necessary for CCS performance

2. Evaluate MD-Acc with the optimal λ on a held-out dataset not used in hyper-parameter tuning to assess true generalization capability beyond the test set used for optimization

3. Test whether the displacement between contrast pairs actually correlates with truth values across different datasets and model architectures to verify the foundational assumption about contrast pair structure