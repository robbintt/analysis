---
ver: rpa2
title: Accurate Neural Network Pruning Requires Rethinking Sparse Optimization
arxiv_id: '2308.02060'
source_url: https://arxiv.org/abs/2308.02060
tags:
- sparse
- training
- sparsity
- dense
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the difficulty of obtaining highly-accurate
  and highly-sparse deep neural networks. The authors show that using standard dense
  training recipes for sparse training is suboptimal, leading to under-training of
  sparse models.
---

# Accurate Neural Network Pruning Requires Rethinking Sparse Optimization

## Quick Facts
- arXiv ID: 2308.02060
- Source URL: https://arxiv.org/abs/2308.02060
- Reference count: 40
- Key outcome: Standard dense training recipes under-train sparse models; extended training with AC/DC++ and modified layer unfreezing achieves state-of-the-art accuracy at high sparsity

## Executive Summary
This paper investigates why standard dense training approaches fail for sparse neural networks, revealing that sparse models under-train when using typical training schedules. The authors demonstrate that sparse optimization is inherently more difficult, requiring longer training to achieve comparable performance. They propose the AC/DC++ algorithm for vision models and a modified gradual layer unfreezing approach for language models, achieving state-of-the-art results in high-sparsity regimes. Their findings show that sparse models converge more slowly and require extended training schedules to reach optimal accuracy.

## Method Summary
The paper introduces AC/DC++, an extended training version of the AC/DC algorithm for sparse pre-training of vision models, and a modified gradual layer unfreezing approach for sparse fine-tuning of language models. The method involves applying sparsity masks to neural networks and training with extended schedules (500-1000 epochs) rather than standard 100 epochs. AC/DC++ incorporates mask exploration through gradient-based weight updates, while the language model approach gradually unfreezes layers during sparse fine-tuning. Both methods address the under-training problem by allowing sparse models more time to converge to optimal accuracy.

## Key Results
- Standard 100-epoch training schedules under-train sparse models, leading to high prediction entropy and suboptimal accuracy
- AC/DC++ achieves state-of-the-art accuracy for highly sparse vision models (up to 98% sparsity) on ImageNet
- Weight decay induces structured sparsity in unstructured sparse models by driving entire channels to zero
- Extended training schedules (500-1000 epochs) are necessary for sparse models to reach accuracy comparable to dense models

## Why This Works (Mechanism)

### Mechanism 1
Sparse models under-train when using standard dense training schedules because they converge more slowly in both training loss and output confidence. Sparse optimization has a more complex loss landscape with multiple barriers; achieving low loss and high output confidence requires more epochs than dense models. The sparsity mask exploration during training is limited compared to dense training, slowing convergence.

### Mechanism 2
Weight decay induces structured sparsity in unstructured sparse models by driving entire channels to zero. High weight decay gradually drives weights toward zero; once a full channel is zeroed out, it remains sparse even during decompression phases. Sparse channels, once zeroed, continue to receive zero gradients and stay zero unless mask exploration reintroduces them.

### Mechanism 3
Extended training mitigates undertraining in sparse models but can cause overfitting if not balanced properly. Longer training allows sparse models to reach low training loss and high confidence; however, sparse models shift to overfitting faster than dense ones due to reduced model capacity.

## Foundational Learning

- **Concept: Sparse optimization difficulty**
  - Why needed here: Explains why dense training schedules fail for sparse models
  - Quick check question: Why do sparse models require longer training than dense models to reach similar accuracy?

- **Concept: Loss landscape analysis**
  - Why needed here: Provides insight into why sparse training is harder (e.g., higher sharpness, mode connectivity barriers)
  - Quick check question: How does sharpness of the loss surface differ between sparse and dense models?

- **Concept: Sparsity mask exploration**
  - Why needed here: Explains why methods like AC/DC outperform others (more mask changes during training)
  - Quick check question: What is the relationship between mask IoU and model accuracy in sparse training?

## Architecture Onboarding

- **Component map**: Dense model → sparsity mask → extended training loop (AC/DC++/modified unfreezing) → evaluation
- **Critical path**: Initialize dense model → apply sparsity mask → train with extended schedule → evaluate accuracy vs dense baseline
- **Design tradeoffs**: Mask exploration (AC/DC) vs computational cost (GMP), uniform vs global sparsity, weight decay magnitude
- **Failure signatures**: High validation entropy, large gap between train and val loss, accuracy plateaus below dense baseline
- **First 3 experiments**:
  1. Train dense ResNet50 for 100 epochs, record train/val loss and entropy
  2. Apply 95% sparsity mask, train for 100 epochs, compare metrics to dense
  3. Repeat step 2 with 500+ epochs, verify if metrics approach dense levels

## Open Questions the Paper Calls Out

### Open Question 1
Does the accuracy of sparse models continue to improve with even longer training schedules beyond 1000 epochs? The paper notes that for 95% and 98% sparse models, the accuracy increase does not appear to be saturated even at 500 epochs, and mentions that RigL trained a 99% sparse model for 100 times the dense training time without saturating accuracy.

### Open Question 2
Is there an optimal weight decay value for different sparsity levels and training durations? The paper investigates the impact of weight decay on sparsity and model performance, finding that the standard value of 10^-4 is close to optimal, but also shows that accuracy varies with different weight decay values.

### Open Question 3
How does the difficulty of sparse optimization compare to dense optimization in terms of computational complexity and convergence rate? The paper suggests that sparse optimization may be inherently harder than dense optimization, as sparse models show evidence of under-training and require longer training to achieve optimal accuracy.

### Open Question 4
What is the relationship between the amount of mask exploration during training and the final model accuracy? The paper shows that AC/DC, which has stronger mask exploration compared to GMP and RigL, achieves better performance, suggesting a link between mask exploration and accuracy.

### Open Question 5
How does the emergence of structured sparsity in unstructured sparse models affect their performance and generalization? The paper observes that channel sparsity increases with sparsity target and training time, and that this phenomenon is linked to weight decay.

## Limitations
- Analysis of sparse optimization difficulty relies heavily on empirical observations without strong theoretical backing
- Weight decay mechanism for inducing structured sparsity lacks rigorous mathematical justification
- Relationship between sparsity level and optimal training duration remains unclear, with accuracy dropping sharply in the 90-95% range

## Confidence
- Medium: Sparse models under-train with standard dense schedules - strong empirical evidence but limited theoretical explanation
- Medium: Extended training approach improves accuracy - results show improvement but overfitting concerns not fully resolved
- Medium: Weight decay induces structured sparsity - supported by observations but lacking rigorous proof
- Low: Claims about loss landscape complexity in sparse training - largely qualitative without quantitative metrics

## Next Checks
1. Systematically vary weight decay and mask exploration parameters to quantify their individual contributions to accuracy gains
2. Compare loss landscape metrics (sharpness, mode connectivity) between sparse and dense models at equivalent convergence points
3. Evaluate the AC/DC++ algorithm across different backbone architectures and vision tasks to test generalizability beyond ResNet50 on ImageNet