---
ver: rpa2
title: Unifying Top-down and Bottom-up Scanpath Prediction Using Transformers
arxiv_id: '2303.09383'
source_url: https://arxiv.org/abs/2303.09383
tags:
- xation
- attention
- visual
- scanpath
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Human Attention Transformer (HAT) to predict
  both top-down and bottom-up human visual attention scanpaths in tasks like visual
  search and free viewing. HAT uses a transformer-based architecture with a simplified
  foveated retina to create a spatio-temporal working memory, avoiding the need for
  discretization and enabling dense heatmap predictions for each fixation.
---

# Unifying Top-down and Bottom-up Scanpath Prediction Using Transformers

## Quick Facts
- arXiv ID: 2303.09383
- Source URL: https://arxiv.org/abs/2303.09383
- Reference count: 40
- Key outcome: Human Attention Transformer (HAT) achieves state-of-the-art performance in predicting both top-down (visual search) and bottom-up (free viewing) human visual attention scanpaths using dense heatmap predictions and a shared working memory.

## Executive Summary
This paper introduces the Human Attention Transformer (HAT), a novel model that predicts human visual attention scanpaths for both top-down (task-driven) and bottom-up (saliency-driven) scenarios. HAT uses a transformer-based architecture with a simplified foveated retina to create a spatio-temporal working memory, enabling dense heatmap predictions for each fixation without discretization loss. The model achieves state-of-the-art performance on COCO-Search18 for visual search and COCO-FreeView for free viewing, with significant improvements in cNSS and cIG metrics. HAT also provides interpretable predictions by analyzing attention contributions from peripheral and foveal tokens.

## Method Summary
HAT predicts human visual attention scanpaths by using a transformer-based architecture with a foveated working memory. It constructs a dynamic memory from low-resolution peripheral features (P1) and high-resolution foveal features (P4) sampled at fixation locations. Scale, spatial, and temporal embeddings are added to tokens to encode fixation order and location. A transformer decoder with task-specific queries aggregates information from the working memory to produce dense fixation heatmaps. The model is trained using behavior cloning with pixel-wise focal loss and binary cross-entropy for termination prediction.

## Key Results
- State-of-the-art performance on COCO-Search18 for both target-present and target-absent visual search.
- Matches or exceeds state-of-the-art performance on COCO-FreeView for free viewing.
- Significant improvements in cNSS and cIG metrics compared to baseline models.
- Joint training on multiple tasks (18 object search tasks plus free viewing) using shared working memory and task-specific queries.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HAT integrates peripheral and foveal visual information through a foveated working memory to predict human scanpaths without discretization loss.
- Mechanism: A dual-resolution feature pyramid (P1 peripheral, P4 foveal) is processed through a transformer encoder that dynamically updates a working memory. The working memory is composed of flattened peripheral tokens (low-res features from P1) and foveal tokens (high-res features sampled at previous fixation locations from P4). Scale, spatial, and temporal embeddings are added to tokens to encode scale, location, and fixation order. A transformer decoder aggregates task-specific information from this memory to produce fixation heatmaps.
- Core assumption: Maintaining a temporally evolving working memory of peripheral and foveal features better approximates human visual attention than static or discretized approaches.
- Evidence anchors:
  - [abstract] "HAT uses a novel transformer-based architecture and a simplified foveated retina that collectively create a spatio-temporal awareness akin to the dynamic visual working memory of humans."
  - [section] "The foveation module constructs a dynamic working memory using the feature maps P1 and P4 to represent the information a person acquires from the peripheral and foveal vision, respectively."
- Break condition: If the peripheral tokens are removed or if the working memory is not updated dynamically, the model's ability to predict scanpaths degrades significantly (see ablation in Table 4).

### Mechanism 2
- Claim: HAT treats scanpath prediction as a dense heatmap prediction problem to avoid information loss from discretization.
- Mechanism: Instead of predicting fixations on a coarse grid, HAT outputs dense fixation heatmaps (H×W probability maps) at each step. This allows continuous, pixel-level supervision and eliminates the precision loss inherent in grid-based discretization methods. During inference, fixations are sampled from the normalized heatmap.
- Core assumption: Dense supervision provides higher fidelity and better alignment with human fixations than discretized categorical action spaces.
- Evidence anchors:
  - [abstract] "Unlike previous methods that rely on a coarse grid of fixation cells and experience information loss due to fixation discretization, HAT features a sequential dense prediction architecture and outputs a dense heatmap for each fixation, thus avoiding discretizing fixations."
  - [section] "HAT takes the same spirit but outputs a dense fixation heatmap. Specifically, HAT outputs a heatmap Yi∈ [0, 1]H×W with each pixel value indicating the chance of the pixel being fixated in the next fixation."
- Break condition: If the heatmap is converted back to a coarse grid for evaluation or prediction, the advantage of dense prediction is lost.

### Mechanism 3
- Claim: HAT achieves generality by jointly modeling top-down (task-driven) and bottom-up (saliency-driven) attention using shared working memory and task-specific queries.
- Mechanism: A single transformer-based architecture is trained to predict scanpaths for multiple tasks (e.g., 18 object search tasks plus free viewing) using N task-specific queries that selectively aggregate from a shared working memory. This allows the model to handle both goal-directed search and free-viewing without retraining separate models.
- Core assumption: Shared low-level visual processing (working memory) can be flexibly recombined for different attentional control modes through task-specific attention mechanisms.
- Evidence anchors:
  - [abstract] "HAT is the new state-of-the-art (SOTA) in predicting the scanpath of fixations made during target-present and target-absent search, and matches or exceeds SOTA in the prediction of 'taskless' free-viewing fixation scanpaths."
  - [section] "HAT consists of four modules: ... an aggregation module that selectively aggregates the information in the working memory using attention mechanism for each task."
- Break condition: If task-specific queries are not used or if the working memory is not shared, the model loses its ability to generalize across tasks.

## Foundational Learning

- Concept: Transformer-based attention mechanisms
  - Why needed here: HAT uses transformers to dynamically update a working memory and to selectively aggregate task-specific information, enabling complex spatio-temporal modeling of fixations.
  - Quick check question: How does a transformer encoder-decoder architecture differ from RNNs in maintaining sequential information for scanpath prediction?

- Concept: Multi-scale feature extraction and foveation modeling
  - Why needed here: HAT uses a feature pyramid (P1, P2, P3, P4) to simulate human foveated vision, where peripheral and foveal information are processed at different resolutions and dynamically combined.
  - Quick check question: Why does HAT discard P2 and P3 in constructing peripheral tokens, and what is the impact on performance?

- Concept: Behavior cloning and dense supervision
  - Why needed here: HAT is trained using behavior cloning on dense fixation heatmaps, avoiding the need for discretization and enabling pixel-level supervision.
  - Quick check question: How does focal loss help in training dense fixation heatmap predictions?

## Architecture Onboarding

- Component map:
  Image -> Feature Pyramid (P1, P2, P3, P4) -> Working Memory (peripheral + foveal tokens + embeddings) -> Transformer Encoder -> Task Queries (via Transformer Decoder) -> Fixation Heatmaps + Termination Probabilities

- Critical path:
  Image -> Feature Pyramid -> Working Memory (peripheral + foveal tokens + embeddings) -> Transformer Encoder -> Task Queries (via Transformer Decoder) -> Fixation Heatmaps + Termination Probabilities

- Design tradeoffs:
  - Dense prediction vs. discretization: HAT avoids discretization loss but requires more compute for dense heatmaps.
  - Peripheral vs. foveal token resolution: Using only P1 for peripheral tokens improves efficiency with minimal performance loss.
  - Shared vs. task-specific models: Joint training enables generality but may require more data and careful balancing of task objectives.

- Failure signatures:
  - Loss of center bias or inhibition of return: May indicate missing spatial or temporal embeddings.
  - Overconfidence in free-viewing: Could indicate overfitting to search tasks; check calibration on free-viewing datasets.
  - Slow inference: May be due to redundant computation of image pyramid or working memory not being cached across steps.

- First 3 experiments:
  1. Ablation: Remove peripheral tokens and retrain; measure drop in cIG, cNSS, SemSS.
  2. Ablation: Remove transformer encoder and use only current P4; compare fixation prediction accuracy.
  3. Generalization: Train on COCO-FreeView and test on MIT1003; measure performance drop in cIG and cNSS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HAT perform when trained on datasets with significantly different visual characteristics, such as indoor vs. outdoor scenes or synthetic vs. natural images?
- Basis in paper: [inferred] The paper evaluates HAT on COCO-Search18, COCO-FreeView, MIT1003, and OSIE datasets, but does not explore its performance across drastically different visual domains.
- Why unresolved: The paper focuses on comparing HAT to other methods within similar datasets rather than testing its robustness to domain shifts.
- What evidence would resolve it: Experiments comparing HAT's performance on datasets with distinct visual characteristics, such as indoor vs. outdoor scenes or synthetic vs. natural images, would provide insights into its generalizability.

### Open Question 2
- Question: Can HAT's predictions be further improved by incorporating additional contextual information, such as scene semantics or task-specific cues, beyond the current foveated working memory?
- Basis in paper: [inferred] The paper mentions the potential for HAT to make highly interpretable predictions and discusses the contribution of peripheral and foveal tokens, but does not explore incorporating additional contextual information.
- Why unresolved: The paper focuses on the effectiveness of the current foveated working memory and does not investigate the potential benefits of incorporating additional contextual information.
- What evidence would resolve it: Experiments comparing HAT's performance with and without additional contextual information, such as scene semantics or task-specific cues, would provide insights into the potential benefits of incorporating such information.

### Open Question 3
- Question: How does HAT's performance scale with increasing input resolution, and what are the computational limitations of the model for high-resolution images?
- Basis in paper: [explicit] The paper mentions that HAT circumvents the need for discretizing fixations, making it applicable to high-resolution input, but does not provide specific details on performance scaling or computational limitations.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis of how HAT's performance and computational requirements change with increasing input resolution.
- What evidence would resolve it: Experiments evaluating HAT's performance and computational requirements on images of varying resolutions, along with theoretical analysis of the model's complexity, would provide insights into its scalability and limitations for high-resolution images.

## Limitations

- The model's computational overhead from dense heatmap predictions is not addressed in terms of efficiency or real-time applicability.
- Generalization to other resolutions or domains (e.g., medical imaging) is not discussed.
- Ablation studies focus on architectural components but do not explore the impact of varying the number of transformer layers or attention heads.

## Confidence

- **High confidence**: The model's state-of-the-art performance on COCO-Search18 and COCO-FreeView is well-supported by quantitative metrics (cNSS, cIG, SemSS). The architectural design choices, such as the use of peripheral and foveal tokens, are clearly motivated and empirically validated.
- **Medium confidence**: The claim that dense heatmap prediction avoids discretization loss is plausible but not directly compared to alternative discretization methods in terms of precision-recall trade-offs. The generality of the model across tasks is demonstrated, but the analysis of failure modes or limitations in specific scenarios (e.g., cluttered scenes) is limited.
- **Low confidence**: The paper does not provide a detailed analysis of the model's robustness to noise or adversarial perturbations, which is critical for real-world deployment.

## Next Checks

1. **Ablation on transformer depth**: Systematically vary the number of transformer layers and attention heads to assess the trade-off between model complexity and performance. Evaluate whether deeper or wider architectures improve generalization to unseen datasets like MIT1003.
2. **Cross-resolution generalization**: Train HAT on images resized to 320x512 and test on images at native resolutions (e.g., 640x960) to evaluate robustness to scale changes. Compare performance metrics (cNSS, cIG) across resolutions.
3. **Robustness to noise**: Introduce Gaussian or salt-and-pepper noise to input images and measure the degradation in fixation prediction accuracy (cNSS, cIG). Analyze whether the model's working memory mechanism helps maintain stability under noisy conditions.