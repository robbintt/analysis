---
ver: rpa2
title: Context Aware Query Rewriting for Text Rankers using LLM
arxiv_id: '2308.16753'
source_url: https://arxiv.org/abs/2308.16753
tags:
- query
- queries
- document
- ranking
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a context-aware query rewriting (CAR) approach
  that uses LLMs to generate natural language rewrites of ambiguous queries for text
  ranking tasks. The key idea is to provide relevant documents as context when prompting
  the LLM during training, and to avoid query rewriting during inference.
---

# Context Aware Query Rewriting for Text Rankers using LLM

## Quick Facts
- arXiv ID: 2308.16753
- Source URL: https://arxiv.org/abs/2308.16753
- Reference count: 40
- This paper proposes context-aware query rewriting (CAR) using LLMs for text ranking tasks.

## Executive Summary
This paper addresses the challenge of ambiguous queries in text ranking by proposing a context-aware query rewriting (CAR) approach. The key innovation is using LLMs to generate natural language rewrites of ambiguous queries during training, while providing relevant documents as context to ground the rewrites. Unlike previous approaches that use LLM rewriting during inference, CAR fine-tunes the ranker on the rewritten queries and avoids LLM rewriting at inference time, reducing computational overhead while maintaining ranking performance. Experiments show significant improvements of up to 33% on passage ranking and 28% on document ranking compared to using original queries.

## Method Summary
The CAR framework uses LLMs to rewrite ambiguous training queries by providing relevant documents as context during prompting. The ranker is then fine-tuned on these rewritten queries instead of the original ambiguous ones. During inference, the fine-tuned ranker directly ranks documents for the original queries without performing any rewriting, thus avoiding the computational cost of LLM inference. The approach is evaluated on MS MARCO passage and document datasets, with TREC-DL-19 and TREC-DL-20 serving as test sets. Query rewrite quality is measured using ROUGE-L and BERTScore, while downstream ranking performance is evaluated using MRR and nDCG@10.

## Key Results
- Fine-tuning a ranker using rewritten queries offers up to 33% improvement on passage ranking tasks
- Up to 28% improvement on document ranking tasks compared to baseline using original queries
- Context-aware prompting with document context significantly improves query rewrite quality
- Avoiding LLM rewriting during inference eliminates computational overhead while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual document grounding reduces topic drift in LLM-generated rewrites.
- Mechanism: When prompting the LLM, the relevant document is provided as context, constraining the rewrite to align with the document's intent and reducing the likelihood of generating queries for unrelated interpretations of the ambiguous query.
- Core assumption: The LLM can effectively use the document context to disambiguate the user's intent and generate a rewrite that is both relevant to the query and grounded in the document's content.
- Evidence anchors:
  - [abstract]: "We rewrite ambiguous training queries by context-aware prompting of LLMs, where we use only relevant documents as context."
  - [section]: "We first generate query rewrites by additionally providing the relevant document as context during training. Consequently, the generated query rewrite is fully aligned with the context improving training of text rankers."
  - [corpus]: Weak - the corpus mentions related works on query rewriting but doesn't directly support this specific mechanism of using document context during training.
- Break condition: If the relevant document itself is ambiguous or covers multiple topics, the grounding may not be effective, and topic drift could still occur.

### Mechanism 2
- Claim: Avoiding LLM rewriting during inference eliminates computational overhead and latency issues.
- Mechanism: By fine-tuning the ranker on the LLM-generated rewrites during training, the ranker learns to understand the disambiguated intent. During inference, the ranker directly ranks documents for the original ambiguous queries without needing to generate rewrites, avoiding the computational cost of LLM inference.
- Core assumption: The ranker can learn to generalize the disambiguation patterns from the training data and effectively rank documents for new ambiguous queries without explicit rewriting.
- Evidence anchors:
  - [abstract]: "Unlike existing approaches, we use LLM-based query rewriting only during the training phase. Eventually, a ranker is fine-tuned on the rewritten queries instead of the original queries during training."
  - [section]: "Secondly, and unlike existing works, we fully avoid any query rewriting using LLMs during inference. In other words, we assume that training a ranker to match LLM-generated queries with relevant documents results in learning a generalized ranking model."
  - [corpus]: Weak - the corpus mentions the problem of "large inference costs during query processing" but doesn't directly support this specific solution of avoiding LLM rewriting during inference.
- Break condition: If the ranker fails to generalize the disambiguation patterns from the training data, it may not perform well on new ambiguous queries during inference.

### Mechanism 3
- Claim: Fine-tuning the ranker on disambiguated queries improves ranking performance on ambiguous queries.
- Mechanism: By training the ranker on the LLM-generated rewrites, which disambiguate the user's intent, the ranker learns to better understand and match the user's information needs, leading to improved ranking performance on ambiguous queries.
- Core assumption: The LLM-generated rewrites effectively disambiguate the user's intent and provide a better representation of the user's information needs for the ranker to learn from.
- Evidence anchors:
  - [abstract]: "In our extensive experiments, we find that fine-tuning a ranker using re-written queries offers a significant improvement of up to 33% on the passage ranking task and up to 28% on the document ranking task when compared to the baseline performance of using original queries."
  - [section]: "At inference time, the ranking model fine-tuned on re-written queries yields much better ranking performance in comparison to the original queries."
  - [corpus]: Weak - the corpus mentions the problem of "concept drift when using only queries as prompts" but doesn't directly support this specific solution of fine-tuning the ranker on disambiguated queries.
- Break condition: If the LLM-generated rewrites are not effective in disambiguating the user's intent or contain errors, the ranker may not learn the correct patterns and may not improve ranking performance.

## Foundational Learning

- Concept: Query rewriting and its role in addressing vocabulary mismatch.
  - Why needed here: Understanding the problem that query rewriting aims to solve (vocabulary mismatch between user queries and documents) is crucial for appreciating the significance of the proposed approach.
  - Quick check question: What is the vocabulary mismatch problem in information retrieval, and how does query rewriting help address it?

- Concept: Large Language Models (LLMs) and their capabilities in natural language generation.
  - Why needed here: The proposed approach relies on using LLMs to generate natural language rewrites of ambiguous queries, so understanding the capabilities and limitations of LLMs is essential.
  - Quick check question: What are the key capabilities of LLMs that make them suitable for generating natural language rewrites of queries?

- Concept: Fine-tuning pre-trained models for specific downstream tasks.
  - Why needed here: The proposed approach involves fine-tuning a pre-trained ranker on the LLM-generated rewrites, so understanding the concept of fine-tuning and its benefits is important.
  - Quick check question: What is fine-tuning, and how does it help adapt pre-trained models to specific downstream tasks?

## Architecture Onboarding

- Component map: Query Rewriter (LLM + document context) -> Disambiguated query -> Ranker (fine-tuned) -> Ranked documents
- Critical path: Ambiguous query → Query Rewriter (LLM + document context) → Disambiguated query → Ranker (fine-tuned) → Ranked documents
- Design tradeoffs:
  - Using LLMs for query rewriting during training vs. using simpler methods or avoiding rewriting altogether
  - Providing document context during LLM prompting vs. using only the query or other forms of context
  - Fine-tuning the ranker on disambiguated queries vs. using the original ambiguous queries or other forms of query expansion
- Failure signatures:
  - Poor ranking performance on ambiguous queries, indicating that the ranker failed to learn the disambiguation patterns
  - Inconsistent or irrelevant query rewrites, suggesting that the LLM is not effectively using the document context or is generating erroneous outputs
  - High computational overhead during inference, indicating that the ranker is not effectively generalizing the disambiguation patterns and still requires explicit rewriting
- First 3 experiments:
  1. Evaluate the quality of query rewrites generated by the LLM with and without document context using automated metrics (e.g., BERTScore, ROUGE-L) and manual inspection
  2. Compare the ranking performance of the fine-tuned ranker on disambiguated queries vs. the original ambiguous queries using standard ranking metrics (e.g., MRR, nDCG@10) on a test set
  3. Analyze the impact of different LLM models (e.g., different parameter scales) and document selection strategies (e.g., linear vs. attention-based) on the quality of query rewrites and ranking performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CAR vary with different sizes and types of document context provided during prompting?
- Basis in paper: [explicit] The paper mentions experimenting with different prompting approaches and varying the document context, and also proposes using passage selection techniques to mitigate topic drift for long documents.
- Why unresolved: While the paper shows that context-aware prompting improves performance compared to non-context approaches, it doesn't systematically explore how different types or granularities of context affect results.
- What evidence would resolve it: A controlled experiment varying the amount, type (e.g., title only vs. full text vs. selected passages), and specificity of document context provided during prompting, measuring downstream ranking performance.

### Open Question 2
- Question: What is the optimal trade-off between query rewriting quality and computational efficiency for different deployment scenarios?
- Basis in paper: [inferred] The paper acknowledges the computational overhead of LLM-based query rewriting during inference as a limitation, and proposes avoiding rewriting during inference by fine-tuning rankers on rewritten queries instead.
- Why unresolved: The paper doesn't explore how much computational savings this approach provides compared to on-the-fly rewriting, or how it performs under different latency requirements and query loads.
- What evidence would resolve it: A comprehensive analysis comparing inference time, memory usage, and ranking performance between fine-tuned rankers using rewritten queries versus direct LLM rewriting under various deployment constraints.

### Open Question 3
- Question: How well does the CAR approach generalize to different domains and types of ambiguous queries beyond the MS MARCO and TREC datasets?
- Basis in paper: [explicit] The paper primarily evaluates on MS MARCO passage/document datasets and TREC Web topics, focusing on certain types of ambiguous queries like acronyms and entities with multiple meanings.
- Why unresolved: The paper doesn't explore how well CAR performs on other types of ambiguity (e.g., temporal, spatial, or user intent ambiguity) or in different domains like healthcare or e-commerce.
- What evidence would resolve it: Evaluating CAR on diverse datasets representing different types of ambiguity and domains, measuring both rewrite quality and downstream ranking performance.

## Limitations

- The reported performance improvements may be dataset-specific to MS MARCO and TREC collections, with unclear generalizability to other domains
- The document selection mechanism for providing context to LLMs is not fully specified, particularly for long documents with multiple topics
- The approach assumes that fine-tuning on rewritten queries will generalize to new ambiguous queries during inference, but this assumption is not rigorously validated across different types of ambiguity

## Confidence

- **High Confidence**: The mechanism of avoiding LLM inference during ranking (Mechanism 2) is well-supported and straightforward to validate. The claim that this reduces computational overhead is straightforward and verifiable.
- **Medium Confidence**: The claim about context-aware prompting reducing topic drift (Mechanism 1) is supported by the proposed approach but lacks direct empirical validation against drift metrics.
- **Medium Confidence**: The ranking improvement claims (Mechanism 3) are supported by reported metrics but could benefit from ablation studies showing the contribution of context-aware prompting versus fine-tuning alone.

## Next Checks

1. **Ablation Study**: Test the ranker's performance when fine-tuned on rewrites without document context versus with document context to quantify the contribution of context-aware prompting.
2. **Generalization Test**: Evaluate the approach on a different IR dataset (e.g., Robust04 or ClueWeb) to assess robustness across domains with varying ambiguity patterns.
3. **Error Analysis**: Manually inspect a sample of rewritten queries and their corresponding rankings to identify systematic errors or failure modes, particularly for different types of ambiguity.