---
ver: rpa2
title: 'UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation'
arxiv_id: '2303.08518'
source_url: https://arxiv.org/abs/2303.08518
tags:
- prompt
- task
- input
- retriever
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UPRISE, a universal prompt retrieval method
  for improving zero-shot evaluation of large language models. UPRISE trains a lightweight
  retriever to automatically retrieve prompts for any given zero-shot task input,
  demonstrating universality across task types and model scales.
---

# UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation

## Quick Facts
- arXiv ID: 2303.08518
- Source URL: https://arxiv.org/abs/2303.08518
- Reference count: 22
- One-line result: Universal prompt retrieval method improves zero-shot evaluation of LLMs across task types and model scales

## Executive Summary
UPRISE introduces a universal prompt retrieval method that trains a lightweight retriever to automatically retrieve effective prompts for any given zero-shot task input. The approach demonstrates cross-task and cross-model universality by training on diverse tasks with a small frozen LLM (GPT-Neo-2.7B) and evaluating on unseen task types with larger LLMs like BLOOM-7.1B, OPT-66B, and GPT3-175B. The method also shows promise in mitigating hallucination problems in ChatGPT during fact-checking tasks.

## Method Summary
UPRISE trains a bi-encoder retriever using contrastive learning with InfoNCE loss to identify effective prompts from a large prompt pool. The retriever is trained on instruction templates converted from FLAN datasets, using task scores from a frozen GPT-Neo-2.7B to identify positive and negative prompt-input pairs. During inference, the retriever retrieves K=3 most relevant prompts which are concatenated with the input for the target LLM. The approach is evaluated across multiple task types and model scales to demonstrate universal applicability without requiring further tuning.

## Key Results
- UPRISE achieves an average performance gain of 2.2% on BLOOM-7.1B across 5 testing clusters compared to zero-shot
- The method demonstrates cross-task retrieval accuracy, working on unseen task types not used during training
- UPRISE mitigates hallucination problems in ChatGPT, improving fact-checking performance through natural language inference prompts

## Why This Works (Mechanism)

### Mechanism 1
The retriever learns to identify prompts that elicit correct task responses from the frozen LLM by using contrastive learning on task scores. The retriever is trained using InfoNCE loss to maximize similarity between encoded prompt and input for positive pairs, and minimize it for negative pairs. Positive prompts are identified by having the frozen LLM achieve good task scores when conditioned on the prompt-input pair. The core assumption is that the frozen LLM's task score on a prompt-input pair reliably signals whether the prompt will help the inference LLM produce correct outputs.

### Mechanism 2
Cross-task and cross-model universality emerges because the retriever learns general prompt selection strategies rather than task-specific patterns. The retriever is trained on multiple task clusters but evaluated on unseen clusters (cross-task), and uses a small frozen LLM for training but is tested on much larger LLMs (cross-model). This forces the retriever to learn prompt selection criteria that transfer across task types and model scales. The core assumption is that effective prompt selection strategies are largely invariant across different task types and model scales when tasks share underlying semantic structures.

### Mechanism 3
The hallucination mitigation works because retrieved prompts from natural language inference tasks encourage the model to reason from its parametric memory rather than generate from patterns. When ChatGPT receives retrieved prompts primarily from natural language inference tasks, it is induced to correctly infer from its built-in knowledge rather than hallucinate, as demonstrated in the fact-checking experiments. The core assumption is that ChatGPT has the necessary factual knowledge in its parametric memory but sometimes fails to correctly access or reason from it, and appropriate prompts can guide it to do so.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The retriever is trained using contrastive learning to distinguish between effective and ineffective prompts for given inputs
  - Quick check question: What is the objective of InfoNCE loss in the context of prompt retrieval?

- Concept: Cross-task generalization and domain adaptation
  - Why needed here: The retriever must perform well on task types it was not trained on, requiring understanding of how knowledge transfers across task domains
  - Quick check question: How does training on multiple task clusters help the retriever generalize to unseen task types?

- Concept: Zero-shot learning and in-context learning
  - Why needed here: The entire approach builds on zero-shot evaluation of LLMs, where models must perform tasks without parameter updates
  - Quick check question: What is the key difference between zero-shot and few-shot prompting in the context of this work?

## Architecture Onboarding

- Component map: Task data converter -> Prompt pool constructor -> Prompt scorer (with frozen LLM) -> Prompt retriever -> Inference pipeline
- Critical path: Task data → Prompt scorer (with frozen LLM) → Prompt retriever training → Prompt retrieval → Inference with target LLM
- Design tradeoffs: Small frozen LLM vs. larger inference LLM (training efficiency vs. potential alignment issues), number of prompts to concatenate (performance vs. inference efficiency), prompt pool construction (demonstration quality vs. computational cost)
- Failure signatures: Performance drops on cross-task evaluation (retriever may be overfitting to training task types), negative results on larger LLMs (prompt selection strategies may not transfer across model scales), no improvement on hallucination tasks (retrieved prompts may not effectively guide reasoning)
- First 3 experiments: 1) Train retriever on single task cluster, evaluate on held-out tasks to verify basic functionality, 2) Train on multiple clusters, evaluate cross-task performance to test generalization, 3) Train with small LLM, evaluate on progressively larger LLMs to test cross-model generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does UPRISE's performance degrade when tested on LLMs with significantly different architectures or training objectives than GPT-Neo-2.7B?
- Basis in paper: The experiments only cover LLMs similar in scale and training methodology to GPT-Neo-2.7B
- Why unresolved: The paper doesn't explore performance on models with fundamentally different architectures or training objectives
- What evidence would resolve it: Experiments testing UPRISE on models like BERT, RoBERTa, or models trained with different objectives (e.g., ELECTRA, DeBERTa)

### Open Question 2
- Question: How does the size of the prompt pool affect UPRISE's performance, and is there an optimal balance between pool size and retrieval quality?
- Basis in paper: The paper mentions using a prompt pool of approximately 180k training examples but doesn't systematically investigate the relationship between pool size and performance
- Why unresolved: The paper doesn't explore how varying the prompt pool size impacts retrieval accuracy or downstream task performance
- What evidence would resolve it: Experiments varying the prompt pool size across different orders of magnitude and measuring corresponding changes in retrieval accuracy and task performance

### Open Question 3
- Question: Can UPRISE be effectively applied to tasks beyond natural language understanding, such as code generation or mathematical reasoning?
- Basis in paper: The paper focuses on natural language understanding tasks, but the method's generality suggests potential applicability to other domains
- Why unresolved: The paper doesn't explore UPRISE's effectiveness on non-NLU tasks
- What evidence would resolve it: Experiments applying UPRISE to code generation tasks (e.g., HumanEval), mathematical reasoning problems, or other specialized language tasks

## Limitations

- Cross-task generalization validity is questionable as the paper only evaluates on 9 task clusters with one held out at a time, and actually shows negative performance on commonsense reasoning and coreference resolution tasks
- The cross-model alignment assumption lacks theoretical foundation - there's no established reason why prompt selection strategies learned with a small frozen LLM should transfer to much larger LLMs
- Hallucination mitigation claims are based on a single observation without systematic investigation or controlled experiments to isolate the effect of different prompt types

## Confidence

**High Confidence (8/10):** The basic retriever architecture using contrastive learning with InfoNCE loss is technically sound and well-established. The experimental methodology for evaluating cross-task and cross-model generalization is rigorous and the reported performance improvements are statistically significant.

**Medium Confidence (5/10):** The universality claims across task types and model scales are supported by experimental evidence but the theoretical foundation for why these transfers should work is weak. The assumption that a small frozen LLM's task scores reliably indicate prompt quality for much larger inference LLMs lacks strong justification.

**Low Confidence (3/10):** The hallucination mitigation mechanism is based on a single observation without systematic investigation. The proposed explanation about "correct inference from parametric memory" is not validated through controlled experiments that isolate the effect of different prompt types.

## Next Checks

**Validation Check 1:** Conduct systematic ablation studies comparing prompt retrieval performance across different model scale ratios (e.g., train with 1B, test with 10B; train with 10B, test with 100B) to establish the limits of cross-model transfer and identify when the small-to-large transfer assumption breaks down.

**Validation Check 2:** Design controlled experiments to test the hallucination mitigation hypothesis by comparing ChatGPT performance on fact-checking tasks using prompts from different source task types (NLI, QA, summarization, etc.) to determine which prompt characteristics actually reduce hallucinations and why.

**Validation Check 3:** Implement a robustness test by evaluating the retriever on truly out-of-distribution task types not represented in the FLAN-derived clusters (e.g., code generation, mathematical reasoning, creative writing) to determine if the learned prompt selection strategies generalize beyond the training distribution or if they're overfitting to specific task patterns.