---
ver: rpa2
title: 'MGAS: Multi-Granularity Architecture Search for Trade-Off Between Model Effectiveness
  and Efficiency'
arxiv_id: '2310.15074'
source_url: https://arxiv.org/abs/2310.15074
tags:
- search
- architecture
- operation
- units
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing model effectiveness
  and efficiency in neural architecture search (NAS). The authors propose Multi-Granularity
  Architecture Search (MGAS), a unified framework that comprehensively explores a
  multi-granularity search space to discover both effective and efficient neural networks.
---

# MGAS: Multi-Granularity Architecture Search for Trade-Off Between Model Effectiveness and Efficiency

## Quick Facts
- arXiv ID: 2310.15074
- Source URL: https://arxiv.org/abs/2310.15074
- Reference count: 33
- Key outcome: Achieves 97.34% accuracy with 2.1M parameters on CIFAR-10, outperforming baselines in accuracy-parameter trade-off.

## Executive Summary
This paper addresses the challenge of balancing model effectiveness and efficiency in neural architecture search (NAS) through Multi-Granularity Architecture Search (MGAS). The method improves upon differentiable architecture search by exploring multiple granularity levels (operations, filters, weights) with adaptive pruning and multi-stage search. MGAS achieves better trade-offs between accuracy and parameter efficiency compared to existing methods, as demonstrated through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet.

## Method Summary
MGAS is a unified framework that comprehensively explores a multi-granularity search space to discover both effective and efficient neural networks. The method improves upon existing differentiable architecture search (DAS) approaches by (1) adaptively adjusting retention ratios of searchable units across different granularity levels through learned discretization functions, and (2) decomposing super-net optimization into multiple stages with progressive re-evaluation to enable re-pruning and regrowth of units. This approach addresses the limitations of manually defined pruning ratios and single-stage optimization that can lead to bias in the search process.

## Key Results
- Achieves 97.34% accuracy with 2.1M parameters on CIFAR-10, outperforming baselines
- Improves accuracy-parameter trade-off across multiple datasets (CIFAR-10, CIFAR-100, ImageNet)
- Demonstrates effectiveness of adaptive pruning and multi-stage search strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-granularity search balances filter-level and weight-level sparsity to achieve optimal accuracy-parameter trade-off.
- Mechanism: The method jointly optimizes architecture parameters (α), filter parameters (β), and weight parameters (ω) across different granularity levels, allowing adaptive pruning of operations, filters, and weights based on their learned importance.
- Core assumption: The relative importance of operations, filters, and weights can be compared and balanced through learned discretization functions that adaptively determine remaining ratios.
- Evidence anchors:
  - [abstract] "We adaptively adjust the retention ratios of searchable units across different granularity levels through adaptive pruning, which is achieved by learning granularity-specific discretization functions along with the evolving architecture."
  - [section] "To tackle these issues, we propose Multi-Granularity Differentiable Architecture Search (MG-DARTS), a unified framework which aims to discover both effective and efficient architectures from scratch by comprehensively yet memory-efficiently exploring a multi-granularity search space."
  - [corpus] Weak - related papers focus on differentiable NAS but don't address multi-granularity balance specifically.
- Break condition: If the learned discretization functions cannot effectively compare importance across different granularity scales, the balance would fail.

### Mechanism 2
- Claim: Multi-stage search reduces memory consumption while maintaining search quality through progressive re-evaluation.
- Mechanism: The super-net is decomposed into sequential sub-nets, each optimized separately, with pruned units allowed to regrow in later stages based on their potential.
- Core assumption: Units that appear unimportant in early stages may become important when evaluated in the context of later stages, so re-evaluation is necessary.
- Evidence anchors:
  - [abstract] "we decompose the super-net optimization and discretization into multiple stages, each operating on a sub-net, and introduce progressive re-evaluation to enable re-pruning and regrowth of previous units, thereby mitigating potential bias."
  - [section] "To reduce the memory consumption while mitigating bias, we propose a multi-stage search strategy. This entails decomposing the super-net optimization and discretization into sequential sub-net stages and progressively re-evaluating the searchable units."
  - [corpus] Weak - related papers discuss decomposition strategies but don't combine with progressive re-evaluation.
- Break condition: If the re-evaluation process introduces too much computational overhead or if the regrowth mechanism selects suboptimal units.

### Mechanism 3
- Claim: Adaptive pruning consistently achieves better granularity balance across different model sizes.
- Mechanism: The method dynamically adjusts pruning criteria for each granularity level based on the evolving architecture, rather than using fixed pruning rates.
- Core assumption: The optimal balance between operations, filters, and weights varies depending on the target model size and the specific architecture being evolved.
- Evidence anchors:
  - [section] "Through extensive experiments, our method demonstrates promising outcomes... Second, it enhances the model performance while maintaining a similar model size compared with existing fine-grained methods."
  - [section] "In this section, we verify whether adaptive pruning can consistently achieve a better granularity balance for different target model sizes."
  - [corpus] Weak - related papers don't address adaptive pruning for multi-granularity balance.
- Break condition: If the adaptive pruning mechanism cannot converge to stable remaining ratios or if it consistently under-prunes or over-prunes certain granularity levels.

## Foundational Learning

- Concept: Differentiable Architecture Search (DARTS)
  - Why needed here: MGAS builds upon DARTS by extending the search space to multiple granularity levels while maintaining differentiable optimization.
  - Quick check question: What is the key difference between traditional NAS and DARTS in terms of how architectures are represented and optimized?

- Concept: Super-net construction and weight-sharing
  - Why needed here: The method constructs a super-net that contains all possible operations, filters, and weight configurations, and uses weight-sharing to enable efficient search.
  - Quick check question: How does weight-sharing in a super-net reduce the computational cost compared to training individual architectures?

- Concept: Discretization functions and gradient estimation
  - Why needed here: The method learns discretization functions to adaptively determine which units to prune, requiring understanding of how to make discretization differentiable.
  - Quick check question: Why is it necessary to make discretization functions differentiable in differentiable NAS?

## Architecture Onboarding

- Component map: Multi-granularity search space (operations, filters, weights) -> Adaptive pruning module with learned discretization functions -> Multi-stage search framework with sub-net decomposition -> Progressive re-evaluation mechanism for regrowth -> Final architecture selection

- Critical path: Search space construction → Super-net optimization with joint granularity parameters → Adaptive pruning based on learned discretization → Multi-stage decomposition → Progressive re-evaluation and regrowth → Final architecture selection

- Design tradeoffs: The method trades increased search space complexity (multiple granularity levels) for better accuracy-parameter trade-off, and trades longer search time for memory efficiency through multi-stage decomposition.

- Failure signatures: If the discovered architectures consistently underperform baselines, check whether the discretization functions are learning meaningful importance scores; if memory consumption is too high, verify the sub-net decomposition is working correctly; if the method is too slow, examine the pruning and regrowth intervals.

- First 3 experiments:
  1. Implement the multi-granularity search space with operation, filter, and weight levels on a small proxy dataset to verify the search space construction works.
  2. Test the adaptive pruning mechanism with learned discretization functions on a simple architecture to ensure units are being pruned based on learned importance.
  3. Implement the multi-stage search with progressive re-evaluation on a small problem to verify the memory reduction and regrowth mechanisms work as expected.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several significant research directions emerge from the work:

- How does the optimal granularity balance vary across different neural network architectures and tasks beyond image classification?
- Can the multi-stage search strategy be effectively combined with other memory reduction techniques like proxy networks or path binarization?
- What is the theoretical relationship between granularity-specific discretization functions and the emergence of optimal sparsity patterns?
- How does the regrowing mechanism affect the convergence properties and final solution quality compared to pure pruning approaches?

## Limitations

- Limited theoretical analysis of why adaptive pruning and multi-stage search lead to optimal solutions
- Focus primarily on image classification tasks, with unclear generalization to other domains
- Additional hyperparameters introduced by multi-stage approach require careful tuning
- Computational overhead of progressive re-evaluation not fully characterized

## Confidence

- Multi-granularity search effectiveness: High
- Adaptive pruning mechanism: Medium
- Multi-stage search benefits: Medium
- Generalization across tasks: Low

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of each granularity level to the overall performance improvement.
2. Test the method on additional datasets and tasks (e.g., object detection, segmentation) to evaluate generalization beyond image classification.
3. Analyze the computational overhead introduced by the progressive re-evaluation mechanism and compare it with the benefits in model quality.