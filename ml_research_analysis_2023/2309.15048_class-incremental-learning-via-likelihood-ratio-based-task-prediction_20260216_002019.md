---
ver: rpa2
title: Class Incremental Learning via Likelihood Ratio Based Task Prediction
arxiv_id: '2309.15048'
source_url: https://arxiv.org/abs/2309.15048
tags:
- task
- learning
- tplr
- each
- task-id
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach for task-id prediction in class
  incremental learning (CIL) by leveraging the likelihood ratio between the target
  task and its complement. Unlike traditional OOD detection, which estimates P(t)
  using proxy distributions due to lack of OOD data, TPLR exploits replay data from
  previous tasks to estimate both P(t) and P(tc) and compute a principled likelihood
  ratio score.
---

# Class Incremental Learning via Likelihood Ratio Based Task Prediction

## Quick Facts
- arXiv ID: 2309.15048
- Source URL: https://arxiv.org/abs/2309.15048
- Reference count: 40
- This paper proposes a new approach for task-id prediction in class incremental learning (CIL) by leveraging the likelihood ratio between the target task and its complement, establishing a new state-of-the-art performance with an average accuracy of 76.21% on CIFAR-10, CIFAR-100, and TinyImageNet datasets.

## Executive Summary
This paper addresses the task-id prediction problem in class incremental learning (CIL) by introducing a novel approach called Task Prediction via Likelihood Ratio (TPLR). Unlike traditional OOD detection methods that ignore replay data, TPLR exploits replay buffer samples from previous tasks to estimate both the target task distribution and its complement, computing a principled likelihood ratio score. The method combines this feature-based score with a logit-based score using an energy-based model framework and applies temperature-controlled softmax to produce confident task predictions.

## Method Summary
TPLR estimates task-id prediction scores by computing the likelihood ratio between the target task distribution Pt and its complement Ptc using replay buffer data. The method employs Mahalanobis distance and KNN-based density estimation in feature space, combines these scores with maximum logit scores through an energy-based model framework, and applies low-temperature softmax (γ=0.05) to encourage confident predictions. The approach is integrated with HAT training, using task-specific adapters and classifiers while maintaining a replay buffer that includes an extra O class during training.

## Key Results
- TPLR achieves state-of-the-art average incremental accuracy of 76.21% across CIFAR-10, CIFAR-100, and TinyImageNet datasets
- Outperforms traditional OOD detection methods (68.69% with MLS alone) by leveraging replay buffer data
- Significant improvement in task-id prediction accuracy enables better class incremental learning performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional OOD detectors are suboptimal for task-id prediction because they ignore available replay data in CIL
- Mechanism: TPLR estimates both P(t) and P(tc) using replay data from previous tasks, enabling principled likelihood ratio computation
- Core assumption: The replay buffer contains representative samples from previous tasks that can be used to estimate P(tc)
- Evidence anchors:
  - [abstract]: "TPLR exploits replay data from previous tasks to estimate both P(t) and P(tc) and compute a principled likelihood ratio score"
  - [section 4.1]: "In CIL, the IND distribution Pt for task t can be interpreted as the marginal distribution PX(t), while Ptc corresponds to a mixture distribution PX(tc)"
- Break condition: If replay buffer data becomes too small or unrepresentative, the P(tc) estimation becomes unreliable

### Mechanism 2
- Claim: Combining feature-based and logit-based scores improves task-id prediction accuracy
- Mechanism: Energy-based model framework integrates likelihood ratio score with maximum logit score using exponential composition
- Core assumption: Different score types capture complementary information about task membership
- Evidence anchors:
  - [section 4.2.2]: "To further improve the task-id prediction score, we combine the feature-based SLR score with a logit based score"
  - [section 5.3]: "Utilizing the OOD detection method MLS (HAT+MLS) only improves the ACC to 68.69%. The final composition of LR and MLS boosted the performance to 76.21%"
- Break condition: If feature space becomes too noisy or logit scores become uninformative, the combination may degrade performance

### Mechanism 3
- Claim: Low-temperature softmax encourages confident task-id predictions
- Mechanism: Temperature parameter γ = 0.05 creates low entropy distributions, pushing predictions toward single tasks
- Core assumption: Confident task assignments are more useful than uncertain ones in CIL setting
- Evidence anchors:
  - [section 4.3]: "To encourage confident task-id prediction, we set a low temperature γ = 0.05"
  - [section 5.1]: "We convert the task-id prediction scores for all tasks to normalized probabilities via softmax with γ = 0.05"
- Break condition: If temperature is too low, model may become overconfident on ambiguous samples

## Foundational Learning

- Concept: Out-of-distribution detection
  - Why needed here: TPLR builds on OOD detection principles but adapts them for CIL's unique replay data availability
  - Quick check question: How does TPLR's use of replay data differ from traditional OOD detection approaches?

- Concept: Energy-based models
  - Why needed here: EBMs provide principled framework for combining different task-id prediction scores
  - Quick check question: What role do normalization constants play in the energy-based model formulation?

- Concept: Catastrophic forgetting and task isolation
  - Why needed here: HAT's masking mechanism prevents forgetting while allowing shared feature extractor
  - Quick check question: How does HAT's mask-based approach enable both plasticity and stability?

## Architecture Onboarding

- Component map:
  Pre-trained transformer backbone (fixed) -> Task-specific adapter layers with HAT masks -> Task classifiers -> Likelihood ratio computation module -> Energy-based score combination layer -> Temperature-controlled softmax layer

- Critical path:
  1. Extract features using task-specific adapters
  2. Compute MD and KNN scores for likelihood ratio
  3. Combine with logit-based score using EBM
  4. Apply temperature-controlled softmax for final task-id probabilities

- Design tradeoffs:
  - Replay buffer size vs. computational cost
  - Temperature parameter affects confidence vs. accuracy
  - Feature space dimensionality impacts KNN efficiency

- Failure signatures:
  - Degraded performance when replay buffer becomes too small
  - Confusion between similar tasks indicates temperature too high
  - Poor task-id predictions suggest feature space misalignment

- First 3 experiments:
  1. Compare TPLR with and without replay buffer to validate Mechanism 1
  2. Test different temperature values (0.01, 0.05, 0.1) to find optimal confidence level
  3. Evaluate impact of KNN k parameter on likelihood ratio estimation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can TPLR be adapted to online continual learning settings where data arrives in streams rather than in batch mode?
- Basis in paper: [explicit] The paper mentions online continual learning as a future direction, noting that data comes in streams rather than in batch mode.
- Why unresolved: The current TPLR implementation assumes batch training and access to all data for each task upfront, which is not feasible in online settings.
- What evidence would resolve it: A modified version of TPLR that processes data incrementally as it arrives, potentially using reservoir sampling or other online buffer management strategies, with experimental validation on streaming data benchmarks.

### Open Question 2
- Question: What alternative feature space estimation methods could replace MD and KNN for computing the likelihood ratio score while maintaining or improving performance?
- Basis in paper: [explicit] The paper mentions that "we can also use some other feature-based estimation methods instead of MD and KNN in SLR(x)" and conducts ablation studies with different methods.
- Why unresolved: While the paper uses MD and KNN as practical choices, it acknowledges that other estimation methods could be explored but doesn't systematically evaluate them.
- What evidence would resolve it: A comprehensive comparison of different feature space density estimation methods (e.g., normalizing flows, variational autoencoders, or other distance metrics) within the TPLR framework across multiple datasets.

### Open Question 3
- Question: How would different buffer management strategies affect TPLR's performance compared to the current random sampling approach?
- Basis in paper: [explicit] The paper states "TPLR uses a naive saving strategy that saves task data randomly to the replay buffer for simplicity" and mentions this as a limitation.
- Why unresolved: The paper acknowledges that the buffer management strategy could be improved but only tests the random sampling baseline.
- What evidence would resolve it: Experiments comparing TPLR with various buffer management strategies (e.g., class-balanced sampling, herding, or exemplar selection methods) while keeping all other components constant, measuring both task-id prediction accuracy and overall CIL performance.

## Limitations
- The method relies heavily on replay buffer quality and size for estimating P(tc) distribution, which may degrade in long task sequences where buffer becomes sparse
- The temperature parameter γ = 0.05 is fixed and may not be optimal across different datasets or task configurations
- Assumes pre-trained fixed transformer backbone is representative, limiting applicability to non-transformer architectures

## Confidence
- High confidence: TPLR significantly outperforms traditional OOD-based task-id prediction (verified by 76.21% Avg vs 68.69% with MLS alone)
- Medium confidence: Energy-based combination of feature and logit scores provides consistent improvement across all datasets
- Medium confidence: Low-temperature softmax improves CIL performance, though optimal temperature may vary

## Next Checks
1. Test TPLR with varying replay buffer sizes to quantify performance degradation as buffer becomes sparse
2. Evaluate sensitivity to temperature parameter γ across different task configurations and datasets
3. Validate whether the KNN-based P(tc) estimation remains stable when tasks have highly overlapping class distributions