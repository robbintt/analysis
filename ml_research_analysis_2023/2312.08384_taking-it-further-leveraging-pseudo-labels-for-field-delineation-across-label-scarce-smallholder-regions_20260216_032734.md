---
ver: rpa2
title: 'Taking it further: leveraging pseudo labels for field delineation across label-scarce
  smallholder regions'
arxiv_id: '2312.08384'
source_url: https://arxiv.org/abs/2312.08384
tags:
- labels
- field
- pseudo
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The scarcity of labeled data for smallholder agriculture in Sub-Saharan
  Africa remains a major bottleneck for large-area field delineation. This study explores
  opportunities of using sparse field delineation pseudo labels for fine-tuning models
  across geographies and sensor characteristics.
---

# Taking it further: leveraging pseudo labels for field delineation across label-scarce smallholder regions

## Quick Facts
- arXiv ID: 2312.08384
- Source URL: https://arxiv.org/abs/2312.08384
- Authors: 
- Reference count: 8
- Key outcome: Model fine-tuned with pseudo labels outperforms baseline pre-trained model in smallholder field delineation across geographies.

## Executive Summary
This study addresses the critical bottleneck of limited labeled data for smallholder agriculture in Sub-Saharan Africa by leveraging sparse pseudo labels for cross-geographic field delineation. Building on a FracTAL ResUNet pre-trained in India, the researchers generated pseudo labels in Mozambique and systematically evaluated different selection strategies. The approach demonstrates that pseudo labels can effectively complement scarce human annotations, enabling significant performance improvements in field delineation tasks across regions with different field sizes and characteristics.

## Method Summary
The research employed a pre-trained FracTAL ResUNet model originally trained on Indian smallholder fields to generate pseudo labels for field delineation in Mozambique. Using Very High Resolution (VHR) RGB imagery from Google Earth Pro, the team created predictions and performed instance generation through hierarchical watershed segmentation. Multiple pseudo label selection strategies were tested, including absolute probability thresholds and adaptive thresholds based on site-level confidence distributions. The human-annotated labels and pseudo labels were then used for model fine-tuning, with performance evaluated against independent human field annotations using both object-level spatial agreement metrics (mIoU, IoU50, Precision, Recall) and site-level field size estimation metrics (mRMSE, mMAE, RÂ²).

## Key Results
- The pre-trained model demonstrated strong baseline performance in both field delineation and field size estimation when applied to Mozambique data
- Fine-tuning with pseudo labels alone showed substantial performance improvements across nearly all evaluation metrics
- Combining human-annotated labels with pseudo labels for additional sites produced the best overall performance, outperforming both the pre-trained model and models fine-tuned with human labels alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained FracTAL ResUNet generates accurate pseudo labels that closely match human annotations in terms of field boundaries and sizes.
- Mechanism: The pre-trained model encodes domain-invariant features from India's smallholder fields and generalizes to Mozambique's structurally similar but different fields, allowing it to produce confident predictions on unlabeled data.
- Core assumption: The basic properties of field appearance across geographies are similar enough for the pre-trained model to generate accurate pseudo labels without fine-tuning.
- Evidence anchors:
  - [abstract]: "We build on a FracTAL ResUNet trained for crop field delineation in India... and use this pre-trained model to generate pseudo labels in Mozambique."
  - [section]: "We used the model to create predictions of cropland and boundary probabilities for our 200 training sites, and performed instance generation from the predictions based on hierarchical watershed segmentation."
  - [corpus]: Weak - corpus lacks direct evidence on model performance for field delineation across geographies.
- Break condition: If the pre-trained model's IoU score on Mozambique data falls below 0.5, indicating poor generalization.

### Mechanism 2
- Claim: Adaptive threshold-based pseudo label selection increases diversity and improves model fine-tuning performance compared to absolute thresholds.
- Mechanism: Adaptive thresholds select the top percentile of predictions per site, allowing for site-specific confidence levels and a more diverse set of pseudo labels.
- Core assumption: Site-level variation in prediction confidence is significant enough to warrant adaptive thresholds for optimal pseudo label selection.
- Evidence anchors:
  - [abstract]: "We designed multiple pseudo label selection strategies based on field-level probability scores and compared the quantities, area properties, seasonal distribution, and spatial agreement of the pseudo labels against human-annotated training labels."
  - [section]: "We tested four absolute probability thresholds... and adaptive thresholds based on the site-level distribution of confidence scores."
  - [corpus]: Weak - corpus lacks direct evidence on the impact of adaptive thresholds on model performance.
- Break condition: If the model's performance using adaptive thresholds is not significantly better than using absolute thresholds.

### Mechanism 3
- Claim: Combining human-annotated labels with pseudo labels for additional sites further improves model performance.
- Mechanism: Human labels provide high-quality training data for a subset of sites, while pseudo labels expand the training data to additional sites, increasing diversity and coverage.
- Core assumption: Human labels and pseudo labels are complementary, and their combination leads to better generalization than either alone.
- Evidence anchors:
  - [abstract]: "We then used the human-annotated labels and the pseudo labels for model fine-tuning and compared predictions against human field annotations."
  - [section]: "Combining human-annotated data with pseudo labels for additional sites improved the performance of the model across all scores."
  - [corpus]: Weak - corpus lacks direct evidence on the impact of combining human and pseudo labels on model performance.
- Break condition: If the model's performance using combined labels is not significantly better than using human labels alone.

## Foundational Learning

- Concept: Transfer Learning
  - Why needed here: To leverage a pre-trained model's knowledge and adapt it to a new, related task with limited labeled data.
  - Quick check question: What are the two main types of transfer learning, and how do they differ in terms of data requirements?

- Concept: Semi-supervised Learning
  - Why needed here: To utilize a large amount of unlabeled data (pseudo labels) in conjunction with limited labeled data for model training.
  - Quick check question: What are the key challenges in semi-supervised learning, and how do they relate to the use of pseudo labels?

- Concept: Domain Adaptation
  - Why needed here: To address the differences between the source domain (India) and target domain (Mozambique) in terms of field sizes, shapes, and image characteristics.
  - Quick check question: What are the main strategies for domain adaptation, and how do they apply to the field delineation task?

## Architecture Onboarding

- Component map: Pre-trained model -> Pseudo label generation -> Instance segmentation -> Confidence-based selection -> Fine-tuning with human/pseudo labels -> Performance evaluation
- Critical path: The generation and selection of high-quality pseudo labels, as they directly impact the model's ability to learn and generalize to the target domain
- Design tradeoffs: The main tradeoff is between the quantity and quality of pseudo labels. Conservative selection strategies (e.g., high confidence thresholds) may yield fewer but more accurate pseudo labels, while liberal strategies may increase quantity but introduce more noise
- Failure signatures: If the model's performance is poor, potential failure points include inaccurate pseudo label generation, inappropriate pseudo label selection, or insufficient human-annotated labels for fine-tuning
- First 3 experiments:
  1. Generate pseudo labels using the pre-trained model and evaluate their spatial agreement with human-annotated labels
  2. Fine-tune the model using human-annotated labels only and assess performance gains over the pre-trained model
  3. Fine-tune the model using pseudo labels selected with different strategies (e.g., absolute vs. adaptive thresholds) and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of field delineation models change when using pseudo labels generated from different types of pre-trained models, such as those trained on larger fields or different crop types?
- Basis in paper: [explicit] The study uses a pre-trained FracTAL ResUNet model trained on Indian smallholder fields, but does not explore the impact of using models trained on different field sizes or crop types.
- Why unresolved: The paper focuses on a single pre-trained model and does not compare the performance of models trained on different datasets or with different characteristics.
- What evidence would resolve it: Comparative studies using pre-trained models with varying training data characteristics (field sizes, crop types, regions) to assess the impact on model performance when fine-tuned with pseudo labels.

### Open Question 2
- Question: What is the optimal balance between the number of human-annotated labels and pseudo labels for achieving the best performance in field delineation tasks?
- Basis in paper: [explicit] The study explores the use of pseudo labels as a complement to human-annotated data, but does not determine the optimal ratio for achieving the best performance.
- Why unresolved: The paper provides insights into the benefits of combining human and pseudo labels but does not quantify the ideal balance for different scenarios or datasets.
- What evidence would resolve it: Experimental studies varying the ratio of human-annotated to pseudo labels in model training to identify the optimal balance for different field sizes, regions, and image resolutions.

### Open Question 3
- Question: How does the performance of pseudo label-based field delineation models generalize to different smallholder farming systems with varying levels of fragmentation and field sizes?
- Basis in paper: [inferred] The study focuses on smallholder farming in Mozambique, but does not assess the generalizability of the approach to other regions with different farming systems or field characteristics.
- Why unresolved: The paper demonstrates the effectiveness of pseudo labels in a specific context but does not explore the approach's robustness across diverse smallholder farming systems.
- What evidence would resolve it: Comparative studies applying the pseudo label approach to multiple smallholder farming regions with varying field sizes, fragmentation levels, and crop types to evaluate the generalizability of the method.

## Limitations
- Cross-geographic generalization relies on visual similarity assumptions without formal domain gap quantification
- Pseudo label quality assessment depends on potentially biased or error-containing human annotations
- Exclusive focus on crop fields limits applicability to mixed-use landscapes without non-cropland examples
- Significant hardware requirements (3xA100 GPUs) and data access barriers create reproduction challenges

## Confidence
- High confidence: Performance improvements from fine-tuning (both with human and pseudo labels) - supported by multiple comparative experiments and clear statistical significance
- Medium confidence: Cross-geographic transfer capability of pre-trained model - reasonable given visual similarity but lacks formal domain adaptation analysis
- Medium confidence: Adaptive threshold superiority over absolute thresholds - demonstrated but requires more rigorous statistical validation

## Next Checks
1. Conduct systematic error analysis on failure cases where pre-trained model underperforms in Mozambique to quantify domain gap and identify specific failure patterns
2. Perform ablation study varying pseudo label selection strategies (minimum size thresholds, confidence score distributions) to optimize trade-off between quantity and quality
3. Test model performance on a held-out validation set from India to establish baseline generalization capability before cross-geographic transfer, enabling more rigorous comparison of transfer performance