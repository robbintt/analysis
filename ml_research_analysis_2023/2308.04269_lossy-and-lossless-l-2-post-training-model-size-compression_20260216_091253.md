---
ver: rpa2
title: Lossy and Lossless (L$^2$) Post-training Model Size Compression
arxiv_id: '2308.04269'
source_url: https://arxiv.org/abs/2308.04269
tags:
- compression
- quantization
- ratio
- weight
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified post-training method combining lossy
  and lossless compression for deep neural networks. It introduces a parametric weight
  transformation to jointly optimize pruning and quantization, and a differentiable
  counter to guide optimization for better lossless compression compatibility.
---

# Lossy and Lossless (L$^2$) Post-training Model Size Compression

## Quick Facts
- arXiv ID: 2308.04269
- Source URL: https://arxiv.org/abs/2308.04269
- Authors: 
- Reference count: 40
- Achieves 10× compression without accuracy loss and 20× with minor loss on various architectures

## Executive Summary
This paper proposes a unified post-training method that combines lossy and lossless compression for deep neural networks. The approach introduces a parametric weight transformation to jointly optimize pruning and quantization, along with a differentiable counter to guide optimization for better lossless compression compatibility. The method achieves stable 10× compression without accuracy loss and 20× compression with minor accuracy loss across various architectures including classification and object detection tasks.

## Method Summary
The method unifies different lossy compression techniques (pruning and quantization) through a parametric weight transformation T(w) that can express both operations as variations of a single rounding operation. A differentiable counter with kernel relaxation enables entropy regularization to be differentiable, guiding weights to be more compressible by lossless methods. The approach combines these components into a joint optimization framework that minimizes accuracy loss while maximizing compression ratio, followed by range coding for final lossless compression.

## Key Results
- Achieves stable 10× compression ratio without accuracy loss
- Reaches 20× compression with minor accuracy degradation
- Validated across multiple architectures including ResNet, MobileNet, ViT, and YOLOv5
- Effective for both image classification and object detection tasks

## Why This Works (Mechanism)

### Mechanism 1
The unified parametric weight transformation allows different lossy compression methods to be jointly optimized in a single stage. By expressing both pruning and quantization as variations of a continuous rounding operation ŵ = T⁻¹(⌊T(w)⌉), the method creates a single optimization problem. This assumes a continuous piece-wise function can effectively represent both discrete operations without losing representational capacity.

### Mechanism 2
The differentiable counter enables entropy regularization to be differentiable, guiding weights to be more compressible by lossless methods. Using a kernel function K(x) to relax the step function in cumulative distribution, the counting operation becomes differentiable. This allows the probability mass function to be learned during optimization, encouraging weights to cluster in ways that reduce entropy.

### Mechanism 3
Joint optimization of pruning and quantization with entropy regularization leads to more compressible weight distributions. By minimizing the entropy term, weights are encouraged to take on fewer distinct values and become sparser, improving effectiveness of subsequent lossless compression. This assumes reducing entropy during lossy compression improves compressibility without sacrificing accuracy beyond acceptable limits.

## Foundational Learning

- Concept: Entropy and information theory
  - Why needed here: Entropy measures average shortest coding length for lossless compression. The method uses entropy regularization to guide weights to be more compressible.
  - Quick check question: What is the relationship between entropy and compression ratio in lossless coding?

- Concept: Differentiable programming and relaxations
  - Why needed here: The differentiable counter uses kernel functions to relax discrete operations to make them differentiable, enabling gradient-based optimization.
  - Quick check question: How does choice of kernel function affect smoothness and bias of relaxed operation?

- Concept: Joint optimization and multi-objective learning
  - Why needed here: The method optimizes multiple objectives (accuracy, compression ratio, entropy) simultaneously by combining them into single loss function.
  - Quick check question: How does balancing different terms in multi-objective loss function affect convergence and final solution?

## Architecture Onboarding

- Component map: Unified weight transformation (T(·) and T⁻¹(·)) -> Differentiable counter (kernel-based counting) -> Entropy regularization term -> Knowledge distillation fine-tuning layers -> Lossless compression backend (range coding)

- Critical path:
  1. Initialize transformation parameters (e, s for each layer)
  2. Forward pass: Apply T(·) → round → T⁻¹(·) to get compressed weights
  3. Compute differentiable count via kernel relaxation
  4. Calculate entropy and add to loss
  5. Backpropagate and update transformation parameters and weights
  6. Apply range coding to compressed weights for final model

- Design tradeoffs:
  - Kernel relaxation factor δ: Smaller δ gives more accurate count but less smooth gradients; larger δ gives smoother gradients but more bias
  - Choice of T(·) function: Linear quantization is simple but may not compress as well as joint pruning+quantization
  - Calibration dataset size: Larger datasets improve accuracy but increase time; too small may hurt performance

- Failure signatures:
  - Weights collapse to zero: Indicates over-pruning or too aggressive entropy regularization
  - No compression gain: Suggests differentiable counter is not effectively guiding optimization
  - Training instability: May result from poor choice of kernel function or relaxation factor

- First 3 experiments:
  1. Run with only quantization (no pruning) and linear T(·) on small model to verify basic functionality
  2. Test different kernel functions (cosine, linear, triangle) with varying δ on ResNet-18 to find best configuration
  3. Compare compression ratios and accuracy with and without entropy regularization to confirm its effect

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method perform on larger and more complex architectures such as Vision Transformers with different patch sizes or hybrid models? The paper mentions applying the method to ViT-base-patch16 but does not explore other ViT variants or hybrid CNN-ViT models. Comprehensive experiments on multiple ViT variants and hybrid models would demonstrate the method's scalability across different architectures.

### Open Question 2
What is the impact of different kernel functions and relaxation factors on the differentiable counter for extremely high compression ratios? The paper explores different kernel functions and resolutions but does not investigate their performance at very high compression ratios (e.g., 30× or 50×). Systematic evaluation at extreme compression levels would reveal optimal settings for pushing compression limits while maintaining accuracy.

### Open Question 3
How does the proposed method compare to other state-of-the-art compression techniques when applied to dense networks like MLPs or graph neural networks? The paper focuses on convolutional and transformer architectures but does not discuss application to dense networks or graph neural networks. Direct comparisons on dense networks and GNNs would demonstrate versatility across different network types.

## Limitations
- The unified weight transformation relies on continuous approximation that may not perfectly capture discrete nature of quantization and pruning
- Differentiable counter introduces approximation error through kernel relaxation, potentially affecting entropy estimation accuracy
- Knowledge distillation component uses intermediate layer outputs with unspecified architectural choices and distillation formulation

## Confidence
**High confidence** in core claim that joint optimization achieves stable compression ratios. Mathematical framework is well-defined and theoretically sound with consistent empirical results.
**Medium confidence** in differentiable counter mechanism. Theoretical foundation is established but practical implementation details significantly impact performance.
**Medium confidence** in scalability claims. Strong results on ResNet and MobileNet, but transformer and detection model performance needs further validation.

## Next Checks
1. **Kernel sensitivity analysis**: Systematically vary kernel relaxation factor δ and test different kernel functions on ResNet-18 to quantify trade-off between counting accuracy and gradient smoothness, measuring effects on compression ratio and accuracy preservation.

2. **Architecture-specific ablation**: Compare compression performance when using only quantization versus joint pruning+quantization across all tested architectures to isolate contribution of each compression component.

3. **Dataset size scaling study**: Evaluate method's sensitivity to calibration dataset size by testing with 100, 1000, 5000, and 10000 images on MobileNetV2, measuring accuracy degradation and compression ratio changes to determine minimum effective dataset size.