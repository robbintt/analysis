---
ver: rpa2
title: Investigating Human-Identifiable Features Hidden in Adversarial Perturbations
arxiv_id: '2309.16878'
source_url: https://arxiv.org/abs/2309.16878
tags:
- perturbations
- features
- attack
- adversarial
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates human-identifiable features hidden in
  adversarial perturbations. The authors average perturbations from multiple models
  to reduce noise and assemble incomplete features, revealing two types of human-identifiable
  features: masking effect (in untargeted attacks) and generation effect (in targeted
  attacks).'
---

# Investigating Human-Identifiable Features Hidden in Adversarial Perturbations

## Quick Facts
- **arXiv ID**: 2309.16878
- **Source URL**: https://arxiv.org/abs/2309.16878
- **Reference count**: 21
- **Primary result**: Human-identifiable features (masking and generation effects) can be revealed in adversarial perturbations through averaging across multiple models, with contour features being more effective at attacking neural networks than background features.

## Executive Summary
This study investigates hidden human-identifiable features within adversarial perturbations that contribute to fooling neural networks. The authors propose a method to average perturbations from multiple models, reducing noise and assembling incomplete features that become recognizable to humans. They identify two distinct types of human-identifiable features: masking effects (in untargeted attacks) that reduce contrast in key classification features, and generation effects (in targeted attacks) that add new features mimicking different classes. Experiments across ImageNet, CIFAR-10, and MNIST datasets with five attack algorithms demonstrate that contours are more effective than backgrounds at attacking models, and perturbations from different algorithms converge when averaged across models. These findings offer insights into both transferability and model interpretability phenomena.

## Method Summary
The study generates adversarial perturbations using multiple models and attack algorithms (BIM, CW, DeepFool, Square, One-pixel) on ImageNet, CIFAR-10, and MNIST datasets. Perturbations are created in two settings: single model (SM) and multiple models with Gaussian noise (MM+G). The MM+G approach averages perturbations across models with added Gaussian noise to reduce noise and assemble incomplete features. Pixel-level annotations extract contour features from perturbations, which are then evaluated for recognizability through human and machine assessments. Attack strength is measured by applying processed perturbations to test images and measuring classification accuracy drops using a VGG-16 model.

## Key Results
- Averaging perturbations from multiple models reveals human-identifiable features by reducing noise and assembling incomplete information
- Two types of human-identifiable features emerge: masking effects (untargeted attacks) and generation effects (targeted attacks)
- Contour features of perturbations are more effective at attacking models than background features
- Perturbations from different attack algorithms show higher cosine similarity when averaged across models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Averaging perturbations from multiple models reduces noise and assembles incomplete human-identifiable features
- Mechanism: Different models produce perturbations with independent noise patterns. When averaged, the noise cancels out while the common human-identifiable features (present across models) reinforce each other.
- Core assumption: Noise in perturbations from different models is independent and human-identifiable features are shared across models
- Evidence anchors:
  - [abstract]: "averaging perturbations from multiple models to reduce noise and assemble incomplete features"
  - [section]: "Since different neural networks usually generate perturbations with noise and incomplete information different from each other, averaging these perturbations...effectively minimizes the effects of noise"
  - [corpus]: Weak evidence - no direct citations in corpus about this averaging mechanism
- Break condition: If models produce correlated noise patterns or if human-identifiable features are highly model-specific

### Mechanism 2
- Claim: Masking effect perturbations resemble original image features with sign inversion, reducing contrast in key classification features
- Mechanism: Perturbations contain features similar to the original image but inverted. When added to the original image, they decrease pixel value contrast in classification-relevant regions, making the model more likely to misclassify.
- Core assumption: Neural networks rely on pixel value contrast in human-identifiable features for classification
- Evidence anchors:
  - [abstract]: "masking effect...prominent in untargeted attacks"
  - [section]: "The masking effect includes important features from the input image, potentially with a sign inversion"
  - [corpus]: No direct corpus evidence supporting this specific mechanism
- Break condition: If models use features that are robust to local contrast changes

### Mechanism 3
- Claim: Generation effect perturbations add new features that mimic a different class, causing misclassification
- Mechanism: Targeted attacks generate perturbations that introduce features characteristic of a different target class. These new features are convincing enough to fool both humans and models into perceiving a different object class.
- Core assumption: Neural networks can be fooled by addition of convincing class-specific features
- Evidence anchors:
  - [abstract]: "generation effect...more common in targeted attacks"
  - [section]: "The generation effect adds new features to the original image, simulating a different class"
  - [corpus]: Weak evidence - related papers discuss adversarial attacks but not specifically generation effect
- Break condition: If models rely on features that are invariant to addition of plausible class features

## Foundational Learning

- Concept: Adversarial perturbations
  - Why needed here: Understanding what adversarial perturbations are and how they work is fundamental to grasping the paper's contribution
  - Quick check question: What is the difference between untargeted and targeted adversarial attacks?

- Concept: Cosine similarity in perturbation analysis
  - Why needed here: Used to quantify similarity between perturbations from different attack algorithms
  - Quick check question: How does cosine similarity differ from Euclidean distance when comparing perturbations?

- Concept: Pixel-level annotation and contour extraction
  - Why needed here: Critical for isolating human-identifiable features (contours) from background in the experiments
  - Quick check question: Why might contours be more important for classification than backgrounds in natural images?

## Architecture Onboarding

- Component map: Image → Source models → Perturbation generation → Gaussian noise addition → Averaging → Testing models → Evaluation
- Critical path: Image → Source models → Perturbation generation → Gaussian noise addition → Averaging → Testing models → Evaluation
- Design tradeoffs: Multiple models increase computational cost but improve feature visibility; Gaussian noise helps but isn't essential; contour extraction focuses on important features but may miss some
- Failure signatures: If perturbations don't reveal human-identifiable features, check if enough models were averaged; if attack strength is low, verify perturbation processing parameters
- First 3 experiments:
  1. Generate perturbations from single model vs multiple models averaging to observe feature emergence
  2. Apply contour extraction and test attack strength difference between contour and background perturbations
  3. Calculate cosine similarity between perturbations from different attack algorithms to verify convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise mechanisms by which human-identifiable features in adversarial perturbations cause misclassification in neural networks?
- Basis in paper: [explicit] The paper demonstrates that perturbations containing human-identifiable features are more effective at attacking models, but does not fully explain the underlying mechanisms.
- Why unresolved: While the paper shows that contours of perturbations are more effective than backgrounds, the specific neural network processes that make these features particularly deceptive are not detailed.
- What evidence would resolve it: Experiments isolating the interaction between human-identifiable features and specific layers or neurons in neural networks, showing how these features disrupt the decision-making process.

### Open Question 2
- Question: How do different types of neural network architectures respond to perturbations with human-identifiable features?
- Basis in paper: [inferred] The paper tests perturbations on various architectures (VGG-16, ResNet-50, DenseNet-121, BN-Inception) but does not deeply analyze how architectural differences affect vulnerability to human-identifiable features.
- Why unresolved: The study shows that perturbations can fool different models, but does not investigate whether certain architectures are more or less susceptible to attacks based on human-identifiable features.
- What evidence would resolve it: Comparative analysis of attack success rates across a broader range of architectures, identifying patterns in which types of networks are most vulnerable to human-identifiable feature-based attacks.

### Open Question 3
- Question: Can the presence of human-identifiable features in perturbations be leveraged to develop more effective defense mechanisms?
- Basis in paper: [explicit] The paper suggests that understanding human-identifiable features can lead to more resilient defense strategies, but does not explore specific defense techniques.
- Why unresolved: While the paper identifies the role of human-identifiable features in attacks, it does not propose or test methods to detect or neutralize these features as a defense strategy.
- What evidence would resolve it: Development and testing of defense algorithms that specifically target the detection and removal of human-identifiable features from perturbations, measuring their effectiveness in preventing attacks.

## Limitations
- Reliance on averaged perturbations across multiple models assumes independence of noise patterns, which requires further validation
- Human evaluation introduces subjectivity that may vary across different participant groups
- Focus primarily on image classification tasks limits generalizability to other domains like object detection or segmentation

## Confidence
- High confidence: The observation that contours are more effective than backgrounds in adversarial attacks
- Medium confidence: The mechanism by which averaging reduces noise and reveals features
- Medium confidence: The existence of masking and generation effects in different attack types
- Low confidence: The specific quantitative thresholds for human recognizability

## Next Checks
1. Conduct ablation studies testing perturbation averaging with varying numbers of models to determine the minimum required for reliable feature emergence
2. Test the robustness of identified human-identifiable features across different participant pools and cultural contexts
3. Verify the independence assumption by analyzing correlation patterns in noise across different model architectures and training procedures