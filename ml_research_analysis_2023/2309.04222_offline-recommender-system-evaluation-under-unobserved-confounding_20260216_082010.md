---
ver: rpa2
title: Offline Recommender System Evaluation under Unobserved Confounding
arxiv_id: '2309.04222'
source_url: https://arxiv.org/abs/2309.04222
tags:
- unobserved
- confounding
- policy
- data
- propensities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper highlights the critical problem of unobserved confounding\
  \ in offline recommender system evaluation, particularly when using policy-based\
  \ estimators with learned propensities. Through theoretical analysis and synthetic\
  \ data experiments, the authors demonstrate that na\xEFve propensity estimation\
  \ under confounding leads to severely biased metric estimates, potentially causing\
  \ incorrect identification of optimal policies."
---

# Offline Recommender System Evaluation under Unobserved Confounding

## Quick Facts
- arXiv ID: 2309.04222
- Source URL: https://arxiv.org/abs/2309.04222
- Reference count: 31
- Key outcome: Unobserved confounding in offline recommender evaluation leads to non-identifiable bias that existing diagnostics cannot detect

## Executive Summary
This paper exposes a critical vulnerability in offline recommender system evaluation: when using policy-based estimators with learned propensities, unobserved confounding creates bias that cannot be detected by standard diagnostics. Through theoretical analysis and synthetic experiments, the authors demonstrate that even when evaluation metrics pass all conventional tests, the estimates can be severely biased, potentially leading to incorrect policy selection. The core insight is that confounding bias is non-identifiable because it depends on the true (unobserved) logging propensities, making the commonly assumed unconfoundedness assumption untestable in practice.

## Method Summary
The authors analyze offline evaluation using Inverse Propensity Scoring (IPS) when logging propensities are estimated from data containing unobserved confounders. They construct a synthetic recommendation setting with two actions and a binary covariate that influences both action selection and rewards. By parameterizing the degree of confounding (Œ±) and selection bias (Œµ), they generate data following specific distributions and compare ideal IPS estimates (using true propensities) against estimated IPS (using empirically learned propensities). The experiments systematically vary these parameters to demonstrate how confounding bias can reverse policy rankings and create Simpson's Paradox scenarios where aggregated metrics misrepresent true performance.

## Key Results
- Confounding bias in IPS estimators is non-identifiable when logging propensities are unobserved
- Existing diagnostics for logged bandit feedback cannot detect confounding bias because empirical counting of propensities yields unbiased marginal estimates regardless of confounding
- Simpson's Paradox can manifest in recommendation evaluation, where aggregated metrics reverse the true relationship between actions and rewards
- Na√Øve propensity estimation under confounding leads to severely biased metric estimates that can incorrectly identify optimal policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confounding bias in IPS estimators is non-identifiable because it depends on unobserved logging propensities.
- Mechanism: When logging propensities are learned from data containing unobserved confounders, the estimated propensities cannot recover the true marginal distribution, creating systematic bias in importance weights that cancels out in expectation.
- Core assumption: The unconfoundedness assumption is violated but this violation cannot be detected from available data alone.
- Evidence anchors: [abstract]: "Because the bias depends directly on the true and unobserved logging propensities, it is non-identifiable."; [section 3]: "As we define the logging propensities to be equal to the empirical action frequencies, it should be clear that this test will trivially pass."
- Break condition: Additional assumptions about confounding structure or external data (interventions, instrumental variables) become available.

### Mechanism 2
- Claim: Existing diagnostics for logged bandit feedback cannot detect confounding bias.
- Mechanism: Standard diagnostics check whether empirical action frequencies match estimated propensities or whether logging policies have full support. When propensities are estimated by counting logged actions, these estimates are unbiased for marginal probabilities regardless of confounding, causing diagnostics to pass even with severe bias.
- Core assumption: Logging propensities are estimated using empirical counting methods that ignore confounders.
- Evidence anchors: [section 3]: "As a result, the diagnostics that were proposed will not detect confounding bias."; [section 3]: "We can show that the control variate remains unbiased as well (Eq. 8)."
- Break condition: Alternative estimation methods for propensities that don't rely on empirical counting.

### Mechanism 3
- Claim: Simpson's Paradox manifests in recommendation evaluation when unobserved confounders create selection bias.
- Mechanism: When a contextual variable influences both action selection and rewards, averaging over this variable can reverse the apparent relationship between actions and rewards. The logging policy's contextual decisions create strata where actions appear better or worse than they actually are when evaluated globally.
- Core assumption: A binary covariate creates different reward distributions for different actions, and the logging policy's selection probabilities vary with this covariate.
- Evidence anchors: [abstract]: "In the extreme case where the estimate changes sign, this is known as 'Simpson's Paradox' [13]."; [section 2]: "We can map this to an intuitive setting: action ùëé1 is of general appeal to the entire population (i.e. ùëÖ ‚ä• ‚ä•ùëã |ùê¥ = ùëé1); whereas action ùëé0, on the other hand, is specifically appealing to a more niche user-base."
- Break condition: When the confounding variable has no effect on rewards, or when the logging policy is uniform across all contexts.

## Foundational Learning

- Concept: Counterfactual reasoning and do-calculus
  - Why needed here: The paper uses Pearl's do-calculus framework to explain why confounding creates bias in off-policy evaluation. Understanding intervention vs observation is crucial for grasping why IPS with estimated propensities fails.
  - Quick check question: What is the difference between P(R|A=a) and P(R|do(A=a)) in the presence of unobserved confounders?

- Concept: Inverse Propensity Scoring (IPS) estimation
  - Why needed here: IPS is the core method being analyzed, and understanding how it works (and fails) under confounding is essential for implementing experiments and interpreting results.
  - Quick check question: Why does the ideal IPS estimator require access to contextual covariates and true logging propensities?

- Concept: Unconfoundedness assumption and its limitations
  - Why needed here: The paper's central argument is that unconfoundedness is untestable and problematic. Engineers need to understand what this assumption means and why it's commonly violated in practice.
  - Quick check question: What makes the unconfoundedness assumption "famously untestable" in causal inference?

## Architecture Onboarding

- Component map: Data collection ‚Üí Propensity estimation ‚Üí Off-policy estimation ‚Üí Evaluation
- Critical path: Data collection ‚Üí Propensity estimation ‚Üí IPS estimation
  - The quality of propensity estimates directly determines bias in final evaluation
  - Any improvement in propensity estimation methodology propagates to better evaluations
- Design tradeoffs: Simplicity vs. bias
  - Simple empirical counting of propensities is easy but introduces confounding bias
  - More sophisticated methods require additional assumptions or data but can reduce bias
- Failure signatures: Passing standard diagnostics while having biased estimates
  - If control variate is close to 1 and action frequencies match estimated propensities, but estimates disagree with online A/B tests
  - If different target policies yield systematically biased comparisons
- First 3 experiments:
  1. Implement the synthetic data generation from Table 1 with varying Œ± and Œµ parameters to reproduce Figure 2
  2. Add standard diagnostics (arithmetic/harmonic mean tests) to verify they pass even when bias is present
  3. Implement an alternative propensity estimation method that uses domain knowledge to partially address confounding and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective methods to detect and mitigate unobserved confounding bias in offline recommender system evaluation when logging propensities are learned from data?
- Basis in paper: [explicit] The paper identifies that existing diagnostics fail to detect confounding bias and that the bias is non-identifiable without additional assumptions or data.
- Why unresolved: Current methods cannot distinguish between selection bias and confounding bias, and the bias depends on unobserved true logging propensities.
- What evidence would resolve it: Development and validation of new diagnostic tools or methods that can reliably detect confounding bias, or empirical studies demonstrating successful mitigation strategies.

### Open Question 2
- Question: How can instrumental variables or additional interventional data be leveraged to improve off-policy evaluation under unobserved confounding in recommendation systems?
- Basis in paper: [explicit] The paper mentions that instrumental variables have been used to test for unconfoundedness and suggests this as a potential research direction.
- Why unresolved: The effectiveness of these methods in recommendation contexts is not explored, and their practical implementation remains unclear.
- What evidence would resolve it: Experimental results showing improved evaluation accuracy using instrumental variables or interventional data in real or synthetic recommendation datasets.

### Open Question 3
- Question: What are the practical implications of unobserved confounding on policy selection and recommendation system deployment decisions?
- Basis in paper: [explicit] The paper demonstrates through synthetic data that confounding can lead to severely biased metric estimates that incorrectly identify optimal policies.
- Why unresolved: While the theoretical bias is characterized, the real-world impact on system performance and user experience is not quantified.
- What evidence would resolve it: Case studies or A/B test results comparing policies selected through confounded versus debiased evaluation methods in production systems.

## Limitations
- The analysis relies heavily on synthetic data generation, which may not fully capture real-world recommendation system complexity
- The paper demonstrates theoretical mechanisms clearly but doesn't explore practical mitigation strategies beyond highlighting the problem
- Real-world validation is limited, as neighboring work in the corpus shows weak direct connections to this contribution

## Confidence
- **High confidence**: The core claim that confounding bias is non-identifiable when logging propensities are unobserved is well-supported by theoretical analysis and synthetic experiments
- **Medium confidence**: The assertion that existing diagnostics fail to detect confounding bias is demonstrated through synthetic example, though real-world validation is limited
- **Medium confidence**: The practical implications for recommender system evaluation are logically sound but haven't been extensively validated on real datasets

## Next Checks
1. Apply the methodology to a real recommendation dataset with known confounders (e.g., user demographics) to verify that theoretical bias manifests in practice and whether standard diagnostics fail as predicted
2. Implement and test at least one deconfounding method (e.g., two-way deconfounder or instrumental variable approaches) to assess whether bias can be reduced in practice and what assumptions are required
3. Design and evaluate new diagnostic tests that could detect confounding bias by examining stability of estimates across different data subsets or testing for Simpson's Paradox patterns in evaluation metrics