---
ver: rpa2
title: 'UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic
  Prompt'
arxiv_id: '2309.11065'
source_url: https://arxiv.org/abs/2309.11065
tags:
- prompts
- task
- pages
- language
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes UniPCM, a universal pre-trained conversation
  model that addresses the limitations of previous multi-task pre-training approaches
  which rely heavily on human-defined input formats or prompts. The core idea is to
  use Task-aware Automatic Prompt generation (TAP) to automatically generate high-quality
  prompts, which enables scaling the pre-training corpus to 122 datasets from 15 dialog-related
  tasks.
---

# UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt

## Quick Facts
- arXiv ID: 2309.11065
- Source URL: https://arxiv.org/abs/2309.11065
- Reference count: 40
- Key outcome: Achieves state-of-the-art results on 9 datasets and demonstrates strong transfer ability, especially in low-resource scenarios.

## Executive Summary
This paper introduces UniPCM, a universal pre-trained conversation model that addresses limitations of previous multi-task pre-training approaches which rely heavily on human-defined input formats or prompts. The core innovation is Task-aware Automatic Prompt generation (TAP), which automatically generates high-quality prompts by leveraging extracted task information and input-output pairs. By scaling the pre-training corpus to 122 datasets from 15 dialog-related tasks, UniPCM achieves state-of-the-art results across multiple datasets and demonstrates superior performance in few-shot learning scenarios. The paper also shows that TAP-generated prompts are on par with those collected through crowdsourcing.

## Method Summary
UniPCM is built on a T5 encoder-decoder architecture and employs a Task-aware Automatic Prompt generation (TAP) method to automatically generate task-centric prompts. The TAP method extracts keywords from task descriptions, paraphrases task names for diversity, and uses a pre-trained T5 model to fill in templates with these keywords and example pairs. Generated prompts are filtered and scored based on perplexity across the task corpus and potential bias toward output labels. The model is pre-trained using a multi-prompt training mechanism that applies multiple prompts to each training instance, aggregating log probabilities to encourage learning task semantics rather than prompt-specific patterns. For few-shot scenarios, PET (Pattern-Exploit Training) is employed, where multiple prompts are used to train voting models that generate pseudo-labels for semi-supervised fine-tuning.

## Key Results
- Achieves state-of-the-art results on 9 different datasets from various dialog-related tasks
- Demonstrates strong transfer ability, especially in low-resource scenarios with few-shot learning
- Shows that TAP-generated prompts are on par with those collected through crowdsourcing
- Outperforms T5 and Flan-T5 baselines by significant margins across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-aware automatic prompt generation (TAP) creates prompts that are both task-specific and coherent by leveraging extracted task information and input-output pairs.
- Mechanism: TAP extracts keywords from task descriptions, paraphrases task names for diversity, and uses a pre-trained T5 model to fill in templates with these keywords and example pairs. Prompts are then filtered and scored based on their perplexity across the task corpus and their potential bias toward output labels.
- Core assumption: Task information encoded in prompts is essential for models to understand and generalize across tasks, and T5 can generate coherent prompts when guided by task-specific keywords.
- Evidence anchors:
  - [abstract]: "We propose to use Task-based Automatic Prompt generation (TAP) to automatically generate high-quality prompts."
  - [section]: "Our TAP method leverages task-related information to generate task-centric prompts and the quality is ensured by scoring and filtering procedure."
  - [corpus]: Weak or missing corpus evidence for prompt quality improvement.
- Break condition: If task information is ambiguous or missing, or if the pre-trained T5 model is not fine-tuned on relevant dialog data, generated prompts may fail to be task-specific or coherent.

### Mechanism 2
- Claim: Multi-prompt training improves model robustness and transfer ability by exposing the model to diverse prompts representing the same task.
- Mechanism: Multiple prompts are applied to each training instance, and the model learns to generalize across these prompts. The training objective aggregates log probabilities over prompt sets, encouraging the model to capture task essence rather than prompt-specific patterns.
- Core assumption: A diverse set of prompts for each task allows the model to learn invariant task representations, reducing overfitting to individual prompts.
- Evidence anchors:
  - [abstract]: "UniPCM is robust to input prompts and capable of various dialog-related tasks."
  - [section]: "The benefits of which is discussed in sec A.2... applying multiple prompts to one single input instance."
  - [corpus]: Weak or missing corpus evidence for robustness claims.
- Break condition: If prompt diversity is insufficient or prompts are too similar, the model may still overfit to prompt-specific cues rather than task semantics.

### Mechanism 3
- Claim: Pattern-exploit training (PET) with multi-prompt fine-tuning improves few-shot performance by leveraging pseudo-labels from ensemble voting models.
- Mechanism: Multiple prompts are used to train voting models, which generate pseudo-labels for unlabeled data. These pseudo-labels, combined with few-shot labeled data, are used to further fine-tune the model, improving its generalization.
- Core assumption: Ensemble voting across multiple prompts reduces noise in pseudo-label generation, and the pre-trained model's strong performance on prompts ensures high-quality pseudo-labels.
- Evidence anchors:
  - [abstract]: "We perform PET (Schick and SchÃ¼tze, 2021a) for semi-supervised training."
  - [section]: "The accuracy of the voting models is greatly improved, therefore advancing the quality of the pseudo labels generated."
  - [corpus]: Weak or missing corpus evidence for PET effectiveness.
- Break condition: If pseudo-labels are of low quality due to model uncertainty or prompt bias, the semi-supervised training may degrade performance.

## Foundational Learning

- Concept: Task information extraction and keyword generation
  - Why needed here: TAP relies on extracting concise, task-relevant keywords from descriptions or task names to guide prompt generation.
  - Quick check question: How does the tf-idf filtering and BERT similarity scoring ensure that extracted keywords are both relevant and representative of the task?

- Concept: Template-based prompt generation with pre-trained language models
  - Why needed here: The T5 model fills in templates using task keywords and example pairs, creating coherent prompts that link task context to expected outputs.
  - Quick check question: Why does using sentinel tokens and harvesting generated text after them help ensure that the generated prompts are task-centric and coherent?

- Concept: Prompt filtering and scoring based on perplexity and bias detection
  - Why needed here: Generated prompts must be evaluated for their ability to generate correct outputs and avoid bias toward specific labels.
  - Quick check question: How does filtering out prompts containing label-specific words prevent the model from learning biased generation patterns?

## Architecture Onboarding

- Component map: Keyword Extraction Module -> Prompt Generation Module -> Prompt Filtering Module -> Multi-Prompt Training Engine -> PET Ensemble Module
- Critical path:
  1. Extract keywords from task descriptions or names.
  2. Generate prompts using T5 with extracted keywords and example pairs.
  3. Filter and score prompts based on quality metrics.
  4. Scale pre-training corpus using multi-prompt training.
  5. Apply PET for semi-supervised fine-tuning in low-resource scenarios.
- Design tradeoffs:
  - Using more prompts increases diversity but also computational cost.
  - Filtering prompts based on bias detection may remove some useful but label-specific prompts.
  - PET with ensemble voting improves pseudo-label quality but requires careful prompt subset selection.
- Failure signatures:
  - Generated prompts are incoherent or irrelevant to the task.
  - Model overfits to specific prompts rather than generalizing across tasks.
  - Semi-supervised training degrades performance due to low-quality pseudo-labels.
- First 3 experiments:
  1. Test prompt generation quality by generating prompts for a few tasks and manually evaluating their task-specificity and coherence.
  2. Evaluate multi-prompt training by comparing model performance with and without multi-prompt training on a small benchmark.
  3. Test PET effectiveness by applying it to a few-shot scenario and comparing pseudo-label quality and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UniPCM scale with the number of training prompts beyond 303?
- Basis in paper: [explicit] The paper shows UniPCM improves with more prompts, but only tested up to 303.
- Why unresolved: The paper does not explore the upper limits of prompt scaling.
- What evidence would resolve it: Training UniPCM with 500-1000+ prompts and comparing performance to the 303-prompt version.

### Open Question 2
- Question: How does UniPCM perform on dialogue tasks not included in UniPreDial?
- Basis in paper: [inferred] UniPCM is trained on 122 datasets from 15 dialog-related tasks, but the paper does not evaluate on out-of-distribution dialogue tasks.
- Why unresolved: The paper focuses on evaluating UniPCM on tasks within UniPreDial.
- What evidence would resolve it: Testing UniPCM on dialogue datasets from new tasks (e.g., persuasion, negotiation) and comparing performance to task-specific models.

### Open Question 3
- Question: What is the impact of prompt quality on UniPCM's performance?
- Basis in paper: [explicit] The paper shows TAP-generated prompts are on par with human-written prompts, but does not explore the impact of prompt quality variations.
- Why unresolved: The paper does not investigate how prompt quality affects UniPCM's performance.
- What evidence would resolve it: Creating prompts with varying quality levels (e.g., using different keyword extraction methods) and measuring UniPCM's performance on each set.

### Open Question 4
- Question: How does UniPCM's performance compare to other foundation models (e.g., GPT-3, T5-XXL) on dialogue tasks?
- Basis in paper: [inferred] The paper compares UniPCM to T5 and Flan-T5, but does not evaluate against larger foundation models.
- Why unresolved: The paper focuses on comparing UniPCM to smaller, task-specific models.
- What evidence would resolve it: Fine-tuning GPT-3 or T5-XXL on UniPreDial and comparing their performance to UniPCM on the same tasks.

## Limitations

- Limited empirical evidence demonstrating that extracted keywords and generated prompts truly capture the full complexity of task semantics
- Claims about PET's effectiveness in improving few-shot performance rely on assumptions about pseudo-label quality that are not empirically validated
- The paper does not explore how UniPCM performs on dialogue tasks outside the 15 tasks included in UniPreDial

## Confidence

**High Confidence**: The claim that UniPCM achieves state-of-the-art results on 9 datasets is well-supported by experimental results presented in the paper. The multi-prompt training mechanism and its implementation details are clearly described and demonstrated to be effective.

**Medium Confidence**: The effectiveness of Task-aware Automatic Prompt generation (TAP) is supported by the paper's results, but the underlying mechanism could benefit from more detailed analysis. The claim that TAP-generated prompts are on par with crowdsourced prompts is stated but not rigorously validated.

**Low Confidence**: The claims about PET's effectiveness in improving few-shot performance rely on the assumption that ensemble voting produces high-quality pseudo-labels, but this is not empirically validated. The robustness claims for multi-prompt training lack sufficient evidence about prompt diversity and its impact on generalization.

## Next Checks

1. **Prompt Quality Validation**: Generate prompts for a diverse set of tasks using TAP and conduct human evaluation studies to assess prompt coherence, task-specificity, and relevance. Compare these results with prompts collected through crowdsourcing to validate the claim that TAP-generated prompts are on par with human-collected prompts.

2. **Prompt Diversity Analysis**: Perform a systematic analysis of prompt diversity by measuring the semantic similarity between prompts for the same task and across different tasks. Visualize prompt embeddings using t-SNE or UMAP to ensure that the multi-prompt training mechanism is indeed exposing the model to diverse task representations.

3. **Pseudo-Label Quality Assessment**: Implement a framework to evaluate the quality of pseudo-labels generated through PET by comparing them with ground truth labels on a held-out validation set. Analyze the correlation between pseudo-label quality and final performance improvements to validate the effectiveness of the semi-supervised training approach.