---
ver: rpa2
title: 'Tailoring with Targeted Precision: Edit-Based Agents for Open-Domain Procedure
  Customization'
arxiv_id: '2311.09510'
source_url: https://arxiv.org/abs/2311.09510
tags:
- customization
- procedure
- procedures
- goal
- plans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the capabilities of current large language models
  (LLMs) to customize open-domain How-to procedures. The authors introduce a new probe
  dataset, CUSTOM PLANS, containing over 200 WikiHow procedures each with a customization
  need.
---

# Tailoring with Targeted Precision: Edit-Based Agents for Open-Domain Procedure Customization

## Quick Facts
- arXiv ID: 2311.09510
- Source URL: https://arxiv.org/abs/2311.09510
- Reference count: 14
- Primary result: Sequential LLM architecture with customization and execution agents outperforms end-to-end prompting by 10.5% in customizing how-to procedures

## Executive Summary
This paper investigates the ability of large language models to customize open-domain how-to procedures according to user needs. The authors introduce the CUSTOM PLANS dataset containing over 200 WikiHow procedures with associated customization requirements. They evaluate three multi-LLM agent architectures: sequential (customization followed by execution), parallel (both agents simultaneously with resolution), and uniform (single agent for both tasks). The sequential architecture significantly outperforms the others, achieving 10.5% higher performance than end-to-end prompting. However, customized procedures are only fully correct about 51% of the time, highlighting the challenge of addressing implicit customization needs.

## Method Summary
The authors propose a multi-agent LLM architecture for procedure customization. The CUSTOMIZATION AGENT edits generic how-to procedures to address user-specific needs, while the EXECUTION AGENT ensures the modified procedure is executable and free of errors. Three architectural approaches are evaluated: SEQUENTIAL (agents applied one after another), PARALLEL (both agents propose edits simultaneously with a RESOLUTION AGENT merging them), and UNIFORM (single agent handles both tasks). Human evaluation is used to assess both executability and customization satisfaction of the generated procedures.

## Key Results
- Sequential architecture (customization followed by execution agents) outperforms end-to-end prompting by 10.5% absolute
- Customized procedures are fully correct only about 51% of the time
- All methods suffer from generating unnecessary steps that hinder achieving the procedure's goal
- Parallel architecture generates procedures with missing steps more frequently than other approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a sequential architecture with separate customization and execution agents outperforms end-to-end prompting because each agent can focus on a specific task, reducing the cognitive load on a single model.
- Mechanism: The customization agent edits the procedure to address the user's needs, while the execution agent ensures the modified procedure is executable. This division of labor allows each agent to specialize and produce better results.
- Core assumption: Separating the tasks of customization and execution leads to better overall performance than combining them in a single model.
- Evidence anchors:
  - [abstract]: "We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5% absolute) an end-to-end prompted LLM."
  - [section]: "SEQUENTIAL - In this setting, we first obtain a set of edits Ec from CUSTOMIZATION AGENT, and apply Ec to obtain Pc. Next, we use Pc to extract another set of edits Ee from EXECUTION AGENT. Finally, we deterministically apply Ee on Pc to obtain the resulting procedure P′."
- Break condition: If the customization agent introduces changes that make the procedure unexecutable, the execution agent may not be able to fix the issues effectively.

### Mechanism 2
- Claim: The parallel architecture with a resolution agent performs worse because it struggles to merge conflicting edits from the customization and execution agents.
- Mechanism: Both agents propose edits simultaneously, leading to potential conflicts. The resolution agent attempts to merge these edits, but this process is complex and error-prone.
- Core assumption: Merging conflicting edits from two agents is more difficult than applying them sequentially.
- Evidence anchors:
  - [abstract]: "Procedures are not direct. For the executability aspect, each method suffers the most from generating unnecessary steps, which often hinder achieving the goal of the procedure G. Furthermore, procedures generated by PARALLEL also suffer from missing steps."
  - [section]: "PARALLEL - Both CUSTOMIZATION AGENT and EXECUTION AGENT propose their set of edits, Ec and Ee, upon P. However, this gives rise to conflicting edits according to different objectives, both of which are important to generate P′. To address this, we use RESOLUTION AGENT, which takes as input two bags of edits, Ec and Ee, and produces a merged set of edits E."
- Break condition: If the resolution agent cannot effectively merge the edits, the resulting procedure may be incomplete or incorrect.

### Mechanism 3
- Claim: The uniform architecture, which uses a single agent for both customization and execution, performs worse because it is more challenging for one model to handle both tasks effectively.
- Mechanism: The agent must understand both how to customize the procedure and how to ensure its executability, which can lead to suboptimal results.
- Core assumption: A single agent handling both tasks is less effective than two specialized agents.
- Evidence anchors:
  - [abstract]: "Using CUSTOMIZATION AGENT and EXECUTION AGENT in SEQUENTIAL order is the best at producing customized procedures P′. It performs better than using just one agent in UNIFORM, which, in turn, is better than using both agents in PARALLEL setting."
  - [section]: "UNIFORM - We first define a single agent that performs the task of suggesting edits about customization as well as executability on P to generate Pc. This agent is required to understand how to perform both customization towards a hint H as well as execution to achieve the goal G."
- Break condition: If the agent cannot effectively balance the requirements of customization and execution, the resulting procedure may be incomplete or incorrect.

## Foundational Learning

- Concept: Multi-agent systems
  - Why needed here: Understanding how to design and implement multi-agent systems is crucial for this work, as the authors use multiple LLM agents to customize procedures.
  - Quick check question: What are the key considerations when designing a multi-agent system, and how do they apply to this specific use case?

- Concept: Natural language processing (NLP) and text generation
  - Why needed here: The authors use LLMs to generate and modify procedural text, so a strong understanding of NLP and text generation techniques is essential.
  - Quick check question: How do LLMs generate text, and what are the challenges associated with generating procedural text?

- Concept: Human evaluation of NLP systems
  - Why needed here: The authors use human evaluation to assess the quality of the generated procedures, so understanding best practices for human evaluation in NLP is important.
  - Quick check question: What are the key considerations when designing a human evaluation study for an NLP system, and how can potential biases be mitigated?

## Architecture Onboarding

- Component map:
  - CUSTOMIZATION AGENT -> EXECUTION AGENT -> Customized procedure
  - For parallel: CUSTOMIZATION AGENT and EXECUTION AGENT -> RESOLUTION AGENT -> Customized procedure
  - Original procedure and customization hint are inputs to both architectures

- Critical path:
  1. Input: Original procedure and customization hint
  2. Customization agent generates edits to address the user's needs
  3. Execution agent (in sequential architecture) or resolution agent (in parallel architecture) processes the edits
  4. Output: Customized, executable procedure

- Design tradeoffs:
  - Sequential vs. parallel architecture: Sequential architecture allows for specialization but may be slower, while parallel architecture is faster but more prone to conflicts
  - Single vs. multiple agents: Multiple agents allow for specialization but increase complexity, while a single agent is simpler but may be less effective

- Failure signatures:
  - Missing steps: The generated procedure does not include all necessary steps to achieve the goal
  - Extra steps: The generated procedure includes unnecessary steps that do not contribute to achieving the goal
  - Incorrect steps: The generated procedure includes steps that are incorrect or misleading
  - Unexecutable procedures: The generated procedure cannot be executed to achieve the goal due to errors or inconsistencies

- First 3 experiments:
  1. Compare the performance of the sequential, parallel, and uniform architectures on a small subset of the CUSTOM PLANS dataset to validate the initial findings
  2. Analyze the types of errors made by each architecture to identify areas for improvement
  3. Experiment with different prompt templates and model parameters for the customization and execution agents to optimize their performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the models' ability to fully address implicit customization needs expressed in natural language?
- Basis in paper: Explicit
- Why unresolved: The error analysis in the paper shows that models often miss important steps when addressing implicit customization needs, indicating a limitation in understanding and incorporating these needs into the generated procedures.
- What evidence would resolve it: Developing models with better natural language understanding capabilities that can accurately interpret and incorporate implicit customization needs into the generated procedures.

### Open Question 2
- Question: Can the multi-agent editing architectures be extended to other customization applications beyond procedure customization?
- Basis in paper: Explicit
- Why unresolved: The paper suggests that multi-agent editing architectures may be worth exploring further for other customization applications like coding and creative writing, but does not provide evidence of their effectiveness in these domains.
- What evidence would resolve it: Conducting experiments to evaluate the effectiveness of multi-agent editing architectures in customizing procedures for coding and creative writing tasks.

### Open Question 3
- Question: How can we reduce the number of unnecessary steps in the generated procedures while maintaining their executability?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that all methods suffer from generating unnecessary steps, which often hinder achieving the goal of the procedure. However, it does not provide a solution to address this issue.
- What evidence would resolve it: Developing techniques to identify and remove unnecessary steps from the generated procedures without compromising their executability.

### Open Question 4
- Question: How can we improve the customizability of the generated procedures while maintaining their executability?
- Basis in paper: Explicit
- Why unresolved: The paper shows that the models are only able to generate customized procedures that are both customizable and executable around 51% of the time, indicating a need for improvement in this aspect.
- What evidence would resolve it: Developing models that can generate procedures that are both highly customizable and executable, potentially by incorporating user feedback and preferences more effectively.

### Open Question 5
- Question: How can we better capture and incorporate user expertise levels into the customization process?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that some procedures can only be performed by domain experts, while others are beginner-friendly, but does not discuss how to effectively incorporate user expertise levels into the customization process.
- What evidence would resolve it: Developing techniques to accurately assess user expertise levels and tailor the customization process accordingly, potentially by incorporating user profiles or prior knowledge into the models.

## Limitations

- The sequential architecture's superior performance comes with a tradeoff in computational efficiency compared to simpler approaches
- Only approximately 51% of generated procedures are fully correct, indicating fundamental limitations in current models' ability to address implicit customization needs
- The generalizability of findings beyond WikiHow-style procedures remains uncertain due to potential dataset composition and domain specificity issues

## Confidence

High confidence: The comparative performance claims between sequential, parallel, and uniform architectures are well-supported by experimental evidence and human evaluation metrics.

Medium confidence: The error analysis conclusions regarding missing steps and unnecessary steps are reasonable but may not capture the full complexity of failure modes.

Low confidence: The generalizability of findings beyond WikiHow-style procedures remains uncertain.

## Next Checks

1. Conduct ablation studies testing individual components of the sequential architecture (customization vs. execution agents) to quantify their specific contributions to performance improvements.

2. Implement a cross-dataset evaluation using procedures from different domains (cooking, technical manuals, medical procedures) to assess the model's adaptability to varied procedural formats and complexity levels.

3. Design a follow-up human evaluation study with more granular error categorization and inter-annotator agreement metrics to better understand the nature and frequency of specific failure modes.