---
ver: rpa2
title: On Model Explanations with Transferable Neural Pathways
arxiv_id: '2309.09887'
source_url: https://arxiv.org/abs/2309.09887
tags:
- neural
- pathways
- sparsity
- class-relevant
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GEN-CNP, a generative model that produces instance-specific
  and class-relevant neural pathways as explanations for image recognition models.
  Unlike prior methods that use a fixed sparsity for all samples, GEN-CNP employs
  instance-specific sparsity and leverages deeper layer features to inject class-relevant
  semantics into earlier layers.
---

# On Model Explanations with Transferable Neural Pathways

## Quick Facts
- arXiv ID: 2309.09887
- Source URL: https://arxiv.org/abs/2309.09887
- Reference count: 40
- Key outcome: GEN-CNP achieves up to 3.09% higher model confidence (mIC) and 59.96% higher class-wise interpretability (acIOU) compared to baseline methods

## Executive Summary
This paper introduces GEN-CNP, a generative model that creates instance-specific and class-relevant neural pathways as explanations for image recognition models. Unlike prior approaches that use fixed sparsity for all samples, GEN-CNP employs instance-specific sparsity and leverages deeper layer features to inject class-relevant semantics into earlier layers. The method includes a Pathway Distillation Network to predict feature importance and a Recursive Pathway Decoder with Distance Aware Quantization to generate binary neural pathways. Experimental results demonstrate superior faithfulness and interpretability compared to existing methods, with successful transferability of class-relevant neural pathways to explain other samples of the same class.

## Method Summary
GEN-CNP generates instance-specific neural pathways by extracting feature maps from a target model, processing them through Recursive Feature Embedders (RFEs) to a common resolution, and using a shared Pathway Distillation Network (PDN) to predict feature importance. The Recursive Pathway Decoders (RPDs) then convert these importance scores to binary neural pathways using Distance Aware Quantization. The model is trained with knowledge distillation loss to maintain faithfulness to the original model while enforcing instance-specific sparsity through l2 regularization. This approach enables the generation of sparse, faithful explanations that can be transferred across same-class samples.

## Key Results
- Achieved mIC up to 3.09% on ImageNet compared to 1.00% for baseline methods
- Demonstrated acIOU up to 59.96% versus 19.40% for DGR R baseline
- Successfully transferred class-relevant neural pathways to explain other samples of the same class

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-relevant semantics propagate from deeper to earlier layers through shared Pathway Distillation Network (PDN)
- Mechanism: PDN takes feature patterns from all layers and learns to map them to importance scores, enabling class-relevant information from deeper layers to influence earlier layer attributions
- Core assumption: Features become more class-specific as depth increases
- Evidence anchors: abstract states "We propose to learn class-relevant information from features of deep and shallow layers such that same-class neural pathways exhibit high similarity"

### Mechanism 2
- Claim: Instance-specific sparsity is achieved through knowledge distillation with l2 regularization
- Mechanism: Knowledge distillation loss (LKD) guides pathway generation to match original model predictions, while l2 regularization (LS) controls sparsity per instance rather than globally
- Core assumption: Instance-level sparsity optimization improves faithfulness compared to global sparsity constraints
- Evidence anchors: abstract mentions "We further impose a faithfulness criterion for GEN-CNP to generate pathways with instance-specific sparsity"

### Mechanism 3
- Claim: Transferability of class-relevant neural pathways enables consistent explanations across same-class samples
- Mechanism: Neural pathways for a class are aggregated by averaging firing patterns across instances, then applied to explain new samples of the same class
- Core assumption: Class-relevant neurons are consistently important across same-class instances
- Evidence anchors: abstract states "We propose to transfer the class-relevant neural pathways to explain samples of the same class"

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Enables training GEN-CNP without ground truth neural pathways by matching original model predictions
  - Quick check question: What loss function ensures generated pathways maintain model performance while being sparse?

- Concept: Sparsity Regularization
  - Why needed here: Controls the number of neurons in neural pathways while maintaining instance-specific optimization
  - Quick check question: Why does the paper use l2-norm instead of l0-norm for sparsity control?

- Concept: Feature Embeddings and Dimensionality Reduction
  - Why needed here: RFEs extract and embed feature patterns from different layer resolutions to the same dimension for PDN processing
  - Quick check question: How do RFEs handle different input resolutions across network layers?

## Architecture Onboarding

- Component map: Input features → RFE → PDN → RPD → DAQ → Binary neural pathways
- Critical path: Feature extraction from target model → Recursive Feature Embedders → Pathway Distillation Network → Recursive Pathway Decoders with Distance Aware Quantization → Binary neural pathways
- Design tradeoffs:
  - Shared vs layer-wise PDN: Shared PDN enables class-relevant semantics propagation but may reduce layer-specific adaptation
  - Instance-specific vs global sparsity: Instance-specific provides better faithfulness but increases computational complexity
  - Convolution vs MLP for RFE: Convolution preserves spatial locality better than MLP but may be less flexible
- Failure signatures:
  - Poor faithfulness (low accuracy): Likely PDN not learning meaningful importance scores
  - High variability across same-class samples: RFEs not extracting consistent features
  - Non-sparse pathways: DAQ not functioning correctly or sparsity regularization too weak
- First 3 experiments:
  1. Train GEN-CNP on a simple CNN with known structure to verify pathway generation
  2. Test instance-specific vs global sparsity on a small dataset to confirm faithfulness improvements
  3. Validate class-relevant pathway transferability on a toy classification task with clear class concepts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the transferability of class-relevant neural pathways generalize across different model architectures (e.g., CNNs vs transformers) or is it specific to the architecture used in the study?
- Basis in paper: [inferred] The paper demonstrates transferability for AlexNet and VGG-11 on image recognition tasks, but does not explore other architectures or domains
- Why unresolved: The study focuses on two specific CNN architectures and image datasets, leaving open whether the approach works for other architectures like transformers or for non-image domains
- What evidence would resolve it: Experiments showing transferability results for models like Vision Transformers or models in different domains (e.g., NLP, speech recognition) would clarify generalizability

### Open Question 2
- Question: How does the choice of hyperparameters (e.g., sparsity thresholds, learning rates) affect the balance between faithfulness and interpretability of the generated neural pathways?
- Basis in paper: [explicit] The paper mentions tunable hyperparameters but does not provide a systematic analysis of their impact on the trade-off between faithfulness and interpretability
- Why unresolved: The study uses specific hyperparameter values but does not explore the sensitivity of results to these choices or provide guidelines for selecting them
- What evidence would resolve it: An ablation study varying key hyperparameters and analyzing their effect on faithfulness metrics (mIC, mDC) and interpretability metrics (acIOU) would clarify their impact

### Open Question 3
- Question: Can the class-relevant neural pathways be used for model compression or knowledge distillation beyond interpretability, and if so, how does their performance compare to traditional pruning methods?
- Basis in paper: [inferred] The paper shows that class-relevant neural pathways achieve high accuracy, suggesting potential for model compression, but does not directly compare to traditional pruning methods or explore knowledge distillation applications
- Why unresolved: While the study demonstrates high accuracy with sparse pathways, it does not benchmark against standard model compression techniques or explore using these pathways for training smaller models
- What evidence would resolve it: Experiments comparing the compressed model's performance (accuracy, inference speed) using class-relevant pathways versus traditional pruning methods would address this question

## Limitations

- Claims about class-relevant semantics propagation lack theoretical grounding and empirical validation beyond design assumptions
- The superiority of instance-specific sparsity over global sparsity is demonstrated through limited comparisons without establishing the full Pareto frontier of approaches
- Transferability claims are based on specific CNN architectures and may not generalize to other model types or domains

## Confidence

- **High Confidence**: The technical architecture description (RFE, PDN, RPD components) and the training procedure are well-specified and reproducible
- **Medium Confidence**: The experimental results showing improved mIC and acIOU metrics appear robust, though the absolute improvements may be modest
- **Low Confidence**: Claims about transferability of class-relevant neural pathways and the superiority of instance-specific sparsity lack sufficient ablation studies to rule out alternative explanations

## Next Checks

1. Conduct an ablation study removing the class-relevant semantics injection mechanism to isolate its contribution to performance
2. Test GEN-CNP on classes with high intra-class variation to determine if transferability breaks down when class-relevant neurons are not consistent
3. Perform a detailed study of how instance-specific sparsity affects faithfulness across different model architectures and datasets beyond the limited comparisons provided