---
ver: rpa2
title: Low-bit Quantization for Deep Graph Neural Networks with Smoothness-aware Message
  Propagation
arxiv_id: '2308.14949'
source_url: https://arxiv.org/abs/2308.14949
tags:
- quantization
- neural
- graph
- accuracy
- gnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of low-bit quantization for deep
  Graph Neural Networks (GNNs) while addressing the oversmoothing problem. The authors
  propose a quantization with learnable ranges (QLR) that learns the quantization
  range and reduces model size while maintaining accuracy even under low-bit quantization.
---

# Low-bit Quantization for Deep Graph Neural Networks with Smoothness-aware Message Propagation

## Quick Facts
- arXiv ID: 2308.14949
- Source URL: https://arxiv.org/abs/2308.14949
- Reference count: 40
- Primary result: Achieves INT2 quantization with accuracy close to full precision while providing 5.11× speedup

## Executive Summary
This paper addresses the challenge of low-bit quantization for deep Graph Neural Networks (GNNs) while mitigating oversmoothing. The authors propose three complementary mechanisms: Quantization with Learnable Ranges (QLR) that adapts quantization ranges to data distribution, Skewness-aware Bitwise Truncation (BT*) that preserves accuracy under non-normal distributions, and Smoothness-aware Message Propagation (SMP) that controls layer-wise changes in node similarities. The proposed method significantly outperforms state-of-the-art quantization approaches, achieving notable accuracy levels even under INT2 quantization while providing substantial inference speedup.

## Method Summary
The method combines three key innovations: QLR learns a scaling parameter γ to optimize quantization ranges during training using backpropagation with straight-through estimation; BT* incorporates skewness information to improve bitwise truncation for non-normal distributions; and SMP enforces smoothness constraints at each message passing layer using Lagrangian optimization to mitigate oversmoothing and quantization error accumulation. The approach is evaluated on benchmark datasets (Cora, PubMed, CiteSeer, CS, Reddit) with various GNN architectures including GCN, SMP, EMP, APPNP, DropEdge, and PairNorm.

## Key Results
- INT2 quantization achieves accuracy levels comparable to INT8, significantly outperforming existing quantization methods
- Inference with INT2 and INT4 representations provides 5.11× and 4.70× speedup compared to full-precision counterparts
- SMP effectively mitigates oversmoothing in deep GNNs while suppressing quantization error accumulation
- The method maintains accuracy across various quantization levels while reducing model size

## Why This Works (Mechanism)

### Mechanism 1
QLR reduces quantization error by learning a scaling parameter γ that optimizes the quantization range [α, β] into a data-aware range [γα, γβ] during training. This dynamic adjustment adapts to the high variance of node embeddings caused by varying neighbor counts, which static ranges cannot efficiently capture. The approach uses backpropagation with straight-through estimation to update γ, making the quantization range responsive to actual data distribution.

### Mechanism 2
BT* preserves accuracy in INT2 quantization by accounting for non-normal, skewed distributions of quantized elements. The mechanism modifies standard bitwise truncation by incorporating input skewness (sk), ensuring the truncation process respects data asymmetry and maintains symmetry of quantized elements. This is particularly effective since GNN activations often exhibit significant kurtosis and skewness under low-bit quantization.

### Mechanism 3
SMP mitigates oversmoothing and quantization error by constraining layer-wise changes in node similarities. The mechanism enforces a smoothness constraint at each message passing layer, formulated as a graph denoising problem with constraints and solved iteratively using a differential multiplier method. This prevents embeddings from converging too quickly to indistinguishable values while controlling quantization error accumulation across layers.

## Foundational Learning

- **Graph Neural Networks and Message Passing**: Understanding the message passing framework where node embeddings are updated by aggregating neighbor information is essential, as the entire quantization strategy is built around this framework. Quick check: What are the three main functions performed at each layer in message passing?

- **Quantization in Neural Networks**: Knowledge of how quantization reduces precision and the trade-offs in accuracy is necessary for understanding the specialized GNN quantization approach. Quick check: What are the two main sources of quantization error in low-bit representations?

- **Oversmoothing vs. Overfitting**: Understanding that oversmoothing in deep GNNs (where node embeddings become indistinguishable) is distinct from overfitting in traditional neural networks is crucial for appreciating SMP's role. Quick check: How does oversmoothing in GNNs differ from overfitting in traditional neural networks?

## Architecture Onboarding

- **Component map**: Graph data → GNN layers → QLR adjusts quantization range → BT* applies truncation → SMP enforces smoothness → Output embeddings

- **Critical path**: Graph data → GNN layers → QLR learns γ → BT* applies skewness-aware truncation → SMP constrains layer-wise smoothness → Output quantized embeddings

- **Design tradeoffs**: Accuracy vs. model size (low-bit quantization reduces size but risks accuracy loss; QLR and BT* mitigate this); Depth vs. oversmoothing (deeper models capture more context but suffer from oversmoothing; SMP addresses this); Complexity vs. efficiency (SMP and BT* add computational overhead but improve accuracy and stability)

- **Failure signatures**: Significant accuracy drops in low-bit settings may indicate poor quantization range optimization or insufficient skewness adjustment; oversmoothing in deep models may suggest SMP constraints are too weak or not properly enforced; unstable training could result from improper learning rates for γ or SMP parameters

- **First 3 experiments**: 1) Implement QLR on a simple GCN with varying γ values and measure quantization error on Cora dataset; 2) Add BT* to the QLR pipeline and compare INT2 accuracy on Cora with and without skewness adjustment; 3) Integrate SMP into a 10-layer GCN and measure layer-wise smoothness and accuracy on CiteSeer dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does QLR compare to other quantization methods across different graph types and tasks? The paper only evaluates on benchmark datasets, leaving generalizability to other graphs and tasks unclear. Additional experiments on diverse graphs would provide more evidence.

### Open Question 2
How does BT* affect accuracy and efficiency across different graph types and tasks? The paper's evaluation is limited to benchmark datasets, making its effectiveness on other graphs uncertain. More diverse experiments would provide additional evidence.

### Open Question 3
How does SMP affect accuracy and efficiency across different graph types and tasks? The paper only evaluates SMP on benchmark datasets, leaving its effectiveness on other graphs unclear. Additional experiments would provide more evidence.

## Limitations

- Generalizability concerns: The effectiveness of QLR, BT*, and SMP across diverse graph structures and node degree distributions remains uncertain
- Distribution assumptions: BT* heavily depends on input distributions exhibiting significant skewness, which may not hold for all GNN applications
- Computational overhead: SMP introduces additional computational cost through Lagrangian optimization, potentially limiting benefits in shallow networks or sparse graphs

## Confidence

- **High Confidence**: Model size reduction and inference speedup claims are well-supported by empirical results across multiple datasets
- **Medium Confidence**: QLR and BT* mechanisms reducing quantization error are supported by theory and ablation studies, though individual contributions vary
- **Medium Confidence**: SMP's effectiveness in mitigating oversmoothing is supported by quantitative metrics, but sensitivity to hyperparameters warrants further investigation

## Next Checks

1. **Distribution Sensitivity Test**: Systematically evaluate BT* performance across graphs with varying skewness distributions to validate the core assumption about skewed data in GNNs

2. **Layer-depth Sensitivity Analysis**: Measure SMP's effectiveness at different network depths and with varying graph densities to establish the limits of its oversmoothing mitigation capabilities

3. **Cross-architecture Generalization**: Apply the proposed quantization framework to non-GCN architectures (GAT, GraphSAGE) to test the generalizability of QLR and BT* mechanisms beyond the tested models