---
ver: rpa2
title: 'Brainformers: Trading Simplicity for Efficiency'
arxiv_id: '2306.00008'
source_url: https://arxiv.org/abs/2306.00008
tags:
- training
- brainformer
- glam
- layer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Brainformer, a complex transformer block
  that improves efficiency by mixing different layer types like sparsely gated feed-forward,
  dense feed-forward, attention, and various normalization and activation functions.
  Brainformer outperforms existing dense and sparse transformer models in both quality
  and efficiency.
---

# Brainformers: Trading Simplicity for Efficiency

## Quick Facts
- arXiv ID: 2306.00008
- Source URL: https://arxiv.org/abs/2306.00008
- Reference count: 13
- Key outcome: Brainformer achieves 2× faster training and 5× faster inference than GLaM while improving downstream task performance by 3% on SuperGLUE

## Executive Summary
Brainformer introduces a complex transformer architecture that improves efficiency by mixing different layer types including sparsely gated feed-forward, dense feed-forward, attention, and various normalization and activation functions. The key innovation is trading architectural simplicity for efficiency gains by composing diverse sub-layers in different orders. Through evolutionary search, Brainformer discovers optimal block compositions that outperform both dense and sparse transformer baselines in quality, training efficiency, and inference speed.

## Method Summary
The paper employs an evolutionary search to find optimal Brainformer block compositions, training proxy models early-stopped at 25% of max steps to evaluate perplexity and step time. The search space includes variations in layer types (attention, dense FFN, sparse MoE), hidden dimensions, gating strategies (token-based vs expert-based), and layer orderings. Selected architectures are then scaled to target sizes (100M, 1B, 8B parameters) and trained on 1.6 trillion tokens using Adafactor optimizer with inverse square root learning rate decay. The method specifically optimizes for both quality and efficiency by fixing wall-clock time per search trial.

## Key Results
- Brainformer with 8B activated parameters trains 2× faster and runs 5× faster than GLaM baseline
- Achieves 3% higher SuperGLUE score with fine-tuning compared to GLaM with similar activated parameters
- Outperforms Primer dense model with similar computation per token on few-shot evaluations
- Demonstrates near-perfect load balance with expert choice routing in sparse layers

## Why This Works (Mechanism)

### Mechanism 1: Sub-layer Permutation and Capacity Scaling
Trading uniform block structure for variable sub-layer permutations yields better quality-efficiency tradeoffs by allowing the architecture to better fit data distribution while keeping computational cost fixed. Core assumption: certain data patterns benefit more from some sub-layer types than others, and optimal ordering varies across the network.

### Mechanism 2: Sparsity via Expert Choice Routing
Replacing token-based top-k gating with expert-based Expert Choice gating improves load balance and speed by having experts select top-k tokens rather than each token selecting top-k experts, ensuring near-perfect load balance and reducing overhead.

### Mechanism 3: Block-wise Architecture Search with Fixed Wall-Clock Budget
Using fixed wall-clock time per search trial encourages discovery of faster-converging architectures by giving more steps to models that train faster within the same time budget, biasing the search toward architectures with better training efficiency.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) routing mechanisms**
  - Why needed here: Brainformer uses sparse MoE layers; understanding gating strategies is essential for modifying or debugging the architecture
  - Quick check question: In token-based routing, how many experts does each token select, and what loss term ensures load balance?

- **Concept: Layer-wise compound scaling**
  - Why needed here: Brainformer scales model dimension and hidden sizes; grasping compound scaling helps tune capacity for target hardware
  - Quick check question: If the model dimension increases by 1.5×, what is the expected change in FFN expansion ratio to maintain efficiency?

- **Concept: Transformer sub-layer interactions**
  - Why needed here: Brainformer composes multiple sub-layer types; knowing how attention, FFN, and MoE layers interact guides debugging and modification
  - Quick check question: Why might interleaving dense and sparse FFN layers improve training stability compared to grouping them?

## Architecture Onboarding

- **Component map:**
  Input tensor [B, L, H] → Attention → Dense FFN → Sparse MoE → Output (sub-layers can be in any order and have independent dimensions)

- **Critical path:**
  Token embedding → Sub-layer 1 → Sub-layer 2 → … → Sub-layer k → Output; for MoE layers: token → gating scores → top-k expert selection → forward in chosen experts → re-combine

- **Design tradeoffs:**
  Complexity vs. regularity: More sub-layer types → better fit but harder to optimize; Capacity vs. sparsity: Larger MoE hidden dims → more experts but slower inference; Gating strategy: Expert Choice → perfect load balance, slower; Top-2 → faster, possible imbalance

- **Failure signatures:**
  Degraded perplexity: likely load imbalance or poor sub-layer ordering; Slow step time: MoE experts underutilized or gating overhead too high; Training divergence: hidden dimension mismatch or activation function mismatch

- **First 3 experiments:**
  1. Swap a dense FFN for an MoE layer in a single block and measure perplexity and step time
  2. Change gating from Top-2 to Expert Choice in a small model; observe load balance and training speed
  3. Modify block ordering (e.g., Attn → MoE → FFN vs Attn → FFN → MoE) and compare convergence curves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of different layer types (attention, feed-forward, sparsely gated feed-forward) in Brainformer blocks?
- Basis in paper: The paper states that "we find that optimizing the architecture, sparsity, and routing mechanism in sparse layers is critical to achieve near-perfect log-scale scaling in quality"
- Why unresolved: The paper does not provide specific optimal ratios for layer types and mentions that "the search algorithm picks" these ratios, but does not elaborate on the optimal values
- What evidence would resolve it: Empirical results showing the performance of Brainformer with different ratios of layer types would help determine the optimal configuration

### Open Question 2
- Question: How does the performance of Brainformer scale with increasing model size and number of experts in the sparsely gated feed-forward layers?
- Basis in paper: The paper discusses scaling Brainformer to different model sizes (100M, 1B, 8B) and mentions that "we can stack an arbitrary number of Brainformer blocks to create a target model"
- Why unresolved: The paper does not provide detailed scaling results for larger models or the impact of increasing the number of experts on performance
- What evidence would resolve it: Scaling studies with larger models and varying numbers of experts would help understand the performance trade-offs

### Open Question 3
- Question: How does Brainformer's performance compare to other sparse transformer architectures like Switch Transformers and Primer on various downstream tasks?
- Basis in paper: The paper mentions that Brainformer "largely outperforms a Primer dense model derived with NAS with similar computation per token on few-shot evaluations"
- Why unresolved: The paper does not provide a comprehensive comparison of Brainformer with other sparse transformer architectures on a wide range of downstream tasks
- What evidence would resolve it: Benchmarking Brainformer against other sparse transformer architectures on multiple downstream tasks would provide a clearer picture of its relative performance

### Open Question 4
- Question: How does the choice of gating function (token-based vs. expert-based) impact Brainformer's performance and training efficiency?
- Basis in paper: The paper discusses token-based and expert-based routing, stating that "there are various routing methods in existing MoE literature" and focuses on these two classes
- Why unresolved: The paper does not provide a detailed comparison of the impact of different gating functions on Brainformer's performance and training efficiency
- What evidence would resolve it: Ablation studies comparing the performance and training efficiency of Brainformer with different gating functions would help determine the optimal choice

## Limitations
- Computational efficiency claims rely heavily on specific TPUv4 hardware configurations that may not generalize
- Evidence for downstream task superiority is less robust, with modest 3% improvement on SuperGLUE
- Evolutionary search methodology makes it difficult to isolate which specific design choices drive performance gains

## Confidence
- **High Confidence**: Mixing different layer types improves training efficiency (2× faster training, 5× faster inference)
- **Medium Confidence**: Better quality-efficiency tradeoffs demonstrated for pre-training but less convincingly for downstream tasks
- **Low Confidence**: Evolutionary search specifically discovers faster-converging architectures without rigorous proof

## Next Checks
1. **Ablation Study on Layer Composition**: Systematically remove or replace individual layer types to quantify each component's contribution to performance
2. **Cross-Platform Efficiency Validation**: Reproduce efficiency benchmarks on alternative hardware (GPUs, AWS Inferentia) to verify generalizability
3. **Few-Shot Learning Benchmark**: Evaluate Brainformer on standardized few-shot learning tasks to directly compare generalization capabilities against dense models