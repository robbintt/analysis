---
ver: rpa2
title: 'ExpNote: Black-box Large Language Models are Better Task Solvers with Experience
  Notebook'
arxiv_id: '2311.07032'
source_url: https://arxiv.org/abs/2311.07032
tags:
- system
- letter
- assistant
- answer
- expnote
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExpNote is a framework that helps black-box LLMs adapt to downstream
  tasks by automatically learning and retrieving task-specific experiences. It guides
  LLMs to reflect on training data, store learned experiences in external memory,
  and retrieve relevant ones during testing.
---

# ExpNote: Black-box Large Language Models are Better Task Solvers with Experience Notebook

## Quick Facts
- arXiv ID: 2311.07032
- Source URL: https://arxiv.org/abs/2311.07032
- Reference count: 7
- Primary result: ExpNote improves LLM performance on diverse tasks by automatically learning and retrieving task-specific experiences, achieving up to 89% accuracy on symbolic reasoning tasks

## Executive Summary
ExpNote is a framework that enables black-box LLMs to adapt to downstream tasks by automatically learning and retrieving task-specific experiences. The framework guides LLMs to reflect on training data, store learned experiences in external memory, and retrieve relevant ones during testing. Experiments on four diverse tasks demonstrate that ExpNote significantly outperforms baseline approaches like CoT, TeachMe, and Reflexion, with abstract task-specific experiences proving more effective than original cases.

## Method Summary
ExpNote is a framework that helps black-box LLMs adapt to downstream tasks by automatically learning and retrieving task-specific experiences. It guides LLMs to reflect on training data, store learned experiences in external memory, and retrieve relevant ones during testing. The framework uses structured commands (THINK, NOTE, RECALL) to enable fully automated reflection, noting, and retrieval without human feedback. Experiments were conducted on four datasets using ChatGPT (gpt-3.5-turbo) via OpenAI API, with 100 test cases per dataset and 2:1 train:test split.

## Key Results
- ExpNote achieves up to 89% accuracy on symbolic reasoning tasks, significantly outperforming baselines
- Abstract task-specific experiences are more effective than original cases for LLM generalization
- Experiences from both correct and incorrect training examples are beneficial for improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ExpNote enables LLMs to generalize from training examples to unseen test cases by storing and retrieving abstracted task-specific experiences.
- Mechanism: During training, LLMs reflect on failed cases, extract abstract rules, and store them in memory with semantic keys. During testing, relevant experiences are retrieved and used to guide reasoning.
- Core assumption: Abstracted rules capture the underlying structure of tasks better than raw examples.
- Evidence anchors:
  - [abstract]: "Experiments on four diverse tasks show ExpNote significantly improves LLM performance compared to baselines... achieving up to 89% accuracy on symbolic reasoning tasks."
  - [section]: "We find that prompting with experiences is more helpful than original cases for LLMs to generalize to new cases..."
  - [corpus]: Weak evidence. Related works like "Generalists vs. Specialists" and "Agent-Pro" discuss generalization but do not provide direct empirical support for abstracted rule storage.
- Break condition: If the task requires case-specific nuance rather than generalizable rules, or if abstraction process fails to preserve essential information.

### Mechanism 2
- Claim: Experiences from both positive (correct) and negative (incorrect) training cases are beneficial for improving LLM performance.
- Mechanism: The framework prompts LLMs to reflect on both successes and failures, extracting experiences from each. These are stored and later retrieved to inform decision-making during testing.
- Core assumption: Learning from mistakes is as valuable as learning from correct examples for pattern generalization.
- Evidence anchors:
  - [abstract]: "...experiences from both correct and incorrect training examples are beneficial."
  - [section]: "We also found that the learned task-specific experiences help LLMs to better generalize than the original cases in the task, and experiences learned from both positive cases and negative cases are valuable."
  - [corpus]: Weak evidence. Related works like "Kov" and "Neuro-Symbolic Integration" discuss learning from failures but focus on adversarial settings, not task generalization.
- Break condition: If the cost of storing and retrieving experiences from failed cases outweighs their utility, or if retrieval introduces noise.

### Mechanism 3
- Claim: Dynamic memory interaction through structured commands (THINK, NOTE, RECALL) enables automated experience management without human feedback.
- Mechanism: Commands guide the LLM through reflection (THINK), experience storage (NOTE), and retrieval (RECALL), enabling fully automated adaptation to tasks.
- Core assumption: LLMs can reliably follow structured commands to manage external memory and perform reflection.
- Evidence anchors:
  - [abstract]: "ExpNote conducts fully automated reflection, noting, and retrieval, without the need of any annotated knowledge and facts or any human feedback."
  - [section]: "We design several commands for LLMs to interact with the memory, including summarizing and applying the experiences (THINK), storing the experiences in the memory (NOTE), and retrieving relevant experiences for the testing instances (RECALL)."
  - [corpus]: Weak evidence. "AutoSAT" and "Make Prompt-based Black-Box Tuning Colorful" discuss automated optimization but not memory interaction via commands.
- Break condition: If LLMs fail to correctly interpret or execute the commands, leading to incorrect experience storage or retrieval.

## Foundational Learning

- Concept: Abstraction of task-specific rules
  - Why needed here: To generalize from specific training examples to unseen test cases.
  - Quick check question: Can the LLM extract a general rule from a specific example without losing critical details?

- Concept: Dynamic memory interaction
  - Why needed here: To enable automated storage and retrieval of experiences without human intervention.
  - Quick check question: Does the LLM correctly follow the THINK, NOTE, and RECALL commands in sequence?

- Concept: Retrieval-augmented reasoning
  - Why needed here: To use stored experiences to guide reasoning on new problems.
  - Quick check question: Can the LLM identify and apply relevant experiences from memory to solve a new instance?

## Architecture Onboarding

- Component map:
  LLM (black-box, e.g., ChatGPT) -> Dynamic memory (key-value store for experiences) -> Retriever (word-based, retrieves up to k=3 experiences) -> Prompt templates (Ptrain, Ptest) -> Command processor (handles THINK, NOTE, RECALL)

- Critical path:
  1. Training: Input → LLM reasoning → Feedback → Reflection → Experience storage
  2. Testing: Input → Experience retrieval → LLM reasoning with retrieved experiences → Output

- Design tradeoffs:
  - Memory size vs. retrieval speed
  - Abstraction quality vs. storage overhead
  - Retrieval accuracy vs. prompt complexity

- Failure signatures:
  - No experiences retrieved (falls back to baseline)
  - Irrelevant experiences retrieved (degrades performance)
  - LLM fails to abstract useful rules (no improvement over baseline)

- First 3 experiments:
  1. Run ExpNote disabled (no retrieval) and compare to CoT baseline to measure impact of retrieval alone.
  2. Run ExpNote with only positive or only negative experiences to isolate their individual contributions.
  3. Vary k (number of retrieved experiences) to find optimal retrieval size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the efficiency of positive versus negative experiences vary across different task types?
- Basis in paper: [explicit] The paper states that experiences from both positive and negative cases are more efficient than the other respectively on two datasets.
- Why unresolved: The paper provides efficiency analysis for only two datasets, leaving open the question of whether this pattern holds across different types of tasks.
- What evidence would resolve it: Conducting efficiency analysis across a wider variety of task types and datasets would provide a more comprehensive understanding of the relative value of positive versus negative experiences.

### Open Question 2
- Question: What is the impact of the number of training samples on the performance of ExpNote?
- Basis in paper: [explicit] The paper mentions that ExpNote's performance on the testing set continually grows with the number of training samples, but does not provide a detailed analysis of this relationship.
- Why unresolved: The paper only provides a single example of the training curve in the CLUTRR dataset, leaving open the question of how this relationship generalizes to other tasks and datasets.
- What evidence would resolve it: Conducting a more extensive analysis of the training curves across multiple tasks and datasets would provide a clearer picture of how the number of training samples impacts ExpNote's performance.

### Open Question 3
- Question: How does ExpNote perform on tasks that require case-by-case reasoning, such as summarizing or creative writing?
- Basis in paper: [explicit] The paper mentions that ExpNote may be less effective on case-by-case tasks, but does not provide empirical evidence to support this claim.
- Why unresolved: The paper does not include any experiments on tasks that require case-by-case reasoning, leaving open the question of how well ExpNote generalizes to these types of tasks.
- What evidence would resolve it: Conducting experiments on tasks that require case-by-case reasoning, such as summarizing or creative writing, would provide empirical evidence of ExpNote's performance on these types of tasks.

## Limitations
- The framework's effectiveness depends heavily on the LLM's ability to extract meaningful abstractions, which may vary across different models or domains.
- The word-based retrieval mechanism could retrieve semantically irrelevant experiences if they share common words.
- Experiments were conducted using only ChatGPT (gpt-3.5-turbo) via API, limiting generalizability to other LLM architectures.

## Confidence

- **High confidence**: ExpNote framework implementation and basic retrieval mechanism function as described
- **Medium confidence**: Claims about improvement over baselines are supported by experimental results
- **Medium confidence**: Abstract experiences improve generalization compared to raw cases
- **Medium confidence**: Experiences from both correct and incorrect examples are beneficial
- **Low confidence**: Claims about the mechanism of abstraction and its universality across tasks

## Next Checks

1. **Retrieval quality validation**: Implement semantic similarity-based retrieval (e.g., embedding similarity) instead of word matching to assess whether retrieval quality impacts performance
2. **Cross-model generalization**: Test ExpNote with different LLM architectures (e.g., Claude, open-source models) to verify framework effectiveness beyond ChatGPT
3. **Experience composition analysis**: Analyze the actual content of stored experiences to verify they capture task-relevant abstractions rather than superficial patterns