---
ver: rpa2
title: Distance Matters For Improving Performance Estimation Under Covariate Shift
arxiv_id: '2308.07223'
source_url: https://arxiv.org/abs/2308.07223
tags:
- accuracy
- distance
- performance
- estimation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of accurately estimating model
  performance under covariate shift without ground truth labels, which is crucial
  for safe AI deployment. The key insight is that samples lying far from the training
  distribution in the embedding space have unreliable confidence estimates and should
  be flagged during accuracy estimation.
---

# Distance Matters For Improving Performance Estimation Under Covariate Shift

## Quick Facts
- arXiv ID: 2308.07223
- Source URL: https://arxiv.org/abs/2308.07223
- Reference count: 40
- Key outcome: A simple distance-check method improves accuracy estimation under covariate shift, achieving state-of-the-art performance on 10 out of 13 tasks with median relative MAE improvements of 30% over ATC and 13% over GDE.

## Executive Summary
This paper addresses the critical problem of accurately estimating model performance under covariate shift without ground truth labels. The authors propose a simple yet effective "distance-check" method that uses nearest-neighbor distances in the embedding space to identify samples lying far from the training distribution. By flagging these untrustworthy samples, the method significantly improves accuracy estimation quality across 13 diverse image classification tasks. The approach is computationally efficient, doesn't require OOD data for calibration, and can be easily combined with existing confidence-based estimators like ATC and agreement-based estimators like GDE.

## Method Summary
The proposed method uses K-NN distances in the penultimate layer embeddings to identify samples that are far from the training distribution. A distance threshold is determined from the validation set (99th percentile of average distances), and samples exceeding this threshold are rejected from accuracy estimation. This plug-in method can be combined with existing estimators like ATC and GDE to improve their performance. The approach also supports class-wise distance thresholds for cases where class clusters have different densities in the embedding space.

## Key Results
- Median relative MAE improvements of 30% over ATC and 13% over GDE across all tasks
- State-of-the-art performance on 10 out of 13 tasks
- Computationally efficient method that doesn't require OOD data for calibration
- Works across various natural and synthetic distribution shifts including ImageNet variants, CIFAR10-C, WILDS datasets, BREEDS, PACS, and PathMNIST

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Samples far from training distribution in embedding space have unreliable confidence estimates.
- Mechanism: The model's confidence scores become ill-calibrated when inputs are far from the training data manifold, leading to over-confidence and unreliable accuracy predictions.
- Core assumption: Distance in embedding space correlates with epistemic uncertainty.
- Evidence anchors:
  - [abstract]: "under dataset shifts confidence scores may become ill-calibrated if samples are too far from the training distribution"
  - [section 1]: "errors may also arise from the fact that the model has never seen this type of input data and does not know how to respond to such inputs. This is referred to as epistemic uncertainty [9] and is not well captured by softmax outputs"
  - [corpus]: No direct evidence in corpus for this specific mechanism
- Break condition: If the embedding space does not preserve meaningful distance relationships between samples and their epistemic uncertainty.

### Mechanism 2
- Claim: Rejecting samples based on distance threshold improves accuracy estimation.
- Mechanism: By identifying and rejecting samples that are too far from the training distribution, the method avoids using unreliable predictions in the accuracy calculation.
- Core assumption: The distance threshold chosen as the 99th percentile of training validation distances is appropriate for all tasks.
- Evidence anchors:
  - [section 3.2]: "we 'reject' samples whose penultimate-layer embeddings lie in a region 'far' from the ID embedding space"
  - [section 3.2]: "The acceptable threshold is determined on the in-distribution validation set as the 99th-percentile of the average distances observed on this set"
  - [corpus]: No direct evidence in corpus for this specific mechanism
- Break condition: If the validation set is not representative of the training distribution, leading to inappropriate distance thresholds.

### Mechanism 3
- Claim: Class-wise distance thresholds improve estimation further.
- Mechanism: Different classes may have different embedding cluster tightness, so class-specific distance thresholds can better identify unreliable samples.
- Core assumption: Class clusters in embedding space have varying densities that justify different thresholds.
- Evidence anchors:
  - [section 3.2]: "we argue that the quality of the distance threshold can be further improved by defining class-wise distance thresholds"
  - [section 3.2]: "for each class c we compute DistThresholdc by taking the 99th percentile of the average distance distribution of the subset of cases labelled as c in the validation set"
  - [corpus]: No direct evidence in corpus for this specific mechanism
- Break condition: If classes have insufficient samples in validation set to compute reliable thresholds.

## Foundational Learning

- Concept: Covariate shift
  - Why needed here: The entire method addresses performance estimation under covariate shift where input distribution changes but labels remain the same
  - Quick check question: What is the difference between covariate shift and concept shift?

- Concept: Softmax calibration
  - Why needed here: The method relies on understanding why softmax confidence becomes unreliable under distribution shift
  - Quick check question: Why do softmax outputs become over-confident under covariate shift?

- Concept: Epistemic vs aleatoric uncertainty
  - Why needed here: The method targets epistemic uncertainty (model uncertainty due to unfamiliarity with inputs) rather than aleatoric uncertainty (inherent data uncertainty)
  - Quick check question: How does epistemic uncertainty differ from aleatoric uncertainty in terms of what causes it?

## Architecture Onboarding

- Component map:
  Feature extractor (penultimate layer of model) -> K-NN distance calculator -> Distance threshold selector (99th percentile on validation) -> Confidence threshold calculator (ATC method) -> Class-wise threshold handler (optional) -> Accuracy estimator (rejection-based counting)

- Critical path:
  1. Extract embeddings from training data
  2. Fit K-nearest neighbors on training embeddings
  3. Calculate distances from validation data to training neighbors
  4. Determine distance threshold from validation distances
  5. For test samples: calculate distances, apply distance threshold, then apply confidence threshold
  6. Estimate accuracy as proportion of samples passing both checks

- Design tradeoffs:
  - K-NN distance vs Mahalanobis distance (K-NN performs better according to ablation)
  - Single global threshold vs class-wise thresholds (class-wise improves performance but requires more data)
  - Distance threshold percentile (99th chosen empirically, may need adjustment)

- Failure signatures:
  - Overestimation of accuracy when distance threshold is too permissive
  - Underestimation when threshold is too strict
  - Poor performance when validation set is not representative
  - Degraded performance on highly imbalanced datasets with missing classes in validation

- First 3 experiments:
  1. Run ATC-Dist on a simple synthetic dataset where you control the distance between train and test distributions
  2. Compare ATC-Dist vs ATC on a dataset with known covariate shift (like CIFAR10-C with varying corruption levels)
  3. Test class-wise vs global distance thresholds on a dataset with clear class cluster differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of distance-based accuracy estimation methods vary across different types of distribution shifts (e.g., synthetic corruption vs natural subpopulation shifts)?
- Basis in paper: [explicit] The paper mentions that distance-based methods improve performance estimation across a wide range of shifts, but does not provide a detailed comparison of effectiveness across shift types.
- Why unresolved: The paper provides overall performance metrics but lacks a breakdown of effectiveness by shift type.
- What evidence would resolve it: Experiments comparing the performance of distance-based methods specifically on different categories of distribution shifts would clarify their relative strengths.

### Open Question 2
- Question: Can the distance threshold for rejecting samples be optimized dynamically during deployment, rather than being fixed based on the validation set?
- Basis in paper: [inferred] The paper uses a static 99th percentile threshold, but mentions the method relies on the representativeness of the validation set.
- Why unresolved: The paper does not explore adaptive thresholding strategies that could adjust to new data patterns during deployment.
- What evidence would resolve it: A study comparing static vs adaptive thresholding approaches on streaming data would demonstrate the potential benefits of dynamic optimization.

### Open Question 3
- Question: How do distance-based methods perform on tasks with continuous output spaces (e.g., regression) compared to discrete classification tasks?
- Basis in paper: [explicit] The paper focuses exclusively on classification tasks and does not address regression scenarios.
- Why unresolved: The methodology is presented in the context of classification, with no exploration of its applicability to regression problems.
- What evidence would resolve it: Extending the distance-based approach to regression tasks and comparing its performance against existing uncertainty quantification methods would determine its generalizability.

## Limitations

- The effectiveness relies heavily on the quality of the embedding space preserving meaningful distance relationships
- The 99th percentile threshold choice is heuristic and may not be optimal across all tasks
- Class-wise distance thresholds require sufficient validation samples per class to be reliable
- The method only addresses epistemic uncertainty and may not capture other sources of error like concept shift

## Confidence

- **High confidence**: The experimental results showing improved accuracy estimation across 13 diverse tasks, with median relative MAE improvements of 30% over ATC and 13% over GDE. The method's simplicity and computational efficiency are well-supported.
- **Medium confidence**: The theoretical justification for why distance in embedding space correlates with epistemic uncertainty. While intuitive, this connection could benefit from more rigorous theoretical analysis.
- **Medium confidence**: The claim of state-of-the-art performance on 10 out of 13 tasks, as this depends on the specific baseline implementations and evaluation protocols used.

## Next Checks

1. **Embedding Space Analysis**: Systematically evaluate how different model architectures and embedding spaces affect the correlation between K-NN distance and epistemic uncertainty. Test with models using different penultimate layer dimensionalities and architectures.

2. **Threshold Sensitivity Analysis**: Conduct a comprehensive ablation study varying the distance threshold percentile (e.g., 90th, 95th, 99th, 99.9th) to determine optimal thresholds for different task types and distributions.

3. **Cross-Domain Generalization**: Test the method on non-image domains (text, tabular data, audio) to evaluate whether the distance-check principle generalizes beyond image classification tasks.