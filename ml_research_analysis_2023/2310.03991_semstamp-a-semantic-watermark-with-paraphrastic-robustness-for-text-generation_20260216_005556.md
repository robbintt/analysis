---
ver: rpa2
title: 'SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation'
arxiv_id: '2310.03991'
source_url: https://arxiv.org/abs/2310.03991
tags:
- sentence
- paraphrase
- watermark
- text
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SemStamp, a semantic watermarking algorithm
  for text generation that is robust to paraphrase attacks. The core idea is to use
  locality-sensitive hashing (LSH) to partition the semantic space of sentence embeddings,
  and then perform sentence-level rejection sampling to generate watermarked sentences.
---

# SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation

## Quick Facts
- arXiv ID: 2310.03991
- Source URL: https://arxiv.org/abs/2310.03991
- Reference count: 19
- Primary result: Semantic watermarking algorithm robust to paraphrase attacks, achieving 0.981 AUROC on RealNews dataset

## Executive Summary
This paper introduces SemStamp, a semantic watermarking algorithm designed to be robust against paraphrase attacks in text generation. Unlike token-level watermarking methods that partition vocabulary based on token hashes, SemStamp uses locality-sensitive hashing (LSH) on sentence embeddings to partition semantic space and employs sentence-level rejection sampling during generation. The algorithm demonstrates superior robustness to bigram paraphrase attacks while maintaining text quality, achieving an AUROC of 0.981 on the RealNews dataset compared to 0.944 for baseline token-level methods.

## Method Summary
SemStamp uses sentence embeddings from a fine-tuned Sentence-BERT model, partitions semantic space using LSH, and performs rejection sampling during generation to ensure sentences fall within watermarked regions. The method includes contrastive learning fine-tuning for paraphrastic robustness and a margin-based constraint to maintain LSH consistency under paraphrasing. The algorithm is evaluated against a bigram paraphrase attack and compared with token-level watermarking baselines on the RealNews dataset using OPT-1.3B as the language model.

## Key Results
- SemStamp achieves 0.981 AUROC on RealNews dataset, outperforming token-level watermarking (0.944 AUROC)
- Robust to bigram paraphrase attacks while maintaining generation quality (PPL, Ent-3, Rep-3 metrics)
- LSH consistency improves with contrastive fine-tuning, reducing semantic shifts under paraphrasing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic watermarking operates at the sentence level rather than token level, preserving robustness against paraphrase attacks.
- Mechanism: Partitions semantic space using LSH on sentence embeddings, then performs rejection sampling to generate sentences within watermarked regions.
- Core assumption: Paraphrasing alters surface tokens but preserves sentence-level semantics, so sentence embeddings remain in the same LSH partition.
- Evidence anchors:
  - [abstract] "we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences"
  - [section] "Our approach is motivated by the intuition that paraphrasing alters the surface-form tokens but preserves sentence-level semantics"
  - [corpus] Weak evidence - no direct corpus support for this mechanism's effectiveness

### Mechanism 2
- Claim: Contrastive learning fine-tuning makes the sentence encoder more robust to paraphrase attacks.
- Mechanism: Fine-tune SBERT with contrastive loss that pulls paraphrases closer than random negative examples by margin δ.
- Core assumption: A paraphrase-robust encoder produces embeddings where paraphrases remain in same LSH region as originals.
- Evidence anchors:
  - [section] "We fine-tune an off-the-shelf encoder with a contrastive learning objective (Wieting et al., 2022) for paraphrastic robustness"
  - [section] "we add an additional rejection sampling requirement that the sampled sentence s(t) must have the absolute value of cosine similarity with each normal vector n(i) larger than a margin m > 0"
  - [corpus] Weak evidence - paper shows contrastive fine-tuning improves LSH consistency but lacks ablation showing contrastive learning is essential

### Mechanism 3
- Claim: Bigram overlap minimization during paraphrasing effectively weakens token-level watermark detection while preserving paraphrase quality.
- Mechanism: Select paraphrase candidate with fewest bigram overlaps subject to BERTScore constraint.
- Core assumption: Token-level watermarks are especially vulnerable to bigram changes because hashing previous token determines vocabulary partition.
- Evidence anchors:
  - [section] "we propose and explore the bigram paraphrase attack, a simple yet effective variant of the basic sentence-level paraphrase attack"
  - [section] "The bigram paraphrase attack is quite effective against the token-level baseline algorithm, whereas SemStamp is relatively unaffected"
  - [corpus] Weak evidence - paper demonstrates effectiveness but doesn't explain theoretical why bigram changes specifically target token-level watermarks

## Foundational Learning

- Concept: Locality-Sensitive Hashing (LSH) for semantic space partitioning
  - Why needed here: LSH provides efficient dimensionality reduction while preserving similarity in high-dimensional semantic space, enabling practical watermark detection
  - Quick check question: How does LSH ensure that semantically similar sentences are hashed to similar signatures? What would happen if we used random hashing instead?

- Concept: Contrastive learning for semantic representation
  - Why needed here: Contrastive learning explicitly trains representations to distinguish paraphrases from non-paraphrases, crucial for maintaining watermark robustness
  - Quick check question: What is the role of the margin parameter in contrastive loss? How would setting it too high or too low affect watermark robustness?

- Concept: Rejection sampling for controlled generation
  - Why needed here: Rejection sampling enforces that generated sentences fall within specific semantic regions while maintaining fluency from the language model
  - Quick check question: What are the computational trade-offs of rejection sampling? How might we modify the algorithm to sample multiple candidates in parallel?

## Architecture Onboarding

- Component map: Sentence encoder → LSH hasher → Valid/blocked region partitioner → Rejection sampler → Language model
- Critical path: Text generation → Sentence embedding → LSH signature → Region validation → Rejection or acceptance → Output
- Design tradeoffs: Semantic robustness vs. generation speed (rejection sampling), partition granularity vs. LSH consistency, margin size vs. sampling efficiency
- Failure signatures: Low LSH consistency under paraphrasing, high rejection rate during generation, poor detection performance on paraphrased text
- First 3 experiments:
  1. Measure LSH consistency on paraphrased sentences with different margin values to find optimal trade-off
  2. Compare detection performance on paraphrased text with different LSH dimensions
  3. Test generation speed and quality with varying rejection margin and valid region ratio parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SEMSTAMP change with different sentence embedding models, and what are the trade-offs between robustness and quality when using alternative encoders?
- Basis in paper: [inferred] The paper uses a fine-tuned Sentence-BERT model for encoding sentences, but does not explore the impact of using other sentence embedding models or the potential trade-offs involved.
- Why unresolved: The paper does not provide empirical evidence or analysis of how different sentence embedding models might affect the performance of SEMSTAMP, leaving open the question of whether there are better alternatives or specific trade-offs to consider.
- What evidence would resolve it: Conducting experiments with various sentence embedding models (e.g., other contrastive learning-based encoders or domain-specific models) and comparing their impact on SEMSTAMP's robustness and generation quality would provide insights into the trade-offs and potential improvements.

### Open Question 2
- Question: What are the limitations of the bigram paraphrase attack in terms of its effectiveness against more sophisticated paraphrasing techniques, and how might SEMSTAMP be further improved to handle such attacks?
- Basis in paper: [explicit] The paper introduces the bigram paraphrase attack and shows its effectiveness against the baseline token-level watermarking method, but acknowledges that SEMSTAMP is relatively unaffected by this attack.
- Why unresolved: The paper does not explore the limitations of the bigram paraphrase attack or discuss potential improvements to SEMSTAMP to handle more advanced paraphrasing techniques, such as those involving sentence reordering or semantic manipulation.
- What evidence would resolve it: Conducting experiments with more sophisticated paraphrasing techniques and analyzing SEMSTAMP's performance against these attacks would help identify its limitations and guide the development of further improvements to enhance its robustness.

### Open Question 3
- Question: How does the computational overhead of SEMSTAMP scale with increasing text length and complexity, and what are the potential optimizations to improve its efficiency?
- Basis in paper: [inferred] The paper mentions that SEMSTAMP is slower than non-watermarked generation due to rejection sampling, but does not provide a detailed analysis of how the computational overhead scales with text length and complexity.
- Why unresolved: The paper does not discuss the scalability of SEMSTAMP's computational overhead or explore potential optimizations to improve its efficiency, leaving open questions about its practical applicability for longer or more complex texts.
- What evidence would resolve it: Conducting experiments to measure SEMSTAMP's computational overhead for varying text lengths and complexities, and exploring optimization techniques such as parallel processing or more efficient rejection sampling strategies, would provide insights into its scalability and potential improvements.

## Limitations

- LSH consistency under paraphrasing has limited experimental validation, particularly for edge cases where paraphrasing might significantly alter sentence embeddings
- Rejection sampling introduces computational overhead with limited analysis of its impact on generation speed and scalability
- Limited exploration of paraphrase techniques beyond bigram attacks, leaving uncertainty about robustness against other paraphrasing methods

## Confidence

- High: Core mechanism of using LSH for semantic space partitioning and rejection sampling is well-supported by experimental results (AUROC of 0.981)
- Medium: Contrastive learning fine-tuning for semantic robustness is supported but lacks ablation studies comparing different methods
- Low: Bigram overlap minimization effectiveness claims are based on limited empirical evidence with unclear theoretical justification

## Next Checks

1. Measure LSH consistency across diverse paraphrase techniques (syntactic transformations, lexical substitutions) to assess robustness beyond bigram attacks
2. Quantify computational overhead of rejection sampling by measuring generation speed and comparing with token-level watermarking under different margin settings
3. Evaluate SemStamp's performance on larger datasets and real-world scenarios to assess practical viability and scalability