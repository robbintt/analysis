---
ver: rpa2
title: 'Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open
  Information Extraction'
arxiv_id: '2305.13981'
source_url: https://arxiv.org/abs/2305.13981
tags:
- syntactic
- robust
- latexit
- robustness
- cliques
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ROBUST, the first large-scale benchmark designed
  to evaluate the robustness of OpenIE models to syntactic and expressive distribution
  shifts. The benchmark consists of 1,272 cliques of sentences, each clique containing
  3-5 sentences with the same knowledge meaning but different syntactic and expressive
  forms.
---

# Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction

## Quick Facts
- arXiv ID: 2305.13981
- Source URL: https://arxiv.org/abs/2305.13981
- Reference count: 40
- Models evaluated show average F1 score degradation of 18 percentage points on ROBUST compared to CaRB benchmark

## Executive Summary
This paper introduces ROBUST, the first large-scale benchmark for evaluating the robustness of Open Information Extraction (OpenIE) models to syntactic and expressive distribution shifts. ROBUST consists of 1,272 cliques of sentences, each containing 3-5 paraphrases with the same knowledge meaning but different syntactic forms. The benchmark reveals significant performance drops (average 18 F1 points) for 6 typical OpenIE models when evaluated on these distribution-shifted inputs compared to standard benchmarks. This demonstrates that existing OpenIE models are brittle to realistic variations in linguistic expression, highlighting the need for future research to prioritize robustness in model design and evaluation.

## Method Summary
ROBUST is constructed by first generating syntactically diverse paraphrases from existing CaRB sentences using a controllable paraphraser, then manually annotating these paraphrases for correctness and knowledge invariance. The benchmark evaluates model robustness using a clique-wise worst-case F1 metric rather than standard sentence-pair matching, penalizing models that fail on any syntactic variant within a knowledge-invariant clique. This approach provides a more realistic assessment of model reliability across distribution shifts that occur in open-world scenarios.

## Key Results
- OpenIE models show average F1 score degradation of 18 percentage points on ROBUST compared to CaRB benchmark
- Significant performance variance exists within cliques, indicating models fail inconsistently across syntactic variants
- Manual annotation confirms high quality of paraphrases with consistent knowledge meaning across syntactic variations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The ROBUST benchmark exposes OpenIE model brittleness by testing across cliques where sentences share meaning but differ in syntax/expression.
- **Mechanism:** Models trained on standard benchmarks are tuned for lexical and syntactic patterns in their training distribution. When evaluated on ROBUST, where these patterns vary while meaning stays constant, performance drops because the models rely on superficial cues rather than robust semantic understanding.
- **Core assumption:** OpenIE models overfit to the syntactic distribution of their training and evaluation data, failing to generalize to distribution shifts in realistic scenarios.
- **Evidence anchors:**
  - [abstract] "Experiments on 6 typical OpenIE models... show significant performance drops on ROBUST compared to the standard CaRB benchmark, with an average F1 score degradation of 18 percentage points."
  - [section] "the robustness of existing OpenIE models is far from satisfied, and suggest that future research should pay more attention to the model construction and evaluation involving robustness ability."
- **Break condition:** If models are explicitly trained with data augmentation that introduces syntactic diversity, or if models are based on architectures that are invariant to syntactic form (e.g., semantic parsing with meaning representations), the performance gap would shrink or disappear.

### Mechanism 2
- **Claim:** Clique-wise evaluation (worst-case F1 within a clique) is a stricter and more realistic measure of robustness than sentence-pair matching.
- **Mechanism:** Traditional metrics evaluate each sentence independently. ROBUST evaluates the worst-case performance across all sentences in a clique, penalizing models that fail on any syntactic/expressional variant of the same knowledge. This reflects real-world need for consistent extraction across paraphrases or rephrasings.
- **Core assumption:** Real-world applications require consistent knowledge extraction across varying linguistic forms; a single failure in a paraphrase chain is unacceptable.
- **Evidence anchors:**
  - [abstract] "By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques."
  - [section] "We extend this scorer on cliques to calculate the robustness scores... We naturally calculate the robustness scores of a model on each clique... select the scores from a sentence with the worst F1 as the ultimate robustness scores."
- **Break condition:** If downstream tasks can tolerate occasional extraction failures (e.g., noisy user queries in IR), the worst-case metric may be overly strict and not reflective of practical utility.

### Mechanism 3
- **Claim:** The human-annotated, knowledge-invariant cliques in ROBUST preserve naturalness and avoid synthetic artifacts, making robustness evaluation realistic.
- **Mechanism:** Synthetic paraphrases often introduce unnatural or semantically divergent sentences. ROBUST uses syntactic controllable paraphrasers followed by human correction and annotation to ensure paraphrases are both syntactically diverse and semantically equivalent, preserving natural language distribution.
- **Core assumption:** Human-annotated data with controlled syntactic variation better reflects real-world linguistic diversity than purely synthetic generation.
- **Evidence anchors:**
  - [abstract] "We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms."
  - [section] "We first adopt a syntactically controllable paraphraser with diversified syntactic sampling and expressive filtering strategies to generate paraphrases... We then design a two-stage annotation pipeline to perform sentence correction and knowledge extraction for each individual paraphrase in cliques based on human experts."
- **Break condition:** If the paraphraser and annotation pipeline introduce biases (e.g., over-correcting to certain syntactic forms), the benchmark may not fully capture the breadth of real-world variation.

## Foundational Learning

- **Concept:** Syntactic distance metrics (e.g., HWS distance, Convolutional Tree Kernels)
  - **Why needed here:** ROBUST relies on quantifying syntactic divergence between sentences in a clique to analyze model robustness. Understanding these metrics is essential to interpret the correlation between syntactic variation and performance drop.
  - **Quick check question:** How does HWS distance differ from CTK similarity in measuring syntactic divergence, and why might one be preferred over the other in ROBUST's context?

- **Concept:** Clique-based evaluation vs. sentence-pair evaluation
  - **Why needed here:** ROBUST introduces a new evaluation paradigm that assesses consistency across a set of paraphrases rather than individual sentences. Understanding this shift is critical for correctly interpreting robustness scores.
  - **Quick check question:** What is the key difference between CaRB's pairwise matching and ROBUST's clique-wise worst-case F1, and how does this affect model ranking?

- **Concept:** Paraphrase generation with syntactic control
  - **Why needed here:** ROBUST uses syntactically controllable paraphrasers to generate diverse yet meaning-preserving sentences. Understanding this technique is important for grasping how the benchmark creates realistic variation.
  - **Quick check question:** How does specifying target syntactic trees in paraphrase generation help ensure both diversity and semantic equivalence?

## Architecture Onboarding

- **Component map:** AESOP paraphraser → Human annotation pipeline → ROBUST scorer (clique-wise worst-case F1) → Analysis tools (HWS distance, CTK similarity)
- **Critical path:** 1) Generate paraphrases with syntactic diversity 2) Human correct and annotate for knowledge invariance 3) Compute robustness scores using clique-wise worst-case F1 4) Analyze correlation between syntactic divergence and performance variance
- **Design tradeoffs:**
  - Synthetic vs. human-annotated paraphrases: Synthetic is scalable but risks unnaturalness; human is accurate but costly.
  - Worst-case F1 vs. average F1: Worst-case is stricter and more realistic for robustness; average may be more forgiving but less indicative of reliability.
  - HWS distance vs. CTK similarity: HWS is efficient but may overcount repeated spans (fixed in ROBUST); CTK is more accurate but computationally heavier.
- **Failure signatures:**
  - If paraphrases are not truly semantically equivalent, robustness scores will be artificially low due to meaning drift, not syntactic variation.
  - If the worst-case F1 is dominated by a single outlier sentence, the robustness score may not reflect overall model capability.
  - If syntactic distance metrics are poorly calibrated, correlations with performance variance may be misleading.
- **First 3 experiments:**
  1. **Baseline sanity check:** Run OpenIE6 on ROBUST and verify the reported ~18% F1 drop vs. CaRB.
  2. **Synthetic vs. human paraphrase comparison:** Generate a small set of synthetic paraphrases and compare their robustness scores to human-annotated ones to assess naturalness impact.
  3. **Correlation analysis replication:** Compute HWS distance and CTK similarity for a subset of cliques and verify the reported inverse correlation with performance variance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do pre-trained language models perform on ROBUST compared to traditional OpenIE systems?
- **Basis in paper:** [explicit] The authors mention evaluating ChatGPT but do not provide comprehensive results for other PLMs.
- **Why unresolved:** The paper only evaluates ChatGPT, leaving a gap in understanding how other PLMs like BERT or T5 perform on the ROBUST benchmark.
- **What evidence would resolve it:** Experimental results comparing the performance of various PLMs (e.g., BERT, T5, RoBERTa) on ROBUST would clarify their robustness to syntactic and expressive variations.

### Open Question 2
- **Question:** Can the robustness of OpenIE models be improved through specific training strategies?
- **Basis in paper:** [inferred] The authors suggest that future research should focus on building models that account for robustness to distribution changes.
- **Why unresolved:** The paper identifies the robustness issue but does not propose or evaluate any specific training strategies to address it.
- **What evidence would resolve it:** Developing and testing training strategies (e.g., data augmentation, adversarial training) designed to improve robustness on ROBUST would provide insights into potential solutions.

### Open Question 3
- **Question:** How does the robustness of OpenIE models vary across different domains?
- **Basis in paper:** [explicit] The authors mention that ROBUST is built from CaRB, which originates from OIE2016 based on Wall Street Journal and Wikipedia.
- **Why unresolved:** The paper does not explore how the robustness of models generalizes to other domains beyond the general domain covered by ROBUST.
- **What evidence would resolve it:** Evaluating OpenIE models on ROBUST and additional domain-specific datasets would reveal the extent of their robustness across different domains.

## Limitations

- ROBUST relies on a relatively small, human-annotated dataset (1,272 cliques) compared to the vast scale of real-world text variation
- The benchmark focuses specifically on syntactic and expressive distribution shifts, potentially missing other robustness dimensions like domain adaptation or noise robustness
- Human annotation process introduces potential biases in how knowledge invariance is defined and annotated

## Confidence

**High confidence:** The empirical finding that OpenIE models show significant performance degradation (18 F1 points) on ROBUST compared to CaRB is well-supported by the experimental results and represents a robust empirical observation about current model limitations.

**Medium confidence:** The claim that ROBUST provides a more realistic evaluation of real-world robustness is reasonable but depends on the assumption that human-annotated, syntactically diverse paraphrases better represent natural language variation than synthetic data.

**Low confidence:** The generalizability of ROBUST's findings to all forms of distribution shift in OpenIE is uncertain, as the benchmark specifically targets syntactic and expressive variations rather than broader distributional changes.

## Next Checks

1. **Benchmark Completeness Validation:** Test whether the 18% performance drop persists across additional OpenIE models beyond the six evaluated, particularly newer models with improved robustness to syntactic variation.

2. **Synthetic vs. Human Paraphrase Comparison:** Conduct a controlled experiment comparing model performance on purely synthetic paraphrases versus human-annotated ones to quantify the impact of annotation quality on robustness scores.

3. **Distribution Shift Generalization:** Evaluate OpenIE models on other benchmarks that test different types of distribution shifts (e.g., domain adaptation, noise robustness) to determine whether the performance degradation observed on ROBUST extends to other robustness dimensions.