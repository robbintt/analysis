---
ver: rpa2
title: 'NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely
  Low-Resource Languages'
arxiv_id: '2309.10661'
source_url: https://arxiv.org/abs/2309.10661
tags:
- languages
- language
- indonesian
- translation
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares three corpus collection methods\u2014online\
  \ scraping, human translation, and paragraph writing by native speakers\u2014for\
  \ 12 underrepresented Indonesian local languages. The study finds that paragraph\
  \ writing produces the highest-quality corpora in terms of lexical diversity and\
  \ cultural relevance, while online scraping suffers from low lexical diversity and\
  \ high loan word ratios in some languages."
---

# NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages

## Quick Facts
- **arXiv ID**: 2309.10661
- **Source URL**: https://arxiv.org/abs/2309.10661
- **Reference count**: 40
- **Primary result**: Paragraph writing by native speakers produces higher-quality corpora than online scraping for underrepresented Indonesian languages, and classical ML baselines outperform neural models on these languages

## Executive Summary
This paper investigates three methods for constructing high-quality corpora for 12 underrepresented Indonesian local languages: online scraping, human translation, and native speaker paragraph writing. The study finds that paragraph writing produces corpora with higher lexical diversity and cultural relevance, while Wikipedia scraping suffers from high loan word ratios and low diversity in some languages. The authors release NusaWrites, a benchmark covering 5 natural language understanding tasks and 1 generation task across these languages. Empirical experiments show that both fine-tuned multilingual models and zero-shot large language models significantly underperform classical machine learning baselines on most languages, indicating these languages are linguistically distinct from existing models' pretraining data.

## Method Summary
The researchers collected data for 12 Indonesian local languages using three methods: online scraping of Wikipedia, human translation of 9,449 sentences from Indonesian, and paragraph writing by native speakers on 20 topics with 20 subtopics each. Quality control involved both manual and automatic validation. They constructed the NusaWrites benchmark with 5 NLU tasks (emotion, sentiment, topic, rhetoric mode classification, and machine translation) and evaluated classical machine learning baselines (Naive Bayes, Logistic Regression, SVM), fine-tuned multilingual models (mBERT, XLM-R, IndoBERT), and zero-shot large language models. Performance was measured using accuracy, F1-score, and perplexity metrics.

## Key Results
- Paragraph writing produces corpora with higher lexical diversity (MATTR, MTLD, MSTTR) and lower loan word ratios compared to Wikipedia scraping
- Classical machine learning baselines outperform fine-tuned multilingual models and zero-shot LLMs on most languages except those closely related to Indonesian
- The target languages show distinct linguistic features not well represented in existing multilingual model pretraining data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paragraph writing by native speakers produces corpora with higher lexical diversity and cultural relevance than online scraping
- Mechanism: Native speakers generate text grounded in local cultural contexts and everyday vocabulary, while Wikipedia scraping tends to include formal or borrowed terms not used in daily speech
- Core assumption: Native speakers naturally use culturally relevant vocabulary and avoid unnecessary foreign loan words when writing for everyday contexts
- Evidence anchors: NusaParagraph shows higher lexical diversity metrics compared to Wikipedia for low-resource languages; NusaParagraph has much lower loan word ratios than Wikipedia

### Mechanism 2
- Claim: Existing multilingual models fail to generalize to underrepresented languages due to linguistic distinctiveness
- Mechanism: Models pretrained on high-resource languages capture patterns not present in the low-resource target languages, so fine-tuning or zero-shot transfer yields poor performance
- Core assumption: The linguistic features of the target languages are sufficiently different from those in the pretraining data to prevent effective transfer
- Evidence anchors: Fine-tuned multilingual models and zero-shot LLMs significantly underperform classical baselines; performance comparison shows classical baselines outperforming fine-tuned models on most languages

### Mechanism 3
- Claim: Classical machine learning methods remain competitive with neural approaches in extremely low-resource settings
- Mechanism: With limited data, simple statistical models can capture reliable patterns without overfitting, whereas neural models may not learn meaningful representations from scarce examples
- Core assumption: The amount of available data is too small for neural models to outperform simpler statistical baselines
- Evidence anchors: Classical baselines achieve performance comparable to fine-tuned Indonesian and multilingual models across NLU and NLG tasks; dataset sizes for target languages are extremely small

## Foundational Learning

- **Concept: Lexical diversity metrics (MATTR, MTLD, MSTTR)**
  - Why needed here: To compare corpus quality independent of size differences between collection methods
  - Quick check question: What does a higher MATTR score indicate about a corpus?

- **Concept: Cross-lingual transfer limitations**
  - Why needed here: Explains why multilingual models fail on underrepresented languages despite pretraining on many languages
  - Quick check question: What factor most limits cross-lingual transfer in this study?

- **Concept: Cultural relevance in language corpora**
  - Why needed here: Justifies the use of native speaker paragraph writing over scraped data
  - Quick check question: How does cultural relevance affect NLP model performance?

## Architecture Onboarding

- **Component map**: Data collection (Wikipedia scraping, human translation, paragraph writing) → Corpus construction (NusaTranslation, NusaParagraph) → Benchmark creation (NusaWrites) → Model evaluation (classical ML, fine-tuned LMs, zero-shot LLMs)
- **Critical path**: Native speaker recruitment → Annotation guideline development → Quality control → Corpus assembly → Benchmark task definition → Model training and evaluation
- **Design tradeoffs**: Manual annotation ensures quality but is expensive and slow; scraping is fast but produces lower quality data; classical baselines are simple but may not scale well with larger datasets
- **Failure signatures**: Low lexical diversity scores, high loan word ratios, classical models outperforming neural models, zero-shot prompting failing on language-specific tasks
- **First 3 experiments**:
  1. Compare MATTR/MTLD/MSTTR scores between Wikipedia and NusaParagraph for a target language
  2. Train a small LM on NusaParagraph data and evaluate perplexity on held-out text
  3. Fine-tune a multilingual model on NusaTranslation data and evaluate on sentiment analysis task

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the lexical diversity metrics (MATTR, MTLD, MSTTR) correlate with downstream task performance for each corpus collection method?
  - Basis in paper: The paper compares three corpus collection methods using lexical diversity metrics but doesn't establish correlation with downstream task performance
  - Why unresolved: Higher diversity scores don't necessarily translate to better task performance
  - What evidence would resolve it: Correlation coefficients between lexical diversity metrics and task-specific performance metrics across different languages and collection methods

- **Open Question 2**: What is the optimal balance between corpus size and lexical diversity when collecting data for extremely low-resource languages?
  - Basis in paper: Wikipedia has larger token counts but lower lexical diversity, while NusaParagraph has smaller size but higher diversity
  - Why unresolved: The paper doesn't explore whether increasing corpus size at the expense of some lexical diversity would improve downstream task performance
  - What evidence would resolve it: Controlled experiments varying corpus size while maintaining different levels of lexical diversity, and measuring downstream task performance

- **Open Question 3**: How does the quality of human translation (NusaTranslation) compare to machine translation when using the collected corpora as training data?
  - Basis in paper: The paper collects human-translated data but only uses it for training human baselines
  - Why unresolved: The paper doesn't explore whether human-translated corpora could train machine translation systems that outperform classical baselines
  - What evidence would resolve it: Training machine translation models on NusaTranslation corpus and comparing performance against classical baselines

## Limitations
- The study's conclusions about neural models' poor performance may be partially attributable to hyperparameter tuning rather than fundamental model limitations
- The cultural relevance assessment relies on loan word ratios as a proxy, which may not fully capture nuanced cultural appropriateness in language use
- The paper doesn't provide detailed hyperparameter search procedures for fine-tuning experiments

## Confidence
- **High confidence**: The comparative quality differences between paragraph writing and scraping methods, supported by multiple lexical diversity metrics and loan word ratio measurements
- **Medium confidence**: The claim about classical models outperforming neural approaches, as this depends on implementation details and hyperparameter choices not fully specified
- **Medium confidence**: The assertion that target languages are linguistically distinct from pretraining data, based on language family analysis but without direct linguistic feature comparisons

## Next Checks
1. Conduct a systematic hyperparameter sweep for fine-tuning experiments to establish whether poor performance stems from model architecture or suboptimal training configurations
2. Perform qualitative human evaluation of cultural relevance by native speakers to validate loan word ratio as an adequate proxy metric
3. Test zero-shot performance on related high-resource languages to establish whether the transfer failure is specific to the target languages or a more general low-resource phenomenon