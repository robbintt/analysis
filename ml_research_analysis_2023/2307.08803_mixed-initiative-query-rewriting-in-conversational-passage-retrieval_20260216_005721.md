---
ver: rpa2
title: Mixed-initiative Query Rewriting in Conversational Passage Retrieval
arxiv_id: '2307.08803'
source_url: https://arxiv.org/abs/2307.08803
tags:
- query
- retrieval
- trec
- ranking
- cast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores mixed-initiative query reformulation for conversational
  passage retrieval in the TREC CAsT 2022 task. The authors propose a system that
  proactively generates clarifying questions to resolve ambiguities in user queries
  (e.g., incomplete, referential, or descriptive) and reformulates queries based on
  user feedback.
---

# Mixed-initiative Query Rewriting in Conversational Passage Retrieval

## Quick Facts
- arXiv ID: 2307.08803
- Source URL: https://arxiv.org/abs/2307.08803
- Reference count: 6
- Primary result: Mixed-initiative query reformulation achieved NDCG@3 of 0.452 on TREC CAsT 2022, significantly outperforming automatic methods

## Executive Summary
This paper addresses the challenge of conversational passage retrieval by introducing a mixed-initiative query reformulation system for the TREC CAsT 2022 task. The approach proactively detects ambiguities in user queries (incomplete, referential, or descriptive) and generates clarifying questions to resolve them through user interaction. A multi-stage retrieval pipeline combining sparse and dense first-stage ranking with T5-based reranking is employed, with reciprocal rank fusion combining multiple ranking runs. Experiments demonstrate that incorporating user feedback through mixed-initiative reformulation significantly improves retrieval performance compared to fully automatic neural and rule-based approaches.

## Method Summary
The system implements a four-stage retrieval pipeline with query reformulation as the first step. For mixed-initiative reformulation, the system detects three types of ambiguities in user queries and generates clarifying questions accordingly. User responses are parsed and incorporated into the raw query before passing to the retrieval pipeline. The first stage uses BM25 (sparse) and TCT-ColBERT (dense) ranking, followed by MonoT5 (pointwise) and DuoT5 (pairwise) reranking. Reciprocal rank fusion combines multiple ranking runs from different reformulators and ranking methods. The T5 rewriter is fine-tuned on TREC 2019-2020 data for domain adaptation.

## Key Results
- Mixed-initiative approach achieved NDCG@3 of 0.452 on TREC CAsT 2022, outperforming non-mixed-initiative best run (0.325)
- Fine-tuned T5 rewriter outperformed original T5 with pre-trained weights on first-stage retrieval
- BM25 with parameters k1=1.24 and b=0.9, combined with TCT-ColBERT, provided effective first-stage ranking
- Reciprocal rank fusion effectively combined multiple ranking runs from different reformulators

## Why This Works (Mechanism)

### Mechanism 1
Mixed-initiative query reformulation outperforms fully automatic methods by resolving ambiguities that generative models cannot handle reliably. The system proactively detects three types of ambiguities (incomplete, referential, descriptive) and asks users clarifying questions. User answers are parsed and incorporated into the raw query, providing explicit disambiguation signals. Core assumption: User-provided clarifications are more accurate than model-generated reformulations for resolving specific ambiguity types.

### Mechanism 2
Multi-stage retrieval pipeline with sparse-dense fusion improves recall while rerankers improve precision. BM25 (sparse) and TCT-ColBERT (dense) first-stage ranking complement each other's lexical and semantic matching strengths. MonoT5 (pointwise) and DuoT5 (pairwise) rerankers refine the initial rankings. Reciprocal rank fusion combines multiple ranking runs. Core assumption: Different ranking methods capture different aspects of relevance, and their combination yields better overall performance than any single method.

### Mechanism 3
Fine-tuning T5 on domain-specific data improves query reformulation quality compared to using pre-trained weights. The T5 rewriter is fine-tuned on TREC 2019 and 2020 data, adapting it to the conversational search domain and reducing ambiguity resolution errors. Core assumption: Domain adaptation of generative models improves their performance on domain-specific tasks by learning task-specific patterns.

## Foundational Learning

- Concept: Ambiguity types in conversational queries (incomplete, referential, descriptive)
  - Why needed here: The mixed-initiative system specifically targets these three ambiguity types for clarification.
  - Quick check question: What are the three ambiguity types the system detects, and how does each manifest in user queries?

- Concept: Multi-stage retrieval architecture
  - Why needed here: The paper builds on this established framework, adding mixed-initiative reformulation as a preprocessing step.
  - Quick check question: What are the four main stages in the retrieval pipeline, and what is the purpose of each?

- Concept: Reciprocal rank fusion
  - Why needed here: Used to combine multiple ranking runs from different reformulators and ranking methods.
  - Quick check question: How does reciprocal rank fusion combine ranked lists, and why is it preferred over simple score averaging?

## Architecture Onboarding

- Component map: Query reformulation → First-stage ranking (BM25 + TCT-ColBERT) → Second-stage reranking (MonoT5 + DuoT5) → Fusion. Mixed-initiative reformulation adds a user interaction layer before reformulation.
- Critical path: User query → Ambiguity detection → Question generation → User answer → Query reformulation → First-stage ranking → Reranking → Fusion → Final results
- Design tradeoffs: Mixed-initiative approach improves accuracy but adds latency and depends on user cooperation; multi-stage pipeline increases complexity but improves performance.
- Failure signatures: Poor user answers lead to ineffective reformulation; low-quality fusion results if one ranking method consistently underperforms; model hallucination in T5 reformulations.
- First 3 experiments:
  1. Compare NDCG@3 with and without mixed-initiative reformulation on TREC CAsT 2022
  2. Test different numbers of canonical passages considered by T5 rewriter (0-3) for optimal performance
  3. Evaluate fusion performance with different numbers of top-probable T5 outputs (1, 3, 5, 7, 10)

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of mixed-initiative query reformulation compare to fully automated neural approaches across different types of query ambiguities (incomplete, referential, descriptive)? While the paper shows overall superiority of mixed-initiative approach, it doesn't break down performance by specific ambiguity types or analyze which ambiguity types benefit most from user interaction.

### Open Question 2
What is the optimal number of clarifying questions to ask per query when implementing mixed-initiative query reformulation? The paper mentions only one interaction is allowed per raw query and discusses priority ordering when multiple ambiguities are detected, suggesting this is a design constraint worth exploring.

### Open Question 3
How does the quality and usefulness of user responses impact retrieval performance in mixed-initiative query reformulation? The paper mentions that some received answers had unexpected bad quality and that authors manually replaced bad-quality answers, indicating awareness of response quality issues.

### Open Question 4
Would incorporating contextual information from previous conversation turns beyond just the most recent canonical passage improve query reformulation performance? The paper explicitly tested whether considering multiple canonical passages helps T5 reformulator, finding that using only the most recent passage performed best, but this question remains about broader contextual incorporation.

## Limitations

- The mixed-initiative approach introduces critical dependency on user cooperation and answer quality, with no analysis of how answer quality affects downstream retrieval performance
- Experimental validation focuses solely on TREC CAsT datasets without external validation across different conversational search domains
- The paper does not address potential user fatigue from repeated clarification requests or establish guidelines for optimal question frequency

## Confidence

- **High confidence**: The multi-stage retrieval pipeline architecture (BM25 + TCT-ColBERT → MonoT5 + DuoT5 → fusion) and its implementation details are well-specified and reproducible
- **Medium confidence**: The effectiveness of mixed-initiative reformulation versus automatic methods is demonstrated on the CAsT datasets, but the lack of user behavior analysis and external validation limits confidence in real-world applicability
- **Medium confidence**: The superiority of fine-tuned T5 over pre-trained weights for query reformulation is supported by internal comparisons, but the specific impact of fine-tuning data quality and quantity is not thoroughly explored

## Next Checks

1. **User behavior analysis**: Conduct a user study to measure response quality to clarifying questions, identify patterns in user refusal or low-quality answers, and establish thresholds for when to proceed without clarification versus requesting alternative responses.

2. **External domain validation**: Test the mixed-initiative system on conversational search datasets from different domains (e.g., e-commerce, customer support) to evaluate generalizability and identify domain-specific adaptation requirements.

3. **Question generation optimization**: Systematically evaluate different strategies for generating clarifying questions (e.g., open-ended vs. multiple-choice, question phrasing variations) and measure their impact on both user response quality and subsequent retrieval performance.