---
ver: rpa2
title: 'ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving
  ASR Robustness in Spoken Language Understanding'
arxiv_id: '2311.11375'
source_url: https://arxiv.org/abs/2311.11375
tags:
- learning
- contrastive
- transcripts
- proc
- manual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes ML-LMCL, a novel framework to improve automatic
  speech recognition (ASR) robustness in spoken language understanding (SLU). The
  key ideas are: 1) Mutual learning between models trained on manual transcripts and
  ASR transcripts to share knowledge.'
---

# ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding

## Quick Facts
- arXiv ID: 2311.11375
- Source URL: https://arxiv.org/abs/2311.11375
- Reference count: 6
- Key outcome: ML-LMCL improves SLU accuracy from 85.64% to 89.16% on SLURP dataset compared to SpokenCSE

## Executive Summary
This paper introduces ML-LMCL, a novel framework that improves automatic speech recognition (ASR) robustness in spoken language understanding (SLU) through three key innovations: mutual learning between models trained on manual and ASR transcripts, large-margin contrastive learning with distance polarization regularization, and a cyclical annealing schedule to mitigate KL vanishing. The framework addresses the challenge of treating manual and ASR transcripts differently while maintaining semantic similarity between them. Experiments on three datasets (SLURP, ATIS, TREC6) demonstrate state-of-the-art performance, with ML-LMCL significantly outperforming previous best models.

## Method Summary
ML-LMCL combines mutual learning, large-margin contrastive learning with distance polarization, and cyclical annealing in a two-stage process. First, RoBERTa is pre-trained using self-supervised contrastive learning with a distance polarization regularizer. Then, two RoBERTa models are fine-tuned simultaneously—one on manual transcripts and one on ASR transcripts—using mutual learning to share knowledge, supervised contrastive learning to improve discrimination, and self-distillation with cyclical annealing to mitigate KL vanishing. The framework jointly optimizes intent detection accuracy while maintaining robustness to ASR errors.

## Key Results
- ML-LMCL achieves 89.16% accuracy on SLURP dataset, improving from 85.64% (SpokenCSE baseline)
- State-of-the-art performance on all three datasets (SLURP, ATIS, TREC6)
- Effective joint accuracy improvement for scenario + action classification on SLURP
- Consistent improvements across varying ASR error rates and dataset sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutual learning between models trained on manual transcripts and ASR transcripts improves ASR robustness by iteratively sharing knowledge between models with complementary strengths.
- Mechanism: Two models are trained simultaneously on clean manual transcripts and ASR transcripts respectively, using Jensen-Shannon divergence to encourage the models to mimic each other. The model trained on clean transcripts has higher accuracy while the model trained on ASR transcripts is more robust to errors. By mutual learning, each model benefits from the strengths of the other.
- Core assumption: The errors in ASR transcripts contain complementary information to the clean manual transcripts that can improve robustness when shared between models.
- Evidence anchors:
  - [abstract] "we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models"
  - [section 2.2] "Mutual learning allows effective dual knowledge transfer (Liao et al., 2020; Zhao et al., 2021; Zhu et al., 2021), which can improve the performance"
- Break condition: If the ASR errors are too severe or systematic (e.g., consistently misrecognizing key domain terms), the mutual learning may reinforce incorrect patterns rather than improving robustness.

### Mechanism 2
- Claim: Large-margin contrastive learning with a distance polarization regularizer improves discrimination between semantically similar and dissimilar pairs while avoiding pushing away similar pairs.
- Mechanism: Conventional contrastive learning pushes away all pairs, which can separate semantically similar pairs (like clean and ASR versions of the same utterance). The distance polarization regularizer encourages pairwise distances to fall into extreme regions (very similar or very dissimilar) rather than the margin region, creating larger margins that better distinguish clusters while keeping similar pairs close.
- Core assumption: The margin region between similar and dissimilar pairs is where most errors occur, and pushing distances to extremes improves discrimination.
- Evidence anchors:
  - [abstract] "We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible"
  - [section 2.1] "We suppose that the matrix D consists of distances Dij and there exists 0 < δ+ < δ- < 1 where the intra-class distances are smaller than δ+ while the inter-class distances are larger than δ-. The proposed distance polarization regularizer Lreg is as follows..."
- Break condition: If the margin parameters (δ+, δ-) are poorly chosen relative to the true data distribution, the regularizer may not effectively separate clusters or may over-separate them.

### Mechanism 3
- Claim: Cyclical annealing schedule mitigates KL vanishing by progressively increasing the KL divergence coefficient during training.
- Mechanism: The coefficient for KL divergence in the self-distillation loss increases from 0 to 1 over several iterations, then stays at 1. This prevents the KL term from vanishing early in training when the model predictions are still unreliable.
- Core assumption: KL vanishing occurs when the KL divergence coefficient is too high early in training, causing unstable gradients or preventing learning.
- Evidence anchors:
  - [abstract] "we use a cyclical annealing schedule to mitigate KL vanishing issue"
  - [section 2.5] "To mitigate KL vanishing issue, we adopt a cyclical annealing schedule, which is also applied for this purpose in Fu et al. (2019); Zhao et al. (2021)"
- Break condition: If the cyclical schedule parameters (R, G) are poorly chosen, the KL term may still vanish or may not have enough influence on the final model.

## Foundational Learning

- Concept: Contrastive learning and distance metrics
  - Why needed here: The paper relies heavily on contrastive learning with custom distance regularization to improve ASR robustness. Understanding cosine similarity, distance metrics, and contrastive loss formulations is essential.
  - Quick check question: What is the difference between supervised and self-supervised contrastive learning, and when would you use each?

- Concept: Mutual learning and knowledge distillation
  - Why needed here: The mutual learning framework is central to the approach. Understanding how two models can teach each other versus traditional teacher-student distillation is important.
  - Quick check question: How does Jensen-Shannon divergence differ from KL divergence in the context of mutual learning between two models?

- Concept: Self-distillation and KL vanishing
  - Why needed here: The paper uses self-distillation with a cyclical annealing schedule to mitigate KL vanishing. Understanding when and why KL vanishing occurs is important.
  - Quick check question: What causes KL vanishing in self-distillation, and how does a cyclical annealing schedule address this?

## Architecture Onboarding

- Component map: Pre-training (RoBERTa + self-supervised contrastive learning + distance polarization) -> Fine-tuning (two RoBERTa models + mutual learning + supervised contrastive learning + self-distillation with cyclical annealing)
- Critical path: Pre-train RoBERTa on SLU corpus → Fine-tune with mutual learning between two instances → Apply supervised contrastive learning and self-distillation
- Design tradeoffs: Using two separate models increases parameters and training cost but enables mutual learning. The distance polarization regularizer adds complexity but improves discrimination. The cyclical annealing schedule adds hyperparameter tuning but mitigates KL vanishing.
- Failure signatures: If accuracy degrades significantly when using manual transcripts (w/o manual transcripts row in Table 2), the mutual learning component may not be working. If performance drops when removing Lreg, the distance polarization may be important. If performance drops when removing cyclical annealing, KL vanishing may be occurring.
- First 3 experiments:
  1. Verify that the pre-training stage improves representation quality by checking embedding similarity between clean and ASR transcripts
  2. Test mutual learning by comparing a single model trained on combined data versus two models with mutual learning
  3. Validate the distance polarization regularizer by visualizing pairwise distances with and without the regularizer using PCA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed framework perform if ASR transcripts were not available during fine-tuning?
- Basis in paper: [explicit] The paper mentions this as a future direction, stating "Future work will focus on improving ASR robustness with only clean manual transcriptions."
- Why unresolved: The current framework relies on having both manual and ASR transcripts during fine-tuning for mutual learning, which may not always be available due to ASR system constraints or privacy concerns.
- What evidence would resolve it: An experiment comparing the performance of the proposed framework with and without ASR transcripts during fine-tuning on the same datasets.

### Open Question 2
- Question: How much does the computational overhead of the proposed framework impact its practical deployment compared to baseline models?
- Basis in paper: [inferred] The paper mentions that "The training and inference runtime of ML-LMCL is larger than that of baselines" and attributes this to having more parameters.
- Why unresolved: While the paper acknowledges the increased computational cost, it does not provide quantitative comparisons of training/inference times or GPU memory requirements between the proposed framework and baselines.
- What evidence would resolve it: Detailed measurements of training/inference time and GPU memory usage for both the proposed framework and baseline models on the same hardware.

### Open Question 3
- Question: How sensitive is the proposed framework's performance to the choice of hyperparameters, particularly those related to the cyclical annealing schedule and distance polarization regularizer?
- Basis in paper: [explicit] The paper mentions specific hyperparameter values used (e.g., R=0.5, G=5000 for cyclical annealing) but does not explore the sensitivity of performance to these choices.
- Why unresolved: The paper does not provide an ablation study or sensitivity analysis for the hyperparameters, particularly those unique to the proposed framework (cyclical annealing and distance polarization regularizer).
- What evidence would resolve it: A comprehensive ablation study varying the key hyperparameters (R, G, λreg, δ+, δ-) to determine their impact on performance across different datasets.

## Limitations
- Framework effectiveness depends heavily on having access to paired manual and ASR transcripts for training
- Approach increases computational complexity by requiring two separate model instances during training
- Performance may degrade with more severe or systematic ASR errors than those present in evaluation datasets

## Confidence
- High Confidence: Core mechanisms of mutual learning and large-margin contrastive learning are well-established
- Medium Confidence: Specific design choices for hyperparameters are based on empirical tuning rather than theoretical guarantees
- Low Confidence: Framework's performance with different ASR error patterns and varying ASR qualities is not extensively explored

## Next Checks
1. **Ablation study across ASR qualities**: Test ML-LMCL with artificially degraded ASR transcripts (varying WER levels) to understand performance bounds and identify when the framework fails.
2. **Generalization to new domains**: Evaluate the pre-trained ML-LMCL model on out-of-domain SLU tasks without fine-tuning to assess true robustness versus dataset-specific optimization.
3. **Real-world deployment validation**: Compare ML-LMCL against traditional approaches using only live ASR output in an end-to-end spoken dialogue system to measure practical improvements in user experience metrics.