---
ver: rpa2
title: Memory-adaptive Depth-wise Heterogeneous Federated Learning
arxiv_id: '2303.04887'
source_url: https://arxiv.org/abs/2303.04887
tags:
- learning
- memory
- training
- clients
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles memory heterogeneity in federated learning (FL),
  where clients have varying device memory capabilities. Existing methods based on
  width-slimming techniques degrade global model performance due to negative impacts
  from aggregating sub-networks of different sizes.
---

# Memory-adaptive Depth-wise Heterogeneous Federated Learning

## Quick Facts
- arXiv ID: 2303.04887
- Source URL: https://arxiv.org/abs/2303.04887
- Reference count: 40
- Key outcome: FEDEPTH achieves 5% and over 10% improvements in top-1 accuracy on CIFAR-10 and CIFAR-100 respectively compared to state-of-the-art approaches.

## Executive Summary
This paper addresses memory heterogeneity in federated learning by proposing FEDEPTH, a depth-wise learning solution that adaptively decomposes full models into blocks based on each client's memory budget. Unlike existing width-slimming techniques that degrade global model performance through negative aggregation effects, FEDEPTH trains full-size models sequentially in a memory-efficient manner. The method employs skip connections and auxiliary classifiers to ensure effective training, achieving significant accuracy improvements on CIFAR datasets while maintaining communication efficiency through mutual knowledge distillation on memory-rich clients.

## Method Summary
FEDEPTH tackles memory heterogeneity in federated learning by decomposing the global model into blocks that fit within each client's memory budget. Instead of training width-reduced sub-networks like previous approaches, FEDEPTH uses depth-wise sequential training where each client trains the full model architecture in blocks, passing information through skip connections to the classifier. The method includes mutual knowledge distillation for clients with sufficient memory to train multiple models, improving performance without additional communication overhead. The approach integrates with standard FedAvg aggregation while maintaining model completeness and avoiding the negative aggregation effects seen in width-slimming methods.

## Key Results
- Achieves 5% improvement in top-1 accuracy on CIFAR-10 compared to state-of-the-art approaches
- Demonstrates over 10% improvement in top-1 accuracy on CIFAR-100
- Shows effectiveness in depth-wise fine-tuning on Vision Transformer (ViT) models
- Maintains communication efficiency through mutual knowledge distillation on memory-rich clients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth-wise sequential learning allows each client to train a full-size model despite memory constraints by decomposing the network into memory-budget-sized blocks and training them sequentially.
- Mechanism: The global model is adaptively decomposed into blocks based on each client's memory budget. Each client trains blocks sequentially using a skip connection to the classifier layer, ensuring the classifier receives gradients from all blocks without requiring the full model to fit in memory at once.
- Core assumption: The memory bottleneck is dominated by intermediate activations, not parameters, and these can be discarded between block training steps.
- Evidence anchors:
  - [abstract] "FEDEPTH, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obtain a full inference model."
  - [section 4.1] "FEDEPTH conducts local training in a memory-adaptive manner. Specifically, for the k-th client the full model W is decomposed to into Jk + 1 blocks..."
  - [corpus] No direct evidence found; weak signal.
- Break condition: If a client's memory budget is too small to fit even the smallest decomposed block, partial training strategy is needed.

### Mechanism 2
- Claim: Aggregating full-size models trained via depth-wise learning avoids the negative impact of sub-network aggregation seen in width-slimming approaches.
- Mechanism: Unlike HeteroFL and SplitMix which aggregate sub-networks of different widths (leading to under-expression of small models), FEDEPTH ensures all clients train the full model architecture, just in a block-wise fashion, so aggregation combines complete models.
- Core assumption: Training the full architecture, even sequentially, captures more global information than training width-reduced sub-networks.
- Evidence anchors:
  - [section 3.2] "Small sub-networks make negative contributions in HeteroFL... the global model obtained via aggregating small sub-networks consistently has worse performance than the global model obtained via only aggregating the full-size neural networks"
  - [section 4] "we introduce a memory-efficient framework FEDEPTH... to train full-size neural networks with memory budget constraints in the FL setting."
  - [corpus] No direct evidence found; weak signal.
- Break condition: If depth-wise training cannot adequately capture inter-block dependencies due to memory constraints.

### Mechanism 3
- Claim: Mutual knowledge distillation (MKD) among multiple models on memory-rich clients improves global model performance without communication overhead.
- Mechanism: Clients with sufficient memory train multiple models and use MKD to align their predictions, ensuring knowledge consensus. Only one model needs to be uploaded for aggregation, leveraging rich memory without extra communication.
- Core assumption: Ensembling and knowledge transfer among locally trained models can improve generalization without sharing raw data.
- Evidence anchors:
  - [section 4.3] "we design a new training and aggregation method based on mutual knowledge distillation (MKD)... clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all models through distillation."
  - [section 5.2] "m-FEDEPTH gains substantial improvements... compared to HeteroFL while gains 3.84 Â± 1.33% average improvement compared to SplitMix."
  - [corpus] No direct evidence found; weak signal.
- Break condition: If the number of models M is too large relative to available memory, causing memory overflow or degraded performance.

## Foundational Learning

- Concept: Memory consumption analysis in neural network training
  - Why needed here: Understanding that activations, not parameters, dominate memory usage is crucial for designing memory-efficient FL algorithms.
  - Quick check question: In standard neural network training, which component typically consumes the most memory: model parameters or intermediate activations?

- Concept: Federated Learning with heterogeneous clients
  - Why needed here: The paper addresses the challenge of varying device capabilities in FL, requiring algorithms that adapt to different memory budgets while maintaining model performance.
  - Quick check question: What are the two main types of heterogeneity in federated learning mentioned in the introduction?

- Concept: Depth-wise vs. width-slimming model adaptation
  - Why needed here: The paper contrasts depth-wise sequential training with width-slimming techniques to show why the former avoids negative aggregation effects.
  - Quick check question: In the context of heterogeneous FL, what is the key difference between depth-wise training and width-slimming approaches?

## Architecture Onboarding

- Component map:
  - Memory-adaptive decomposition module: Splits the global model into blocks based on client memory budgets
  - Depth-wise sequential trainer: Trains each block sequentially with skip connections to classifier
  - Partial training handler: Manages cases where even smallest blocks exceed memory budget
  - Mutual knowledge distillation module: Enables multiple model training on rich-memory clients
  - Aggregation coordinator: Combines updated models from all clients

- Critical path:
  1. Server initializes global model and broadcasts to clients
  2. Each client determines its block decomposition based on memory budget
  3. Client trains blocks sequentially using depth-wise learning
  4. Client uploads updated model parameters to server
  5. Server aggregates received models to update global model
  6. Repeat for multiple rounds

- Design tradeoffs:
  - Memory vs. communication: Depth-wise training reduces memory needs but may require more communication rounds
  - Model completeness vs. efficiency: Full model training vs. sub-network training (HeteroFL/SplitMix)
  - Complexity vs. performance: Adding MKD for rich-memory clients improves performance but adds implementation complexity

- Failure signatures:
  - Poor convergence: May indicate insufficient training due to aggressive decomposition
  - Memory overflow errors: Client memory budget too small even for finest decomposition
  - Degraded performance: Skip connections or partial training not implemented correctly

- First 3 experiments:
  1. Implement depth-wise training on a simple CNN with varying memory budgets to verify block decomposition and sequential training
  2. Compare FEDEPTH with HeteroFL on CIFAR-10 under non-IID conditions to validate performance improvements
  3. Test partial training strategy on clients with extremely limited memory to ensure model quality is maintained

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FEDEPTH's performance compare to other federated learning methods when applied to large-scale real-world datasets beyond CIFAR-10 and CIFAR-100?
- Basis in paper: [explicit] The authors demonstrate FEDEPTH's effectiveness on CIFAR-10 and CIFAR-100, and show its potential on ViT, but do not evaluate it on larger, more complex datasets.
- Why unresolved: Evaluating FEDEPTH on larger datasets would require more computational resources and a larger number of clients, which may not be feasible for all research groups.
- What evidence would resolve it: Comparative studies of FEDEPTH's performance on large-scale real-world datasets, such as ImageNet or large-scale medical imaging datasets, would provide evidence of its effectiveness in more realistic scenarios.

### Open Question 2
- Question: How does FEDEPTH handle heterogeneous clients with varying computational capabilities, not just memory constraints?
- Basis in paper: [inferred] The paper focuses on memory heterogeneity, but does not explicitly address how FEDEPTH would perform when clients have varying computational capabilities, such as different processing speeds or network bandwidths.
- Why unresolved: The impact of varying computational capabilities on FEDEPTH's performance is not well understood, and addressing this issue could lead to further improvements in the algorithm.
- What evidence would resolve it: Experiments comparing FEDEPTH's performance on clients with different computational capabilities, such as varying processing speeds or network bandwidths, would provide insights into its robustness to heterogeneous clients.

### Open Question 3
- Question: How does FEDEPTH's depth-wise learning strategy compare to other model decomposition techniques, such as channel-wise or layer-wise decomposition, in terms of global model performance and memory efficiency?
- Basis in paper: [explicit] The authors introduce FEDEPTH's depth-wise learning strategy and demonstrate its effectiveness, but do not compare it to other model decomposition techniques.
- Why unresolved: The choice of model decomposition technique can significantly impact the performance and efficiency of federated learning algorithms, and understanding the relative strengths and weaknesses of different approaches is crucial for advancing the field.
- What evidence would resolve it: Comparative studies of FEDEPTH's depth-wise learning strategy against other model decomposition techniques, such as channel-wise or layer-wise decomposition, would provide insights into the relative merits of each approach and guide future research in federated learning.

## Limitations

- The memory consumption estimation method for block decomposition is not fully specified, making it difficult to assess generalization to other model architectures
- The computational overhead of sequential block training compared to standard FedAvg is not quantified
- Scalability analysis to larger models and non-vision tasks remains underexplored

## Confidence

- **High Confidence:** The core mechanism of depth-wise sequential training with skip connections is technically sound and well-supported by the experimental results on CIFAR-10 and CIFAR-100.
- **Medium Confidence:** The claimed superiority over width-slimming approaches (HeteroFL and SplitMix) is supported by experiments, but the analysis could benefit from more rigorous ablation studies isolating the contribution of each component.
- **Low Confidence:** The scalability analysis to larger models like ViT and the generalization to non-vision tasks remains underexplored.

## Next Checks

1. **Ablation Study on Block Size:** Systematically vary the number of blocks and measure the trade-off between memory efficiency and model performance to identify optimal decomposition strategies.
2. **Memory Overhead Analysis:** Quantify the additional computation time required for sequential block training compared to standard FedAvg to assess practical feasibility.
3. **Cross-Domain Generalization:** Validate FEDEPTH on non-vision datasets (e.g., text or tabular data) to confirm the approach's applicability beyond image classification tasks.