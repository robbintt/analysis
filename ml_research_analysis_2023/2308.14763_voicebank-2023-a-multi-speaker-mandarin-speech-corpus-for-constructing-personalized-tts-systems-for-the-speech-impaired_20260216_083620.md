---
ver: rpa2
title: 'VoiceBank-2023: A Multi-Speaker Mandarin Speech Corpus for Constructing Personalized
  TTS Systems for the Speech Impaired'
arxiv_id: '2308.14763'
source_url: https://arxiv.org/abs/2308.14763
tags:
- speech
- corpus
- recording
- text
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The VoiceBank-2023 corpus is a 29.78-hour multi-speaker Mandarin
  speech corpus developed for constructing personalized TTS systems for speech-impaired
  individuals, particularly ALS patients. The corpus contains recordings from 111
  native Mandarin speakers, including 39 ALS patients, 63 voice donors, and 9 unknowns,
  with varying degrees of speech impairment.
---

# VoiceBank-2023: A Multi-Speaker Mandarin Speech Corpus for Constructing Personalized TTS Systems for the Speech Impaired

## Quick Facts
- **arXiv ID**: 2308.14763
- **Source URL**: https://arxiv.org/abs/2308.14763
- **Reference count**: 40
- **Primary result**: 29.78-hour multi-speaker Mandarin speech corpus for personalized TTS systems for speech-impaired individuals

## Executive Summary
VoiceBank-2023 is a 29.78-hour multi-speaker Mandarin speech corpus developed to construct personalized text-to-speech (TTS) systems for speech-impaired individuals, particularly ALS patients. The corpus contains recordings from 111 native Mandarin speakers, including 39 ALS patients, 63 voice donors, and 9 unknowns, with varying degrees of speech impairment. It is divided into two parts: Part-1 (VoiceBanking) with 7,625 utterances of short paragraphs and Part-2 (Common Phrases) with 5,250 utterances of daily phrases. TTS systems built using this corpus achieved mean opinion scores of 3.92 for speaker similarity from ALS patients and 3.55 from caregivers.

## Method Summary
The VoiceBank-2023 corpus was constructed through a multi-phase approach involving text selection for phonetic balance, web-based recording with USB microphones, and comprehensive data labeling. Text selection used iterative paragraph selection to ensure broad acoustic coverage with minimal utterances. Recordings were collected via a web-based platform allowing self-service recording in home environments. The corpus includes detailed metadata such as speaker information, transcriptions, SNRs, and speaking rates. TTS systems were constructed using a modular approach with speaker adaptation techniques, trained on the corpus and evaluated through mean opinion scores for speaker similarity.

## Key Results
- 29.78-hour corpus with 12,875 utterances from 111 speakers
- Mean opinion scores of 3.92 for speaker similarity from ALS patients and 3.55 from caregivers
- Phonetic balance achieved through iterative paragraph selection with 76 paragraphs covering 36 initials and 28 finals
- Web-based recording with USB microphones achieved comparable or higher SNR than professional studio recordings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Phonetic balance achieved through iterative paragraph selection ensures broad acoustic coverage with minimal utterances.
- **Mechanism**: The corpus design sorts 376 paragraphs by unique initial/final type count, then iteratively adds paragraphs until each acoustic unit appears at least twice, ensuring efficient phonetic coverage.
- **Core assumption**: Sorting by unique acoustic unit count before syllable count maximizes phonetic diversity early in the recording sequence.
- **Evidence anchors**: Corpus contains 29.78 hours of utterances; priority set V updated by checking if |T({V ∪ Pi})| > |T(V)|; no explicit phonetic balance metrics reported.

### Mechanism 2
- **Claim**: Adaptive TTS models trained on large reference corpora can produce acceptable speech with limited ALS patient data through speaker adaptation.
- **Mechanism**: The TTS system uses speaker-adaptive prosodic models and acoustic models with speaker embedding biases, trained jointly with large Treebank-SR and Danei corpora, then adapted to small patient recordings.
- **Core assumption**: Speaker adaptation techniques can transfer prosodic and acoustic patterns from reference speakers to ALS patients despite speech impairment.
- **Evidence anchors**: TTS systems achieved mean opinion scores of 3.92 for speaker similarity from ALS patients; PG and SG modules use speaker embedding as biases; no direct evidence that adaptation improves intelligibility.

### Mechanism 3
- **Claim**: Web-based recording with USB microphones in home environments can produce speech quality comparable to professional studio recordings.
- **Mechanism**: The VoiceBanking website provides GUI-based recording with quality USB microphones, and metadata shows similar or higher SNR compared to onsite recordings.
- **Core assumption**: Modern USB microphones and quiet home environments can capture sufficient quality speech for TTS training without professional equipment.
- **Evidence anchors**: Speakers' self-service recording with quality USB microphones generally made recorded utterances have similar or even higher SNR than on-site recorded utterances; GUI provides recording control and waveform display.

## Foundational Learning

- **Concept**: Speaker adaptation in TTS systems
  - **Why needed here**: ALS patients provide limited speech data, requiring adaptation from larger reference corpora to build personalized models.
  - **Quick check question**: How does speaker embedding in neural TTS models enable adaptation to new speakers with limited data?

- **Concept**: Phonetic coverage and balance in speech corpora
  - **Why needed here**: Efficient phonetic coverage ensures TTS models learn all necessary acoustic units without requiring excessive patient recording time.
  - **Quick check question**: What metrics would you use to evaluate whether a corpus provides adequate phonetic coverage for TTS training?

- **Concept**: Speech quality assessment (SNR, MOS)
  - **Why needed here**: Evaluating recording quality and synthesized speech intelligibility requires understanding signal-to-noise ratio and mean opinion score methodologies.
  - **Quick check question**: How would you design a listening test to distinguish between recording quality issues and speech impairment effects in ALS patient recordings?

## Architecture Onboarding

- **Component map**: Text selection -> Web recording -> Data purging/correction -> Labeling -> TTS model training -> Evaluation
- **Critical path**: Text selection → Recording → Data purging/correction → Labeling → TTS model training → Evaluation
- **Design tradeoffs**: Professional onsite recording vs. web-based recording trades quality control for accessibility and scalability; phonetic balance vs. recording efficiency trades coverage completeness for patient workload
- **Failure signatures**: Low SNR recordings → poor TTS quality; insufficient phonetic coverage → pronunciation errors; mismatched transcriptions → ASR correction failures
- **First 3 experiments**:
  1. Test phonetic coverage by analyzing acoustic unit distribution across recorded utterances vs. target distribution
  2. Compare TTS quality (MOS) between web-recorded and professionally recorded subsets to validate recording approach
  3. Evaluate speaker adaptation effectiveness by training TTS models with varying amounts of patient data and measuring intelligibility vs. speaker similarity trade-offs

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the quality and usability of the VoiceBank-2023 corpus be further improved for constructing personalized TTS systems for speech-impaired individuals?
  - **Basis in paper**: The paper mentions the corpus is available for non-commercial use and welcomes contributions to improve services for the speech impaired. It discusses the need for automatic mechanisms to label degrees of dysarthria, voice quality, and sound quality.
  - **Why unresolved**: The paper highlights potential for improvement but does not provide specific methods or mechanisms to achieve this.
  - **What evidence would resolve it**: Development and implementation of automatic labeling mechanisms for dysarthria degrees, voice quality, and sound quality would demonstrate progress.

- **Open Question 2**: How can the VoiceBank-2023 corpus be expanded to include a more diverse range of speech-impaired individuals and their specific needs?
  - **Basis in paper**: The paper focuses on ALS patients but mentions recordings from 111 native Mandarin speakers with varying degrees of speech impairment. It discusses the need for common phrases to enrich communicative functions.
  - **Why unresolved**: The paper does not provide information on specific needs of different speech-impaired individuals or how to expand the corpus accordingly.
  - **What evidence would resolve it**: Collection and inclusion of speech data from a wider range of speech-impaired individuals, along with detailed analysis of their specific needs.

- **Open Question 3**: How can the VoiceBank-2023 corpus be utilized to improve the performance of personalized TTS systems for speech-impaired individuals beyond ALS patients?
  - **Basis in paper**: The paper focuses on ALS patients but mentions the corpus contains recordings from speakers with varying degrees of speech impairment. It discusses evaluation using the Part-1 VoiceBanking sub-corpus.
  - **Why unresolved**: The paper does not provide information on how the corpus can be used for other types of speech impairments or individuals with different levels of impairment.
  - **What evidence would resolve it**: Development and evaluation of personalized TTS systems using the corpus for individuals with different types and levels of speech impairment.

## Limitations

- Web-based recording introduces uncontrolled variability in recording environments that may compromise TTS model robustness
- Phonetic balance mechanism relies on iterative paragraph selection but lacks independent validation of completeness
- Limited individual speaker data (average ~16 minutes per speaker) may constrain quality of personalized TTS systems
- No quantitative assessment of speech intelligibility or naturalness, only subjective similarity judgments

## Confidence

- **High Confidence**: Corpus construction methodology, speaker composition (39 ALS patients, 63 voice donors, 9 unknowns), and basic corpus statistics (29.78 hours, 12,875 utterances)
- **Medium Confidence**: TTS system architecture and speaker adaptation approach; reported mean opinion scores for speaker similarity
- **Low Confidence**: Effectiveness of phonetic balance achievement; quality comparison between web-based and professional recordings; intelligibility of synthesized speech for actual communication

## Next Checks

1. **Phonetic Coverage Analysis**: Conduct independent acoustic analysis to verify all target acoustic units are covered at required frequency across all speakers, particularly focusing on ALS patient recordings where speech impairment may have affected completeness.

2. **Recording Quality Comparison**: Perform systematic comparison of signal-to-noise ratios and other quality metrics between web-based recordings and any available professional recordings within the corpus to empirically validate USB microphone recording quality claims.

3. **Speech Intelligibility Testing**: Design and conduct comprehensive intelligibility test using blinded listening studies where participants attempt to understand synthesized speech from ALS patients, measuring word error rates and comprehension scores in addition to subjective similarity ratings.