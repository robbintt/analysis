---
ver: rpa2
title: Effective Decision Boundary Learning for Class Incremental Learning
arxiv_id: '2301.05180'
source_url: https://arxiv.org/abs/2301.05180
tags:
- learning
- data
- classes
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses decision boundary overfitting in class incremental
  learning (CIL), which is caused by insufficient old class data for knowledge distillation
  and imbalanced learning between old and new classes. The proposed method, EDBL,
  tackles these issues through two key improvements: 1) Re-sampling Mixup Knowledge
  Distillation (Re-MKD), which synthesizes more data consistent with the old class
  distribution for better knowledge distillation; 2) Incremental Influence Balance
  (IIB) method, which re-weights samples based on their influence to create a proper
  decision boundary.'
---

# Effective Decision Boundary Learning for Class Incremental Learning

## Quick Facts
- arXiv ID: 2301.05180
- Source URL: https://arxiv.org/abs/2301.05180
- Reference count: 40
- Achieves 66.65% and 64.73% average accuracy on CIFAR-100 with 5 and 10 phases respectively

## Executive Summary
This paper addresses decision boundary overfitting in class incremental learning (CIL), which arises from insufficient old class data for knowledge distillation and imbalanced learning between old and new classes. The proposed EDBL method tackles these issues through Re-sampling Mixup Knowledge Distillation (Re-MKD) to synthesize better KD data, and Incremental Influence Balance (IIB) to re-weight samples based on their influence on decision boundaries. Experiments demonstrate state-of-the-art performance on CIFAR-10/100 and Tiny-Imagenet datasets.

## Method Summary
EDBL employs a two-phase training approach to address CIL challenges. Phase 1 uses Re-sampling Mixup Knowledge Distillation (Re-MKD) to generate synthetic data from old and new classes that better aligns with old class distributions, improving knowledge distillation quality. Phase 2 applies Incremental Influence Balance (IIB) to re-weight samples based on their influence on decision boundaries, creating a balanced loss function that mitigates imbalanced data learning. The method combines these components to simultaneously address both insufficient KD and class imbalance issues.

## Key Results
- Achieves average accuracy of 66.65% on CIFAR-100 with 5 phases and 64.73% with 10 phases
- Outperforms state-of-the-art methods including BiC, LUCIR, and RKD-based approaches
- Ablation study confirms both Re-MKD and IIB components contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1: Re-sampling Mixup Knowledge Distillation
Re-MKD synthesizes data that are more consistent with old class latent distribution than new class data alone. By combining re-sampling with Mixup, the method generates mixed data from old and new classes that provide better alignment with the original training distribution of old classes, improving KD performance.

### Mechanism 2: Incremental Influence Balance
IIB addresses imbalanced data learning by re-weighting samples based on their influence on decision boundaries. The method extends influence balance from long-tail learning to CIL setting, decomposing the weighting factor into classification and KD components to create a balanced loss function.

### Mechanism 3: Combined EDBL Algorithm
EDBL effectively combines Re-MKD and IIB to address both insufficient KD and imbalanced data learning simultaneously. The two-phase training approach first improves knowledge transfer through better KD data, then balances training through influence-based re-weighting.

## Foundational Learning
- **Catastrophic forgetting in neural networks**: Understanding why CIL methods need to address forgetting is fundamental to grasping why EDBL's dual approach is necessary. Quick check: What happens to a neural network's performance on old tasks when it's trained on new tasks without any mitigation strategy?
- **Knowledge distillation (KD)**: Re-MKD builds upon KD, and understanding KD's mechanism is essential for grasping why OOD data degrades performance. Quick check: In KD, what is the role of the teacher model, and how does it help the student model retain knowledge?
- **Imbalanced learning and long-tail classification**: IIB method extends techniques from imbalanced learning to CIL, so understanding these concepts is crucial. Quick check: How does class imbalance typically affect model performance, and what are common strategies to address it?

## Architecture Onboarding

- **Component map:** Data augmentation module (Re-sampling Mixup) -> Knowledge distillation training phase -> Influence calculation module -> IIB loss computation module -> Two-phase training orchestrator
- **Critical path:** Data augmentation → Phase 1 MKD training → Phase 2 IIB balancing → Final model
- **Design tradeoffs:** Re-sampling Mixup increases training time and memory usage but improves KD quality; IIB requires additional computation for influence calculation but provides better decision boundaries; Two-phase training doubles training time but addresses both root causes effectively
- **Failure signatures:** Poor performance on old classes indicates insufficient KD; poor performance on new classes indicates imbalance or aggressive re-weighting; high variance in results suggests sensitivity to α hyperparameter
- **First 3 experiments:** 1) Compare Re-MKD vs vanilla Mixup on CIFAR-10/100 to verify improvement in KD performance; 2) Test IIB vs standard CE loss on imbalanced CIFAR datasets to validate balancing effectiveness; 3) Run ablation study (Re-MKD only, IIB only, both) to confirm complementary benefits

## Open Questions the Paper Calls Out

### Open Question 1
How does the EDBL algorithm perform when applied to non-image data domains, such as text or speech, where the data distribution characteristics might differ significantly from image datasets? The study is limited to image classification tasks, leaving the generalizability of the approach to other data types untested.

### Open Question 2
What is the impact of varying the memory budget size on the performance of EDBL, and how does it scale with larger datasets or more classes? The experiments are conducted with fixed memory budgets, and the paper does not discuss the scalability of EDBL with increasing dataset size or number of classes.

### Open Question 3
How does the choice of the hyperparameter α in the Incremental Influence Balance method affect the balance between stability and plasticity in EDBL, and what is the optimal strategy for setting this parameter? The ablation study shows that α impacts performance, but the paper does not offer a systematic approach for selecting the optimal value of α.

## Limitations
- Effectiveness depends on degree of out-of-distribution (OOD) between old and new classes, which may vary across datasets
- Two-phase training approach increases computational complexity and training time
- Performance is sensitive to hyperparameter α, which requires careful tuning for different tasks

## Confidence

**High Confidence:** The theoretical foundation connecting OOD data degradation in KD to catastrophic forgetting is well-established in the literature. The decision to combine Mixup with re-sampling for KD improvement is supported by prior work.

**Medium Confidence:** The extension of influence balance from long-tail learning to CIL is reasonable but may not generalize perfectly. The specific implementation details of IIB weighting could significantly impact performance.

**Low Confidence:** The paper claims to address both insufficient KD and imbalanced learning simultaneously, but the interaction between these two mechanisms and their combined effect on decision boundaries is not fully validated.

## Next Checks
1. **OOD Sensitivity Test:** Systematically vary the similarity between old and new class distributions to quantify how Re-MKD performance degrades as OOD decreases.
2. **Influence Metric Validation:** Compare the influence-based re-weighting against simpler class-balancing techniques (e.g., class weights or oversampling) to isolate the contribution of the influence calculation.
3. **Computational Overhead Analysis:** Measure the additional training time and memory requirements of EDBL compared to baseline methods to assess practical deployment feasibility.