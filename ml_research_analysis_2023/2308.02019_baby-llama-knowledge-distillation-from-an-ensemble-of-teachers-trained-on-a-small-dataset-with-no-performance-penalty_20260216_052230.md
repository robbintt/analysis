---
ver: rpa2
title: 'Baby Llama: knowledge distillation from an ensemble of teachers trained on
  a small dataset with no performance penalty'
arxiv_id: '2308.02019'
source_url: https://arxiv.org/abs/2308.02019
tags:
- llama
- performance
- language
- trained
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors distill an ensemble of two larger models (GPT-2 and
  LLaMA) into a smaller 58M-parameter LLaMA model, trained on the 10M-word BabyLM
  dataset. They show that this distilled model not only matches but often exceeds
  the performance of both teachers and a similar non-distilled model across zero-shot
  and fine-tuning benchmarks.
---

# Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty

## Quick Facts
- **arXiv ID:** 2308.02019
- **Source URL:** https://arxiv.org/abs/2308.02019
- **Reference count:** 14
- **Primary result:** Distilling GPT-2 and LLaMA teachers trained on 10M-word BabyLM dataset into a 58M-parameter LLaMA student yields models that match or exceed both teacher and non-distilled baseline performance on multiple NLP benchmarks.

## Executive Summary
Baby Llama demonstrates that knowledge distillation can be highly effective even when training on extremely small datasets. The authors train an ensemble of GPT-2 and LLaMA models on the 10M-word BabyLM corpus, then distill this ensemble into a smaller 58M-parameter LLaMA model. Remarkably, this distilled student model not only matches but often exceeds the performance of both its teachers and a similar non-distilled model across zero-shot and fine-tuning benchmarks. The approach achieves these results efficiently, requiring less than 3 hours of training on a single GPU.

## Method Summary
The method involves training two teacher models (GPT-2 and LLaMA) on the BabyLM dataset, then using a combined loss function during distillation that includes both cross-entropy with true labels and KL divergence with teacher soft targets. The soft targets are generated using temperature scaling (temperature=2) to provide richer gradient signals. The student model (58M LLaMA) is trained to minimize this combined loss, effectively learning to approximate the ensemble's collective knowledge. The distillation process uses equal weighting between the hard target loss and the distillation loss.

## Key Results
- Baby Llama outperforms baseline models (OPT, RoBERTa, T5) on most zero-shot tasks by 1-2 percentage points
- The distilled model exceeds performance of both individual teachers and the simple ensemble average
- Training is highly efficient, requiring less than 3 hours on a single GPU
- Improvements are consistent across multiple benchmarks including BLiMP, SuperGLUE, and MSGS

## Why This Works (Mechanism)

### Mechanism 1: Soft Target Regularization
The distillation loss combines cross-entropy with teacher soft targets using temperature 2 and equal weighting, leading to improved zero-shot performance. The student learns from the relative likelihoods between incorrect answers provided by teacher soft probabilities, capturing richer information than hard labels alone.

### Mechanism 2: Ensemble Diversity Integration
Multiple heterogeneous teachers (GPT-2 and LLaMA) provide diverse perspectives on the same limited data. The student learns to integrate these perspectives better than simple averaging during inference, capturing complementary knowledge from different architectural biases.

### Mechanism 3: Small Dataset Regularization Effect
When teachers are trained on very limited data, they may overfit in ways that distillation can smooth out in the student model. The distillation process acts as a form of regularization, forcing the student to learn robust representations that generalize better than potentially overfitted teacher parameters.

## Foundational Learning

- **Knowledge distillation and temperature scaling**: Understanding how temperature affects probability distributions is crucial since the entire method relies on training a student using teacher soft targets with temperature=2. *Quick check:* If a teacher outputs logits [2.0, 0.5, -1.0] with temperature 2, what are the resulting probabilities before KL divergence calculation?

- **Ensemble methods and model averaging**: The approach uses an ensemble of different architectures as teachers, requiring understanding of how heterogeneous models can complement each other. *Quick check:* Why might averaging logits from GPT-2 and LLaMA teachers be less effective than distilling their knowledge into a single student?

- **Curriculum learning and dataset complexity**: Though not used in the final approach, awareness of how data ordering affects learning is important given the paper's brief mention of curriculum learning attempts. *Quick check:* How might ordering training data by complexity affect the distillation process differently than the baseline training?

## Architecture Onboarding

- **Component map:** BabyLM dataset (10M words) → Teacher ensemble (GPT-2 + LLaMA) → Student LLaMA (58M params)
- **Critical path:** Data preprocessing → Teacher training → Distillation training → Evaluation
- **Design tradeoffs:** Smaller student size vs. teacher performance; equal weighting vs. adaptive weighting in loss function
- **Failure signatures:** Loss divergence during distillation; student performance worse than individual teachers; eval loss not correlating with benchmarks
- **First 3 experiments:**
  1. Train individual teachers on BabyLM and verify their baseline performance
  2. Implement temperature scaling in distillation loss and test with different values (1.0, 2.0, 4.0)
  3. Try distillation with single teacher vs. ensemble to quantify ensemble benefit

## Open Questions the Paper Calls Out

### Open Question 1
Does the distillation improvement scale with larger models and datasets, or is it limited to small-scale settings? The study is limited to models 10³–10⁴ times smaller than state-of-the-art, and trained with significantly more parameters than tokens, unlike current large-scale models.

### Open Question 2
Is the improved performance from distillation additive with other training techniques like curriculum learning or sharpness-aware minimization? The authors tried various techniques but found no further improvements when combined with distillation, or negative results in some cases.

### Open Question 3
Does distillation from an ensemble of heterogeneous teachers provide consistent benefits across different model architectures and data modalities? The study used GPT-2 and LLaMA teachers for text-based language modeling; authors suggest further experimentation is needed for other modalities and domains.

## Limitations

- Results are demonstrated on a very small dataset (10M words) and may not generalize to larger-scale settings
- The study only tests one specific combination of teacher architectures (GPT-2 and LLaMA)
- Temperature and weighting parameters are not systematically ablated, leaving optimal settings unclear
- Benchmark coverage is limited to specific tasks without broader testing across diverse NLP capabilities

## Confidence

- **High confidence:** The distillation methodology itself is well-established and implementation details are clearly specified
- **Medium confidence:** The claim that the distilled model "exceeds" teacher performance is supported by reported numbers but lacks statistical significance analysis
- **Low confidence:** The generalization claim about distillation exceeding teacher performance on sufficiently small datasets extrapolates beyond the specific experimental conditions

## Next Checks

1. **Ablation on temperature and weighting parameters:** Systematically vary the temperature (1.0, 2.0, 4.0) and weighting parameter α (0.3, 0.5, 0.7) to determine sensitivity and identify optimal settings for different benchmark types.

2. **Teacher diversity analysis:** Replace one teacher with a more similar architecture (e.g., both LLaMA variants) and measure the impact on distillation performance to quantify the value of architectural heterogeneity.

3. **Dataset size scaling study:** Repeat the distillation process on progressively larger subsets of the BabyLM dataset (1M, 5M, 20M, 50M words) to identify the point at which teacher performance plateaus and student gains diminish.