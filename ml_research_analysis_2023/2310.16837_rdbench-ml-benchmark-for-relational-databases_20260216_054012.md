---
ver: rpa2
title: 'RDBench: ML Benchmark for Relational Databases'
arxiv_id: '2310.16837'
source_url: https://arxiv.org/abs/2310.16837
tags:
- graph
- relational
- table
- databases
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RDBench, the first standardized benchmark
  for machine learning on relational databases (RDBs). RDBench provides 11 diverse
  RDB datasets spanning 4 complexity levels, with 69 prediction tasks across tabular,
  homogeneous graph, and heterogeneous graph interfaces.
---

# RDBench: ML Benchmark for Relational Databases

## Quick Facts
- arXiv ID: 2310.16837
- Source URL: https://arxiv.org/abs/2310.16837
- Reference count: 36
- Key outcome: Introduces RDBench, the first standardized benchmark for machine learning on relational databases, enabling fair comparison across 10 baseline methods on 11 diverse datasets with 69 prediction tasks

## Executive Summary
This paper introduces RDBench, the first standardized benchmark for machine learning on relational databases (RDBs). RDBench provides 11 diverse RDB datasets spanning 4 complexity levels, with 69 prediction tasks across tabular, homogeneous graph, and heterogeneous graph interfaces. The unified task definition enables fair comparisons across ML methods like XGBoost, GCN, GIN, GraphSAGE, GAT, HGCN, and HGT. Extensive experiments show XGBoost dominates in small/medium RDBs, while graph neural networks excel on complex RDBs with 10+ tables. RDBench facilitates reproducible ML research on RDBs and enables new interdisciplinary collaborations.

## Method Summary
RDBench provides a unified benchmark for ML on relational databases through three key innovations: (1) unified task definition across tabular, homogeneous graph, and heterogeneous graph interfaces by transforming relational databases into graph structures, (2) hierarchical dataset organization by table complexity (3-5, 6-10, and 10+ tables) to enable targeted evaluation, and (3) averaging results over multiple tasks per dataset to enhance experimental robustness. The benchmark includes 11 diverse RDB datasets with 69 prediction tasks, comparing 10 baseline methods including XGBoost, MLP, GCN, GIN, GraphSAGE, GAT, HGCN, and HGT. Implementation details specify learning rates, epochs, and data splitting procedures.

## Key Results
- XGBoost outperforms all other methods on RDBs with 3-10 tables, achieving accuracy scores of 0.9929, 0.9408, and 0.9833 on RDBench-L1, L2, and L3 respectively
- Graph neural networks show superior performance on complex RDBs with 10+ tables, with HGT achieving the highest accuracy (0.9901) on RDBench-L3
- For homogeneous graph models, GCN performs best with 0.9790 accuracy on RDBench-L3, while GraphSAGE achieves 0.9826 accuracy on RDBench-L2

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Unified task definition across three data formats enables fair comparison between heterogeneous ML methods.
- **Mechanism**: RDBench defines prediction tasks identically for tabular, homogeneous graph, and heterogeneous graph interfaces by converting relational databases into graph structures where column prediction becomes node property prediction.
- **Core assumption**: The transformation from relational database to graph preserves task semantics and information content.
- **Evidence anchors**:
  - [abstract]: "for any given database, RDBench exposes three types of interfaces including tabular data, homogeneous graphs, and heterogeneous graphs, sharing the same underlying task definition."
  - [section 5.2]: "we provide a transformation procedure between graphs and relational databases... we also propose to transform graphs back into relational databases"
  - [corpus]: Weak evidence - corpus contains papers on graph modeling of RDBs but lacks direct validation of unified task preservation.
- **Break condition**: If the graph transformation loses critical relational information or introduces bias that affects different model types unequally.

### Mechanism 2
- **Claim**: Hierarchical dataset organization by table complexity enables targeted evaluation of model capabilities.
- **Mechanism**: RDBench organizes 11 datasets into 4 levels based on the number of tables (3-5, 6-10, 10+ tables), allowing systematic assessment of how model performance scales with relational complexity.
- **Core assumption**: Number of tables is a reasonable proxy for relationship complexity in relational databases.
- **Evidence anchors**:
  - [abstract]: "RDBench exhibits 11 datasets and 69 tasks; in order to provide more robust results, the presented results are the average results for identical task types performed on the same dataset. For hierarchical purposes, RDBench organizes datasets into 3 different levels based on the number of tables"
  - [section 7]: Detailed presentation of RDBench-L1 (3-5 tables), RDBench-L2 (6-10 tables), and RDBench-L3 (10+ tables) with experimental results.
  - [corpus]: Weak evidence - corpus papers mention benchmarking RDBs but don't validate table count as complexity metric.
- **Break condition**: If datasets within levels have inconsistent complexity or if table count poorly correlates with actual modeling difficulty.

### Mechanism 3
- **Claim**: Averaging results over multiple tasks per dataset enhances experimental robustness.
- **Mechanism**: For each dataset, RDBench selects multiple target columns for prediction tasks and reports averaged results, reducing variance from task-specific factors.
- **Core assumption**: Multiple target columns provide representative coverage of a dataset's modeling challenges.
- **Evidence anchors**:
  - [abstract]: "We design multiple classification and regression tasks for each RDB dataset and report averaged results over the same dataset, further enhancing the robustness of the experimental findings."
  - [section 6.3]: "in order to enhance robustness, for the same dataset and task, we select multiple target columns to define a task, performing an averaging operation to get the final result."
  - [corpus]: No direct evidence - corpus focuses on graph modeling approaches rather than benchmarking methodology.
- **Break condition**: If selected target columns are not diverse enough or if averaging masks important performance differences.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) for relational data
  - Why needed here: RDBench leverages GNNs as one of the primary model families for graph representations of relational databases
  - Quick check question: How do GNNs aggregate information from neighboring nodes in a graph structure?

- **Concept**: Relational database schema and foreign key relationships
  - Why needed here: Understanding how tables connect through foreign keys is fundamental to transforming RDBs into graphs
  - Quick check question: What is the difference between a primary key and a foreign key in relational databases?

- **Concept**: Supervised learning task formulation (classification vs regression)
  - Why needed here: RDBench defines prediction tasks as column value prediction, requiring understanding of different task types
  - Quick check question: When would you choose classification over regression for a prediction task?

## Architecture Onboarding

- **Component map**: RDB datasets (11) -> Task selection (69) -> Interface transformation (tabular/homogeneous/heterogeneous) -> Model training (10 baselines) -> Cross-validation evaluation -> Result averaging

- **Critical path**: 
  1. Load dataset via unified API
  2. Select target column and task type
  3. Transform data to desired interface format
  4. Train and evaluate baseline models
  5. Aggregate results across tasks

- **Design tradeoffs**:
  - Flexibility vs standardization: Providing 3 interfaces enables diverse method comparison but increases complexity
  - Granularity vs robustness: Multiple target columns per dataset increases robustness but requires more computation
  - Breadth vs depth: 11 datasets across domains provides coverage but may limit depth on any single domain

- **Failure signatures**:
  - Performance degradation when increasing table count beyond certain threshold
  - Inconsistent results between different data interfaces for the same task
  - Target leakage when columns are deterministically derived from others

- **First 3 experiments**:
  1. Run XGBoost on rdb1-accdt tabular interface (classification task) - expect strong performance on medium complexity
  2. Run GCN on rdb2-hbv homogeneous graph interface - test graph method on medium complexity
  3. Compare XGBoost vs HGT on rdb3-toxic heterogeneous graph interface - test performance gap at high complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed graph transformations handle cyclic dependencies in relational databases?
- Basis in paper: [explicit] The paper discusses transforming relational databases to graphs but doesn't explicitly address cyclic dependencies in the data.
- Why unresolved: The transformation procedure and its implications for cyclic dependencies are not detailed in the paper.
- What evidence would resolve it: A detailed analysis of the transformation procedure's behavior with cyclic dependencies, including examples and potential solutions.

### Open Question 2
- Question: How does the performance of graph neural networks on relational databases compare to traditional SQL-based analytics for complex queries?
- Basis in paper: [inferred] The paper benchmarks GNNs on relational databases but doesn't compare their performance to traditional SQL analytics.
- Why unresolved: The paper focuses on ML methods and doesn't include a comparison with traditional database query optimization techniques.
- What evidence would resolve it: A comprehensive study comparing GNN performance on relational databases with optimized SQL queries for various complex analytical tasks.

### Open Question 3
- Question: What is the impact of database normalization on the effectiveness of graph neural networks for prediction tasks?
- Basis in paper: [inferred] The paper doesn't discuss the relationship between database normalization and GNN performance.
- Why unresolved: The paper presents a general transformation method but doesn't analyze how different database schemas affect GNN results.
- What evidence would resolve it: Empirical studies comparing GNN performance across databases with different normalization levels, including analysis of feature propagation and information loss.

## Limitations

- The averaging methodology for enhancing robustness is medium confidence without deeper analysis of whether selected target columns are truly representative
- The assumption that table count accurately represents relational complexity is medium confidence, as some databases with fewer tables may have more complex relationships
- The transformation procedure between relational databases and graph structures requires empirical validation to confirm preservation of task semantics across interfaces

## Confidence

- Unified task definition across interfaces: Medium confidence
- Table count as complexity proxy: Medium confidence  
- Averaging methodology for robustness: Low confidence

## Next Checks

1. **Cross-interface consistency validation**: Compare performance of the same model type (e.g., XGBoost) on tabular vs graph interfaces for identical tasks to quantify information loss/gain in transformations.

2. **Complexity correlation analysis**: Measure correlation between table count and actual model performance variance within and across complexity levels to validate the table-based complexity metric.

3. **Target column diversity assessment**: Analyze the correlation structure between selected target columns to verify they capture independent modeling challenges rather than redundant information.