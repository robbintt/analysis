---
ver: rpa2
title: Enhancing Automated Program Repair through Fine-tuning and Prompt Engineering
arxiv_id: '2304.07840'
source_url: https://arxiv.org/abs/2304.07840
tags:
- code
- dataset
- both
- codet5
- repair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) pre-trained\
  \ on both natural language (NL) and programming language (PL) can improve automated\
  \ program repair by incorporating code review comments. The authors fine-tune two\
  \ state-of-the-art PL-NL models\u2014PLBART and CodeT5\u2014on two code repair datasets\
  \ and compare their performance against previous methods."
---

# Enhancing Automated Program Repair through Fine-tuning and Prompt Engineering

## Quick Facts
- arXiv ID: 2304.07840
- Source URL: https://arxiv.org/abs/2304.07840
- Authors: [Not specified in source]
- Reference count: 29
- Key outcome: Fine-tuning PL-NL pre-trained models (PLBART and CodeT5) on code repair datasets with code review comments significantly improves repair accuracy, with CodeT5 generally outperforming PLBART.

## Executive Summary
This paper investigates whether large language models pre-trained on both natural language and programming language can improve automated program repair by incorporating code review comments. The authors fine-tune two state-of-the-art PL-NL models—PLBART and CodeT5—on two code repair datasets and compare their performance against previous methods. Results show that both fine-tuned models outperform earlier approaches by 9.91% (PLBART) and 8.45% (CodeT5) on the Review4Repair dataset, and by 20.41% (PLBART) and 24.72% (CodeT5) on the Tufano et al. dataset in Top-10 accuracy. The study reveals that pre-training weights are crucial for performance, with model architecture playing a smaller role. CodeT5 generally performs better, especially on complex repair tasks, highlighting the value of integrating natural language understanding into code repair systems.

## Method Summary
The authors fine-tune PLBART and CodeT5 models on two code repair datasets by concatenating buggy code with code review comments as input, using special tokens to encapsulate the reviews. They evaluate performance using exact match accuracy (Top-1, Top-5, Top-10) and BLEU/CodeBLEU metrics. The study includes ablation experiments to assess the impact of pre-training weights and architectural components, and compares results against previous automated program repair methods.

## Key Results
- Fine-tuned PLBART and CodeT5 models outperform previous methods by 9.91% and 8.45% on Review4Repair dataset in Top-10 accuracy
- CodeT5 generally outperforms PLBART, especially on complex repair tasks
- Pre-training weights are essential for performance, with ablation showing substantial drops when removed
- Incorporating code review comments significantly improves repair accuracy compared to buggy code alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training PL-NL models on both programming and natural language corpora improves downstream program repair accuracy.
- Mechanism: Pre-training enables the model to learn universal language representations that capture both code syntax and semantics, as well as natural language understanding, which is leveraged during fine-tuning on repair datasets.
- Core assumption: The learned parameters from PL-NL pre-training are essential for the performance boost, not just the model architecture.
- Evidence anchors:
  - [abstract]: "Large language models, trained with Natural Language (NL) and Programming Language (PL), can contain inherent knowledge of both."
  - [section II]: "Transformer-based pre-trained models are trained on large corpora to acquire universal language representations and may then be used for downstream NLP (Natural Language Processing) tasks..."
  - [corpus]: Weak anchor — neighbor papers focus on parameter-efficient tuning rather than pre-training itself.
- Break condition: If pre-training is removed (weights reset), the performance drop would be substantial (as observed in ablation studies).

### Mechanism 2
- Claim: Incorporating code review comments with buggy code during fine-tuning significantly boosts repair performance.
- Mechanism: Code reviews provide natural language descriptions of bugs, allowing the model to map NL intent to code fixes using its PL-NL understanding.
- Core assumption: The model can effectively fuse NL (review) and PL (buggy code) inputs to generate correct repairs.
- Evidence anchors:
  - [abstract]: "Results show that both fine-tuned models outperform the earlier models by 9.91% (PLBART) and 8.45% (CodeT5) on the Review4Repair dataset..."
  - [section III.A]: "We concatenated the buggy code and its respective code review into a single line, with the code review encapsulated by special tokens..."
  - [corpus]: Moderate anchor — related work explicitly tests fine-tuning with and without code reviews.
- Break condition: If reviews are omitted during fine-tuning, accuracy drops significantly (see Section V.B).

### Mechanism 3
- Claim: CodeT5 outperforms PLBART on program repair tasks due to better PL-NL alignment via identifier-aware pre-training.
- Mechanism: CodeT5's identifier-aware denoising and bimodal dual generation objectives capture richer code semantics and improve NL-PL alignment.
- Core assumption: Identifier tagging and type prediction help the model better understand code context during repair.
- Evidence anchors:
  - [abstract]: "Among the two models, CodeT5 generally performs better, especially on complex repair tasks."
  - [section II.C.2]: "CodeT5 ... enhance the denoising sequence-to-sequence goal in T5 by proposing two identifier tagging and prediction objectives..."
  - [corpus]: Weak anchor — neighbor papers do not focus on identifier-aware objectives.
- Break condition: If identifier tagging objectives are removed, CodeT5's advantage over PLBART may diminish.

## Foundational Learning

- Concept: Sequence-to-sequence modeling
  - Why needed here: Both PLBART and CodeT5 are seq2seq models that transform buggy code+review into fixed code.
  - Quick check question: In a seq2seq model, what are the roles of the encoder and decoder during program repair?
- Concept: Transformer architecture (multi-head attention, feed-forward layers)
  - Why needed here: The underlying model structure affects how well NL and PL are integrated.
  - Quick check question: How does multi-head attention enable the model to attend to both code tokens and review tokens simultaneously?
- Concept: Fine-tuning vs. pre-training
  - Why needed here: Fine-tuning adapts pre-trained weights to the specific repair task using limited labeled data.
  - Quick check question: Why might fine-tuning be more efficient than training a model from scratch on repair datasets?

## Architecture Onboarding

- Component map: Tokenizer -> Encoder -> Cross-attention + Self-attention -> Decoder -> Output ranking (beam search) -> Exact match evaluation
- Critical path: Tokenization → Encoder encoding → Cross-attention + self-attention → Decoder generation → Output ranking (beam search) → Exact match evaluation
- Design tradeoffs:
  - Input length capped at 512 tokens to fit GPU memory; may truncate long functions
  - Beam size vs. latency: Larger beams improve Top-K accuracy but increase compute
  - Use of special tokens for review encapsulation vs. implicit concatenation
- Failure signatures:
  - Sudden accuracy drop when pre-trained weights are reset (ablated)
  - Low BLEU/CodeBLEU but high exact match → model is precise but rigid
  - Model fails to generate valid syntax → tokenization or decoding issue
- First 3 experiments:
  1. Fine-tune PLBART on Review4Repair with review comments; measure Top-1 accuracy
  2. Repeat without reviews (only buggy code); compare accuracy drop
  3. Reset PLBART weights (no pre-training) and fine-tune again; measure performance loss

## Open Questions the Paper Calls Out
- How do pre-trained transformer models perform on program repair tasks involving multiple programming languages beyond Java?
- What is the impact of code review quality and complexity on the performance of automated program repair systems using pre-trained transformer models?
- How do pre-trained transformer models compare to other automated program repair techniques in terms of computational efficiency and scalability?

## Limitations
- Findings are limited to Java code and may not generalize to other programming languages
- Performance gains may be specific to the code review comment format and bug types in the studied datasets
- Evaluation metrics (exact match accuracy, BLEU/CodeBLEU) may not fully capture semantic correctness of repairs

## Confidence
- High Confidence: PL-NL pre-trained models significantly outperform previous methods; pre-training weights are essential for performance; incorporating code review comments improves repair accuracy
- Medium Confidence: CodeT5 consistently outperforms PLBART on complex repair tasks; performance boost comes primarily from pre-training rather than model architecture; identifier-aware pre-training objectives are the key differentiator for CodeT5's superior performance

## Next Checks
- Ablation on Review Components: Test performance when code review comments are progressively removed or replaced with synthetic descriptions to validate whether specific review content drives performance gains
- Cross-Dataset Generalization: Evaluate fine-tuned models on entirely different program repair dataset without additional fine-tuning to test generalizability beyond specific bug types and comment styles
- Architectural Isolation Experiment: Create controlled experiment isolating architectural differences by training two models with identical pre-training but different decoder configurations to directly test whether claimed architectural advantages of CodeT5 are responsible for performance differences