---
ver: rpa2
title: Conformal Prediction for Deep Classifier via Label Ranking
arxiv_id: '2310.06430'
source_url: https://arxiv.org/abs/2310.06430
tags:
- saps
- prediction
- size
- raps
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sorted Adaptive Prediction Sets (SAPS), a
  novel conformal prediction algorithm that addresses the issue of large prediction
  sets caused by miscalibrated softmax probabilities. The key insight is that the
  exact probability values are not necessary for constructing non-conformity scores
  in conformal prediction.
---

# Conformal Prediction for Deep Classifier via Label Ranking

## Quick Facts
- arXiv ID: 2310.06430
- Source URL: https://arxiv.org/abs/2310.06430
- Authors: 
- Reference count: 29
- Key outcome: Introduces Sorted Adaptive Prediction Sets (SAPS), a novel conformal prediction algorithm that produces significantly smaller prediction sets than Adaptive Prediction Sets while maintaining marginal and improving conditional coverage rates.

## Executive Summary
This paper addresses the inefficiency of Adaptive Prediction Sets (APS) in producing large prediction sets due to reliance on potentially miscalibrated softmax probabilities. The proposed Sorted Adaptive Prediction Sets (SAPS) algorithm removes the dependence on absolute probability values by retaining only the maximum softmax probability and using a constant to represent rank information. This approach preserves uncertainty information while dramatically reducing prediction set sizes, particularly for large-scale classification tasks. The method is theoretically grounded and demonstrates superior performance across ImageNet, CIFAR-100, and CIFAR-10 datasets.

## Method Summary
SAPS computes non-conformity scores by sorting softmax probabilities, retaining only the maximum probability value and the rank of the true label, with all other probability values replaced by a constant λ. This modification maintains coverage guarantees while making prediction set size depend primarily on model accuracy rather than absolute probability magnitudes. The method requires temperature scaling for calibration and involves hyperparameter tuning for λ, which is searched over an arithmetic sequence [0.02, 0.6] with step 0.03.

## Key Results
- SAPS reduces average prediction set size from 20.95 to 2.98 on ImageNet (α=0.1)
- Maintains marginal coverage rates while improving conditional coverage (lower ESCV)
- Performance advantage increases with dataset size and classification complexity
- Effective across multiple architectures including ResNet, DenseNet, VGG, Inception, ShuffleNet, ViT, DeiT, and CLIP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probability values beyond the maximum softmax are not necessary for constructing valid non-conformity scores in conformal prediction.
- Mechanism: By replacing all non-maximum probability values with a constant (λ), the method decouples prediction set size from the long-tail softmax distribution, making sets smaller and more adaptive.
- Core assumption: The rank ordering of labels and the maximum probability contain sufficient information for uncertainty quantification.
- Evidence anchors:
  - [abstract] "The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information."
  - [section] "We empirically show that the probability value is not necessary in APS. Specifically, APS without probability value generates smaller prediction sets than vanilla APS."
  - [corpus] Weak evidence; the corpus focuses on related conformal methods but does not directly confirm the redundancy claim.
- Break condition: If the maximum probability is poorly calibrated or the model has very low confidence, the constant λ may dominate and degrade performance.

### Mechanism 2
- Claim: SAPS produces prediction sets whose size is inversely related to model accuracy.
- Mechanism: The expected set size depends only on the accuracy of the top k predictions, not on the absolute probability magnitudes.
- Core assumption: Well-calibrated models ensure that the maximum probability is a reliable indicator of correctness.
- Evidence anchors:
  - [section] "Theoretically, we show that, by removing the probability value, the size of prediction sets generated by APS is consistent with model prediction accuracy."
  - [section] "a higher model accuracy will lead to a smaller value of k, indicating a smaller prediction sets."
  - [corpus] No direct corpus evidence; the claim relies on theoretical analysis and ablation experiments.
- Break condition: If the model is miscalibrated, the maximum probability may mislead the size calculation, breaking the accuracy-size relationship.

### Mechanism 3
- Claim: SAPS improves conditional coverage and adapts prediction set size to instance difficulty.
- Mechanism: The non-conformity score incorporates both the maximum probability and the label rank, so easy examples (high max prob, low rank) get smaller sets, while hard examples (low max prob, high rank) get larger sets.
- Core assumption: The rank of the ground-truth label in the sorted softmax probabilities is a valid proxy for instance difficulty.
- Evidence anchors:
  - [abstract] "SAPS can produce sets of small size and communicate instance-wise uncertainty."
  - [section] "the prediction set can be smaller for easy inputs than for hard ones."
  - [corpus] No corpus evidence; this is supported by the empirical results in Figure 3b of the paper.
- Break condition: If the ranking is noisy or the calibration set is too small, the adaptation to difficulty may degrade.

## Foundational Learning

- Concept: Conformal prediction and non-conformity scores
  - Why needed here: The entire method relies on constructing valid non-conformity scores that guarantee coverage; understanding how APS and RAPS work is essential to see why SAPS improves on them.
  - Quick check question: In APS, how is the non-conformity score computed from the softmax probabilities?
- Concept: Softmax probability calibration
  - Why needed here: SAPS depends on a well-calibrated maximum probability; without calibration, the method may fail.
  - Quick check question: What is temperature scaling and how does it affect the maximum softmax probability?
- Concept: Label ranking and instance difficulty
  - Why needed here: SAPS uses the rank of the true label to adapt set size; understanding this link is key to grasping the adaptation mechanism.
  - Quick check question: In a K-class problem, what does it mean if the true label has rank 1 versus rank K in the sorted softmax list?

## Architecture Onboarding

- Component map: Pre-trained model → softmax probabilities → sort labels by probability → retain max probability and label rank → compute SAPS non-conformity score → calibrate threshold → produce prediction sets
- Critical path:
  1. Sort probabilities for each input
  2. Extract max probability and ground-truth rank
  3. Compute non-conformity score
  4. Find threshold from calibration set
  5. Produce prediction set for each test input
- Design tradeoffs:
  - Using a constant λ instead of full probabilities reduces set size but may reduce sensitivity to very uncertain examples.
  - Calibration quality directly impacts SAPS performance; poor calibration leads to suboptimal set sizes.
  - λ must be tuned; too small and the method behaves like APS; too large and all sets become large.
- Failure signatures:
  - Set sizes do not shrink even with accurate models → likely calibration issue or λ too large.
  - Very small sets with poor coverage → λ too small or calibration threshold too high.
  - Large variance in set sizes across similar examples → rank information noisy or calibration set too small.
- First 3 experiments:
  1. Run APS and SAPS on a small calibrated dataset (e.g., CIFAR-10) and compare average set sizes and coverage.
  2. Sweep λ on a validation set and plot set size vs. λ to find the optimal value.
  3. Test SAPS on a calibrated model with known high accuracy and verify that set size decreases as accuracy increases.

## Open Questions the Paper Calls Out

- How does SAPS perform under different levels of model miscalibration?
- What is the theoretical relationship between the hyperparameter λ and prediction set size?
- How does SAPS scale to extreme classification scenarios with millions of classes?
- What is the impact of using different ranking methods (e.g., weighted rankings vs simple rank indices)?

## Limitations
- Core mechanism lacks direct corpus validation despite empirical demonstration
- Performance under severe distribution shifts shows less pronounced improvement over APS
- Theoretical claim about expected set size domination depends on assumption that probability values are redundant, which lacks independent corroboration

## Confidence
- Mechanism 1 (probability redundancy): Medium - ablation shows smaller sets, but no independent theoretical or empirical validation of the core assumption
- Mechanism 2 (accuracy-set size relationship): Medium - theoretical analysis supported by CIFAR experiments, but not extensively tested across diverse architectures
- Mechanism 3 (conditional coverage improvement): Low - ESCV results show improvement but the method's sensitivity to calibration quality is not thoroughly explored

## Next Checks
1. Test SAPS with uncalibrated models to quantify degradation in coverage and set size, confirming the importance of temperature scaling
2. Evaluate SAPS under severe covariate shift (e.g., ImageNet-Sketch vs. ImageNet) with multiple confidence levels (α=0.05, 0.1, 0.2) to assess robustness
3. Implement a hyperparameter sensitivity analysis for λ across different architectures and dataset sizes to determine optimal search strategies