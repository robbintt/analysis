---
ver: rpa2
title: 'CRAB: Assessing the Strength of Causal Relationships Between Real-world Events'
arxiv_id: '2311.04284'
source_url: https://arxiv.org/abs/2311.04284
tags:
- causal
- causality
- events
- event
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CRAB is a benchmark designed to assess the ability of large language
  models (LLMs) to understand the causal relationships between real-world events.
  It contains fine-grained, contextual causality annotations for approximately 2.7K
  pairs of real-world events extracted from news articles.
---

# CRAB: Assessing the Strength of Causal Relationships Between Real-world Events

## Quick Facts
- arXiv ID: 2311.04284
- Source URL: https://arxiv.org/abs/2311.04284
- Reference count: 15
- Key outcome: CRAB benchmark evaluates LLMs' ability to understand causal relationships between real-world events, revealing models perform poorly especially on complex causal structures and cross-document reasoning.

## Executive Summary
CRAB is a benchmark designed to evaluate how well large language models can understand and reason about causal relationships between real-world events. The benchmark contains approximately 2.7K pairs of events with fine-grained causality annotations extracted from news articles. It addresses the challenge of assessing causal reasoning when the causal signal comes from different contexts and evaluates performance on both simple and complex causal structures. Experiments with several LLMs show that most models struggle with causal reasoning, particularly when events are derived from complex causal structures or extracted from different documents.

## Method Summary
The CRAB benchmark uses prompting strategies with various LLMs (GPT-3, GPT-4, Flan-Alpaca) to assess pairwise causality between events and rank potential causes for effects. The dataset contains event pairs with contextual causality annotations, categorized into different causal structures including mediation, confounding, and collider patterns. Models are evaluated using macro F1-scores for binary, multi-class, and graded causality tasks, as well as exact match scores for causal chain inference tasks. The benchmark also distinguishes between in-document and cross-document event pairs to assess contextual reasoning capabilities.

## Key Results
- LLMs perform significantly worse on causal reasoning when events come from different documents versus the same document
- Models struggle particularly with complex causal structures like collider cases compared to simple linear chains
- Most models tend to predict high causality for related but not necessarily causally connected events, confusing correlation with causation

## Why This Works (Mechanism)

### Mechanism 1
LLM causal reasoning fails on complex structures because they cannot properly model mediating, confounding, and collider relationships. The benchmark isolates event pairs from different causal chain structures and shows models perform worse on collider cases where two causes converge to an effect without being related to each other.

### Mechanism 2
Models rely on explicit textual cues rather than implicit reasoning when events come from different documents. The benchmark shows better performance on in-document pairs versus cross-document pairs, suggesting models use explicit causal language rather than inferring relationships across contexts.

### Mechanism 3
Models confuse correlation with causation by assigning high causality scores to related but not causally connected events. Binary causality experiments show models tend to predict causation for event pairs with gold scores above 50, indicating they treat relatedness as causal strength.

## Foundational Learning

- **Concept: Actual causality vs correlation** - Why needed here: The benchmark evaluates models' ability to distinguish true causal relationships from mere correlations. Quick check question: Can you explain the difference between a mediating event and a confounding event?
- **Concept: Causal chain structures** - Why needed here: The benchmark analyzes model performance on different causal chain patterns (mediation, confounding, collider). Quick check question: In a collider structure, how are the two causes related to each other?
- **Concept: Contextualized causality assessment** - Why needed here: The benchmark requires models to assess causality based on document contexts rather than isolated event pairs. Quick check question: How might the context of an event change your assessment of its causal relationship to another event?

## Architecture Onboarding

- **Component map**: Data pipeline → Event extraction → Timeline construction → Causality annotation → Model evaluation
- **Critical path**: Event extraction accuracy → Timeline temporal ordering → Causality annotation quality → Model performance
- **Design tradeoffs**: Generative event extraction provides semantic granularity but risks hallucination; crowdsourcing provides diverse perspectives but introduces subjectivity
- **Failure signatures**: Poor performance on cross-document pairs → issues with contextual reasoning; poor performance on collider structures → inability to handle complex causal patterns
- **First 3 experiments**:
  1. Test model performance on in-document pairs vs cross-document pairs to verify the contextual reasoning challenge
  2. Evaluate model performance on simple linear chains vs complex structures to verify the structural reasoning challenge
  3. Test model ability to distinguish high vs low causality pairs to verify the correlation vs causation distinction

## Open Questions the Paper Calls Out

### Open Question 1
How do large language models' causal reasoning capabilities change when trained on data that includes explicit causal annotations? The paper mentions that models perform poorly on causal reasoning tasks, especially for complex structures and cross-document events, and that memorization vs. generalization is a concern.

### Open Question 2
Can fine-tuning large language models on the CRAB dataset improve their performance on causal reasoning tasks? The paper states that fine-tuned DeBERTa-large and Llama2-7B models perform poorly on CRAB, but it does not explore the potential benefits of fine-tuning on this specific dataset.

### Open Question 3
How do different causal structures (e.g., mediation, confounding, collider) affect the performance of large language models on causal reasoning tasks? The paper analyzes models' performance on different causal structures and finds that they struggle more with complex structures like collider cases.

## Limitations
- The benchmark relies on human-generated annotations for causality, introducing subjectivity despite majority voting
- Evaluation focuses on English-language news events, limiting generalizability to other domains or languages
- The study does not address temporal ordering challenges in event extraction, which could affect causality assessment

## Confidence

- **High confidence**: Findings related to model performance differences between in-document and cross-document pairs, consistently observed across multiple LLMs and task types
- **Medium confidence**: Claims about model difficulties with complex causal structures, as evidence is correlational and depends on accuracy of causal frame categorization
- **Medium confidence**: Correlation vs causation distinction findings, as binary threshold approach may oversimplify graded nature of causality
- **Low confidence**: Generalizability of results to other domains or languages due to English news corpus limitation

## Next Checks

1. **Temporal Validation**: Implement systematic check of event temporal ordering accuracy to assess how temporal ambiguities affect causality predictions, including evaluating whether models can correctly identify temporal sequence before making causality judgments.

2. **Cross-Domain Generalization**: Test model performance on events from different domains (scientific literature, social media, historical texts) to evaluate whether observed limitations extend beyond news articles.

3. **Alternative Annotation Methods**: Compare results using different annotation schemes, such as continuous causality scores rather than binary thresholds, or incorporating multiple expert annotators with specialized causal reasoning training.