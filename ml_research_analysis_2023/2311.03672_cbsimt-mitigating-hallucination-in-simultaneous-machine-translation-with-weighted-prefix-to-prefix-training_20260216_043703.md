---
ver: rpa2
title: 'CBSiMT: Mitigating Hallucination in Simultaneous Machine Translation with
  Weighted Prefix-to-Prefix Training'
arxiv_id: '2311.03672'
source_url: https://arxiv.org/abs/2311.03672
tags:
- translation
- source
- simt
- hallucination
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the hallucination problem in simultaneous
  machine translation (SiMT) by proposing the Confidence-Based Simultaneous Machine
  Translation (CBSiMT) framework. The framework leverages model confidence to perceive
  and mitigate hallucination tokens through weighted prefix-to-prefix training.
---

# CBSiMT: Mitigating Hallucination in Simultaneous Machine Translation with Weighted Prefix-to-Prefix Training

## Quick Facts
- arXiv ID: 2311.03672
- Source URL: https://arxiv.org/abs/2311.03672
- Authors: 
- Reference count: 4
- Key outcome: CBSiMT improves SiMT translation quality by up to 2 BLEU points at low latency through confidence-based weighted training

## Executive Summary
This paper addresses hallucination in simultaneous machine translation (SiMT) by introducing the Confidence-Based Simultaneous Machine Translation (CBSiMT) framework. The framework uses model confidence to identify and downweight hallucinated tokens during training through a weighted prefix-to-prefix training approach. By calculating token-level and sentence-level weights based on predicted probabilities and reordering costs, CBSiMT effectively reduces hallucination while maintaining low latency. Experiments on MuST-C En→Zh and WMT15 De→En tasks demonstrate consistent improvements in translation quality across multiple latency regimes.

## Method Summary
CBSiMT extends the prefix-to-prefix training framework by introducing weighted loss functions based on model confidence. The method calculates token-level weights using predicted probabilities as confidence scores, sentence-level weights using reordering costs, and applies diagonal regularization to encourage predictions along optimal READ/WRITE paths. During training, the model computes hidden states for multiple source prefixes, calculates confidence matrices, and applies the weighted loss function. At inference time, an adaptive threshold policy based on confidence scores determines when to start generating target tokens. The framework is implemented as a fine-tuning procedure after pre-training an offline NMT model.

## Key Results
- CBSiMT achieves up to 2 BLEU score improvements on MuST-C En→Zh at low latency regimes
- Consistent quality improvements across most latency settings compared to baseline SiMT models
- Effective reduction in hallucination tokens through confidence-based weighting
- Maintains competitive latency-quality trade-offs compared to adaptive strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level weights based on model confidence reduce the impact of hallucination tokens during training
- Mechanism: The model calculates predicted probabilities for each target token given each source prefix, treating these probabilities as confidence scores. Tokens predicted with low confidence receive lower weights in the loss function, reducing their influence on model training
- Core assumption: Low confidence predictions correspond to hallucinated tokens that are unfaithful to the source prefix
- Evidence anchors: [abstract]: "token-level and sentence-level weights are calculated based on model confidence and acted on the loss function"; [section 3.2.1]: "We exactly use token-level predicted probabilities as token-level confidence, which can reflect the correctness of each generated token"

### Mechanism 2
- Claim: Sentence-level weights based on reordering cost reduce the negative impact of non-monotonic sentence pairs during training
- Mechanism: The model calculates a reordering score C for each sentence pair based on the difference between source and target positions. Sentence pairs with high reordering costs receive lower weights in the loss function, reducing their influence on model training
- Core assumption: Non-monotonic sentence pairs are more likely to cause hallucination problems in SiMT models
- Evidence anchors: [abstract]: "employ the sentence-level weight to alleviate the disturbance of sentence pairs with serious word order differences on the model"; [section 2.2]: "as the monotonicity of the training data decreases, the performance decline of the wait-k model is more significant than that of the offline model, accompanied by a massive increase in the hallucination rate"

### Mechanism 3
- Claim: Diagonal regularization in token-level weights encourages predictions along ideal READ/WRITE paths with minimal latency
- Mechanism: The model applies a distance-based regularization to token-level weights, where predictions further from the diagonal in the confidence matrix receive lower weights. This discourages both high-latency predictions and hallucination
- Core assumption: Predictions along the diagonal represent near-optimal READ/WRITE paths with minimal latency
- Evidence anchors: [section 3.2.1]: "Intuitively, predictions close to the diagonal are nearly ideal READ/WRITE paths with 0 latency, and positions farther from the diagonal should be assigned smaller weights"

## Foundational Learning

- Concept: Simultaneous Machine Translation (SiMT) and prefix-to-prefix framework
  - Why needed here: The paper builds on the prefix-to-prefix framework as the foundation for its confidence-based approach. Understanding how SiMT works with partial source prefixes is crucial for grasping why hallucination occurs and how confidence can help
  - Quick check question: In SiMT, when does the model start generating target tokens relative to reading source tokens?

- Concept: Model confidence and its relationship to prediction quality
  - Why needed here: The entire CBSiMT framework relies on using model confidence as a proxy for faithfulness. Understanding how confidence relates to prediction quality is essential for understanding the mechanism
  - Quick check question: According to the preliminary study, what relationship exists between model confidence and hallucination tokens?

- Concept: Word order differences and monotonicity in parallel corpora
  - Why needed here: The sentence-level weighting mechanism specifically addresses the problem of non-monotonic sentence pairs. Understanding why word order differences matter is crucial for grasping this component
  - Quick check question: How does the Average Anticipation (AA) metric relate to sentence pair monotonicity?

## Architecture Onboarding

- Component map: Unidirectional encoder with masked self-attention -> Decoder that processes hidden streams -> Confidence calculation module -> Weight calculation module -> Weighted loss function
- Critical path: Source prefix → Encoder (multiple prefixes) → Hidden stream → Decoder (multiple predictions) → Confidence matrix → Weight calculation → Weighted loss
- Design tradeoffs:
  - Memory vs. accuracy: Calculating hidden states for multiple prefixes increases memory usage but provides more training signal
  - Latency vs. quality: Diagonal regularization reduces hallucination but may increase latency if too restrictive
  - Generalization vs. specificity: Sentence-level weighting reduces non-monotonic interference but may remove useful training examples
- Failure signatures:
  - BLEU score decreases without improvement in hallucination rate
  - AL values become too high, indicating excessive latency
  - Training instability or divergence due to extreme weight values
- First 3 experiments:
  1. Implement basic prefix-to-prefix training without weights to establish baseline performance
  2. Add token-level confidence weighting and measure impact on hallucination rate
  3. Add sentence-level weighting and diagonal regularization, measuring both BLEU and hallucination improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CBSiMT framework perform on language pairs with significantly different word orders (e.g., English-Japanese)?
- Basis in paper: [inferred] The paper mentions that hallucination issues may be more serious for translation tasks where word order differs more, but does not explore performance on such pairs.
- Why unresolved: The paper only tests CBSiMT on English-Chinese and German-English tasks, which have relatively similar word orders compared to English-Japanese.
- What evidence would resolve it: Testing CBSiMT on English-Japanese and reporting hallucination rates and BLEU scores would provide evidence of its effectiveness on language pairs with significant word order differences.

### Open Question 2
- Question: How does CBSiMT compare to other hallucination mitigation techniques in SiMT, such as domain-specific training or data augmentation?
- Basis in paper: [explicit] The paper mentions that Chen et al. (2021) generate monotonic pseudo data to reduce hallucination, but does not compare CBSiMT to this approach or other techniques.
- Why unresolved: The paper only compares CBSiMT to baseline SiMT models (wait-k, adaptive wait-k, CTC+ASN) and does not explore other hallucination mitigation techniques.
- What evidence would resolve it: Implementing and comparing CBSiMT to domain-specific training, data augmentation, or other hallucination mitigation techniques on the same tasks would provide evidence of its relative effectiveness.

### Open Question 3
- Question: How does the CBSiMT framework handle very long sentences in SiMT, given the computational constraints of calculating hidden states for all prefixes?
- Basis in paper: [explicit] The paper mentions that calculating hidden states for all prefixes may cause memory overflow for long sentences and introduces a sampling mechanism to control the size of hidden and probability streams.
- Why unresolved: The paper does not provide details on the sampling mechanism or how it affects the performance of CBSiMT on long sentences.
- What evidence would resolve it: Providing details on the sampling mechanism and testing CBSiMT on long sentences would demonstrate its effectiveness in handling very long sentences in SiMT.

## Limitations

- The framework's effectiveness depends on the assumption that low-confidence predictions correlate with hallucinated tokens, which may not hold universally across all language pairs
- The diagonal regularization assumption may not be optimal for all language pairs, particularly those with significantly different word orders
- The framework primarily demonstrates effectiveness on English-Chinese and German-English language pairs, limiting generalizability to other language combinations

## Confidence

**High Confidence**: The framework's core mechanism of using model confidence for weighted training is technically sound and well-supported by preliminary experiments showing correlation between confidence scores and hallucination tokens. The mathematical formulation of the weighted loss function and confidence calculations is clearly specified and reproducible.

**Medium Confidence**: The empirical results showing consistent BLEU improvements across multiple latency regimes are reasonably convincing, though the improvement magnitudes vary significantly between datasets (up to 2 BLEU points on En→Zh vs. smaller gains on De→En). The adaptive inference policy based on confidence thresholds shows promise but lacks detailed analysis of threshold optimization.

**Low Confidence**: The claim that diagonal regularization consistently leads to optimal latency-quality trade-offs across different language pairs and latency settings requires more extensive validation. The sentence-level weighting's effectiveness in handling non-monotonic sentence pairs needs broader testing across diverse datasets with varying degrees of word order differences.

## Next Checks

1. **Cross-language validation**: Test CBSiMT on additional language pairs with varying degrees of word order differences (e.g., English-French, English-Japanese) to verify generalization beyond the two tested pairs and assess whether the diagonal regularization assumption holds across typologically diverse languages.

2. **Ablation study on weight components**: Conduct systematic ablation studies removing token-level weights, sentence-level weights, and diagonal regularization separately to quantify each component's individual contribution and identify potential redundancy or interaction effects between the weighting mechanisms.

3. **Hallucination detection analysis**: Implement fine-grained analysis of hallucination types (e.g., content hallucination vs. entity hallucination) to determine whether CBSiMT specifically addresses certain hallucination categories more effectively than others, and whether the confidence-based approach might introduce new types of translation errors.