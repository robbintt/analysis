---
ver: rpa2
title: Sequential Knockoffs for Variable Selection in Reinforcement Learning
arxiv_id: '2303.14281'
source_url: https://arxiv.org/abs/2303.14281
tags:
- state
- variables
- cient
- algorithm
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses variable selection in high-dimensional Markov
  decision processes (MDPs), where the goal is to identify a minimal sufficient state
  that preserves the Markov property and optimal policy. The authors propose a novel
  Sequential Knockoffs (SEEK) algorithm that extends the Model-X knockoffs framework
  to sequential decision making.
---

# Sequential Knockoffs for Variable Selection in Reinforcement Learning

## Quick Facts
- arXiv ID: 2303.14281
- Source URL: https://arxiv.org/abs/2303.14281
- Reference count: 40
- Key outcome: This paper addresses variable selection in high-dimensional MDPs, proposing the SEEK algorithm that extends Model-X knockoffs to sequential decision making, achieving FDR control and selection consistency under β-mixing conditions.

## Executive Summary
This paper tackles the problem of variable selection in high-dimensional Markov decision processes (MDPs), where the goal is to identify a minimal sufficient state that preserves the Markov property and optimal policy. The authors propose a novel Sequential Knockoffs (SEEK) algorithm that extends the Model-X knockoffs framework to sequential decision making. SEEK works by iteratively applying knockoffs to select state variables that are significant for predicting rewards and state transitions, using a data-splitting approach to handle temporal dependence. Theoretical results show that SEEK controls the false discovery rate and achieves selection consistency under β-mixing conditions, with empirical experiments demonstrating superior performance compared to competing methods.

## Method Summary
The Sequential Knockoffs (SEEK) algorithm addresses variable selection in high-dimensional MDPs by iteratively applying knockoffs to select state variables significant for predicting rewards and state transitions. The method handles temporal dependence through a data-splitting approach, partitioning the data into K subsets with temporal gaps, and uses majority voting to aggregate results. The algorithm starts by selecting variables significant for immediate rewards, then iteratively selects variables significant for state transitions, repeating until convergence. Theoretical guarantees include FDR control and selection consistency under β-mixing conditions, with empirical validation on both synthetic and real data.

## Key Results
- SEEK controls FDR asymptotically even when temporal dependence exists in the data
- The algorithm identifies the minimal sufficient state by iteratively selecting variables significant for predicting rewards and state transitions
- Power approaches one asymptotically when using LASSO or generic ML methods for feature importance statistics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential knockoffs (SEEK) controls FDR asymptotically even when temporal dependence exists in the data.
- Mechanism: By splitting data into K subsets with temporal gaps of at least K and using Berbee's coupling lemma, the temporal dependence within each subset becomes negligible. The knockoffs procedure then controls FDR within each subset, and aggregation via majority vote preserves this control.
- Core assumption: The MDP satisfies exponential β-mixing with coefficient O(ρ^k) for some 0 < ρ < 1.
- Evidence anchors:
  - [abstract]: "Under a β-mixing condition, consistently recovers the minimal sufficient state, and controls the false discovery rate"
  - [section]: "Theorem 1... the knockoff algorithm in Algorithm 2 achieves a valid FDR control... The presence of the second term is due to the temporal dependence among transition tuples, which diverges to zero as the number of observations in Dk grows to infinity."
- Break condition: If the β-mixing condition fails (e.g., long-range dependencies persist), the error term K^(-1)(NT)^(-c) may not vanish, breaking FDR control.

### Mechanism 2
- Claim: SEEK identifies the minimal sufficient state by iteratively selecting variables significant for predicting rewards and state transitions.
- Mechanism: The algorithm starts by selecting variables significant for immediate rewards, then treats these selected variables as responses and selects variables significant for their transitions, repeating until convergence. This captures both direct and indirect effects on cumulative reward.
- Core assumption: The transition kernel is strictly positive (ensuring uniqueness of minimal sufficient state).
- Evidence anchors:
  - [abstract]: "identifies a minimal sufficient state that preserves the Markov property and optimal policy"
  - [section]: "Proposition 4... the iterative approach is able to correctly recover the minimal sufficient state for any MDP provided the transition kernel is strictly positive"
- Break condition: If the transition kernel is not strictly positive, multiple minimal sufficient states may exist, and SEEK may not identify the true minimal one.

### Mechanism 3
- Claim: Power approaches one asymptotically when using LASSO or generic ML methods for feature importance statistics.
- Mechanism: Under mild signal strength conditions (Conditions 3 and 5), the feature importance statistics separate strong signals from noise with high probability. This ensures that all variables in the minimal sufficient state are selected across iterations and data subsets.
- Core assumption: Minimal signal strength conditions that are much weaker than requiring all non-zero coefficients to be large.
- Evidence anchors:
  - [abstract]: "In large samples, the proposed method achieves selection consistency"
  - [section]: "Theorem 3... The power of SEEK using LASSO is lower bounded by Power := E(|ˆG∩GM|/|GM|) ≥ 1−O{p−c1}, which approaches one as NT →∞"
- Break condition: If signal strengths are too weak relative to noise (violating Conditions 3 or 5), the power bound 1−O{p−c1} may not approach one.

## Foundational Learning

- Concept: β-mixing and exponential ergodicity
  - Why needed here: The β-mixing condition is crucial for justifying the data-splitting approach and proving FDR control when observations are temporally dependent.
  - Quick check question: What does it mean for a process to be exponentially β-mixing, and why is this property important for the data-splitting step in SEEK?

- Concept: Model-X knockoffs framework
  - Why needed here: SEEK extends the Model-X knockoffs methodology to sequential decision making, requiring understanding of how knockoffs work in the regression setting.
  - Quick check question: How do Model-X knockoffs ensure FDR control in regression, and what properties must knockoff variables satisfy?

- Concept: Minimal sufficient state in MDPs
  - Why needed here: This is the target of variable selection - the smallest state subset preserving the Markov property and optimal policy.
  - Quick check question: What conditions must a state subset satisfy to be considered "minimal sufficient" in an MDP, and how does this differ from sufficiency in regression?

## Architecture Onboarding

- Component map: Data splitting -> Knockoffs construction -> Feature importance -> Iterative selection -> Majority vote -> Output selected variables

- Critical path: Data splitting → Knockoffs construction → Feature importance → Iterative selection → Majority vote → Output selected variables

- Design tradeoffs:
  - Larger K reduces temporal dependence but decreases sample size per subset
  - Different feature importance methods (LASSO vs RF vs deep learning) trade off interpretability vs flexibility
  - Majority vote threshold α balances between selection stringency and power

- Failure signatures:
  - High FDR despite theoretical guarantees: indicates β-mixing assumption violated or K too small
  - Low power: suggests signal strengths don't meet Conditions 3/5 or feature importance method inadequate
  - Inconsistent selections across runs: may indicate unstable feature importance estimates or insufficient sample size

- First 3 experiments:
  1. Validate FDR control on synthetic AR(1) MDP with known minimal sufficient state
  2. Test power analysis comparing LASSO vs random forest vs deep learning implementations
  3. Sensitivity analysis on K-selection algorithm across different mixing coefficients and sample sizes

## Open Questions the Paper Calls Out
- How does the proposed algorithm perform when the Markov property is only approximately satisfied, rather than exactly?
- Can the sequential knockoffs approach be extended to handle non-stationary environments where the set of significant variables changes over time?
- How sensitive is the algorithm's performance to the choice of the majority vote threshold α and target FDR level q?
- Can the algorithm be adapted for partially observed MDPs (POMDPs) where the full state is not directly observable?

## Limitations
- Strong β-mixing assumptions required for theoretical guarantees, which may be difficult to verify empirically
- Empirical evaluation focuses on synthetic data with limited real-world validation
- Computational complexity may limit scalability to very high-dimensional state spaces

## Confidence
- High confidence in FDR control guarantees under β-mixing conditions
- Medium confidence in selection consistency results
- Medium confidence in practical performance claims

## Next Checks
1. Empirical validation of β-mixing coefficient estimation on real-world sequential data from diverse domains (finance, healthcare, robotics)
2. Simulation study varying transition kernel positivity to quantify performance degradation
3. Benchmark comparison against alternative variable selection methods (LASSO, random forest importance, SHAP) on high-dimensional MDPs with varying degrees of temporal dependence and state dimensionality