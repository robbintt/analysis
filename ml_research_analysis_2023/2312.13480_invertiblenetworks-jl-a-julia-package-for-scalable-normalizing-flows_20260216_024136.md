---
ver: rpa2
title: 'InvertibleNetworks.jl: A Julia package for scalable normalizing flows'
arxiv_id: '2312.13480'
source_url: https://arxiv.org/abs/2312.13480
tags:
- normalizing
- arxiv
- flows
- memory
- package
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InvertibleNetworks.jl is a Julia package that implements normalizing
  flows with a focus on memory efficiency for high-dimensional applications. The package
  achieves this by manually implementing gradients for invertible layers, allowing
  intermediate activations to be recomputed instead of stored during backpropagation.
---

# InvertibleNetworks.jl: A Julia package for scalable normalizing flows

## Quick Facts
- arXiv ID: 2312.13480
- Source URL: https://arxiv.org/abs/2312.13480
- Reference count: 31
- Primary result: A Julia package achieving memory-efficient normalizing flows through hand-written gradients for invertible layers

## Executive Summary
InvertibleNetworks.jl is a Julia package designed for memory-efficient normalizing flows, particularly suited for high-dimensional applications. The package implements hand-written gradients for invertible layers, enabling recomputation of intermediate activations during backpropagation rather than storing them in memory. This approach achieves significant memory savings compared to automatic differentiation-based frameworks like PyTorch, allowing training on larger image sizes and deeper networks. The package supports various invertible layers including GLOW 1x1 convolutions, coupling layers, Haar wavelet transforms, and HINT, while maintaining compatibility with Julia's ChainRules for non-invertible components.

## Method Summary
The package implements normalizing flows with manually written gradients for invertible layers, leveraging the invertibility property to recompute intermediate activations during backpropagation instead of storing them. This approach is contrasted with automatic differentiation frameworks that store all intermediate values, creating memory bottlenecks for deep or high-dimensional models. The implementation includes integration with Julia's ChainRules to handle automatic differentiation of non-invertible components seamlessly. Memory consumption is benchmarked against PyTorch implementations using equivalent GLOW architectures on a 40GB A100 GPU with varying image sizes and network depths.

## Key Results
- Memory consumption remains constant as network depth increases, unlike PyTorch implementations where memory grows with depth
- Successfully trains on 1024x1024 images using a 40GB A100 GPU, while PyTorch equivalents fail at 480x480 due to memory constraints
- Compatible with Julia's ChainRules for automatic differentiation of non-invertible components within the same computational graph

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hand-written gradients enable memory-efficient backpropagation in invertible normalizing flows
- Mechanism: Manual gradient implementation allows recomputation of intermediate activations during backpropagation instead of storage, leveraging invertibility
- Core assumption: Invertibility enables exact reconstruction of intermediate activations
- Evidence anchors: [abstract] package excels in memory efficiency by leveraging inherent invertibility; [section] hand-written gradients enable scaling to large inputs
- Break condition: Non-invertible layers or numerical instability preventing accurate reconstruction

### Mechanism 2
- Claim: Memory consumption remains constant as network depth increases
- Mechanism: Recomputation in each layer accumulates linearly, offsetting memory needed for additional layers
- Core assumption: Memory savings from recomputation accumulate linearly across layers
- Evidence anchors: [section] memory consumption does not increase with depth due to invertibility; [section] package displays constant memory behavior vs PyTorch
- Break condition: Prohibitive recomputation costs or imperfect invertibility

### Mechanism 3
- Claim: ChainRules integration enables seamless combination of invertible and non-invertible components
- Mechanism: Compatibility with ChainRules allows handling automatic differentiation of non-invertible parts while maintaining memory efficiency
- Core assumption: ChainRules framework handles both hand-written and automatically differentiated components
- Evidence anchors: [section] package fully compatible with ChainRules; [section] integration allows arbitrary neural networks with automatic differentiation
- Break condition: ChainRules integration bottlenecks or compatibility issues

## Foundational Learning

- Concept: Normalizing flows and their invertibility property
  - Why needed here: Understanding invertibility's crucial role in memory efficiency
  - Quick check question: How does invertibility enable memory-efficient backpropagation?

- Concept: Automatic differentiation and its memory implications
  - Why needed here: To contrast with hand-written gradient approach and understand benefits
  - Quick check question: Why do standard AD frameworks struggle with memory efficiency in deep normalizing flows?

- Concept: ChainRules and differentiable programming in Julia
  - Why needed here: Understanding package integration with Julia ecosystem for non-invertible components
  - Quick check question: How does ChainRules enable seamless integration of hand-written and AD components?

## Architecture Onboarding

- Component map: Invertible layers (GLOW 1x1 convolutions, coupling layers, Haar wavelet transforms, HINT) -> ChainRules integration for non-invertible components -> GPU support -> Continuous integration testing
- Critical path: Implement hand-written gradients for new invertible layers → Integrate with ChainRules → Test for invertibility and gradient correctness → Benchmark memory efficiency
- Design tradeoffs: Memory efficiency vs. expressiveness, hand-written gradients vs. automatic differentiation, Julia-specific vs. cross-language compatibility
- Failure signatures: Out-of-memory errors during training, non-invertible layers, incorrect gradient implementations, ChainRules integration issues
- First 3 experiments:
  1. Replicate memory benchmark from Figure 1 to verify memory efficiency advantage
  2. Implement simple new invertible layer and test memory usage and correctness
  3. Create model combining invertible layers with non-invertible components and verify ChainRules integration

## Open Questions the Paper Calls Out

- Question: How does performance compare to AD frameworks when implementing non-invertible components within normalizing flows?
  - Basis in paper: [explicit] Mentions compatibility with ChainRules but lacks performance comparisons
  - Why unresolved: Paper focuses on memory efficiency for invertible layers, not mixed components
  - What evidence would resolve it: Benchmark studies comparing memory usage and training speed with integrated non-invertible components

- Question: What is the practical limit of input dimensionality for InvertibleNetworks.jl on consumer-grade GPUs?
  - Basis in paper: [explicit] Demonstrates scalability on 40GB A100 but lacks testing on smaller GPUs
  - Why unresolved: Establishes performance on high-end hardware but lacks data on common configurations
  - What evidence would resolve it: Systematic benchmarking across GPUs with varying memory capacities

- Question: How does the "channel explosion" problem affect scalability of invertible down/upsampling operations in 3D applications?
  - Basis in paper: [explicit] References channel explosion issue in future work but lacks empirical data
  - Why unresolved: Acknowledges as research avenue but doesn't investigate magnitude or mitigation
  - What evidence would resolve it: Quantitative analysis of memory/computational requirements for 3D applications

## Limitations

- Implementation-specific advantage tied to Julia's approach may not generalize to other languages
- Benchmark specificity limited to GLOW architecture with particular layer types
- Hardware dependency on 40GB A100 GPU may limit reproducibility on different configurations

## Confidence

- High confidence: Fundamental mechanism of hand-written gradients enabling memory efficiency is well-established
- Medium confidence: Claim about constant memory with increasing depth is supported but lacks comprehensive scaling studies
- Medium confidence: Practical advantages in specific applications are asserted but not quantitatively validated

## Next Checks

1. Implement and benchmark memory efficiency across different normalizing flow architectures (RealNVP, FFJORD) to verify generalizability
2. Implement equivalent hand-written gradient versions in PyTorch or another framework to isolate language-specific effects
3. Conduct extensive numerical tests to verify recomputation maintains precision without error accumulation in very deep networks