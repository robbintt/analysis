---
ver: rpa2
title: Improved Compositional Generalization by Generating Demonstrations for Meta-Learning
arxiv_id: '2305.13092'
source_url: https://arxiv.org/abs/2305.13092
tags:
- lturn
- pull
- while
- walk
- push
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes generating support demonstrations for in-context
  learning in grounded language tasks, as retrieval-based methods are insufficient
  for some splits of the gSCAN dataset. The core idea is to use a masked language
  model to generate relevant instruction samples conditioned on the current state,
  rank them with a state-instruction CLIP-like model, then generate the corresponding
  actions with a transformer trained on the training set.
---

# Improved Compositional Generalization by Generating Demonstrations for Meta-Learning

## Quick Facts
- **arXiv ID**: 2305.13092
- **Source URL**: https://arxiv.org/abs/2305.13092
- **Reference count**: 40
- **Key outcome**: Generating support demonstrations substantially improves performance on a previously unsolved gSCAN compositional split (H) without sacrificing performance on other splits

## Executive Summary
This paper addresses the challenge of compositional generalization in grounded language tasks by proposing a method to generate support demonstrations for in-context learning. The authors identify that retrieval-based methods are insufficient for some gSCAN splits because the optimal supports simply don't exist in the training data. They propose using a masked language model to generate instruction samples conditioned on the current state, ranking them with a state-instruction CLIP-like model, and then generating the corresponding actions with a transformer trained on the training set. Experiments show that this approach substantially improves performance on a challenging gSCAN split (H) without sacrificing performance on other splits, while retrieval-based methods fail to generate the necessary support demonstrations.

## Method Summary
The method generates support demonstrations for meta-learning in grounded language tasks by first using a masked language model to sample replacement tokens for masked tokens in the query instruction, creating multiple instruction variations. These generated instructions are then ranked by a CLIP-like model that encodes states and instructions into representations and scores them by the dot product of their embeddings. The top-ranked instructions are used as support inputs, and their corresponding actions are generated using a transformer trained on the training data. The meta-seq2seq architecture then uses these generated support inputs and outputs as context to predict query outputs, with symbol-index permutations forcing the model to use the supports.

## Key Results
- DemoGen substantially improves performance on gSCAN split H, achieving 67% success rate compared to retrieval methods at 17%
- The approach maintains or improves performance on all other gSCAN splits (A-G, I) compared to retrieval baselines
- Generated support sets have approximately 77% validity rate and 50% correctness rate, demonstrating the effectiveness of the generation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating support demonstrations is necessary because the optimal supports for certain test splits simply don't exist in the training data
- Mechanism: The method uses a masked language model to generate instruction samples conditioned on the current state, then ranks them with a state-instruction CLIP-like model, and finally generates the corresponding actions with a transformer trained on the training set
- Core assumption: The output generation model generalizes in-distribution, allowing it to generate probable outputs for generated support inputs
- Evidence anchors: [abstract]: "choosing good supports from the training data for a given test query is already a difficult problem, but in some cases solving this may not be enough"; [section]: "Even if we identify a helpful support instruction, we might not be able to find support outputs corresponding to that input in the same state"
- Break condition: If the output generation model fails to generalize in-distribution, the generated support outputs will be incorrect and unhelpful for meta-learning

### Mechanism 2
- Claim: Meta-learning with generated supports can solve previously unsolved compositional behavior splits without sacrificing performance on other splits
- Mechanism: The meta-seq2seq architecture uses support inputs and outputs as context to predict query outputs, with symbol-index permutations forcing the model to use the supports
- Core assumption: A sufficiently powerful model can use the generated supports and ignore the irrelevant ones
- Evidence anchors: [abstract]: "We show substantially improved performance on a previously unsolved compositional behaviour split without a loss of performance on other splits"; [section]: "We believe that as long as the instructions and actions in green are included in the support set, a sufficiently powerful model will be able to use them and ignore the other supports"
- Break condition: If the model is not sufficiently powerful or the relevant supports are not generated, the meta-learning approach will fail to solve the compositional generalization problem

### Mechanism 3
- Claim: The presence of irrelevant supports is not a particularly large problem as long as the other useful supports are also present in the support set
- Mechanism: The CLIP-like model ranks generated instructions by their applicability to the current state, filtering out unsolvable supports
- Core assumption: The ranking model can effectively distinguish between relevant and irrelevant support instructions
- Evidence anchors: [section]: "We show in the experiments that the presence of irrelevant supports is not a particularly large problem as long as the other useful supports are also present in the support set"; [section]: "Support inputs with a high score are likely to also be solvable and should be preferred over inputs that seem unusual or do not match the current context and therefore receive a low score"
- Break condition: If the ranking model fails to effectively distinguish relevant from irrelevant supports, the presence of irrelevant supports could degrade meta-learning performance

## Foundational Learning

- Concept: Meta-learning and in-context learning
  - Why needed here: The paper proposes using meta-learning with generated supports to solve compositional generalization problems in grounded language tasks
  - Quick check question: What is the difference between meta-learning and traditional supervised learning in the context of this paper?

- Concept: Masked Language Models (MLMs)
  - Why needed here: MLMs are used to generate support instructions by sampling replacement tokens for masked tokens in the query instruction
  - Quick check question: How does the MLM generate support instructions that are both related to the query and potentially solvable in the current state?

- Concept: CLIP-like models for ranking
  - Why needed here: A CLIP-like model is used to rank generated instructions by their applicability to the current state, filtering out unsolvable supports
  - Quick check question: How does the CLIP-like model encode instructions and states to produce representations that can be used for ranking?

## Architecture Onboarding

- Component map: Masked Language Model -> CLIP-like model -> Transformer -> Meta-seq2seq architecture
- Critical path:
  1. Generate support instructions using MLM
  2. Rank generated instructions using CLIP-like model
  3. Generate support actions using trained transformer
  4. Train meta-seq2seq with generated supports
  5. Evaluate on test splits
- Design tradeoffs:
  - Using generated supports vs. retrieved supports from training data
  - Number of generated supports (k) vs. computational cost
  - Ranking model quality vs. relevance of selected supports
- Failure signatures:
  - Poor performance on compositional generalization splits despite high in-distribution performance
  - Generated supports that are invalid or incorrect in the current state
  - Meta-seq2seq unable to effectively use the generated supports
- First 3 experiments:
  1. Compare performance of DemoGen vs. retrieval-based methods on gSCAN splits
  2. Analyze properties of generated support sets (validity, correctness, relevance)
  3. Evaluate impact of irrelevant supports on meta-learning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the approach be scaled to handle larger vocabulary sizes while maintaining the effectiveness of the permuter block mechanism?
- Basis in paper: [explicit] The paper mentions that the meta-seq2seq method may be difficult to scale to large vocabulary sizes due to the permutations of symbol/index mappings used during each training step.
- Why unresolved: The authors propose possible approaches to handle this problem but do not provide concrete solutions or experimental results.
- What evidence would resolve it: Experimental results showing the performance of the proposed approach with different vocabulary sizes and the effectiveness of the suggested solutions.

### Open Question 2
- Question: How does the quality of generated support inputs and outputs impact the performance of the meta-learning approach, and can this quality be further improved?
- Basis in paper: [explicit] The paper analyzes the properties of generated support sets and mentions that about 77% of generated support inputs are valid and 50% of the support pairs are both correct and valid.
- Why unresolved: The paper does not investigate the impact of the quality of generated supports on the performance of the meta-learning approach or explore ways to improve the quality of generated supports.
- What evidence would resolve it: Experiments showing the correlation between the quality of generated supports and the performance of the meta-learning approach, as well as studies on methods to improve the quality of generated supports.

### Open Question 3
- Question: How does the proposed approach compare to other methods for compositional generalization in grounded language learning, especially in terms of scalability and generalizability?
- Basis in paper: [inferred] The paper presents a novel approach for generating support sets for meta-learning in grounded language learning and shows improved performance on a challenging split of the gSCAN dataset. However, it does not provide a comprehensive comparison with other methods.
- Why unresolved: The paper does not include a thorough comparison with other state-of-the-art methods for compositional generalization in grounded language learning.
- What evidence would resolve it: A detailed comparison of the proposed approach with other methods, including performance metrics, scalability, and generalizability, on various benchmarks and datasets.

## Limitations
- The approach requires training multiple components (MLM, CLIP-like model, transformer) in addition to the meta-seq2seq model, increasing computational complexity
- The method's effectiveness depends on the quality of the generated supports, with only 50% of support pairs being both correct and valid
- Scaling to larger vocabulary sizes may be challenging due to the permutations of symbol/index mappings used during training

## Confidence
- **High confidence**: The core observation that retrieval-based support selection is insufficient for certain gSCAN splits (B-H) where optimal supports don't exist in training data
- **Medium confidence**: The methodology of generating support demonstrations using MLM sampling and CLIP-like ranking is sound and addresses a real limitation
- **Medium confidence**: The empirical results showing improved performance on split H without sacrificing other splits, though independent replication would strengthen this claim

## Next Checks
1. **Architectural verification**: Implement the MLM, ViLBERT encoder, and CLIP-like ranking model exactly as described and verify they can generate valid support instructions with >75% validity rate
2. **Ablation study**: Test the meta-seq2seq model with varying numbers of generated supports (k=1, 3, 5, 10) to determine the minimum effective support set size and computational tradeoffs
3. **Generalization test**: Apply the DemoGen approach to a different compositional generalization benchmark (e.g., SCAN or COGS) to assess whether the methodology transfers beyond gSCAN