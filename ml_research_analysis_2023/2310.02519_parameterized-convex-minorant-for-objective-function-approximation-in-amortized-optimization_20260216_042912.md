---
ver: rpa2
title: Parameterized Convex Minorant for Objective Function Approximation in Amortized
  Optimization
arxiv_id: '2310.02519'
source_url: https://arxiv.org/abs/2310.02519
tags:
- function
- objective
- convex
- optimization
- approximator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach to objective function approximation
  in amortized optimization, which aims to learn related functions in advance to quickly
  find the minimizer of parametric optimization problems in operation. The proposed
  method, called Parameterized Convex Minorant (PCM), constructs an objective function
  approximator as the sum of a PCM and a non-negative gap function, where the PCM
  is a parameterized convex function that lower bounds the objective function approximator.
---

# Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization

## Quick Facts
- **arXiv ID**: 2310.02519
- **Source URL**: https://arxiv.org/abs/2310.02519
- **Reference count**: 8
- **Key outcome**: PCM method enables reliable global minimizer retrieval through convex optimization even for non-convex objectives

## Executive Summary
This paper introduces the Parameterized Convex Minorant (PCM) method for approximating objective functions in amortized optimization. The approach constructs an objective function approximator as the sum of a parameterized convex minorant (PCM) and a non-negative gap function. The key innovation is that minimizing the PCM yields the global minimizer of the entire objective function approximator, enabling reliable and fast retrieval of optimal solutions through convex optimization. The method is theoretically proven to be a universal approximator for continuous functions and practically demonstrated through numerical simulations of non-parameterized-convex objective function approximation and learning-based nonlinear model predictive control.

## Method Summary
The PCM method approximates objective functions by constructing them as the sum of a parameterized convex minorant and a non-negative gap function. The parameterized convex minorant (PCM) is a parameterized convex function that lower bounds the objective function approximator, while the gap function captures the residual between the target function and the PCM. The paper proposes an extended parameterized log-sum-exp (EPLSE) network as a practical realization of the PCM method. Training is performed using mini-batch supervised learning with the Adam optimizer, and the global minimizer is retrieved by minimizing the PCM using convex optimization algorithms. The method is theoretically proven to be a universal approximator for continuous functions and practically demonstrated through numerical simulations of non-parameterized-convex objective function approximation and learning-based nonlinear model predictive control.

## Key Results
- The PCM method enables reliable global minimizer retrieval through a single convex optimization step, even when the objective function is non-convex
- The proposed objective function approximator is a universal approximator for continuous functions, enabling high expressiveness while maintaining global optimality guarantees
- The extended parameterized log-sum-exp (EPLSE) network provides a practical realization of the PCM method that guarantees unique minimizers and enables differentiable convex optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The parameterized convex minorant (PCM) method enables reliable global minimizer retrieval through a single convex optimization step, even when the objective function is non-convex.
- **Mechanism**: The objective function approximator is constructed as the sum of a PCM (a parameterized convex function that lower bounds the approximator) and a non-negative gap function. Since the PCM is convex in the optimization variable for any parameter, minimizing the PCM yields the global minimizer of the entire objective function approximator. The gap function ensures expressiveness while preserving convexity for optimization.
- **Core assumption**: The PCM is a shape-preserving universal approximator for parameterized convex continuous functions, and the gap function is a universal approximator for continuous functions.
- **Evidence anchors**:
  - [abstract]: "The proposed method can be used to learn objective functions and to find a global minimizer reliably and quickly by using convex optimization algorithms."
  - [section 3.1]: Theorem 5 proves that the minimizer of the PCM attains the global minimum of the objective function approximator.
  - [corpus]: Weak evidence; no directly relevant corpus neighbors identified.
- **Break condition**: If the PCM fails to provide a sufficiently tight lower bound, the gap function may dominate, potentially reintroducing non-convexity or reducing approximation accuracy.

### Mechanism 2
- **Claim**: The proposed objective function approximator is a universal approximator for continuous functions, enabling high expressiveness while maintaining global optimality guarantees.
- **Mechanism**: By constructing the approximator as PCM + gap function, where PCM approximates the parameterized greatest convex minorant and the gap function captures the residual, the method achieves universal approximation. The parameterized greatest convex minorant ensures convexity preservation while the gap function adds expressiveness.
- **Core assumption**: Both PCM and gap function components are universal approximators for their respective function classes (parameterized convex continuous and continuous functions).
- **Evidence anchors**:
  - [abstract]: "The proposed objective function approximator is a universal approximator for continuous functions..."
  - [section 3.2]: Theorem 10 proves the universal approximation property of the objective function approximator.
  - [corpus]: Weak evidence; no directly relevant corpus neighbors identified.
- **Break condition**: If the gap function becomes too large relative to the PCM, the approximation quality degrades, and the convexity advantage may be compromised.

### Mechanism 3
- **Claim**: The extended parameterized log-sum-exp (EPLSE) network provides a practical realization of the PCM method that guarantees unique minimizers and enables differentiable convex optimization.
- **Mechanism**: EPLSE modifies the parameterized LSE network by nullifying one set of parameters, making it likely strictly convex in statistical settings. This ensures unique minimizers, which is critical for differentiable convex optimization layers. The network maintains the universal approximation property while being amenable to standard optimization tools.
- **Core assumption**: The modified PLSE network (PLSE+) is strictly convex enough to guarantee unique minimizers for practical purposes.
- **Evidence anchors**:
  - [section 3.3]: "It is straightforward to show that PLSE+ network is parameterized convex continuous" and Theorem 11 proves its universal approximation property.
  - [abstract]: "As a realization of the proposed method, extended parameterized log-sum-exp (EPLSE) network is proposed..."
  - [corpus]: Weak evidence; no directly relevant corpus neighbors identified.
- **Break condition**: If the PLSE+ network is not sufficiently convex for certain parameter values, the minimizer may not be unique, preventing the use of differentiable convex optimization layers.

## Foundational Learning

- **Concept**: Convex analysis and parameterized convexity
  - **Why needed here**: The PCM method relies on convexity properties to ensure global optimality and enable efficient optimization. Understanding parameterized convexity is crucial for grasping how the method extends convexity to parametric optimization problems.
  - **Quick check question**: What is the difference between a convex function and a parameterized convex function?

- **Concept**: Universal approximation theorems
  - **Why needed here**: The method's effectiveness depends on the PCM and gap function being universal approximators. Understanding universal approximation is essential for appreciating how the method achieves high expressiveness while maintaining convexity.
  - **Quick check question**: What are the key requirements for a function approximator to be considered a universal approximator?

- **Concept**: Set-valued analysis and multivalued functions
  - **Why needed here**: The method involves set-valued minimizer functions and approximate selections, which are concepts from set-valued analysis. Understanding these concepts is important for grasping the theoretical foundations of the method.
  - **Quick check question**: What is the difference between a single-valued function and a multivalued function (correspondence)?

## Architecture Onboarding

- **Component map**: EPLSE network -> PCM + gap function -> objective function approximator -> global minimizer retrieval
- **Critical path**: 1) Construct the objective function approximator as PCM + gap function 2) Train the network parameters using gradient-based methods 3) Retrieve the global minimizer by minimizing the PCM using convex optimization
- **Design tradeoffs**:
  - Expressiveness vs. convexity: The gap function adds expressiveness but may compromise convexity if too large
  - Training time vs. optimization speed: Training requires convex optimization, which may be slower than non-convex training but enables faster optimization during deployment
  - Unique minimizer guarantee vs. approximation accuracy: Strict convexity ensures unique minimizers but may limit approximation accuracy for certain functions
- **Failure signatures**:
  - Poor approximation quality: Gap function dominates, reducing the effectiveness of the PCM
  - Non-unique minimizers: PLSE+ network is not sufficiently convex, preventing use of differentiable convex optimization layers
  - Slow training: Convex optimization during training is computationally expensive
- **First 3 experiments**:
  1. Implement the EPLSE network and verify its universal approximation property on a simple test function
  2. Compare the training time and optimization speed of EPLSE with other approximators (FNN, PLSE, DLSE) on a non-parameterized-convex objective function
  3. Apply the EPLSE network to a learning-based nonlinear model predictive control problem and evaluate its performance against linear MPC and other approximators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the training time of the proposed PCM method be reduced compared to other methods?
- Basis in paper: [explicit] The paper mentions this as a limitation and suggests it as future work.
- Why unresolved: The paper does not provide any specific solutions or techniques to address the long training time issue.
- What evidence would resolve it: Development and demonstration of new training methodologies that significantly reduce the training time of the PCM method.

### Open Question 2
- Question: How does the performance of the EPLSE network compare to other universal approximators in terms of approximation accuracy and computational efficiency for various types of objective functions?
- Basis in paper: [inferred] The paper demonstrates the performance of the EPLSE network for non-parameterized-convex objective functions and learning-based nonlinear MPC, but does not provide a comprehensive comparison with other universal approximators.
- Why unresolved: The paper does not provide a thorough comparison of the EPLSE network with other universal approximators for different types of objective functions.
- What evidence would resolve it: Extensive numerical simulations comparing the EPLSE network with other universal approximators for various types of objective functions in terms of approximation accuracy and computational efficiency.

### Open Question 3
- Question: How does the choice of the gap function affect the performance of the PCM method?
- Basis in paper: [inferred] The paper uses a specific form of the gap function, but does not explore the impact of different choices on the performance of the PCM method.
- Why unresolved: The paper does not investigate the effect of different gap functions on the performance of the PCM method.
- What evidence would resolve it: Numerical simulations comparing the performance of the PCM method with different choices of gap functions for various objective functions.

## Limitations
- Weak empirical evidence: The paper demonstrates results on only two numerical examples, which is insufficient to establish broad applicability
- Limited performance comparison: The comparison with competing methods lacks statistical significance testing and broader benchmarking
- Practical tightness uncertainty: The real-world performance depends heavily on the quality of the PCM and gap function approximations, which remains unclear

## Confidence
- **High confidence**: Theoretical foundations regarding convexity preservation and universal approximation properties are well-established through formal proofs
- **Medium confidence**: Practical effectiveness demonstrated through numerical simulations is promising but limited in scope and scale
- **Medium confidence**: Mechanism for reliable global minimizer retrieval through convex optimization is theoretically sound, but real-world performance depends on approximation quality

## Next Checks
1. Benchmark the PCM method against a broader range of competing approximators on diverse test functions with varying degrees of non-convexity
2. Conduct statistical significance testing across multiple random seeds and dataset splits to establish robustness of performance advantages
3. Apply the method to a real-world optimization problem from operations research or engineering design where parametric nature and non-convexity are well-established