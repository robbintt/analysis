---
ver: rpa2
title: 'Towards Reliable Domain Generalization: A New Dataset and Evaluations'
arxiv_id: '2309.06142'
source_url: https://arxiv.org/abs/2309.06142
tags:
- domain
- dataset
- methods
- generalization
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new domain generalization task for handwritten
  Chinese character recognition and constructs a large-scale dataset called PaHCC
  with 1000 categories and 996478 samples. It evaluates 18 domain generalization methods
  on PaHCC and shows that existing methods cannot handle this task well.
---

# Towards Reliable Domain Generalization: A New Dataset and Evaluations

## Quick Facts
- arXiv ID: 2309.06142
- Source URL: https://arxiv.org/abs/2309.06142
- Reference count: 40
- Key outcome: Evaluates 18 domain generalization methods on a new handwritten Chinese character dataset, showing existing methods perform poorly and standard evaluation protocols are unreliable

## Executive Summary
This paper addresses the challenge of domain generalization in handwritten Chinese character recognition by constructing a new large-scale dataset called PaHCC with 1000 categories and 996478 samples. The dataset is specifically designed to force models to learn structural features rather than pixel statistics by training on synthetic printed characters and testing on handwritten characters. The paper evaluates 18 domain generalization methods and reveals that standard evaluation protocols like leave-one-domain-out are unreliable, proposing two new dynamic criteria for more comprehensive evaluation. The results demonstrate that Chinese character recognition is a more challenging domain generalization task than object recognition, highlighting significant room for improvement in existing methods.

## Method Summary
The paper constructs the PaHCC dataset by collecting 280 printed fonts and 720 handwritten fonts across 1000 Chinese character categories, totaling 996478 samples. After preprocessing (resizing to 64x64 and binarization), the authors evaluate 18 domain generalization methods using the DomainBed codebase with ResNet-18 backbone. The evaluation uses both standard leave-one-domain-out protocol and newly proposed dynamic protocols that test performance across different training domain combinations and varying numbers of training domains. The study compares methods like ERM, Mixup, SagNet, CORAL, DANN, CDANN, MMD, IRM, VREx, IB-ERM, IB-IRM, SelfReg, RSC, GroupDRO, ANDMask, SANDMask, SD, and Fish.

## Key Results
- Existing domain generalization methods cannot handle the printed-to-handwritten Chinese character recognition task well, with no method achieving over 80% accuracy
- Standard leave-one-domain-out protocol is unreliable for evaluating DG methods, as performance varies significantly with training domain selection
- Chinese character recognition is shown to be more challenging than object recognition for domain generalization, with existing methods performing significantly worse on PaHCC compared to DomainNet
- Binarization of images reveals shortcut learning behavior, where models trained with ERM primarily use pixel statistics rather than structural information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset construction forces the model to learn structural features rather than pixel statistics.
- Mechanism: By training on synthetic printed characters (which have uniform foreground/background pixel values) and testing on handwritten characters (which have uneven foreground pixel values), the model must rely on structural stroke information to generalize.
- Core assumption: Models trained with ERM will default to learning pixel-level correlations when available, but will be forced to learn structure when those correlations fail.
- Evidence anchors:
  - [abstract] "After we binarize all the images, the generalization between different printed fonts does not have much effect, while the printed-to-handwritten task is greatly improved (from 16% to 60%), which verifies our above analysis."
  - [section] "It indicates that the deep model trained with ERM mainly uses statistics of pixels rather than structural information of characters to classify, which is a shortcut learning phenomenon."
  - [corpus] Weak evidence - the corpus mentions "domain generalization" but doesn't specifically address the pixel vs structure distinction.
- Break condition: If the handwritten data also exhibits uniform pixel statistics (e.g., through preprocessing), the structural learning pressure disappears.

### Mechanism 2
- Claim: The proposed "dynamic" evaluation protocol reveals method weaknesses that static leave-one-domain-out protocols miss.
- Mechanism: By testing how methods perform when adding/removing training domains and when selecting different training domain combinations, the evaluation captures how methods handle varying degrees and types of distribution shifts.
- Core assumption: Methods that perform well under leave-one-domain-out may fail when the number or composition of training domains changes.
- Evidence anchors:
  - [abstract] "we reveal more properties of DG methods and argue that only the leave-one-domain-out protocol is unreliable."
  - [section] "We show that the common leave-one-domain-out protocol is unreliable and propose two new criteria for more comprehensive and reliable evaluations."
  - [corpus] Weak evidence - the corpus mentions "domain generalization" but doesn't discuss evaluation protocol reliability.
- Break condition: If all methods perform consistently across all domain selection variations, the dynamic protocol adds no additional information.

### Mechanism 3
- Claim: The Chinese character domain provides a harder generalization challenge than object recognition datasets.
- Mechanism: Chinese characters have 1000+ categories with complex structural variations, while object recognition datasets typically have fewer categories and less structural complexity.
- Core assumption: The number of categories and structural complexity directly correlates with the difficulty of domain generalization.
- Evidence anchors:
  - [abstract] "Our experiments show that existing DG methods cannot handle this task well, and there is still much room for improvement."
  - [section] "Chinese character recognition differs from object recognition on distribution shifts and is a more challenging generalization scenario."
  - [corpus] Moderate evidence - the corpus includes "Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts" which supports the idea that different domains have different difficulty levels.
- Break condition: If Chinese character recognition is shown to have similar or easier generalization properties than object recognition under controlled conditions.

## Foundational Learning

- Concept: Domain generalization and distribution shifts
  - Why needed here: The entire paper is about evaluating methods for handling distribution shifts across domains.
  - Quick check question: What is the difference between domain adaptation and domain generalization?

- Concept: Dataset construction and bias
  - Why needed here: Understanding how the PaHCC dataset is constructed (printed to handwritten) is crucial for interpreting the results.
  - Quick check question: Why does the paper binarize images, and what does this reveal about model behavior?

- Concept: Evaluation protocols and their limitations
  - Why needed here: The paper argues that standard evaluation protocols are unreliable, so understanding these protocols is essential.
  - Quick check question: What is the leave-one-domain-out protocol, and why might it be insufficient?

## Architecture Onboarding

- Component map: Dataset construction (PaHCC) -> Method evaluation (18 DG methods) -> Performance analysis (standard protocol) -> Dynamic protocol evaluation -> Comparative analysis
- Critical path: Dataset construction → Method evaluation (ERM baseline, then 18 DG methods) → Performance analysis (standard protocol) → Dynamic protocol evaluation → Comparative analysis
- Design tradeoffs: The paper chose a printed-to-handwritten character recognition task for practical relevance, but this may limit generalizability to other domains. The dynamic evaluation adds complexity but provides more reliable insights.
- Failure signatures: Poor performance on handwritten data despite good performance on printed data indicates shortcut learning. Method performance degradation when adding domains suggests inability to handle distribution shifts. Performance sensitivity to training domain selection indicates instability.
- First 3 experiments:
  1. Run ERM on mini-PaHCC with grayscale images to establish baseline performance and observe the pixel shortcut problem.
  2. Evaluate a subset of DG methods (e.g., ERM, MMD, CORAL, SagNet) on mini-PaHCC with binary images to test if preprocessing helps.
  3. Run dynamic evaluation on mini-PaHCC by testing different training domain combinations to observe performance variability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain generalization methods be designed to effectively handle the unique distribution shifts in handwritten Chinese character recognition?
- Basis in paper: [explicit] The paper identifies that existing DG methods fail to solve the printed-to-handwritten Chinese character recognition task well and there is still much room for improvement.
- Why unresolved: The paper shows that methods like MMD, CORAL, SagNet, IB-ERM, ANDMask, RSC, and Mixup improve performance but none achieve over 80% accuracy, indicating the problem is not fully solved.
- What evidence would resolve it: A DG method achieving significantly higher accuracy (>90%) on the PaHCC dataset while maintaining performance on other DG benchmarks would demonstrate effective handling of Chinese character distribution shifts.

### Open Question 2
- Question: What are the fundamental reasons why the leave-one-domain-out protocol is unreliable for evaluating domain generalization methods?
- Basis in paper: [explicit] The paper demonstrates through experiments that the selection of training domains affects performance and that some methods deteriorate when increasing the number of training domains, making the leave-one-domain-out protocol unreliable.
- Why unresolved: The paper identifies the unreliability but doesn't provide a complete theoretical explanation of why this protocol fails to capture the true generalization capabilities of methods.
- What evidence would resolve it: A comprehensive theoretical framework explaining the limitations of leave-one-domain-out protocol and proposing alternative evaluation metrics that consistently predict real-world performance would resolve this question.

### Open Question 3
- Question: What makes Chinese character recognition more challenging for domain generalization than object recognition?
- Basis in paper: [explicit] The paper shows that DG methods perform significantly worse on the PaHCC dataset compared to the DomainNet dataset, and that Chinese character recognition differs from object recognition on distribution shifts.
- Why unresolved: While the paper demonstrates the difference, it doesn't fully explain the underlying characteristics of Chinese characters that make them more challenging for DG.
- What evidence would resolve it: A detailed analysis identifying specific properties of Chinese characters (e.g., stroke complexity, font variations, writer styles) that create more severe distribution shifts would explain the increased difficulty.

## Limitations

- The superiority of Chinese character recognition as a domain generalization testbed is asserted but not rigorously compared against controlled object recognition experiments.
- The assertion that dynamic evaluation protocols reveal method weaknesses is supported by experimental evidence but could be domain-specific.
- The claim that binarization reveals shortcut learning behavior depends on the assumption that handwritten data consistently exhibits non-uniform pixel statistics after binarization.

## Confidence

**Low confidence claims:**
- The superiority of Chinese character recognition as a domain generalization testbed is asserted but not rigorously compared against controlled object recognition experiments.

**Medium confidence claims:**
- The assertion that dynamic evaluation protocols reveal method weaknesses is supported by experimental evidence but could be domain-specific.
- The claim that binarization reveals shortcut learning behavior is mechanistically sound but depends on the assumption that handwritten data consistently exhibits non-uniform pixel statistics after binarization.

**High confidence claims:**
- The PaHCC dataset construction and its scale are verifiable facts from the paper.
- The observation that ERM methods fail to generalize from printed to handwritten characters is well-supported by the experimental results.
- The inadequacy of existing DG methods on this task is directly demonstrated through the comparative evaluation.

## Next Checks

1. **Cross-domain validation**: Test whether methods that perform poorly on PaHCC also fail on other domain generalization tasks (e.g., Office-Home or PACS datasets) to determine if the observed limitations are task-specific or method-specific.

2. **Controlled complexity comparison**: Create simplified character recognition tasks with controlled numbers of categories and structural complexity to empirically verify the relationship between these factors and generalization difficulty.

3. **Alternative preprocessing analysis**: Evaluate model behavior using different preprocessing pipelines (grayscale, edge detection, skeletonization) to confirm that the binarization findings are robust to preprocessing choices and not artifacts of a specific transformation.