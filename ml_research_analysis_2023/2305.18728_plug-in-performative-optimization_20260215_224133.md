---
ver: rpa2
title: Plug-in Performative Optimization
arxiv_id: '2305.18728'
source_url: https://arxiv.org/abs/2305.18728
tags:
- performative
- distribution
- plug-in
- risk
- misspecification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the benefits of using models for feedback in
  performative prediction, where predictions influence future observations. The authors
  propose a "plug-in performative optimization" protocol that uses parametric models
  of the distribution map, even if misspecified, to optimize the performative risk.
---

# Plug-in Performative Optimization

## Quick Facts
- arXiv ID: 2305.18728
- Source URL: https://arxiv.org/abs/2305.18728
- Reference count: 40
- One-line primary result: Misspecified models can outperform model-agnostic strategies in performative prediction by achieving O(1/√n) statistical rates while being robust to moderate model misspecification

## Executive Summary
This paper introduces plug-in performative optimization, a protocol that uses parametric models of distribution maps in performative prediction settings where predictions influence future observations. The key insight is that even when models are misspecified, they can provide faster statistical convergence rates (O(1/√n)) compared to exponentially slower model-agnostic strategies like bandit algorithms. The authors show that as long as misspecification is not extreme, the plug-in approach can achieve lower excess risk by leveraging the parametric structure while being robust to moderate model errors.

## Method Summary
The plug-in performative optimization protocol involves deploying initial models according to an exploration distribution, collecting (prediction, observation) pairs, and fitting a parametric model of the distribution map using empirical risk minimization. The estimated model is then used to compute a performative optimum through standard optimization. The method is compared against bandit algorithms and gradient-based approaches on strategic regression and location family problems, with theoretical analysis showing that excess risk decomposes into irreducible misspecification error and vanishing statistical error at O(1/√n) rate under smoothness and regularity conditions.

## Key Results
- Plug-in optimization with misspecified models can achieve lower excess risk than model-agnostic bandit methods when misspecification is moderate
- Statistical error vanishes at O(1/√n) rate for regular ERM problems under convexity and subexponential gradient conditions
- The excess risk is bounded by 2(MisspecErr + StatErrn), allowing for controlled performance even with model misspecification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Misspecified models can outperform model-agnostic strategies due to faster statistical convergence rates
- Mechanism: The plug-in approach uses parametric models to estimate the distribution map, achieving O(1/√n) statistical error compared to exponentially slower rates of bandit methods
- Core assumption: The misspecification error is bounded and the distribution atlas is smooth
- Evidence anchors:
  - [abstract] "We show that plug-in performative optimization can be far superior to model-agnostic strategies, as long as the misspecification is not too extreme"
  - [section 2] "Our results support the hypothesis that models—even if misspecified—can indeed help with learning in performative settings"
- Break condition: When misspecification error exceeds the benefit of faster statistical convergence

### Mechanism 2
- Claim: The error decomposition into misspecification and statistical terms allows for bounded excess risk
- Mechanism: The excess risk is bounded by 2(MisspecErr + StatErrn), where misspecification error is irreducible but statistical error vanishes at O(1/√n) rate
- Core assumption: The distribution atlas is η-misspecified and ϵ-smooth in the chosen distance metric
- Evidence anchors:
  - [section 3] "Theorem 2 illuminates the benefits of feedback models: if the model is a reasonable approximation, the misspecification error is not too large; at the same time, due to the parametric specification of the distribution atlas, the statistical error vanishes quickly"
  - [section 3.1] "Corollary 1 shows that plug-in performative optimization is efficient as long as the distribution atlas is smooth, not too misspecified, and the rate of estimation Cn is fast"
- Break condition: When smoothness assumption fails or estimation rate is slower than O(1/√n)

### Mechanism 3
- Claim: Regular ERM problems allow for fast estimation of β* with rate O(log n/√(n(dβ + log(1/δ))))
- Mechanism: Under regularity conditions (convexity, subexponential gradients), the empirical risk minimizer achieves fast convergence to the population limit
- Core assumption: Assumption 1 conditions are satisfied (convexity, subexponential gradients)
- Evidence anchors:
  - [section 3.3] "We show that for a broad class of problems the rate is O(log n/√(n(dβ + log(1/δ))))"
  - [section 3.3] "Lemma 1 shows that regular ERM problems allow for fast estimation of β*"
- Break condition: When regularity conditions (Assumption 1) are violated

## Foundational Learning

- Concept: Performative prediction and distribution maps
  - Why needed here: The entire framework relies on understanding how predictions influence future observations through the distribution map D(θ)
  - Quick check question: What is the difference between the performative risk and the empirical risk in this context?

- Concept: Parametric vs model-agnostic optimization strategies
  - Why needed here: The paper contrasts the fast O(1/√n) rates of parametric methods against exponentially slower bandit methods
  - Quick check question: Under what conditions would a model-agnostic strategy outperform a parametric approach despite slower convergence?

- Concept: Error decomposition in statistical learning
  - Why needed here: The excess risk is decomposed into misspecification error (irreducible) and statistical error (vanishes with n)
  - Quick check question: How does the smoothness parameter ϵ affect the statistical error term in the excess risk bound?

## Architecture Onboarding

- Component map:
  - Distribution atlas DB = {Dβ}β∈B -> Exploration strategy ˜D -> Map-fitting algorithm dMap -> Loss function ℓ(z; θ) -> Plug-in performative optimum computation

- Critical path:
  1. Deploy θi ~ ˜D and collect (θi, zi) pairs
  2. Fit distribution map to obtain ˆβ using dMap
  3. Compute ˆθPO = arg min PRˆβ(θ)
  4. Evaluate excess risk PR(ˆθPO) - PR(θPO)

- Design tradeoffs:
  - Parametric model flexibility vs misspecification risk
  - Exploration distribution choice affecting estimation quality
  - Map-fitting algorithm complexity vs estimation accuracy
  - Choice of distance metric (TV vs Wasserstein) for smoothness analysis

- Failure signatures:
  - Excess risk plateaus at non-zero value indicating dominant misspecification error
  - Slow convergence suggesting violated smoothness assumptions
  - High variance in estimates pointing to insufficient exploration

- First 3 experiments:
  1. Verify error decomposition by measuring misspecification and statistical components separately on synthetic data
  2. Test sensitivity to smoothness parameter by varying ϵ in controlled simulations
  3. Compare convergence rates against bandit baselines across different misspecification levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the plug-in performative optimization protocol perform in settings where the feedback model is significantly misspecified, beyond the cases studied in the paper?
- Basis in paper: [explicit] The paper mentions that the protocol can be efficient as long as the distribution atlas is smooth, not too misspecified, and the rate of estimation is fast. It also provides examples of misspecification, such as the "typically well-specified" model and omitted-variable bias.
- Why unresolved: The paper provides theoretical bounds on the excess risk under certain misspecification assumptions, but it does not explore the performance of the protocol in settings with more extreme misspecification.
- What evidence would resolve it: Experimental results or theoretical analysis showing the performance of the protocol under various degrees of misspecification.

### Open Question 2
- Question: Can the plug-in performative optimization protocol be extended to handle time-varying distribution shifts, multi-agent settings, and causality and robustness, as mentioned in the related work section?
- Basis in paper: [explicit] The paper mentions these settings as potential extensions of the theory on the use of models.
- Why unresolved: The paper focuses on the general protocol and its applications in strategic regression, binary strategic classification, and location families, but does not explore these specific settings.
- What evidence would resolve it: Theoretical analysis or experimental results demonstrating the performance of the protocol in these settings.

### Open Question 3
- Question: How does the choice of the exploration distribution affect the performance of the plug-in performative optimization protocol?
- Basis in paper: [inferred] The protocol involves deploying θi according to some exploration distribution ˜D, but the paper does not discuss the impact of this choice on the performance.
- Why unresolved: The paper assumes a fixed exploration distribution but does not analyze how different choices of ˜D might affect the excess risk or convergence rate.
- What evidence would resolve it: Theoretical analysis or experimental results comparing the performance of the protocol under different exploration distributions.

## Limitations
- The theoretical analysis assumes the true distribution belongs to the parametric family or is close to it, with vague conditions for "not too extreme" misspecification
- The smoothness assumption in the distribution atlas may not hold in many real-world scenarios
- The paper does not explore the impact of exploration distribution choice on protocol performance

## Confidence
- **High Confidence**: The O(1/√n) statistical convergence rate for parametric models versus exponentially slower rates for bandit methods
- **Medium Confidence**: The error decomposition framework and its implications for excess risk bounds
- **Medium Confidence**: The regularity conditions enabling fast ERM estimation

## Next Checks
1. **Empirical validation of error decomposition**: Conduct experiments that explicitly measure and compare the misspecification error and statistical error components across different levels of model misspecification to verify the theoretical bounds.
2. **Robustness to smoothness violations**: Test the plug-in optimization method on problems where the smoothness assumption is intentionally violated to understand the impact on convergence rates and excess risk.
3. **Scalability evaluation**: Assess the performance of the plug-in optimization approach on high-dimensional problems and problems with non-convex loss functions to determine the practical limits of the method.