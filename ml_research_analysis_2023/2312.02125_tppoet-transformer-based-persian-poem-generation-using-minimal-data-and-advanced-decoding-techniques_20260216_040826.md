---
ver: rpa2
title: 'TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced
  Decoding Techniques'
arxiv_id: '2312.02125'
source_url: https://arxiv.org/abs/2312.02125
tags:
- arxiv
- generation
- poetry
- decoding
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TPPoet, a transformer-based model designed
  to generate classical Persian poetry using minimal data. The primary challenge addressed
  is the difficulty of training language models on small datasets while maintaining
  creative quality in poetry generation.
---

# TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques

## Quick Facts
- arXiv ID: 2312.02125
- Source URL: https://arxiv.org/abs/2312.02125
- Reference count: 0
- Primary result: TPPoet generates Persian poetry using only 33M parameters and outperforms GPT2-Persian (328M parameters) in fluency, coherence, meaningfulness, and poetic quality while being recognized as human-generated 78% of the time

## Executive Summary
TPPoet introduces a transformer-based model for generating classical Persian poetry using minimal data. The model addresses the challenge of training language models on small datasets while maintaining creative quality by employing a decoder-only transformer architecture trained from scratch on a specialized dataset of 673,743 couplets. The key innovation is a novel decoding method combining dynamic temperature inspired by simulated annealing with an Anti-LM model to enhance coherence and meaningfulness while managing the trade-off between diversity and quality.

## Method Summary
TPPoet uses a decoder-only transformer trained on 673,743 couplets (approximately 10 million tokens) without pretraining. The model employs Byte-Pair Encoding (BPE) with an 8k vocabulary for tokenization. Training uses a 95% training and 5% validation split, with 12 epochs, batch size 32, label smoothing, and Adam optimizer. The decoding combines Top-K sampling (k=20), Nucleus sampling (threshold=0.9), an Anti-LM model that penalizes repetitive 2-grams, and a simulated annealing-inspired temperature schedule that starts at 0.9 and decreases toward 0.5 during generation.

## Key Results
- TPPoet achieves superior performance compared to GPT2-Persian with significantly fewer parameters (33M vs. 328M)
- TPPoet is recognized as human-generated 78% of the time compared to 30% for GPT2-Persian
- The model demonstrates high quality, diversity, and coherence in generated Persian poetry according to BLEU and Self-BLEU scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic temperature scheduling inspired by simulated annealing balances exploration and exploitation during decoding, improving both quality and diversity
- Mechanism: Temperature starts high (T₀ = 0.9) to encourage diverse sampling, then decreases step-wise toward a lower bound (T_f = 0.5) as more tokens are generated
- Core assumption: Coherence improves with lower temperature; diversity improves with higher temperature; balancing them improves overall poem quality
- Evidence anchors: [abstract] "propose a novel decoding method including dynamic temperature inspired by the principles of the simulated annealing optimization algorithm"; [section 3.4] "we look at the trade-off between quality and diversity as exploitation and exploration trade-off in optimization problems and propose a novel methodology for choosing temperature inspired by the principles of the simulated annealing optimization algorithm"
- Break condition: If the model has not yet learned coherent patterns, annealing may converge to incoherent outputs; if temperature drops too quickly, diversity loss may outweigh coherence gains

### Mechanism 2
- Claim: Anti-LM model reduces repetitive generation patterns without sacrificing coherence
- Mechanism: After nucleus + top-k sampling, an auxiliary penalty model identifies and downweights high-frequency 2-grams in the candidate set, discouraging repetition
- Core assumption: Repetition is a key source of low diversity in transformer-generated text; simple n-gram penalties are sufficient to mitigate it
- Evidence anchors: [section 3.4] "we apply an Anti-LM model [37] to further improve the diversity and quality of generated couplets by penalizing repetitive 2-grams"; [abstract] "a simple Anti-LM model" included in decoding method
- Break condition: If the penalty threshold is too aggressive, the model may skip valid continuations; if too lenient, repetition remains unchecked

### Mechanism 3
- Claim: Training a decoder-only transformer from scratch on a specialized small dataset outperforms fine-tuning a large general model
- Mechanism: Model is trained only on 673,743 couplets (~10M tokens) without pretraining; decoder-only design reduces parameters (33M vs. 328M) while focusing learning capacity on poetry-specific patterns
- Core assumption: Domain alignment matters more than scale; specialized data with a compact model can outperform a general model fine-tuned on the same data
- Evidence anchors: [abstract] "training a Persian classical poetry generation model using a transformer architecture on a specialized dataset with no pretraining"; [section 3.1] "decoder-only transformer model is going to be trained to unconditionally generate rhyming couplets in the style of classical Persian poets"; [section 4.4] "TPPoet was able to generate fluent, meaningful, and human-like poems, outperforming a model ~900% bigger and trained on a huge corpus of general Persian data"
- Break condition: If the dataset is too small or unrepresentative, the model may overfit; if the poetry domain is too broad, lack of pretraining may hinder generalization

## Foundational Learning

- Concept: Byte-Pair Encoding (BPE) tokenization
  - Why needed here: Reduces vocabulary size while preserving morphological richness; enables efficient handling of Persian's agglutinative morphology
  - Quick check question: Why did the authors choose a vocabulary size of 8k instead of 10k or 16k?
- Concept: Transformer decoder-only architecture
  - Why needed here: Allows autoregressive generation without bidirectional context; simpler than encoder-decoder for unconditional poem generation
  - Quick check question: What is the role of the masked self-attention in the decoder?
- Concept: Nucleus (top-p) and Top-K sampling
  - Why needed here: Controls randomness while maintaining quality; prevents degenerate sampling in small datasets
  - Quick check question: How do top-k and top-p differ in controlling the candidate pool?

## Architecture Onboarding

- Component map: Input text → BPE tokenization (8k vocab) → Embedding layer (dim=512) → Decoder stack (8 layers, 8 heads) → Position-wise feed-forward (dim=2048) → Output projection → Logits → Nucleus + Top-K + Anti-LM + Simulated Annealing temperature scheduler → Generated text
- Critical path: Input → Tokenization → Embedding → Decoder → Output projection → Decoding loop (sampling + temperature annealing + anti-LM penalty) → Text generation
- Design tradeoffs:
  - Small model size (33M) vs. expressiveness: Chosen for efficiency on small data
  - No pretraining vs. fast convergence: Risk of underfitting mitigated by specialized dataset
  - Dynamic temperature vs. fixed temperature: Balances diversity and coherence at generation time
- Failure signatures:
  - High BLEU but low self-BLEU → Repetitive but locally fluent outputs
  - Low BLEU and low self-BLEU → Low quality and low diversity (underfit)
  - High self-BLEU but BLEU varies → Lack of diversity even with good local fluency
- First 3 experiments:
  1. Train baseline decoder-only transformer (no dynamic temperature, no Anti-LM) on 10% of data; evaluate BLEU/self-BLEU to establish floor
  2. Add dynamic temperature annealing; compare BLEU/self-BLEU trade-off curve to baseline
  3. Add Anti-LM penalty; evaluate impact on self-BLEU and human ratings for repetition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TPPoet's performance compare to other transformer-based models when trained on larger datasets of Persian poetry?
- Basis in paper: [explicit] The authors mention that TPPoet is trained on a small dataset of 673,743 couplets (~10 million tokens) and outperforms GPT2-Persian, which is trained on a much larger corpus. The paper suggests that future work could involve training TPPoet on entire poems rather than just couplets.
- Why unresolved: The paper does not explore the performance of TPPoet when trained on larger datasets. It is unclear whether the model's effectiveness is primarily due to its architecture and decoding techniques or if it could benefit from more training data.
- What evidence would resolve it: Conducting experiments to train TPPoet on larger datasets of Persian poetry and comparing its performance (in terms of fluency, coherence, meaningfulness, and poetic qualities) with its performance on the smaller dataset used in the study.

### Open Question 2
- Question: Can TPPoet be adapted to generate poetry in other languages or styles while maintaining its performance?
- Basis in paper: [inferred] The paper focuses on generating classical Persian poetry, but the transformer architecture and decoding techniques used by TPPoet could potentially be applied to other languages or poetic styles. The authors mention that TPPoet's performance is attributed to its architecture and decoding methods, suggesting that these techniques might be transferable.
- Why unresolved: The paper does not explore the adaptability of TPPoet to other languages or poetic styles. It is unclear whether the model's effectiveness is specific to Persian poetry or if it can be generalized to other forms of poetry.
- What evidence would resolve it: Conducting experiments to adapt TPPoet's architecture and decoding techniques to generate poetry in other languages or styles, and evaluating its performance in terms of fluency, coherence, meaningfulness, and poetic qualities.

### Open Question 3
- Question: How does TPPoet's decoding method compare to other advanced decoding techniques, such as those based on reinforcement learning or adversarial training?
- Basis in paper: [explicit] The authors propose a novel decoding method for TPPoet that incorporates dynamic temperature inspired by simulated annealing and an Anti-LM model. They compare TPPoet's performance to GPT2-Persian, which uses a different decoding approach.
- Why unresolved: The paper does not compare TPPoet's decoding method to other advanced techniques, such as those based on reinforcement learning or adversarial training. It is unclear whether TPPoet's decoding method is superior to these other approaches or if there are potential improvements that could be made.
- What evidence would resolve it: Conducting experiments to compare TPPoet's decoding method with other advanced techniques, such as those based on reinforcement learning or adversarial training, and evaluating their performance in terms of diversity, quality, and coherence of generated poetry.

## Limitations
- Evaluation scope is limited to a single baseline (GPT2-Persian) without comparison to other strong models or pretraining approaches
- The specific contribution of each decoding technique (dynamic temperature, Anti-LM, top-k/top-p) is not isolated through ablation studies
- Human evaluation protocol lacks transparency regarding rater training, inter-rater reliability, and evaluator numbers

## Confidence

- **High Confidence**: The architectural design (decoder-only transformer) and training methodology (specialized dataset, BPE tokenization, standard hyperparameters) are clearly specified and reproducible. The BLEU and Self-BLEU results are automatically computable metrics.
- **Medium Confidence**: The comparative performance against GPT2-Persian is documented but based on limited baselines. The qualitative advantages (fluency, coherence, meaningfulness) rely on human evaluation without full methodological transparency.
- **Low Confidence**: The specific contribution of each decoding technique cannot be isolated. The mechanism by which dynamic temperature improves poetry quality is theoretically sound but empirically under-supported.

## Next Checks

1. **Ablation Study**: Train and evaluate TPPoet variants with individual decoding components removed (no dynamic temperature, no Anti-LM, no top-k/top-p combination) to quantify each technique's contribution to the final performance.

2. **Expanded Baseline Comparison**: Compare TPPoet against additional strong baselines including larger pretrained models fine-tuned on the same dataset, and other specialized poetry models for different languages to establish relative performance.

3. **Human Evaluation Replication**: Conduct a new human evaluation with documented rater training, inter-rater reliability metrics (e.g., Krippendorff's alpha), and larger evaluator pools to verify the 78% human recognition rate and other qualitative assessments.