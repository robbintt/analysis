---
ver: rpa2
title: 'Lights out: training RL agents robust to temporary blindness'
arxiv_id: '2312.02665'
source_url: https://arxiv.org/abs/2312.02665
tags:
- open-loop
- agent
- state
- controller
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of training reinforcement learning
  agents to be robust to temporary blindness, where the agent cannot observe the state
  for some timesteps. They propose a novel architecture combining a neural network
  with hidden state representations and an n-step loss function.
---

# Lights out: training RL agents robust to temporary blindness

## Quick Facts
- arXiv ID: 2312.02665
- Source URL: https://arxiv.org/abs/2312.02665
- Authors: 
- Reference count: 11
- Key outcome: RL agents trained with LSTM and n-step loss can navigate mazes even when blinded for periods longer than seen during training

## Executive Summary
This paper addresses the challenge of training reinforcement learning agents to handle temporary blindness - periods where observations are unavailable. The authors propose a novel architecture combining a recurrent neural network with hidden state representations and an n-step loss function. The key insight is using an LSTM to predict future states and rewards based on past actions, allowing the agent to continue acting during blindness. Experiments on maze navigation tasks demonstrate that agents can handle blindness lengths much longer than seen during training, achieving near-optimal performance even with significant blindness periods.

## Method Summary
The proposed method builds upon Deep Q-Networks by incorporating a recurrent neural network architecture. An encoder function converts observations to initial hidden states, which are then propagated through an LSTM using sequences of past actions. The network predicts both rewards and values from the hidden states. During training, an n-step loss function compares predictions made during blindness with actual values observed when observations return. The training process mixes closed-loop (observation-based) and open-loop (blind) trajectories to prevent distribution shift. The method uses a probability parameter p to determine how often to sample open-loop trajectories during training.

## Key Results
- Agents trained with the proposed method achieve near-optimal performance even with blindness periods much longer than seen during training
- The method enables agents to switch between open-loop (blind) and closed-loop control seamlessly
- Performance is robust across different maze shapes and sizes, demonstrating the method's effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LSTM-based recurrent architecture allows the agent to maintain a hidden state representation that enables open-loop control during blindness.
- Mechanism: When the agent encounters a blindness mask, instead of relying on observations, it uses the hidden state from the previous timestep as input to the LSTM. The LSTM then predicts future hidden states based on past actions, allowing the agent to continue taking actions without observations.
- Core assumption: The hidden state representation captures sufficient information about the environment state to make optimal decisions during blindness.
- Evidence anchors:
  - [abstract] "The key idea is to use a recurrent neural network to predict future states and rewards based on past actions, allowing the agent to continue acting even when blind."
  - [section] "Using an LSTM allows us to give an action sequence input of variable length, therefore allowing us to use the same neural network for both the closed- and open-loop controller, based on Q-values"
- Break condition: If the hidden state loses critical information over time or the environment dynamics change significantly, the LSTM predictions will become inaccurate and the agent will fail to navigate correctly during blindness.

### Mechanism 2
- Claim: The n-step loss function trains the agent to predict future values and rewards even without observations.
- Mechanism: The loss function compares the predicted value after n open-loop steps (using the LSTM to propagate the hidden state) with the actual value that would be achieved if observations were available. This forces the network to learn accurate predictions during blindness.
- Core assumption: The n-step predictions can be trained effectively using trajectories that mix closed-loop and open-loop segments.
- Evidence anchors:
  - [abstract] "a novel n-step loss function"
  - [section] "The goal of this loss function is that value estimations after n open-loop steps need to be as close as possible to closed-loop value estimations."
- Break condition: If n is too large relative to the training data, the predictions will diverge and the loss will not converge. Also, if the environment has high stochasticity, long-term predictions become unreliable.

### Mechanism 3
- Claim: Sampling open-loop trajectories during training prevents distribution shift between training and evaluation.
- Mechanism: By randomly setting the agent to blindness mode during training (with probability p/n), the replay buffer contains both closed-loop and open-loop trajectories. This ensures the agent learns policies that work in both modes.
- Core assumption: The distribution of states and actions during open-loop control is similar enough to closed-loop to allow effective learning.
- Evidence anchors:
  - [section] "Because of the potential distribution shift when training the open-loop controller on closed-loop trajectories, we can train both policies on trajectories drawn from both policies."
  - [section] "we can conclude that there is no distribution shift in this dataset, i.e. the trajectories accessed by both policies during training are similar"
- Break condition: If the environment has state-dependent blindness (e.g., certain states always trigger blindness), the open-loop and closed-loop distributions will differ significantly and the agent will struggle.

## Foundational Learning

- Concept: Deep Q-Networks (DQN)
  - Why needed here: The paper builds upon DQN as the base algorithm, modifying it with recurrent architecture and n-step loss.
  - Quick check question: What is the Bellman optimality equation that DQN tries to approximate?

- Concept: Recurrent Neural Networks (RNNs) and LSTMs
  - Why needed here: The LSTM maintains hidden state across timesteps, enabling the agent to function without observations.
  - Quick check question: How does an LSTM differ from a standard feedforward neural network in handling sequential data?

- Concept: Distribution Shift in Offline RL
  - Why needed here: The paper addresses potential distribution shift between open-loop and closed-loop trajectories.
  - Quick check question: What is distribution shift and why is it problematic in offline reinforcement learning?

## Architecture Onboarding

- Component map: Observation → Encoder function gθ → LSTM function fθ → Reward function rθ & Value function vθ → Action selection
- Critical path: The agent receives observations, encodes them to hidden states, propagates through LSTM with action inputs, predicts rewards/values, and selects actions. During blindness, it skips observations and uses LSTM predictions directly.
- Design tradeoffs:
  - Larger n provides better blindness robustness but requires more training data and may suffer from prediction error accumulation
  - Higher p samples more open-loop trajectories but may slow down learning of the primary closed-loop policy
  - Deeper LSTM architectures can capture more complex temporal dependencies but are harder to train
- Failure signatures:
  - Agent gets stuck repeating the same action during blindness (LSTM predictions become degenerate)
  - Performance degrades significantly as blindness length exceeds training length
  - Agent fails to switch back to closed-loop control after blindness ends
- First 3 experiments:
  1. Train with n=1 (pure closed-loop) and evaluate on blindness masks to establish baseline performance
  2. Train with small n (2-4) and test maximum blindness length the agent can handle
  3. Vary p parameter while keeping n constant to measure impact of open-loop trajectory sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale to larger state spaces and more complex environments beyond gridworld mazes?
- Basis in paper: [inferred] The authors note that their experiments were conducted in a small gridworld environment and suggest that further experiments in larger mazes are needed to explore the method's scalability.
- Why unresolved: The paper only evaluates the method on simple gridworld mazes with a small state space. No experiments or analysis are provided on how the method would perform in more complex, high-dimensional environments.
- What evidence would resolve it: Experiments applying the method to standard benchmark environments like Atari games or continuous control tasks, and analyzing performance and scalability as state space complexity increases.

### Open Question 2
- Question: How sensitive is the method to the choice of hyperparameters like the embedding size, LSTM hidden size, and n-step loss horizon?
- Basis in paper: [explicit] The authors note that they only tested a limited set of hyperparameter values and observed high variance in results across seeds. They suggest further experiments are needed to understand hyperparameter sensitivity.
- Why unresolved: The paper only evaluates a small subset of possible hyperparameter values and does not provide a systematic sensitivity analysis. The high variance observed suggests hyperparameter choices may significantly impact performance.
- What evidence would resolve it: A comprehensive hyperparameter sweep across a wide range of values for key hyperparameters, along with statistical analysis of performance variance to identify robust settings.

### Open Question 3
- Question: How does the proposed method compare to other approaches for handling observation robustness in reinforcement learning?
- Basis in paper: [inferred] The authors frame their work as addressing the observation robustness problem, but do not directly compare to other methods proposed for this problem like ensemble methods or uncertainty-based approaches.
- Why unresolved: The paper focuses solely on the proposed method and does not benchmark it against other state-of-the-art approaches for handling observation robustness. It's unclear how it stacks up in terms of performance and sample efficiency.
- What evidence would resolve it: Experiments comparing the proposed method head-to-head with other observation robustness approaches on the same benchmarks, measuring performance, sample efficiency, and robustness to different types of observation perturbations.

## Limitations
- Experimental validation limited to simple gridworld mazes with deterministic dynamics
- Performance on continuous control tasks and stochastic environments untested
- Claims about scalability to larger state spaces not empirically validated

## Confidence
- High confidence: The core architectural contribution (LSTM + n-step loss) and its theoretical motivation
- Medium confidence: Experimental results on gridworld mazes showing blindness robustness
- Low confidence: Claims about generalization to more complex environments and scalability

## Next Checks
1. Test the method on continuous control benchmarks (MuJoCo, PyBullet) with temporary sensor occlusion to evaluate scalability
2. Evaluate performance when blindness masks are state-dependent (e.g., blindness triggered by entering certain regions) to test distribution shift handling
3. Measure LSTM prediction accuracy during blindness versus actual observed states to quantify information retention over time