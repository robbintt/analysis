---
ver: rpa2
title: 'Don''t Waste a Single Annotation: Improving Single-Label Classifiers Through
  Soft Labels'
arxiv_id: '2311.05265'
source_url: https://arxiv.org/abs/2311.05265
tags:
- label
- soft
- confidence
- labels
- annotator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of traditional hard-label
  annotation methods for single-label classification tasks, which discard valuable
  information from annotator disagreement and confidence. The authors propose using
  soft labels generated from annotator confidence scores and secondary labels to improve
  classifier performance.
---

# Don't Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels

## Quick Facts
- arXiv ID: 2311.05265
- Source URL: https://arxiv.org/abs/2311.05265
- Reference count: 15
- Primary result: Soft labels generated from annotator confidence and secondary labels outperform hard labels in single-label classification

## Executive Summary
This paper addresses the limitations of traditional hard-label annotation methods for single-label classification tasks, which discard valuable information from annotator disagreement and confidence. The authors propose using soft labels generated from annotator confidence scores and secondary labels to improve classifier performance. They introduce a Bayesian method to calibrate confidence scores across annotators based on agreement, and demonstrate that combining confidence and secondary label information leads to more effective soft labels. Experiments on two datasets show that soft labels significantly outperform hard labels, with the Bayesian calibrated soft labels achieving the best results.

## Method Summary
The method involves generating soft labels from annotations that include annotator confidence scores and optional secondary labels. A Bayesian calibration approach is used to align confidence levels across annotators based on their agreement patterns, creating a reliability weight for each annotator. These calibrated confidences are then converted to soft labels where primary labels receive probability based on calibrated confidence and secondary labels receive the remaining probability mass. The soft labels are used to fine-tune a BERT model using cross-entropy loss, with performance evaluated using macro-F1 and Expected Calibration Error.

## Key Results
- Soft labels significantly outperform hard labels across both datasets tested (VaxxHesitancy and CDS)
- Bayesian calibrated soft labels achieve the best performance, particularly on datasets with good annotator overlap
- Including secondary label information provides modest but consistent improvements in classification performance
- The method successfully addresses the information loss inherent in traditional hard-label aggregation

## Why This Works (Mechanism)

### Mechanism 1
Bayesian calibration aligns annotator confidence levels by leveraging agreement patterns to reweight confidences. The method computes P(y=lim|cim) using prior and likelihood, then adjusts it using P(am|lim) derived from annotator disagreement counts. This reweights each annotator's confidence based on their historical agreement with other annotators. The core assumption is that annotator agreement is a reliable proxy for annotator reliability.

### Mechanism 2
Soft labels preserve information from annotator uncertainty that would be lost in hard label aggregation. Instead of reducing all annotations to a single hard label, soft labels encode the probability distribution of annotator choices. Primary labels receive probability based on calibrated confidence, secondary labels receive the remaining probability mass. The core assumption is that annotator uncertainty contains useful signal for model training.

### Mechanism 3
Combining primary and secondary labels creates more informative soft labels than using primary labels alone. Secondary labels represent annotator uncertainty about their primary choice. By incorporating both into the soft label (primary gets calibrated confidence probability, secondary gets remaining probability), the soft label better represents the annotator's full belief state. The core assumption is that secondary labels are meaningful indicators of alternative plausible classifications.

## Foundational Learning

- **Annotator agreement as reliability proxy**
  - Why needed here: The Bayesian calibration method relies on using agreement patterns to estimate annotator reliability, which is fundamental to the confidence reweighting approach.
  - Quick check question: If two annotators always agree but are both systematically wrong, will this method correctly identify their reliability?

- **Soft label generation from multiple sources**
  - Why needed here: The paper combines confidence scores, primary labels, and secondary labels to create soft labels. Understanding how these components interact is crucial for implementation.
  - Quick check question: How does the method distribute probability mass when an annotator provides only a primary label versus when they provide both primary and secondary labels?

- **Cross-validation with uneven data splits**
  - Why needed here: The experimental setup uses 5-fold cross-validation where the train-test split contains uneven agreement distributions, which is necessary to have a gold-standard test set while still training on mixed agreement data.
  - Quick check question: Why can't we simply train on high-agreement samples and test on low-agreement samples if our goal is to evaluate soft label benefits?

## Architecture Onboarding

- **Component map**: Raw annotations → Confidence calibration → Soft label conversion → Model training → Evaluation
- **Critical path**: Raw annotations → Confidence calibration → Soft label conversion → Model training → Evaluation
- **Design tradeoffs**: The Bayesian method trades computational complexity for potentially better calibration, but requires sufficient annotator overlap to function properly.
- **Failure signatures**: Poor performance improvement likely indicates insufficient annotator overlap for reliable Bayesian calibration; high variance across folds may indicate dataset imbalance or presence of "hard-to-classify" samples.
- **First 3 experiments**:
  1. Implement basic soft label generation without Bayesian calibration to establish baseline performance
  2. Add Bayesian calibration and compare performance changes, particularly focusing on datasets with good annotator overlap
  3. Test the impact of including secondary labels by comparing performance with and without secondary label information

## Open Questions the Paper Calls Out

### Open Question 1
How can the Bayesian confidence calibration method be improved to handle datasets with a larger number of classes and fewer annotator overlaps? The paper mentions that the Bayesian method performed poorly on the CDS dataset due to the larger number of classes and fewer annotator overlaps.

### Open Question 2
How does the collection of secondary labels from the same annotators who provided the primary labels affect the quality and performance of soft labels compared to secondary labels collected separately? The paper uses secondary labels collected separately from the original annotators.

### Open Question 3
What are the effects of different confidence score conversion scales on the performance of soft labels, and how can an optimal conversion scale be determined? The paper manually tested different confidence conversion scales and found that the chosen scale yielded the best classification performance.

### Open Question 4
How does the performance of soft labels compare to traditional soft labels generated by aggregating many annotator labels per sample? The paper mentions that its soft label method is not compared against traditional soft labels due to the impracticality of large-scale annotation.

### Open Question 5
How can the high variance in cross-validation folds be addressed, particularly in the presence of hard-to-classify samples? The paper observed high variance across folds during cross-validation, attributed to the small size of the test set and the presence of hard-to-classify samples.

## Limitations
- The Bayesian calibration method requires sufficient annotator overlap to function effectively, limiting its applicability to datasets with many classes or few annotators
- The effectiveness of secondary labels assumes they represent meaningful uncertainty rather than random noise, which may not always hold true
- The conversion of confidence scores from various scales to a unified 0-1 scale is a potential source of uncertainty that could affect calibration quality

## Confidence

| Claim | Confidence |
|-------|------------|
| Soft labels preserve information from annotator uncertainty | High |
| Bayesian calibration mechanism's effectiveness | Medium |
| Combination of primary and secondary labels creating more informative soft labels | Medium |

## Next Checks
1. **Annotator Overlap Sensitivity Analysis**: Systematically vary the amount of annotator overlap in the dataset and measure how the Bayesian calibration performance degrades. This would quantify the minimum overlap required for the method to be effective.

2. **Secondary Label Quality Validation**: Design an experiment where secondary labels are intentionally corrupted (e.g., randomly assigned or intentionally wrong) and measure the impact on soft label quality and model performance. This would validate whether secondary labels truly add meaningful information.

3. **Cross-Dataset Generalization Test**: Apply the method to a third dataset with significantly different characteristics (e.g., more classes, different annotation scales, or different domain) to test the robustness of the approach. This would help determine whether the method's success is dataset-specific or generalizable.