---
ver: rpa2
title: 'Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT'
arxiv_id: '2310.07582'
source_url: https://arxiv.org/abs/2310.07582
tags:
- layer
- heads
- attention
- linear
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether transformer models truly understand
  the world or merely memorize patterns by examining Othello-GPT, a simple transformer
  trained on the game Othello. The authors find that Othello-GPT linearly encodes
  a representation of opposing pieces ("yours" vs.
---

# Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT

## Quick Facts
- arXiv ID: 2310.07582
- Source URL: https://arxiv.org/abs/2310.07582
- Reference count: 1
- This study finds that Othello-GPT linearly encodes a representation of opposing pieces ("yours" vs. "mine") in its activations, which causally influences its decision-making.

## Executive Summary
This study investigates whether transformer models truly understand the world or merely memorize patterns by examining Othello-GPT, a simple transformer trained on the game Othello. The authors find that Othello-GPT linearly encodes a representation of opposing pieces ("yours" vs. "mine") in its activations, which causally influences its decision-making. Using linear probes, they decode the board state from activation vectors with near-unity accuracy in deeper layers. Causal interventions reveal that this representation is utilized by the model in middle layers for predicting moves, but not in the final layers or shallow networks. The findings suggest that transformers develop interpretable world models that guide decision-making, offering insights into the emergence of intelligence in foundation models.

## Method Summary
The authors train transformer models of varying depths (1-8 layers) and attention heads (1-8 heads) on a synthetic Othello dataset of random legal moves. They implement linear probes to map activation vectors from each layer to the board state representation ("yours" vs. "mine" vs. empty). They also design and apply causal interventions at each layer to test whether the linear world model causally influences the model's next-move predictions.

## Key Results
- Othello-GPT linearly encodes a representation of opposing pieces ("yours" vs. "mine") in its activations
- Causal interventions reveal this representation is utilized by the model in middle layers for predicting moves
- Shallow models (1-layer) can encode the board state linearly but don't causally use it for decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Othello-GPT develops a linearly separable world representation of "mine" vs "yours" pieces.
- Mechanism: The transformer's activation vectors in deeper layers encode the board state such that a simple linear transformation (linear probe) can recover the classification of each tile as "mine," "yours," or "empty" with near-unity accuracy.
- Core assumption: The model learns to compress game state information into a representation where semantic distinctions align with linear separability in activation space.
- Evidence anchors:
  - [abstract] "Othello-GPT linearly encodes a representation of opposing pieces ("yours" vs. "mine") in its activations"
  - [section] "Utilizing linear probes to decode neuron activations across transformer layers, coupled with causal interventions, this paper underscores the enhanced world model of Othello-GPT"
- Break condition: If the board state cannot be decoded by a linear probe, or if the accuracy drops significantly in deeper layers.

### Mechanism 2
- Claim: The world representation is causally utilized by the model for decision-making in middle layers.
- Mechanism: Causal interventions that alter the activation vector to simulate a different board state lead to changes in the model's next-move prediction logits, indicating the representation is actively used for decision-making.
- Core assumption: The model uses the encoded world state information to evaluate possible moves rather than relying solely on surface statistics.
- Evidence anchors:
  - [abstract] "causal interventions reveal that this representation is utilized by the model in middle layers for predicting moves"
  - [section] "If the intervention is indeed causal, then we would see that the post-intervention logit distribution of move option 2 is very similar to that of move option 1"
- Break condition: If interventions in middle layers do not change the logit distribution, or if changes occur only in shallow or final layers.

### Mechanism 3
- Claim: The emergence of the world model depends on layer depth and model complexity.
- Mechanism: Shallow models (1 layer) can encode the board state linearly but do not causally use it for decisions; deeper models develop and utilize the representation more effectively.
- Core assumption: The model's capacity to process and utilize semantic information increases with depth, allowing for both encoding and causal use of world representations.
- Evidence anchors:
  - [abstract] "interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity"
  - [section] "we conduct causal intervention studies for these models... we see that though there is a linear representation of the board states in shallow networks (1-layer), the next-move decision-making is not using the information causally"
- Break condition: If shallow models begin to causally use the world representation, or if deeper models do not show improved causal usage.

## Foundational Learning

- Concept: Linear separability and linear probes
  - Why needed here: To understand how the model's activation vectors can be decoded to recover the board state.
  - Quick check question: If a linear classifier achieves high accuracy on a task, what does that imply about the underlying data representation?

- Concept: Causal interventions in neural networks
  - Why needed here: To determine whether the encoded representation is actually used by the model for decision-making.
  - Quick check question: How can you distinguish between a model using a representation causally versus it being an epiphenomenon?

- Concept: Attention mechanisms and their role in information processing
  - Why needed here: To interpret how different attention heads track "mine" vs "yours" pieces and contribute to the world model.
  - Quick check question: What is the difference between content-based and positional attention in transformers?

## Architecture Onboarding

- Component map: Input sequence of game states -> Transformer layers (multi-headed self-attention + feed-forward networks) -> Linear probe (external classifier) -> Causal intervention module -> Prediction adjustment
- Critical path: Game state encoding → activation vector formation → linear probe decoding → causal intervention → prediction adjustment
- Design tradeoffs:
  - Shallow vs. deep architectures: Shallow models encode linearly but don't causally use the representation; deeper models do both.
  - Attention head specialization: Some heads focus on "mine" pieces, others on "yours," affecting how information is aggregated.
- Failure signatures:
  - Linear probe accuracy drops in deeper layers: Possible information loss or non-linear encoding.
  - Causal interventions fail in middle layers: The model may not be using the representation for decisions at that depth.
  - Attention heads lose specialization: Uniform attention may indicate loss of semantic tracking.
- First 3 experiments:
  1. Train a linear probe on activations from each layer and plot accuracy vs. layer depth.
  2. Perform causal interventions at each layer and measure changes in next-move logits.
  3. Analyze attention head patterns across layers to identify specialization in tracking "mine" vs "yours" pieces.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the linear world representations in Othello-GPT relate to its actual game-playing performance and strategy development?
- Basis in paper: [explicit] The paper demonstrates that linear probes can decode board state information from activations, but questions remain about whether this representation is causally used for decision-making, especially in deeper layers.
- Why unresolved: While the paper shows that linear representations exist and can be causally intervened with in middle layers, it does not fully explore how these representations translate into strategic gameplay or whether they improve performance beyond basic legal move prediction.
- What evidence would resolve it: Experiments comparing game outcomes and strategy complexity between models with and without access to these linear representations, or ablation studies showing the impact of removing or modifying these representations on gameplay quality.

### Open Question 2
- Question: Why do causal interventions fail to influence decision-making in the final layers of Othello-GPT despite high linear probe accuracy?
- Basis in paper: [explicit] The paper observes that interventions in the last two layers do not causally affect next-move decisions, even though linear probes show near-peak accuracy in these layers.
- Why unresolved: The paper hypothesizes that representations are "finalized" in middle layers, but does not provide a mechanistic explanation for why this occurs or what role the final layers play if not decision-making.
- What evidence would resolve it: Detailed mechanistic interpretability studies of the final layers, including circuit analysis and attention pattern investigations, to understand what information processing occurs in these layers.

### Open Question 3
- Question: How general are these findings to other games or domains beyond Othello?
- Basis in paper: [inferred] The study is limited to a single synthetic task (Othello), raising questions about whether similar linear world representations and causal mechanisms would emerge in other environments.
- Why unresolved: The paper provides no comparative analysis with other games or tasks, leaving open whether the observed phenomena are specific to Othello's structure or represent a more general principle in transformer learning.
- What evidence would resolve it: Replication of the study across multiple games or domains (chess, Go, or different synthetic tasks) to test whether similar linear representations and causal patterns emerge consistently.

## Limitations

- The causal intervention methodology relies on assumptions about how board states map back to activation space, introducing potential confounding factors.
- The Othello dataset consists of random legal moves rather than strategic gameplay, which may limit how the model's decision-making generalizes to more complex scenarios.
- The study is limited to a single synthetic task, raising questions about the generalizability of findings to other games or domains.

## Confidence

- **High confidence**: The linear separability of board state representations in deeper layers (Mechanism 1). The linear probe results with near-unity accuracy are directly measurable and reproducible.
- **Medium confidence**: The causal utilization of representations in middle layers (Mechanism 2). While interventions show effects, the exact mechanism of how representations influence predictions requires further investigation.
- **Medium confidence**: The depth-dependent emergence of world models (Mechanism 3). The contrast between shallow and deep models is clear, but the precise threshold for when causal usage emerges is not established.

## Next Checks

1. Implement and compare multiple causal intervention techniques (e.g., direct activation patching vs. gradient-based interventions) to verify that the observed effects are robust to methodology.

2. Train Othello-GPT on a dataset with strategic moves rather than random play, then repeat the linear probe and causal intervention analyses to assess whether world model development depends on the nature of the training data.

3. Systematically disable individual attention heads in middle layers to determine which specific heads are critical for the causal use of world representations, and whether "mine" vs "yours" specialization is necessary for effective decision-making.