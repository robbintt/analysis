---
ver: rpa2
title: The Risks of Recourse in Binary Classification
arxiv_id: '2306.00497'
source_url: https://arxiv.org/abs/2306.00497
tags:
- recourse
- risk
- classifier
- case
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a learning-theoretic framework to analyze
  the impact of algorithmic recourse on classification accuracy. The framework compares
  risks (expected losses) with and without recourse, considering both compliant and
  defiant user responses.
---

# The Risks of Recourse in Binary Classification

## Quick Facts
- arXiv ID: 2306.00497
- Source URL: https://arxiv.org/abs/2306.00497
- Reference count: 40
- Primary result: Providing algorithmic recourse often increases classification risk by pushing users toward decision boundaries where class uncertainty is highest

## Executive Summary
This paper introduces a learning-theoretic framework to analyze how algorithmic recourse affects classification accuracy. The core finding is that providing recourse explanations often increases risk because it moves users toward decision boundaries where uncertainty is maximal. This occurs even with optimal classifiers and holds for probabilistic classifiers with minor estimation errors. The authors also show that when the party deploying the classifier can strategically select it, they can maintain original accuracy while providing recourse, but this creates systemic inefficiency that benefits no one.

## Method Summary
The paper develops a theoretical framework comparing risks with and without recourse using 0/1 loss and surrogate losses. It analyzes both compliant (users follow recourse) and defiant (users don't follow recourse) cases, starting with Bayes-optimal classifiers before extending to probabilistic classifiers. The framework incorporates strategic classifier selection where the deploying party can optimize its choice given recourse mechanisms. Experiments validate theoretical findings using synthetic datasets (Moons, Circles, Gaussians) and real-world datasets (Credit, Census, HELOC) with various classifiers including Logistic Regression, Gradient Boosting, and Decision Trees.

## Key Results
- Recourse increases risk because it moves users toward decision boundaries where class uncertainty is maximal
- Even with optimal classifiers, recourse increases risk in both compliant and defiant cases
- Strategic classifier selection can maintain original accuracy while providing recourse, but provides no benefit to users
- Experiments on synthetic and real-world data confirm theoretical findings, with risk increases observed in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recourse increases risk because it moves users toward decision boundaries where class uncertainty is maximal.
- Mechanism: When users are denied (negative class) and receive recourse, the recourse mechanism pushes them toward the decision boundary where the classifier's confidence is lowest (P(Y=1|X)=0.5). This higher uncertainty leads to more misclassifications.
- Core assumption: The recourse mechanism projects users to the decision boundary, and class uncertainty is highest at this boundary.
- Evidence anchors:
  - [abstract]: "recourse pushes users to regions of higher class uncertainty and therefore leads to more mistakes"
  - [section 3]: "recourse will move data from a region where the prediction is relatively certain...to the decision boundary, where things are the least certain"
  - [corpus]: Weak - related papers discuss robustness and hacking but don't directly address this mechanism
- Break condition: If the recourse mechanism doesn't push users to the decision boundary, or if uncertainty isn't maximal at boundaries, this mechanism fails.

### Mechanism 2
- Claim: Even with optimal classifiers, recourse increases risk due to distributional shifts that increase uncertainty.
- Mechanism: When users comply with recourse recommendations, their true label distribution changes. Moving users to regions of higher uncertainty (near decision boundaries) increases the probability they'll be misclassified even if their underlying probability distribution shifts favorably.
- Core assumption: Users move to regions where the conditional probability is closer to 0.5, creating higher misclassification risk.
- Evidence anchors:
  - [abstract]: "recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty"
  - [section 3]: Theorem 1 shows risk increase for both compliant and defiant cases with optimal classifiers
  - [corpus]: Moderate - papers on "Explanation Hacking" discuss how users might game explanations, but not this specific distributional shift
- Break condition: If the classifier is perfectly calibrated everywhere or if users don't actually move to high-uncertainty regions.

### Mechanism 3
- Claim: Strategic classifier selection by the deploying party can maintain original accuracy while providing recourse, but this creates systemic inefficiency.
- Mechanism: When parties anticipate providing recourse, they can strategically choose classifiers that compensate for recourse effects, maintaining the original decision boundary. This makes recourse provide no benefit to users while still imposing implementation costs.
- Core assumption: The classifier class is invariant under recourse (can compensate for recourse effects).
- Evidence anchors:
  - [abstract]: "the party deploying the classifier has an incentive to strategize...to the detriment of their users"
  - [section 5]: Theorem 4 shows risks are identical with and without recourse in defiant case when strategizing
  - [corpus]: Moderate - papers on "strategic classification" discuss similar dynamics but not this specific compensation mechanism
- Break condition: If the classifier class isn't invariant under recourse, or if parties don't strategically compensate.

## Foundational Learning

- Concept: Learning-theoretic framework for comparing risks with and without recourse
  - Why needed here: The paper needs a formal way to quantify whether recourse is beneficial or harmful at population level
  - Quick check question: What is the difference between RP(f) and RQf(f) in this framework?

- Concept: Bayes-optimal classifier and its properties
  - Why needed here: The analysis starts with Bayes-optimal classifiers to establish baseline results before considering estimation errors
  - Quick check question: Why does the Bayes classifier sign(P(Y=1|X) - 0.5) achieve minimum risk?

- Concept: Performative prediction and strategic classification
  - Why needed here: The framework needs to account for how classifier choice affects data distribution when users respond strategically
  - Quick check question: How does strategic classification differ from standard classification in terms of data distribution?

## Architecture Onboarding

- Component map: Data generation module (P for original, Qf for with recourse) -> Recourse mechanism (φ function mapping X0 to X) -> Classifier (f or g) with risk calculation -> Strategy module (for section 5 analysis)
- Critical path: Generate data → Apply recourse → Calculate risk difference → Determine benefit/harm
- Design tradeoffs:
  - Generality vs specificity: Framework is very general but requires specialization for concrete results
  - Computational cost: Exact Bayes-optimal calculations vs approximate methods for real data
  - Compliant vs defiant models: Different assumptions lead to different conclusions
- Failure signatures:
  - Risk increase smaller than sampling error (indicates recourse might be beneficial)
  - Classifier invariance under recourse not holding (breaks strategic analysis)
  - Assumption (A) or (B) from Theorem 2 violated (limits probabilistic classifier results)
- First 3 experiments:
  1. Test Theorem 1 on synthetic Gaussian data with known Bayes boundary
  2. Verify Theorem 2 bounds on a simple 2D dataset with continuous probabilistic classifier
  3. Implement strategic classifier selection on linear classifiers to confirm Theorem 4 result

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does algorithmic recourse actually decrease classification risk, rather than increase it?
- Basis in paper: [explicit] The paper shows that recourse increases risk in most scenarios, but notes that strategic classifier selection in the compliant case can potentially reduce risk.
- Why unresolved: The paper provides theoretical bounds but doesn't identify concrete real-world scenarios where risk decreases.
- What evidence would resolve it: Empirical studies on diverse datasets showing cases where recourse leads to lower risk, or formal identification of necessary and sufficient conditions for risk reduction.

### Open Question 2
- Question: How do different cost functions for counterfactual explanations affect the relationship between recourse and classification risk?
- Basis in paper: [explicit] The paper assumes monotonically increasing cost functions but doesn't explore how different cost structures impact risk.
- Why unresolved: The analysis focuses on a general framework without examining specific cost function families.
- What evidence would resolve it: Comparative studies using various cost functions (e.g., L1, L2, learned costs) and their impact on risk across multiple datasets.

### Open Question 3
- Question: What is the optimal balance between the accuracy of the conditional probability estimator and the benefits of providing recourse?
- Basis in paper: [inferred] The experiments use large amounts of data to estimate conditional probabilities, suggesting estimation accuracy matters.
- Why unresolved: The paper doesn't analyze how estimation errors in the conditional distribution affect the risk calculations.
- What evidence would resolve it: Studies varying the amount of calibration data and measuring the trade-off between estimation accuracy and recourse effectiveness.

## Limitations
- Results depend on whether users are compliant or defiant, with strategic behavior introducing additional complexity
- The analysis focuses on binary classification, limiting generalizability to multi-class settings
- Real-world recourse mechanisms may have constraints (e.g., feasibility, cost) not captured in the theoretical model

## Confidence
- High confidence: The theoretical framework for comparing risks with and without recourse is sound
- Medium confidence: The core finding that recourse often increases risk holds under the stated assumptions
- Medium confidence: Strategic classifier selection results are mathematically valid but may not reflect practical constraints

## Next Checks
1. Test the framework on real-world recourse mechanisms (e.g., credit scoring appeals) to validate the boundary-pushing assumption
2. Extend analysis to multi-class classification to assess generalizability of the findings
3. Evaluate the impact of recourse constraints (cost, feasibility) on the theoretical risk increases