---
ver: rpa2
title: Evaluating Open-QA Evaluation
arxiv_id: '2305.12421'
source_url: https://arxiv.org/abs/2305.12421
tags:
- evaluation
- question
- these
- answers
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task, QA-Evaluation (QA-Eval), to assess
  the reliability of automatic evaluation methods for Open Question Answering (Open-QA).
  The authors annotate a dataset (EVOUNA) with human evaluations of AI-generated answers,
  and use it to evaluate several evaluation methods, including lexical match, neural-score,
  and large language models.
---

# Evaluating Open-QA Evaluation

## Quick Facts
- arXiv ID: 2305.12421
- Source URL: https://arxiv.org/abs/2305.12421
- Reference count: 9
- Key outcome: Introduces QA-Eval task to assess reliability of automatic evaluation methods for Open-QA; finds human evaluation remains most reliable while automatic methods struggle with answer length

## Executive Summary
This paper addresses the critical need for reliable evaluation methods in Open Question Answering (Open-QA) by introducing a new task called QA-Evaluation (QA-Eval). The authors create the EVOUNA dataset with human annotations of AI-generated answers and use it to evaluate various automatic evaluation methods including lexical match, BERT-score, and large language model (LLM) based evaluators. Their findings reveal that human evaluation remains the most trustworthy approach, while current automatic methods face significant limitations, particularly when dealing with lengthy answers. The work establishes a foundation for developing more effective automatic evaluation tools for Open-QA systems.

## Method Summary
The paper introduces QA-Eval, a task to evaluate the reliability of automatic evaluation methods for Open-QA. The authors construct the EVOUNA dataset by generating answers from NQ and TriviaQA datasets using Open-QA models, then having human annotators evaluate answer correctness. They compare lexical match, BERT-score, and LLM-based evaluators against human judgments using accuracy and F1 metrics. The evaluation pipeline processes questions through retriever-reader systems or LLMs, generates answers, and applies multiple evaluation methods to assess performance.

## Key Results
- Human evaluation remains the most reliable method for assessing answer correctness in Open-QA
- LLM-based evaluators show inverse correlation between accuracy and answer length, with accuracy dropping from 90% to 40% as token count increases
- Retrieval mechanisms significantly improve LLM performance, with Bing Chat's accuracy dropping approximately 15 percentage points when retrieval is disabled

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human evaluation remains the most reliable method for assessing answer correctness in Open-QA
- Mechanism: Human evaluators provide nuanced judgment by considering semantic understanding, factual accuracy, and context, which automatic methods like lexical matching or BERT-score fail to capture
- Core assumption: Human judgment is less prone to errors caused by answer length, complexity, or semantic similarity mismatches
- Evidence anchors:
  - [abstract] "human evaluation still remains the most reliable approach"
  - [section] "human evaluation still remains the most trustworthy approach to assess the correctness of AI-generated answers"
- Break condition: Human evaluation becomes unreliable if annotators lack domain expertise or if inter-annotator agreement drops below acceptable thresholds

### Mechanism 2
- Claim: Automatic evaluation methods degrade with increasing answer length
- Mechanism: Longer answers introduce more tokens, making it harder for models like GPT-3.5 to maintain context and accurately judge correctness, leading to lower accuracy scores
- Core assumption: Token count correlates with evaluation difficulty due to context retention issues in LLMs
- Evidence anchors:
  - [section] "GPT-3.5's evaluation accuracy exhibits an inverse correlation with the length of the answer"
  - [section] Figure 1 shows accuracy dropping from 90 to 40 as token count increases
- Break condition: Break condition occurs when evaluation models incorporate context-aware mechanisms that scale better with input length

### Mechanism 3
- Claim: Retrieval mechanisms significantly improve LLM performance in Open-QA
- Mechanism: By providing relevant passages, retrieval helps LLMs ground their answers in factual information, reducing hallucination and improving accuracy
- Core assumption: LLMs benefit from external knowledge injection when internal knowledge is insufficient or outdated
- Evidence anchors:
  - [section] "Bing Chat's performance when retrieval is disabled... dropping approximately 15 percentage points"
  - [section] "Retrieval component significantly boosts the performance of the LLM underpinning Bing Chat"
- Break condition: Break condition occurs when LLMs develop sufficient internal knowledge or when retrieval introduces noise that degrades performance

## Foundational Learning

- Concept: Inter-annotator agreement
  - Why needed here: Ensures consistency and reliability of human evaluations used as ground truth
  - Quick check question: What is the Cohen's Kappa score threshold considered acceptable for reliable annotations?

- Concept: Semantic similarity metrics
  - Why needed here: Core to evaluating how well automatic methods capture answer correctness beyond lexical matching
  - Quick check question: How does BERT-score differ from exact match in handling paraphrased answers?

- Concept: Retrieval-augmented generation
  - Why needed here: Understanding how retrieval components interact with LLMs is crucial for evaluating Open-QA systems
  - Quick check question: What are the two main components in retriever-reader approaches for Open-QA?

## Architecture Onboarding

- Component map: Question → Retriever (if applicable) → Reader/LLM → Answer → Evaluation → Human annotation (ground truth)
- Critical path: Question → Retriever (if applicable) → Reader/LLM → Answer → Evaluation → Human annotation (ground truth)
- Design tradeoffs: Lexical Match is fast but fails on paraphrasing; BERT-score handles semantics but struggles with long answers; LLM evaluators are flexible but computationally expensive
- Failure signatures: Low inter-annotator agreement, accuracy dropping with answer length, significant performance gaps between human and automatic evaluation
- First 3 experiments:
  1. Run NQ dataset through all Open-QA models and compare human vs automatic evaluation accuracy
  2. Test BERT-score threshold sensitivity on NQ vs TriviaQA datasets
  3. Measure inter-annotator agreement on a subset of generated answers to validate annotation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of automatic evaluation methods for Open-QA be improved to match or surpass human evaluation?
- Basis in paper: [explicit] The paper highlights the limitations of current automatic evaluation methods and suggests that human evaluation remains the most reliable approach. It also introduces the QA-Eval task to facilitate the development of more effective automatic evaluation tools.
- Why unresolved: The paper does not provide specific solutions or methods to improve the performance of automatic evaluation methods to match or surpass human evaluation. It only mentions the limitations and the need for further research in this area.
- What evidence would resolve it: Evidence could include new evaluation methods that demonstrate higher correlation with human evaluations than current methods, or case studies showing improved performance of automatic evaluation methods in Open-QA tasks.

### Open Question 2
- Question: How does the inclusion of excessive information in answers affect the performance of current evaluation methods, and what strategies can be employed to mitigate this issue?
- Basis in paper: [explicit] The paper discusses the pitfalls of current methods, such as their inability to accurately judge responses that contain excessive information.
- Why unresolved: The paper does not provide specific strategies or solutions to address the issue of excessive information in answers and its impact on evaluation performance. It only highlights the problem.
- What evidence would resolve it: Evidence could include the development and testing of new evaluation methods that can accurately assess answers with excessive information, or experimental results showing improved performance when dealing with such answers.

### Open Question 3
- Question: How does the performance of LLM-based evaluators compare to traditional evaluation methods, and what are the key factors contributing to their differences in performance?
- Basis in paper: [explicit] The paper introduces LLM-based evaluators and compares their performance with traditional evaluation methods, such as lexical match and neural-score. It also discusses the potential improvements to LLM-based evaluators.
- Why unresolved: The paper does not provide a detailed analysis of the key factors contributing to the differences in performance between LLM-based evaluators and traditional methods. It only presents the comparison results and suggests potential improvements.
- What evidence would resolve it: Evidence could include a comprehensive analysis of the strengths and weaknesses of LLM-based evaluators and traditional methods, as well as experimental results showing the impact of different factors on their performance.

## Limitations
- Human annotation process details are sparse, with inter-annotator agreement scores and specific guidelines not fully documented
- Performance degradation of LLM-based evaluators with answer length is based on internal experiments without comparison to alternative context-aware models
- Retrieval mechanism's impact is demonstrated only for Bing Chat, without broader testing across different retriever-reader architectures

## Confidence

- High confidence: Human evaluation reliability claim - well-supported by multiple evidence anchors and aligns with established research
- Medium confidence: Answer length correlation with LLM performance - internal experiments without alternative approaches tested
- Medium confidence: Retrieval's positive impact - single system demonstration without broader architectural testing

## Next Checks

1. Replicate the inter-annotator agreement analysis on a subset of EVOUNA annotations to verify the reliability of human evaluations and identify potential annotation inconsistencies

2. Test alternative semantic similarity metrics (e.g., SBERT, MoverScore) on the NQ dataset to determine if BERT-score's limitations are specific to that method or indicative of broader challenges in semantic evaluation

3. Evaluate multiple retriever-reader combinations (e.g., DPR+BART, BM25+T5) on TriviaQA to assess whether retrieval benefits are consistent across different architectures and datasets