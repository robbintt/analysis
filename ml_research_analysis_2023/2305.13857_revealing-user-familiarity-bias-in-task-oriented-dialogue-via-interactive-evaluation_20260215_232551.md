---
ver: rpa2
title: Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation
arxiv_id: '2305.13857'
source_url: https://arxiv.org/abs/2305.13857
tags:
- user
- system
- users
- systems
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates user familiarity bias in task-oriented\
  \ dialogue (TOD) systems by comparing user interactions under closed-goal (system-constrained)\
  \ and open-goal (realistic, unconstrained) settings. An interactive user study with\
  \ 20 participants conversing with a state-of-the-art TOD system reveals that open-goal\
  \ dialogues result in significantly more errors and failures\u201492% encountered\
  \ significant issues versus 52% in closed-goal settings."
---

# Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation

## Quick Facts
- arXiv ID: 2305.13857
- Source URL: https://arxiv.org/abs/2305.13857
- Reference count: 40
- Primary result: Open-goal user interactions with TOD systems reveal catastrophic failures (92% with issues) versus closed-goal settings (52% with issues)

## Executive Summary
This paper investigates user familiarity bias in task-oriented dialogue (TOD) systems by comparing user interactions under closed-goal (system-constrained) and open-goal (realistic, unconstrained) settings. Through an interactive user study with 20 participants conversing with a state-of-the-art TOD system, the research reveals that open-goal dialogues result in significantly more errors and failures compared to closed-goal settings. The study identifies six error types, with "pretending" behavior—where systems provide false information for unsupported requests—being particularly problematic. These findings highlight the critical gap between benchmark performance and real-world usability of TOD systems.

## Method Summary
The study employs an interactive user study methodology where participants converse with the GALAXY (He et al., 2022) TOD model through a Gradio chat interface. Twenty participants (10 for closed goals, 10 for open goals) each complete 5 dialogues, with goals either following MultiWOZ 2.1 constraints or including exceptional requests generated via InstructGPT. Dialogues are annotated for errors across six categories, and termination patterns are analyzed using mixed-effects models to account for both fixed effects (goal type) and random effects (participant differences).

## Key Results
- Open-goal dialogues lead to catastrophic failures: 92% had significant issues compared to 52% in closed-goal settings
- "Pretending" behavior identified as a novel failure mode where systems provide false information for unsupported requests
- Statistical analysis shows significant fixed effect of goal type on dialogue termination patterns (p = .002)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: User familiarity bias in TOD datasets leads to catastrophic failures when deployed with open-goal users.
- **Mechanism**: Datasets collected via Wizard-of-Oz protocols constrain user goals to system capabilities, training models to expect well-formed, supported requests. When users deviate (open-goal), the model's learned patterns break down, resulting in irrelevant or pretending responses.
- **Core assumption**: The model has only seen closed-goal conversations during training and evaluation.
- **Evidence anchors**:
  - [abstract] "Most task-oriented dialogue (TOD) benchmarks assume users that know exactly how to use the system by constraining the user behaviors within the system's capabilities via strict user goals, namely 'user familiarity' bias."
  - [section 1] "We hypothesize that the major source of this instability lies in the naive assumption about the users during TOD data collection."
  - [corpus] FMR score 0.658 for "Infusing Emotions into Task-oriented Dialogue Systems" suggests high semantic overlap with familiarity bias topic.
- **Break condition**: The model encounters a request outside its training distribution (open-goal) that it cannot fulfill, triggering pretending behavior or irrelevant responses.

### Mechanism 2
- **Claim**: Pretending behavior is a distinct failure mode where TOD systems hallucinate service-specific information to maintain conversation flow.
- **Mechanism**: When faced with unsupported requests, the model generates plausible-sounding but false information about system capabilities (e.g., wheelchair access, gluten-free options) because it has learned to always respond rather than admit limitations.
- **Core assumption**: The model prioritizes fluency and task completion over factual accuracy when uncertain.
- **Evidence anchors**:
  - [abstract] "We discover a novel 'pretending' behavior, in which the system pretends to handle the user requests even though they are beyond the system's capabilities."
  - [section 4.5] "Pretending represents system behaviors pretending to handle user's requests beyond the system's scope or answer user's non-verifiable questions."
  - [corpus] FMR score 0.658 indicates strong semantic similarity to hallucination research.
- **Break condition**: The user attempts to verify the pretended information, revealing the falsehood and breaking trust.

### Mechanism 3
- **Claim**: Open-goal settings reveal a significant gap between benchmark performance and real-world usability.
- **Mechanism**: Static benchmark evaluations using closed-goals show high task success rates, but interactive user studies with open-goals expose fundamental limitations in handling realistic user behavior, with 92% of open-goal dialogues encountering significant issues.
- **Core assumption**: Static evaluations don't capture the full range of user behaviors and system failures.
- **Evidence anchors**:
  - [abstract] "Our study reveals that conversations in open-goal settings lead to catastrophic failures of the system, in which 92% of the dialogues had significant issues."
  - [section 4.2] "The maximum-likelihood test reveals that there is a significant random effect of participants (p < .0001, t(17.98) = 5.06), and a significant fixed effect of goal type (p = .002, t(18.14) = 3.71)."
  - [corpus] FMR score 0.528 for "Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement Learning" suggests relevance to evaluation methodology gaps.
- **Break condition**: The model's performance metrics are based on closed-goal scenarios that don't reflect real-world usage patterns.

## Foundational Learning

- **Concept**: Understanding the Wizard-of-Oz data collection methodology
  - **Why needed here**: This methodology creates the user familiarity bias by constraining user behaviors to system capabilities during data collection.
  - **Quick check question**: How does the Wizard-of-Oz protocol differ from natural user interactions with TOD systems?

- **Concept**: Distinguishing between hallucination and pretending behaviors
  - **Why needed here**: Pretending is service-specific hallucination that's harder to detect because users can't easily verify the information.
  - **Quick check question**: What makes pretending behavior more problematic than generic hallucination in TOD systems?

- **Concept**: Mixed-effects modeling for analyzing user study data
  - **Why needed here**: This statistical approach accounts for both fixed effects (goal type) and random effects (individual participant differences) in analyzing termination patterns.
  - **Quick check question**: Why is it important to model participant as a random effect when analyzing dialogue termination patterns?

## Architecture Onboarding

- **Component map**: User input → Dialogue state tracking → Policy management → Response generation → Error detection → User feedback
- **Critical path**: User input → Dialogue state tracking → Policy management → Response generation → Error detection → User feedback
- **Design tradeoffs**: 
  - Closed-goal evaluation provides clean metrics but misses real-world failures
  - Open-goal evaluation reveals system limitations but introduces more variance
  - Pretended responses maintain conversation flow but erode user trust
- **Failure signatures**:
  - High frequency of Irrelevant and Pretending errors in open-goal settings
  - System contradictions across dialogue turns
  - User forced termination due to repetitive or awkward responses
- **First 3 experiments**:
  1. Replicate the user study with a different TOD model (e.g., Soloist) to verify generality of findings
  2. Implement a simple out-of-scope detection mechanism and measure impact on pretending behavior
  3. Conduct A/B testing comparing transparent vs. non-transparent error handling approaches

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions in the traditional sense, but several important areas for future research emerge from the findings:

1. How to effectively detect and handle out-of-scope requests in TOD systems
2. Methods for improving transparency when systems cannot fulfill user requests
3. Developing more robust evaluation methodologies that capture real-world user behaviors

## Limitations

- The study is based on a single TOD model (GALAXY), limiting generalizability across different architectures
- Error annotation relies on subjective judgments by authors, potentially introducing bias
- The user sample size (20 participants) is relatively small for drawing broad conclusions

## Confidence

- **High Confidence**: The statistical significance of goal type effects on dialogue termination patterns (p = .002 for fixed effect of goal type)
- **Medium Confidence**: The identification of "pretending" as a distinct failure mode, given its theoretical basis but limited empirical examples
- **Medium Confidence**: The 92% failure rate in open-goal settings, though this needs replication across different models and user populations

## Next Checks

1. Replicate the study with at least two additional state-of-the-art TOD models (e.g., Soloist, SimpleTOD) to test the generalizability of the user familiarity bias findings
2. Conduct a larger-scale user study (N > 50) with diverse demographic representation to validate the error patterns and failure rates
3. Implement and test an out-of-scope detection mechanism in the dialogue system to measure its impact on reducing pretending behavior