---
ver: rpa2
title: 'Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration'
arxiv_id: '2312.03987'
source_url: https://arxiv.org/abs/2312.03987
tags:
- batch
- uni00000013
- question
- prompting
- demonstration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cost-effective batch prompting for entity
  resolution (ER) using large language models (LLMs). The authors introduce a framework
  BATCHER that consists of demonstration selection and question batching.
---

# Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration

## Quick Facts
- arXiv ID: 2312.03987
- Source URL: https://arxiv.org/abs/2312.03987
- Reference count: 40
- Key outcome: Batch prompting framework (BATCHER) achieves 4x-7x cost savings while improving accuracy for entity resolution using large language models

## Executive Summary
This paper introduces BATCHER, a framework for cost-effective entity resolution using large language models through in-context learning. The framework addresses the challenge of balancing accuracy and monetary cost by introducing two key components: demonstration selection and question batching. Through extensive experiments on well-known ER benchmarks, the authors demonstrate that batch prompting significantly outperforms both traditional fine-tuned models and standard LLM prompting in terms of both accuracy and cost-effectiveness. The covering-based demonstration selection strategy achieves the highest accuracy while incurring the lowest cost, providing a practical solution for real-world entity resolution tasks.

## Method Summary
BATCHER is a batch prompting framework that consists of two main components: demonstration selection and question batching. For demonstration selection, the paper introduces a covering-based strategy that selects a minimal set of demonstrations to cover all questions in a batch, formulated as a set cover problem. For question batching, it employs a diversity-based approach that clusters similar entity pairs and samples questions from each cluster to ensure both coverage and diversity. The framework uses feature extraction to convert entity pairs into numerical representations, with two specific extractors evaluated: semantics-based (using SBERT) and structure-aware (using FLAIR). The entire system is evaluated using GPT-3.5-turbo-0301 as the underlying LLM with temperature set to 0.01.

## Key Results
- Batch prompting achieves 4x-7x cost savings compared to standard prompting while improving precision
- The covering-based demonstration selection strategy provides 10x-100x labeling cost savings compared to Topk-question strategy
- Batch prompting outperforms both PLM-based methods and standard LLM prompting in both accuracy and cost-effectiveness
- The covering-based strategy achieves the highest accuracy while incurring the lowest cost across all evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch prompting improves accuracy over standard prompting for entity resolution.
- Mechanism: By grouping multiple questions into a single prompt, the LLM can leverage contextual information from multiple entity pairs simultaneously, improving its ability to differentiate subtle differences between entities.
- Core assumption: The LLM's performance on a given question benefits from exposure to related questions within the same batch, allowing it to identify shared characteristics and patterns.
- Evidence anchors:
  - [abstract] "Through extensive experiments, we find that batch prompting is very cost-effective for ER, compared with not only PLM-based methods fine-tuned with extensive labeled data but also LLM-based methods with manually designed prompting."
  - [section] "Figure 6 shows that batch prompting achieves much higher Precision than standard prompting, while their Recall scores are comparable. This is mainly attributed to the batching mechanism, where the LLM can refer to not only the provided demonstrations, but also the answers generated for previous questions within the same batch."
  - [corpus] Corpus evidence for this specific mechanism is limited, but the high FMR scores of related papers on few-shot NER and in-context learning suggest a broader trend of improved performance through contextual learning.
- Break condition: If the questions within a batch are too dissimilar, the LLM may struggle to identify relevant patterns, potentially leading to degraded performance.

### Mechanism 2
- Claim: Batch prompting significantly reduces the monetary cost of entity resolution.
- Mechanism: By grouping multiple questions into a single prompt, batch prompting reduces the number of API calls required to process a large set of entity pairs, directly translating to lower costs.
- Core assumption: The cost of an API call is primarily determined by the number of tokens in the prompt, and batch prompting reduces the total number of prompts needed.
- Evidence anchors:
  - [abstract] "Batch prompting can bring 4x-7x cost saving and achieve higher and more stable accuracy than standard prompting."
  - [section] "Table III shows that batch prompting can achieve 4x-7x cost saving on API callings compared to standard prompting."
  - [corpus] Corpus evidence for this specific mechanism is limited, but the high FMR scores of related papers on few-shot NER and in-context learning suggest a broader trend of improved efficiency through contextual learning.
- Break condition: If the batch size becomes too large, the prompt may exceed the LLM's maximum token limit, negating the cost savings.

### Mechanism 3
- Claim: The covering-based demonstration selection strategy effectively balances accuracy and cost.
- Mechanism: By selecting a minimal set of demonstrations that cover all questions in a batch, the covering-based strategy reduces the labeling cost while ensuring that each question has access to relevant examples.
- Core assumption: The relevance of a demonstration to a question can be measured by the distance between their feature vectors, and a demonstration that covers multiple questions is more cost-effective than multiple demonstrations covering individual questions.
- Evidence anchors:
  - [abstract] "We also devise a covering-based demonstration selection strategy that achieves an effective balance between matching accuracy and monetary cost."
  - [section] "Cover is much more cost-effective than Topk-question on demonstration labeling, e.g., brings 10x-100x labeling cost savings on the former five large datasets and 5x savings on the latter three small datasets."
  - [corpus] Corpus evidence for this specific mechanism is limited, but the high FMR scores of related papers on few-shot NER and in-context learning suggest a broader trend of improved efficiency through selective demonstration use.
- Break condition: If the distance threshold for determining coverage is set too high, the selected demonstrations may not be relevant enough to effectively guide the LLM, leading to reduced accuracy.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the core learning paradigm used by LLMs to perform entity resolution without explicit model training. Understanding ICL is crucial for grasping how batch prompting works.
  - Quick check question: How does ICL differ from traditional fine-tuning approaches in terms of model adaptation and data requirements?

- Concept: Feature extraction for entity pairs
  - Why needed here: Feature extraction is used to convert entity pairs into numerical representations that can be used for clustering and demonstration selection. The choice of feature extractor significantly impacts the performance of batch prompting.
  - Quick check question: What are the key differences between semantics-based and structure-aware feature extractors, and how do they affect the selection of relevant demonstrations?

- Concept: Set cover problem
  - Why needed here: The covering-based demonstration selection strategy is formulated as a set cover problem, which is known to be NP-hard. Understanding this problem is essential for grasping the efficiency and limitations of the strategy.
  - Quick check question: Why is the set cover problem relevant to demonstration selection, and what are the trade-offs between finding an optimal solution and using a greedy approximation algorithm?

## Architecture Onboarding

- Component map: Feature extraction -> Question batching -> Demonstration selection -> Prompt construction -> LLM inference -> Result aggregation
- Critical path: Question batching → Demonstration selection → Prompt construction → LLM inference → Result aggregation
- Design tradeoffs:
  - Batch size vs. token limit: Larger batches reduce API calls but risk exceeding the LLM's token limit
  - Coverage threshold vs. accuracy: A higher threshold reduces labeling cost but may lead to less relevant demonstrations
  - Feature extractor choice: Semantics-based extractors capture task-agnostic information, while structure-aware extractors focus on entity-specific attributes
- Failure signatures:
  - Low accuracy: Questions within a batch are too dissimilar, or demonstrations are not relevant enough
  - High cost: Batch size is too small, or the coverage threshold is too low
  - Slow performance: Feature extraction or clustering is computationally expensive
- First 3 experiments:
  1. Compare batch prompting with standard prompting on a small dataset to validate the accuracy improvement
  2. Vary the batch size and measure the impact on API cost and accuracy to find the optimal batch size
  3. Compare the performance of semantics-based and structure-aware feature extractors on a representative dataset to determine the best choice for entity resolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of batch prompting for entity resolution scale with different batch sizes?
- Basis in paper: [inferred] The paper explores batch prompting but focuses on a batch size of 8 for evaluation.
- Why unresolved: The paper does not investigate how varying batch sizes affects the trade-off between accuracy and cost in entity resolution.
- What evidence would resolve it: Experiments varying batch sizes and measuring the impact on both accuracy and cost would provide insights into the optimal batch size for different entity resolution tasks.

### Open Question 2
- Question: How do different feature extractors, beyond the ones evaluated in the paper, impact the performance of batch prompting for entity resolution?
- Basis in paper: [explicit] The paper mentions two feature extractors (semantics-based and structure-aware) and suggests exploring other extractors.
- Why unresolved: The paper only evaluates two specific feature extractors and does not investigate the impact of other potential extractors.
- What evidence would resolve it: Experiments using different feature extractors, such as transformer-based models or domain-specific extractors, would reveal their impact on batch prompting performance.

### Open Question 3
- Question: Can the covering-based demonstration selection strategy be adapted for other data integration tasks beyond entity resolution?
- Basis in paper: [explicit] The paper introduces a novel covering-based demonstration selection strategy for entity resolution.
- Why unresolved: The paper does not explore the applicability of this strategy to other data integration tasks.
- What evidence would resolve it: Applying the covering-based strategy to tasks like schema matching or data fusion and evaluating its effectiveness would demonstrate its generalizability.

## Limitations

- Restricted evaluation scope to specific ER benchmarks may not generalize to other entity matching tasks or domains
- Covering-based demonstration selection strategy's performance depends heavily on the chosen distance threshold and feature extractor
- Paper does not address potential quality degradation when scaling to larger datasets or more complex entity matching scenarios

## Confidence

- High confidence: The core mechanism of batch prompting reducing API calls and associated costs through demonstration reuse
- Medium confidence: The effectiveness of covering-based demonstration selection across all dataset types
- Medium confidence: The generalizability of cost-effectiveness claims across different LLM providers and pricing models

## Next Checks

1. **Cross-domain validation**: Test the batch prompting framework on non-retail entity matching datasets (e.g., academic citations, biomedical entities) to assess generalizability beyond the evaluated Walmart-Amazon and Abt-Buy benchmarks.

2. **Cost model verification**: Conduct a comprehensive cost analysis using different LLM APIs (GPT-4, Claude, PaLM) with varying pricing structures to validate the claimed 4x-7x cost savings across multiple providers.

3. **Threshold sensitivity analysis**: Systematically evaluate how different distance thresholds in the covering-based demonstration selection strategy affect both accuracy and cost across datasets of varying sizes and complexities.