---
ver: rpa2
title: Using ChatGPT as a CAT tool in Easy Language translation
arxiv_id: '2308.11563'
source_url: https://arxiv.org/abs/2308.11563
tags:
- language
- easy
- text
- translation
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluates ChatGPT for translating German administrative
  texts into Easy Language for people with reading impairments. Two translation strategies
  were tested: holistic and linguistic-level-dependent.'
---

# Using ChatGPT as a CAT tool in Easy Language translation

## Quick Facts
- arXiv ID: 2308.11563
- Source URL: https://arxiv.org/abs/2308.11563
- Reference count: 7
- Key outcome: ChatGPT outputs were easier to read than source texts but mostly failed to meet Easy Language standards (HIX <18), with only 20% of holistic translations achieving acceptable readability and 62.5% containing content errors.

## Executive Summary
This study evaluates ChatGPT's performance in translating German administrative texts into Easy Language for people with reading impairments. Two translation strategies were tested: holistic and linguistic-level-dependent. While ChatGPT generated texts were easier to read than source texts, most failed to meet Easy Language readability standards (HIX <18). Content accuracy was notably poor, with 62.5% of texts containing errors. The authors conclude that ChatGPT may serve as a draft for professional translators but requires significant post-editing.

## Method Summary
The study used 20 German administrative texts (179-672 words each) from public authority websites, translated using ChatGPT with two strategies: holistic and linguistic-level-dependent. Quality was assessed through automated readability analysis using HIX index, syntactic complexity analysis via dependency parsing with Stanza, and manual evaluation of content correctness. The researchers explored how role assignment and contextual framework might improve output quality.

## Key Results
- Only 20% of holistic translations achieved acceptable Easy Language readability (HIX <18)
- ChatGPT outputs showed higher syntactic complexity than source texts
- 62.5% of translated texts contained content errors
- Holistic approach yielded better readability (mean HIX 15.3) than linguistic-level-dependent approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role assignment improves ChatGPT output quality for Easy Language translation
- Mechanism: Assigning ChatGPT a specific identity (e.g., "You are a translator for Easy Language") primes the model to apply domain-specific constraints during generation
- Core assumption: ChatGPT's responses are influenced by the framing and context provided in the prompt
- Evidence anchors:
  - [abstract] "One way to improve the quality of the answers is to assign ChatGPT a role"
  - [section 6] "when telling the tool that it is a translator for Easy Language before asking it to translate a text into Easy Language, it seems that the output is less complex than without the previous role assignment"
- Break condition: Role assignment has no measurable effect on output quality metrics (readability scores, syntactic complexity, or content correctness)

### Mechanism 2
- Claim: Contextual framework improves translation output
- Mechanism: Providing background information about Easy Language rules before requesting translation helps ChatGPT generate more compliant output
- Core assumption: ChatGPT can utilize additional context to adjust its generation strategy
- Evidence anchors:
  - [abstract] "ChatGPT seems also to deliver more appropriate outcomes if a context is set before asking for a translation"
  - [section 6] "it may be helpful to ask ChatGPT about German Easy Language rules and then ask for a translation into German Easy Language"
- Break condition: Setting context does not improve output quality compared to direct translation requests

### Mechanism 3
- Claim: Holistic approach yields more readable output than linguistic-level dependent approach
- Mechanism: Treating translation as a complete task rather than breaking it into linguistic components preserves semantic coherence and produces more readable text
- Core assumption: ChatGPT's generation process benefits from maintaining global context rather than processing local linguistic features independently
- Evidence anchors:
  - [abstract] "the holistic approach had the highest comprehensibility, with a mean HIX value of 15.3"
  - [section 5.2] "In comparison, the holistic approach yielded four texts with a HIX value of at least 18, so that – according to this criterion – 20% of the texts could indeed be classified as being easy to understand"
- Break condition: Linguistic-level dependent approach produces equivalent or better readability scores

## Foundational Learning

- Concept: Readability metrics and HIX index
  - Why needed here: To evaluate whether generated texts meet Easy Language standards (HIX ≥ 18)
  - Quick check question: What HIX score threshold distinguishes Easy Language from standard language texts?

- Concept: Syntactic complexity measurement using dependency parsing
  - Why needed here: To analyze structural differences between source texts and generated translations
  - Quick check question: Which dependency relations (acl, advcl, ccomp, csubj, xcomp, parataxis) indicate syntactic complexity in the UD framework?

- Concept: Controlled language principles
  - Why needed here: Easy Language follows specific rules on text, sentence, and word levels that must be considered when evaluating translations
  - Quick check question: What are the three main levels at which Easy Language rules operate?

## Architecture Onboarding

- Component map: Source text → Prompt engineering → ChatGPT generation → Quality assessment → Post-editing decision
- Critical path: Source text → Prompt formulation (role assignment, context setting) → ChatGPT generation → Quality assessment (HIX readability, syntactic complexity analysis, content correctness evaluation) → Post-editing decision
- Design tradeoffs: Role assignment vs. direct translation requests; holistic vs. linguistic-level approaches; automated vs. human evaluation
- Failure signatures: Low HIX scores (<18), high syntactic complexity (excessive dependency relations), content errors, or lack of semantic preservation
- First 3 experiments:
  1. Compare outputs from role-assigned vs. non-role-assigned prompts using identical source texts
  2. Test holistic vs. linguistic-level approaches on the same source material
  3. Evaluate impact of pre-prompting ChatGPT about Easy Language rules before translation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompting techniques beyond role assignment and context setting could significantly improve ChatGPT's performance in Easy Language translation?
- Basis in paper: [explicit] The authors mention they discovered some prompting techniques like assigning ChatGPT a role or setting contextual framework could improve output quality, but state these effects need investigation in larger-scale studies.
- Why unresolved: The paper only mentions these techniques exist but doesn't systematically test different prompting strategies or quantify their impact on translation quality.
- What evidence would resolve it: Controlled experiments testing various prompting strategies (multiple role assignments, different context settings, prompt engineering techniques) with quantitative measures of readability and correctness would show which techniques are most effective.

### Open Question 2
- Question: How do the syntactic complexity patterns in ChatGPT-generated Easy Language compare to human-generated Easy Language translations across different text types?
- Basis in paper: [inferred] The study finds ChatGPT outputs have more complex syntactic structures than source texts but doesn't compare to human-generated Easy Language, which would be the gold standard.
- Why unresolved: The analysis only compares ChatGPT outputs to source texts, not to professionally translated Easy Language texts, making it unclear whether the complexity issues are unique to ChatGPT or inherent to automated approaches.
- What evidence would resolve it: Direct comparison of ChatGPT-generated texts with professionally translated Easy Language texts using the same syntactic complexity metrics would reveal whether the tool's patterns differ significantly from human translators.

### Open Question 3
- Question: What is the optimal post-editing workflow that combines ChatGPT-generated drafts with human expertise to maximize efficiency while maintaining quality standards?
- Basis in paper: [explicit] The authors conclude ChatGPT might serve as a draft for professional translators requiring post-editing, but don't investigate what this workflow should look like or how much time it saves.
- Why unresolved: While the paper suggests ChatGPT could be useful as a starting point, it doesn't explore how translators should approach the post-editing process or measure the efficiency gains compared to translating from scratch.
- What evidence would resolve it: Time-motion studies comparing different post-editing approaches (light vs. full post-editing, different editing orders) on ChatGPT drafts versus manual translation from source texts, measuring both time and quality outcomes.

## Limitations
- Content accuracy remains problematic, with 62.5% of translated texts containing errors
- Only 20% of holistic translations met Easy Language readability standards (HIX <18)
- Manual correctness evaluation criteria lack detailed documentation
- Study relies on automated readability metrics that may not fully capture comprehensibility for target audiences

## Confidence

- **High confidence**: ChatGPT generates syntactically more complex output than source texts; automated readability analysis shows measurable differences between approaches
- **Medium confidence**: Role assignment and context setting improve output quality; holistic approach yields better readability than linguistic-level-dependent approach
- **Low confidence**: ChatGPT can serve as a draft for professional translators without significant post-editing; HIX <18 threshold reliably indicates non-compliance with Easy Language standards

## Next Checks

1. **Prompt engineering validation**: Systematically test different role assignment and context-setting prompts with controlled source texts to measure their impact on HIX scores and error rates

2. **Human evaluation with target users**: Conduct comprehension testing with actual Easy Language readers to validate whether HIX scores correlate with real-world readability and understanding

3. **Error type analysis**: Categorize and quantify different types of content errors (factual errors, omissions, additions) to understand systematic failure modes and inform post-editing workflows