---
ver: rpa2
title: 'DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer'
arxiv_id: '2312.03724'
source_url: https://arxiv.org/abs/2312.03724
tags:
- prompt
- prompts
- output
- privacy
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to tune prompts for large language
  models in a privacy-preserving manner. The core idea is to use a local LLM to generate
  prompts using differentially private ensemble of in-context learning, then transfer
  these prompts to cloud models.
---

# DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer

## Quick Facts
- arXiv ID: 2312.03724
- Source URL: https://arxiv.org/abs/2312.03724
- Authors: 
- Reference count: 40
- One-line primary result: DP-OPT generates privacy-preserving prompts that transfer across models with competitive performance

## Executive Summary
DP-OPT addresses the challenge of prompt tuning for large language models while preserving data privacy. The method uses a local LLM to generate discrete prompts through differentially private ensemble of in-context learning, then transfers these prompts to cloud models for inference. This approach protects both private data and model ownership while maintaining competitive performance compared to non-private methods.

## Method Summary
DP-OPT operates in two phases: first, it generates private prompts using a local LLM and differentially private ensemble methods on confidential training data; second, it transfers these prompts to cloud models for public inference. The method partitions private data into disjoint subsets, generates tokens via voting from each subset, applies the LimitedDomain mechanism to select top-k tokens with DP noise, and uses exponential mechanism to select the best prompt while protecting validation set privacy.

## Key Results
- Prompts generated by DP-OPT transfer across different model architectures with only 6.9% accuracy loss
- The method provides strong privacy guarantees while maintaining competitive performance against non-private baselines
- DP-OPT effectively protects against membership inference attacks while enabling prompt engineering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP-OPT can generate prompts that transfer across different model architectures without significant performance loss.
- Mechanism: The local LLM generates discrete prompts via differentially-private ensemble of in-context learning, which are then applied to cloud models. These prompts are coherent and task-specific instructions rather than model-specific embeddings.
- Core assumption: Discrete prompts generated by LLMs capture task semantics that are generalizable across models, unlike soft prompts optimized in embedding space.
- Evidence anchors:
  - [abstract] "We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising performance significantly."
  - [section 4.1] "We extend the same experiment to training on Vicuna-7b and testing on DaVinci-003. Similarly, we observe a 6.9% loss of accuracy upon transfer."
- Break condition: If the local LLM fails to generate coherent task instructions or if the task semantics are too model-specific to transfer.

### Mechanism 2
- Claim: Differentially-private ensemble generation prevents private data leakage during prompt creation.
- Mechanism: The method partitions private data into disjoint subsets, generates tokens based on voting results from each subset, and applies the LimitedDomain mechanism to select tokens with top-k votes while adding DP noise.
- Core assumption: The ensemble approach with DP noise can generate useful prompts while bounding the privacy cost per token.
- Evidence anchors:
  - [section 4.2] "We leverage the classicsample-and-aggregate paradigm... we generate each token based on the voting results formed by querying the language model with each disjoint subset."
- Break condition: If the privacy budget is exhausted before generating enough tokens, or if the ensemble voting fails to produce coherent tokens.

### Mechanism 3
- Claim: Private prompt selection via exponential mechanism protects validation set privacy while choosing the best prompt.
- Mechanism: After generating candidate prompts, the method uses exponential mechanism to select the prompt with highest count of correct predictions on validation set in a differentially private manner.
- Core assumption: The validation set is disjoint from training data, so privacy cost doesn't compound with prompt generation privacy cost.
- Evidence anchors:
  - [section 4.2] "We use the exponential mechanism to select the best-generated prompt that achieves the highest count of correct predictions on the validation set in a differentially private manner."
- Break condition: If the validation set is too small to provide meaningful selection signal, or if the exponential mechanism introduces too much noise for effective selection.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP provides the mathematical framework to bound privacy leakage when using private data to generate prompts.
  - Quick check question: What is the difference between (ε, δ)-DP and Rényi DP, and why would you use one over the other in this context?

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is the foundation for how LLMs can learn from demonstrations without parameter updates, which DP-OPT leverages for prompt generation.
  - Quick check question: How does ICL differ from traditional fine-tuning, and what are its privacy implications?

- Concept: Transfer Learning
  - Why needed here: Understanding when and why prompts transfer across models is crucial for the offsite tuning approach.
  - Quick check question: What factors determine whether a prompt generated on one model will work well on another model?

## Architecture Onboarding

- Component map: Private data → Local LLM → DP Ensemble Generation → Prompt Selection → Cloud Model → Output
- Critical path: Local LLM generates prompts using private data, which are then transferred to cloud models for inference
- Design tradeoffs:
  - Privacy vs Utility: Tighter privacy budgets reduce leakage but may degrade prompt quality
  - Ensemble Size vs Efficiency: Larger ensembles improve quality but increase computation
  - Token Generation vs Privacy Budget: More tokens consume more privacy budget
- Failure signatures:
  - Prompts fail to generate (privacy budget exhausted)
  - Generated prompts contain private information (insufficient DP noise)
  - Poor transfer performance (prompts too model-specific)
- First 3 experiments:
  1. Generate prompts using DP-OPT on SST-2 task and evaluate transfer to DaVinci-003
  2. Compare DP-OPT prompts against non-private OPT and DLN-1 on multiple tasks
  3. Vary privacy budget (ε) and measure privacy-utility tradeoff curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DP-OPT scale with increasing model sizes, and what is the relationship between model size and privacy-utility tradeoff?
- Basis in paper: Inferred from the discussion of privacy-utility trade-off and the observation that larger models present stronger zero-shot ability.
- Why unresolved: The paper provides some empirical evidence on the impact of model size on performance, but a more comprehensive analysis of the scaling behavior and the relationship between model size and privacy-utility tradeoff is needed.
- What evidence would resolve it: A systematic study varying model sizes and analyzing the performance and privacy guarantees of DP-OPT across different scales would provide insights into the scaling behavior and the relationship between model size and privacy-utility tradeoff.

### Open Question 2
- Question: How effective is DP-OPT in protecting against membership inference attacks when the adversary has access to auxiliary information or uses more sophisticated attack techniques?
- Basis in paper: Inferred from the evaluation of privacy risks using membership inference attacks (MIA) and the observation of non-trivial AUCs for some methods.
- Why unresolved: The paper evaluates the effectiveness of DP-OPT against a specific attack (Likelihood Ratio test) but does not explore the impact of auxiliary information or more advanced attack techniques.
- What evidence would resolve it: Evaluating the performance of DP-OPT against various membership inference attacks with different levels of auxiliary information and attack sophistication would provide a more comprehensive understanding of its privacy protection capabilities.

### Open Question 3
- Question: Can DP-OPT be extended to support multi-task learning and handle scenarios where the training data is imbalanced across different tasks?
- Basis in paper: Inferred from the focus on sentiment classification tasks and the potential for applying DP-OPT to other natural language processing tasks.
- Why unresolved: The paper demonstrates the effectiveness of DP-OPT on sentiment classification tasks but does not explore its applicability to multi-task learning or imbalanced data scenarios.
- What evidence would resolve it: Conducting experiments on multi-task learning scenarios and evaluating the performance of DP-OPT on imbalanced datasets would provide insights into its generalizability and effectiveness in handling diverse and imbalanced data.

### Open Question 4
- Question: How does the choice of the local LLM affect the performance of DP-OPT, and are there specific characteristics or architectures that are more suitable for prompt generation?
- Basis in paper: Inferred from the use of Vicuna-7b as the local model and the potential impact of different LLMs on prompt quality and transferability.
- Why unresolved: The paper uses a specific local LLM (Vicuna-7b) but does not explore the impact of different LLMs on the performance of DP-OPT or identify the characteristics that make certain models more suitable for prompt generation.
- What evidence would resolve it: Conducting experiments with different local LLMs and analyzing their impact on the performance and transferability of prompts generated by DP-OPT would provide insights into the relationship between the choice of the local LLM and the effectiveness of the method.

### Open Question 5
- Question: How does the performance of DP-OPT compare to other privacy-preserving methods for prompt tuning, such as soft prompt tuning with differential privacy or text sanitization techniques?
- Basis in paper: Inferred from the comparison with soft prompt tuning (PromptDPSGD) and the discussion of text sanitization methods.
- Why unresolved: The paper compares DP-OPT with soft prompt tuning and text sanitization methods but does not provide a comprehensive evaluation of its performance relative to other privacy-preserving approaches for prompt tuning.
- What evidence would resolve it: Conducting a comparative study of DP-OPT with other privacy-preserving methods for prompt tuning, including soft prompt tuning with differential privacy and various text sanitization techniques, would provide insights into its relative effectiveness and advantages.

## Limitations

- The paper lacks rigorous privacy budget composition analysis across the ensemble generation and prompt selection phases
- Transferability claims across different model architectures are supported by limited empirical evidence
- The evaluation scope is narrow, focusing primarily on sentiment analysis and natural language inference tasks

## Confidence

- High Confidence: The core methodology of using local LLMs with DP mechanisms for prompt generation is technically sound and follows established DP principles
- Medium Confidence: The privacy guarantees claimed, while theoretically plausible, require more rigorous verification
- Low Confidence: The transferability claims across different model architectures lack sufficient empirical support

## Next Checks

1. **Privacy Budget Composition Analysis**: Perform a detailed privacy accounting that properly composes the privacy losses from the ensemble generation phase and the prompt selection phase. Use advanced composition theorems to verify that the claimed (ε, δ) guarantees actually hold across the full pipeline.

2. **Transferability Stress Test**: Systematically vary the model architectures (e.g., test transfer from Vicuna-7b to both larger and smaller models, different architectural families) and measure transfer performance across a broader range of tasks. Include qualitative analysis of why certain transfers succeed or fail.

3. **Privacy Leakage Detection**: Implement automated tests to detect potential privacy leakage in generated prompts by checking if prompts contain n-grams or patterns that appear in the training data but not in the validation/test sets. This would provide empirical validation of the privacy guarantees.