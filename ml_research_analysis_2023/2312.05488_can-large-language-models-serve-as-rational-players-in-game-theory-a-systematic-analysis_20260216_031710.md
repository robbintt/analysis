---
ver: rpa2
title: Can Large Language Models Serve as Rational Players in Game Theory? A Systematic
  Analysis
arxiv_id: '2312.05488'
source_url: https://arxiv.org/abs/2312.05488
tags:
- llms
- game
- belief
- theory
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a systematic analysis of Large Language Models
  (LLMs) in the context of game theory. The core idea is to evaluate whether LLMs
  can act as rational players, based on three fundamental characteristics: building
  a clear desire, refining belief about uncertainty, and taking optimal actions.'
---

# Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis

## Quick Facts
- arXiv ID: 2312.05488
- Source URL: https://arxiv.org/abs/2312.05488
- Reference count: 2
- Large Language Models (LLMs) exhibit substantial disparities compared to humans in game theory, struggling with desire formation from uncommon preferences, belief refinement from simple patterns, and optimal action selection.

## Executive Summary
This paper systematically evaluates whether Large Language Models can act as rational players in game theory by examining three core characteristics: building clear desires, refining beliefs about uncertainty, and taking optimal actions. The authors test GPT-3, GPT-3.5, and GPT-4 across three classical games - dictator game, Rock-Paper-Scissors, and ring-network game. The results reveal that even state-of-the-art LLMs like GPT-4 perform significantly below human levels, particularly struggling with uncommon preferences, simple pattern recognition, and autonomous belief-action integration. The study concludes that caution is warranted when introducing LLMs into social science game experiments.

## Method Summary
The study evaluates LLM performance across three classical games using textual prompts. In the dictator game, LLMs are assigned different preference types (Equality, Common-Interest, Self-Interest, Altruism) and their choices are evaluated for preference consistency. Rock-Paper-Scissors experiments measure average payoff against opponents with various patterns (constant, loop, copy, counter). The ring-network game tests belief-action integration using three prompt forms: implicit reasoning, explicit decomposition, and given beliefs. Three LLMs (GPT-3, GPT-3.5, GPT-4) are tested across these scenarios with performance measured through accuracy metrics and payoff calculations.

## Key Results
- LLMs show near-perfect accuracy on common preferences (Equality, Common-Interest) but significant errors on uncommon preferences (Altruism, Self-Interest)
- GPT-4 demonstrates rising confidence but suboptimal performance in Rock-Paper-Scissors pattern recognition
- Explicit decomposition of belief and action steps significantly improves LLM performance compared to implicit reasoning approaches

## Why This Works (Mechanism)

### Mechanism 1
- LLMs exhibit distinct behavior in building desire based on preference type
- Mechanism: LLMs can map textual prompts to preference-consistent choices for common preferences but struggle with uncommon preferences due to mathematical confusion or preference misinterpretation
- Core assumption: The mapping between textual preference prompts and decision-making is consistent and testable
- Evidence anchors: Abstract finding that LLMs struggle with uncommon preferences; section 4.1 experimental accuracy table showing performance disparities; weak corpus support
- Break condition: More explicit prompts or fine-tuning on uncommon preference scenarios may improve performance

### Mechanism 2
- LLMs cannot reliably refine belief from simple opponent patterns
- Mechanism: LLMs fail to detect and exploit statistical regularities in opponent behavior, indicating limitations in belief formation
- Core assumption: Belief refinement is necessary for optimal play in games with non-random opponents
- Evidence anchors: Abstract finding about failing to refine belief from simple patterns; section 4.2 showing GPT-4's suboptimal performance; weak corpus support
- Break condition: Explicit decomposition of reasoning steps may improve performance

### Mechanism 3
- LLMs benefit from explicit decomposition of decision-making steps
- Mechanism: When game process is broken into explicit sub-tasks (belief refinement followed by action selection), performance improves significantly
- Core assumption: LLMs cannot autonomously follow the implicit human decision-making flow in game theory
- Evidence anchors: Abstract finding about overlooking refined belief; section 4.3 showing explicit form yields higher accuracy; weak corpus support
- Break condition: Further structured prompts or reinforced intermediate reasoning may yield additional improvements

## Foundational Learning

- **Game theory as a framework for modeling strategic decision-making**
  - Why needed here: Provides theoretical foundation for evaluating LLM rationality across desire, belief, and action
  - Quick check question: What are the three core components of a rational player in game theory according to this paper?

- **Belief refinement and belief distribution**
  - Why needed here: Central to understanding how LLMs process game information and predict opponent actions
  - Quick check question: How does the paper define belief in the context of game theory?

- **Preference types and their impact on decision-making**
  - Why needed here: Explains why LLMs perform differently on common vs. uncommon preferences
  - Quick check question: Which two preferences are most common in game theory, and which is rarely observed?

## Architecture Onboarding

- **Component map**: Game selection (dictator, RPS, ring-network) → Prompt design → LLM execution → Performance evaluation → Analysis
- **Critical path**: Prompt design → LLM execution → Performance evaluation → Analysis → Insight generation
- **Design tradeoffs**: Simple games enable controlled testing but may not capture full complexity of real-world strategic interaction
- **Failure signatures**: Inability to build desire from uncommon preferences, failure to detect simple patterns, overlooking refined belief in action selection
- **First 3 experiments**:
  1. Replicate dictator game with additional preference types to test mathematical ability
  2. Test belief refinement on progressively complex patterns beyond those in the paper
  3. Compare explicit vs. implicit reasoning in a new game with different strategic structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the mathematical ability of LLMs be improved when assigned uncommon preferences in games like the dictator game?
- Basis in paper: The paper found that LLMs struggle to build desires from uncommon preferences, with GPT-3 showing decreased mathematical ability when assigned the altruism preference
- Why unresolved: The paper identifies this as a limitation but does not explore potential solutions or training methods
- What evidence would resolve it: Experiments demonstrating improved performance on uncommon preference tasks after targeted training or architectural modifications

### Open Question 2
- Question: Can LLMs be trained to autonomously follow human-like game processes without explicit decomposition of belief and action?
- Basis in paper: The paper found that LLMs cannot autonomously follow the ideal game process in the ring-network game
- Why unresolved: While the paper suggests explicit decomposition as a solution, it does not explore whether LLMs can be trained to follow human-like processes without this explicit guidance
- What evidence would resolve it: Experiments showing LLMs can successfully navigate complex game scenarios without explicit decomposition, matching or exceeding human performance

### Open Question 3
- Question: What are the fundamental limitations of LLMs in refining belief from complex patterns in games?
- Basis in paper: The paper found that LLMs struggle to refine belief from various patterns in Rock-Paper-Scissors
- Why unresolved: The paper identifies this as a limitation but does not explore the underlying reasons or potential solutions
- What evidence would resolve it: Analysis of LLM internal representations and reasoning processes during belief refinement tasks, identifying specific bottlenecks or limitations

## Limitations

- Experiments rely on text-based prompts which may not fully capture human strategic reasoning richness
- Selected games, while classical, may not represent the full spectrum of real-world strategic interactions
- Study does not account for potential fine-tuning or adaptation of LLMs to specific game-theoretic contexts

## Confidence

- **High confidence**: Observation that LLMs exhibit distinct behavior based on preference type, supported by experimental data
- **Medium confidence**: Assertion that LLMs struggle with belief refinement from simple patterns, based on limited game scenarios
- **Medium confidence**: Claim that explicit decomposition of decision-making steps improves LLM performance, given controlled experimental conditions

## Next Checks

1. Replicate dictator game experiments with broader range of preference types and more nuanced prompt designs to further test LLMs' mathematical abilities and preference interpretation
2. Extend Rock-Paper-Scissors experiments to include more complex and dynamic patterns, assessing LLMs' ability to refine beliefs in increasingly challenging environments
3. Conduct experiments in new game settings with different strategic structures to evaluate the generalizability of the explicit decomposition benefit observed in the ring-network game