---
ver: rpa2
title: Empowering Cross-lingual Behavioral Testing of NLP Models with Typological
  Features
arxiv_id: '2307.05454'
source_url: https://arxiv.org/abs/2307.05454
tags:
- language
- languages
- tests
- features
- palm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M2C, a morphologically-aware framework for
  cross-lingual behavioral testing of NLP models. M2C generates tests probing model
  behavior on specific linguistic features across 12 typologically diverse languages.
---

# Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features

## Quick Facts
- arXiv ID: 2307.05454
- Source URL: https://arxiv.org/abs/2307.05454
- Authors: 
- Reference count: 19
- Key outcome: M2C framework reveals model weaknesses in cross-lingual generalization, particularly on morphological features like time expressions in Swahili and compounding possessives in Finnish

## Executive Summary
This paper introduces M2C, a morphologically-aware framework for behavioral testing of NLP models across 12 typologically diverse languages. The framework generates tests that probe model behavior on specific linguistic features using template-based generation with morphological placeholders. Experiments reveal that while models perform well on English, they struggle with language-specific features like time expressions in Swahili and compounding possessives in Finnish. The results highlight the need for more robust multilingual models that can generalize to diverse linguistic characteristics beyond Indo-European languages.

## Method Summary
M2C uses template-based generation with morphological placeholders to create controlled tests for specific linguistic features. The framework integrates UnimorphInflect for inflection generation across 55 languages, then relies on manual verification for quality control. Tests probe six linguistic feature categories (negation, numerals, spatial/temporal expressions, comparatives, and language-specific features) across 12 diverse languages. Five pre-trained language models are evaluated in both zero-shot and one-shot settings using question-answering format.

## Key Results
- Models show high accuracy on English tests but struggle with language-specific features in morphologically rich languages
- Time expressions in Swahili and compounding possessives in Finnish represent significant challenges for current models
- One-shot evaluation improves results across all models, with larger improvements for smaller models
- Morphological errors persist even in large models, particularly in languages with complex inflectional systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: M2C enables systematic probing of morphological features by generating controlled template-based tests with explicit morphological feature dependencies
- Mechanism: The framework allows users to specify morphological feature dependencies between placeholders using a syntax like {X.<Y.D>} where dimension D from placeholder Y is applied to placeholder X, enabling agreement constraints to be enforced during test generation
- Core assumption: Morphological feature dependencies can be adequately represented using UniMorph Schema dimensions and custom dimensions for language-specific exceptions
- Evidence anchors:
  - [abstract]: "We propose M2C, a morphologically-aware framework for behavioral testing of NLP models."
  - [section]: "We introduce a syntax describing the morphological dependence between placeholders: {X.<Y.D>} signifies that X should have the same feature for dimensionD asY."
  - [corpus]: Weak - no direct mention of morphological feature dependency representation in related papers

### Mechanism 2
- Claim: M2C bridges the gap between simple template expansion and full generative approaches by providing semi-automated inflection generation with manual verification
- Mechanism: The framework integrates UnimorphInflect for generating inflections across 55 languages, then relies on manual inspection for quality control, balancing scalability with reliability for languages with good model performance
- Core assumption: Semi-automated inflection generation with manual verification provides sufficient quality for behavioral testing while being more efficient than fully manual generation
- Evidence anchors:
  - [section]: "We integrate UnimorphInflect... As Unimorph models are imperfect... we envision a workflow where inflections are generated at scale using UnimorphInflect and then manually inspected by annotators for correctness."
  - [corpus]: Weak - related work focuses on typological representations but doesn't address the inflection generation workflow

### Mechanism 3
- Claim: M2C reveals model weaknesses in cross-lingual generalization by testing specific linguistic features that differ across languages, rather than general task performance
- Mechanism: By creating tests that isolate specific linguistic features (negation, numerals, spatial expressions, temporal expressions, comparatives, and language-specific features), M2C can identify where models fail to generalize beyond Indo-European language patterns
- Core assumption: Specific linguistic features can be isolated and tested independently to reveal model weaknesses in cross-lingual generalization
- Evidence anchors:
  - [abstract]: "While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finnish."
  - [section]: "We design tests that probe model capabilities in light of practically relevant typological differences."
  - [corpus]: Weak - related papers discuss typological features but don't present frameworks for isolating and testing specific features

## Foundational Learning

- Concept: Morphological agreement and feature dependencies
  - Why needed here: Understanding how morphological features like gender, number, and case interact across different parts of speech is essential for creating templates that test these dependencies in NLP models
  - Quick check question: How would you ensure that an adjective agrees with a noun in gender and number in a template for Spanish?

- Concept: Cross-lingual typological variation
  - Why needed here: Recognizing how linguistic features are instantiated differently across languages (e.g., negation particles vs. verb morphology) is crucial for designing tests that probe model understanding of these variations
  - Quick check question: What are the key differences in how negation is expressed in English versus Swahili, and why would this matter for testing NLP models?

- Concept: Behavioral testing methodology
  - Why needed here: Understanding how controlled template-based testing differs from standard benchmark evaluation helps in designing effective tests that isolate specific capabilities
  - Quick check question: How does testing negation in context versus in the question structure help reveal different aspects of model understanding?

## Architecture Onboarding

- Component map:
  - Template engine -> Inflection generator -> Test validator -> Configuration manager -> Output formatter

- Critical path:
  1. User creates template with morphological placeholders
  2. Template engine processes dependencies and generates test cases
  3. Inflection generator provides morphological forms (manual verification step)
  4. Tests are executed on target model
  5. Validator checks predictions for correctness and morphological accuracy

- Design tradeoffs:
  - Flexibility vs. complexity: Rich templating syntax enables coverage of diverse phenomena but increases learning curve
  - Automation vs. accuracy: Semi-automated inflection generation balances scalability with quality control
  - Generality vs. specificity: Framework supports many languages but requires language-specific knowledge for template creation

- Failure signatures:
  - High morphological error rates suggest inflection generation or agreement logic issues
  - Unexpected test failures may indicate template design problems or model understanding gaps
  - Performance discrepancies across languages reveal generalization weaknesses

- First 3 experiments:
  1. Create a simple negation template in English and Spanish to verify template processing and inflection generation
  2. Design a spatial expression test that requires case agreement in Russian to test morphological dependency handling
  3. Build a numeral reasoning test that requires noun inflection agreement in Slovak to validate complex morphological generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the models' morphological inflection errors correlate with the specific morphological dimensions (e.g., case, number, gender) across different languages?
- Basis in paper: [explicit] The paper mentions that PaLM-L makes morphological errors in Slovak (accents and suffixes), Russian (inflection of numerals), and Finnish (case and number agreement)
- Why unresolved: The paper provides examples but doesn't systematically analyze which morphological dimensions cause the most errors across languages
- What evidence would resolve it: A detailed breakdown of error types by morphological dimension for each language, showing which dimensions (e.g., case agreement, number agreement, gender agreement) are most problematic for the models

### Open Question 2
- Question: Does the model scale (size) relationship with morphological accuracy follow a predictable pattern across languages with different morphological complexity?
- Basis in paper: [inferred] The paper shows that larger models (PaLM-L, PaLM 2) make fewer morphological errors than smaller ones (mT5-XXL, PaLM-S), but doesn't analyze if this relationship varies by language complexity
- Why unresolved: The paper shows overall improvements with scale but doesn't test if morphologically complex languages (e.g., Finnish, Russian) show different scaling patterns compared to less complex ones
- What evidence would resolve it: Comparative analysis of error reduction rates across models of different sizes for languages with varying morphological complexity (e.g., comparing error reduction in Finnish vs. English)

### Open Question 3
- Question: How does the performance gap between zero-shot and one-shot settings vary across different types of linguistic features and languages?
- Basis in paper: [explicit] The paper mentions that one-shot evaluation generally improves results and that improvements are larger for smaller models, but doesn't provide detailed analysis by feature type or language
- Why unresolved: The paper shows aggregate improvements but doesn't analyze whether certain linguistic features (e.g., negation, numerals, spatial expressions) or certain language families benefit more from one-shot settings
- What evidence would resolve it: Detailed breakdown of zero-shot vs one-shot performance improvements by linguistic feature type and by language family, showing which combinations benefit most from exemplars

## Limitations
- Manual verification requirement creates scalability bottleneck for morphological inflection generation
- Limited to 12 languages with uneven representation across language families
- Claims about cross-lingual generalization may not hold with broader typological coverage

## Confidence
- High confidence: The framework's core templating mechanism and morphological dependency representation work as described
- Medium confidence: The empirical findings about model weaknesses on specific linguistic features are supported by the experimental results
- Low confidence: The claim that M2C enables systematic cross-lingual behavioral testing at scale is questionable given the manual verification bottleneck and limited language coverage

## Next Checks
1. Measure the time and resources required to manually verify morphological inflections for a new language with complex morphology (e.g., Turkish or Japanese) to quantify the manual verification bottleneck
2. Implement and test the framework on 3-5 additional non-Indo-European languages to assess whether the observed generalization failures hold across broader typological diversity
3. Replicate the morphological error analysis for a subset of tests on a different language model (e.g., XGLM or BLOOM) to verify whether the identified failure patterns are consistent across model architectures