---
ver: rpa2
title: 'Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio'
arxiv_id: '2311.06285'
source_url: https://arxiv.org/abs/2311.06285
tags:
- latexit
- sound
- audio
- body
- sha1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to generate accurate 3D spatial audio
  from head-mounted microphones and body pose data. The system uses an audio encoder,
  pose encoder, and audio decoder to predict the sound field surrounding a human body.
---

# Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio

## Quick Facts
- arXiv ID: 2311.06285
- Source URL: https://arxiv.org/abs/2311.06285
- Reference count: 29
- Primary result: Method generates accurate 3D spatial audio from head-mounted microphones and body pose, outperforming baselines on sound source localization and audio quality.

## Executive Summary
This paper presents a method to generate accurate 3D spatial audio for human bodies using head-mounted microphones and body pose data. The system employs an audio encoder, pose encoder, and audio decoder to predict the sound field surrounding a human body. Trained on a novel multimodal dataset with 345-channel microphone recordings and body pose data from 8 participants, the method demonstrates that incorporating full body pose significantly improves sound source localization and audio quality compared to using no pose or head pose only. The proposed shift-ℓ2 loss function enables accurate sound field reconstruction, outperforming traditional losses like ℓ2 and multiscale STFT.

## Method Summary
The method uses a multi-modal network with audio and pose encoders feeding into an audio decoder to generate spatial audio. Time-warping is applied to audio inputs based on geometric propagation delays from body joints to microphones. The model predicts audio signals at discrete microphone positions on a sphere and encodes them into spherical harmonics for sound field rendering. Training uses a custom shift-ℓ2 loss combined with multiscale STFT loss. The approach is evaluated on speech and non-speech body sounds using signal-to-distortion ratio, amplitude error, and phase error metrics.

## Key Results
- Full body pose information significantly improves sound source localization compared to no pose or head pose only.
- The shift-ℓ2 loss function outperforms traditional ℓ2 and STFT losses for reconstructing spatially aligned body sounds.
- The method achieves high signal-to-distortion ratios and low phase errors on both speech and non-speech audio.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-warping audio inputs according to potential sound source locations significantly improves spatial audio reconstruction accuracy.
- Mechanism: By simulating the geometric propagation delay from each of six key body joints to the head-mounted microphones, the model receives audio features that are aligned to the spatial origins of the sounds. This alignment reduces the burden on the network to learn temporal shifts and enables better separation of sound sources based on body pose.
- Core assumption: The sound source location is highly correlated with specific body joints, and warping the audio signal to simulate emission from each of these joints will provide the model with disambiguated spatial cues.
- Evidence anchors:
  - [section]: "As sound might emerge from various locations on the body, we create a copy of the input signal for multiple body joints and shift the input as if it originated from each of these joints."
  - [corpus]: Weak evidence; corpus neighbors do not discuss time-warping or spatial alignment strategies.
- Break condition: If the correlation between body joint positions and sound emission locations weakens (e.g., due to atypical body movements or occlusion), the time-warping alignment becomes less effective.

### Mechanism 2
- Claim: The shift-ℓ2 loss function outperforms traditional ℓ2 and STFT losses for reconstructing spatially aligned body sounds.
- Mechanism: The shift-ℓ2 loss computes the ℓ2 error over sliding windows with a window function that penalizes larger offsets more heavily. This design tolerates small temporal misalignments that naturally occur in body sounds (especially impulsive sounds like claps), while still penalizing larger errors. This results in better preservation of impulsive sound energy and temporal structure compared to standard ℓ2 or STFT losses.
- Core assumption: Small shifts in impulsive sound signals cause disproportionately large ℓ2 errors, and this mismatch between alignment tolerance and loss sensitivity can be corrected by introducing a sliding-window penalty scheme.
- Evidence anchors:
  - [section]: "We propose a variation of ℓ2 that is more forgiving to small shift offsets... The loss, referred here a shift-ℓ2, is computed over sliding windows with higher penalty applied to higher offsets."
  - [section]: "Optimizing for correct spatialization of body sounds is challenging due to both the nature of the task... To avoid speech distortions, a loss on the amplitude spectrogram is generally preferred. On the other hand, such loss is unable to achieve accurate temporal alignment."
- Break condition: If the dataset contains sounds with extremely long delays or highly non-linear propagation paths, the sliding window approach may still fail to align signals accurately.

### Mechanism 3
- Claim: Full body pose information provides critical spatial cues for localizing sound sources beyond what headpose alone can offer.
- Mechanism: Full body pose encodes the 3D positions of all major joints, enabling the model to disambiguate the origin of sounds across the entire body (e.g., left hand vs. right hand). This spatial information is essential for correctly rendering the sound field around the body. Headpose alone provides limited spatial cues and often results in the model incorrectly localizing all sounds to the head region.
- Core assumption: Sound source localization is significantly improved when the model has access to the full 3D joint positions of the body, as opposed to only head orientation.
- Evidence anchors:
  - [section]: "The ablation of pose information is studied... our model with full body pose outperforms variants with no pose or headpose only on all three metrics because full body pose provides important position information of sound sources."
  - [section]: "With only headpose, the model puts all sound sources in the head region where speech would be produced. Finally, when provided with full body pose, the model is able to correctly locate the snap at the right hand location above the head."
- Break condition: If the body pose data becomes noisy or occluded (e.g., due to clothing or body part overlap), the benefit of full body pose information diminishes.

## Foundational Learning

- Concept: Spherical harmonics and sound field encoding/decoding
  - Why needed here: The model predicts audio signals at discrete microphone positions on a sphere, but the goal is to render sound at arbitrary 3D positions. Spherical harmonics provide a compact representation of the 3D sound field that can be efficiently transformed into signals at any point.
  - Quick check question: What is the maximum harmonic order K that can be encoded given N microphones, and why is this limit imposed?

- Concept: Time-warping for geometric audio alignment
  - Why needed here: Due to the finite speed of sound, there is a time delay between when a sound is emitted from a body joint and when it is recorded by head-mounted microphones or target microphones on the sphere. Without alignment, the model struggles to learn these delays and spatialize sounds correctly.
  - Quick check question: How does the model determine the appropriate time delay for a given sound source and microphone pair?

- Concept: Multimodal fusion of audio and pose data
  - Why needed here: Neither audio nor pose alone contains sufficient information to reconstruct the full 3D sound field around a human body. Audio provides information about the sound characteristics and timing, while pose provides spatial context for sound emission. Effective fusion of these modalities is critical for accurate spatial audio generation.
  - Quick check question: What are the key challenges in fusing audio and pose data in this context, and how does the proposed architecture address them?

## Architecture Onboarding

- Component map: Input audio → Time-warping → Audio encoder → Audio decoder (conditioned on pose and target mic position) → Output audio → Spherical harmonics encoding → Sound field rendering
- Critical path: Input audio → Time-warping → Audio encoder → Audio decoder (conditioned on pose and target mic position) → Output audio → Spherical harmonics encoding → Sound field rendering
- Design tradeoffs:
  - Using time-warped audio signals increases input dimensionality but provides crucial spatial alignment cues.
  - The shift-ℓ2 loss is more tolerant of small misalignments but may still struggle with large errors or non-linear propagation.
  - Modeling high-order spherical harmonics (up to 17th order) improves spatial resolution but increases computational complexity.
- Failure signatures:
  - Underestimation of sound field energy (likely due to loss function or insufficient model capacity)
  - Incorrect sound source localization (likely due to inadequate pose information or noisy pose data)
  - Temporal misalignment in the output audio (likely due to issues with time-warping or the shift-ℓ2 loss)
- First 3 experiments:
  1. Train the model without time-warping to assess the impact of geometric audio alignment on spatial audio reconstruction accuracy.
  2. Replace the shift-ℓ2 loss with standard ℓ2 or STFT losses to evaluate the importance of the proposed loss function for handling impulsive body sounds.
  3. Train the model with only headpose information to quantify the benefit of full body pose for sound source localization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed shift-ℓ2 loss function perform compared to other time-alignment-sensitive losses like dynamic time warping (DTW) or soft-DTW?
- Basis in paper: [inferred] The paper proposes shift-ℓ2 as a more forgiving alternative to ℓ2 loss for handling temporal misalignment in body sounds, but does not compare it to other alignment methods.
- Why unresolved: The paper only compares shift-ℓ2 to ℓ2 and multiscale STFT losses, leaving open the question of whether other alignment-aware losses could perform better.
- What evidence would resolve it: Experiments comparing shift-ℓ2 to DTW-based or soft-DTW losses on the same dataset and metrics would clarify its relative performance.

### Open Question 2
- Question: How would incorporating environmental acoustics (e.g., room reverberation) into the model affect the quality of spatial audio rendering?
- Basis in paper: [explicit] The paper mentions that the dataset was collected in an anechoic chamber and that handling real-world noise and reverberation is outside the scope of this work.
- Why unresolved: The paper does not explore how the model performs with reverberant audio input, which is more realistic for AR/VR applications.
- What evidence would resolve it: Testing the model on a dataset with room reverberation and comparing the results to the anechoic data would show the impact of environmental acoustics.

### Open Question 3
- Question: Can the model generalize to other body types and clothing materials not represented in the dataset?
- Basis in paper: [explicit] The dataset includes 8 participants with different body types and clothing, but it is not clear if this is sufficient for generalization.
- Why unresolved: The paper does not perform cross-participant or cross-clothing material evaluations to test generalization.
- What evidence would resolve it: Evaluating the model on a held-out participant or with clothing materials not seen during training would indicate its ability to generalize.

## Limitations
- Dataset Dependency: The model's performance heavily relies on the novel multimodal dataset collected in an anechoic chamber, which may constrain generalization to diverse body types, clothing, and environmental conditions.
- Pose Estimation Accuracy: The method assumes accurate body pose estimation from OpenPose, but occlusions, fast movements, and clothing can degrade pose accuracy, directly impacting sound source localization.
- Computational Complexity: The model uses high-order spherical harmonics (up to 17th order) and time-warping across multiple body joints, increasing computational demands without discussion of real-time inference capabilities.

## Confidence
- High Confidence: The claim that full body pose significantly improves sound source localization compared to no pose or head pose only.
- Medium Confidence: The claim that the shift-ℓ2 loss function outperforms traditional ℓ2 and STFT losses for impulsive body sounds.
- Low Confidence: The claim that time-warping audio according to body joint positions is the primary mechanism for improved spatial audio reconstruction.

## Next Checks
1. **Real-world Generalization**: Test the model on audio recorded in reverberant environments with background noise to assess robustness beyond the anechoic chamber dataset.
2. **Pose Error Sensitivity**: Evaluate model performance with artificially degraded pose data (varying noise levels, missing joints) to quantify the impact of pose estimation errors on spatial audio quality.
3. **Real-time Performance**: Measure inference latency and computational requirements for different spherical harmonic orders to determine the feasibility of real-time spatial audio generation for AR/VR applications.