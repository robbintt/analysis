---
ver: rpa2
title: Label Denoising through Cross-Model Agreement
arxiv_id: '2308.13976'
source_url: https://arxiv.org/abs/2308.13976
tags:
- deca
- learning
- recommendation
- denoising
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called DeCA for denoising labels in
  both binary and multi-class scenarios. The key idea is that different models make
  more consistent predictions on clean examples than on noisy ones.
---

# Label Denoising through Cross-Model Agreement

## Quick Facts
- arXiv ID: 2308.13976
- Source URL: https://arxiv.org/abs/2308.13976
- Reference count: 40
- Key outcome: DeCA improves recommendation Recall/NDCG by up to 15% and image classification accuracy by up to 10% on noisy labels

## Executive Summary
This paper introduces DeCA, a label denoising method that leverages cross-model agreement to improve learning from noisy labels in both binary (recommendation) and multi-class (image classification) scenarios. The core insight is that different models make more consistent predictions on clean examples than on noisy ones. DeCA minimizes the KL-divergence between predictions from two models while maximizing the likelihood of data observation, effectively using model disagreement as a signal to identify and correct noisy labels. Experimental results show significant improvements over normal training and other denoising methods across multiple datasets and model architectures.

## Method Summary
DeCA works by training two models simultaneously on the same noisy dataset, encouraging them to agree on their predictions while maintaining individual model capacity. The loss function combines the likelihood of observed noisy labels with KL-divergence between the two models' predicted distributions. The method can be instantiated with various model pairs (MF+GMF, MF+NeuMF, MF+LightGCN for recommendation; ResNet32 for image classification) and includes an extension (DeCA(p)) that uses a pre-trained model as a fixed prior. An iterative training routine with class-specific hyperparameters (C1, C2) further refines the denoising process by adjusting the weight of clean versus noisy examples during training.

## Key Results
- Recommendation tasks: Up to 15% improvement in Recall and NDCG metrics over normal training
- Image classification tasks: Up to 10% improvement in accuracy on CIFAR-10 and Fashion-MNIST with synthetic noise
- DeCA outperforms other denoising methods including Forward Loss Correction, M-correction, and DivideMix
- Consistent improvements across multiple model architectures and noise levels (up to 40% label corruption)

## Why This Works (Mechanism)

### Mechanism 1
Models trained independently on the same noisy dataset produce more consistent predictions on clean examples than on noisy ones. When both models encounter a clean example, their learned representations align toward the true label, producing similar outputs. On noisy examples, each model's errors diverge, leading to dissimilar predictions. Different models capture different parts of the noise distribution, while the clean signal is shared and thus robustly learned by all.

### Mechanism 2
Minimizing KL-divergence between two model's predicted label distributions acts as a denoising signal. By encouraging two models to agree, we implicitly push both models toward the shared clean signal while letting them disagree on noisy examples. This disagreement is penalized, forcing the models to align on the robust component of the data. The true label distribution is the common signal both models should converge to, and noisy labels introduce model-specific errors.

### Mechanism 3
Pre-training one model on clean data and fixing it as a prior helps the other model avoid overfitting to noise. The pre-trained model acts as a stable prior distribution over labels, guiding the learning of the target model away from noise by providing a clean reference. A model trained with different random seeds captures the underlying clean label distribution more robustly than a model trained on noisy data.

## Foundational Learning

- **Concept: Variational Auto-Encoder (VAE) objective**
  - Why needed here: The DeCA(p) loss function is exactly the ELBO of a VAE where the true label is the latent variable
  - Quick check question: What are the encoder, decoder, and prior in the DeCA(p) formulation?

- **Concept: Kullback-Leibler (KL) divergence**
  - Why needed here: KL-divergence measures the difference between two probability distributions and is used to encourage agreement between models
  - Quick check question: Why do we need to compute both ùê∑ [ùëÉùëî || ùëÉùëì ] and ùê∑ [ùëÉùëì || ùëÉùëî] in the loss?

- **Concept: Implicit feedback and negative sampling**
  - Why needed here: The recommendation experiments use implicit feedback where only positive interactions are observed, and negatives are sampled from missing interactions
  - Quick check question: How does the noise in implicit feedback differ from label flipping in image classification?

## Architecture Onboarding

- **Component map**: Target model (ùëìùúÉ) -> Auxiliary model (ùëîùúá or ùëìùúÉ ‚Ä≤) -> ‚Ñé-models (parameterize noise process)

- **Critical path**:
  1. Sample a batch of (user, item) or (image, label) pairs
  2. Compute predictions from both models
  3. Compute KL-divergence between their predicted distributions
  4. Compute likelihood of observed noisy labels given predicted true labels
  5. Backpropagate combined loss to update target model parameters

- **Design tradeoffs**:
  - Using MF as auxiliary model: Simple and fast but may have limited capacity
  - Using same architecture as auxiliary: More expressive but increases computation
  - Co-training vs. pre-training: Co-training adapts both models but is slower; pre-training is faster but depends on quality of initial clean model

- **Failure signatures**:
  - If both models converge to the same poor local minimum, performance won't improve over normal training
  - If ‚Ñé-models are poorly initialized, the likelihood term may dominate incorrectly
  - If ùê∂1 and ùê∂2 hyperparameters are set too low, the iterative training may not focus properly on class-specific denoising

- **First 3 experiments**:
  1. Run DeCA with MF as both target and auxiliary model on MovieLens to verify basic functionality
  2. Compare DeCA vs. DeCA(p) on a small image classification dataset with synthetic noise
  3. Perform ablation by removing either DP or DN sub-tasks to measure their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed DeCA method perform compared to other denoising methods on datasets with different types of label noise beyond random noise? The paper mentions that DeCA can be applied to various kinds of noises but does not provide experimental results on datasets with specific types of label noise other than random noise.

### Open Question 2
How does the performance of DeCA change when applied to different recommendation models, such as deep learning-based models or models with different architectures? The paper demonstrates the effectiveness of DeCA on a specific set of recommendation models, but does not explore its performance on a broader range of models with different architectures or deep learning-based models.

### Open Question 3
How does the proposed DeCA method handle datasets with imbalanced class distributions or skewed label distributions? The paper does not explicitly discuss the handling of imbalanced class distributions or skewed label distributions, but it is a common challenge in real-world datasets.

## Limitations
- Lacks ablation studies isolating the contribution of KL-divergence versus likelihood terms
- No convergence analysis provided for the ‚Ñé-models that parameterize the noise process
- Assumes different model architectures capture noise differently, which may not hold for very similar architectures

## Confidence
- Mechanism 1 (Cross-model agreement on clean vs. noisy examples): High
- Mechanism 2 (KL-divergence as denoising signal): High
- Mechanism 3 (Pre-trained prior for denoising): Medium
- Overall performance claims: Medium (lacks statistical significance testing)

## Next Checks
1. Perform ablation study by removing either the DP or DN sub-tasks in the iterative training routine to measure their individual contributions
2. Systematically vary Œ± and C1/C2 hyperparameters to identify optimal settings and understand robustness
3. Monitor and report the training dynamics of ‚Ñé-models to verify they properly capture the noise distribution and don't diverge during training