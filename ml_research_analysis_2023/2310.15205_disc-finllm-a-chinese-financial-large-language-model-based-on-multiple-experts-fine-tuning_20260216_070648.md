---
ver: rpa2
title: 'DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts
  Fine-tuning'
arxiv_id: '2310.15205'
source_url: https://arxiv.org/abs/2310.15205
tags:
- financial
- dataset
- instructions
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present DISC-FinLLM, a Chinese financial large language model
  built using a multiple experts fine-tuning framework. Our approach improves general
  LLMs by endowing them with multi-turn question answering, domain text processing,
  mathematical computation, and retrieval-enhanced generation capabilities.
---

# DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning

## Quick Facts
- arXiv ID: 2310.15205
- Source URL: https://arxiv.org/abs/2310.15205
- Authors: 
- Reference count: 34
- Key outcome: Chinese financial LLM built using multiple experts fine-tuning framework, achieving 2-9 point improvement over baselines across six financial NLP tasks

## Executive Summary
DISC-FinLLM is a Chinese financial large language model developed using a Multiple Experts Fine-tuning Framework (MEFF). The model enhances general LLMs with specialized capabilities including multi-turn question answering, domain text processing, mathematical computation, and retrieval-enhanced generation. By training separate LoRA modules on distinct financial instruction categories, the framework enables specialization without catastrophic forgetting while maintaining the base model's general capabilities.

## Method Summary
The approach uses a base Baichuan-13B LLM fine-tuned with four separate LoRA modules, each trained on different aspects of financial instruction data: consulting, NLP tasks, computing, and retrieval-augmented generation. The training dataset DISC-FinSFT contains 246k samples across these four categories, constructed from diverse sources including existing NLP datasets, unlabeled financial texts, forum data, and computational problems. The model also incorporates four computational tools for numerical reasoning and is trained on 18k research reports and 69k news articles for knowledge retrieval.

## Key Results
- Average performance improvement of 2-9 points across six unseen financial NLP tasks compared to untrained base models
- Strong performance on financial computing tasks with tool invocation capabilities
- Effective specialization across four distinct financial capabilities through separate LoRA modules
- Successful integration of retrieval-augmented generation with knowledge base of 18k research reports and 69k news articles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple Experts Fine-tuning Framework enables specialization without catastrophic forgetting
- Mechanism: Training separate LoRA modules on disjoint financial instruction categories allows each module to specialize in distinct capabilities while sharing a common base LLM
- Core assumption: LoRA parameters are low-rank enough to capture task-specific patterns without interfering with each other when swapped
- Evidence anchors: [abstract] "train four individual Low-rank adaptation (LoRA) modules"; [section 4.1] "switching between different features simply involves replacing the LoRA parameters"

### Mechanism 2
- Claim: Instruction dataset diversity improves generalization across financial tasks
- Mechanism: Constructing DISC-FinLLM-SFT from multiple sources exposes the model to varied financial scenarios and language patterns
- Core assumption: The model can learn to map diverse instruction formats to appropriate responses through exposure to multiple task types
- Evidence anchors: [abstract] "instruction samples of four categories"; [section 3] Details construction from 10+ financial NLP datasets plus 1.8M unlabeled paragraphs

### Mechanism 3
- Claim: Plugin architecture enables precise numerical computation within financial texts
- Mechanism: Training on financial computing instructions that include tool call commands teaches the model when and how to invoke external computational tools
- Core assumption: The model learns to recognize computational contexts and generate correct tool invocation syntax
- Evidence anchors: [abstract] "mathematical computation skills" and "four tools, namely expression calculator, equation solver, counter, and probability table"; [section 3.3] "we construct calculation plugin instruction data"

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: Enables zero-shot task generalization by training on diverse instruction-response pairs
  - Quick check question: How does instruction tuning differ from standard supervised fine-tuning?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Allows efficient parameter-efficient fine-tuning while maintaining base model capabilities
  - Quick check question: What advantage does LoRA provide over full fine-tuning for multiple expert modules?

- Concept: Chain-of-Thought prompting
  - Why needed here: Helps the model generate step-by-step reasoning for complex financial calculations and analysis
  - Quick check question: How does CoT prompting improve performance on numerical reasoning tasks?

## Architecture Onboarding

- Component map: Base LLM (Baichuan-13B-Chat) → Four LoRA modules (consulting, task, computing, retrieval) → Plugin tools (4 calculators) → Knowledge base (18k research reports + 69k news articles)
- Critical path: Dataset construction → LoRA module training → Model evaluation → Deployment with module switching
- Design tradeoffs: Separate LoRA modules provide specialization but require module selection logic; full fine-tuning would be more integrated but computationally expensive
- Failure signatures: Poor performance on specific task types indicates LoRA module underfitting; module conflicts suggest parameter space overlap; tool invocation failures indicate computational plugin issues
- First 3 experiments:
  1. Train individual LoRA modules on their respective instruction subsets and evaluate each module separately
  2. Test module switching functionality by loading different LoRA weights and measuring performance consistency
  3. Evaluate computational plugin effectiveness by testing numerical reasoning tasks with and without tool invocation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DISC-FinLLM compare to other financial LLMs when evaluated on completely unseen task types beyond the financial domain?
- Basis in paper: [inferred] The paper mentions it is difficult to evaluate on completely unseen task types due to limited financial NLP datasets.
- Why unresolved: The paper focuses on evaluating DISC-FinLLM within the financial domain. Comparing its performance on non-financial tasks would provide insights into its generalizability.
- What evidence would resolve it: Evaluating DISC-FinLLM on a diverse set of non-financial NLP tasks and comparing its performance to other LLMs would provide evidence of its generalizability.

### Open Question 2
- Question: What is the impact of the size of the financial instruction-tuning dataset (DISC-FIN-SFT) on the performance of DISC-FinLLM?
- Basis in paper: [explicit] The paper constructs DISC-FIN-SFT with approximately 250k examples.
- Why unresolved: The paper does not explore the impact of varying the size of the instruction-tuning dataset on the model's performance.
- What evidence would resolve it: Conducting experiments with different sizes of the financial instruction-tuning dataset and evaluating the corresponding performance of DISC-FinLLM would provide insights into the relationship between dataset size and model performance.

### Open Question 3
- Question: How does the performance of DISC-FinLLM vary across different financial domains (e.g., banking, insurance, investment) within the financial industry?
- Basis in paper: [inferred] The paper mentions that DISC-FinLLM is designed to handle various financial scenarios.
- Why unresolved: The paper does not provide a detailed analysis of DISC-FinLLM's performance across different financial domains.
- What evidence would resolve it: Evaluating DISC-FinLLM on tasks specific to different financial domains and comparing its performance across these domains would provide insights into its domain-specific capabilities.

## Limitations

- Dataset construction quality is not detailed, making it difficult to verify whether the 246k samples truly capture the complexity of financial scenarios
- Generalization beyond financial domain is uncertain, as the paper doesn't provide evidence for how well the model maintains general language capabilities when LoRA modules are loaded
- Module switching overhead is not quantified, leaving uncertainty about computational costs and latency impact in production environments

## Confidence

**High Confidence Claims:**
- The Multiple Experts Fine-tuning Framework architecture is technically sound and implementable
- LoRA parameter swapping for module switching is a proven technique
- The general approach of instruction tuning for specialization is well-established

**Medium Confidence Claims:**
- The 2-9 point improvement over baseline models is reported but depends on the specific benchmarks used
- The model's capabilities in multi-turn Q&A and retrieval-augmented generation are claimed but not extensively validated
- The computational plugin architecture works as described but real-world robustness is uncertain

**Low Confidence Claims:**
- The dataset construction methodology is insufficiently detailed to verify quality
- The practical utility of module switching in real applications is not demonstrated
- The model's performance on truly unseen financial scenarios is not tested

## Next Checks

1. **Module Independence Test**: Conduct ablation studies where individual LoRA modules are removed to verify that each module's contributions are additive and don't create interference effects.

2. **Cross-Domain Performance**: Evaluate the model's performance on general Chinese language tasks (non-financial) with different LoRA modules loaded to measure domain adaptation costs.

3. **Real-World Deployment Simulation**: Test the model in a simulated multi-turn financial consultation scenario with module switching to measure response time overhead and accuracy degradation under realistic usage patterns.