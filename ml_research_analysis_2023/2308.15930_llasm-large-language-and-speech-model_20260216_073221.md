---
ver: rpa2
title: 'LLaSM: Large Language and Speech Model'
arxiv_id: '2308.15930'
source_url: https://arxiv.org/abs/2308.15930
tags:
- data
- speech
- language
- audio
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaSM, a large language and speech model
  that integrates speech and text modalities. The authors address the gap in multi-modal
  large language models by focusing on speech as an input modality, rather than vision.
---

# LLaSM: Large Language and Speech Model

## Quick Facts
- arXiv ID: 2308.15930
- Source URL: https://arxiv.org/abs/2308.15930
- Authors: 
- Reference count: 29
- This paper introduces LLaSM, a large language and speech model that integrates speech and text modalities.

## Executive Summary
This paper introduces LLaSM, a large language and speech model that integrates speech and text modalities. The authors address the gap in multi-modal large language models by focusing on speech as an input modality, rather than vision. LLaSM uses Whisper as a speech encoder, a modal adaptor to align speech and text embeddings, and a large language model for instruction following. The model is trained in two stages: pre-training with ASR data for modality adaptation and fine-tuning with a custom cross-modal instruction dataset. The authors release LLaSM-Audio-Instructions, a dataset with 199k conversations and 508k samples in English and Chinese, constructed using text-to-speech on open-source conversational data. Experiments show that LLaSM enables a more natural and convenient interaction with AI through speech. The model follows the architecture of LLaVA, leveraging pre-trained components for efficiency. The authors plan to extend LLaSM with vision capabilities in future work.

## Method Summary
LLaSM uses Whisper as a speech encoder to process raw audio data. The training data consists of public ASR datasets (Aishell, LibriSpeech, Magicdata, Primewords) for modality adaptation and a custom cross-modal instruction dataset (LLaSM-Audio-Instructions) constructed from WizardLM, ShareGPT, and GPT-4-LLM conversations with speech data generated using Microsoft Azure text-to-speech API. The model follows a two-stage training process: 1) Modality adaptation pre-training where the modal adaptor aligns speech and text embeddings using ASR data, and 2) Cross-modal instruction fine-tuning where the modal adaptor and LLM are jointly trained with multi-task instructions using the custom dataset. The architecture leverages pre-trained components (Whisper, LLaMA2-7B) to enable efficient training while focusing adaptation on the modal adaptor.

## Key Results
- LLaSM enables more natural and convenient human-AI interaction through speech
- The model follows LLaVA's architecture, leveraging pre-trained components for efficiency
- Authors release LLaSM-Audio-Instructions dataset with 199k conversations and 508k samples in English and Chinese
- Future work includes extending LLaSM with vision capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modality adaptor aligns speech embeddings with LLM text embeddings without retraining the entire model.
- Mechanism: The speech encoder (Whisper) produces fixed audio embeddings; the adaptor learns a transformation to map these embeddings into the same space as LLM text embeddings. This allows the LLM to process speech data directly as if it were text.
- Core assumption: The adaptor can learn to align speech and text embeddings effectively using ASR data in the pre-training stage.
- Evidence anchors:
  - [abstract] "we use Whisper [14] as a speech encoder to encode the speech signals into embeddings. Then a modal adaptor learns to align speech embeddings with the input text embeddings"
  - [section] "the modal adaptor is trained with public ASR data to align the audio and the text embeddings"
  - [corpus] Weak - no direct corpus evidence found for adaptor alignment effectiveness
- Break condition: If the adaptor fails to align embeddings well, the LLM cannot process speech inputs correctly, leading to poor performance.

### Mechanism 2
- Claim: Two-stage training (pre-training + fine-tuning) enables efficient cross-modal instruction following.
- Mechanism: First stage freezes the speech encoder and LLM, training only the adaptor on ASR data for modality alignment. Second stage freezes only the speech encoder, training adaptor and LLM jointly on speech-text instruction data for conversational abilities.
- Core assumption: The two-stage approach allows efficient training by leveraging pre-trained components and focusing adaptation on the most critical parameters.
- Evidence anchors:
  - [abstract] "The training process is divided into two stages. In the first stage, we use the public ASR datasets for the modality adaptation pre-training. In the second stage, we use cross-modal instruction data for training to provide the model with the capacity to process cross-modal conversations"
  - [section] "During the pre-training stage, the modal encoder and the LLM remain frozen... During the cross-modal instruction fine-tuning, only the modal encoder is frozen, the modal adaptor and the LLM are joint-trained"
  - [corpus] Weak - no direct corpus evidence for two-stage training effectiveness
- Break condition: If the two-stage training fails, the model may not learn to effectively process cross-modal instructions.

### Mechanism 3
- Claim: Large-scale speech-text instruction dataset enables effective cross-modal fine-tuning.
- Mechanism: LLaSM-Audio-Instructions dataset provides 199k conversations and 508k samples in English and Chinese, constructed using text-to-speech on open-source conversational data. This dataset allows the model to learn cross-modal conversational abilities.
- Core assumption: The large-scale, high-quality speech-text instruction dataset is sufficient for effective cross-modal fine-tuning.
- Evidence anchors:
  - [abstract] "we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions... constructed by carefully selecting dialogues from GPT4-LLM [15], ShareGPT [16], WizardLM [17]"
  - [section] "we build and release a speech-text cross-modal instruction-following dataset LLaSM-Audio-Instructions. The dataset is constructed by carefully selecting dialogues from GPT4-LLM [15], ShareGPT [16], WizardLM [17]"
  - [corpus] Weak - no direct corpus evidence for dataset effectiveness on cross-modal fine-tuning
- Break condition: If the dataset is not representative or large enough, the model may not learn effective cross-modal instruction following.

## Foundational Learning

- Concept: Modality alignment in multimodal models
  - Why needed here: Understanding how different modalities (speech and text) can be aligned in a shared embedding space is crucial for LLaSM's architecture.
  - Quick check question: How does the modal adaptor learn to align speech embeddings with text embeddings?

- Concept: Two-stage training in large language models
  - Why needed here: LLaSM uses a two-stage training process (pre-training for modality adaptation and fine-tuning for cross-modal instructions) to efficiently leverage pre-trained components.
  - Quick check question: Why does LLaSM freeze the speech encoder during the cross-modal instruction fine-tuning stage?

- Concept: Speech encoding and embedding extraction
  - Why needed here: LLaSM uses Whisper as a speech encoder to convert raw audio data into embeddings, which are then processed by the LLM through the modal adaptor.
  - Quick check question: What is the role of Whisper in LLaSM's architecture, and how does it contribute to the overall model's performance?

## Architecture Onboarding

- Component map:
  Speech input → Whisper encoding → Modal adaptor alignment → LLM processing → Output generation

- Critical path: Speech input → Whisper encoding → Modal adaptor alignment → LLM processing → Output generation

- Design tradeoffs:
  - Using pre-trained components (Whisper, LLAMA2-7B) for efficiency vs. potential limitations in fine-tuning
  - Two-stage training approach to focus adaptation on critical parameters vs. potential loss of information from frozen components
  - Large-scale dataset construction for effective fine-tuning vs. potential quality issues from automated TTS generation

- Failure signatures:
  - Poor speech-text alignment: LLM struggles to process speech inputs correctly
  - Ineffective cross-modal instruction following: Model fails to respond appropriately to speech-text instructions
  - Limited language support: Model performs well in one language but poorly in another

- First 3 experiments:
  1. Test modality alignment: Feed simple speech inputs and evaluate LLM's ability to process them correctly
  2. Evaluate cross-modal instruction following: Test model's performance on speech-text instruction tasks from the LLaSM-Audio-Instructions dataset
  3. Assess language support: Compare model's performance on English and Chinese speech-text instructions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the alignment quality of speech embeddings with text embeddings in LLaSM compare to traditional ASR-to-text conversion methods in terms of information preservation and accuracy?
- Basis in paper: [inferred] The paper discusses the cascading paradigm methods using ASR to convert speech to text, which leads to information consumption and potential ASR errors. LLaSM aims to directly process speech embeddings, potentially avoiding this information loss.
- Why unresolved: The paper does not provide direct comparative experiments between LLaSM's speech processing and traditional ASR-to-text conversion in terms of information preservation or accuracy metrics.
- What evidence would resolve it: Comparative experiments measuring information retention, word error rates, and task performance between LLaSM and cascaded ASR-to-LLM approaches on identical tasks.

### Open Question 2
- Question: What is the long-term performance and generalization capability of LLaSM when handling diverse speech accents, dialects, and noisy environments beyond the training data?
- Basis in paper: [inferred] The paper mentions using Whisper as a speech encoder and training on ASR datasets, but does not discuss the model's robustness to diverse speech variations or real-world noisy conditions.
- Why unresolved: The paper lacks evaluation on diverse speech datasets with varying accents, dialects, or noise levels, and does not report performance degradation or adaptation capabilities in such conditions.
- What evidence would resolve it: Extensive testing on diverse speech datasets (e.g., Common Voice, TED-LIUM) with different accents, dialects, and noise conditions, including performance metrics and robustness analysis.

### Open Question 3
- Question: How does the computational efficiency and resource consumption of LLaSM compare to traditional multi-modal models when scaling to larger speech and language datasets?
- Basis in paper: [explicit] The paper mentions that LLaSM is "resource-friendly" by leveraging pre-trained components and only training the modal adaptor during pre-training, but does not provide detailed computational efficiency metrics or comparisons.
- Why unresolved: The paper lacks quantitative comparisons of computational resources (e.g., GPU hours, memory usage) and efficiency metrics (e.g., inference time, energy consumption) between LLaSM and other multi-modal models.
- What evidence would resolve it: Detailed benchmarking studies comparing LLaSM's computational requirements, inference speed, and energy consumption with other speech-language multi-modal models on identical hardware setups.

## Limitations

- The modal adaptor architecture is not fully specified, limiting reproducibility
- Dataset construction using TTS may introduce domain mismatch with real-world speech
- Evaluation focuses on synthetic speech rather than naturalistic audio conditions
- Limited discussion of model robustness to diverse speech variations and noisy environments

## Confidence

**High confidence**: The core architectural design (Whisper encoder + modal adaptor + LLM) is sound and follows established multimodal integration patterns. The two-stage training methodology is well-justified and commonly used in multimodal transfer learning.

**Medium confidence**: The dataset construction approach and overall training methodology are reasonable, though the lack of detailed implementation specifics and hyperparameter specifications reduces reproducibility confidence. The evaluation metrics and results are presented clearly but may not fully capture real-world performance.

**Low confidence**: The generalization capabilities of the modal adaptor to diverse speech conditions (accents, background noise, emotional speech) are not demonstrated. The effectiveness of the TTS-based dataset for teaching conversational abilities is largely assumed rather than empirically validated.

## Next Checks

1. **Cross-linguality robustness test**: Evaluate the model's performance on speech inputs that mix English and Chinese within the same conversation, testing whether the modal adaptor can maintain alignment across language switches during real-time dialogue.

2. **Out-of-distribution speech evaluation**: Test the model with naturalistic speech containing background noise, varied speaking rates, and emotional content not present in the ASR training data to assess generalization beyond clean, TTS-generated audio.

3. **Ablation study on two-stage training**: Compare performance when fine-tuning with different freezing strategies (e.g., partially unfreezing the speech encoder, or using continuous training rather than discrete stages) to quantify the benefits and limitations of the current approach.