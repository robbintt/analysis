---
ver: rpa2
title: 'Rethinking PGD Attack: Is Sign Function Necessary?'
arxiv_id: '2312.01260'
source_url: https://arxiv.org/abs/2312.01260
tags:
- adversarial
- robust
- perturbation
- update
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the necessity of the sign function in projected
  gradient descent (PGD) attacks by analyzing how update mechanisms influence attack
  performance. The authors theoretically and empirically show that sign-based updates
  achieve larger perturbation changes per step, driving more pixels toward the constraint
  boundary and improving adversarial gains.
---

# Rethinking PGD Attack: Is Sign Function Necessary?

## Quick Facts
- arXiv ID: 2312.01260
- Source URL: https://arxiv.org/abs/2312.01260
- Reference count: 40
- This paper challenges the necessity of the sign function in PGD attacks and proposes Raw Gradient Descent (RGD) as a superior alternative.

## Executive Summary
This paper challenges the long-standing assumption that the sign function is essential for effective PGD attacks. Through theoretical analysis and empirical experiments, the authors demonstrate that the failure of raw gradients in PGD stems not from the gradient itself but from the clipping operation that discards magnitude information. They propose Raw Gradient Descent (RGD), which introduces an unclipped hidden perturbation variable to transform the constrained optimization problem into an unconstrained one. RGD consistently outperforms both sign-based and raw PGD across multiple datasets, models, and perturbation levels without additional computational cost.

## Method Summary
The paper introduces Raw Gradient Descent (RGD) as an alternative to traditional PGD attacks. While PGD computes the gradient of the loss with respect to a clipped perturbation and updates using the sign of this gradient, RGD maintains two perturbation variables: a clipped version for loss computation and an unclipped hidden variable for updates. This design preserves gradient magnitude information that would otherwise be lost to clipping. The method transforms the L∞ norm-based constrained optimization into an unconstrained problem by updating the unclipped variable with raw gradient steps while only clipping for loss computation. The approach is evaluated across CIFAR-10, CIFAR-100, and ImageNet datasets with various perturbation budgets.

## Key Results
- RGD consistently outperforms both PGD (with sign) and PGD (raw) across CIFAR-10, CIFAR-100, and ImageNet datasets
- RGD shows particular advantage with larger perturbation budgets (e.g., ε = 16/255)
- Integration of RGD into AutoAttack improves performance, especially in the first two steps
- RGD maintains its advantage in transferability experiments across different model architectures

## Why This Works (Mechanism)

### Mechanism 1
The sign function in PGD causes larger perturbation changes per step by pushing more pixels to the constraint boundary. By converting gradient signs into unit-magnitude steps, PGD forces more perturbation components to reach the ϵ-ball boundary. Once at the boundary, the gradient direction is preserved but magnitude is clipped, so subsequent updates maximize boundary contact and improve adversarial gain. This mechanism assumes most pixels are initially far from the boundary, and moving them to the boundary quickly yields higher adversarial loss.

### Mechanism 2
Raw gradient magnitude is largely discarded by the clipping operation in PGD, causing ineffective updates. When most pixels are already at the ϵ boundary, the raw gradient update is clamped to ±ϵ, erasing magnitude information. This reduces the effective step size and stalls attack progress. This mechanism assumes the clipping function operates element-wise and eliminates magnitude differences when pixels are at the boundary.

### Mechanism 3
Introducing an unclipped hidden perturbation variable transforms the constrained problem into an unconstrained one, preserving gradient magnitude. By maintaining a hidden δt that is not clipped during updates, the algorithm accumulates raw gradient steps without truncation. The clipped variable δt_c is only used for computing the loss gradient, so gradient magnitude is retained throughout training. This mechanism assumes the gradient of the loss with respect to the unclipped variable can still be computed using the clipped variable for the constraint.

## Foundational Learning

- Concept: Projected Gradient Descent (PGD) and L∞ norm constraints
  - Why needed here: Understanding PGD's iterative sign-based updates and the ϵ-ball clipping operation is essential to grasp why raw gradients fail and how RGD differs.
  - Quick check question: What does the σϵ(·) operation do to a perturbation vector, and how does it differ from a simple clipping?

- Concept: Adversarial examples and gradient-based attacks
  - Why needed here: The goal of maximizing loss while staying within constraints drives the design choices in PGD, raw PGD, and RGD.
  - Quick check question: How does maximizing the loss function g(x+δ) relate to making a model misclassify an input?

- Concept: Constrained vs unconstrained optimization
  - Why needed here: RGD's key innovation is converting a constrained problem into an unconstrained one by introducing an auxiliary variable.
  - Quick check question: In what way does maintaining an unclipped perturbation variable change the nature of the optimization problem?

## Architecture Onboarding

- Component map:
  Input image x -> Model f(x; w) -> Loss function l(f(x+δ); w), y) -> Clipped perturbation δc = σϵ(δ) for loss computation -> Hidden unclipped perturbation δ for gradient accumulation -> Gradient ∇δl computed from δc, applied to δ -> Final output: σϵ(δ) after T steps

- Critical path:
  1. Initialize δ = 0
  2. For t = 1 to T:
     - Compute δc = σϵ(δ)
     - Compute gradient ∇δl(f(x+δc); w), y)
     - Update δ ← δ + α∇δl
  3. Return σϵ(δ) as adversarial example

- Design tradeoffs:
  - RGD keeps magnitude information but requires maintaining an extra variable
  - Sign-based PGD is simpler but loses magnitude and may converge slower in early steps
  - Raw PGD without sign is prone to clipping loss, leading to poor performance

- Failure signatures:
  - If most pixels are stuck at zero after many steps, the update is ineffective
  - If the final perturbation is far from the boundary, the attack is weak
  - If the hidden variable grows unbounded, numerical instability may occur

- First 3 experiments:
  1. Run PGD, PGD(raw), and RGD on CIFAR-10 with ε = 8/255, step size α tuned for each; compare robust accuracy after 7 steps.
  2. Visualize perturbation pixel distributions at each step for all three methods to confirm boundary ratio differences.
  3. Integrate RGD into a 100-step APGDCE attack (replacing first two steps) and measure final robust accuracy versus vanilla APGDCE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed RGD algorithm perform compared to other state-of-the-art adversarial attack methods beyond PGD and AutoAttack?
- Basis in paper: The paper extensively compares RGD with PGD and AutoAttack, but does not explore comparisons with other attack methods.
- Why unresolved: The paper focuses on comparing RGD with PGD and AutoAttack, leaving the performance of RGD against other attack methods unexplored.
- What evidence would resolve it: Conducting experiments to compare RGD with a wider range of state-of-the-art adversarial attack methods, such as C&W and DeepFool, would provide insights into its performance relative to other approaches.

### Open Question 2
- Question: How does the performance of RGD vary with different step sizes and initialization methods?
- Basis in paper: The paper mentions that the step sizes and initialization methods for RGD are carefully tuned, but does not provide a detailed analysis of how these factors affect its performance.
- Why unresolved: The paper does not explore the impact of different step sizes and initialization methods on RGD's performance, leaving this aspect unaddressed.
- What evidence would resolve it: Conducting experiments to evaluate the performance of RGD with various step sizes and initialization methods would provide insights into the optimal configurations for different scenarios.

### Open Question 3
- Question: How does the proposed RGD algorithm perform in black-box attack scenarios?
- Basis in paper: The paper focuses exclusively on white-box attack scenarios and does not explore the performance of RGD in black-box settings.
- Why unresolved: The paper does not address the performance of RGD in black-box attack scenarios, which are important in practical applications where the model architecture is not accessible.
- What evidence would resolve it: Conducting experiments to evaluate the performance of RGD in black-box attack scenarios, such as transfer attacks or query-based attacks, would provide insights into its effectiveness in real-world settings.

## Limitations

- The paper does not fully explore the trade-off between step size and attack effectiveness for RGD versus PGD, particularly in high-ε regimes where clipping becomes severe.
- The assumption that sign-based updates are superior because they drive more pixels to the boundary may not hold for all initializations or for images where the gradient is already near-uniform in sign.
- The transformation from constrained to unconstrained optimization via an unclipped hidden variable is innovative, but the paper does not rigorously prove that this always leads to better adversarial gains, especially for very large ε or different loss landscapes.

## Confidence

- Mechanism 1 (sign function advantage): High - well-supported by both theory and empirical boundary ratio analysis.
- Mechanism 2 (clipping loss in raw PGD): Medium - plausible but the analysis could benefit from more ablation studies isolating clipping effects.
- Mechanism 3 (RGD transformation): High - the algorithmic change is clearly defined and empirically validated.

## Next Checks

1. Run ablation studies comparing RGD with a variant that clips only after multiple steps (not per step) to isolate the effect of per-step clipping in PGD.
2. Test RGD on datasets and models not included in the original experiments (e.g., smaller datasets, non-RobustBench models) to check generalizability.
3. Analyze the variance of RGD's performance across random seeds and initializations to ensure robustness, particularly for high-ε settings.