---
ver: rpa2
title: Risk-Sensitive Stochastic Optimal Control as Rao-Blackwellized Markovian Score
  Climbing
arxiv_id: '2312.14000'
source_url: https://arxiv.org/abs/2312.14000
tags:
- control
- stochastic
- learning
- algorithm
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to risk-sensitive stochastic
  optimal control as Markovian score climbing under samples drawn from a conditional
  particle filter. The authors formulate the problem as maximum likelihood estimation
  in a state-space model and use a Rao-Blackwellized conditional sequential Monte
  Carlo (RB-CSMC) kernel to obtain low-variance, unbiased estimates of the score function.
---

# Risk-Sensitive Stochastic Optimal Control as Rao-Blackwellized Markovian Score Climbing

## Quick Facts
- arXiv ID: 2312.14000
- Source URL: https://arxiv.org/abs/2312.14000
- Authors: 
- Reference count: 11
- Key outcome: Novel approach to risk-sensitive stochastic optimal control using Rao-Blackwellized conditional sequential Monte Carlo (RB-CSMC) for gradient-based policy optimization without explicit value function learning

## Executive Summary
This paper introduces a novel approach to risk-sensitive stochastic optimal control by reformulating the problem as maximum likelihood estimation in a state-space model. The authors develop a Rao-Blackwellized conditional sequential Monte Carlo (RB-CSMC) kernel that provides low-variance, unbiased estimates of the score function, enabling consistent policy optimization through Markovian score climbing. The method is evaluated on several stochastic dynamical systems and demonstrates competitive performance with state-of-the-art reinforcement learning algorithms.

## Method Summary
The method formulates risk-sensitive stochastic optimal control as maximum likelihood estimation in an equivalent state-space model with binary optimality variables. It uses a Rao-Blackwellized conditional sequential Monte Carlo (RB-CSMC) kernel to sample from the smoothing distribution and compute unbiased estimates of the score function. The policy is parameterized as a neural network and optimized using stochastic gradient ascent on the log marginal likelihood objective. The Rao-Blackwellization step reduces variance by reusing all particles generated during the forward pass, not just the reference trajectory.

## Key Results
- RB-CSMC outperforms related particle smoothing approaches on pendulum, cart-pole, and double pendulum environments
- The method performs competitively with state-of-the-art reinforcement learning algorithms like PPO and TRPO
- Variance reduction through Rao-Blackwellization is demonstrated, particularly beneficial when using few particles
- The approach successfully handles nonlinear, non-Gaussian, and bounded domains without approximations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Risk-sensitive stochastic control can be reformulated as maximum likelihood estimation in an equivalent state-space model
- Mechanism: By introducing a binary optimality variable y_t with likelihood g(y_t = 1|x_t, u_t) = exp{-η c(x_t, u_t)}, the control problem becomes equivalent to maximizing the log marginal likelihood of this state-space model
- Core assumption: The optimality variable y_t correctly captures the risk-sensitive objective through the exponential weighting with inverse temperature η
- Evidence anchors: [abstract]: "We formulate the problem as maximum likelihood estimation in a state-space model"
- Break condition: If the control objective is not well-represented by the exponential weighting, or if the state-space model assumptions are violated

### Mechanism 2
- Claim: Markovian score climbing with Rao-Blackwellized conditional SMC provides low-variance, unbiased gradient estimates
- Mechanism: The RB-CSMC kernel provides samples from the smoothing distribution p(z_0:T|y_0:T) without the bias of particle smoothing. The Markovian score climbing algorithm uses these samples to compute unbiased estimates of the score function ∇θ log p(y_0:T = 1|θ)
- Core assumption: The Markov chain defined by the CSMC kernel converges geometrically fast to the target smoothing distribution
- Evidence anchors: [abstract]: "provides asymptotically unbiased estimates for gradient-based policy optimization"
- Break condition: If the CSMC kernel fails to converge to the smoothing distribution, or if the number of particles N is insufficient

### Mechanism 3
- Claim: The Rao-Blackwellization step reduces variance by reusing all particles generated during the forward pass
- Mechanism: Unlike standard conditional SMC which only keeps T particles out of T×N generated, the Rao-Blackwellized version explicitly reuses all generated trajectories to compute the score estimate
- Core assumption: The backward sampling distribution is tractable and can be computed for all particles
- Evidence anchors: [section 3.3]: "Cardoso et al. (2023) to propose explicitly reusing all generated trajectories"
- Break condition: If the backward sampling becomes computationally intractable for large state spaces

## Foundational Learning

- Concept: State-space models and filtering theory
  - Why needed here: The entire framework relies on interpreting the control problem as inference in a state-space model
  - Quick check question: What is the difference between filtering and smoothing distributions in state-space models?

- Concept: Markov chain Monte Carlo and ergodicity
  - Why needed here: The policy optimization relies on Markovian score climbing, which requires understanding of MCMC kernels and convergence
  - Quick check question: What are the sufficient conditions for a Markov chain to be ergodic?

- Concept: Risk-sensitive control and exponential utility
  - Why needed here: The control objective is reformulated using exponential utility with inverse temperature η
  - Quick check question: How does the inverse temperature parameter η affect the risk-sensitivity of the control objective?

## Architecture Onboarding

- Component map: State-space model -> RB-CSMC kernel -> Score estimation -> Policy update -> Environment interaction
- Critical path: Environment dynamics → State-space model → RB-CSMC kernel → Score estimation → Policy update → Environment interaction
- Design tradeoffs:
  - Particle count vs. computational cost: More particles reduce variance but increase computation
  - Kernel choice vs. convergence guarantees: Different SMC kernels have different ergodicity properties
  - Policy parameterization vs. expressivity: Neural networks vs. simpler distributions
- Failure signatures:
  - High variance in gradient estimates → Increase particle count or improve Rao-Blackwellization
  - Poor policy performance → Check if smoothing distribution is being correctly approximated
  - Slow convergence → Verify MCMC kernel ergodicity or adjust learning rate
- First 3 experiments:
  1. Verify that the state-space model correctly represents the control problem by checking log marginal likelihood gradients on a simple linear system
  2. Test the RB-CSMC kernel on a small problem to ensure it generates samples from the correct smoothing distribution
  3. Compare the performance of RB-CSMC vs. standard particle smoothing on a benchmark problem to verify variance reduction claims

## Open Questions the Paper Calls Out

- Question: How can the inverse temperature parameter η be selected in a principled manner for different control tasks?
  - Basis in paper: The paper mentions that η modulates the risk trade-off of the objective but does not provide a systematic method for its selection
  - Why unresolved: The paper acknowledges that the choice of η affects the risk sensitivity but does not offer a principled approach to determine its value
  - What evidence would resolve it: Empirical studies comparing performance across different η values for various control tasks

- Question: How can the proposed method be extended to address infinite-horizon stochastic optimal control problems?
  - Basis in paper: The paper focuses on finite-horizon control problems, with brief mention of the need to address infinite-horizon objectives
  - Why unresolved: The proposed method is formulated for finite-horizon problems, requiring modifications for infinite-horizon scenarios
  - What evidence would resolve it: A theoretical framework and empirical results demonstrating application to infinite-horizon control problems

- Question: How can online model inference be integrated with the proposed control method?
  - Basis in paper: The discussion section mentions the need to address online model inference as a limitation
  - Why unresolved: The current method requires full access to the stochastic dynamics, which may not be available in practice
  - What evidence would resolve it: An extension of the method that incorporates online model learning, along with empirical results

## Limitations

- Scalability concerns: Computational complexity scales with the number of particles (T×N), potentially limiting application to high-dimensional state spaces
- Model-based assumption: Requires known transition dynamics, limiting applicability to model-free settings where dynamics must be learned
- Benchmark scope: Performance comparisons with PPO and TRPO are limited to specific environments and may not generalize broadly

## Confidence

- Theoretical foundations: Medium confidence - The theoretical analysis is sound but scalability to high dimensions remains uncertain
- Experimental results: High confidence - Results are well-documented and demonstrate clear advantages over baseline methods

## Next Checks

1. Test the method on a high-dimensional stochastic control problem to assess scalability limitations
2. Implement a model-free variant using learned dynamics models to relax the known dynamics assumption
3. Compare performance across a broader range of risk-sensitivity levels (η values) to understand behavior in different risk regimes