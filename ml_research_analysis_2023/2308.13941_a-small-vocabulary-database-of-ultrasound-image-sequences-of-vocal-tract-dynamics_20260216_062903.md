---
ver: rpa2
title: A small vocabulary database of ultrasound image sequences of vocal tract dynamics
arxiv_id: '2308.13941'
source_url: https://arxiv.org/abs/2308.13941
tags:
- speech
- ultrasound
- tongue
- images
- database
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new database of ultrasound image sequences
  of vocal tract dynamics, containing concurrent articulatory and acoustic speech
  data. The articulatory data consists of ultrasound videos capturing the tongue's
  upper contour during speech production, while the acoustic data includes 30 short
  sentences spoken by 17 young Colombian Spanish speakers from the Santander region.
---

# A small vocabulary database of ultrasound image sequences of vocal tract dynamics

## Quick Facts
- arXiv ID: 2308.13941
- Source URL: https://arxiv.org/abs/2308.13941
- Reference count: 38
- Introduces a new database of ultrasound image sequences capturing vocal tract dynamics with concurrent articulatory and acoustic speech data

## Executive Summary
This paper presents a new database of ultrasound image sequences capturing vocal tract dynamics during speech production, containing concurrent articulatory and acoustic speech data from 17 Colombian Spanish speakers. The database includes 30 short sentences per speaker, with ultrasound videos of tongue contours and synchronized acoustic signals. The authors developed a semi-automatic approach for tongue contour extraction and manual phoneme segmentation, creating a resource for various speech-related applications including speech therapy, language learning, and acoustic-to-articulatory inversion research.

## Method Summary
The authors created a database by recording ultrasound videos (12-15 fps) and acoustic signals from 17 speakers reading 30 Spanish sentences each. A Python script synchronized the triggering of both devices during acquisition. Tongue contours were extracted semi-automatically using EdgeTrak software with manual verification and correction frame by frame. Acoustic signals were manually segmented into phonemes using WaveSurfer. The resulting database contains ultrasound images in .jpg format, acoustic signals in .wav format, and tongue contour coordinates as 50-point sequences per frame.

## Key Results
- Successfully created a database with concurrent articulatory (ultrasound) and acoustic speech data
- Developed semi-automatic tongue contour extraction methodology requiring manual verification
- Established synchronization between ultrasound and acoustic recordings using custom Python script
- Manual phoneme segmentation completed for all acoustic signals in the corpus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ultrasound imaging provides sufficient temporal resolution to capture articulatory dynamics for speech-related applications.
- Mechanism: The authors argue that articulatory movements, being biological systems, tend to move slowly. They cite literature estimating the maximum frequency content of slowly varying articulatory signals to be below 6-7 Hz, which is within the capability of their 12-15 fps ultrasound system.
- Core assumption: The temporal resolution of 12-15 fps is adequate for capturing the relevant articulatory information for the purposes of this database.
- Evidence anchors:
  - [section]: "The temporal resolution of articulatory movements is analyzed in several works such as [17]–[19]. The authors in [19] quantify the effective maximum frequency content of slowly articulatory varying signals by calculating the frequency fc below which a certain percentage (say 95%) of the total energy is contained. They estimated its corresponding mean µfc and standard deviation σfc values for the speakers in the MOCHA-TIMIT database. From the data reported in [19], and assuming a normal distribution for those fastest articulators, it can be inferred that there is a probability of 0.977 P (fc ≤ µfc + 2σfc) that 95% of the total energy is located below 6.61 Hz and 6.87 Hz for the case of male and female subjects, respectively."
- Break condition: If speakers produce sounds requiring faster articulatory movements (e.g., alveolar trill r or alveolar tap R), the temporal resolution would be insufficient, as noted by the authors' exclusion of these sounds from the corpus.

### Mechanism 2
- Claim: Semi-automatic tongue contour extraction balances accuracy and efficiency for large-scale database creation.
- Mechanism: The authors use EdgeTrak software for initial contour detection, followed by manual verification and correction frame by frame. This approach leverages automated algorithms while maintaining high accuracy through human oversight.
- Core assumption: Manual correction of automatically detected contours is feasible and necessary to achieve the desired accuracy for research applications.
- Evidence anchors:
  - [section]: "For this purpose, we used the software EdgeTrak [21]. The procedure is carried out as follows: first, each ultrasound image is loaded into the software, then the area of interest is adjusted (where the contour of the tongue is located). In figure 4 is shown the area of interest as the green window. Then the contour must be initialized with 6 points along the tongue and finally the software applies an algorithm based on active contours [22] which allows to obtain the complete contour. In this final step and similar to [11], we observed that in practice the algorithm lost the track of the tongue frequently. For that reason, it was necessary to verify frame by frame visually, and when necessary, to correct the contours manually. Thus, we adopted a semi-automatic approach."
- Break condition: If the proportion of frames requiring manual correction becomes too high, the efficiency gains of semi-automatic processing would be lost, making fully manual or fully automatic approaches more practical.

### Mechanism 3
- Claim: Concurrent acquisition of ultrasound and acoustic data enables multiple speech processing applications.
- Mechanism: By recording ultrasound videos of the vocal tract alongside acoustic signals, the database supports research in acoustic-to-articulatory inversion, speech therapy, and speaker recognition systems.
- Core assumption: The temporal alignment between ultrasound and acoustic data is sufficient for the intended applications.
- Evidence anchors:
  - [abstract]: "This paper introduces a new database of ultrasound image sequences of vocal tract dynamics, containing concurrent articulatory and acoustic speech data."
  - [section]: "A Python script was developed to synchronize the triggering of acoustic and ultrasound devices for data acquisition."
- Break condition: If synchronization errors occur or if the temporal resolution of one modality is significantly different from the other, the utility for time-sensitive applications like acoustic-to-articulatory inversion would be compromised.

## Foundational Learning

- Concept: Understanding of ultrasound imaging principles and limitations in speech research.
  - Why needed here: The paper relies on ultrasound as the primary method for capturing articulatory data. Knowledge of ultrasound imaging principles is crucial for understanding the data quality and potential applications.
  - Quick check question: What are the main advantages and limitations of using ultrasound imaging for capturing vocal tract dynamics in speech research?

- Concept: Familiarity with speech production mechanisms and articulatory phonetics.
  - Why needed here: The database focuses on vocal tract dynamics during speech production. Understanding how different sounds are produced and the role of various articulators is essential for interpreting the data and its potential applications.
  - Quick check question: How do the positions and movements of the tongue, lips, and other articulators contribute to the production of different speech sounds?

- Concept: Basic knowledge of signal processing and feature extraction techniques.
  - Why needed here: The paper mentions several applications that rely on processing the acoustic and articulatory data, such as acoustic-to-articulatory inversion and speaker recognition. Familiarity with signal processing concepts is necessary to understand these applications.
  - Quick check question: What are some common feature extraction techniques used in speech processing, and how might they be applied to both acoustic and articulatory data?

## Architecture Onboarding

- Component map: Ultrasound probe and microphone -> Python synchronization script -> Storage system (.wav and .jpg files) -> EdgeTrak software -> Manual verification and correction -> WaveSurfer annotation
- Critical path: Data acquisition → Synchronization → Storage → Preprocessing (contour extraction) → Annotation
- Design tradeoffs:
  - Temporal resolution vs. data size: Higher frame rate would provide more detailed articulatory information but would significantly increase data storage requirements.
  - Automation vs. accuracy: Fully automatic contour extraction would be faster but less accurate than the semi-automatic approach used.
  - Corpus size vs. diversity: A larger corpus with more speakers and sentences would be more representative but would require more resources to create and process.
- Failure signatures:
  - Poor synchronization between ultrasound and acoustic data, leading to misaligned articulatory and acoustic information.
  - Inaccurate tongue contour extraction, resulting in unreliable articulatory features for downstream applications.
  - Incomplete or inconsistent phoneme segmentation, limiting the utility of the database for certain research questions.

- First 3 experiments:
  1. Verify synchronization accuracy: Compare timestamps of ultrasound and acoustic data to ensure proper alignment.
  2. Evaluate tongue contour extraction accuracy: Manually check a subset of extracted contours against the original ultrasound images to assess the quality of the semi-automatic approach.
  3. Test acoustic-to-articulatory inversion: Use a subset of the data to train and evaluate an acoustic-to-articulatory inversion model, measuring the accuracy of predicted articulatory features against the ground truth ultrasound data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the current semi-automatic tongue contour extraction method compared to fully manual or fully automatic approaches?
- Basis in paper: [explicit] The paper mentions using a semi-automatic approach combining the EdgeTrak software with manual corrections, noting that the algorithm "lost the track of the tongue frequently" requiring frame-by-frame visual verification and manual correction.
- Why unresolved: The paper doesn't provide quantitative comparison of accuracy, speed, or reliability between the semi-automatic method and other approaches.
- What evidence would resolve it: Comparative studies measuring contour extraction accuracy, processing time, and user effort for semi-automatic, fully manual, and fully automatic methods on the same dataset.

### Open Question 2
- Question: What is the optimal ultrasound sampling rate for capturing relevant articulatory information in speech production?
- Basis in paper: [explicit] The paper discusses the low sampling rate of the ultrasound probe (12-15 fps) and references studies suggesting articulatory movements have low-pass characteristics with cutoff frequencies around 6-7 Hz.
- Why unresolved: The paper doesn't experimentally determine if higher sampling rates would provide additional useful information or if the current rate is sufficient for all speech sounds.
- What evidence would resolve it: Systematic experiments comparing tongue contour tracking accuracy and speech production analysis results at different ultrasound sampling rates.

### Open Question 3
- Question: How can the acoustic-to-articulatory inversion (AAI) models trained on this ultrasound database improve speech recognition or speaker verification performance?
- Basis in paper: [explicit] The paper mentions that AAI could improve speech recognition, speech synthesis, speaker verification, and talking heads, and notes that ultrasound is a cost-effective alternative to EMA for AAI.
- Why unresolved: The paper doesn't present any AAI models or demonstrate their effectiveness on downstream tasks using the collected data.
- What evidence would resolve it: Development and evaluation of AAI models using this database, with performance metrics on speech recognition accuracy, speaker verification equal error rates, or speech synthesis quality compared to baseline systems.

## Limitations
- The 12-15 fps temporal resolution may be insufficient for capturing rapid articulatory movements like alveolar trills
- The semi-automatic tongue contour extraction process requires significant manual verification and correction, limiting scalability
- The corpus size (30 sentences from 17 speakers) is relatively small, potentially limiting representativeness for diverse speech applications

## Confidence

- High confidence: The database provides concurrent articulatory and acoustic data with proper synchronization, as evidenced by the described acquisition setup and Python script for triggering devices.
- Medium confidence: The semi-automatic tongue contour extraction methodology balances accuracy and efficiency, though the actual proportion of frames requiring manual correction is not quantified.
- Medium confidence: The temporal resolution of 12-15 fps is adequate for capturing slowly varying articulatory signals, based on cited literature showing that 95% of energy in articulatory movements is below 6-7 Hz.

## Next Checks

1. Conduct a quantitative analysis of manual correction requirements: Sample 100 randomly selected frames from the database and measure the percentage requiring manual contour adjustment to validate the efficiency claims of the semi-automatic approach.

2. Test synchronization accuracy across the entire corpus: Measure the temporal offset between ultrasound and acoustic signals across different speakers and sentence lengths to ensure consistent alignment.

3. Evaluate contour extraction accuracy: Compare automatically extracted contours against manually annotated ground truth for a subset of frames to establish the baseline accuracy and identify systematic errors in the EdgeTrak software.