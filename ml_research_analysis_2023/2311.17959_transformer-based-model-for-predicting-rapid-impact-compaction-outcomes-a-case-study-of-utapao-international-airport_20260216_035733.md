---
ver: rpa2
title: 'Transformer Based Model for Predicting Rapid Impact Compaction Outcomes: A
  Case Study of Utapao International Airport'
arxiv_id: '2311.17959'
source_url: https://arxiv.org/abs/2311.17959
tags:
- compaction
- data
- soil
- layer
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a deep learning model based on transformer
  architecture to predict the outcome of Rapid Impact Compaction (RIC), a ground improvement
  technique used in large infrastructure projects. The model incorporates various
  factors such as compaction effort, fine content, fill thickness, and initial soil
  profile to predict the cone resistance after RIC.
---

# Transformer Based Model for Predicting Rapid Impact Compaction Outcomes: A Case Study of Utapao International Airport

## Quick Facts
- arXiv ID: 2311.17959
- Source URL: https://arxiv.org/abs/2311.17959
- Reference count: 0
- Primary result: Transformer-based model predicts RIC outcomes with MAE of 0.0982 and RMSE of 0.4264

## Executive Summary
This paper introduces a transformer-based deep learning model for predicting Rapid Impact Compaction (RIC) outcomes in geotechnical engineering. The model leverages self-attention mechanisms to capture nonlinear spatial dependencies in layered soil profiles while incorporating compaction effort, fine content, fill thickness, and initial soil conditions. Tested on data from Utapao International Airport in Thailand, the model demonstrates superior prediction accuracy compared to existing methods and provides interpretable attention maps showing feature importance.

## Method Summary
The approach uses a transformer-based sequence-to-sequence architecture that processes soil profile data as sequential depth measurements. The model employs multi-head self-attention to model interactions between different soil layers while conditioning predictions on compaction parameters through feature concatenation. During inference, the model generates predictions sequentially, using previous outputs as inputs to simulate cumulative compaction effects. The architecture combines CNN layers for local feature extraction with attention mechanisms for long-range dependencies.

## Key Results
- Achieves MAE of 0.0982 and RMSE of 0.4264 on test dataset for cone resistance prediction
- Outperforms existing methods in both prediction accuracy and computational efficiency
- Generates interpretable attention maps revealing feature importance for RIC prediction
- Successfully predicts cone resistance at 0.25m intervals throughout the soil profile

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer attention layers effectively model nonlinear spatial dependencies in layered soil profiles by assigning adaptive weights to each depth interval.
- Mechanism: Self-attention computes scaled dot-product scores between depth-encoded query vectors and all other depth key vectors, producing weighted contributions to the predicted cone resistance at each depth. This captures cross-depth interactions without fixed kernel windows.
- Core assumption: Depth sequences are sufficiently short (28 points) that full attention computation remains tractable, and that interactions between layers are better modeled by direct comparison than by fixed convolutional receptive fields.
- Evidence anchors: [abstract] "provides interpretable attention maps that reveal the importance of different features for RIC prediction." [section] 3.3.1: "The model employs multi-head attention (Table 8) to extract features from the input soil profile..."

### Mechanism 2
- Claim: Concatenating compaction effort, fill thickness, and fine content into the attention embedding space allows the model to jointly condition predictions on both physical and sequence features.
- Mechanism: The features are embedded and concatenated with the depth encoding tensor before attention computation. This augmented tensor passes through multi-head attention, letting the model modulate depth interactions based on compaction parameters.
- Core assumption: The compaction features are stationary across depths for a given sample, so concatenation is valid; and the attention network can learn to prioritize or de-emphasize certain depths depending on feature values.
- Evidence anchors: [abstract] "incorporates various factors affecting the RIC performance, such as compaction effort, fine content, fill thickness, and initial soil profile." [section] 3.3.1: "The RIC feature is incorporated into the model in the encoding part..."

### Mechanism 3
- Claim: The sequence-to-sequence decoder that uses previous predictions as input (teacher forcing during training, self-feeding during inference) enables the model to simulate cumulative compaction effects layer-by-layer.
- Mechanism: At each decoder step, the model receives the latent representation of the initial profile plus the predicted cone resistance from the previous depth. This propagates compaction-induced changes downward, mimicking energy transfer in RIC.
- Core assumption: Each depth's cone resistance depends on the immediately preceding depth's state, so sequential prediction is appropriate; and the training teacher-forcing schedule matches inference usage to avoid distribution shift.
- Evidence anchors: [abstract] "The model also provides interpretable attention maps that reveal the importance of different features for RIC prediction." [section] 3.5: "The proposed model can generate the outcome of the RIC test..."

## Foundational Learning

- Concept: Mutual information regression for feature relevance.
  - Why needed here: To quantify how much each compaction feature (blows, fill thickness, fine content) reduces uncertainty about the RIC outcome before committing to complex modeling.
  - Quick check question: If fine content has MI = 0.15 and initial cone resistance has MI = 0.45, what fraction of the total MI is contributed by fine content?

- Concept: Layer normalization and residual connections in transformers.
  - Why needed here: To stabilize gradients across deep attention layers and preserve the original input signal while learning residual corrections.
  - Quick check question: What is the output of layer normalization when the input tensor has zero mean and unit variance?

- Concept: Teacher forcing vs. scheduled sampling in sequence generation.
  - Why needed here: To understand why the generative model may degrade during inference compared to training, and how to mitigate error accumulation.
  - Quick check question: If the model is trained with 100% teacher forcing but used at inference with 100% self-feeding, what is the expected effect on prediction quality for long sequences?

## Architecture Onboarding

- Component map: Input preprocessing → 1D positional embedding → Feature concatenation → Encoder (multi-head self-attention + residuals + LN) → Latent vector → Decoder (self-attention on shifted outputs + cross-attention to encoder + feature concatenation + MLP) → Output regression
- Critical path: Encoder self-attention → cross-attention + feature fusion → decoder MLP → prediction. Any bottleneck here directly limits accuracy.
- Design tradeoffs:
  - Attention head size vs. parameter count: Larger heads capture richer interactions but increase compute.
  - Number of encoder layers: More layers can model deeper abstractions but risk overfitting with small datasets.
  - Dropout rates: Higher dropout combats overfitting but may underfit if dataset is already limited.
- Failure signatures:
  - High training loss but low validation loss → underfitting; consider more layers or less dropout.
  - Low training loss but high validation loss → overfitting; add dropout, reduce model size, or augment data.
  - Slow convergence or exploding gradients → check layer norm placement, learning rate, or gradient clipping.
- First 3 experiments:
  1. Train baseline FNN_S model (concatenate all features and depths) to establish a reference error level.
  2. Add a single-layer transformer encoder with multi-head attention, keeping decoder shallow, to test attention benefit.
  3. Introduce sequential decoder self-attention and compare teacher-forcing vs. scheduled sampling schedules for cumulative error behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the transformer model compare to other deep learning models for predicting RIC outcomes in different soil types and conditions?
- Basis in paper: [explicit] The paper states that the proposed transformer-based model outperforms existing methods in terms of prediction accuracy and efficiency, but it does not provide a comprehensive comparison with other deep learning models for different soil types and conditions.
- Why unresolved: The paper only evaluates the performance of the transformer model on a single dataset from the Utapao International Airport in Thailand, which may not be representative of all soil types and conditions encountered in RIC projects.
- What evidence would resolve it: A comprehensive study comparing the performance of the transformer model with other deep learning models on a diverse set of datasets from different RIC projects with varying soil types and conditions.

### Open Question 2
- Question: What are the limitations and challenges of using transformer models for predicting RIC outcomes in real-world applications?
- Basis in paper: [explicit] The paper discusses the limitations and future directions of applying deep learning methods to RIC prediction, but it does not provide specific details on the limitations and challenges of using transformer models in real-world applications.
- Why unresolved: The paper does not provide enough information on the practical aspects of using transformer models for RIC prediction, such as the computational resources required, the interpretability of the model outputs, and the robustness of the model to noisy or incomplete data.
- What evidence would resolve it: A case study or a field trial that demonstrates the use of the transformer model for predicting RIC outcomes in a real-world project, along with a discussion of the challenges and limitations encountered.

### Open Question 3
- Question: How can the transformer model be used to optimize the design and execution of RIC projects?
- Basis in paper: [explicit] The paper mentions that the transformer model can be used as a generative model to predict the unforeseen value prior and during construction, which can be useful for geotechnical projects in design and supervision. However, it does not provide specific details on how the model can be used to optimize the design and execution of RIC projects.
- Why unresolved: The paper does not provide enough information on how the transformer model can be used to optimize the selection of compaction parameters, such as the number of blows, the fill thickness, and the fine content, or to monitor the progress of RIC projects and adjust the compaction parameters accordingly.
- What evidence would resolve it: A study that demonstrates how the transformer model can be used to optimize the design and execution of RIC projects, including the selection of compaction parameters and the monitoring of project progress.

## Limitations
- Specialized dataset from single site (32 CPTs from Utapao International Airport) limits generalizability across different soil types and compaction conditions
- Architectural details of the LSTM-ATT-S2S model remain underspecified, making exact replication difficult
- Lack of comparative baseline experiments against simpler models (pure CNN or LSTM) to strengthen claims about transformer superiority

## Confidence

- **High confidence**: The transformer architecture's ability to capture nonlinear spatial dependencies in soil profiles (supported by attention interpretability results)
- **Medium confidence**: The claim of outperforming existing methods (based on reported MAE/RMSE, but limited by dataset size and lack of comparative baseline details)
- **Low confidence**: The model's generalizability to sites with different soil characteristics or compaction parameters outside the training distribution

## Next Checks
1. **Dataset expansion test**: Validate model performance on an independent RIC dataset from a different site to assess generalizability and identify potential overfitting to the Utapao dataset characteristics.
2. **Architecture ablation study**: Systematically remove transformer components (attention heads, self-attention) and compare performance against the full model to quantify the specific contribution of each architectural element.
3. **Error propagation analysis**: Measure prediction error growth along depth sequences under different teacher-forcing ratios during training to optimize the scheduled sampling strategy and minimize inference-time degradation.