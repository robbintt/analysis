---
ver: rpa2
title: 'Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable
  Data with Learnable Examples'
arxiv_id: '2305.09241'
source_url: https://arxiv.org/abs/2305.09241
tags:
- data
- training
- unlearnable
- diffusion
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals a critical vulnerability in unlearnable examples
  (UEs), a popular data protection mechanism against unauthorized exploitation. While
  UEs aim to make data unexploitable by adding imperceptible perturbations, the authors
  demonstrate that this protection can be easily bypassed.
---

# Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples

## Quick Facts
- arXiv ID: 2305.09241
- Source URL: https://arxiv.org/abs/2305.09241
- Reference count: 40
- Primary result: Demonstrates that unlearnable examples (UEs) can be easily bypassed using learnable examples (LEs) created through joint-conditional diffusion purification

## Executive Summary
This paper exposes a critical vulnerability in unlearnable examples (UEs), a popular data protection mechanism designed to make data unexploitable by adding imperceptible perturbations. The authors introduce "learnable examples" (LEs), which are UEs with their protection removed, enabling standard training and representation learning. They achieve this through a joint-conditional diffusion purification process that projects UEs onto a learnable data manifold. Extensive experiments show that LEs consistently outperform state-of-the-art countermeasures across supervised and unsupervised UEs on multiple benchmark datasets, even when the learned data distribution differs significantly from the original clean distribution.

## Method Summary
The core approach is a joint-conditional diffusion purification process that projects unlearnable examples (UEs) onto a learnable data manifold. The method uses a diffusion model trained on newly collected unprotected data, conditioned on both pixel and perceptual similarity between UEs and their denoised versions. By iterating the purification process multiple times with a relatively small number of diffusion steps, the unlearnable perturbation can be fully removed while minimizing semantic loss. The method is practical as it requires significantly less data than existing countermeasures and remains effective even when the learned data distribution differs substantially from the original clean distribution.

## Key Results
- LEs achieve 93.1% accuracy on CIFAR-10 against error-minimizing noise attacks, outperforming adversarial augmentation (90.8%) and image compression methods (93.0%)
- The approach remains effective even with large distributional differences between newly collected data and clean data
- LEs consistently outperform state-of-the-art countermeasures across both supervised and unsupervised UE attacks on multiple benchmark datasets
- The method requires significantly less data than competing approaches like AVATAR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models can remove unlearnable perturbations while preserving semantic content
- Mechanism: The forward diffusion process adds Gaussian noise that submerges the unlearnable perturbation, while the reverse process denoises the image. By iterating the purification multiple times with a relatively small number of diffusion steps, the unlearnable perturbation can be fully removed while minimizing semantic loss.
- Core assumption: Unlearnable perturbations are imperceptible to humans and bounded by small pixel distances, so they can be submerged by Gaussian noise and removed through denoising
- Evidence anchors: [abstract] "the core of this approach is a novel purification process that projects UEs onto the manifold of LEs"; [section] "assume that the unlearnable example ˜x0 =x0 +δ, we first diffuse the unlearnable image for Tp steps by adding Gaussian noise to submerge the unlearnable perturbation"
- Break condition: If the unlearnable perturbation is too large to be submerged by the Gaussian noise, or if the diffusion process causes too much semantic loss

### Mechanism 2
- Claim: Joint-conditional guidance improves purification by preserving perceptual similarity
- Mechanism: The reverse diffusion process is conditioned on both pixel distance (MSE) and perceptual similarity (LPIPS) between the unlearnable example and the denoised version. This encourages the denoised image to be visually close to the unlearnable example while removing the unlearnable perturbation.
- Core assumption: Unlearnable examples have small pixel and perceptual differences from clean samples, so preserving these similarities will reconstruct the clean data
- Evidence anchors: [abstract] "a new joint-conditional diffusion model which denoises UEs conditioned on the pixel and perceptual similarity between UEs and LEs"; [section] "we propose a novel purification process based on diffusion model, called Joint-conditional Diffusion Purification (JCDP). JCDP leverages both pixel and perception distance guidance"
- Break condition: If the perceptual distance metric fails to capture true perceptual similarity, or if the guidance strength is not properly tuned

### Mechanism 3
- Claim: Learnable examples can be created from data with different distributions than the original clean data
- Mechanism: The diffusion model trained on newly collected raw data can still purify UEs even when the learned data distribution differs significantly from the original clean distribution. The joint-conditional terms in the reverse process shift the mean of the learned data distribution, pulling the purified image towards the clean data distribution.
- Core assumption: The joint-conditional guidance can overcome distributional differences and reconstruct the clean data manifold
- Evidence anchors: [abstract] "Surprisingly, we found that our approach still retains effectiveness even when there is a large distributional difference between the newly collected data (utilized in training a learnable data manifold) and the clean data"; [section] "we found that our approach still retains effectiveness even when there is a large distributional difference between the newly collected data and the clean data"
- Break condition: If the distributional difference is too large for the joint-conditional guidance to overcome, or if the learned data manifold is too different from the clean data manifold

## Foundational Learning

- Concept: Diffusion models and their reverse process
  - Why needed here: The paper uses diffusion models as the core mechanism for removing unlearnable perturbations
  - Quick check question: What are the two main processes in a diffusion model and what do they do?

- Concept: Perceptual similarity metrics (LPIPS)
  - Why needed here: The paper uses LPIPS to measure perceptual similarity between unlearnable examples and their denoised versions
  - Quick check question: What is LPIPS and how does it differ from pixel-wise distance metrics?

- Concept: Data manifold learning and projection
  - Why needed here: The paper projects unlearnable examples onto a learnable data manifold learned from newly collected raw data
  - Quick check question: What is a data manifold and how can it be used to transform data from one distribution to another?

## Architecture Onboarding

- Component map: Input (Unlearnable examples) -> Diffusion model (Unconditional DDPM) -> Joint-conditional guidance (Pixel distance MSE + Perceptual similarity LPIPS) -> Output (Learnable examples)

- Critical path:
  1. Train unconditional DDPM on newly collected raw data
  2. Diffuse unlearnable examples with Gaussian noise
  3. Denoise with joint-conditional guidance
  4. Iterate purification process multiple times

- Design tradeoffs:
  - Number of diffusion steps vs. semantic preservation
  - Guidance strength vs. purification effectiveness
  - Amount of newly collected data vs. model quality

- Failure signatures:
  - Purification process fails to remove unlearnable perturbation
  - Purified images lose semantic content or become distorted
  - Joint-conditional guidance does not improve purification performance

- First 3 experiments:
  1. Train unconditional DDPM on CIFAR-10 test set and use it to purify unlearnable CIFAR-10 training set
  2. Compare joint-conditional diffusion purification with unconditional diffusion purification on CIFAR-100
  3. Evaluate learnable examples on both supervised and unsupervised UE attacks across multiple benchmark datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Joint Conditional Diffusion Purification (JCDP) vary with different amounts of newly collected unprotected data for training the diffusion model?
- Basis in paper: [explicit] The paper mentions using 10,000 images for CIFAR-10/CIFAR-100 and 26,032 for SVHN, and suggests that the amount of data used by LE is much smaller than AVATAR, but does not explore the relationship between data amount and performance.
- Why unresolved: The paper does not provide an analysis of how the performance scales with the amount of training data, leaving the optimal data requirements unclear.
- What evidence would resolve it: Experimental results showing the performance of JCDP with varying amounts of training data, identifying the point of diminishing returns.

### Open Question 2
- Question: What is the theoretical explanation for why LEs remain effective even when the learned data distribution is significantly different from the original clean distribution?
- Basis in paper: [explicit] The paper observes that LEs remain effective even with large distributional differences between the newly collected data and the clean data, but attributes this to the joint-conditional terms without providing a theoretical explanation.
- Why unresolved: The authors speculate about the cause but do not provide a rigorous theoretical analysis of why this phenomenon occurs.
- What evidence would resolve it: A formal mathematical proof or theoretical model explaining the mechanism by which JCDP maintains effectiveness across distributional shifts.

### Open Question 3
- Question: How does the effectiveness of LEs compare to other countermeasures when applied to real-world scenarios where data distributions are continuously evolving?
- Basis in paper: [inferred] The paper demonstrates LE effectiveness on benchmark datasets but does not test performance on dynamically changing real-world data distributions.
- Why unresolved: The experiments use static benchmark datasets, which may not capture the challenges of real-world data evolution over time.
- What evidence would resolve it: A longitudinal study comparing LEs to other countermeasures on real-world datasets with evolving distributions over extended periods.

## Limitations

- The method requires access to additional unprotected data from the same distribution, which may not always be available in real-world scenarios
- The effectiveness of LEs across different UE protection mechanisms and datasets needs further validation
- The paper does not thoroughly address potential defenses against their own attack method

## Confidence

- High confidence in the technical feasibility of the diffusion-based purification approach
- Medium confidence in the generalizability across different UE protection methods
- Medium confidence in the claim about distributional robustness of the approach

## Next Checks

1. Test the method's effectiveness when only a small amount of unprotected data is available
2. Evaluate the approach's robustness against potential countermeasures specifically designed to detect or resist diffusion-based purification
3. Investigate the method's performance on real-world datasets where the clean data distribution may be unknown or difficult to access