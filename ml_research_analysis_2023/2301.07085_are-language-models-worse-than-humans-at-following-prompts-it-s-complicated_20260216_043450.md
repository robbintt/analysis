---
ver: rpa2
title: Are Language Models Worse than Humans at Following Prompts? It's Complicated
arxiv_id: '2301.07085'
source_url: https://arxiv.org/abs/2301.07085
tags:
- misleading
- task
- instructions
- prompts
- irrelevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study challenges the assumption that humans would perform
  poorly on natural language inference tasks when given intentionally irrelevant or
  misleading prompts. The authors conducted human experiments using the same setup
  as prior work on language models, finding that humans can reliably ignore irrelevant
  instructions and perform well on the underlying task.
---

# Are Language Models Worse than Humans at Following Prompts? It's Complicated

## Quick Facts
- arXiv ID: 2301.07085
- Source URL: https://arxiv.org/abs/2301.07085
- Reference count: 40
- Primary result: Humans can reliably ignore irrelevant instructions while faithfully following misleading ones, contrasting with prior assumptions about human instruction-following behavior

## Executive Summary
This study challenges the common assumption that humans would perform poorly on natural language inference tasks when given intentionally irrelevant or misleading prompts. Through human experiments using the same setup as prior work on language models, the authors found that humans can reliably ignore irrelevant instructions and perform well on the underlying task. However, unlike language models, humans faithfully follow misleading instructions. This mixed result suggests that prior work oversimplifying assumptions about human behavior, and future research should empirically validate human behaviors before using them as a benchmark for model evaluation.

## Method Summary
The authors conducted human experiments using Mechanical Turk with participants from the US, collecting single test items per participant followed by control items. They evaluated two instruction-tuned models (T0++ and Flan-T5) on the same prompts using rank classification. The study compared human and model performance across instructive, irrelevant, misleading, and null conditions using RTE and MNLI datasets, with manually curated examples for the misleading condition.

## Key Results
- Humans reliably ignore irrelevant instructions while performing well on the underlying NLI task
- Unlike models, humans faithfully follow misleading instructions and perform the specified surface task
- Humans quickly adapt to few-shot examples and detect misalignment between instructions and feedback

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Humans can reliably ignore irrelevant instructions in zero-shot settings while models tend to default to NLI.
- **Mechanism:** When humans see a premise-hypothesis pair with intervening irrelevant text, they interpret the core semantic relationship between the two sentences and default to NLI as a "background task," ignoring the distracting text.
- **Core assumption:** Humans have an innate tendency to resolve semantic relationships between paired sentences unless given explicit conflicting instructions.
- **Evidence anchors:** Humans perform well on NLI when given irrelevant prompts, showing similar patterns to models but with less variance.
- **Break condition:** If the irrelevant text introduces strong contextual cues that shift the task framing, humans may no longer ignore it.

### Mechanism 2
- **Claim:** Humans faithfully follow misleading instructions in zero-shot, but adapt when given few-shot examples or labels.
- **Mechanism:** In zero-shot, humans interpret instructions literally and perform the specified surface task (e.g., paraphrasing, grammaticality). With few-shot examples or labels, they detect misalignment between instructions and feedback and adjust their interpretation to match the underlying NLI task.
- **Core assumption:** Humans use feedback from examples/labels to infer the intended task when instructions appear misleading.
- **Evidence anchors:** Humans perform the surface task when given misleading prompts without feedback, but adapt when given NLI labels as feedback.
- **Break condition:** If examples are ambiguous or feedback is inconsistent, humans may fail to detect misalignment.

### Mechanism 3
- **Claim:** Models perform NLI regardless of instructions due to their training objectives and prompt conditioning.
- **Mechanism:** Instruction-tuned models are fine-tuned on NLI and other NLP tasks, causing them to default to task-appropriate patterns (e.g., entailment classification) when given paired sentences, even if instructions suggest a different task.
- **Core assumption:** Models prioritize task completion based on input patterns over literal instruction interpretation.
- **Evidence anchors:** Models perform well on NLI even with misleading or irrelevant instructions, suggesting they default to the underlying task.
- **Break condition:** If models are trained with explicit instruction adherence objectives or with task disambiguation mechanisms.

## Foundational Learning

- **Concept:** Zero-shot vs. few-shot learning
  - Why needed here: The study compares human and model performance under both settings to understand instruction-following behavior.
  - Quick check question: In zero-shot, what is the primary difference between how humans and models interpret misleading instructions?

- **Concept:** Natural Language Inference (NLI)
  - Why needed here: NLI is the underlying task used to evaluate whether participants perform it despite different instructions.
  - Quick check question: How does NLI differ from tasks like paraphrasing or grammaticality judgment?

- **Concept:** Prompt conditioning in language models
  - Why needed here: Understanding how models use prompts to condition their outputs is crucial for interpreting their performance on pathological instructions.
  - Quick check question: What is the role of instruction tuning in shaping model responses to prompts?

## Architecture Onboarding

- **Component map:** Design prompts -> Recruit participants -> Collect responses -> Analyze human vs. model performance -> Interpret differences
- **Critical path:** Design prompts → Recruit participants → Collect responses → Analyze human vs. model performance → Interpret differences
- **Design tradeoffs:** Balancing ecological validity (real-world instruction-following) with experimental control (isolated variables). Ensuring prompts are unambiguous yet pathological enough to test instruction-following.
- **Failure signatures:** If participants perform poorly on NLI controls, it suggests incompetence with the task rather than instruction-following differences. If models perform poorly on irrelevant prompts, it contradicts prior findings.
- **First 3 experiments:**
  1. Replicate the zero-shot experiment with a new dataset to verify results.
  2. Conduct a few-shot experiment with explicit label feedback to observe adaptation patterns.
  3. Test model performance with corrupted instructions to explore robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do language models perform on the underlying task (e.g., NLI) when given irrelevant prompts, and how does this compare to human performance?
- **Basis in paper:** [explicit] The authors find that humans perform surprisingly well on NLI when given irrelevant prompts, and models show a similar pattern. However, the authors also note that humans show less variance than models in performing the NLI task when the instructions are irrelevant.
- **Why unresolved:** While the authors observe a similar pattern between humans and models with irrelevant prompts, the specific performance of models on the underlying task is not fully explored. Additionally, the authors note that humans show less variance than models, but the reasons for this difference are not fully explained.
- **What evidence would resolve it:** Further experiments comparing the performance of humans and models on the underlying task with irrelevant prompts, as well as analysis of the factors contributing to the observed differences in variance.

### Open Question 2
- **Question:** How do humans and models differ in their interpretation and execution of misleading prompts, and what factors contribute to these differences?
- **Basis in paper:** [explicit] The authors find that humans faithfully follow misleading prompts, whereas models do not. They also observe that humans perform the surface task specified by the instruction, while models tend to perform the underlying task regardless of the instruction.
- **Why unresolved:** While the authors identify a clear difference in behavior between humans and models with misleading prompts, the specific factors contributing to this difference are not fully explored. Additionally, the authors note that humans may adapt their interpretation of the instruction based on feedback, but the extent and implications of this adaptation are not fully explored.
- **What evidence would resolve it:** Further experiments investigating the factors influencing human and model behavior with misleading prompts, as well as analysis of the impact of feedback on human interpretation and execution of instructions.

### Open Question 3
- **Question:** How do humans and models differ in their ability to infer the underlying task from examples in the few-shot setting, and what factors contribute to these differences?
- **Basis in paper:** [explicit] The authors find that humans can quickly infer the underlying task from examples in the few-shot setting, even when given misleading prompts. They also observe that humans may adapt their interpretation of the instruction based on feedback, while models tend to perform the underlying task regardless of the instruction.
- **Why unresolved:** While the authors identify a clear difference in behavior between humans and models in the few-shot setting, the specific factors contributing to this difference are not fully explored. Additionally, the authors note that humans may adapt their interpretation of the instruction based on feedback, but the extent and implications of this adaptation are not fully explored.
- **What evidence would resolve it:** Further experiments investigating the factors influencing human and model behavior in the few-shot setting, as well as analysis of the impact of feedback on human interpretation and execution of instructions.

## Limitations

- The study focuses only on zero-shot and few-shot settings, leaving open questions about model behavior in other contexts like few-shot with inconsistent labels or cross-lingual instruction following.
- The comparison between human and model instruction-following may not generalize to more complex or ambiguous prompts where task boundaries are less clear.
- The study relies on a specific set of pathological instructions, which may not capture the full range of real-world instruction-following scenarios.

## Confidence

- **High confidence**: The finding that humans can ignore irrelevant instructions while faithfully following misleading ones is well-supported by the experimental data and aligns with established cognitive psychology principles.
- **Medium confidence**: The mechanistic explanations for why humans adapt to few-shot examples but not zero-shot misleading instructions are plausible but could benefit from additional cognitive science literature support.
- **Medium confidence**: The claim that instruction-tuned models default to NLI due to training objectives is reasonable but may oversimplify the complex interplay between prompt conditioning and task completion strategies.

## Next Checks

1. Replicate the study with a larger sample size and more diverse participant pool to verify the robustness of human behavior patterns across different demographics and cultural contexts.
2. Test model performance on corrupted or ambiguous instructions to explore the limits of instruction-following and identify failure modes not captured by the current pathological instruction design.
3. Conduct a comparative analysis of human and model performance on multi-step instructions or instructions requiring world knowledge to understand how complexity affects instruction-following behavior.