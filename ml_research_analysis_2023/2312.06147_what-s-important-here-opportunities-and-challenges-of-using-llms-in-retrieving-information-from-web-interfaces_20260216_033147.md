---
ver: rpa2
title: '"What''s important here?": Opportunities and Challenges of Using LLMs in Retrieving
  Information from Web Interfaces'
arxiv_id: '2312.06147'
source_url: https://arxiv.org/abs/2312.06147
tags:
- task
- elements
- prompt
- user
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the capability of large language models
  (LLMs) to retrieve important UI elements from web interfaces given user task descriptions.
  The study focuses on four key prompt components: example selection, specificity
  of natural language commands, HTML truncation strategy, and the persona assumed
  by the LLM.'
---

# "What's important here?": Opportunities and Challenges of Using LLMs in Retrieving Information from Web Interfaces

## Quick Facts
- arXiv ID: 2312.06147
- Source URL: https://arxiv.org/abs/2312.06147
- Reference count: 40
- One-line primary result: LLMs can retrieve important UI elements from web interfaces with reasonable performance, but face significant challenges including hallucination and instruction-following failures.

## Executive Summary
This paper investigates how well large language models can identify important UI elements from web interfaces based on user task descriptions. Using the Mind2Web dataset and Claude2 model, the authors systematically evaluate four key prompt engineering components: example selection, specificity of natural language commands, HTML truncation strategy, and assumed persona. The study reveals that while LLMs show reasonable performance in retrieving UI elements, there remains substantial room for improvement, particularly around hallucination of non-existing elements and failure to follow instructions. The authors provide concrete recommendations for prompt engineering while identifying critical limitations that need addressing for practical deployment.

## Method Summary
The study uses the Mind2Web dataset containing 124 web interface examples with ground truth important UI elements. The authors evaluate Claude2's performance across three test sets (Cross-Task, Cross-Website, Cross-Domain) using few-shot prompting with varying components. HTML truncation is applied using a DeBERTa-based filtering module to select top-k elements (Top-10, Top-50), while task descriptions are varied across three specificity levels (detailed, simplified, abstract). The evaluation uses recall and element accuracy metrics, with experiments testing different combinations of kNN-selected examples, persona prompts (Web Assistant, Generic User, UI Designer), and truncation strategies.

## Key Results
- Truncation significantly improved performance compared to no truncation, particularly resolving 2-shot prompting issues by reducing input token size
- Using semantically similar few-shot examples boosted recall by 9.70% in 1-shot prompting but decreased performance by 13.17% in 2-shot prompting
- The Web Assistant persona performed significantly better than Generic User or UI Designer personas across most test conditions
- LLMs showed consistent failure modes including hallucination of non-existing DOM elements and failure to follow output format instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truncating HTML to top-k elements improves LLM performance by reducing context length and filtering noise.
- Mechanism: The HTML filtering module ranks elements by importance, allowing the LLM to focus on a smaller, more relevant subset of the DOM. This reduces the input token size, which mitigates performance degradation from long contexts and improves retrieval accuracy.
- Core assumption: The filtering module accurately ranks elements so that the ground truth element is within the top-k most of the time.
- Evidence anchors:
  - "Truncation significantly improved the performance compared to cases when no truncation was performed. It also resolved the issue with 2-shot prompting observed in table 2 by effectively reducing the input token size in the LLM prompt."
  - "For majority of the test samples, the ground truth element is present within the Top-10 elements returned by the model."

### Mechanism 2
- Claim: Example selection for few-shot prompting impacts performance through semantic relevance and context length trade-offs.
- Mechanism: Providing semantically similar examples via kNN search helps the LLM understand the task better, but adding too many examples increases context length and can degrade performance. The optimal number of examples depends on the test set characteristics.
- Core assumption: The embedding space captures semantic similarity relevant to UI element retrieval, and LLM performance scales with example relevance but degrades with context length.
- Evidence anchors:
  - "Using few-shot examples that has semantically similar task descriptions, can boost the recall by 9.70% in 1-shot prompting; however, it decreases the performance by 13.17% in 2-shot prompting."
  - "Providing the kNN examples from the train set significantly helps in the Cross-Task test set."

### Mechanism 3
- Claim: The persona assumed by the LLM affects its interpretation of "important UI elements" based on the implied user goals.
- Mechanism: By prompting the LLM with different personas (Web Assistant, Generic User, UI Designer), we shape its understanding of what constitutes important elements. The Web Assistant persona performed best, suggesting it aligns most closely with the task requirements.
- Core assumption: The LLM's response is sensitive to the persona prompt and that the Web Assistant persona naturally focuses on task completion.
- Evidence anchors:
  - "Web Assistant persona performed significantly better (except for the User Persona for 2-shot prompting on Cross-Task) than the rest of the persona."
  - "the perception of 'relevant UI elements' might vary from user to user."

## Foundational Learning

- Concept: HTML DOM structure and element hierarchy
  - Why needed here: The LLM processes HTML code to identify important UI elements, so understanding how HTML is structured (tags, attributes, nesting) is crucial for interpreting its outputs and designing effective prompts.
  - Quick check question: What is the difference between a parent element and a child element in HTML, and why does this matter for UI element retrieval?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The study heavily relies on different prompting strategies (few-shot examples, personas, specificity levels) to guide the LLM's behavior. Understanding how to craft effective prompts is essential for replicating and extending the work.
  - Quick check question: How does the choice of few-shot examples affect the LLM's performance, and what trade-offs exist between example relevance and context length?

- Concept: Evaluation metrics for information retrieval
  - Why needed here: The paper uses recall and element accuracy to measure performance. Understanding these metrics and their limitations (e.g., why recall is emphasized over precision) is important for interpreting results and designing new experiments.
  - Quick check question: Why does the paper emphasize recall over precision for this task, and how is element accuracy calculated?

## Architecture Onboarding

- Component map: Mind2Web dataset -> HTML truncation module -> LLM prompt construction -> Claude2 API -> Output parsing -> Evaluation
- Critical path:
  1. Load Mind2Web example (HTML, task description, ground truth element)
  2. Apply HTML truncation (Top-10 or Top-50 elements)
  3. Construct prompt with chosen components (examples, persona, specificity)
  4. Send prompt to Claude2 API
  5. Parse LLM output for UI element IDs
  6. Compare predicted elements with ground truth using evaluation metrics
- Design tradeoffs:
  - Context length vs. information completeness: Longer HTML provides more context but may degrade LLM performance
  - Example relevance vs. prompt size: More relevant examples improve performance but increase context length
  - Specificity level vs. real-world applicability: Detailed descriptions improve accuracy but may not reflect actual user queries
- Failure signatures:
  - Hallucination: LLM outputs non-existent element IDs
  - Instruction failure: LLM doesn't follow output format requirements (e.g., missing <IDs> tags)
  - Low recall: LLM consistently misses ground truth elements, indicating issues with prompt design or HTML truncation
- First 3 experiments:
  1. Reproduce Table 2: Test Claude2 with fixed vs. kNN examples across specificity levels (Detailed, Simplified, Abstract) to verify the impact of example selection and specificity on performance.
  2. Replicate Table 3: Apply HTML truncation at Top-10 and Top-50 levels to confirm that truncation improves performance and resolves 2-shot prompting issues.
  3. Recreate Table 5: Test different personas (Web Assistant, Generic User, UI Designer) to validate that persona choice significantly affects retrieval accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLMs (beyond Claude2) perform on the task of retrieving important UI elements, and what specific architectural features or training approaches make certain models more effective for this domain?
- Basis in paper: [explicit] The paper states that their study is "solely based on Anthropic's Claude2 model" and suggests future work should extend to other models like GPT-4, Llama V2, Vicuna, and PaLM 2, noting that these models have different context length limitations.
- Why unresolved: The paper only evaluates Claude2, leaving open questions about how other state-of-the-art LLMs would perform on the same tasks and what architectural differences might contribute to performance variations.
- What evidence would resolve it: A comparative study evaluating multiple LLMs (GPT-4, Llama V2, etc.) on the same Mind2Web dataset with identical prompt engineering approaches, measuring recall and element accuracy across different test sets.

### Open Question 2
- Question: What is the optimal balance between specificity in user queries and LLM performance, and can LLMs be trained to better handle abstract queries through instruction tuning or fine-tuning?
- Basis in paper: [explicit] The paper identifies a correlation between specificity of user query and LLM performance, noting that performance gradually decays from detailed to abstract descriptions, but fixed examples with abstract descriptions performed consistently well, suggesting LLMs' sensitivity to few-shot example design.
- Why unresolved: While the paper demonstrates that specificity affects performance, it doesn't explore whether LLMs can be improved to better understand abstract user intentions or what training approaches would be most effective for this adaptation.
- What evidence would resolve it: Experiments fine-tuning or instruction tuning LLMs on datasets containing abstract user queries paired with their intended UI interactions, measuring performance improvement on abstract query test sets.

### Open Question 3
- Question: How can HTML truncation strategies be optimized to maximize performance while minimizing information loss, and what is the theoretical limit of how much HTML context can be removed before performance degrades significantly?
- Basis in paper: [inferred] The paper shows that truncation at Top-10 and Top-50 improves performance compared to no truncation, particularly resolving issues with 2-shot prompting, but doesn't systematically explore the trade-off between context reduction and performance degradation.
- Why unresolved: The paper only tests two truncation levels (Top-10 and Top-50) without exploring intermediate values or analyzing the diminishing returns of context reduction, leaving open questions about optimal truncation strategies.
- What evidence would resolve it: A systematic analysis varying truncation levels (Top-5, Top-20, Top-30, etc.) while measuring performance degradation, identifying the optimal truncation point where further reduction causes significant performance loss.

## Limitations

- The study is limited to Claude2 and doesn't evaluate other state-of-the-art LLMs like GPT-4, Llama V2, or PaLM 2, leaving questions about model-specific performance differences
- The HTML truncation approach relies on a DeBERTa-based filtering module whose effectiveness depends on the quality and representativeness of its training data
- The paper doesn't systematically explore the trade-off between context reduction and performance degradation across multiple truncation levels

## Confidence

**High confidence** in the core finding that LLMs can retrieve UI elements with reasonable performance but have significant room for improvement. The experimental methodology is sound and the results are reproducible.

**Medium confidence** in the specific performance numbers and rankings between different prompting strategies. The results are sensitive to the exact examples used, truncation parameters, and model version, which aren't fully specified.

**Low confidence** in the generalizability of the HTML truncation approach. The effectiveness depends heavily on the DeBERTa model's training data and may not transfer well to web interfaces with different characteristics than those in Mind2Web.

## Next Checks

1. **Ground truth preservation rate**: Measure what percentage of ground truth elements are retained within the Top-10 truncation across all test samples, and analyze whether performance correlates with preservation rate.

2. **Embedding space validation**: Test whether the kNN example selection actually retrieves semantically relevant examples by manually evaluating a sample of example pairs and measuring the correlation between embedding distance and example usefulness.

3. **Persona prompt isolation**: Conduct controlled experiments where only the persona prompt varies while keeping all other prompt components constant, to isolate the specific impact of persona choice on performance.