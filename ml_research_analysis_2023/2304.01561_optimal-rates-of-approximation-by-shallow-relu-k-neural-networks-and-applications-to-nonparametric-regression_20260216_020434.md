---
ver: rpa2
title: Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications
  to nonparametric regression
arxiv_id: '2304.01561'
source_url: https://arxiv.org/abs/2304.01561
tags:
- neural
- networks
- approximation
- function
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies approximation rates for shallow ReLU neural
  networks. The authors show that sufficiently smooth functions are contained in the
  function space of shallow ReLU networks with finite variation norms.
---

# Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression

## Quick Facts
- arXiv ID: 2304.01561
- Source URL: https://arxiv.org/abs/2304.01561
- Reference count: 16
- This paper establishes optimal approximation rates for shallow ReLU neural networks and applies these results to nonparametric regression.

## Executive Summary
This paper studies the approximation capabilities of shallow ReLU$^k$ neural networks by analyzing the function spaces they can represent with finite variation norms. The authors establish conditions under which sufficiently smooth functions are exactly representable and derive approximation rates for less smooth functions. These theoretical results are then applied to analyze the performance of different neural network architectures (shallow, over-parameterized, and convolutional) in nonparametric regression tasks, showing that shallow networks can achieve minimax optimal rates for learning Hölder functions.

## Method Summary
The paper analyzes shallow ReLU$^k$ networks by defining the function space F$_\sigma^k$(M) with variation norm constraints. Using spherical harmonic analysis and Funk-Hecke formulas, the authors establish containment results for smooth functions and approximation bounds for less smooth functions. These theoretical results are then applied to nonparametric regression by connecting approximation rates to covering numbers and generalization bounds. Three neural network models are analyzed: shallow networks F$_\sigma$(N,M), over-parameterized networks NN(W,L,M), and convolutional networks CNN(s,L).

## Key Results
- Shallow ReLU$^k$ networks can achieve the optimal approximation rate O(N$^{-\alpha/d}$) for Hölder functions when $\alpha < (d + 2k + 1)/2$
- Functions with smoothness index $\alpha > (d + 2k + 1)/2$ are exactly representable in the shallow ReLU$^k$ network function space
- Shallow neural networks can achieve minimax optimal rates for learning Hölder functions, complementing recent results for deep networks
- Over-parameterized neural networks can achieve nearly optimal rates for nonparametric regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smooth functions with sufficient smoothness index $\alpha$ are contained in the shallow ReLU$^k$ network function space with finite variation norm.
- Mechanism: The authors establish that if the smoothness index $\alpha$ exceeds $(d + 2k + 1)/2$ or equals this critical value and is an even integer, then any function in the Hölder class H$_\alpha$ can be exactly represented by a shallow ReLU$^k$ network with bounded weights.
- Core assumption: The variation norm constraint on the network weights is sufficient to capture the regularity of smooth functions.
- Evidence anchors:
  - [abstract]: "It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms."
  - [section]: Theorem 2.1 states the condition $\alpha > (d + 2k + 1)/2$ for H$_\alpha$ $\subseteq$ F$_\sigma^k$(M).
- Break condition: If the smoothness $\alpha$ falls below the critical threshold, the containment no longer holds and only approximation is possible.

### Mechanism 2
- Claim: For less smooth functions ($\alpha < (d + 2k + 1)/2$), the approximation error by shallow ReLU$^k$ networks can be characterized by the variation norm.
- Mechanism: The authors prove that the approximation error is bounded by $M^{-2\alpha/(d+2k+1-2\alpha)}$, where $M$ is the variation norm bound. This rate improves upon previous bounds and is sharp for certain cases.
- Core assumption: The variation norm provides a meaningful measure of the network's approximation capability for non-smooth functions.
- Evidence anchors:
  - [abstract]: "For functions with less smoothness, the approximation rates in terms of the variation norm are established."
  - [section]: Theorem 2.1 gives the bound $\sup_{h\in H_\alpha} \inf_{f\in F_\sigma^k(M)} \|h-f\|_{L^\infty(B_d)} \lesssim M^{-2\alpha/(d+2k+1-2\alpha)}$.
- Break condition: If the variation norm $M$ is too small relative to the function's complexity, the approximation error will be unacceptably large.

### Mechanism 3
- Claim: The optimal approximation rate O(N$^{-\alpha/d}$) for Hölder functions by shallow ReLU$^k$ networks can be achieved by controlling the network weights.
- Mechanism: By combining the approximation bounds from Theorem 2.1 with random approximation results, the authors show that shallow ReLU$^k$ networks with $N$ neurons and appropriately constrained weights can achieve the optimal rate O(N$^{-\alpha/d}$) for $\alpha < (d + 2k + 1)/2$.
- Core assumption: The random approximation bounds from the literature can be applied to the specific structure of shallow ReLU$^k$ networks.
- Evidence anchors:
  - [abstract]: "Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks."
  - [section]: Corollary 2.4 derives the rate O(N$^{-\alpha/d}$) for shallow ReLU$^k$ networks.
- Break condition: If the activation function or network architecture deviates significantly from the assumptions, the rate may no longer be achievable.

## Foundational Learning

- Concept: Variation spaces corresponding to shallow ReLU$^k$ neural networks
  - Why needed here: Understanding the function spaces that shallow ReLU$^k$ networks can represent is crucial for analyzing their approximation properties.
  - Quick check question: What is the definition of the variation norm $\gamma(f)$ for a function $f$ in the context of shallow ReLU$^k$ networks?

- Concept: Hölder spaces and smoothness indices
  - Why needed here: The smoothness of functions being approximated determines the approximation rates achievable by neural networks.
  - Quick check question: How is the Hölder smoothness index $\alpha$ related to the differentiability and continuity properties of a function?

- Concept: Spherical harmonic analysis and Funk-Hecke formula
  - Why needed here: These tools are used to transfer the approximation problem from the unit ball to the sphere, enabling the use of harmonic analysis techniques.
  - Quick check question: What is the Funk-Hecke formula, and how does it relate the Fourier coefficients of a convolution on the sphere to the Fourier coefficients of the convolved functions?

## Architecture Onboarding

- Component map:
  - Shallow ReLU$^k$ neural network: N neurons, weights bounded by M (variation norm)
  - Deep neural network: Width W, depth L, weights bounded by M
  - Convolutional neural network: Filter length s, depth L
  - Activation function: ReLU$^k$ (k-th power of rectified linear unit)

- Critical path:
  1. Define the function space F$_\sigma^k$(M) for shallow ReLU$^k$ networks.
  2. Establish approximation bounds for smooth functions ($\alpha > (d + 2k + 1)/2$).
  3. Derive approximation rates for less smooth functions ($\alpha < (d + 2k + 1)/2$).
  4. Apply results to deep and convolutional neural networks.
  5. Analyze nonparametric regression performance using the approximation rates.

- Design tradeoffs:
  - Increasing the smoothness index $\alpha$ improves approximation rates but may require more complex functions.
  - Larger variation norm bounds $M$ allow for better approximation but may increase generalization error.
  - Deeper networks can potentially achieve better rates but may require more parameters and computational resources.

- Failure signatures:
  - If the smoothness index $\alpha$ is too low relative to the dimension $d$, the approximation rates will degrade.
  - If the variation norm bound $M$ is not properly chosen, the approximation may not achieve the optimal rate.
  - If the network architecture deviates significantly from the assumptions, the theoretical bounds may not hold.

- First 3 experiments:
  1. Implement a shallow ReLU$^k$ network with varying numbers of neurons $N$ and variation norm bounds $M$. Test the approximation error on functions from Hölder classes with different smoothness indices $\alpha$.
  2. Construct a deep neural network with varying widths $W$ and depths $L$, constrained by the weight bounds. Compare the approximation rates to those of shallow networks.
  3. Implement a convolutional neural network with varying filter lengths $s$ and depths $L$. Analyze the approximation performance on functions from Hölder classes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimal approximation rate O(N$^{-α/d}$) for shallow ReLU neural networks be extended to all ReLU$^k$ activations (not just ReLU and Heaviside)?
- Basis in paper: The paper establishes this rate for ReLU and Heaviside activations, but notes it's unknown for general ReLU$^k$.
- Why unresolved: The proof techniques used for ReLU and Heaviside may not generalize to all ReLU$^k$.
- What evidence would resolve it: Proving or disproving that shallow ReLU$^k$ neural networks can achieve O(N$^{-α/d}$) approximation rate for any ReLU$^k$ activation.

### Open Question 2
- Question: Can over-parameterized neural networks achieve the optimal rate for learning functions in Hölder space H$_\alpha$?
- Basis in paper: The paper shows over-parameterized networks can achieve nearly optimal rates, but the optimal rate is unknown.
- Why unresolved: The analysis of generalization error for over-parameterized networks may not be tight enough to prove optimality.
- What evidence would resolve it: Proving or disproving that over-parameterized networks can achieve the same rate as optimal networks for learning H$_\alpha$ functions.

### Open Question 3
- Question: Can the rate O(L$^{-2α/d}$) for deep neural networks be improved to O(L$^{-2α/d}$) for convolutional neural networks (CNNs)?
- Basis in paper: The paper establishes O(L$^{-α/d}$) for CNNs, while the rate O(L$^{-2α/d}$) is known for fully connected networks.
- Why unresolved: The bit extraction technique used for fully connected networks may not be applicable to CNNs.
- What evidence would resolve it: Proving or disproving that CNNs can achieve O(L$^{-2α/d}$) approximation rate using a similar technique to fully connected networks.

## Limitations

- The paper focuses on approximation in L$^\infty$(B$_d$) norm, while L$^2$(μ) norm is more relevant for regression applications.
- The weight constraint M needs careful calibration; too small leads to poor approximation, too large may hurt generalization.
- While optimality is proven for shallow networks, the comparative advantage over deep networks in practical settings remains unclear.

## Confidence

- **Containment of smooth functions**: High-confidence. The proof relies on well-established harmonic analysis techniques and the condition α > (d + 2k + 1)/2 is theoretically sound.
- **Approximation rates for non-smooth functions**: Medium-confidence. While theoretically derived, the tightness of the M$^{-2α/(d+2k+1-2α)}$ rate needs empirical validation.
- **Application to nonparametric regression**: Medium-confidence. The theoretical framework is sound but practical implementation details could affect observed rates.

## Next Checks

1. **Empirical verification of approximation rates**: Implement shallow ReLU$^k$ networks with varying numbers of neurons and variation norm bounds. Test approximation errors on functions from different Hölder classes to verify the theoretical O(N$^{-α/d}$) rates.

2. **Robustness to activation function variants**: Test whether the theoretical bounds hold for related activation functions (e.g., Leaky ReLU, ELU) that satisfy similar smoothness conditions but differ in specific properties.

3. **Generalization bounds verification**: Conduct experiments on nonparametric regression tasks to empirically validate the expected excess risk bounds, particularly examining the transition between the shallow network regime and over-parameterized regime.