---
ver: rpa2
title: On The Truthfulness of 'Surprisingly Likely' Responses of Large Language Models
arxiv_id: '2311.07692'
source_url: https://arxiv.org/abs/2311.07692
tags:
- questions
- language
- information
- mxratio
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method inspired by game-theoretic truthful
  information elicitation literature to improve the accuracy of responses from large
  language models (LLMs). The method, called the "surprisingly likely" criterion,
  rewards LLM responses that are more commonly reported than predicted a priori by
  the model.
---

# On The Truthfulness of 'Surprisingly Likely' Responses of Large Language Models

## Quick Facts
- arXiv ID: 2311.07692
- Source URL: https://arxiv.org/abs/2311.07692
- Reference count: 40
- Key outcome: Novel method improves LLM truthfulness by up to 24 percentage points on TruthfulQA benchmark

## Executive Summary
This paper introduces a novel approach to improving LLM truthfulness by leveraging game-theoretic principles from truthful information elicitation. The method, called the "surprisingly likely" criterion, rewards responses that are more commonly reported than predicted a priori by the model. The authors demonstrate significant improvements in accuracy across multiple benchmarks including TruthfulQA, COPA, and StoryCloze, with gains of up to 24 percentage points. The approach shows particular promise in mitigating inverse scaling effects where larger models become less truthful.

## Method Summary
The surprisingly likely criterion works by comparing posterior likelihood (probability of response given context) with prior likelihood (probability of response given minimal context). For each candidate response, the method computes the ratio or difference between these two probabilities, selecting responses where the posterior is disproportionately high compared to the prior. This rewards answers that are more commonly reported in the training data than would be expected based on general domain frequency, effectively identifying responses that are "surprisingly common" given their context.

## Key Results
- TruthfulQA accuracy improved by up to 24 percentage points using MxRatio method
- MxRatio method showed robustness to inverse scaling across different model sizes
- Top2MinPr variant (considering top two responses) achieved additional improvements
- Method maintained performance under 4-bit quantization of LLaMA-2 70B model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surprisingly likely criterion improves LLM truthfulness by favoring responses that are both frequent in training data and contextually appropriate, correcting for naive likelihood maximization.
- Mechanism: The method computes the ratio between posterior likelihood P(r|q) and prior likelihood P(r|'?'), selecting responses where this ratio is high. This rewards answers more commonly reported than predicted a priori.
- Core assumption: Training corpus contains more accurate than misinformation on average, and prior captures general domain frequency while posterior captures context-specific appropriateness.
- Break condition: If training data is heavily biased toward misinformation, or if prior context ' ?' does not adequately represent general domain frequency.

### Mechanism 2
- Claim: The method mitigates inverse scaling where larger models become less truthful by leveraging statistical regularities in training corpus.
- Mechanism: By comparing prior and posterior, the method compensates for larger models' tendency to overfit popular but incorrect patterns, instead selecting responses unusually common given context.
- Core assumption: Larger models amplify biases present in training data, and these biases can be detected by comparing context-specific and general frequencies.
- Break condition: If model's probability estimates are poorly calibrated or if prior context does not capture relevant statistical baseline.

### Mechanism 3
- Claim: The criterion aligns with game-theoretic truthful elicitation by rewarding responses that would be truthful in a crowd setting.
- Mechanism: The method implicitly implements a peer prediction mechanism where reward is proportional to mutual information between answer and other potential answers, similar to Bayesian Truth Serum.
- Core assumption: LLM training data can be modeled as reports from a crowd, and truthful information is more likely to be surprisingly common than false information.
- Break condition: If training data does not represent a diverse crowd or if false information is systematically more common than true information.

## Foundational Learning

- Concept: Bayesian inference and posterior/prior probability
  - Why needed here: The method relies on computing and comparing posterior and prior likelihoods to identify surprisingly likely responses
  - Quick check question: What is the difference between posterior P(r|q) and prior P(r|'?') probability in this context?

- Concept: Pointwise mutual information (PMI)
  - Why needed here: The ratio between posterior and prior is mathematically similar to PMI, which measures the association between context and response
  - Quick check question: How does the PMI between context and response relate to the surprisingly likely criterion?

- Concept: Inverse scaling in LLMs
  - Why needed here: Understanding why larger models may become less truthful helps explain why the surprisingly likely criterion is particularly useful
  - Quick check question: What is inverse scaling, and how might it affect LLM truthfulness on benchmarks like TruthfulQA?

## Architecture Onboarding

- Component map: LLM inference engine → prior likelihood computation (empty or minimal context) → posterior likelihood computation (full context) → ratio calculation → response selection
- Critical path: Compute P(r|'?') and P(r|q) for candidate responses → calculate ratio/difference → select response with highest score
- Design tradeoffs: Computing priors requires additional forward passes or cached probabilities; using difference vs ratio affects sensitivity to low-probability responses
- Failure signatures: Method degrades when training data is heavily biased, when prior context poorly represents domain frequency, or when probability estimates are poorly calibrated
- First 3 experiments:
  1. Implement MxRatio selection on TruthfulQA with GPT-2 small to verify improvement over MxPost
  2. Test Top2MinPr method to confirm gains from considering multiple top responses
  3. Evaluate robustness to 4-bit quantization by comparing MxRatio performance on full vs quantized LLaMA-2 70B

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what theoretical conditions does the surprisingly likely criterion improve LLM truthfulness?
- Basis in paper: The authors conclude by noting "It will be interesting future work to construct precise theoretical models under which an approach like this works or does not work well for LLMs."
- Why unresolved: The paper presents empirical results but lacks theoretical analysis of when the approach succeeds or fails.
- What evidence would resolve it: Mathematical proofs or formal models showing the conditions (e.g., types of questions, data distributions) where the surprisingly likely criterion is guaranteed to improve truthfulness.

### Open Question 2
- Question: What are the most effective ways to operationalize the surprisingly likely criterion for real-time LLM response generation?
- Basis in paper: The authors note that "it remains crucial future work to show how this can be best operationalised to make LLMs generate more correct responses in the first place."
- Why unresolved: The paper only demonstrates the criterion's effectiveness in post-hoc answer selection, not in guiding generation.
- What evidence would resolve it: Experiments comparing different implementation strategies (e.g., modifying decoding algorithms, incorporating into pre-training or RLHF) for improving generation accuracy.

### Open Question 3
- Question: How does conditioning on different types of "prior" contexts affect the performance of the surprisingly likely criterion?
- Basis in paper: The authors state "there may exist other ways [to measure surprising likeliness] depending on the theoretical model. For example, for prior, one may condition on less specific contexts (e.g. some keywords from the question) instead of conditioning on just '?'."
- Why unresolved: The paper only experiments with two types of priors (empty context vs. question context) and suggests other possibilities without testing them.
- What evidence would resolve it: Systematic experiments comparing the criterion's performance when using different prior contexts (keywords, related questions, etc.) across multiple benchmarks.

## Limitations

- The method assumes training corpora contain more accurate than inaccurate information on average, which may not hold for domains where misinformation is prevalent.
- The prior context string ' ?' is proposed as a proxy for general domain frequency, but its effectiveness depends on how well it captures the relevant statistical baseline.
- Results show significant improvements on specific benchmarks but may not generalize to other domains or question types.

## Confidence

High confidence in the core methodology: The surprisingly likely criterion is well-defined, the implementation details are clear, and the improvements on standard benchmarks are substantial and reproducible.

Medium confidence in mechanism claims: While the theoretical motivation from game-theoretic literature is sound, the paper provides limited empirical evidence directly connecting the method to crowd-truthfulness principles.

Low confidence in generalizability: Results are based on specific benchmarks and model architectures; performance on real-world applications with different data distributions remains untested.

## Next Checks

1. **Dataset Diversity Test**: Evaluate the method on additional benchmarks beyond TruthfulQA, COPA, and StoryCloze, including those focused on technical domains, current events, or specialized knowledge where training data quality may vary significantly.

2. **Robustness to Misinformation**: Create or identify test sets where common but incorrect answers are prevalent in training data, then assess whether the surprisingly likely criterion can distinguish these from correct but less common answers.

3. **Calibration Analysis**: Measure the calibration of posterior and prior probability estimates across different model sizes and domains, testing whether poor calibration undermines the method's effectiveness in practice.