---
ver: rpa2
title: 'BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection'
arxiv_id: '2306.08852'
source_url: https://arxiv.org/abs/2306.08852
tags:
- detection
- training
- entropy
- label
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for out-of-distribution
  (OOD) detection in NLP using bi-encoder-based detectors. The method leverages bi-encoders
  to extract meaningful representations from textual data and employs efficient detection
  mechanisms.
---

# BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2306.08852
- Source URL: https://arxiv.org/abs/2306.08852
- Authors: 
- Reference count: 0
- Primary result: Novel bi-encoder-based detectors achieve superior OOD detection performance without labeled OOD samples across multiple benchmark datasets.

## Executive Summary
This paper introduces a novel approach for out-of-distribution (OOD) detection in NLP using bi-encoder-based detectors. The method leverages bi-encoders to extract meaningful representations from textual data and employs efficient detection mechanisms. Experimental results on benchmark datasets (CLINC150, ROSTD-Coarse, SNIPS, YELLOW) show that the proposed bi-encoder-based detectors outperform other methods, both those requiring OOD labels in training and those that do not, across all datasets. The detectors achieve superior detection performance without access to labeled OOD samples during training, simplifying the training process and improving scalability.

## Method Summary
The method uses a bi-encoder architecture with two separate encoders for input text and target/reference text. Training uses cosine similarity loss to maximize similarity between related pairs and minimize it for unrelated pairs. The approach extracts embeddings using feature extractors like MPNET, BERT, USE, and GLOVE, then computes similarity/distance scores using various metrics (cosine, Euclidean, Mahalanobis, etc.). OOD classification is performed by comparing these scores against thresholds. The method supports multiple variants including BiEncoderCosine, BiEncoderEuclidean, BiEncoderMaha, and PCA-based variants.

## Key Results
- Bi-encoder-based detectors outperform methods requiring OOD labels during training across all benchmark datasets
- MPNET feature extractor demonstrates superior performance compared to BERT, USE, and GLOVE
- Different bi-encoder variants (cosine, Euclidean, Mahalanobis) are needed for different data distributions
- The method achieves high F1-Score, MCC, AUPR, and AUROC while maintaining low false positive rates (FPR@90, FPR@95)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bi-encoder-based detectors achieve superior OOD detection performance without labeled OOD samples by leveraging cosine similarity between input utterances and training utterances.
- Mechanism: The bi-encoder model encodes both the input utterance and training utterances into embeddings, then computes cosine similarity scores. Inputs with similarity scores below a threshold are classified as OOD. This approach does not require explicit OOD labels during training, only in-domain intent utterances.
- Core assumption: The semantic similarity captured by the bi-encoder embeddings is discriminative enough to separate in-domain from out-of-domain utterances.
- Evidence anchors:
  - [abstract] "The key advantage of our proposed method lies in its ability to achieve superior OOD detection performance, even without access to labeled OOD samples during the training phase."
  - [section] "Our proposed algorithm used cosine-similarity loss to ensure our model is fine-tuned to recognize the similarity of sentences."
  - [corpus] Weak corpus evidence for this specific bi-encoder mechanism.
- Break condition: If the bi-encoder embeddings do not capture semantic similarity well, or if in-domain and OOD utterances have similar semantic structures, the method will fail.

### Mechanism 2
- Claim: Using MPNET as the feature extractor provides more effective embeddings for OOD detection compared to other feature extractors like BERT and USE.
- Mechanism: MPNET is a pre-trained transformer model fine-tuned on a large and diverse dataset, producing embeddings that better capture semantic nuances relevant to intent classification and OOD detection.
- Core assumption: The semantic information captured by MPNET embeddings is more discriminative for intent classification and OOD detection than embeddings from other feature extractors.
- Evidence anchors:
  - [section] "Last but not least, we found that, across all datasets, MPNET appears to be the most effective feature extractor compared to USE and BERT."
  - [abstract] No direct mention of MPNET superiority, but the overall method's effectiveness implies good feature extraction.
  - [corpus] No corpus evidence directly supporting MPNET superiority for OOD detection.
- Break condition: If MPNET embeddings do not capture the relevant semantic information for the specific dataset domains, or if other feature extractors perform equally well, this mechanism breaks.

### Mechanism 3
- Claim: Different variants of BiEncoder (Cosine, Euclidean, Mahalanobis, etc.) are needed for different data distributions, allowing flexibility in handling diverse datasets.
- Mechanism: The paper experiments with multiple similarity/distance metrics and dimensionality reduction (PCA) variants to find the optimal configuration for each dataset's characteristics.
- Core assumption: The optimal similarity/distance metric and dimensionality reduction approach varies depending on the dataset's distribution and characteristics.
- Evidence anchors:
  - [section] "We also found that different variants of BiEncoder are required for different data distributions."
  - [abstract] No direct mention of different variants being needed for different datasets.
  - [corpus] No corpus evidence directly supporting this claim.
- Break condition: If a single variant works well across all datasets, or if the optimal variant cannot be determined without extensive experimentation, this mechanism breaks.

## Foundational Learning

- Concept: Cosine similarity and distance metrics (Euclidean, Mahalanobis)
  - Why needed here: These metrics are used to compare embeddings and determine OOD vs. in-domain classification.
  - Quick check question: What is the difference between cosine similarity and Euclidean distance when comparing embeddings?

- Concept: Dimensionality reduction (PCA)
  - Why needed here: PCA is used to transform embeddings before calculating similarity/distance scores, potentially improving discrimination.
  - Quick check question: When might applying PCA to embeddings before similarity calculation improve OOD detection performance?

- Concept: Feature extraction with pre-trained models (MPNET, BERT, USE)
  - Why needed here: Feature extractors convert raw text into meaningful embeddings that capture semantic information.
  - Quick check question: What are the key differences between MPNET, BERT, and USE in terms of their pre-training objectives and architectures?

## Architecture Onboarding

- Component map: Text input → Feature extraction → Bi-encoder embedding → Similarity/distance calculation → Threshold comparison → OOD classification

- Critical path: Text input → Feature extraction → Bi-encoder embedding → Similarity/distance calculation → Threshold comparison → OOD classification

- Design tradeoffs:
  - MPNET vs. other feature extractors: MPNET is more effective but may be larger/more computationally expensive
  - Different similarity/distance metrics: Each has different characteristics (e.g., cosine is scale-invariant, Euclidean is sensitive to magnitude)
  - Whether to use OOD labels in training: Without OOD labels simplifies training but may reduce performance

- Failure signatures:
  - High FPR@95/FPR@90: Model is too conservative, flagging too many in-domain samples as OOD
  - Low F1-Score/MCC: Poor overall classification performance, possibly due to poor feature extraction or inappropriate similarity metric
  - No improvement over baselines: Feature extractor or similarity metric is not effective for the dataset

- First 3 experiments:
  1. Train BiEncoderCosine with MPNET on CLINC150, evaluate F1-Score and FPR@95
  2. Compare BiEncoderCosine vs. BiEncoderEuclidean on the same dataset to determine which metric works better
  3. Try using BERT instead of MPNET to see if performance degrades, confirming MPNET's effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the bi-encoder-based detectors perform on datasets with significantly different data distributions compared to the ones used in this study?
- Basis in paper: [explicit] The paper mentions that the proposed method is evaluated on benchmark datasets including CLINC150, ROSTD-Coarse, SNIPS, and YELLOW, covering diverse domains and data distributions.
- Why unresolved: The study focuses on specific benchmark datasets, and the performance of the bi-encoder-based detectors on other datasets with different characteristics is not explored.
- What evidence would resolve it: Evaluating the bi-encoder-based detectors on additional datasets with varying data distributions and comparing their performance to other methods.

### Open Question 2
- Question: What is the impact of using different feature extractors on the performance of the bi-encoder-based detectors?
- Basis in paper: [explicit] The paper mentions the use of various feature extractors such as USE, BERT, MPNET, and GLOVE, and suggests that MPNET appears to be the most effective feature extractor.
- Why unresolved: The paper does not provide a detailed comparison of the performance of bi-encoder-based detectors when using different feature extractors, and the reasons behind the effectiveness of MPNET are not explored.
- What evidence would resolve it: Conducting experiments using different feature extractors with the bi-encoder-based detectors and analyzing the impact on their performance.

### Open Question 3
- Question: How do the bi-encoder-based detectors perform in real-world scenarios with noisy and imbalanced data?
- Basis in paper: [inferred] The paper mentions that the proposed method is applicable to real-world scenarios and demonstrates superior detection performance, but it does not explicitly address the performance in the presence of noisy and imbalanced data.
- Why unresolved: The study focuses on benchmark datasets, which may not fully represent the challenges of real-world data, such as noise and class imbalance.
- What evidence would resolve it: Evaluating the bi-encoder-based detectors on real-world datasets with noisy and imbalanced data and comparing their performance to other methods.

## Limitations
- The optimal similarity metric and feature extractor appear to vary by dataset, requiring extensive hyperparameter tuning rather than being a universal solution
- MPNET's superiority over other feature extractors lacks corpus evidence, relying primarily on experimental results from this single study
- The method's performance without OOD labels is demonstrated but not theoretically guaranteed to be superior to methods that use OOD information

## Confidence
**High Confidence**: The bi-encoder architecture itself is well-established in NLP, and the general approach of using similarity metrics for OOD detection is theoretically sound. The experimental results showing the method works on the tested datasets are reproducible if the implementation details can be matched.

**Medium Confidence**: The claim that the method works without OOD labels during training is supported by experimental results but lacks theoretical guarantees. The superiority of MPNET over other feature extractors is demonstrated empirically but not explained mechanistically.

**Low Confidence**: The claim that different bi-encoder variants are needed for different datasets is based on experimental observation but lacks theoretical justification for why specific metrics would work better for specific distributions.

## Next Checks
1. **Ablation study with OOD labels**: Train the same bi-encoder architecture but include OOD samples in training to determine if the claimed advantage of not needing OOD labels is genuine or if the method performs better when OOD information is available.

2. **Feature extractor comparison on held-out datasets**: Test the MPNET vs. BERT vs. USE comparison on additional datasets not used in the original study to validate whether MPNET's superiority is consistent across domains or specific to the tested datasets.

3. **Single-variant cross-dataset validation**: Rather than tuning the similarity metric per dataset, test whether a single bi-encoder variant (e.g., BiEncoderCosine) can achieve competitive performance across all datasets without per-dataset optimization.