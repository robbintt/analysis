---
ver: rpa2
title: 'NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation'
arxiv_id: '2312.05979'
source_url: https://arxiv.org/abs/2312.05979
tags:
- what
- knowledge
- commonsense
- novacomet
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NovaCOMET, an open commonsense knowledge
  model that combines the advantages of both knowledge models and general task models.
  NovaCOMET models commonsense knowledge with an open format, allowing it to be applied
  to general reasoning tasks.
---

# NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation

## Quick Facts
- arXiv ID: 2312.05979
- Source URL: https://arxiv.org/abs/2312.05979
- Reference count: 40
- Key outcome: NovaCOMET combines symbolic knowledge distillation with open-format training to outperform comparable open task models on commonsense generation tasks while demonstrating competitive performance on discriminative benchmarks.

## Executive Summary
NovaCOMET is an open commonsense knowledge model that leverages symbolic knowledge distillation to create an auditable, reusable knowledge graph from proprietary language models. By using natural language queries as relations and allowing flexible masking of context-query-inference fields, NovaCOMET achieves superior performance on commonsense generation tasks compared to traditional fixed-relation knowledge models. The model demonstrates the advantage of explicitly modeling commonsense knowledge through its strong results on abductive infilling and explanation generation tasks.

## Method Summary
The method involves distilling commonsense knowledge from GPT-3 and GPT-3.5 models into an open-format knowledge graph (NovATOMIC) using natural language queries. Contexts are generated using 21 varied prompts, and CQI triples are created using handwritten examples and language model generation. Human annotations provide plausibility scores that filter or condition the training data. The model is trained on T5 1.1 xxl with commonsense field masking, and evaluated on both generative and discriminative commonsense benchmarks using automatic metrics and human evaluation.

## Key Results
- NovaCOMET outperforms comparable open task models like Flan-T5 on commonsense generation tasks including αNLG and abductive infilling
- The model shows competitive performance on discriminative commonsense benchmarks when used as a plausibility critic
- Models using plausibility information demonstrate improved generation quality over basic versions
- Open-format training objective enables better generalization compared to fixed relation sets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Symbolic knowledge distillation from large language models into an open, discrete knowledge graph enables both transparency and scalability.
- **Mechanism**: The distillation process transforms opaque LLM outputs into a structured, queryable format (NovATOMIC) that can be audited, filtered, and used to train smaller models like NovaCOMET.
- **Core assumption**: LLM-generated commonsense knowledge is high quality enough that symbolic distillation preserves its reasoning ability while making it usable for downstream tasks.
- **Evidence anchors**: [abstract] "NovaCOMET leverages the knowledge of opaque proprietary models to create an open knowledge pipeline." [section] "knowledge is symbolically distilled into NovATOMIC, a publicly-released discrete knowledge graph which can be audited, critiqued, and filtered."

### Mechanism 2
- **Claim**: Using open-format queries (natural language relations) instead of fixed relation sets improves task generalization.
- **Mechanism**: Training on CQI (context-query-inference) triples with masked spans allows the model to generate arbitrary relations and flexibly apply commonsense to novel tasks.
- **Core assumption**: Natural language queries can encode richer and more diverse commonsense relations than a predefined fixed set.
- **Evidence anchors**: [abstract] "NovaCOMET uses an open-format training objective, replacing the fixed relation sets of past knowledge models." [section] "We use natural language queries as relations, and allow masked generation of all aspects of the data."

### Mechanism 3
- **Claim**: Combining plausibility annotations with symbolic knowledge distillation improves generation quality.
- **Mechanism**: Human-annotated plausibility scores filter or condition the training data, biasing the model toward more likely commonsense inferences.
- **Core assumption**: Plausibility annotations correlate with downstream task performance and human judgment of commonsense quality.
- **Evidence anchors**: [abstract] "optionally augmented with human annotation, matches or exceeds comparable open task models." [section] "we use a simple filtering-based technique for improving generation with plausibility scores."

## Foundational Learning

- **Concept**: Symbolic knowledge distillation
  - Why needed here: Transforms opaque LLM knowledge into an auditable, reusable format for training smaller models.
  - Quick check question: What is the difference between symbolic and implicit knowledge distillation?

- **Concept**: Open-format query modeling
  - Why needed here: Enables flexible generation of commonsense relations rather than being constrained to a fixed set.
  - Quick check question: How does masking CQI fields improve model flexibility?

- **Concept**: Plausibility scoring
  - Why needed here: Provides a filter or conditioning signal to improve the quality of generated commonsense knowledge.
  - Quick check question: Why is human annotation important for plausibility scoring in this pipeline?

## Architecture Onboarding

- **Component map**: GPT-based knowledge generator → NovATOMIC knowledge graph → MTurk annotation interface → Plausibility scores → T5-based generation model → NovaCOMET variants → Evaluation harnesses
- **Critical path**: 1. Generate contexts and CQI triples from GPT models. 2. Collect plausibility annotations via MTurk. 3. Filter or condition training data based on plausibility. 4. Fine-tune T5 on filtered/conditioned data. 5. Evaluate on downstream commonsense benchmarks.
- **Design tradeoffs**:
  - Fixed relations: Faster training, limited expressiveness
  - Open-format queries: More flexible, higher computational cost
  - Plausibility filtering: Higher quality, smaller dataset
  - Reward conditioning: More complex training, potentially better quality
- **Failure signatures**:
  - High perplexity on masked CQI fields → masking strategy too aggressive
  - Low accuracy on plausibility evaluation → annotations misaligned with model
  - Poor performance on αNLG → missing reasoning patterns in training data
- **First 3 experiments**:
  1. Train baseline T5 on raw NovATOMIC without masking; evaluate perplexity
  2. Train with full CQI masking; compare to baseline on generation tasks
  3. Apply plausibility filtering (0.99 threshold); compare performance to unfiltered model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NovaCOMET compare to proprietary models like GPT-4 or ChatGPT when evaluated on the same tasks?
- Basis in paper: [inferred] The paper mentions that NovaCOMET matches or exceeds comparable open task models like Flan-T5 on commonsense generation tasks, but does not directly compare to proprietary models like GPT-4 or ChatGPT.
- Why unresolved: The paper focuses on comparing NovaCOMET to other open models rather than proprietary ones. A direct comparison with the latest proprietary models would provide a clearer picture of NovaCOMET's relative performance.
- What evidence would resolve it: Running NovaCOMET and proprietary models (GPT-4, ChatGPT) on the same set of commonsense tasks and comparing their performance metrics.

### Open Question 2
- Question: How does the quality of knowledge generated by NovaCOMET vary across different domains or types of commonsense knowledge?
- Basis in paper: [explicit] The paper mentions that Physical IQA (PIQA), which focuses on physical commonsense, was a subcategory that the base generator seemed to produce less naturally.
- Why unresolved: While the paper touches on this for one specific domain, it does not provide a comprehensive analysis of how NovaCOMET performs across various domains of commonsense knowledge.
- What evidence would resolve it: Systematically evaluating NovaCOMET's performance on a diverse set of commonsense benchmarks covering different domains (social, physical, temporal, etc.) and analyzing the results.

### Open Question 3
- Question: How does the use of plausibility annotations impact the performance of NovaCOMET on different types of tasks?
- Basis in paper: [explicit] The paper mentions that NovaCOMET models that use plausibility information outperform the basic NovaCOMET base, but also notes that this was not the case for the TellMeWhy dataset.
- Why unresolved: The paper provides some insights into the impact of plausibility annotations but does not offer a detailed analysis of how this varies across different task types.
- What evidence would resolve it: Conducting a detailed study comparing the performance of NovaCOMET with and without plausibility annotations on a wide range of commonsense tasks, and analyzing the results to identify patterns or trends.

## Limitations
- The distillation process relies on the assumption that proprietary model outputs are high-quality commonsense knowledge, but this is difficult to audit due to the opacity of the source models.
- The open-format approach, while more flexible, may introduce ambiguity in relation generation compared to fixed relation sets.
- Plausibility filtering depends heavily on the quality and representativeness of human annotations.

## Confidence
- **High Confidence**: The empirical improvements on commonsense generation tasks (αNLG, Abductive Infilling) are well-supported by automatic and human evaluation metrics.
- **Medium Confidence**: The mechanism of symbolic knowledge distillation preserving reasoning ability is plausible but not directly validated in isolation.
- **Medium Confidence**: The advantage of open-format queries over fixed relations is demonstrated in task performance but not explicitly compared in ablation studies.

## Next Checks
1. **Ablation Study on Relation Format**: Train a version of NovaCOMET with fixed relations and compare its performance to the open-format version on the same benchmarks to isolate the contribution of query flexibility.
2. **Quality Audit of Distilled Knowledge**: Sample and manually evaluate a subset of NovATOMIC triples to assess the quality and diversity of knowledge before and after plausibility filtering.
3. **Robustness to Annotation Noise**: Train NovaCOMET with varying levels of synthetic annotation noise to measure the impact of annotation quality on downstream performance.