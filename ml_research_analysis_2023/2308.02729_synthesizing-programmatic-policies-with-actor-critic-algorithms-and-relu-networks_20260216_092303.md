---
ver: rpa2
title: Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks
arxiv_id: '2308.02729'
source_url: https://arxiv.org/abs/2308.02729
tags:
- policies
- neural
- relu
- trees
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that PIRL-specific algorithms are not needed for
  encoding policies in if-then-else structures, linear transformations, and PID operations.
  Instead, one can directly train a neural policy with actor-critic algorithms and
  translate it into a programmatic policy using a connection between ReLU neural networks
  and oblique decision trees.
---

# Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks

## Quick Facts
- arXiv ID: 2308.02729
- Source URL: https://arxiv.org/abs/2308.02729
- Reference count: 3
- This paper shows that PIRL-specific algorithms are not needed for encoding policies in if-then-else structures, linear transformations, and PID operations. Instead, one can directly train a neural policy with actor-critic algorithms and translate it into a programmatic policy using a connection between ReLU neural networks and oblique decision trees.

## Executive Summary
This paper presents a novel approach to synthesizing programmatic policies for control problems by leveraging actor-critic reinforcement learning algorithms and ReLU neural networks. The key insight is that ReLU networks can be directly translated into oblique decision trees, which are equivalent to if-then-else programs with linear transformations. This eliminates the need for specialized PIRL algorithms that require an oracle neural policy for synthesis. The method is evaluated on several control problems from OpenAI Gym and MuJoCo, demonstrating that the resulting programmatic policies are competitive with and often superior to those synthesized by existing PIRL methods.

## Method Summary
The approach trains small neural network policies using actor-critic methods (PPO or SAC), then translates them into programmatic policies using a connection between ReLU networks and oblique decision trees. The translation uses the RES algorithm to extract the decision boundaries and linear transformations from the trained network. The resulting programmatic policies are represented as oblique decision trees with if-then-else structures and linear transformations of the input. The method employs L1 regularization during training to induce sparsity in the resulting trees, improving interpretability.

## Key Results
- Programmatic policies synthesized through neural network translation achieve competitive or superior performance compared to PIRL-specific methods
- The translation approach successfully learns short and effective policies for various control problems
- L1 regularization effectively induces sparse oblique trees, improving interpretability without significant performance loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU networks can be mapped directly to oblique decision trees, producing interpretable programmatic policies.
- Mechanism: Each activation pattern in a ReLU network defines a linear region, which corresponds to a path in an oblique decision tree. The tree's nodes encode the inequalities that define which neurons are active for a given input, and the leaves output linear transformations of the input.
- Core assumption: The mapping between ReLU activation patterns and oblique decision tree paths is exact and reversible.
- Evidence anchors:
  - [abstract] "We use a connection between ReLU neural networks and oblique decision trees (ODT) to translate the policy learned with actor-critic algorithms into programmatic policies."
  - [section] "Since an ODT is a program with if-then-else structures and linear transformations of the input, the ODT we obtain from the translation procedure is, in fact, a program written in a language previously used to encode programmatic policies."
  - [corpus] Weak evidence; no corpus papers directly discuss ReLU-to-ODT mapping in RL context.
- Break condition: If the ReLU network uses activation functions other than ReLU, or if the network has multiple output neurons with Softmax (unless handled specially), the mapping breaks down.

### Mechanism 2
- Claim: Actor-critic algorithms can directly train small ReLU networks that are then converted to programmatic policies, avoiding the need for oracle-guided PIRL search.
- Mechanism: Small actor networks (with few neurons) are trained using PPO or SAC, then the ReLU-to-ODT translation produces a compact programmatic policy. The critic can be arbitrarily large, since only the actor is translated.
- Core assumption: Small ReLU networks are sufficient to learn effective policies for the tested control problems.
- Evidence anchors:
  - [abstract] "Instead of training a neural policy to serve as an oracle for the synthesis procedure, we simply train a neural policy with an actor-critic algorithm and translate it into a programmatic one."
  - [section] "We use large networks for the critic and small ones for the actor as it is the latter that is translated into a program, and smaller networks result in shorter and often more interpretable programs."
  - [corpus] Weak evidence; no corpus papers compare actor-critic with PIRL directly.
- Break condition: If the control problem requires complex, non-linear decision boundaries that cannot be captured by a small ReLU network, the resulting programmatic policy will be ineffective.

### Mechanism 3
- Claim: L1 regularization on ReLU networks induces sparse oblique trees, increasing interpretability.
- Mechanism: During training, L1 regularization drives some weights toward zero, causing some neurons in the first hidden layer to become irrelevant. This translates to nodes in the oblique tree with zeroed-out weights, making the tree sparser and more interpretable.
- Core assumption: L1 regularization effectively induces sparsity in the first layer of the ReLU network.
- Evidence anchors:
  - [section] "R ES allows for the induction of sparse trees if one uses L1-regularization while training the underlying neural network."
  - [section] "L1 regularization is effective in inducing sparse oblique trees with R ES if the network has a single hidden layer."
  - [corpus] Weak evidence; no corpus papers discuss L1 regularization in this specific mapping context.
- Break condition: If the problem requires dense decision boundaries, L1 regularization may overly simplify the policy, reducing performance.

## Foundational Learning

- Concept: ReLU activation functions and their properties
  - Why needed here: The entire translation mechanism relies on ReLU networks mapping to oblique decision trees via activation patterns.
  - Quick check question: What is the output of a ReLU neuron when its pre-activation input is negative?

- Concept: Oblique decision trees (ODTs)
  - Why needed here: The programmatic policies are represented as ODTs, which are equivalent to if-then-else programs with linear transformations.
  - Quick check question: How does an ODT node decide which child to follow for a given input?

- Concept: Actor-critic reinforcement learning algorithms (PPO, SAC)
  - Why needed here: These algorithms are used to train the neural network policies that are then translated.
  - Quick check question: What is the difference between the actor and critic networks in actor-critic methods?

## Architecture Onboarding

- Component map:
  - Environment (OpenAI Gym/MuJoCo) -> Actor network (small ReLU network) -> Critic network (large neural network) -> Translation module (RES algorithm) -> Program output (oblique decision tree / programmatic policy)

- Critical path:
  1. Initialize actor and critic networks
  2. Train actor and critic using actor-critic algorithm (PPO/SAC)
  3. Extract actor weights and biases
  4. Apply RES translation to convert actor to oblique decision tree
  5. Output programmatic policy

- Design tradeoffs:
  - Small actor networks yield shorter, more interpretable programs but may limit policy expressiveness.
  - Large critic networks improve value estimation but don't affect program length.
  - L1 regularization increases sparsity and interpretability but may reduce performance.

- Failure signatures:
  - Poor training performance: Check actor network size, learning rate, and algorithm hyperparameters.
  - Translated program is too large: Reduce actor network size or apply RES(k) pruning.
  - Translated program performs poorly: Check if problem requires more complex decision boundaries than small ReLU network can capture.

- First 3 experiments:
  1. Train a small actor network (1-2 hidden neurons) on Pendulum using SAC, then apply RES translation. Verify the resulting program balances the pendulum.
  2. Train an actor network on MountainCarContinuous, apply RES, and check if the program learns the "build momentum by moving left first" strategy.
  3. Apply L1 regularization during training on Reacher, then use RES to generate a sparse program. Compare performance and interpretability with unregularized version.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the translation of ReLU networks to oblique decision trees scale to very deep networks or very wide networks in terms of preserving the interpretability and computational efficiency?
- Basis in paper: [explicit] The paper mentions that R ES can handle larger networks and longer programs but does not empirically evaluate the scaling limits.
- Why unresolved: The empirical results only show translation of depth-6 and depth-32 networks. The interpretability and computational efficiency of deeper or wider networks remains unexplored.
- What evidence would resolve it: Experiments showing the performance and interpretability of R ES when applied to networks with more than 32 neurons per layer or more than 6 layers.

### Open Question 2
- Question: Can the R ES method be extended to handle programmatic policies that include loops, conditional branching with more complex conditions, or state-based transitions?
- Basis in paper: [inferred] The paper states that the current approach only supports if-then-else structures and linear transformations, while other methods (e.g., NDPS, PROPEL) can potentially handle more expressive languages.
- Why unresolved: The connection between ReLU networks and oblique decision trees inherently limits the expressiveness to tree-like structures without loops or complex state transitions.
- What evidence would resolve it: A modified R ES approach that successfully synthesizes programs with loops or more complex control flow and evaluates its performance on problems requiring such expressiveness.

### Open Question 3
- Question: How does the interpretability of policies synthesized by R ES compare to policies synthesized by other PIRL methods when evaluated by human subjects?
- Basis in paper: [explicit] The paper assumes that oblique decision trees are inherently interpretable but does not empirically test this assumption with human evaluations.
- Why unresolved: The paper only assumes interpretability based on the tree structure without validating this assumption through human subject studies.
- What evidence would resolve it: User studies where human subjects rate the interpretability of R ES policies versus policies from other PIRL methods on the same tasks.

## Limitations

- Weak empirical evidence connecting ReLU-to-ODT translation with policy performance across diverse control problems
- Limited ablation studies on how actor network architecture choices affect translated program quality
- Low confidence in L1 regularization's effectiveness for inducing sparsity without degrading performance

## Confidence

- **High confidence** in the theoretical mapping between ReLU networks and oblique decision trees
- **Medium confidence** in actor-critic algorithms' ability to train effective small ReLU networks for control tasks
- **Low confidence** in L1 regularization's effectiveness for inducing sparsity without degrading performance

## Next Checks

1. Perform systematic ablation studies varying actor network sizes and architectures to identify optimal configurations for different control problems
2. Compare policy performance and interpretability across different translation methods (RES vs. alternatives) on the same problems
3. Test the approach on problems requiring more complex decision boundaries than those in the current evaluation to assess scalability limits