---
ver: rpa2
title: 'KoLA: Carefully Benchmarking World Knowledge of Large Language Models'
arxiv_id: '2306.09296'
source_url: https://arxiv.org/abs/2306.09296
tags:
- knowledge
- entity
- data
- tasks
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents KoLA, a carefully designed Knowledge-oriented
  LLM Assessment benchmark. KoLA focuses on evaluating world knowledge of LLMs by
  undertaking meticulous designs on three key factors: (1) For ability modeling, it
  uses a four-level cognitive taxonomy to organize 19 tasks, aiming to model the intrinsic
  connections between knowledge-related abilities.'
---

# KoLA: Carefully Benchmarking World Knowledge of Large Language Models

## Quick Facts
- arXiv ID: 2306.09296
- Source URL: https://arxiv.org/abs/2306.09296
- Reference count: 40
- Primary result: KoLA benchmark evaluates 21 LLMs across 19 knowledge tasks using Wikipedia and evolving data, finding larger models memorize more and alignment helps higher-level abilities but may harm memorization

## Executive Summary
KoLA is a carefully designed Knowledge-oriented LLM Assessment benchmark that evaluates world knowledge of large language models through a four-level cognitive taxonomy. The benchmark uses both Wikipedia and continuously collected emerging corpora to assess models' capacity to handle evolving knowledge. A unique self-contrast metric automatically evaluates knowledge hallucination by comparing freely generated completions with knowledge-grounded outputs. The benchmark reveals that larger models tend to memorize more knowledge, alignment unleashes higher-level abilities but may harm memorization, and open-source models generally underperform commercial models.

## Method Summary
KoLA evaluates LLMs across 19 tasks organized into four cognitive levels: Knowledge Memorization, Understanding, Applying, and Creating. The benchmark uses two data sources - Wikipedia as known data and recently collected articles (last 90 days) as evolving data. Models are evaluated through zero-shot or few-shot inference with in-context prompts. Evaluation combines standardized z-scores for numerical comparability with a self-contrast metric that measures knowledge hallucination by comparing free generation with knowledge-grounded generation. Human evaluation is used for knowledge creation tasks.

## Key Results
- Larger models show superior knowledge memorization capabilities
- Alignment helps models perform better on higher-level cognitive tasks but may reduce memorization ability
- Open-source models exhibit overall inferiority compared to commercial models
- The dual-data approach (Wikipedia + evolving) enables fair evaluation across models with different training periods

## Why This Works (Mechanism)

### Mechanism 1
The four-level cognitive taxonomy provides structured diagnosis of LLM knowledge deficiencies by organizing 19 tasks into Knowledge Memorization, Understanding, Applying, and Creating levels. This hierarchy assumes LLM performance follows a similar structure as human cognition. Core assumption: cognitive progression in LLMs mirrors human learning theory. Break condition: If empirical results show no correlation between performance across levels, the taxonomy loses diagnostic value.

### Mechanism 2
The self-contrast metric effectively measures knowledge hallucination without human evaluation by comparing freely generated completion (T) with knowledge-grounded completion (Tk) and human reference (R). Core assumption: If a model's freely generated knowledge closely matches its knowledge-grounded output, it indicates reasonable knowledge creation. Break condition: If models learn to game the metric by simply copying the prompt knowledge rather than demonstrating genuine understanding.

### Mechanism 3
Using both known (Wikipedia) and evolving data sources ensures fair evaluation across model training periods. Wikipedia represents common training data while evolving data (recent 90 days) tests handling of unseen knowledge. Core assumption: Recent articles are unlikely to be included in pre-training corpora, making them suitable for fair evaluation. Break condition: If models with frequent updates consistently outperform others on evolving tasks, the fairness assumption breaks.

## Foundational Learning

- Concept: Cognitive taxonomy levels
  - Why needed here: Understanding the four-level structure is essential for interpreting KoLA results and diagnosing model capabilities
  - Quick check question: Can you explain the difference between Knowledge Understanding and Knowledge Applying tasks?

- Concept: Self-contrast evaluation
  - Why needed here: The metric is central to KoLA's knowledge creation assessment and differs from standard generation evaluation
  - Quick check question: How does the self-contrast metric distinguish between good generation and knowledge hallucination?

- Concept: Evolving vs known data
  - Why needed here: This design choice is fundamental to KoLA's fairness claims and impacts how results should be interpreted
  - Quick check question: Why does KoLA use both Wikipedia and recent articles instead of just testing on novel data?

## Architecture Onboarding

- Component map: Data collection pipeline (Wikipedia + evolving web scraping) → Annotation platform → Task construction engine → Evaluation runner (with API/Open-source model support) → Leaderboard/dashboard
- Critical path: Data collection → Annotation → Task construction → Model evaluation → Result standardization → Leaderboard update
- Design tradeoffs: KoLA prioritizes depth over breadth by focusing on world knowledge rather than covering all LLM abilities, trading comprehensiveness for more diagnostic value
- Failure signatures: If evolving tasks show no correlation with known tasks, data design may be flawed; if standardized scores show high variance across tasks, standardization approach needs refinement; if self-contrast metric shows no correlation with human judgment, the metric needs revision
- First 3 experiments:
  1. Run a single task from each cognitive level to verify the taxonomy structure works as intended
  2. Test the self-contrast metric on a small sample to ensure it produces meaningful differentiation
  3. Compare performance on Wikipedia vs evolving data for a single model to validate the dual-data approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the four-level cognitive taxonomy for organizing knowledge-related abilities (KM, KU, KA, KC) accurately capture the progression of LLM abilities, or are there additional important abilities or relationships not captured by this taxonomy?
- Basis in paper: The paper describes a four-level cognitive ability taxonomy inspired by Bloom's taxonomy
- Why unresolved: The paper does not provide evidence that this taxonomy fully captures LLM abilities or is superior to other taxonomies
- What evidence would resolve it: Experiments comparing this taxonomy to others, or analyzing if certain LLM abilities don't fit neatly into these categories

### Open Question 2
- Question: How much does the choice of data source (known vs. evolving) impact the fairness and validity of LLM evaluation? Could other data sources be better for certain abilities?
- Basis in paper: The paper uses Wikipedia as known data and recent news/novels as evolving data, arguing this enables fair comparisons
- Why unresolved: The paper does not compare to other data sources or analyze if this choice introduces bias for certain abilities
- What evidence would resolve it: Experiments using different data sources, or analyzing if performance differs more on certain abilities for known vs. evolving data

### Open Question 3
- Question: Does the self-contrast metric for evaluating knowledge hallucination in text generation accurately capture hallucination, or are there better approaches?
- Basis in paper: The paper introduces a self-contrast metric comparing free generation to knowledge-grounded generation
- Why unresolved: The paper does not compare this metric to other hallucination evaluation approaches
- What evidence would resolve it: Experiments comparing self-contrast to other hallucination metrics, or analyzing cases where self-contrast fails to detect hallucination

## Limitations
- The cognitive taxonomy's hierarchical structure may not hold for models with different training objectives or architectures
- The self-contrast metric lacks independent validation against human judgment for knowledge hallucination detection
- The fairness claim regarding evolving vs. known data sources lacks rigorous verification

## Confidence

**High confidence**: The benchmark design is methodologically sound with clear task definitions, proper data collection procedures, and systematic evaluation protocols. The correlation findings between model size and knowledge memorization, and between alignment and higher-level abilities, are empirically supported.

**Medium confidence**: The cognitive taxonomy's diagnostic value and the self-contrast metric's effectiveness require further validation. While the design appears logical, empirical confirmation across diverse model architectures is needed.

**Low confidence**: The fairness claim regarding evolving vs. known data sources lacks rigorous verification. Without knowing exactly which models have been exposed to which data, the dual-data approach's fairness benefit remains theoretical.

## Next Checks

1. **Correlation analysis across cognitive levels**: Perform statistical analysis to verify that performance on lower-level tasks (memorization, understanding) correlates with higher-level tasks (applying, creating) across all 21 models. This would validate the cognitive taxonomy's hierarchical structure.

2. **Human evaluation of self-contrast metric**: Conduct blind human evaluation comparing freely generated responses versus knowledge-grounded responses for a subset of knowledge creation tasks. Measure correlation between human judgments of knowledge hallucination and the self-contrast metric scores.

3. **Data exposure verification**: For commercial models with known update schedules, analyze performance differences on evolving tasks to determine whether the "unseen data" assumption holds. This would validate the fairness claim of the dual-data approach.