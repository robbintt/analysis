---
ver: rpa2
title: 'Cold & Warm Net: Addressing Cold-Start Users in Recommender Systems'
arxiv_id: '2309.15646'
source_url: https://arxiv.org/abs/2309.15646
tags:
- cold-start
- user
- users
- cold
- warm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Cold & Warm Net addresses the user cold-start problem in the matching\
  \ stage of recommender systems by using two expert models\u2014one for cold-start\
  \ users and one for warm-up users\u2014combined through a gate network. Dynamic\
  \ knowledge distillation is introduced to prevent underfitting of the cold-start\
  \ expert by selectively transferring knowledge from the warm-up expert."
---

# Cold & Warm Net: Addressing Cold-Start Users in Recommender Systems

## Quick Facts
- arXiv ID: 2309.15646
- Source URL: https://arxiv.org/abs/2309.15646
- Authors: 
- Reference count: 17
- Key outcome: Cold & Warm Net addresses the user cold-start problem in the matching stage of recommender systems by using two expert models—one for cold-start users and one for warm-up users—combined through a gate network. Dynamic knowledge distillation is introduced to prevent underfitting of the cold-start expert by selectively transferring knowledge from the warm-up expert. A bias net explicitly models user behavior bias using features selected via mutual information. The model is evaluated on public datasets (MovieLens 1M and Little-World) and deployed in a short video platform, showing significant improvements in hit rate, NDCG, app dwell time, and user retention rate. It outperforms state-of-the-art models on all user types, particularly cold-start users, with up to 30% improvement in key metrics.

## Executive Summary
Cold & Warm Net addresses the user cold-start problem in recommender systems by using a dual-expert architecture that separately models cold-start and warm-up users. The system combines two specialized experts through a gate network and incorporates dynamic knowledge distillation to prevent underfitting of the cold-start expert. Additionally, a bias net explicitly models user behavior bias using features selected via mutual information. The model shows significant improvements in both offline metrics (Hit Rate, NDCG) and online metrics (app dwell time, user retention rate) across different user types, particularly excelling with cold-start users.

## Method Summary
The Cold & Warm Net model uses a two-tower architecture with separate expert networks for cold-start and warm-up users. User features are divided into profile, action, and state components, with user group embeddings generated through k-means clustering. The cold-start expert uses profile features and group embeddings, while the warm-up expert additionally uses action features. A gate network dynamically weights the two experts based on state features. Dynamic knowledge distillation selectively transfers knowledge from the warm-up expert to the cold-start expert when beneficial. A bias net uses mutual information-selected features to explicitly model behavioral differences between user types. The model is trained with binary cross-entropy loss and evaluated on MovieLens 1M and Little-World datasets.

## Key Results
- Outperforms state-of-the-art models (FM, YouTubeDNN, DSSM, Mind, UMI) on both MovieLens 1M and Little-World datasets
- Shows up to 30% improvement in key metrics (Hit Rate, NDCG) for cold-start users
- Achieves significant online improvements: +9.83% app dwell time, +10.83% user retention rate, +10.87% video play integrity
- Maintains strong performance across all user types while excelling specifically with cold-start users

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cold & Warm Net addresses cold-start by using separate expert models for cold-start and warm-up users, then combining their outputs through a gate network.
- Mechanism: The system divides users into cold-start and warm-up categories based on interaction frequency. Each expert is trained on its respective user group, allowing specialized representation learning. The gate network dynamically weights the two experts based on user state features (e.g., login state, active degree).
- Core assumption: Cold-start and warm-up users have fundamentally different interaction patterns that require different modeling strategies.
- Evidence anchors:
  - [abstract] "based on expert models who are responsible for modeling cold-start and warm-up users respectively"
  - [section 3.2] "According to the frequency of interaction, users can be briefly divided into three categories: cold-start users, warm-up users, and active users"
- Break condition: If user behavior patterns don't differ significantly between cold-start and warm-up users, the dual-expert approach adds unnecessary complexity.

### Mechanism 2
- Claim: Dynamic knowledge distillation prevents the cold-start expert from underfitting by selectively transferring knowledge from the warm-up expert.
- Mechanism: For each sample, the system compares the prediction loss of both experts. If the warm-up expert performs better, knowledge is distilled to the cold-start expert. This is controlled by an auxiliary loss term that only activates when beneficial.
- Core assumption: The warm-up expert can provide useful information to improve cold-start user representations without causing the cold-start expert to collapse.
- Evidence anchors:
  - [abstract] "dynamic knowledge distillation acting as a teacher selector is introduced to assist experts in better learning user representation"
  - [section 3.2] "Dynamic knowledge distillation is applied to cold-start and warm-up experts using a teacher selector"
- Break condition: If the warm-up expert's predictions are consistently worse than the cold-start expert's, distillation becomes counterproductive.

### Mechanism 3
- Claim: The bias net explicitly models user behavior bias using features selected via mutual information, improving cold-start recommendations.
- Mechanism: Mutual information is used to identify features most relevant to user behavior differences between cold-start and warm-up users. These features are fed into a separate bias network that learns to correct systematic prediction biases.
- Core assumption: There are systematic behavioral differences between cold-start and warm-up users that can be captured by specific feature sets.
- Evidence anchors:
  - [abstract] "explicitly models user behavior bias using features selected via mutual information"
  - [section 3.2] "To mine the key features of the behavior bias, mutual information is used to measure the relevance between user features and behaviors"
- Break condition: If mutual information selection fails to identify meaningful behavioral differences, the bias net provides no improvement.

## Foundational Learning

- Concept: User cold-start problem
  - Why needed here: Understanding that cold-start users lack sufficient interaction history to train standard collaborative filtering models is fundamental to grasping why specialized approaches are needed.
  - Quick check question: What distinguishes a cold-start user from a warm-up user in terms of interaction data?

- Concept: Knowledge distillation
  - Why needed here: Dynamic knowledge distillation is a key component that transfers learning from the warm-up expert to the cold-start expert only when beneficial.
  - Quick check question: How does dynamic knowledge distillation differ from standard knowledge distillation in terms of when knowledge transfer occurs?

- Concept: Mutual information for feature selection
  - Why needed here: Mutual information is used to select features that best capture the behavioral differences between user types, which is crucial for the bias net's effectiveness.
  - Quick check question: Why is mutual information particularly suitable for selecting features that capture behavioral bias?

## Architecture Onboarding

- Component map: User features → Cold-start expert / Warm-up expert → Gate network weighting → Combined user embedding → Item embedding → Similarity score → Bias net correction → Final prediction
- Critical path: User features → Cold-start expert / Warm-up expert → Gate network weighting → Combined user embedding → Item embedding → Similarity score → Bias net correction → Final prediction
- Design tradeoffs: The dual-expert approach adds complexity but allows specialized modeling. Dynamic distillation adds computational overhead but prevents underfitting. The bias net adds a separate training objective but captures systematic differences.
- Failure signatures: Poor cold-start performance despite good warm-up performance suggests the gate network isn't weighting experts correctly. Similar performance across all user types suggests the experts aren't learning distinct representations.
- First 3 experiments:
  1. Train with only the cold-start expert to establish baseline cold-start performance
  2. Add the warm-up expert and gate network to test if the combination improves overall performance
  3. Enable dynamic knowledge distillation to verify it prevents cold-start expert underfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Cold & Warm Net vary with different values of the hyperparameter α in the dynamic knowledge distillation (DKD) module?
- Basis in paper: [explicit] The paper mentions that α is a hyperparameter in the loss function of the cold-start expert, but does not provide a detailed analysis of its impact on model performance.
- Why unresolved: The paper does not conduct an ablation study or sensitivity analysis on the α parameter to determine its optimal value or the effect of different values on model performance.
- What evidence would resolve it: An ablation study varying α values and reporting corresponding performance metrics (e.g., HR, NDCG) would provide insights into the impact of this hyperparameter on the model's effectiveness.

### Open Question 2
- Question: How does the choice of k in the k-means clustering algorithm for generating user group embeddings affect the performance of Cold & Warm Net?
- Basis in paper: [explicit] The paper mentions using k-means clustering to generate user group embeddings, but does not discuss the impact of the number of clusters (k) on model performance.
- Why unresolved: The paper does not provide an analysis of how different values of k influence the quality of user group embeddings and subsequently the model's performance.
- What evidence would resolve it: An experiment varying the number of clusters (k) and evaluating the corresponding performance of Cold & Warm Net would reveal the optimal number of user groups for this approach.

### Open Question 3
- Question: How does the Cold & Warm Net model perform on datasets with different user behavior patterns or interaction frequencies compared to the MovieLens and Little-World datasets?
- Basis in paper: [inferred] The paper evaluates the model on two specific datasets (MovieLens 1M and Little-World) but does not discuss its performance on datasets with different characteristics or user behavior patterns.
- Why unresolved: The paper does not provide evidence of the model's effectiveness across diverse datasets or user interaction scenarios, limiting the generalizability of the results.
- What evidence would resolve it: Evaluating Cold & Warm Net on additional datasets with varying user behavior patterns, interaction frequencies, or domain-specific characteristics would demonstrate its robustness and applicability across different recommendation scenarios.

## Limitations
- Focuses only on the matching stage of recommender systems, not applicable to ranking-stage cold-start problems
- Relies on synthetic cold-start user creation through interaction removal, which may not fully capture real-world dynamics
- Online results are from a single short video platform deployment, limiting generalizability across domains

## Confidence
- **High Confidence**: Dual-expert architecture with gate network effectively addresses cold-start and warm-up user differences (supported by consistent offline and online improvements)
- **Medium Confidence**: Dynamic knowledge distillation prevents cold-start expert underfitting (theoretically sound but limited ablation study)
- **Medium Confidence**: Mutual information-based bias net captures meaningful behavioral differences (relies on quality of feature engineering)

## Next Checks
1. Test the model's performance when cold-start users have heterogeneous behavior patterns across different item categories to verify expert specialization
2. Conduct ablation studies comparing dynamic knowledge distillation with static distillation to quantify the benefit of selective transfer
3. Evaluate the model's robustness to different cold-start user definitions (varying thresholds for interaction frequency) to test sensitivity to user categorization