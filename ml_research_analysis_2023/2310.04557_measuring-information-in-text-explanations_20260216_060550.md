---
ver: rpa2
title: Measuring Information in Text Explanations
arxiv_id: '2310.04557'
source_url: https://arxiv.org/abs/2310.04557
tags:
- information
- explanation
- rationale
- scores
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified framework for evaluating text-based
  explanations in XAI by modeling the explanation process as communication channels.
  The authors define two information-theoretic scores: input-explanan mutual information
  (relevance) and target-explanan mutual information (informativeness).'
---

# Measuring Information in Text Explanations

## Quick Facts
- arXiv ID: 2310.04557
- Source URL: https://arxiv.org/abs/2310.04557
- Reference count: 34
- Key outcome: Unified framework using InfoNCE and V-information to quantify relevance and informativeness of text explanations

## Executive Summary
This paper introduces a unified framework for evaluating text-based explanations in explainable AI (XAI) by modeling the explanation process as communication channels. The authors define two information-theoretic scores—relevance (I(X; E)) and informativeness (I(Y; E))—to quantify how much an explanation preserves input information and reveals target information, respectively. They estimate these scores using InfoNCE and V-information on the e-SNLI dataset with rationale and natural language explanations (NLEs). Results show that relevance correlates with lexical/semantic overlap and NLEs exhibit a trade-off between input and target information transmission, while rationales do not.

## Method Summary
The framework estimates mutual information between input (X), target (Y), and explanan (E) using InfoNCE for I(X; E) and V-information for I(Y; E). Explanations from the e-SNLI dataset are embedded using Cohere, OpenAI, or RoBERTa models. InfoNCE uses a log-bilinear model to contrast positive (x,e) pairs against negatives, while V-information compares predictive performance with and without E. The authors correlate these scores with silver labels like lexical overlap and GPTScore to validate their framework.

## Key Results
- Relevance score I(X; E) correlates with lexical/semantic overlap measures
- NLEs show a trade-off between relevance and informativeness; rationales do not
- V-information performs better than InfoNCE for estimating I(Y; E) when Y is scalar
- Embedding choice affects score patterns but not overall framework validity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relevance score I(X; E) quantifies the degree to which an explanation preserves information from the input.
- Mechanism: The InfoNCE estimator lower-bounds mutual information between input X and explanan E, capturing shared information through a contrastive loss.
- Core assumption: InfoNCE can effectively estimate mutual information between high-dimensional input and explanan representations.
- Evidence anchors:
  - [abstract]: "We set up tools for quantifying two information scores: relevance and informativeness."
  - [section]: "We make it possible to estimate these MI terms. We examine the suitability of a battery of methods for this purpose and find the two most appropriate methods: InfoNCE (Oord et al., 2018)"
  - [corpus]: Weak correlation with corpus neighbors; no direct evidence in related papers about InfoNCE use for explanation relevance.
- Break condition: If the embedding space distorts semantic similarity, the relevance score may not reflect true input preservation.

### Mechanism 2
- Claim: Informativeness score I(Y; E) quantifies how much an explanation reveals about the target prediction.
- Mechanism: V-information estimator measures mutual information between target Y and explanan E by comparing predictive performance with and without E.
- Core assumption: Predictive V-information approximates true mutual information when predictors are sufficiently powerful.
- Evidence anchors:
  - [abstract]: "We set up tools for quantifying two information scores: relevance and informativeness."
  - [section]: "The predictive V-information is defined as: IV(E → Y) = HV(Y) − HV(Y | E)"
  - [corpus]: No direct evidence in related papers; weak corpus support for V-information in explanation contexts.
- Break condition: If the predictor family V is too limited, the informativeness score may underestimate true target-explanan mutual information.

### Mechanism 3
- Claim: The trade-off between relevance and informativeness distinguishes NLEs from rationales.
- Mechanism: NLEs, being free-form text, can balance input-related and target-related information differently than rationales, which are constrained subsets of the input.
- Core assumption: The flexibility of NLE generation allows a trade-off that rationales cannot achieve due to their constrained form.
- Evidence anchors:
  - [abstract]: "NLEs trade-off slightly between transmitting the input-related information and the target-related information, whereas the rationales do not exhibit such a trade-off mechanism."
  - [section]: "The relevance score I(X ; E) and the informativeness score I(Y ; E) show weak negative correlations for NLE."
  - [corpus]: No direct evidence in related papers; weak corpus support for trade-off observations.
- Break condition: If embedding methods collapse the distinction between NLEs and rationales, the trade-off may not be observable.

## Foundational Learning

- Concept: Mutual information as a measure of shared information between variables.
  - Why needed here: The framework relies on quantifying shared information between input/target and explanan.
  - Quick check question: What does mutual information I(X; E) = H(E) - H(E|X) represent in terms of uncertainty reduction?
- Concept: Contrastive estimation (InfoNCE) for mutual information.
  - Why needed here: InfoNCE provides a tractable way to estimate high-dimensional mutual information.
  - Quick check question: How does the InfoNCE loss encourage the model to distinguish true pairs (x,e) from negative samples?
- Concept: Predictive V-information as an alternative mutual information estimator.
  - Why needed here: V-information is more suitable when one variable (target) is scalar and the other (explanan) is high-dimensional.
  - Quick check question: Why might V-information be preferred over InfoNCE when estimating I(Y; E)?

## Architecture Onboarding

- Component map: e-SNLI dataset -> rationale/NLE generation -> embedding (Cohere/OpenAI/RoBERTa) -> InfoNCE module -> V-information module -> Evaluation with silver labels
- Critical path:
  1. Embed explanans using chosen model
  2. Compute I(X; E) via InfoNCE
  3. Compute I(Y; E) via V-information
  4. Correlate scores with silver labels
  5. Analyze trade-offs and embedding effects
- Design tradeoffs:
  - Embedding choice affects score magnitude and patterns
  - InfoNCE vs V-information: InfoNCE better for I(X; E), V-information better for I(Y; E)
  - Silver labels vs human evaluation: Silver labels are scalable but may miss nuanced qualities
- Failure signatures:
  - Low variance in InfoNCE estimates → poor batch size or learning rate
  - Inconsistent I(Y; E) across embeddings → embedding space misalignment with target semantics
  - Weak correlation with silver labels → estimator mismatch or silver label inadequacy
- First 3 experiments:
  1. Verify InfoNCE estimation on synthetic correlated Gaussian data before applying to e-SNLI
  2. Compare I(X; E) and I(Y; E) distributions across embeddings on a small e-SNLI subset
  3. Test correlation between I(X; E) and type overlap ratio on a few examples to validate relevance interpretation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the information-theoretic framework be extended to multimodal explanation channels?
- Basis in paper: [explicit] The authors discuss the potential for extending the explanation channels framework to multimodal problems, mentioning that recent text-to-image and image-to-text models like DALL-E, CLIP, and BLIP2 indicate an affirmative answer, but empirical evidence will be necessary.
- Why unresolved: The paper focuses on text-based explanations and does not provide empirical evidence for multimodal extensions. The authors suggest that the framework can be generalized to additional text-based XAI methods, but do not explore multimodal scenarios.
- What evidence would resolve it: Conducting experiments with multimodal embeddings (e.g., combining text and image embeddings) to estimate relevance and informativeness scores for multimodal explanations. Comparing the performance of these scores with traditional multimodal evaluation metrics.

### Open Question 2
- Question: What are the societal impacts of different types of information in explanations?
- Basis in paper: [inferred] The authors mention that depending on the actual problems to explain, not all types of relevant information are appropriate, and some bits of information may be disparaging, unwelcoming, and biased. They also state that identifying and elaborating on the types of information and their societal impacts would be crucial for understanding the explanation quality.
- Why unresolved: The paper focuses on the technical aspects of measuring information in explanations and does not delve into the societal implications of different types of information. The authors acknowledge the importance of this aspect but do not provide a comprehensive analysis.
- What evidence would resolve it: Conducting studies to identify and categorize different types of information in explanations, and their potential societal impacts. Developing guidelines for selecting appropriate types of information based on the context and stakeholders involved.

### Open Question 3
- Question: How can the explanation channels framework be used to evaluate the quality of explanations for different stakeholders?
- Basis in paper: [inferred] The authors mention that the information-theoretic framework can provide a common ground to evaluate two text-based post-hoc explanations, but they do not discuss how it can be used to evaluate explanations for different stakeholders (e.g., domain experts, end-users, regulators).
- Why unresolved: The paper focuses on the technical aspects of measuring information in explanations and does not address the evaluation of explanations for different stakeholders. The authors acknowledge the importance of this aspect but do not provide a comprehensive analysis.
- What evidence would resolve it: Conducting studies to identify the information needs and preferences of different stakeholders, and developing methods to adapt the explanation channels framework to evaluate explanations based on these needs and preferences. Comparing the performance of these adapted methods with traditional stakeholder-specific evaluation metrics.

## Limitations

- The framework's effectiveness depends on embedding quality, which may not capture semantic similarity accurately
- Silver label correlation substitutes for human evaluation without establishing whether automated metrics capture human judgment
- The trade-off between relevance and informativeness may be an artifact of embedding choices rather than a fundamental property of explanation types

## Confidence

**High Confidence**: The mathematical framework for defining relevance and informativeness using mutual information is sound and well-established in information theory. The observation that rationales and NLEs differ in their score distributions is reproducible given the methodology.

**Medium Confidence**: The choice of InfoNCE and V-information as appropriate estimators for their respective score types is justified by theoretical properties, but practical effectiveness in this specific application context needs more rigorous validation. The negative correlation between relevance and informativeness for NLEs suggests a trade-off but may be influenced by embedding choices.

**Low Confidence**: The claim that NLEs specifically trade-off between input and target information transmission is based on weak corpus evidence and may be an artifact of the evaluation methodology rather than a fundamental property of explanation types.

## Next Checks

1. **Estimator Validation on Synthetic Data**: Before applying to e-SNLI, validate InfoNCE and V-information estimators on synthetic datasets with known mutual information properties. Create controlled scenarios where input-explanan and target-explanan correlations are precisely controlled to verify the estimators recover expected values.

2. **Human Evaluation Correlation**: Conduct a small-scale human evaluation study comparing human ratings of explanation quality with the framework's scores. Select examples spanning the relevance-informativeness spectrum and have multiple annotators rate explanation helpfulness, then compute correlation with the information-theoretic scores.

3. **Embedding Robustness Analysis**: Test the framework's sensitivity to embedding choices by computing scores using multiple embedding methods (Cohere, OpenAI, RoBERTa) on the same explanations. Analyze whether the observed patterns (trade-offs, score distributions) are consistent across embeddings or specific to particular embedding spaces.