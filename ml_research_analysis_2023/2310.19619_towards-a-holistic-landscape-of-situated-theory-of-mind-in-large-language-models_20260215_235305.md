---
ver: rpa2
title: Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models
arxiv_id: '2310.19619'
source_url: https://arxiv.org/abs/2310.19619
tags:
- agent
- llms
- task
- language
- mind
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper presents a comprehensive framework for evaluating
  Theory of Mind (ToM) in large language models (LLMs) through a taxonomy of 7 mental
  state categories. The authors identify limitations in current ToM benchmarks, including
  narrow focus on specific mental states, data contamination, and susceptibility to
  shortcuts.
---

# Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models

## Quick Facts
- arXiv ID: 2310.19619
- Source URL: https://arxiv.org/abs/2310.19619
- Reference count: 36
- Key outcome: Proposes a taxonomy of 7 mental state categories and situated evaluation approach for comprehensive ToM assessment in LLMs

## Executive Summary
This position paper presents a comprehensive framework for evaluating Theory of Mind (ToM) in large language models (LLMs) through a taxonomy of 7 mental state categories. The authors identify limitations in current ToM benchmarks, including narrow focus on specific mental states, data contamination, and susceptibility to shortcuts. They propose a "situated evaluation" approach where LLMs are treated as agents physically situated in environments and socially situated in interactions with humans. A pilot study in a 2D grid world demonstrates the feasibility of testing diverse ToM aspects through tasks like belief reasoning, emotional reactions, and intention prediction. Results show LLMs exhibit some ToM capabilities but struggle with higher-order beliefs, perceptual limitations, and preference reasoning, highlighting the need for more comprehensive evaluation methods.

## Method Summary
The paper introduces a taxonomy of 7 mental state categories (beliefs, intentions, desires, emotions, knowledge, percepts, non-literal communication) and proposes a "situated evaluation" framework for assessing ToM in LLMs. The approach treats LLMs as agents physically situated in environments and socially situated in interactions with humans. A pilot study uses a 2D grid world environment (MiniGrid) with 10 tasks covering various ToM aspects. Prompts are generated with environment descriptions, agent descriptions, action sequences, and questions. The study evaluates GPT-3.5 and GPT-4 models using zero-shot, one-shot, and chain-of-thought prompting methods, recording accuracy for each task.

## Key Results
- LLMs demonstrate varying performance across ToM tasks, with accuracy ranging from 40-95% depending on task complexity and prompting strategy
- Higher-order beliefs and perceptual limitations pose significant challenges for LLMs, with accuracy dropping substantially for second-order belief tasks
- Situated evaluation approach successfully creates controlled scenarios unlikely to appear in training data, potentially mitigating data contamination risks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Situated evaluation mitigates data contamination by using simulated environments unlikely in LLM training data
- Mechanism: By creating controlled scenarios in synthetic environments, the evaluation data cannot have been seen during training
- Core assumption: LLM training corpora do not contain extensive coverage of the specific simulated environments used
- Evidence anchors:
  - [abstract] "Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage"
  - [section 4.1] "A situated ToM evaluation can mitigate data contamination, as researchers can design scenarios in simulated settings that are unlikely to be part of the LLM's training data"
  - [corpus] Weak evidence - only mentions "NegotiationToM" and "ToM-SSI" but no direct evidence about contamination mitigation
- Break condition: If LLM training data includes extensive grid world or similar simulation environments, contamination risk remains

### Mechanism 2
- Claim: Situated evaluation reduces shortcut learning by randomizing environment states and action traces
- Mechanism: Randomization of environment layouts, object positions, and agent trajectories prevents LLMs from learning spurious statistical patterns
- Core assumption: Shortcuts emerge from predictable patterns in static benchmarks rather than genuine ToM reasoning
- Evidence anchors:
  - [abstract] "Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage"
  - [section 4.1] "In a situated setting, on the contrary, we rely on simulated environments to manipulate evaluation data at scale, so that the environment, the states, and the action traces in the environment can be randomized to avoid the statistical spurious correlations"
  - [corpus] Weak evidence - mentions "Stress testing Social Reasoning" but no direct evidence about randomization effectiveness
- Break condition: If randomization is insufficient or patterns remain detectable, shortcuts can still emerge

### Mechanism 3
- Claim: Physical and social situatedness enables testing of ToM aspects impossible with text-only benchmarks
- Mechanism: Real-time perception, action trajectories, and social interactions in simulated environments capture mental states like perceptual limitations and belief formation
- Core assumption: Text descriptions cannot fully capture the complexity of physical and social environments needed for complete ToM assessment
- Evidence anchors:
  - [abstract] "We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans"
  - [section 4.1] "Although it is possible to frame the situations as narratives and cover all mental states using text-only benchmarks, certain aspects of ToM can only be effectively studied within specific physical or social environment"
  - [corpus] Moderate evidence - "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions" directly supports this claim
- Break condition: If text descriptions can adequately capture all necessary environmental and social details, situatedness becomes unnecessary

## Foundational Learning

- Concept: Theory of Mind (ToM) taxonomy
  - Why needed here: Understanding the 7 mental state categories (beliefs, intentions, desires, emotions, knowledge, percepts, non-literal communication) is essential for designing comprehensive evaluation tasks
  - Quick check question: What are the three categories of mental states that were most under-explored in existing benchmarks according to Figure 2?

- Concept: Situated evaluation design
  - Why needed here: Creating effective situated ToM tasks requires understanding how to translate abstract mental state concepts into concrete physical and social scenarios
  - Quick check question: In the grid world pilot study, how did the unexpected transfer task differ from traditional text-based Sally-Anne tests?

- Concept: Data contamination mitigation
  - Why needed here: Understanding how to create evaluation data that cannot have been seen during training is crucial for valid ToM assessment
  - Quick check question: What specific technique did the authors propose to prevent LLMs from accessing evaluation data during training?

## Architecture Onboarding

- Component map: MiniGrid simulator -> Task generation engine -> Prompt construction -> LLM inference -> Result analysis
- Critical path: Environment generation → Agent trajectory planning → Prompt construction → LLM inference → Answer evaluation
- Design tradeoffs: Grid world simplicity vs. complexity needed to capture real-world ToM, randomization level vs. task consistency, prompt detail vs. context window limitations
- Failure signatures: LLMs performing well on reality check but poorly on higher-order beliefs suggests they memorize physical rules but lack genuine ToM reasoning
- First 3 experiments:
  1. Reality check task with varying levels of environment complexity
  2. First-order belief task with different prompting strategies (zero-shot, one-shot, CoT)
  3. Emotion prediction task with varying observer knowledge levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design scalable benchmarks with high-quality human annotations that can effectively test LLM's theory of mind capabilities while avoiding data contamination and shortcuts?
- Basis in paper: [explicit] The paper identifies data contamination and shortcuts as major roadblocks in ToM evaluation, and calls for new benchmarks with scalable sizes and high-quality human annotations.
- Why unresolved: Current benchmarks suffer from data contamination issues due to training on internet-scale data, and shortcuts due to template-based approaches. The paper calls for better design but doesn't provide specific solutions.
- What evidence would resolve it: A benchmark design framework that demonstrates effective mitigation of both data contamination and shortcuts while maintaining scalability and annotation quality.

### Open Question 2
- Question: What are the specific neural mechanisms in LLMs that enable or prevent the emergence of different aspects of theory of mind, such as beliefs, intentions, and emotions?
- Basis in paper: [inferred] The paper discusses LLMs' limited capabilities in various ToM aspects (beliefs, intentions, emotions, etc.) but doesn't explore the underlying neural mechanisms that cause these limitations.
- Why unresolved: While the paper identifies limitations in LLM ToM capabilities, it doesn't investigate why these limitations exist at a neural level or how they could be addressed through architectural changes.
- What evidence would resolve it: Neural network analysis techniques (like mechanistic interpretability) that reveal how LLMs process and represent different mental states.

### Open Question 3
- Question: How can we develop more sophisticated situated evaluation protocols that go beyond passive observation to test active engagement and mutual theory of mind in LLMs?
- Basis in paper: [explicit] The paper calls for situated evaluation where LLMs are treated as agents physically and socially situated in environments, but current benchmarks primarily use passive observer roles.
- Why unresolved: While the paper advocates for situated evaluation, it only presents a basic 2D grid world proof-of-concept and doesn't explore more complex multi-agent interactions or mutual theory of mind scenarios.
- What evidence would resolve it: Evaluation protocols that successfully test active engagement, mutual understanding, and symmetric theory of mind in more complex simulated environments.

## Limitations

- The framework's claims about situated evaluation mitigating data contamination and shortcuts lack direct empirical validation in the paper
- The pilot study uses only two LLM models (GPT-3.5 and GPT-4) and covers a limited set of 10 tasks without statistical significance testing
- The framework does not explore multi-agent interactions or mutual theory of mind scenarios beyond passive observation

## Confidence

- Claims about data contamination mitigation: Medium
- Claims about shortcut reduction: Medium
- Claims about situatedness enabling new ToM aspects: Medium

## Next Checks

1. Conduct a controlled experiment comparing LLM performance on identical ToM scenarios presented in both text-only and situated formats to quantify the actual reduction in data contamination and shortcut learning
2. Expand the pilot study to include 5+ additional LLM models across different architectures and sizes, with statistical significance testing across multiple random seeds
3. Design and test at least 3 new situated tasks specifically targeting the under-explored mental state categories (percepts, preferences, non-literal communication) to validate the taxonomy's completeness