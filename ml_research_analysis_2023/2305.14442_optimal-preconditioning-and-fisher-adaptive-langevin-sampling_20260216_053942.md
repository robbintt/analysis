---
ver: rpa2
title: Optimal Preconditioning and Fisher Adaptive Langevin Sampling
arxiv_id: '2305.14442'
source_url: https://arxiv.org/abs/2305.14442
tags:
- matrix
- where
- mala
- adaptive
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an optimal preconditioning matrix for Langevin
  sampling by maximizing the expected squared jumped distance, which yields the inverse
  Fisher information matrix. The method learns this preconditioner online from the
  history of gradient samples using a computationally efficient recursive update with
  quadratic cost per iteration.
---

# Optimal Preconditioning and Fisher Adaptive Langevin Sampling

## Quick Facts
- arXiv ID: 2305.14442
- Source URL: https://arxiv.org/abs/2305.14442
- Authors: 
- Reference count: 40
- Primary result: Optimal preconditioner for Langevin sampling is inverse Fisher information matrix; FisherMALA outperforms standard adaptive methods in high dimensions

## Executive Summary
This paper addresses the fundamental question of optimal preconditioning for Langevin sampling by deriving the inverse Fisher information matrix as the optimal preconditioner. The authors propose an adaptive MALA algorithm (FisherMALA) that learns this preconditioner online using a computationally efficient recursive update. The method is evaluated on Gaussian targets and Bayesian logistic regression problems, demonstrating significant performance improvements over standard adaptive MCMC methods in high-dimensional settings.

## Method Summary
FisherMALA is an adaptive MALA algorithm that learns the optimal preconditioner (inverse Fisher information matrix) online from gradient samples. The algorithm uses a recursive update with quadratic cost per iteration, avoiding direct computation of the full inverse matrix. Rao-Blackwellization is applied to score function increments to reduce variance during adaptation. The method adapts both the preconditioner and step size to achieve optimal acceptance rates while maintaining computational efficiency.

## Key Results
- FisherMALA achieves significantly higher ESS scores than AdaMALA and comparable performance to mMALA on Gaussian targets
- Superior performance demonstrated on Bayesian logistic regression with dimensions up to 785 (MNIST dataset)
- Effective preconditioner estimation shown through Frobenius norm convergence on anisotropic Gaussian targets
- Competitive performance against HMC in high-dimensional settings

## Why This Works (Mechanism)

### Mechanism 1
The optimal preconditioner is the inverse Fisher information matrix (I⁻¹), not the covariance matrix. Maximizing expected squared jumped distance yields I⁻¹, which aligns with target curvature and enables efficient exploration in high dimensions. Core assumption: differentiable target with finite Fisher information.

### Mechanism 2
Recursive online estimation using Rao-Blackwellized score function increments is computationally efficient and stable. The algorithm updates with quadratic cost per iteration and reduces variance through marginalization. Core assumption: score function increments are zero-mean and centered during transient phases.

### Mechanism 3
FisherMALA significantly outperforms other adaptive MCMC methods in high-dimensional settings. By learning optimal preconditioner I⁻¹, it achieves superior ESS scores compared to standard adaptive MALA, position-dependent Riemannian manifold MALA, and HMC. Core assumption: target distribution is high-dimensional and potentially anisotropic.

## Foundational Learning

- **Langevin diffusion and its discretization**: Needed as the basis for MALA and optimal preconditioner derivation. Quick check: What continuous-time process with stationary distribution π is used as starting point?

- **Fisher information matrix and its properties**: Essential for understanding optimal preconditioner definition and its relationship to log-target gradients. Quick check: How is I defined in terms of log-target gradients and what is its relationship to Hessian?

- **Adaptive MCMC and online learning**: Crucial for understanding how FisherMALA learns preconditioner from gradient history. Quick check: How does FisherMALA update empirical Fisher estimate and square root matrix per iteration?

## Architecture Onboarding

- **Component map**: Target π(x) → Gradient ∇ log π(x) → Empirical Fisher În → Inverse An → Square root Rn → Proposal q(yn|xn) → Acceptance probability → New sample

- **Critical path**: 1) Initialize x1, σ², R, burn-in with simple MALA 2) Propose yn using current preconditioner 3) Compute MH acceptance probability 4) Update Fisher estimate and square root matrix 5) Adapt step size 6) Accept/reject

- **Design tradeoffs**: Computational cost (quadratic updates vs full inverse), stability (damping parameter + Rao-Blackwellization), accuracy (learns I⁻¹ vs covariance)

- **Failure signatures**: Poor mixing (inaccurate preconditioner or step size), instability (non-zero-mean increments or small damping), computational issues (high dimensionality)

- **First 3 experiments**: 1) Test FisherMALA on 2D Gaussian to verify I⁻¹ learning 2) Compare with AdaMALA and mMALA on high-dimensional anisotropic target 3) Test stability on Bayesian logistic regression with high-dimensional posterior

## Open Questions the Paper Calls Out

### Open Question 1
Does Rao-Blackwellized score function difference introduce bias in estimating inverse Fisher matrix? The paper claims no observable bias for Gaussian targets but lacks rigorous proof for general distributions. Evidence needed: formal theoretical analysis or empirical bias tests on non-Gaussian targets.

### Open Question 2
Can FisherMALA be extended to learn preconditioners for Hamiltonian Monte Carlo? Paper acknowledges complexity since both mass matrix and its inverse are needed for HMC. Evidence needed: successful HMC implementation and validation showing improved performance.

### Open Question 3
How does FisherMALA perform in extremely high-dimensional settings (d > 1000)? Paper demonstrates up to d=785 but doesn't explore higher dimensions. Evidence needed: empirical studies on problems with d>1000 comparing computational efficiency to other adaptive MCMC methods.

## Limitations
- Theoretical guarantees for adaptive MCMC ergodicity under optimal preconditioning are not rigorously proven
- Performance advantage may diminish in lower-dimensional or more isotropic settings
- Quadratic computational cost per iteration may become prohibitive for extremely high-dimensional problems

## Confidence
- High confidence: Theoretical derivation of optimal preconditioner as inverse Fisher information matrix
- Medium confidence: Stability and efficiency of recursive online estimation method
- Medium confidence: Experimental results showing superior performance of FisherMALA

## Next Checks
1. Rigorously prove ergodicity of adaptive chain under optimal preconditioning to verify theoretical guarantees
2. Conduct experiments on lower-dimensional and more isotropic target distributions to assess performance robustness
3. Investigate computational scalability for extremely high-dimensional problems and explore optimizations to reduce quadratic cost per iteration