---
ver: rpa2
title: 'T-MARS: Improving Visual Representations by Circumventing Text Feature Learning'
arxiv_id: '2307.03132'
source_url: https://arxiv.org/abs/2307.03132
tags:
- data
- text
- t-mars
- visual
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes T-MARS, a data filtering method for large-scale
  vision-language datasets. The authors observe that 40% of LAION dataset images contain
  text overlapping with captions, which encourages models to perform OCR rather than
  learning visual features.
---

# T-MARS: Improving Visual Representations by Circumventing Text Feature Learning

## Quick Facts
- arXiv ID: 2307.03132
- Source URL: https://arxiv.org/abs/2307.03132
- Reference count: 40
- 40% of LAION dataset images contain text overlapping with captions

## Executive Summary
T-MARS addresses a fundamental problem in vision-language pretraining: web-scale datasets contain many images where text is the dominant correlated feature with captions, leading models to learn OCR rather than visual representations. The method filters these problematic samples by masking text regions and computing CLIP similarity scores, retaining only pairs with high masked image-caption similarity. Experiments show T-MARS improves DataComp benchmark performance by 6.5% ImageNet accuracy and 4.7% VTAB accuracy, with linear accuracy gains as data and compute scale exponentially.

## Method Summary
T-MARS is a data filtering method that identifies and removes images where text dominates visual features in vision-language datasets. The method uses FAST text detection to identify text regions, masks them by replacing with average surrounding pixel values, then computes CLIP similarity between masked images and captions. Images below the median similarity score are filtered out. The approach is applied to LAION-400M and evaluated on DataComp benchmark datasets (12.8M and 128M samples) using CLIP-style contrastive learning with ViT-B-32 and ResNet-50 architectures.

## Key Results
- T-MARS improves DataComp benchmark by 6.5% ImageNet accuracy and 4.7% VTAB accuracy
- Accuracy gains increase linearly as data and compute scale exponentially
- T-MARS outperforms previous filtering methods across all tested pool sizes (2M to 64M samples)

## Why This Works (Mechanism)

### Mechanism 1
Text-dominated images encourage models to solve OCR rather than learn visual features. When text is the sole correlated feature with the caption, contrastive learning aligns text embeddings instead of visual ones. The core assumption is that CLIP similarity between text and caption exceeds similarity between visual content and caption for text-dominated images. This breaks if text detection/masking fails to identify all text regions, or if visual features coincidentally align with captions despite text presence.

### Mechanism 2
Masking text before computing similarity reveals true visual-caption alignment. By removing text features, similarity score reflects only visual features' alignment with caption. This relies on accurate text detection and masking to isolate visual features. The mechanism fails if text masking leaves residual text artifacts, or if visual features are damaged during masking.

### Mechanism 3
Filtering low visual-caption similarity images improves downstream visual representations. Removing images where text dominates prevents model from learning text-based shortcuts. This assumes visual-only features have higher utility for downstream vision tasks than text-only features. The approach fails if high CLIP similarity images contain both useful visual and text features, causing valuable data to be removed.

## Foundational Learning

- **Concept**: Contrastive learning in vision-language models
  - Why needed: T-MARS modifies how similarity is computed by masking text before evaluation
  - Quick check: What is the loss function used when training CLIP-style models, and how does masking text change the effective loss landscape?

- **Concept**: Text detection and masking techniques
  - Why needed: T-MARS relies on FAST text detection to identify text regions for masking
  - Quick check: What are the failure modes of text detection algorithms on web-scale images, and how might they affect T-MARS performance?

- **Concept**: Data filtering impact on scaling laws
  - Why needed: The paper shows linear accuracy gains as data scales exponentially
  - Quick check: How do different data filtering strategies affect the power law relationship between model performance and dataset size?

## Architecture Onboarding

- **Component map**: FAST text detection → Text masking → CLIP similarity computation → Filtering threshold → Filtered dataset
- **Critical path**: 
  1. Load image-caption pairs
  2. Run FAST text detection on images
  3. Mask detected text regions
  4. Compute CLIP similarity between masked images and captions
  5. Apply threshold (median similarity) to filter data
  6. Train model on filtered subset

- **Design tradeoffs**:
  - Masking method: Average color vs. inpainting vs. black boxes - affects visual feature preservation
  - Threshold selection: Fixed 50% vs. adaptive thresholds - impacts dataset size and quality
  - Text detection accuracy: Fast but potentially incomplete detection vs. slower, more thorough methods

- **Failure signatures**:
  - Poor downstream performance: Text detection missing text regions, masking damaging visual features
  - Inflated similarity scores: Residual text artifacts affecting masked image embeddings
  - Unexpected dataset composition: Filtering removes too many visual-text images or retains too many text-only images

- **First 3 experiments**:
  1. Run T-MARS on a small LAION subset and visualize masked images to verify text detection/masking quality
  2. Compare similarity score distributions between original and masked images to validate the masking approach
  3. Train a small CLIP model on filtered vs. unfiltered subsets and measure ImageNet zero-shot accuracy to confirm effectiveness

## Open Questions the Paper Calls Out

The paper highlights several areas for future work but doesn't provide specific open questions. The authors suggest potential improvements through dynamic data curation where the utility of different data points is assessed in a dynamic fashion and the data pool is refreshed with samples worth learning.

## Limitations
- The 40% text-overlap statistic may not generalize to other web-scale image-text corpora beyond LAION
- Text masking with average color replacement could inadvertently remove or corrupt visual features in complex image regions
- The filtering threshold (median similarity) is heuristic and may not be optimal across different datasets or domains

## Confidence

**Major uncertainties:**
- Text masking quality and its impact on visual feature preservation
- Generalizability of filtering effectiveness beyond LAION dataset
- Optimal threshold selection for different dataset distributions

**Confidence assessment:**
- **High confidence**: T-MARS filtering improves zero-shot ImageNet accuracy on DataComp benchmark (6.5% gain)
- **Medium confidence**: The linear scaling relationship between accuracy gains and data/compute resources
- **Medium confidence**: Text-dominated images negatively impact visual representation learning

## Next Checks

1. Test T-MARS filtering on alternative vision-language datasets (e.g., Conceptual Captions, YFCC100M) to verify generalizability
2. Compare different text masking strategies (black boxes, inpainting, context-aware filling) to assess impact on visual feature preservation
3. Evaluate T-MARS-filtered models on domain-specific vision tasks to measure practical utility beyond benchmark datasets