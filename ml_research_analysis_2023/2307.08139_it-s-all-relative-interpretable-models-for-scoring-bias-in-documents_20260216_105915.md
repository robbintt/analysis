---
ver: rpa2
title: 'It''s All Relative: Interpretable Models for Scoring Bias in Documents'
arxiv_id: '2307.08139'
source_url: https://arxiv.org/abs/2307.08139
tags:
- bias
- wikipedia
- articles
- text
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an interpretable bias scoring model for web
  documents using pairwise comparisons. The model is trained on pairs of Wikipedia
  revisions where one is more biased than the other, using features based on word
  embeddings and text topic.
---

# It's All Relative: Interpretable Models for Scoring Bias in Documents

## Quick Facts
- arXiv ID: 2307.08139
- Source URL: https://arxiv.org/abs/2307.08139
- Reference count: 11
- Primary result: 77.56% accuracy in pairwise bias classification

## Executive Summary
This paper introduces an interpretable model for scoring bias in web documents using pairwise comparisons. The model is trained on pairs of Wikipedia revisions where one version is more biased than another, using features based on word embeddings and document topics. It achieves 77.56% accuracy in pairwise bias classification, outperforming strong baselines. The approach is computationally efficient and interpretable, allowing identification of bias-inducing words and phrases. The model generalizes well to news and legal domains, showing higher bias in news articles compared to Wikipedia and lower bias in legal texts.

## Method Summary
The method trains a discrete choice model using pairs of Wikipedia revisions where one is more biased than the other. It computes bias scores for words using static or contextual word embeddings, then aggregates these to score entire documents. The model is optimized via likelihood maximization on Wikipedia training pairs, and can identify bias-inducing words through the GB(w) scoring function. It demonstrates strong generalization to news and legal domains despite being trained only on Wikipedia data.

## Key Results
- Achieves 77.56% accuracy in pairwise bias classification on Wikipedia test data
- Successfully identifies bias-inducing words and multi-word phrases in documents
- Generalizes to news and legal domains, ranking news outlets by bias and distinguishing legal text bias levels
- Captures temporal evolution of bias in Wikipedia articles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pairwise comparison approach reduces subjectivity in bias detection by avoiding absolute bias thresholds.
- Mechanism: By framing bias detection as relative comparisons between document versions, the model sidesteps the need for defining subjective thresholds for bias. This aligns with human judgment patterns where relative comparisons are more consistent than absolute classifications.
- Core assumption: Humans can more reliably agree on which of two texts is more biased than on whether a single text is biased.
- Evidence anchors:
  - [abstract] "we are able to develop a useful model for scoring bias by learning to perform pairwise comparisons of bias accurately"
  - [section] "Previous works have found greater inter-annotator agreement and higher human accuracy for this task, compared to absolute-bias classification"
  - [corpus] Weak evidence - no direct corpus studies on inter-annotator agreement mentioned
- Break condition: If domain-specific cultural or contextual factors make pairwise bias comparisons equally subjective as absolute judgments.

### Mechanism 2
- Claim: The interpretable scoring mechanism enables word-level bias attribution through the bias contribution formula.
- Mechanism: The model assigns bias scores to individual words based on their vector embeddings and their interaction with document-specific topic vectors. This allows identification of bias-inducing words through the GB(w) scoring function.
- Core assumption: Word embeddings capture semantic relationships relevant to bias, and these relationships can be meaningfully weighted by document context.
- Evidence anchors:
  - [abstract] "we can interpret the parameters of the trained model to discover the words most indicative of bias"
  - [section] "We model the bias score of a text as the sum of the bias contributions of the words present in the text"
  - [corpus] Moderate evidence - corpus shows model successfully identifies adjectives and adverbs as more bias-inducing than nouns
- Break condition: If word embeddings fail to capture bias-relevant semantics or if document context doesn't meaningfully modify word bias.

### Mechanism 3
- Claim: The model generalizes across domains despite being trained only on Wikipedia data.
- Mechanism: The model learns bias patterns that transcend specific domains, allowing it to score bias in news and legal texts even though these weren't in the training data.
- Core assumption: Bias-inducing language patterns are domain-agnostic and can be learned from Wikipedia revisions.
- Evidence anchors:
  - [abstract] "we demonstrate that the outputs of the model can be explained and validated, even for the two domains that are outside the training-data domain"
  - [section] "we study the application of the model to a variety of document domains and demonstrate its generalisability"
  - [corpus] Strong evidence - corpus shows model successfully ranks news outlets by bias and distinguishes legal text bias levels
- Break condition: If domain-specific bias patterns are fundamentally different and cannot be captured by the learned representations.

## Foundational Learning

- Concept: Bradley-Terry pairwise comparison model
  - Why needed here: Provides the theoretical foundation for comparing two documents rather than classifying absolute bias
  - Quick check question: How does the Bradley-Terry model handle ties in pairwise comparisons?

- Concept: Word embedding mathematics (static vs contextual)
  - Why needed here: Core to how the model represents text and computes bias scores
  - Quick check question: What's the key difference between static and contextual word embeddings in terms of bias modeling?

- Concept: Discrete choice theory
  - Why needed here: Framework for modeling the probability of one document being more biased than another
  - Quick check question: How does discrete choice theory differ from traditional classification approaches?

## Architecture Onboarding

- Component map: Input preprocessing -> Embedding generation -> Bias score computation -> Pairwise comparison -> Output ranking
- Critical path: Document preprocessing -> Embedding computation -> Bias score aggregation -> Comparison decision
- Design tradeoffs: Speed vs accuracy (static embeddings faster but less nuanced than contextual), interpretability vs complexity (simpler models more interpretable)
- Failure signatures: Poor performance on domain shifts, overfitting to Wikipedia-specific patterns, inability to identify bias-inducing phrases
- First 3 experiments:
  1. Compare model performance on Wikipedia test set vs random baseline
  2. Test word-level interpretability by examining GB(w) scores for known bias words
  3. Validate cross-domain generalization by testing on news articles from different ideological spectrums

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the interpretability of the bias scoring model be improved to identify multi-word phrases that contribute to bias more effectively?
- Basis in paper: [explicit] The paper mentions that the Contextual Quadratic model can identify multi-word phrases that induce bias, but the Static Quadratic model fails to identify such phrases and incorrectly identifies single words.
- Why unresolved: The paper does not provide a detailed analysis of how to improve the interpretability of the model for identifying multi-word phrases that contribute to bias.
- What evidence would resolve it: Experimental results comparing the performance of different models in identifying multi-word phrases that contribute to bias, and a detailed analysis of the reasons behind the differences in performance.

### Open Question 2
- Question: How can the model be extended to handle more complex forms of bias, such as those that arise from the framing of an issue or the selection of sources?
- Basis in paper: [inferred] The paper focuses on identifying words and phrases that contribute to bias, but does not address more complex forms of bias such as framing or source selection.
- Why unresolved: The paper does not provide a framework for extending the model to handle more complex forms of bias.
- What evidence would resolve it: Experimental results demonstrating the model's ability to identify more complex forms of bias, and a detailed analysis of the challenges and opportunities in extending the model to handle such bias.

### Open Question 3
- Question: How can the model be adapted to handle bias in languages other than English?
- Basis in paper: [inferred] The paper focuses on bias in English-language Wikipedia articles, news articles, and legal texts, but does not address bias in other languages.
- Why unresolved: The paper does not provide a framework for adapting the model to handle bias in other languages.
- What evidence would resolve it: Experimental results demonstrating the model's ability to identify bias in other languages, and a detailed analysis of the challenges and opportunities in adapting the model to handle bias in different languages.

## Limitations
- Reliance on Wikipedia revision data may introduce domain-specific biases
- Use of static word embeddings may miss contextual nuances
- Focus on English-language content limits applicability to multilingual contexts

## Confidence
- High Confidence: The pairwise comparison approach's effectiveness and the model's ability to generalize across domains (news and legal texts)
- Medium Confidence: The interpretability of word-level bias scores and the temporal analysis of bias evolution in Wikipedia articles
- Low Confidence: The absolute accuracy of bias scores across different cultural contexts and the model's performance on highly specialized technical content

## Next Checks
1. Test model performance on a multilingual corpus to assess cross-linguistic bias detection capabilities
2. Compare results using contextual embeddings (BERT, RoBERTa) against the current static embedding approach
3. Conduct human evaluation studies to validate the model's bias scores against expert judgments across different domains and cultural contexts