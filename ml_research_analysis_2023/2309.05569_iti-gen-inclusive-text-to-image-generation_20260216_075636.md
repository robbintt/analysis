---
ver: rpa2
title: 'ITI-GEN: Inclusive Text-to-Image Generation'
arxiv_id: '2309.05569'
source_url: https://arxiv.org/abs/2309.05569
tags:
- images
- iti-g
- attributes
- headshot
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses bias in text-to-image generative models by
  proposing ITI-GEN, a method that leverages reference images to create inclusive
  prompts. The core idea is to learn a set of prompt embeddings using reference images
  that represent all desired attribute categories, aligning the direction of the prompt
  embeddings with the image embeddings in a shared vision-language space.
---

# ITI-GEN: Inclusive Text-to-Image Generation

## Quick Facts
- arXiv ID: 2309.05569
- Source URL: https://arxiv.org/abs/2309.05569
- Reference count: 40
- Key outcome: ITI-GEN significantly improves inclusive image generation without model fine-tuning, achieving lower DKL scores and competitive FID scores

## Executive Summary
ITI-GEN addresses bias in text-to-image generative models by learning inclusive prompt embeddings from reference images. The method uses direction alignment and semantic consistency losses in CLIP space to create prompts that generate balanced attribute distributions. By avoiding model fine-tuning and requiring only a few dozen reference images per category, ITI-GEN offers a computationally efficient solution compatible with existing text-to-image models in a plug-and-play manner.

## Method Summary
ITI-GEN learns a set of inclusive prompt embeddings using reference images that represent all desired attribute categories. The method aligns the direction of prompt embeddings with image embeddings in CLIP's shared vision-language space through a direction alignment loss. To prevent language drift, a semantic consistency loss constrains the learned prompts to remain close to the original prompt's semantics. For multi-attribute generation, learned tokens are aggregated through summation, allowing each attribute to be learned independently then combined. The entire process operates without fine-tuning the base text-to-image model, making it computationally efficient.

## Key Results
- Achieves significantly lower DKL scores (0.0007-0.0032) compared to baseline models (0.0359-0.1222), indicating more balanced attribute distributions
- Maintains competitive FID scores (4.59-5.28) while improving inclusivity, demonstrating that better diversity doesn't sacrifice image quality
- Successfully generalizes across multiple attributes (skin tone, eyeglasses, gender) and domains (faces, scenes) without model modification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direction alignment loss effectively translates visual attribute differences into text embedding differences
- **Mechanism:** The method computes the difference between averaged CLIP image embeddings for two categories of an attribute, then forces the difference between their corresponding prompt embeddings to align with this visual direction
- **Core assumption:** CLIP embedding space preserves meaningful attribute directions that can be captured through pairwise category comparisons
- **Evidence anchors:** [abstract] "we design a new training objective to align the directions of the image and prompt features"; [section 3.2] "we draw inspiration from [55, 26] to induce the direction between the prompt P_i^m and P_j^m to be aligned with the direction between the averaged embeddings of the reference images"
- **Break condition:** If CLIP's image embeddings don't preserve meaningful attribute directions, or if attribute differences are too subtle to capture through pairwise alignment

### Mechanism 2
- **Claim:** Semantic consistency loss prevents language drift while maintaining prompt quality
- **Mechanism:** The method constrains learned prompt embeddings to remain close to the original prompt's CLIP embedding, preventing them from drifting too far from natural language semantics
- **Core assumption:** The original prompt's CLIP embedding represents meaningful language semantics that should be preserved
- **Evidence anchors:** [abstract] "we propose semantic consistency loss to address language drift"; [section 3.2] "We observe that direction alignment loss alone may result in language drift... To resolve this issue, we design a semantic consistency objective"
- **Break condition:** If the semantic consistency constraint is too weak to prevent drift, or too strong to allow effective attribute representation

### Mechanism 3
- **Claim:** Aggregating multiple attribute tokens through summation enables multi-attribute generation without interference
- **Mechanism:** When generating images with multiple attributes, the method sums the inclusive tokens for each attribute category, allowing each attribute to be learned independently then combined
- **Core assumption:** Attribute tokens learned from marginal distributions can be combined additively without loss of information or creation of interference
- **Evidence anchors:** [abstract] "we define the inclusive prompt set as follows: P_total = {P_o1o2...oM = [T ; Î£_m S_om] }"; [section 3.1] "By aggregating multiple tokens learned with separate reference datasets in marginal distributions can implicitly disentangle attribute learning"
- **Break condition:** If attributes interact non-linearly, or if the CLIP embedding space doesn't support additive composition of attribute vectors

## Foundational Learning

- **Concept: CLIP embedding space alignment**
  - Why needed here: The entire method relies on CLIP's ability to map both images and text to a shared embedding space where attribute differences are meaningful
  - Quick check question: How does CLIP learn to align visual concepts with text embeddings, and why is this alignment suitable for attribute representation?

- **Concept: Direction-based contrastive learning**
  - Why needed here: The method uses direction alignment between image and prompt embeddings, which requires understanding contrastive learning principles
  - Quick check question: Why might aligning directions be more effective than minimizing absolute distances in this context?

- **Concept: Prompt tuning and token injection**
  - Why needed here: The method learns additional tokens to append to prompts, requiring understanding of how token embeddings affect model outputs
  - Quick check question: How does adding learnable tokens after the original prompt affect the generated image distribution?

## Architecture Onboarding

- **Component map:** Reference images -> CLIP image encoder -> Averaged embeddings -> Direction alignment loss -> Inclusive tokens; Original prompt -> CLIP text encoder -> Semantic consistency loss -> Inclusive tokens; Inclusive tokens -> Summation -> Text-to-image model

- **Critical path:** 1. Load reference images and compute CLIP image embeddings; 2. Initialize inclusive tokens as zero vectors; 3. Compute averaged embeddings for each category; 4. Calculate direction alignment loss between prompt and image directions; 5. Apply semantic consistency loss; 6. Update inclusive tokens via gradient descent; 7. Use learned tokens to generate inclusive images

- **Design tradeoffs:** Frozen model vs. fine-tuning (computationally efficient but limited to CLIP space alignment); Few reference images vs. many (low data requirement but potential noise sensitivity); Direction alignment vs. direct matching (better attribute focus but requires careful loss design); Token summation vs. concatenation (simpler aggregation but may lose some information)

- **Failure signatures:** High KL divergence despite training (direction alignment not capturing attribute differences); Language drift (poor image quality, semantic consistency loss too weak); Attribute interference in multi-attribute generation (token aggregation not preserving independence); Overfitting to reference images (too few reference images or excessive training)

- **First 3 experiments:** 1. Single binary attribute test: Train with male/female faces and verify balanced generation; 2. Direction alignment validation: Check if learned prompt differences match image embedding differences; 3. Semantic consistency verification: Ensure generated images remain faithful to original prompt semantics

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the quality and diversity of reference images impact ITI-GEN's performance in terms of image quality and distribution discrepancy? The paper mentions ITI-GEN can leverage various image sources but doesn't provide detailed analysis of how reference image quality affects performance.

- **Open Question 2:** Can ITI-GEN effectively handle highly entangled attributes like facial expressions and accessories? The paper notes limitations with entangled attributes like "heavy makeup" being correlated with gender, but doesn't provide comprehensive evaluation of performance on such combinations.

- **Open Question 3:** How does ITI-GEN perform in terms of generalization and transferability when applied to different domains and tasks beyond image generation, such as image editing and image-to-image translation? The paper provides a few examples but lacks systematic evaluation across various tasks and domains.

## Limitations
- Relies heavily on CLIP's embedding space preserving meaningful attribute directions without independent validation
- Uses only 25 reference images per category, raising concerns about statistical robustness and potential overfitting
- Assumes additive aggregation of attribute tokens preserves independence, which may not hold for complex, interacting attributes
- Evaluation focuses on distribution uniformity (DKL) but doesn't fully address semantic coherence beyond training-time constraints

## Confidence

- **High Confidence:** The overall framework of using reference images to learn inclusive prompt embeddings is technically sound and the mathematical formulation is clear
- **Medium Confidence:** The direction alignment mechanism is plausible given contrastive learning principles, but effectiveness depends critically on CLIP's embedding properties which aren't independently verified
- **Low Confidence:** The assumption that additive token aggregation preserves attribute independence lacks strong empirical support, especially for correlated or interacting attributes

## Next Checks

1. **Direction Alignment Validation:** After training, measure the cosine similarity between learned prompt direction vectors and their corresponding image embedding direction vectors across all attribute categories to verify if the core alignment mechanism is working as intended.

2. **Ablation on Reference Image Count:** Repeat the training with varying numbers of reference images (5, 25, 50, 100) per category to quantify sensitivity to dataset size and identify potential overfitting thresholds.

3. **Multi-attribute Interference Test:** Systematically generate images with all possible combinations of binary attributes (e.g., male+glasses, female+no glasses) and measure both attribute accuracy and image quality to detect any interference effects from the additive token aggregation assumption.