---
ver: rpa2
title: The Adversarial Consistency of Surrogate Risks for Binary Classification
arxiv_id: '2305.09956'
source_url: https://arxiv.org/abs/2305.09956
tags:
- adversarial
- loss
- implies
- classi
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes a complete characterization of adversarial\
  \ consistency for surrogate losses in binary classification. The key finding is\
  \ that a surrogate loss \u03C6 is adversarially consistent if and only if the infimum\
  \ of \u03C6(\u03B1)/2 + \u03C6(-\u03B1)/2 over \u03B1 is strictly less than \u03C6\
  (0)."
---

# The Adversarial Consistency of Surrogate Risks for Binary Classification

## Quick Facts
- arXiv ID: 2305.09956
- Source URL: https://arxiv.org/abs/2305.09956
- Reference count: 29
- Primary result: A surrogate loss φ is adversarially consistent if and only if inf α φ(α)/2 + φ(−α)/2 < φ(0)

## Executive Summary
This paper establishes a complete characterization of adversarial consistency for surrogate losses in binary classification. The authors prove that a surrogate loss φ is adversarially consistent if and only if the infimum of φ(α)/2 + φ(−α)/2 over α is strictly less than φ(0). This resolves the open question of which surrogate losses can be used in adversarial training without affecting the minimizing sequences of the original adversarial risk. The paper also proves a quantitative bound for the ρ-margin loss, showing that excess adversarial risk is linearly bounded by the adversarial classification error.

## Method Summary
The paper uses theoretical analysis combining minimax theorems, complementary slackness conditions, and properties of minimizing sequences to characterize adversarial consistency. The authors define adversarial risk and classification risk, then establish necessary and sufficient conditions for when surrogate risks can replace the 0-1 loss without affecting minimizing sequences. The analysis focuses on non-increasing, non-negative, continuous loss functions with lim α→∞ φ(α) = 0, and proves that convex losses fail to be adversarially consistent while certain non-convex losses satisfy the required conditions.

## Key Results
- A surrogate loss φ is adversarially consistent if and only if inf α φ(α)/2 + φ(−α)/2 < φ(0)
- Convex losses are not adversarially consistent, while the ρ-margin loss is both consistent and provides a linear bound on excess adversarial risk
- Minimizers of the φ-risk must be bounded away from zero to ensure adversarial consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A surrogate loss φ is adversarially consistent if and only if inf α φ(α)/2 + φ(−α)/2 < φ(0)
- Mechanism: This condition ensures that minimizers of the φ-risk are bounded away from zero, preventing the adversarial risk from being minimized by functions that do not minimize the classification risk
- Core assumption: The loss function φ is non-increasing, non-negative, continuous, and lim α→∞ φ(α) = 0
- Evidence anchors: [abstract] "The key finding is that a surrogate loss φ is adversarially consistent if and only if the infimum of φ(α)/2 + φ(−α)/2 over α is strictly less than φ(0)."
- Break condition: If φ satisfies inf α φ(α)/2 + φ(−α)/2 = φ(0), as is the case for convex losses, then it is not adversarially consistent

### Mechanism 2
- Claim: The ρ-margin loss φρ(α) = min(1, max(1 − α/ρ, 0)) is adversarially consistent and provides a linear bound on excess adversarial risk
- Mechanism: The ρ-margin loss satisfies the condition for adversarial consistency and allows for a direct comparison between the adversarial surrogate risk and the adversarial classification risk
- Core assumption: The loss function φρ is non-increasing, non-negative, continuous, and lim α→∞ φρ(α) = 0
- Evidence anchors: [abstract] "The authors also prove a quantitative bound for the ρ-margin loss, showing that the excess adversarial risk is linearly bounded by the adversarial classification error."
- Break condition: If the loss function does not satisfy the conditions for adversarial consistency or does not provide a linear bound on excess adversarial risk

### Mechanism 3
- Claim: Minimizers of the φ-risk are bounded away from zero, preventing the adversarial risk from being minimized by functions that do not minimize the classification risk
- Mechanism: This condition ensures that the sequence of functions minimizing the φ-risk also minimizes the classification risk, even in the presence of adversarial perturbations
- Core assumption: The loss function φ is non-increasing, non-negative, continuous, and lim α→∞ φ(α) = 0
- Evidence anchors: [section] "The fundamental reason such losses are adversarially consistent is that minimizers of Cφ(η, ·) are bounded away from zero."
- Break condition: If the minimizers of the φ-risk are not bounded away from zero, the adversarial risk may be minimized by functions that do not minimize the classification risk

## Foundational Learning

- Concept: Minimax theorems for adversarial risks
  - Why needed here: These theorems allow for the comparison of minimizing sequences of the adversarial φ-risk and the adversarial classification risk
  - Quick check question: How do the minimax theorems for adversarial risks relate to the consistency of surrogate losses?

- Concept: Complementary slackness conditions
  - Why needed here: These conditions help establish the relationship between minimizing sequences of the adversarial φ-risk and the adversarial classification risk
  - Quick check question: What role do complementary slackness conditions play in proving the consistency of surrogate losses?

- Concept: Non-increasing, non-negative, continuous loss functions
  - Why needed here: These properties of the loss function are essential for the consistency results and the quantitative bounds
  - Quick check question: Why are non-increasing, non-negative, continuous loss functions important for the consistency of surrogate losses in adversarial training?

## Architecture Onboarding

- Component map:
  Loss function φ -> Adversarial risk Rεφ -> Classification risk Rε -> Minimax theorems -> Complementary slackness conditions

- Critical path:
  1. Define the loss function φ and ensure it satisfies the required properties
  2. Prove the consistency of the loss function using the condition inf α φ(α)/2 + φ(−α)/2 < φ(0)
  3. Establish the quantitative bound for the ρ-margin loss, if applicable
  4. Use the minimax theorems and complementary slackness conditions to compare minimizing sequences of the adversarial φ-risk and the adversarial classification risk

- Design tradeoffs:
  - Convex vs. non-convex losses: Convex losses are not adversarially consistent, while certain non-convex losses may be
  - Computational complexity: Minimizing the adversarial φ-risk may be more computationally intensive than minimizing the standard φ-risk
  - Robustness vs. accuracy: Adversarial training can improve robustness but may slightly decrease accuracy on clean data

- Failure signatures:
  - If the loss function does not satisfy the condition inf α φ(α)/2 + φ(−α)/2 < φ(0), it is not adversarially consistent
  - If the quantitative bound for the ρ-margin loss cannot be established, the effectiveness of the loss function in minimizing the adversarial classification risk may be limited
  - If the minimax theorems or complementary slackness conditions cannot be applied, the relationship between minimizing sequences of the adversarial φ-risk and the adversarial classification risk may not be well-understood

- First 3 experiments:
  1. Verify the consistency of a given loss function by checking if it satisfies the condition inf α φ(α)/2 + φ(−α)/2 < φ(0)
  2. Compute the quantitative bound for the ρ-margin loss and compare it to the adversarial classification risk
  3. Apply the minimax theorems and complementary slackness conditions to compare minimizing sequences of the adversarial φ-risk and the adversarial classification risk for a specific dataset and loss function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Theorem 4's quantitative bound on the ρ-margin loss be extended to other adversarially consistent losses?
- Basis in paper: [explicit] The authors state "Extending the bound in Section 5 to further losses remains an open question."
- Why unresolved: The paper only proves this bound for the ρ-margin loss, leaving open whether similar bounds exist for other consistent losses like the shifted sigmoid
- What evidence would resolve it: Proving similar excess risk bounds for other adversarially consistent losses would establish the general applicability of Theorem 4's approach

### Open Question 2
- Question: Does the equivalence between conditions 1 and 3 in Proposition 1 hold in the adversarial setting?
- Basis in paper: [explicit] "Notice that all convex losses satisfy C*φ(1/2) = φ(0): The opposite inequality follows from the observation that C*φ(1/2) ≤ Cφ(1/2,0) = φ(0). In contrast, recall that a convex loss φ with φ′(0) < 0 is consistent [Bartlett et al., 2006]."
- Why unresolved: The paper shows that convex losses are not adversarially consistent, while they are consistent in the standard setting, suggesting the equivalence breaks down in the adversarial case
- What evidence would resolve it: Proving or disproving whether minimizing sequences of Rφ always minimize R in the adversarial setting would resolve this

### Open Question 3
- Question: Can the structure of adversarial consistency be generalized to other perturbation sets beyond ǫ-balls?
- Basis in paper: [inferred] The authors prove their main result for perturbations in an ǫ-ball and suggest "The technique that proved consistency extends to perturbation sets which satisfy existence and minimax theorems analogous to Theorems 1 and 2."
- Why unresolved: The paper focuses on the ǫ-ball case, leaving open whether their results extend to other perturbation sets
- What evidence would resolve it: Proving analogous consistency results for other perturbation sets satisfying similar existence and minimax theorems would establish the general applicability of their approach

## Limitations
- The paper provides theoretical characterization but lacks practical guidelines for selecting surrogate losses in specific applications
- No comprehensive list of loss functions that satisfy the consistency condition beyond the ρ-margin loss
- Does not explore empirical validation of theoretical results or computational complexity considerations

## Confidence

**High confidence**: The characterization of adversarial consistency through the condition inf α φ(α)/2 + φ(−α)/2 < φ(0) is mathematically rigorous and well-established through the proof framework. The necessity and sufficiency of this condition are clearly demonstrated.

**Medium confidence**: The quantitative bound for the ρ-margin loss showing linear relationship between excess adversarial risk and adversarial classification error is theoretically sound, though the practical implications of this bound require further empirical validation.

**Low confidence**: The practical implications of using non-convex, adversarially consistent losses in real-world adversarial training scenarios, particularly regarding computational complexity and generalization performance, remain unexplored.

## Next Checks
1. Empirical validation: Test the ρ-margin loss and other proposed consistent surrogates on benchmark datasets (MNIST, CIFAR-10) under various adversarial attacks to verify the theoretical bounds hold in practice

2. Computational analysis: Evaluate the computational complexity of minimizing adversarially consistent losses compared to standard convex surrogates, and assess the trade-offs in terms of training time and resource requirements

3. Robustness comparison: Conduct ablation studies comparing models trained with adversarially consistent losses against those trained with standard convex surrogates under various attack strengths and types to quantify the practical benefits of consistency