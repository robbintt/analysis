---
ver: rpa2
title: Active Learning for Video Classification with Frame Level Queries
arxiv_id: '2307.05587'
source_url: https://arxiv.org/abs/2307.05587
tags:
- video
- learning
- active
- videos
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an active learning framework for video classification
  where human annotators only need to review a subset of frames from each video rather
  than watching entire videos. The proposed method selects informative videos using
  an uncertainty and diversity criterion, then selects exemplar frames from each video
  using coreset sampling.
---

# Active Learning for Video Classification with Frame Level Queries

## Quick Facts
- arXiv ID: 2307.05587
- Source URL: https://arxiv.org/abs/2307.05587
- Authors: 
- Reference count: 40
- Primary result: Achieves up to 58.66% correct annotation rate with only 100 frames per video

## Executive Summary
This paper presents an active learning framework for video classification that significantly reduces human annotation effort by requiring annotators to review only a subset of frames from each video rather than entire videos. The method combines uncertainty and diversity criteria to select informative videos, then applies coreset sampling to select representative frames from each video. Experiments on UCF-101 and Kinetics datasets demonstrate that the approach outperforms baseline methods, achieving high model performance while minimizing annotation requirements.

## Method Summary
The framework implements a three-stage active learning process: (1) uncertainty-based video selection using Shannon entropy of model predictions, (2) diversity-aware video selection using kernelized distance matrices, and (3) coreset-based frame selection to identify representative frames from each video. An oracle model simulates human annotation behavior by only labeling videos where prediction entropy falls below a threshold. The method uses a CNN-RNN architecture (InceptionV3 + GRU) and iteratively selects batches of videos and frames to annotate, updating the model after each iteration.

## Key Results
- Achieves up to 58.66% correct annotation rate with only 100 frames per video
- Outperforms random selection and entropy-only baselines on both UCF-101 and Kinetics datasets
- Demonstrates effective reduction in annotation effort while maintaining high model performance

## Why This Works (Mechanism)

### Mechanism 1
- Uncertainty and diversity-based video selection improves model generalization by selecting videos with high prediction uncertainty and complementary information
- Core assumption: Uncertain videos contain more learnable information, and diverse videos provide complementary knowledge
- Evidence: Abstract states uncertainty and diversity criterion identifies informative videos; section III-A1 describes Shannon entropy computation for uncertainty
- Break condition: Poor diversity matrix variance leads to redundant video selection; inaccurate uncertainty estimates from poor model calibration

### Mechanism 2
- Coreset-based frame selection maintains model performance while reducing annotation effort by selecting representative frames that cover video content space
- Core assumption: Representative frames can capture essential video information for classification
- Evidence: Section III-B describes coreset techniques for exemplar data point selection; mentions model trained on coreset achieves similar performance to full dataset
- Break condition: Highly redundant video frames lead to insufficient unique examples; rapid content changes make 100 frames inadequate

### Mechanism 3
- Oracle simulation with entropy threshold creates realistic annotation scenario by only labeling confident predictions
- Core assumption: Low-entropy predictions represent confident, reliable labels similar to human annotators
- Evidence: Section IV-B describes oracle model trained on separate data with no information leakage to AL algorithm
- Break condition: Oracle model accuracy too low provides incorrect labels; threshold set too high discards too many videos

## Foundational Learning

- Concept: Shannon entropy as uncertainty measure
  - Why needed here: Quantifies model confidence to select videos where model is uncertain and likely to learn most
  - Quick check: What is the range of Shannon entropy values for a C-class classification problem?

- Concept: NP-hard optimization and approximation algorithms
  - Why needed here: Video selection involves integer quadratic programming which is NP-hard
  - Quick check: Why is k-center problem NP-hard and what approximation guarantee does greedy algorithm provide?

- Concept: Coreset and representative sampling theory
  - Why needed here: Frame selection relies on coreset principles to ensure frames represent full video content
  - Quick check: What theoretical guarantee does coreset sampling provide for model performance vs full dataset training?

## Architecture Onboarding

- Component map: Base model (CNN-RNN) -> Oracle model (CNN-RNN) -> Active learning loop (Video selection -> Frame selection -> Oracle annotation -> Model update) -> Data pipeline (UCF-101/Kinetics datasets)
- Critical path: Model training → Entropy computation → Diversity matrix calculation → Video selection optimization → Frame selection → Oracle simulation → Model retraining
- Design tradeoffs: Video budget (b) vs frame budget (k); oracle accuracy vs threshold; kernel choice for diversity
- Failure signatures: Oracle discards too many videos; test accuracy plateaus early; test accuracy fluctuates; model overfits to selected frames
- First 3 experiments: 1) b=25, k=100 on UCF-101 with random baseline; 2) uncertainty-only video selection; 3) test different frame budgets (k=10, 20, 50, 100)

## Open Questions the Paper Calls Out
- How would the framework perform with real human annotators instead of simulated oracle model? (Acknowledged limitation in paper)
- How does performance scale with larger and more diverse video datasets? (Paper mentions need to validate on other temporal data applications)
- What is optimal tradeoff between number of videos and frames per iteration? (Paper notes batch size varies by application but doesn't comprehensively study combined effects)

## Limitations
- Oracle simulation may not accurately represent real human annotation behavior, fatigue, and attention patterns
- Computational complexity of O(|U|²) for diversity matrix computation may limit scalability to large datasets
- Experiments conducted on relatively small datasets (UCF-101 and Kinetics) with only 5 classes each

## Confidence
- **High confidence**: General framework combining uncertainty and diversity for video selection is sound
- **Medium confidence**: Experimental results promising but oracle simulation introduces variability
- **Low confidence**: Computational efficiency claims and scalability to large-scale datasets not fully validated

## Next Checks
1. **Oracle Calibration Validation**: Verify oracle model accuracy on Toracle set (should be 70-75%), run ablation studies with varying oracle accuracy
2. **Diversity Matrix Verification**: Implement small-scale test (50 videos) to verify diversity matrix computation and iterative truncated power algorithm
3. **Coreset Representativeness Check**: Visualize selected k frames for sample video, measure frame similarity between consecutive selected frames to ensure adequate coverage