---
ver: rpa2
title: Filtered Semi-Markov CRF
arxiv_id: '2311.18028'
source_url: https://arxiv.org/abs/2311.18028
tags:
- semi-crf
- segments
- sequence
- computational
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Filtered Semi-Markov CRF is a new algorithm for text segmentation
  tasks such as Named Entity Recognition. It addresses two key weaknesses of Semi-CRF:
  quadratic complexity and inferior performance compared to CRF.'
---

# Filtered Semi-Markov CRF

## Quick Facts
- arXiv ID: 2311.18028
- Source URL: https://arxiv.org/abs/2311.18028
- Reference count: 24
- Primary result: Achieves up to 2.5 F1 score improvement over CRF and 1.1 over Semi-CRF, with up to 20x and 137x speedups respectively

## Executive Summary
Filtered Semi-Markov CRF is a new algorithm for text segmentation tasks such as Named Entity Recognition that addresses two key weaknesses of Semi-CRF: quadratic complexity and inferior performance compared to CRF. The core innovation is a filtering step that prunes irrelevant segments before the expensive global scoring, reducing complexity from O(L³) to O(L) in practice. Experiments on CoNLL 2003, OntoNotes 5.0, and Arabic ACE datasets show it outperforms both baseline methods while being significantly faster.

## Method Summary
The model combines a lightweight local classifier that scores and filters candidate segments with a global Semi-CRF scorer that operates on the filtered graph. During training, both local and global losses are optimized jointly, with the local classifier learning to identify high-quality segments that make the global scoring problem easier. The filtered graph is constructed by retaining only segments whose top predicted label is non-null, then connecting them via Liang-style edges for efficient Viterbi decoding. This approach maintains the expressiveness of segment-based models while achieving linear-time complexity.

## Key Results
- Achieves up to 2.5 F1 score improvement over CRF and 1.1 over Semi-CRF
- Inference is up to 20x faster than CRF and 137x faster than Semi-CRF
- Graph size grows linearly with sequence length in practice
- Joint training with local and global losses consistently improves performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering reduces the Semi-CRF search space by eliminating low-quality candidate segments before the expensive global scoring step.
- Mechanism: A local classifier ϕlocal scores each possible labeled segment. Segments whose top label is null are pruned, shrinking the set of nodes V from L(L+1)/2 to S ≤ L (number of non-null segments). This cuts the edge count from L(L−1)(L+1)/6 to at most S−1, lowering worst-case decoding from O(L³) to O(L).
- Core assumption: The local classifier's top label prediction is sufficiently accurate to keep all true entity spans and remove most nulls.
- Evidence anchors:
  - [abstract] "incorporating a filtering step to eliminate irrelevant segments, reducing complexity and search space."
  - [section 3.1] "Local classifier ... assigns a score to a labelled segment ... removing the segments sk = (ik, jk, lk) for which lk = arg maxl ϕlocal(ik, jk, l|x) and lk = null."
  - [corpus] Weak: no ablation shown for ϕlocal alone, only the joint model.
- Break condition: If ϕlocal accuracy drops below ~70%, many true entities are filtered out, causing recall collapse.

### Mechanism 2
- Claim: Joint training of local and global losses stabilizes the filtering decision and aligns segment quality with global optimality.
- Mechanism: Llocal (negative log-likelihood of gold segments) trains ϕlocal to recognize high-quality spans; Lglobal (semi-CRF loss on the filtered graph) trains ϕglobal and transitions. Because both are updated together, ϕlocal learns to produce segments that make the global scoring problem easier, and ϕglobal learns to rank those segments well.
- Core assumption: The filtering loss is weighted to avoid over-suppression of null segments; the global loss provides corrective gradients.
- Evidence anchors:
  - [section 4.1] "assign a lower weight to the loss of null segments ... down-weight ... by a ratio β ∈]0, 1]"
  - [section 4.2] "impose certain constraints to ensure that the gold segmentation y forms a valid path in the filtered graph"
  - [section 7] "FSemiCRF w/o global loss ... still performs competitively ... including global loss consistently improves the overall scores"
- Break condition: If β is too small, nulls dominate training and ϕlocal never learns to detect entities; if β is too large, nulls are over-suppressed, causing false positives.

### Mechanism 3
- Claim: Graph construction with missing segments generalizes Semi-CRF scoring while preserving dynamic-programming tractability.
- Mechanism: The filtered graph edges connect segments whose start positions are consecutive after removing nulls; the Viterbi-style DP over this graph computes the best segmentation. Because edges only link non-overlapping segments in the right order, the DP remains O(|V| + |E|).
- Core assumption: The filtered graph remains sparse enough that |V| + |E| ≤ O(L) in practice.
- Evidence anchors:
  - [section 3.1] "we propose to define the edges following Liang et al. (1991): ∀(sk′, sk) ∈ V 2, sk′ → sk ∈ E if jk′ < ik and there is no sk∗ ∈ V such that jk′ < ik∗ and ik∗ < jk."
  - [section 4.4] "Empirical Analysis ... graph size ... seems to grow in a linear fashion ... stays smaller than the sequence length."
  - [corpus] Weak: only runtime measurements shown, not explicit sparsity proofs.
- Break condition: If ϕlocal lets through many overlapping or near-overlapping segments, |E| can balloon, erasing the linear complexity gain.

## Foundational Learning

- Concept: Conditional Random Fields (CRFs) and dynamic programming for sequence labeling
  - Why needed here: Filtered Semi-CRF inherits the CRF-style scoring and Viterbi decoding; understanding partition functions and marginals is essential to grasp why the global loss is a log-likelihood.
  - Quick check question: In a linear-chain CRF, what is the complexity of computing the partition function with the forward algorithm, and why?

- Concept: Semi-Markov models and segment-based representations
  - Why needed here: The original Semi-CRF scores arbitrary-length spans; knowing how ϕglobal replaces per-token scores with per-segment scores clarifies the expressiveness gain and quadratic blow-up.
  - Quick check question: How many possible segments are there in a sequence of length L, and how does that compare to the number of token positions?

- Concept: Transformer-based token representations and segment featurization
  - Why needed here: ϕlocal and ϕglobal both rely on aggregating BERT token embeddings (e.g., sum over the span); understanding this aggregation explains why ϕlocal is "lightweight" yet effective.
  - Quick check question: If you sum BERT token vectors for a span, what is the dimensionality of the resulting segment feature, and how is it used in the linear classifier?

## Architecture Onboarding

- Component map: Input encoder -> Local classifier -> Graph builder -> Global scorer -> Trainer -> Decoder
- Critical path:
  1. Tokenize and embed → h₁…h_L
  2. Enumerate all segments (i,j) and label l → compute ϕlocal scores
  3. Filter → keep only top non-null segments → V
  4. Build edges → E
  5. During training: compute Llocal (over gold segments) + Lglobal (DP partition function)
  6. During inference: Viterbi DP on (V,E) → best segmentation

- Design tradeoffs:
  - Speed vs recall: tighter filtering → faster decoding but risk of missing entities
  - Local vs global accuracy: better ϕlocal → smaller graph, but too aggressive filtering hurts global optimality
  - Segment max-width K: caps |V| to O(LK), trades completeness for tractability

- Failure signatures:
  - Low recall: filtering removes too many true entity spans (ϕlocal over-suppresses)
  - Low precision: many false-positive spans survive filtering (ϕlocal over-generates)
  - Training instability: loss spikes when gold segments are accidentally filtered out
  - Slow inference: filtered graph still large (ϕlocal not selective enough)

- First 3 experiments:
  1. Ablation: run FSemiCRF with ϕlocal frozen at random weights → confirm graph size and timing baseline.
  2. Sensitivity: sweep β (null label loss weight) on dev set → find sweet spot between precision and recall.
  3. Max-width study: vary K on dev set → measure recall drop vs decoding speedup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Filtered Semi-CRF scale with the maximum segment width K?
- Basis in paper: [explicit] The paper mentions that restricting segment width to K reduces complexity from O(L²) to O(LK) but does not provide detailed performance analysis across different values of K.
- Why unresolved: The paper only mentions using a fixed K without exploring its impact on performance across various datasets or tasks.
- What evidence would resolve it: Empirical results showing F1 scores, training/inference times for different values of K on multiple datasets would clarify the trade-off between performance and efficiency.

### Open Question 2
- Question: Can Filtered Semi-CRF be extended to handle nested entity structures effectively?
- Basis in paper: [explicit] The conclusion mentions investigating the extension to nested segment structures as future work.
- Why unresolved: The current model is designed for non-overlapping entities, and the paper does not explore how it would handle overlapping or nested entities.
- What evidence would resolve it: A modified version of Filtered Semi-CRF tested on datasets with nested entities (e.g., ACE 2005) demonstrating performance improvements over existing methods.

### Open Question 3
- Question: How does the choice of segment featurizer f impact the model's performance?
- Basis in paper: [explicit] The paper states that a simple sum operation for f provides strong performance but does not explore other options or justify why this choice is optimal.
- Why unresolved: The paper does not compare different featurizers or analyze their impact on model performance.
- What evidence would resolve it: Ablation studies comparing different featurizers (e.g., mean pooling, attention mechanisms) on the same datasets to determine their effect on F1 scores and training efficiency.

### Open Question 4
- Question: What is the impact of the filtering step on the model's ability to generalize to unseen data?
- Basis in paper: [inferred] The filtering step is designed to reduce the search space and improve efficiency, but its effect on generalization is not explicitly discussed.
- Why unresolved: The paper focuses on performance gains over baselines but does not analyze how the filtering step affects the model's ability to generalize to new, unseen data.
- What evidence would resolve it: Cross-validation results or tests on out-of-domain datasets to assess whether the filtering step introduces bias or limits the model's generalization capability.

## Limitations

- Dependence on local classifier quality: If the filtering classifier fails to retain true entity segments, the global scorer cannot recover them, creating an unavoidable recall bottleneck.
- Missing hyperparameter details: The paper does not report the exact values of maximum segment width K or null-segment loss weighting ratio β, limiting reproducibility.
- Theoretical complexity claim: The O(L) decoding complexity is supported empirically but not rigorously proven, leaving open the possibility of worst-case scenarios where the filtered graph could still be large.

## Confidence

- **High Confidence**: The claim that Filtered Semi-CRF achieves faster inference times than both CRF and Semi-CRF is well-supported by empirical runtime measurements across all three datasets. The mechanism by which filtering reduces the number of segments and edges in the decoding graph is clearly described and logically sound.
- **Medium Confidence**: The claim that Filtered Semi-CRF achieves higher F1 scores than both CRF and Semi-CRF is supported by experimental results, but the ablation studies are limited. While the paper shows that including the global loss improves performance, it does not fully isolate the contribution of the filtering step itself. The joint training mechanism is plausible, but the exact dynamics of how local and global losses interact are not fully explored.
- **Low Confidence**: The theoretical claim of O(L) decoding complexity is not rigorously proven. The paper only shows that the graph size grows linearly in practice, but does not rule out worst-case scenarios where the filtered graph could still be large. Without a formal proof or a more thorough analysis of edge cases, this claim remains speculative.

## Next Checks

1. **Ablation of Local Classifier**: Run Filtered Semi-CRF with the local filtering classifier frozen at random weights (i.e., no learned filtering). Measure the resulting graph size, decoding time, and F1 score. This will quantify how much of the speedup and accuracy gain is due to the learned filtering versus the global Semi-CRF scoring.

2. **Hyperparameter Sensitivity Analysis**: Systematically sweep the maximum segment width K and the null-segment loss weighting ratio β on the development set. Report the resulting F1 scores, graph sizes, and inference times for each setting. This will reveal the robustness of the method to hyperparameter choices and help identify optimal configurations.

3. **Worst-Case Complexity Analysis**: Construct synthetic datasets where the local classifier is forced to retain a large number of overlapping or near-overlapping segments (e.g., by using a weak or adversarial local classifier). Measure the resulting graph size and decoding time. Compare these results to the theoretical O(L) prediction to assess whether the complexity claim holds in edge cases.