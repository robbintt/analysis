---
ver: rpa2
title: 'Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing
  and Polar Box Embeddings'
arxiv_id: '2305.12027'
source_url: https://arxiv.org/abs/2305.12027
tags:
- entity
- duck
- entities
- type
- mention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach to infuse type information in
  the latent space of dense-retrieval entity linking methods, improving their performance.
  The key idea is to define entity types based on their relations in a knowledge graph,
  following the concept of duck typing.
---

# Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings

## Quick Facts
- arXiv ID: 2305.12027
- Source URL: https://arxiv.org/abs/2305.12027
- Reference count: 39
- Sets new state-of-the-art results on standard entity linking benchmarks, outperforming other type-aware approaches and matching results of generative models with 18 times more parameters.

## Executive Summary
This paper introduces DUCK, a novel approach to entity linking that infuses type information into dense-retrieval methods by defining entity types based on their relations in a knowledge graph. Inspired by duck typing from programming, DUCK represents relations as box embeddings in spherical polar coordinates and trains the model to cluster entities with similar types. The method achieves state-of-the-art performance on standard entity linking benchmarks without requiring candidate sets, matching the results of much larger generative models.

## Method Summary
DUCK employs a bi-encoder architecture with RoBERTa-based encoders for entities, mentions, and relations. Relations are represented as box embeddings in spherical polar coordinates, where entities are encouraged to fall within boxes corresponding to their relations. The model jointly optimizes entity disambiguation and duck typing objectives using negative sampling, with training performed in multiple stages including hard-negative mining. The approach works without candidate sets and leverages Wikidata as the knowledge graph for defining entity types.

## Key Results
- Achieves new state-of-the-art results on AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, CWEB, and WIKI entity linking benchmarks
- Outperforms other type-aware approaches while matching the performance of generative models with 18× more parameters
- Successfully links entities without requiring candidate sets, unlike many existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Duck typing defines entity types based on relations in a knowledge graph, enabling fine-grained clustering without explicit type labels.
- Mechanism: Relations are converted into box embeddings in spherical polar coordinates, and entities are placed inside boxes corresponding to their relations, encouraging similar entities to cluster.
- Core assumption: Relations in a knowledge graph carry sufficient semantic information to determine entity types.
- Evidence anchors:
  - [abstract] "Inspired by duck typing in programming languages, we propose to define the type of an entity based on the relations that it has with other entities in a knowledge graph."
  - [section] "Extending this concept to KGs, without any need for type labels, we can describe the type of an entity based on the relations that it has with other entities in the graph."
  - [corpus] "Average neighbor FMR=0.447" (suggests related papers cover similar ground, but direct evidence is weak)
- Break condition: If relations are sparse or noisy, type definitions based on them become unreliable, breaking the clustering.

### Mechanism 2
- Claim: Using box embeddings in spherical polar coordinates aligns with the dot product similarity function, preserving prior entity norms while structuring latent space.
- Mechanism: Boxes are defined by two angle vectors in polar coordinates, ensuring cosine similarity constraints while leaving radial norms free to encode entity priors.
- Core assumption: Dot product is the primary similarity function for entity linking, making polar box embeddings natural.
- Evidence anchors:
  - [abstract] "We define this representation as it naturally aligns with the use of the dot product (or the cosine similarity) as the similarity function for entity and mention embeddings"
  - [section] "Since we are using this similarity to rank entities for a given mention, the norm of the mention embedding is irrelevant, whereas the entity norms encode a 'prior' over entities."
  - [corpus] "Average neighbor FMR=0.447" (weak direct evidence, inference needed)
- Break condition: If a different similarity function (e.g., L2 distance) is used, polar box embeddings may not be optimal.

### Mechanism 3
- Claim: The negative-sampling loss with box distance encourages entities to lie inside boxes of their relations while avoiding others, improving disambiguation.
- Mechanism: Entities are pushed toward the center of boxes for their relations and penalized for being outside, with gradient emphasis on distant entities.
- Core assumption: The box distance function effectively distinguishes entities inside vs outside boxes, with appropriate gradient scaling.
- Evidence anchors:
  - [abstract] "We optimize the model to structure the latent space in such a way that entities fall within the boxes corresponding to their relations"
  - [section] "This objective forces the distance between an entity e and relations r+∈R(e) to be small, while keeping the entity far from boxes corresponding to the negative relations r−."
  - [corpus] "Average neighbor FMR=0.447" (weak direct evidence, inference needed)
- Break condition: If the box distance function is poorly calibrated, entities may not cluster effectively, harming performance.

## Foundational Learning

- Concept: Knowledge graph embeddings and their use in entity linking.
  - Why needed here: Understanding how entities and relations are represented and used for linking tasks is foundational to grasping DUCK's approach.
  - Quick check question: What is the difference between entity embeddings and relation embeddings in knowledge graph models?
- Concept: Spherical coordinate systems and box embeddings.
  - Why needed here: DUCK uses box embeddings in spherical polar coordinates, so understanding this representation is key to understanding how entities are clustered.
  - Quick check question: How do box embeddings in spherical coordinates differ from those in Cartesian coordinates?
- Concept: Negative sampling and contrastive learning objectives.
  - Why needed here: DUCK's loss function uses negative sampling to encourage entities to cluster with their relations, so understanding this technique is crucial.
  - Quick check question: What is the purpose of negative sampling in contrastive learning, and how does it differ from standard cross-entropy?

## Architecture Onboarding

- Component map:
  - Entity encoder (RoBERTa-based) -> entity embeddings
  - Mention encoder (RoBERTa-based) -> mention embeddings
  - Relation encoder (RoBERTa-based) -> relation embeddings -> box parameters
  - Box distance function -> loss computation
- Critical path:
  - Input mention -> mention encoder -> mention embedding -> similarity computation with entity embeddings -> entity ranking
- Design tradeoffs:
  - Using box embeddings in polar coordinates vs Cartesian coordinates (polar aligns with dot product, Cartesian simpler but less natural)
  - Training with vs without candidate sets (DUCK works without, but performance may vary)
  - Hard-negative mining vs batch negatives (hard negatives improve quality but add complexity)
- Failure signatures:
  - Poor clustering: entities not grouping by type, suggesting box embeddings or distance function issues
  - Overfitting: model memorizing training pairs, possibly due to insufficient negative sampling
  - Slow convergence: learning rate or batch size may need adjustment
- First 3 experiments:
  1. Replace box embeddings with simple point embeddings to test if box structure is necessary.
  2. Use Cartesian coordinates instead of spherical to see if polar alignment is beneficial.
  3. Remove duck typing loss and train only with entity-disambiguation loss to measure its impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DUCK vary when using different underlying language models, such as BERT or RoBERTa, for the entity and mention encoders?
- Basis in paper: [explicit] The paper mentions using RoBERTa as the underlying language model for the entity and mention encoders, but does not explore other options like BERT.
- Why unresolved: The choice of language model can significantly impact the performance of the model, and it is unclear whether RoBERTa is the optimal choice or if other models like BERT could lead to better results.
- What evidence would resolve it: Experiments comparing the performance of DUCK using different language models, such as BERT, RoBERTa, or other transformer-based models, would provide insights into the impact of the choice of language model on the model's performance.

### Open Question 2
- Question: How does the performance of DUCK change when using different knowledge graphs, such as DBpedia or YAGO, instead of Wikidata for defining entity types and relations?
- Basis in paper: [inferred] The paper uses Wikidata as the knowledge graph for defining entity types and relations, but does not explore the impact of using other knowledge graphs like DBpedia or YAGO.
- Why unresolved: Different knowledge graphs may have varying levels of completeness, accuracy, and coverage, which could affect the performance of DUCK. It is unclear whether using a different knowledge graph would lead to better or worse results.
- What evidence would resolve it: Experiments comparing the performance of DUCK using different knowledge graphs, such as DBpedia, YAGO, or other large-scale knowledge graphs, would provide insights into the impact of the choice of knowledge graph on the model's performance.

### Open Question 3
- Question: How does the performance of DUCK vary when using different similarity functions, such as cosine similarity or Euclidean distance, instead of the dot product for ranking entities?
- Basis in paper: [explicit] The paper uses the dot product as the similarity function for ranking entities, but does not explore other options like cosine similarity or Euclidean distance.
- Why unresolved: The choice of similarity function can impact the performance of the model, and it is unclear whether the dot product is the optimal choice or if other similarity functions could lead to better results.
- What evidence would resolve it: Experiments comparing the performance of DUCK using different similarity functions, such as cosine similarity, Euclidean distance, or other distance metrics, would provide insights into the impact of the choice of similarity function on the model's performance.

## Limitations
- The approach relies heavily on the quality and completeness of relations in the knowledge graph, which may vary across different domains.
- The method assumes dot product similarity is optimal, potentially limiting applicability to systems using different similarity measures.
- Only compared against a limited set of baselines, leaving questions about relative efficiency compared to more recent generative approaches.

## Confidence
- High confidence: The mechanism by which box embeddings in polar coordinates align with dot product similarity and preserve entity norms.
- Medium confidence: The duck typing concept for defining entity types based on relations without explicit labels - conceptually sound but dependent on relation quality.
- Medium confidence: The effectiveness of the negative sampling loss with box distance for encouraging proper entity clustering - theoretically justified but sensitive to hyperparameter tuning.

## Next Checks
1. **Architecture Ablation**: Remove the duck typing loss entirely and compare performance to the full model on AIDA-CoNLL to quantify the specific contribution of type-aware clustering versus standard entity disambiguation.
2. **Coordinate System Comparison**: Implement the same box embedding approach using Cartesian coordinates instead of spherical polar coordinates and measure performance degradation to validate the claimed alignment benefits.
3. **Relation Sparsity Stress Test**: Systematically reduce the number of relations per entity in the knowledge graph and measure performance decline to establish the minimum viable relation density for the duck typing approach to work effectively.