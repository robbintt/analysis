---
ver: rpa2
title: Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning
arxiv_id: '2312.16409'
source_url: https://arxiv.org/abs/2312.16409
tags:
- learning
- data
- unlabeled
- distillation
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of semi-supervised continual learning
  (SSCL), where models learn from partially labeled data with unknown categories.
  The key issue addressed is the unreliable distribution of unlabeled data, which
  leads to unstable training and refinement, severely impacting SSCL performance.
---

# Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning

## Quick Facts
- arXiv ID: 2312.16409
- Source URL: https://arxiv.org/abs/2312.16409
- Reference count: 7
- Primary result: Proposed Dynamic Sub-graph Distillation (DSGD) method achieves up to 60% memory savings over state-of-the-art approaches in semi-supervised continual learning.

## Executive Summary
This paper addresses the challenge of semi-supervised continual learning (SSCL) where models must learn from partially labeled data with unknown categories while avoiding catastrophic forgetting. The key problem tackled is the unreliable distribution of unlabeled data leading to unstable training. The authors propose Dynamic Sub-graph Distillation (DSGD), a novel approach that leverages both semantic and structural information to achieve more stable knowledge distillation on unlabeled data. DSGD uses personalized PageRank vectors over dynamically constructed graphs to preserve local neighborhood structures, showing robustness against distribution bias. Experiments on CIFAR10, CIFAR100, and ImageNet-100 demonstrate effectiveness in mitigating catastrophic forgetting while achieving significant memory savings.

## Method Summary
DSGD constructs dynamic topology graphs for both new and replayed data using cosine similarities of embeddings, then enforces similarity between personalized PageRank (PPR) vectors over these graphs. The method computes K-step transition probabilities to derive PPR vectors for sub-graph distillation, reducing reliance on pseudo-labels and absolute representation alignment. It combines this with FixMatch-style consistency loss and integrates with a memory replay buffer. The approach enables end-to-end training and adaptability to scale up tasks while achieving significant memory occupation savings compared to existing methods.

## Key Results
- DSGD achieves up to 60% memory occupation savings over existing state-of-the-art approaches
- Demonstrates effectiveness in mitigating catastrophic forgetting in semi-supervised continual learning scenarios
- Shows robustness across various supervision ratios on CIFAR10, CIFAR100, and ImageNet-100 datasets
- Outperforms iCaRL&Fix and DER&Fix baselines in average incremental accuracy and last incremental accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method stabilizes semi-supervised continual learning by preserving local sub-graph structures rather than relying on instance-level representations that are prone to distribution bias.
- Mechanism: It constructs dynamic topology graphs for both new and replayed data, then enforces similarity between personalized PageRank (PPR) vectors over these graphs. This ensures that the local neighborhood structure of each sample is preserved across tasks.
- Core assumption: The local structure of data (neighborhood relationships) is more stable across tasks than absolute instance representations, which are vulnerable to distribution drift.
- Evidence anchors:
  - [abstract] "leverages both semantic and structural information to achieve more stable knowledge distillation on unlabeled data and exhibit robustness against distribution bias."
  - [section] "We formalize the proposed method as follows... design a distillation objective to ensure its sub-structure preserving property"
  - [corpus] Weak - only one neighbor paper has relevant terms, but does not discuss PPR-based sub-graph distillation.
- Break condition: If the graph construction method fails to capture meaningful associations, or if the PPR-based similarity metric becomes uninformative due to high dimensionality or sparse connectivity.

### Mechanism 2
- Claim: The dynamic construction of topology graphs at each batch allows the model to adapt to evolving knowledge without needing to store the entire dataset.
- Mechanism: At each task, the method builds a new graph G(Dt, PN) from current representations and an old graph G(MR, PR) from replayed exemplars. These graphs are used to compute transition probabilities and derive PPR vectors for sub-graph distillation.
- Core assumption: The feature space evolves in a way that still preserves relative similarities, allowing the transition matrix to remain meaningful across tasks.
- Evidence anchors:
  - [section] "To ensure the consistency of the graph structure in such scenarios, it becomes essential to design an approach capable of adapting to dynamically changing graph structures."
  - [section] "The appearance of new tasks leads to the acquisition of new knowledge... Our graph structure is specifically designed to adapt to these dynamic processes"
  - [corpus] Weak - no corpus evidence directly supporting this dynamic graph adaptation claim.
- Break condition: If feature drift is too large, the cosine similarity matrix may become uninformative, breaking the graph structure's relevance.

### Mechanism 3
- Claim: The sub-graph distillation loss reduces the reliance on pseudo-labels and absolute representation alignment, which are error-prone in semi-supervised settings.
- Mechanism: Instead of aligning raw logits or features, it aligns PPR vectors derived from the graph structure. These vectors capture high-order neighborhood information, making the distillation more robust to mislabeled or poorly distributed samples.
- Core assumption: High-order neighborhood information is less sensitive to label noise and distribution shifts than direct feature or output alignment.
- Evidence anchors:
  - [section] "Through using graph-based techniques, we build a projection from the old topology graph to the new one and preserve essential local structures."
  - [section] "By relying less on absolute representations, our DSGD strategy can mitigate the influence of data distribution bias and pseudo-label errors"
  - [corpus] No direct evidence; the neighbor papers discuss semi-supervised continual learning but not PPR-based distillation.
- Break condition: If pseudo-label errors are systematic or if the neighborhood structure itself becomes biased, the PPR vectors may still carry misleading signals.

## Foundational Learning

- Concept: Graph-based knowledge representation and personalized PageRank (PPR)
  - Why needed here: The method uses PPR to quantify node proximity and sub-graph similarity across tasks, which is central to its robustness claim.
  - Quick check question: What does a high PPR value between two nodes indicate in the context of this method?

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: The paper addresses the problem of forgetting learned knowledge when new tasks arrive, especially in the semi-supervised regime where unlabeled data may be unreliable.
  - Quick check question: How does semi-supervised continual learning differ from standard continual learning in terms of data availability?

- Concept: Semi-supervised learning (SSL) and consistency regularization
  - Why needed here: The method builds upon SSL baselines like FixMatch, which rely on consistency between weakly and strongly augmented views of data.
  - Quick check question: In FixMatch, what is the role of pseudo-labels and how are they used in the loss?

## Architecture Onboarding

- Component map: Feature extractor -> Graph construction module -> Transition matrix computation -> PPR vector computation -> Sub-graph distillation loss -> SSL loss integration -> Memory replay buffer

- Critical path:
  1. Extract embeddings from current and replayed data.
  2. Build similarity-based adjacency matrices.
  3. Compute transition matrices and PPR vectors.
  4. Calculate sub-graph distillation loss.
  5. Combine with SSL loss and backpropagate.

- Design tradeoffs:
  - Memory vs. graph quality: Larger replay buffers yield more stable graphs but increase memory usage.
  - K-order PPR vs. computation: Higher K captures broader neighborhoods but increases cost.
  - Feature extractor choice: Affects graph structure quality; mismatches across tasks can degrade performance.

- Failure signatures:
  - Degraded accuracy on old tasks indicates catastrophic forgetting.
  - High variance in PPR vectors across batches suggests unstable graph construction.
  - No improvement over baselines may signal ineffective sub-graph preservation.

- First 3 experiments:
  1. Validate sub-graph distillation on CIFAR100-20 with varying K values to assess sensitivity.
  2. Compare DSGD against iCaRL&Fix baseline on CIFAR10-30 to measure forgetting mitigation.
  3. Test robustness by intentionally introducing label noise and measuring degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Dynamic Sub-graph Distillation (DSGD) method scale to even larger datasets and more complex tasks compared to the evaluated benchmarks?
- Basis in paper: [inferred] The paper evaluates DSGD on CIFAR10, CIFAR100, and ImageNet-100, but does not explore scaling to larger datasets or more complex tasks.
- Why unresolved: The paper only provides experimental results on relatively small-scale datasets. It does not investigate the method's performance on larger, more challenging datasets or tasks with higher complexity.
- What evidence would resolve it: Experiments on larger-scale datasets (e.g., ImageNet-1K, JFT-300M) and more complex tasks (e.g., object detection, semantic segmentation) would demonstrate the scalability of DSGD.

### Open Question 2
- Question: How does the choice of the parameter K in the K-order PPR value affect the performance of DSGD, and what is the optimal value for different datasets and tasks?
- Basis in paper: [explicit] The paper mentions that the performance of DSGD changes with different values of K, but does not provide a systematic study on the optimal value of K for different datasets and tasks.
- Why unresolved: The paper only provides a limited analysis of the impact of K on the performance of DSGD, without exploring the optimal value of K for different datasets and tasks.
- What evidence would resolve it: A comprehensive study on the impact of K on the performance of DSGD across different datasets and tasks, along with the identification of the optimal value of K for each case, would provide insights into the role of K in the method's performance.

### Open Question 3
- Question: How does DSGD compare to other state-of-the-art methods for semi-supervised continual learning in terms of computational efficiency and memory usage?
- Basis in paper: [inferred] The paper mentions that DSGD achieves up to 60% memory occupation savings over existing state-of-the-art approaches, but does not provide a comprehensive comparison of computational efficiency and memory usage with other methods.
- Why unresolved: The paper only provides a limited comparison of memory usage with other methods, without exploring the computational efficiency of DSGD relative to other state-of-the-art approaches.
- What evidence would resolve it: A detailed comparison of computational efficiency and memory usage of DSGD with other state-of-the-art methods for semi-supervised continual learning, including wall-clock time and memory consumption, would provide a comprehensive understanding of the method's efficiency.

## Limitations
- The core assumption that local sub-graph structures are more stable than instance-level representations across tasks is not rigorously tested across diverse data distributions
- Performance on ImageNet-100 is based on a single run without statistical significance testing
- Memory savings claim of up to 60% lacks clear baseline comparison methodology
- Computational overhead of graph construction is not thoroughly analyzed

## Confidence
- **High confidence**: The method's ability to reduce catastrophic forgetting when combined with replay buffers is well-supported by experimental results across multiple datasets
- **Medium confidence**: The robustness claim against distribution bias is demonstrated but could be strengthened with more diverse distribution shift scenarios and systematic ablation studies
- **Low confidence**: The scalability claim to larger datasets and tasks is not empirically validated beyond ImageNet-100, and the computational overhead of graph construction is not thoroughly analyzed

## Next Checks
1. Conduct systematic ablation studies varying K in PPR computation and graph construction parameters to identify optimal configurations and sensitivity to hyperparameters
2. Test DSGD on larger-scale datasets (e.g., ImageNet-1K) and more complex task sequences to validate scalability claims and computational efficiency
3. Design controlled experiments introducing different types of distribution shifts (covariate shift, label shift) to rigorously evaluate robustness against distribution bias beyond the current experimental setup