---
ver: rpa2
title: Is Self-Repair a Silver Bullet for Code Generation?
arxiv_id: '2306.09896'
source_url: https://arxiv.org/abs/2306.09896
tags:
- number
- code
- program
- input
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of self-repair for code
  generation using large language models. The authors propose a new evaluation metric,
  pass@t, that accounts for the computational cost of self-repair by measuring the
  likelihood of obtaining a correct program as a function of the total number of tokens
  sampled.
---

# Is Self-Repair a Silver Bullet for Code Generation?
## Quick Facts
- arXiv ID: 2306.09896
- Source URL: https://arxiv.org/abs/2306.09896
- Reference count: 40
- Key outcome: Self-repair is only effective with GPT-4 and shows modest performance gains, with feedback quality being the primary bottleneck.

## Executive Summary
This paper investigates whether self-repair techniques can improve code generation performance using large language models. The authors propose a new evaluation metric, pass@t, that accounts for the computational cost of self-repair by measuring the likelihood of obtaining a correct program as a function of total tokens sampled. Experiments on the APPS dataset with GPT-3.5 and GPT-4 reveal that self-repair is only effective with GPT-4, and performance gains are modest. The study finds that self-repair is bottlenecked by the model's ability to provide accurate feedback on its own code.

## Method Summary
The study uses a four-stage self-repair process: initial code generation, code execution, feedback generation, and code repair. The authors evaluate performance using the pass@t metric on 300 Python programming challenges from the APPS dataset. They test various configurations of number of initial programs (np), number of feedback instances (nf), and number of repairs (nr), comparing self-repair against a baseline of i.i.d. sampling from the same model.

## Key Results
- Self-repair only improves performance with GPT-4, showing modest gains over baseline approaches
- GPT-3.5 cannot effectively repair its own code regardless of computational budget
- Replacing GPT-3.5's feedback with GPT-4's feedback significantly improves performance
- Human feedback substantially outperforms model-generated feedback even for GPT-4

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Self-repair improves performance when using GPT-4 but not GPT-3.5.
- Mechanism: GPT-4 has stronger introspective ability to identify and correct its own code errors, while GPT-3.5 lacks this capability.
- Core assumption: The model's ability to provide accurate self-feedback is a primary bottleneck for self-repair effectiveness.
- Evidence anchors:
  - [abstract]: "self-repair is only effective with GPT-4, and performance gains are modest"
  - [section 4.1]: "pass@t is lower than or equal to the corresponding baseline for all settings of np, nf r" for GPT-3.5

### Mechanism 2
- Claim: Self-repair is bottlenecked by the quality of feedback in the self-feedback stage.
- Mechanism: Using a stronger model (GPT-4) to generate feedback for a weaker code model (GPT-3.5) improves performance significantly.
- Core assumption: The feedback stage is more critical than the repair stage for successful self-repair.
- Evidence anchors:
  - [abstract]: "Replacing GPT-3.5's feedback with GPT-4's feedback or using human feedback significantly improves performance"
  - [section 4.2]: "MP = GPT-3.5, MF = GPT-4 does break through the performance barrier and becomes marginally more efficient than i.i.d. sampling from GPT-3.5"

### Mechanism 3
- Claim: Human feedback significantly outperforms model-generated feedback even for GPT-4.
- Mechanism: Human programmers provide more accurate and contextually appropriate feedback than GPT-4 can generate for its own code.
- Core assumption: Human expertise in code debugging exceeds current LLM capabilities.
- Evidence anchors:
  - [abstract]: "Replacing GPT-4's own explanations with those of a human programmer improves repair significantly, leading to a 57% increase"
  - [section 4.3]: "the overall success rate is increased by over 1.57× when we replace GPT-4's own debugging with that of our human participants"

## Foundational Learning
- Concept: pass@t metric evaluation
  - Why needed here: Traditional pass@k doesn't account for computational cost of self-repair, making it unsuitable for comparing self-repair vs baseline approaches
  - Quick check question: How does pass@t differ from pass@k in measuring self-repair effectiveness?

- Concept: Repair tree structure
  - Why needed here: Understanding the hierarchical structure of self-repair (initial code → feedback → repair) is crucial for analyzing performance bottlenecks
  - Quick check question: What are the three main components of a repair tree in the self-repair process?

- Concept: Baseline comparison methodology
  - Why needed here: Properly comparing self-repair against i.i.d. sampling requires equivalent token budgets and understanding of computational costs
  - Quick check question: Why is it important to compare self-repair performance against a baseline with the same token budget?

## Architecture Onboarding
- Component map: Code generation model (MP) → Code execution → Feedback generation model (MF) → Code repair model (MP)
- Critical path: Initial code generation → Error detection → Feedback generation → Repair generation → Test validation
- Design tradeoffs: Joint vs separated feedback/repair stages; model selection for each component; hyper-parameter choices (np, nf, nr)
- Failure signatures: Poor initial code diversity; inadequate feedback quality; repair model inability to implement feedback; computational budget exhaustion
- First 3 experiments:
  1. Compare pass@t for self-repair vs baseline with GPT-4 at various token budgets
  2. Test performance impact of using GPT-4 feedback for GPT-3.5 code
  3. Measure human feedback effectiveness on GPT-4-generated code

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the effectiveness of self-repair depend on the complexity of the programming task, or is it consistent across different difficulty levels?
- Basis in paper: [inferred] The paper mentions varying results across different difficulty levels (introductory, interview, competition) but does not provide a detailed analysis of how task complexity affects self-repair performance.
- Why unresolved: The paper does not explicitly analyze the relationship between task complexity and self-repair effectiveness, only providing aggregated results.
- What evidence would resolve it: A detailed breakdown of self-repair performance across different task complexities, potentially showing a correlation between task difficulty and the effectiveness of self-repair.

### Open Question 2
- Question: How does the diversity of initial program samples impact the success rate of self-repair, and is there an optimal balance between diversity and computational cost?
- Basis in paper: [explicit] The paper discusses the importance of diversity in initial program samples but does not explore the optimal balance between diversity and computational cost.
- Why unresolved: The paper only mentions the trend that increasing the number of initial samples improves performance but does not investigate the trade-off between diversity and computational cost.
- What evidence would resolve it: Experiments that systematically vary the diversity of initial samples while measuring both performance and computational cost, identifying an optimal balance.

### Open Question 3
- Question: Can the feedback generation model be further improved to enhance self-repair performance, and what specific improvements would yield the most significant gains?
- Basis in paper: [explicit] The paper shows that using a stronger feedback model (GPT-4 for GPT-3.5) improves performance, but does not explore other potential improvements to the feedback generation process.
- Why unresolved: The paper only considers the impact of using a stronger feedback model and does not investigate other potential improvements, such as more sophisticated feedback generation techniques or incorporating additional information sources.
- What evidence would resolve it: Experiments that test various feedback generation improvements, such as incorporating external knowledge sources, using more advanced models, or employing different feedback generation strategies, and measure their impact on self-repair performance.

## Limitations
- Dataset Scope Limitation: The study uses only 300 problems from the APPS dataset, which may not be representative of the broader code generation landscape.
- Human Feedback Constraints: The human evaluation involved only two undergraduate participants with programming experience but no professional coding background.
- Computational Cost Trade-off: The paper doesn't fully explore the practical trade-offs between self-repair cost and alternative approaches like ensembling or using larger models directly.

## Confidence
- High Confidence: The finding that self-repair performance is bottlenecked by feedback quality is well-supported by the ablation studies showing significant improvements when using GPT-4 for feedback instead of GPT-3.5.
- Medium Confidence: The conclusion that GPT-4 self-repair is only modestly better than baseline sampling has strong empirical support, but the practical significance depends on specific use-case requirements.
- Low Confidence: The generalizability of findings to other code generation tasks, model architectures, or problem domains remains uncertain given the limited dataset scope.

## Next Checks
1. **Scale Validation**: Test the self-repair framework on the complete APPS dataset (rather than 300 samples) and additional code generation benchmarks to assess whether findings hold across broader problem distributions and difficulty ranges.

2. **Feedback Quality Analysis**: Conduct a detailed error analysis of GPT-4 feedback quality across different error types (syntax errors, logic errors, edge cases) to identify specific feedback bottlenecks and potential targeted improvements.

3. **Cost-Benefit Analysis**: Perform a comprehensive evaluation comparing self-repair against alternative approaches (ensembling, model scaling, few-shot prompting) under equivalent computational budgets to determine practical scenarios where self-repair provides optimal value.