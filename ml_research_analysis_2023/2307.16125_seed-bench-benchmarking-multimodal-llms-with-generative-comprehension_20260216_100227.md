---
ver: rpa2
title: 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension'
arxiv_id: '2307.16125'
source_url: https://arxiv.org/abs/2307.16125
tags:
- evaluation
- understanding
- questions
- image
- dimensions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEED-Bench introduces a comprehensive benchmark for evaluating
  multimodal large language models (MLLMs) on generative comprehension tasks. The
  benchmark contains 19K multiple-choice questions with human-verified ground truth
  answers across 12 evaluation dimensions, spanning both spatial understanding (scene
  understanding, object identity, location, attributes, counting, spatial relations,
  interaction, visual reasoning, text recognition) and temporal understanding (action
  recognition, prediction, and procedure understanding).
---

# SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension

## Quick Facts
- **arXiv ID**: 2307.16125
- **Source URL**: https://arxiv.org/abs/2307.16125
- **Reference count**: 40
- **Primary result**: SEED-Bench is a comprehensive benchmark with 19K multiple-choice questions across 12 evaluation dimensions for assessing multimodal LLMs on generative comprehension tasks

## Executive Summary
SEED-Bench introduces a comprehensive benchmark for evaluating multimodal large language models (MLLMs) on generative comprehension tasks. The benchmark contains 19K multiple-choice questions with human-verified ground truth answers across 12 evaluation dimensions, spanning both spatial understanding (scene understanding, object identity, location, attributes, counting, spatial relations, interaction, visual reasoning, text recognition) and temporal understanding (action recognition, prediction, and procedure understanding). The benchmark is evaluated on 18 models including LLMs, ImageLLMs, and VideoLLMs, with InstructBLIP achieving the highest overall performance. The evaluation strategy computes log-likelihood scores for each answer option to select the most probable choice, avoiding reliance on instruction-following capabilities.

## Method Summary
The benchmark is constructed using a pipeline that leverages foundation models to extract visual information from images and videos, then uses ChatGPT/GPT-4 to generate questions and answer options. Automatic filtering removes questions answerable without visual input, followed by human verification to ensure accuracy and categorization. The evaluation strategy computes log-likelihood scores for each answer option to select the most probable choice, avoiding reliance on instruction-following capabilities. The benchmark is evaluated on 18 models including LLMs, ImageLLMs, and VideoLLMs, with InstructBLIP achieving the highest overall performance.

## Key Results
- SEED-Bench contains 19K multiple-choice questions across 12 evaluation dimensions covering spatial and temporal understanding
- InstructBLIP achieves the highest overall performance among evaluated models, excelling particularly in spatial understanding tasks
- Most models show limited performance across evaluation dimensions, with VideoLLMs particularly struggling with temporal understanding tasks
- MLLMs demonstrate superior global image comprehension but struggle with fine-grained spatial relationships, text recognition, and temporal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using log-likelihood scoring instead of instruction-following improves evaluation objectivity
- Mechanism: Instead of relying on models to output "A/B/C/D" as instructed, the evaluation computes log-likelihood scores for each answer option and selects the highest one, removing dependency on the model's instruction-following capabilities
- Core assumption: Log-likelihood scoring provides a more objective measure of answer confidence than forced-choice instruction following
- Evidence anchors:
  - [abstract]: "we follow GPT-3 [ 32] to calculate log-likelihood for each candidate option and select the one with the highest value as the final prediction, without relying on the instruction-following capabilities of models to output 'A' or 'B' or 'C' or 'D'"
  - [section]: "Different from MMBench [ 26] that employs ChatGPT to match a model's prediction to one of the choices in a multiple-choice question (achieves only 87.0% alignment rate), we adopt the answer ranking strategy [10, 32, 39] for evaluating existing MLLMs with multiple-choice questions"
- Break condition: If the model's log-likelihood scores become unreliable or if instruction-following capabilities improve significantly, this advantage may diminish

### Mechanism 2
- Claim: Human verification after automated filtering ensures high-quality benchmark data
- Mechanism: The pipeline uses automated filtering to remove questions answerable without visual input, then employs human annotators to verify correct answers and classify questions into evaluation dimensions
- Core assumption: Human verification can catch errors that automated filtering misses, ensuring benchmark quality
- Evidence anchors:
  - [abstract]: "Human Annotaon SEED-Bench...we employ human annotators to select the correct option and classify each question into one evaluation dimension, resulting in a clean, high-quality benchmark"
  - [section]: "Finally, we employ human annotators to choose the correct option of each multiple-choice question and classify each question into one evaluation dimension"
- Break condition: If human annotator consistency decreases or if automated filtering becomes sufficiently advanced, the need for human verification may reduce

### Mechanism 3
- Claim: Using multiple foundation models for visual information extraction provides comprehensive coverage
- Mechanism: The pipeline extracts visual information using image captions, instance-level descriptions, and textual elements through multiple pretrained models (BLIP2, Tag2Text, SAM, GRiT, PaddleOCR), providing rich multimodal input for question generation
- Core assumption: Different foundation models capture different aspects of visual information, and their combination provides more comprehensive coverage than any single model
- Evidence anchors:
  - [abstract]: "For images, we utilize various foundation models to extract their visual information including image-level captions [6, 27], instance-level descriptions [28, 29, 30] and textual elements [31]"
  - [section]: "The extraction of visual information for images includes the following parts: Image Captions... Instance Descriptions... Textual Elements"
- Break condition: If foundation models become sufficiently advanced to capture all necessary information with fewer models, or if computational costs become prohibitive, this multi-model approach may be optimized

## Foundational Learning

- **Multimodal Large Language Models (MLLMs)**: Understanding what MLLMs are and how they process both visual and textual information is fundamental to understanding the benchmark's purpose
  - Quick check: What distinguishes an MLLM from a traditional LLM in terms of input modalities?

- **Log-likelihood scoring**: The evaluation method relies on computing log-likelihood scores rather than instruction-following, which is a key methodological contribution
  - Quick check: How does log-likelihood scoring differ from traditional classification accuracy metrics?

- **Foundation models for visual understanding**: The benchmark construction pipeline relies on multiple foundation models for extracting visual information, which is crucial for understanding how the benchmark questions are generated
  - Quick check: What types of visual information can foundation models extract from images, and how do these complement each other?

## Architecture Onboarding

- **Component map**: Visual information extraction → Question generation → Automated filtering → Human verification → Benchmark completion
- **Critical path**: Visual information extraction → Question generation → Automated filtering → Human verification → Benchmark completion
- **Design tradeoffs**: The multi-stage pipeline ensures quality but increases complexity and cost; using multiple foundation models provides comprehensive coverage but increases computational requirements
- **Failure signatures**: Poor evaluation results may indicate either model limitations or benchmark construction issues; inconsistent human annotations may indicate unclear evaluation dimensions
- **First 3 experiments**:
  1. Test the automated filtering step by running the same questions through different LLM combinations to measure consistency
  2. Evaluate the impact of removing one foundation model from the visual information extraction pipeline to assess redundancy
  3. Compare evaluation results using log-likelihood scoring versus traditional instruction-following to quantify the improvement in objectivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal LLMs be improved to better handle fine-grained temporal reasoning and procedure understanding in videos?
- Basis in paper: [explicit] The paper notes that most MLLMs exhibit unsatisfactory performance on fine-grained temporal understanding, with VideoLLMs failing to achieve competitive performance on temporal understanding compared to ImageLLMs.
- Why unresolved: Current models struggle with recognizing and sorting key actions in videos, as evidenced by their poor performance on the procedure understanding evaluation dimension. The paper suggests that existing VideoLLMs have limited capabilities for fine-grained action recognition, temporal relationship understanding, and temporal reasoning.
- What evidence would resolve it: Developing new architectures or training strategies that significantly improve performance on temporal understanding tasks, particularly procedure understanding, would demonstrate progress. A successful model would need to outperform current VideoLLMs by a substantial margin on these specific tasks.

### Open Question 2
- Question: What architectural modifications or training approaches could enable multimodal LLMs to achieve better spatial relationship understanding between objects?
- Basis in paper: [explicit] The paper observes that while MLLMs achieve high performance on instance localization, they show weaker abilities in understanding spatial relationships between objects, with InstructBLIP achieving only 40% accuracy on spatial relations compared to 63.37% on instance location.
- Why unresolved: Spatial relationship recognition is more challenging than instance localization due to increased variability in object arrangements and potential ambiguity in determining relationships. The paper suggests this requires higher-level spatial reasoning capabilities that current models lack.
- What evidence would resolve it: A model achieving significantly higher accuracy on spatial relation tasks while maintaining strong performance on instance localization would indicate successful resolution. This could be demonstrated through architectural innovations like improved spatial reasoning modules or specialized training objectives.

### Open Question 3
- Question: How can multimodal LLMs be developed to better handle text recognition in images, given that most current models perform poorly on this task?
- Basis in paper: [explicit] The paper notes that most MLLMs show poor performance for text recognition, with only InstructBLIP achieving reasonable accuracy while other models score below 40%. This is attributed to the lack of textual elements in multimodal pre-training datasets.
- Why unresolved: Current models were not pre-trained on datasets rich in textual elements, limiting their ability to identify and extract text from images. The paper suggests that future work should develop models better equipped to handle text recognition through pre-training on appropriate datasets.
- What evidence would resolve it: A model achieving high accuracy on text recognition tasks through either architectural modifications or specialized training on text-rich datasets would demonstrate progress. This could involve integrating OCR capabilities or developing attention mechanisms specifically tuned for textual elements in images.

## Limitations

- The evaluation strategy relies on log-likelihood scoring, but the paper does not provide empirical validation that this method is superior to traditional instruction-following approaches for multiple-choice questions
- The benchmark's focus on specific datasets (CC3M for images, Something-Something-v2, Epic-kitchen 100, and Breakfast for videos) may limit generalizability to other visual domains
- The paper does not address potential bias introduced by using ChatGPT/GPT-4 for question generation, which could affect the distribution of question difficulty or types

## Confidence

- **High confidence**: The construction pipeline methodology (visual information extraction → question generation → automated filtering → human verification) is well-specified and reproducible. The overall structure of the 12 evaluation dimensions and the distinction between spatial and temporal understanding is clearly articulated.
- **Medium confidence**: The claim that SEED-Bench provides more comprehensive coverage than existing benchmarks is supported by the 12 evaluation dimensions, but direct quantitative comparisons with other benchmarks are limited. The effectiveness of the automated filtering step in removing visually-independent questions is claimed but not extensively validated with ablation studies.
- **Low confidence**: The specific superiority of log-likelihood scoring over instruction-following for multiple-choice evaluation is not empirically demonstrated. The paper asserts that this approach removes dependency on instruction-following capabilities, but does not show this improves correlation with true model comprehension abilities.

## Next Checks

1. Conduct an ablation study comparing evaluation results using log-likelihood scoring versus traditional instruction-following methods on the same SEED-Bench questions to quantify any improvements in objectivity and correlation with true comprehension.

2. Perform inter-annotator agreement analysis on a subset of questions to establish the reliability of human verification, including computing Cohen's kappa or similar metrics for both answer selection and dimension classification tasks.

3. Test the sensitivity of benchmark results to the choice of foundation models in the visual information extraction pipeline by systematically removing or replacing individual models (e.g., testing without GRiT or with an alternative to BLIP2) and measuring changes in downstream evaluation performance.