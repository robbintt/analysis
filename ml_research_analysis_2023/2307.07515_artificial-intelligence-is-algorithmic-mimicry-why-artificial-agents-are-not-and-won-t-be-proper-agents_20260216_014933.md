---
ver: rpa2
title: 'Artificial intelligence is algorithmic mimicry: why artificial "agents" are
  not (and won''t be) proper agents'
arxiv_id: '2307.07515'
source_url: https://arxiv.org/abs/2307.07515
tags:
- https
- world
- intelligence
- have
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that artificial general intelligence (AGI) is
  not achievable within the current algorithmic framework of AI research due to fundamental
  differences between living and algorithmic systems. The author identifies three
  key distinctions: (1) living systems are autopoietic (self-manufacturing) and can
  set intrinsic goals, while algorithms operate within externally imposed target functions;
  (2) living systems have tight integration between hardware and software (embodiment),
  while algorithms run on architectures that isolate software from hardware; (3) living
  systems experience a "large world" where problems are ill-defined, while algorithms
  exist in a "small world" where all problems are well-defined.'
---

# Artificial intelligence is algorithmic mimicry: why artificial "agents" are not (and won't be) proper agents

## Quick Facts
- arXiv ID: 2307.07515
- Source URL: https://arxiv.org/abs/2307.07515
- Reference count: 6
- Primary result: Current AI approaches cannot achieve true artificial general intelligence due to fundamental differences between living and algorithmic systems

## Executive Summary
This paper argues that artificial general intelligence (AGI) is not achievable within the current algorithmic framework of AI research. The author identifies three fundamental distinctions between living systems and algorithmic systems: living systems are autopoietic and can set intrinsic goals, while algorithms operate within externally imposed target functions; living systems have tight integration between hardware and software, while algorithms run on architectures that isolate software from hardware; and living systems experience a "large world" where problems are ill-defined, while algorithms exist in a "small world" where all problems are well-defined. The paper concludes that AGI is unlikely to emerge from current AI approaches, and discussions about AI should focus on the actual capabilities and risks of narrow AI rather than the improbable prospect of true agency in artificial systems.

## Method Summary
The paper systematically compares living and algorithmic systems, with a special focus on the notion of "agency." The author identifies three key distinctions: (1) Living systems are autopoietic and can set intrinsic goals, while algorithms operate within externally imposed target functions; (2) Living systems have tight integration between hardware and software (embodiment), while algorithms run on architectures that isolate software from hardware; (3) Living systems experience a "large world" where problems are ill-defined, while algorithms exist in a "small world" where all problems are well-defined. Through these distinctions, the paper argues that true AGI cannot be developed in the current algorithmic framework of AI research.

## Key Results
- Current AI systems are fundamentally limited by their algorithmic nature and cannot achieve true agency
- The three key distinctions (autopoiesis, embodiment, large vs small world) create insurmountable barriers to AGI in current frameworks
- True AGI would require major conceptual breakthroughs and radical innovations in materials and computational architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Algorithmic mimicry lacks natural agency due to absence of autopoietic self-manufacturing
- Mechanism: Living systems generate their own physical components through organizational closure, while algorithms depend on external hardware provision
- Core assumption: Autopoiesis requires simultaneous constraint generation and physical embodiment that cannot be simulated
- Evidence anchors:
  - [abstract] "Living systems are autopoietic, that is, self-manufacturing, and therefore able to set their own intrinsic goals, while algorithms exist in a computational environment with target functions that are both provided by an external agent."
  - [section 3] "Autopoiesis arises when each constraint within the system is not only generated by but also generates at least one other constraint"
  - [corpus] No direct corpus evidence available for autopoiesis distinction
- Break condition: Discovery of computational architecture enabling hardware self-generation from software goals

### Mechanism 2
- Claim: Algorithms cannot develop true embodiment due to strict hardware-software separation
- Mechanism: Digital computers maintain maximal separation between symbolic software and physical hardware, preventing the semantic closure found in living systems
- Core assumption: Semantic closure requires hardware components to be generated by software processes themselves
- Evidence anchors:
  - [abstract] "Living systems are embodied in the sense that there is no separation between their symbolic and physical aspects, while algorithms run on computational architectures that maximally isolate software from hardware."
  - [section 4] "A symbolic system that works in this integrated way is no longer formally equivalent to a Turing machine"
  - [corpus] No direct corpus evidence for embodiment distinction
- Break condition: Development of computational paradigms achieving hardware-software integration beyond current digital architecture

### Mechanism 3
- Claim: Algorithms exist in small worlds while organisms experience large worlds with ill-defined problems
- Mechanism: Algorithmic environments are closed, formalized, and well-defined, while biological environments present ambiguous, context-dependent problems requiring relevance realization
- Core assumption: Large world problem solving requires intrinsic goal-setting and frame flexibility unavailable to algorithms
- Evidence anchors:
  - [abstract] "Living systems experience a large world, in which most problems are ill-defined (and not all definable), while algorithms exist in a small world, in which all problems are well-defined."
  - [section 5] "In an algorithm's closed small world, there is only one frame of reference that includes its entire 'universe'"
  - [corpus] No direct corpus evidence for large/small world distinction
- Break condition: Algorithmic systems developing capability to handle truly ill-defined problems without external formalization

## Foundational Learning

- Concept: Autopoiesis and organizational closure
  - Why needed here: Understanding why self-manufacturing distinguishes living systems from algorithms
  - Quick check question: What makes a system autopoietic versus merely self-organizing?

- Concept: Semantic closure and embodiment
  - Why needed here: Explains why hardware-software separation prevents true agency in algorithms
  - Quick check question: How does the relationship between genome and protein synthesis illustrate semantic closure?

- Concept: Large versus small worlds
  - Why needed here: Distinguishes algorithmic problem spaces from biological problem spaces
  - Quick check question: Why can't algorithms handle relevance realization in the same way organisms do?

## Architecture Onboarding

- Component map:
  - Input processing: Data formatting and preprocessing layers
  - Core algorithm: Symbolic processing engine (current limitation)
  - Hardware interface: External effectors and sensors
  - Environment: Predefined computational architecture

- Critical path:
  - Data → Algorithm → Output
  - No direct hardware generation or self-modification capability

- Design tradeoffs:
  - Flexibility vs. efficiency in current architectures
  - Simulation capability vs. true embodiment
  - Well-defined problem solving vs. handling ill-defined problems

- Failure signatures:
  - Inability to generate novel goals without external input
  - Dependence on predefined computational architecture
  - Limited problem-solving to well-defined scenarios only

- First 3 experiments:
  1. Test algorithm's ability to handle ambiguous, context-dependent problems
  2. Measure hardware-software integration in neuromorphic systems
  3. Evaluate goal-setting capabilities in different computational architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What would be the minimal set of organizational properties required to create a truly autopoietic system in silico?
- Basis in paper: [explicit] The paper discusses that current AI architectures lack the organizational complexity required for autopoiesis and asks what an architecture enabling true natural agency would look like.
- Why unresolved: The paper states we don't know what such an architecture would look like and that achieving it would require major conceptual breakthroughs in algorithm design and radical innovations in materials and computational architecture.
- What evidence would resolve it: Development and demonstration of a computational system exhibiting self-manufacturing properties where each constraint within the system is generated by and generates at least one other constraint, showing organizational closure while remaining thermodynamically open.

### Open Question 2
- Question: Can semantic closure (the interconnectedness between symbol and matter) be achieved in any current computational architecture?
- Basis in paper: [explicit] The paper argues that living systems achieve semantic closure through the intimate intermingling of symbolic and physical aspects, while algorithms exist in a "physics-free" symbolic realm with maximal separation of hardware and software.
- Why unresolved: The paper states that overcoming this limitation would require a completely different computational paradigm that achieves maximum universality without strict separation of hardware and software, and that current efforts toward neuromorphic computing are still far from this goal.
- What evidence would resolve it: Demonstration of a computational system where the symbolic aspects are not merely simulated but are directly generated from and constitutive of the physical processes, creating a system where the hardware is the software is the hardware.

### Open Question 3
- Question: Is there any formal way to solve the frame problem that would enable algorithms to deal with relevance realization in a large world?
- Basis in paper: [explicit] The paper discusses that organisms live in a large world where most problems are ill-defined and relevance must be determined, while algorithms exist in a small world where all problems are well-defined and the frame problem simply does not exist.
- Why unresolved: The paper argues this is not a technological problem but a philosophical one, stating that as long as algorithms exist in a closed world, there will be no solution to the problem of relevance in the current framework of algorithmic mimicry.
- What evidence would resolve it: Development of an algorithm capable of autonomously determining what aspects of a problem are relevant without requiring predefined target functions or search spaces, demonstrating the ability to switch frames of reference and deal with ambiguous, ill-defined problems in a manner similar to living organisms.

## Limitations
- The autopoiesis distinction relies heavily on theoretical frameworks from Maturana and Varela that remain contested in contemporary biology
- The embodiment argument assumes current computational architectures represent the only possible framework for artificial systems
- The large versus small world distinction lacks precise operational definitions that would allow empirical testing

## Confidence

- Autopoiesis and intrinsic goal-setting: High confidence
- Embodiment and hardware-software integration: Medium-Low confidence
- Large world problem-solving: Medium-Low confidence

## Next Checks

1. Empirical test of algorithmic systems attempting to handle genuinely ill-defined problems without external formalization, measuring performance against biological analogs.

2. Analysis of emerging computational architectures (neuromorphic, quantum) to assess whether they fundamentally alter the hardware-software relationship posited in the paper.

3. Systematic review of cases where algorithms have demonstrated goal-setting behavior independent of external reward structures, testing the autopoiesis argument's boundaries.