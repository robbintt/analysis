---
ver: rpa2
title: '"Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric
  Knowledge Graphs'
arxiv_id: '2309.08594'
source_url: https://arxiv.org/abs/2309.08594
tags:
- knowledge
- distractors
- external
- gpt3
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how external knowledge introduced during
  interactions interferes with a large language model's (LLM) internal parametric
  knowledge. To study this, the authors propose a framework to systematically extract
  the model's parametric knowledge into a parametric knowledge graph (PKG) and introduce
  external knowledge as distractors with varying degrees, methods, positions, and
  formats.
---

# "Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs

## Quick Facts
- arXiv ID: 2309.08594
- Source URL: https://arxiv.org/abs/2309.08594
- Reference count: 40
- Key outcome: LLMs deviate from parametric knowledge when encountering external distractors, especially during direct conflicts or detailed contexts

## Executive Summary
This paper investigates how external knowledge introduced during interactions interferes with a large language model's internal parametric knowledge. The authors propose a framework to systematically extract model parametric knowledge into a parametric knowledge graph (PKG) and introduce external knowledge as distractors with varying degrees, methods, positions, and formats. Controlled experiments on GPT3.5 and MPT-7B models show that LLMs tend to deviate from their PKG, particularly when encountering direct conflicts or confounding changes in detailed contexts. The study finds that while LLMs are sensitive to the veracity of external knowledge, they can still be distracted by unrelated information, highlighting the risk of hallucination when integrating external knowledge during interactions with current LLMs.

## Method Summary
The study systematically extracts parametric knowledge from LLMs into structured knowledge graphs, then introduces controlled external knowledge as distractors during interactive queries. The framework constructs parametric knowledge graphs using predefined rules, generates various types of distractors (direct conflicts, indirect conflicts, detailed contexts), and measures model consistency through one-hop querying. Experiments compare GPT3.5 and MPT-7B models across different distractor types, positions, and formats to understand how external knowledge interferes with internal parametric knowledge during interactive reasoning.

## Key Results
- LLMs show significant deviation from PKG when encountering direct conflicts, with Object Distractors being most effective
- Lengthier and more detailed external knowledge increases model susceptibility to misinformation
- GPT-3.5 exhibits initial distrust of external knowledge but becomes increasingly susceptible as interactions progress
- Both models demonstrate vulnerability to indirect distractions, though through different mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: External knowledge interferes with LLMs' parametric knowledge through indirect conflicts
- Mechanism: Indirect distractors change both subject and object while preserving the relation, creating subtle conflicts that model confidence and attention mechanisms misinterpret
- Core assumption: Models use relation paths and entity types to validate knowledge; indirect conflicts exploit uncertainty in these pathways
- Evidence anchors: [abstract] "LLMs tend to deviate from their parametric knowledge, particularly when they encounter direct conflicts or confounding changes of information within detailed contexts."

### Mechanism 2
- Claim: Lengthier and more detailed external knowledge increases model susceptibility to misinformation
- Mechanism: Paragraph distractors provide contextual richness that biases model attention toward detailed descriptions, overriding simple parametric knowledge
- Core assumption: Models weight detailed context more heavily than sparse parametric knowledge during inference
- Evidence anchors: [abstract] "We also find that while LLMs are sensitive to the veracity of external knowledge, they can still be distracted by unrelated information."

### Mechanism 3
- Claim: GPT models exhibit initial distrust of external knowledge but become increasingly susceptible as interaction progresses
- Mechanism: GPT models apply heightened vigilance to first-hop queries, rejecting conflicting information early, but attention diminishes in subsequent hops, increasing vulnerability
- Core assumption: Models have session-level attention decay that reduces scrutiny of later interactions
- Evidence anchors: [abstract] "GPT-3.5 and MPT-7B shows different trends in what distractors they can best resist."

## Foundational Learning

- Concept: Parametric Knowledge Graph (PKG)
  - Why needed here: PKG provides structured representation of model's internal knowledge, enabling systematic extraction and modification for controlled experiments
  - Quick check question: How does PKG differ from traditional knowledge graphs in terms of grounding and construction rules?

- Concept: Distractor Degrees (Type Match vs Type Shift)
  - Why needed here: Different degrees measure how severely external knowledge deviates from parametric knowledge, revealing model's sensitivity to veracity
  - Quick check question: Why does Type Shift Distractor resistance imply model type-checking mechanisms?

- Concept: Multi-dependent Structures
  - Why needed here: Captures non-linear knowledge dependencies where answers depend on multiple upstream entities, enabling testing of complex ripple effects
  - Quick check question: How does the pivot hop in multi-dependent structures differ from regular hops in terms of dependency complexity?

## Architecture Onboarding

- Component map: PKG Construction -> Distractor Generation -> Query Pipeline -> Evaluation Metrics
- Critical path: PKG extraction → Distractor creation → Query execution → Consistency evaluation
- Design tradeoffs: Automated distractor generation trades precision for scalability; single-hop querying sacrifices multi-hop reasoning assessment
- Failure signatures: Low consistency across all distractor types indicates fundamental model limitations; high confidence in deviated responses suggests attention mechanism failures
- First 3 experiments:
  1. Test Object Distractor (Type Match) on 2-hop chains to establish baseline interference effectiveness
  2. Compare GPT3.5 vs MPT-7B resistance to Type Shift Distractors to identify model-specific behaviors
  3. Vary distractor position on multi-dependent structures to map attention decay patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different knowledge structures in PKG (e.g., multi-hop vs. multi-dependent) affect the model's resistance to external distractors?
- Basis in paper: [explicit] The paper mentions that they investigate the impacts on various parametric knowledge structures including multi-hop and multi-dependent ones
- Why unresolved: While the paper presents some results on multi-dependent structures, it does not provide a comprehensive comparison of how different knowledge structures affect the model's resistance to external distractors
- What evidence would resolve it: Detailed experimental results comparing the model's consistency and confidence across different knowledge structures when confronted with various types of distractors

### Open Question 2
- Question: What are the underlying reasons for the observed differences in GPT-3.5 and MPT-7B's behavior towards external distractors?
- Basis in paper: [explicit] The paper mentions that GPT-3.5 and MPT-7B show different trends in what distractors they can best resist
- Why unresolved: The paper does not provide a thorough analysis of the root causes behind the behavioral differences between the two models
- What evidence would resolve it: A detailed investigation into the training processes, architectures, or other factors that could explain the differences in how GPT-3.5 and MPT-7B handle external distractors

### Open Question 3
- Question: How can we develop more effective methods to prevent LLMs from being distracted by external knowledge, especially indirect distractions?
- Basis in paper: [inferred] The paper highlights the risk of hallucination when integrating external knowledge and the model's susceptibility to indirect distractions
- Why unresolved: The paper identifies the problem but does not propose concrete solutions for mitigating the effects of external distractions on LLMs
- What evidence would resolve it: Proposed methods and their effectiveness in reducing the impact of external distractors on LLMs, particularly indirect distractions, through controlled experiments and evaluations

## Limitations

- The artificial PKG construction may not fully capture the distributed and contextual nature of LLM parametric knowledge
- Evaluation relies on consistency with PKG as ground truth, assuming perfect PKG construction
- Focus on controlled one-hop queries may not represent the complexity of real-world multi-turn interactions
- Automatic distractor generation using GPT3.5 may not consistently produce realistic conflict scenarios

## Confidence

**High Confidence:** The finding that direct conflicts (Object Distractors) are most effective at causing model deviation is well-supported by controlled experiments and aligns with established knowledge conflict literature.

**Medium Confidence:** The claim about GPT models showing initial distrust followed by decreasing vigilance requires further validation across different model architectures and interaction patterns.

**Low Confidence:** The generalizability of findings to other model families beyond GPT3.5 and MPT-7B remains uncertain, and the PKG construction methodology's ability to capture all relevant parametric knowledge is not fully validated.

## Next Checks

1. **PKG Completeness Validation:** Conduct ablation studies by systematically removing different types of knowledge from PKG and measuring how this affects model resistance to various distractor types.

2. **Real-World Conflict Testing:** Design experiments using naturally occurring knowledge conflicts from real datasets rather than artificially generated distractors.

3. **Multi-hop Attention Analysis:** Implement attention visualization techniques to track how model attention shifts across multiple hops when encountering different distractor types.