---
ver: rpa2
title: Federated Learning for Heterogeneous Bandits with Unobserved Contexts
arxiv_id: '2303.17043'
source_url: https://arxiv.org/abs/2303.17043
tags:
- agents
- agent
- algorithm
- reward
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a federated linear contextual bandit problem
  where M agents collaborate to learn optimal actions without observing the exact
  context vectors, only their distributions. The agents communicate periodically with
  a central server to share local estimates and receive global estimates, enabling
  federated learning without sharing raw data.
---

# Federated Learning for Heterogeneous Bandits with Unobserved Contexts

## Quick Facts
- **arXiv ID:** 2303.17043
- **Source URL:** https://arxiv.org/abs/2303.17043
- **Reference count:** 39
- **Primary result:** Federated phased elimination algorithm achieves O(L/ℓ √dKM (log(K(logT)/δ) + min{d, logM}) √T) regret bound for linear contextual bandits with unobservable contexts.

## Executive Summary
This paper addresses federated linear contextual bandit problems where agents cannot observe exact context vectors but only their distributions. The authors propose a federated phased elimination algorithm (Fed-PECD) that enables collaborative learning while preserving privacy by sharing only parameter estimates rather than raw data. Through feature vector transformation, the algorithm handles unobservable contexts and achieves regret bounds matching those for observable contexts. The approach is validated on synthetic data and the MovieLens dataset, demonstrating improved regret with more agents and practical communication efficiency.

## Method Summary
The method uses feature vector transformation ψa,µi = Eci~µi[φa,ci] to handle unobservable contexts, enabling unbiased reward estimation. Agents operate in phases of length f p + K, exploring active arms according to frequencies computed via multi-agent G-optimal design. The central server aggregates local estimates θ p
a,i to compute global estimates θ p
a and matrices V p
a, then broadcasts frequency allocations f p
a,i back to agents. Suboptimal arms are eliminated based on confidence bounds. The algorithm maintains privacy by sharing only parameter estimates, with communication scaling as O(Md²K logT).

## Key Results
- Proved regret bound O(L/ℓ √dKM (log(K(logT)/δ) + min{d, logM}) √T) matching observable context setting
- Communication cost scales as O(Md²K logT)
- Numerical experiments show regret decreases as number of agents increases
- Performance gap between exact (observable context) and hidden (unobservable context) settings quantified
- Validated on both synthetic data and MovieLens dataset with non-negative matrix factorization preprocessing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature vector transformation enables unbiased reward estimation even when contexts are unobservable.
- Mechanism: Maps hidden contexts through known distributions to construct ψa,µi = Eci~µi[φa,ci], replacing unobserved φa,ci in linear reward model.
- Core assumption: Context distribution µi is known and stationary for each agent i.
- Evidence anchors: [abstract], [section IV-A], [corpus] (weak)
- Break condition: If µi changes over time or is mis-specified, ψa,µi estimates become biased.

### Mechanism 2
- Claim: Collaborative exploration via G-optimal design achieves near-optimal information sharing without exposing raw data.
- Mechanism: Server computes f p
a,i values using multi-agent G-optimal design that maximizes information gain while maintaining privacy.
- Core assumption: Reward parameters θa are shared across agents while contexts remain private.
- Evidence anchors: [section IV-A], [abstract], [corpus] (moderate)
- Break condition: If θa is heterogeneous across agents, collaborative benefit disappears.

### Mechanism 3
- Claim: Phased elimination structure ensures sublinear regret while maintaining communication efficiency.
- Mechanism: Operates in phases of increasing length f p + K, exploring active arms, updating estimates, and eliminating statistically inferior arms.
- Core assumption: Suboptimal gap is bounded away from zero.
- Evidence anchors: [section IV-A], [abstract], [section IV-B]
- Break condition: If suboptimal gap is too small or zero, elimination criterion becomes unreliable.

## Foundational Learning

- **Concept:** Sub-Gaussian random variables and concentration inequalities
  - Why needed here: Algorithm relies on Hoeffding's inequality and sub-Gaussian noise properties for confidence bounds
  - Quick check question: Why does 1-subgaussian noise ensure the sum of n independent samples is √n-subgaussian?

- **Concept:** Linear regression with regularization and matrix concentration
  - Why needed here: Algorithm maintains matrices V p
a aggregating information from multiple agents
  - Quick check question: What condition on design matrix ensures V p
a is invertible with bounded estimation error?

- **Concept:** G-optimal design and information theory
  - Why needed here: Communication protocol uses G-optimal design to allocate exploration frequencies
  - Quick check question: How does maximizing determinant of information matrix relate to minimizing estimation variance?

## Architecture Onboarding

- **Component map:** Agent module -> Local exploration and estimate computation -> Communication to server -> Server module -> Global aggregation and frequency allocation -> Communication back to agents -> Active set update -> Next phase

- **Critical path:** Agent exploration → Local estimate computation → Communication to server → Global aggregation → Frequency allocation → Communication back to agents → Active set update → Next phase

- **Design tradeoffs:**
  - Privacy vs. performance: Sharing only parameter estimates preserves privacy but may be less informative
  - Communication frequency vs. regret: More frequent communication could reduce regret but increases cost
  - Phase length growth rate: Faster growth reduces regret but may delay learning

- **Failure signatures:**
  - High variance in estimates across agents: May indicate heterogeneous reward parameters or insufficient exploration
  - Active arm sets not shrinking: Could suggest incorrect confidence bounds or very small suboptimal gaps
  - Communication bottlenecks: May require adjusting phase length or exploring more efficient protocols

- **First 3 experiments:**
  1. Single agent validation: Run Fed-PECD with M=1 to verify it recovers known single-agent contextual bandit algorithm performance
  2. Communication cost measurement: Measure actual communication volume vs. theoretical O(Md²K logT) bound
  3. Privacy leakage test: Attempt to reconstruct context distributions from exchanged parameter estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to handle the tradeoff between exploration and exploitation in federated linear contextual bandits with unobservable contexts?
- Basis in paper: [explicit] Authors mention identifying suitable exploration-exploitation tradeoff to utilize as few pulls as possible.
- Why unresolved: Paper proposes Fed-PECD but doesn't explicitly discuss optimal strategy for balancing exploration and exploitation.
- What evidence would resolve it: Experimental results comparing different exploration-exploitation strategies in terms of regret and communication cost.

### Open Question 2
- Question: How does Fed-PECD performance compare to other federated learning algorithms for contextual bandits with unobservable contexts?
- Basis in paper: [explicit] Paper mentions validation and comparison with baseline but lacks detailed comparison with other federated learning algorithms.
- Why unresolved: Paper focuses on proposing Fed-PECD without extensively comparing its performance to other federated learning algorithms.
- What evidence would resolve it: Comprehensive comparison of Fed-PECD with other federated learning algorithms in terms of regret, communication cost, and scalability.

### Open Question 3
- Question: What are the implications of using feature vector transformation for unobservable contexts on accuracy and robustness of learned models?
- Basis in paper: [explicit] Paper mentions using feature vector transformation and proving regret bounds but doesn't discuss implications on model accuracy and robustness.
- Why unresolved: Paper focuses on theoretical analysis and regret bounds without exploring practical implications of transformation on model accuracy and robustness.
- What evidence would resolve it: Experimental results evaluating accuracy and robustness of learned models using different feature vector transformation techniques.

## Limitations

- The feature vector transformation relies heavily on known and stationary context distributions, which may not hold in practice
- Specific implementation details for solving the multi-agent G-optimal design remain underspecified
- Experimental validation uses synthetic data with controlled parameters that may not capture real-world complexity

## Confidence

- **Medium confidence:** Mechanism for handling unobservable contexts through feature vector transformation is theoretically sound but relies on strong assumptions about context distributions
- **Medium confidence:** Collaborative exploration via G-optimal design is theoretically justified but practical implementation challenges remain
- **Low confidence:** Communication efficiency claims are derived but practical challenges in matrix stability and real-world network constraints are not fully addressed

## Next Checks

1. **Robustness to distribution drift:** Test Fed-PECD when context distributions µi change over time, measuring degradation in regret performance and identifying failure modes.

2. **Heterogeneous reward parameters:** Modify the algorithm to handle cases where θa differs across agents, comparing performance against the current shared-parameter assumption to quantify the cost of heterogeneity.

3. **Communication overhead measurement:** Implement the complete federated system with realistic network constraints to measure actual communication costs versus the theoretical O(Md²K logT) bound across different agent scales.