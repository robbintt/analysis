---
ver: rpa2
title: An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare
  Datasets
arxiv_id: '2311.03425'
source_url: https://arxiv.org/abs/2311.03425
tags:
- aequity
- bias
- patients
- black
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AEquity, a data-centric, model-agnostic approach
  to detect and mitigate bias in healthcare datasets by evaluating how easily different
  groups are learned at small sample sizes. The method quantifies bias through a novel
  metric calculated via a compressive autoencoder applied to either the raw dataset
  or the latent space of deep learning models.
---

# An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare Datasets

## Quick Facts
- arXiv ID: 2311.03425
- Source URL: https://arxiv.org/abs/2311.03425
- Reference count: 40
- Primary result: AEquity method detects and mitigates bias in healthcare datasets, improving classifier performance for Black patients by up to 96.5% in chest X-ray diagnosis

## Executive Summary
This paper introduces AEquity, a data-centric, model-agnostic approach to detect and mitigate bias in healthcare datasets by evaluating how easily different groups are learned at small sample sizes. The method quantifies bias through a novel metric calculated via a compressive autoencoder applied to either the raw dataset or the latent space of deep learning models. The authors demonstrate its application on two cases: healthcare utilization prediction using EHR data and chest X-ray diagnosis. For EHR data, AEquity revealed that using active chronic conditions as an outcome measure instead of cost-based metrics reduced racial bias in resource allocation predictions. In chest X-ray diagnosis, AEquity-guided data curation improved classifier performance for Black patients by up to 96.5% for certain diagnoses, particularly by addressing sampling and complexity biases. The approach was validated across multiple datasets and showed robustness to various fairness metrics.

## Method Summary
AEquity quantifies dataset bias by measuring how sample size affects model generalization differently across subgroups. The method estimates the minimum convergence sample size (MCSE) for each group-label combination using an autoencoder; larger MCSE implies harder generalization and higher bias. Different bias types (sampling, complexity, label) manifest as distinct AEquity patterns when groups are combined. Sampling bias → joint AEq ≤ over-sampled group; Complexity bias → joint AEq > individual group AEqs; Label bias → AEq differs for same label across groups. Guided data collection based on AEquity reduces measured bias across fairness metrics by identifying whether to diversify (sampling bias) or prioritize the under-generalizing group (complexity bias).

## Key Results
- AEquity-guided data curation improved classifier performance for Black patients by up to 96.5% for certain chest X-ray diagnoses
- Bias for each diagnostic finding in the chest radiograph dataset decreased by between 29% and 96.5% following intervention
- Using active chronic conditions as outcome measure instead of cost-based metrics reduced racial bias in EHR resource allocation predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AEquity quantifies dataset bias by measuring how sample size affects model generalization differently across subgroups.
- Mechanism: The metric estimates the minimum convergence sample size (MCSE) for each group-label combination using an autoencoder; larger MCSE implies harder generalization and higher bias.
- Core assumption: The learning difficulty captured by autoencoder loss correlates with real model generalization performance.
- Evidence anchors: "AEquity works by appending a simple compressive network to a dataset or the latent space of existing deep learning models to generate a single value characterizing a label with respect to a group" and "A smaller n tends to indicate a better generalization performance, whereas a larger n indicates poor generalization performance"
- Break condition: If autoencoder loss does not reflect classifier performance (e.g., highly regularized latent space), the MCSE estimate becomes invalid.

### Mechanism 2
- Claim: Different bias types (sampling, complexity, label) manifest as distinct AEquity patterns when groups are combined.
- Mechanism: Sampling bias → joint AEq ≤ over-sampled group; Complexity bias → joint AEq > individual group AEqs; Label bias → AEq differs for same label across groups.
- Core assumption: Group balancing alone fixes sampling bias; complexity bias requires more data from the complex group; label bias needs better labels.
- Evidence anchors: "When predictive bias is driven primarily by sampling bias in the dataset, combining groups drives the AEq value at or below the over-sampled data" and "In complexity bias, prioritizing data collection of the protected population, due to its relative heterogeneity compared to the over-represented group, is necessary to mitigate bias"
- Break condition: If bias is driven by factors not captured by MCSE (e.g., feature distribution shift unrelated to sample size), the pattern mapping breaks.

### Mechanism 3
- Claim: Guided data collection based on AEquity reduces measured bias across fairness metrics.
- Mechanism: AEquity identifies whether to diversify (sampling bias) or prioritize the under-generalizing group (complexity bias); interventions shrink performance gaps between groups.
- Core assumption: The sample sizes suggested by AEquity are sufficient to equalize generalization.
- Evidence anchors: "AEquity-guided data curation improved classifier performance for Black patients by up to 96.5% for certain diagnoses" and "Bias for each diagnostic finding in the chest radiograph dataset decreased by between 29% and 96.5% following intervention"
- Break condition: If the underlying data distribution cannot be changed (e.g., no more samples available) or if bias is algorithmic rather than data-driven, intervention efficacy drops.

## Foundational Learning

- Information bottleneck principle:
  - Why needed here: AEquity relies on the idea that autoencoder compression loss reflects the information needed to generalize.
  - Quick check question: What is the relationship between I(X;Y) and I(X;X) when Y semantically encodes X?

- Minimum convergence sample estimation:
  - Why needed here: MCSE is the core numeric output of AEquity.
  - Quick check question: How is MCSE approximated using autoencoder loss instead of actual classifier validation?

- Bias taxonomy (sampling vs complexity vs label):
  - Why needed here: AEquity patterns are interpreted through these categories to decide interventions.
  - Quick check question: What AEquity pattern indicates complexity bias versus sampling bias?

## Architecture Onboarding

- Component map: Data loader → encoder/decoder network → MCSE calculator → bias pattern detector → intervention selector
- Critical path: Balanced subset → compute per-group AEq → identify bias type → apply data collection strategy → retrain → evaluate fairness metrics
- Design tradeoffs: Small subset (3%) is fast but may miss rare subgroups; full latent space is more accurate but slower. Choice of autoencoder architecture (fully connected vs CNN) depends on data modality.
- Failure signatures: AEq values flat across groups → likely sampling bias already mitigated or metric invalid. High joint AEq but similar individual AEqs → possible label bias or insufficient sample size.
- First 3 experiments: 1) Run AEquity on MIMIC-CXR subset to confirm pneumothorax shows higher AEq for Black patients. 2) Apply diversification intervention and measure change in AUROC gap. 3) Apply population prioritization for edema and compare bias reduction to diversification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How generalizable is AEquity across different types of protected groups beyond race, sex, and age, such as disability status or sexual orientation?
- Basis in paper: [explicit] The authors acknowledge that their study primarily focused on race, sex, and age, and note that "validation on a broader scale across all phenotypes, protected groups, data modalities and model types is essential to further validating our approach."
- Why unresolved: The current study only examined a limited set of protected characteristics. Healthcare datasets often contain diverse populations with varying social determinants of health that could manifest as bias in different ways.
- What evidence would resolve it: Systematic application of AEquity to datasets containing disability status, sexual orientation, socioeconomic indicators, and intersectional groups across multiple healthcare domains would demonstrate its generalizability.

### Open Question 2
- Question: Can AEquity effectively detect and mitigate bias in non-classification tasks such as regression or generation models?
- Basis in paper: [explicit] The authors state that "AEq in its current form focuses primarily on classification tasks with an unregularized latent space" and suggest extending it to "generative models may help investigate and mitigate bias in those settings."
- Why unresolved: The methodology is specifically designed for classification with compressive autoencoders. Many important healthcare applications involve continuous predictions or synthetic data generation.
- What evidence would resolve it: Development and validation of AEquity variants for regression tasks (e.g., predicting hospital stay duration) and generative models (e.g., synthetic patient data generation) would establish its applicability beyond classification.

### Open Question 3
- Question: What is the optimal sample size threshold for applying AEquity to detect bias reliably without requiring excessive computational resources?
- Basis in paper: [inferred] The authors use approximately 3% of available data (1,024 samples) for initial AEquity calculations and later scale to 30,000 samples for interventions, but note computational constraints limited bootstrapping to 50 iterations.
- Why unresolved: The trade-off between statistical power, computational efficiency, and practical implementation needs is not quantified. Healthcare organizations may have limited resources for bias assessment.
- What evidence would resolve it: Systematic analysis of AEquity performance (sensitivity, specificity for bias detection) across varying sample sizes and bootstrapping iterations would establish minimum requirements for reliable bias detection.

## Limitations
- AEquity's reliance on autoencoder loss as a proxy for generalization performance may not hold for highly regularized latent spaces
- The 3% balanced subset approach may miss rare subgroups or fail to capture complex interactions in highly imbalanced datasets
- The method assumes bias is primarily data-driven rather than algorithmic, which may not hold for all model architectures

## Confidence
- High confidence in the core mechanism of using sample size estimation to quantify bias
- Medium confidence in the pattern-based classification of bias types and their interventions
- Medium confidence in the reproducibility of bias reduction results, given dependencies on preprocessing choices and hyperparameter tuning

## Next Checks
1. Verify AEquity patterns across additional clinical datasets with known demographic disparities to test generalizability of the sampling/complexity/label bias taxonomy
2. Compare AEquity-guided interventions against baseline approaches (random sampling, cost-sensitive learning) on the same datasets to quantify relative effectiveness
3. Test sensitivity of AEquity values to different autoencoder architectures and latent space dimensions to establish robustness boundaries