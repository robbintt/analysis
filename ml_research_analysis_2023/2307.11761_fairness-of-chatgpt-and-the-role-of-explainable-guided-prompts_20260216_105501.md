---
ver: rpa2
title: Fairness of ChatGPT and the Role Of Explainable-Guided Prompts
arxiv_id: '2307.11761'
source_url: https://arxiv.org/abs/2307.11761
tags:
- knowledge
- domain
- fairness
- credit
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the potential of OpenAI\u2019s GPT models\
  \ for credit risk assessment, demonstrating that with carefully designed prompts\
  \ and integration of domain knowledge, these models can achieve comparable performance\
  \ to traditional ML models using 40 times less training data (20 vs 800 samples).\
  \ The GPT models show strong precision, recall, and F1 scores while minimizing false\
  \ positives and maintaining fairness across genders."
---

# Fairness of ChatGPT and the Role Of Explainable-Guided Prompts

## Quick Facts
- arXiv ID: 2307.11761
- Source URL: https://arxiv.org/abs/2307.11761
- Reference count: 16
- One-line primary result: GPT models achieve comparable performance to traditional ML models using 40x less training data while maintaining fairness across genders

## Executive Summary
This study evaluates OpenAI's GPT models for credit risk assessment, demonstrating that carefully designed prompts with domain knowledge integration can achieve performance comparable to traditional ML models using significantly less training data (20 vs 800 samples). The research focuses on binary classification tasks using the German Credit dataset, with fairness as a central concern. The GPT models show strong precision, recall, and F1 scores while minimizing false positives and maintaining fairness across gender groups. The results indicate that GPT models, when properly guided through prompt engineering, offer a viable alternative to classical ML approaches for financial risk assessment tasks.

## Method Summary
The study compares OpenAI's GPT models against six classical ML models (RF, LR, MLP, KNN, XGB, AdaBoost) for credit risk assessment using the German Credit dataset with 1,000 individuals and 21 attributes. The GPT models use only 20 training examples while ML models use 800 samples. Prompt engineering is employed with five key components: task instruction, in-context examples, attribute description, domain knowledge integration, and a question. Ten different prompt variants are tested, incorporating domain knowledge identified through ML feature importance analysis. Gender fairness is evaluated using bootstrap sampling with 1000 resamples to compare true positive rates across genders.

## Key Results
- GPT models achieve comparable performance to traditional ML models with 40x less training data (20 vs 800 samples)
- Strong precision, recall, and F1 scores while minimizing false positives in credit risk assessment
- GPT models maintain fairness across genders with non-significant differences in true positive rates
- Certain prompts achieve fair outcomes that contrast with machine learning models' performance disparities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can achieve comparable performance to traditional ML models using significantly less training data (40x reduction).
- Mechanism: Pre-trained LLMs leverage vast general knowledge from their training corpus, enabling them to generalize from very few examples when guided by carefully crafted prompts.
- Core assumption: The pre-training corpus contains sufficient relevant patterns for the target task (credit risk assessment).
- Evidence anchors:
  - [abstract]: "LLMs can parallel the performance of traditional Machine Learning (ML) models. Intriguingly, they achieve this with significantly less data—40 times less, utilizing merely 20 data points compared to the ML's 800."
  - [section 2.1]: "This technique provides task-oriented instruction for the OpenAI model, delivering the necessary context."

### Mechanism 2
- Claim: Prompt engineering can effectively incorporate domain knowledge to improve LLM performance.
- Mechanism: Structured prompts with domain knowledge integration guide the LLM's reasoning process, effectively simulating expert knowledge without requiring additional training.
- Core assumption: LLMs can interpret and apply structured domain knowledge when presented in natural language format.
- Evidence anchors:
  - [abstract]: "LLMs, when directed by judiciously designed prompts and supplemented with domain-specific knowledge, can parallel the performance of traditional Machine Learning (ML) models."
  - [section 2.1]: "We introduced three categories of domain knowledge... 'dk0' (prompt-0) represents a base case with no domain knowledge... 'Odd dk' (prompt-1, prompt-3, prompt-5, prompt-7, prompt-9) or Machine Learning Feature Importance (MLFI) refers to the scenario where important features are identified by ML algorithms..."

### Mechanism 3
- Claim: LLMs can achieve better fairness outcomes compared to traditional ML models for the same task.
- Mechanism: The inherent bias patterns in LLMs may differ from traditional ML models, and prompt engineering can be used to mitigate specific fairness concerns.
- Core assumption: The pre-training process and prompt design can produce different bias patterns than traditional feature-based ML models.
- Evidence anchors:
  - [abstract]: "LLMs particularly excel in minimizing false positives and enhancing fairness, both being vital aspects of risk analysis."
  - [section 4]: "In contrast to machine learning models, certain prompts could achieve fair outcomes, i.e., a non-significant difference in the efforts required by different genders."

## Foundational Learning

- Concept: Bootstrap sampling and statistical significance testing
  - Why needed here: To assess fairness by comparing group-level performance metrics (like TPR) while accounting for sampling variability
  - Quick check question: What statistical test would you use to determine if two bootstrap distributions of TPR are significantly different?

- Concept: Prompt engineering and in-context learning
  - Why needed here: To effectively guide the LLM without fine-tuning, using carefully structured instructions and examples
  - Quick check question: What are the five parts of the prompt structure used in this study?

- Concept: Feature importance and domain knowledge integration
  - Why needed here: To enhance LLM performance by providing structured knowledge about which features matter and their relative importance
  - Quick check question: How does MLFI-ord differ from MLFI in terms of the domain knowledge provided?

## Architecture Onboarding

- Component map: Input data → Prompt construction (task instruction, examples, attribute description, domain knowledge, question) → OpenAI API call → Response parsing → Evaluation metrics
- Critical path: Data preparation → Prompt engineering → API interaction → Result validation
- Design tradeoffs: Data efficiency vs. control (prompts vs. fine-tuning), interpretability vs. performance (LLMs vs. traditional ML)
- Failure signatures: Poor performance despite good prompts (domain mismatch), unexpected bias patterns, API rate limiting or cost issues
- First 3 experiments:
  1. Test baseline LLM performance with minimal prompting (dk0)
  2. Compare MLFI vs MLFI-ord prompt variants
  3. Evaluate fairness metrics across different prompt types using bootstrap sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OpenAI models scale with larger training datasets compared to traditional ML models?
- Basis in paper: [explicit] The paper mentions that OpenAI models use 40 times less data (20 samples) compared to traditional ML models (800 samples), but doesn't explore performance scaling with increased data.
- Why unresolved: The study deliberately used minimal data to demonstrate efficiency, leaving open questions about optimal dataset sizes and how performance changes with more training examples.
- What evidence would resolve it: Comparative experiments varying training data sizes (e.g., 20, 100, 200, 800 samples) for both OpenAI and traditional ML models, measuring accuracy, fairness, and resource efficiency trade-offs.

### Open Question 2
- Question: Which specific prompt engineering techniques most effectively improve fairness outcomes across different demographic groups?
- Basis in paper: [explicit] The paper demonstrates that certain prompts achieve fair outcomes with non-significant gender differences, but doesn't identify which prompt components are most influential.
- Why unresolved: While the study shows prompts can improve fairness, it doesn't systematically analyze which prompt elements (task instruction, in-context examples, attribute description, domain knowledge integration) have the greatest impact on fairness metrics.
- What evidence would resolve it: Ablation studies testing individual prompt components and their combinations, measuring fairness metrics (TPR disparities) across multiple demographic attributes beyond gender.

### Open Question 3
- Question: How transferable are the prompt engineering techniques across different ML tasks and domains?
- Basis in paper: [explicit] The authors state "insights and methodologies could potentially extend to other LLM types" but don't empirically test this transferability beyond credit risk assessment.
- Why unresolved: The study focuses solely on binary classification for credit risk assessment, leaving unknown whether the prompt engineering framework generalizes to regression tasks, multi-class classification, or completely different domains like healthcare or education.
- What evidence would resolve it: Systematic application of the prompt engineering framework to diverse ML tasks (regression, multi-class classification) and domains, comparing performance and fairness outcomes relative to task-specific baselines.

## Limitations

- The study relies on a single dataset (German Credit) which may not generalize to other credit risk assessment contexts
- The fairness analysis focuses solely on gender as a protected attribute, potentially missing other important fairness dimensions
- The 40x data efficiency claim requires validation across different domains and tasks

## Confidence

- High confidence: The basic premise that LLMs can perform credit risk classification with prompt engineering is well-supported by the methodology and results
- Medium confidence: The 40x data efficiency claim requires replication across multiple datasets and tasks to be considered robust
- Low confidence: The fairness conclusions are limited by the single protected attribute analysis and lack of comparison to established fairness benchmarks in ML literature

## Next Checks

1. **Dataset Generalization Test**: Replicate the experiment using at least two additional credit risk or financial classification datasets to validate the 40x data efficiency claim across domains.

2. **Multi-attribute Fairness Analysis**: Extend the fairness evaluation to include race, age, and income as protected attributes, using established fairness metrics (demographic parity, equalized odds) for comparison with traditional ML approaches.

3. **Cost-Benefit Analysis**: Calculate total operational costs including API calls, prompt engineering time, and latency compared to traditional ML model training and deployment costs across a representative sample of credit decisions.