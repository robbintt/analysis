---
ver: rpa2
title: '$\rm SP^3$: Enhancing Structured Pruning via PCA Projection'
arxiv_id: '2308.16475'
source_url: https://arxiv.org/abs/2308.16475
tags:
- matrix
- tcsp
- layer
- compression
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compressing transformer-based
  language models by focusing on reducing the hidden dimension (d), which is often
  overlooked in existing structured pruning methods. The proposed method, Structured
  Pruning with PCA Projection (SP3), projects the model's features into a PCA-defined
  subspace before masking, enabling significant reduction in d.
---

# $\rm SP^3$: Enhancing Structured Pruning via PCA Projection

## Quick Facts
- **arXiv ID:** 2308.16475
- **Source URL:** https://arxiv.org/abs/2308.16475
- **Reference count:** 40
- **Key outcome:** SP3 reduces hidden dimension by 70%, compresses 94% of BERTbase, maintains >96% accuracy, and outperforms other methods in accuracy at same compression ratio.

## Executive Summary
SP3 introduces a novel structured pruning method that leverages PCA projection to reduce the hidden dimension of transformer models. By projecting features into a PCA-defined subspace before masking, SP3 achieves significant compression while preserving model accuracy. The method is compatible with other pruning techniques and effective across various models including BERT, OPT, and Llama.

## Method Summary
SP3 compresses transformer models by first generating a projection matrix via PCA on sampled feature activations. This projection matrix is then fused with the model's weight matrices to reduce their effective dimensionality. The compressed model is fine-tuned on the full dataset to restore accuracy. The method targets the hidden dimension while leaving other dimensions (like attention head size and filter count) open for complementary pruning techniques.

## Key Results
- Achieves 70% reduction in hidden dimension (d)
- Compresses 94% of BERTbase parameters while maintaining >96% accuracy
- Outperforms existing methods in accuracy at the same compression ratio

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA projection compresses the hidden dimension by exploiting the low-rank structure of transformer features.
- Mechanism: SVD decomposes feature matrix X into singular vectors, yielding projection matrix P that captures principal directions. Fusing P with weight matrices reduces dimensionality from d to k (where k << d).
- Core assumption: Feature matrices from transformer layers predominantly lie in a low-dimensional subspace.
- Evidence anchors: Abstract mentions PCA-defined subspace projection; section states features tend to reside in low-dimensional subspace.
- Break condition: If X is full rank, compression yields minimal benefit or hurts accuracy.

### Mechanism 2
- Claim: Method preserves accuracy by reconstructing forward computation in reduced subspace.
- Mechanism: Projects inputs to k-dim space, applies compressed weights, projects outputs back. Normalization reconstruction ensures pre/post-projection compatibility.
- Core assumption: Reduced-dimension computation approximates original well enough for task performance.
- Evidence anchors: Section describes adding dimensionality reduction/enhancement operations while preserving outcomes; matrix fusion with normalization.
- Break condition: If reduced subspace loses critical task information, performance degrades sharply.

### Mechanism 3
- Claim: Method is compatible with other structured pruning techniques.
- Mechanism: TCSP compresses only hidden dimension d, leaving attention head size dh and filter count df for other pruning methods without interference.
- Core assumption: Different compression dimensions (d, dh, df) are orthogonal in their effects.
- Evidence anchors: Section states TCSP leaves df and dh open for prior pruning methods; compatibility highlighted.
- Break condition: If dh or df compression interacts non-additively with PCA compression, cumulative degradation occurs.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and Principal Component Analysis (PCA)
  - Why needed here: PCA via SVD identifies principal subspace of feature activations, enabling dimensionality reduction while preserving most variance.
  - Quick check question: Given feature matrix X, what do top-k columns of U from SVD(X) represent in terms of PCA?

- Concept: Structured pruning vs unstructured pruning
  - Why needed here: Structured pruning removes entire weight blocks enabling hardware acceleration; TCSP is structured pruning targeting hidden dimensions.
  - Quick check question: How does structured pruning differ from unstructured pruning in terms of acceleration benefits and hardware compatibility?

- Concept: Low-rank matrix factorization
  - Why needed here: TCSP uses low-rank factorization of feature matrix to derive projection matrix; understanding this underpins compression rationale.
  - Quick check question: Why is decomposing X into UΣV^T useful for finding low-dimensional projection of transformer features?

## Architecture Onboarding

- Component map: Input sampling -> Feature extraction -> SVD computation -> Projection matrix generation -> Weight fusion -> Fine-tuning
- Critical path:
  1. Data sampling and feature extraction
  2. SVD computation to derive P
  3. Matrix fusion across all layers
  4. Fine-tuning on entire dataset
- Design tradeoffs:
  - SVD cost vs compression benefit: Large feature matrices make SVD expensive; sampling reduces cost but may affect projection quality
  - Subspace rank k vs accuracy: Lower k increases compression but risks accuracy loss
  - Orthogonal compression vs joint compression: TCSP leaves other dimensions open for complementary methods
- Failure signatures:
  - Large accuracy drop after compression → projection matrix inadequate or fine-tuning insufficient
  - SVD runtime/memory error → feature matrix too large; need more aggressive sampling
  - Model fails to load → dimension mismatch after matrix fusion
- First 3 experiments:
  1. Run SVD on small synthetic feature matrix to verify top-k eigenvector extraction
  2. Apply TCSP with k = 0.5*d on small fine-tuned BERT and evaluate task accuracy
  3. Combine TCSP with filter pruning (TFP) and test cumulative compression vs accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of normalization function (LayerNorm, RMSNorm, BatchNorm) impact TCSP performance?
- Basis in paper: [explicit] Paper mentions TCSP applies to models with different normalization functions and provides handling details in appendix
- Why unresolved: Paper lacks experimental results comparing TCSP across normalization functions
- What evidence would resolve it: Experiments comparing TCSP on models with LayerNorm, RMSNorm, and BatchNorm

### Open Question 2
- Question: How does number of sampled tokens affect projection matrix quality and TCSP performance?
- Basis in paper: [explicit] Paper mentions sampling 2000 tokens and evaluating sampled token effects
- Why unresolved: Paper lacks comprehensive analysis of relationship between sampled tokens and projection quality/performance
- What evidence would resolve it: Detailed study varying sampled tokens and analyzing resulting projection quality and model performance

### Open Question 3
- Question: How does TCSP perform on larger models like GPT-3 or T5-11B compared to smaller models?
- Basis in paper: [inferred] Paper demonstrates TCSP on BERT-base, T5-base, and Llama but not larger models
- Why unresolved: Paper lacks experimental results on larger models which are increasingly common
- What evidence would resolve it: Experiments applying TCSP to larger models like GPT-3 or T5-11B and comparing performance to smaller models

### Open Question 4
- Question: How does combining TCSP with other compression methods like quantization or knowledge distillation affect overall compression ratio and performance?
- Basis in paper: [explicit] Paper mentions TCSP can combine with other compression methods but doesn't explore combinations
- Why unresolved: Paper lacks experimental results on combining TCSP with other compression methods
- What evidence would resolve it: Experiments combining TCSP with quantization or knowledge distillation and analyzing resulting compression ratio and performance

## Limitations

- The paper's effectiveness claims rely heavily on the assumption that transformer feature matrices lie predominantly in low-dimensional subspaces, which lacks strong corpus support
- The paper doesn't provide detailed information on the sampling method for generating the feature matrix, which could impact projection quality
- Experimental validation is limited to specific models and benchmarks without comprehensive testing across diverse architectures

## Confidence

- **High Confidence**: SP3 can reduce hidden dimension by 70% and compress 94% of BERTbase while maintaining >96% accuracy, supported by GLUE and SQuAD experiments
- **Medium Confidence**: SP3 compatibility with other structured pruning techniques and effectiveness on models like OPT and Llama is based on paper's experiments but lacks direct corpus support
- **Low Confidence**: The assumption that transformer feature matrices predominantly lie in low-dimensional subspaces is a key premise but poorly supported by corpus

## Next Checks

1. Evaluate impact of different sampling methods on projection matrix quality and TCSP effectiveness
2. Test SP3 on additional models and tasks to validate compatibility claims and broader effectiveness
3. Investigate the low-rank assumption through empirical studies testing whether transformer feature matrices actually lie in low-dimensional subspaces