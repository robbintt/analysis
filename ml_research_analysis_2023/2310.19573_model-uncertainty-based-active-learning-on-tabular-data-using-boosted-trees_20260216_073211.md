---
ver: rpa2
title: Model Uncertainty based Active Learning on Tabular Data using Boosted Trees
arxiv_id: '2310.19573'
source_url: https://arxiv.org/abs/2310.19573
tags:
- uncertainty
- sampling
- learning
- data
- ceal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses active learning (AL) for tabular data using
  boosted trees, specifically focusing on model uncertainty sampling as a replacement
  for traditional entropy-based methods. The authors propose using virtual ensembles
  and staged predictions to compute model uncertainty for both classification and
  regression tasks.
---

# Model Uncertainty based Active Learning on Tabular Data using Boosted Trees

## Quick Facts
- arXiv ID: 2310.19573
- Source URL: https://arxiv.org/abs/2310.19573
- Reference count: 40
- Key outcome: Model uncertainty sampling using virtual ensembles and staged predictions outperforms or matches entropy-based methods for active learning on tabular data, with CEAL variants reducing label acquisition costs.

## Executive Summary
This paper addresses active learning (AL) for tabular data using boosted trees, specifically focusing on model uncertainty sampling as a replacement for traditional entropy-based methods. The authors propose using virtual ensembles and staged predictions to compute model uncertainty for both classification and regression tasks. They also introduce novel cost-effective AL (CEAL) frameworks that combine high-confidence pseudo-labels with model uncertainty filtering. Experiments on three classification and three regression datasets show that model uncertainty sampling can outperform random sampling and matches or exceeds entropy-based methods. For regression, IBUG-based uncertainty performs best, while virtual ensemble uncertainty excels in classification. The proposed CEAL variants improve performance by reducing label acquisition costs while maintaining or improving accuracy.

## Method Summary
The paper proposes using model uncertainty estimation methods (virtual ensembles, staged predictions, and IBUG) as alternatives to entropy-based uncertainty sampling for active learning on tabular data. The approach leverages CatBoost's existing tree ensemble structure to compute uncertainty without requiring additional Bayesian machinery. For classification, staged predictions and virtual ensembles are used, while for regression, IBUG is also incorporated. The authors also introduce CEAL frameworks that combine high-confidence pseudo-labeling with model uncertainty filtering to reduce oracle queries while maintaining performance.

## Key Results
- Model uncertainty sampling outperforms random sampling and matches or exceeds entropy-based methods for both classification and regression tasks
- IBUG-based uncertainty performs best for regression tasks, while virtual ensemble uncertainty excels in classification
- CEAL variants improve performance by reducing label acquisition costs while maintaining or improving accuracy
- Staged predictions offer the fastest computation time while virtual ensembles provide better accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model uncertainty computed via virtual ensembles and staged predictions can effectively replace entropy-based uncertainty sampling for active learning on tabular data.
- Mechanism: Boosted trees naturally produce ensembles of dependent trees. By leveraging these existing trees, the method approximates epistemic uncertainty without requiring additional Bayesian machinery or full retraining. Virtual ensembles simulate multiple model draws by truncating sub-models, while staged predictions use individual tree outputs to estimate variance.
- Core assumption: The ranking of data points by uncertainty (not the absolute uncertainty value) is what matters for active learning, so approximate uncertainty measures suffice.
- Evidence anchors:
  - [abstract] states that "model uncertainty sampling is a promising alternative to traditional uncertainty measures for active learning with tabular data."
  - [section] describes virtual ensembles as "a drop-in replacement of entropy" and shows they "perform better than entropy in most cases."
  - [corpus] provides limited support; no direct citations are given, so evidence is weak here.
- Break condition: If the ranking of uncertainties no longer correlates with the true information gain, or if the boosted tree structure changes such that tree dependencies break the approximation.

### Mechanism 2
- Claim: Cost-effective active learning (CEAL) improves label efficiency by incorporating high-confidence pseudo-labels while filtering out uncertain ones.
- Mechanism: High-confidence instances are identified using low entropy or low model uncertainty thresholds. These instances are pseudo-labeled and added to the training set without querying the oracle. Filtering with model uncertainty reduces the risk of noisy pseudo-labels caused by overconfident but incorrect predictions.
- Core assumption: The model's uncertainty estimates are reliable enough to distinguish truly high-confidence instances from falsely confident ones.
- Evidence anchors:
  - [section] proposes a "two-stage filtering criteria" using both entropy and model uncertainty for classification, and notes this improves upon traditional CEAL.
  - [abstract] confirms that "the proposed CEAL variants improve performance by reducing label acquisition costs while maintaining or improving accuracy."
  - [corpus] lacks supporting citations; this remains an assumption-based improvement.
- Break condition: If the model is overconfident or poorly calibrated, pseudo-labels may introduce noise, degrading performance.

### Mechanism 3
- Claim: Combining multiple model uncertainty estimation methods (virtual ensembles, staged predictions, IBUG) allows robust uncertainty estimates across different regression and classification tasks.
- Mechanism: Different uncertainty estimation techniques capture different aspects of model behavior. Virtual ensembles measure total uncertainty, staged predictions approximate variance across trees, and IBUG uses nearest-neighbor distributions. Using multiple methods allows selecting the best-performing one per dataset or task.
- Core assumption: No single uncertainty method dominates across all datasets, so flexibility improves robustness.
- Evidence anchors:
  - [section] reports that "IBUG tends to converge to the least MSE score in the final AL iterations" for regression, while "virtual ensembles has the least time complexity."
  - [abstract] shows that "for regression, IBUG-based uncertainty performs best, while virtual ensemble uncertainty excels in classification."
  - [corpus] provides no external citations; this is inferred from experimental results.
- Break condition: If computational cost of multiple methods outweighs benefits, or if differences in uncertainty estimates are negligible across datasets.

## Foundational Learning

- Concept: Epistemic vs. aleatoric uncertainty
  - Why needed here: The method focuses on epistemic uncertainty (reducible uncertainty due to lack of knowledge) as more relevant for active learning, per recent literature.
  - Quick check question: What is the difference between epistemic and aleatoric uncertainty, and why is epistemic preferred for active learning?

- Concept: Gradient boosting and CatBoost specifics
  - Why needed here: The work leverages CatBoost's sequential tree structure and native categorical feature support; understanding how it builds symmetric trees and uses ordered boosting is key to applying the uncertainty methods.
  - Quick check question: How does CatBoost's ordered boosting prevent target leakage, and why is this relevant for uncertainty estimation?

- Concept: Active learning query strategies
  - Why needed here: The paper replaces entropy-based sampling with model uncertainty sampling, so understanding traditional AL strategies is necessary to appreciate the novelty.
  - Quick check question: What is the difference between uncertainty sampling and diversity sampling in active learning?

## Architecture Onboarding

- Component map:
  Base model (CatBoost) -> Uncertainty estimators (Virtual ensembles, Staged predictions, IBUG) -> Active learning loop -> CEAL modules

- Critical path:
  1. Train initial CatBoost model on small labeled set
  2. Compute uncertainty scores for unlabeled instances
  3. Select top-M uncertain instances for labeling
  4. If CEAL enabled, filter high-confidence instances for pseudo-labeling
  5. Add new labeled (or pseudo-labeled) instances
  6. Retrain and repeat until budget exhausted

- Design tradeoffs:
  - Virtual ensembles: Good accuracy, moderate time complexity, CatBoost-specific
  - Staged predictions: Fast, easy to implement, CatBoost-specific
  - IBUG: Applicable to any GBDT, higher time complexity, better regression performance
  - CEAL: Reduces oracle queries but risks noise if thresholds are poorly set

- Failure signatures:
  - CEAL degrades performance: Check pseudo-label quality and threshold calibration
  - Uncertainty sampling underperforms random: Verify uncertainty estimates are well-calibrated and ranking is meaningful
  - Slow iteration: Profile uncertainty computation; consider staged predictions for speed

- First 3 experiments:
  1. Compare staged predictions vs. entropy on a binary classification dataset (e.g., Adult)
  2. Test IBUG vs. virtual ensembles on a regression dataset (e.g., California Housing)
  3. Evaluate CEAL with entropy vs. hybrid CEAL on a multiclass dataset (e.g., Thyroid)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of model uncertainty sampling compare to entropy-based sampling across different dataset sizes and domains?
- Basis in paper: [explicit] The authors note that model uncertainty sampling can outperform random sampling and match or exceed entropy-based methods, but results vary across datasets (e.g., Amazon dataset shows random sampling outperforming active methods in low data regimes)
- Why unresolved: While the paper shows promising results, the variability in performance across datasets suggests the need for more systematic analysis of when and why model uncertainty sampling succeeds or fails
- What evidence would resolve it: Comprehensive ablation studies across diverse tabular datasets with varying characteristics (feature types, class balance, noise levels) to identify conditions favoring model uncertainty sampling

### Open Question 2
- Question: How can the computational efficiency of IBUG be improved while maintaining its superior performance in regression tasks?
- Basis in paper: [explicit] The authors note that IBUG has the highest time complexity among model uncertainty estimation methods, despite showing the best performance in regression tasks
- Why unresolved: The paper acknowledges IBUG's computational overhead but does not explore optimization strategies or trade-offs between accuracy and efficiency
- What evidence would resolve it: Empirical comparison of IBUG with optimized implementations or approximations, or development of hybrid approaches that balance accuracy and computational cost

### Open Question 3
- Question: How can threshold tuning be automated or made more robust for CEAL methods in both classification and regression tasks?
- Basis in paper: [explicit] The authors mention that CEAL involves proper tuning of thresholding parameters and propose a two-stage filtering approach, but acknowledge this requires tuning of multiple thresholds
- Why unresolved: The paper demonstrates CEAL's effectiveness but relies on manual threshold selection, which limits practical deployment and scalability
- What evidence would resolve it: Development and validation of adaptive thresholding mechanisms or automated methods for determining optimal confidence thresholds based on dataset characteristics or early training dynamics

## Limitations
- The improvements from model uncertainty sampling over entropy are modest and dataset-dependent, suggesting the approach may not generalize well to all tabular data problems.
- The CEAL framework introduces additional hyperparameters (uncertainty and entropy thresholds) that are not systematically explored, potentially limiting reproducibility.
- The computational complexity analysis focuses only on time complexity without considering memory usage or scalability to larger datasets.

## Confidence

- High confidence: The experimental methodology is clearly described, and the implementation of uncertainty sampling methods is technically sound for the datasets tested.
- Medium confidence: The claim that model uncertainty sampling is a "promising alternative" to entropy is supported by experimental results but lacks theoretical justification or external validation.
- Low confidence: The CEAL framework's effectiveness depends heavily on threshold selection, which is not rigorously optimized or justified in the paper.

## Next Checks

1. Test the proposed methods on additional tabular datasets (e.g., from OpenML or Kaggle) to assess generalizability beyond the six datasets used in the paper.

2. Conduct ablation studies to determine the impact of each uncertainty estimation method (staged predictions, virtual ensembles, IBUG) and identify which contributes most to performance improvements.

3. Perform sensitivity analysis on CEAL threshold parameters to understand their impact on performance and identify optimal settings for different dataset characteristics.