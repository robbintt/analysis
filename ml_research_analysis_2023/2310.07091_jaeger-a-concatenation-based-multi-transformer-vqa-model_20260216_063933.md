---
ver: rpa2
title: 'Jaeger: A Concatenation-Based Multi-Transformer VQA Model'
arxiv_id: '2310.07091'
source_url: https://arxiv.org/abs/2310.07091
tags:
- question
- language
- large
- arxiv
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Jaeger, a novel concatenation-based multi-transformer
  model for document-based visual question answering (VQA). The model addresses challenges
  of prolonged response times, extended inference durations, and imprecision in matching
  by leveraging pre-trained large language models (RoBERTa-large and GPT2-xl) for
  feature extraction from questions, combined with BERT and ResNet-101 for processing
  document content and visual features.
---

# Jaeger: A Concatenation-Based Multi-Transformer VQA Model

## Quick Facts
- arXiv ID: 2310.07091
- Source URL: https://arxiv.org/abs/2310.07091
- Reference count: 16
- Exact Matching Accuracy (EMA) of 35.87% on validation and 33.63% on test sets for document-based VQA

## Executive Summary
Jaeger is a novel concatenation-based multi-transformer model designed for document-based visual question answering (VQA). The model addresses challenges of prolonged response times, extended inference durations, and imprecision in matching by leveraging pre-trained large language models (RoBERTa-large and GPT2-xl) for feature extraction from questions, combined with BERT and ResNet-101 for processing document content and visual features. A key innovation is the concatenation of features from different pre-trained models to enhance representational capability. Experimental results on the PDF-VQA Dataset show that Jaeger achieves competitive performance, outperforming baseline models like VisualBERT, ViLT, LXMERT, and LoSpa.

## Method Summary
Jaeger concatenates features from RoBERTa-large and GPT2-xl for question encoding, and BERT and ResNet-101 for document text and visual features. After concatenation, dimensionality reduction is applied to improve computational efficiency. The model is trained using the PDF-VQA Dataset with Exact Matching Accuracy as the primary metric. The approach aims to capture complementary information from diverse linguistic representations while reducing computational burden.

## Key Results
- Achieved EMA of 35.87% on PDF-VQA validation set and 33.63% on test set
- Outperformed baseline models including VisualBERT, ViLT, LXMERT, and LoSpa
- Demonstrated improved performance on hierarchical relationship questions compared to previous approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating features from multiple pre-trained language models (RoBERTa-large and GPT2-xl) enhances the representational capability for document-based VQA.
- Mechanism: The concatenation operation allows the model to capture complementary information from diverse linguistic representations, strengthening the overall feature set used for question understanding.
- Core assumption: Different pre-trained models encode question semantics in complementary ways that are beneficial when combined.
- Evidence anchors:
  - [abstract] "We leverage the exceptional capabilities of RoBERTa large and GPT2-xl as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process."
  - [section] "This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability."
- Break condition: If the concatenated features become redundant or if the models encode similar semantic information, the benefit of concatenation diminishes.

### Mechanism 2
- Claim: Dimensionality reduction after concatenation reduces computational burden while preserving discriminative information.
- Mechanism: After concatenating features from multiple sources, reducing dimensionality helps maintain computational efficiency during inference and training.
- Core assumption: The concatenated feature space contains redundant or non-discriminative information that can be removed without significant loss of performance.
- Evidence anchors:
  - [abstract] "After concatenation, we apply dimensionality reduction to the output features, reducing the model's computational effectiveness and inference time."
  - [section] "This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability."
- Break condition: Excessive dimensionality reduction may discard important features, leading to performance degradation.

### Mechanism 3
- Claim: Using pre-trained models for feature extraction (RoBERTa-large, GPT2-xl, BERT, ResNet-101) provides robust representations that outperform custom-trained models on document-based VQA tasks.
- Mechanism: Pre-trained models have learned rich linguistic and visual representations from large corpora, which transfer effectively to the VQA domain.
- Core assumption: The pre-trained models' learned representations are sufficiently general to capture the semantic and visual features needed for document-based question answering.
- Evidence anchors:
  - [abstract] "To derive question features, we leverage the exceptional capabilities of RoBERTa large and GPT2-xl as feature extractors."
  - [section] "By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the performance of these models through concatenation."
- Break condition: If the document domain differs significantly from the pre-training data, the representations may not transfer effectively.

## Foundational Learning

- Concept: Feature concatenation in multimodal learning
  - Why needed here: Understanding how combining features from different modalities or models can create richer representations is fundamental to grasping Jaeger's approach.
  - Quick check question: What is the difference between early fusion and late fusion in multimodal learning, and which does Jaeger use?

- Concept: Pre-trained language models and transfer learning
  - Why needed here: Jaeger relies on pre-trained models for feature extraction, so understanding how these models transfer knowledge to new tasks is essential.
  - Quick check question: How does RoBERTa differ from BERT in its pre-training objectives, and why might this matter for feature extraction?

- Concept: Dimensionality reduction techniques
  - Why needed here: Jaeger applies dimensionality reduction after concatenation, so familiarity with techniques like PCA or autoencoders is important.
  - Quick check question: What are the trade-offs between linear dimensionality reduction (e.g., PCA) and non-linear methods (e.g., autoencoders) for preserving information?

## Architecture Onboarding

- Component map: Question → RoBERTa-large → GPT2-xl → Concat → DimReduce → BERT/ResNet → Document features → Fusion → Answer prediction
- Critical path: Question features from RoBERTa-large and GPT2-xl are concatenated and reduced in dimensionality, then fused with document features from BERT and ResNet-101 to produce the final VQA prediction.
- Design tradeoffs:
  - Multiple pre-trained models provide rich features but increase inference time
  - Concatenation captures diverse information but may introduce redundancy
  - Dimensionality reduction improves efficiency but risks information loss
- Failure signatures:
  - Poor performance on hierarchical relationship questions may indicate insufficient document-level understanding
  - Slow inference times could suggest the dimensionality reduction is too weak
  - Degradation when removing one pre-trained model suggests redundancy in the remaining models
- First 3 experiments:
  1. Test individual pre-trained models (RoBERTa-only, GPT2-only) to measure contribution of each to final performance
  2. Vary the dimensionality reduction ratio to find optimal balance between efficiency and accuracy
  3. Replace ResNet-101 with a lighter backbone (e.g., MobileNet) to assess computational efficiency vs. accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of large language models used for feature extraction affect the performance and computational efficiency of the Jaeger model?
- Basis in paper: [explicit] The paper states "we will also examine the number of large language models to be involved when extracting features" as future work.
- Why unresolved: The current study uses two large language models (RoBERTa-large and GPT2-xl) for feature extraction but does not explore the impact of using different numbers of models.
- What evidence would resolve it: Experiments comparing the performance and computational efficiency of Jaeger when using different numbers of large language models for feature extraction, such as using only one model, using three or more models, and analyzing the trade-offs between performance gains and increased computational costs.

### Open Question 2
- Question: What specific fine-tuning strategies could be tailored to the document-based VQA task to further boost performance while maintaining efficiency?
- Basis in paper: [explicit] The paper mentions "investigating fine-tuning strategies tailored to our specific task, aiming to further boost performance while maintaining efficiency would also be valuable" as future work.
- Why unresolved: The current study does not explore any fine-tuning strategies beyond using pre-trained models for feature extraction.
- What evidence would resolve it: Experiments testing various fine-tuning strategies, such as task-specific pre-training, progressive fine-tuning, or multi-task learning, and analyzing their impact on the performance and efficiency of Jaeger compared to the current approach.

### Open Question 3
- Question: How does the concatenation strategy used in Jaeger compare to other feature fusion techniques, such as attention-based fusion or gating mechanisms, in terms of performance and representational capability?
- Basis in paper: [inferred] The paper introduces Jaeger's concatenation strategy as a key innovation but does not compare it to other feature fusion techniques.
- Why unresolved: The paper does not provide a comparison between the concatenation strategy and other potential feature fusion methods.
- What evidence would resolve it: Experiments comparing the performance and representational capability of Jaeger's concatenation strategy with other feature fusion techniques, such as attention-based fusion or gating mechanisms, on the document-based VQA task using the same dataset and evaluation metrics.

## Limitations
- The paper does not provide detailed information about the dimensionality reduction technique used after concatenation
- Performance on hierarchical relationship questions, while improved, still lags behind human-level understanding
- Computational overhead of using multiple large pre-trained models may limit practical deployment in resource-constrained environments

## Confidence
- High Confidence: The experimental results showing Jaeger's competitive performance against baseline models on the PDF-VQA dataset
- Medium Confidence: The claim that concatenating features from multiple pre-trained models enhances representational capability
- Low Confidence: The assertion that dimensionality reduction preserves discriminative information while improving computational efficiency

## Next Checks
1. Conduct ablation studies removing each pre-trained model component to quantify individual contributions
2. Systematically vary the dimensionality reduction ratio and measure its impact on both computational efficiency and accuracy
3. Evaluate Jaeger on a different document-based VQA dataset to assess generalization beyond PDF-VQA