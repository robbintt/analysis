---
ver: rpa2
title: The Trickle-down Impact of Reward (In-)consistency on RLHF
arxiv_id: '2309.16155'
source_url: https://arxiv.org/abs/2309.16155
tags:
- uni00000013
- uni00000044
- uni00000055
- uni00000033
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores reward inconsistency in Reinforcement Learning
  from Human Feedback (RLHF), where reward models (RMs) trained with standard ranking
  objectives fail to properly align with human preferences for different prompts.
  The authors introduce CONTRAST INSTRUCTIONS, a benchmarking strategy to measure
  reward consistency by testing whether RMs assign higher rewards to corresponding
  instruction-response pairs versus distractors.
---

# The Trickle-down Impact of Reward (In-)consistency on RLHF

## Quick Facts
- arXiv ID: 2309.16155
- Source URL: https://arxiv.org/abs/2309.16155
- Reference count: 40
- Primary result: Standard RMs trained with ranking objectives show near-random consistency on CONTRAST INSTRUCTIONS benchmark; enhancing consistency through CONVEX DA and REWARD FUSION improves downstream RLHF model quality

## Executive Summary
This paper addresses reward inconsistency in Reinforcement Learning from Human Feedback (RLHF), where reward models (RMs) fail to properly align with human preferences when prompts are lexically similar but semantically different. The authors introduce CONTRAST INSTRUCTIONS, a benchmark that measures whether RMs assign higher rewards to corresponding instruction-response pairs versus distractors. Experiments reveal that standard RMs perform near random on this benchmark, indicating a significant gap between RM predictions and human judgments. To address this, the paper proposes two techniques: CONVEX DA (data augmentation during training) and REWARD FUSION (reward averaging during inference). The key finding is that RLHF models trained with more consistent RMs generate more useful responses, demonstrating the trickle-down effect of reward inconsistency.

## Method Summary
The paper introduces CONTRAST INSTRUCTIONS to measure RM consistency by evaluating whether RMs assign higher rewards to instruction-response pairs that match semantically versus lexically similar distractors. Standard RMs are trained using pairwise ranking loss on human preference data. To enhance consistency, the authors propose CONVEX DA (selecting convex hull vertices from augmented responses during training) and REWARD FUSION (averaging rewards from similar training examples during inference). The pipeline involves training RMs on human preference datasets, evaluating consistency on CONTRAST INSTRUCTIONS, applying the proposed techniques to improve consistency, and then using the enhanced RMs in downstream RLHF to optimize language models via policy gradients.

## Key Results
- Standard RMs achieve near-random performance (Cres and Cins close to 0.5) on CONTRAST INSTRUCTIONS, indicating poor consistency for semantically similar but distinct instructions
- CONVEX DA and REWARD FUSION significantly improve consistency metrics (Cres and Cins) without requiring additional human preference data
- RLHF models trained with more consistent RMs generate responses with statistically significant improvements in usefulness (p < 0.01)
- Larger RMs demonstrate better consistency than smaller ones, but still exhibit substantial inconsistency gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard ranking objectives fail to capture semantic correspondence between instruction-response pairs
- Core assumption: Standard RLHF reward modeling objectives are insufficient for capturing fine-grained semantic distinctions between instruction-response pairs
- Evidence: Standard RMs achieve near-random performance on CONTRAST INSTRUCTIONS compared to human performance
- Break condition: If RM is explicitly trained to recognize semantic correspondence or evaluation includes more diverse semantic variations

### Mechanism 2
- Claim: Reward inconsistency propagates downstream through RLHF pipeline
- Core assumption: RLHF models are highly sensitive to reward signal quality and inherit RM inconsistencies
- Evidence: RLHF models trained with consistent RMs show statistically significant improvement in response usefulness (p < 0.01)
- Break condition: If RLHF process includes additional robustness mechanisms or explicit consistency regularization

### Mechanism 3
- Claim: Data augmentation and reward fusion can enhance consistency without additional training cost
- Core assumption: Training-time augmentation and inference-time averaging can improve semantic consistency without labeled data
- Evidence: CONVEX DA and REWARD FUSION improve Cres and Cins metrics with empirical results showing consistent gains
- Break condition: If semantic space cannot be captured by embedding model or convex hull selection fails to identify diverse examples

## Foundational Learning

- Concept: Semantic similarity vs. lexical similarity
  - Why needed: Paper relies on distinguishing between lexically similar but semantically different instructions
  - Quick check: If two instructions have cosine similarity 0.8 but refer to completely different concepts, what type of similarity are we measuring?

- Concept: Ranking loss vs. regression loss
  - Why needed: Standard ranking loss may not be optimal for capturing semantic consistency
  - Quick check: How would regression loss predicting human preference scores differ from pairwise ranking loss?

- Concept: Convex hull selection in data augmentation
  - Why needed: CONVEX DA uses convex hull vertices to select diverse augmented examples
  - Quick check: With 5 augmented versions of a response, how do you determine which one to select as the vertex of a 2D convex hull?

## Architecture Onboarding

- Component map: Human Preference Data -> RM Training (Ranking Loss) -> CONTRAST INSTRUCTIONS Evaluation -> CONVEX DA/REWARD FUSION Enhancement -> Enhanced RM -> RLHF Pipeline -> Policy Optimization

- Critical path: 1) Obtain human preference data, 2) Train RM with ranking loss, 3) Evaluate consistency using Contrast Instructions, 4) Apply CONVEX DA/REWARD FUSION, 5) Use enhanced RM in RLHF, 6) Evaluate downstream model quality

- Design tradeoffs: Standard ranking vs. consistency-focused training; CONVEX DA vs. random augmentation; REWARD FUSION vs. single reward computation

- Failure signatures: RM performs near random on Contrast Instructions; RLHF generates inconsistent responses; high variance in reward scores; data augmentation fails to improve metrics

- First 3 experiments: 1) Implement Contrast Instructions benchmark and evaluate standard RM performance, 2) Apply CONVEX DA during training and measure Cres/Cins improvement, 3) Implement REWARD FUSION during inference and evaluate consistency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does reward inconsistency impact RLHF model safety and alignment beyond response usefulness?
- Basis: Paper mentions RMs' vulnerability to adversarial attacks suggesting safety implications
- Why unresolved: Paper focuses on response quality, not safety implications
- Evidence needed: Experiments measuring safety metrics for RLHF models trained with consistent vs. inconsistent RMs

### Open Question 2
- Question: Can consistency enhancement techniques generalize to other reward modeling tasks?
- Basis: Paper demonstrates effectiveness for RLHF but doesn't explore other tasks
- Why unresolved: No investigation of performance on different reward modeling tasks
- Evidence needed: Applying techniques to RMs for text summarization, machine translation, etc.

### Open Question 3
- Question: How does RM scale affect consistency, and what's the optimal balance?
- Basis: Paper notes larger RMs show better consistency but doesn't analyze scale-consistency relationship
- Why unresolved: No exploration of how scale impacts consistency or identification of optimal scale
- Evidence needed: Experiments with RMs of different scales evaluating consistency-performance trade-off

## Limitations
- Reliance on synthetic CONTRAST INSTRUCTIONS benchmark rather than real-world preference distributions
- Limited validation of trickle-down effect beyond one model size (7B) and specific task domain
- Proposed consistency enhancement methods need validation on actual human preference alignment

## Confidence

- High confidence: Standard RMs exhibit reward inconsistency on semantically similar but distinct instructions (well-supported by empirical evidence)
- Medium confidence: Trickle-down effect of inconsistency on RLHF quality (demonstrated but limited to one model size/domain)
- Medium confidence: Proposed solutions improve consistency (shown effective but impact on human preference alignment needs validation)

## Next Checks

1. Evaluate trickle-down effect on RLHF models of different sizes beyond the 7B model tested
2. Test consistency enhancement methods on real-world scenarios where reward inconsistency naturally occurs
3. Conduct ablation studies to isolate contributions of CONVEX DA and REWARD FUSION components