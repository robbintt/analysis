---
ver: rpa2
title: 'MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for
  Language Model Evaluation'
arxiv_id: '2310.14088'
source_url: https://arxiv.org/abs/2310.14088
tags:
- sentence
- language
- tasks
- report
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MedEval, a comprehensive medical benchmark
  dataset for evaluating language models. It covers 35 body regions and 8 examination
  modalities with 22,779 sentences and 21,228 reports annotated by experts.
---

# MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation

## Quick Facts
- arXiv ID: 2310.14088
- Source URL: https://arxiv.org/abs/2310.14088
- Reference count: 23
- Key outcome: Introduces MedEval, a comprehensive medical benchmark with 22,779 sentences and 21,228 reports annotated by experts across 35 body regions and 8 examination modalities, evaluating 10 language models on abnormality identification, ambiguity detection, report coding, and summarization tasks

## Executive Summary
MedEval presents a comprehensive multi-level, multi-task, and multi-domain medical benchmark for evaluating language models in healthcare applications. The dataset covers 35 body regions and 8 examination modalities with expert annotations at both sentence and document levels. The authors evaluate both domain-adapted pre-trained language models (PLMs) and large language models (LLMs) across natural language understanding and generation tasks, revealing task-specific effectiveness patterns. Adapted PLMs excel at NLU tasks while prompted LLMs show superior performance in generation tasks, with instruction tuning identified as critical for LLM effectiveness.

## Method Summary
The authors created MedEval by collecting 22,779 sentences and 21,228 reports from medical imaging data, covering 35 body regions and 8 examination modalities. Expert annotators labeled the data for multiple tasks including abnormality identification, ambiguity detection, report coding, and summarization at both sentence and document levels. The evaluation framework tested 10 language models: six domain-adapted PLMs (BERT, RadBERT, BioBERT, ClinicalBERT, BlueBERT, BioMed-ReBERTa) and four LLMs (GPT-3, ChatGPT, Vicuna-7B, BioMedLM) using zero-shot and few-shot learning approaches. Models were evaluated on classification metrics for NLU tasks and BLEU/ROUGE scores for generation tasks.

## Key Results
- Adapted PLMs outperform prompted LLMs in NLU tasks at both sentence and document levels
- Prompted LLMs excel at generation tasks compared to adapted PLMs
- Instruction tuning is critical for LLM performance in medical tasks
- Few-shot LLMs achieve comparable performance to fine-tuned PLMs on certain generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapted PLMs outperform prompted LLMs in NLU tasks at both sentence and document levels
- Mechanism: Fine-tuning adapted PLMs injects specific domain knowledge and task-related understanding, enabling them to capture medical semantics more effectively than general LLMs without fine-tuning
- Core assumption: Domain-specific fine-tuning provides better adaptation to narrow medical domains than zero/few-shot prompting
- Evidence anchors: [abstract], [section], [corpus] Weak

### Mechanism 2
- Claim: Prompted LLMs excel at generation tasks compared to adapted PLMs
- Mechanism: Large-scale pre-training and instruction tuning in LLMs enable them to generate more human-like language and follow complex instructions better than fine-tuned PLMs
- Core assumption: The benefits of large-scale pre-training and instruction tuning outweigh the domain-specific knowledge gained through fine-tuning for generation tasks
- Evidence anchors: [abstract], [section], [corpus] Weak

### Mechanism 3
- Claim: Instruction tuning is critical for LLM performance in medical tasks
- Mechanism: Instruction tuning enables LLMs to better understand and follow task instructions, producing more relevant outputs
- Core assumption: LLMs without instruction tuning struggle to interpret and execute task-specific instructions
- Evidence anchors: [abstract], [section], [corpus] Weak

## Foundational Learning

- Concept: Multi-task learning
  - Why needed here: The benchmark covers multiple tasks at different levels (sentence and document), requiring models to handle diverse objectives
  - Quick check question: Can you explain how a model can be trained to perform both classification and generation tasks?

- Concept: Domain adaptation
  - Why needed here: Medical text has unique characteristics that require models to be adapted from general language models
  - Quick check question: What are the key differences between general language models and domain-adapted models?

- Concept: Prompt engineering
  - Why needed here: The study evaluates few-shot learning with LLMs, which heavily relies on effective prompt design
  - Quick check question: How would you design a prompt for a few-shot learning task with a language model?

## Architecture Onboarding

- Component map: Data collection → Expert annotation → Model evaluation → Results analysis → Insights generation
- Critical path: Data collection → Expert annotation → Model evaluation → Results analysis → Insights generation
- Design tradeoffs: Adapted PLMs vs. Prompted LLMs (fine-tuning vs. prompting flexibility); Sentence-level vs. Document-level (focused vs. contextual)
- Failure signatures: Poor NLU performance with prompted LLMs (needs fine-tuning or better prompts); Low generation scores with adapted PLMs (needs larger models or different fine-tuning)
- First 3 experiments: 1) Baseline classifier on each task; 2) Few-shot LLM on simple task; 3) Single-task fine-tuned PLM comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would BioMedLM perform with instruction fine-tuning, and would this close the performance gap with other LLMs?
- Basis in paper: [explicit] The paper explicitly notes that BioMedLM lacks instruction tuning, which significantly impacts its performance, and suggests this as a critical step for specialized applications
- Why unresolved: The authors excluded BioMedLM from comparative analysis due to its poor performance and format issues, but did not evaluate a version with instruction tuning
- What evidence would resolve it: Testing a version of BioMedLM with instruction fine-tuning on the same tasks would provide direct comparison data

### Open Question 2
- Question: What is the optimal number of examples for few-shot learning across different task types (NLU vs NLG) and domains?
- Basis in paper: [inferred] The paper mentions exploring different shot settings but notes inconsistent trends in NLG tasks and varying performance across domains, suggesting this is not yet optimized
- Why unresolved: The authors used fixed shot settings (2-shot for NLU, 3-shot for NLG) but acknowledge that optimal numbers may vary by task type and domain
- What evidence would resolve it: Systematic testing of various shot numbers (1-9) across all task types and domains would reveal optimal configurations

### Open Question 3
- Question: How would larger language models like Vicuna-60B or OPT-175B perform on MedEval compared to the evaluated models?
- Basis in paper: [explicit] The authors explicitly state they could not evaluate these larger models due to computing constraints but suggest their inclusion in future work
- Why unresolved: Computing limitations prevented evaluation of these models, leaving a gap in understanding the upper bounds of LLM performance on medical tasks
- What evidence would resolve it: Performance evaluation of these larger models on the same MedEval benchmark would provide this comparison

## Limitations
- The prompt engineering methodology for LLMs is not fully specified, which could significantly impact performance variations
- Evaluation primarily focuses on English-language medical text, limiting generalizability to other languages or clinical settings
- Comparison between fine-tuned PLMs and prompted LLMs may be affected by differing evaluation protocols

## Confidence

**High Confidence**: Adapted PLMs outperform prompted LLMs on NLU tasks - well-supported by experimental results and aligns with established domain adaptation principles

**Medium Confidence**: Prompted LLMs excel at generation tasks compared to adapted PLMs - supported by results but requires careful interpretation due to different evaluation methodologies

**Medium Confidence**: Instruction tuning is critical for LLM performance - supported by BioMedLM comparison but limited by sample size

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary prompt formulations, shot counts, and example selection strategies for LLMs across all tasks to quantify the impact of prompt engineering on performance variations

2. **Cross-domain generalization testing**: Evaluate model performance on medical text from specialties beyond radiology (e.g., pathology, clinical notes) to assess the generalizability of observed performance patterns across medical domains

3. **Controlled fine-tuning comparison**: Implement a consistent evaluation protocol where both PLMs and LLMs undergo similar fine-tuning procedures on the MedEval dataset, then compare performance to isolate the effects of model architecture versus training methodology