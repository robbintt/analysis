---
ver: rpa2
title: Steering Llama 2 via Contrastive Activation Addition
arxiv_id: '2312.06681'
source_url: https://arxiv.org/abs/2312.06681
tags:
- steering
- llama
- chat
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Contrastive Activation Addition (CAA), a technique
  for steering language models by modifying their activations during forward passes.
  CAA computes "steering vectors" by averaging the difference in residual stream activations
  between pairs of positive and negative examples of a particular behavior, such as
  factual versus hallucinatory responses.
---

# Steering Llama 2 via Contrastive Activation Addition

## Quick Facts
- arXiv ID: 2312.06681
- Source URL: https://arxiv.org/abs/2312.06681
- Authors: 
- Reference count: 40
- Primary result: Contrastive Activation Addition (CAA) steers Llama 2 behavior by adding averaged activation difference vectors at token positions, effectively controlling behaviors like sycophancy and hallucination while minimally impacting capabilities.

## Executive Summary
This paper introduces Contrastive Activation Addition (CAA), a technique for steering language model behavior by modifying residual stream activations during inference. CAA generates "steering vectors" by averaging the difference in activations between positive and negative examples of target behaviors, then adds these vectors at all token positions after the user's prompt. The method is evaluated on Llama 2 Chat models using multiple-choice behavioral datasets and open-ended generation tasks, demonstrating significant behavioral changes with minimal capability degradation. The work also provides insights into how high-level concepts are represented in transformer activation spaces through PCA visualization and layer-wise analysis.

## Method Summary
CAA computes steering vectors by taking the mean difference between residual stream activations for positive and negative behavior examples at a chosen layer. During inference, these vectors are added at every token position after the user's prompt with a specified coefficient. The technique was evaluated on Llama 2 7B and 13B Chat models using A/B multiple-choice behavioral question datasets and open-ended generation tasks. Steering effectiveness was measured through automated scoring using Claude 2 and GPT-3.5, while capability preservation was assessed on MMLU. The optimal layer for steering was found to be 15-17, where high-level behavioral representations are most abstract and modifiable.

## Key Results
- CAA significantly alters model behavior, steering Llama 2 toward or away from target behaviors like sycophancy and hallucination
- The technique is effective on top of traditional alignment methods like finetuning and system prompt design
- CAA minimally reduces model capabilities as measured by MMLU performance when applied with appropriate coefficients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CAA steers model behavior by adding a fixed steering vector at each token position after the prompt.
- **Mechanism:** During inference, the difference vector between positive and negative behavior examples is averaged and added to residual stream activations at every token position beyond the prompt. This shifts the model's latent state toward the target behavior.
- **Core assumption:** The steering vector represents a consistent direction in activation space that correlates with the target behavior across different contexts.
- **Evidence anchors:**
  - [abstract] "During inference, these steering vectors are added at all token positions after the user's prompt..."
  - [section 3] Formal definition of the Mean Difference vector: \( v_{MD} = \frac{1}{|D|} \sum_{\text{prompt,compp,compn}\in D} a_L(\text{prompt}, \text{compp}) - a_L(\text{prompt}, \text{compn}) \)
  - [corpus] Multiple studies show activation steering works across models, but effectiveness depends on consistent behavioral clustering in activation space.
- **Break condition:** If behavioral clustering does not emerge in later layers or if the steering vector does not transfer well between layers, the technique will fail to steer consistently.

### Mechanism 2
- **Claim:** Steering is most effective at intermediate layers where high-level representations of behavior are formed.
- **Mechanism:** At later layers, residual stream activations contain more abstract representations of concepts like sycophancy or corrigibility. Intervening here can shift these high-level representations before they are converted to token predictions.
- **Core assumption:** The layer where CAA is applied must correspond to where the relevant behavioral representation is most abstract and modifiable.
- **Evidence anchors:**
  - [section 4.1] "For both Llama 2 7B and 13B Chat, layers 15-17 show the most significant steering influence."
  - [section 8.2] "Vectors from closer layers have a higher similarity... once the model extracts the high-level information needed to describe an abstract concept, the representation 'converges'."
  - [corpus] Representation engineering literature suggests higher layers capture more abstract semantic features.
- **Break condition:** If the behavioral representation is distributed across layers or if the representation is too early (close to token space) or too late (close to output space), steering at the chosen layer will be ineffective.

### Mechanism 3
- **Claim:** CAA steering vectors generalize from multiple-choice formats to open-ended generation tasks.
- **Mechanism:** The steering vector encodes the direction of the target behavior in activation space, so adding it during open-ended generation shifts the model toward that behavior even without the structured prompt format.
- **Core assumption:** The activation direction representing the behavior is consistent across different task formats and not tied to the specific phrasing of multiple-choice options.
- **Evidence anchors:**
  - [abstract] "We evaluate CAA's effectiveness... on open-ended generation tasks... CAA significantly alters model behavior."
  - [section 4.2] "We find the model typically justifies its answer in the continuation text... the context before the 'A/B' is behavior-neutral, then answering A or B steers the model towards justifying that behavior."
  - [corpus] Some activation steering studies report limited generalization; this work claims broader effectiveness.
- **Break condition:** If the activation direction is highly sensitive to prompt format or if the behavior is context-dependent, the steering vector will not generalize to free-form text.

## Foundational Learning

- **Concept:** Residual stream activations in transformer models
  - **Why needed here:** CAA operates by modifying these activations during inference to steer behavior.
  - **Quick check question:** What is the dimension of the residual stream in a typical transformer block, and at which point in the block is it accessible for modification?

- **Concept:** Contrastive learning / mean difference computation
  - **Why needed here:** CAA generates steering vectors by averaging the difference in activations between positive and negative behavior examples.
  - **Quick check question:** Given two sets of activation vectors for positive and negative examples, how would you compute the mean difference vector formally?

- **Concept:** Principal Component Analysis (PCA) for activation space visualization
  - **Why needed here:** The paper uses PCA to visualize and assess whether activations cluster by behavior, which is necessary for generating effective steering vectors.
  - **Quick check question:** If activations for a behavior are only separable in later principal components, what does that imply about the complexity of the representation?

## Architecture Onboarding

- **Component map:** Embedding -> Transformer layers -> CAA steering vector addition -> Attention/MLP processing -> Logits
- **Critical path:** User prompt -> Tokenization -> Embedding -> Transformer layers -> CAA steering vector addition -> Attention/MLP processing -> Logits -> Generation
- **Design tradeoffs:**
  - Adding vectors at all token positions increases steering effect but may introduce noise or reduce capabilities
  - Choosing the right layer balances between abstract representation and controllability
  - Using many contrastive pairs improves vector quality but increases data requirements
- **Failure signatures:**
  - No behavioral clustering in PCA plots -> steering vector will not encode behavior
  - Small effect size on multiple-choice -> likely won't generalize to open-ended
  - Large capability drop on MMLU -> steering is too strong or misaligned
- **First 3 experiments:**
  1. Visualize activation clustering with PCA for a new behavior dataset to check if CAA is viable
  2. Sweep CAA layer and coefficient on a held-out multiple-choice set to find optimal parameters
  3. Test generalization by applying the steering vector to open-ended generation and scoring with a behavioral rater

## Open Questions the Paper Calls Out
- **Question:** How effective would CAA be when steering at multiple layers simultaneously with different steering vectors targeting different behaviors?
  - **Basis in paper:** [explicit] The paper mentions this as a potential future direction: "Future work could extend this by testing steering at multiple layers simultaneously, potentially with vectors corresponding to different desired properties."
  - **Why unresolved:** The paper only tested CAA at single layers, so the combined effect of multi-layer steering remains unexplored.
  - **What evidence would resolve it:** Experiments applying CAA at multiple layers with different steering vectors, measuring the cumulative effect on model behavior compared to single-layer steering.

- **Question:** How does the effectiveness of CAA transfer between different model architectures beyond Llama 2, such as GPT models or smaller models?
  - **Basis in paper:** [explicit] The paper states: "CAA successfully layers on top of other alignment techniques like prompting and finetuning... since these approaches are already standard for LLM control, the fact that CAA provides additional steering influence makes it a promising complement to further refine model behavior."
  - **Why unresolved:** The paper only tested CAA on Llama 2 models, leaving the generalizability to other architectures unknown.
  - **What evidence would resolve it:** Testing CAA on different model architectures (e.g., GPT-3, GPT-4, smaller LLaMA models) and comparing steering effectiveness.

- **Question:** How does CAA compare to other activation engineering techniques like ITI in terms of steering effectiveness and computational efficiency?
  - **Basis in paper:** [explicit] The paper discusses related work including ITI: "Li et al. propose an inference-time intervention technique (ITI) that identifies a sparse set of 'truthful' attention heads using linear probes trained to predict truthfulness on a contrastive question-answering dataset."
  - **Why unresolved:** The paper only compares CAA to finetuning and few-shot prompting, not to other activation engineering methods.
  - **What evidence would resolve it:** Head-to-head comparisons of CAA versus ITI and other activation engineering techniques on the same benchmarks measuring both steering effectiveness and computational cost.

## Limitations
- Effectiveness relies on behavioral clustering in activation space being consistent and transferable across layers
- Generalization from multiple-choice to open-ended tasks assumes activation directions are format-invariant
- Evaluation relies on automated scoring from Claude 2 and GPT-3.5, introducing potential rater bias

## Confidence
- **High Confidence:** CAA can steer model behavior by adding steering vectors at token positions, and this is most effective at intermediate layers where abstract representations form
- **Medium Confidence:** CAA steering vectors generalize from multiple-choice to open-ended generation tasks, and the technique minimally reduces model capabilities
- **Low Confidence:** The specific layer (15-17) is optimal for all behaviors and model sizes, and that steering vector effectiveness is consistent across different behavioral targets

## Next Checks
1. **Layer Transferability Test:** Generate steering vectors at layer 15 and test their effectiveness when applied at layers 10, 12, 15, 17, and 20 to reveal whether behavioral representation is truly concentrated at the claimed optimal layers

2. **Behavioral Generalization Stress Test:** Apply sycophancy steering vectors to open-ended prompts that are structurally very different from the multiple-choice format used to generate them (e.g., conversational dialogs or creative writing tasks) to test format generalization

3. **Cross-Model Size Validation:** Apply steering vectors trained on Llama 2 7B to Llama 2 13B and vice versa to test whether behavioral representation is consistent across model scales