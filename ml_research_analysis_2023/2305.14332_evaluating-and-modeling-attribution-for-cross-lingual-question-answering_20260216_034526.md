---
ver: rpa2
title: Evaluating and Modeling Attribution for Cross-Lingual Question Answering
arxiv_id: '2305.14332'
source_url: https://arxiv.org/abs/2305.14332
tags:
- attribution
- language
- answer
- answers
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies attribution for cross-lingual question answering,
  where answers may be supported by passages in a different language than the query.
  It collects a new dataset of ~10K samples across 5 languages (Bengali, Finnish,
  Japanese, Russian, Telugu) to evaluate the attribution level of a state-of-the-art
  cross-lingual QA system.
---

# Evaluating and Modeling Attribution for Cross-Lingual Question Answering

## Quick Facts
- arXiv ID: 2305.14332
- Source URL: https://arxiv.org/abs/2305.14332
- Reference count: 18
- Key outcome: Up to 50% of correct answers in cross-lingual QA are unattributable to retrieved passages, highlighting need for better attribution detection

## Executive Summary
This paper investigates attribution in cross-lingual question answering systems, where answers may be supported by passages in languages different from the query. The authors collect a new dataset of ~10K samples across 5 languages (Bengali, Finnish, Japanese, Russian, Telugu) to evaluate attribution levels. Surprisingly, they find that a substantial portion of correct answers (up to 50%) cannot be attributed to any retrieved passage, despite the system being able to attend directly to the text. To address this, the paper explores using PaLM 2 and NLI models for automatic attribution detection, finding that PaLM 2 fine-tuned on small samples outperforms other models. An NLI-based reranking approach improves attribution by up to 5 points, demonstrating the potential for improving trustworthiness in cross-lingual QA systems.

## Method Summary
The study evaluates attribution in a cross-lingual QA system (CORA) consisting of a multilingual dense retriever (MDPR) and generator (MGEN). The authors collect attribution data by asking annotators to determine if answers can be attributed to any retrieved passage. They fine-tune PaLM 2 and NLI models on small samples of this data for binary attribution classification. The NLI models are also used as rerankers to improve attribution by selecting passages most likely to support the answer. The attribution accuracy (AIS) metric measures the proportion of answers with an attributed passage.

## Key Results
- Up to 50% of correct answers are unattributable to any retrieved passage despite model attention
- PaLM 2 fine-tuned on ~200 samples outperforms other models for attribution detection
- NLI-based reranking improves attribution by up to 5 points
- Cross-lingual passages contribute minimally to attribution, especially for Telugu
- In-language attribution yields higher scores than in-English attribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual QA systems can generate correct answers without supporting evidence passages due to language model memorization.
- Mechanism: Language models, especially large ones, have the capacity to memorize facts during pretraining. When generating answers, they may rely on this memorized knowledge rather than using retrieved passages, leading to correct answers that are unattributable.
- Core assumption: The language model has memorized sufficient factual knowledge during pretraining to answer a significant portion of questions correctly without needing to consult retrieved passages.
- Evidence anchors:
  - [abstract] "To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 50% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text."
  - [section] "We hypothesize that language models 'memorize' knowledge in their parameters (Petroni et al., 2019; Roberts et al., 2020), which could enable this."
- Break condition: If the language model has not memorized sufficient factual knowledge for the domain or if the questions are highly specific and unlikely to be covered in the pretraining data.

### Mechanism 2
- Claim: PaLM 2 can effectively detect attribution with minimal fine-tuning data due to its strong multilingual capabilities and large-scale pretraining.
- Mechanism: PaLM 2, being a large multilingual language model, has been pretrained on a vast amount of multilingual data. This pretraining allows it to understand the relationship between questions, answers, and passages across languages. Fine-tuning on a small amount of attribution data allows it to adapt this understanding to the specific task of attribution detection.
- Core assumption: The multilingual pretraining of PaLM 2 provides a strong foundation for understanding cross-lingual relationships between text, which can be leveraged for attribution detection with minimal additional training.
- Evidence anchors:
  - [abstract] "PaLM 2 fine-tuned on a very small sample of the data outperforms other models."
  - [section] "We find that PaLM 2 outperforms all the other models for attribution detection despite being fine-tuned on a very small samples of our collected data (about 200 samples)."
- Break condition: If the attribution task requires understanding nuances or cultural contexts that are not well-represented in the pretraining data or if the languages involved are extremely low-resource and not well-covered in pretraining.

### Mechanism 3
- Claim: NLI models can effectively detect attribution because attribution can be framed as a textual entailment problem.
- Mechanism: Attribution detection can be viewed as determining whether the answer (hypothesis) is entailed by or supported by the passage (premise). NLI models, which are trained to determine the relationship between pairs of sentences, can be adapted to this task.
- Core assumption: The relationship between a correct answer and its supporting passage can be effectively modeled as a textual entailment problem, where the answer is entailed by the passage.
- Evidence anchors:
  - [abstract] "We find that Natural Language Inference models and PaLM 2 fine-tuned on a very small amount of attribution data can accurately detect attribution."
  - [section] "We find that the NLI fine-tuned models outperform the question-answering fine-tuned models and the baseline for all the languages in the translate-test setting."
- Break condition: If the relationship between answers and passages is not well-captured by textual entailment, such as when the answer requires combining information from multiple passages or when the answer is implicit and not directly stated in the passage.

## Foundational Learning

- Concept: Cross-lingual QA systems and their components (retriever, generator)
  - Why needed here: Understanding the basic components and functioning of cross-lingual QA systems is crucial to grasp the attribution problem and the proposed solutions.
  - Quick check question: What are the two main components of a cross-lingual QA system and what are their roles?

- Concept: Attribution in QA
  - Why needed here: Attribution is the core concept being studied in this paper. Understanding what it means and why it's important is essential.
  - Quick check question: What is attribution in the context of QA and why is it important for trustworthiness?

- Concept: Textual entailment and NLI models
  - Why needed here: NLI models are used for attribution detection. Understanding how they work and how they can be applied to attribution is key.
  - Quick check question: How can attribution detection be framed as a textual entailment problem and what are the implications of this framing?

## Architecture Onboarding

- Component map:
  Cross-lingual QA system (CORA) -> MDPR retrieves passages -> MGEN generates answer -> Attribution detection model evaluates -> (Optional) NLI reranking selects best passage

- Critical path:
  1. Query is input to the cross-lingual QA system
  2. MDPR retrieves relevant passages in multiple languages
  3. MGEN generates an answer using the retrieved passages
  4. Attribution detection model evaluates if the answer is attributable to any retrieved passage
  5. If using reranking, the best passage is selected based on the attribution detection score

- Design tradeoffs:
  - Using PaLM 2 vs. fine-tuning a smaller model: PaLM 2 achieves high performance with minimal fine-tuning data but is more expensive to use. Fine-tuning a smaller model is cheaper but requires more data.
  - In-language vs. in-English attribution: In-language attribution is more user-centric but requires annotators in each language. In-English attribution is easier to scale but may introduce translation errors.

- Failure signatures:
  - High rate of non-attributable answers: Indicates that the QA system is relying too much on memorization and not enough on retrieved evidence.
  - Low agreement between in-language and in-English attribution: Suggests that translation may be introducing errors or that cultural context is important for attribution.

- First 3 experiments:
  1. Evaluate the baseline attribution level of the cross-lingual QA system on a small dataset.
  2. Fine-tune PaLM 2 on a small sample of attribution data and evaluate its performance.
  3. Implement the NLI-based reranking system and measure its impact on attribution level.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve cross-lingual retrieval systems to increase the attribution of answers to passages, particularly for languages like Telugu where cross-lingual passages contribute minimally?
- Basis in paper: [explicit] The paper discusses the impact of cross-language attribution, noting that between 0.3% and 4.0% of answers can be attributed to an English passage, with Telugu showing minimal contribution from cross-lingual passages.
- Why unresolved: The paper identifies this as a critical research direction but does not provide specific solutions or methods to enhance cross-lingual retrieval systems.
- What evidence would resolve it: Experimental results showing improved attribution scores after implementing specific cross-lingual retrieval techniques or models.

### Open Question 2
- Question: What are the specific reasons for the disagreement between in-language and in-English attribution scenarios, and how can this be mitigated to ensure consistent attribution assessment?
- Basis in paper: [explicit] The paper notes disagreements between in-language and in-English attribution scenarios, partially attributed to demographic and cultural backgrounds of raters.
- Why unresolved: While the paper hypothesizes about cultural influences, it does not provide a detailed analysis or solutions to address these discrepancies.
- What evidence would resolve it: A comprehensive study analyzing the impact of cultural and demographic factors on attribution, along with strategies to standardize attribution assessment across languages.

### Open Question 3
- Question: How can large language models like PaLM 2 be further utilized to enhance attribution detection and evaluation metrics for cross-lingual question answering systems?
- Basis in paper: [explicit] The paper demonstrates that PaLM 2 outperforms other models for attribution detection but leaves open the exploration of its potential for reranking and evaluation.
- Why unresolved: The paper suggests future work but does not explore the full capabilities of PaLM 2 in these areas.
- What evidence would resolve it: Experiments showing improved performance in attribution detection and evaluation metrics when using PaLM 2 for reranking and as a benchmark for cross-lingual QA systems.

## Limitations

- The study relies on exact string matching for attribution detection, which may underestimate true attribution rates by missing paraphrased or semantically equivalent answers.
- The attribution mechanisms (memorization vs. translation artifacts) remain speculative without direct causal evidence from ablation studies.
- The small fine-tuning sample size (200 samples) raises concerns about the robustness and generalizability of attribution detection models, particularly for low-resource languages.

## Confidence

High confidence in empirical findings regarding attribution gaps across all five languages, supported by direct measurements showing up to 50% non-attributable answers.

Medium confidence in the proposed mechanisms (language model memorization vs. translation artifacts), as these remain hypotheses without definitive causal evidence.

Medium confidence in the effectiveness of PaLM 2 and NLI models for attribution detection, given the limited fine-tuning data and potential overfitting concerns.

## Next Checks

1. **Causal Analysis of Attribution Gaps**: Conduct ablation studies comparing models with and without access to retrieved passages during inference to quantify the contribution of memorization versus retrieval. This would provide direct evidence for the proposed mechanism and inform architectural improvements.

2. **Robustness Testing Across QA Architectures**: Evaluate attribution detection performance across multiple cross-lingual QA systems (different retrievers, generators, and fine-tuning strategies) to assess the generalizability of findings beyond the CORA system.

3. **Paraphrase-Aware Attribution Evaluation**: Implement semantic similarity measures (e.g., embedding-based matching) alongside exact string matching to capture paraphrased answers and obtain more accurate attribution rates, particularly important for languages with different morphological structures.