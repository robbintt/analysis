---
ver: rpa2
title: Attributing Learned Concepts in Neural Networks to Training Data
arxiv_id: '2310.03149'
source_url: https://arxiv.org/abs/2310.03149
tags:
- concept
- training
- images
- probe
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to attribute concept learning
  in neural networks to training data. The method combines concept probing with data
  attribution, using TRAK to identify which training examples were most important
  for learning specific concepts at different layers of a model.
---

# Attributing Learned Concepts in Neural Networks to Training Data

## Quick Facts
- arXiv ID: 2310.03149
- Source URL: https://arxiv.org/abs/2310.03149
- Authors: 
- Reference count: 10
- Key outcome: Introduces a novel approach combining concept probing with data attribution to identify which training examples were most important for learning specific concepts in neural networks

## Executive Summary
This paper presents a novel approach to understanding how neural networks learn human-interpretable concepts by combining concept probing with data attribution. The method uses TRAK to identify which training examples were most important for learning specific concepts at different layers of a model. Experiments on two concept datasets (snakes and high-low frequencies) with ResNet-18 models trained on ImageNet10p show that concepts are learned in a robust, diffuse manner across exemplars rather than depending on a few critical examples. The study reveals that concept detection accuracy varies with network depth, with snakes showing monotonic increase and high-low frequencies peaking at intermediate layers.

## Method Summary
The method combines linear concept probing with TRAK data attribution to identify which training examples were most important for learning specific concepts. First, ResNet-18 models are trained on ImageNet10p, then linear probes are trained on different layers to detect concepts. TRAK attribution is applied to identify the most important training examples for concept predictions. The study tests robustness by removing top-attributed examples and retraining, and examines interpretability through sparse probing with iterative hard thresholding.

## Key Results
- Concepts are learned in a robust, diffuse manner across exemplars rather than depending on few critical examples
- Concept detection accuracy varies with network depth, with snakes showing monotonic increase and high-low frequencies peaking at intermediate layers
- Sparse probing can identify more interpretable concept directions while maintaining reasonable detection accuracy
- Removing the top 10,000 attributed images for a concept and retraining did not significantly change concept detection accuracy

## Why This Works (Mechanism)

### Mechanism 1
Concepts are learned in a robust, diffuse manner across exemplars rather than depending on few critical examples. TRAK attribution identifies which training images were most important for learning a concept. When the top 10,000 attributed images are removed and the model is retrained, concept detection accuracy does not significantly change, indicating that no small subset of examples is critical for learning the concept. Core assumption: The attribution scores across training examples are relatively similar, so removing a fraction of them won't have a large effect on concept learning.

### Mechanism 2
Concept presence and detection accuracy varies with network depth, with different patterns for different types of concepts. Linear probes are trained on different layers of the network to detect concepts. The accuracy of these probes on validation data indicates how well the concept is encoded at each layer. Snakes concept shows monotonic increase in accuracy with depth, while high-low frequency peaks at intermediate layers. Core assumption: The features learned at different layers capture different levels of abstraction, with deeper layers learning more complex features.

### Mechanism 3
Sparse probing can identify more interpretable concept directions while maintaining reasonable detection accuracy. Probes are trained with an iterative hard thresholding approach, forcing sparsity by keeping only the k parameters of highest absolute value. The accuracy of these sparse probes on validation data indicates how learnable the concept is with sparse representations. Core assumption: Semantically meaningful representations tend to be sparse in the neuron basis, so forcing sparsity can lead to more interpretable concept directions.

## Foundational Learning

- Concept: Linear probing for concept detection
  - Why needed here: Linear probing is used to detect whether a specific human-interpretable concept is encoded in a hidden layer of a model. The accuracy of the probe on validation data indicates how well the concept is learned at that layer.
  - Quick check question: How does the accuracy of a linear probe on validation data relate to the presence of a concept in a network layer?

- Concept: Data attribution methods
  - Why needed here: Data attribution methods are used to quantify the importance of individual training examples for a model's predictions. In this work, TRAK attribution is used to identify which training images were most important for learning a concept at a given layer.
  - Quick check question: What is the difference between concept attribution and data attribution, and how are they combined in this work?

- Concept: Ensemble methods for model training
  - Why needed here: TRAK requires an ensemble of trained models to compute data attribution scores. In this work, an ensemble of 20 models is used, each trained on the same data but with different random seeds.
  - Quick check question: Why is an ensemble of models used for TRAK attribution, and how does this help identify important training examples for concept learning?

## Architecture Onboarding

- Component map: Base model (ResNet-18) -> Concept datasets (Snakes, High-Low Frequencies) -> Linear probes at different layers -> TRAK attribution -> Sparse probing
- Critical path: 1. Train base models on ImageNet10p, 2. Train linear probes on different layers for each concept, 3. Use TRAK to attribute concept predictions to training data, 4. Analyze attribution scores to identify important examples, 5. Retrain models with top-attributed examples removed to test robustness, 6. Train sparse probes to test interpretability
- Design tradeoffs: Using a subset of ImageNet (10%) for base model training vs. full dataset for better accuracy, Number of models in the ensemble for TRAK (20 used here) vs. computational cost, Sparsity level for probing (k=1 to 100) vs. detection accuracy
- Failure signatures: Low concept detection accuracy with linear probes, Highly skewed attribution scores (few examples with very high scores), Significant drop in concept detection accuracy when top-attributed examples are removed, Poor performance of sparse probes compared to non-sparse probes
- First 3 experiments: 1. Train a base ResNet-18 on ImageNet10p and evaluate its accuracy on the validation set, 2. Train linear probes on different layers of the base model for the Snakes concept and evaluate their detection accuracy, 3. Use TRAK to attribute concept predictions to training data for the Snakes concept and analyze the distribution of attribution scores

## Open Questions the Paper Calls Out

### Open Question 1
Do concepts learned by neural networks depend on a few critical training examples or are they more robust to the removal of individual examples? While the paper finds evidence for robustness, the experiments only test removal of up to 10,000 images (out of 1.2 million total) and use only two specific concepts on one dataset.

### Open Question 2
How similar are the concept representations learned by different independently trained models? The paper mentions that "different probes typically converge to similar performance" but doesn't directly measure similarity of concept representations.

### Open Question 3
Is the observed robustness of concept learning specific to TRAK attribution method or would other attribution methods show similar results? The paper states "we only use TRAK for data attribution, and it would be interesting to know the extent to which our experimental results are particular to TRAK."

### Open Question 4
Do sparse probes capture the same concepts as dense probes, or do they learn fundamentally different representations? The paper trains sparse probes and observes different attribution patterns but doesn't systematically compare the concepts learned.

### Open Question 5
How does concept learning change with network depth and what determines the optimal layer for detecting specific concepts? The paper observes different depth patterns for snakes (monotonic increase) vs. high-low frequencies (peaking at intermediate layers) but doesn't explain why.

## Limitations
- Findings based on ResNet-18 models trained on ImageNet10p (10% of ImageNet) may not generalize to larger or differently structured models
- Concept datasets (snakes and high-low frequencies) represent specific types of visual concepts, results may differ for other concept categories
- TRAK attribution method relies on ensemble models with different random seeds, but optimal ensemble size and seed variability are not fully explored

## Confidence
**High Confidence:** The observation that concept learning is robust and diffuse across training examples is well-supported by experimental results showing no significant change in concept detection accuracy after removing top-attributed images.

**Medium Confidence:** The interpretation that sparse probing leads to more interpretable concept directions while maintaining reasonable detection accuracy is supported by results but requires further validation with additional concept types and model architectures.

**Low Confidence:** The generalization of findings to other model architectures beyond ResNet-18 and to other datasets beyond the two concept types tested remains uncertain without additional experiments.

## Next Checks
1. Replicate the experiment with a larger ensemble size (e.g., 50 models) to verify the stability of TRAK attribution scores and their impact on concept robustness conclusions.

2. Test the same methodology on a different model architecture (e.g., Vision Transformer) to assess whether the depth-dependent concept detection patterns hold across architectures.

3. Conduct experiments with additional concept types (e.g., texture, color, shape) to determine if the observed patterns of concept learning and attribution generalize beyond the snakes and frequency concepts used in this study.