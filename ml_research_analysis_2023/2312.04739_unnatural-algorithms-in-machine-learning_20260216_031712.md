---
ver: rpa2
title: Unnatural Algorithms in Machine Learning
arxiv_id: '2312.04739'
source_url: https://arxiv.org/abs/2312.04739
tags:
- group
- natural
- learning
- gradient
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a categorical framework for analyzing the parameterization
  invariance of machine learning optimization algorithms. The key insight is that
  natural gradient descent can be viewed as a discrete approximation of a natural
  transformation between functors, which explains its robust training behavior across
  network reparameterizations.
---

# Unnatural Algorithms in Machine Learning

## Quick Facts
- arXiv ID: 2312.04739
- Source URL: https://arxiv.org/abs/2312.04739
- Authors: 
- Reference count: 6
- Key outcome: This paper develops a categorical framework for analyzing the parameterization invariance of machine learning optimization algorithms, showing that natural gradient descent is natural while most other algorithms are not.

## Executive Summary
This paper introduces a categorical framework to analyze the parameterization invariance of machine learning optimization algorithms. The key insight is that natural gradient descent can be viewed as a discrete approximation of a natural transformation between functors, explaining its robust training behavior across network reparameterizations. The paper demonstrates that most popular optimization algorithms (gradient descent, Nesterov's accelerated gradient, Adam, Newton's method) are "unnatural" due to hidden unnatural isomorphisms between tangent and cotangent bundles, while algorithms using generalized Gauss-Newton matrices to precondition gradients are shown to be natural. The framework provides a foundation for analyzing limiting behavior as network size increases and derives accelerated natural training equations.

## Method Summary
The paper establishes a categorical framework where optimization algorithms are analyzed through their behavior under reparameterizations of the neural network configuration manifold. By treating the configuration manifold as a smooth manifold and examining how algorithms transform under diffeomorphisms, the authors characterize naturality through the lens of natural transformations between functors. The method involves identifying the state space functors for different optimizers, deriving their limiting ODEs as learning rate approaches zero, and checking whether these flows satisfy the commutativity requirement of natural transformations. For algorithms found to be unnatural, the paper identifies their "naturalizer" - the set of reparameterizations that preserve equivariant behavior.

## Key Results
- Natural gradient descent is natural (parameterization invariant) as it can be viewed as a discrete approximation of a natural transformation between functors
- Most popular optimization algorithms (gradient descent, Nesterov's accelerated gradient, Adam, Newton's method) are unnatural due to hidden unnatural isomorphisms between tangent and cotangent bundles
- Algorithms using generalized Gauss-Newton matrices to precondition gradients are natural because these matrices provide natural isomorphisms between tangent and cotangent bundles
- The framework provides a foundation for analyzing limiting behavior as network size increases and derives accelerated natural training equations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural gradient descent maintains parameterization invariance through a natural transformation between functors
- Mechanism: The paper establishes that natural gradient descent can be viewed as a discrete approximation of a natural transformation between functors. Specifically, it maps between the functor determining an optimizer's state space from the diffeomorphism group of its configuration manifold, and the functor determining that state space's tangent bundle from this group. This categorical structure ensures that the algorithm's flow in the limit of vanishing learning rate is invariant under smooth reparameterizations.
- Core assumption: The configuration manifold of a neural network is a smooth manifold, and the training algorithm converges to a flow that can be characterized as an infranatural transformation
- Evidence anchors:
  - [abstract]: "We show that optimization algorithms with this property can be viewed as discrete approximations of natural transformations from the functor determining an optimizer's state space from the diffeomorphism group if its configuration manifold, to the functor determining that state space's tangent bundle from this group."
  - [section]: "We see that gradient descent is not natural, since it converges to gradient flow, which is not a component of a natural transformation, since it does not obey the commutativity requirement of natural transformations."
- Break condition: If the neural network's configuration manifold is not a smooth manifold, or if the algorithm does not converge to a flow in the small learning rate limit, the natural transformation framework breaks down.

### Mechanism 2
- Claim: Most popular optimization algorithms are "unnatural" due to hidden unnatural isomorphisms between tangent and cotangent bundles
- Mechanism: The paper demonstrates that algorithms like gradient descent, Nesterov's accelerated gradient, Adam, and Newton's method (without affine connections) are unnatural because they implicitly rely on isomorphisms between tangent and cotangent bundles that are not natural in the categorical sense. These isomorphisms depend on the basis induced by the network's parameterization, which breaks the equivariance required for natural transformations.
- Core assumption: The gradient of the loss function is a section of the cotangent bundle T*M, while parameter updates are sections of the tangent bundle TM, and these bundles are isomorphic but not naturally so
- Evidence anchors:
  - [section]: "As such, we see that gradient descent is not natural, since it converges to gradient flow, which is not a component of a natural transformation, since it does not obey the commutativity requirement of natural transformations. This unnaturality can be seen by the fact that the left hand side of equation 1 is a tangent vector field, i.e. a section of the tangent bundle TM, while the right hand side is a section of the cotangent bundle T*M."
  - [section]: "Because of the square root appearing in the denominator, if we rescale the gradient ∂L/∂θ → γ ∂L/∂θ, with γ > 0, then m̂ → γm̂ and v̂ → γ²v̂, hence dθ/dξ is unchanged. For γ < 0, a sign change is introduced."
- Break condition: If the algorithm explicitly uses a natural isomorphism between tangent and cotangent bundles (such as the Fisher information matrix in natural gradient descent), or if the loss function has special properties that make the unnatural isomorphism irrelevant.

### Mechanism 3
- Claim: Algorithms using generalized Gauss-Newton matrices to precondition gradients are natural
- Mechanism: The paper shows that when algorithms use the inverse of generalized Gauss-Newton matrices to precondition gradients, they become natural because these matrices provide a natural isomorphism between the tangent and cotangent bundles. The generalized Gauss-Newton matrix is a section of T*M ⊗ T*M, which transforms appropriately under reparameterizations, preserving the equivariance required for natural transformations.
- Core assumption: The generalized Gauss-Newton matrix is invertible and provides a natural way to map between tangent and cotangent spaces
- Evidence anchors:
  - [section]: "Assuming that Gij is invertible, it is easy to see that an evolution of the form dθi/dξ = -∑j(G^-1)ij ∂L/∂θj transforms to dθi/dξ = -∑j(Ḡ^-1)ij ∂L/∂θj, which is precisely the same flow that we obtain by transforming to the barred parameters first and then computing the evolution, thanks to the transformation law for the inverse of the Fisher information matrix."
  - [section]: "As such, upon a reparameterization, it transforms as Ḡij = ∑k,l(∂θk/∂θi)Gkl(∂θl/∂θj)."
- Break condition: If the generalized Gauss-Newton matrix is not invertible, or if the preconditioning matrix does not transform appropriately under reparameterizations, the algorithm loses its naturality.

## Foundational Learning

- Category Theory:
  - Why needed here: The paper uses category theory to formalize the notion of natural transformations and equivariance, which are central to understanding why natural gradient descent works. Concepts like functors, natural transformations, and automorphism groups are used to characterize the behavior of optimization algorithms.
  - Quick check question: What is the difference between a natural transformation and an infranatural transformation?

- Differential Geometry:
  - Why needed here: The paper relies on concepts from differential geometry, such as manifolds, tangent bundles, and cotangent bundles, to describe the configuration space of neural networks and how optimization algorithms navigate this space. Understanding these concepts is crucial for grasping why certain algorithms are "unnatural."
  - Quick check question: Why is there no natural isomorphism between the tangent and cotangent bundles of a manifold?

- Group Theory:
  - Why needed here: Group theory is used to formalize the notion of equivariance and symmetry in the context of optimization algorithms. Concepts like group actions, automorphisms, and equivariant functions are used to characterize the behavior of algorithms under reparameterizations.
  - Quick check question: What is the relationship between group actions and natural transformations in this context?

## Architecture Onboarding

- Component map:
  - Mathematical framework: Category theory, differential geometry, and group theory
  - Core algorithm: Natural gradient descent and generalized Gauss-Newton methods
  - Implementation considerations: Numerical integration schemes, matrix inversion, and preconditioning

- Critical path:
  1. Define the configuration manifold of the neural network
  2. Choose an appropriate preconditioning matrix (e.g., Fisher information matrix or generalized Gauss-Newton matrix)
  3. Implement the natural gradient descent algorithm with numerical integration
  4. Test the algorithm on benchmark datasets and compare with standard methods

- Design tradeoffs:
  - Computational cost: Natural gradient descent and generalized Gauss-Newton methods are more computationally expensive than standard gradient descent due to the need to compute and invert large matrices
  - Numerical stability: The preconditioning matrices may be ill-conditioned, requiring careful numerical treatment
  - Flexibility: The framework can be applied to a wide range of neural network architectures and optimization problems

- Failure signatures:
  - Poor convergence: If the preconditioning matrix is not well-suited to the problem, the algorithm may converge slowly or get stuck in local minima
  - Numerical instability: If the preconditioning matrix is ill-conditioned, the algorithm may produce numerical errors or diverge
  - Loss of naturality: If the implementation does not properly handle reparameterizations, the algorithm may lose its invariance properties

- First 3 experiments:
  1. Implement natural gradient descent for a simple feedforward neural network on a benchmark dataset (e.g., MNIST) and compare its performance with standard gradient descent
  2. Implement a generalized Gauss-Newton method for a convolutional neural network on a computer vision task (e.g., CIFAR-10) and compare its performance with natural gradient descent
  3. Test the invariance properties of the algorithms by applying different reparameterizations to the network and verifying that the optimization behavior is unchanged

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a functorial discretization scheme be developed to preserve naturality properties at finite learning rates?
- Basis in paper: [explicit] The paper notes that naturality breaks down at finite learning rates and suggests geometric integration schemes could help preserve structure.
- Why unresolved: Current numerical integration methods introduce discretization errors that destroy the equivariant properties, and no existing scheme explicitly maintains natural transformation properties.
- What evidence would resolve it: A working numerical integration algorithm that provably preserves the natural transformation structure and maintains reparameterization invariance at practical learning rates.

### Open Question 2
- Question: How can non-isomorphic networks be compared through training algorithms when projection maps between architectures are nontrivial?
- Basis in paper: [explicit] The paper discusses the challenge of defining projection maps for non-isomorphic networks to enable comparison through inverse limits.
- Why unresolved: Neural network parameterization is nonlinear, making it unclear how to define consistent projection maps that would allow training behavior comparison.
- What evidence would resolve it: A formal construction of projection maps between network architectures that preserve training dynamics, demonstrated through empirical validation.

### Open Question 3
- Question: What quantitative measures can predict when natural gradient descent will outperform standard gradient descent in poorly parameterized networks?
- Basis in paper: [inferred] The paper shows natural gradient descent performs better in poorly parameterized networks but doesn't provide criteria for when this advantage is significant.
- Why unresolved: While the mathematical framework explains reparameterization invariance, it doesn't provide practical metrics for assessing parameterization quality or predicting performance gains.
- What evidence would resolve it: A metric correlating network parameterization characteristics (e.g., condition number of Fisher information) with performance gains when using natural gradient methods.

## Limitations
- The framework assumes idealized continuous-time dynamics, but real implementations use discrete updates with finite learning rates
- The practical benefits of natural algorithms (in terms of convergence speed, generalization, etc.) are not quantified
- The computational overhead of maintaining naturality (computing and inverting preconditioning matrices) may outweigh theoretical benefits

## Confidence
- High confidence: The mathematical derivation showing standard gradient descent is not natural (due to the unnatural isomorphism between tangent and cotangent bundles)
- Medium confidence: The claim that generalized Gauss-Newton preconditioning produces natural algorithms, as this relies on the invertibility of G⁻¹ which may fail in practice
- Medium confidence: The assertion that natural transformations provide better training behavior, as empirical validation is limited to theoretical arguments

## Next Checks
1. **Empirical naturality test**: Implement a parameterized neural network where the same model can be expressed with different parameterizations. Train using both standard gradient descent and natural gradient descent, then compare training trajectories to verify invariance properties predicted by the categorical framework.

2. **Convergence speed comparison**: For a range of benchmark tasks, compare the convergence properties of natural gradient descent (with appropriate preconditioning) against standard optimizers. Measure both wall-clock time and number of iterations to reach specified accuracy levels.

3. **Numerical stability analysis**: Systematically evaluate the condition number of Fisher information matrices across different network architectures and parameterizations. Identify when and why the preconditioning matrices become ill-conditioned, and test regularization strategies to maintain naturality while ensuring numerical stability.