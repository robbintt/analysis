---
ver: rpa2
title: How Well Do Large Language Models Truly Ground?
arxiv_id: '2311.09069'
source_url: https://arxiv.org/abs/2311.09069
tags:
- grounding
- context
- performance
- contexts
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a stricter definition of grounding for large
  language models (LLMs) that evaluates whether responses fully utilize knowledge
  from provided contexts without exceeding their scope. To measure this, the authors
  propose a new dataset with factors like entity popularity and context length, along
  with a grounding metric based on atomic fact decomposition.
---

# How Well Do Large Language Models Truly Ground?

## Quick Facts
- **arXiv ID**: 2311.09069
- **Source URL**: https://arxiv.org/abs/2311.09069
- **Reference count**: 40
- **Key outcome**: Introduces stricter grounding definition for LLMs, shows instruction tuning has stronger impact than model size on grounding performance, and demonstrates that answer accuracy doesn't guarantee strong grounding

## Executive Summary
This paper presents a novel framework for evaluating how well large language models (LLMs) ground their responses in provided external contexts. Unlike previous approaches that simply check if context is mentioned, this work defines grounding more strictly as whether responses fully utilize knowledge from contexts without exceeding their scope. Through experiments with 13 diverse LLMs, the authors find that instruction tuning and reinforcement learning from human feedback (RLHF) have more pronounced effects on grounding performance than model size alone. Surprisingly, larger models often show greater performance drops when given revised or distracting contexts, and high answer accuracy does not ensure strong grounding.

## Method Summary
The study introduces a new dataset with 480 instances of questions, answers, contexts, and gold atomic facts from Wikipedia, along with revised versions containing modified facts and additional distractor contexts. Grounding performance is measured using an automatic metric based on atomic fact decomposition and an NLI-based evaluation approach using the MiniLM model. The evaluation assesses whether atomic facts from the context are present in generated responses through precision and recall calculations. The experiments test four dataset scenarios (Original-Gold, Revised-Gold, Original-Dist, Revised-Dist) across 13 LLMs of varying sizes and training methods.

## Key Results
- Instruction tuning and RLHF have more pronounced impact on grounding performance than model size
- Larger models often show greater performance drops when given revised or distracting contexts
- High answer accuracy does not ensure high grounding performance
- Open-source models perform comparably to proprietary ones in grounding tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Models with instruction tuning and RLHF demonstrate stronger grounding ability than larger models without these training methods.
- **Mechanism**: Instruction tuning and RLHF teach models to follow instructions and maintain coherence with provided context, making them more likely to utilize external context rather than relying on parametric knowledge.
- **Core assumption**: The training signals from instruction tuning and RLHF explicitly encourage context adherence over parametric knowledge generation.
- **Evidence anchors**:
  - [abstract]: "Training methods (Instruction Tuning & RLHF) have a more pronounced impact on grounding performance than the size of the model."
  - [section 4.2]: "The left figure in Figure 3 shows that model size tends to show a small effect on the grounding performance of Original-Gold, but how the model was tuned tends to show a stronger effect; for high grounding performance, instruction tuning seems to be necessary and RLHF also seems to help."
  - [corpus]: Weak - only 25 related papers found, no direct evidence on training method effects on grounding.
- **Break condition**: If instruction-tuned models start generating responses that heavily rely on parametric knowledge despite explicit instructions to use provided context, this mechanism would break down.

### Mechanism 2
- **Claim**: Models show greater performance degradation when given revised or distracting contexts, with larger models showing higher reduction rates.
- **Mechanism**: Larger models have more parametric knowledge, making them more likely to rely on this knowledge rather than the provided context, especially when the context contains modified or irrelevant information.
- **Core assumption**: Parametric knowledge becomes increasingly dominant in larger models, leading them to default to this knowledge when external context is less reliable.
- **Evidence anchors**:
  - [abstract]: "larger models often show greater performance drops when given revised or distracting contexts."
  - [section 4.2]: "Figure 5 shows that larger models experience greater degradation in grounding performance with Revised-Gold than Original-Gold when experimenting over various sizes of TÜLU."
  - [section 4.2]: "Figure 17 demonstrates that larger models tend to show higher degradation when distractor contexts are added."
- **Break condition**: If smaller models consistently outperform larger models across all context conditions, suggesting size is inversely related to grounding ability.

### Mechanism 3
- **Claim**: Answer accuracy does not guarantee strong grounding performance.
- **Mechanism**: Models can generate correct answers by drawing on parametric knowledge without properly utilizing the provided context, leading to high answer accuracy but low grounding scores.
- **Core assumption**: The evaluation of answer accuracy only checks for the presence of correct information, not its source or completeness of context utilization.
- **Evidence anchors**:
  - [abstract]: "High answer accuracy, commonly used to assess how well a model incorporates context in previous works, does not ensure high grounding performance."
  - [section 4.2]: "However, though there is a correlation between grounding performance (Table 1) and answer accuracy (Table 10), high answer accuracy does not ensure high grounding performance as grounding performance in the same range of answer accuracy highly diverges."
- **Break condition**: If grounding performance and answer accuracy become perfectly correlated across all models and contexts, this mechanism would no longer hold.

## Foundational Learning

- **Concept: Atomic Fact Decomposition**
  - Why needed here: Grounding performance requires evaluating whether individual pieces of knowledge from the context are present in the response, not just the overall answer.
  - Quick check question: Can you break down the sentence "Napoleon is a French general" into its atomic facts?

- **Concept: NLI-based Evaluation**
  - Why needed here: To determine whether knowledge from the context is present in the generated response, we need a way to measure semantic similarity beyond exact string matching.
  - Quick check question: How would you use an NLI model to check if the statement "The sky is blue" is entailed by the context "On a clear day, the sky appears blue"?

- **Concept: Context Popularity Effects**
  - Why needed here: Understanding how the popularity of context topics affects model behavior is crucial for designing robust grounding systems.
  - Quick check question: Why might a model perform better on less popular topics when provided with external context?

## Architecture Onboarding

- **Component map**: Context Processor -> Response Generator -> Response -> Grounding Evaluator -> Grounding Score
- **Critical path**: Context → Response Generator → Response → Grounding Evaluator → Grounding Score
- **Design tradeoffs**:
  - Granularity vs. efficiency: More atomic facts provide finer evaluation but increase computational cost
  - Context length vs. model performance: Longer contexts may improve grounding but could overwhelm model capacity
  - Evaluation model choice: Different Meval models may yield varying correlation with human judgment

- **Failure signatures**:
  - High answer accuracy but low grounding scores
  - Consistent performance degradation with distractor contexts
  - Size-dependent performance differences across context conditions

- **First 3 experiments**:
  1. Test grounding performance on simple fact-based questions with single context vs. multiple contexts
  2. Compare grounding scores for popular vs. less popular topics using the same models
  3. Evaluate performance degradation when adding distractor contexts of varying relevance levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the grounding performance of LLMs vary when provided with private or proprietary contexts, as opposed to publicly available sources like Wikipedia?
- Basis in paper: The paper acknowledges the limitation of using Wikipedia as the primary source for contexts, noting that it may have been used during pretraining. The authors suggest collecting datasets with private contexts as future work.
- Why unresolved: The paper does not experiment with private or proprietary contexts, leaving the impact of such contexts on grounding performance unexplored.
- What evidence would resolve it: Conducting experiments with datasets containing private or proprietary contexts and comparing the grounding performance of LLMs to that observed with publicly available sources.

### Open Question 2
- Question: To what extent does the performance of the evaluation model (Meval) influence the accuracy and reliability of the grounding performance metrics?
- Basis in paper: The paper mentions that the performance of Meval, a model-based evaluation approach, could be influenced by its own performance, potentially affecting the outcome of grounding performance evaluations.
- Why unresolved: The paper does not provide a detailed analysis of how variations in Meval's performance impact the grounding performance metrics.
- What evidence would resolve it: Conducting experiments to assess the sensitivity of grounding performance metrics to changes in Meval's performance, and exploring alternative evaluation methods.

### Open Question 3
- Question: How do larger context sizes, beyond the typical 2048 token limit, affect the grounding performance of LLMs?
- Basis in paper: The paper notes that due to the 2048 token limit in open-source models, it could not cover scenarios involving a large number of contexts as input.
- Why unresolved: The paper does not explore the impact of larger context sizes on grounding performance, leaving this aspect unexplored.
- What evidence would resolve it: Conducting experiments with models that support larger context sizes or using techniques to handle longer contexts, and evaluating the grounding performance in these scenarios.

## Limitations
- The grounding evaluation relies on a single Meval model (MiniLM), though different models may yield varying correlation with human judgment
- The dataset size of 480 instances may not capture the full complexity of real-world grounding scenarios
- The focus on English Wikipedia-based content limits generalizability to other languages and domains

## Confidence

**High confidence**: The observation that answer accuracy does not guarantee strong grounding performance (supported by direct comparison data in Tables 1 and 10)

**Medium confidence**: The finding that instruction tuning and RLHF have stronger impact on grounding than model size (based on Figure 3 analysis, but with potential confounding factors from different model architectures)

**Medium confidence**: The claim that larger models show greater performance degradation with revised or distracting contexts (observed across TÜLU variants, but may reflect training data differences rather than size effects)

## Next Checks

1. **Cross-model validation**: Repeat the grounding experiments using multiple Meval models to verify that results are not specific to MiniLM's judgment patterns.

2. **Scaling analysis**: Test additional models with identical training methods but varying sizes to isolate the effects of model size from architectural differences.

3. **Domain generalization**: Evaluate grounding performance on non-Wikipedia datasets (e.g., scientific papers, news articles) to assess the robustness of findings across different content types and knowledge domains.