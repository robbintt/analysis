---
ver: rpa2
title: On the stability, correctness and plausibility of visual explanation methods
  based on feature importance
arxiv_id: '2311.12860'
source_url: https://arxiv.org/abs/2311.12860
tags:
- explanation
- metrics
- methods
- evaluation
- correctness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates visual explanation methods for image classifiers
  by examining their stability, correctness, and plausibility. It identifies limitations
  in existing evaluation metrics, showing they do not always agree and may penalize
  correct methods in unstable regions.
---

# On the stability, correctness and plausibility of visual explanation methods based on feature importance

## Quick Facts
- arXiv ID: 2311.12860
- Source URL: https://arxiv.org/abs/2311.12860
- Reference count: 35
- Key outcome: Visual explanation methods for image classifiers are evaluated using new metrics that account for local model behavior, revealing that existing metrics often disagree and may penalize correct methods in unstable regions.

## Executive Summary
This paper critically evaluates visual explanation methods for image classifiers by examining their stability, correctness, and plausibility. The authors identify fundamental limitations in existing evaluation metrics, showing they often disagree and may unfairly penalize correct explanation methods when operating in unstable model regions. To address these issues, they propose new metrics—Local Surrogate Stability (LSS) for stability and Local Relative Correctness (LRC) for correctness—that explicitly account for local model behavior. Experiments on the Salicon50 dataset demonstrate that the proposed metrics show better consistency and correlation with correctness than their predecessors, highlighting the need for more sophisticated evaluation frameworks in explainable AI.

## Method Summary
The authors evaluate nine visual explanation methods (Grad-CAM, Gradients, Integrated Gradients, SmoothGrads, GBP, FEM, ML-FEM, Fake-CAM, and CB-CAM) on the Salicon50 dataset using ResNet50. They generate perturbed samples within an L2 radius ε=250 around each image using both uniform and adversarial strategies. For each method, they compute explanation maps and evaluate them using existing metrics (LIP, CLE, PCC, SIM) and proposed metrics (LSS, LRC). The evaluation pipeline involves generating explanations, building local surrogate models to approximate model behavior, and computing metric scores. They measure consensus between metrics using Spearman rank correlation and analyze the impact of sampling strategies on metric consistency.

## Key Results
- Existing evaluation metrics for stability, correctness, and plausibility often disagree, with consensus correlation ranging from 0.55 to 0.86.
- The proposed LSS metric shows significantly better robustness to adversarial samples than LIP, with 0.84 correlation to correctness compared to LIP's 0.48.
- LRC demonstrates improved consistency over CLE when using adversarial sampling strategies.
- Metrics disagree most strongly in regions where the model is unstable, suggesting current metrics may penalize correct methods in these areas.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluation metrics for visual explanation methods often disagree because they measure different properties that are not always aligned with each other.
- Mechanism: The paper identifies that stability, correctness, and plausibility are three key properties of explanations, but existing metrics for these properties use different definitions and assumptions. For example, stability metrics like LIP assume explanations should be similar for similar inputs, while correctness metrics like CLE assume explanations should accurately predict model behavior in local neighborhoods. These different assumptions lead to conflicting evaluations of the same explanation method.
- Core assumption: Different evaluation metrics have fundamentally different definitions of what makes a "good" explanation, and these definitions are not always compatible.
- Evidence anchors:
  - [abstract] "It shows that existing metrics for evaluating these properties may disagree, raising the issue of what constitutes a good evaluation metric for explanations."
  - [section] "However, this metric does not take into account the possible instability of the model g in the neighbourhood of X0."
- Break condition: If the properties of stability, correctness, and plausibility could be shown to be fundamentally aligned or if a unified metric could be developed that captures all three properties simultaneously.

### Mechanism 2
- Claim: Local behavior of the model affects the validity of evaluation metrics, particularly for stability and correctness.
- Mechanism: The paper proposes new metrics (LSS for stability and LRC for correctness) that take into account the local behavior of the model. These metrics recognize that in regions where the model is unstable (e.g., near decision boundaries), traditional metrics may penalize correct explanation methods. The new metrics measure stability relative to model behavior changes and correctness relative to local model output variations.
- Core assumption: Model behavior is not uniform across the input space, and evaluation metrics need to account for local variations in model stability.
- Evidence anchors:
  - [abstract] "Finally, in the particular case of stability and correctness, it shows the limitations of some existing metrics and proposes new ones that take into account the local behaviour of the model under test."
  - [section] "Since saliency maps aim to capture the local behaviour of the model, a correct explanation method applied to an unstable model might return a high value LIP (X0) and be discarded in favour of a less correct, but seemingly more 'stable' method."
- Break condition: If model behavior were uniform across the input space, making local variations irrelevant for evaluation metrics.

### Mechanism 3
- Claim: Sample-based evaluation metrics are sensitive to the choice of perturbed samples, affecting their consistency and reliability.
- Mechanism: The paper demonstrates that sample-based metrics (LIP, LSS, CLE, LRC) can produce different results depending on whether uniform or adversarial sampling strategies are used. Adversarial sampling helps identify regions of model instability and acts as a sanity check for evaluation metrics. The proposed LSS and LRC metrics show improved consistency compared to their predecessors when using different sampling strategies.
- Core assumption: The choice of perturbed samples significantly influences the evaluation of explanation methods, and adversarial sampling provides a more rigorous test of metric robustness.
- Evidence anchors:
  - [section] "When using the adversarial strategy, each sample ˜X is generated with the goal of minimizing the value g(˜X) as follows..."
  - [section] "Table 3 indicates that while our LRC metric offers a marginal improvement in consistency over CLE, our LSS metric is significantly more robust to adversarial samples than LIP."
- Break condition: If evaluation metrics were shown to be insensitive to the choice of perturbed samples, making sampling strategy irrelevant.

## Foundational Learning

- Concept: Local surrogate models
  - Why needed here: The paper builds surrogate models to approximate local model behavior for evaluating explanation methods. Understanding how these surrogates work is crucial for grasping the proposed LSS and LRC metrics.
  - Quick check question: How does the surrogate model E_X0(X) = s(X0)^T(X - X0) + g(X0) approximate the local behavior of the original model g?

- Concept: Lipschitz continuity and stability metrics
  - Why needed here: The LIP metric is based on Lipschitz continuity, which measures how much explanations change with small input perturbations. Understanding this concept is essential for comparing LIP with the proposed LSS metric.
  - Quick check question: What does it mean for an explanation method to have low Lipschitz constant, and why might this not always indicate a good explanation?

- Concept: Spearman Rank Correlation Coefficient (SR)
  - Why needed here: The paper uses SR to measure consensus between different evaluation metrics. Understanding SR helps interpret the results showing lack of agreement between metrics.
  - Quick check question: What does a Spearman correlation coefficient close to 1 or -1 indicate about the relationship between two sets of metric scores?

## Architecture Onboarding

- Component map: ResNet50 classifier -> Explanation methods (Grad-CAM, Gradients, Integrated Gradients, etc.) -> Evaluation metrics (LIP, LSS, CLE, LRC, PCC, SIM) -> Sampling strategies (uniform and adversarial)

- Critical path: For evaluating an explanation method: 1) Generate explanation s(X0) for input X0, 2) Generate perturbed samples using uniform or adversarial strategy, 3) Build surrogate models EX0 and E_˜X, 4) Compute LSS and LRC metrics using the surrogate models and perturbed samples.

- Design tradeoffs: The paper trades computational efficiency for more accurate evaluation by using local surrogate models instead of direct model evaluations. The choice between uniform and adversarial sampling represents a tradeoff between computational simplicity and rigorous testing of metric robustness.

- Failure signatures: Metrics disagreeing on which explanation method is best, high variance in metric scores across different sampling strategies, explanation methods performing well on one metric but poorly on others, and metrics being insensitive to model instability in local regions.

- First 3 experiments:
  1. Reproduce the LIP and LSS stability metric comparison on Salicon50 dataset to verify that LSS is less sensitive to model instability.
  2. Test the consistency of CLE and LRC metrics using both uniform and adversarial sampling to confirm the improved consistency of LRC.
  3. Evaluate the consensus between all metrics (stability, correctness, plausibility) on a subset of Salicon50 to replicate the finding that metrics often disagree.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the formal definition of a "correct" explanation method in the context of visual classification tasks, and how can it be consistently measured across different evaluation metrics?
- Basis in paper: [explicit] The paper discusses the limitations of current correctness metrics and the lack of consensus between different evaluation methods, highlighting the need for a formal definition of correctness.
- Why unresolved: Existing correctness metrics (e.g., deletion, insertion, CLE) do not always agree, and there is no universally accepted formal definition of what constitutes a correct explanation.
- What evidence would resolve it: A formal definition of correctness that aligns with multiple evaluation metrics and can be consistently applied across different explanation methods and models.

### Open Question 2
- Question: How can evaluation metrics for explanation methods be improved to account for the local behavior of the model, particularly in regions of high instability or adversarial perturbations?
- Basis in paper: [explicit] The paper proposes new metrics (LSS and LRC) that take into account the local behavior of the model, addressing the limitations of existing stability and correctness metrics.
- Why unresolved: Current metrics do not adequately consider the model's local behavior, leading to potential penalization of correct explanation methods in unstable regions.
- What evidence would resolve it: Experimental results demonstrating that the proposed metrics (LSS and LRC) consistently outperform existing metrics in evaluating stability and correctness, especially in regions of model instability.

### Open Question 3
- Question: Can explanations generated from a classifier trained using ground-truth gaze fixation density maps (GFDMs) as auxiliary annotations reconcile the properties of stability, correctness, and plausibility?
- Basis in paper: [inferred] The paper suggests that training a classifier with GFDMs as auxiliary annotations could potentially reconcile the three properties, but this hypothesis remains untested.
- Why unresolved: The paper does not provide experimental evidence on whether training with GFDMs improves the consensus between stability, correctness, and plausibility metrics.
- What evidence would resolve it: Comparative experiments showing that explanations from a classifier trained with GFDMs achieve higher consensus between stability, correctness, and plausibility metrics compared to standard training.

## Limitations
- Implementation details for proposed LSS and LRC metrics are not fully specified, requiring careful interpretation for accurate reproduction.
- Results are based on a single dataset (Salicon50) and architecture (ResNet50), limiting generalizability to other domains and models.
- The impact of sampling strategy on different explanation methods is not fully characterized across all tested methods.

## Confidence
- High confidence: The identification of disagreement between existing evaluation metrics and the demonstration that they may penalize correct methods in unstable regions.
- Medium confidence: The proposed LSS and LRC metrics represent improvements over existing metrics, though exact implementation details are unclear.
- Low confidence: Generalization of findings to other datasets, models, and explanation methods beyond those tested.

## Next Checks
1. Implement and verify LSS/LRC metrics: Carefully implement the LSS and LRC metrics using the provided equations, ensuring correct handling of local surrogate models and perturbed samples.
2. Cross-dataset validation: Test the proposed metrics on additional datasets (e.g., ImageNet validation set) to verify robustness across different domains and model architectures.
3. Ablation study on sampling strategies: Systematically vary the number of perturbed samples and sampling distributions to quantify their impact on metric stability and identify optimal sampling parameters.