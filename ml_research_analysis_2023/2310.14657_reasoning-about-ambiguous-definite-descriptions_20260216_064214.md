---
ver: rpa2
title: Reasoning about Ambiguous Definite Descriptions
arxiv_id: '2310.14657'
source_url: https://arxiv.org/abs/2310.14657
tags:
- reasoning
- language
- ambiguity
- ambiguous
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark for evaluating the ability
  of language models to resolve ambiguity through explicit reasoning. The dataset
  consists of ambiguous definite descriptions involving de dicto/de re ambiguities,
  where a temporal operator creates two possible interpretations.
---

# Reasoning about Ambiguous Definite Descriptions

## Quick Facts
- arXiv ID: 2310.14657
- Source URL: https://arxiv.org/abs/2310.14657
- Reference count: 5
- Primary result: GPT-4 with chain-of-thought prompting achieves 83.2% accuracy on de dicto/de re ambiguity resolution

## Executive Summary
This paper introduces a new benchmark for evaluating language models' ability to resolve de dicto/de re ambiguities through explicit reasoning. The dataset consists of ambiguous definite descriptions involving temporal operators, where a sentence like "In 2010, the pope was a native German speaker" can have two interpretations depending on whether "the pope" refers to the pope at that time or to the current pope being a German speaker in 2010. The authors generate instances using Wikidata, ensuring all necessary information for disambiguation is included in the prompt. They evaluate several large language models using both direct and chain-of-thought prompting approaches.

## Method Summary
The authors generated a dataset of 500 ambiguous definite descriptions by combining property pairs from Wikidata, creating sentences with temporal operators that create de dicto/de re ambiguities. For each instance, they included all relevant facts about the entities involved in the prompt, then randomly assigned which interpretation (de dicto or de re) was correct. They evaluated four language models (Koala-13B, OpenAssistant-33B, GPT-3.5, and GPT-4) using both direct prompting and chain-of-thought prompting, where the model is instructed to reason step-by-step before answering. Performance was measured by accuracy in identifying the correct interpretation.

## Key Results
- GPT-4 with chain-of-thought prompting achieves the highest accuracy at 83.2%
- All models benefit from chain-of-thought prompting compared to direct prompting
- De re instances are significantly harder than de dicto instances, with only GPT-4 + CoT performing above random guessing
- Models often fail to consider the de re interpretation and may hallucinate additional conditions for correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including all necessary disambiguation information in the prompt forces model errors to stem from reasoning failure rather than missing knowledge.
- Mechanism: By embedding premises about both entities and their temporal relations, the model has access to the complete logical structure needed to resolve de dicto vs de re ambiguity. If the model fails, it must be because it cannot apply the reasoning step correctly.
- Core assumption: The model's knowledge is sufficient for the task; failure is due to reasoning ability, not factual gaps.
- Evidence anchors:
  - [abstract] "Our method includes all information required to resolve the ambiguity in the prompt, which means a model does not require anything but reasoning to do well."
  - [section 4] "We include all facts relevant to the main entity in each prompt whether they are necessary to resolve the ambiguity or not, this allows us to measure if models can work around distractor facts."
  - [corpus] Weak: No direct corpus evidence for this mechanism; the paper's design is the primary anchor.

### Mechanism 2
- Claim: Chain-of-thought prompting improves reasoning performance by structuring the model's thought process.
- Mechanism: CoT instructions encourage the model to consider both interpretations step-by-step before selecting an answer, reducing premature conclusion bias.
- Core assumption: The model can benefit from explicit reasoning scaffolding even without fine-tuning on reasoning tasks.
- Evidence anchors:
  - [section 5] "Models marked with â€  were used with 4-bit OPTQ quantization... Each model benefits from the use of chain-of-thought prompting."
  - [section 5.1] "The performance for OAsst-33B is particularly low on the P6_P19 pair. This can be explained by the average number of premises (41.14) which is over twice the amount as included for the other property pairs... The long prompts exceed the number of tokens included in the context window during training for OAsst-33B (and Koala-13B)."
  - [corpus] Weak: Related work shows CoT helps in general reasoning tasks but not specifically for ambiguity resolution.

### Mechanism 3
- Claim: De re interpretations are harder because they require reasoning about the current entity's properties in a past context.
- Mechanism: De re requires the model to track entity identity across time and apply properties conditionally, which is more complex than de dicto's straightforward temporal scoping.
- Core assumption: The difficulty difference is due to the reasoning complexity, not dataset imbalance or prompt phrasing.
- Evidence anchors:
  - [section 5.1] "Finally, we observe that the de re instances are the more difficult class; only GPT4 with chain-of-thought is able to perform better than random guessing on these instances."
  - [section 5.2] "About a third of all de re chain-of-thought answers conclude that neither option is correct... This seems to indicate that the models completely fail to consider the de re option."
  - [corpus] Weak: No corpus evidence; the paper's experimental results are the primary anchor.

## Foundational Learning

- Concept: De dicto vs de re ambiguity
  - Why needed here: Understanding this distinction is essential to grasp why the task is challenging and what the correct reasoning path looks like.
  - Quick check question: In "In 2010, the pope was a native German speaker," which interpretation (de dicto or de re) is correct if we know the pope in 2010 was Benedict and he was indeed a native German speaker?

- Concept: Non-extensional operators
  - Why needed here: Temporal operators like "In 2010" create the ambiguity by scoping over different parts of the sentence.
  - Quick check question: Which of these operators would also create de dicto/de re ambiguity: "believes that," "hopes that," "in 2010," "is"?

- Concept: Chain-of-thought prompting
  - Why needed here: CoT is the intervention being tested to improve reasoning performance.
  - Quick check question: What is the key difference between direct and chain-of-thought prompts in this paper's experimental setup?

## Architecture Onboarding

- Component map:
  - Wikidata query engine -> Prompt template generator -> Premise shuffler -> LLM interface -> Evaluation module

- Critical path:
  1. Generate instance from Wikidata property pairs
  2. Construct prompt with premises, question, interpretations
  3. Apply prompt style (direct vs CoT)
  4. Send to LLM
  5. Parse response and categorize as correct/incorrect/neither
  6. Aggregate results by model, prompt style, and class

- Design tradeoffs:
  - Including distractor premises increases realism but may exceed context windows for smaller models
  - Randomizing premise order prevents positional bias but may increase variance in results
  - Using only 5 property pairs ensures diversity but limits generalizability to other ambiguity types

- Failure signatures:
  - "Neither" responses indicate complete failure to consider one interpretation
  - Consistent preference for de dicto suggests bias toward one interpretation type
  - Early prediction before CoT reasoning completes indicates poor instruction following

- First 3 experiments:
  1. Test if removing distractor premises improves performance for models with small context windows
  2. Evaluate if rephrasing prompts to avoid verb tense disambiguation improves overall accuracy
  3. Test if providing explicit regularity statements (e.g., "Military units do not change size class") improves de re performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on ambiguous definite descriptions compare to other types of ambiguity resolution tasks?
- Basis in paper: [explicit] The paper mentions that existing benchmarks may include various types of ambiguity but are not well-suited to evaluate reasoning about ambiguity specifically.
- Why unresolved: The paper only focuses on de dicto/de re ambiguities in definite descriptions and doesn't compare performance to other ambiguity types.
- What evidence would resolve it: Conduct experiments comparing LLM performance on various ambiguity types (e.g., pronoun resolution, lexical ambiguity, syntactic ambiguity) using similar evaluation methods.

### Open Question 2
- Question: Can LLMs be trained to better handle de re interpretations of ambiguous definite descriptions?
- Basis in paper: [explicit] The paper notes that models struggle particularly with de re instances and often fail to consider this interpretation.
- Why unresolved: The paper only evaluates zero-shot performance and doesn't explore training methods to improve de re interpretation.
- What evidence would resolve it: Train LLMs on datasets focused on de re ambiguities and evaluate if performance improves compared to zero-shot results.

### Open Question 3
- Question: How does the size and diversity of the knowledge base (e.g., Wikidata) affect LLM performance on this task?
- Basis in paper: [explicit] The paper uses Wikidata as a knowledge source but doesn't explore how different knowledge bases might impact results.
- Why unresolved: The paper uses a fixed knowledge source and doesn't test variations in size or domain.
- What evidence would resolve it: Test models using different knowledge bases (e.g., different sizes, domains, or representations) and compare performance across these variations.

## Limitations
- The dataset is limited to 5 property pairs from Wikidata, which may not generalize to other types of de dicto/de re ambiguity.
- The paper does not fully specify the prompt templates or the exact chain-of-thought instructions, making exact reproduction difficult.
- The evaluation does not distinguish between models that understand the ambiguity but fail to resolve it versus those that don't recognize the ambiguity at all.

## Confidence
- **High Confidence**: GPT-4 with chain-of-thought prompting achieves highest accuracy; de re instances are consistently harder than de dicto instances.
- **Medium Confidence**: Including all disambiguation information ensures failures are due to reasoning rather than knowledge gaps; CoT improves reasoning by structuring thought processes.
- **Low Confidence**: Explanation for de re difficulty (requiring entity tracking across time) is speculative and not directly validated.

## Next Checks
1. Test models on a larger set of property pairs from Wikidata to assess whether performance patterns generalize beyond the initial 5 pairs.
2. Conduct ablation studies where distractor premises are systematically removed to determine if models with smaller context windows perform better with only essential information.
3. Generate similar benchmarks for other non-extensional operators (e.g., "believes that," "hopes that") to determine if the de dicto/de re difficulty pattern holds across different types of ambiguity.