---
ver: rpa2
title: The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency
  of Language Models
arxiv_id: '2311.01307'
source_url: https://arxiv.org/abs/2311.01307
tags:
- consistency
- retrieval
- atlas
- pararel
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the factual consistency of large language
  models (LLMs) and explores strategies to improve it. Consistency is measured using
  ParaRel, a benchmark that evaluates the invariance of model predictions to semantically
  equivalent questions.
---

# The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models

## Quick Facts
- arXiv ID: 2311.01307
- Source URL: https://arxiv.org/abs/2311.01307
- Reference count: 7
- One-line primary result: Retrieval augmentation improves factual consistency more efficiently than model scaling

## Executive Summary
This study investigates factual consistency in large language models through two mitigation strategies: model up-scaling and retrieval augmentation. Using the ParaRel* benchmark, which evaluates invariance of model predictions to semantically equivalent questions, the research demonstrates that both strategies improve consistency, with retrieval augmentation being notably more efficient. The Atlas model, a retrieval-augmented LLM, outperforms non-retrieval models of similar scale and shows superior consistency. The work identifies potential sources of inconsistency including evaluation task format artifacts and dependencies on retrieved information, providing insights into improving LLM robustness and interpretability.

## Method Summary
The study evaluates factual consistency using the ParaRel* benchmark on LLaMA models (7B-65B parameters) and Atlas models (330M-880M parameters) with Wikipedia 2017 as retrieval corpus. Zero-shot evaluation is performed using LLaMA with log likelihood scoring and Atlas using T5 format with constrained decoding. Consistency is measured through pairwise agreement between semantically equivalent prompts, alongside accuracy metrics from the original LAMA prompt accuracy. The research compares scaling effects across model sizes, retrieval augmentation effects, and analyzes format artifacts in evaluation tasks.

## Key Results
- Retrieval augmentation improves factual consistency more efficiently than model scaling
- Atlas (retrieval-augmented) outperforms non-retrieval models of similar scale
- Evaluation task format artifacts contribute to factual inconsistency in language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation improves factual consistency more efficiently than model scaling
- Mechanism: Retrieval augmentation conditions predictions on external factual passages, reducing reliance on parametric knowledge which may be inconsistent
- Core assumption: The retrieved passages contain accurate and relevant factual information that can override inconsistent parametric knowledge
- Evidence anchors:
  - [abstract] "Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient"
  - [section 3.3] "Retrieval augmentation has a sizeable effect on consistency and accuracy" and "Retrieval augmentation is more efficient than up-scaling in increasing consistency"
  - [corpus] Weak - no direct citations, only mentions of factual inconsistency in retrieval-augmented generation

### Mechanism 2
- Claim: Model scaling improves factual consistency with diminishing returns
- Mechanism: Larger models have more parameters to store factual knowledge, reducing the need to "hallucinate" or rely on incomplete information
- Core assumption: The additional parameters in larger models are effectively utilized to store and retrieve factual information
- Evidence anchors:
  - [abstract] "Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient"
  - [section 3.3] "Model scaling has a sizeable effect on consistency and accuracy" and "up-scaling LLaMA yields diminishing returns with respect to consistency"
  - [corpus] Weak - no direct citations, only mentions of scaling laws in the context of factual inconsistency

### Mechanism 3
- Claim: Retrieval consistency correlates with prediction consistency
- Mechanism: Consistent retrieval results across semantically equivalent queries provide a stable factual foundation for the reader model, leading to consistent predictions
- Core assumption: The reader model effectively conditions on the retrieved information and uses it to generate consistent predictions
- Evidence anchors:
  - [section 5.1] "We observe a weak correlation between all retriever consistency metrics and the Atlas consistency"
  - [section 5.2] "Consistent and relevant retrieval augmentation causes more consistent predictions"
  - [corpus] Weak - no direct citations, only mentions of retrieval consistency in the context of factual inconsistency

## Foundational Learning

- Concept: Causal inference and intervention analysis
  - Why needed here: The paper uses causal reasoning to analyze the effects of different components (retriever, reader) on model consistency
  - Quick check: Verify that experimental design isolates individual contributions of retrieval quality and model scaling

- Concept: Evaluation metric design and benchmarking
  - Why needed here: The study relies on ParaRel* benchmark to measure factual consistency across semantically equivalent prompts
  - Quick check: Ensure benchmark captures relevant dimensions of factual consistency and isn't biased by format artifacts

- Concept: Retrieval-augmented generation (RAG) architecture
  - Why needed here: Understanding how retrieval components interact with reader models to influence consistency
  - Quick check: Verify that retrieved information is properly conditioning model predictions

## Architecture Onboarding

- Component map: ParaRel* benchmark -> LLaMA/Atlas models -> Consistency metrics (pairwise agreement) -> Accuracy metrics (LAMA)
- Critical path: Input prompt -> Retrieval (Atlas) or direct prediction (LLaMA) -> Model output -> Consistency evaluation
- Design tradeoffs: Model scaling vs. retrieval augmentation for efficiency in improving factual consistency
- Failure signatures: Inconsistent predictions across semantically equivalent prompts, low retrieval consistency
- First experiments: 1) Compare consistency across LLaMA model sizes, 2) Evaluate Atlas consistency vs. LLaMA of similar scale, 3) Test format artifact impact on consistency

## Open Questions the Paper Calls Out

1. How does the consistency of retrieval-augmented models change when using more diverse or less homogeneous training data for the retriever?
2. Can retrieval-augmented models achieve perfect consistency if the retrieved information is perfectly consistent and relevant across paraphrases?
3. How do the consistency and accuracy of retrieval-augmented models change when using different retrieval corpora?
4. How does the consistency of retrieval-augmented models change when using more advanced or specialized retriever architectures?
5. How does the consistency of retrieval-augmented models change when using different prompt engineering techniques or template variations?

## Limitations

- The study focuses on a single benchmark (ParaRel*) which may not capture all dimensions of factual consistency
- Analysis is limited to specific model families (LLaMA and Atlas) and knowledge base completion tasks
- Format artifacts identified as inconsistency sources but their root causes and prevalence across scenarios remain unclear

## Confidence

**High confidence**: Retrieval augmentation improves factual consistency is well-supported by direct experimental comparisons across multiple model scales and metrics.

**Medium confidence**: Claims about diminishing returns from model scaling are supported but may be specific to LLaMA architecture and evaluated tasks.

**Low confidence**: The weak correlation between retrieval consistency and prediction consistency (0.2 Spearman) suggests a more complex relationship than presented.

## Next Checks

1. Evaluate the same models on multiple factual consistency benchmarks to verify improvements generalize across different methodologies and knowledge domains

2. Systematically test individual contributions of retrieval quality, reader conditioning strength, and corpus relevance through ablation studies

3. Design controlled experiments that systematically vary question formatting while holding semantic content constant to definitively establish format-induced inconsistency magnitude and mechanism