---
ver: rpa2
title: Active Vision Reinforcement Learning under Limited Visual Observability
arxiv_id: '2306.00975'
source_url: https://arxiv.org/abs/2306.00975
tags:
- learning
- policy
- sensory
- observation
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SUGARL, a framework for active reinforcement
  learning under limited visual observability. SUGARL models motor and sensory policies
  separately but jointly learns them using an intrinsic sensorimotor reward, which
  incentivizes the sensory policy to select observations optimal for motor action
  inference.
---

# Active Vision Reinforcement Learning under Limited Visual Observability

## Quick Facts
- arXiv ID: 2306.00975
- Source URL: https://arxiv.org/abs/2306.00975
- Reference count: 40
- Primary result: SUGARL outperforms baselines on modified Atari and DeepMind Control suite environments under limited observability

## Executive Summary
This paper introduces SUGARL, a framework for active reinforcement learning under limited visual observability. SUGARL models motor and sensory policies separately but jointly learns them using an intrinsic sensorimotor reward, which incentivizes the sensory policy to select observations optimal for motor action inference. Experiments on modified Atari and DeepMind Control suite environments show that SUGARL outperforms baselines across a range of observability conditions and is adaptable to existing RL algorithms like DQN and SAC. The learned sensory policies exhibit effective active vision strategies, akin to human eye movements, and in some cases even surpass agents with full observations.

## Method Summary
SUGARL introduces a framework for active reinforcement learning where an agent learns both a motor policy for task completion and a sensory policy for controlling visual observations in partially observable environments. The framework uses a shared visual encoder with separate motor and sensory policy heads, trained jointly using a combination of environmental and intrinsic sensorimotor rewards. The sensorimotor reward is generated by a dedicated module that predicts motor actions from observation transitions, creating a learning signal that aligns sensory policy with motor policy needs. The framework optionally includes a Persistence-of-Vision Memory (PVM) module that combines recent partial observations to expand the effective observable area.

## Key Results
- SUGARL outperforms baselines across multiple games and tasks under limited observability conditions
- Joint learning significantly improves performance compared to training policies separately
- In some cases, SUGARL with limited observations surpasses agents with full observations
- SUGARL is adaptable to existing RL algorithms including DQN, SAC, and DrQv2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sensorimotor reward module guides the sensory policy to select observations that are optimal for motor action inference.
- Mechanism: The module predicts the motor action based on observation transitions, and the prediction error serves as an intrinsic reward signal. Lower prediction error indicates better observations for action inference, creating a learning signal that aligns sensory policy with motor policy needs.
- Core assumption: Observations that are easier to predict motor actions from are also more useful for actual motor policy learning.
- Evidence anchors:
  - [abstract] "This learnable reward is assigned by sensorimotor reward module, incentivizes the sensory policy to select observations that are optimal to infer its own motor action"
  - [section 3.2] "The module is trained to have the sensorimotor understanding, and is used to indicate the goodness of the sensory policy"
- Break condition: If the sensorimotor reward module's predictions don't correlate with actual motor policy performance, or if the prediction task becomes trivial (e.g., always predicting the same action).

### Mechanism 2
- Claim: Joint learning of motor and sensory policies using a shared reward function enables effective coordination between them.
- Mechanism: By training both policies with the combined environmental and sensorimotor rewards, they learn to coordinate - the sensory policy learns to provide useful observations while the motor policy learns to act on whatever observations are provided.
- Core assumption: The environmental reward signal alone is insufficient for the sensory policy to learn useful behavior.
- Evidence anchors:
  - [section 3.1] "The motor and sensory policies are jointly trained using a shared reward function, which is the combination of environmental reward and our sensorimotor reward"
  - [section 5.3] "Joint learning significantly improves the policy, indicating its effectiveness in coordinating two policies"
- Break condition: If the sensory policy overfits to the sensorimotor reward at the expense of environmental reward, or if the joint learning destabilizes training.

### Mechanism 3
- Claim: The Persistence-of-Vision Memory (PVM) expands effective observable area by stitching multiple recent partial observations.
- Mechanism: PVM stores recent observations and combines them into a single view, effectively increasing the agent's field of view over time while maintaining computational efficiency.
- Core assumption: Recent observations remain relevant enough to be useful when stitched together.
- Evidence anchors:
  - [section 3.3] "PVM stores the observations from B past steps in a buffer and combines them into a single PVM observation"
  - [section 5.3] "PVM also considerably enhances the algorithm by increasing the effective observable area as expected"
- Break condition: If the environment changes too rapidly, making old observations irrelevant, or if the stitching operation introduces significant artifacts that confuse the policy.

## Foundational Learning

- Concept: Inverse dynamics prediction
  - Why needed here: The sensorimotor reward module uses inverse dynamics prediction to assess observation quality, which is a standard approach in self-supervised learning for robotics.
  - Quick check question: What is the difference between forward dynamics prediction and inverse dynamics prediction?

- Concept: Joint policy learning with intrinsic rewards
  - Why needed here: SUGARL combines environmental and intrinsic rewards to train both motor and sensory policies simultaneously, requiring understanding of multi-objective RL.
  - Quick check question: How do you balance multiple reward signals of different scales in reinforcement learning?

- Concept: Partial observability and its challenges
  - Why needed here: The entire framework addresses the challenge of learning effective policies under limited visual observability, requiring understanding of POMDPs and observation aliasing.
  - Quick check question: What is perceptual aliasing and why does it make learning harder in partially observable environments?

## Architecture Onboarding

- Component map: Shared visual encoder -> Motor policy head and Sensory policy head -> Actions -> Environment -> Reward (environmental + sensorimotor) -> Policy update. Sensorimotor reward module trained separately but uses same experience data.
- Critical path: Observation → Shared encoder → Motor and sensory policy heads → Actions → Environment → Reward (environmental + sensorimotor) → Policy update. The sensorimotor reward module is trained separately but uses the same experience data.
- Design tradeoffs: Separate vs. joint action spaces (separate performs better), absolute vs. relative sensory action spaces (depends on observation size), with vs. without PVM (PVM helps but adds complexity), with vs. without peripheral observation (peripheral observation can help but isn't always necessary).
- Failure signatures: If the sensory policy learns to select random or degenerate views, check if the sensorimotor reward is properly scaled or if the module is learning effectively. If training is unstable, check the reward balance parameter β.
- First 3 experiments:
  1. Run with only environmental reward (no sensorimotor reward) to confirm it's necessary for sensory policy learning.
  2. Compare absolute vs. relative sensory action spaces on a simple task to see which performs better.
  3. Test with and without PVM on a task with limited field of view to measure its impact on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sensorimotor reward module's performance vary with different network architectures or training regimes?
- Basis in paper: [inferred] The paper mentions that the sensorimotor reward module is implemented as an independent neural network and trained using inverse dynamics prediction, but does not explore architectural variations.
- Why unresolved: The paper focuses on demonstrating the effectiveness of SUGARL with a standard architecture, leaving open the question of whether alternative designs could improve performance.
- What evidence would resolve it: Systematic ablation studies comparing different architectures (e.g., depth, width, activation functions) and training regimes (e.g., pre-training, data augmentation) for the sensorimotor reward module.

### Open Question 2
- Question: Can SUGARL be extended to environments with more complex visual structures, such as 3D scenes or scenes with dynamic lighting?
- Basis in paper: [explicit] The paper evaluates SUGARL on 2D Atari and DeepMind Control environments, but does not explore more complex visual settings.
- Why unresolved: The paper demonstrates effectiveness in limited settings, but the generalizability to more complex visual environments remains unexplored.
- What evidence would resolve it: Experiments applying SUGARL to environments with 3D scenes, dynamic lighting, or other complex visual structures, comparing performance against baselines.

### Open Question 3
- Question: How does the choice of sensory action space (absolute vs. relative) affect the sample efficiency and final performance of SUGARL?
- Basis in paper: [explicit] The paper compares absolute and relative sensory action spaces, finding that absolute modeling performs better in smaller observable regions, while relative modeling performs better in larger observable regions.
- Why unresolved: While the paper provides initial insights, a more comprehensive analysis of the trade-offs between absolute and relative modeling across various tasks and observation sizes is needed.
- What evidence would resolve it: Extensive experiments varying the sensory action space type across a wider range of tasks, observation sizes, and training regimes, measuring both sample efficiency and final performance.

## Limitations
- The framework's reliance on inverse dynamics prediction assumes that better action prediction from observations directly correlates with observation utility for task performance, which may not hold in all environments.
- The claim that SUGARL can surpass agents with full observations is based on limited experiments and may not generalize across all task types.
- The paper only evaluates on 2D environments, leaving the generalizability to more complex visual structures unexplored.

## Confidence
- High confidence: The experimental results showing SUGARL outperforming baselines across multiple games and tasks under limited observability conditions.
- Medium confidence: The claim that SUGARL is adaptable to existing RL algorithms like DQN and SAC, as the paper only shows results for specific hyperparameter settings.
- Low confidence: The assertion that SUGARL can surpass agents with full observations, as this claim is based on a limited number of experiments.

## Next Checks
1. Test SUGARL on environments with rapidly changing dynamics to evaluate whether PVM's assumption of observation relevance over time holds.
2. Implement a variant where the sensorimotor reward is computed using forward dynamics prediction instead of inverse dynamics to assess the sensitivity to this design choice.
3. Evaluate the framework's performance when the sensory policy's action space is constrained to only allow small view adjustments, testing the robustness of the learned strategies to action granularity.