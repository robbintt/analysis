---
ver: rpa2
title: Uncovering the Hidden Cost of Model Compression
arxiv_id: '2308.14969'
source_url: https://arxiv.org/abs/2308.14969
tags:
- dense
- lottery
- ilm-vp
- transfer
- tickets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how model compression, particularly via\
  \ lottery tickets (sparse networks), affects visual prompting-based transfer learning\
  \ compared to linear probing. While prior work studied model compression in standard\
  \ transfer learning, its impact on visual prompting\u2014where a pre-trained model\
  \ is reprogrammed using a trainable input perturbation without updating weights\u2014\
  remained unclear."
---

# Uncovering the Hidden Cost of Model Compression

## Quick Facts
- arXiv ID: 2308.14969
- Source URL: https://arxiv.org/abs/2308.14969
- Reference count: 40
- Primary result: Lottery tickets generally hurt performance under both linear probing and visual prompting, with degradation more pronounced in visual prompting, especially at low data regimes.

## Executive Summary
This paper investigates how model compression via lottery tickets affects visual prompting-based transfer learning compared to linear probing. While prior work studied model compression in standard transfer learning, its impact on visual prompting—where a pre-trained model is reprogrammed using a trainable input perturbation without updating weights—remained unclear. The study compares transfer performance across eight datasets using ResNet-50 models at various sparsity levels, revealing that lottery tickets generally hurt performance under both methods, with the degradation more pronounced in visual prompting, especially at low data regimes.

## Method Summary
The study compares transfer performance across eight datasets using ResNet-50 models at various sparsity levels. Visual prompting (VP) constructs a masked perturbation added to zero-padded images to align target features to the source distribution without updating model weights. Linear probing (LP) fits a linear classifier on frozen features extracted from the pre-trained model. Lottery tickets are identified by masking weights in the dense model after training and retraining from the same initialization. The experiments evaluate accuracy and Expected Calibration Error (ECE) across different sparsity levels and data budgets.

## Key Results
- Lottery tickets generally hurt performance under both linear probing and visual prompting, with degradation more pronounced in visual prompting
- Visual prompting exhibits less severe calibration degradation than linear probing, with up to 2-3x higher ECE in linear probing at extreme sparsity
- The effect is dataset-dependent: some tasks benefit from sparse models while others suffer
- Few-shot settings amplify the negative impact of sparsity on both methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual prompting uses a trainable additive perturbation applied as a universal mask over the input, enabling the model to remap target data into the source domain without updating model weights
- Mechanism: VP constructs a masked perturbation M ⊙ δ that is added to zero-padded images x′, where δ is learned to align target features to the source distribution, and output mapping hk remaps source logits to target classes
- Core assumption: The perturbation can effectively realign target feature distributions to match those of the source task without changing the underlying feature extractor
- Evidence anchors: [abstract] "visual prompting... maps the input data to the domain of the source data on which the model was originally pre-trained on"; [section] "A masked perturbation, denoted by M ⊙ δ, is appended to a zero-padded version of {xi} in order to match the input dimension of the pre-trained model"
- Break condition: If the source and target domains are too dissimilar, the perturbation cannot bridge the distribution gap and performance collapses

### Mechanism 2
- Claim: Linear probing fits a linear classifier on frozen features, so it inherits the geometry of the source feature space directly
- Mechanism: LP extracts frozen features from the pre-trained model and trains only a new linear head, making it sensitive to how pruning alters the feature space
- Core assumption: The linear separator can still separate classes in the pruned feature space if the important dimensions are preserved
- Evidence anchors: [abstract] "linear probing (LP)... aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model"; [section] "LP... the performance degradation due to LT was, however, not as severe as that in ILM-VP"
- Break condition: Pruning removes critical dimensions, so the linear separator no longer separates classes effectively

### Mechanism 3
- Claim: Lottery tickets are sparse subnetworks that retain performance in dense models, but their transfer efficacy depends on the target domain and sparsity level
- Mechanism: LTs are identified by masking weights in the dense model after training and retraining from the same initialization; transfer success depends on preserving features relevant to the downstream task
- Core assumption: The lottery ticket contains a subnetwork that still encodes the features required for the target task at the chosen sparsity
- Evidence anchors: [abstract] "lottery tickets... are sparse sub-networks... which show similar or often better performance compared to the original model on training with the same initial parametrization"; [section] "We observe that certain target domains benefit from transferring an LT instead of their parent network, while in others it causes a negative impact on performance"
- Break condition: Sparsity exceeds the point where critical features are pruned, causing task performance to degrade sharply

## Foundational Learning

- Concept: Sparse subnetworks and pruning mechanics
  - Why needed here: Understanding how lottery tickets are derived and how sparsity levels affect representational capacity is central to interpreting performance differences
  - Quick check question: What is the difference between magnitude-based pruning and iterative pruning when finding lottery tickets?

- Concept: Calibration and Expected Calibration Error (ECE)
  - Why needed here: The paper evaluates reliability by comparing ECE across LP and VP; grasping this metric is essential to interpret calibration trends
  - Quick check question: How does ECE differ from accuracy, and why might a model have high accuracy but poor calibration?

- Concept: Transfer learning paradigms (full fine-tuning, linear probing, visual prompting)
  - Why needed here: The comparative analysis relies on knowing how each method updates parameters and adapts features, especially under constraints like data and model sparsity
  - Quick check question: In visual prompting, which parameters are updated during training?

## Architecture Onboarding

- Component map: Pre-trained backbone (ResNet-50) -> frozen feature extractor -> Mask M and perturbation δ (VP) or linear classifier (LP) -> Output mapping functions hk (VP) for label remapping -> Evaluation pipeline: accuracy, ECE, N-shot scaling

- Critical path:
  1. Load lottery ticket at desired sparsity
  2. For VP: build masked perturbation layer and ILM mapping
  3. For LP: attach linear head to frozen backbone
  4. Train only VP or LP modules on target data
  5. Evaluate accuracy and ECE

- Design tradeoffs:
  - Higher sparsity -> smaller memory footprint but risk of losing critical features
  - VP vs LP: VP may be more robust to pruning but less interpretable; LP is simpler but can overfit in low-data regimes
  - Calibration: VP tends to maintain lower ECE under sparsity; LP ECE rises more sharply

- Failure signatures:
  - VP: poor alignment between target and source feature distributions -> accuracy collapse
  - LP: pruning removes discriminative features -> linear classifier cannot separate classes
  - Both: very low data budgets -> overfitting -> unreliable calibration

- First 3 experiments:
  1. Compare dense vs 11.571% LT accuracy and ECE on CIFAR-10 for VP and LP at full data
  2. Measure N-shot scaling (1, 2, 5, 10 shots) for both methods on SVHN to observe data sparsity effects
  3. Sweep sparsity levels (79.122% -> 11.571%) on GTSRB to quantify calibration degradation in ECE for both methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pruning strategies beyond lottery tickets (e.g., magnitude pruning, structured pruning) affect the performance and calibration of visual prompting compared to linear probing?
- Basis in paper: [inferred] The paper focuses on lottery tickets but mentions that further analysis using other pruned models might reveal additional patterns
- Why unresolved: The study is limited to lottery tickets, leaving the impact of alternative pruning methods unexplored
- What evidence would resolve it: Experiments comparing visual prompting and linear probing across multiple pruning strategies (magnitude, structured, etc.) on the same datasets and tasks

### Open Question 2
- Question: Does the reliability advantage of visual prompting over linear probing in terms of calibration persist when transferring to cross-domain tasks or when using foundation models from different modalities (e.g., vision transformers, language models)?
- Basis in paper: [explicit] The authors suggest studying broader model classes including transformers as future work
- Why unresolved: All experiments use ResNet-50 and ImageNet-pretrained models; generalization to other architectures and domains is untested
- What evidence would resolve it: Comparative calibration analysis of visual prompting vs. linear probing on cross-domain tasks using diverse model families (ViT, CLIP, etc.)

### Open Question 3
- Question: What are the specific subpopulations of data that benefit or suffer from transfer using sparse models under visual prompting versus linear probing, and how does this vary with sparsity level?
- Basis in paper: [explicit] The authors propose studying "influence estimation" to understand data subpopulation contributions under different sparsity and transfer methods
- Why unresolved: The paper analyzes aggregate performance but does not examine per-subpopulation effects
- What evidence would resolve it: Influence function or subpopulation analysis identifying which data groups improve or degrade with sparsity under each transfer method

## Limitations

- The paper focuses exclusively on ResNet-50 architecture, limiting generalizability to other model families
- Only lottery ticket pruning is evaluated, leaving the impact of alternative pruning strategies unexplored
- No statistical significance testing is conducted across multiple runs to verify calibration differences

## Confidence

High: Core empirical findings showing lottery tickets degrade performance more severely under visual prompting than linear probing
Medium: Calibration claims given the limited dataset diversity and lack of statistical significance testing

## Next Checks

1. Test the same sparsity-pruning framework on ViT and ConvNeXt architectures to assess architectural robustness
2. Compare lottery ticket performance against magnitude-based pruning at identical sparsity levels to isolate the pruning method effect
3. Conduct statistical significance testing across multiple runs to verify that calibration differences are not due to random variation