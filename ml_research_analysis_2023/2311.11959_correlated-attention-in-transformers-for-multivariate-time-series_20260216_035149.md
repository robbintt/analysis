---
ver: rpa2
title: Correlated Attention in Transformers for Multivariate Time Series
arxiv_id: '2311.11959'
source_url: https://arxiv.org/abs/2311.11959
tags:
- attention
- series
- time
- transformer
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of capturing cross-correlation
  between features in multivariate time series data using Transformers. The authors
  propose a novel correlated attention mechanism that efficiently learns both instantaneous
  and lagged cross-correlations, as well as auto-correlation.
---

# Correlated Attention in Transformers for Multivariate Time Series

## Quick Facts
- arXiv ID: 2311.11959
- Source URL: https://arxiv.org/abs/2311.11959
- Reference count: 21
- This paper proposes a novel correlated attention mechanism that efficiently learns both instantaneous and lagged cross-correlations in multivariate time series data, achieving state-of-the-art results on imputation, anomaly detection, and classification tasks.

## Executive Summary
This paper addresses the challenge of capturing cross-correlation between features in multivariate time series data using Transformers. The authors introduce a correlated attention mechanism that computes cross-covariance matrices between queries and keys with different lag values, enabling efficient learning of both instantaneous and lagged cross-correlations. The method can be seamlessly integrated with existing Transformer architectures and demonstrates consistent improvements over base models across multiple non-predictive tasks.

## Method Summary
The method introduces a Correlated Attention Block (CAB) that captures lagged cross-correlations by rolling key and value matrices across time lags and computing cross-covariance matrices in the feature dimension. CAB uses FFT for efficient computation of lagged cross-correlations, reducing complexity from O(d²kT²) to O(d²kT log T). The correlated attention is integrated with existing Transformers through a mixture-of-head attention mechanism that splits heads between temporal and correlated attention, allowing the model to learn both temporal and feature-wise dependencies simultaneously.

## Key Results
- Correlated attention consistently improves base Transformer performance across imputation, anomaly detection, and classification tasks
- Achieves state-of-the-art results on three non-predictive multivariate time series tasks
- FFT-based computation reduces complexity from O(d²kT²) to O(d²kT log T) while maintaining accuracy
- The mixture-of-head attention seamlessly integrates with existing Transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The correlated attention block (CAB) captures lagged cross-correlation by rolling the key and value matrices across time lags and computing cross-covariance matrices in the feature dimension.
- Mechanism: CAB first normalizes queries and keys, then for each lag value l, computes the cross-correlation matrix ROLL( ˆK, l )⊤ ˆQ. It selects the top-k lags based on a weighted combination of diagonal (auto-correlation) and non-diagonal (cross-correlation) entries. Finally, it aggregates representations using these lagged correlations.
- Core assumption: Rolling operations preserve the temporal structure while enabling computation of lagged correlations; top-k selection identifies the most informative lags.
- Evidence anchors:
  - [abstract]: "correlated attention operates across feature channels to compute cross-covariance matrices between queries and keys with different lag values"
  - [section 3.2.2]: Formal definition of lagged cross-correlation filtering and score aggregation
  - [corpus]: No direct evidence; the corpus neighbors focus on different architectural approaches
- Break condition: If rolling causes excessive padding/circular effects that distort the true lagged relationship, or if top-k selection misses important but lower-scoring lags.

### Mechanism 2
- Claim: The mixture-of-head attention seamlessly integrates CAB with existing temporal attention mechanisms by splitting heads between temporal and correlated attention.
- Mechanism: Instead of replacing the base model's multi-head attention entirely, CAB heads are added alongside the original temporal attention heads. The mixture threshold m controls the split, allowing the model to learn from both temporal and feature-wise dependencies.
- Core assumption: The base Transformer's temporal attention heads and CAB heads can be learned jointly without interference.
- Evidence anchors:
  - [abstract]: "can be seamlessly integrated within the encoder blocks of existing well-known Transformers"
  - [section 3.2.3]: Equation 8 showing mixture-of-head attention combining temporal and correlated attention
  - [corpus]: No direct evidence; neighbors don't discuss hybrid attention designs
- Break condition: If the model cannot balance the two types of attention, leading to one dominating or the heads interfering with each other's learning.

### Mechanism 3
- Claim: The use of FFT for computing lagged cross-correlations reduces computational complexity from O(d²kT²) to O(d²kT log T).
- Mechanism: By leveraging the Cross-correlation Theorem, FFT is applied to the normalized keys and queries in the frequency domain, enabling efficient computation of all lag values simultaneously rather than iterating through each lag.
- Core assumption: FFT can be directly applied to the cross-correlation computation required by CAB, and the transformation preserves the necessary information.
- Evidence anchors:
  - [section 3.2.2]: Detailed FFT-based computation description and complexity analysis
  - [abstract]: "Efficient computation of CAB" subsection explicitly mentions FFT acceleration
  - [corpus]: No direct evidence; neighbors don't discuss FFT optimization for attention
- Break condition: If the FFT approximation introduces numerical errors that degrade correlation quality, or if the overhead of FFT outweighs benefits for small T.

## Foundational Learning

- Concept: Time series cross-correlation and auto-correlation
  - Why needed here: CAB explicitly models these statistical properties through its lagged cross-correlation filtering and auto-correlation detection
  - Quick check question: What is the difference between cross-correlation and auto-correlation in time series analysis?

- Concept: Multi-head attention mechanism in Transformers
  - Why needed here: CAB is integrated via mixture-of-head attention, which modifies the standard multi-head attention architecture
  - Quick check question: How does multi-head attention in Transformers enable learning different representation subspaces?

- Concept: Fast Fourier Transform and the Cross-correlation Theorem
  - Why needed here: FFT is used to accelerate the computation of lagged cross-correlations across all lag values
  - Quick check question: How does the Cross-correlation Theorem enable efficient computation of sliding inner products using FFT?

## Architecture Onboarding

- Component map: Input normalization layer → Correlated attention block (normalization, lagged cross-correlation filtering, score aggregation) → Mixture-of-head attention (combines temporal and correlated heads) → Base Transformer encoder block
- Critical path: 1. Normalize queries and keys 2. Compute lagged cross-correlation matrices via FFT 3. Select top-k lags based on weighted correlation scores 4. Aggregate representations using selected lags 5. Combine with temporal attention heads in mixture
- Design tradeoffs:
  - More CAB heads vs. more temporal heads: Balance between feature-wise and temporal dependency modeling
  - Larger k (more lags) vs. computational cost: Trade-off between capturing long-range correlations and efficiency
  - Learnable λ and β vs. fixed values: Flexibility vs. simplicity and potential overfitting
- Failure signatures:
  - Performance worse than base model: CAB heads may be interfering with temporal attention learning
  - Training instability: Learnable parameters λ and β may need careful initialization or regularization
  - Memory issues: Large sequence lengths with many lags can exceed memory limits despite FFT optimization
- First 3 experiments:
  1. Ablation study: Replace self-attention entirely with pure CAB (no temporal attention heads) on a simple imputation task
  2. Hyperparameter sweep: Vary the head split ratio m and observe impact on imputation accuracy vs. computation time
  3. Complexity test: Measure actual runtime vs. theoretical O(d²kT log T) on increasing sequence lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the correlated attention mechanism perform on long-term forecasting tasks compared to other state-of-the-art Transformer models?
- Basis in paper: [inferred] The paper mentions that the current form of correlated attention has not been designed to be fully integrated into decoder architectures of Transformers, and suggests that with proper masking mechanism, the performance increase can be further improved. The paper also provides preliminary results on MTS long-term forecasting in Appendix C, but the results are not conclusive.
- Why unresolved: The paper does not provide extensive experimental results on long-term forecasting tasks, and the current design of correlated attention may not be optimal for such tasks.
- What evidence would resolve it: More extensive experimental results on long-term forecasting tasks using correlated attention, and a comparison with other state-of-the-art Transformer models for long-term forecasting.

### Open Question 2
- Question: How does the correlated attention mechanism perform on other multivariate time series analysis tasks, such as classification or anomaly detection, when compared to other state-of-the-art models?
- Basis in paper: [explicit] The paper extensively experiments on imputation, anomaly detection, and classification tasks, but does not compare the correlated attention mechanism to other state-of-the-art models for these tasks.
- Why unresolved: The paper only compares the correlated attention mechanism to other Transformer-based models, and does not consider other types of models that may be better suited for these tasks.
- What evidence would resolve it: More extensive experimental results on classification and anomaly detection tasks using correlated attention, and a comparison with other state-of-the-art models for these tasks.

### Open Question 3
- Question: How does the correlated attention mechanism perform on multivariate time series data with high-dimensional feature spaces?
- Basis in paper: [inferred] The paper mentions that the correlated attention mechanism can be flexibly plugged into follow-up Transformer architectures for efficiency gain, but does not provide experimental results on high-dimensional multivariate time series data.
- Why unresolved: The paper does not provide experimental results on high-dimensional multivariate time series data, and the performance of the correlated attention mechanism on such data is unknown.
- What evidence would resolve it: More extensive experimental results on high-dimensional multivariate time series data using correlated attention, and a comparison with other state-of-the-art models for such data.

## Limitations

- The current correlated attention mechanism is not designed for decoder architectures, limiting its applicability to predictive tasks like long-term forecasting
- Limited empirical validation across diverse multivariate time series datasets with varying correlation structures
- The optimal split ratio between temporal and correlated attention heads (m) is not thoroughly explored through systematic ablation studies

## Confidence

- **High Confidence**: The computational complexity claims (O(d²kT log T) vs O(d²kT²)) are mathematically sound and well-supported by the FFT-based implementation. The core architectural integration with existing Transformers is clearly specified.
- **Medium Confidence**: The empirical results showing state-of-the-art performance across three tasks are promising but based on relatively few datasets. The mixture-of-head attention approach appears sound, but the optimal split ratio (m) is not thoroughly explored.
- **Low Confidence**: The claims about capturing "both instantaneous and lagged cross-correlations" are theoretically justified but lack comprehensive empirical validation across diverse MTS characteristics.

## Next Checks

1. **Generalization Test**: Apply the correlated attention mechanism to additional multivariate time series datasets with different correlation structures (e.g., highly periodic vs. chaotic) to validate robustness across data types.

2. **Ablation on Attention Split**: Systematically vary the head split ratio m from 0 to 1 in increments and measure the impact on performance and convergence, identifying the optimal balance between temporal and correlated attention.

3. **Memory-Computation Tradeoff Analysis**: Empirically measure the actual runtime and memory usage against theoretical complexity predictions across varying sequence lengths (T) and feature dimensions (d) to validate the claimed efficiency gains from FFT optimization.