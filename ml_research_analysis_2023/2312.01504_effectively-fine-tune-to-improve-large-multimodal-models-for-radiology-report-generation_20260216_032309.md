---
ver: rpa2
title: Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report
  Generation
arxiv_id: '2312.01504'
source_url: https://arxiv.org/abs/2312.01504
tags:
- visual
- language
- fine-tuning
- arxiv
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the task of automating radiology report generation
  using large multimodal models, which is challenging due to the need for domain expertise
  and the high computational cost of fine-tuning large language models. The authors
  propose a two-stage fine-tuning strategy that aligns visual features to the LLM's
  text embedding space as soft visual prompts, avoiding distortion of pretrained visual
  features.
---

# Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation

## Quick Facts
- arXiv ID: 2312.01504
- Source URL: https://arxiv.org/abs/2312.01504
- Reference count: 40
- Primary result: Two-stage fine-tuning achieves state-of-the-art radiology report generation with BLEU4 6.9, ROUGE-L 23.5, F1-CXB-14 32.0 using OpenLLaMA-7B

## Executive Summary
This paper addresses the challenge of automating radiology report generation from chest X-ray images using large multimodal models. The authors propose a two-stage fine-tuning strategy that first aligns visual features to the LLM's text embedding space through a mapping network, then fine-tunes the vision encoder to improve clinical accuracy. Their approach achieves state-of-the-art performance on the MIMIC-CXR dataset without extensive domain-specific pretraining, outperforming prior methods while avoiding catastrophic forgetting of pretrained visual features.

## Method Summary
The method employs a visual encoder (pretrained ResNet50) whose features are projected into the LLM's embedding space via a mapping network (single transformer decoder layer). The LLM (GPT2-S/L or OpenLLaMA-7B) receives these projected visual features as soft prompts. The two-stage fine-tuning strategy first freezes the vision encoder to train the mapping network and LoRA adapters, then unfreezes and jointly fine-tunes all components. This approach improves clinical accuracy while preserving pretrained visual feature representations.

## Key Results
- Two-stage fine-tuning consistently improves F1-CXB-14 scores from ~30.0 to ~32.0 across all model sizes
- Larger language models (OpenLLaMA-7B) achieve better overall performance than smaller variants (GPT2-S, GPT2-L)
- Analysis reveals larger models allocate less attention to visual prompts, potentially reducing grounding
- Fine-tuning vision encoder improves clinical accuracy compared to freezing it

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning the vision encoder together with the mapping network and language model improves clinical accuracy. The vision encoder learns to extract features more relevant to radiology report generation when jointly fine-tuned, rather than remaining frozen. Core assumption: Pretrained vision features contain generalizable knowledge that can be adapted for medical image understanding without catastrophic forgetting. Break condition: If the vision encoder overfits to the specific dataset and loses its general visual understanding capabilities.

### Mechanism 2
Two-stage fine-tuning prevents distortion of pretrained visual features while still allowing adaptation. First stage freezes vision encoder to let mapping network learn alignment, then unfreezes vision encoder for adaptation. Core assumption: Early gradient updates from a randomly initialized mapping network would otherwise distort the vision encoder's learned representations. Break condition: If the mapping network doesn't learn proper alignment in the first stage, or if unfreezing causes catastrophic forgetting.

### Mechanism 3
Large language models pay less attention to visual prompts, potentially reducing grounding. As language models scale up, they rely more heavily on their language modeling capabilities rather than visual information. Core assumption: Larger models have stronger language priors that can overshadow visual context. Break condition: If attention mechanisms are modified to force greater visual grounding, or if smaller models perform better for this task.

## Foundational Learning

- **Visual-text alignment through embedding space mapping**: Why needed here: The mapping network must project visual features into the same space as text embeddings for the LLM to process them effectively. Quick check question: What happens if the visual features and text embeddings are in incompatible spaces?

- **Catastrophic forgetting in fine-tuning**: Why needed here: Understanding when and how fine-tuning can degrade pretrained model performance is crucial for the two-stage approach. Quick check question: What mechanisms in neural networks make them susceptible to forgetting previously learned features?

- **Attention mechanisms in transformers**: Why needed here: The attention allocation patterns reveal how much the model relies on visual vs. language information. Quick check question: How does the softmax attention mechanism distribute weights across different token types?

## Architecture Onboarding

- **Component map**: X-ray image → visual features → mapping network → soft visual prompts → LLM with LoRA adapters → generated report
- **Critical path**: Visual encoder (ResNet50) → Mapping network (transformer decoder layer) → Language model (GPT2 variants or OpenLLaMA) with LoRA adapters
- **Design tradeoffs**: Freezing vs. fine-tuning vision encoder (accuracy vs. preservation of pretrained features), single vs. two-stage fine-tuning (simplicity vs. performance), model size vs. attention to visual content
- **Failure signatures**: Low F1-CXB scores despite good BLEU/ROUGE scores (hallucination), attention visualization showing minimal visual prompt focus, performance degradation when unfreezing vision encoder
- **First 3 experiments**:
  1. Compare frozen vs. fine-tuned vision encoder performance on MIMIC-CXR validation set
  2. Implement one-epoch freeze followed by unfreeze (two-stage) and measure improvement
  3. Visualize attention weights for different model sizes to verify scaling effects on visual grounding

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between fine-tuning the vision encoder and preserving pretrained visual features for radiology report generation? While the two-stage fine-tuning strategy shows promising results, the optimal balance between fine-tuning the vision encoder and preserving pretrained visual features may depend on the specific task, dataset, and model architecture. Conducting extensive experiments with different fine-tuning strategies would provide insights into the optimal balance.

### Open Question 2
How can we improve the grounding of generated reports to visual content when using large language models for multimodal tasks? The authors observe that larger language models allocate less attention to visual prompts compared to smaller models, suggesting that generated reports may be less grounded to visual content. Investigating alternative architectures or training strategies that explicitly encourage the model to attend to visual prompts would help address this issue.

### Open Question 3
How can we improve the confidence calibration of radiology report generation models to better signal potential mistakes to end users? The authors analyze the confidence scores of generated reports and find that the average confidence distribution of true positive and false developing novel techniques for confidence calibration, such as temperature scaling or ensemble methods, and evaluating their effectiveness on radiology report generation tasks would help address this issue.

## Limitations
- Larger language models show reduced attention to visual prompts, potentially compromising grounding in generated reports
- Evaluation relies on automated metrics that may not capture clinically meaningful errors or subtle hallucinations
- Results are limited to chest X-rays and may not generalize to other medical imaging modalities or report types

## Confidence

**High Confidence:**
- Fine-tuning the vision encoder improves clinical efficacy metrics (F1-CXB-14 scores increase from ~30.0 to ~32.0)
- The two-stage fine-tuning strategy consistently outperforms single-stage approaches across all model sizes
- Larger language models (OpenLLaMA-7B) achieve better overall performance than smaller variants

**Medium Confidence:**
- The claim that fine-tuning prevents catastrophic forgetting of pretrained visual features
- That larger models pay less attention to visual prompts, potentially reducing grounding
- State-of-the-art-level performance claims relative to prior work

**Low Confidence:**
- The practical clinical utility of generated reports based solely on automated metrics
- Whether the observed attention patterns reflect genuine understanding versus learned correlations
- The robustness of results across different medical imaging domains or report types

## Next Checks

1. **Attention Mechanism Validation**: Conduct controlled experiments varying the prominence of visual prompts to determine whether larger models can be forced to utilize visual information more effectively, and measure the impact on clinical accuracy.

2. **Human Clinical Evaluation**: Deploy a blinded study where radiologists evaluate generated reports from different model sizes and fine-tuning strategies, comparing them to ground truth reports to validate whether automated metrics correlate with clinical utility.

3. **Generalization Testing**: Apply the two-stage fine-tuning approach to a different medical imaging task (e.g., radiology report generation from X-rays of other body parts) to assess whether the benefits extend beyond chest X-rays.