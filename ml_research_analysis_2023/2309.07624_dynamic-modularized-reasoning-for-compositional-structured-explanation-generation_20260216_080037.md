---
ver: rpa2
title: Dynamic MOdularized Reasoning for Compositional Structured Explanation Generation
arxiv_id: '2309.07624'
source_url: https://arxiv.org/abs/2309.07624
tags:
- reasoning
- morse
- compositional
- association
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of compositional generalization
  in reasoning tasks, specifically focusing on the structured explanation generation
  task. The authors propose a new setting for this task that requires models to generalize
  from entailment trees with a limited number of reasoning steps to trees involving
  more steps, testing the models' ability to compose different inference rules on
  the sentence level.
---

# Dynamic MOdularized Reasoning for Compositional Structured Explanation Generation

## Quick Facts
- arXiv ID: 2309.07624
- Source URL: https://arxiv.org/abs/2309.07624
- Reference count: 26
- Key outcome: MORSE achieves 53.31 F1 score for Steps and 57.78 F1 score for Intermediates on EntailmentBank-Length, outperforming baseline EntailmentWriter

## Executive Summary
This paper addresses compositional generalization in structured explanation generation, where models must generate longer entailment trees from shorter ones. The authors propose MORSE (Dynamic MOdularized Reasoning for Compositional Structured Explanation), a novel approach that factorizes the inference process into specialized modules using modularized self-attention. MORSE dynamically routes inputs to dedicated heads based on functional similarity, enabling each module to specialize in specific reasoning functions. The method demonstrates superior performance on two benchmarks (EntailmentBank and DBpedia) when tested for generalization over proof lengths and shapes.

## Method Summary
MORSE is a dynamically modularized reasoning model that addresses compositional generalization in structured explanation generation. It uses a modularized self-attention mechanism where each module represents a functional unit. The model computes cosine similarity between trainable module representations and input functional representations to dynamically select and route inputs to specialized heads. The architecture employs frozen module representations to preserve learned functions while allowing dynamic adaptation through masking. MORSE is built on a T5 backbone and trained with Adam optimizer on rearranged datasets focusing on productivity (length) and systematicity (shape).

## Key Results
- MORSE achieves 53.31 F1 score for Steps and 57.78 F1 score for Intermediates on EntailmentBank-Length dataset
- Outperforms EntailmentWriter baseline (52.80 and 56.62 F1 respectively) on same benchmark
- Demonstrates effectiveness through ablation studies showing performance drops with random masking
- Shows superior generalization to longer and more complex reasoning chains compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic modularized self-attention enables specialized modules to route inputs based on functional similarity
- Mechanism: MORSE uses trainable module representations (rephi) and input functional representations (fj) to compute cosine similarity. When similarity exceeds threshold τ, inputs are routed to specialized heads via dynamic masks, allowing each head to focus on specific inference functions.
- Core assumption: Different reasoning types require different input patterns, and functional similarity can effectively identify these patterns
- Evidence anchors:
  - [abstract]: "we adopt modularized self-attention to dynamically select and route inputs to dedicated heads, which specializes them to specific functions"
  - [section]: "we construct dynamic masks mi for each module hi dynamically, given sequential inputs and different module objectives"
- Break condition: If the threshold τ is poorly chosen or if reasoning types don't have distinguishable input patterns, the routing mechanism fails to specialize modules effectively

### Mechanism 2
- Claim: Freezing module representations preserves learned functions while allowing dynamic adaptation through masking
- Mechanism: When module representations (rephi) are frozen, the model retains knowledge of what each module should specialize in. The dynamic mask generation continues to adapt which inputs each module receives, allowing flexible reasoning without losing functional specialization.
- Core assumption: The learned module representations capture essential characteristics of their target functions that remain valid across different reasoning scenarios
- Evidence anchors:
  - [section]: "By freezing parameters, we preserve the module functions, but expect a comparative performance by re-using learned functions"
  - [section]: "We first freeze the module representation repi (-rep_embed) and further parameters in each specialized module ( -module)"
- Break condition: If the frozen representations become too rigid and cannot accommodate new types of reasoning patterns, the model's adaptability suffers

### Mechanism 3
- Claim: Modularized architecture outperforms monolithic transformers on compositional generalization tasks
- Mechanism: By factorizing the inference process into specialized modules, MORSE can better handle longer and more complex reasoning chains. Each module becomes expert at specific reasoning types, and dynamic routing ensures appropriate module selection for each inference step.
- Core assumption: Compositional generalization benefits from having dedicated components for different reasoning types rather than a single general-purpose mechanism
- Evidence anchors:
  - [abstract]: "MORSE factorizes the inference process into a combination of modules, where each module represents a functional unit"
  - [section]: "To address this challenging task, models need to perform compositional generalization"
- Break condition: If the task doesn't benefit from compositional structure or if the module granularity is poorly chosen, the modularized approach may underperform simpler architectures

## Foundational Learning

- Concept: Dynamic routing/masking in neural networks
  - Why needed here: MORSE uses dynamic masks to selectively route inputs to different modules based on functional similarity, enabling specialized reasoning
  - Quick check question: How does MORSE determine which inputs should be routed to which module?

- Concept: Compositional generalization in reasoning tasks
  - Why needed here: The paper focuses on generating longer entailment trees from shorter ones, requiring the model to combine known reasoning patterns in novel ways
  - Quick check question: What makes compositional generalization challenging for neural models compared to humans?

- Concept: Modular neural network architectures
  - Why needed here: MORSE factorizes reasoning into specialized modules, each potentially handling different types of inference steps
  - Quick check question: How does MORSE ensure that each module develops a specialized function rather than all modules learning similar behaviors?

## Architecture Onboarding

- Component map: Input embedding → Transformer blocks (for word representation) → Modularized Transformer blocks (for reasoning) → Decoder Transformer blocks → Output generation
- Critical path: Input → Dynamic mask generation → Modularized self-attention routing → Module specialization → Tree generation
- Design tradeoffs: Number of modules vs. model complexity; threshold τ vs. routing precision; frozen representations vs. adaptability
- Failure signatures: Poor performance on longer trees suggests insufficient module specialization; random masking causing drops indicates dynamic routing is crucial
- First 3 experiments:
  1. Test different threshold values (τ) to find optimal routing precision
  2. Compare frozen vs. non-frozen module representations on generalization performance
  3. Evaluate different numbers of modularized heads to find optimal specialization granularity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of dynamic modules in MORSE affect its ability to handle complex reasoning tasks with a large number of potential logic rules?
- Basis in paper: [inferred] The paper mentions that MORSE assumes a pre-defined number of modules for reasoning in various scenarios, and that the number of modules interacts with the model's ability to handle complex reasoning tasks.
- Why unresolved: The paper does not provide empirical evidence on how varying the number of modules affects MORSE's performance on complex reasoning tasks.
- What evidence would resolve it: Experiments comparing MORSE's performance on complex reasoning tasks with different numbers of modules would provide insights into the optimal number of modules for handling complex reasoning tasks.

### Open Question 2
- Question: How does the dynamic masking mechanism in MORSE contribute to its ability to generalize to novel reasoning configurations?
- Basis in paper: [explicit] The paper discusses the role of dynamic masking in MORSE, stating that it helps the model to connect existing modules and adapt to novel configurations.
- Why unresolved: While the paper provides insights into the role of dynamic masking, it does not provide a detailed analysis of how it contributes to MORSE's generalization abilities.
- What evidence would resolve it: A comprehensive analysis of how different masking strategies affect MORSE's performance on novel reasoning configurations would provide insights into the contribution of dynamic masking to generalization.

### Open Question 3
- Question: How does MORSE's performance compare to other neural-symbolic approaches in terms of compositional generalization?
- Basis in paper: [inferred] The paper mentions that MORSE outperforms competitive baselines on two benchmarks, but it does not provide a direct comparison with other neural-symbolic approaches.
- Why unresolved: The paper does not include experiments comparing MORSE's performance to other neural-symbolic approaches that use pre-defined inference rules for iterative reasoning.
- What evidence would resolve it: Experiments comparing MORSE's performance to other neural-symbolic approaches on the same benchmarks would provide insights into its relative strengths and weaknesses in terms of compositional generalization.

## Limitations

- Limited exploration of optimal number of modules for different reasoning complexities
- Lack of comparison with other neural-symbolic approaches that use pre-defined inference rules
- Focus on specific datasets (EntailmentBank and DBpedia) limits generalizability to other reasoning tasks

## Confidence

- Medium: Core claims about MORSE's effectiveness in compositional generalization
- Medium: Analysis of dynamic routing mechanism's importance
- Low: Detailed understanding of why modularization specifically enables generalization

## Next Checks

1. Implement ablation studies varying the number of modules to determine optimal specialization granularity
2. Test MORSE on additional reasoning datasets beyond EntailmentBank and DBpedia to assess generalizability
3. Compare MORSE against other compositional approaches (e.g., meta-learning, neural modular networks) on identical benchmarks