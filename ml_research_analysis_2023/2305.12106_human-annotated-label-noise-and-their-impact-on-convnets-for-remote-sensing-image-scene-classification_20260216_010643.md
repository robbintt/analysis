---
ver: rpa2
title: Human-annotated label noise and their impact on ConvNets for remote sensing
  image scene classification
arxiv_id: '2305.12106'
source_url: https://arxiv.org/abs/2305.12106
tags:
- noise
- errors
- labeling
- convnets
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how human labeling errors impact convolutional
  neural networks (ConvNets) for remote sensing image scene classification. Researchers
  collected real-world labels from 32 participants for 300 remote sensing images and
  compared ConvNet performance when trained on these labels versus clean labels.
---

# Human-annotated label noise and their impact on ConvNets for remote sensing image scene classification

## Quick Facts
- arXiv ID: 2305.12106
- Source URL: https://arxiv.org/abs/2305.12106
- Authors: 
- Reference count: 9
- Primary result: Human labeling errors in remote sensing images exhibit class and instance dependence, significantly impacting ConvNet performance with a 0.5% accuracy drop per 1% increase in labeling error.

## Executive Summary
This study investigates how human labeling errors impact convolutional neural networks (ConvNets) for remote sensing image scene classification. Researchers collected real-world labels from 32 participants for 300 remote sensing images and compared ConvNet performance when trained on these labels versus clean labels. The results showed that human labeling errors exhibit significant class and instance dependence, and that a 1% increase in labeling error leads to a 0.5% decrease in ConvNet classification accuracy. The error patterns in ConvNet predictions strongly correlate with participant labeling errors. Comparisons with simulated label noise (uniform and class-dependent) revealed that human labeling errors have a greater impact than uniform noise but similar impact to class-dependent noise, indicating that class-dependent errors are the primary driver of the observed effects. These findings highlight the need to account for real-world label noise when training ConvNets and provide insights for developing more robust algorithms.

## Method Summary
The study collected human-labeled data for 600 remote sensing images from the UCMerced dataset across 6 categories, with 32 participants providing labels through a structured learning-verification-labeling session. Three ConvNet architectures (VGG16, GoogLeNet, ResNet-50) were trained on the human-labeled data using RAdam optimizer with learning rate 1e-4, batch size 128, and cross-entropy loss, with 75% training and 25% validation split. Model performance was evaluated on a clean test set using overall accuracy (OA). The study compared human label noise to simulated uniform and class-dependent noise, and used mixed effects models to analyze class and instance dependence patterns in the labeling errors.

## Key Results
- Human labeling errors exhibit significant class and instance dependence across all categories except beach
- A 1% increase in labeling error rate leads to a 0.5% decrease in ConvNet classification accuracy
- Class-dependent noise has greater impact than uniform noise but similar impact to human labeling errors
- ConvNet prediction errors show strong correlation with participant labeling errors, particularly for mislabeling patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human labeling errors propagate through ConvNets in a class-dependent manner, where mislabeling one class biases predictions toward specific other classes.
- Mechanism: ConvNets learn to map noisy labels to features. If training labels are systematically mislabeled (e.g., "airplane" labeled as "runway"), the model associates runway features with airplane images. During inference, this causes higher misclassification rates between the mislabeled classes.
- Core assumption: Training and test data share the same class-dependent noise structure.
- Evidence anchors:
  - [abstract] "human labeling errors have significant class and instance dependence"
  - [section] "Post-hoc contrast analyses further showed that the error rate of airplane is lower than all categories except beach"
  - [corpus] Weak - corpus neighbors focus on noise detection/correction, not class-dependent propagation patterns
- Break condition: If noise structure differs between training and test data, or if ConvNets use denoising techniques that decouple feature learning from label corruption.

### Mechanism 2
- Claim: ConvNets tolerate human labeling errors better than random noise because class-dependent errors preserve some semantic structure.
- Mechanism: Uniform noise destroys class boundaries completely, while class-dependent noise maintains partial semantic similarity between true and corrupted labels (e.g., runway vs. freeway). ConvNets can leverage these semantic similarities to partially correct predictions during inference.
- Core assumption: The semantic similarity between classes influences ConvNet's ability to generalize from noisy labels.
- Evidence anchors:
  - [abstract] "class-dependent errors are the primary driver of the observed effects"
  - [section] "Class-dependent noise was more impactful than uniform noise but has an impact similar to human labeling errors"
  - [corpus] Weak - corpus focuses on detection/correction methods rather than comparing tolerance mechanisms
- Break condition: If class similarity is low or if ConvNet architecture prevents exploitation of semantic relationships.

### Mechanism 3
- Claim: Instance dependence in human labeling errors creates heterogeneous error patterns that ConvNets must learn to handle.
- Mechanism: Some images within a class are consistently mislabeled (e.g., certain freeway images mistaken for runways), while others are correctly labeled. ConvNets must learn instance-specific features that distinguish hard-to-classify images from easy ones, requiring more complex feature representations.
- Core assumption: Instance-level difficulty varies and affects labeling accuracy.
- Evidence anchors:
  - [section] "outliers have been identified for all categories except for beach, suggesting that participants were more likely to make error on some pictures than others"
  - [abstract] "human labeling errors have significant class and instance dependence"
  - [corpus] Weak - corpus neighbors don't address instance-level error heterogeneity
- Break condition: If instance difficulty is uniform across classes or if ConvNet architecture cannot learn instance-specific features.

## Foundational Learning

- Concept: Label noise types and their impact on supervised learning
  - Why needed here: Understanding how different noise patterns (uniform vs. class-dependent vs. instance-dependent) affect ConvNet training is crucial for interpreting the experimental results and designing appropriate noise models.
  - Quick check question: How would uniform noise affect ConvNet performance differently than class-dependent noise in a multi-class classification task?

- Concept: Linear mixed effects models for analyzing nested experimental data
  - Why needed here: The study uses mixed effects models to analyze participant errors nested within categories and images. Understanding this statistical approach is essential for interpreting the significance of class and instance dependence findings.
  - Quick check question: Why is it important to include both participant and picture as random effects when analyzing labeling errors?

- Concept: Convolutional neural network architectures and training procedures
  - Why needed here: The study compares three ConvNet architectures (VGG16, GoogLeNet, ResNet-50) and their performance under different noise conditions. Understanding these architectures and training details is necessary for implementing similar experiments.
  - Quick check question: What architectural feature distinguishes ResNet-50 from VGG16 that might affect its noise tolerance?

## Architecture Onboarding

- Component map: Data pipeline → Behavioral experiment (participant labeling) → ConvNet training (multiple architectures) → Performance evaluation (OA metrics) → Statistical analysis (mixed effects, regression)
- Critical path: Participant labeling → ConvNet training → Error analysis → Comparison with simulated noise
- Design tradeoffs: Using real human labels provides ecological validity but limits control over noise patterns compared to simulated noise. Multiple architectures provide robustness but increase computational cost.
- Failure signatures: High variance in participant accuracy suggests labeling difficulty; inconsistent error patterns between participants and ConvNets indicate poor noise propagation; poor correlation between simulated and human noise suggests model inadequacy.
- First 3 experiments:
  1. Replicate participant labeling experiment with smaller dataset to validate noise collection methodology
  2. Train single ConvNet architecture with controlled uniform noise to establish baseline performance degradation
  3. Generate class-dependent noise matching participant error patterns and compare ConvNet performance to human-labeled training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise statistical properties of human label noise in remote sensing images that make it fundamentally different from simulated label noise?
- Basis in paper: [explicit] The paper states that human label noise exhibits significant class and instance dependence, which is fundamentally different from simulated noise in previous studies.
- Why unresolved: The paper demonstrates the differences exist but doesn't provide a complete mathematical characterization of these properties or their underlying causes.
- What evidence would resolve it: A comprehensive statistical analysis comparing the distribution, correlation structure, and generative mechanisms of human versus simulated label noise across multiple datasets and annotator groups.

### Open Question 2
- Question: How do instance-dependent label noise patterns specifically impact ConvNet performance compared to class-dependent patterns?
- Basis in paper: [explicit] The paper concludes that instance dependence in human label noise affects ConvNets relatively little, but this is based on comparison with class-dependent noise rather than direct analysis of instance-dependent patterns.
- Why unresolved: The study focused on comparing human noise to uniform and class-dependent simulated noise, leaving the specific contribution of instance dependence unexplored.
- What evidence would resolve it: Direct comparison of ConvNet performance when trained on instance-dependent noise versus class-dependent noise, controlling for other variables.

### Open Question 3
- Question: What are the optimal strategies for mitigating real-world label noise in ConvNet training for remote sensing applications?
- Basis in paper: [inferred] The paper demonstrates that ConvNets tolerate human label noise to some degree but doesn't investigate or propose specific mitigation strategies.
- Why unresolved: While the paper identifies the problem and characterizes the noise, it doesn't explore solutions beyond noting that ConvNets show some robustness.
- What evidence would resolve it: Systematic evaluation of different noise-robust training methods (loss correction, sample weighting, semi-supervised approaches) specifically tailored to the characteristics of human label noise in remote sensing.

## Limitations

- The study is based on a relatively small sample of 32 participants and 600 images, which may limit generalizability to larger-scale applications.
- Class-dependent noise simulation assumes perfect knowledge of error patterns, which may not be achievable in real-world scenarios.
- The study focuses on three specific ConvNet architectures, limiting conclusions about generalizability across different model types.

## Confidence

The study provides compelling evidence that human labeling errors exhibit class and instance dependence, with strong correlations between participant errors and ConvNet predictions. However, the findings are based on a relatively small sample of 32 participants and 600 images, which may limit generalizability to larger-scale applications. The class-dependent noise simulation assumes perfect knowledge of error patterns, which may not be achievable in real-world scenarios. Confidence in the core claims is **High** for the existence of class and instance dependence in human labeling errors, **Medium** for the quantitative relationship between labeling error rates and ConvNet performance degradation, and **Low** for the generalizability of noise tolerance comparisons across different ConvNet architectures. Three concrete next validation checks include: (1) replicating the study with a larger and more diverse participant pool, (2) testing the noise tolerance findings across additional ConvNet architectures beyond the three studied, and (3) evaluating the impact of label noise on ConvNet performance in different remote sensing domains and tasks.

## Next Checks

1. Replicate the study with a larger and more diverse participant pool to validate the generalizability of class and instance dependence findings.
2. Test the noise tolerance findings across additional ConvNet architectures beyond the three studied (VGG16, GoogLeNet, ResNet-50) to assess architectural generalizability.
3. Evaluate the impact of label noise on ConvNet performance in different remote sensing domains and tasks to assess domain transferability.