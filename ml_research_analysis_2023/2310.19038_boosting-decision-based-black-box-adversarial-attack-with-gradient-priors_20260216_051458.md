---
ver: rpa2
title: Boosting Decision-Based Black-Box Adversarial Attack with Gradient Priors
arxiv_id: '2310.19038'
source_url: https://arxiv.org/abs/2310.19038
tags:
- gradient
- adversarial
- attack
- image
- decision-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes DBA-GP, a decision-based black-box adversarial
  attack framework that integrates data-dependent and time-dependent gradient priors
  to improve query efficiency. The method addresses two key limitations in existing
  gradient priors: the edge gradient discrepancy issue and the successive iteration
  gradient direction issue.'
---

# Boosting Decision-Based Black-Box Adversarial Attack with Gradient Priors

## Quick Facts
- arXiv ID: 2310.19038
- Source URL: https://arxiv.org/abs/2310.19038
- Reference count: 9
- Primary result: Proposed DBA-GP framework integrates data-dependent and time-dependent gradient priors to improve query efficiency in decision-based black-box adversarial attacks

## Executive Summary
This paper introduces DBA-GP, a novel decision-based black-box adversarial attack framework that significantly improves query efficiency by integrating two gradient priors. The method addresses key limitations in existing gradient estimation approaches by preserving edge gradients through joint bilateral filtering while automatically adjusting successive iteration directions to accelerate convergence. Extensive experiments demonstrate DBA-GP outperforms state-of-the-art baselines across multiple datasets and even achieves success against a real-world face recognition API.

## Method Summary
DBA-GP is a decision-based black-box adversarial attack framework that integrates data-dependent and time-dependent gradient priors into the gradient estimation procedure. The data-dependent prior uses joint bilateral filtering to smooth gradients for spatially close pixels with similar values while preserving edge characteristics, addressing the edge gradient discrepancy issue. The time-dependent prior introduces a new gradient updating strategy that automatically adjusts successive iteration gradient directions based on distance and similarity conditions, accelerating convergence speed. The framework seamlessly combines these priors with the HSJA baseline, using Monte Carlo gradient estimation with random perturbations to generate adversarial examples by only accessing final model predictions.

## Key Results
- DBA-GP achieves up to 48% attack success rate with only 3K queries on ImageNet
- Outperforms state-of-the-art baselines in mean squared error and attack success rate under same query budget
- Demonstrates strong performance against real-world Face++ API with 70% success rate at 1K queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint bilateral filter preserves edge gradients while smoothing similar pixel values.
- Mechanism: The filter computes weights based on both spatial proximity (gs) and intensity similarity (gr), so pixels with similar values but different gradients at edges remain unsmoothed.
- Core assumption: Edge locations correspond to sharp changes in pixel intensity, and preserving these changes is critical for accurate gradient estimation.
- Evidence anchors:
  - [section] "The joint bilateral filter [Petschnigg et al., 2004] is a non-linear, edge-preserving, and noise-reducing smoothing filter, which has the following advantages. If nearby pixels are similar, it can replace the intensity of each pixel with a weighted average of intensity values from nearby pixels. If nearby pixels are diverse, it will not smooth these nearby pixels."
  - [abstract] "First, by leveraging the joint bilateral filter to deal with each random perturbation, DBA-GP can guarantee that the generated perturbations in edge locations are hardly smoothed, i.e., alleviating the edge gradient discrepancy, thus remaining the characteristics of the original image as much as possible."

### Mechanism 2
- Claim: Time-dependent gradient adjustment accelerates convergence by exploring new directions when gradients become too similar.
- Mechanism: The algorithm tracks cosine similarity between current and past gradients, and when similarity exceeds a threshold, it subtracts an averaged past gradient direction to force exploration.
- Core assumption: In decision-based attacks, early iterations involve large jumps, so successive gradients need not be highly correlated; only later iterations benefit from temporal smoothing.
- Evidence anchors:
  - [section] "We define the set A(t) = {x(j)adv : max(1, t − k) ≤ j ≤ t − 1, Dt,j < τ, St,j > ρ }, which contains intermediate-process generated adversarial images that satisfy some conditions. Dt,j = d(x(t)adv, x(j)adv) is the distance between x(t)adv to x(j)adv. St,j is the cosine similarity between g∇S(x(t)adv, δt) and d∇S(x(j)adv, δj), where g∇S(x(t)adv, δt) is the estimated gradient in the t-th iteration with Eq. (4), and d∇S(x(j)adv, δj) is the final estimated gradient in the j-th iteration."
  - [abstract] "Second, by utilizing a new gradient updating strategy to automatically adjust the successive iteration gradient direction, DBA-GP can accelerate the convergence speed, thus improving the query efficiency."

### Mechanism 3
- Claim: Integrating both priors into the same gradient estimation loop improves query efficiency over using either alone.
- Mechanism: Data-dependent smoothing (via joint bilateral filter) ensures edge fidelity, while time-dependent adjustment ensures exploration; both are applied before the final gradient update step.
- Core assumption: Edge gradient discrepancy and successive iteration direction issues are independent and can be addressed simultaneously without interfering with each other.
- Evidence anchors:
  - [section] "In this paper, we propose a novel Decision-based Blackbox Attack framework with Gradient Priors (DBA-GP), which seamlessly integrates the data-dependent gradient prior and time-dependent prior into the gradient estimation procedure."
  - [section] "To confirm our observation, we randomly select 50 images from ImageNet [Deng et al., 2009]. For each image, we first normalize all pixels into [0, 1], calculate the absolute values of adjacent pixel differences, and assign them into five intervals [0, 2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8] and (0.8, 1.0]. Then we calculate the ratio of similar adjacent pixel gradients averaged over all images."

## Foundational Learning

- Concept: Monte Carlo gradient estimation with random perturbations.
  - Why needed here: Decision-based attacks only have access to binary outputs, so gradients must be approximated via finite differences.
  - Quick check question: In Eq. (4), why do we divide by B after summing the weighted perturbations?

- Concept: Bilateral filtering and its parameters (σs, σr).
  - Why needed here: The data-dependent prior relies on joint bilateral filtering to preserve edges while smoothing similar pixel values.
  - Quick check question: What happens to the filter output if σs is set much larger than σr?

- Concept: Cosine similarity and its role in detecting gradient direction convergence.
  - Why needed here: The time-dependent prior uses cosine similarity to decide when to adjust gradient direction.
  - Quick check question: If St,j = 1 for all past gradients, what does that imply about the optimization trajectory?

## Architecture Onboarding

- Component map: Input image → initial adversarial image → iterative loop (gradient estimation → direction update → boundary approach) → output adversarial image
- Critical path: Gradient estimation step, because query efficiency depends entirely on how accurately the gradient is estimated.
- Design tradeoffs: Larger B improves gradient accuracy but increases query count; joint bilateral filtering adds computation but reduces required queries.
- Failure signatures: If MSE plateaus early, likely edge gradient discrepancy; if attack success rate is low, likely poor gradient direction adjustment.
- First 3 experiments:
  1. Run DBA-GP on MNIST with B=50 vs B=100 and compare MSE convergence curves.
  2. Disable the joint bilateral filter and measure impact on edge preservation and attack success rate.
  3. Set ρ=0 (always explore) and observe how quickly query count explodes.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the conclusion section.

## Limitations
- Requires careful hyperparameter tuning of joint bilateral filter parameters and gradient updating thresholds, which may not generalize well across different datasets and attack scenarios.
- Computational overhead introduced by bilateral filtering could be prohibitive for real-time applications.
- Effectiveness relies heavily on the quality of gradient estimation in early iterations, which may degrade for highly non-linear decision boundaries.

## Confidence
- High confidence: The core mechanism of using joint bilateral filtering to preserve edges while smoothing similar pixel values is well-established in image processing literature and directly applicable to gradient estimation.
- Medium confidence: The time-dependent gradient adjustment strategy shows promise but may require dataset-specific tuning of similarity thresholds to avoid premature exploration.
- Medium confidence: The integration of both priors is theoretically sound, but the independence assumption between edge preservation and temporal exploration needs further validation on diverse model architectures.

## Next Checks
1. Conduct ablation studies systematically disabling each gradient prior to quantify their individual contributions to query efficiency improvements.
2. Test DBA-GP against defense mechanisms that specifically target gradient estimation (e.g., gradient masking, input preprocessing) to evaluate robustness.
3. Implement cross-dataset generalization tests where model trained on one dataset (e.g., ImageNet) is attacked using DBA-GP tuned on a different dataset (e.g., Celeba) to assess parameter transferability.