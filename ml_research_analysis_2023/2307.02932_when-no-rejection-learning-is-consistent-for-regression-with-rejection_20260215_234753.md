---
ver: rpa2
title: When No-Rejection Learning is Consistent for Regression with Rejection
arxiv_id: '2307.02932'
source_url: https://arxiv.org/abs/2307.02932
tags:
- loss
- learning
- rejection
- rejector
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the regression with rejection problem, where
  a predictor and rejector jointly decide whether to predict or defer a sample to
  humans. It shows that under weak realizability (the function class covers the conditional
  expectation), the standard no-rejection learning strategy is optimal.
---

# When No-Rejection Learning is Consistent for Regression with Rejection

## Quick Facts
- arXiv ID: 2307.02932
- Source URL: https://arxiv.org/abs/2307.02932
- Reference count: 36
- Primary result: Shows that standard no-rejection learning is optimal for regression with rejection under weak realizability, and proposes a kernel-based algorithm for rejector calibration

## Executive Summary
This paper studies regression with rejection, where a predictor and rejector jointly decide whether to predict or defer to humans. The authors establish that no-rejection learning (training predictor on all data using standard loss) is optimal when the predictor function class covers the conditional expectation function. For the general case, they introduce a truncated loss that isolates predictor learning and proves the squared loss serves as a consistent surrogate. A nonparametric kernel-based algorithm is proposed for rejector calibration, and experiments on UCI datasets demonstrate superior performance over several benchmark methods.

## Method Summary
The method employs a two-stage approach: first, a neural network predictor is trained on all training data using standard squared loss without rejection; second, a nonparametric kernel-based algorithm calibrates the rejector using validation data. The rejector estimates the conditional expected loss as a function of the feature and thresholds at the deferral cost to decide whether to predict or reject. This approach isolates predictor learning from rejector calibration, enabling theoretical analysis and practical implementation.

## Key Results
- Under weak realizability (function class covers conditional expectation), no-rejection learning is optimal for any rejector
- Truncated loss analysis enables generalization bounds with estimation and approximation error terms
- Kernel-based rejector calibration outperforms several benchmark methods on UCI regression datasets
- Proposed approach achieves better RwR loss and rejection rate tradeoffs compared to Triage, SelNet, and kNN baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: No-rejection learning is optimal when the predictor function class covers the conditional expectation function
- Mechanism: Under weak realizability, the conditional expectation function minimizes the regression with rejection loss for any rejector, so using all data to train the predictor directly aligns with the optimal solution
- Core assumption: The function class F includes the conditional expectation function E[Y|X=x]
- Evidence anchors:
  - [abstract]: "We first establish the consistency for such a strategy under the weak realizability condition."
  - [section]: "Proposition 2. If the weak realizability condition holds, i.e.,¯f(·) ∈ F, then ¯f(·) minimizes the expected RwR loss for any measurable rejector functionr(·)."
  - [corpus]: Weak evidence - no direct mention of weak realizability, but related work on regression with rejection exists
- Break condition: If the function class does not include the conditional expectation function, the optimal predictor may not be in F, leading to suboptimality

### Mechanism 2
- Claim: The truncated loss isolates predictor learning and enables surrogate property analysis
- Mechanism: By truncating the expected loss at the rejection cost c, the truncated loss singles out the predictor learning problem, allowing the squared loss to serve as a consistent surrogate
- Core assumption: The rejector class G is rich enough to cover all measurable functions
- Evidence anchors:
  - [abstract]: "Then for the case without the weak realizability, we show that the excessive risk can also be upper bounded with the sum of two parts: prediction error and calibration error."
  - [section]: "Proposition 4 establishes the truncated loss as a proxy of theRwR loss and states the relationship between these two."
  - [corpus]: Weak evidence - no direct mention of truncated loss, but related work on surrogate losses exists
- Break condition: If the rejector class is not rich enough, the gap between truncated loss and RwR loss may not be controlled

### Mechanism 3
- Claim: Nonparametric kernel-based calibration provides a flexible rejector that adapts to any regressor
- Mechanism: Algorithm 1 uses kernel estimation to learn the conditional expected loss as a function of the feature, then thresholds at the deferral cost to determine rejection
- Core assumption: The validation data is independent of the training data and representative of the test distribution
- Evidence anchors:
  - [section]: "Algorithm 1 learns the rejector based on a given regressorˆf. It first uses a nonparametric approach to estimate the condition expected lossE[(Y − ˆf(X))2|X = x] at a new data point."
  - [corpus]: Weak evidence - no direct mention of nonparametric calibration, but related work on uncertainty quantification exists
- Break condition: If the validation data is not representative or the kernel bandwidth is poorly chosen, the rejector calibration may fail

## Foundational Learning

- Concept: Weak realizability
  - Why needed here: It is the key condition under which no-rejection learning is optimal, as it ensures the conditional expectation function is in the predictor function class
  - Quick check question: What happens to the optimality of no-rejection learning if the function class does not include the conditional expectation function?

- Concept: Truncated loss
  - Why needed here: It isolates the predictor learning problem and enables surrogate loss analysis, providing a bridge between the original RwR loss and the squared loss
  - Quick check question: How does the truncated loss relate to the original RwR loss, and what role does the rejector class richness play?

- Concept: Surrogate loss
  - Why needed here: It allows the use of convex optimization techniques and provides generalization bounds, enabling theoretical analysis of no-rejection learning
  - Quick check question: What properties must a surrogate loss satisfy to be useful for theoretical analysis of the RwR problem?

## Architecture Onboarding

- Component map:
  - Neural network predictor (trained on all data with squared loss) -> Kernel-based rejector (calibrated on validation data) -> RwR loss computation

- Critical path:
  1. Train predictor on all training data using squared loss
  2. Estimate conditional expected loss using kernel method on validation data
  3. Calibrate rejector by thresholding estimated loss at deferral cost
  4. Evaluate RwR loss on test data

- Design tradeoffs:
  - Predictor expressiveness vs. overfitting: Richer function classes improve performance but may overfit
  - Kernel bandwidth selection: Affects accuracy of conditional loss estimation and rejector calibration
  - Validation data usage: Separating validation from training ensures unbiased rejector calibration

- Failure signatures:
  - Poor predictor performance: High RwR loss, low rejection rate
  - Poor rejector calibration: High RwR loss, high rejection rate
  - Overfitting: Low training loss but high validation/test loss

- First 3 experiments:
  1. Vary predictor function class richness (e.g., linear, MLP, wide MLP) and measure RwR loss
  2. Vary kernel bandwidth and measure rejector calibration accuracy on validation data
  3. Compare no-rejection learning against joint learning baselines on UCI regression datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the no-rejection learning strategy remain optimal for regression with rejection when the function class F is not rich enough to cover the conditional expectation function, but is still reasonably large?
- Basis in paper: [inferred] The paper shows that no-rejection learning is optimal under weak realizability (when F covers the conditional expectation function). However, it also introduces a truncated loss that allows full flexibility for the rejector, leading to a generalization bound with estimation and approximation error terms. This suggests that the optimality of no-rejection learning may depend on the richness of the function class F.
- Why unresolved: The paper does not explicitly investigate the performance of no-rejection learning when the function class F is not rich enough to cover the conditional expectation function, but is still reasonably large.
- What evidence would resolve it: Experimental results comparing the performance of no-rejection learning with other methods for different levels of richness in the function class F.

### Open Question 2
- Question: How does the performance of the proposed nonparametric kernel-based algorithm for calibrating the rejector compare to other calibration methods in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The paper proposes a nonparametric kernel-based algorithm for calibrating the rejector and demonstrates its advantage over several benchmark methods on UCI datasets. However, it does not provide a comprehensive comparison with other calibration methods.
- Why unresolved: The paper does not provide a detailed comparison of the proposed algorithm with other calibration methods in terms of accuracy and computational efficiency.
- What evidence would resolve it: Experimental results comparing the performance of the proposed algorithm with other calibration methods on a variety of datasets, considering both accuracy and computational efficiency.

### Open Question 3
- Question: Can the theoretical results and insights from the regression with rejection problem be extended to other types of prediction tasks, such as multi-output regression or time series forecasting?
- Basis in paper: [explicit] The paper mentions that while the discussions have mainly focused on the regression problem, the theoretical results and insights generalize to the classification problem as well. However, it does not explicitly discuss the extension to other types of prediction tasks.
- Why unresolved: The paper does not provide a detailed investigation of the applicability of the theoretical results and insights to other types of prediction tasks beyond regression and classification.
- What evidence would resolve it: Theoretical analysis and experimental results demonstrating the extension of the results to other types of prediction tasks, such as multi-output regression or time series forecasting.

## Limitations

- Theoretical results rely on strong weak realizability assumptions that may not hold in practical settings
- Empirical validation is limited to 4 UCI datasets with relatively small sample sizes
- Kernel-based rejector calibration introduces additional hyperparameters (bandwidth selection) that could affect performance
- No comparison against modern deep learning approaches or end-to-end learned joint models

## Confidence

- Theoretical optimality under weak realizability: Medium - The proofs appear sound but depend on strong assumptions about function class coverage
- Surrogate loss analysis: Medium - The truncated loss approach is novel but the relationship to actual RwR performance needs more empirical validation
- Algorithm performance: Low-Medium - Limited experimental scope and no comparison against modern deep learning approaches

## Next Checks

1. Test algorithm performance on larger, more diverse datasets with varying noise levels to assess robustness to weak realizability violations
2. Conduct ablation studies on the kernel bandwidth selection process to understand its impact on rejector calibration quality
3. Compare against end-to-end learned approaches (e.g., neural networks trained with joint predictor-rejector objectives) to validate the no-rejection learning strategy's practical benefits