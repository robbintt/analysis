---
ver: rpa2
title: 'DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight
  Factorization'
arxiv_id: '2312.13211'
source_url: https://arxiv.org/abs/2312.13211
tags:
- dsformer
- compression
- factorization
- which
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DSFormer introduces a dense-sparse factorization scheme to compress
  transformer models more effectively than existing low-rank approaches. Instead of
  low-rank decomposition, it approximates each weight matrix block using a sparse
  coefficient matrix and a dense basis matrix, which better matches the natural weight
  distribution in transformers.
---

# DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization

## Quick Facts
- **arXiv ID**: 2312.13211
- **Source URL**: https://arxiv.org/abs/2312.13211
- **Reference count**: 12
- **Primary result**: Up to 40% better compression than state-of-the-art low-rank factorization methods (DRONE, FWSVD) while maintaining accuracy

## Executive Summary
DSFormer introduces a novel dense-sparse factorization scheme that achieves superior compression of transformer models compared to traditional low-rank approaches. By expressing weight matrices as the product of a small dense basis matrix and a semi-structured sparse coefficient matrix, DSFormer better captures the natural weight distribution in transformers. The method includes a novel Straight-Through Factorizer (STF) optimizer that learns factorization structure during task-specific training, improving accuracy over task-agnostic approaches. On GLUE and SQuAD benchmarks, DSFormer achieves competitive or superior performance compared to baselines like DistilBERT and TinyBERT while being significantly smaller and faster.

## Method Summary
DSFormer works by factorizing each weight matrix into a sparse coefficient matrix S and a dense basis matrix D, where the weight matrix W is approximated as W ≈ DS. The method uses block-wise factorization with block size B, where each block of width B is approximated using inner dimension K and sparsity S. The sparse matrix S has exactly S non-zero values in each contiguous range of K rows, creating a semi-structured sparsity pattern. DSFormer employs a Straight-Through Factorizer (STF) that jointly learns the factorization structure during task-specific training through gradient updates and recomputation of S using Orthogonal Matching Pursuit in each forward pass.

## Key Results
- Achieves up to 40% better compression than state-of-the-art low-rank factorization methods (DRONE, FWSVD)
- Maintains competitive accuracy on GLUE and SQuAD benchmarks compared to baselines like DistilBERT and TinyBERT
- Hardware-friendly due to semi-structured sparsity, enabling efficient inference on CPUs and GPUs
- Orthogonal to other compression techniques, allowing additional 50% compression when combined with distillation or quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense-sparse factorization achieves more accurate weight approximations than low-rank factorization for the same compression ratio.
- Mechanism: The DSFormer scheme divides each weight matrix into blocks and approximates each block using a sparse coefficient matrix and a dense basis matrix. This creates a locally low-rank approximation where each row is represented as a linear combination of a small subset of basis vectors.
- Core assumption: Transformer weight matrices are geometrically well-spread and occupy full-dimensional space, making global low-rank assumptions restrictive, but allowing for locally low-rank approximations.
- Evidence anchors: The paper shows transformer weights are geometrically well-spread and occupy full-dimensional space, making low-rank assumptions restrictive.

### Mechanism 2
- Claim: The semi-structured sparsity pattern enables hardware-efficient implementations on both CPUs and GPUs.
- Mechanism: The sparse coefficient matrix S has exactly S non-zero values in each contiguous range of K rows, creating a block-structured sparsity pattern. This predictable access pattern allows better cache utilization and is supported by specialized hardware like NVIDIA Ampere's sparse tensor cores.
- Core assumption: Modern hardware architectures can efficiently handle semi-structured sparsity patterns with predictable access patterns.
- Evidence anchors: The paper references NVIDIA Ampere's sparse tensor cores and discusses how block structure creates predictable data access patterns.

### Mechanism 3
- Claim: Straight-Through Factorizer (STF) improves accuracy by learning the factorization structure in a task-aware manner rather than using task-agnostic initialization.
- Mechanism: STF gradually refines the dense factor D through gradient updates and recomputes the sparse factor S in each forward pass using Orthogonal Matching Pursuit. This allows the factorization to adapt to the specific task objective rather than being fixed by initial approximation.
- Core assumption: The optimal factorization structure depends on the specific task and can be learned through joint optimization with the model parameters.
- Evidence anchors: The paper demonstrates STF improves accuracy over K-SVD on GLUE and SQuAD tasks by learning factorization structure during task-specific training.

## Foundational Learning

- **Matrix factorization and low-rank approximation**: Understanding the difference between global low-rank factorization and local dense-sparse factorization is crucial for grasping DSFormer's approach. Quick check: What is the key difference between approximating a matrix as UV^T versus using block-wise dense-sparse factorization?

- **Semi-structured sparsity patterns**: The hardware efficiency of DSFormer relies on understanding how structured sparsity differs from unstructured sparsity. Quick check: How does semi-structured sparsity differ from both dense matrices and unstructured sparse matrices in terms of computational efficiency?

- **Straight-through estimators and gradient estimation for discrete operations**: STF uses straight-through estimation to handle the discrete sparsity constraints during backpropagation. Quick check: How does the straight-through estimator allow gradient-based optimization through discrete operations like sparsity selection?

## Architecture Onboarding

- **Component map**: Dense basis matrix D (K × B) -> Sparse coefficient matrix S (M × K) -> Weight matrix W (M × B), with Straight-Through Factorizer handling the forward and backward passes for factorization

- **Critical path**: 1. Fine-tune pre-trained BERT on task (FT stage), 2. Apply DS-factorization to all weight matrices in self-attention blocks, 3. Train with Straight-Through Factorizer (STF stage), 4. Inference using efficient block-wise semi-structured sparse matrix multiplication

- **Design tradeoffs**: Compression vs accuracy (higher compression leads to more approximation error), hardware efficiency vs flexibility (semi-structured sparsity enables acceleration but limits patterns), computational cost vs accuracy (STF provides better accuracy but increases training time)

- **Failure signatures**: Accuracy degradation (insufficient compression ratio or poor parameter choices), slow inference (hardware doesn't support semi-structured sparsity efficiently), training instability (STF parameters not properly tuned)

- **First 3 experiments**: 1. Implement block-wise dense-sparse factorization on a single weight matrix and compare approximation error to low-rank factorization, 2. Benchmark the inference speed of DSFormer on CPU vs GPU for different sparsity patterns, 3. Compare the accuracy of STF vs traditional task-agnostic factorization on a simple task like SST-2

## Open Questions the Paper Calls Out

- How does the computational intensity of DSFormer compare to other semi-structured sparsity approaches like ASP and NxMTransformer under different hardware constraints?
- Can DSFormer be effectively extended to multi-task learning scenarios where a single compressed model needs to perform well across multiple NLP tasks?
- What is the theoretical relationship between the block size B in DSFormer and the optimal values of γ and δ for achieving maximum compression without accuracy loss?

## Limitations
- Limited ablation studies on DSFormer design choices (block size B, sparsity pattern parameters)
- Hardware efficiency claims rely on theoretical benefits without extensive empirical validation across diverse platforms
- STF optimizer requires significantly more computation than task-agnostic initialization, limiting practical applicability for very large models

## Confidence
- **High Confidence**: Dense-sparse factorization provides more accurate weight approximations than low-rank factorization for the same compression ratio (well-supported by theoretical arguments)
- **Medium Confidence**: Hardware efficiency claims for semi-structured sparsity patterns (limited empirical validation across hardware platforms)
- **Medium Confidence**: Effectiveness of STF optimizer in improving accuracy over task-agnostic initialization (computational overhead and scalability concerns)

## Next Checks
1. **Ablation Study on Design Parameters**: Systematically evaluate the impact of block size B, sparsity ratio δ, and inner dimension ratio γ on both compression ratio and accuracy across multiple tasks to identify optimal configurations.

2. **Hardware Performance Benchmarking**: Measure actual inference speed and memory usage of DSFormer models on diverse hardware platforms (different GPU architectures, mobile devices) to validate the claimed hardware efficiency benefits of semi-structured sparsity.

3. **Scalability Analysis**: Test DSFormer on larger transformer models (e.g., BERT-large, RoBERTa) and downstream tasks to evaluate whether the accuracy improvements and computational efficiency of STF scale effectively with model size.