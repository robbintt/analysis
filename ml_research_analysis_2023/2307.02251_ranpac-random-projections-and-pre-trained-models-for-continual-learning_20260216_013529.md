---
ver: rpa2
title: 'RanPAC: Random Projections and Pre-trained Models for Continual Learning'
arxiv_id: '2307.02251'
source_url: https://arxiv.org/abs/2307.02251
tags:
- learning
- training
- pre-trained
- task
- petl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles continual learning with pre-trained models,
  focusing on class-incremental and domain-incremental scenarios. The authors propose
  RanPAC, a method that leverages frozen random projections and class-prototype accumulation
  to bypass the issue of catastrophic forgetting during parameter updates.
---

# RanPAC: Random Projections and Pre-trained Models for Continual Learning

## Quick Facts
- arXiv ID: 2307.02251
- Source URL: https://arxiv.org/abs/2307.02251
- Authors: Various
- Reference count: 40
- Primary result: RanPAC reduces final error rates by 20% to 62% on seven class-incremental benchmarks using ViT-B/16 without rehearsal memory.

## Executive Summary
RanPAC introduces a simple yet effective approach for continual learning with pre-trained models, combining frozen random projections with class-prototype accumulation. The method addresses catastrophic forgetting by avoiding parameter updates during incremental learning phases, instead using a random projection layer to enhance feature separability and decorrelating class-prototypes to improve classification calibration. Experiments demonstrate significant improvements over prior rehearsal-free methods across multiple benchmarks, with the approach being particularly effective when combined with first-session PETL adaptation.

## Method Summary
RanPAC leverages frozen random projections with nonlinear activation between pre-trained features and the output head to improve linear separability for class-prototype-based continual learning. The method accumulates class-prototypes and their Gram matrix across tasks, then applies ridge regression with decorrelation to compute classification weights. Optionally, PETL parameters can be trained only on the first task to bridge domain gaps. The entire system avoids iterative parameter updates, preventing catastrophic forgetting while maintaining competitive accuracy.

## Key Results
- Reduces final error rates by 20% to 62% compared to previous rehearsal-free methods
- Outperforms prior approaches on seven class-incremental benchmarks
- Achieves results approaching joint-training upper bounds when combined with first-session PETL
- Demonstrates effectiveness across multiple architectures (ViT, ResNet, CLIP) and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random projections with nonlinearity create higher-dimensional feature interactions that are more linearly separable than raw pre-trained features.
- Mechanism: A frozen random projection layer expands dimensionality from L to M > L, then applies element-wise nonlinear activation. This simulates higher-order feature interactions and creates an isotropic Gaussian-like distribution in the projected space, which enhances linear separability for class-prototype-based classification.
- Core assumption: Pre-trained model features are informative but may not be optimally separable for incremental learning; nonlinear random projections can improve this without risking catastrophic forgetting since weights are frozen.
- Evidence anchors:
  - [abstract]: "we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL."
  - [section]: "We show that random projections are particularly useful when also using PETL methods with first-session training... Accuracy with this combination approaches the CL joint training upper bound on some datasets."
  - [corpus]: Weak/no direct evidence; this is a novel contribution not covered in the neighbor papers.
- Break condition: If M is too small or nonlinearity is omitted, separability gains disappear (see Figure 3).

### Mechanism 2
- Claim: Decorrelating class-prototypes using second-order statistics reduces distribution disparity and improves classification calibration.
- Mechanism: By accumulating the Gram matrix G and class-prototypes C across tasks, then computing Wo = (G + λI)^{-1}C, the method effectively decorrelates prototypes and learns a linear transform equivalent to LDA or ZCA whitening, which improves class separation.
- Core assumption: Raw class-prototypes from pre-trained models are highly correlated between classes, leading to poorly calibrated cosine similarities; decorrelation mitigates this domain shift.
- Evidence anchors:
  - [abstract]: "We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations."
  - [section]: "We show in Section 5 that Eqn. (2) leads to better results than NCM... when using Eqn. (2), the result is to increase the training-set accuracy from 64% to 75%, coinciding with reduced overlap between inter- and true-class similarity distributions."
  - [corpus]: Weak/no direct evidence; the decorrelation insight is novel to this work.
- Break condition: If the pre-trained features are already well-decorrelated or the domain gap is negligible, gains may be minimal.

### Mechanism 3
- Claim: First-session PETL adaptation bridges the domain gap between pre-training and downstream data without risking forgetting in later tasks.
- Mechanism: PETL parameters are trained only on the first task using SGD with cross-entropy loss, then frozen. This adapts the feature extractor to the new domain while avoiding iterative fine-tuning that causes forgetting.
- Core assumption: The first task's data is sufficiently representative of the downstream domain to benefit later tasks; PETL can improve feature quality without catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "we show that random projections are particularly useful when also using PETL methods with first-session training... Accuracy with this combination approaches the CL joint training upper bound on some datasets."
  - [section]: "First-session training of PETL parameters requires a temporary linear output layer, learned using SGD with cross-entropy loss and softmax and only K1 classes, which is discarded prior to Phase 2."
  - [corpus]: Weak/no direct evidence; this combination strategy is novel to this work.
- Break condition: If the first task is not representative or the domain shift is too large, PETL benefits may be limited or absent.

## Foundational Learning

- Concept: Class-incremental learning (CIL) and domain-incremental learning (DIL)
  - Why needed here: The method must handle both scenarios—disjoint classes per task (CIL) and same classes with distribution shift (DIL)—without rehearsal buffers.
  - Quick check question: In CIL, are class labels disjoint across tasks, and is task identity unknown during inference?

- Concept: Catastrophic forgetting and rehearsal-free strategies
  - Why needed here: The approach explicitly avoids forgetting by freezing parameters and using accumulation instead of iterative updates; understanding why rehearsal is avoided is key.
  - Quick check question: Why does freezing the random projection layer and class-prototype accumulation prevent catastrophic forgetting?

- Concept: Linear discriminant analysis (LDA) and Mahalanobis distance
  - Why needed here: The decorrelation step is mathematically equivalent to LDA classification; knowing this helps understand why second-order statistics matter.
  - Quick check question: How does using the Gram matrix inverse in Eqn. (2) relate to LDA and Mahalanobis distance?

## Architecture Onboarding

- Component map:
  Pre-trained backbone (e.g., ViT-B/16) -> frozen feature extractor -> Random projection layer (L × M weights, frozen, Gaussian) -> Element-wise nonlinearity (e.g., ReLU or square) -> Class-prototype accumulator (Gram matrix G, prototype matrix C) -> Ridge regression solver (λ optimization) -> Wo = (G + λI)^{-1}C -> Optional PETL adapter (trained only on first task, then frozen)

- Critical path:
  1. Phase 1: Train PETL parameters on first task only (if used)
  2. Phase 2: For each task and sample, extract features, apply frozen RP + nonlinearity, update G and C, compute Wo after each task
  3. Inference: Apply RP + nonlinearity to test features, compute scores via Eqn. (5)

- Design tradeoffs:
  - Larger M -> better separability but higher memory/compute for Gram matrix
  - More PETL adapters -> potentially better domain adaptation but increased first-task cost
  - Frozen vs learned RP -> no forgetting but no task-specific adaptation

- Failure signatures:
  - Degraded accuracy when M is too small or nonlinearity is omitted
  - No benefit over NCM if domain gap is negligible or features are already decorrelated
  - PETL adds little value in DIL due to cross-domain training

- First 3 experiments:
  1. Run Algorithm 1 on split CIFAR100 with M=10000, compare to NCM baseline
  2. Vary M (100, 1000, 10000) on CIFAR100, plot accuracy vs M
  3. Enable/disable PETL (Phase 1) on CIFAR100, measure impact on final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dimensionality M for random projections in different CL scenarios and datasets?
- Basis in paper: [explicit] The paper mentions M = 10000 for most experiments and shows in Appendix F.5 that accuracy increases with M up to 15000 for CIFAR100.
- Why unresolved: The paper does not provide a systematic study of M across different datasets, model architectures, or CL scenarios (CIL vs DIL). The optimal M may depend on these factors.
- What evidence would resolve it: A comprehensive study varying M across multiple datasets, model architectures (CNNs, transformers), and CL scenarios, with analysis of how M affects accuracy and computational efficiency.

### Open Question 2
- Question: How does the performance of RanPAC compare to methods that use rehearsal buffers?
- Basis in paper: [inferred] The paper emphasizes that RanPAC does not use rehearsal buffers, but does not directly compare to methods that do use them.
- Why unresolved: The paper only compares to rehearsal-free methods. It's unclear how much performance gain could be achieved by adding a small rehearsal buffer to RanPAC.
- What evidence would resolve it: Experiments comparing RanPAC with and without small rehearsal buffers on the same benchmarks, measuring the trade-off between memory usage and accuracy.

### Open Question 3
- Question: Can RanPAC be effectively combined with prompting strategies for transformers?
- Basis in paper: [inferred] The paper mentions that prompting strategies have been successful for CL with transformers, but does not explore combining them with RanPAC.
- Why unresolved: The paper focuses on CP strategies and does not investigate how RP could enhance prompting methods, which could potentially combine the benefits of both approaches.
- What evidence would resolve it: Experiments applying RanPAC's RP layer to prompting-based CL methods, measuring whether this improves performance over either approach alone.

## Limitations
- The method's dependence on large random projection dimensionality (M) raises scalability concerns for very high-dimensional pre-trained features.
- The decorrelation step assumes linear separability in the projected space, which may not hold for highly complex or overlapping class distributions.
- PETL benefits are only shown for first-session adaptation, with unclear generalization to later tasks or different domain shifts.
- No comparison to rehearsal-based methods that use limited memory, making it unclear if the rehearsal-free advantage is significant.

## Confidence
- **High**: Frozen random projections with nonlinearity improve linear separability; decorrelation reduces class-prototype correlation.
- **Medium**: PETL first-session adaptation bridges domain gap; overall method outperforms prior rehearsal-free approaches.
- **Low**: The method approaches joint-training upper bounds; random projections are "particularly useful" only with PETL.

## Next Checks
1. Run ablation: disable decorrelation (use NCM) and measure increase in final error on CIFAR100.
2. Vary M from 100 to 20000 on CIFAR100 and plot final accuracy to find optimal dimensionality.
3. Test PETL on second task instead of first (break the "first-session" assumption) and measure performance drop.