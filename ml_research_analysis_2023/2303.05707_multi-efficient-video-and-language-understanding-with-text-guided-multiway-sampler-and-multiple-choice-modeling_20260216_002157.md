---
ver: rpa2
title: 'MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler
  and Multiple Choice Modeling'
arxiv_id: '2303.05707'
source_url: https://arxiv.org/abs/2303.05707
tags:
- video
- text
- features
- feature
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MuLTI, an efficient and effective video-and-language
  understanding model that addresses the challenge of processing dense video frames
  and long text descriptions prevalent in industrial applications. MuLTI introduces
  a Text-Guided MultiWay-Sampler based on adapt-pooling residual mapping and self-attention
  modules to sample long sequences and fuse multi-modal features, reducing computational
  costs and addressing performance degradation.
---

# MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler and Multiple Choice Modeling

## Quick Facts
- **arXiv ID**: 2303.05707
- **Source URL**: https://arxiv.org/abs/2303.05707
- **Reference count**: 37
- **Primary result**: MuLTI achieves state-of-the-art performance on seven English video-and-language tasks and one Chinese multi-label classification task, with examples like 47.8% accuracy on MSRVTT-QA and 75.6% accuracy on TGIF-Frame.

## Executive Summary
MuLTI addresses the challenge of processing dense video frames and long text descriptions in industrial video-and-language understanding applications. The model introduces a Text-Guided MultiWay-Sampler that efficiently samples long sequences and fuses multi-modal features, reducing computational costs and improving performance. Additionally, MuLTI incorporates an attention-based adapter for memory-efficient fine-tuning and proposes a new pretraining task, Multiple Choice Modeling, to bridge the gap between pretraining and downstream video question answering tasks. Experimental results demonstrate state-of-the-art performance across multiple benchmarks.

## Method Summary
MuLTI processes video frames and text descriptions through separate encoders, then employs a Text-Guided MultiWay-Sampler to condense text features and fuse multi-modal features efficiently. The model uses an attention-based adapter for fine-tuning shallow features with low memory consumption. For pretraining, MuLTI introduces Multiple Choice Modeling, which requires the model to match videos with the most relevant text descriptions from a constructed collection. The model is pretrained on WebVid-2M and CC-3M datasets for 10 epochs before being fine-tuned on downstream tasks with task-specific hyperparameters.

## Key Results
- Achieves 47.8% accuracy on MSRVTT-QA and 75.6% accuracy on TGIF-Frame
- Demonstrates state-of-the-art performance on seven English video-and-language tasks
- Shows effective handling of dense video frames and long text descriptions
- Introduces Multiple Choice Modeling, a new pretraining task for video question answering

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MultiWay-Sampler reduces memory cost and improves accuracy by sampling long sequence features and fusing multi-modal features efficiently.
- **Mechanism**: Uses self-attention modules and attention-based residual mapping to condense text features and fuse multi-modal features, sharing self-attention modules and reserving different feed-forward networks for different modalities.
- **Core assumption**: Sampling long sequence features using self-attention modules and attention-based residual mapping can reduce memory cost while maintaining or improving accuracy.
- **Evidence anchors**: Abstract mentions MultiWay-Sampler based on adapt-pooling residual mapping and self-attention modules to sample long sequences and fuse multi-modal features. Section describes MultiWay-Sampler as adapted from transformer decoder to condense text features and fuse different modal features efficiently.

### Mechanism 2
- **Claim**: Attention-Adapter improves model performance with low memory consumption by fine-tuning shallow features instead of all layers.
- **Mechanism**: Introduces a lightweight attention module to the adapter, allowing the model to focus more on critical tokens while fine-tuning shallow features using much fewer parameters compared to unfreezing all layers.
- **Core assumption**: Fine-tuning shallow features with an attention-based adapter can improve performance while reducing memory consumption.
- **Evidence anchors**: Abstract mentions introducing an attention-based adapter to encoders that finetunes shallow features to improve performance with low GPU memory consumption. Section describes attention-based adapter allowing model to optimize shallow features using much fewer parameters.

### Mechanism 3
- **Claim**: Multiple Choice Modeling (MCM) bridges the gap between pretraining and downstream tasks, particularly in video question answering.
- **Mechanism**: Introduces a new pretraining task that requires the model to find text descriptions that match the video most from a randomly constructed collection, enhancing alignment between video and text features.
- **Core assumption**: Constructing multiple-choice question answering tasks on large-scale video-text datasets can improve the model's ability to align video and text features.
- **Evidence anchors**: Abstract mentions proposing Multiple Choice Modeling to enhance model performance and fill lack of pretraining tasks in video question answering. Section describes MCM aiming to reduce task gap between pretraining and downstream tasks by introducing videoQA tasks during pretraining.

## Foundational Learning

- **Concept**: Feature fusion in multi-modal learning
  - **Why needed here**: MuLTI needs to efficiently fuse video and text features to achieve accurate video-and-language understanding.
  - **Quick check question**: How does the MultiWay-Sampler condense text features and fuse multi-modal features in MuLTI?

- **Concept**: Pretraining tasks for video-and-language models
  - **Why needed here**: MuLTI introduces a new pretraining task, Multiple Choice Modeling, to bridge the gap between pretraining and downstream tasks.
  - **Quick check question**: What is the purpose of the Multiple Choice Modeling task in MuLTI's pretraining?

- **Concept**: Memory-efficient model design
  - **Why needed here**: MuLTI aims to handle longer sequences with limited GPU memory by using efficient feature fusion and training strategies.
  - **Quick check question**: How does MuLTI's Attention-Adapter contribute to memory-efficient model design?

## Architecture Onboarding

- **Component map**: Video encoder -> Text encoder -> MultiWay-Sampler (feature fusion module) -> Attention-Adapter (training strategy)
- **Critical path**: Video and text encoders extract features → MultiWay-Sampler condenses and fuses features → Attention-Adapter fine-tunes shallow features → MCM improves alignment between video and text features
- **Design tradeoffs**: Balancing computational efficiency and accuracy in feature fusion, choosing between unfreezing all layers or using an Attention-Adapter for training, selecting appropriate pretraining tasks
- **Failure signatures**: Decreased accuracy in downstream tasks, increased memory consumption during training or inference, poor alignment between video and text features
- **First 3 experiments**:
  1. Implement and test the MultiWay-Sampler's ability to condense text features and fuse multi-modal features efficiently.
  2. Evaluate the effectiveness of the Attention-Adapter in fine-tuning shallow features and reducing memory consumption.
  3. Assess the impact of the Multiple Choice Modeling task on improving the alignment between video and text features during pretraining.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, several areas for future exploration are implied:
- How does the MultiWay-Sampler's performance scale with increasing sequence length beyond the tested range?
- What is the impact of different query initialization strategies on the MultiWay-Sampler's performance?
- How does the Attention-Adapter's performance change when applied to different layers of the text encoder?

## Limitations
- Limited direct evidence supporting the effectiveness of core mechanisms from corpus search
- Unknown details of the in-house multi-label classification dataset
- Complex architecture with multiple novel components that may complicate implementation and fine-tuning

## Confidence
- **MultiWay-Sampler Efficiency**: Medium confidence
- **Attention-Adapter Effectiveness**: Medium confidence
- **Multiple Choice Modeling Impact**: Medium confidence

## Next Checks
1. Implement and test the MultiWay-Sampler on a standard benchmark to isolate its contribution to performance.
2. Evaluate the Attention-Adapter's memory efficiency by comparing it against traditional fine-tuning methods on a memory-intensive task.
3. Conduct an ablation study to assess the impact of the Multiple Choice Modeling pretraining task on downstream video question answering performance.