---
ver: rpa2
title: 'Hate Speech and Offensive Content Detection in Indo-Aryan Languages: A Battle
  of LSTM and Transformers'
arxiv_id: '2312.05671'
source_url: https://arxiv.org/abs/2312.05671
tags:
- hate
- speech
- language
- https
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting hate speech and offensive
  content in low-resource Indo-Aryan languages, including Bengali, Assamese, Bodo,
  Sinhala, and Gujarati. The authors conduct a comparative analysis of various pre-trained
  models, including BERT variants, XLM-R, and LSTM models, to assess their performance
  in identifying hate speech across these languages.
---

# Hate Speech and Offensive Content Detection in Indo-Aryan Languages: A Battle of LSTM and Transformers

## Quick Facts
- arXiv ID: 2312.05671
- Source URL: https://arxiv.org/abs/2312.05671
- Reference count: 40
- Key outcome: BERT Base Multilingual Cased and XLM-R outperform other models in hate speech detection across low-resource Indo-Aryan languages, with LSTM attention baselines strong in few-shot settings.

## Executive Summary
This paper tackles the challenge of detecting hate speech and offensive content in low-resource Indo-Aryan languages including Bengali, Assamese, Bodo, Sinhala, and Gujarati. The authors conduct a comparative analysis of pre-trained transformer models (BERT variants, XLM-R) against an LSTM baseline with attention, fine-tuning each on language-specific datasets and evaluating using macro-F1 scores. Results show that multilingual transformer models generally outperform monolingual or simpler architectures, with BERT Base Multilingual Cased excelling for Bodo and Assamese, XLM-R for Sinhala, and LSTM attention for Gujarati in few-shot settings. The study highlights the importance of model selection in multilingual, low-resource contexts and demonstrates the viability of transfer learning for hate speech detection.

## Method Summary
The study fine-tunes several pre-trained multilingual models—BERT Base Multilingual Cased, XLM-Roberta, and others—on datasets for five Indo-Aryan languages, with an LSTM model with attention head as a baseline. Each model is trained using 5-fold cross-validation, with ensemble predictions made by averaging logits across folds and applying a 0.5 threshold for binary classification (HOF/NOT). Preprocessing includes text cleaning, emoji conversion to text using emot2, and tokenization. Emoji2vec embeddings are concatenated with text embeddings to capture non-textual sentiment cues. Performance is measured using macro-F1 scores per language.

## Key Results
- BERT Base Multilingual Cased achieves F1 scores of 0.67027 (Bengali) and 0.70525 (Assamese).
- For Bodo, BERT Base Multilingual Cased significantly outperforms others with F1 score 0.83009.
- XLM-R stands out for Sinhala with F1 score 0.83493.
- LSTM with attention excels in few-shot setting for Gujarati with F1 score 0.76601.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning multilingual transformer models significantly outperforms monolingual LSTM baselines in hate speech detection across low-resource Indo-Aryan languages.
- Mechanism: Pre-trained multilingual models like BERT Multilingual Cased and XLM-R leverage vast multilingual corpora to capture linguistic patterns across related languages, enabling zero-shot or few-shot transfer learning. Fine-tuning on small task-specific datasets adapts these general representations to the task of detecting hate speech, improving macro-F1 scores by leveraging shared linguistic features and semantic embeddings across languages.
- Core assumption: Low-resource languages share enough linguistic or semantic features to benefit from transfer learning through multilingual models.
- Evidence anchors:
  - [abstract] Results show that BERT Base Multilingual Cased achieves high F1 scores (e.g., 0.83009 for Bodo, 0.70525 for Assamese), outperforming other models, and XLM-R excels for Sinhala (F1 0.83493), while LSTM-based models perform competitively in few-shot settings like Gujarati (F1 0.76601).
  - [section 4.2] The paper leverages transformer-based language models for fine-tuning, leaving embedding layers frozen, and experimenting with models like BERT Base Multilingual Cased, XLM-Roberta, and others, achieving strong performance.
- Break condition: If linguistic features between languages are too dissimilar, or if datasets are too small to fine-tune effectively, the performance gains may not materialize.

### Mechanism 2
- Claim: Custom LSTM with attention head serves as a strong baseline in few-shot learning settings for hate speech detection in low-resource languages.
- Mechanism: LSTM networks with attention mechanisms can capture sequential dependencies and focus on relevant parts of the input text, providing strong performance even with limited training data. The attention mechanism helps the model identify critical tokens or phrases indicative of hate speech, compensating for the lack of large-scale data.
- Core assumption: Sequential modeling with attention can effectively capture hate speech indicators even in low-resource settings.
- Evidence anchors:
  - [abstract] The LSTM baseline poses strong competition, with the highest F1 score (0.76601) for Gujarati, outperforming several transformer models in the few-shot setting.
  - [section 4.2] The LSTM-attention baseline is described as a strong competitor, achieving high performance, especially in low-resource scenarios.
- Break condition: If the dataset size is too small to learn meaningful patterns, or if hate speech patterns are too complex for simple LSTM architectures, the model may underperform.

### Mechanism 3
- Claim: Incorporating emoji semantics via emoji2vec embeddings enhances hate speech detection performance by capturing non-textual cues.
- Mechanism: Emojis often convey sentiment or context that text alone may not capture. By converting emojis into vector representations using emoji2vec and concatenating them with text embeddings, the model gains additional contextual information, improving detection accuracy for content where emojis play a significant role in conveying hate or offense.
- Core assumption: Emojis carry meaningful sentiment or contextual information relevant to hate speech detection.
- Evidence anchors:
  - [section 4.2] The paper mentions using emoji2vec embeddings on top of emojis, noting that this approach has shown competitive results in previous works.
  - [section 4.1] Preprocessing includes converting emojis to textual descriptions using emot2 library, indicating their importance in the model input.
- Break condition: If emojis are not prevalent in the dataset or do not carry significant sentiment relevant to hate speech, the additional embeddings may not improve performance.

## Foundational Learning

- Concept: Multilingual transfer learning
  - Why needed here: Low-resource Indo-Aryan languages lack large-scale labeled datasets; leveraging pre-trained models trained on multiple languages allows effective transfer of linguistic knowledge.
  - Quick check question: Why might a multilingual model perform better on low-resource languages than a monolingual model trained only on that language?

- Concept: Few-shot learning
  - Why needed here: Datasets for some languages (e.g., Gujarati) are extremely small (200 samples), requiring models that can learn effectively from limited data.
  - Quick check question: What challenges arise when training deep learning models on datasets with fewer than 1000 samples?

- Concept: Attention mechanisms in sequence models
  - Why needed here: Hate speech often depends on specific words or phrases; attention helps the model focus on these critical elements rather than treating all tokens equally.
  - Quick check question: How does an attention mechanism improve a model's ability to detect hate speech compared to a standard LSTM?

## Architecture Onboarding

- Component map: Input preprocessing -> Embedding layer (text + emoji2vec) -> Model architecture (LSTM/transformer) -> Classification head (dense + sigmoid) -> Ensemble layer (fold averaging)
- Critical path: 1. Data preprocessing -> 2. Embedding generation -> 3. Model forward pass -> 4. Ensemble prediction -> 5. Thresholding at 0.5 for final label
- Design tradeoffs:
  - Transformer vs LSTM: Transformers offer better transfer learning but require more compute; LSTMs are lighter and competitive in few-shot settings.
  - Multilingual vs monolingual models: Multilingual models enable transfer learning but may include noise from unrelated languages; monolingual models are cleaner but lack transfer benefits.
  - Emoji inclusion: Adds contextual information but increases input dimensionality and may not help if emojis are rare.
- Failure signatures:
  - Low precision/recall: Model may be confused by code-mixed content or insufficient training data.
  - High variance across folds: Indicates instability due to small dataset size or poor generalization.
  - Degradation with emoji embeddings: Suggests emojis are not meaningful in the dataset or embeddings are noisy.
- First 3 experiments:
  1. Train LSTM baseline with attention on all languages; compare macro-F1 scores.
  2. Fine-tune BERT Base Multilingual Cased on each language dataset; compare against LSTM baseline.
  3. Add emoji2vec embeddings to both LSTM and transformer models; measure impact on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do pre-trained transformer models perform on zero-shot or few-shot settings for hate speech detection in low-resource Indo-Aryan languages?
- Basis in paper: [explicit] The paper discusses leveraging transformer-based language models for the downstream task at hand and experimenting with various multi-lingual transformer-based models for fine-tuning in a zero-shot and few-shot settings.
- Why unresolved: The paper does not provide detailed results or comparisons of transformer models' performance in zero-shot or few-shot settings specifically.
- What evidence would resolve it: Experimental results comparing the performance of pre-trained transformer models on hate speech detection in low-resource Indo-Aryan languages in zero-shot and few-shot settings.

### Open Question 2
- Question: How effective are LSTM-based models compared to transformer-based models for hate speech detection in low-resource Indo-Aryan languages?
- Basis in paper: [explicit] The paper presents a strong LSTM with an attention head baseline model and compares its performance with various pre-trained transformer models for hate speech detection in low-resource Indo-Aryan languages.
- Why unresolved: The paper does not provide a detailed analysis of the effectiveness of LSTM-based models compared to transformer-based models specifically for hate speech detection in low-resource Indo-Aryan languages.
- What evidence would resolve it: A comprehensive comparison of the performance of LSTM-based models and transformer-based models for hate speech detection in low-resource Indo-Aryan languages, including metrics such as accuracy, precision, recall, and F1-score.

### Open Question 3
- Question: What are the challenges and limitations of using pre-trained models for hate speech detection in low-resource Indo-Aryan languages?
- Basis in paper: [explicit] The paper mentions that the available datasets have a limited number of samples per language and discusses the need for adequate preprocessing techniques for datasets.
- Why unresolved: The paper does not provide a detailed analysis of the challenges and limitations of using pre-trained models for hate speech detection in low-resource Indo-Aryan languages.
- What evidence would resolve it: A comprehensive analysis of the challenges and limitations of using pre-trained models for hate speech detection in low-resource Indo-Aryan languages, including issues such as data scarcity, language-specific nuances, and model generalizability.

## Limitations
- Findings are limited to Indo-Aryan languages and specific social media platforms, reducing generalizability.
- The study does not address code-mixing, which is prevalent in these languages, nor does it explore domain adaptation.
- LSTM baselines may not scale as effectively to larger datasets or more complex linguistic phenomena.
- Emoji2vec embeddings are promising but not thoroughly validated across all languages, and the impact of emoji prevalence is unclear.

## Confidence

- **High confidence**: The superiority of BERT Base Multilingual Cased for Bodo and Assamese, and XLM-R for Sinhala, is well-supported by the reported F1 scores and comparative analysis.
- **Medium confidence**: The competitive performance of LSTM with attention in few-shot settings (e.g., Gujarati) is plausible but may depend on dataset size and quality, which are not fully specified.
- **Low confidence**: The contribution of emoji2vec embeddings to overall performance is asserted but lacks detailed ablation studies or cross-language validation.

## Next Checks
1. **Ablation study on emoji embeddings**: Remove emoji2vec embeddings from both LSTM and transformer models and re-evaluate macro-F1 scores to quantify their impact on hate speech detection performance across all five languages.
2. **Cross-platform robustness test**: Evaluate the best-performing models on a held-out dataset from a different social media platform (e.g., Facebook instead of X) to assess domain generalization and robustness to platform-specific language use.
3. **Code-mixing handling evaluation**: Introduce a small amount of code-mixed text (e.g., English-Hindi or English-Bengali) into the test sets and measure model performance degradation to assess resilience to linguistic variability.