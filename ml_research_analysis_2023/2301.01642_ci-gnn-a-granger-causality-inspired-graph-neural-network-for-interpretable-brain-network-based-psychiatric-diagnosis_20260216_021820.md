---
ver: rpa2
title: 'CI-GNN: A Granger Causality-Inspired Graph Neural Network for Interpretable
  Brain Network-Based Psychiatric Diagnosis'
arxiv_id: '2301.01642'
source_url: https://arxiv.org/abs/2301.01642
tags:
- ci-gnn
- graph
- causal
- brain
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a built-in interpretable graph neural network
  (GNN) for brain network-based psychiatric diagnosis that addresses the problem of
  existing post-hoc explainers that rely on auxiliary models and do not consider causal
  relationships, leading to spurious correlations and weak faithfulness. The proposed
  CI-GNN learns disentangled subgraph-level representations alpha and beta that encode,
  respectively, the causal and noncausal aspects of the original graph under a graph
  variational autoencoder framework, regularized by a conditional mutual information
  (CMI) constraint.
---

# CI-GNN: A Granger Causality-Inspired Graph Neural Network for Interpretable Brain Network-Based Psychiatric Diagnosis

## Quick Facts
- arXiv ID: 2301.01642
- Source URL: https://arxiv.org/abs/2301.01642
- Reference count: 20
- Key outcome: CI-GNN achieves best performance in a wide range of metrics and provides more reliable and concise explanations which have clinical evidence.

## Executive Summary
This paper proposes CI-GNN, a built-in interpretable graph neural network for brain network-based psychiatric diagnosis that addresses the problem of existing post-hoc explainers relying on auxiliary models and not considering causal relationships. The method learns disentangled subgraph-level representations α and β that encode causal and noncausal aspects of the original graph under a graph variational autoencoder framework, regularized by a conditional mutual information (CMI) constraint. The approach theoretically justifies the validity of CMI regulation in capturing causal relationships and empirically evaluates performance against baseline GNNs and state-of-the-art GNN explainers on synthetic data and three large-scale brain disease datasets.

## Method Summary
CI-GNN uses a GraphVAE encoder to learn two disentangled latent representations, α (causal) and β (non-causal), where only α directly influences the decision label Y. The model enforces independence between α and β through CMI regularization to ensure explanations are causally related to outputs rather than spuriously correlated. For entropy estimation, CI-GNN uses matrix-based Rényi's δ-order entropy functional instead of neural estimators for stable, differentiable computation. The model generates instance-level edge explanations for brain connectivity patterns, providing individualized explanations that map to specific brain region interactions rather than node-level features.

## Key Results
- CI-GNN achieves best performance across accuracy, F1-score, and Matthew's Correlation Coefficient on synthetic and brain disease datasets
- The method provides more reliable and concise explanations with clinical evidence compared to baseline approaches
- Edge-level explanations are shown to be more clinically relevant for psychiatric diagnosis than node-level alternatives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CI-GNN's built-in interpretability eliminates the need for post-hoc explanation models, reducing spurious correlations.
- **Mechanism**: The model learns two disentangled latent representations, α (causal) and β (non-causal), where only α directly influences the decision label Y. By enforcing independence between α and β through CMI regularization, CI-GNN ensures that the explanation is causally related to the output.
- **Core assumption**: Granger causality framework can be approximated in latent space by CMI between α and Y given β.
- **Evidence anchors**: [abstract] "CI-GNN learns disentangled subgraph-level representations α and β that encode, respectively, the causal and noncausal aspects of the original graph under a graph variational autoencoder framework, regularized by a conditional mutual information (CMI) constraint."
- **Break condition**: If the disentanglement fails (high CMI between α and β), spurious correlations may persist, invalidating the causal claim.

### Mechanism 2
- **Claim**: Using matrix-based Rényi's δ-order entropy functional enables stable and differentiable computation of CMI.
- **Mechanism**: Instead of using neural estimators that can be unstable, CI-GNN uses a closed-form entropy estimator based on Gram matrices and eigenvalues. This allows end-to-end training without auxiliary networks and avoids negative mutual information values.
- **Core assumption**: The matrix-based Rényi entropy estimator is a valid approximation of Shannon-based CMI for causal regularization.
- **Evidence anchors**: [section] "we use the matrix-based Rényi's δ-order entropy functional... to estimate different entropy terms... This newly proposed estimator can be simply computed (without density estimation or any auxiliary neural network) and is also differentiable which suits well for deep learning applications."
- **Break condition**: If the kernel choice or δ-order parameter is poorly tuned, the entropy estimates may become inaccurate, weakening the causal regularization.

### Mechanism 3
- **Claim**: Instance-level edge explanations are more clinically relevant for psychiatric diagnosis than node-level or group-level explanations.
- **Mechanism**: By generating causal subgraphs at the edge level (functional connectivity), CI-GNN provides individualized explanations that map directly to specific brain region interactions. This aligns with clinical understanding that altered connections, not isolated nodes, reveal brain disorder properties.
- **Core assumption**: Edge-level functional connectivity is a more informative biomarker for psychiatric disorders than node-level ROI features.
- **Evidence anchors**: [abstract] "CI-GNN... is able to identify the most influential subgraph... that is causally related to the decision... For brain disorders, it is recognized that connections rather than single nodes alterations can reveal properties of brain disorders..."
- **Break condition**: If the underlying brain disorder is better characterized by node-level or global network metrics, edge-level explanations may miss critical patterns.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs)**
  - Why needed here: CI-GNN extends GNNs to incorporate interpretability and causal reasoning for brain network classification.
  - Quick check question: What is the key difference between GCN, GAT, and GIN in terms of aggregation strategy?

- **Concept: Conditional Mutual Information (CMI)**
  - Why needed here: CMI quantifies the causal influence of α on Y while controlling for β, ensuring the explanation is not confounded by spurious correlations.
  - Quick check question: How does CMI differ from standard mutual information in measuring causal relationships?

- **Concept: Granger Causality**
  - Why needed here: Provides the theoretical foundation for interpreting CMI as a measure of causal influence in the latent space.
  - Quick check question: What is the key criterion for establishing Granger causality between two time series?

## Architecture Onboarding

- **Component map**: GraphVAE -> Causal Effect Estimator -> Causal Subgraph Generator -> Classifier

- **Critical path**: GraphVAE → Causal Effect Estimator → Causal Subgraph Generator → Classifier

- **Design tradeoffs**:
  - Built-in vs. post-hoc: Built-in is more faithful but requires more complex training
  - Edge-level vs. node-level: Edge-level is more clinically interpretable but may lose global context
  - Disentanglement vs. reconstruction: Strong disentanglement may hurt reconstruction quality

- **Failure signatures**:
  - High CMI between α and β: Disentanglement failed, spurious correlations remain
  - Low classification accuracy: Causal subgraph generator or classifier may be misaligned
  - Unstable training: Kernel bandwidth or δ-order parameter may need tuning

- **First 3 experiments**:
  1. Train CI-GNN on synthetic BA-2Motif dataset and verify it correctly identifies House/Cycle motifs vs. Tree motifs
  2. Evaluate CMI between α and β during training to confirm disentanglement is occurring
  3. Perform ablation: Remove CMI regularization and observe increase in spurious correlations in explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CI-GNN compare to other state-of-the-art GNN explainers on large-scale, multi-site brain disease datasets?
- Basis in paper: [explicit] The paper states that CI-GNN achieves the best performance in a wide range of metrics on two large-scale brain disease datasets (ABIDE and REST-meta-MDD) containing more than 1,000 participants across 17 independent sites.
- Why unresolved: While the paper shows that CI-GNN outperforms other baselines, a more comprehensive comparison with a wider range of SOTA GNN explainers on even larger datasets would further strengthen the claim of CI-GNN's superiority.
- What evidence would resolve it: Testing CI-GNN on additional large-scale brain disease datasets and comparing its performance to a broader set of state-of-the-art GNN explainers using various evaluation metrics.

### Open Question 2
- Question: Can CI-GNN's explanations be validated against ground truth causal subgraphs in real-world brain disease datasets?
- Basis in paper: [inferred] The paper mentions that CI-GNN provides more reliable and concise explanations which have clinical evidence, but it does not explicitly validate the explanations against ground truth causal subgraphs in real-world brain disease datasets.
- Why unresolved: Without validation against ground truth causal subgraphs, it is difficult to assess the accuracy and reliability of CI-GNN's explanations in real-world applications.
- What evidence would resolve it: Conducting experiments to validate CI-GNN's explanations against ground truth causal subgraphs in real-world brain disease datasets, either through expert annotation or by leveraging known causal relationships in the brain.

### Open Question 3
- Question: How does the choice of hyperparameters (e.g., feature dimensions K and L) affect the performance and interpretability of CI-GNN?
- Basis in paper: [explicit] The paper mentions that the feature dimensions K and L for α and β are set to 56 and 8, respectively, but does not explore the impact of different hyperparameter choices on the model's performance and interpretability.
- Why unresolved: Understanding the sensitivity of CI-GNN to hyperparameter choices is crucial for its practical application and generalizability.
- What evidence would resolve it: Conducting a comprehensive hyperparameter sensitivity analysis to determine the optimal settings for K, L, and other relevant hyperparameters, and assessing their impact on CI-GNN's performance and interpretability.

## Limitations

- The CMI-based disentanglement relies heavily on the validity of the matrix-based Rényi entropy estimator as a proxy for causal relationships, which has not been thoroughly validated against established causal inference benchmarks.
- The clinical relevance of edge-level explanations for psychiatric diagnosis, while supported by domain knowledge, lacks direct empirical validation showing improved patient outcomes or diagnostic utility.
- The claim of "best performance" is limited by the relatively small number of baseline comparisons (three GNNs and four explainers) and the absence of comparisons against state-of-the-art transformer-based graph models.

## Confidence

- Mechanism 1 (causal disentanglement): Medium - theoretically sound but depends on estimator validity
- Mechanism 2 (matrix-based CMI): Medium - novel approach but lacks benchmark comparisons
- Mechanism 3 (edge-level clinical relevance): Medium - domain-supported but not empirically validated

## Next Checks

1. Benchmark CI-GNN's CMI estimates against established causal inference methods (e.g., do-calculus, instrumental variables) on synthetic causal graphs
2. Conduct clinician study comparing interpretability and diagnostic utility of CI-GNN's edge-level explanations vs. node-level alternatives
3. Extend evaluation to include recent transformer-based graph models (e.g., GraphGPS, GraphFormers) to verify performance claims against current state-of-the-art