---
ver: rpa2
title: Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice
  Capabilities in Chinchilla
arxiv_id: '2307.09458'
source_url: https://arxiv.org/abs/2307.09458
tags:
- correct
- letter
- token
- heads
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that mechanistic interpretability techniques
  like logit attribution, attention pattern visualization, and activation patching
  can be successfully applied to a large 70B parameter language model (Chinchilla)
  to identify the circuit responsible for multiple-choice question answering. The
  authors identify a set of "output nodes" (attention heads and MLMs) that directly
  contribute to selecting the correct answer label, with 45 nodes explaining 80% of
  the model's performance.
---

# Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla

## Quick Facts
- arXiv ID: 2307.09458
- Source URL: https://arxiv.org/abs/2307.09458
- Reference count: 40
- This paper demonstrates that mechanistic interpretability techniques like logit attribution, attention pattern visualization, and activation patching can be successfully applied to a large 70B parameter language model (Chinchilla) to identify the circuit responsible for multiple-choice question answering.

## Executive Summary
This paper demonstrates that mechanistic interpretability techniques can be successfully applied to a 70B parameter language model (Chinchilla) to identify and understand the circuit responsible for multiple-choice question answering. Through logit attribution, attention pattern visualization, and activation patching, the authors identify 45 key nodes (attention heads and MLPs) that explain 80% of the model's performance on multiple-choice tasks. The study also shows that the query and key subspaces of "correct letter" attention heads can be compressed to 3 dimensions without loss of performance, providing preliminary evidence that these subspaces encode a general "Nth item in an enumeration" feature, though this explanation only partially accounts for the heads' behavior across different label formats.

## Method Summary
The authors apply established mechanistic interpretability techniques to analyze the 70B Chinchilla model's behavior on multiple-choice questions from the MMLU benchmark. They use logit attribution to identify nodes that directly contribute to final logits, activation patching to measure total effects of node removal, and attention pattern visualization to understand how specific heads process information. The analysis focuses on 6 high-performing MMLU topics using 0-shot prompting with answer options A, B, C, D. The authors also employ singular value decomposition to compress the query, key, and value subspaces of identified "correct letter" attention heads, testing the generalization of these compressed representations across different label formats.

## Key Results
- Identified 45 output nodes (attention heads and MLPs) that explain 80% of Chinchilla's multiple-choice question answering performance
- Successfully compressed query, key, and value subspaces of "correct letter" heads to 3 dimensions without loss of performance
- Found that "correct letter" heads implement a "nth item in enumeration" feature to some extent, but the behavior only partially generalizes to different label formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logit attribution with fixed RMS scaling allows parallel identification of direct effects from all nodes
- Mechanism: Since the model output is a linear combination of node contributions scaled by a fixed RMS, direct effects can be computed in parallel by multiplying each node's activation by the unembedding matrix and dividing by RMS
- Core assumption: The RMS of the final residual stream remains stable across prompts and the fixed scaling assumption holds
- Evidence anchors:
  - [abstract] "through a combination of logit attribution and attention pattern visualization, we identify 'correct letter' heads that perform the algorithmic task"
  - [section 3.1] "we can exploit the fact that for a fixed scaling factor, the final logits are the sum of the individual components' contributions"

### Mechanism 2
- Claim: Low-rank SVD approximation of query/key subspaces captures the essential features for correct letter identification
- Mechanism: The query and key deltas contain most of the distinguishing information between answer labels, and 3 principal components explain 65-80% of the variance, allowing compression without performance loss
- Core assumption: The low-rank subspace learned on MMLU questions generalizes to other multiple choice formats
- Evidence anchors:
  - [abstract] "we significantly compress the query, key and value subspaces of the head without loss of performance"
  - [section 4.1] "using 3 components captures roughly 65-80% of the variance for all heads for keys and queries"

### Mechanism 3
- Claim: Correct letter heads implement a "nth item in enumeration" feature combined with token identity features
- Mechanism: The heads use low-rank query/key subspaces to select the correct label position and values to copy the token identity, implementing an algorithm that identifies the correct answer letter
- Core assumption: The observed behavior generalizes beyond the specific ABCD format to other enumerations
- Evidence anchors:
  - [abstract] "we show that the query and key subspaces represent an 'Nth item in an enumeration' feature to at least some extent"
  - [section 4.2] "replacing ABCD with random capital letters works worse when using the low-rank approximation, this suggests that part of it could be related to the specific token identity as well"

## Foundational Learning

- Concept: Transformer residual stream architecture
  - Why needed here: Understanding how node outputs combine linearly to form logits is crucial for logit attribution and circuit analysis
  - Quick check question: How does the residual stream architecture enable direct computation of node contributions to final logits?

- Concept: Attention mechanism and QKV decomposition
  - Why needed here: Analyzing attention patterns and understanding how heads select information from different positions requires understanding QKV operations
  - Quick check question: What role do queries, keys, and values play in determining which tokens an attention head attends to?

- Concept: Singular value decomposition and dimensionality reduction
  - Why needed here: Identifying low-rank subspaces that capture essential features requires understanding SVD and its interpretation
  - Quick check question: How does SVD help identify the most important directions in high-dimensional activation space?

## Architecture Onboarding

- Component map: Input → embedding → residual stream through 80 layers with 64 attention heads each → MLPs at each position → final residual stream → unembedding → logits
- Critical path: Input → embedding → residual stream through layers → attention heads and MLPs → final residual stream → unembedding → logits
- Design tradeoffs: Fixed RMS scaling simplifies logit attribution but may mask non-linear effects; low-rank approximation reduces complexity but may lose specialized features
- Failure signatures: High variance in total effects suggests compensation between nodes; poor generalization of low-rank subspaces suggests overfitting to specific formats
- First 3 experiments:
  1. Verify logit attribution by comparing direct effects computed via formula vs. ablation studies
  2. Test low-rank approximation by patching in low-rank attention and measuring performance degradation
  3. Validate subspace semantics by creating mutated prompts and measuring loss changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific features in the query and key subspaces of the "correct letter" heads are invariant across different label sets (e.g. ABCD vs random letters vs numbers)?
- Basis in paper: [explicit] The paper shows that the low-rank approximation of Q and K spaces works for random capital letters but not for numbers, suggesting partial invariance.
- Why unresolved: The paper only shows that projections are in similar directions but different magnitudes, without identifying the exact features that are invariant.
- What evidence would resolve it: Detailed analysis of the principal components in the low-rank subspaces to identify which dimensions correspond to general vs specific features.

### Open Question 2
- Question: How does the model implement "backup behavior" that could explain why nodes with zero median total effect still show significant effects in some prompts?
- Basis in paper: [inferred] The paper notes this phenomenon in the total effect analysis and mentions backup behavior from Wang et al. (2022) as a possible explanation.
- Why unresolved: The paper observes the phenomenon but doesn't investigate the underlying mechanism.
- What evidence would resolve it: Ablation studies on individual nodes while monitoring the behavior of other nodes to identify compensatory mechanisms.

### Open Question 3
- Question: What is the exact mechanism by which content gatherer heads aggregate information from answer contents to the final token position?
- Basis in paper: [explicit] The paper identifies content gatherer heads (like L24 H18 ) that attend to correct answer contents but doesn't explain the aggregation mechanism.
- Why unresolved: The paper speculates about compression of the original question but doesn't test this hypothesis.
- What evidence would resolve it: Analysis of attention patterns across multiple content tokens and their contribution to the final token's residual stream.

## Limitations

- The study focuses on a specific task (multiple-choice answering) with a particular format (ABCD labels) in a single model (Chinchilla), limiting generalizability to other tasks or model architectures
- The techniques used remain computationally expensive and may not scale efficiently to much larger models despite demonstrating scalability to 70B parameters
- Claims about the specific algorithmic role of "correct letter" heads and their exact operational semantics remain speculative with only suggestive evidence

## Confidence

- **High Confidence**: The identification of output nodes and their relative contributions to model performance is well-supported by logit attribution and activation patching results
- **Medium Confidence**: The compression of query/key subspaces to 3 dimensions is technically demonstrated, but the interpretation of these subspaces as representing "nth item in enumeration" features is only partially supported
- **Low Confidence**: Claims about the specific algorithmic role of "correct letter" heads and their exact operational semantics remain speculative

## Next Checks

1. Test the low-rank subspace generalization by applying the identified 3-dimensional subspaces to multiple-choice tasks with different label formats (e.g., 1-4, True/False, or descriptive labels) and measure performance retention
2. Conduct ablation studies where identified "correct letter" heads are selectively removed or modified to determine their individual contributions to overall performance beyond statistical attribution
3. Apply the same circuit analysis methodology to a different large language model (e.g., GPT-3 or PaLM) to verify whether similar "correct letter" heads emerge and whether the same interpretability techniques scale across architectures