---
ver: rpa2
title: Can LLMs facilitate interpretation of pre-trained language models?
arxiv_id: '2305.13386'
source_url: https://arxiv.org/abs/2305.13386
tags:
- concepts
- list
- names
- concept
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a novel approach to interpreting pre-trained language
  models (PLMs) by leveraging large language models (LLMs) as annotators. We discover
  latent concepts within PLMs by applying agglomerative hierarchical clustering to
  contextualized representations, and then use ChatGPT to annotate these concepts.
---

# Can LLMs facilitate interpretation of pre-trained language models?

## Quick Facts
- **arXiv ID:** 2305.13386
- **Source URL:** https://arxiv.org/abs/2305.13386
- **Reference count:** 40
- **Primary result:** ChatGPT provides accurate and semantically richer annotations of latent concepts compared to human annotations, achieving 90.7% acceptance rate with Fleiss' kappa of 0.71

## Executive Summary
This paper introduces a novel approach to interpreting pre-trained language models (PLMs) by using large language models (LLMs) as annotators. The authors discover latent concepts within PLMs through agglomerative hierarchical clustering of contextualized representations, then use ChatGPT to annotate these concepts. Their findings demonstrate that ChatGPT produces more accurate and semantically richer annotations compared to human-annotated concepts, with high acceptance rates and inter-annotator agreement. The approach enables fine-grained interpretation analysis, including probing frameworks and neuron interpretation, and the authors release a substantial ConceptNet dataset comprising 39,000 annotated latent concepts.

## Method Summary
The authors apply agglomerative hierarchical clustering to contextualized representations from pre-trained transformer models (BERT, RoBERTa, XLNet, ALBERT, XLM-R) using the WMT News 2018 dataset. They use ChatGPT with zero-shot prompts to annotate the discovered latent concept clusters. For probing analysis, they train linear classifiers on GPT-annotated data to measure how well model representations capture specific linguistic properties. For neuron analysis, they employ the Probeless method to identify neurons most associated with specific concepts by comparing activation patterns. The methodology enables interpretation of PLMs at a fine-grained level beyond predefined linguistic categories.

## Key Results
- ChatGPT achieves 90.7% acceptance rate for concept annotations with Fleiss' kappa of 0.71, outperforming human annotations in most cases
- Probe classifiers trained on GPT-annotated concepts achieve up to 99% accuracy for fine-grained properties like "Gender-related nouns and pronouns" and "LGBTQ+"
- Neuron analysis reveals specific neurons capturing detailed aspects of concepts, with average alignment of 0.36 between super concepts and their sub-concepts
- The authors release a ConceptNet dataset with 39,000 annotated latent concepts for further research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT provides accurate and semantically richer annotations compared to human annotations for latent concepts.
- Mechanism: ChatGPT leverages its large-scale training on textual data to infer semantic relationships between words in a concept, producing natural language labels that capture the underlying meaning more effectively than predefined ontologies or manual annotations.
- Core assumption: ChatGPT's training enables it to understand and generalize semantic relationships in ways that align with human interpretation of latent concepts.
- Evidence anchors:
  - [abstract]: "Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts, achieving a 90.7% acceptance rate with a Fleiss' kappa of 0.71."
  - [section]: "Our findings indicate that the annotations produced by ChatGPT are semantically richer and accurate compared to the human-annotated concepts for example BCN (BERT Concept NET)."
  - [corpus]: Weak - corpus does not directly provide evidence for ChatGPT's semantic understanding capability. The corpus search results show related work on using LLMs for annotation tasks but do not specifically validate semantic richness claims.
- Break condition: If the concepts are too specialized or rely on context that ChatGPT cannot access (e.g., without additional context sentences), the annotations may become imprecise or incorrect.

### Mechanism 2
- Claim: Fine-grained probing classifiers can be trained using GPT-annotated concepts to analyze model representations beyond predefined linguistic categories.
- Mechanism: By extracting feature vectors for GPT-annotated concepts and training binary classifiers, researchers can probe for detailed linguistic properties that were previously inaccessible due to lack of annotations.
- Core assumption: The GPT-annotated concepts capture meaningful linguistic distinctions that are learnable from the model's representations.
- Evidence anchors:
  - [abstract]: "We trained linear probes from GPT-annotated concept representations to predict a fine-grained property of interest. This enables investigating models for concepts beyond human-defined linguistic categories."
  - [section]: "Using GPT-annotations, we perform fine-grained concept probing by extracting feature vectors from annotated data through a forward pass on the model of interest. Then, we train a binary classifier to predict the concept and use the probe accuracy as a qualitative measure of how well the model represents the concept."
  - [corpus]: Weak - corpus does not provide direct evidence of probing effectiveness with GPT-annotated concepts. The related papers focus on annotation tasks rather than probing methodologies.
- Break condition: If the GPT annotations are imprecise or if the underlying concepts are not well-represented in the model's embeddings, probe accuracy will be low, indicating poor alignment between annotation and representation.

### Mechanism 3
- Claim: Neuron analysis can identify specific neurons that capture fine-grained aspects of concepts using GPT-annotated latent concepts.
- Mechanism: The Probeless method computes neuron rankings by comparing activation patterns for a concept versus random concepts, revealing which neurons are most associated with specific semantic properties at a detailed level.
- Core assumption: Individual neurons or groups of neurons encode specific semantic aspects that can be discovered through comparison with annotated concept representations.
- Evidence anchors:
  - [abstract]: "We obtained neuron rankings for latent concepts using a corpus-search method called as Probeless (Antverg and Belinkov, 2022). Such fine-grained interpretation analyses of latent spaces has various benefits such as identifying potential bias in pLMs."
  - [section]: "We employed the Probeless method (Antverg and Belinkov, 2022) to search for neurons associated with specific linguistic properties. By generating a neuron ranking for the super adverb concept and comparing it with the ranking obtained for each sub adverb concept, we discovered that the top neurons responsible for learning the super adverb concept tended to distribute among the top neurons for specialized adverbial concepts."
  - [corpus]: Weak - corpus does not contain evidence about neuron analysis results. The related work focuses on annotation rather than neuron-level interpretation.
- Break condition: If the concept annotations are too coarse or if the model architecture does not support interpretable neuron-level representations, the neuron analysis may not reveal meaningful patterns.

## Foundational Learning

- Concept: Agglomerative hierarchical clustering
  - Why needed here: This clustering method is used to discover latent concepts by grouping contextualized word representations based on their similarity in the embedding space.
  - Quick check question: How does Ward's minimum variance criterion determine which clusters to merge during agglomerative clustering?

- Concept: Probing classifiers
  - Why needed here: Probing classifiers measure how well a model's representations capture specific linguistic properties by training a classifier to predict those properties from the representations.
  - Quick check question: What does high probe accuracy indicate about the relationship between a model's representations and the target concept?

- Concept: Neuron ranking methods (Probeless)
  - Why needed here: Neuron ranking methods like Probeless identify which individual neurons are most associated with specific concepts by comparing activation patterns.
  - Quick check question: How does the Probeless method calculate the relevance score for a neuron with respect to a specific concept?

## Architecture Onboarding

- Component map: Data preprocessing → Context extraction → Forward pass through PLM → Clustering → GPT annotation → Probe training / Neuron analysis
- Critical path: 1. Extract contextualized representations from PLM, 2. Apply agglomerative hierarchical clustering to discover concepts, 3. Use ChatGPT to annotate discovered concepts, 4. For probing: Train classifiers on GPT-annotated concepts, 5. For neuron analysis: Apply Probeless method to find concept-associated neurons
- Design tradeoffs:
  - Using tokens vs types: Tokens provide richer context but may trigger content policy filters; types avoid this but may lose semantic nuance
  - Number of context sentences: More context improves annotation quality but increases cost and token limits
  - Clustering granularity (K parameter): Higher K finds more specific concepts but may overfit; lower K finds broader concepts but may miss fine-grained distinctions
- Failure signatures:
  - Low probe accuracy despite high GPT annotation quality → Concept not well-represented in model embeddings
  - ChatGPT content policy rejections → Need to use types instead of tokens or modify prompt
  - Inconsistent neuron rankings across runs → Randomization in clustering or neuron selection process
- First 3 experiments:
  1. Run clustering on a small subset of the WMT dataset and manually verify that discovered clusters correspond to meaningful linguistic concepts
  2. Test ChatGPT annotation with different prompts (lexical, POS, semantic) on a sample of clusters to determine optimal prompt strategy
  3. Train a probe classifier on one GPT-annotated concept and validate accuracy on held-out data to confirm the probing methodology works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs reliably annotate sensitive concepts (e.g., gender, race, religion) despite content policy filters?
- Basis in paper: [explicit] The paper notes that content policy filters prevent LLMs from labeling sensitive concepts, limiting their application to tasks that are not sensitive to content policy.
- Why unresolved: It is unclear whether LLMs can be adapted or fine-tuned to overcome content policy restrictions while maintaining ethical standards.
- What evidence would resolve it: Experiments demonstrating successful annotation of sensitive concepts using modified prompts or fine-tuned LLMs, along with ethical safeguards.

### Open Question 2
- Question: How does the performance of LLMs as annotators compare to human annotators for tasks requiring deep contextual understanding?
- Basis in paper: [explicit] The paper shows that ChatGPT outperforms human annotations in most cases but struggles with tasks requiring linguistic ontologies or insufficient context.
- Why unresolved: The paper does not provide a comprehensive comparison across diverse tasks or evaluate the impact of context on annotation quality.
- What evidence would resolve it: A large-scale study comparing LLM and human annotations across various tasks, including those requiring deep contextual understanding, with varying levels of context provided.

### Open Question 3
- Question: Can LLMs be used to annotate concepts in low-resource languages where human expertise is scarce?
- Basis in paper: [inferred] The paper focuses on English-language concepts and does not address the applicability of LLMs to low-resource languages.
- Why unresolved: The paper does not explore the generalizability of LLM annotations to languages with limited linguistic resources or expertise.
- What evidence would resolve it: Experiments evaluating the performance of LLMs as annotators for concepts in low-resource languages, comparing results to human annotations where available.

## Limitations

- The content policy limitations of ChatGPT create potential gaps in annotation coverage, particularly for sensitive concepts important for bias detection
- The clustering granularity (K parameter) effects on concept quality and interpretability are not fully explored
- Neuron analysis results lack statistical validation and baseline comparisons to assess practical significance

## Confidence

**High Confidence:** The core methodology of using LLMs for annotation tasks is well-established, and technical implementation follows standard practices with up to 99% probe accuracy for specific concepts.

**Medium Confidence:** Semantic richness claims are supported by acceptance rates and agreement metrics, but lack detailed error analysis comparing differences between ChatGPT and human annotations.

**Low Confidence:** Neuron analysis results are presented without sufficient statistical validation or context to determine if alignment scores are practically meaningful.

## Next Checks

1. **Cross-domain annotation validation:** Apply the same ChatGPT annotation pipeline to a different corpus (e.g., biomedical or legal texts) and compare acceptance rates and agreement metrics to assess domain robustness.

2. **Ablation study on clustering parameters:** Systematically vary the K parameter in agglomerative clustering and measure how probe accuracy and neuron alignment scores change, establishing the sensitivity of downstream analyses to clustering granularity.

3. **Bias analysis of missing annotations:** Quantitatively analyze which types of concepts are most frequently rejected by ChatGPT's content policy, and assess whether this creates systematic blind spots in the interpretation framework for bias detection and fairness analysis.