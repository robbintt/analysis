---
ver: rpa2
title: 'Visually Grounded Language Learning: a review of language games, datasets,
  tasks, and models'
arxiv_id: '2312.02431'
source_url: https://arxiv.org/abs/2312.02431
tags:
- language
- learning
- tasks
- grounded
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a systematic review of vision-and-language
  (V+L) tasks and models using Wittgenstein''s concept of "language games." The authors
  categorize V+L tasks into three types: discriminative games (e.g., VQA), generative
  games (e.g., image captioning), and interactive games (e.g., embodied instruction
  following). They survey 50 datasets and 51 models, analyzing their capabilities
  in terms of embodiment, dialogue, compositionality, and visual representation.'
---

# Visually Grounded Language Learning: a review of language games, datasets, tasks, and models

## Quick Facts
- arXiv ID: 2312.02431
- Source URL: https://arxiv.org/abs/2312.02431
- Reference count: 39
- Key outcome: Systematic review categorizing V+L tasks into discriminative, generative, and interactive games using Wittgenstein's framework, surveying 50 datasets and 51 models

## Executive Summary
This paper provides a comprehensive survey of vision-and-language tasks and models through the lens of Wittgenstein's "language games" framework. The authors categorize V+L tasks into three families: discriminative games (e.g., VQA), generative games (e.g., image captioning), and interactive games (e.g., embodied instruction following). They analyze 50 datasets and 51 models, arguing that future research should focus on interactive games that require communication to resolve ambiguities and achieve goals. The paper emphasizes the importance of embodiment, systematic generalization, and multimodal robustness in building truly grounded language models.

## Method Summary
The paper employs a systematic literature review approach, categorizing vision-and-language tasks using Wittgenstein's language games framework. The authors survey 50 datasets and 51 models, analyzing their capabilities in terms of embodiment, dialogue, compositionality, and visual representation. They propose new evaluation frameworks to assess systematic generalization and multimodal robustness. The minimum viable reproduction plan involves pretraining multimodal transformer architectures on large-scale image-text pairs, then fine-tuning on specific downstream tasks while evaluating on systematic generalization benchmarks.

## Key Results
- Language games framework provides systematic categorization of V+L tasks into discriminative, generative, and interactive types
- Interactive games requiring communication lead to better symbol grounding by forcing agents to negotiate meaning
- Embodied interactive tasks produce richer grounded representations than static image-based tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Wittgenstein's "language games" framework enables systematic categorization of vision-and-language tasks
- Mechanism: The framework provides conceptual lens to classify tasks based on interaction patterns and goals rather than output modality
- Core assumption: Language games inherently capture functional and cooperative aspects of language use
- Evidence anchors: [abstract] "We rely on Wittgenstein's idea of 'language games' to categorise such tasks into 3 different families: 1) discriminative games, 2) generative games, and 3) interactive games."

### Mechanism 2
- Claim: Interactive language games requiring communication between agents lead to better symbol grounding
- Mechanism: Communication forces agents to develop shared understanding of symbols and concepts, mirroring human language learning
- Core assumption: Symbol grounding is fundamentally a social process requiring communication and coordination
- Evidence anchors: [abstract] "Our analysis of the literature provides evidence that future work should be focusing on interactive games where communication in Natural Language is important to resolve ambiguities about object referents and action plans"

### Mechanism 3
- Claim: Embodied interactive tasks lead to richer grounded representations than static image-based tasks
- Mechanism: Physical embodiment provides causal experience of how objects behave and how actions affect environment
- Core assumption: True understanding requires not just perceptual recognition but also sensorimotor experience
- Evidence anchors: [abstract] "that physical embodiment is essential to understand the semantics of situations and events"

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: Paper discusses how different models combine visual and language representations
  - Quick check question: What are the three main approaches to multimodal fusion described in the paper, and how do they differ?

- Concept: Systematic generalization
  - Why needed here: Paper emphasizes importance of compositionality and ability to generalize to novel combinations
  - Quick check question: How does the paper define systematic generalization, and why is it considered essential?

- Concept: Symbol grounding problem
  - Why needed here: This is the foundational problem the entire paper addresses
  - Quick check question: According to Harnad's formulation, what are the two key aspects of the symbol grounding problem?

## Architecture Onboarding

- Component map: Data collection pipelines → Multimodal model architectures → Training with appropriate losses → Evaluation on systematic generalization tasks → Analysis of representation quality
- Critical path: Data collection → Model architecture selection → Training → Evaluation → Analysis
- Design tradeoffs: Single-stream vs dual-stream transformers (simplicity vs modularity), object-centric vs grid features (fine-grained vs efficiency), scripted vs learned dialogue policies (control vs adaptability)
- Failure signatures: Poor performance on out-of-distribution examples indicates lack of systematic generalization; inability to handle missing modalities suggests poor robustness
- First 3 experiments:
  1. Implement simple CNN-RNN model on VQA dataset to establish baseline
  2. Compare single-stream vs dual-stream transformer architectures on visual dialogue task
  3. Test systematic generalization by training on CLEVR and evaluating on CLEVR-CoGenT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop evaluation frameworks that better assess systematic generalization in visually grounded language models?
- Basis in paper: [explicit] Need for evaluation frameworks testing models' ability to generalize to novel combinations of concepts
- Why unresolved: Current frameworks focus on held-out test sets with known categories
- What evidence would resolve it: New datasets and benchmarks explicitly testing systematic generalization

### Open Question 2
- Question: How can we create multimodal models robust to missing or noisy modalities?
- Basis in paper: [explicit] Need for models that can handle missing/noisy modalities and leverage unimodal data
- Why unresolved: Most models designed for specific tasks struggle with missing/noisy modalities
- What evidence would resolve it: Multimodal architectures that can encode/reconstruct missing modalities

### Open Question 3
- Question: How can we advance perceptual symbol systems to learn language directly from multimodal perceptual experiences?
- Basis in paper: [explicit] Potential of perceptual symbol systems to derive language from visual/audio inputs
- Why unresolved: Current models rely on language-specific tokenizers limiting applicability
- What evidence would resolve it: Models learning language representations directly from perceptual inputs

## Limitations

- The categorization of tasks into three types may oversimplify increasingly complex hybrid tasks
- Emphasis on interactive games based on theoretical arguments rather than extensive empirical validation
- Analysis of capabilities is primarily qualitative rather than quantitative

## Confidence

- **High Confidence**: Systematic categorization using language games framework; identification of key modeling approaches; survey of major datasets
- **Medium Confidence**: Argument for interactive games focus; importance of embodiment; need for systematic generalization evaluation
- **Low Confidence**: Specific recommendations for new evaluation benchmarks; relative weighting of different capabilities; assertion about perceptual symbol systems

## Next Checks

1. **Empirical Validation of Task Categorization**: Conduct quantitative analysis of task characteristics to validate whether three-category framework adequately captures the landscape

2. **Controlled Comparison of Embodied vs. Non-Embodied Learning**: Design experiment directly comparing symbol grounding performance between embodied and non-embodied agents with identical capabilities

3. **Systematic Generalization Benchmark Implementation**: Implement proposed evaluation framework by creating controlled test sets that systematically vary object attributes and compositions