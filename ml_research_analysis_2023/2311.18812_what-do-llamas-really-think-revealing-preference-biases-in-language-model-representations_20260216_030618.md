---
ver: rpa2
title: What Do Llamas Really Think? Revealing Preference Biases in Language Model
  Representations
arxiv_id: '2311.18812'
source_url: https://arxiv.org/abs/2311.18812
tags:
- probe
- llms
- bias
- word
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a logistic Bradley-Terry probe to detect implicit
  sociodemographic biases in the contextualized embeddings of large language models
  (LLMs). The probe predicts word pair preferences from hidden vectors, outperforming
  the standard word embedding association test (WEAT) by a relative 27% in error rate.
---

# What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations

## Quick Facts
- arXiv ID: 2311.18812
- Source URL: https://arxiv.org/abs/2311.18812
- Reference count: 8
- Primary result: Logistic Bradley-Terry probe outperforms WEAT by 27% relative error rate in detecting LLM bias

## Executive Summary
This paper introduces a novel logistic Bradley-Terry probe to detect implicit sociodemographic biases in large language model (LLM) representations. The probe predicts word pair preferences from contextualized embeddings and outperforms traditional WEAT methods by 27% in error rate. By transferring probes trained on harmless tasks to controversial ones, the authors reveal substantial biases in nationality, politics, religion, and gender despite instruction fine-tuning, suggesting that current alignment methods do not effectively debias latent representations.

## Method Summary
The authors develop a logistic Bradley-Terry probe that predicts pairwise preferences between words using contextualized embeddings from specific transformer layers. They train this probe on innocuous attribute word sets (actions, emotions, numbers) where LLMs can generate coherent preferences, then transfer it to controversial word pairs involving nationality, politics, religion, and gender. The method identifies optimal layers for bias detection (typically middle layers at 50% depth) and reveals implicit preferences even when models decline to answer offensive queries directly.

## Key Results
- Bradley-Terry probe outperforms WEAT by 27% relative error rate in detecting word pair preferences
- Middle transformer layers (30-60%, optimal at 50%) best capture preference information
- Mistral model shows preference for Europe over Africa, Christianity over Judaism, and left-wing over right-wing politics despite safety alignment
- Instruction fine-tuning does not necessarily debias contextualized embeddings

## Why This Works (Mechanism)

### Mechanism 1
The Bradley-Terry probe directly optimizes for pairwise preference discrimination using contextualized embeddings rather than measuring global set similarity like WEAT. It learns a linear decision boundary in the difference space of embeddings (h_α - h_β) trained to maximize likelihood of predicting the LLM's own preference outputs, making it task-specific and more effective.

### Mechanism 2
The probe transfer methodology reveals implicit biases without requiring LLMs to explicitly answer sensitive questions. By training on innocuous tasks where the model generates coherent responses, the learned decision boundary can be applied to controversial word pairs. If the probe still prefers one group, it indicates the embeddings encode bias even when the LLM refuses to answer directly.

### Mechanism 3
Middle transformer layers (around 50% depth) optimally capture word pair preferences by balancing contextual information from deeper layers with task-specific refinement from earlier layers. This "just right" level of abstraction captures the strongest preference signals across different LLMs and tasks.

## Foundational Learning

- **Logistic Bradley-Terry model for pairwise comparisons**: This framework converts linear scores into probabilities of preferring one option over another, essential for the probe's mathematical foundation. Quick check: How does the Bradley-Terry model convert a linear score into a probability of preferring one option over another?

- **Contextualized embeddings and layer-wise representation**: Understanding how information flows through transformer layers and how embeddings capture context is crucial for choosing the right layer and interpreting results. Quick check: Why might middle layers be better than early or late layers for capturing word pair preferences?

- **Probe training and transfer learning**: The methodology involves training probes on one dataset and transferring to another, requiring understanding of probe training objectives and transfer conditions. Quick check: What conditions must hold for a probe trained on innocuous tasks to successfully detect bias in controversial tasks?

## Architecture Onboarding

- **Component map**: Prompt generation → LLM inference → Hidden state extraction → Compute embedding difference → Apply probe → Get preference prediction
- **Critical path**: Prompt → LLM → Extract hidden vectors → Compute difference → Apply probe → Get preference prediction
- **Design tradeoffs**: Linear probes offer interpretability but may miss complex patterns; fixed 50% layer selection works generally but may not be optimal for all tasks; LP probes use LLM predictions while HD probes use human labels, affecting validity
- **Failure signatures**: Low probe accuracy across all layers indicates weak preference signal; layer-wise accuracy that doesn't peak in middle suggests different optimal layer; transfer failure (near 50% win rates) suggests embeddings don't generalize between tasks
- **First 3 experiments**: 1) Verify probe accuracy on ACTION dataset across different layers to identify optimal layer depth, 2) Compare LP vs HD probe performance on EMOTE dataset to understand training method impact, 3) Test probe transfer from ACTION to NUMBER to validate cross-task generalization before applying to controversial tasks

## Open Questions the Paper Calls Out

### Open Question 1
How does training data composition affect observed biases in LLMs? The study uses fixed datasets and doesn't experimentally manipulate training data to isolate causal impact on bias. What evidence would resolve it: Controlled experiments varying pretraining corpora and measuring resulting bias shifts in probes.

### Open Question 2
Can debiasing methods effectively reduce implicit bias in contextualized embeddings without harming model performance? The study only evaluates existing fine-tuning approaches and doesn't test new debiasing techniques. What evidence would resolve it: Applying and evaluating explicit debiasing interventions on embeddings and measuring bias reduction and task performance.

### Open Question 3
How do bias patterns in LLM embeddings generalize across different languages and cultural contexts? The experiments are limited to English models and datasets. What evidence would resolve it: Probing embeddings of multilingual models and non-English LLMs on comparable bias tasks to compare patterns across linguistic and cultural contexts.

## Limitations

- The probe assumes linear decision boundaries can capture complex preference relationships, which may not generalize to all preference types or LLM architectures
- Fixed 50% layer selection may not be optimal for all tasks or models, particularly given different layers capture different semantic aspects
- The probe transfer methodology assumes embeddings for innocuous and controversial tasks are sufficiently similar, but this assumption hasn't been thoroughly validated across diverse domains

## Confidence

**High confidence** for probe architecture and layer selection results, supported by systematic experimentation across multiple models and datasets. **Medium confidence** for bias detection results in controversial domains, as the transfer methodology, while novel, hasn't been validated against ground truth human preferences in these sensitive areas.

## Next Checks

1. **Probe generalization test**: Validate the probe transfer methodology by first applying it to controversial domains where human ground truth preferences are available (e.g., comparing abstract concepts like "freedom" vs "security") to establish whether the probe transfer captures known human biases before applying it to sensitive sociodemographic comparisons.

2. **Layer-depth sensitivity analysis**: Conduct a more granular analysis of probe performance across layer depths (e.g., 10% intervals rather than 30-60% ranges) and test whether different controversial domains show optimal layers at different depths, which would challenge the current fixed 50% layer selection approach.

3. **Cross-model bias consistency**: Test whether the same probes trained on one model family (e.g., LLaMA) can successfully detect biases in completely different architectures (e.g., GPT or Claude models) to validate that the preference signal is architecture-independent and not just capturing idiosyncrasies of specific training datasets.