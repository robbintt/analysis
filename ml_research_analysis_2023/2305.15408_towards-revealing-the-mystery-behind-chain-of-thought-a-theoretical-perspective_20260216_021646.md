---
ver: rpa2
title: 'Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective'
arxiv_id: '2305.15408'
source_url: https://arxiv.org/abs/2305.15408
tags:
- transformer
- input
- layer
- output
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why Chain-of-Thought (CoT) prompting is
  essential for Large Language Models (LLMs) to solve mathematical and decision-making
  problems. The authors theoretically show that bounded-depth Transformers without
  CoT cannot solve basic arithmetic/equation tasks unless the model size grows super-polynomially
  with respect to the input length.
---

# Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective

## Quick Facts
- arXiv ID: 2305.15408
- Source URL: https://arxiv.org/abs/2305.15408
- Authors: 
- Reference count: 40
- Key outcome: This paper theoretically shows bounded-depth Transformers cannot solve basic arithmetic/equation tasks without CoT unless model size grows super-polynomially, while autoregressive Transformers with CoT can solve these tasks with constant size and handle Dynamic Programming problems.

## Executive Summary
This paper provides theoretical justification for why Chain-of-Thought (CoT) prompting is essential for Large Language Models to solve mathematical and decision-making problems. The authors prove that bounded-depth Transformers without CoT cannot solve basic arithmetic and equation tasks unless the model size grows super-polynomially with respect to input length. In contrast, autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. The paper also shows that LLMs with CoT can handle Dynamic Programming problems, justifying its power in tackling complex real-world tasks.

## Method Summary
The paper combines theoretical analysis using circuit complexity theory with extensive empirical experiments. The theoretical analysis proves impossibility results for bounded-depth Transformers without CoT and demonstrates that autoregressive Transformers with CoT can solve both arithmetic/equation tasks and Dynamic Programming problems. The empirical validation uses standard Transformer models (hidden dimension 256, 4 heads) trained on four tasks: Arithmetic Expression, Linear Equation, Longest Increasing Subsequence, and Edit Distance. Models are trained with and without CoT using the AdamW optimizer, and performance is evaluated on test accuracy across varying difficulty levels.

## Key Results
- Transformers without CoT fail to solve arithmetic and equation tasks unless model size grows super-polynomially
- Autoregressive Transformers with CoT achieve near-perfect performance using constant model size
- CoT enables handling of Dynamic Programming problems through complete reasoning chain generation
- Models trained with CoT generalize better to longer sequences than models trained for direct prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive Transformers with CoT can solve arithmetic and equation tasks with constant model size by generating step-by-step intermediate derivations.
- Mechanism: The autoregressive generation increases the "effective depth" of the Transformer circuit proportionally to the number of CoT steps, bypassing the TC0 limitation of bounded-depth Transformers.
- Core assumption: The intermediate derivations can be generated using a commonly-used math language format that allows the model to break complex problems into simpler subproblems.
- Evidence anchors:
  - [abstract] "autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly used math language format"
  - [section 3] "a constant-size autoregressive Transformer already suffices to generate solutions for both math problems"
  - [corpus] Weak - no direct citations about math CoT mechanism
- Break condition: If the problem cannot be decomposed into a sequence of simpler subproblems, or if the math language format cannot capture the intermediate steps.

### Mechanism 2
- Claim: CoT enables Transformers to handle Dynamic Programming (DP) problems by generating the complete reasoning chain.
- Mechanism: The autoregressive generation allows the model to sequentially compute and store the solutions to subproblems, effectively implementing the DP transition function and aggregation function.
- Core assumption: The DP problem satisfies Assumptions 4.2-4.5, meaning the state space is polynomially bounded, and all basic functions can be approximated with polynomial efficiency by a perceptron of constant size.
- Evidence anchors:
  - [abstract] "LLMs with CoT can handle a general class of decision-making problems known as Dynamic Programming"
  - [section 4] "LLMs with CoT can generate the complete chain and output the correct answer"
  - [corpus] Weak - no direct citations about DP CoT mechanism
- Break condition: If the DP problem violates any of the assumptions, particularly if the state space grows super-polynomially or if the transition function cannot be approximated efficiently.

### Mechanism 3
- Claim: CoT improves model performance by providing more structured and informative training signals compared to direct answer prediction.
- Mechanism: The CoT format provides a sequence of intermediate steps that the model can learn to generate, effectively decomposing the problem-solving process into smaller, more manageable tasks.
- Core assumption: The CoT format is learnable by the Transformer model, and the intermediate steps are sufficient to reconstruct the final answer.
- Evidence anchors:
  - [abstract] "extensive experiments on four tasks show that, while Transformers always fail to directly predict the answers, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations"
  - [section 5] "Transformers with CoT achieve near-perfect performance for all tasks and all difficulty levels"
  - [corpus] Weak - no direct citations about CoT training signal mechanism
- Break condition: If the CoT format is too complex for the model to learn, or if the intermediate steps are not sufficient to reconstruct the final answer.

## Foundational Learning

- Concept: Circuit complexity theory (TC0, NC1, P classes)
  - Why needed here: To establish the theoretical limits of Transformer expressivity and prove impossibility results for certain tasks without CoT.
  - Quick check question: What is the relationship between TC0 and NC1 complexity classes, and why is this important for understanding Transformer limitations?

- Concept: Dynamic Programming (DP) framework
  - Why needed here: To analyze the power of CoT in solving decision-making problems and provide a general framework for understanding the step-by-step reasoning process.
  - Quick check question: What are the three key ingredients of a DP algorithm, and how do they relate to the CoT generation process?

- Concept: Autoregressive generation and effective depth
  - Why needed here: To understand how CoT bypasses the limitations of bounded-depth Transformers by increasing the effective depth of the computation through sequential generation.
  - Quick check question: How does the autoregressive generation process increase the "effective depth" of the Transformer circuit, and why is this important for solving complex tasks?

## Architecture Onboarding

- Component map: Input embeddings -> Positional embeddings -> Transformer blocks (multi-head self-attention + feed-forward network) -> Softmax classifier
- Critical path: Generate intermediate derivations sequentially through autoregressive decoding, with each step depending on previous ones to arrive at final answer
- Design tradeoffs: CoT allows solving complex tasks with constant model size but requires longer sequences of intermediate steps; without CoT, larger models may be needed for direct prediction
- Failure signatures: Incorrect generation of CoT steps leads to wrong final answers; failure to learn CoT format prevents task solving even with CoT
- First 3 experiments:
  1. Train a Transformer on the Arithmetic task with and without CoT to verify CoT effectiveness
  2. Vary the number of CoT steps in training data and observe impact on model performance
  3. Test model's ability to generalize to longer sequences or more complex tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt designs affect the emergence of Chain-of-Thought reasoning in large language models?
- Basis in paper: [explicit] The paper mentions that carefully-designed prompts greatly matter LLMs' performance, particularly the CoT prompting that triggers models to output intermediate derivations.
- Why unresolved: The paper focuses on theoretical expressivity of LLMs with CoT, rather than investigating which specific prompts can trigger this process.
- What evidence would resolve it: Experiments comparing different prompt designs on the same mathematical and decision-making tasks, measuring quality and consistency of CoT outputs.

### Open Question 2
- Question: What is the relationship between model size and the ability to generate Chain-of-Thought solutions?
- Basis in paper: [inferred] The paper proves constant-size Transformers can solve tasks with CoT, but scaling model size significantly improves CoT ability in empirical observations.
- Why unresolved: The paper focuses on proving theoretical capability of constant-size Transformers without exploring how model size affects CoT quality or reliability.
- What evidence would resolve it: Experiments varying model size while keeping CoT format fixed, measuring performance and analyzing how CoT quality changes with scale.

### Open Question 3
- Question: How well do large language models generalize their Chain-of-Thought reasoning to out-of-distribution data?
- Basis in paper: [explicit] The paper mentions models trained on CoT datasets generalize well to longer input sequences, suggesting they learned underlying reasoning process rather than memorizing input-output distributions.
- Why unresolved: While paper shows some generalization to longer sequences, it doesn't comprehensively study limits of this generalization or explore other forms of distribution shift.
- What evidence would resolve it: Systematic experiments testing models on various out-of-distribution scenarios and comparing performance between CoT-trained and directly-trained models.

## Limitations

- Theoretical analysis relies on circuit complexity assumptions that may not fully capture practical LLM behavior
- Empirical validation limited to four specific task types with controlled difficulty levels
- Does not provide sufficient evidence to distinguish genuine multi-step reasoning from sophisticated pattern matching

## Confidence

- High Confidence: Theoretical impossibility results for bounded-depth Transformers without CoT are well-established in circuit complexity theory
- Medium Confidence: Theoretical analysis of CoT's ability to solve DP problems assumes specific problem structures that may not hold for all real-world problems
- Low Confidence: Insufficient evidence to distinguish genuine multi-step reasoning from pattern matching in generated CoT steps

## Next Checks

1. **Generalization Test:** Evaluate trained models on arithmetic expressions with significantly longer sequences (20+ operators) and DP problems with larger state spaces to test limits of CoT effectiveness

2. **Mechanism Probing:** Analyze attention patterns and intermediate representations during CoT generation to determine whether model is genuinely performing multi-step reasoning versus memorizing patterns

3. **Efficiency Analysis:** Measure computational cost (inference time, memory usage) of CoT generation versus direct prediction across all tasks, quantifying trade-off between performance gains and resource requirements