---
ver: rpa2
title: How Different Is Stereotypical Bias Across Languages?
arxiv_id: '2307.07331'
source_url: https://arxiv.org/abs/2307.07331
tags:
- bias
- language
- different
- data
- stereotypical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the study of stereotypical bias in language models
  by conducting the first comprehensive multilingual analysis. Using a semi-automated
  translation approach, the authors create German, French, Spanish, and Turkish versions
  of the StereoSet benchmark.
---

# How Different Is Stereotypical Bias Across Languages?

## Quick Facts
- arXiv ID: 2307.07331
- Source URL: https://arxiv.org/abs/2307.07331
- Authors: 
- Reference count: 34
- Primary result: First comprehensive multilingual analysis of stereotypical bias across English, German, French, Spanish, and Turkish

## Executive Summary
This work extends stereotypical bias research in language models by conducting the first comprehensive multilingual analysis. Using a semi-automated translation approach, the authors create German, French, Spanish, and Turkish versions of the StereoSet benchmark and evaluate both monolingual and multilingual models across three transformer architectures (BERT, GPT-2, T5). The study reveals that English models show the strongest bias, Turkish models display the least stereotypical bias, and mGPT-2 exhibits partly anti-stereotypical behavior across languages. These findings highlight the importance of multilingual evaluation and demonstrate that bias patterns vary significantly across languages.

## Method Summary
The authors translate the English StereoSet benchmark to German, French, Spanish, and Turkish using Amazon Web Services translation services with special handling for "BLANK" tokens. They evaluate three transformer architectures (BERT, GPT-2, T5) in both monolingual and multilingual configurations across all languages. The evaluation uses both generative and discriminative approaches for intra-sentence and inter-sentence tests. Multilingual GPT-2 and T5 models are fine-tuned on NSP tasks using Wikipedia data. Bias is measured using ICAT scores that combine Language Modeling Score (LMS) and Stereotype Score (SS).

## Key Results
- mGPT-2 exhibits partly anti-stereotypical behavior across all tested languages
- English (monolingual) models exhibit the strongest stereotypical bias
- Turkish models display the least stereotypical bias across all architectures
- Bias patterns vary significantly across languages, highlighting the importance of multilingual evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual translation of StereoSet enables comparative bias analysis across languages and architectures.
- Mechanism: Semi-automatic AWS translation preserves structural integrity while allowing cross-linguistic evaluation of PLM bias patterns.
- Core assumption: Translated datasets maintain semantic equivalence to English originals for valid bias measurement.
- Evidence anchors:
  - [abstract] "Using a semi-automated translation approach, the authors create German, French, Spanish, and Turkish versions of the StereoSet benchmark."
  - [section 5.1] "We translate StereoSet to German, French, Spanish, and Turkish using Amazon Web Service (AWS) translation services in Python (boto3)."
  - [corpus] Weak evidence - corpus only shows related work on multilingual bias but no direct validation of translation quality.
- Break condition: Semantic drift in translation causes bias patterns to reflect language-specific artifacts rather than true stereotype differences.

### Mechanism 2
- Claim: Different transformer architectures exhibit distinct bias patterns when evaluated multilingually.
- Mechanism: Encoder (BERT), decoder (GPT-2), and encoder-decoder (T5) architectures have fundamentally different training objectives that affect how they internalize and reproduce stereotypes.
- Core assumption: Architectural differences in pre-training objectives lead to measurable differences in stereotypical behavior across languages.
- Evidence anchors:
  - [abstract] "The key findings show that mGPT-2 exhibits partly anti-stereotypical behavior across languages, English models show the strongest bias, and Turkish models display the least stereotypical bias."
  - [section 3.2] "We evaluate all three different commonly used pre-trained transformer architectures: encoder, decoder, and encoder-decoder."
  - [corpus] Weak evidence - corpus neighbors discuss multilingual bias but don't specifically address architectural differences.
- Break condition: If training data composition overwhelms architectural effects, differences become negligible across languages.

### Mechanism 3
- Claim: Monolingual models show stronger bias than multilingual counterparts, except for Turkish.
- Mechanism: Multilingual models benefit from broader linguistic exposure that dilutes stereotypical associations present in single-language training data.
- Core assumption: Training on multiple languages provides regularization that reduces stereotypical bias manifestation.
- Evidence anchors:
  - [abstract] "English (monolingual) models exhibit the strongest bias, and the stereotypes reflected in the data set are least present in Turkish models."
  - [section 6.2] "Regarding the biasedness of the different models, we observe that English models have the most severe stereotypical tendency."
  - [corpus] Weak evidence - corpus shows multilingual bias work but lacks specific comparison of monolingual vs multilingual bias strength.
- Break condition: If multilingual models inherit stereotypes from dominant training languages, they may not show reduced bias.

## Foundational Learning

- Concept: Transformer architectures and their pre-training objectives
  - Why needed here: Different architectures (BERT, GPT-2, T5) require different evaluation approaches due to their training objectives (MLM, LM, span corruption).
  - Quick check question: Why does GPT-2 require a generative approach for intra-sentence tests while BERT uses discriminative evaluation?

- Concept: Statistical bias measurement in NLP
  - Why needed here: Understanding how Language Modeling Score (LMS) and Stereotype Score (SS) combine to form ICAT is crucial for interpreting results.
  - Quick check question: What does an ICAT score of 50% indicate about a model's behavior?

- Concept: Cross-linguistic NLP evaluation
  - Why needed here: Translating benchmarks across languages requires understanding grammatical differences and their impact on bias measurement.
  - Quick check question: How might grammatical gender in German/French/Spanish affect bias measurement compared to gender-neutral Turkish?

## Architecture Onboarding

- Component map: Translation pipeline → Model evaluation framework → Statistical analysis module
- Critical path: Translate StereoSet → Load models → Evaluate intra-sentence → Evaluate inter-sentence → Calculate scores → Analyze results
- Design tradeoffs: Semi-automated translation vs manual translation (speed vs accuracy), generative vs discriminative evaluation approaches (generality vs precision)
- Failure signatures: Poor LMS scores indicate language modeling issues; low SS indicates lack of stereotypical bias; high SS indicates strong bias; Turkish consistently low scores may indicate cultural differences in stereotypes
- First 3 experiments:
  1. Run intra-sentence evaluation on mBERT across all languages to establish baseline performance
  2. Compare mGPT-2 generative vs discriminative inter-sentence approaches for German
  3. Evaluate Turkish monolingual models to verify unexpectedly low bias findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does mGPT-2 exhibit partly anti-stereotypical behavior across languages, and what underlying mechanisms in its training or architecture cause this unexpected pattern?
- Basis in paper: [explicit] The authors observe that mGPT-2 shows surprising anti-stereotypical behavior across all languages tested, which contradicts typical bias patterns in language models.
- Why unresolved: The paper identifies this as a key finding but does not investigate the causal factors behind this anti-stereotypical tendency.
- What evidence would resolve it: Comparative analysis of mGPT-2's training data, architecture differences, or fine-tuning procedures compared to other models could reveal why it shows this opposite bias pattern.

### Open Question 2
- Question: How do cultural differences between language communities influence the manifestation of stereotypical bias in language models, and can these differences be quantified?
- Basis in paper: [inferred] The authors observe that Turkish models show the least stereotypical bias and suggest this may be due to cultural differences, but do not systematically investigate this relationship.
- Why unresolved: The paper only speculates about cultural influence without conducting cross-cultural analysis of stereotypes or comparing data sources from different regions.
- What evidence would resolve it: Cross-cultural studies comparing stereotypes across different language communities, along with analysis of training data provenance, could quantify cultural influences on model bias.

### Open Question 3
- Question: How would larger, more diverse data sets that capture a broader range of stereotypes affect the measurement of bias across different languages and model architectures?
- Basis in paper: [inferred] The authors acknowledge that their data set may be limited to Western stereotypes and suggest future work should build different data sets for different cultural groups.
- Why unresolved: The current analysis is constrained by the limited scope of the StereoSet data, which was created primarily for English and may not capture culturally-specific stereotypes.
- What evidence would resolve it: Creating and testing models with culturally-diverse data sets across multiple regions, then comparing bias measurements across these sets, would reveal how data diversity affects bias detection.

## Limitations
- Semi-automated translation may introduce semantic drift affecting bias measurement validity
- Same multilingual model (mBERT) used for both monolingual and multilingual evaluations across different languages
- No statistical significance testing to determine whether observed differences are meaningful

## Confidence
- **High Confidence**: English models show strongest bias - well-supported data and aligns with NLP research expectations
- **Medium Confidence**: Turkish models show least bias - requires validation as could reflect translation artifacts or genuine cultural differences
- **Low Confidence**: mGPT-2 anti-stereotypical behavior - lacks sufficient explanation of underlying mechanism

## Next Checks
1. Conduct human evaluation of translated sentences to assess semantic equivalence with English originals and establish translation reliability
2. Run controlled experiments comparing same architecture (BERT) in monolingual vs multilingual configurations across all languages to isolate training effects
3. Perform statistical significance testing (paired t-tests or Wilcoxon signed-rank tests) on ICAT scores across languages with confidence intervals for all reported differences