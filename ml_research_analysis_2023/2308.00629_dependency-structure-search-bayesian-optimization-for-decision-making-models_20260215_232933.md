---
ver: rpa2
title: Dependency Structure Search Bayesian Optimization for Decision Making Models
arxiv_id: '2308.00629'
source_url: https://arxiv.org/abs/2308.00629
tags:
- policy
- predprey
- learning
- role
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hessian-Aware GP-UCB (HA-GP-UCB), a Bayesian
  optimization method for optimizing decision-making policies in multi-agent systems.
  The key idea is to leverage the concept of roles and role interactions to model
  agent policies compactly, enabling tractable optimization in high-dimensional spaces.
---

# Dependency Structure Search Bayesian Optimization for Decision Making Models

## Quick Facts
- arXiv ID: 2308.00629
- Source URL: https://arxiv.org/abs/2308.00629
- Reference count: 40
- One-line primary result: HA-GP-UCB optimizes compact multi-agent policies using Hessian-observed dependency structure learning, achieving improved regret bounds and outperforming MARL methods in sparse reward scenarios.

## Executive Summary
This paper introduces Hessian-Aware GP-UCB (HA-GP-UCB), a Bayesian optimization method for optimizing decision-making policies in multi-agent systems. The key innovation is leveraging Hessian observations during just-in-time compilation to learn the dependency structure among policy parameters, enabling efficient optimization in high-dimensional spaces through additive decomposition. The method uses role-based abstractions to create compact policy representations, making it suitable for memory-constrained devices.

## Method Summary
HA-GP-UCB uses a metamodel architecture that JIT-compiles parameters into policy models with role assignment and interaction abstracted into static algorithms. During compilation, the Hessian of the policy value function is observed, revealing the dependency structure among parameters. This structure is learned using a streaming algorithm (Sec 4.5) that updates GP kernels to reflect the additive decomposition. The method balances exploration and exploitation using the GP-UCB acquisition function while exploiting the learned structure for efficient optimization.

## Key Results
- HA-GP-UCB outperforms standard GP-UCB, TurBO, AleBO, TreeBO, LineBO, and GIBO on multi-agent coordination tasks
- Achieves superior performance compared to MARL algorithms (MADDPG, FACMAC, COMIX, RODE, CDS) under sparse reward scenarios
- Demonstrates effectiveness in optimizing compact policies suitable for memory-constrained devices

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: HA-GP-UCB learns the dependency structure of policy parameters via Hessian observations during JIT compilation, enabling additive decomposition.
- **Mechanism**: During just-in-time compilation, the Hessian of the policy value function is observed. This Hessian reveals the dependency structure among policy parameters, allowing the high-dimensional problem to be decomposed into independent low-dimensional subproblems that GP-UCB can optimize efficiently.
- **Core assumption**: The Hessian of the policy value function directly encodes the additive structure of the underlying objective, and this structure can be inferred from noisy Hessian observations.
- **Evidence anchors**:
  - [abstract]: "By exploiting the Hessian of the policy value function during this compilation, HA-GP-UCB efficiently learns the dependency structure among policy parameters"
  - [section 4.5]: "We propose learning the dependency structure during JIT-compilation... We can measure a surrogate Hessian during JIT-compilation which significantly simplifies the task of learning the additive structure"
  - [corpus]: Weak - No direct citations about Hessian-based structure learning in BO literature
- **Break condition**: If the Hessian observations are too noisy relative to the true signal (σ²ₙ ≫ σ²ₕ), the dependency structure cannot be reliably learned, and the regret bounds fail.

### Mechanism 2
- **Claim**: The role-based abstraction reduces the effective dimensionality of multi-agent policy search while maintaining expressiveness.
- **Mechanism**: Agents are assigned to roles based on state-dependent affinity functions, and interactions occur only between roles through a message-passing neural network. This creates a compact policy representation where the number of parameters grows with the number of roles rather than agents.
- **Core assumption**: Multi-agent coordination can be effectively captured by role assignments and role interactions, and this structure is known a priori.
- **Evidence anchors**:
  - [section 4.2]: "To achieve a compact and tractable policy space, we consider policies under the useful abstractions of role and role interaction"
  - [section 4.3]: "To overcome these challenges, we propose a metamodel which JIT-compiles into a graphical model... The JIT-compilation is conditional on the agents' state thus capturing dynamic role interactions"
  - [corpus]: Weak - Limited direct evidence from corpus about role-based multi-agent policy search
- **Break condition**: If the optimal policy cannot be decomposed into role-based interactions, the abstraction becomes a bottleneck and performance degrades.

### Mechanism 3
- **Claim**: Sparse and malformed reward environments degrade gradient-based RL approaches while BO remains effective.
- **Mechanism**: Gradient-based methods require informative gradients to make progress, but sparse rewards provide little gradient information, causing these methods to fail. BO, being gradient-free, can still find optimal policies by directly evaluating policy performance.
- **Core assumption**: Sparse or uninformative reward feedback severely degrades the performance of gradient-based RL algorithms.
- **Evidence anchors**:
  - [abstract]: "Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance"
  - [section 5.3]: "We compare against several competing RL and MARL algorithms under malformed reward scenarios... Table 1 shows that the performance of competing algorithms is severely degraded with sparse reward and HA-GP-UCB outperforms competing approaches"
  - [corpus]: Weak - No direct citations about BO robustness to sparse rewards
- **Break condition**: If reward feedback becomes extremely sparse (e.g., reward every 1000+ steps), even BO may require too many evaluations to find good policies.

## Foundational Learning

- **Concept: Bayesian Optimization with Gaussian Process surrogates**
  - Why needed here: HA-GP-UCB builds directly on GP-UCB, using Gaussian processes to model the unknown policy value function and guide the search for optimal parameters.
  - Quick check question: What acquisition function does GP-UCB use to balance exploration and exploitation?

- **Concept: Additive decomposition in high-dimensional optimization**
  - Why needed here: The core insight is decomposing a high-dimensional policy optimization problem into independent low-dimensional subproblems, which HA-GP-UCB achieves by learning the dependency structure.
  - Quick check question: How does additive decomposition reduce the computational complexity of Bayesian optimization in high dimensions?

- **Concept: Markov Random Fields and message passing**
  - Why needed here: Role interactions are modeled as a graphical model, with the policy using message passing neural networks to compute actions based on role states and interactions.
  - Quick check question: What is the key advantage of using message passing neural networks for role interaction modeling?

## Architecture Onboarding

- **Component map**: Parameters -> JIT compilation -> Policy evaluation -> Hessian observation -> Structure learning -> Acquisition function update -> Next parameters

- **Critical path**: Parameters → JIT compilation → Policy evaluation → Hessian observation → Structure learning → Acquisition function update → Next parameters

- **Design tradeoffs**: Compact role-based policies vs. expressiveness of full neural networks; Hessian observations vs. computational overhead; additive decomposition vs. potential loss of interactions

- **Failure signatures**: Poor performance on tasks that don't decompose well into roles; sensitivity to Hessian noise; slow convergence when role interactions are dense

- **First 3 experiments**:
  1. Validate Hessian structure learning on a simple 2D function with known additive structure
  2. Compare role-based policy vs. flat neural network on multi-agent ant task with dense rewards
  3. Test sparse reward performance on drone delivery task with varying reward sparsity levels

## Open Questions the Paper Calls Out
1. How does the surrogate Hessian Hπ relate to the true Hessian Hv of the value function v(θ), and under what conditions is Hπ a reliable proxy for learning the dependency structure?
2. How sensitive is HA-GP-UCB to the choice of the cutoff constant ch, and what is the impact of noise in the surrogate Hessian Hπ on the performance of HA-GP-UCB?
3. Can HA-GP-UCB be extended to handle larger policy spaces beyond the compact policies considered in this paper, such as those found in reinforcement learning?

## Limitations
- Weak evidence for Hessian-based structure learning claims with no direct corpus support
- Role-based abstraction may not capture optimal policies that don't decompose into role interactions
- Sparse reward robustness claims lack direct supporting evidence from literature

## Confidence
- **High confidence**: The metamodel architecture with JIT compilation and role-based abstractions is clearly specified and implementable.
- **Medium confidence**: The theoretical regret bounds and the general approach of using Hessian information for structure learning are sound, but practical effectiveness depends on the quality of Hessian observations.
- **Low confidence**: The claims about superior performance in sparse reward environments lack direct supporting evidence from the corpus.

## Next Checks
1. **Controlled ablation study**: Compare HA-GP-UCB with and without Hessian structure learning on a synthetic function with known additive structure to quantify the benefit of Hessian observations.

2. **Role expressiveness evaluation**: Test HA-GP-UCB on a multi-agent task where the optimal policy cannot be decomposed into role-based interactions to identify the limits of the role abstraction.

3. **Noise sensitivity analysis**: Systematically vary the noise level in Hessian observations to determine the threshold below which structure learning fails and regret bounds no longer hold.