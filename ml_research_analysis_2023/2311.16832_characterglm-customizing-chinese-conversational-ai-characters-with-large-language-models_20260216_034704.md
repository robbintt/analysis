---
ver: rpa2
title: 'CharacterGLM: Customizing Chinese Conversational AI Characters with Large
  Language Models'
arxiv_id: '2311.16832'
source_url: https://arxiv.org/abs/2311.16832
tags:
- character
- dialogue
- characterglm-66b
- uni00000048
- characters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CharacterGLM is a series of Chinese conversational AI models ranging
  from 6B to 66B parameters, designed for character-based dialogue generation. By
  configuring character attributes (identity, interests, viewpoints, etc.) and behaviors
  (linguistic features, emotional expressions, etc.), CharacterGLM enables customization
  of various AI characters for engaging and human-like conversations.
---

# CharacterGLM: Customizing Chinese Conversational AI Characters with Large Language Models

## Quick Facts
- arXiv ID: 2311.16832
- Source URL: https://arxiv.org/abs/2311.16832
- Reference count: 20
- Key outcome: CharacterGLM series (6B-66B parameters) for Chinese character-based dialogue generation, outperforming mainstream models in consistency, human-likeness, and engagement through attribute-behavior separation and self-refinement.

## Executive Summary
CharacterGLM is a family of Chinese conversational AI models designed to generate engaging dialogue for customizable AI characters. The models range from 6B to 66B parameters and are built by fine-tuning ChatGLM on a large-scale Chinese CharacterDial corpus. By separating character traits into attributes (static properties like identity and interests) and behaviors (dynamic linguistic patterns), CharacterGLM enables precise control over character consistency and human-like expression. The system incorporates data augmentation and self-refinement via human feedback to improve generalization and engagement.

## Method Summary
CharacterGLM adapts ChatGLM models through supervised fine-tuning on the Chinese CharacterDial corpus, using character prompts that combine static attributes and dynamic behaviors. The training pipeline includes data augmentation (summarization, paraphrasing, stylization) to improve prompt generalization, and self-refinement via human-prototype interaction to iteratively improve model responses. The model was evaluated on consistency, human-likeness, and engagement metrics through manual pairwise comparisons against mainstream models including GPT series.

## Key Results
- CharacterGLM-6B achieved 2.36, 2.43, and 2.50 scores on consistency, human-likeness, and engagement respectively in manual evaluations
- Outperformed mainstream models including GPT series in consistency, human-likeness, and engagement according to manual pairwise comparisons
- 6B version released publicly, larger versions available via API

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CharacterGLM improves long-term conversational consistency by training on augmented character prompts that vary in linguistic style while preserving core attributes.
- Mechanism: Data augmentation (summarization, paraphrasing, stylization) generates multiple prompt variants per character profile, exposing the model to diverse phrasings of the same semantic content during supervised fine-tuning.
- Core assumption: The model can learn to generalize across prompt variations and maintain consistent character identity regardless of phrasing.
- Evidence anchors:
  - [section]: "To enhance the character's generalization, we employ data augmentation methods including summarization, paraphrasing, and stylization, utilizing Claude-2 [Anthropic, 2023] to synthesize diverse prompts."
  - [section]: "The character prompt is concatenated with the dialogue for fine-tuning. Notably, our training data expands linearly with the number of augmented character prompts."
  - [corpus]: Weak - No direct corpus evidence; relies on internal augmentation pipeline.
- Break condition: If augmented prompts introduce contradictions or drift from original character attributes, consistency will degrade rather than improve.

### Mechanism 2
- Claim: Self-refinement via human-prototype interaction data improves engagement by correcting model failures in real conversational contexts.
- Mechanism: After initial deployment, users interact with the prototype, modifying responses that don't meet expectations. This iterative feedback loop is then used for additional supervised fine-tuning, allowing the model to learn from corrected outputs.
- Core assumption: Human corrections capture nuanced conversational expectations that automated metrics miss, and these corrections are representative of desired behavior.
- Evidence anchors:
  - [section]: "To further refine the model, we recruit seed users of the system in a collaborative human-prototype interaction process... We prompt the user to make appropriate modifications until the response satisfies their own needs if a character's response does not align with the user's expectations."
  - [section]: "Subsequently, we involve the interaction data in the supervised fine-tuning process, thereby facilitating continuous self-refinement of the model."
  - [corpus]: Weak - No direct corpus evidence; relies on post-deployment interaction data.
- Break condition: If user modifications introduce biases or inconsistencies, or if the self-refinement loop overfits to specific user preferences.

### Mechanism 3
- Claim: Separating attributes (static character traits) from behaviors (dynamic linguistic patterns) enables more targeted modeling of character consistency and human-likeness.
- Mechanism: The design principle explicitly categorizes character features into attributes (identities, interests, viewpoints, etc.) and behaviors (linguistic features, emotional expressions, interaction patterns), allowing the model to learn distinct representations for each.
- Core assumption: This separation aligns with how humans perceive and maintain character consistency in conversation, making it easier for the model to balance stable traits with dynamic expression.
- Evidence anchors:
  - [section]: "In this context, we analyze the human traits that influence conversational expressions and divide them into two primary categories: attributes and behaviors."
  - [section]: "Attributes are predominantly reflected in the content of responses, whereas behaviors focus on more tone and style."
  - [corpus]: Weak - No direct corpus evidence; conceptual framework based on design principles.
- Break condition: If the separation is too rigid, it may prevent natural blending of traits and expressions, making responses feel artificial.

## Foundational Learning

- Concept: Supervised fine-tuning with character prompts
  - Why needed here: Adapts a general-purpose LLM to the specific task of character-based dialogue generation by conditioning on character profiles
  - Quick check question: What format is used to combine character profiles with dialogue during training?

- Concept: Data augmentation for prompt generalization
  - Why needed here: Increases model robustness to variations in how character profiles are expressed while maintaining semantic consistency
  - Quick check question: Which three augmentation methods are applied to character profiles?

- Concept: Self-refinement through human feedback
  - Why needed here: Enables continuous improvement of the model based on real user interactions and corrections, addressing limitations of initial training data
  - Quick check question: How is the human-prototype interaction data collected and used in the training pipeline?

## Architecture Onboarding

- Component map:
  - Data collection pipeline (human role-playing, synthesis via LLMs, literary extraction, human-prototype interaction)
  - Character prompt generation (natural language formalization + augmentation)
  - Training stack (ChatGLM backbone + supervised fine-tuning + self-refinement)
  - Evaluation framework (pointwise and pairwise comparisons across multiple dimensions)

- Critical path: Character profile → Prompt augmentation → Dialogue collection → Supervised fine-tuning → Deployment → Human feedback collection → Self-refinement → Evaluation

- Design tradeoffs:
  - Model size vs. deployment accessibility (6B version released publicly vs. larger versions via API)
  - Data quality vs. scale (manual extraction for quality vs. synthesis for volume)
  - Prompt diversity vs. consistency (augmentation improves generalization but risks attribute drift)

- Failure signatures:
  - Inconsistent character behavior across dialogue turns
  - Responses that contradict character profile attributes
  - Low engagement scores in pairwise evaluations
  - High repetition rates in fine-grained error analysis

- First 3 experiments:
  1. Compare consistency scores between models trained with and without prompt augmentation
  2. Measure engagement improvement after self-refinement training with human feedback data
  3. Evaluate the impact of different augmentation methods (summarization vs. paraphrasing vs. stylization) on character generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective techniques for long-term memory and growth of AI characters?
- Basis in paper: [explicit] The paper identifies "Long-term memorization and growth of AI characters" as a key challenge for future work, noting that current LLMs have finite context windows.
- Why unresolved: The paper acknowledges this limitation but does not propose specific solutions for enabling AI characters to retain interactions and evolve over extended periods.
- What evidence would resolve it: Empirical studies comparing different memory architectures (e.g., external memory, replay buffers) on AI character performance in long-term interactions, measuring engagement and relationship depth.

### Open Question 2
- Question: How can self-awareness be implemented in AI characters to maintain distinct personalities and knowledge boundaries?
- Basis in paper: [explicit] The paper highlights "Self-awareness of AI characters" as a future challenge, emphasizing the need for characters to understand their own traits, limitations, and knowledge.
- Why unresolved: While the paper recognizes the importance of self-awareness, it does not provide concrete methods for achieving this in LLM-based characters.
- What evidence would resolve it: Development and evaluation of AI characters with explicit self-modeling capabilities, demonstrating improved consistency, appropriate uncertainty expression, and user trust compared to non-self-aware characters.

### Open Question 3
- Question: What are the benefits and challenges of social interactions between AI characters?
- Basis in paper: [explicit] The paper suggests exploring "Social interaction between AI characters" as a future direction, proposing the concept of a 'character society' where characters learn from each other.
- Why unresolved: The paper introduces this idea but does not investigate the potential impacts on character development or user interactions.
- What evidence would resolve it: Comparative studies of AI characters trained with and without character-character interaction data, measuring improvements in response quality, diversity, and user engagement.

## Limitations

- Weak empirical support for core mechanisms: The paper describes data augmentation and self-refinement as key mechanisms but doesn't provide experimental validation of their individual contributions to performance improvements.
- Limited transparency in data sources: The paper mentions multiple data sources but provides insufficient detail on their composition, scale, and quality control, making it difficult to assess potential biases.
- Narrow evaluation scope: Manual evaluations focus on consistency, human-likeness, and engagement but lack automated metrics for coherence, repetition, and factual correctness, and don't include statistical significance testing.

## Confidence

- High confidence: The model architecture and training pipeline are clearly specified. The use of character prompts for supervised fine-tuning is a well-established approach, and the technical implementation details are adequately documented.
- Medium confidence: The claim that CharacterGLM outperforms mainstream models (including GPT series) is supported by manual evaluations, but the evaluation methodology lacks transparency regarding rater selection, training, and potential biases. The comparison would be stronger with automated metrics and larger-scale evaluation.
- Low confidence: The effectiveness of the self-refinement mechanism is particularly weakly supported, as the paper doesn't provide quantitative evidence of improvement after human feedback integration or describe the sample size and selection process for the human-prototype interactions.

## Next Checks

1. **Ablation study on augmentation methods**: Compare model performance with different combinations of augmentation techniques (summarization only, paraphrasing only, stylization only, and all three combined) to isolate which methods contribute most to consistency improvements.

2. **Pre/post self-refinement analysis**: Evaluate the same character interactions before and after self-refinement training to measure specific improvements in engagement and consistency, including statistical significance testing.

3. **Long-context consistency testing**: Design evaluation prompts that require maintaining character consistency over extended dialogues (20+ turns) to test whether the model can sustain character identity across longer conversational contexts than reported in the current evaluation.