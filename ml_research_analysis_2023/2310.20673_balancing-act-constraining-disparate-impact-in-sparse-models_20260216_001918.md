---
ver: rpa2
title: 'Balancing Act: Constraining Disparate Impact in Sparse Models'
arxiv_id: '2310.20673'
source_url: https://arxiv.org/abs/2310.20673
tags:
- accuracy
- ceag
- race
- pruning
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of disparate impact in model pruning,
  where some data sub-groups suffer higher accuracy drops than others. The authors
  propose a constrained optimization approach (CEAG) that directly bounds group-level
  accuracy gaps between dense and sparse models, providing interpretable success criteria.
---

# Balancing Act: Constraining Disparate Impact in Sparse Models

## Quick Facts
- **arXiv ID**: 2310.20673
- **Source URL**: https://arxiv.org/abs/2310.20673
- **Reference count**: 40
- **Primary result**: CEAG mitigates pruning-induced disparity while maintaining aggregate performance, scaling to hundreds of groups.

## Executive Summary
This paper addresses the problem of disparate impact in model pruning, where accuracy drops are unevenly distributed across data subgroups. The authors propose a constrained optimization approach (CEAG) that directly bounds group-level accuracy gaps between dense and sparse models, providing interpretable success criteria. Their method uses proxy constraints and replay buffers to handle non-differentiable constraints and reduce variance. Experiments across multiple architectures, datasets, and sparsity levels show that CEAG consistently mitigates pruning-induced disparity while maintaining comparable aggregate performance.

## Method Summary
The paper proposes a constrained optimization approach (CEAG) to mitigate disparate impact in model pruning. The method bounds the excess accuracy gaps (EAGs) between dense and sparse models for each subgroup, using proxy constraints (excess negative loss gaps) to handle non-differentiable accuracy metrics. Replay buffers store recent per-sample accuracy measurements to reduce variance in constraint estimation. The optimization alternates between updating model parameters to minimize loss while respecting constraints, and updating Lagrange multipliers to enforce the EAG bounds.

## Key Results
- CEAG consistently reduces disparity metrics (max_g ψ_g and Ψ_PW) across all tested architectures and datasets
- The method scales to hundreds of groups (CIFAR-100 with 100 classes) while maintaining performance
- Replay buffers significantly improve disparity metrics, especially for the Equalized Loss baseline
- CEAG achieves lower disparity than baselines while maintaining comparable aggregate accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CEAG directly controls disparate impact by bounding group-level accuracy gaps between dense and sparse models.
- **Mechanism**: Instead of relying on surrogate metrics like loss, CEAG imposes constraints on the excess accuracy gaps (EAGs), which quantify how much more each group's accuracy degrades compared to the overall model degradation. This provides an interpretable success criterion.
- **Core assumption**: The disparity in pruning-induced accuracy drops is best captured by comparing each group's accuracy change to the overall accuracy change, rather than equalizing losses across groups.
- **Evidence anchors**:
  - [abstract]: "our formulation bounds the accuracy change between the dense and sparse models, for each sub-group"
  - [section 3.1]: "Disparate Impact of Pruning. Following our discussion above, we say a sparse model hθs experiences low disparate impact... if the changes in performance are similar across sub-groups, i.e. ∆g (θs, θd) ≈ ∆g′ (θs, θd)"
- **Break condition**: If the dense model already has significant accuracy disparity across groups, bounding EAGs may not sufficiently address the overall fairness concern.

### Mechanism 2
- **Claim**: Using replay buffers reduces variance in constraint estimation, enabling stable training with many groups.
- **Mechanism**: Replay buffers store the k most recent per-sample accuracy measurements for each group. This smooths out the noise in accuracy gap estimates, especially for under-represented groups or when the number of constraints is large relative to batch size.
- **Core assumption**: The accuracy gap estimates have high variance across mini-batches, particularly for minority groups, and this variance can be reduced by aggregating information across multiple batches.
- **Evidence anchors**:
  - [section 4.2]: "We overcome these issues by estimating constraints based on information across multiple mini-batches... We refer to the data structure that stores historic accuracies as a replay buffer"
  - [section 5.3]: "Disparity metrics for EL and CEAG are better when employing replay buffers, both on the train and test sets. This difference is more notable for EL."
- **Break condition**: If the buffer size k is too small, variance reduction is insufficient. If k is too large, estimates become biased towards outdated measurements.

### Mechanism 3
- **Claim**: Proxy constraints enable optimization of non-differentiable accuracy gap constraints using differentiable surrogates.
- **Mechanism**: Since accuracy gaps are discrete and non-differentiable, CEAG uses excess (negative) loss gaps as surrogates when computing gradients for the primal parameters. The actual accuracy gap constraints are preserved for updating the dual variables.
- **Core assumption**: Drops in accuracy for the sparse model correspond to increases in loss, making negative loss gaps a reasonable surrogate for accuracy gaps.
- **Evidence anchors**:
  - [section 4.1]: "we must resort to a surrogate ˜ψg for computing gradients with respect to θs... we choose surrogates ˜ψg given by the excess (negative) loss gaps"
  - [section 4]: "This update scheme is inspired by the proxy-constraint technique introduced by Cotter et al. (2019)"
- **Break condition**: If the relationship between accuracy drops and loss increases is not monotonic or consistent across different architectures/datasets, the surrogate may lead optimization astray.

## Foundational Learning

- **Concept: Constrained optimization with Lagrange multipliers**
  - Why needed here: CEAG is formulated as a constrained optimization problem where Lagrange multipliers are used to enforce the EAG constraints while minimizing the loss.
  - Quick check question: What is the relationship between the Lagrangian and the original constrained problem, and under what conditions does a saddle point of the Lagrangian correspond to a global constrained minimizer?

- **Concept: Variance reduction in stochastic optimization**
  - Why needed here: The replay buffer technique is a form of variance reduction that stabilizes the estimation of group-level accuracy gaps, which are crucial for the CEAG constraints.
  - Quick check question: How does aggregating information across multiple mini-batches (as done with replay buffers) reduce the variance of gradient estimates, and what is the tradeoff with bias?

- **Concept: Non-convex-concave min-max optimization**
  - Why needed here: The Lagrangian of CEAG leads to a non-convex-concave min-max problem, which is optimized using alternating gradient descent-ascent.
  - Quick check question: What are the convergence guarantees for alternating gradient descent-ascent in non-convex-concave settings, and how do they compare to other methods like extragradient?

## Architecture Onboarding

- **Component map**:
  Dense model -> Replay buffers -> Proxy constraints -> Lagrange multipliers -> Sparse model

- **Critical path**:
  1. Initialize dense model accuracy on each group.
  2. For each training iteration:
     - Sample mini-batch and compute sparse model predictions.
     - Update replay buffers with per-sample accuracy.
     - Compute EAGs from replay buffers and dense model accuracies.
     - Compute surrogate loss gaps for gradient calculation.
     - Update Lagrange multipliers based on EAG constraint violations.
     - Update sparse model parameters using weighted loss gradients.

- **Design tradeoffs**:
  - Replay buffer size (k): Larger k reduces variance but increases bias and memory usage.
  - Tolerance (ϵ): Larger ϵ makes the problem easier to solve but may allow more disparity.
  - Constraint formulation: CEAG constrains only positive EAGs for flexibility, but could also constrain negative EAGs for stricter fairness.

- **Failure signatures**:
  - Multipliers growing without bound: Indicates infeasibility or poor optimization dynamics.
  - High variance in maxg ψg across seeds: Suggests insufficient variance reduction or unstable constraints.
  - Disparity metrics worse on test than train: Points to overfitting or poor generalization of the mitigation.

- **First 3 experiments**:
  1. Verify CEAG reduces disparity on training set for a simple task (e.g., UTKFace race prediction at moderate sparsity).
  2. Test the effect of replay buffer size on variance reduction and final disparity metrics.
  3. Compare CEAG's performance to equalized loss and NFT baselines on a task with many groups (e.g., CIFAR-100).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we improve the generalization of constrained optimization methods for mitigating disparate impact of pruning to unseen data?
- **Basis in paper**: [explicit] The paper acknowledges that all mitigation methods considered, including CEAG, may fail to generalize to unseen data.
- **Why unresolved**: Generalization issues are a fundamental challenge in machine learning, and the specific factors contributing to poor generalization in this context are not fully understood.
- **What evidence would resolve it**: Empirical studies comparing the performance of different mitigation methods on various datasets and architectures, along with theoretical analysis of the generalization bounds.

### Open Question 2
- **Question**: What is the optimal balance between the number of constraints and the complexity of the constrained optimization problem for mitigating disparate impact of pruning?
- **Basis in paper**: [inferred] The paper discusses the trade-off between using more constraints to better control disparity and the increased computational cost and optimization challenges associated with a larger number of constraints.
- **Why unresolved**: The optimal balance likely depends on the specific task, dataset, and architecture, and finding it requires further empirical and theoretical investigation.
- **What evidence would resolve it**: Comparative studies evaluating the performance of constrained optimization methods with varying numbers of constraints on different tasks, along with theoretical analysis of the complexity and scalability of the optimization problem.

### Open Question 3
- **Question**: How can we design more effective surrogate functions for non-differentiable constraints in constrained optimization for mitigating disparate impact of pruning?
- **Basis in paper**: [explicit] The paper uses a surrogate function for the non-differentiable accuracy gaps, but acknowledges that this choice may not be optimal.
- **Why unresolved**: Finding an effective surrogate function requires a deep understanding of the relationship between the surrogate and the original constraint, as well as the optimization dynamics of the resulting problem.
- **What evidence would resolve it**: Comparative studies evaluating the performance of different surrogate functions on various tasks, along with theoretical analysis of the properties of effective surrogates.

## Limitations

- The paper doesn't test CEAG's performance on completely unseen data or tasks, leaving generalization uncertain
- Replay buffer size k introduces hyperparameter sensitivity that varies with group distribution and dataset size
- The proxy constraint mechanism assumes a monotonic relationship between accuracy drops and loss increases, which may not hold universally

## Confidence

- **High confidence**: The mechanism by which replay buffers reduce variance in constraint estimation is well-established in the optimization literature. The experimental results showing disparity reduction on training and test sets within datasets are robust across multiple seeds and sparsity levels.
- **Medium confidence**: The claim that CEAG "reliably scales to hundreds of groups" is supported by CIFAR-100 experiments, but the complexity of managing hundreds of constraints with replay buffers hasn't been tested at even larger scales.
- **Medium confidence**: The interpretation of EAGs as the appropriate metric for disparate impact is theoretically sound, but alternative formulations (e.g., constraining negative EAGs or using equalized odds) might be more appropriate for certain fairness objectives.

## Next Checks

1. Test CEAG on a held-out dataset or task not seen during training to assess generalization of the disparity mitigation.
2. Evaluate the sensitivity of disparity metrics to replay buffer size k across different group distributions and sparsity levels.
3. Compare CEAG's performance against a naive baseline that simply prunes and fine-tunes without any disparity constraints to quantify the absolute benefit.