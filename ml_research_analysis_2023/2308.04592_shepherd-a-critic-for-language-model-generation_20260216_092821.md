---
ver: rpa2
title: 'Shepherd: A Critic for Language Model Generation'
arxiv_id: '2308.04592'
source_url: https://arxiv.org/abs/2308.04592
tags:
- feedback
- answer
- output
- data
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Shepherd, a language model specifically tuned
  to critique model-generated outputs and provide feedback for improvement. Shepherd
  is trained on a high-quality feedback dataset curated from community forums and
  human annotations.
---

# Shepherd: A Critic for Language Model Generation

## Quick Facts
- arXiv ID: 2308.04592
- Source URL: https://arxiv.org/abs/2308.04592
- Reference count: 20
- Key outcome: Shepherd, a 7B parameter model, outperforms larger models like ChatGPT on critique tasks using specialized fine-tuning on high-quality feedback data.

## Executive Summary
Shepherd is a specialized language model designed to critique and provide feedback on model-generated outputs. Despite its relatively small size (7B parameters), Shepherd demonstrates superior performance compared to larger models including ChatGPT, achieving 53-87% win rates in automatic and human evaluations. The model is fine-tuned on a curated dataset combining community feedback from forums like Stack Exchange and Reddit with human-annotated feedback, enabling it to identify diverse errors and provide actionable improvement suggestions across various domains.

## Method Summary
The authors fine-tune LLaMA-7B on a high-quality feedback dataset consisting of community feedback from Stack Exchange and Reddit, combined with human-annotated feedback from 8 diverse datasets. The training objective focuses on critique generation, with the model learning to identify specific error types (factual inaccuracies, logical errors, coherence issues) and provide constructive suggestions. Evaluation uses both GPT-4 pairwise comparison and human assessment to compare Shepherd's critiques against baselines including Alpaca, SelFee, and ChatGPT.

## Key Results
- Shepherd achieves 53-87% win rates over established models including ChatGPT in both automatic and human evaluations
- Despite being only 7B parameters, Shepherd outperforms much larger models on critique tasks
- The model successfully identifies diverse error types including factual inaccuracies, logical errors, and coherence issues
- Human evaluation shows Shepherd's critiques are preferred for their specificity and actionability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shepherd outperforms larger models on critique tasks despite being smaller (7B parameters) due to its specialized fine-tuning on high-quality feedback data.
- Mechanism: The model is fine-tuned on a curated dataset combining community feedback from Stack Exchange/Reddit and human-annotated feedback, allowing it to learn specific error identification and constructive suggestion patterns.
- Core assumption: High-quality, diverse feedback data contains sufficient signal for training effective critique capabilities.
- Evidence anchors: [abstract]: "Even though Shepherd is small (7B parameters), its critiques are either equivalent or preferred to those from established models including ChatGPT." [section]: "Trained on a combination of these datasets, Shepherd shows impressive results, outperforming ChatGPT (OpenAI, 2022) models on multiple downstream tasks."

### Mechanism 2
- Claim: The pairwise comparison evaluation method using GPT-4 provides more reliable assessment than independent Likert scoring for critique quality.
- Mechanism: By asking GPT-4 to choose between two feedback examples rather than score each independently, the evaluation reduces individual scoring biases and captures relative quality differences more effectively.
- Core assumption: GPT-4's pairwise comparisons align better with human judgment than independent scoring.
- Evidence anchors: [section]: "We recommend using GPT-4 to rate in a pairwise comparison style, which aligns well with human evaluation." [section]: "When GPT-4 is asked to rate each feedback independently, it tends to give a high rating for all feedback."

### Mechanism 3
- Claim: Community feedback data provides more diverse and informative critiques than human-annotated data, but human data improves performance when added.
- Mechanism: Community data captures real-world interactions and diverse error types, while human-annotated data provides cleaner, more focused examples. The combination leverages both strengths.
- Core assumption: Diverse, real-world feedback patterns are more valuable than clean but potentially limited human annotations alone.
- Evidence anchors: [section]: "Close inspection of influences of community feedback and human annotated feedback data confirms that the community data is more informative and diverse than human-annotated data, yet leans towards informality." [section]: "Compared to SelFee which uses much more finetuning data, we demonstrate our high quality community data and human annotated data are more useful for building critique models."

## Foundational Learning

- Concept: Feedback dataset curation and quality filtering
  - Why needed here: The quality and diversity of training data directly determines the critique model's effectiveness
  - Quick check question: What criteria were used to filter community feedback data to ensure quality?

- Concept: Evaluation methodology for critique models
  - Why needed here: Proper evaluation is crucial to demonstrate the model's capabilities and compare against baselines
  - Quick check question: Why does pairwise comparison evaluation outperform independent scoring for this task?

- Concept: Error type taxonomy in natural language feedback
  - Why needed here: Understanding different error types (arithmetic, coherence, veracity, etc.) is essential for effective critique generation
  - Quick check question: What are the main error types identified in the human annotation process?

## Architecture Onboarding

- Component map: Community feedback data -> Human-annotated feedback data -> LLaMA-7B fine-tuning -> GPT-4 pairwise evaluation -> Human evaluation
- Critical path: Data collection → Quality filtering → Model fine-tuning → Evaluation → Iterative improvement
- Design tradeoffs:
  - Model size vs. performance: 7B parameters chosen for efficiency while maintaining strong performance
  - Data diversity vs. quality: Balancing community data richness with human annotation precision
  - Evaluation complexity vs. reliability: Pairwise comparison more reliable but requires more computation
- Failure signatures:
  - High scores on independent evaluation but poor pairwise performance indicates format bias
  - Consistently missing specific error types suggests training data gaps
  - Performance degradation on out-of-domain tasks indicates limited generalization
- First 3 experiments:
  1. Compare Shepherd performance on CritiqueEval dataset vs. public datasets to measure generalization
  2. Test ablation study by training models with only community data vs. only human data vs. combined
  3. Evaluate different fine-tuning strategies (e.g., full fine-tuning vs. parameter-efficient methods) on the same dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several key limitations and areas for future research are implied:

## Limitations
- Evaluation relies heavily on GPT-4 pairwise comparisons, which may introduce model-specific biases
- Human evaluation sample size (100 outputs) is relatively small for establishing robust claims
- Training data may not capture full diversity of real-world generation errors, particularly in specialized domains
- Performance advantage is context-dependent and may not generalize to all error types or domains

## Confidence
- High confidence: Shepherd's ability to identify and critique specific error types (factual errors, logical inconsistencies, coherence issues)
- Medium confidence: Community feedback data is more informative than human-annotated data, but requires more rigorous ablation studies
- Medium confidence: Pairwise comparison methodology's superiority over independent scoring, though sensitive to prompt engineering
- Low confidence: Generalization claims to out-of-domain tasks are not thoroughly validated

## Next Checks
1. Conduct ablation study on training data composition by training separate models using only community data, only human-annotated data, and various combinations to quantify marginal contributions.
2. Test Shepherd on critique tasks in domains not represented in training data (medical advice, legal analysis, technical documentation) to assess true generalization capabilities.
3. Conduct comprehensive human evaluation study with at least 500 outputs across diverse tasks, using multiple annotators per example to establish statistical significance and identify potential blind spots.