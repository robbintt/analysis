---
ver: rpa2
title: Surpassing GPT-4 Medical Coding with a Two-Stage Approach
arxiv_id: '2311.13735'
source_url: https://arxiv.org/abs/2311.13735
tags:
- code
- llm-codex
- coding
- codes
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-codex introduces a two-stage approach to improve medical coding
  accuracy and explainability. It first segments long EHRs and uses an LLM to generate
  ICD code proposals, then applies an LSTM-based verifier that learns from both the
  LLM's high-recall proposals and human expert's high-precision labels.
---

# Surpassing GPT-4 Medical Coding with a Two-Stage Approach

## Quick Facts
- arXiv ID: 2311.13735
- Source URL: https://arxiv.org/abs/2311.13735
- Reference count: 40
- Surpasses GPT-4 medical coding accuracy with 10% higher F1 on rare codes and 5% higher F1 on limited training data

## Executive Summary
LLM-codex introduces a two-stage approach that significantly improves medical coding accuracy and explainability. The method first segments long electronic health records (EHRs) and uses an LLM to generate ICD code proposals, then applies an LSTM-based verifier that learns from both the LLM's high-recall proposals and human expert's high-precision labels. This dual-supervision design addresses the challenge of LLMs over-predicting codes while maintaining superior performance on rare codes. The approach achieves state-of-the-art results on MIMIC datasets without requiring human-annotated evidence for training.

## Method Summary
LLM-codex is a two-stage approach for automated ICD code assignment to EHRs. First, long documents are segmented into equal-sized sentence chunks and processed by GPT-4 to generate ICD code proposals at the document level and extract sentence-level evidence. Second, an LSTM verifier model is trained using a custom loss function that leverages dual supervision: the LLM's high-recall silver labels and human expert's high-precision gold labels. This design allows the model to achieve high accuracy on both common and rare ICD codes while providing explainable evidence for coding decisions without requiring human-annotated evidence during training.

## Key Results
- Achieves over 10% higher F1 score on rare ICD codes compared to state-of-the-art methods
- Shows 5% higher F1 score performance on limited training data scenarios
- Provides more precise sentence-level evidence for coding decisions without requiring human-annotated evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-codex improves rare code prediction by leveraging LLM-generated silver labels for supervision
- Mechanism: The LSTM verifier is trained on both high-recall LLM predictions and high-precision expert labels, creating a dual-supervision signal that boosts rare code recall without sacrificing precision
- Core assumption: LLM predictions contain sufficient signal about rare codes even when noisy
- Evidence anchors:
  - [abstract]: "The LSTM learns from both the LLM's high recall and human expert's high precision, using a custom loss function."
  - [section 4.4]: "Our Verifier model is an LSTM trained with a custom loss function leveraging dual labels: the LLM-assigned ICD code as a sentence-level, silver-label (high recall), and the expert-assigned ICD code as the document-level, gold-label (high precision)."
  - [corpus]: Weak - corpus shows similar two-stage approaches but lacks direct comparison of rare code performance
- Break condition: If LLM predictions for rare codes are too sparse or incorrect, the silver label signal degrades

### Mechanism 2
- Claim: Segmenting long EHRs improves LLM recall by mitigating context truncation issues
- Mechanism: By splitting long documents into smaller segments, each segment fits within LLM context limits, allowing the model to process all relevant information without truncation
- Core assumption: GPT-4 struggles with information extraction from the middle of long contexts
- Evidence anchors:
  - [section 4.2]: "To effectively manage long documents, we first split it into multiple segments containing an equal number of sentences and passed each segment individually to the LLM."
  - [section 5.4]: "This finding aligns with literature reports that LLMs face challenges in extracting information from the middle of long contexts (Liu et al., 2023)."
  - [corpus]: Weak - corpus mentions segmentation but lacks ablation studies comparing different segment counts
- Break condition: If segmentation disrupts contextual relationships across sentences critical for code prediction

### Mechanism 3
- Claim: Sentence-level evidence extraction without human annotation is feasible using LLM verification
- Mechanism: The verifier model uses the LLM's sentence-level evidence predictions as silver labels, learning to identify accurate evidence without requiring human-annotated evidence data
- Core assumption: LLM-generated sentence-level evidence correlates sufficiently with ground truth for the verifier to learn from
- Evidence anchors:
  - [abstract]: "Our model is the only approach that simultaneously achieves state-of-the-art results in medical coding accuracy, accuracy on rare codes, and sentence-level evidence identification to support coding decisions without training on human-annotated evidence."
  - [section 4.4]: "The Verifier model assessed the accuracy of a silver label (which consists of an ICD code and its corresponding evidence sentence index) predicted by the LLM."
  - [corpus]: Weak - corpus mentions evidence extraction but lacks ablation studies on evidence accuracy with/without silver labels
- Break condition: If LLM evidence predictions are too noisy, the verifier cannot learn meaningful patterns

## Foundational Learning

- Concept: Multi-label classification with label imbalance
  - Why needed here: ICD coding involves thousands of possible codes with extreme class imbalance (rare vs common codes)
  - Quick check question: How does the model handle scenarios where most codes have very few training examples?

- Concept: Dual-supervision learning
  - Why needed here: The verifier needs to learn from both high-recall (LLM) and high-precision (expert) signals simultaneously
  - Quick check question: What happens to verifier performance if we remove the silver label component from training?

- Concept: Context window limitations in LLMs
  - Why needed here: Long EHR documents must be processed in segments to fit within GPT-4's context limits
  - Quick check question: How does the number of segments affect overall prediction performance?

## Architecture Onboarding

- Component map:
  EHR Document -> Segmentation -> GPT-4 (Code Prediction) -> Aggregation -> GPT-4 (Evidence Extraction) -> LSTM Verifier -> Verified ICD Codes with Evidence

- Critical path:
  1. Segment EHR into equal-sized sentence chunks
  2. Run LLM on each segment for document-level codes
  3. Aggregate document-level predictions
  4. Run LLM on segments for sentence-level evidence
  5. Train LSTM verifier on dual supervision
  6. Generate final verified predictions

- Design tradeoffs:
  - Segment size vs. context preservation: Smaller segments fit context limits but may lose document-level context
  - Single vs. multiple evidence sentences: Current design outputs one evidence sentence per code for simplicity
  - Cost vs. performance: GPT-4 inference is expensive but provides superior performance

- Failure signatures:
  - Low precision: Verifier is overconfident in LLM predictions
  - Low recall: LLM fails to identify relevant codes in segments
  - Evidence mismatch: Verifier selects wrong sentence as evidence
  - Long sentence issues: GPT-4 misses codes in lengthy sentences with multiple conditions

- First 3 experiments:
  1. Ablation study: Remove silver labels and measure impact on rare code performance
  2. Segmentation study: Test different numbers of segments to find optimal balance
  3. Evidence study: Compare comprehensive evidence extraction (multiple sentences) vs. single sentence approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cost of using GPT-4 for medical coding compare to other methods, and what are the potential ways to reduce this cost?
- Basis in paper: [explicit] The paper mentions that running LLM-Codex on the MIMIC-III dataset costs approximately $0.50 per discharge summary and has a latency of about 10 seconds per document
- Why unresolved: The paper does not provide a detailed comparison of costs with other methods or discuss specific strategies for reducing costs
- What evidence would resolve it: A comprehensive cost analysis comparing LLM-Codex to other medical coding methods, along with proposed cost-reduction strategies, would resolve this question

### Open Question 2
- Question: How does the performance of LLM-Codex vary across different ICD codes, and which components of the model are most critical for its performance?
- Basis in paper: [inferred] The paper mentions that LLM-Codex's performance varies across individual ICD codes and that both stages of the model contribute to its predictive performance, but it does not provide a detailed analysis of performance across specific ICD codes
- Why unresolved: The paper does not provide a comprehensive analysis of LLM-Codex's performance across different ICD codes or identify the most critical components for its performance
- What evidence would resolve it: A detailed analysis of LLM-Codex's performance across various ICD codes, along with an ablation study to identify the most critical components, would resolve this question

### Open Question 3
- Question: How does the accuracy of LLM-Codex change when extracting comprehensive evidence for predicted ICD codes, and what modifications could improve its recall?
- Basis in paper: [explicit] The paper mentions that LLM-Codex's current constraint of generating only one sentence-level evidence per predicted ICD code may limit its recall when extracting comprehensive evidence
- Why unresolved: The paper does not explore the impact of increasing the number of evidence sentences returned by the Verifier model on LLM-Codex's accuracy and recall
- What evidence would resolve it: Experiments evaluating the impact of increasing the number of evidence sentences on LLM-Codex's accuracy and recall would resolve this question

### Open Question 4
- Question: How does the performance of LLM-Codex compare to other large language models, such as Llama2, and what are the potential benefits of distilling GPT-4's performance into these models?
- Basis in paper: [explicit] The paper mentions that future work could distill GPT-4 ICD coding performance into other large language models such as Llama2 to reduce cost and latency
- Why unresolved: The paper does not provide a comparison of LLM-Codex's performance with other large language models or discuss the potential benefits of distillation
- What evidence would resolve it: A comparative analysis of LLM-Codex's performance with other large language models and an evaluation of the benefits of distillation would resolve this question

## Limitations
- Computational cost barrier due to multiple GPT-4 API calls per document
- Dataset specificity limited to American clinical records (MIMIC datasets)
- Potential context length trade-offs when segmentation disrupts cross-segment relationships

## Confidence
- High Confidence: Document-level ICD code prediction performance improvements (5-10% F1 gains)
- Medium Confidence: Sentence-level evidence extraction quality
- Low Confidence: Rare code performance claims

## Next Checks
1. **Ablation Study on Silver Labels**: Remove the LLM-generated silver labels from verifier training and measure the impact on rare code performance to validate the dual-supervision mechanism's effectiveness
2. **Cross-Dataset Generalization Test**: Apply the trained model to a non-MIMIC clinical dataset (different hospital system or country) to assess real-world generalizability beyond the training distribution
3. **Context Dependency Analysis**: Conduct controlled experiments where ICD codes requiring cross-segment information are artificially created to test whether segmentation actually impairs prediction of context-dependent codes