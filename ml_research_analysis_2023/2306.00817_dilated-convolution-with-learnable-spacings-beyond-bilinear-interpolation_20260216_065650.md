---
ver: rpa2
title: 'Dilated Convolution with Learnable Spacings: beyond bilinear interpolation'
arxiv_id: '2306.00817'
source_url: https://arxiv.org/abs/2306.00817
tags:
- kernel
- interpolation
- self
- gaussian
- dilated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dilated Convolution with Learnable Spacings (DCLS) replaces fixed
  grid spacing in dilated convolutions with learnable spacings, addressing the limitation
  of standard dilated convolutions' rigid structure. This is achieved by making kernel
  element positions learnable and handling non-integer positions through interpolation,
  enabling well-defined gradients.
---

# Dilated Convolution with Learnable Spacings: beyond bilinear interpolation

## Quick Facts
- arXiv ID: 2306.00817
- Source URL: https://arxiv.org/abs/2306.00817
- Reference count: 40
- Primary result: Gaussian interpolation in DCLS achieves 82.60% top-1 accuracy on ImageNet1k with ConvNeXt-T, outperforming bilinear interpolation

## Executive Summary
This paper extends Dilated Convolution with Learnable Spacings (DCLS) by replacing the original bilinear interpolation with Gaussian interpolation using learnable standard deviations. The method enables kernel elements to influence a wider spatial extent while maintaining differentiability for gradient-based learning. Experiments on ImageNet1k with ConvNeXt-T and ConvFormer-S18 architectures demonstrate that Gaussian interpolation consistently outperforms bilinear interpolation, achieving higher classification accuracy without increasing parameter count.

## Method Summary
The method replaces standard depthwise separable convolutions with DCLS using Gaussian interpolation. Kernel element positions become learnable parameters that can take non-integer values, requiring interpolation to compute their contributions to output pixels. Gaussian interpolation with learnable standard deviations allows each kernel element to influence a smooth, continuous weight distribution over a wider spatial extent compared to bilinear interpolation's 4 nearest neighbors. The framework maintains the original DCLS's parameter efficiency while providing adaptive spatial resolution control through learned kernel widths.

## Key Results
- ConvNeXt-T with DCLS-Gauss achieves 82.60% top-1 accuracy on ImageNet1k
- Gaussian interpolation outperforms bilinear interpolation across tested architectures
- 17x17 Gaussian kernel with 9 kernel elements provides optimal trade-off between accuracy and throughput
- Method maintains parameter efficiency while improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gaussian interpolation allows kernel elements to influence more distant pixels than bilinear interpolation, enabling richer spatial sampling.
- Mechanism: The Gaussian kernel with learnable standard deviation σ defines a smooth, continuous weight distribution over a wider spatial extent. This lets each kernel element contribute to multiple output positions, with influence that decays smoothly rather than abruptly cutting off at 4 nearest neighbors.
- Core assumption: Smooth interpolation with longer range is beneficial for learning optimal receptive fields.
- Evidence anchors:
  - [abstract] "longer range interpolations, and in particular a Gaussian interpolation, allow improving performance"
  - [section] "Gaussian interpolation performs significantly better" and "a 17x17 Gaussian kernel...still outperforms the bilinear case"
  - [corpus] Weak evidence; no corpus papers discuss interpolation range effects directly.
- Break condition: If longer-range interpolation causes oversmoothing or introduces noise that harms learning, performance would degrade.

### Mechanism 2
- Claim: Learnable standard deviations provide adaptive spatial resolution control per kernel element.
- Mechanism: Each kernel element's Gaussian width is optimized via gradient descent. Wider Gaussians cover more context but blur details; narrower ones capture fine structure. The network learns optimal widths for each position.
- Core assumption: Different spatial locations benefit from different effective receptive field sizes.
- Evidence anchors:
  - [abstract] "Gaussian interpolation with learnable standard deviations" and "without increasing the number of parameters"
  - [section] "learning the standard deviations costs two additional learnable parameters and two additional FLOPs"
  - [corpus] No direct corpus support; this is a novel mechanism in DCLS.
- Break condition: If the network fails to converge on useful σ values, or if optimization becomes unstable due to σ initialization or scaling.

### Mechanism 3
- Claim: Normalization across the dilated kernel preserves gradient flow and prevents dominance by single elements.
- Mechanism: After computing raw interpolation weights, they are divided by their sum (plus epsilon) to ensure the total contribution equals 1. This maintains stable gradients and prevents numerical instability.
- Core assumption: Normalized weights prevent gradient explosion/vanishing in deep networks with many kernel elements.
- Evidence anchors:
  - [section] "To make the sum of the interpolation over the dilated kernel size equal to 1, we divide the interpolations by the following normalization term"
  - [section] "we divide the interpolations by the following normalization term" with explicit formula
  - [corpus] No corpus evidence; this is standard practice in interpolation literature.
- Break condition: If normalization removes too much signal variation, or if epsilon is too large/small affecting precision.

## Foundational Learning

- Concept: Bilinear interpolation as piecewise linear kernel density estimation
  - Why needed here: DCLS builds directly on bilinear interpolation as baseline; understanding its limitations motivates the Gaussian extension
  - Quick check question: What is the mathematical form of the triangle (Λ) function used in bilinear interpolation, and why does it only consider 4 nearest neighbors?

- Concept: Continuous convolution and kernel density estimation
  - Why needed here: DCLS constructs kernels with non-integer positions, requiring interpolation theory to maintain differentiability
  - Quick check question: How does replacing discrete kernel positions with continuous ones affect the convolution operation mathematically?

- Concept: Gradient flow through interpolation operations
  - Why needed here: The key innovation is making kernel positions learnable via backpropagation through interpolation
  - Quick check question: What property must an interpolation function have to allow gradients to flow back to kernel positions?

## Architecture Onboarding

- Component map:
  - Kernel constructor (ConstructKernel2d): generates 2D kernels from learnable weights, positions, and σ values
  - Forward pass: integrates with PyTorch conv layers, replacing depthwise separable convs
  - Interpolation engine: implements Λ and Gaussian interpolation functions
  - Parameter management: handles shared positions across layers, learning rate scaling

- Critical path:
  1. Initialize positions randomly (std=0.5), σ=0.23 (Gauss) or 0 (Λ)
  2. For each forward pass, construct kernel via interpolation
  3. Apply kernel in convolution operation
  4. Backpropagate through interpolation to update positions and σ

- Design tradeoffs:
  - Wider Gaussians increase receptive field but reduce spatial precision
  - Larger dilated kernel size improves accuracy but reduces throughput
  - Shared positions reduce parameters but may limit layer specialization

- Failure signatures:
  - NaN/Inf gradients: check σ initialization and normalization
  - Poor convergence: verify learning rate scaling and weight decay settings
  - Overfitting: monitor kernel count vs baseline parameter count

- First 3 experiments:
  1. Replace one depthwise conv in ConvNeXt with DCLS-Gauss (kernel size 17, count 9) and compare training loss
  2. Vary σ initialization (0.1, 0.23, 0.5) to test sensitivity
  3. Compare Λ vs Gaussian interpolation on same architecture and dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DCLS with Gaussian interpolation compare to other state-of-the-art interpolation methods, such as sinc or Lorentz functions?
- Basis in paper: [inferred] The paper mentions that other interpolation functions like Lorentz, hyper-Gaussians, and sinc functions were tested but did not show significant improvement over the Gaussian interpolation.
- Why unresolved: The paper does not provide detailed results or comparisons between these interpolation methods and Gaussian interpolation, making it difficult to determine the relative performance of each method.
- What evidence would resolve it: Conducting experiments with various interpolation functions and comparing their performance on ImageNet1k classification tasks using the same architectures (ConvNeXt and ConvFormer) would provide a clearer understanding of their relative effectiveness.

### Open Question 2
- Question: What is the impact of learning a correlation parameter ρ or rotation parameter θ in the bivariate normal distribution density on the performance of DCLS?
- Basis in paper: [explicit] The paper mentions that learning a correlation parameter ρ or a rotation parameter θ did not improve performance, possibly because cardinal orientations predominate in natural images.
- Why unresolved: The paper does not provide a detailed analysis of the reasons behind the lack of improvement when learning these parameters, nor does it explore alternative approaches to address this issue.
- What evidence would resolve it: Further experimentation with different correlation and rotation parameters, as well as exploring alternative methods to incorporate orientation information into the DCLS framework, could help determine if there are specific scenarios where these parameters could be beneficial.

### Open Question 3
- Question: What is the optimal dilated kernel size for DCLS with Gaussian interpolation, and how does it affect the trade-off between accuracy and throughput?
- Basis in paper: [explicit] The paper mentions that for ConvNeXt-dcls with bilinear interpolation, a dilated kernel size of 17 was found to be optimal. However, with Gaussian and Λ interpolations, accuracy tends to increase logarithmically as the size grows, with improvements observed up to kernel sizes of 51.
- Why unresolved: The paper does not provide a comprehensive analysis of the relationship between dilated kernel size, accuracy, and throughput for DCLS with Gaussian interpolation, nor does it identify a specific optimal kernel size.
- What evidence would resolve it: Conducting a systematic study of the performance of DCLS with Gaussian interpolation across a range of dilated kernel sizes, while measuring both accuracy and throughput, would help determine the optimal kernel size and the trade-offs involved.

## Limitations

- Limited ablation studies on the individual contributions of learnable standard deviations versus interpolation function choice
- Empirical learning rate scaling factor without theoretical justification
- Incomplete analysis of trade-offs between accuracy gains and computational throughput

## Confidence

- **High Confidence**: The mathematical framework for differentiable kernel construction is sound, and the normalization approach for gradient stability is well-established in interpolation literature.
- **Medium Confidence**: The empirical performance improvements on ImageNet1k are reproducible based on the provided implementation, but the robustness across different architectures and tasks remains uncertain.
- **Low Confidence**: The claimed superiority of Gaussian interpolation over bilinear is demonstrated on limited model variants without comprehensive hyperparameter optimization or comparison to alternative continuous kernel approaches.

## Next Checks

1. **Ablation Study**: Run controlled experiments isolating the effects of learnable standard deviations versus fixed Gaussian interpolation to quantify their individual contributions to performance gains.
2. **Cross-Architecture Generalization**: Test DCLS-Gauss on additional backbone architectures (e.g., EfficientNet, ResNets) and tasks (detection, segmentation) to evaluate generalizability beyond ImageNet classification.
3. **Optimization Sensitivity**: Systematically vary the learning rate scaling factor and initialization schemes for kernel positions and standard deviations to determine robustness to hyperparameter choices.