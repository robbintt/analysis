---
ver: rpa2
title: Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents
  in Open-Domain Question Answering
arxiv_id: '2310.17490'
source_url: https://arxiv.org/abs/2310.17490
tags:
- answer
- documents
- reader
- zero-shot
- irrelevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of distraction from irrelevant
  documents when using LLMs as zero-shot readers in ODQA tasks. The authors propose
  a method called Distraction-aware Answer Selection (DAS) to mitigate this issue.
---

# Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2310.17490
- Source URL: https://arxiv.org/abs/2310.17490
- Reference count: 30
- Key outcome: Zero-shot reader with DAS improves performance by reducing distraction from irrelevant documents in ODQA tasks.

## Executive Summary
This paper addresses the challenge of distraction from irrelevant documents when using large language models (LLMs) as zero-shot readers in open-domain question answering (ODQA) tasks. The authors propose a method called Distraction-aware Answer Selection (DAS) that mitigates this issue by incorporating an unanswerable instruction and adjusting answer scores based on query-document relevance. DAS enables the model to abstain from answering irrelevant documents and improves answer selection by reflecting query generation scores. Experiments on multiple ODQA benchmarks demonstrate significant performance improvements, with the zero-shot reader with DAS showing superior transferability compared to supervised readers.

## Method Summary
The paper proposes Distraction-aware Answer Selection (DAS) to mitigate the negative impact of irrelevant documents on zero-shot ODQA readers. DAS consists of two main components: document selection and answer selection. In the document selection step, the model is given an "unanswerable" instruction, allowing it to abstain from answering when the context does not contain sufficient information. In the answer selection step, the answer scores are adjusted by multiplying them with the query generation scores, which reflect the relevance between the query and the document. The final answer is selected based on the adjusted scores. The method is evaluated on four ODQA datasets (NQ, TQA, WebQ, SQuAD) using BM25 and DPR retrievers and two LLMs (FLAN-T5-XL, OPT-IML-MAX).

## Key Results
- DAS significantly improves the performance of zero-shot readers on ODQA benchmarks by reducing the negative impact of irrelevant documents.
- The zero-shot reader with DAS demonstrates superior transferability compared to supervised readers, highlighting the potential of LLMs as zero-shot readers.
- DAS effectively mitigates hallucination in LLMs when used as readers in ODQA tasks by filtering out irrelevant documents and adjusting overconfident answer scores.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unanswerable instruction reduces distraction by enabling the model to abstain from generating responses to irrelevant documents.
- Mechanism: By providing an "unanswerable" instruction, the LLM is given the option to refuse generating answers when the context does not contain sufficient information, effectively filtering out irrelevant documents.
- Core assumption: The LLM can accurately identify when a document lacks the necessary information to answer the question and will reliably abstain from answering in such cases.
- Evidence anchors:
  - [abstract]: "We provide models with an 'unanswerable' instruction, allowing them to abstain from answering irrelevant documents..."
  - [section]: "We utilize the unanswerable instruction to enhance the deduction capability by giving the option not to respond. We exclude responses that belong to the unanswerable response set..."
- Break condition: The model becomes overconfident and generates answers even when the context is insufficient, or the unanswerable instruction is misinterpreted.

### Mechanism 2
- Claim: Score adjustment using query generation scores improves answer selection by incorporating query-document relevance.
- Mechanism: The answer score is adjusted by multiplying it with the query generation score, which reflects the relevance between the query and the document. This helps in prioritizing answers from more relevant documents.
- Core assumption: The query generation score is a reliable indicator of query-document relevance and can be effectively used to adjust answer scores.
- Evidence anchors:
  - [abstract]: "...we adjust the answer scores by reflecting the query generation score as the relevance between the given query-document pairs."
  - [section]: "We adjust the answer score by multiplying the query generation score in consideration for the query-document relevance."
- Break condition: The query generation score does not correlate well with actual relevance, or the multiplication of scores leads to numerical instability.

### Mechanism 3
- Claim: The combination of document selection and answer selection steps effectively mitigates hallucination and improves overall performance.
- Mechanism: The two-step process first filters out irrelevant documents using the unanswerable instruction, then adjusts answer scores based on query-document relevance, resulting in a more robust answer selection process.
- Core assumption: The sequential application of document selection and answer selection leads to a cumulative improvement in answer quality.
- Evidence anchors:
  - [abstract]: "To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection."
  - [section]: "We present simple yet effective Distraction-aware Answer Selection (DAS) for a zero-shot reader. We aim to reduce the negative impact of irrelevant documents in a two-step answering pipeline."
- Break condition: The two-step process introduces significant computational overhead or the benefits do not outweigh the costs.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study aims to leverage LLMs as zero-shot readers without any parameter updates, relying on their inherent capabilities.
  - Quick check question: What is the main advantage of using zero-shot learning in this context compared to supervised learning?

- Concept: Open-domain question answering (ODQA)
  - Why needed here: The task involves answering questions using evidence documents fetched from a large corpus, which is the core problem being addressed.
  - Quick check question: How does the ODQA framework differ from closed-domain QA?

- Concept: Hallucination in LLMs
  - Why needed here: The study addresses the issue of LLMs generating incorrect but plausible answers when presented with irrelevant documents, which is a form of hallucination.
  - Quick check question: What are the potential causes of hallucination in LLMs when used as readers in ODQA tasks?

## Architecture Onboarding

- Component map: Retriever -> Reader (LLM) -> DAS (Document Selection + Answer Selection) -> Evaluation (EM accuracy)
- Critical path:
  1. Query is passed to the retriever to fetch relevant documents
  2. Retrieved documents are passed to the reader along with the query and instructions
  3. Reader generates answer candidates and their corresponding scores
  4. DAS is applied to filter out irrelevant documents and adjust answer scores
  5. Final answer is selected based on the adjusted scores
- Design tradeoffs:
  - Using larger LLMs may improve performance but increases computational cost
  - More complex retriever models may improve document relevance but also increase latency
  - Stricter unanswerable instruction may reduce false positives but also increase false negatives
- Failure signatures:
  - Significant drop in performance when the number of retrieved documents increases
  - Inconsistent results across different LLMs or retriever models
  - High sensitivity to changes in instructions or templates
- First 3 experiments:
  1. Evaluate the performance of the baseline reader (without DAS) on a small subset of the data to establish a reference point.
  2. Implement the document selection step of DAS and evaluate its impact on filtering out irrelevant documents.
  3. Combine both document selection and answer selection steps of DAS and evaluate the overall improvement in performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DAS vary across different sizes of retrieved document sets (e.g., top-5, top-50, top-100)?
- Basis in paper: [explicit] The paper discusses the impact of the number of retrieved documents on performance, noting improvements with DAS as the document set size increases.
- Why unresolved: While the paper provides results for top-20 and top-100 documents, it lacks a comprehensive analysis across a wider range of document set sizes.
- What evidence would resolve it: Conducting experiments with varying numbers of retrieved documents (e.g., top-5, top-20, top-50, top-100) and comparing the performance of DAS across these settings would provide a clearer understanding of its effectiveness at different scales.

### Open Question 2
- Question: How does the effectiveness of DAS change when using larger language models (e.g., 10B+ parameters)?
- Basis in paper: [explicit] The paper mentions that the effectiveness of DAS is dependent on the capabilities and characteristics of the LLM, and suggests that further investigation is required with larger LLMs.
- Why unresolved: The experiments were conducted with relatively small LLMs (3B and 1.3B parameters), leaving the performance of DAS with larger models unexplored.
- What evidence would resolve it: Running experiments with larger LLMs (e.g., 10B+ parameters) and comparing their performance with and without DAS would provide insights into the scalability of the approach.

### Open Question 3
- Question: How does the performance of DAS compare to other hallucination mitigation techniques in ODQA tasks?
- Basis in paper: [inferred] The paper focuses on the effectiveness of DAS in mitigating hallucination caused by irrelevant documents, but does not directly compare it to other hallucination mitigation methods.
- Why unresolved: While DAS shows promising results, its performance relative to other hallucination mitigation techniques remains unclear.
- What evidence would resolve it: Conducting a comparative study between DAS and other hallucination mitigation techniques (e.g., re-ranking, answer verification) in ODQA tasks would provide a more comprehensive understanding of its effectiveness.

## Limitations

- The query generation score computation method is only briefly described, which could impact reproducibility.
- Performance gains are measured primarily against EM accuracy without ablation studies on individual DAS components.
- The zero-shot transfer advantage is claimed but not rigorously tested across domain shifts.

## Confidence

- **High**: The basic mechanism of using unanswerable instructions to filter irrelevant documents is well-established and clearly demonstrated
- **Medium**: The score adjustment approach using query generation scores is theoretically sound but depends on implementation details not fully specified
- **Medium**: The claim of superior zero-shot transferability is supported by results but lacks rigorous cross-domain validation

## Next Checks

1. Replicate the query generation score computation method independently to verify the claimed improvements are reproducible.
2. Perform ablation studies removing either the document selection or answer selection components to quantify their individual contributions.
3. Test the method on out-of-domain datasets to empirically validate the zero-shot transfer claims.