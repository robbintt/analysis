---
ver: rpa2
title: 'Self Attention with Temporal Prior: Can We Learn More from Arrow of Time?'
arxiv_id: '2310.18932'
source_url: https://arxiv.org/abs/2310.18932
tags:
- data
- time
- attention
- kernels
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Self-Attention with Temporal Prior (SAT-Transformer),
  a method to enhance Transformer models for time series data by incorporating learnable
  kernels that capture short-term temporal dependencies. The authors hypothesize that
  closer time stamps in time series data have higher interrelation, and they design
  exponential and periodic kernels to encode this bias into attention matrices.
---

# Self Attention with Temporal Prior: Can We Learn More from Arrow of Time?

## Quick Facts
- arXiv ID: 2310.18932
- Source URL: https://arxiv.org/abs/2310.18932
- Reference count: 12
- Key outcome: SAT-Transformer achieves up to 10% performance gains on EHR classification tasks by encoding temporal bias into attention matrices.

## Executive Summary
SAT-Transformer introduces learnable exponential and periodic kernels to standard Transformer attention, encoding the assumption that closer time stamps in time series have higher interrelation. The method is validated on three EHR datasets (PhysioNet, MIMIC-III, eICU) for tasks including sepsis prediction and mortality prediction, showing consistent improvements over vanilla Transformers and GRU-D models. The approach requires minimal additional computation through vectorized kernel application and demonstrates particular effectiveness when training data is limited.

## Method Summary
SAT-Transformer modifies standard Transformer attention by applying learnable temporal kernels to the query and key matrices before computing attention scores. The kernels—exponential (capturing short-term dependencies) and periodic (capturing repeating patterns)—are constructed as matrices dependent only on time stamps, enabling efficient vectorized multiplication. The method assumes that time series data has inherent temporal structure where closer time stamps are more interrelated, and learns kernel parameters during training to amplify these relationships in the attention mechanism.

## Key Results
- Maximum performance gain of 10% AUPRC over vanilla Transformer on EHR classification tasks
- Consistent improvements across PhysioNet Challenge 2019, MIMIC-III, and eICU datasets
- Particularly strong performance with limited training data, suggesting effective inductive bias
- Minimal additional computational overhead compared to standard Transformers

## Why This Works (Mechanism)

### Mechanism 1
Temporal kernels encode inductive bias for short-term dependencies, improving performance especially with limited data. Learnable exponential and periodic kernels are applied element-wise to attention matrices, amplifying interactions between closer time stamps. Core assumption: closer time stamps in time series inherently have higher interrelation due to the asymmetric, linear flow of time.

### Mechanism 2
Vectorized kernel application improves computational efficiency compared to naive element-wise multiplication. Kernels are constructed as matrices C(e) and C(p) and applied via matrix multiplication, enabling parallelism. Core assumption: kernels are fixed for a given sequence length and depend only on time stamps, not input values.

### Mechanism 3
Temporal kernels introduce diversity in multi-head attention, mitigating the "collapse" issue observed in vanilla Transformers. Different heads learn different kernel shapes (exponential, periodic, or near-identity), leading to diverse attention matrices. Core assumption: attention matrices in multi-head Transformers tend to become similar across heads, limiting representational capacity.

## Foundational Learning

- Concept: Self-attention mechanism in Transformers
  - Why needed here: SAT-Transformer builds on standard Transformer attention; understanding its mechanics is essential.
  - Quick check question: In standard scaled dot-product attention, what is the formula for computing attention weights between query Q and key K?

- Concept: Exponential and periodic covariance functions
  - Why needed here: These functions form the basis of the learnable kernels applied to attention matrices.
  - Quick check question: Write the mathematical form of an exponential covariance function Ce(Xt1, Xt2) = σ2e−(αh)β and identify what h represents.

- Concept: EHR data characteristics and missing value patterns
  - Why needed here: SAT-Transformer was validated on EHR datasets with specific temporal patterns and missingness.
  - Quick check question: In the MIMIC-III dataset, what is the task and how many clinical variables are used per ICU stay?

## Architecture Onboarding

- Component map: Input embeddings -> Kernelized Q,K computation -> Scaled dot-product attention -> Feed-forward network -> Output
- Critical path: Compute Q, K, V from input embeddings → Apply temporal kernels to Q and K separately → Compute scaled dot-product attention with kernelized Q and K → Apply residual connections and feed-forward networks
- Design tradeoffs: Kernel complexity vs. overfitting risk (more parameters may require more data) → Kernel type selection (exponential vs. periodic) based on domain knowledge → Sequence length constraints for precomputing kernel matrices
- Failure signatures: Performance drops to vanilla Transformer levels when kernel parameters converge to identity → Increased training instability with overly large kernel parameter initialization → Memory issues with very long sequences due to kernel matrix size
- First 3 experiments: 1) Train SAT-Transformer with only exponential kernels on PhysioNet sepsis prediction; compare AUPRC to vanilla Transformer → 2) Test variable-length sequence handling by training on eICU with kernel recomputation per batch → 3) Perform ablation study removing periodic kernels to quantify their contribution on periodic medical data

## Open Questions the Paper Calls Out

- Question: What is the impact of SAT-Transformer's performance on extremely large time series datasets compared to vanilla Transformer?
- Basis in paper: The paper suggests that with enough data, the underlying regularities in time series data can eventually be learned by models without inductive bias, such as the vanilla Transformer.
- Why unresolved: The paper does not provide experimental results on extremely large time series datasets to compare the performance of SAT-Transformer and vanilla Transformer.
- What evidence would resolve it: Experimental results on extremely large time series datasets comparing the performance of SAT-Transformer and vanilla Transformer would provide evidence to resolve this question.

## Limitations
- Temporal regularity assumption may not generalize beyond EHR data to domains with irregular temporal patterns
- No theoretical analysis of why exponential and periodic kernels specifically work better than other kernel functions
- Efficiency claims depend on fixed sequence lengths and may not scale well to very long or variable-length sequences

## Confidence

- High Confidence: The core mechanism of applying learnable temporal kernels to attention matrices is well-specified and technically sound. The experimental methodology for EHR tasks is appropriate and reproducible.
- Medium Confidence: The performance improvements over baselines are substantial but may be partially attributed to dataset-specific characteristics rather than the kernel mechanism alone. The efficiency claims are reasonable but depend on implementation details not fully specified.
- Low Confidence: The theoretical justification for why temporal kernels should work is weak, and the attention diversity mechanism lacks rigorous validation.

## Next Checks

1. Cross-domain validation: Test SAT-Transformer on non-medical time series datasets (e.g., traffic data, financial time series, sensor data) to verify if the temporal kernel mechanism generalizes beyond EHR data where the underlying assumptions may not hold.

2. Kernel ablation analysis: Systematically disable either exponential or periodic kernels across all experiments to quantify their individual contributions and determine if one type consistently outperforms the other across different tasks and data characteristics.

3. Attention matrix visualization: Conduct a detailed analysis of attention matrix diversity across heads by visualizing and quantifying the similarity between attention matrices in SAT-Transformer versus vanilla Transformer, measuring whether the claimed diversity improvement translates to measurable performance gains.