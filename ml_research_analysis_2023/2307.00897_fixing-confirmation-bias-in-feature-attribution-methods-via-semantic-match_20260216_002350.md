---
ver: rpa2
title: Fixing confirmation bias in feature attribution methods via semantic match
arxiv_id: '2307.00897'
source_url: https://arxiv.org/abs/2307.00897
tags:
- explanations
- semantic
- https
- match
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses confirmation bias in feature attribution methods
  by proposing a structured approach to test semantic match between human concepts
  and model explanations. The core method involves formulating hypotheses about model
  behavior and using metrics like AUC and median distance to measure the overlap between
  explanations and these hypotheses.
---

# Fixing confirmation bias in feature attribution methods via semantic match

## Quick Facts
- arXiv ID: 2307.00897
- Source URL: https://arxiv.org/abs/2307.00897
- Reference count: 31
- Primary result: Proposed framework tests semantic match between human concepts and model explanations to prevent confirmation bias in XAI

## Executive Summary
This paper addresses confirmation bias in feature attribution methods by proposing a structured approach to test semantic match between human concepts and model explanations. The core method involves formulating hypotheses about model behavior and using metrics like AUC and median distance to measure the overlap between explanations and these hypotheses. Experiments on synthetic tabular data and image data from MALeViC show that the proposed approach can reveal both desirable and undesirable model behaviors, helping to prevent confirmation bias by grounding intuitions with quantitative analysis.

## Method Summary
The method formulates hypotheses about model behavior, generates SHAP explanations for model predictions, and computes semantic match metrics (AUC and median distance) to quantify the overlap between explanation patterns and hypothesis-defined data subsets. The approach involves defining distance metrics between explanations, selecting data points based on hypothesis satisfaction, and evaluating how well explanations align with expected behavior patterns.

## Key Results
- Semantic match framework successfully identifies both desired and undesired model behaviors in controlled experiments
- Choice of distance metric significantly affects semantic match results, with hypothesis-specific metrics outperforming naive approaches
- Framework reveals confirmation bias in intuitive interpretation of explanations by providing quantitative grounding for hypothesis testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured semantic match testing prevents confirmation bias by replacing intuitive judgments with quantitative overlap measures between explanations and hypotheses
- Mechanism: The paper formalizes a procedure where a hypothesis θ is tested by measuring the overlap between two sets: (1) data points with explanations close to a reference explanation ec, and (2) data points satisfying θ. Metrics like AUC and median distance provide quantitative grounding that counters intuitive confirmation bias
- Core assumption: Local feature attribution explanations have sufficient faithfulness to the model such that overlap with hypothesis-defined data points reflects actual model behavior
- Evidence anchors:
  - [abstract]: "we propose a structured approach to evaluate semantic match in practice... can give insight into both desirable... and undesirable model behaviors"
  - [section]: "we can side-step our intuition and thus avoid confirmation bias"
- Break condition: If explanations are not faithful to the model, semantic match scores will reflect explanation-model mismatch rather than actual model behavior, perpetuating rather than preventing bias

### Mechanism 2
- Claim: The semantic match framework allows detection of both desired and undesired model behaviors by formulating precise hypotheses and testing them against explanations
- Mechanism: By formulating specific hypotheses about model behavior (e.g., "model focuses on target object") and measuring semantic match metrics, the framework can reveal whether the model behaves as intended or exhibits spurious correlations
- Core assumption: Hypotheses can be precisely formulated and operationalized as subsets of the data space
- Evidence anchors:
  - [abstract]: "we show how the assessment of semantic match can give insight into both desirable... and undesirable model behaviors"
  - [section]: "we can conclude that we have a reasonable level of semantic match... we are confident that the explanations reveal what our hypothesis has described"
- Break condition: If hypotheses cannot be precisely operationalized or are too vague, the semantic match framework cannot provide meaningful insights

### Mechanism 3
- Claim: The choice of distance metric between explanations significantly affects semantic match results, requiring hypothesis-driven distance definitions
- Mechanism: The paper demonstrates that naive distance metrics (e.g., Euclidean distance across all features) can introduce noise when hypotheses are local. Refining the distance metric to only consider relevant features improves semantic match detection
- Core assumption: The distance metric can be tailored to the specific hypothesis being tested
- Evidence anchors:
  - [section]: "Revising our notion of distance to only consider dimension x1 and x3, we see a drop in median distance reaching 0.09, while semantic match AUC remains high at 0.92"
  - [section]: "the role of the logical structure of the hypotheses remains to be investigated"
- Break condition: If distance metrics cannot be effectively tailored to hypotheses, semantic match results will be confounded by irrelevant features

## Foundational Learning

- Concept: Local feature attribution methods and their faithfulness to models
  - Why needed here: The entire semantic match framework depends on explanations accurately reflecting model behavior
  - Quick check question: If an explanation says feature X is important for a prediction, what conditions must hold for this to be a faithful representation of the model?

- Concept: Hypothesis formulation and operationalization in machine learning
  - Why needed here: The framework requires precise hypotheses that can be mapped to data subsets
  - Quick check question: How would you operationalize the hypothesis "the model focuses on the target object" as a data subset criterion?

- Concept: Distance metrics and their properties in high-dimensional spaces
  - Why needed here: The choice of distance metric between explanations critically affects semantic match results
  - Quick check question: What are the trade-offs between using Euclidean distance versus hypothesis-specific distance metrics?

## Architecture Onboarding

- Component map: Hypothesis formulation module → Distance metric definition module → Explanation generation module (SHAP) → Semantic match evaluation module with AUC and median distance metrics → Visualization module for results
- Critical path: Hypothesis formulation → Distance metric definition → Explanation generation → Semantic match calculation → Result interpretation
- Design tradeoffs: Using SHAP provides widespread applicability but limits exploration of other attribution methods; precise hypothesis formulation improves results but requires domain expertise; complex distance metrics may improve accuracy but reduce interpretability
- Failure signatures: High AUC with large median distance indicates good discrimination but poor coherence; low AUC indicates poor semantic match; inconsistent results across different hypotheses suggest framework limitations
- First 3 experiments:
  1. Replicate the synthetic tabular data experiment to verify the basic semantic match framework works as expected
  2. Apply the framework to a different tabular dataset with known feature interactions to test generalizability
  3. Test the framework on a computer vision task with simpler hypotheses to validate the methodology on image data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of baseline in SHAP explanations affect the results of semantic match analysis?
- Basis in paper: [explicit] The paper notes that "the methods tend to be sensitive to the choice of baseline" and discusses how baselines impact feature importance scores
- Why unresolved: The paper only experiments with SHAP values and doesn't systematically vary the baseline to test its impact on semantic match results
- What evidence would resolve it: Systematic experiments varying the baseline in SHAP while measuring changes in semantic match AUC and median distance metrics

### Open Question 2
- Question: Can the semantic match framework be extended to post-hoc concept-based explanations like TCAV or concept activation vectors?
- Basis in paper: [inferred] The paper discusses limitations of feature attribution methods and mentions related approaches like concept bottleneck models and prototypes, suggesting a connection to concept-based explanations
- Why unresolved: The paper only applies the framework to SHAP values and doesn't test it on other explanation methods
- What evidence would resolve it: Applying the semantic match procedure to TCAV or similar concept-based methods and comparing results to feature attribution approaches

### Open Question 3
- Question: What is the optimal balance between precision and recall when measuring semantic match for different types of hypotheses?
- Basis in paper: [explicit] The paper introduces both q1 (precision) and q2 (recall) metrics and shows they can diverge, particularly in Figure 3 and Figure 4
- Why unresolved: The paper presents both metrics but doesn't provide guidance on when to prioritize one over the other or how to balance them
- What evidence would resolve it: Empirical studies showing how different hypothesis types (e.g., necessity vs sufficiency) correlate with optimal precision-recall trade-offs

## Limitations
- Framework effectiveness relies heavily on faithful explanations, but the paper doesn't thoroughly validate this assumption across different model types
- Hypothesis formulation requires significant domain expertise and may not work well for complex, multi-faceted behaviors
- The framework doesn't address how to handle situations where multiple competing hypotheses exist simultaneously

## Confidence

- **High confidence**: The technical framework for computing semantic match metrics (AUC and median distance) is clearly specified and reproducible. The synthetic tabular experiment demonstrates the method works as intended in controlled settings
- **Medium confidence**: The claim that structured hypothesis testing prevents confirmation bias is logically compelling but lacks strong empirical validation beyond the presented experiments. The mechanism is sound but unproven at scale
- **Low confidence**: Claims about the framework's effectiveness on complex real-world hypotheses and its ability to reveal nuanced model behaviors in diverse domains remain largely speculative without broader validation

## Next Checks

1. **Corpus expansion**: Search for additional papers that empirically validate semantic match testing approaches or demonstrate confirmation bias in XAI settings. Look specifically for studies comparing intuitive explanation interpretation versus structured hypothesis testing

2. **Framework stress test**: Apply the semantic match framework to a dataset with known confirmation bias pitfalls (e.g., spurious correlations in medical imaging) and test whether the structured approach successfully identifies these issues compared to intuitive analysis

3. **Metric sensitivity analysis**: Systematically vary distance metrics and threshold parameters across multiple hypothesis types to determine how sensitive semantic match scores are to these choices, and develop guidelines for metric selection