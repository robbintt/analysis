---
ver: rpa2
title: 'ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing
  Style Detection'
arxiv_id: '2307.14913'
source_url: https://arxiv.org/abs/2307.14913
tags:
- style
- ceur-ws
- task
- proceedings
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a transition-focused natural language inference
  approach for detecting writing style changes in multi-author documents. The task
  is formulated as an NLI problem where consecutive paragraphs are paired and classified
  as either having the same author or a style change.
---

# ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection

## Quick Facts
- arXiv ID: 2307.14913
- Source URL: https://arxiv.org/abs/2307.14913
- Reference count: 38
- Primary result: Transition-focused truncation with DeBERTa achieves macro F1 scores of 0.987 (easy), 0.812 (medium), and 0.746 (hard) in PAN 2023 multi-author style detection task

## Executive Summary
This paper presents a transition-focused natural language inference approach for detecting writing style changes in multi-author documents. The task is formulated as an NLI problem where consecutive paragraphs are paired and classified as either having the same author or a style change. To handle input truncation, the authors propose a transition-focused truncation strategy that prioritizes the last tokens of the first paragraph and first tokens of the second paragraph, capturing logical connections between them. Various Transformer-based encoders including BERT, RoBERTa, DeBERTa, and BigBird are evaluated with and without warmup training. The highest performing model uses DeBERTa with warmup training and transition-focused truncation, achieving macro F1 scores of 0.987 on the easy setup, 0.812 on medium, and 0.746 on hard. This model ranked second in all three subtasks of the PAN 2023 shared task.

## Method Summary
The method formulates multi-author writing style detection as a natural language inference problem by pairing consecutive paragraphs and classifying them as same author (0) or style change (1). The key technical contribution is transition-focused truncation, which prioritizes the last tokens of the first paragraph and first tokens of the second paragraph to capture transition cues. The approach uses Transformer-based encoders (BERT, RoBERTa, DeBERTa, BigBird) with warmup training (warmup ratio 0.1). Input length is limited to 512 tokens total (256 per paragraph), except BigBird which supports 1024 tokens. The model is trained for 5 epochs with learning rate 5e-5, batch size 4, and evaluated using macro F1 scores across three difficulty levels (Easy, Medium, Hard).

## Key Results
- DeBERTa with transition-focused truncation and warmup training achieves highest performance
- Macro F1 scores: 0.987 (easy setup), 0.812 (medium setup), 0.746 (hard setup)
- Ranked second place in all three subtasks of PAN 2023 shared task
- Transition-focused truncation outperforms standard head-only truncation strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transition-focused truncation captures the logical boundary between consecutive paragraphs more effectively than head-only truncation
- Mechanism: By prioritizing the last tokens of the first paragraph and first tokens of the second paragraph, the model can better capture transition cues that signal authorial shifts
- Core assumption: Writing style changes manifest most clearly in the transition zones between paragraphs, not in the interior content of paragraphs
- Evidence anchors: [abstract] "Our approach focuses on transitions between paragraphs while truncating input tokens for the task" and [section] "Since transitions provide logical connections between paragraphs in documents"
- Break condition: If style changes are distributed uniformly throughout paragraphs rather than concentrated at boundaries, this approach would lose critical context

### Mechanism 2
- Claim: NLI formulation transforms a complex multi-author detection problem into a simpler binary classification task
- Mechanism: By pairing consecutive paragraphs and asking whether they share the same author, the problem becomes a sequence classification task that leverages pre-trained language models
- Core assumption: Consecutive paragraph pairs are the most informative unit for detecting authorship transitions, rather than considering entire documents
- Evidence anchors: [abstract] "We formulate the task as a natural language inference problem where two consecutive paragraphs are paired" and [section] "We prepare input by concatenating consecutive paragraphs and using the SEP token between them"
- Break condition: If authorship style changes are not binary (same/different) but exist on a spectrum, the model may miss nuanced transitions

### Mechanism 3
- Claim: Warmup training with low learning rate helps models find better minima in this specific task
- Mechanism: Starting with a very low learning rate allows the model to explore the loss landscape more thoroughly before converging
- Core assumption: The NLI task for style detection has a complex loss landscape with multiple local minima, making warmup beneficial
- Evidence anchors: [abstract] "As backbone models, we employ different Transformer-based language models with warmup phase during training" and [section] "We use the warmup with the warmup ration is 0.1. In warmup steps, the model trains with a very low learning rate"
- Break condition: If the task's loss landscape is relatively smooth or if the model architecture already provides good initialization, warmup may provide minimal benefit

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The entire approach relies on Transformer-based models (BERT, RoBERTa, DeBERTa, BigBird) as the core text encoders
  - Quick check question: How does the attention mechanism in Transformers differ from recurrent architectures like LSTMs in handling long-range dependencies?

- Concept: Natural Language Inference (NLI) formulation
  - Why needed here: The paper explicitly frames style detection as an NLI problem, which is crucial for understanding the methodology
  - Quick check question: What are the key components of an NLI model, and how does this relate to the pair-wise classification approach used here?

- Concept: Token truncation strategies and their impact
  - Why needed here: The transition-focused truncation is the paper's main technical contribution, requiring understanding of how input length constraints affect model performance
  - Quick check question: What are the trade-offs between different truncation strategies (head-only, tail-only, balanced) for document-level tasks?

## Architecture Onboarding

- Component map: Input paragraphs → Truncation module → Transformer encoder → CLS pooling → Classification layer → Output (0/1 label)
- Critical path: Token preprocessing → Transition-focused truncation → Encoder forward pass → Pooling → Classification decision
- Design tradeoffs:
  - Token length vs. information retention: 256+256 truncation balances context with model limits
  - Encoder choice: DeBERTa performs best but may be more computationally expensive than BERT
  - Warmup duration: 0.1 ratio provides exploration but adds training time
- Failure signatures:
  - Low scores across all setups suggest fundamental architectural issues
  - Medium/hard setups performing worse than easy suggests difficulty handling topical consistency
  - High variance between warmup and non-warmup runs suggests optimization instability
- First 3 experiments:
  1. Compare transition-focused vs. longest-first truncation on validation set to verify the core hypothesis
  2. Test warmup vs. no warmup training on the same encoder to measure optimization benefits
  3. Run ablation study removing warmup and transition-focus to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the transition-focused truncation strategy compare to other potential truncation strategies (e.g., random sampling, attention-based selection) for handling long paragraphs in multi-author style detection?
- Basis in paper: [inferred] The paper proposes transition-focused truncation but does not compare it to other truncation strategies
- Why unresolved: The authors only evaluated their proposed transition-focused truncation against the default longest-first truncation
- What evidence would resolve it: Comparative experiments showing performance of transition-focused truncation against alternative strategies like random sampling, attention-based selection, or other heuristics

### Open Question 2
- Question: How do the performance results vary when using different pre-training objectives (e.g., ELECTRA, XLNet) instead of BERT-based models for the NLI formulation?
- Basis in paper: [inferred] The paper uses BERT, RoBERTa, DeBERTa, and BigBird, but does not explore other pre-training objectives
- Why unresolved: Only BERT-like encoders were evaluated as backbone models
- What evidence would resolve it: Experimental results comparing the proposed method using different pre-training objectives like ELECTRA, XLNet, or T5

### Open Question 3
- Question: What is the impact of increasing the input sequence length beyond 512 tokens on the performance of the proposed method, particularly for the hard setup where paragraphs may be longer?
- Basis in paper: [inferred] The paper mentions BigBird supports 1024 tokens but does not explore longer sequences or analyze the impact of sequence length on performance
- Why unresolved: Only tested BigBird with 1024 tokens; did not experiment with longer sequences or analyze how sequence length affects different difficulty levels
- What evidence would resolve it: Experiments varying input sequence lengths and analyzing performance changes across easy, medium, and hard setups

## Limitations
- The transition-focused truncation strategy is presented as superior but lacks empirical comparison with alternative truncation methods
- The binary NLI formulation may oversimplify nuanced writing style changes that exist on a spectrum
- Strong performance on validation set doesn't guarantee generalization without test set results

## Confidence
- High confidence: Transformer-based models for text classification tasks are well-established methodology
- Medium confidence: The ranking of second place in the shared task provides external validation of competitiveness
- Low confidence: The specific claim that transition-focused truncation is superior lacks direct comparative evidence

## Next Checks
1. **Truncation strategy comparison**: Run controlled experiments comparing transition-focused truncation against standard head-only and balanced truncation strategies on the same validation set, measuring both macro F1 scores and examining which token regions contribute most to correct predictions

2. **Extended context analysis**: Evaluate model performance when using full paragraphs (or longer sequences) without truncation to determine if the 256-token limit per paragraph is constraining the model's ability to capture stylistic patterns that extend beyond local transitions

3. **Cross-validation on difficulty levels**: Perform stratified cross-validation within each difficulty level to assess whether the model's performance is consistent across different document characteristics, and identify which aspects of the Easy, Medium, and Hard setups most challenge the current approach