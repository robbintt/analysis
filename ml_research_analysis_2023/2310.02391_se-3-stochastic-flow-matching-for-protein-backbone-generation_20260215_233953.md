---
ver: rpa2
title: SE(3)-Stochastic Flow Matching for Protein Backbone Generation
arxiv_id: '2310.02391'
source_url: https://arxiv.org/abs/2310.02391
tags:
- fold
- flow
- page
- cited
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents FoldFlow, a family of generative models for
  protein backbone design based on flow-matching in SE(3), the group of rigid motions.
  The authors introduce three variants: FoldFlow-Base (deterministic), FoldFlow-OT
  (incorporating Riemannian optimal transport for stability), and FoldFlow-SFM (stochastic).'
---

# SE(3)-Stochastic Flow Matching for Protein Backbone Generation

## Quick Facts
- arXiv ID: 2310.02391
- Source URL: https://arxiv.org/abs/2310.02391
- Reference count: 40
- Key outcome: FoldFlow achieves higher designability, diversity, and novelty scores than FrameDiff-Improved for protein backbone generation while being 2-3x faster to train

## Executive Summary
FoldFlow introduces a family of generative models for protein backbone design based on flow-matching in SE(3), the group of rigid motions. The models learn to map invariant source distributions to invariant target distributions over SE(3)^N, enabling equilibrium conformation generation not possible with diffusion models. Three variants are presented: FoldFlow-Base (deterministic), FoldFlow-OT (optimal transport), and FoldFlow-SFM (stochastic). These models outperform existing approaches on designability, diversity, and novelty metrics while achieving faster training times.

## Method Summary
FoldFlow models learn conditional vector fields that transport probability distributions over SE(3)^N. The framework uses Conditional Flow Matching (CFM) to learn time-dependent vector fields that generate probability paths between source and target distributions. Three variants are implemented: FoldFlow-Base learns deterministic dynamics directly, FoldFlow-OT incorporates Riemannian optimal transport for more stable flows, and FoldFlow-SFM learns stochastic dynamics by coupling Riemannian OT with simulation-free training. The models use an AF2-like structure module with invariant point attention layers and are trained with flow matching losses on both SO(3) rotation and R3 translation components.

## Key Results
- FoldFlow models achieve higher designability scores (scRMSD < 2.0Å) than FrameDiff-Improved
- FoldFlow-SFM achieves the highest novelty scores among all variants
- Models are 2-3x faster to train compared to diffusion-based approaches
- Successfully generate diverse, designable protein backbones up to 300 amino acids in length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FoldFlow-Base learns deterministic continuous-time dynamics by directly regressing the time-dependent vector field that generates the probability path between any invariant source and target distribution over SE(3)^N.
- Mechanism: The model uses Conditional Flow Matching (CFM) to learn the conditional vector field ut(xt|z) that transports r0 ~ ρ0 to r1 ~ ρ1. The CFM objective minimizes the difference between a parametric vector field vθ(t, xt) and the true conditional vector field ut(xt|z), allowing simulation-free training without requiring score computation.
- Core assumption: The probability path ρt(xt|z) can be generated by a deterministic vector field that can be learned through regression.
- Evidence anchors: [abstract] "FoldFlow-Base, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on SE(3)"; [section 3.1] "We seek to construct a conditional vector field ut(rt|z), lying on the tangent space TrtSO(3), that transports r0 ~ ρ0 to r1 ~ ρ1"
- Break condition: If the conditional probability path cannot be represented as a deterministic flow, or if the vector field vθ cannot approximate ut accurately.

### Mechanism 2
- Claim: FoldFlow-OT accelerates training by constructing straighter and simpler flows using Riemannian Optimal Transport (OT) on SE(3)^N, resulting in more stable flows.
- Mechanism: The model leverages the Monge map on SE(3)^N to define OT displacement interpolants. By drawing samples from the optimal transport plan π(x0, x1) and computing the geodesic interpolant between them, FoldFlow-OT creates conditional probability paths that are length-minimizing curves in the space of distributions P(SO(3)).
- Core assumption: The existence of a Monge map on SE(3)^N allows for the construction of optimal transport plans that generate straighter flows.
- Evidence anchors: [abstract] "We next accelerate training by incorporating Riemannian optimal transport to create FoldFlow-OT, leading to the construction of both more simple and stable flows"; [section 3.2] "We prove the existence of a Monge map on SE(3)^N allowing us to define the OT displacement interpolants"
- Break condition: If the OT problem on SE(3)^N cannot be solved efficiently, or if the Monge map does not exist under the given conditions.

### Mechanism 3
- Claim: FoldFlow-SFM learns stochastic continuous-time dynamics over SE(3) by coupling Riemannian OT and simulation-free training, enabling mapping from arbitrary distributions on SE(3)^N.
- Mechanism: The model builds a stochastic bridge using a guided diffusion SDE that conditions on reaching the target distribution at t=1. It uses a simulation-free approximation with an isotropic Gaussian distribution on SO(3) to sample from the conditional bridge without expensive SDE simulation.
- Core assumption: The simulation-free approximation of the Brownian bridge on SO(3) closely matches the true conditional probability path.
- Evidence anchors: [abstract] "Finally, we design FoldFlow-SFM coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over SE(3)"; [section 3.3] "We approximate ρt with the simulation-free alternative, ˆρt(˜rt|r0, r1) = IG SO(3)(˜rt; expr0(t logr0(r1)), γ2(t)t(1 − t))"
- Break condition: If the approximation error between the simulation-free method and true Brownian bridge becomes too large, or if the stochastic dynamics cannot be learned effectively.

## Foundational Learning

- Concept: Riemannian manifolds and Lie groups
  - Why needed here: Protein backbones are represented as elements of SE(3)^N, which requires understanding the geometry of rigid transformations and how to define probability distributions and flows on these manifolds
  - Quick check question: What is the difference between a Lie group and a Riemannian manifold, and how does SE(3) fit into both categories?

- Concept: Flow matching and conditional flow matching
  - Why needed here: The core training approach relies on learning vector fields that generate probability paths, which requires understanding the relationship between deterministic flows, conditional flows, and the flow matching objective
  - Quick check question: How does the CFM objective differ from the standard flow matching objective, and why is this difference important for protein backbone generation?

- Concept: Optimal transport and Monge maps
  - Why needed here: FoldFlow-OT uses Riemannian optimal transport to create straighter flows, which requires understanding the Monge formulation, the existence of Monge maps, and how they relate to optimal transport plans
  - Quick check question: Under what conditions does a Monge map exist on SE(3)^N, and how does this guarantee straighter probability paths?

## Architecture Onboarding

- Component map: PDB protein data -> Preprocessing (centering, rotation normalization) -> SE(3)^N representation -> AF2-like structure module (IPA layers) -> MLP head (oxygen torsion angle φ) -> Predicted initial state ˆx0 -> Velocity computation -> Flow matching loss

- Critical path:
  1. Sample from source and target distributions
  2. Compute geodesic/interpolant between samples
  3. Generate conditional bridge (deterministic or stochastic)
  4. Pass through AF2-like network to predict ˆx0
  5. Compute velocity from predicted ˆx0
  6. Calculate flow matching loss
  7. Backpropagate and update parameters

- Design tradeoffs:
  - Deterministic vs stochastic: FoldFlow-BASE/OT are deterministic and faster, FoldFlow-SFM is stochastic and achieves higher novelty at the cost of designability
  - OT vs no OT: FoldFlow-OT creates more stable flows but may reduce diversity compared to FoldFlow-BASE
  - Simulation-free vs simulated: FoldFlow-SFM uses simulation-free approximation for speed, but this may introduce approximation error

- Failure signatures:
  - Mode collapse: If the model only generates proteins from a subset of possible conformations
  - Numerical instability: If training diverges due to poor gradient flow or unstable vector field predictions
  - Low designability: If generated structures cannot be refolded by ESMFold with high probability
  - Overfitting: If the model memorizes training proteins rather than learning the underlying distribution

- First 3 experiments:
  1. Train FoldFlow-BASE on synthetic SO(3) data to verify the conditional vector field learning mechanism and compare against ground truth
  2. Train FoldFlow-OT on the same synthetic data to validate the OT-based approach and compare Wasserstein distances
  3. Train FoldFlow-SFM on synthetic SO(3) data to verify the stochastic bridge approximation and compare against simulated Brownian bridge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FoldFlow models scale with increasing protein length beyond 300 amino acids?
- Basis in paper: [inferred] The paper evaluates FoldFlow models up to 300 amino acids and notes a trade-off between designability and diversity/novelty for longer sequences.
- Why unresolved: The paper does not provide data or analysis for protein lengths exceeding 300 amino acids, leaving uncertainty about the models' effectiveness and limitations at longer scales.
- What evidence would resolve it: Empirical results showing designability, diversity, and novelty metrics for protein lengths beyond 300 amino acids would clarify the scalability and potential bottlenecks of FoldFlow models.

### Open Question 2
- Question: How sensitive is the designability metric to the threshold of RMSD < 2.0Å, and how should this threshold be adjusted for longer protein sequences?
- Basis in paper: [explicit] The paper mentions that a threshold of < 2.0Å for designability is standard but may be unreasonably strict for longer backbones, and it is unclear how this threshold should decay with increasing sequence length.
- Why unresolved: The paper does not explore the sensitivity of the designability metric to different RMSD thresholds or propose a method to adjust the threshold for varying protein lengths.
- What evidence would resolve it: Experimental data showing designability scores across a range of RMSD thresholds for different protein lengths would help determine an appropriate adaptive threshold.

### Open Question 3
- Question: How do FoldFlow models compare to other state-of-the-art methods when trained on larger and more diverse protein datasets?
- Basis in paper: [inferred] The paper notes that Genie is trained on a larger dataset (22k vs. 195k), which hinders rigorous comparisons with FoldFlow, and RFDiffusion uses a significantly larger and different training set.
- Why unresolved: The paper's comparisons are limited to models trained on similar or smaller datasets, making it difficult to assess FoldFlow's relative performance against methods trained on larger, more diverse data.
- What evidence would resolve it: Comparative experiments where FoldFlow models are trained on the same large and diverse datasets as other state-of-the-art methods would provide a fair assessment of their relative performance.

## Limitations

- The evaluation relies heavily on designability scores computed through ESMFold, which may not fully capture biological validity
- The paper lacks ablation studies isolating the impact of individual components (e.g., AF2 backbone vs. flow matching objective)
- While the models show improved performance on designability and diversity metrics, the computational cost of sampling and the model's ability to generalize to unseen protein families remain unexplored

## Confidence

- **High**: The core flow matching framework and its implementation on SE(3)^N manifolds is technically sound and well-supported by the mathematical foundations presented
- **Medium**: The empirical improvements over FrameDiff-Improved are convincing, but the evaluation could be more comprehensive (e.g., including stability metrics, energy-based validation)
- **Low**: The claims about FoldFlow-SFM achieving higher novelty scores are based on limited comparisons, and the trade-off between novelty and designability requires further investigation

## Next Checks

1. **Ablation Study**: Conduct controlled experiments to isolate the contribution of each component (AF2 backbone, flow matching objective, OT regularization) to overall performance
2. **Generalization Test**: Evaluate the models on protein families not well-represented in the training data to assess their ability to generate novel folds beyond the training distribution
3. **Biological Validation**: Perform energy minimization and molecular dynamics simulations on generated structures to assess their stability and biological plausibility beyond designability scores