---
ver: rpa2
title: Soft-prompt Tuning for Large Language Models to Evaluate Bias
arxiv_id: '2306.04735'
source_url: https://arxiv.org/abs/2306.04735
tags:
- bias
- language
- group
- prompt
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses soft-prompt tuning to adapt large language models
  for sentiment classification, evaluating bias across sensitive attributes like sexuality
  and age. Soft prompts are trained to condition models toward sentiment tasks, and
  bias is measured using group fairness metrics (accuracy and false positive rate
  gaps) across protected groups.
---

# Soft-prompt Tuning for Large Language Models to Evaluate Bias

## Quick Facts
- arXiv ID: 2306.04735
- Source URL: https://arxiv.org/abs/2306.04735
- Reference count: 37
- Key outcome: Soft-prompt tuning adapts LLMs to sentiment tasks and reveals bias across sensitive attributes like sexuality and age, with certain groups (e.g., asexual, homosexual) consistently disadvantaged.

## Executive Summary
This paper presents a method for evaluating bias in large language models using soft-prompt tuning, a parameter-efficient technique that adapts LLMs to sentiment classification tasks without modifying model weights. The approach trains continuous prompt embeddings to guide the frozen model toward sentiment-appropriate outputs and measures fairness across protected groups using accuracy and false positive rate gaps. Results show that bias patterns vary by dataset and attribute, with some groups like asexual and homosexual consistently disadvantaged, while others like heterosexual and adult are more often favored. The method avoids human bias in prompt design and provides a scalable, reproducible way to detect and analyze model bias before deployment.

## Method Summary
The method uses soft-prompt tuning to adapt pre-trained LLMs (OPT and Galactica) to sentiment classification tasks by training 8 continuous prompt token embeddings while keeping model weights frozen. Sentiment datasets (SemEval and SST-5) are mapped to 3-way sentiment (negative, neutral, positive). Bias is evaluated using synthetic templates from Czarnowska et al. [3] with identity adjectives for protected groups (Age, Sexuality). Group fairness metrics—accuracy gap and false positive rate gap—are computed across protected groups. Prompt tuning uses Adam optimizer with grid-searched learning rates, early stopping, and 15 random seeds per prompt, selecting the top 5 prompts by validation accuracy for final evaluation.

## Key Results
- Soft-prompt tuning effectively adapts LLMs to sentiment classification without labeled data or weight modification.
- Certain groups (asexual, homosexual) are consistently disadvantaged in terms of model errors, while others (heterosexual, adult) are more often favored.
- Bias patterns differ depending on the dataset and attribute studied, revealing that model biases are context-dependent.
- The approach avoids human bias in prompt design and provides a scalable, reproducible way to detect model bias.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft-prompt tuning can effectively adapt large language models to sentiment classification tasks without requiring labeled data for fine-tuning.
- Mechanism: By training a small set of continuous prompt token embeddings to be provided alongside the standard natural language input, the model's performance on the downstream task is optimized. The prompt tokens act as a task-specific conditioning mechanism that guides the frozen language model to produce sentiment-appropriate outputs.
- Core assumption: The pre-trained language model has learned generalizable representations that can be effectively conditioned through prompt optimization to perform sentiment classification, even though it was not explicitly trained for this task.
- Evidence anchors:
  - [abstract] "Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data"
  - [section] "Soft-prompt tuning has been used to induce high performance for several LLMs on various downstream tasks such as question answering, text summarization and text classification"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If the pre-trained language model lacks sufficient task-relevant knowledge or if the sentiment classification task requires complex reasoning beyond the model's learned capabilities, soft-prompt tuning may fail to achieve competitive performance.

### Mechanism 2
- Claim: Soft-prompt tuning enables fine-grained analysis and understanding of an LLM's bias towards underrepresented groups by conditioning the model on sentiment tasks and measuring fairness metrics across protected groups.
- Mechanism: By training prompts on sentiment datasets and evaluating the resulting model's performance across different sensitive attributes (e.g., age, sexuality) using group fairness metrics like accuracy gaps and false positive rate gaps, soft-prompt tuning reveals which groups are consistently advantaged or disadvantaged by the model's errors.
- Core assumption: The sentiment datasets used for prompt training are representative and free from the biases being measured, and that the resulting model's errors across protected groups are indicative of the underlying language model's biases rather than artifacts of the prompt tuning process.
- Evidence anchors:
  - [abstract] "Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts"
  - [section] "We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If the sentiment datasets used for prompt tuning contain biases or if the group fairness metrics fail to capture the relevant aspects of bias, the soft-prompt tuning approach may not accurately reveal the LLM's true biases towards underrepresented groups.

### Mechanism 3
- Claim: Soft-prompt tuning minimizes the potential influence of biases existing in supervised training tasks by restricting the number of learned parameters and removing the human element of prompt design.
- Mechanism: By freezing the underlying language model weights and only training a small set of prompt embeddings, soft-prompt tuning ensures that the model's biases are not explicitly modified through task-specific fine-tuning. Additionally, the automated prompt optimization process eliminates the possibility of human bias injection through manual prompt design.
- Core assumption: The biases present in the language model are a result of its pre-training data and architecture, and that minimizing the number of learned parameters in the prompt tuning process helps isolate these pre-existing biases rather than introducing new ones.
- Evidence anchors:
  - [section] "Because the weights of the underlying language model are frozen throughout the training process, producing task-specific representations does not explicitly modify biases inherited from the language model pretraining data"
  - [section] "On the other hand, the optimized prompts help ensure that the model performs the downstream task as well as a fully fine-tuned model, which naturally reflects the settings of practical deployment"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If the prompt tuning process inadvertently amplifies or introduces biases not present in the pre-trained language model, or if the frozen model weights contain biases that are not representative of real-world deployment scenarios, the soft-prompt tuning approach may not accurately reflect the true biases of the LLM.

## Foundational Learning

- Concept: Soft-prompt tuning and parameter-efficient tuning of LLMs
  - Why needed here: Understanding the mechanism of soft-prompt tuning is crucial for grasping how it can be used to adapt LLMs to sentiment classification tasks and evaluate bias without requiring large amounts of labeled data or modifying the underlying model weights.
  - Quick check question: How does soft-prompt tuning differ from traditional fine-tuning in terms of the number of parameters updated and the potential impact on model biases?

- Concept: Group fairness and bias quantification metrics
  - Why needed here: To evaluate the biases of LLMs across sensitive attributes and protected groups, it is essential to understand the concept of group fairness and the specific metrics used to quantify bias, such as accuracy gaps and false positive rate gaps.
  - Quick check question: What is the difference between accuracy gaps and false positive rate gaps in the context of group fairness, and how do they help identify which groups are advantaged or disadvantaged by model errors?

- Concept: Synthetic datasets for bias evaluation
  - Why needed here: The paper uses synthetic datasets generated from templates to evaluate bias in LLMs. Understanding the rationale behind using synthetic data and its limitations is crucial for interpreting the results and designing future bias evaluation studies.
  - Quick check question: What are the advantages and disadvantages of using synthetic datasets for bias evaluation compared to real-world datasets, and how might the simplicity of the templates impact the observed bias patterns?

## Architecture Onboarding

- Component map:
  Input text -> Prompt tokens (8 learned embeddings) -> Pre-trained transformer model (frozen weights) -> Sentiment classification output

- Critical path:
  1. Prepare sentiment classification dataset and templates for bias evaluation
  2. Initialize soft prompts with beginning-of-sequence token embeddings
  3. Train soft prompts using sentiment dataset while keeping language model weights frozen
  4. Evaluate resulting model's performance on bias evaluation dataset
  5. Compute group fairness metrics (accuracy gaps, false positive rate gaps) across sensitive attributes and protected groups

- Design tradeoffs:
  - Parameter efficiency vs. task performance: Soft-prompt tuning updates a small number of parameters compared to full fine-tuning, but may result in slightly lower task performance
  - Bias isolation vs. practical relevance: Freezing language model weights helps isolate pre-existing biases but may not reflect the biases that would emerge in real-world deployment scenarios with full fine-tuning

- Failure signatures:
  - Low task performance despite successful prompt tuning may indicate that the pre-trained language model lacks sufficient task-relevant knowledge
  - Inconsistent bias patterns across different sensitive attributes or protected groups may suggest that the sentiment datasets used for prompt tuning contain biases or that the group fairness metrics fail to capture the relevant aspects of bias

- First 3 experiments:
  1. Train soft prompts on SemEval dataset and evaluate resulting model's performance on bias evaluation dataset using group fairness metrics
  2. Repeat experiment with SST-5 dataset and compare bias patterns to those observed with SemEval
  3. Train soft prompts on a combination of SemEval and SST-5 datasets and analyze the impact on task performance and bias evaluation results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the prompt-tuning datasets (SemEval and SST-5) influence the fairness outcomes observed in the models?
- Basis in paper: [explicit] The authors mention that the quality of these datasets strongly impacts the soft-prompts produced and suggest exploring how better-quality datasets might affect both downstream task performance and observed bias.
- Why unresolved: The study uses standard sentiment datasets (SemEval from tweets and SST-5 from movie reviews) without exploring alternative or improved datasets. The direct impact of dataset quality on bias measurement remains untested.
- What evidence would resolve it: Comparative experiments using higher-quality, more diverse, or domain-specific sentiment datasets to assess changes in bias metrics and model fairness.

### Open Question 2
- Question: Would using more complex or varied templates for bias evaluation lead to different or more conclusive bias patterns across protected groups?
- Basis in paper: [explicit] The authors note that the templates from Czarnowska et al. [3] are simple and easily understood by LLMs, which may cause less conclusive results. They suggest trying more complicated templates as future work.
- Why unresolved: The current study relies on a fixed set of simple templates, which may not fully capture nuanced biases or stress-test model fairness across different contexts.
- What evidence would resolve it: Experiments with more complex, context-rich, or adversarial templates to see if bias patterns become more pronounced or shift across groups.

### Open Question 3
- Question: How do different bias quantification metrics compare in identifying favored and unfavored groups, and can they be reconciled?
- Basis in paper: [explicit] The authors acknowledge that different bias quantification formulations might not be concurrently achievable and that a group flagged as favorable by one metric may be unfavorable by another.
- Why unresolved: The study uses accuracy and false positive rate gaps, but does not compare these with other metrics (e.g., false negative rate, demographic parity) or explore their consistency across groups.
- What evidence would resolve it: A systematic comparison of multiple bias metrics on the same models and datasets to determine which groups are consistently advantaged or disadvantaged.

## Limitations

- The use of synthetic templates for bias evaluation may not capture the full complexity of real-world language patterns, potentially leading to artificially amplified or diminished bias signals.
- Findings are limited to OPT and Galactica models, and generalizability to other large language models (e.g., GPT, BERT) remains unclear.
- The selection and definition of sensitive attributes and protected groups may not be exhaustive or representative of all relevant social categories, potentially overlooking important sources of bias.

## Confidence

- High: The technical feasibility of soft-prompt tuning for sentiment classification and the mathematical formulation of group fairness metrics are well-established and clearly demonstrated.
- Medium: The methodology for bias evaluation using soft-prompt tuning is sound, but the interpretation of results depends on the representativeness of synthetic datasets and the completeness of protected group definitions.
- Low: The ecological validity of bias measurements and the generalizability of findings to real-world deployment scenarios are uncertain due to the simplified nature of synthetic templates and the focus on specific model architectures.

## Next Checks

1. Validate the synthetic templates against real-world datasets to assess the ecological validity of bias measurements and compare bias patterns observed in synthetic and real-world data to identify potential discrepancies.

2. Evaluate the bias patterns in a diverse set of large language models (e.g., GPT, BERT) to assess the generalizability of findings and analyze whether similar bias trends emerge across different model architectures and training approaches.

3. Expand the analysis to include additional sensitive attributes and protected groups (e.g., race, gender, socioeconomic status) to ensure a more comprehensive assessment of model bias and validate the impact of protected group definitions on observed bias patterns.