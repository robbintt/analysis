---
ver: rpa2
title: Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?
arxiv_id: '2307.10472'
source_url: https://arxiv.org/abs/2307.10472
tags:
- bias
- response
- arxiv
- language
- biased
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an evaluation of instruction fine-tuned language\
  \ models\u2019 ability to identify social bias through zero-shot prompting. The\
  \ authors restructure the BBQ dataset for bias identification and evaluate LLaMA\
  \ and its instruction fine-tuned versions, Alpaca and Koala, using various prompts\
  \ including Chain-of-Thought (CoT) prompts."
---

# Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?

## Quick Facts
- **arXiv ID**: 2307.10472
- **Source URL**: https://arxiv.org/abs/2307.10472
- **Reference count**: 8
- **Primary result**: Alpaca 7B achieves 56.7% accuracy on bias identification task using basic+specific+reason prompt

## Executive Summary
This paper evaluates instruction fine-tuned language models' ability to identify social bias through zero-shot prompting. The authors restructure the BBQ dataset and test LLaMA base models along with Alpaca and Koala instruction fine-tuned variants using various prompt templates including Chain-of-Thought prompts. Across all models and prompts, Alpaca 7B achieves the highest accuracy of 56.7% on the bias identification task. The work demonstrates that instruction fine-tuning and prompt specificity improve bias detection performance, though overall accuracy remains modest. The research represents the first component of a bias mitigation framework and will be updated with additional results.

## Method Summary
The authors restructure the BBQ dataset into conversational format and evaluate LLaMA, Alpaca, and Koala models (7B and 13B variants) on bias identification tasks. Models are tested using zero-shot prompting with multiple prompt templates including basic, specific, reasoning, and Chain-of-Thought variants. Performance is measured through accuracy metrics averaged over 5 runs, with 10K stratified samples from the 58,492 example BBQ dataset used for inference.

## Key Results
- Alpaca 7B achieves highest accuracy of 56.7% using basic+specific+reason prompt
- Instruction fine-tuned models outperform base LLaMA models on bias identification
- Koala 13B surpasses Alpaca 13B, confirming that scaling up model size and data diversity improves performance
- Specifying bias categories and requesting reasoning improves model performance compared to basic prompts

## Why This Works (Mechanism)

### Mechanism 1
- Instruction fine-tuning improves model ability to follow bias identification instructions compared to base models
- Exposure to diverse task formats and reasoning patterns enables better zero-shot generalization to bias identification
- Core assumption: Models trained on varied instruction formats can transfer that understanding to novel bias detection tasks

### Mechanism 2
- Providing specific bias categories and reasoning requirements improves model performance
- Explicitly naming bias types and requesting explanations provides semantic scaffolding that guides model attention
- Core assumption: Models can leverage categorical information when explicitly prompted to identify specific bias types

### Mechanism 3
- Scaling model size and instruction fine-tuning data diversity improves bias identification performance
- Larger models with more diverse training data can better capture nuanced social patterns and reasoning required for bias detection
- Core assumption: Model capacity and data diversity are primary drivers of improved bias identification capability

## Foundational Learning

- **Zero-shot prompting**: Models identify bias without task-specific training; needed because evaluation relies on models' inherent understanding; Quick check: How does zero-shot prompting differ from few-shot prompting in terms of required model capabilities?

- **Chain-of-Thought reasoning**: CoT prompts test explicit reasoning steps to improve bias identification; needed to assess whether step-by-step reasoning helps detection; Quick check: What are key differences between standard and CoT prompting in terms of model output structure?

- **Instruction fine-tuning principles**: Understanding IFT mechanics is crucial for interpreting performance differences; needed to contextualize why different models perform differently; Quick check: How does instruction fine-tuning differ from standard supervised fine-tuning in terms of training data format and objectives?

## Architecture Onboarding

- **Component map**: BBQ dataset restructuring -> Model layer (LLaMA/Alpaca/Koala) -> Prompting system (multiple templates) -> Output parsing (custom logic)
- **Critical path**: Restructure BBQ dataset samples into conversational format → Apply prompt template with specific instructions and reasoning requirements → Generate model output and parse for bias identification → Aggregate results across multiple runs and samples
- **Design tradeoffs**: Prompt complexity vs model understanding; Model size vs computational cost; Dataset sampling vs coverage
- **Failure signatures**: Consistently low accuracy suggests fundamental model limitations; High variance indicates dataset/prompting instability; Systematic false positive/negative bias reveals prompt interpretation issues
- **First 3 experiments**: 1) Test basic prompt variant on LLaMA 7B to establish baseline; 2) Compare basic+specific+reason prompt across Alpaca 7B and 13B to measure scaling effects; 3) Evaluate CoT prompt variants on Koala models to assess reasoning prompt effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Does instruction fine-tuning improve the ability of large language models to identify social bias in text?
- The paper directly investigates this by comparing LLaMA, Alpaca, and Koala models' performance
- Results are mixed: Alpaca 7B performs best but Alpaca 13B underperforms compared to Alpaca 7B
- Resolution requires further experiments comparing models on different bias identification tasks with control for factors like model size and fine-tuning dataset

### Open Question 2
- How does scaling up model size and data diversity affect instruction fine-tuned models' bias identification ability?
- Paper demonstrates scaling benefits but only experiments with 7B and 13B versions
- Impact of scaling to larger models (33B, 65B) and effect of data diversity on bias identification are not fully explored
- Resolution requires experiments comparing models of different sizes and data diversity levels on bias identification tasks

### Open Question 3
- Are there specific prompt variations more effective at eliciting bias identification from instruction fine-tuned models?
- Paper tests various prompt variations and suggests specifying bias types with reasoning improves performance
- Only tests a limited set of prompt variations; other variations may be more effective
- Resolution requires experiments testing a wider range of prompt variations to determine most effective formulations

## Limitations
- BBQ dataset restructuring may alter original bias manifestations through conversational format conversion
- Evaluation relies on relatively simple accuracy metrics without considering false positive/negative trade-offs across bias categories
- Overall accuracy remains modest (best at 56.7%) suggesting fundamental limitations in current approaches

## Confidence

- **High Confidence**: Instruction fine-tuned models outperform base models on bias identification tasks
- **Medium Confidence**: Claims about scaling benefits based on Koala 13B vs Alpaca 13B performance differences
- **Low Confidence**: Generalization to broader bias detection scenarios beyond restructured BBQ dataset format

## Next Checks

1. Conduct ablation studies testing each prompt component (basic, specific, reason) independently to quantify individual contributions to performance gains
2. Evaluate model performance on additional bias detection benchmarks beyond BBQ to assess generalization across different bias typologies and evaluation formats
3. Perform statistical significance testing across multiple random seeds and dataset samples to establish robustness of observed accuracy differences between model variants