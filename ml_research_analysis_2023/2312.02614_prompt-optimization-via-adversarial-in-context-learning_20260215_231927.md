---
ver: rpa2
title: Prompt Optimization via Adversarial In-Context Learning
arxiv_id: '2312.02614'
source_url: https://arxiv.org/abs/2312.02614
tags:
- prompt
- adv-icl
- training
- discriminator
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adversarial In-Context Learning (adv-ICL),
  a new method to optimize prompts for in-context learning (ICL) by employing three
  large language models (LLMs) in an adversarial framework. The generator LLM aims
  to produce realistic outputs given inputs and exemplars, while the discriminator
  LLM tries to classify these outputs as real or generated.
---

# Prompt Optimization via Adversarial In-Context Learning

## Quick Facts
- **arXiv ID**: 2312.02614
- **Source URL**: https://arxiv.org/abs/2312.02614
- **Reference count**: 29
- **Primary result**: adv-ICL improved ChatGPT's accuracy on MMLU from 71.0% to 74.0%

## Executive Summary
This paper introduces Adversarial In-Context Learning (adv-ICL), a novel method for optimizing prompts in in-context learning by employing three large language models in an adversarial framework. The approach uses a generator LLM to produce outputs, a discriminator LLM to classify them as real or generated, and a prompt modifier LLM to update prompts based on discriminator loss. This iterative process improves performance without updating model parameters, making it computationally efficient and effective in low-resource settings. Experimental results show significant improvements over state-of-the-art prompt optimization techniques across 11 tasks including summarization, arithmetic reasoning, and machine translation.

## Method Summary
adv-ICL implements a two-player game between generator and discriminator LLMs, where the generator aims to produce realistic outputs that fool the discriminator while the discriminator tries to accurately differentiate between real and generated outputs. A third prompt modifier LLM generates variations of task instructions and demonstrations, selecting those that most improve the adversarial loss. The method updates only prompts rather than model parameters, making it computationally efficient and easily extensible to any LLM and task. The iterative process continues for a fixed number of iterations with hyperparameter tuning to find optimal configurations for different tasks and models.

## Key Results
- adv-ICL improved ChatGPT's MMLU accuracy from 71.0% to 74.0%
- Significant improvements over state-of-the-art prompt optimization techniques across 11 generation and classification tasks
- Effective in low-resource settings without requiring parameter updates
- Easy to extend to any LLM and task through prompt-level optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The adversarial objective between generator and discriminator improves prompt quality by creating a minimax game that forces realistic outputs.
- **Mechanism:** The generator LLM produces outputs given inputs and exemplars, while the discriminator LLM classifies these outputs as real or generated. The prompt modifier updates both prompts based on discriminator loss, creating a feedback loop that iteratively improves the generator's outputs without updating model parameters.
- **Core assumption:** The discriminator and generator are sufficiently capable LLMs that can effectively distinguish and generate realistic outputs when given appropriate prompts.
- **Evidence anchors:**
  - [abstract] "As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator."
  - [section] "The generator aims to generate output realistic enough that the discriminator classifies it as real (i.e., not generated), while the discriminator aims to differentiate between generator output and training data samples as accurately as possible."
- **Break condition:** If either the generator or discriminator LLM lacks sufficient capacity to effectively distinguish or generate realistic outputs, the adversarial game fails to converge to better prompts.

### Mechanism 2
- **Claim:** Prompt-level optimization through the prompt modifier is computationally efficient and effective in low-resource settings.
- **Mechanism:** Instead of updating model parameters through backpropagation, the prompt modifier generates r variations of task instructions and demonstrations, then selects the ones that most improve the adversarial loss. This avoids expensive parameter updates while still improving performance.
- **Core assumption:** LLMs have sufficient in-context learning capability to follow human instructions for prompt modification and can generate meaningful variations that improve the adversarial objective.
- **Evidence anchors:**
  - [abstract] "because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings."
  - [section] "We employ a third LLM to serve as the prompt modifier, given a prompt's task instruction I or demonstration (x, y) as input, M generates r possible variations on it."
- **Break condition:** If the prompt modifier LLM cannot effectively follow instructions to generate meaningful prompt variations, or if the selected variations don't improve the adversarial objective, the method fails to optimize prompts effectively.

### Mechanism 3
- **Claim:** The adversarial framework converges to optimal prompts when using sufficiently capable models with appropriate hyperparameters.
- **Mechanism:** The theoretical analysis shows that with infinite capacity models and sufficient sampling, the adversarial objective converges. Practically, hyperparameter tuning (iterations T, samples per iteration m, variations r) finds good combinations that achieve strong performance without requiring many training rounds or data samples.
- **Core assumption:** The theoretical assumptions of infinite capacity models and sufficient sampling are approximately satisfied by the practical implementation with reasonable hyperparameters.
- **Evidence anchors:**
  - [abstract] "we show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks"
  - [section] "We use a simple hyperparameter search method to select a good combination... We compute the performance of adv-ICL on S and select the best combination"
- **Break condition:** If the hyperparameter search fails to find good combinations, or if the theoretical assumptions are too far from reality, the method may not converge to optimal prompts or may require excessive resources.

## Foundational Learning

- **Concept:** In-context learning (ICL)
  - **Why needed here:** This work builds on ICL as the foundation for prompt optimization, using LLMs to learn tasks through demonstration rather than parameter updates.
  - **Quick check question:** Can you explain the difference between fine-tuning a model and using in-context learning with demonstrations?

- **Concept:** Generative Adversarial Networks (GANs)
  - **Why needed here:** The method adapts the adversarial training framework from GANs to the prompt optimization setting, replacing parameter updates with prompt modifications.
  - **Quick check question:** What is the minimax objective in GANs and how does it translate to the prompt optimization context?

- **Concept:** Prompt engineering and optimization
  - **Why needed here:** Understanding how prompts affect LLM performance is crucial for implementing and evaluating the prompt modification strategies used in this work.
  - **Quick check question:** What are the key differences between continuous and discrete prompt optimization approaches?

## Architecture Onboarding

- **Component map:** Generator LLM -> Discriminator LLM -> Prompt Modifier LLM -> Updated Prompts
- **Critical path:**
  1. Initialize prompts for generator and discriminator
  2. Generator produces outputs given inputs and current prompt
  3. Discriminator classifies input-output pairs as real/generated
  4. Prompt modifier generates r variations for each component of both prompts
  5. Recompute adversarial loss for each variation
  6. Select variations that most improve adversarial loss
  7. Update prompts and repeat for T iterations

- **Design tradeoffs:**
  - Using same LLM for all three components vs different LLMs (efficiency vs specialization)
  - Number of variations r (more exploration vs computational cost)
  - Number of iterations T and samples per iteration m (convergence vs overfitting)
  - Same model for generator/discriminator vs different models (balance vs performance gap)

- **Failure signatures:**
  - Discriminator always classifies outputs as generated (generator too weak)
  - Discriminator always classifies outputs as real (discriminator too weak or prompt modifier failing)
  - No improvement in adversarial loss over iterations (poor prompt modifier or wrong hyperparameters)
  - Performance worse than baseline (overfitting, wrong hyperparameters, or poor initial prompts)

- **First 3 experiments:**
  1. Implement basic version with fixed prompts to verify generator and discriminator work independently
  2. Add prompt modifier with single variation to test basic optimization loop
  3. Implement full version with r variations and multiple iterations on simple classification task

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of generator and discriminator models affect the performance of adv-ICL?
- **Basis in paper:** [explicit] The paper states that "Given a generator, it is also important to answer how we can select a suitable discriminator to deploy our framework." It also mentions that "we observe with a stronger generator the performance is likely improved contrasting to with a stronger discriminator, the performance is potentially harmed."
- **Why unresolved:** The paper only provides preliminary results on the impact of model choice and suggests that the discriminator and generator should be on the same performance level. However, it does not provide a comprehensive analysis of how different model combinations affect adv-ICL's performance.
- **What evidence would resolve it:** Systematic experiments comparing adv-ICL's performance using various combinations of generator and discriminator models, with a focus on understanding the impact of model size, architecture, and training data on the final results.

### Open Question 2
- **Question:** How does the number of training iterations (T) and data samples per iteration (m) impact adv-ICL's performance, and what is the optimal configuration for different tasks and models?
- **Basis in paper:** [explicit] The paper mentions that "we perform hyperparameter search with three datasets including WebNLG, GSM8k and MMLU" and that "we observe the best performance achieved with T = 3 and m = 5 for both settings." However, it does not provide a comprehensive analysis of the impact of T and m on adv-ICL's performance.
- **Why unresolved:** The paper only provides a limited analysis of the impact of T and m on adv-ICL's performance, focusing on a specific set of tasks and models. A more comprehensive analysis is needed to understand the optimal configuration for different tasks and models.
- **What evidence would resolve it:** Extensive experiments varying T and m across different tasks and models, with a focus on identifying the optimal configuration for each combination.

### Open Question 3
- **Question:** How does the performance of adv-ICL scale with the size of the training data and the number of tasks it is applied to?
- **Basis in paper:** [inferred] The paper mentions that adv-ICL is "effective in low-resource settings" and is "easy to extend to any LLM and task." However, it does not provide a systematic analysis of how adv-ICL's performance scales with the size of the training data and the number of tasks.
- **Why unresolved:** The paper only provides a limited analysis of adv-ICL's performance in low-resource settings and does not investigate how it scales with the size of the training data and the number of tasks.
- **What evidence would resolve it:** Experiments varying the size of the training data and the number of tasks adv-ICL is applied to, with a focus on understanding how its performance scales in these scenarios.

### Open Question 4
- **Question:** How does adv-ICL compare to other prompt optimization methods in terms of computational efficiency and ease of implementation?
- **Basis in paper:** [explicit] The paper mentions that adv-ICL is "computationally efficient" and "easy to implement." However, it does not provide a comprehensive comparison of adv-ICL's computational efficiency and ease of implementation with other prompt optimization methods.
- **Why unresolved:** The paper only provides a limited comparison of adv-ICL with other prompt optimization methods in terms of performance and does not investigate its computational efficiency and ease of implementation.
- **What evidence would resolve it:** A comprehensive comparison of adv-ICL with other prompt optimization methods in terms of computational efficiency and ease of implementation, with a focus on understanding the trade-offs between performance, efficiency, and ease of use.

## Limitations

- The method requires three capable LLMs, creating significant computational overhead and dependency on model quality
- Theoretical assumptions of infinite capacity models and sufficient sampling may not hold in practical implementations
- Experimental evaluation lacks thorough ablation studies showing individual component contributions

## Confidence

- **High confidence**: The core claim that adv-ICL improves performance over baseline prompt optimization techniques is well-supported by the experimental results on multiple tasks and models.
- **Medium confidence**: The claim about computational efficiency relative to parameter-based methods is plausible but depends heavily on the relative costs of prompt optimization versus fine-tuning in specific use cases.
- **Medium confidence**: The assertion that the method is "easy to extend to any LLM and task" is supported by experimental diversity but lacks systematic analysis of what makes tasks more or less amenable to adv-ICL.

## Next Checks

1. **Ablation study on component contributions**: Systematically disable each component (generator, discriminator, prompt modifier) to quantify their individual impact on performance improvements.

2. **Failure mode analysis**: Design experiments specifically targeting known failure conditions, such as mismatched model capabilities or prompt modifier limitations, to understand method robustness boundaries.

3. **Cross-domain generalization test**: Apply adv-ICL to tasks outside the paper's scope (e.g., code generation, mathematical theorem proving) to evaluate the claim of easy extension to any task.