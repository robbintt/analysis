---
ver: rpa2
title: How many Neurons do we need? A refined Analysis for Shallow Networks trained
  with Gradient Descent
arxiv_id: '2309.08044'
source_url: https://arxiv.org/abs/2309.08044
tags:
- neural
- proposition
- probability
- least
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the generalization properties of two-layer
  neural networks in the neural tangent kernel (NTK) regime, trained with gradient
  descent. The authors derive fast rates of convergence that are known to be minimax
  optimal in the framework of non-parametric regression in reproducing kernel Hilbert
  spaces.
---

# How many Neurons do we need? A refined Analysis for Shallow Networks trained with Gradient Descent

## Quick Facts
- arXiv ID: 2309.08044
- Source URL: https://arxiv.org/abs/2309.08044
- Authors: 
- Reference count: 40
- Key outcome: Derives fast minimax optimal rates of convergence for two-layer neural networks in NTK regime, precisely tracking neuron requirements as M ≥ O(n^(2r/(2r+b)))

## Executive Summary
This paper provides a refined analysis of two-layer neural networks trained with gradient descent in the neural tangent kernel (NTK) regime. The authors derive fast rates of convergence that are known to be minimax optimal in non-parametric regression, precisely tracking the number of hidden neurons required for generalization. They show that weights remain bounded near initialization during training, with the radius dependent on smoothness assumptions and eigenvalue decay of the NTK operator.

## Method Summary
The authors analyze two-layer neural networks with M hidden neurons trained via gradient descent with constant step size. They work in the NTK regime where the network function remains close to its linearization around initialization. The analysis uses RKHS framework with source conditions and spectral theory of integral operators. Key results include deriving early stopping times Tn = O(n^(1/(2r+b))) and neuron requirements M ≥ O(n^(2r/(2r+b))) for well-specified cases where the regression function belongs to the RKHS.

## Key Results
- Achieves minimax optimal rates of convergence O(n^(-2r/(2r+b))) for smooth regression functions in NTK regime
- Shows weights remain bounded in a vicinity of initialization during training, with radius dependent on smoothness r and eigenvalue decay b
- Precisely tracks neuron requirements: M ≥ O(n^(2r/(2r+b))) for well-specified cases (r ≥ 1/2)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** NTK approximation becomes increasingly accurate as network width grows, allowing gradient descent to behave like kernel gradient descent.
- **Mechanism:** As width M increases, the function computed by the two-layer network remains close to its linearization around initialization during training. This linearization is determined by the NTK, which acts as a fixed kernel that governs the training dynamics.
- **Core assumption:** Weights remain bounded in a vicinity of their initialization throughout training.
- **Evidence anchors:**
  - [abstract]: "We further show that the weights during training remain in a vicinity around initialization, the radius being dependent on structural assumptions such as degree of smoothness of the regression function and eigenvalue decay of the integral operator associated to the NTK."
  - [section 3.2]: "We prove that during GD with constant step size, in the well-specified case, the weights stay bounded if M ≥ O(T^2r_n) (for r ≥ 1/2), up to a logarithmic factor."
- **Break condition:** If weights escape the initialization neighborhood, NTK approximation breaks down.

### Mechanism 2
- **Claim:** Early stopping at time Tn = O(n^(1/(2r+b))) achieves minimax optimal generalization rates.
- **Mechanism:** The stopping time balances approximation error (decreases with training time) against estimation error (increases with training time). The exponents 2r+b come from the interplay between function smoothness r and eigenvalue decay rate b of the NTK operator.
- **Core assumption:** Regression function satisfies source condition g_ρ = L^r_∞ h_ρ for some h_ρ in L², and NTK operator has polynomially decaying eigenvalues with rate b.
- **Evidence anchors:**
  - [abstract]: "For early stopped GD we derive fast rates of convergence that are known to be minimax optimal in the framework of non-parametric regression in reproducing kernel Hilbert spaces."
  - [section 3.1]: "We derive an early stopping time Tn = O(n^(1/(2r+b)) that leads to minimax optimal rates of convergence."
- **Break condition:** If 2r+b ≤ 1, the learning problem becomes "hard" and derived rates no longer apply.

### Mechanism 3
- **Claim:** Number of neurons needed scales as M ≥ O(n^(2r/(2r+b))) for optimal generalization.
- **Mechanism:** This neuron count ensures sufficient approximation power via random features and control over weight deviation during training. The scaling matches random feature approximation requirements in RKHS.
- **Core assumption:** NTK spectrum has polynomial decay and regression function is smooth enough to be well-approximated by NTK RKHS.
- **Evidence anchors:**
  - [abstract]: "We present a refined number of neurons that are needed for optimality of order M_n ≥ O(n^(2r/(2r+b)) = O(T^2r_n) for r ≥ 1/2, i.e. for well-specified cases where the regression function belongs to the RKHS"
  - [section 3.1]: "The number of hidden neurons required for generalization and improve over existing results."
- **Break condition:** If M grows slower than required rate, approximation error dominates; if M grows faster, computational resources are wasted.

## Foundational Learning

- **Concept:** Neural Tangent Kernel approximation
  - **Why needed here:** The entire analysis relies on NTK regime where networks behave linearly during training. Understanding this approximation is essential for following proof techniques.
  - **Quick check question:** What is the key condition that ensures NTK approximation remains valid throughout training?

- **Concept:** Reproducing Kernel Hilbert Spaces and spectral theory
  - **Why needed here:** The analysis uses RKHS framework with source conditions and eigenvalue decay of integral operators. This is crucial for deriving convergence rates.
  - **Quick check question:** How does the source condition g_ρ = L^r_∞ h_ρ characterize the smoothness of the regression function?

- **Concept:** Non-parametric regression and minimax rates
  - **Why needed here:** The paper frames results in terms of optimal rates for non-parametric regression, requiring understanding of what minimax optimality means in this context.
  - **Quick check question:** What does it mean for a learning algorithm to achieve minimax optimal rates in RKHS framework?

## Architecture Onboarding

- **Component map:** NTK approximation module -> Gradient descent training loop with early stopping -> Error decomposition analysis -> Weight deviation tracking -> Spectral analysis of NTK operator
- **Critical path:** NTK approximation → Gradient descent dynamics → Error decomposition → Weight deviation bounds → Final generalization bound
- **Design tradeoffs:** Width vs. generalization (more neurons improve approximation but increase computational cost), stopping time vs. bias-variance tradeoff, initialization scale vs. NTK accuracy
- **Failure signatures:** Weights escaping initialization neighborhood, NTK approximation breaking down, early stopping too late/too early, eigenvalue decay assumptions violated
- **First 3 experiments:**
  1. Verify NTK approximation accuracy for increasing network width on a simple regression problem
  2. Test early stopping time sensitivity by varying the smoothness parameter r
  3. Validate weight deviation bounds by tracking ∥θ_t - θ_0∥ during training for different neuron counts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the precise relationship between the number of hidden neurons required for optimal generalization and the smoothness parameter r of the regression function in the NTK regime?
- **Basis in paper:** [explicit] The paper derives different bounds for the number of neurons needed depending on whether r ≥ 1/2 or r ≤ 1/2.
- **Why unresolved:** The analysis provides bounds but doesn't explore the exact transition point or provide a unified formula for all r values.
- **What evidence would resolve it:** A closed-form expression for the minimum number of neurons as a function of r that is valid for all r > 0.

### Open Question 2
- **Question:** How does the eigenvalue decay rate b of the kernel integral operator affect generalization performance in deep neural networks beyond the two-layer case?
- **Basis in paper:** [inferred] The paper analyzes the two-layer case with a polynomial decay rate b, but deep networks are mentioned only briefly.
- **Why unresolved:** The analysis is limited to shallow networks, and extension to deep networks is not explored.
- **What evidence would resolve it:** Empirical studies or theoretical analysis of deep networks showing how b affects generalization rates.

### Open Question 3
- **Question:** What are the optimal learning rate schedules for gradient descent in the NTK regime to achieve the fastest convergence rates?
- **Basis in paper:** [explicit] The paper uses constant step size but mentions that decaying step sizes have been used in previous work.
- **Why unresolved:** The paper focuses on constant step size and doesn't explore the impact of different learning rate schedules.
- **What evidence would resolve it:** Comparative analysis of different learning rate schedules on convergence rates in NTK regime.

## Limitations
- Analysis assumes polynomial eigenvalue decay (b > 0) and sufficient smoothness (2r + b > 1), which may not hold for all practical problems
- Symmetric initialization assumption is restrictive and results may not extend to asymmetric initialization schemes
- Results focus on shallow networks and may not generalize to deep architectures
- Theory assumes asymptotic regimes that may not reflect real-world training scenarios with finite-width networks

## Confidence
- **High Confidence:** NTK approximation mechanism and its role in enabling kernel-like training behavior
- **Medium Confidence:** Specific scaling relationships for neuron requirements (M ≥ O(n^(2r/(2r+b)))) and early stopping times (Tn = O(n^(1/(2r+b))))
- **Low Confidence:** Practical applicability of these bounds for finite-width networks with moderate neuron counts

## Next Checks
1. **Empirical NTK approximation verification:** Train networks with varying widths M on synthetic regression problems with known smoothness r, and measure deviation of actual training dynamics from NTK predictions
2. **Early stopping sensitivity analysis:** Systematically vary smoothness parameter r in controlled experiments to validate whether derived stopping time Tn = O(n^(1/(2r+b))) achieves optimal generalization rates
3. **Weight deviation monitoring:** Track ∥θ_t - θ_0∥ during training across different neuron counts M to verify whether theoretical bounds on weight deviation are empirically tight and whether weights remain within required vicinity of initialization