---
ver: rpa2
title: Can persistent homology whiten Transformer-based black-box models? A case study
  on BERT compression
arxiv_id: '2312.10702'
source_url: https://arxiv.org/abs/2312.10702
tags:
- bert
- homology
- each
- which
- persistent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Optimus BERT Compression and Explainability
  (OBCE), a novel methodology to bring explainability to BERT models using persistent
  homology. The approach aims to measure the importance of each neuron by studying
  the topological characteristics of their outputs.
---

# Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression

## Quick Facts
- arXiv ID: 2312.10702
- Source URL: https://arxiv.org/abs/2312.10702
- Reference count: 40
- Primary result: OBCE achieves 58.47% parameter reduction for BERT Base and 52.3% for BERT Large while maintaining or improving GLUE performance

## Executive Summary
This paper introduces Optimus BERT Compression and Explainability (OBCE), a novel approach that applies zero-dimensional persistent homology to measure neuron importance in BERT models. By analyzing the topological characteristics of neuron outputs across a dataset, OBCE identifies which neurons contribute most information to the inference process. The method successfully compresses BERT models while providing explainability - achieving parameter reductions of over 40% while maintaining or improving performance on GLUE benchmark tasks.

The key innovation is using persistent homology as a "whitening" mechanism that reveals the internal structure of black-box models. The approach converts neuron output vectors into geometric points and tracks when connected components form and merge, with the radius at which all components first merge (rf) quantifying each neuron's information contribution. This allows selective pruning of less important neurons while compensating for their removal through bias adjustments.

## Method Summary
OBCE applies zero-dimensional persistent homology to BERT neuron outputs to measure importance. For each layer, the method extracts CLS token outputs from a dataset, converts these into geometric points, and computes rf values that quantify output spread. Neurons are ranked by rf and pruned at 30th, 50th, or 70th percentiles. Removed neurons' contributions are preserved by adding mean+std to layer bias. The approach explicitly preserves LayerNorm components due to their critical role. After pruning, models are fine-tuned on GLUE tasks for 40 epochs and evaluated against original and state-of-the-art compressed models.

## Key Results
- OBCE reduces BERT Base to 58.47% of original parameters and BERT Large to 52.3%
- Performance maintained or improved on most GLUE tasks compared to original models
- Outperforms other state-of-the-art BERT compression techniques
- Demonstrates effective explainability by revealing neuron importance through topological analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-dimensional persistent homology captures neuron importance by measuring the variability of neuron outputs across a dataset.
- **Mechanism:** The method converts neuron output vectors into geometric points in a high-dimensional space. By applying zero-dimensional persistent homology, it tracks when connected components (clusters of outputs) form and merge as a radius grows uniformly. The radius value rf at which all components first merge quantifies the "spread" of the neuron's outputs.
- **Core assumption:** A neuron with more variable outputs (higher rf) carries more information relevant to the model's decision-making.
- **Evidence anchors:**
  - [abstract]: "measure the importance of each neuron by studying the topological characteristics of their outputs"
  - [section]: "The larger rf, the farther are neuron's outputs from one other, meaning they exhibit a higher variability"
  - [corpus]: Missing direct evidence; the corpus contains general persistent homology papers but no specific BERT/transformer application.
- **Break condition:** If neuron outputs are uniformly distributed or have no clear clustering structure, rf may not meaningfully distinguish neuron importance.

### Mechanism 2
- **Claim:** Pruning neurons based on rf percentiles simplifies the model without significant loss of performance.
- **Mechanism:** After computing rf for all neurons in a layer, the method uses statistical thresholds (30th, 50th, 70th percentiles) to decide which neurons to remove. Neurons with low rf values are pruned, and their contribution is preserved by adding mean+std to the layer bias.
- **Core assumption:** Removing low-rf neurons does not disrupt the model's ability to capture essential patterns because these neurons contribute less topological information.
- **Evidence anchors:**
  - [abstract]: "This allows for the removal of neurons that provide less information, compressing the network while maintaining performance"
  - [section]: "Using the persistence diagram, we analyze which neurons can be removed"
  - [corpus]: Weak; no direct evidence in corpus about pruning strategies or performance retention.
- **Break condition:** If critical neurons happen to have low rf values (e.g., due to dataset bias or representation redundancy), pruning them could degrade performance.

### Mechanism 3
- **Claim:** Preserving the LayerNorm operation is essential for maintaining model performance after neuron pruning.
- **Mechanism:** The method explicitly retains the Attention Output and Output components because their LayerNorm operations combine current state and original input. Experiments showed that removing these components caused significant performance loss.
- **Core assumption:** LayerNorm operations in BERT layers contribute indispensable normalization that cannot be replicated by simple bias adjustments after pruning.
- **Evidence anchors:**
  - [section]: "we consistently see that the Attention Output and Output components generate the least information... However, they cannot be simplified because their implementation involves a LayerNorm operation... the network lost a significant portion of its predictive capacity"
  - [abstract]: "achieving results that outperform other state-of-the-art techniques for BERT compression"
  - [corpus]: Missing evidence; no corpus entry discusses LayerNorm preservation or its impact on pruning.
- **Break condition:** If an alternative normalization strategy could replace LayerNorm without loss, the pruning scope could be expanded.

## Foundational Learning

- **Concept: Persistent homology and Vietoris-Rips complexes**
  - Why needed here: The method relies on building simplicial complexes from neuron outputs to analyze topological features. Understanding how points form simplices and how radii define connections is essential.
  - Quick check question: In a Vietoris-Rips complex, what condition must hold for a simplex to be included when the radius is r?

- **Concept: Zero-dimensional homology and persistence diagrams**
  - Why needed here: The method uses 0D homology to track connected components over growing radii, and persistence diagrams to visualize when components are born and die. This is the core measurement tool for neuron importance.
  - Quick check question: In a persistence diagram for 0D homology, what does the y-coordinate of a point represent?

- **Concept: Percentile-based pruning thresholds**
  - Why needed here: The method uses rf percentiles to decide which neurons to prune. Understanding how percentiles partition data is key to interpreting pruning levels.
  - Quick check question: If a layer has 100 neurons and you prune at the 70th percentile, how many neurons remain?

## Architecture Onboarding

- **Component map:** Input preprocessing -> Persistent homology engine -> Pruning engine -> Model reconstruction -> Evaluation pipeline
- **Critical path:** Dataset selection → Zero-dimensional persistent homology computation → rf distribution analysis → Neuron pruning → Model reconstruction → GLUE fine-tuning → Performance comparison
- **Design tradeoffs:**
  - Compression vs. performance: Higher pruning percentiles yield more compression but risk performance loss.
  - Layer selection: Starting pruning after layer 2 reduces compression but stabilizes fine-tuning.
  - Bias compensation: Adding mean+std preserves pruned neuron contributions but may not fully replicate their function.
- **Failure signatures:**
  - rf values clustered near zero across all neurons → pruning ineffective.
  - Sudden GLUE metric drops after pruning → critical neurons removed or LayerNorm impact underestimated.
  - Persistent homology computation timeouts → dataset too large or dimension too high.
- **First 3 experiments:**
  1. Run zero-dimensional persistent homology on a single BERT Base layer using a small Wikipedia subset; plot persistence diagrams and verify rf extraction.
  2. Apply 30th percentile pruning to the same layer; reconstruct and test if forward pass still works.
  3. Evaluate a 2-layer pruned BERT on a single GLUE task (e.g., SST-2) and compare accuracy to the original.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The direct link between topological spread (rf) and actual information contribution to model predictions remains theoretical rather than empirically validated
- Pruning thresholds (30th, 50th, 70th percentiles) appear arbitrary without justification for why these specific values optimally balance compression and performance
- Preservation of LayerNorm components while removing other neurons creates an asymmetry that may limit further compression potential

## Confidence
**High Confidence**: The core mechanism of using zero-dimensional persistent homology to measure neuron output variability is mathematically sound and computationally feasible. The implementation details for computing rf values and applying percentile-based pruning are clearly specified.

**Medium Confidence**: The claim that pruning neurons based on rf values maintains or improves GLUE performance is supported by empirical results, but the generalizability across different model architectures and datasets needs further validation. The LayerNorm preservation decision is empirically justified but lacks deeper theoretical grounding.

**Low Confidence**: The explainability claims - that rf values directly correspond to neuron importance in model reasoning - are asserted but not independently verified through interpretability analyses or attribution studies.

## Next Checks
1. **Ablation Study on LayerNorm Components**: Systematically test whether preserving LayerNorm components is truly necessary by implementing alternative normalization strategies post-pruning and measuring performance impact.

2. **Cross-Dataset Generalization**: Apply the OBCE methodology to non-GLUE datasets (e.g., biomedical text, legal documents) to evaluate whether rf-based pruning maintains its effectiveness across different domains and vocabulary distributions.

3. **Attribution Analysis**: Conduct integrated gradients or similar attribution studies on both original and OBCE-pruned models to verify whether neurons with high rf values indeed contribute more to final predictions than low-rf neurons.