---
ver: rpa2
title: Noisy Interpolation Learning with Shallow Univariate ReLU Networks
arxiv_id: '2307.15396'
source_url: https://arxiv.org/abs/2307.15396
tags:
- lemma
- have
- then
- therefore
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies noisy interpolation learning using minimum\
  \ \u21132-norm two-layer ReLU networks for univariate regression. The authors rigorously\
  \ analyze how overfitting behaves for different loss functions, building on prior\
  \ empirical observations of \"tempered overfitting\" where risk doesn't diverge\
  \ but also doesn't reach Bayes optimal error."
---

# Noisy Interpolation Learning with Shallow Univariate ReLU Networks

## Quick Facts
- arXiv ID: 2307.15396
- Source URL: https://arxiv.org/abs/2307.15396
- Reference count: 40
- One-line primary result: Minimum ℓ2-norm ReLU interpolators exhibit tempered overfitting for Lp losses with p<2 but catastrophic overfitting for p≥2, even in simple univariate settings.

## Executive Summary
This paper rigorously analyzes noisy interpolation learning with minimum ℓ2-norm two-layer ReLU networks for univariate regression. The authors demonstrate that overfitting behavior critically depends on the loss function exponent p, with tempered behavior (risk converges to finite multiple of Bayes risk) for p<2 and catastrophic behavior (risk diverges to infinity) for p≥2. This finding challenges previous characterizations of tempered overfitting and reveals a subtle dependence on loss function choice that has important implications for neural network training with noisy data.

## Method Summary
The study examines minimum ℓ2-norm interpolators for univariate regression with noisy labels. The authors characterize the interpolator geometrically as linear splines and analyze risk behavior through spike formation mechanisms. For p<2 losses, spike-induced errors accumulate slowly enough to maintain tempered overfitting, while for p≥2 losses, spike risk grows faster than suppression probability, causing catastrophic divergence. The analysis distinguishes between convergence in probability versus expectation, revealing that tempered behavior can appear catastrophic when measured by expectation alone due to heavy-tailed risk distributions.

## Key Results
- Tempered overfitting occurs for Lp losses with p<2, where risk converges in probability to a finite multiple of Bayes risk
- Catastrophic overfitting occurs for p≥2 losses, with risk diverging to infinity despite bounded high-probability risk
- The distinction between convergence in probability versus expectation reveals tempered behavior that appears catastrophic under expectation measurement
- Spike formation in regions of rapid curvature change drives the difference between tempered and catastrophic behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The min-norm interpolator forms "spikes" in regions where curvature sign changes rapidly, and these spikes' magnitude determines whether overfitting is tempered or catastrophic.
- Mechanism: When two consecutive special points (where curvature changes sign) are close together with different labels, the interpolator must "spike" to pass through both points while minimizing weight norm. The spike height is proportional to ℓ²/max(ℓ₁,ℓ₃) where ℓ represents interval lengths.
- Core assumption: The minimum ℓ₂-norm solution is unique and can be characterized geometrically in terms of linear splines and curvature changes.
- Evidence anchors:
  - [abstract] "They are more conservative in the number of linear 'pieces'... This could create additional 'spikes', extending above and below the data points"
  - [section] "We show overfitting is catastrophic with respect to the L₂ loss, or when taking an expectation over the training set"
- Break condition: If the data distribution prevents rapid curvature sign changes (e.g., uniformly spaced grid), the spike mechanism cannot operate and tempered behavior emerges.

### Mechanism 2
- Claim: The loss function exponent p critically determines whether spike-induced errors accumulate or cancel.
- Mechanism: For p < 2, the Lp risk of spikes grows slower than the probability of spike formation, leading to tempered behavior. For p ≥ 2, spike risk grows faster than suppression probability, causing catastrophic divergence.
- Core assumption: Spike formation events are sufficiently rare but their impact is large enough to dominate risk calculations when p ≥ 2.
- Evidence anchors:
  - [abstract] "We show that overfitting is tempered (with high probability) when measured with respect to the L₁ loss, but also show that the situation is more complex... overfitting is catastrophic with respect to the L₂ loss"
  - [section] "For losses with p < 2 (including L1), overfitting is shown to be tempered... However, for p ≥ 2 losses (including squared loss), overfitting becomes catastrophic"
- Break condition: If spike formation probability becomes zero (e.g., in the grid setting) or if the loss function has p < 2, the mechanism shifts to tempered behavior.

### Mechanism 3
- Claim: The distinction between convergence in probability versus convergence of expectation reveals a subtle aspect of tempered overfitting.
- Mechanism: While the population risk converges in probability to a finite multiple of Bayes risk for p ∈ [1,2), its expectation remains infinite due to rare but extreme spike events. This creates tempered behavior that appears catastrophic when measured by expectation alone.
- Core assumption: The risk distribution has heavy tails due to spike formation, making the expectation sensitive to rare events while probability convergence remains stable.
- Evidence anchors:
  - [abstract] "Although the risk Lp( ˆfS) converges in probability to a tempered behavior... its expectation is infinite: ES[Lp( ˆfS)] = ∞"
  - [section] "we can get a high probability upper bound for the Lp loss... Thus, we get a bound on Lp loss in the entire domain [0, 1] w.h.p"
- Break condition: If the risk distribution has finite variance or if spike events become impossible, the expectation-convergence gap closes.

## Foundational Learning

- Concept: Minimum norm interpolation and its geometric characterization
  - Why needed here: The paper's entire analysis depends on understanding how the minimum ℓ₂-norm solution behaves geometrically in function space
  - Quick check question: What geometric property of the min-norm solution creates spikes when curvature changes sign rapidly?

- Concept: Lp risk analysis and its sensitivity to tail events
  - Why needed here: Different p values lead to dramatically different risk behaviors, with p ≥ 2 causing catastrophic divergence
  - Quick check question: Why does the Lp risk for p ≥ 2 diverge while p < 2 remains bounded?

- Concept: Probability convergence vs. expectation convergence
  - Why needed here: The paper distinguishes between high-probability bounds and expected risk, revealing tempered behavior that appears catastrophic under expectation
  - Quick check question: How can a risk be tempered in probability but infinite in expectation?

## Architecture Onboarding

- Component map: Data generation -> Min-norm solver -> Risk evaluation -> Spike detection
- Critical path:
  1. Generate synthetic data from distribution D
  2. Compute min-norm interpolator using geometric characterization
  3. Evaluate population risk for different p values
  4. Analyze spike formation and its impact on risk

- Design tradeoffs:
  - Exact vs. approximate min-norm computation (geometric characterization enables exact solutions)
  - Grid vs. random data spacing (affects spike formation probability)
  - Lp vs. other loss functions (determines whether behavior is tempered or catastrophic)

- Failure signatures:
  - Infinite risk expectations despite bounded high-probability bounds
  - Spike formation in regions of rapid curvature change
  - Catastrophic behavior specifically for p ≥ 2 losses

- First 3 experiments:
  1. Compare min-norm interpolator risk for p=1 vs p=2 on synthetic data with varying noise levels
  2. Evaluate tempered vs. catastrophic behavior on grid data vs. random data
  3. Test whether spike formation probability correlates with risk divergence for p ≥ 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What happens to overfitting behavior when the number of samples n grows faster than the spacing between grid points?
- Basis in paper: [explicit] Theorem 5 shows tempered overfitting for fixed grid spacing 1/n, while the main results (Theorems 2-4) show tempered vs catastrophic behavior depends on whether points are randomly spaced or on a grid
- Why unresolved: The paper only considers fixed grid spacing. It's unclear if tempered behavior persists when n grows faster than the inverse spacing
- What evidence would resolve it: Analysis of min-norm interpolation with varying grid spacings that shrink faster than 1/n as n grows

### Open Question 2
- Question: How does the overfitting behavior change for univariate regression with other activation functions beyond ReLU?
- Basis in paper: [inferred] The analysis relies heavily on ReLU properties (piecewise linearity, convexity) that enable geometric characterization of the min-norm interpolator
- Why unresolved: The paper focuses exclusively on ReLU networks. Other activations (e.g. sigmoid, tanh) have different geometric properties that may lead to different overfitting behavior
- What evidence would resolve it: Analysis of min-norm interpolation with other activation functions, characterizing their geometric properties and resulting overfitting behavior

### Open Question 3
- Question: What is the exact relationship between the Lipschitz constant of the target function and the constant factor in tempered overfitting?
- Basis in paper: [explicit] Theorem 2 shows tempered overfitting with constant factor C/(2-p) for p < 2, where C depends on the Lipschitz constant G
- Why unresolved: The paper provides an upper bound but doesn't characterize the exact dependence on G or show it's tight
- What evidence would resolve it: Matching upper and lower bounds on the tempered overfitting constant that explicitly depend on the target's Lipschitz constant

## Limitations

- The analysis is limited to univariate regression and may not generalize to multivariate settings
- The geometric characterization of min-norm interpolators relies on specific properties of ReLU networks
- The distinction between tempered and catastrophic behavior is somewhat arbitrary at p=2 given the smooth transition in risk behavior

## Confidence

- High: The geometric characterization of min-norm interpolators and spike formation mechanism (p<2 tempered behavior)
- Medium: The catastrophic behavior for p≥2 losses and expectation vs. probability convergence distinction
- Low: The specific numerical constants and exact convergence rates for different loss functions

## Next Checks

1. **Spike Probability Analysis**: Empirically measure spike formation probability across different data distributions and noise levels to verify the claimed correlation with risk divergence for p≥2.

2. **Boundary Behavior Testing**: Systematically test p values near 2 (e.g., p=1.9, 1.95, 2.05) to characterize the smooth transition between tempered and catastrophic behavior.

3. **Grid vs. Random Data Comparison**: Compare overfitting behavior on structured grid data versus random data to validate the theoretical prediction that grid data prevents spike formation and maintains tempered behavior regardless of p.