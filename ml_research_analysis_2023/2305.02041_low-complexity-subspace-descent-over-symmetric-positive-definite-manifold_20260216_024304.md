---
ver: rpa2
title: Low-complexity subspace-descent over symmetric positive definite manifold
arxiv_id: '2305.02041'
source_url: https://arxiv.org/abs/2305.02041
tags:
- riemannian
- complexity
- descent
- matrix
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes low-complexity Riemannian subspace descent
  algorithms for optimization over the symmetric positive definite (SPD) manifold.
  The key innovation is utilizing carefully chosen subspaces that allow updates to
  be expressed as products of Cholesky factors and sparse matrices, avoiding costly
  operations like matrix exponentiation and dense multiplication.
---

# Low-complexity subspace-descent over symmetric positive definite manifold

## Quick Facts
- arXiv ID: 2305.02041
- Source URL: https://arxiv.org/abs/2305.02041
- Reference count: 14
- Key outcome: Proposes low-complexity Riemannian subspace descent algorithms with O(n) and O(n^2) per-iteration complexity for optimization over SPD manifolds, avoiding costly matrix operations

## Executive Summary
This paper introduces Riemannian subspace descent algorithms that achieve significantly lower per-iteration complexity than existing methods for optimization over the symmetric positive definite (SPD) manifold. The key innovation is expressing updates as products of Cholesky factors and sparse matrices, which avoids expensive operations like matrix exponentiation and dense multiplication. The algorithms are demonstrated to be particularly effective for large-scale covariance estimation and matrix square root problems, achieving superior runtime performance compared to Riemannian gradient descent methods.

## Method Summary
The paper proposes Riemannian subspace descent algorithms that maintain Cholesky factors of SPD matrices throughout optimization. For functions in a specific class F, the Riemannian gradients can be computed efficiently by maintaining intermediate matrices M1,p and M2,q. Updates are expressed as products of the current Cholesky factor and sparse update matrices, avoiding costly matrix exponentiation and dense multiplication. The algorithms include uni-directional variants with O(n) per-iteration complexity and multi-directional variants with O(n^2) complexity. A greedy subspace selection heuristic is also introduced to improve convergence rates while maintaining low computational complexity.

## Key Results
- Achieves O(n) and O(n^2) per-iteration complexity for uni-directional and multi-directional variants, respectively
- Avoids O(n^3) or higher complexity of existing Riemannian gradient descent methods
- Demonstrated superior runtime and complexity on large-scale covariance estimation and matrix square root problems
- Linear convergence rates established for strongly convex functions within class F

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed algorithm achieves lower per-iteration complexity by maintaining and updating Cholesky factors directly instead of full SPD matrices.
- Mechanism: By expressing updates as products of the Cholesky factor of the current iterate and a sparse update matrix, expensive operations like matrix exponentiation and dense matrix multiplication are avoided.
- Core assumption: The SPD manifold structure allows updates to be written in terms of Cholesky factors with sparse matrices.
- Evidence anchors:
  - [abstract]: "utilizing carefully chosen subspaces that allow updates to be expressed as products of Cholesky factors and sparse matrices"
  - [section 3.1.2]: "the update can be carried out as Bt+1 = BtBup_t+1 where Bup_t+1 is sparse"
  - [corpus]: Weak - no direct evidence about Cholesky factorization in related papers
- Break condition: If the function class F does not allow efficient gradient computation, or if the chosen subspaces lead to dense update matrices

### Mechanism 2
- Claim: The proposed algorithm works efficiently for a broad class of functions F by maintaining intermediate variables.
- Mechanism: For functions in class F, the Riemannian gradients can be computed efficiently by maintaining intermediate matrices M1,p(Xt) and M2,q(Xt) that track terms like B^{-1}_t Cp B^{-T}_t and B^T_t Dq Bt.
- Core assumption: Functions in class F have a specific structure that allows gradient computation through these intermediate variables.
- Evidence anchors:
  - [abstract]: "We further identify a broad class of functions... over which the Riemannian gradients can be calculated efficiently"
  - [section 4]: Detailed derivation of how β_ij depends on M1,p and M2,q matrices
  - [corpus]: Weak - no direct evidence about this specific function class in related papers
- Break condition: If functions fall outside class F, or if maintaining intermediate variables becomes computationally prohibitive

### Mechanism 3
- Claim: The proposed greedy subspace selection achieves good convergence while maintaining low complexity.
- Mechanism: The greedy algorithm selects non-overlapping basis vectors corresponding to the largest gradient entries, ensuring descent directions are chosen without computing all gradient entries.
- Core assumption: Selecting largest gradient entries corresponds to steepest descent directions on the SPD manifold.
- Evidence anchors:
  - [section 4.3]: "selecting the basis vectors that correspond to the largest values of|βij|, which correspond to the directions of steepest descent"
  - [section 5.2]: Lemma 8 proves that greedy selection maintains sufficient descent
  - [corpus]: Weak - related papers discuss coordinate descent but not greedy selection on SPD manifolds
- Break condition: If greedy selection leads to poor subspaces that don't capture sufficient descent information

## Foundational Learning

- Concept: Riemannian manifolds and geodesic convexity
  - Why needed here: The optimization problem is defined on the SPD manifold, which is a Riemannian manifold, and convergence analysis relies on geodesic convexity properties
  - Quick check question: What is the key difference between geodesic convexity and standard convexity on Euclidean spaces?

- Concept: Cholesky factorization and SPD matrices
  - Why needed here: The algorithm maintains Cholesky factors of SPD matrices throughout optimization, requiring understanding of how SPD matrices can be factorized
  - Quick check question: How does the Cholesky factorization of an SPD matrix relate to its eigenvalue decomposition?

- Concept: Coordinate descent and subspace selection
  - Why needed here: The algorithm is a Riemannian extension of coordinate descent, requiring understanding of how to select subspaces for descent on manifolds
  - Quick check question: Why can't standard coordinate descent be directly applied to Riemannian manifolds?

## Architecture Onboarding

- Component map: Subspace selection -> Gradient computation (maintains M1,p, M2,q) -> Cholesky update -> Convergence check
- Critical path: Subspace selection → gradient computation → Cholesky update → convergence check
- Design tradeoffs:
  - Memory vs computation: Uni-directional version uses O(n) memory but needs more iterations; multi-directional uses O(n²) memory but converges faster
  - Random vs greedy selection: Random is simpler but greedy often converges faster despite higher per-iteration cost
  - Function class restriction: Limiting to class F enables efficiency but excludes some problems
- Failure signatures:
  - Poor convergence: Check if function is actually in class F or if subspace selection is ineffective
  - Memory overflow: Likely with multi-directional version on very large matrices
  - Numerical instability: Could occur if Cholesky updates become ill-conditioned
- First 3 experiments:
  1. Implement uni-directional version on synthetic covariance estimation problem with known solution
  2. Compare convergence of random vs greedy subspace selection on moderate-sized problems
  3. Benchmark per-iteration time complexity for increasing matrix sizes to verify O(n) scaling

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas for future research are implied:

### Open Question 1
- Question: How can the proposed subspace descent algorithms be extended to non-convex functions over the SPD manifold?
- Basis in paper: [inferred] The paper mentions that iteration complexity analysis for non-convex functions satisfying the Polyak- Lojasiewicz (PL) inequality would be of great importance.
- Why unresolved: The paper only provides convergence analysis for geodesically convex functions, and extending the analysis to non-convex cases is non-trivial.
- What evidence would resolve it: A rigorous convergence analysis and complexity bounds for the proposed algorithms applied to a class of non-convex functions over the SPD manifold.

### Open Question 2
- Question: Can the per-iteration complexity of the multi-directional variants be further reduced by using a more sophisticated subspace selection strategy?
- Basis in paper: [inferred] The paper discusses a greedy heuristic for selecting non-overlapping basis vectors, but notes that calculating and sorting all entries of F(X) is expensive.
- Why unresolved: The greedy heuristic has a high per-iteration complexity, and it's unclear if there exists a more efficient strategy that still provides good convergence.
- What evidence would resolve it: A subspace selection strategy with lower per-iteration complexity that maintains similar convergence rates to the greedy heuristic.

### Open Question 3
- Question: How does the performance of the proposed algorithms compare to Riemannian accelerated gradient descent methods in terms of iteration complexity and wall-clock time?
- Basis in paper: [explicit] The paper compares the proposed algorithms to RGD and RAGD in terms of per-iteration complexity, but does not provide a direct comparison of their convergence rates or wall-clock times.
- Why unresolved: The paper focuses on the per-iteration complexity and does not provide a comprehensive comparison of the algorithms' overall performance.
- What evidence would resolve it: A thorough empirical comparison of the proposed algorithms and Riemannian accelerated gradient descent methods on various optimization problems over the SPD manifold, including iteration complexity and wall-clock time measurements.

## Limitations
- The function class F, while broad, may not cover all practical SPD optimization problems
- The greedy subspace selection heuristic's effectiveness depends on the specific structure of the problem
- Numerical stability considerations for large-scale problems are not thoroughly discussed

## Confidence

**Major Uncertainties:**
- The function class F, while broad, may not cover all practical SPD optimization problems. The paper does not provide empirical evidence of performance on functions outside this class.
- The greedy subspace selection heuristic's effectiveness depends on the specific structure of the problem, but the paper lacks systematic analysis of when this approach might fail.
- Numerical stability considerations for large-scale problems are not thoroughly discussed, particularly regarding potential ill-conditioning during Cholesky updates.

**Confidence Labels:**
- High confidence: The O(n) and O(n^2) per-iteration complexity claims are well-supported by the algorithm structure and verified through the provided examples.
- Medium confidence: The convergence analysis for strongly convex functions within class F is rigorous, but extension to non-convex cases is not explored.
- Low confidence: The empirical comparison against other Riemannian optimization methods could be more comprehensive, particularly for non-convex problems.

## Next Checks

1. **Function Class Verification**: Systematically test the algorithms on functions that lie at the boundary of class F to identify where the efficiency gains break down.

2. **Numerical Stability Analysis**: Implement rigorous condition number monitoring during Cholesky updates across diverse problem instances to identify stability thresholds.

3. **Cross-Manifold Applicability**: Adapt the subspace descent framework to other Riemannian manifolds (e.g., Stiefel manifold) to test the generality of the Cholesky-based update approach.