---
ver: rpa2
title: Controlling Text-to-Image Diffusion by Orthogonal Finetuning
arxiv_id: '2306.07280'
source_url: https://arxiv.org/abs/2306.07280
tags:
- text
- prompt
- coft
- generation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses how to effectively guide large text-to-image
  diffusion models for downstream tasks like subject-driven generation and controllable
  generation. It introduces Orthogonal Fine-tuning (OFT), a method that finetunes
  diffusion models while provably preserving hyperspherical energy, which characterizes
  the pairwise neuron relationship on the unit hypersphere.
---

# Controlling Text-to-Image Diffusion by Orthogonal Finetuning

## Quick Facts
- arXiv ID: 2306.07280
- Source URL: https://arxiv.org/abs/2306.07280
- Authors: 
- Reference count: 40
- Key outcome: Orthogonal Fine-tuning (OFT) preserves hyperspherical energy during diffusion model fine-tuning, achieving superior performance in subject-driven generation and controllable generation compared to DreamBooth and LoRA

## Executive Summary
This paper addresses the challenge of effectively guiding large text-to-image diffusion models for downstream tasks while preserving their semantic generation ability. The authors introduce Orthogonal Fine-tuning (OFT), which finetunes diffusion models using layer-shared orthogonal transformations that provably preserve hyperspherical energy - a measure characterizing pairwise neuron relationships on the unit hypersphere. OFT outperforms existing approaches in generation quality, convergence speed, and stability. The method is specifically evaluated on subject-driven generation (using DreamBooth dataset) and controllable generation (using segmentation maps), demonstrating better subject fidelity, prompt fidelity, and control consistency.

## Method Summary
OFT modifies attention weights in Stable Diffusion by applying layer-shared orthogonal transformations parameterized via Cayley transformation. The method preserves hyperspherical energy during training by constraining the orthogonal matrix R to satisfy R^T R = I. COFT extends this by adding a radius constraint ||R - I|| ≤ ε to further improve stability. The training process involves initializing R as identity, applying the orthogonal transformation to update neuron weights, and optimizing with the preservation objective. The approach is evaluated on subject-driven generation (comparing subject fidelity, prompt fidelity, and sample diversity) and controllable generation (measuring control signal consistency and generation performance).

## Key Results
- OFT achieves better subject fidelity and prompt fidelity metrics than DreamBooth and LoRA for subject-driven generation
- For controllable generation, OFT converges faster and requires fewer training samples while maintaining strong control consistency
- COFT improves finetuning stability with radius constraint ε = 6 × 10^-5
- OFT demonstrates superior generation performance measured by FID scores across both tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal transformations preserve hyperspherical energy, characterizing pairwise neuron relationships and semantic generation ability
- Mechanism: Layer-shared orthogonal transformations update neuron weights while preserving pairwise angles between neurons on the unit hypersphere
- Core assumption: Hyperspherical energy is a valid measure of semantic preservation in text-to-image diffusion models
- Evidence anchors:
  - [abstract] "OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere"
  - [section 3.1] "Hyperspherical energy is defined as the sum of hyperspherical similarity (e.g., cosine similarity) between all pairwise neurons in the same layer"
  - [corpus] "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization" (related work using orthogonal methods)
- Break condition: If hyperspherical energy does not correlate with semantic preservation, the theoretical foundation fails

### Mechanism 2
- Claim: Orthogonal transformations provide sufficient flexibility for downstream task adaptation while maintaining stability
- Mechanism: Orthogonal matrices allow rotation/reflection of neuron directions without changing their magnitudes, enabling semantic modification while preserving pretrained knowledge
- Core assumption: Neuron directions (angles) encode the majority of semantic information, not magnitudes
- Evidence anchors:
  - [section 3.1] "the phase spectrum, which is angular information between input and basis, preserves the major part of semantics"
  - [section 3.1] "changing the neuron directions is the key to semantically modifying the generated image"
  - [section 3.2] "OFT seeks to optimize the orthogonal matrix R... changing the forward pass from z = (W^0)^T x to z = W^T x = (R · W^0)^T x"
- Break condition: If neuron magnitudes contain significant semantic information not captured by angles, orthogonal transformations would be insufficient

### Mechanism 3
- Claim: Constrained Orthogonal Fine-tuning (COFT) adds radius constraint for improved finetuning stability
- Mechanism: COFT limits the deviation of the orthogonal matrix from identity (||R - I|| ≤ ε), constraining the finetuned model within a hypersphere around pretrained neurons
- Core assumption: Constraining deviation from pretrained model improves stability while maintaining sufficient flexibility
- Evidence anchors:
  - [abstract] "To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere"
  - [section 3.4] "z = W^T x = (R · W^0)^T x, s.t. R^T R = RR^T = I, ||R - I|| ≤ ε"
  - [section 5.1] "we use ε = 6 × 10^-5 to constrain the orthogonal matrices"
- Break condition: If the radius constraint is too restrictive, it may prevent effective adaptation to downstream tasks

## Foundational Learning

- Concept: Hyperspherical energy and its relationship to neural network generalization
  - Why needed here: The paper's core mechanism relies on preserving hyperspherical energy during finetuning
  - Quick check question: What is hyperspherical energy and why does minimizing it lead to better generalization in classification networks?

- Concept: Orthogonal transformations and their properties in high-dimensional spaces
  - Why needed here: OFT uses layer-shared orthogonal transformations to update neuron weights
  - Quick check question: How does an orthogonal transformation preserve pairwise angles between neurons?

- Concept: Text-to-image diffusion models and their architecture
  - Why needed here: Understanding the model being fine-tuned is essential for implementing OFT
  - Quick check question: What are the key components of Stable Diffusion v1.5 that would be modified by OFT?

## Architecture Onboarding

- Component map: Pretrained weights → Orthogonal matrix initialization (identity) → Cayley parameterization → Forward pass with transformed weights → Training with preservation objective
- Critical path: Pretrained weights → Orthogonal matrix initialization (identity) → Cayley parameterization → Forward pass with transformed weights → Training with preservation objective
- Design tradeoffs: Flexibility vs stability (standard finetuning is most flexible but unstable; OFT balances both; COFT prioritizes stability)
- Failure signatures: Model collapse during training, loss of subject identity in subject-driven generation, poor control consistency in controllable generation
- First 3 experiments:
  1. Implement OFT on a simple fully-connected layer and verify hyperspherical energy preservation
  2. Apply OFT to attention weights in Stable Diffusion and test subject-driven generation on DreamBooth dataset
  3. Compare COFT vs OFT stability by training with same learning rate and measuring convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of block-diagonal parameter r affect the trade-off between flexibility and parameter efficiency in OFT, and is there an optimal value of r that works well across different tasks and datasets?
- Basis in paper: [explicit] The paper discusses the block-diagonal structure of R in OFT and its effect on parameter efficiency. It mentions that smaller r generally works better than larger r, but also suggests that a good trade-off exists.
- Why unresolved: The paper provides some empirical evidence for the effect of r on a specific task (segmentation to image), but it doesn't explore the impact of r across a wide range of tasks and datasets. Additionally, the paper doesn't provide a theoretical analysis of how r affects the trade-off between flexibility and parameter efficiency.
- What evidence would resolve it: A comprehensive study that evaluates the performance of OFT with different values of r across multiple tasks and datasets. Additionally, a theoretical analysis of how r affects the flexibility and parameter efficiency of OFT.

### Open Question 2
- Question: How does the hyperspherical energy preservation property of OFT contribute to the preservation of semantic generation ability in text-to-image diffusion models, and can this property be mathematically formalized?
- Basis in paper: [explicit] The paper states that preserving hyperspherical energy is crucial for preserving the semantic generation ability of text-to-image diffusion models, but it doesn't provide a mathematical formalization of this relationship.
- Why unresolved: While the paper demonstrates the empirical effectiveness of OFT in preserving semantic generation ability, it doesn't provide a rigorous mathematical explanation of why hyperspherical energy preservation leads to better semantic preservation.
- What evidence would resolve it: A mathematical analysis that establishes a formal relationship between hyperspherical energy preservation and semantic generation ability in text-to-image diffusion models. This could involve theoretical proofs or empirical studies that validate the relationship.

### Open Question 3
- Question: How does the Constrained Orthogonal Fine-tuning (COFT) variant improve the finetuning stability compared to the original OFT, and what is the optimal value of the deviation constraint parameter ε?
- Basis in paper: [explicit] The paper introduces COFT as a variant of OFT that imposes an additional radius constraint to improve finetuning stability. It mentions that the deviation constraint parameter ε is important for controlling the flexibility of COFT.
- Why unresolved: The paper provides some empirical evidence for the effectiveness of COFT in improving finetuning stability, but it doesn't provide a detailed analysis of how the deviation constraint parameter ε affects the stability and performance of COFT. Additionally, the paper doesn't explore the optimal value of ε for different tasks and datasets.
- What evidence would resolve it: A comprehensive study that evaluates the performance of COFT with different values of ε across multiple tasks and datasets. Additionally, a theoretical analysis of how ε affects the stability and performance of COFT.

## Limitations

- The theoretical foundation relies on the assumption that hyperspherical energy preservation directly correlates with semantic preservation, which requires empirical validation
- Implementation details for the radius constraint in COFT are somewhat vague, particularly regarding how the constraint interacts with training dynamics
- The paper does not provide sensitivity analysis for the radius constraint parameter ε or discuss how it was chosen

## Confidence

**High Confidence**: The empirical results showing OFT outperforming baseline methods in subject-driven generation and controllable generation tasks. The quantitative metrics (FID, subject fidelity scores, control consistency metrics) are directly measured and reported.

**Medium Confidence**: The theoretical claims about hyperspherical energy preservation and its relationship to semantic preservation. While the mathematics is sound, the direct applicability to diffusion model behavior is not rigorously proven.

**Low Confidence**: The claim that OFT provides a fundamental advantage over other parameter-efficient fine-tuning methods in terms of convergence speed and stability. The paper shows comparative results but does not provide ablation studies or theoretical justification for why orthogonal transformations specifically enable these benefits.

## Next Checks

1. **Ablation study on hyperspherical energy preservation**: Train OFT with and without the energy preservation constraint to quantify the impact on generation quality and semantic preservation.

2. **Radius constraint sensitivity analysis**: Systematically vary the ε parameter in COFT across multiple orders of magnitude to identify the optimal range and understand the tradeoff between stability and flexibility.

3. **Comparison with alternative orthogonal methods**: Implement and compare OFT with other orthogonal fine-tuning approaches (such as those mentioned in related work) to isolate the benefits of the specific orthogonal transformation strategy used.