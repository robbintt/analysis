---
ver: rpa2
title: 'Interpretability at Scale: Identifying Causal Mechanisms in Alpaca'
arxiv_id: '2305.08809'
source_url: https://arxiv.org/abs/2305.08809
tags:
- causal
- boundless
- task
- input
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Boundless DAS, a method for scaling causal
  interpretability techniques to large language models. The approach extends Distributed
  Alignment Search by learning the dimensionality of causal variable alignments rather
  than brute-forcing over possible dimensionalities.
---

# Interpretability at Scale: Identifying Causal Mechanisms in Alpaca

## Quick Facts
- **arXiv ID**: 2305.08809
- **Source URL**: https://arxiv.org/abs/2305.08809
- **Reference count**: 40
- **Primary result**: Boundless DAS reveals interpretable boolean variables in Alpaca for numerical reasoning, with IIA reaching 0.86-0.95

## Executive Summary
This paper introduces Boundless DAS, an efficient method for scaling causal interpretability to large language models by learning the dimensionality of causal variable alignments rather than brute-forcing over possible dimensionalities. Applied to the 7B parameter Alpaca model on a simple numerical reasoning task, Boundless DAS discovers that Alpaca implements two interpretable boolean variables checking if inputs fall within specified boundaries. The alignments are robust across different inputs, instructions, and contexts, demonstrating that interpretable causal structure can be found in large models, enabling faithful explanations of their behavior.

## Method Summary
Boundless DAS extends Distributed Alignment Search by learning the dimensionality of causal variable alignments through gradient-based optimization of learnable continuous boundary parameters. The method uses weighted distributed interchange interventions with sigmoid-based masks that gradually become binary during training, enabling gradual discovery of causal alignments. A rotation matrix maps neural representations to a rotated space where causal variables are aligned with low-dimensional subspaces. Interchange intervention accuracy (IIA) measures the proportion of aligned interventions with matching outputs, providing a graded measure of causal abstraction between neural networks and interpretable algorithms.

## Key Results
- Boundless DAS successfully identifies two interpretable boolean variables in Alpaca that check boundary conditions for numerical reasoning
- Interchange intervention accuracy reaches 0.86-0.95, demonstrating robust alignments across different inputs and contexts
- The method is efficient and scalable, enabling causal interpretability analysis of large language models that was previously computationally prohibitive

## Why This Works (Mechanism)

### Mechanism 1: Dimensionality Learning
- **Claim**: Boundless DAS learns the dimensionality of causal variable alignments instead of brute-forcing over possible dimensionalities, enabling efficient scaling to large models.
- **Mechanism**: Introduces learnable continuous boundary index parameters that define the dimensionality of orthogonal linear subspaces in the rotated vector space, optimized via gradient descent.
- **Core assumption**: Causal variables can be aligned with low-dimensional subspaces of neural representations that can be discovered through gradient-based optimization.
- **Evidence anchors**: Boundless DAS extension of DAS that learns dimensionality using neural PDE-inspired methods; weak corpus support focusing on benchmarking rather than dimensionality learning.
- **Break condition**: If causal variables require full-dimensional representations or gradient optimization fails to converge to meaningful subspaces.

### Mechanism 2: Weighted Interchange Interventions
- **Claim**: Weighted distributed interchange interventions enable gradual learning of causal alignments by interpolating between source inputs.
- **Mechanism**: Uses continuous sigmoid-based masks that weight contributions of each source input to the rotated space, gradually becoming binary during training.
- **Core assumption**: Causal alignments can be discovered through soft interventions that gradually become hard and can be optimized via gradient descent.
- **Evidence anchors**: Learnable boundary parameters with sigmoid masks; annealing of β throughout training; weak corpus support on weighted interchange interventions.
- **Break condition**: If annealing schedule is too fast/slow or sigmoid boundaries don't converge to meaningful values.

### Mechanism 3: Interchange Intervention Accuracy
- **Claim**: IIA provides a faithful measure of causal abstraction between neural networks and interpretable algorithms.
- **Mechanism**: Computes proportion of aligned interchange interventions on algorithm and neural network that have same output, bounded between 0.0 and 1.0.
- **Core assumption**: IIA is a valid measure of causal abstraction that can distinguish accurate from inaccurate hypotheses about neural network behavior.
- **Evidence anchors**: IIA definition as graded measure of abstraction; bounded between 0.0 and 1.0; weak corpus support on IIA as causal abstraction measure.
- **Break condition**: If IIA is not correlated with true causal structure or influenced by factors unrelated to hypothesized algorithm.

## Foundational Learning

- **Concept: Causal Abstraction**
  - Why needed here: Provides mathematical framework for claiming one model implements another
  - Quick check question: Can you explain the difference between observational and interventional methods in causal analysis?

- **Concept: Gradient Descent Optimization**
  - Why needed here: Used to optimize both rotation matrix and boundary parameters, crucial for method's efficiency
  - Quick check question: How does the orthogonal parameterization of rotation matrix affect optimization process?

- **Concept: Interchange Intervention**
  - Why needed here: Core operation used to test whether neural representations align with causal variables
  - Quick check question: What's the difference between standard intervention and interchange intervention in causal analysis?

## Architecture Onboarding

- **Component map**: Rotation matrix -> Boundary parameters -> Sigmoid masks -> Interchange intervention accuracy
- **Critical path**: Generate counterfactual data → Initialize rotation matrix and boundaries → Optimize via gradient descent → Evaluate IIA → Snap boundaries to binary → Report final alignment
- **Design tradeoffs**: Trades off exploration (learning boundaries) vs exploitation (optimizing rotation matrix); trades computational efficiency vs potential accuracy
- **Failure signatures**: Low IIA despite high task performance indicates incorrect hypothesized causal model; high IIA with low task performance suggests different mechanisms for different inputs; unstable boundary learning indicates optimization issues
- **First 3 experiments**:
  1. Run Boundless DAS on "Left Boundary" model with default price tagging task to verify basic functionality
  2. Test "Mid-point Distance" model to confirm it fails to find alignments, establishing IIA as discriminative metric
  3. Apply Boundless DAS to "Left and Right Boundary" model across different layers to identify where causal variables are most strongly represented

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can Boundless DAS be applied to find circuits in a model?
- **Basis in paper**: [explicit] "Can Boundless DAS be applied to find circuits in a model? Yes, it can."
- **Why unresolved**: Paper mentions Boundless DAS can find circuits but lacks detailed experimental results or case studies
- **What evidence would resolve it**: Experimental results demonstrating Boundless DAS successfully identifying circuits in various models, such as Transformer architectures

### Open Question 2
- **Question**: How well will the findings in the paper generalize to other reasoning tasks?
- **Basis in paper**: [inferred] Acknowledges limitations in testing tasks beyond Price Tagging game and mentions challenges finding other reasoning tasks solvable by current LLMs
- **Why unresolved**: Focuses on single task family without extensively exploring generalization to other reasoning tasks
- **What evidence would resolve it**: Applying Boundless DAS to diverse set of reasoning tasks and evaluating consistency and transferability of findings

### Open Question 3
- **Question**: Is it possible to apply Boundless DAS over the whole layer representation?
- **Basis in paper**: [explicit] "Is it possible to apply Boundless DAS over the whole layer representation? It is possible for smaller models if we are learning a full-rank square rotation matrix. It is impossible for Alpaca now."
- **Why unresolved**: Highlights current limitations without exploring potential future advancements or modifications to enable whole layer representation alignment
- **What evidence would resolve it**: Successful application to whole layer representations in larger models or proposed methods to overcome current computational constraints

### Open Question 4
- **Question**: Can Boundless DAS work with 175B models?
- **Basis in paper**: [explicit] "Can Boundless DAS work with 175B models? Yes, it does."
- **Why unresolved**: Confirms feasibility but lacks detailed results or challenges specific to extremely large models
- **What evidence would resolve it**: Detailed experimental results showing Boundless DAS applied to models like Bloom-176B, including performance metrics and resource requirements

### Open Question 5
- **Question**: How does the dimensionality of aligned causal variables affect accuracy and interpretability of Boundless DAS?
- **Basis in paper**: [inferred] Discusses boundary learning dynamics and shrinking of boundary width, suggesting relationship between dimensionality and alignment accuracy
- **Why unresolved**: Touches on dimensionality but doesn't deeply explore how varying dimensions impact effectiveness of alignment
- **What evidence would resolve it**: Systematic experiments varying dimensionality of causal variables and analyzing impact on alignment accuracy and interpretability

## Limitations
- Generalizability to more complex reasoning tasks and different model architectures remains untested
- Role of instruction tuning in shaping causal mechanisms is not addressed
- Whether discovered boolean variables represent complete mechanistic explanation is unclear

## Confidence
- **High Confidence**: Boundless DAS successfully learns dimensionality of causal alignments; IIA reliably distinguishes models implementing hypothesized causal structure; Alpaca implements two interpretable boolean variables
- **Medium Confidence**: Learned alignments are robust across different inputs/instructions/contexts; orthogonal rotation parameterization enables efficient optimization; sigmoid-based weighted interchange interventions enable gradual discovery
- **Low Confidence**: Method will scale to significantly larger models without substantial modifications; discovered causal variables represent complete mechanistic explanation; method generalizes to non-numerical reasoning tasks with similar performance

## Next Checks
1. **Cross-task validation**: Apply Boundless DAS to diverse reasoning tasks (logical, spatial) to test consistency of discovering interpretable causal structures
2. **Model size scaling**: Test Boundless DAS on progressively larger language models (13B, 30B, 70B parameters) to empirically determine performance scaling
3. **Ablation study on rotation parameterization**: Compare orthogonal rotation parameterization against unconstrained alternatives to quantify impact on optimization efficiency and alignment quality