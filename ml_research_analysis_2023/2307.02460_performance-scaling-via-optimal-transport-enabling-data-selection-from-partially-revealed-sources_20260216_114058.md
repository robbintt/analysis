---
ver: rpa2
title: 'Performance Scaling via Optimal Transport: Enabling Data Selection from Partially
  Revealed Sources'
arxiv_id: '2307.02460'
source_url: https://arxiv.org/abs/2307.02460
tags:
- data
- performance
- dval
- sources
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles data selection in partially observable settings
  where only pilot samples are available from each data source. It proposes a two-stage
  performance prediction framework that leverages Optimal Transport distance to map
  data distributions to model performance and a parameter-free projection technique
  for scaling to larger data sizes.
---

# Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources

## Quick Facts
- arXiv ID: 2307.02460
- Source URL: https://arxiv.org/abs/2307.02460
- Reference count: 40
- Key outcome: Outperforms existing methods in data selection from partially observable sources with <2% MAE on CIFAR-10 and up to 3% accuracy improvement on ImageNet-100

## Executive Summary
This paper addresses the challenge of selecting optimal data sources when only pilot samples are available from each source. The authors propose a two-stage performance prediction framework that uses Optimal Transport (OT) distance to map data distributions to model performance, combined with a parameter-free projection technique for scaling predictions to larger data sizes. The method enables efficient gradient-based optimization of data source composition and demonstrates superior accuracy and efficiency compared to existing approaches across vision and NLP tasks.

## Method Summary
The framework operates in two stages: First, it computes OT distances between pilot data mixtures and a validation set, then fits an affine or pseudo-quadratic model to predict performance at small scales. Second, it uses a parameter-free log-linear scaling relationship to project these predictions to target data sizes. The method enables efficient gradient-based optimization of data source composition using OT gradients, allowing selection of the optimal data mix without full-scale training. The approach is validated through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-100 datasets.

## Key Results
- Achieves under 2% mean absolute error (MAE) in performance prediction on CIFAR-10
- Improves model accuracy by up to 3% over baseline methods on ImageNet-100
- Demonstrates superior performance projection across various mixing ratios and scales
- Enables more effective data source selection compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal Transport distance between training and validation data distributions provides a performance proxy that is convex in data composition
- Mechanism: The OT distance upper bounds the difference between training and validation loss, and this bound is affine in the OT distance itself. Since OT distance is convex in data mixing ratios, the performance predictor built on this relationship is also convex.
- Core assumption: The Lipschitz constant of the model with respect to input distributions is approximately constant or can be estimated empirically
- Evidence anchors:
  - [abstract] "leveraging the Optimal Transport distance to predict the model's performance"
  - [section] "existing theoretical results require assumptions on the Lipschitz constant...the constant is rarely known in practice"

### Mechanism 2
- Claim: Two-stage prediction separates the composition-dependent and size-dependent performance scaling
- Mechanism: Stage 1 uses OT distance to predict performance at a fixed data scale for any composition. Stage 2 applies parameter-free log-linear scaling to project this prediction to different data sizes without retraining
- Core assumption: Performance scales log-linearly with data size at a given composition
- Evidence anchors:
  - [abstract] "extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique"
  - [section] "log-linear scaling relationship depending on both data size N and data composition p"

### Mechanism 3
- Claim: Gradient-based optimization of data source composition is efficient due to convexity and availability of OT gradients
- Mechanism: The performance predictor is convex in composition, and OT provides gradients with respect to probability mass, enabling fast gradient descent
- Core assumption: The OT solver provides calibrated gradients that ensure compositions remain valid probability distributions
- Evidence anchors:
  - [abstract] "efficient gradient-based method to select data sources"
  - [section] "we solve it iteratively with the following procedure...gradient update"

## Foundational Learning

- Concept: Optimal Transport distance as a metric between probability distributions
  - Why needed here: OT distance provides a principled way to measure distributional similarity that correlates with model performance
  - Quick check question: What properties make OT distance preferable to KL divergence for comparing data distributions?

- Concept: Neural scaling laws and their dependence on data composition
  - Why needed here: Understanding how performance scales with data size at different compositions is crucial for the projection stage
  - Quick check question: How does the scaling exponent typically vary with data quality or distribution?

- Concept: Convex optimization and gradient-based methods
  - Why needed here: The performance predictor is convex in composition, allowing efficient optimization to find the best data mix
  - Quick check question: Why is convexity important for ensuring the gradient-based optimization finds the global optimum?

## Architecture Onboarding

- Component map: Pilot data ingestion → OT distance computation → Stage 1 performance prediction → Stage 2 scaling projection → Gradient-based optimization → Data source selection

- Critical path:
  1. Compute OT distances between pilot mixtures and validation set
  2. Fit affine/nonlinear transformation to map OT distances to performance
  3. Use scaling laws to project performance to target data sizes
  4. Optimize composition via gradient descent using OT gradients

- Design tradeoffs:
  - Parameter efficiency vs. fitting accuracy (CS vs. PQ predictors)
  - Computational cost of OT computation vs. prediction accuracy
  - Number of pilot scales needed vs. projection accuracy

- Failure signatures:
  - Poor extrapolation when OT distance saturates
  - Overfitting in predictor fitting with too few training points
  - Gradient instability if OT solver gradients are inaccurate

- First 3 experiments:
  1. Verify OT distance correlates with performance on a simple dataset (e.g., MNIST with known distribution shifts)
  2. Test predictor fitting accuracy on held-out pilot mixtures
  3. Validate scaling projection accuracy by comparing predicted vs. actual performance at larger scales

## Open Questions the Paper Calls Out

- Question: How can the influence of validation data be quantified and potentially reduced in the performance prediction framework?
  - Basis in paper: [explicit] The paper assumes access to a validation set representing the target data distribution, but notes this may not always be available during data exchange and its quality may vary.
  - Why unresolved: The paper does not provide methods for quantifying or reducing the dependence on validation data in the performance prediction process.
  - What evidence would resolve it: Developing and evaluating methods to estimate or approximate the validation data's role in the performance prediction, or techniques to make predictions without relying on validation data.

- Question: How do estimation errors in the scaling law for data sizes affect the accuracy of performance predictions on larger scales?
  - Basis in paper: [explicit] The paper acknowledges that the framework could be vulnerable to estimation errors in the scaling law for data sizes, which could lead to magnified prediction errors on larger scales.
  - Why unresolved: The paper does not provide an analysis of how errors in the scaling law propagate through the performance prediction process or how significant these errors can be in practice.
  - What evidence would resolve it: Empirical studies or theoretical analysis quantifying the relationship between scaling law estimation errors and performance prediction accuracy on various scales.

- Question: How can the framework be extended to handle data sources with misaligned feature spaces?
  - Basis in paper: [inferred] The paper mentions that extending to data sources that are misaligned in feature space is an exciting direction, but does not explore this scenario.
  - Why unresolved: The paper focuses on cases where data sources are aligned in feature space, and does not address the challenges of combining or comparing data from sources with different feature representations.
  - What evidence would resolve it: Developing and evaluating methods for aligning or comparing data sources with different feature spaces, and demonstrating their effectiveness in the performance prediction framework.

## Limitations

- Data distribution assumptions: The approach assumes pilot samples adequately represent full source distributions, which may not hold when pilot samples are too small or unrepresentative.
- Scaling law validity: The log-linear scaling assumption may break down for extreme data sizes or when mixing sources with vastly different quality distributions.
- Computational constraints: OT computation requires solving optimization problems for each pilot mixture, with costs scaling with the number of sources and pilot samples.

## Confidence

**High confidence**: The core mechanism of using OT distance as a performance proxy and the two-stage prediction framework are well-supported by theoretical arguments and experimental validation. The convexity property enabling efficient gradient-based optimization is rigorously established.

**Medium confidence**: The empirical results demonstrating superior performance over baselines (2% MAE on CIFAR-10, 3% accuracy improvement on ImageNet-100) are convincing but based on specific datasets and models. Generalization to other domains requires further validation.

**Low confidence**: The claim about parameter-free scaling projection working across arbitrary mixing ratios and scales needs more scrutiny, particularly for extreme compositions or when sources have very different scaling behaviors.

## Next Checks

1. **Robustness to pilot sample size**: Systematically vary the number of pilot samples per source and measure how prediction accuracy degrades to quantify minimum viable pilot data requirements.

2. **Cross-domain generalization**: Apply the framework to non-vision domains (e.g., speech, time series) and compare performance against baselines to test universality of OT distance as a performance proxy.

3. **Stress test scaling extrapolation**: Evaluate prediction accuracy when projecting to data sizes 10x or 100x larger than the largest pilot scale to reveal limits of log-linear scaling assumptions.