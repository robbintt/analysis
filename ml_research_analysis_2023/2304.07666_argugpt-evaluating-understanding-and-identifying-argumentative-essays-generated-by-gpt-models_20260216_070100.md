---
ver: rpa2
title: 'ArguGPT: evaluating, understanding and identifying argumentative essays generated
  by GPT models'
arxiv_id: '2304.07666'
source_url: https://arxiv.org/abs/2304.07666
tags:
- essays
- human
- machine
- essay
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a balanced corpus of machine- and human-written
  argumentative essays, and performs human evaluation, linguistic analysis, and builds
  AIGC detectors to distinguish machine-generated texts from human-written ones. Results
  show that human evaluators have an accuracy of 61.6% in detecting machine-generated
  essays when first exposed to them, which rises to 67.7% after minimal self-training.
---

# ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models

## Quick Facts
- arXiv ID: 2304.07666
- Source URL: https://arxiv.org/abs/2304.07666
- Reference count: 40
- Key outcome: Human evaluators achieve 61.6% accuracy in detecting machine-generated essays initially, rising to 67.7% after minimal self-training, while ML classifiers achieve 99% essay-level and 93% sentence-level accuracy.

## Executive Summary
This paper presents ArguGPT, a balanced corpus of machine- and human-written argumentative essays, to evaluate and understand AI-generated content (AIGC) detection. The study systematically compares essays generated by seven GPT models with human essays across three proficiency levels, conducting human evaluation, linguistic analysis, and building ML classifiers. Results reveal that while human evaluators can detect machine-generated essays with moderate accuracy that improves with minimal training, ML classifiers achieve near-perfect detection rates. The analysis shows machines produce syntactically more complex sentences but human essays exhibit greater lexical richness.

## Method Summary
The study creates a balanced corpus by collecting human essays from WECCL, TOEFL, and GRE datasets, scoring them using the YouDao automated system, and generating machine essays using seven GPT models with standardized prompts. Human evaluation involves 43 English instructors assessing essays on a 6-point Likert scale. Linguistic analysis uses Lu's complexity measures to compare syntactic and lexical features. Machine learning classifiers (SVM and RoBERTa) are trained on the corpus to distinguish between human and machine-generated essays at both essay and sentence levels.

## Key Results
- Human evaluators achieve 61.6% accuracy in detecting machine-generated essays initially, improving to 67.7% after minimal self-training
- Machines produce syntactically more complex sentences while human essays show greater lexical complexity
- RoBERTa classifier achieves 99% accuracy at essay level and 93% at sentence level for detecting machine-generated content

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Balanced prompt pairing with human essay scores enables robust machine-essay generation and comparison.
- **Mechanism**: By collecting human essays from three proficiency levels (low, medium, high) and generating machine essays using the same prompts, the study creates a controlled corpus where linguistic features can be directly compared.
- **Core assumption**: Human essays accurately reflect the target proficiency levels and can be matched to machine essays via automated scoring systems.
- **Evidence anchors**:
  - [abstract] "machine-generated texts are paired with roughly equal number of human-written essays with three score levels matched in essay prompts."
  - [section 2.2] "To keep it balanced with the TOEFL subcorpus, we down-sample WECCL into 1,845 essays, ensuring the ratio of low:medium:high is 1:3:1."
  - [corpus] Weak: Automated scoring used to match levels but no validation against human raters provided.
- **Break condition**: If automated scoring misclassifies human essay levels, comparisons become biased.

### Mechanism 2
- **Claim**: GPT models produce essays with greater syntactic complexity but lower lexical richness than human writers.
- **Mechanism**: Comparative linguistic analysis using established L2 writing complexity measures shows that machine essays have longer T-units and more coordinating phrases, while human essays exhibit higher type-token ratios and more diverse verb usage.
- **Core assumption**: The chosen complexity metrics validly distinguish machine from human writing.
- **Evidence anchors**:
  - [abstract] "Machines produce syntactically more complex sentences while human essays tend to be lexically more complex."
  - [section 4.2.2] "text-davinci-003 and gpt-3.5-turbo produce syntactically more complex essays than even the high level English learners."
  - [section 4.2.3] "advanced learners exceed gpt-3.5-turbo in lexical richness measures."
- **Break condition**: If metric thresholds change or new models adopt human-like lexical patterns.

### Mechanism 3
- **Claim**: Machine-learning classifiers can easily distinguish machine-generated from human-written essays with high accuracy.
- **Mechanism**: Training SVMs and RoBERTa models on the balanced corpus achieves near-perfect classification by learning patterns in syntactic structure, function word usage, and surface linguistic features.
- **Core assumption**: The linguistic differences between machine and human essays are consistent enough for statistical models to capture.
- **Evidence anchors**:
  - [abstract] "a RoBERTa fine-tuned with the training set of ArguGPT achieves above 90% accuracy in both essay- and sentence-level classification."
  - [section 5.2] "RoBERTa easily achieves 99 accuracy in detecting AI generated essays."
  - [corpus] Strong: The corpus provides balanced, labeled data across multiple models and proficiency levels.
- **Break condition**: If models converge toward human-like writing patterns or if adversarial training is applied.

## Foundational Learning

- **Concept: Corpus balancing**
  - Why needed here: Ensures fair comparison between machine and human essays by matching prompts and proficiency levels.
  - Quick check question: What three proficiency levels are used to categorize human essays in this study?

- **Concept: Automated essay scoring**
  - Why needed here: Provides objective quality metrics to pair machine and human essays at comparable levels.
  - Quick check question: Which automated scoring system was selected for the ArguGPT corpus?

- **Concept: Syntactic vs lexical complexity metrics**
  - Why needed here: Distinguishes the different ways machines and humans express complexity in writing.
  - Quick check question: Which measure indicates syntactic complexity while another indicates lexical richness?

## Architecture Onboarding

- **Component map**: Corpus collection → Human evaluation pipeline → Linguistic analysis tools → ML classifier training → Detection system deployment
- **Critical path**: Prompt selection → Machine essay generation → Human essay scoring → Corpus balancing → Model training and evaluation
- **Design tradeoffs**: 
  - Using automated scoring speeds corpus creation but may introduce bias if scores don't align with human judgment.
  - Focusing on argumentative essays limits generalizability to other genres.
  - High classifier accuracy may not translate to real-world detection where prompts differ from training data.
- **Failure signatures**: 
  - Human evaluators unable to distinguish essays (accuracy near 50%) suggests insufficient feature differences.
  - Low classifier accuracy indicates corpus imbalance or insufficient feature capture.
  - Inconsistent linguistic patterns across models suggest prompt sensitivity issues.
- **First 3 experiments**:
  1. Test classifier performance when trained only on essays from a single GPT model.
  2. Evaluate whether adding more syntactic features improves human evaluator accuracy.
  3. Measure detection accuracy when classifiers are tested on essays generated with prompts outside the training set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the linguistic features of machine-generated essays evolve as more advanced GPT models are used?
- Basis in paper: [explicit] The paper discusses the linguistic analysis of essays generated by different GPT models (GPT2-XL, text-davinci-001, text-davinci-002, text-davinci-003, and gpt-3.5-turbo) and compares them to human essays. It finds that more advanced models produce essays with greater syntactic complexity but lower lexical richness compared to humans.
- Why unresolved: The paper does not provide a detailed analysis of how the linguistic features evolve across the different models, nor does it explore the reasons behind these changes.
- What evidence would resolve it: A longitudinal study tracking the linguistic features of essays generated by different GPT models over time, coupled with an analysis of the model architectures and training data, could provide insights into the evolution of linguistic features.

### Open Question 2
- Question: How does the performance of human evaluators in detecting machine-generated essays change with increased exposure and training?
- Basis in paper: [explicit] The paper mentions that human evaluators initially have a 61.6% accuracy in detecting machine-generated essays, which increases to 67.7% after minimal self-training. However, the paper does not explore the long-term effects of exposure and training on detection accuracy.
- Why unresolved: The paper only provides a snapshot of the evaluators' performance after one round of training. It does not explore how their accuracy might change with further exposure and training over time.
- What evidence would resolve it: A longitudinal study tracking the performance of human evaluators over multiple rounds of training and exposure to machine-generated essays could provide insights into the long-term effects of training on detection accuracy.

### Open Question 3
- Question: How effective are AIGC detectors in real-world scenarios, where the domain and style of the essays may vary significantly from the training data?
- Basis in paper: [inferred] The paper mentions that machine-learning classifiers can easily distinguish between machine-generated and human-authored essays with high accuracy. However, it does not explore the performance of these classifiers in real-world scenarios with diverse essay domains and styles.
- Why unresolved: The paper only tests the classifiers on the ArguGPT corpus, which is limited to argumentative essays. It does not explore how the classifiers perform on essays from different domains or with varying styles.
- What evidence would resolve it: Testing the classifiers on a diverse set of essays from different domains and styles, and comparing their performance to human evaluators, could provide insights into the effectiveness of AIGC detectors in real-world scenarios.

## Limitations

- Corpus relies on automated scoring systems without validation against human raters, potentially introducing bias in proficiency level matching
- Study focuses exclusively on argumentative essays, limiting generalizability to other writing genres
- High detection accuracy may not translate to real-world scenarios where AIGC detection prompts differ from training data

## Confidence

- **High confidence**: The observation that human evaluators show improved detection accuracy (61.6% to 67.7%) after minimal self-training is well-supported by the experimental design and results.
- **Medium confidence**: The linguistic complexity findings showing machines produce syntactically more complex but lexically simpler essays are plausible given the analysis methods, though the generalizability across different GPT models could vary.
- **Medium confidence**: The ML classifier performance claims are supported by the experimental results, but the near-perfect accuracy may be inflated due to the controlled corpus conditions.

## Next Checks

1. Conduct human evaluation with a separate set of prompts not used in the corpus to test real-world detection capabilities
2. Validate the YouDao automated scoring system against human raters to ensure accurate proficiency level matching between human and machine essays
3. Test classifier performance when trained on essays from only a single GPT model to assess model-specific versus general AIGC detection capabilities