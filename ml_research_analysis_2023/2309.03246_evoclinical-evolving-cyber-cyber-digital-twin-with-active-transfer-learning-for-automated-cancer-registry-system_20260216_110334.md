---
ver: rpa2
title: 'EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning
  for Automated Cancer Registry System'
arxiv_id: '2309.03246'
source_url: https://arxiv.org/abs/2309.03246
tags:
- cancer
- guri
- ccdt
- evoclinical
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes EvoCLINICAL, a method for evolving a cyber-cyber
  digital twin (CCDT) for an automated cancer registry system (GURI) using active
  transfer learning. EvoCLINICAL treats the CCDT from the previous version of GURI
  as a pretrained model and fine-tunes it with a dataset labelled by the new GURI
  version.
---

# EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System

## Quick Facts
- arXiv ID: 2309.03246
- Source URL: https://arxiv.org/abs/2309.03246
- Reference count: 40
- The paper proposes a method for evolving a cyber-cyber digital twin for automated cancer registry systems using active transfer learning

## Executive Summary
This paper presents EvoCLINICAL, a novel approach for evolving a cyber-cyber digital twin (CCDT) in automated cancer registry systems when validation rules change between versions. The method leverages transfer learning by treating the CCDT from the previous version as a pretrained model and fine-tuning it with a dataset labeled by the new version. To minimize the number of queries to the new system, EvoCLINICAL employs a genetic algorithm for active learning to select an optimal subset of cancer messages for labeling. The approach demonstrates significant improvements in precision, recall, and F1 scores compared to baseline methods.

## Method Summary
EvoCLINICAL treats the CCDT from the previous GURI version as a pretrained model and fine-tunes it with a dataset labeled by querying the new GURI version. The method employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset, minimizing the number of queries needed. A multi-input CNN architecture processes both categorical/numerical and string features, with separate branches for each type. The system trains new modules from scratch for any rules added in the target GURI version while fine-tuning the pretrained modules for existing rules.

## Key Results
- EvoCLINICAL achieves higher precision, recall, and F1 scores compared to baseline methods
- Active learning significantly improves performance compared to random selection of cancer messages
- The performance of EvoCLINICAL increases with candidate dataset size, though the growth rate decelerates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pretrained source CCDT transfers feature extraction knowledge to the target CCDT, reducing the need for large amounts of new labelled data.
- Mechanism: The source CCDT learns how to extract meaningful features from cancer message fields during its training. This learned feature extraction capability is transferred to the target CCDT through fine-tuning, enabling it to process new messages effectively with fewer new labels.
- Core assumption: The structure and semantics of cancer message fields remain consistent across GURI versions, allowing feature extraction knowledge to transfer.
- Evidence anchors:
  - [abstract] "EvoCLINICAL considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version."
  - [section 3.2.2] "We consider CCDT-S as the pretrained model for CCDT-T, sharing its trained parameters as ùëÉCCDT‚àíS's initialization."

### Mechanism 2
- Claim: Active learning selects the most valuable cancer messages for GURI to label, minimizing the number of queries needed while maintaining high-quality fine-tuning.
- Mechanism: EvoCLINICAL uses a genetic algorithm to optimize a subset of cancer messages based on five objectives: size minimization, diversity, result code diversity, false prediction proportion, and prediction uncertainty. This ensures that each query to GURI-T provides maximum information gain.
- Core assumption: Cancer messages that cause high prediction uncertainty or false predictions by the source CCDT are most informative for improving the target CCDT.
- Evidence anchors:
  - [abstract] "EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it."
  - [section 3.2.1] "We define the search space of all possible solutions as: ùëÉùëÜùëÜ‚Üíùëá = {ùëùùë†1, ùëùùë†2, ..., ùëùùë†ùëõùëùùë† }, where ùëõùëùùë† is the total number of possible solutions."

### Mechanism 3
- Claim: The multi-objective optimization balances multiple competing goals when selecting cancer messages for active learning.
- Mechanism: The genetic algorithm optimizes for multiple objectives simultaneously: minimizing dataset size, maximizing diversity, ensuring result code diversity, maximizing false prediction proportion, and maximizing prediction uncertainty. This multi-objective approach ensures a balanced selection that captures various aspects of the problem space.
- Core assumption: Optimizing for multiple objectives leads to a more representative and informative dataset than optimizing for a single objective.
- Evidence anchors:
  - [section 3.2.1] "We define five objectives and employ a genetic search algorithm (i.e., IBEA) to find an optimal subset of cancer messages."
  - [section 3.2.1] "Our optimization problem can be represented as follows: Given a set of cancer messages ùê∂ùëÄùëÜ‚Üíùëá with ùëõùëê being its size, find solution ùëùùë†ùëò ‚àà ùëÉùëÜùëÜ‚Üíùëá that satisfies five conditions."

## Foundational Learning

- Concept: Transfer learning in neural networks
  - Why needed here: Understanding how knowledge from one model (source CCDT) can be transferred to another (target CCDT) through parameter initialization and fine-tuning is crucial for implementing EvoCLINICAL.
  - Quick check question: What happens to the source CCDT's parameters during the fine-tuning process with the target dataset?

- Concept: Active learning strategies
  - Why needed here: Knowing how different active learning strategies (uncertainty sampling, diversity sampling, etc.) work is essential for understanding why EvoCLINICAL's multi-objective approach is effective.
  - Quick check question: How does uncertainty sampling differ from random sampling in selecting data points for labeling?

- Concept: Multi-objective optimization
  - Why needed here: Understanding how to balance competing objectives using algorithms like IBEA is necessary for implementing and potentially improving EvoCLINICAL's message selection process.
  - Quick check question: What is the advantage of using a multi-objective optimization algorithm over optimizing each objective separately?

## Architecture Onboarding

- Component map:
  - Feature preprocessing: One-hot encoding, USE embedding, min-max normalization
  - CNN-based module architecture: Two CNN branches (categorical/numerical and string) with fully connected layers
  - Multi-output classification: One module per validation rule
  - Transfer learning pipeline: Pretraining on source GURI, fine-tuning on actively selected target GURI data
  - Active learning selection: IBEA-based genetic algorithm with five optimization objectives

- Critical path:
  1. Preprocess cancer messages into vector representations
  2. Train source CCDT on labeled data from source GURI
  3. Use genetic algorithm to select valuable cancer messages from candidate dataset
  4. Query target GURI to label selected messages
  5. Fine-tune source CCDT with labeled messages to create target CCDT
  6. Evaluate target CCDT performance

- Design tradeoffs:
  - Dataset size vs. computational cost: Larger candidate datasets provide more options for active learning but increase computational overhead
  - Model complexity vs. training efficiency: More complex models may capture nuances better but require more data and training time
  - Active learning vs. random sampling: Active learning requires more complex selection algorithms but achieves better results with fewer queries

- Failure signatures:
  - Low precision/recall: Indicates poor feature extraction or insufficient training data
  - High false positive rate: Suggests overfitting to source GURI's validation rules
  - Poor diversity in selected messages: May indicate convergence to local optima in genetic algorithm
  - High uncertainty in predictions: Could indicate the model hasn't learned the new validation rules effectively

- First 3 experiments:
  1. Train source CCDT on a small subset of source GURI data and evaluate on a held-out validation set to verify basic functionality
  2. Implement the genetic algorithm with simplified objectives (just size and diversity) to verify message selection works before adding complexity
  3. Perform a baseline comparison between active learning and random sampling on a small dataset to demonstrate the value of active learning before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EvoCLINICAL compare to using active learning without transfer learning?
- Basis in paper: [inferred] The paper compares EvoCLINICAL to baselines using transfer learning without active learning (OTS) and active learning without transfer learning (EvoCLINICAL-RS). However, it does not directly compare active learning without transfer learning to random selection.
- Why unresolved: The paper does not present results comparing active learning without transfer learning to random selection.
- What evidence would resolve it: Experiments comparing the performance of active learning without transfer learning to random selection on the same dataset.

### Open Question 2
- Question: What is the impact of the candidate dataset size on the performance of EvoCLINICAL when the dataset size is very large (e.g., millions of cancer messages)?
- Basis in paper: [explicit] The paper shows that the performance of EvoCLINICAL increases as the candidate dataset size grows, but the growth rate decelerates. It does not explore the impact of very large dataset sizes.
- Why unresolved: The paper only experiments with candidate dataset sizes up to 8000.
- What evidence would resolve it: Experiments evaluating the performance of EvoCLINICAL on candidate datasets with millions of cancer messages.

### Open Question 3
- Question: How does the performance of EvoCLINICAL vary with different types of cancer messages (e.g., different cancer types or different stages of cancer)?
- Basis in paper: [inferred] The paper does not analyze the performance of EvoCLINICAL on different types of cancer messages.
- Why unresolved: The paper does not present results broken down by cancer type or stage.
- What evidence would resolve it: Experiments evaluating the performance of EvoCLINICAL on different subsets of the cancer message dataset based on cancer type or stage.

## Limitations
- The paper doesn't provide extensive details on the CNN architecture, which could impact reproducibility
- The IBEA parameters are mostly set to defaults, and the effectiveness of the five-objective optimization could be further validated with ablation studies
- The paper focuses on a specific medical domain (cancer registry), and the generalizability of the approach to other domains remains to be tested

## Confidence
- Transfer learning mechanism: High
- Active learning approach: Medium

## Next Checks
1. Perform an ablation study removing each of the five objectives from the genetic algorithm to quantify their individual contributions to performance
2. Test the approach on a different domain (e.g., another medical registry or a non-medical rule-based system) to assess generalizability
3. Implement the CNN architecture with variations in layer configurations to determine sensitivity to architectural choices and establish robustness