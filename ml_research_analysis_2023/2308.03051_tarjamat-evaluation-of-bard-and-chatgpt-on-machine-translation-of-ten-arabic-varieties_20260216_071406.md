---
ver: rpa2
title: 'TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic
  Varieties'
arxiv_id: '2308.03051'
source_url: https://arxiv.org/abs/2308.03051
tags:
- translation
- arabic
- chatgpt
- bard
- varieties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of large language
  models (LLMs), including Google Bard, OpenAI ChatGPT, and GPT-4, on machine translation
  tasks involving ten Arabic varieties. The study benchmarks these models against
  commercial systems like Google Translate and a supervised baseline, NLLB, using
  a newly created dataset.
---

# TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties

## Quick Facts
- arXiv ID: 2308.03051
- Source URL: https://arxiv.org/abs/2308.03051
- Reference count: 30
- Primary result: LLMs excel in translating Arabic dialects but lag behind commercial systems on Classical Arabic and Modern Standard Arabic.

## Executive Summary
This paper presents a comprehensive evaluation of large language models (LLMs), including Google Bard, OpenAI ChatGPT, and GPT-4, on machine translation tasks involving ten Arabic varieties. The study benchmarks these models against commercial systems like Google Translate and a supervised baseline, NLLB, using a newly created dataset. Results show that while LLMs excel in translating Arabic dialects, they lag behind commercial systems in translating Classical Arabic and Modern Standard Arabic. Bard, in particular, struggles with following human instructions, often providing translations in the wrong language or failing to produce any output. The study underscores the need for further development to enhance the inclusivity and reliability of LLMs in handling diverse linguistic and cultural contexts.

## Method Summary
The study evaluates large language models on machine translation tasks involving ten Arabic varieties using a manually created dataset of 200 sentences per variety. The models are compared against commercial systems like Google Translate and a supervised baseline, NLLB. Evaluation metrics include BLEU, ChrF++, ChrF, and TER scores. The study also investigates the LLMs' ability to follow human instructions in translation contexts through n-shot experiments (0, 1, 3, 5 shots).

## Key Results
- LLMs excel in translating Arabic dialects but lag behind commercial systems in translating Classical Arabic and Modern Standard Arabic.
- Bard struggles with following human instructions, often providing translations in the wrong language or failing to produce any output.
- Commercial MT systems outperform LLMs on high-resource varieties (CA/MSA) due to specialized training data and architecture optimization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs trained on web-scale data encode linguistic patterns across many Arabic varieties, enabling zero-shot translation performance even without explicit variety-specific fine-tuning.
- Mechanism: Pretraining on heterogeneous web text exposes models to code-switched, dialectal, and classical forms, allowing them to generalize to unseen varieties during inference.
- Core assumption: The pretraining corpus contains sufficient instances of low-resource varieties (e.g., Algerian, Mauritanian) for implicit learning.
- Evidence anchors:
  - [abstract] "LLMs may encounter challenges with dialects for which minimal public datasets exist, such as Algerian and Mauritanian dialects."
  - [section] "Our evaluation targets ten diverse varieties of Arabic... to benchmark the models on new test sets that we manually prepare for this work."
  - [corpus] Weak signal: corpus shows only 25 related papers, many with FMR < 0.6, indicating sparse literature on this niche.
- Break condition: Pretraining corpus lacks adequate dialectal coverage; model overfits to MSA/CA and underperforms on low-resource varieties.

### Mechanism 2
- Claim: Instruction-tuned LLMs can follow simple translation prompts, but their adherence varies by variety and prompt phrasing.
- Mechanism: Fine-tuning on instruction-response pairs teaches models to parse and execute user directives; effectiveness depends on prompt clarity and variety familiarity.
- Core assumption: The instruction-tuning data includes diverse translation instructions across languages and dialects.
- Evidence anchors:
  - [abstract] "our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts."
  - [section] "We leverage these contexts to carry out a human evaluation study investigating usefulness of the model, allowing us to reveal a number of limitations of Bard."
  - [corpus] No direct evidence; weak anchor from related work on prompting effectiveness.
- Break condition: Instruction-tuning dataset lacks variety-specific translation examples; model defaults to MSA or wrong target language.

### Mechanism 3
- Claim: Commercial MT systems outperform LLMs on high-resource varieties (CA/MSA) due to specialized training data and architecture optimization.
- Mechanism: Commercial systems use curated, high-quality parallel corpora and architecture tuned for specific language pairs, yielding superior translation quality for well-resourced varieties.
- Core assumption: Training data for commercial systems is cleaner, larger, and more focused on CA/MSA than web-crawled pretraining data.
- Evidence anchors:
  - [abstract] "On CA and MSA, instruction-tuned LLMs, however, trail behind commercial systems such as Google Translate."
  - [section] "Google Translate outperforms other commercial systems across all varieties except YEM."
  - [corpus] Weak signal: few citations to commercial MT literature in corpus.
- Break condition: Commercial systems lack dialectal data; performance degrades on low-resource varieties where LLMs excel.

## Foundational Learning

- Concept: Statistical Machine Translation (SMT) vs Neural Machine Translation (NMT)
  - Why needed here: Understanding the shift from phrase-based SMT to attention-based NMT explains why LLMs can generalize to low-resource varieties.
  - Quick check question: What key architectural change in NMT allows better handling of morphology and syntax compared to SMT?

- Concept: Prompt Engineering and Few-Shot Learning
  - Why needed here: LLMs rely on well-crafted prompts and in-context examples to perform translation tasks without explicit fine-tuning.
  - Quick check question: How does increasing the number of in-context examples (shots) typically affect translation quality in LLMs?

- Concept: Evaluation Metrics in MT (BLEU, ChrF++, TER)
  - Why needed here: Accurate assessment of translation quality requires understanding what each metric captures (n-gram overlap, character n-grams, edit distance).
  - Quick check question: Which metric would be most sensitive to character-level errors in Arabic script?

## Architecture Onboarding

- Component map: Data pipeline (curation → tokenization → model inference → metric computation) → human evaluation → error analysis.
- Critical path: Dataset preparation → prompt template selection → model inference (0/1/3/5-shot) → automatic metric computation → human error tagging → final analysis.
- Design tradeoffs: Manual dataset creation ensures no data leakage but is labor-intensive; using existing benchmarks risks contamination; human evaluation adds qualitative insights but introduces subjectivity.
- Failure signatures: Wrong target language (often MSA instead of English), no translation output, degeneration (repetitive output), content filtering (overly cautious filtering).
- First 3 experiments:
  1. Run a pilot with 100 MSA sentences using English vs Arabic prompts; compare BLEU scores to select prompt template.
  2. Test 0/1/3/5-shot settings on a subset of dialects; measure performance delta to confirm few-shot benefit.
  3. Compare LLM outputs against commercial systems (Google Translate) on CA/MSA to quantify relative performance gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific linguistic features that make Bard more effective at translating Classical Arabic and Modern Standard Arabic compared to dialects?
- Basis in paper: [explicit] The paper mentions that Bard performs better on Classical Arabic (CA) and Modern Standard Arabic (MSA) but does not specify the linguistic features contributing to this performance.
- Why unresolved: The paper does not provide a detailed linguistic analysis of the features that differentiate CA and MSA from dialects, which would help in understanding Bard's performance differences.
- What evidence would resolve it: A detailed linguistic analysis comparing the features of CA, MSA, and dialects, alongside Bard's performance metrics on these features, would help identify the specific elements that Bard handles better.

### Open Question 2
- Question: How does the lack of public data for certain Arabic dialects impact the performance of LLMs like Bard and ChatGPT in machine translation tasks?
- Basis in paper: [explicit] The paper indicates that LLMs encounter challenges with dialects for which minimal public datasets exist, such as Algerian and Mauritanian dialects.
- Why unresolved: The paper suggests a correlation between data availability and LLM performance but does not provide empirical evidence or detailed analysis of how data scarcity specifically affects translation quality.
- What evidence would resolve it: Empirical studies comparing LLM performance on dialects with varying levels of public data availability, coupled with analysis of how data scarcity influences translation accuracy, would provide insights into this issue.

### Open Question 3
- Question: What are the specific limitations of Bard's instruction-following capability in translation tasks, and how do these limitations manifest across different Arabic varieties?
- Basis in paper: [explicit] The paper notes that Bard struggles with following human instructions, often providing translations in the wrong language or failing to produce any output, but does not detail the specific limitations or their manifestations across different Arabic varieties.
- Why unresolved: The paper identifies the problem but does not explore the underlying causes or provide a comprehensive analysis of how these limitations vary across different Arabic varieties.
- What evidence would resolve it: A systematic study analyzing Bard's responses to translation prompts across all Arabic varieties, identifying common patterns of failure, and linking these to specific linguistic or contextual factors would clarify these limitations.

## Limitations
- Data Construction Transparency: The study lacks detailed disclosure of selection criteria, dialectal representativeness, and inter-annotator agreement for the manually curated dataset.
- Model Configuration Opacity: Specific model versions, temperature settings, or API configurations used during inference are not disclosed, preventing exact replication.
- Evaluation Metric Limitations: Reliance on automated metrics (BLEU, ChrF++, TER) may not adequately capture the morphological complexity and script-specific nuances of Arabic.

## Confidence
- High Confidence: Commercial MT systems outperform LLMs on CA/MSA translation quality.
- Medium Confidence: LLMs demonstrate superior performance on low-resource Arabic dialects compared to commercial systems.
- Low Confidence: Bard's specific instruction-following failures are accurately characterized.

## Next Checks
1. **Dataset Representativeness Audit**: Conduct a dialectal coverage analysis of the evaluation dataset by comparing sentence distributions across regions and verifying against independent dialectal corpora to ensure the 200-sentence samples adequately represent each variety's linguistic diversity.
2. **Metric Correlation Study**: Perform a human evaluation study on a subset of translations to compute correlation coefficients between automatic metrics (BLEU, ChrF++) and human quality assessments, particularly focusing on dialectal varieties where metric validity is most questionable.
3. **Prompt Ablation Experiment**: Systematically vary prompt formats, temperature settings, and shot configurations for each LLM to isolate whether performance differences stem from model capabilities or experimental design choices, with particular attention to Bard's instruction-following behavior.