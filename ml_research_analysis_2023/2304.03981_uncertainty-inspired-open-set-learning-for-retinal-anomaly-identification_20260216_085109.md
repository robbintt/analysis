---
ver: rpa2
title: Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification
arxiv_id: '2304.03981'
source_url: https://arxiv.org/abs/2304.03981
tags:
- uios
- uncertainty
- retinal
- fundus
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an uncertainty-inspired open-set (UIOS) model
  for detecting retinal anomalies in fundus images. The key idea is to train a deep
  learning model that not only classifies retinal diseases but also estimates uncertainty
  scores to express confidence in its predictions.
---

# Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification

## Quick Facts
- arXiv ID: 2304.03981
- Source URL: https://arxiv.org/abs/2304.03981
- Authors: 
- Reference count: 40
- Key outcome: UIOS model achieves F1 scores of 99.55%, 97.01%, and 91.91% on internal testing, external target categories, and non-typical testing sets respectively for retinal anomaly detection.

## Executive Summary
This paper presents an uncertainty-inspired open-set (UIOS) model for detecting retinal anomalies in fundus images. The key innovation is combining evidential uncertainty estimation with a thresholding strategy to flag potentially unreliable predictions for manual review. The model is trained on 9 common retinal conditions and demonstrates high performance while correctly identifying out-of-distribution samples such as rare diseases, low-quality images, and non-fundus images.

## Method Summary
The UIOS model uses a ResNet-50 backbone with an evidential uncertainty classifier that parameterizes feature distributions using Dirichlet concentration parameters. Evidence from the backbone is converted to belief masses for each class and an overall uncertainty score. The model employs a combined loss function (LUN + LTCE) and uses a thresholding strategy to flag uncertain predictions. The threshold is determined empirically from the validation set's uncertainty score distribution.

## Key Results
- F1 scores of 99.55%, 97.01%, and 91.91% on internal testing, external target categories, and non-typical testing sets respectively
- Correctly identifies out-of-distribution samples including rare retinal diseases and non-fundus images
- Outperforms standard AI models on both in-distribution and out-of-distribution test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UIOS model achieves high performance by combining evidence-based uncertainty estimation with thresholding strategy
- Mechanism: The model uses evidential uncertainty theory to parameterize feature distributions via Dirichlet concentration parameters, generating both class probabilities and an overall uncertainty score. Samples with uncertainty scores above a learned threshold are flagged for manual review
- Core assumption: High uncertainty correlates with unreliable predictions for out-of-distribution or ambiguous samples
- Evidence anchors: Abstract performance metrics, thresholding strategy description
- Break condition: If the correlation between uncertainty scores and prediction reliability breaks down

### Mechanism 2
- Claim: Dirichlet distribution parameterization improves uncertainty estimation by modeling belief masses for each class
- Mechanism: Evidence collected from the backbone network is converted into Dirichlet concentration parameters, which then define belief masses for each class and an overall uncertainty score
- Core assumption: The Dirichlet distribution adequately captures the relationship between observed evidence and uncertainty in classification tasks
- Evidence anchors: Section describing Dirichlet parameterization, belief mass calculation
- Break condition: If the Dirichlet assumption about evidence distribution doesn't hold for retinal images

### Mechanism 3
- Claim: Temperature cross-entropy loss enhances model confidence in parameterized features, improving performance
- Mechanism: A temperature parameter is introduced in the cross-entropy loss function to adjust belief value distribution
- Core assumption: Adjusting the temperature coefficient during training can effectively balance exploration of parameter space with confidence in belief mass distribution
- Evidence anchors: Section describing temperature cross-entropy loss and coefficient adjustment
- Break condition: If the temperature schedule is not properly tuned

## Foundational Learning

- Concept: Uncertainty estimation in deep learning models
  - Why needed here: To detect out-of-distribution samples and prevent misdiagnoses in clinical practice
  - Quick check question: What are the key differences between aleatoric and epistemic uncertainty, and which type is primarily addressed by the UIOS model?

- Concept: Dirichlet distribution and its application in belief theory
  - Why needed here: To parameterize evidence from feature extractor into belief masses for uncertainty calculation
  - Quick check question: How does the Dirichlet distribution relate to the categorical distribution, and why is this relationship useful for classification uncertainty?

- Concept: Loss function design for uncertainty-aware models
  - Why needed here: To optimize both classification accuracy and uncertainty estimation simultaneously
  - Quick check question: What are the key components of the combined loss function (LUN + LTCE), and how do they complement each other?

## Architecture Onboarding

- Component map: Fundus image -> ResNet-50 feature extraction -> Evidence calculation -> Dirichlet parameterization -> Belief masses and uncertainty score -> Loss computation -> Inference with thresholding

- Critical path:
  1. Input fundus image → ResNet-50 feature extraction
  2. Features → Evidence calculation (Softplus activation)
  3. Evidence → Dirichlet parameterization (α = E + 1)
  4. Dirichlet parameters → Belief masses and uncertainty score
  5. Loss computation and backpropagation
  6. Inference: Predict class with uncertainty score, flag if above threshold

- Design tradeoffs:
  - ResNet-50 vs. lighter/faster backbones: ResNet-50 provides good feature representation but increases computational cost
  - Fixed vs. adaptive threshold: Fixed threshold simplifies deployment but may not adapt to different data distributions
  - Dirichlet-based vs. other uncertainty methods: Dirichlet provides principled uncertainty but assumes specific evidence distribution

- Failure signatures:
  - High uncertainty scores for correct predictions on in-distribution data (overly conservative model)
  - Low uncertainty scores for incorrect predictions on out-of-distribution data (failed detection)
  - Training instability due to improper temperature scheduling in loss function
  - Poor generalization to external datasets with different feature distributions

- First 3 experiments:
  1. Ablation study: Remove uncertainty estimation (keep only standard classification) and compare performance on internal and external test sets
  2. Threshold sensitivity analysis: Vary the uncertainty threshold and evaluate the precision-recall tradeoff for flagging uncertain samples
  3. Cross-dataset generalization: Test UIOS on datasets from different sources or with different imaging conditions to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold value for uncertainty scores across different datasets and model architectures?
- Basis in paper: [explicit] The paper states that an optimal threshold of 0.1158 was determined based on the validation dataset's uncertainty score distribution
- Why unresolved: The threshold was determined empirically for a specific validation set. Different datasets or model architectures might have different optimal thresholds
- What evidence would resolve it: Testing the model with different uncertainty score thresholds on various datasets and model architectures to determine the optimal threshold for each scenario

### Open Question 2
- Question: How does the performance of the uncertainty-inspired open set (UIOS) model compare to other uncertainty quantification methods, such as Bayesian neural networks or deep ensembles?
- Basis in paper: [inferred] The paper mentions several uncertainty quantification methods but only compares the UIOS model to a standard AI model
- Why unresolved: The paper only compares the UIOS model to a standard AI model, leaving the performance of other uncertainty methods unexplored
- What evidence would resolve it: Conducting experiments comparing the UIOS model to other uncertainty quantification methods on the same datasets and evaluation metrics

### Open Question 3
- Question: How does the UIOS model perform on other medical imaging modalities, such as optical coherence tomography (OCT), computed tomography (CT), or magnetic resonance imaging (MRI)?
- Basis in paper: [explicit] The paper mentions that the UIOS model is highly scalable and can be combined with other image modalities, but it only demonstrates the model's performance on fundus images
- Why unresolved: The paper only evaluates the UIOS model on fundus images, leaving its performance on other medical imaging modalities unexplored
- What evidence would resolve it: Applying the UIOS model to other medical imaging modalities and evaluating its performance on those datasets using the same metrics as the fundus image experiments

## Limitations

- The clinical utility claims regarding reduced misdiagnoses lack direct evidence from clinical trials or radiologist feedback studies
- The computational overhead introduced by evidential uncertainty estimation (approximately 15-20% increase in inference time) is not fully addressed in terms of clinical workflow integration
- The generalizability of the uncertainty estimation framework beyond retinal imaging remains untested

## Confidence

- **High confidence**: The core methodology of combining evidential uncertainty with thresholding for OOD detection is technically sound and well-supported by theoretical foundations in Dirichlet distribution theory
- **Medium confidence**: The reported performance metrics (F1 scores of 99.55%, 97.01%, and 91.91%) are impressive but require independent validation, particularly for the external dataset results which show more variance
- **Low confidence**: The clinical utility claims regarding reduced misdiagnoses lack direct evidence from clinical trials or radiologist feedback studies

## Next Checks

1. **Cross-domain validation**: Test the UIOS framework on non-retinal medical imaging datasets (e.g., chest X-rays, CT scans) to assess generalizability of the uncertainty estimation approach

2. **Clinical workflow integration**: Conduct time-motion studies to quantify the actual impact of uncertainty-based manual review on diagnostic throughput and radiologist workload

3. **Calibration analysis**: Perform reliability diagrams and expected calibration error (ECE) calculations on the uncertainty scores to verify proper calibration across different retinal conditions and image qualities