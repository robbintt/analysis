---
ver: rpa2
title: Self-Supervised Pre-Training with Contrastive and Masked Autoencoder Methods
  for Dealing with Small Datasets in Deep Learning for Medical Imaging
arxiv_id: '2308.06534'
source_url: https://arxiv.org/abs/2308.06534
tags:
- pre-training
- learning
- downstream
- methods
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates self-supervised pre-training methods for convolutional
  neural networks on CT images, focusing on small annotated datasets. The authors
  compare state-of-the-art contrastive learning methods (BYOL, MoCo V2, SwAV) with
  the masked autoencoder method SparK.
---

# Self-Supervised Pre-Training with Contrastive and Masked Autoencoder Methods for Dealing with Small Datasets in Deep Learning for Medical Imaging

## Quick Facts
- arXiv ID: 2308.06534
- Source URL: https://arxiv.org/abs/2308.06534
- Authors: 
- Reference count: 40
- Primary result: SparK outperforms contrastive methods (BYOL, MoCo V2, SwAV) for medical imaging classification tasks with small datasets

## Executive Summary
This study evaluates self-supervised pre-training methods for convolutional neural networks on CT images, focusing on small annotated datasets. The authors compare state-of-the-art contrastive learning methods (BYOL, MoCo V2, SwAV) with the masked autoencoder method SparK. They pre-train models on a large unannotated CT dataset and fine-tune on three CT classification tasks with varying dataset sizes. The results show that SparK is more robust to dataset reduction compared to contrastive methods, especially for tasks with less than 100-150 samples per class. SparK outperformed all other methods on the smallest dataset (Brain task) and maintained performance better than contrastive methods when reducing the training data.

## Method Summary
The study uses ResNet50 as the base architecture and evaluates four self-supervised pre-training methods: BYOL, MoCo V2, SwAV, and SparK. Models are pre-trained on the LIDC-IDRI dataset (244,527 CT slices) without labels, then fine-tuned on three downstream classification tasks (COVID-19, OrganSMNIST, Brain) with varying dataset sizes. Performance is evaluated using 5-fold cross-validation with metrics including AUC, F1 score, and accuracy. The authors systematically reduce dataset sizes to test method robustness, using reductions of 25%, 50%, 75%, and 90% of the original training data.

## Key Results
- SparK outperformed all other methods on the smallest dataset (Brain task) with less than 100 samples per class
- SparK maintained better performance than contrastive methods when reducing dataset sizes across all tasks
- The authors propose having at least 60 images per class for decent results with self-supervised pre-training
- Contrastive methods showed similar performance to each other but were less robust to dataset reduction than SparK

## Why This Works (Mechanism)

### Mechanism 1
SparK's sparse convolution design preserves mask patterns better than standard convolutions in masked autoencoders. Standard convolutions in CNNs lose mask patterns through overlapping sliding windows. SparK uses submanifold sparse convolutions that only compute when the center of a sliding window kernel is covered by a non-masked patch, preventing information from masked regions from leaking into unmasked regions.

### Mechanism 2
SparK is more robust to dataset reduction because it focuses on local relations rather than global contrastive pairs. Masked autoencoders learn by reconstructing masked patches, emphasizing local spatial relationships within images. This local focus is more stable when fine-tuning data is scarce, compared to contrastive methods that learn from global relationships between different images.

### Mechanism 3
The hierarchical encoding and decoding structure in SparK provides multi-scale feature learning that benefits downstream tasks. SparK uses a U-Net style decoder with three blocks of upsampling layers receiving feature maps from the encoder in four different resolutions. This hierarchical structure captures both fine-grained and coarse-grained features that are useful for diverse medical imaging tasks.

## Foundational Learning

- Concept: Self-supervised learning
  - Why needed here: Medical imaging datasets are often small and expensive to annotate, making supervised learning difficult. Self-supervised learning allows models to learn useful representations from unlabeled data before fine-tuning on small annotated datasets.
  - Quick check question: What is the main advantage of self-supervised pre-training for medical imaging tasks with small annotated datasets?

- Concept: Contrastive learning
  - Why needed here: Contrastive methods are popular for self-supervised pre-training in medical imaging, learning to distinguish between positive pairs (different views of the same image) and negative pairs (views from different images).
  - Quick check question: How do contrastive methods create positive and negative pairs from unlabeled images?

- Concept: Masked autoencoders
  - Why needed here: Masked autoencoders learn by reconstructing masked portions of images, which is particularly useful when adapted for CNNs through sparse convolutions as in SparK.
  - Quick check question: What is the key challenge in applying masked autoencoders to convolutional neural networks?

## Architecture Onboarding

- Component map: Large unlabeled CT dataset -> self-supervised method (BYOL, MoCo V2, SwAV, or SparK) -> pre-trained encoder -> fine-tuning stage -> task-specific model
- Critical path: The most critical path is the pre-training -> fine-tuning pipeline, particularly the choice of pre-training method and the number of samples per class in the downstream dataset.
- Design tradeoffs:
  - Computational cost vs. performance: SparK requires more complex sparse convolutions but shows better performance on small datasets
  - Batch size requirements: MoCo and BYOL are batch-size independent, while SwAV has lower but not eliminated batch-size requirements
  - Model complexity: Hierarchical encoding/decoding in SparK adds complexity but provides multi-scale features
- Failure signatures:
  - If F1 scores drop below 0.7 on test sets, the dataset is too small for effective fine-tuning regardless of pre-training method
  - If contrastive methods show rapid performance degradation with dataset reduction while SparK remains stable, this indicates SparK's advantage in small-data scenarios
  - If performance is consistently poor across all methods, the pre-training dataset may not be sufficiently representative
- First 3 experiments:
  1. Replicate the baseline results: Train ResNet50 from scratch on each downstream task without pre-training to establish baseline performance
  2. Compare all four pre-training methods (BYOL, MoCo V2, SwAV, SparK) on the complete downstream datasets to identify relative strengths
  3. Perform systematic dataset reduction on each downstream task (25%, 50%, 75% reduction steps) to evaluate robustness to dataset size across pre-training methods

## Open Questions the Paper Calls Out

### Open Question 1
How does SparK's performance compare to contrastive methods when pre-training on non-CT medical imaging modalities like MRI? The authors note that "it would be interesting to evaluate SparK on further pre-training datasets and other downstream tasks such as segmentation or object detection" and specifically mention MRI as a potential area for investigation.

### Open Question 2
What is the optimal mask ratio for SparK when applied to CT images, and how does this affect performance on downstream tasks? The authors state they use a mask ratio of 60% based on the original SparK paper, but do not investigate the impact of different mask ratios on CT data.

### Open Question 3
Why does SparK show greater robustness to dataset reduction compared to contrastive methods, particularly for small annotated datasets? The authors note that "masked autoencoders like SparK focus more on learning local relations in the images to perform the reconstruction task, while contrastive learning focuses more on the relationship between different images" and hypothesize this may explain SparK's performance.

## Limitations
- The study's findings are limited by the specific dataset composition, with LIDC-IDRI containing predominantly lung CT scans that may bias representations toward thoracic anatomy
- The downstream tasks represent a relatively small sample of possible medical imaging classification problems
- Results may not directly translate to other modalities like MRI or X-ray without further validation
- The study focuses exclusively on CT imaging, limiting generalizability to other medical imaging modalities

## Confidence
- High confidence in SparK's superior performance on the Brain task with very small datasets (<100 samples/class)
- Medium confidence in the relative ranking of methods across all tasks, as differences between contrastive methods are relatively small
- Medium confidence in the proposed threshold of 60 images per class for decent results, as this is based on limited task diversity
- Low confidence in the generalizability of these results to non-CT imaging modalities or tasks with different visual characteristics

## Next Checks
1. Test the same pre-training methods on a different imaging modality (MRI or X-ray) to evaluate cross-modality generalization of SparK's advantages
2. Evaluate performance with datasets containing between 30-60 samples per class to more precisely identify the minimum effective sample size threshold
3. Compare against supervised pre-training using ImageNet or other large natural image datasets to determine whether SparK's advantage persists when high-quality labeled data is available