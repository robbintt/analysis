---
ver: rpa2
title: Pure Exploration under Mediators' Feedback
arxiv_id: '2308.15552'
source_url: https://arxiv.org/abs/2308.15552
tags:
- mediators
- holds
- which
- lemma
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces best-arm identification under mediators' feedback,
  where a learner sequentially chooses among mediators to query on their behalf to
  identify the optimal arm with high probability. The authors derive a lower bound
  on the sample complexity and propose an algorithm based on Track and Stop that matches
  this bound asymptotically for both known and unknown mediator policies.
---

# Pure Exploration under Mediators' Feedback

## Quick Facts
- arXiv ID: 2308.15552
- Source URL: https://arxiv.org/abs/2308.15552
- Authors: 
- Reference count: 40
- Key outcome: Best-arm identification under mediators' feedback with asymptotic optimality guarantees

## Executive Summary
This work introduces a novel setting for best-arm identification (BAI) in stochastic multi-armed bandits where a learner queries mediators who then sample arms on the learner's behalf. The authors establish a sample complexity lower bound and propose an algorithm based on Track and Stop that matches this bound asymptotically. The approach decouples arm selection from reward observation, with mediators acting as stochastic policy selectors. The framework generalizes classical BAI and provides insights into how indirect feedback affects statistical complexity.

## Method Summary
The method adapts the Track and Stop algorithm to identify optimal mediator proportions rather than directly tracking arm pulls. For known mediator policies, the algorithm tracks oracle weights that minimize the sample complexity. When policies are unknown, the algorithm simultaneously estimates them while tracking optimal proportions. The approach uses a GLR statistic for stopping and forced exploration to ensure accurate estimation. The key innovation is replacing direct arm sampling with mediator selection, where each mediator samples arms according to its stochastic policy.

## Key Results
- Asymptotic optimality: The proposed algorithm achieves the lower bound sample complexity for both known and unknown mediator policies
- Known policy advantage: When mediator policies are known, the algorithm can avoid redundant queries and improve performance
- Generalization: The mediator feedback framework recovers classical BAI when each mediator directly samples a unique arm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mediator feedback setting generalizes classical best-arm identification by decoupling arm pulls from agent decisions.
- Mechanism: Instead of the agent directly selecting arms, it selects mediators who then sample arms according to their stochastic policies. This decouples the action selection from the reward observation process.
- Core assumption: Each arm must be reachable by at least one mediator with positive probability (Assumption 1).
- Evidence anchors:
  - [abstract] "we consider the scenario in which the learner has access to a set of mediators, each of which selects the arms on the agent's behalf according to a stochastic and possibly unknown policy."
  - [section] "Whenever (i) the mediators' policies are known, (ii) E = K and, (iii) for all action a ∈ [K], πEa is a Dirac distribution on action a, we recover the usual best-arm identification problem."

### Mechanism 2
- Claim: The statistical complexity of mediator feedback BAI is characterized by a max-min game over mediator proportions and alternative bandit models.
- Mechanism: The lower bound involves choosing mediator sampling proportions (first player) to minimize the maximum difficulty of identifying the optimal arm across alternative models (second player). This mirrors classical BAI complexity but restricted to achievable arm distributions via mediators.
- Core assumption: The KL divergence between reward distributions provides the appropriate divergence measure for the lower bound.
- Evidence anchors:
  - [section] "T ∗(µ, π)− 1 can be seen as a max-min game where the first player chooses a pull proportion among the different arms, and the second player chooses a hard-to-identify alternative problem where the optimal arm is different"
  - [section] "T ∗(µ, π)− 1 can be rewritten as: sup ˜π∈ ˜ΣK inf λ∈ Alt(µ) K∑ a=1 ˜πad(µ a, λ a)"

### Mechanism 3
- Claim: The Track and Stop algorithm can be adapted to mediator feedback by tracking optimal mediator proportions rather than arm proportions.
- Mechanism: Instead of tracking which arms to pull, the algorithm tracks which mediators to query based on their contribution to arm identification. The C-tracking mechanism ensures the empirical arm distribution converges to the optimal one achievable through mediators.
- Core assumption: The oracle weights ω∗(µ, π) are convex and the mapping (µ, π) → ω∗(µ, π) is upper hemicontinuous.
- Evidence anchors:
  - [section] "we adopt C-tracking of the oracle mediator proportions ω∗ (µ, π)"
  - [section] "Proposition 2 will play a crucial role for the analysis of our algorithmic solution"

## Foundational Learning

- Concept: KL divergence and exponential family distributions
  - Why needed here: The lower bound derivation and statistical analysis rely on KL divergence between reward distributions, which is well-defined for exponential families.
  - Quick check question: Can you explain why KL divergence is the appropriate measure for distinguishing between bandit models in the fixed-confidence setting?

- Concept: Max-min optimization and game-theoretic interpretation
  - Why needed here: The sample complexity lower bound is derived from a max-min game between choosing sampling proportions and choosing alternative hard instances.
  - Quick check question: How does the max-min structure of the lower bound relate to the minimax theorem in game theory?

- Concept: Track and Stop algorithm and forced exploration
  - Why needed here: The algorithm needs to track optimal mediator proportions while ensuring sufficient exploration to accurately estimate arm means and the optimal proportions themselves.
  - Quick check question: What role does the forced exploration component play in ensuring the algorithm's asymptotic optimality?

## Architecture Onboarding

- Component map: Mediator selection -> Arm pull via mediator -> Reward observation -> Update estimates -> Check stopping condition -> (if not stopped) repeat
- Critical path: Mediator selection → Arm pull via mediator → Reward observation → Update estimates → Check stopping condition → (if not stopped) repeat
- Design tradeoffs:
  - Known vs. unknown mediator policies: Known policies allow direct tracking of optimal proportions; unknown policies require simultaneous estimation and tracking
  - Exploration vs. exploitation: Need to balance forced exploration for accurate estimation against exploitation of current best estimates
  - Computational complexity: Computing optimal mediator proportions may be expensive for large numbers of mediators
- Failure signatures:
  - Poor convergence to optimal mediator proportions: Could indicate issues with the tracking algorithm or insufficient exploration
  - High sample complexity compared to theoretical bounds: May suggest suboptimal implementation of the stopping rule or estimation errors
  - Inability to identify the optimal arm within reasonable time: Could indicate that Assumption 1 is violated or that the mediator policies are too restrictive
- First 3 experiments:
  1. Verify the algorithm correctly identifies the optimal arm in a simple case with known policies where one mediator directly samples each arm
  2. Test the algorithm with unknown policies where the true policies need to be learned from data
  3. Compare performance against uniform sampling baseline to demonstrate the benefit of the mediator feedback approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the statistical complexity of best-arm identification under mediators' feedback change in the moderate confidence regime (δ not tending to zero)?
- Basis in paper: [inferred] The paper focuses on the asymptotic regime δ → 0 and derives optimal sample complexity bounds. The authors explicitly state that investigating the moderate regime is an open direction.
- Why unresolved: The analysis in the paper relies heavily on asymptotic techniques that may not extend to finite δ values.
- What evidence would resolve it: Theoretical bounds on sample complexity for finite δ values, or empirical studies showing how performance scales with δ.

### Open Question 2
- Question: How can the mediators' feedback framework be extended to handle time-varying mediator policies?
- Basis in paper: [explicit] The authors mention that extending the formulation to cases where mediator policies evolve over time is a captivating avenue for future research.
- Why unresolved: The current analysis assumes fixed mediator policies, and time-varying policies would require new theoretical tools to handle the non-stationarity.
- What evidence would resolve it: A formal extension of the lower bound and algorithm to the time-varying case, with theoretical guarantees on sample complexity.

### Open Question 3
- Question: What is the impact of redundant mediators (identical policies) on the sample complexity, and how can the algorithm be modified to account for this?
- Basis in paper: [explicit] The authors discuss how knowing the mediator policies allows the algorithm to avoid querying identical mediators, leading to performance gains. They also show experimental evidence of this effect.
- Why unresolved: While the theoretical analysis assumes distinct mediators, the practical impact of redundant mediators and how to optimally handle them is not fully explored.
- What evidence would resolve it: A theoretical analysis of the impact of redundant mediators on sample complexity, and an algorithm modification that provably accounts for this redundancy.

## Limitations
- Asymptotic results: The theoretical guarantees are only valid as δ → 0, limiting applicability to finite confidence regimes
- Restricted reward distributions: The analysis assumes bounded KL divergence, which may not hold for all practical reward distributions
- Limited experimental scope: Validation is primarily on Gaussian bandits with small problem sizes

## Confidence
- High: The asymptotic optimality of the Track and Stop algorithm (Theorem 2) is well-supported by the theoretical analysis and the relationship to classical BAI results
- Medium: The sample complexity lower bound (Theorem 1) is derived rigorously, but its tightness depends on the validity of the max-min game interpretation and the convex relaxation
- Medium: The experimental results demonstrate the approach works in controlled settings, but the limited scope (Gaussian rewards, small problem sizes) reduces confidence in broader applicability

## Next Checks
1. Stress test the mediator policy assumptions: Systematically vary mediator policies to include cases where Assumption 1 is barely satisfied, and measure the impact on sample complexity and convergence
2. Test with non-Gaussian reward distributions: Evaluate the algorithm with Bernoulli or other exponential family distributions to verify the KL divergence-based analysis holds beyond the Gaussian case
3. Scale to larger problem instances: Test with K=20+ arms and E=10+ mediators to identify computational bottlenecks in tracking optimal mediator proportions and to validate the asymptotic analysis at realistic problem scales