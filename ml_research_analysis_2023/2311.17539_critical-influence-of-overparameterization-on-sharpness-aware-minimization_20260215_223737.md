---
ver: rpa2
title: Critical Influence of Overparameterization on Sharpness-aware Minimization
arxiv_id: '2311.17539'
source_url: https://arxiv.org/abs/2311.17539
tags:
- parameters
- accuracy
- overparameterization
- generalization
- minima
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how overparameterization affects sharpness-aware
  minimization (SAM). The authors prove that SAM converges linearly under overparameterization
  and show that linearly stable minima found by SAM are flatter and have more uniformly
  distributed Hessian moments than those found by SGD.
---

# Critical Influence of Overparameterization on Sharpness-aware Minimization

## Quick Facts
- arXiv ID: 2311.17539
- Source URL: https://arxiv.org/abs/2311.17539
- Reference count: 40
- Key outcome: Overparameterization enables linear convergence of SAM and amplifies its generalization advantage through selection of flatter, more uniform minima

## Executive Summary
This paper establishes that overparameterization is critical for Sharpness-Aware Minimization (SAM)'s effectiveness. The authors prove that SAM achieves linear convergence under overparameterization by satisfying the Polyak-Lojasiewicz condition, and show through linear stability analysis that SAM's minima are flatter and have more uniformly distributed Hessian moments than those found by SGD. Experiments confirm that the generalization improvement of SAM increases with model size, and propose sparsification as a practical way to achieve overparameterization without full computational cost.

## Method Summary
The paper analyzes SAM's behavior under varying degrees of overparameterization through theoretical convergence proofs and empirical validation. SAM updates are implemented as xt+1 = xt - η∇f(xt + ρ∇f(xt)/||∇f(xt)||²) with perturbation bound ρ. Experiments vary model size from small to highly overparameterized across MNIST, CIFAR-10, and ImageNet datasets. Hessian analysis measures sharpness (max eigenvalue) and moment uniformity (s2, s3, s4). Sparsification is evaluated using random and SNIP pruning methods.

## Key Results
- SAM achieves linear convergence rate under overparameterization when interpolation and smoothness conditions hold
- SAM-selected minima have lower maximum Hessian eigenvalues and more uniform Hessian moment distributions than SGD minima
- The generalization gap between SAM and SGD increases with model size, confirming overparameterization amplifies SAM's advantage
- Sparsification can effectively achieve overparameterization benefits at reduced computational cost

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Overparameterization enables linear convergence of SAM by ensuring the PL-condition and smoothing the loss landscape.
- **Mechanism**: When the model is overparameterized, the interpolation condition holds, making the loss function satisfy the Polyak-Lojasiewicz (PL) condition. This ensures that gradients are large enough to provide consistent descent directions, while smoothness bounds perturbations so they align well with true gradients.
- **Core assumption**: Interpolation (zero training loss and zero gradients for every sample) plus smoothness of individual loss terms.
- **Evidence anchors**:
  - [abstract] "...we show that SAM can achieve a linear convergence rate under overparameterization in a stochastic setting."
  - [section] Theorem 7 proves linear convergence under interpolation and smoothness; Lemma 10 shows perturbation alignment scales with β and ρ.
  - [corpus] Weak; no direct evidence in cited papers, only general discussion of overparameterization effects.
- **Break condition**: If interpolation fails or smoothness is violated, the alignment and PL conditions break down, reverting SAM to sublinear convergence.

### Mechanism 2
- **Claim**: SAM selects flatter minima than SGD by imposing stricter sharpness and Hessian moment constraints for linear stability.
- **Mechanism**: The linearized SAM dynamics show that flatness (low max eigenvalue of Hessian) and more uniform Hessian moments (lower non-uniformity sk) are necessary for linear stability. SAM's implicit bias toward flatter regions is stronger than SGD's.
- **Core assumption**: Linear stability analysis applies to the true minima found by SAM, and the linearized approximation is accurate near convergence.
- **Evidence anchors**:
  - [abstract] "...we show that the linearly stable minima found by SAM are indeed flatter and have more uniformly distributed Hessian moments compared to those of SGD."
  - [section] Theorem 9 derives bounds on sharpness and non-uniformity; Figure 2 shows empirical flatness and uniformity differences.
  - [corpus] Weak; neighboring papers mention sharpness-aware methods but not the specific linear stability characterization.
- **Break condition**: If perturbations are too large or smoothness assumptions fail, the linearized stability analysis no longer predicts the actual minima structure.

### Mechanism 3
- **Claim**: Overparameterization amplifies the generalization gap between SAM and SGD because SAM can exploit the expanded solution space to find flatter, more uniform minima.
- **Mechanism**: With more parameters, the solution space grows, increasing the chance of finding minima with the desired flatness and uniformity properties. SAM's min-max search becomes more effective at navigating this space.
- **Core assumption**: The solution space truly expands with overparameterization and that flatness correlates with generalization.
- **Evidence anchors**:
  - [abstract] "...we present both empirical and theoretical findings that reveal its critical influence on SAM's effectiveness... the generalization improvement made by SAM increases with more parameters."
  - [section] Figure 3 shows the accuracy gap between SAM and SGD increasing with parameter count; Figure 5 shows optimal ρ increasing with model size.
  - [corpus] Weak; neighbors discuss sharpness-aware methods but not the overparameterization amplification effect.
- **Break condition**: If the correlation between flatness and generalization weakens or overparameterization leads to overfitting without proper regularization, the benefit may plateau or reverse.

## Foundational Learning

- **Concept**: Polyak-Lojasiewicz (PL) condition
  - **Why needed here**: PL condition ensures linear convergence under smoothness, which is central to proving SAM's linear rate in overparameterized regimes.
  - **Quick check question**: Given a smooth function f with PL constant α, what inequality relates the gradient norm to the suboptimality gap?

- **Concept**: Linear stability in dynamical systems
  - **Why needed here**: Determines which minima are attractors for iterative optimization; SAM's bias toward flatter minima is formalized via linear stability constraints.
  - **Quick check question**: In the linearized update x_{t+1} = (I - A)x_t, what spectral condition on A guarantees stability?

- **Concept**: Sharpness and Hessian moment uniformity
  - **Why needed here**: Sharpness (max eigenvalue) and non-uniformity (higher moments) quantify flatness and landscape regularity; SAM's minima have lower values than SGD's.
  - **Quick check question**: If H is the Hessian at a minimum, how is the sharpness defined and how does it relate to eigenvalue distribution?

## Architecture Onboarding

- **Component map**: Training loop -> Perturbation computation -> Loss evaluation -> Parameter update
- **Critical path**: Perturbation computation → gradient aggregation → parameter update; sparsification adds mask application before each step.
- **Design tradeoffs**:
  - Perturbation bound ρ: larger ρ → flatter minima but higher cost; must scale with model size.
  - Mini-batch size: larger B reduces gradient variance but increases memory; SAM doubles gradient cost per step.
  - Sparsity method: random vs. SNIP; SNIP better preserves trainability but needs extra sensitivity computation.
- **Failure signatures**:
  - Divergence or oscillation: ρ too large or step size mismatched.
  - No generalization gain: ρ too small or overparameterization insufficient.
  - Memory blowup: dense overparameterization without sparsity.
- **First 3 experiments**:
  1. Vary ρ on a small overparameterized MLP (MNIST) and plot training loss/accuracy vs. iterations.
  2. Compare Hessian max eigenvalue and moment non-uniformity for SAM vs SGD minima on a synthetic quadratic problem.
  3. Apply random vs. SNIP sparsification to an overparameterized ResNet18 on CIFAR10; measure generalization gap vs. sparsity level.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why the optimal perturbation bound ρ* increases with model size in SAM?
- Basis in paper: [explicit] The paper shows empirically that ρ* tends to increase as models become more overparameterized, but does not provide a theoretical justification for this trend.
- Why unresolved: The paper observes this pattern but does not develop a formal theoretical explanation linking model size to optimal perturbation magnitude.
- What evidence would resolve it: A theoretical analysis showing how perturbation effectiveness scales with model dimensionality and the relationship between ρ* and the ratio of model parameters to training data.

### Open Question 2
- Question: How does SAM's behavior differ theoretically when applied to extremely overparameterized regimes beyond current practical limits?
- Basis in paper: [inferred] The paper notes that scaling to extremely overparameterized regimes is computationally infeasible with current architectures, but suggests this would be an interesting direction.
- Why unresolved: Current experiments are limited by computational resources and the paper only speculates about behavior in extreme overparameterization.
- What evidence would resolve it: Experiments on extremely large models or theoretical analysis of SAM's convergence and stability properties in the infinite-parameter limit.

### Open Question 3
- Question: What is the precise relationship between sparsity patterns and SAM's effectiveness, beyond just parameter count?
- Basis in paper: [explicit] The paper observes that ρ* sometimes differs between small dense and large sparse models with similar parameter counts, suggesting factors beyond parameter count affect SAM.
- Why unresolved: While the paper identifies that sparsity patterns matter, it does not characterize what specific patterns are optimal or why they matter.
- What evidence would resolve it: Systematic experiments varying different sparsity patterns (e.g., structured vs. unstructured) and theoretical analysis of how different patterns affect the loss landscape geometry that SAM optimizes.

## Limitations
- Theoretical claims rely on strong assumptions (interpolation, smoothness) that may not hold in practice for deep networks
- Linear stability analysis assumes small perturbations near minima, but SAM's effectiveness may depend on behavior farther from convergence
- Empirical validation focuses on vision tasks where overparameterization is straightforward to achieve, limiting generalizability to other domains

## Confidence
- **High Confidence**: The linear convergence proof under overparameterization (Theorem 7) and the SAM vs SGD sharpness comparison (Theorem 9) have solid theoretical grounding and experimental support.
- **Medium Confidence**: The amplification mechanism - that overparameterization specifically increases SAM's generalization advantage - is demonstrated empirically but lacks rigorous theoretical explanation for why this relationship exists.
- **Low Confidence**: The sparsification approach as a practical solution to overparameterization costs needs more extensive validation across diverse architectures and tasks.

## Next Checks
1. Test SAM's convergence rate on a non-interpolating problem (e.g., underparameterized network) to verify the interpolation requirement is critical.
2. Conduct ablation studies varying ρ independently of model size to determine if the optimal ρ scaling is truly linear with parameter count.
3. Evaluate sparsification effectiveness on architectures beyond vision (e.g., Transformers for NLP) to assess generalizability of the proposed solution.