---
ver: rpa2
title: Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL
arxiv_id: '2310.04411'
source_url: https://arxiv.org/abs/2310.04411
tags:
- steps
- seem
- q-value
- divergence
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the divergence issue in offline reinforcement
  learning, particularly the instability of Q-value estimation due to out-of-distribution
  action queries. The authors identify self-excitation as the root cause of divergence
  and propose a novel Self-Excite Eigenvalue Measure (SEEM) based on Neural Tangent
  Kernel to detect and predict divergence.
---

# Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL

## Quick Facts
- **arXiv ID**: 2310.04411
- **Source URL**: https://arxiv.org/abs/2310.04411
- **Reference count**: 40
- **Key outcome**: Identifies self-excitation as the root cause of Q-value divergence in offline RL and proposes SEEM (Self-Excite Eigenvalue Measure) to predict and mitigate this divergence, achieving state-of-the-art results especially with limited data (1% of dataset).

## Executive Summary
This paper addresses the critical problem of Q-value divergence in offline reinforcement learning, where the agent's value estimation becomes unstable due to out-of-distribution action queries. The authors identify "self-excitation" as the fundamental cause of divergence, where the Q-network's generalization inadvertently increases the target Q-value estimate, creating a positive feedback loop. They propose a novel Self-Excite Eigenvalue Measure (SEEM) based on Neural Tangent Kernel that can predict divergence before it occurs. Through extensive empirical studies, they demonstrate that LayerNorm in the critic network effectively mitigates divergence without introducing detrimental bias, achieving state-of-the-art performance on challenging tasks like Antmaze and X% Mujoco datasets.

## Method Summary
The paper proposes a three-pronged approach to understanding and resolving Q-value divergence in offline RL. First, it identifies self-excitation as the root cause, where Q-value iteration leads to a positive feedback loop due to the network's generalization behavior. Second, it introduces SEEM (Self-Excite Eigenvalue Measure), a theoretically grounded metric based on Neural Tangent Kernel that can predict whether training will diverge and even estimate the growth rate. Third, it demonstrates that LayerNorm regularization in the critic network effectively mitigates divergence by constraining the NTK between dataset points and extreme inputs, making Q-value iteration non-expansive. The method is easily integrated into modern offline RL algorithms like TD3 and SAC, requiring only the insertion of LayerNorm before each nonlinear activation.

## Key Results
- LayerNorm consistently outperforms other normalization methods (BatchNorm, WeightNorm) in reducing SEEM and preventing divergence
- The method achieves state-of-the-art performance on challenging tasks, particularly when using only 1% of the dataset
- SEEM successfully predicts divergence before it occurs, with theoretical guarantees on growth rates
- Policy constraints, while reducing divergence, introduce detrimental bias and hurt performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-excitation in the Q-value network is the root cause of divergence in offline RL.
- Mechanism: During Q-value iteration, the network's generalization inadvertently increases the target Q-value estimate, creating a positive feedback loop where the prediction chases an ever-increasing target.
- Core assumption: The Q-network's generalization behavior causes it to assign inflated values to out-of-distribution actions, particularly extreme points at the boundaries of the action space.
- Evidence anchors:
  - [abstract] "We first identify a fundamental pattern, self-excitation, as the primary cause of Q-value estimation divergence in offline RL."
  - [section 3.3] "Such mirage-like property causes the model's parameter and its prediction value to diverge."
- Break condition: If the policy action becomes unstable or the NTK direction fails to converge to a steady state.

### Mechanism 2
- Claim: SEEM (Self-Excite Eigenvalue Measure) reliably predicts divergence before it occurs.
- Mechanism: SEEM is defined as the largest eigenvalue of the matrix A = γGθt(X∗t, X) - Gθt(X, X). When this eigenvalue is positive, the TD error grows exponentially during training.
- Core assumption: After a critical point t0, the policy action stabilizes and the NTK converges to a terminal direction, allowing the dynamics to be characterized by linear iteration.
- Evidence anchors:
  - [abstract] "For the first time, our theory can reliably decide whether the training will diverge at an early stage, and even predict the order of the growth for the estimated Q-value, the model's norm, and the crashing step when an SGD optimizer is used."
  - [section 3.1] "Theorem 3 states that when the policy action ˆπθt(s) that maximizes Q-values keep stable, each Q-value iteration essentially updates the TD error vector by a matrix At."
- Break condition: If the policy action does not stabilize or the NTK fails to converge to a steady direction.

### Mechanism 3
- Claim: LayerNorm in the critic network mitigates divergence by regularizing the model's generalization behavior.
- Mechanism: LayerNorm constrains the NTK between any point and extreme inputs to be bounded, reducing the SEEM value and making Q-value iteration non-expansive.
- Core assumption: Without LayerNorm, the network becomes approximately linear for extreme inputs, causing unbounded NTK values between dataset points and these extremes.
- Evidence anchors:
  - [abstract] "Through extensive empirical studies, we identify LayerNorm as a good solution to effectively avoid divergence without introducing detrimental bias."
  - [section 4] "Therefore, a simple method to accomplish this would be to insert a LayerNorm prior to each non-linear activation."
- Break condition: If LayerNorm is placed incorrectly (e.g., only on some layers) or if learnable affine parameters are disabled.

## Foundational Learning

- **Neural Tangent Kernel (NTK)**
  - Why needed here: NTK measures how similar two inputs are from the perspective of the neural network's current parameters, which is crucial for understanding how the Q-value iteration updates the TD error.
  - Quick check question: What does a large NTK value between two inputs imply about their value predictions during a parameter update?

- **Function Approximation in RL**
  - Why needed here: Understanding how neural networks generalize is essential for explaining why offline RL is prone to divergence when querying out-of-distribution actions.
  - Quick check question: Why does querying out-of-distribution actions during bootstrapping lead to cumulative extrapolation errors?

- **Linear Iteration Dynamics**
  - Why needed here: The convergence or divergence of Q-value estimation can be characterized by whether the linear iteration ut+1 = (I + ηAt)ut grows or shrinks over time.
  - Quick check question: Under what condition on the eigenvalues of At does the TD error vector ut diverge?

## Architecture Onboarding

- **Component map**: Critic network -> Target network -> Policy network -> Optimizer
- **Critical path**: 1. Initialize critic network 2. For each batch, compute TD target using target network 3. Compute TD error and update critic via gradient descent 4. Update target network (EMA or direct copy) 5. Monitor SEEM and Q-value for divergence
- **Design tradeoffs**:
  - LayerNorm vs. other normalizations: LayerNorm consistently reduces SEEM while BatchNorm may oscillate
  - EMA vs. direct copy: EMA stabilizes training but obscures theoretical analysis
  - Depth vs. width: Deeper networks have polynomial Q-value growth, shallower have exponential
- **Failure signatures**: Q-value explosion to infinity or NaN, policy actions gravitating toward action space boundaries, SEEM value becoming large and positive, NTK similarity and action similarity failing to stabilize
- **First 3 experiments**:
  1. Run TD3 on a D4RL task with and without LayerNorm in the critic, monitoring SEEM and Q-value curves
  2. Test different normalization methods (LayerNorm, BatchNorm, WeightNorm) on the 10% Mujoco dataset
  3. Verify the homogeneity property in Lemma 1 by scaling network parameters and checking output/gradient/NTK scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the critical point phenomenon (where NTK and policy stabilize) depend on specific network architectures or initialization schemes?
- Basis in paper: [explicit] The paper observes this phenomenon empirically but states "The reason for the existence of this critical point is elusive and still unknown."
- Why unresolved: The paper hypothesizes about attractors in parameter space but doesn't provide rigorous theoretical analysis of when and why this critical point emerges.
- What evidence would resolve it: Systematic experiments varying network architectures, initialization schemes, and dataset properties to identify conditions that guarantee or prevent the critical point phenomenon.

### Open Question 2
- Question: How does the LayerNorm solution generalize to different network architectures beyond MLPs, such as convolutional networks or transformers?
- Basis in paper: [inferred] The analysis focuses on MLPs with ReLU activation, and while LayerNorm is suggested as a solution, its effectiveness on other architectures is not explored.
- Why unresolved: The paper only demonstrates LayerNorm's effectiveness on MLPs in offline RL settings, leaving open whether the same mechanism applies to other architectures.
- What evidence would resolve it: Experiments applying LayerNorm-based solutions to convolutional networks and transformers in both online and offline RL settings, measuring SEEM values and convergence properties.

### Open Question 3
- Question: What is the relationship between the eigenvalue measure SEEM and other spectral properties of the neural tangent kernel, such as the spectral gap or condition number?
- Basis in paper: [explicit] The paper defines SEEM as the largest eigenvalue of a specific matrix derived from NTK but doesn't explore its relationship to other spectral properties.
- Why unresolved: While SEEM is shown to predict divergence, the paper doesn't analyze how it relates to other important spectral properties that might provide complementary insights.
- What evidence would resolve it: Comparative analysis of SEEM against other spectral measures (spectral gap, condition number) across various network architectures and training scenarios, examining their predictive power for divergence.

## Limitations

- The theoretical analysis relies heavily on assumptions about NTK convergence and policy stability that may not hold in practice for all offline RL tasks
- The effectiveness of LayerNorm as a universal solution needs further validation across diverse network architectures and RL algorithms
- The SEEM metric, while theoretically grounded, requires empirical verification that it consistently predicts divergence before it occurs in real-world scenarios

## Confidence

- **High confidence**: LayerNorm effectively reduces Q-value divergence in practice (supported by empirical results on D4RL benchmarks)
- **Medium confidence**: Self-excitation is the primary cause of divergence across all offline RL scenarios (mechanism is well-theorized but may have exceptions)
- **Medium confidence**: SEEM reliably predicts divergence before it occurs (theoretical foundation is strong, but real-world validation is limited to specific settings)

## Next Checks

1. Test the proposed LayerNorm solution on additional offline RL algorithms beyond TD3 and SAC to verify generalizability
2. Evaluate SEEM's predictive capability on noisy or non-stationary offline datasets where policy actions may not stabilize
3. Investigate whether alternative normalization methods (e.g., GroupNorm, LayerScale) can achieve similar or better divergence mitigation results