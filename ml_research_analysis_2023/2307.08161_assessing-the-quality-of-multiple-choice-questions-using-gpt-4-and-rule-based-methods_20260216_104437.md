---
ver: rpa2
title: Assessing the Quality of Multiple-Choice Questions Using GPT-4 and Rule-Based
  Methods
arxiv_id: '2307.08161'
source_url: https://arxiv.org/abs/2307.08161
tags:
- questions
- question
- methods
- evaluation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares two automated methods for evaluating the quality
  of student-generated multiple-choice questions using the Item-Writing Flaws (IWF)
  rubric. A rule-based method correctly detected 91% of flaws identified by human
  annotators across 200 questions from four subject areas, outperforming GPT-4's 79%
  accuracy.
---

# Assessing the Quality of Multiple-Choice Questions Using GPT-4 and Rule-Based Methods

## Quick Facts
- arXiv ID: 2307.08161
- Source URL: https://arxiv.org/abs/2307.08161
- Reference count: 40
- Primary result: Rule-based method detected 91% of flaws vs GPT-4's 79% accuracy

## Executive Summary
This study evaluates two automated approaches for detecting item-writing flaws in student-generated multiple-choice questions. The rule-based method employs explicit programmatic logic for each of the 19 Item-Writing Flaws criteria, while GPT-4 uses prompt-based classification. Across 200 questions from four subject areas, the rule-based approach outperformed GPT-4 in accuracy, correctly identifying 91% of human-annotated flaws compared to GPT-4's 79%. The methods identified implausible distractors and convergence cues as the most common flaws in student-generated questions.

## Method Summary
The study compared two automated evaluation methods using the Item-Writing Flaws (IWF) rubric on 200 student-generated MCQs from Chemistry, Biochemistry, Statistics, and CollabU courses. The rule-based method translated each of the 19 IWF criteria into programmatic checks using string manipulation, NLP tagging, and LLM classification. GPT-4 received prompts for each criterion and provided binary classifications. Both methods were evaluated against human annotators using standard classification metrics including F1 scores, Hamming loss, and exact match ratios.

## Key Results
- Rule-based method correctly detected 91% of flaws identified by human annotators
- GPT-4 method matched 78.89% of human classifications across all questions
- Plausible distractors and convergence cues were the most common flaws identified
- Rule-based method maintained better interpretability and domain independence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based methods outperform GPT-4 by applying explicit, interpretable logic tailored to each flaw.
- Mechanism: The rule-based method decomposes the 19 IWF criteria into programmable checks (string manipulation, NLP tagging, and LLM classification) so each flaw is evaluated with deterministic rules rather than probabilistic model outputs.
- Core assumption: Flaws can be captured by explicit, domain-agnostic patterns that do not require contextual inference beyond what rule components provide.
- Evidence anchors:
  - [abstract] "A rule-based method correctly detected 91% of flaws identified by human annotators... outperforming GPT-4's 79% accuracy."
  - [section 3.3] "Working alongside the human evaluators, we constructed a script that is composed of a programmatic method for each of the 19 IWF rubric criteria."

### Mechanism 2
- Claim: GPT-4 tends to over-flag flaws, especially those involving verbose stems or proper nouns, due to lack of domain context.
- Mechanism: GPT-4 applies a single prompt per flaw and relies on its general language model to interpret criteria; without fine-tuned domain awareness, it flags borderline cases as violations.
- Core assumption: GPT-4's output correlates with flaw detection only when the criteria align with general language patterns, not domain-specific usage.
- Evidence anchors:
  - [section 4.1] "GPT-4 method matched 78.89% of human classifications... identified even more potential flaws in the questions compared to both the human and rule-based methods."
  - [section 5] "GPT-4's worse performance... may be due to proper nouns being included in the question text."

### Mechanism 3
- Claim: Across domains, the rule-based method maintains more consistent performance because it does not depend on semantic reasoning that degrades in jargon-heavy contexts.
- Mechanism: The rule-based system uses lightweight NLP techniques and LLM classifiers trained on general acceptability data (e.g., CoLA) rather than subject-specific corpora, so it applies the same logic regardless of domain.
- Core assumption: The IWF rubric criteria are sufficiently independent of subject matter to be detected by generic language patterns.
- Evidence anchors:
  - [section 4.2] "Both the rule-based and GPT-4 methods have a lower micro-average F1 score... for the Chemistry and Biochemistry courses compared to Statistics and CollabU."
  - [section 3.3] "This logic for many of the criteria involved string manipulation... Other criteria involved the use of standard NLP techniques."

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: Each MCQ can violate multiple IWFs simultaneously; the system must assign a set of labels rather than one.
  - Quick check question: If a question has 3 flaws, how many labels should the classifier output? (Answer: 3)

- Concept: Hamming loss
  - Why needed here: The study reports Hamming loss to measure per-label error rates, which is more informative than overall accuracy for multi-label tasks.
  - Quick check question: What does a Hamming loss of 0.09 mean in this context? (Answer: On average, 9% of flaw labels were misclassified.)

- Concept: Inter-rater reliability (IRR)
  - Why needed here: Human evaluators' agreement on flaw detection sets the baseline for comparing automatic methods.
  - Quick check question: Why is Cohen's Îº reported alongside percent agreement? (Answer: To account for chance agreement in categorical coding.)

## Architecture Onboarding

- Component map:
  - Data loader -> human-annotated MCQ dataset (stem + 4 options) -> Rule engine (19 modular criteria modules) -> outputs binary flaw flags
  - Data loader -> human-annotated MCQ dataset (stem + 4 options) -> GPT-4 prompt pipeline -> outputs yes/no per criterion
  - Rule engine outputs -> GPT-4 outputs -> Evaluation module -> computes F1, Hamming loss, exact match ratio
  - Evaluation module -> Visualization (confusion matrices) -> quality classification output

- Critical path:
  1. Load MCQs
  2. Apply rule-based checks (string/NLP/LLM per criterion)
  3. Send same MCQs to GPT-4 with per-criterion prompts
  4. Compare both outputs to human labels
  5. Aggregate and report performance metrics

- Design tradeoffs:
  - Rule-based: interpretable, lightweight, limited by rule expressiveness; GPT-4: flexible, less interpretable, prone to over-flagging without domain tuning.

- Failure signatures:
  - Rule-based: high false negatives on criteria requiring deep semantic inference.
  - GPT-4: high false positives on verbose stems or proper noun usage; inconsistent per-run due to model randomness.

- First 3 experiments:
  1. Run rule-based evaluation on a small MCQ subset, inspect per-criterion logic for correctness.
  2. Compare GPT-4 outputs with human labels on a single flaw type (e.g., "none of the above") to measure baseline over-flagging.
  3. Compute micro-averaged F1 scores for each domain separately to identify where each method degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated evaluation methods be improved to better handle domain-specific terminology and jargon in educational MCQs?
- Basis in paper: [explicit] The authors note that both rule-based and GPT-4 methods performed worse on Chemistry and Biochemistry datasets due to terminology and jargon.
- Why unresolved: While the paper identifies this as a limitation, it doesn't propose specific solutions for improving automated methods' handling of domain-specific language.
- What evidence would resolve it: Comparative studies testing various NLP techniques or specialized domain-specific training data to improve automated MCQ evaluation across different subject areas.

### Open Question 2
- Question: Can automated methods be developed to not only identify item-writing flaws but also provide constructive suggestions for improving flawed questions?
- Basis in paper: [explicit] The authors mention this as a promising future direction, noting that while GPT-4 may not be as accurate at identifying flaws, it can provide explanations and suggest improvements.
- Why unresolved: The paper only discusses identifying flaws, not providing solutions for fixing them.
- What evidence would resolve it: Development and evaluation of automated systems that both detect flaws and generate specific, actionable feedback for question improvement.

### Open Question 3
- Question: How does the performance of automated MCQ evaluation methods change when applied to questions from higher-level courses with more complex cognitive processes beyond recall?
- Basis in paper: [inferred] The authors note that current automated methods rarely utilize questions from complex domains that go beyond recall-level cognitive processes.
- Why unresolved: The study focuses on introductory-level questions from four subject areas, not exploring higher-level educational content.
- What evidence would resolve it: Comparative analysis of automated MCQ evaluation methods across a diverse range of question complexities and cognitive levels.

## Limitations
- Limited to single rubric (IWF) and four subject domains, raising generalizability concerns
- Rule-based method may degrade with complex linguistic structures or domain-specific terminology
- GPT-4 evaluation could vary significantly with different prompt engineering strategies

## Confidence
- Rule-based method superiority (91% vs 79% accuracy): High - Directly supported by comparative metrics and human annotation alignment
- Domain independence of rule-based approach: Medium - Performance variation across subjects suggests some domain sensitivity
- GPT-4 over-flagging as primary weakness: Medium - While observed, the extent depends on prompt design and evaluation criteria

## Next Checks
1. Test both methods on MCQs from additional domains (e.g., humanities, professional certification) to verify cross-domain performance claims
2. Conduct ablation studies on the rule-based method to identify which criteria contribute most to accuracy differences
3. Experiment with alternative GPT-4 prompt engineering approaches to determine if over-flagging can be reduced while maintaining accuracy