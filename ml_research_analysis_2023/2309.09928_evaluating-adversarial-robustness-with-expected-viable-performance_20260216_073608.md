---
ver: rpa2
title: Evaluating Adversarial Robustness with Expected Viable Performance
arxiv_id: '2309.09928'
source_url: https://arxiv.org/abs/2309.09928
tags:
- robustness
- performance
- perturbation
- adversarial
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a metric for evaluating the robustness of
  a classifier, with particular attention to adversarial perturbations, in terms of
  expected functionality with respect to possible adversarial perturbations. The authors
  define a classifier as non-functional (that is, has a functionality of zero) with
  respect to a perturbation bound if a conventional measure of performance, such as
  classification accuracy, is less than a minimally viable threshold when the classifier
  is tested on examples from that perturbation bound.
---

# Evaluating Adversarial Robustness with Expected Viable Performance

## Quick Facts
- arXiv ID: 2309.09928
- Source URL: https://arxiv.org/abs/2309.09928
- Reference count: 21
- Key outcome: Introduces Expected Viable Performance (EVP) as a metric for evaluating classifier robustness to adversarial perturbations by integrating performance above a viability threshold.

## Executive Summary
This paper introduces Expected Viable Performance (EVP), a new metric for evaluating adversarial robustness that integrates classification accuracy over a range of perturbation magnitudes only where performance exceeds a minimally viable threshold. The metric provides a more nuanced measure of robustness than existing approaches by considering both the extent of the functional region and average performance within it. The authors demonstrate EVP's utility on ResNet50 models trained on CIFAR and ImageNet datasets, showing how it can differentiate between natural and adversarially trained models and inform the development of robust ML systems.

## Method Summary
EVP is computed by generating adversarial examples across multiple L2 perturbation magnitudes using Projected Gradient Descent (PGD), measuring classification accuracy at each perturbation level, applying a viability threshold to identify the functional region, and integrating accuracy over this region using the trapezoid rule. The metric requires pretrained models (natural and adversarially trained ResNet50s), CIFAR or ImageNet datasets, and a viability threshold parameter. Computational cost is higher than some alternatives due to the need to evaluate accuracy at multiple perturbation levels.

## Key Results
- EVP provides a more nuanced robustness measure by integrating performance across perturbation magnitudes above a viability threshold
- The metric allows standardized comparison of robustness across different models and training methods
- EVP incentivizes development of models with graceful degradation over wider perturbation ranges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EVP provides a more nuanced measure of robustness by integrating performance across a range of perturbation magnitudes rather than evaluating at a single threshold.
- Mechanism: By calculating the area under the performance-perturbation curve only over the region where performance exceeds a viability threshold, EVP captures both the extent of the functional region and the average performance within it.
- Core assumption: The viability threshold τ accurately reflects the minimum acceptable performance for the application, and the uniform distribution assumption over perturbations is reasonable.
- Evidence anchors:
  - [abstract] "A classifier is assumed to be non-functional (that is, has a functionality of zero) with respect to a perturbation bound if a conventional measure of performance, such as classification accuracy, is less than a minimally viable threshold when the classifier is tested on examples from that perturbation bound."
  - [section] "We propose a new ML robustness metric which takes this form, which we term Expected Viable Performance (EVP)."
- Break condition: If the chosen viability threshold does not align with real-world application requirements, EVP may misrepresent true robustness.

### Mechanism 2
- Claim: EVP allows for a standardized comparison of robustness across different models and training methods by using a consistent metric formulation.
- Mechanism: The metric is calculated as an integral over perturbation space, providing a single scalar value that summarizes robustness, making it easier to compare models trained with different techniques (e.g., adversarial training vs. natural training).
- Core assumption: The performance-perturbation curve is smooth and well-behaved, allowing for accurate numerical integration via methods like the trapezoid rule.
- Evidence anchors:
  - [abstract] "The primary result is that this metric, called Expected Viable Performance (EVP), provides a more nuanced and practical measure of robustness than existing metrics, as it takes into account both the performance inside the region of viability and the size of that region."
  - [section] "The flexibility of the EVP to be defined for different viability thresholds allows a user to customize the metric to their application."
- Break condition: If the performance degrades abruptly near the viability threshold, the integral approximation may be inaccurate.

### Mechanism 3
- Claim: EVP encourages the development of models that maintain acceptable performance across a wider range of perturbations, not just at a single point.
- Mechanism: By discounting performance gains below the viability threshold, EVP incentivizes training methods that achieve graceful degradation over a larger perturbation range rather than optimizing for a narrow range.
- Core assumption: The cost of achieving robustness over a larger perturbation range does not excessively compromise performance on clean data beyond acceptable limits.
- Evidence anchors:
  - [abstract] "Defining robustness in terms of an expected value is motivated by a domain general approach to robustness quantification."
  - [section] "EVP does not, because the gains are accruing below the viability threshold and are therefore discounted."
- Break condition: If the viability threshold is set too high, EVP may unfairly penalize models that are robust but have slightly lower clean accuracy.

## Foundational Learning

- Concept: Adversarial robustness
  - Why needed here: The paper focuses on evaluating how well classifiers maintain performance under adversarial perturbations, which is a key concern in machine learning security.
  - Quick check question: What is the difference between adversarial robustness and general robustness to perturbations?

- Concept: Expected value and integration
  - Why needed here: EVP is defined as an expected value over a range of perturbations, requiring an understanding of integration to compute the area under the performance-perturbation curve.
  - Quick check question: How does the choice of integration method (e.g., trapezoid rule) affect the accuracy of EVP?

- Concept: Threshold-based evaluation
  - Why needed here: EVP uses a viability threshold to determine the region where the classifier is functional, which is central to its formulation.
  - Quick check question: Why is it important to have a viability threshold, and how should it be chosen?

## Architecture Onboarding

- Component map:
  Data pipeline -> Models (ResNet50) -> Attack method (PGD) -> Performance metric (accuracy) -> EVP calculator

- Critical path:
  1. Load pretrained models
  2. Generate adversarial examples across a range of perturbation magnitudes
  3. Compute accuracy for each perturbation level
  4. Apply viability threshold to define functional region
  5. Integrate accuracy over functional region to obtain EVP

- Design tradeoffs:
  - Choice of viability threshold τ: A higher threshold emphasizes clean performance but may overlook robustness to larger perturbations.
  - Sampling interval: Smaller intervals provide more accurate EVP but increase computational cost.
  - Integration method: Trapezoid rule is simple but may be less accurate for highly non-linear performance curves.

- Failure signatures:
  - EVP values are inconsistent across runs with the same models and attacks
  - EVP does not differentiate between models with significantly different robustness profiles
  - EVP is highly sensitive to the choice of viability threshold

- First 3 experiments:
  1. Compute EVP for a naturally trained ResNet50 and an adversarially trained ResNet50 on CIFAR using a viability threshold of 70% accuracy.
  2. Vary the viability threshold and observe how EVP changes for different models.
  3. Test the impact of sampling interval on EVP accuracy by comparing results with intervals of 0.25 and 0.1 L2 distance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for determining the viability threshold (τ) in different application contexts?
- Basis in paper: [explicit] The paper discusses the flexibility of EVP to be defined for different viability thresholds and mentions the need for a threshold based on concrete cost-benefit concerns for a particular application.
- Why unresolved: The paper provides a suggested method based on Cohen's d but acknowledges that the choice of τ should be motivated by concrete cost-benefit concerns, which are application-specific and not fully explored.
- What evidence would resolve it: Empirical studies across various applications comparing the performance of EVP with different threshold determination methods, or a theoretical framework for deriving application-specific viability thresholds.

### Open Question 2
- Question: How does EVP compare to other robustness metrics in terms of computational efficiency and practical applicability?
- Basis in paper: [explicit] The paper mentions that EVP is more computationally expensive than some other metrics due to the number of adversarial accuracies that must be computed.
- Why unresolved: While the paper introduces EVP and its advantages, it doesn't provide a comprehensive comparison of computational efficiency and practical applicability against existing robustness metrics.
- What evidence would resolve it: Benchmark studies comparing EVP to other robustness metrics across various model architectures and datasets, measuring both computational time and the ability to differentiate between robust and non-robust models.

### Open Question 3
- Question: How can EVP be extended to handle multivariate functionality and soft viability thresholds?
- Basis in paper: [explicit] The paper mentions the possibility of using multiple performance metrics and soft viability thresholds as future work.
- Why unresolved: The current formulation of EVP is based on a single performance metric and a hard viability threshold. Extending it to handle more complex scenarios is mentioned but not explored.
- What evidence would resolve it: Development and validation of multivariate EVP formulations, and exploration of soft threshold implementations with corresponding mathematical proofs and empirical validation.

## Limitations

- EVP depends critically on the choice of viability threshold τ, which must be carefully aligned with application requirements
- The metric assumes a uniform distribution over perturbations, which may not reflect real-world attack scenarios
- EVP is more computationally expensive than some alternative robustness metrics due to the need to evaluate accuracy at multiple perturbation levels

## Confidence

- Claim: EVP provides a more nuanced robustness measure than existing metrics -> Medium
- Claim: EVP allows standardized comparison across models and training methods -> High
- Claim: EVP incentivizes development of models with broader robustness -> Medium
- Claim: Numerical integration approach is accurate for smooth performance curves -> High

## Next Checks

1. Conduct sensitivity analysis of EVP to viability threshold choices across multiple application domains
2. Compare EVP rankings with alternative robustness metrics (e.g., area under the curve without thresholding) to validate the added value of the viability constraint
3. Test EVP's ability to differentiate between models with similar clean accuracy but different robustness profiles on datasets beyond CIFAR and ImageNet