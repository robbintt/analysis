---
ver: rpa2
title: On the Unexpected Abilities of Large Language Models
arxiv_id: '2308.09720'
source_url: https://arxiv.org/abs/2308.09720
tags:
- language
- llms
- skills
- abilities
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews recent research on the cognitive abilities developed
  by large language models (LLMs) and their relation to human cognition. The author
  argues that these abilities emerge indirectly as a byproduct of training LLMs to
  predict the next word in a text, rather than being explicitly trained for them.
---

# On the Unexpected Abilities of Large Language Models

## Quick Facts
- arXiv ID: 2308.09720
- Source URL: https://arxiv.org/abs/2308.09720
- Reference count: 0
- This paper reviews research on cognitive abilities developed by large language models and argues they emerge indirectly as a byproduct of predicting next words in text.

## Executive Summary
This paper examines how large language models (LLMs) develop complex cognitive abilities not through direct training, but as indirect consequences of the next-word prediction task. The author argues that these abilities - including reasoning, theory of mind, and spatial understanding - emerge because accurate prediction requires deep text comprehension. The indirect acquisition process creates integrated capabilities that work synergistically, unlike traditional modular AI systems. The paper also compares LLMs to natural intelligence, highlighting key differences in how knowledge is acquired and processed.

## Method Summary
The paper reviews theoretical foundations and empirical research on emergent abilities in LLMs, focusing on how the next-word prediction task indirectly leads to complex cognitive capabilities. It analyzes the mechanism by which prediction errors serve as informative signals about comprehension, discusses power-law scaling relationships in model performance, and examines the architectural features of transformer models that enable these emergent properties. The analysis synthesizes findings from multiple studies on LLM behavior and cognitive capabilities.

## Key Results
- LLMs develop complex cognitive abilities indirectly through next-word prediction rather than direct training
- The indirect acquisition process creates integrated abilities that work synergistically together
- The set of possible abilities is constrained by what's necessary to understand human-written text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large Language Models develop complex cognitive abilities indirectly as a byproduct of predicting next words.
- Mechanism: The prediction task requires deep text comprehension, which in turn necessitates cognitive skills like reasoning, theory of mind, and spatial understanding. These skills are not directly trained but emerge because accurate word prediction depends on them.
- Core assumption: Prediction error is highly informative and directly reflects the system's comprehension level.
- Evidence anchors:
  - [abstract] "these abilities emerge indirectly as a byproduct of training LLMs to predict the next word"
  - [section] "To predict as accurately as possible the words that the characters of a story will say next, the system should infer the goals of the characters... acquire an ability to reason and to identify the mental states of the characters"
  - [corpus] Found 25 related papers; no direct citations found in corpus (average citations 0.0)
- Break condition: If prediction error is not sufficiently informative about comprehension, or if the relationship between word prediction and cognitive skill is too indirect, the emergence of these abilities would not occur.

### Mechanism 2
- Claim: The indirect acquisition process leads to integrated abilities that work synergistically.
- Mechanism: Because multiple cognitive skills are acquired simultaneously during training (co-shaped), each skill is shaped to work in harmony with others, creating an integrated system rather than isolated modules.
- Core assumption: Skills acquired in parallel during training develop interdependencies that enhance overall performance.
- Evidence anchors:
  - [section] "the acquisition of integrated capabilities, i.e., the acquisition of skills that are organized to work in synergy with the other acquired skills"
  - [section] "The integrated nature of these capabilities allow to recover the knowledge possessed by LLMs through natural language queries"
  - [corpus] No direct evidence in corpus about integrated abilities (average neighbor FMR 0.381)
- Break condition: If skills were acquired sequentially or independently, integration would not occur, and the system would behave like a collection of separate modules.

### Mechanism 3
- Claim: The set of abilities LLMs can develop is constrained by what's necessary to understand human-written text.
- Mechanism: Since LLMs are trained on human text, they can only develop abilities that humans possess and use in writing, as language is shaped by human cognitive capabilities.
- Core assumption: Human language contains sufficient information to extract all cognitive abilities necessary for comprehension.
- Evidence anchors:
  - [section] "The set of abilities that can be used to comprehend human-written text is closed and is probably restricted to the set of abilities possessed by the humans who wrote the text"
  - [section] "The set of abilities that can be acquired is probably restricted to the set of abilities possessed by the humans who wrote the text used for training"
  - [corpus] No direct corpus evidence supporting this constraint hypothesis
- Break condition: If language contained information about abilities humans don't possess, or if the training corpus included non-human perspectives, this constraint would be violated.

## Foundational Learning

- Concept: Indirect skill acquisition
  - Why needed here: Understanding how LLMs develop abilities without direct training is central to the paper's argument about emergent capabilities
  - Quick check question: Can you explain how predicting next words leads to theory of mind skills without explicit training?

- Concept: Power-law scaling in neural networks
  - Why needed here: The paper discusses how performance scales with model size, which is fundamental to understanding when abilities emerge
  - Quick check question: What happens to prediction error as model size increases according to the power-law relationship?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper's analysis depends on understanding how the underlying architecture enables these emergent abilities
  - Quick check question: How does the multi-head attention mechanism in transformers differ from traditional recurrent architectures in processing sequences?

## Architecture Onboarding

- Component map:
  Input tokenization layer -> Transformer encoder/decoder stack -> Position encoding -> Output projection layer -> Loss function

- Critical path: Tokenization → Transformer layers → Position encoding → Output projection → Loss calculation → Parameter updates

- Design tradeoffs:
  - Depth vs. width: Deeper networks may capture more complex patterns but risk optimization difficulties
  - Context window size: Larger windows allow better long-range dependencies but increase computational cost
  - Vocabulary size: Larger vocabularies capture more nuance but require more parameters and training data

- Failure signatures:
  - Mode collapse: Model generates repetitive or template-like text
  - Catastrophic forgetting: Fine-tuning on new tasks degrades performance on original tasks
  - Hallucination: Model generates factually incorrect information with high confidence

- First 3 experiments:
  1. Train a small transformer (4 layers, 128 embedding dim) on a simple text corpus and observe when basic language skills emerge
  2. Compare prediction error trajectories for models of increasing size on the same task to verify power-law scaling
  3. Implement a diagnostic probe to measure theory of mind capabilities at different training stages to identify emergence patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise set of cognitive abilities that can be acquired by LLMs through predicting next words in human-written text, and how does this set compare to the abilities possessed by humans who wrote the text?
- Basis in paper: [explicit] The paper hypothesizes that "the set of abilities that can be acquired is probably restricted to the set that is necessary and sufficient to understand human-written text" and that this set is "restricted to the set of abilities possessed by the humans who wrote the text used for training."
- Why unresolved: The paper acknowledges this as a hypothesis and does not provide empirical evidence to fully support or refute it. The question of whether LLMs can develop abilities beyond those possessed by humans remains open.
- What evidence would resolve it: Comparative studies examining the specific abilities acquired by LLMs and comparing them to the abilities of humans who wrote the training data. Additionally, experiments testing whether LLMs can develop novel abilities not present in the training data would provide insights.

### Open Question 2
- Question: How do the prediction errors in LLMs provide reliable measures of the knowledge and skills of the system, and what factors contribute to the high informative nature of these errors?
- Basis in paper: [explicit] The paper mentions that "the high informative nature of the prediction error, i.e., the fact that it provides a very reliable measure of the knowledge and skills of the system" is an enabling factor for indirect acquisition of skills.
- Why unresolved: The paper does not provide a detailed explanation of how prediction errors serve as reliable measures of knowledge and skills. The specific factors contributing to the informative nature of these errors are not explicitly discussed.
- What evidence would resolve it: Detailed analysis of the relationship between prediction errors and the knowledge/skills of LLMs. Investigation of the factors that make prediction errors highly informative, such as the structure of the training data and the model architecture.

### Open Question 3
- Question: To what extent can the abilities acquired by LLMs be predicted, and what factors influence the predictability of these abilities?
- Basis in paper: [explicit] The paper discusses the predictability of abilities acquired by LLMs, mentioning that "the specific abilities that will be developed by a model of a certain size are not necessarily predictable" and that "the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance."
- Why unresolved: The paper does not provide a definitive answer to the predictability of LLM abilities. The factors influencing predictability, such as the size of the model and the training data, are not fully explored.
- What evidence would resolve it: Empirical studies examining the predictability of LLM abilities across different model sizes and training data. Investigation of the relationship between model size, training data, and the emergence of specific abilities. Analysis of the metrics used to evaluate performance and their impact on the observed transitions in ability acquisition.

## Limitations

- The mechanism linking word prediction to complex cognitive abilities remains largely theoretical rather than empirically demonstrated
- The claim that abilities are "integrated" rather than modular is difficult to verify without detailed analysis of internal representations
- The constraint that LLMs can only develop abilities present in human text is speculative and may underestimate extrapolation potential

## Confidence

- High confidence: The basic premise that LLMs develop abilities as a byproduct of next-word prediction is well-established empirically
- Medium confidence: The claim about integrated capabilities working synergistically is reasonable given the architecture but lacks direct experimental validation
- Low confidence: The assertion that language constrains possible abilities to those humans possess is speculative and difficult to test

## Next Checks

1. **Causal intervention experiments**: Design experiments that directly manipulate the prediction task (e.g., by introducing controlled noise or structural modifications) and measure effects on specific cognitive abilities like reasoning or theory of mind, establishing causal links rather than just correlations.

2. **Modular vs. integrated capability assessment**: Develop systematic tests that isolate individual cognitive abilities and measure both their standalone performance and their interaction effects when combined, using ablation studies and controlled interference experiments to distinguish between modular and integrated architectures.

3. **Cross-linguistic and cross-cultural validation**: Train models on diverse language corpora from different cultures and linguistic structures, then systematically compare emergent abilities across these models to test whether the indirect acquisition mechanism is universal or culturally bounded.