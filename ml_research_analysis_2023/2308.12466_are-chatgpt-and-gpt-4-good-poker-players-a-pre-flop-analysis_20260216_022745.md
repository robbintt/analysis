---
ver: rpa2
title: Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis
arxiv_id: '2308.12466'
source_url: https://arxiv.org/abs/2308.12466
tags:
- poker
- chatgpt
- hands
- player
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores the poker-playing capabilities of ChatGPT\
  \ and GPT-4, focusing on their pre-flop decision-making in No-Limit Hold\u2019em\
  \ poker. The research reveals that both models exhibit a strong understanding of\
  \ poker fundamentals, such as hand rankings and positional strategy, but fail to\
  \ achieve game theory optimal (GTO) play."
---

# Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis

## Quick Facts
- arXiv ID: 2308.12466
- Source URL: https://arxiv.org/abs/2308.12466
- Reference count: 2
- Both models understand poker fundamentals but fail to achieve GTO play

## Executive Summary
This study evaluates ChatGPT and GPT-4's pre-flop poker decision-making in No-Limit Hold'em. Both models demonstrate strong understanding of poker fundamentals including hand rankings and positional strategy, but neither achieves game theory optimal (GTO) play. ChatGPT exhibits conservative tendencies with frequent limping, while GPT-4 shows aggressive play with no limping. When prompted to play GTO, ChatGPT becomes tighter and more aggressive, while GPT-4 becomes even more aggressive, deviating further from optimal strategy. The research highlights the critical role of prompt design and model parameters in eliciting optimal responses.

## Method Summary
The study tests both models using carefully crafted system and user prompts in No-Limit Hold'em poker scenarios. Researchers evaluate raise-first-in (RFI) pre-flop decisions across different positions, comparing model outputs against GTO strategy charts. Testing involves multiple temperature and top-p parameter settings to find optimal configurations, with each hand queried multiple times to determine the most common decision. The analysis focuses on decision matrices for raise, fold, and limp actions.

## Key Results
- ChatGPT and GPT-4 understand poker fundamentals but fail to achieve GTO play
- ChatGPT is conservative with frequent limping; GPT-4 is aggressive with no limping
- Prompt formatting significantly influences model decision quality
- Neither model achieves GTO play even when explicitly prompted to do so

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model behavior reflects learned statistical associations from training corpus rather than strategic reasoning
- Mechanism: Models generate actions by sampling from token distributions conditioned on prompt context, where poker patterns are frequent in internet discussions
- Core assumption: Internet poker discussions and strategy articles dominate the relevant token space
- Evidence anchors:
  - Models understand poker fundamentals through implicit learning from internet data
  - ChatGPT demonstrates understanding of hand rankings, suited hands, and pocket pairs
- Break condition: If training corpus contains contradictory or sparse poker data, model outputs will deviate significantly from optimal strategy

### Mechanism 2
- Claim: Prompt formatting controls model behavior more than model capacity
- Mechanism: Specific phrasing ("GTO", "ranked order", "short-type") steers token generation toward statistically similar contexts
- Core assumption: The model has seen similar phrasings in training data with consistent strategic implications
- Evidence anchors:
  - ChatGPT raises UTG with Q-2-offsuit when cards are unranked, but plays more optimally with ranked format
  - Short-type prompts perform better than verbose prompts for GTO decision making
- Break condition: If the model has not encountered similar prompt structures, formatting changes will have minimal effect

### Mechanism 3
- Claim: Model-specific tendencies emerge from fine-grained statistical patterns in pretraining data
- Mechanism: GPT-4's absence of limps and ChatGPT's higher limp rate reflect differential exposure to aggressive vs passive poker strategies
- Core assumption: Pretraining data contains sufficient variance in poker strategy discussions to imprint distinct behavioral biases
- Evidence anchors:
  - GPT-4 has no limps in its entire range, making it more aggressive than ChatGPT
  - ChatGPT naturally contains many limps in its starting ranges, making it less GTO
- Break condition: If pretraining data is too homogeneous in poker strategy, both models will converge to similar behaviors

## Foundational Learning

- Concept: Token probability sampling
  - Why needed here: Models choose actions by sampling from probability distributions over possible next tokens
  - Quick check question: What happens to model consistency when temperature is increased from 0.2 to 1.0?

- Concept: Positional strategy in poker
  - Why needed here: Models adjust hand ranges based on position
  - Quick check question: Why should a player fold more hands from early positions than late positions?

- Concept: Game theory optimal (GTO) strategy
  - Why needed here: Provides benchmark for evaluating model decisions
  - Quick check question: What is the GTO pre-flop action for pocket Aces from any position?

## Architecture Onboarding

- Component map: Prompt → Tokenizer → Transformer layers → Probability distribution → Sampling → Output
- Critical path: Prompt formatting → Token generation → Action selection → Strategy evaluation
- Design tradeoffs: Higher temperature increases exploration but reduces consistency; shorter prompts may be more effective but less explicit
- Failure signatures: Unexpected limps from GPT-4, excessive folding from ChatGPT, inconsistent responses across temperature settings
- First 3 experiments:
  1. Compare verbose vs short prompt outputs for identical hands to quantify formatting impact
  2. Test temperature sweep (0.1 to 1.0) on action consistency for key hands
  3. Swap prompt order (ranked vs unranked cards) to measure positional understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do variations in model temperature and top-p parameters affect the strategic tendencies (e.g., aggression vs. passivity) of ChatGPT and GPT-4 in poker?
- Basis in paper: The paper discusses the impact of temperature and top-p values on decision-making but focuses on temperature=0.2 and top-p=0.95 for optimal results
- Why unresolved: The paper does not explore the full range of temperature and top-p values to determine how they influence the strategic tendencies of the models
- What evidence would resolve it: Conducting experiments with a wider range of temperature and top-p values to observe changes in the models' strategic tendencies

### Open Question 2
- Question: Can fine-tuning or additional training improve the game theory optimal (GTO) performance of ChatGPT and GPT-4 in poker?
- Basis in paper: The paper notes that both models are not GTO despite their understanding of poker fundamentals, suggesting potential for improvement
- Why unresolved: The paper does not explore the effects of fine-tuning or additional training on the models' poker performance
- What evidence would resolve it: Testing the models after fine-tuning or additional training sessions to evaluate improvements in GTO performance

### Open Question 3
- Question: How do ChatGPT and GPT-4 adapt their strategies when playing against opponents with known playing styles?
- Basis in paper: The paper mentions the importance of understanding human behavior and psychology in poker, which is not fully explored
- Why unresolved: The paper does not investigate how the models adjust their strategies based on opponent behavior
- What evidence would resolve it: Simulating games where the models face opponents with distinct playing styles to observe strategic adaptations

## Limitations

- Analysis focuses exclusively on pre-flop decisions, leaving post-flop play unexamined
- Evaluation relies on comparing model outputs against GTO charts rather than empirical performance against human players
- Models were tested in isolation without opponent modeling or adaptive learning capabilities

## Confidence

- **High Confidence**: Models demonstrate basic poker knowledge including hand rankings and positional awareness
- **Medium Confidence**: Identified behavioral patterns (ChatGPT conservative with limping, GPT-4 aggressive with no limping) reflect genuine model tendencies
- **Low Confidence**: Claim that neither model can achieve GTO play without significant guidance

## Next Checks

1. Analyze the training corpus for actual poker-related token frequencies and strategy distributions to confirm whether models' behaviors reflect genuine corpus patterns
2. Test models across all 9 positions with a complete matrix of hand combinations to validate whether observed patterns hold across the full strategy space
3. Evaluate additional poker-specialized models like PokerGPT against ChatGPT and GPT-4 to determine if limitations are inherent to general LLMs or specific to these particular models