---
ver: rpa2
title: Learning to Prompt Knowledge Transfer for Open-World Continual Learning
arxiv_id: '2312.14990'
source_url: https://arxiv.org/abs/2312.14990
tags:
- task
- learning
- knowledge
- prompt
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles open-world continual learning (OwCL), where
  a model must learn a sequence of tasks while handling new classes and identifying
  unknowns without forgetting previous knowledge. The authors propose Pro-KT, a prompt-enhanced
  knowledge transfer method that uses a prompt bank to encode both task-generic and
  task-specific knowledge and a task-aware open-set boundary for distinguishing knowns
  from unknowns.
---

# Learning to Prompt Knowledge Transfer for Open-World Continual Learning

## Quick Facts
- arXiv ID: 2312.14990
- Source URL: https://arxiv.org/abs/2312.14990
- Authors: Yujie Li, Mingrui Liu, Qian Wang, et al.
- Reference count: 10
- One-line primary result: Pro-KT significantly outperforms existing methods in open-world continual learning, achieving up to 42.68 percentage points higher AU CN and 5.19 percentage points higher average final accuracy.

## Executive Summary
This paper addresses the challenge of open-world continual learning (OwCL), where models must learn sequential tasks while identifying unknown classes and avoiding catastrophic forgetting. The authors propose Pro-KT, a prompt-enhanced knowledge transfer method that uses a prompt bank to encode and transfer task knowledge, along with a task-aware open-set boundary for distinguishing knowns from unknowns. Experiments on Split CIFAR100 and 5-datasets demonstrate that Pro-KT significantly outperforms existing methods in both unknown detection and known classification tasks.

## Method Summary
Pro-KT employs a prompt bank mechanism to encode both task-generic and task-specific knowledge, enabling flexible knowledge transfer across sequential tasks. The method uses sample-wise prompt matching to select relevant prompts from the bank, concatenates them with input embeddings, and feeds them to a unified classifier. For open-set detection, Pro-KT learns adaptive thresholds based on softmax entropy distributions of known samples. During testing, it uses task-specific or the latest threshold to classify samples as known or unknown. The framework is trained sequentially on each task, updating the prompt bank and threshold selection mechanisms throughout.

## Key Results
- Pro-KT achieves up to 42.68 percentage points higher AU CN (unknown detection) compared to existing methods
- Pro-KT shows up to 5.19 percentage points higher average final accuracy (known classification) than baseline approaches
- The method demonstrates significant improvements on both Split CIFAR100 and 5-datasets benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The prompt bank enables effective knowledge transfer by storing both task-generic and task-specific prompts that can be flexibly selected for new tasks.
- Mechanism: Pro-KT maintains a prompt bank that stores learned prompts from all previous tasks. During training of a new task, it uses a sample-wise prompt matching mechanism to select the most relevant prompts from the bank and concatenates them with input embeddings before classification.
- Core assumption: Prompts can effectively encode task knowledge and that the cosine similarity-based matching can identify the most relevant prompts for each input.
- Evidence anchors:
  - [abstract] "a prompt bank to encode and transfer both task-generic and task-specific knowledge"
  - [section] "We propose to use a prompt bank for knowledge transfer... These prompts serve as instructions for directing the model in task execution"
  - [corpus] Weak - The related papers discuss open-world learning but don't specifically validate prompt-based knowledge transfer mechanisms
- Break condition: If the prompt matching mechanism fails to identify relevant prompts, or if the prompts cannot effectively encode the necessary task knowledge, the knowledge transfer would break down.

### Mechanism 2
- Claim: The task-aware open-set boundary with adaptive threshold selection can effectively distinguish knowns from unknowns in new tasks.
- Mechanism: Pro-KT learns a threshold based on the mean softmax entropy of prompt-enhanced training samples for each task. During testing, it uses either task-specific thresholds (when task IDs are available) or the latest threshold (when task IDs are unavailable) to classify samples as known or unknown.
- Core assumption: The distribution of softmax entropy values for known samples is sufficiently distinct from unknown samples that a threshold can effectively separate them.
- Evidence anchors:
  - [abstract] "a task-aware open-set boundary to identify unknowns in the new tasks"
  - [section] "we devise two simple but efficient strategies to choose the most appropriate threshold for the open-set detection boundary"
  - [corpus] Missing - The related papers discuss open-world learning but don't specifically validate threshold-based unknown detection
- Break condition: If the entropy distributions of known and unknown samples overlap significantly, or if the threshold adaptation fails to keep pace with changing data distributions, the boundary would become ineffective.

### Mechanism 3
- Claim: The prompt-enhanced embeddings improve classification performance by incorporating relevant task knowledge into the representation space.
- Mechanism: Input samples are projected into a feature space, matched with relevant prompts from the prompt bank, and concatenated to form enhanced embeddings that are fed to the classifier.
- Core assumption: Concatenating relevant prompt embeddings with input features creates a more informative representation that improves classification accuracy.
- Evidence anchors:
  - [section] "The enriched embeddings are then forwarded to the unified classifier, yielding unscaled logits scores"
  - [section] "After this, if a sample is detected as an unknown object, the model then annotates the sample with [unknown] for future tasks"
  - [corpus] Weak - While the related papers discuss continual learning, they don't specifically validate the effectiveness of prompt-enhanced embeddings
- Break condition: If the prompt-enhanced embeddings introduce noise or irrelevant information, or if the concatenation doesn't create meaningful feature interactions, classification performance would degrade.

## Foundational Learning

- Concept: Continual Learning
  - Why needed here: The paper addresses learning a sequence of tasks without forgetting previous knowledge, which is the core challenge of continual learning
  - Quick check question: What is catastrophic forgetting and how does it affect sequential task learning?

- Concept: Open-World Learning
  - Why needed here: The model must handle unknown classes that appear in test data but weren't in training, which requires open-world recognition capabilities
  - Quick check question: How does open-world learning differ from traditional closed-world classification?

- Concept: Prompt Learning
  - Why needed here: The paper uses prompts to encode and transfer knowledge between tasks, leveraging the effectiveness of prompt-based approaches
  - Quick check question: What is the difference between prompt tuning and full fine-tuning of large language models?

## Architecture Onboarding

- Component map:
  - Query Function -> Prompt Bank -> Sample-wise Matching -> Pre-trained Backbone -> Classifier -> Threshold Selection

- Critical path: Input → Query Function → Prompt Matching → Embedding Concatenation → Backbone → Classifier → Threshold Decision

- Design tradeoffs:
  - Prompt bank size vs. knowledge transfer effectiveness
  - Prompt selection size vs. computational efficiency
  - Task-specific vs. task-agnostic threshold selection
  - Fixed vs. adaptive threshold strategies

- Failure signatures:
  - High false positive rate for unknown detection
  - Significant accuracy drop on previous tasks (forgetting)
  - Poor prompt matching leading to irrelevant knowledge transfer
  - Threshold saturation or oscillation

- First 3 experiments:
  1. Verify prompt bank stores and retrieves prompts correctly
  2. Test sample-wise matching selects appropriate prompts
  3. Validate threshold selection effectively separates knowns from unknowns on validation data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can advanced clustering methods be integrated to improve fine-grained classification of unknown objects in OwCL?
- Basis in paper: [explicit] The authors mention this as a future direction, stating "integrating advanced clustering methods for the fine-grained classification of unknowns."
- Why unresolved: Current methods may not adequately address the nuances and variations within unknown object categories, leading to less precise classification.
- What evidence would resolve it: Comparative studies demonstrating improved classification accuracy and precision of unknown objects when advanced clustering methods are employed in OwCL settings.

### Open Question 2
- Question: What are the optimal forms of knowledge representation for unknowns that can be leveraged to aid future task learning in OwCL?
- Basis in paper: [explicit] The authors propose exploring "the forms of knowledge for unknowns and further using the knowledge to help future task learning."
- Why unresolved: Existing approaches may not fully capture the complexity and diversity of unknown objects, limiting their utility in subsequent learning tasks.
- What evidence would resolve it: Experimental results showing enhanced performance in future tasks when optimized knowledge representations for unknowns are utilized, demonstrating their effectiveness in knowledge transfer.

### Open Question 3
- Question: How does the size and composition of the prompt bank affect the adaptability and performance of the model in highly dissimilar task sequences?
- Basis in paper: [explicit] The authors discuss parameter sensitivity analysis and note that "when tasks are more dissimilar, the M is more sensitive."
- Why unresolved: The impact of prompt bank size and composition on model performance in diverse task scenarios is not fully understood, particularly for highly dissimilar tasks.
- What evidence would resolve it: Empirical studies showing the relationship between prompt bank size, task similarity, and model performance, identifying optimal configurations for various task diversity levels.

## Limitations

- The effectiveness of prompt-based knowledge transfer is primarily validated on synthetic task boundaries and curated datasets, which may not reflect real-world complexity
- The adaptive threshold mechanism's robustness under varying data distributions and potential for threshold saturation is not thoroughly explored
- The generalizability of the approach to tasks with significantly different characteristics (e.g., text vs. image) is not tested

## Confidence

- **High Confidence**: The overall framework design and the reported performance improvements on the evaluated datasets
- **Medium Confidence**: The generalizability of the prompt bank mechanism to tasks with significantly different characteristics
- **Medium Confidence**: The robustness of the adaptive threshold selection under varying data conditions

## Next Checks

1. **Prompt Bank Generalization Test**: Evaluate Pro-KT on a dataset with tasks that have significantly different data distributions (e.g., combining text and image tasks) to verify the prompt bank's ability to handle heterogeneous knowledge.

2. **Threshold Robustness Analysis**: Conduct a systematic ablation study varying the threshold selection hyperparameters and measuring performance across different data distributions to identify potential failure modes and saturation points.

3. **Long-Tailed Task Sequence Evaluation**: Test Pro-KT on sequences with imbalanced task distributions and varying numbers of classes per task to assess performance under realistic, non-uniform task conditions.