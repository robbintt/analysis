---
ver: rpa2
title: 'Games for Artificial Intelligence Research: A Review and Perspectives'
arxiv_id: '2304.13269'
source_url: https://arxiv.org/abs/2304.13269
tags:
- games
- game
- research
- intelligence
- python
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews and categorizes publicly available game-based
  platforms for AI research, covering 53 games and platforms published between 2016-2022.
  The survey identifies trends including growing interest in open-world games for
  artificial general intelligence, under-explored modern game genres, and emerging
  environments for AI creativity.
---

# Games for Artificial Intelligence Research: A Review and Perspectives

## Quick Facts
- arXiv ID: 2304.13269
- Source URL: https://arxiv.org/abs/2304.13269
- Reference count: 17
- Key outcome: This paper reviews and categorizes publicly available game-based platforms for AI research, covering 53 games and platforms published between 2016-2022.

## Executive Summary
This comprehensive survey examines 53 game-based platforms for AI research published between 2016-2022, identifying key trends and research directions. The paper reveals that while most environments focus on tile-based level generation, there is growing interest in open-world games for artificial general intelligence and under-explored modern game genres. The authors find that 13 new environments for content design have emerged since 2016, but accessibility remains a challenge for advancing AI creativity research.

## Method Summary
The survey methodology involved reviewing major AI conference competitions (NeurIPS, IJCAI, AAAI, etc.), conducting Google Scholar searches with specific terms, and verifying source code availability. The authors systematically categorized platforms by game genres, number of agents, observability, content types, programming languages, and associated competitions to create a comprehensive taxonomy of game-based AI research environments.

## Key Results
- Most game-based environments focus on tile-based level generation rather than diverse content types
- Modern game genres like RPGs pose new challenges requiring real-time decision-making and player modeling
- 13 new environments for content design have emerged since 2016, but accessibility remains limited

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Game-based platforms serve as effective test-beds for AI research due to their controllable, cost-effective, and realistic simulation environments.
- Mechanism: Games abstract real-world complexities into manageable environments where AI agents can safely explore, learn, and be evaluated under varying conditions without real-world risks or costs.
- Core assumption: The characteristics that make games suitable for AI research (controllability, cost-effectiveness, realistic simulation) are transferable to real-world problem-solving scenarios.
- Evidence anchors:
  - [abstract]: "Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios."
  - [section]: "Games have played an essential role in the rapid development of artificial intelligence (AI). As a controllable, cost-effective, and realistic simulator, games provide a virtual world for AI to explore."
  - [corpus]: Weak corpus support - only 5 related papers found with average neighbor FMR=0.321, suggesting limited direct evidence in current literature.
- Break condition: If the game environment fails to accurately represent real-world dynamics or becomes too complex for meaningful AI learning.

### Mechanism 2
- Claim: The diversity of game genres and mechanics creates diverse challenges that drive AI research advancement.
- Mechanism: Different game types (board games, video games, strategy games) require different AI approaches, pushing the development of generalized AI capabilities across multiple domains.
- Core assumption: Solving increasingly complex game challenges translates to improved AI capabilities for real-world applications.
- Evidence anchors:
  - [abstract]: "Various games have been used in game-based platforms, from single-player to multi-player, and from puzzles to real-time strategy games, with the emergence of diverse game genres and mechanisms."
  - [section]: "Such diversity produces a large number of diverse challenges for AI to solve. For instance, it has been demonstrated in various games (e.g., Chess, Atari, Poker, Go, AlphaStar II and Dota 2) that AI can perform at a level beyond that of humans."
  - [corpus]: No direct corpus support for this mechanism.
- Break condition: When game diversity becomes too fragmented, preventing the development of transferable AI skills.

### Mechanism 3
- Claim: Game-based platforms enable safe exploration of "what-if" scenarios that are difficult to test in real-world conditions.
- Mechanism: Virtual game environments allow researchers to test AI responses to edge cases, failure modes, and novel situations without real-world consequences.
- Core assumption: Insights gained from game-based experimentation can inform real-world AI deployment strategies.
- Evidence anchors:
  - [abstract]: "Game-based platforms have provided an ideal playground for AI researchers to study, explore, evaluate and experiment with different ideas in a controllable and safe environment. Such platforms also enable us to study and answer what-if questions that are hard to do safely in the real world."
  - [section]: "They can even stimulate researchers to ask new research questions."
  - [corpus]: No direct corpus support for this mechanism.
- Break condition: If game simulations oversimplify real-world physics or social dynamics, leading to misleading results.

## Foundational Learning

- Concept: Procedural Content Generation (PCG)
  - Why needed here: PCG is fundamental to creating diverse game environments that can test AI generalization and creativity.
  - Quick check question: How does PCG differ from traditional content creation in terms of scalability and AI learning opportunities?

- Concept: Multi-agent Reinforcement Learning
  - Why needed here: Many game-based platforms involve multiple agents competing or collaborating, requiring advanced RL techniques.
  - Quick check question: What are the key differences between single-agent and multi-agent RL in game environments?

- Concept: Domain-Specific Languages for Game Description
  - Why needed here: Languages like VGDL (Video Game Description Language) enable rapid prototyping of new game scenarios for AI testing.
  - Quick check question: How does VGDL facilitate the creation of new games compared to traditional programming approaches?

## Architecture Onboarding

- Component map: Game Environment Layer -> Interface Layer -> AI Agent -> Performance Evaluation
- Critical path: Game Environment → Interface Layer → AI Agent → Performance Evaluation
- Design tradeoffs:
  - Complexity vs. Accessibility: More complex games offer richer AI challenges but may be harder to implement and use
  - Realism vs. Control: Highly realistic simulations may be harder to control and reproduce than simplified environments
  - Generalizability vs. Specialization: Platforms that support many game types may sacrifice depth in specific domains

- Failure signatures:
  - AI agents that perform well in games but fail to generalize to real-world scenarios
  - Platforms that become too complex for new researchers to use effectively
  - Game environments that create unrealistic expectations about AI capabilities

- First 3 experiments:
  1. Implement a simple agent using the Gym interface on a classic Atari game to understand basic RL concepts
  2. Test a multi-agent scenario in the Neural MMO environment to explore collaborative and competitive dynamics
  3. Use the GVGAI platform to create a new simple game and test how existing agents adapt to it, examining generalization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific barriers preventing the development of more open-sourced game-based platforms with diverse genres and content types?
- Basis in paper: [explicit] The paper states "More open-sourced environments with diverse genres and content types, user-friendly APIs and documents, are expected to advance the research in AI creativity and human-AI co-creativity."
- Why unresolved: The paper identifies the need but does not specify what technical, resource, or design barriers prevent their development.
- What evidence would resolve it: Case studies of existing platform development efforts, surveys of platform developers, or analysis of technical challenges in implementing diverse game genres.

### Open Question 2
- Question: How can game description languages like VGDL be extended or improved to support modern game genres beyond classic arcade-style games?
- Basis in paper: [explicit] The paper mentions that VGDL facilitates adding new games to GVGAI but suggests "Using game description language or new ways to facilitate the implementation of games will help enrich the scenarios and tasks for studying AI."
- Why unresolved: The paper acknowledges the limitation but doesn't propose specific extensions or improvements needed for modern genres.
- What evidence would resolve it: Prototype implementations of VGDL extensions for modern genres, comparative studies of different description languages, or analysis of specific limitations in current approaches.

### Open Question 3
- Question: What metrics or evaluation frameworks can effectively measure AI creativity and human-AI co-creativity in game content generation?
- Basis in paper: [explicit] The paper discusses emerging environments for AI creativity but notes that "the environments are rarely open-sourced" and implies a need for better evaluation methods.
- Why unresolved: The paper identifies the importance of evaluating AI creativity but doesn't propose specific metrics or frameworks for assessment.
- What evidence would resolve it: Development and validation of new evaluation metrics, comparative studies of existing approaches, or case studies of successful creativity assessments in game AI research.

## Limitations
- The survey methodology relies heavily on publicly available platforms, potentially missing valuable but proprietary or recently developed environments
- With only 53 platforms identified, the sample size may not fully represent the diversity of game-based AI research
- The categorization system, while systematic, may not capture all nuances of platform capabilities and design philosophies

## Confidence
- High Confidence: The taxonomy of playing and designing tasks, as this is based on direct platform analysis and clearly defined criteria
- Medium Confidence: The identification of research trends, as these are inferred from platform characteristics and require interpretation of the broader research landscape
- Low Confidence: The assessment of unexplored modern game genres, as this depends on the authors' perspective and may overlook niche or emerging research areas

## Next Checks
1. Verify the completeness of the platform search by conducting independent searches using different search terms and databases to identify potentially missed platforms
2. Test the categorization framework by applying it to a subset of platforms to ensure consistent and accurate classification across different reviewers
3. Validate the identified research trends by analyzing recent conference proceedings and publications to confirm the observed patterns in game-based AI research focus areas