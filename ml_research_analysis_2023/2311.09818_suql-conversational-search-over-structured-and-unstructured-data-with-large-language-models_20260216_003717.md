---
ver: rpa2
title: 'SUQL: Conversational Search over Structured and Unstructured Data with Large
  Language Models'
arxiv_id: '2311.09818'
source_url: https://arxiv.org/abs/2311.09818
tags:
- suql
- data
- association
- structured
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SUQL, a query language that extends SQL with
  free-text primitives to enable hybrid structured and unstructured data access. The
  authors present a conversational agent based on large language models that can parse
  user queries into SUQL and execute them over real-world restaurant data combining
  structured fields and unstructured reviews.
---

# SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models

## Quick Facts
- arXiv ID: 2311.09818
- Source URL: https://arxiv.org/abs/2311.09818
- Authors: 
- Reference count: 27
- Key outcome: SUQL achieves 93.8% retrieval precision on single-turn questions and 89.3% on conversations, outperforming linearization baselines by 36.8% and 24.3% respectively.

## Executive Summary
This paper introduces SUQL, a query language that extends SQL with free-text primitives to enable hybrid structured and unstructured data access. The authors present a conversational agent based on large language models that can parse user queries into SUQL and execute them over real-world restaurant data combining structured fields and unstructured reviews. Experiments on a crowdsourced dataset show that their few-shot approach achieves high retrieval precision and semantic parsing accuracy, demonstrating the effectiveness of SUQL for hybrid data sources.

## Method Summary
The authors introduce SUQL, a query language that extends SQL with free-text primitives (answer, summary) to enable precise hybrid data access. They build a conversational agent that uses a few-shot semantic parser to convert natural language queries into SUQL, which is then executed by a SUQL compiler with optimizations. The system is evaluated on a crowdsourced dataset of restaurant-related questions and conversations, demonstrating high retrieval precision and semantic parsing accuracy compared to linearization baselines.

## Key Results
- SUQL achieves 93.8% retrieval precision on single-turn questions and 89.3% on conversations.
- SUQL semantic parser achieves 94.0% accuracy on single-turn and 91.4% on conversational queries.
- SUQL significantly outperforms linearization baselines by 36.8% and 24.3% on retrieval precision.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SUQL extends SQL with free-text primitives (answer, summary) to enable precise hybrid data access.
- Mechanism: By augmenting SQL with functions that can query both structured columns and unstructured text fields, SUQL allows users to express complex queries that span both data types in a single, formal language.
- Core assumption: Free-text primitives can be implemented effectively using LLMs to answer questions or summarize text.
- Evidence anchors:
  - [abstract] "Specifically, SUQL extends SQL with free-text primitives (summary and answer), so information retrieval can be composed with structured data accesses arbitrarily in a formal, succinct, precise, and interpretable notation."
  - [section] "In SUQL, we introduce a new type, Free Text. An attribute with SQL type Text can be declared with the type Free Text. SUQL augments SQL with three important free-text primitives..."
  - [corpus] Weak evidence - related work focuses on hybrid retrieval but lacks direct SUQL comparisons.
- Break condition: LLM-based answer/summary functions fail to provide accurate or relevant responses for the free-text queries.

### Mechanism 2
- Claim: The SUQL compiler optimizes query execution by prioritizing structured predicates and using lazy evaluation.
- Mechanism: The compiler converts filter predicates to DNF, executes structured predicates first, and uses lazy evaluation to stop processing once enough results are found, improving efficiency.
- Core assumption: Structured predicates are computationally cheaper than LLM-based free-text queries.
- Evidence anchors:
  - [section] "Specifically, for each SELECT clause with filter predicates, the SUQL compiler converts them to disjoint normal form (DNF)... Within each AND, it prioritizes executing structured predicates over executing answer functions."
  - [section] "The SUQL compiler adopts this concept. Specifically, when a LIMIT n clause is present, it no longer evaluates the remaining rows once the required number of rows has been filled."
  - [corpus] No direct evidence - optimization details are unique to this work.
- Break condition: The assumption that structured predicates are always cheaper fails in cases where complex joins or aggregations are required.

### Mechanism 3
- Claim: Few-shot in-context learning with LLMs can effectively parse natural language into SUQL queries.
- Mechanism: The semantic parser uses a few-shot prompt with conversation history, schema definition, and examples to generate the corresponding SUQL query for the latest user utterance.
- Core assumption: Current LLMs can learn the SUQL syntax and semantics from a small number of examples.
- Evidence anchors:
  - [abstract] "We show that this language can be effectively parsed by current LLMs, with a high accuracy of 94.0% and 91.4% on single-turn questions and conversations."
  - [section] "At turn n, given a dialogue history, the agent first classifies whether accessing the database is needed... Next, a few-shot semantic parser generates a SUQL query given the conversation history."
  - [corpus] Weak evidence - related work uses few-shot prompting but not specifically for SUQL.
- Break condition: LLMs struggle to generalize SUQL syntax beyond the provided few-shot examples or fail to capture complex user intents.

## Foundational Learning

- Concept: Semantic parsing
  - Why needed here: SUQL is a formal language that needs to be translated from natural language user queries.
  - Quick check question: What is the difference between semantic parsing and semantic role labeling?

- Concept: In-context learning
  - Why needed here: The semantic parser uses few-shot prompts to learn SUQL syntax and semantics without explicit training.
  - Quick check question: How does in-context learning differ from traditional fine-tuning?

- Concept: Hybrid data retrieval
  - Why needed here: SUQL combines structured data access (SQL) with unstructured data retrieval (LLM-based functions).
  - Quick check question: What are the challenges in retrieving information from both structured and unstructured data sources?

## Architecture Onboarding

- Component map:
  - User Interface -> Input Classifier -> Semantic Parser -> SUQL Compiler -> Database/LLM Service -> User Interface

- Critical path:
  1. User query → Input Classifier
  2. If database access needed → Semantic Parser
  3. SUQL query → SUQL Compiler
  4. Results → LLM response generation → User

- Design tradeoffs:
  - SUQL vs. linearization: SUQL provides precise, compositional queries but requires a custom compiler; linearization is simpler but less expressive.
  - Few-shot vs. fine-tuning: Few-shot is faster and requires no training data but may have lower accuracy; fine-tuning can achieve higher accuracy but requires labeled data.

- Failure signatures:
  - Low semantic parsing accuracy: Few-shot examples may not cover all query patterns.
  - Inaccurate free-text results: LLM-based answer/summary functions may fail on complex or domain-specific queries.
  - Slow query execution: Compiler optimizations may not be effective for all query patterns.

- First 3 experiments:
  1. Measure semantic parsing accuracy on a held-out set of user queries.
  2. Compare retrieval precision of SUQL system vs. linearization baseline on the crowdsourced dataset.
  3. Profile query execution time for different types of SUQL queries to identify optimization opportunities.

## Open Questions the Paper Calls Out

- Question: How does the SUQL approach scale to larger databases with millions of rows?
  - Basis in paper: [inferred] The paper mentions that their approach is applicable to "large databases" and experiments with "thousands of rows," but doesn't explicitly address scalability to much larger datasets.
  - Why unresolved: The paper only tests SUQL on a dataset with 1,828 restaurants. There's no empirical evidence or analysis of how the approach would perform with significantly larger datasets.
  - What evidence would resolve it: Running experiments with progressively larger databases (10K, 100K, 1M+ rows) to measure query execution time, retrieval precision, and semantic parsing accuracy.

- Question: How does SUQL handle temporal data (dates, times) in structured columns?
  - Basis in paper: [explicit] The paper mentions various data types in the restaurant database (name, cuisines, price, rating, etc.) but doesn't discuss how SUQL would handle temporal data.
  - Why unresolved: Temporal data is common in many real-world databases, but the paper doesn't address how SUQL's operators would work with date/time fields or how temporal reasoning would be incorporated into free-text primitives.
  - What evidence would resolve it: Demonstrating SUQL queries that involve temporal constraints (e.g., "Find Italian restaurants that were rated highly last month") and showing how the system handles these cases.

- Question: How robust is the SUQL semantic parser to domain shifts and out-of-distribution questions?
  - Basis in paper: [inferred] The paper shows high semantic parsing accuracy (94.0% on single-turn, 91.4% on conversational) on the restaurant domain, but doesn't test robustness to other domains or unusual questions.
  - Why unresolved: The experiments are limited to restaurant-related questions. It's unclear how well the few-shot approach would generalize to completely different domains or handle questions that don't fit typical patterns.
  - What evidence would resolve it: Testing SUQL on multiple diverse domains (medical, financial, legal, etc.) and including adversarial or out-of-distribution questions to measure robustness.

## Limitations

- The paper's claims about SUQL's effectiveness rely heavily on the accuracy of LLM-based answer and summary functions, which are not directly evaluated.
- The few-shot semantic parser's performance could degrade significantly if the prompt examples are not representative of real-world query diversity.
- The optimization benefits of the SUQL compiler are assumed rather than empirically validated, and the system's performance on datasets with different characteristics (e.g., larger scale, different domains) remains untested.

## Confidence

- High confidence in the core mechanism of extending SQL with free-text primitives (supported by detailed technical specification and evaluation).
- Medium confidence in the semantic parser's accuracy claims (based on evaluation metrics but with limited details on prompt design).
- Medium confidence in the optimization benefits (mechanism described but not empirically validated).
- Low confidence in generalizability claims (evaluation limited to restaurant domain with crowdsourced queries).

## Next Checks

1. **Ablation study on free-text primitives**: Compare retrieval precision when using only structured queries vs. full SUQL with answer/summary functions to quantify the contribution of free-text capabilities.

2. **Semantic parser robustness test**: Evaluate the few-shot parser on out-of-distribution queries (different domains, complex multi-turn conversations) to assess generalization limits.

3. **Compiler optimization validation**: Measure query execution times for various SUQL patterns with and without compiler optimizations to empirically verify the claimed performance benefits.