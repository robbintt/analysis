---
ver: rpa2
title: 'Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of
  the Driver''s License Knowledge Test'
arxiv_id: '2308.11827'
source_url: https://arxiv.org/abs/2308.11827
tags:
- context
- prompt
- gpt-3
- questions
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the use of GPT-3 in answering driving knowledge\
  \ test questions by providing contextual information from the California Driver\u2019\
  s Handbook. The proposed method involves preprocessing the context, embedding queries\
  \ and contexts, constructing prompts by integrating relevant contexts, and generating\
  \ answers using GPT-3."
---

# Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test

## Quick Facts
- arXiv ID: 2308.11827
- Source URL: https://arxiv.org/abs/2308.11827
- Reference count: 22
- GPT-3 with context achieved 96% passing score vs 82% without context

## Executive Summary
This paper investigates whether GPT-3 can effectively answer domain-specific questions when provided with relevant context from the California Driver's Handbook. The study demonstrates that retrieval-augmented generation (RAG) significantly improves performance on driving knowledge tests, achieving a 96% passing score compared to 82% without context. The research highlights key factors affecting performance including prompt length, text formatting, and the model's tendency to hallucinate, providing insights for applying LLMs to specialized question-answering tasks.

## Method Summary
The methodology involves preprocessing the California Driver's Handbook into chunks, generating embeddings for each chunk using text-embedding-ada-002, and retrieving the most relevant chunks for each DMV question using cosine similarity. These relevant chunks are prepended to prompts with the question, creating a context-aware input for GPT-3 (text-davinci-003). The study tested various prompt lengths (500, 1200, and 1900 tokens) and formatting variations (bullet points vs comma-separated text) to optimize performance. Answers are generated using GPT-3 with temperature 0.0 and compared against correct answers to calculate passing scores.

## Key Results
- GPT-3 achieved 96% passing score (48/50 correct) with context vs 82% (41/50) without context
- Optimal prompt length was 1900 tokens, with no improvement from 500 to 1200 tokens
- Text formatting significantly impacted performance - bullet points improved accuracy vs comma-separated text
- GPT-3 still failed on some questions even with context, indicating room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context retrieval via embeddings significantly improves GPT-3's performance on domain-specific questions.
- Mechanism: The method preprocesses the California Driver's Handbook into chunks, computes embeddings for each chunk, and retrieves the most relevant chunks for each question using cosine similarity. These relevant chunks are prepended to the prompt, providing GPT-3 with necessary domain-specific knowledge that was not in its training data.
- Core assumption: Embedding similarity accurately reflects semantic relevance for the retrieval task, and the retrieved chunks contain the answer.
- Evidence anchors:
  - [abstract] "The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models."
  - [section] "The resulting tabular dataset retained the 'title' and 'heading' columns but replaced the 'content' and 'tokens' columns with 1536-dimensional embedding vectors."
  - [corpus] Weak: No direct corpus evidence on retrieval-augmented performance for this specific method; relies on general RAG literature.
- Break condition: If the embeddings fail to capture semantic similarity accurately, or if the answer is not present in the retrieved chunks, the performance will degrade to the no-context baseline.

### Mechanism 2
- Claim: Prompt length has a non-linear effect on GPT-3's performance, with an optimal range balancing context and model capacity.
- Mechanism: Experiments tested prompt lengths of 500, 1200, and 1900 tokens. Performance improved from 82% (no context) to 96% at 1900 tokens, but did not improve from 500 to 1200 tokens, suggesting a threshold effect where only prompts above a certain length include the necessary context.
- Core assumption: The model's performance is constrained by the presence of the answer within the prompt, and excessive context can dilute focus.
- Evidence anchors:
  - [section] "Our findings indicated that a 1900-word prompt length delivered optimal results for most questions."
  - [section] "Extending the prompt length from 500 to 1200 tokens did not yield improved performance, as the supplementary paragraphs were superfluous and devoid of pertinent information."
  - [corpus] Weak: No corpus evidence on optimal prompt length for this specific domain; relies on general findings in prompt engineering.
- Break condition: If the answer is consistently located in a fixed section, shorter prompts might suffice, making the 1900-token length unnecessary.

### Mechanism 3
- Claim: Text formatting (e.g., bullet points vs. comma-separated text) affects GPT-3's ability to parse and use context correctly.
- Mechanism: The study found that when context text was reformatted from bullet points to comma-separated text, GPT-3's accuracy dropped. Reintroducing bullet points restored accuracy, indicating the model's sensitivity to formatting cues for information extraction.
- Core assumption: GPT-3's internal representations are sensitive to surface-level text formatting, not just semantic content.
- Evidence anchors:
  - [section] "the GPT-3 model generated incorrect answers when the context was delivered without bullet points. In contrast, when the context was formatted with bullet points, the model's output aligned with the correct answer."
  - [section] "GPT-3 model is sensitive to text formatting."
  - [corpus] Weak: No corpus evidence on formatting sensitivity; this is a novel finding from the study.
- Break condition: If the model is fine-tuned or uses a tokenizer that normalizes formatting, this sensitivity might disappear.

## Foundational Learning

- Concept: Vector embeddings and cosine similarity for semantic search.
  - Why needed here: The method relies on embedding chunks of text and using cosine similarity to retrieve the most relevant context for each question.
  - Quick check question: What does a high cosine similarity between a query embedding and a document chunk embedding indicate?

- Concept: Token limits and prompt engineering in large language models.
  - Why needed here: GPT-3 has a maximum token limit (2048 tokens), and the study had to balance prompt length with the inclusion of relevant context.
  - Quick check question: Why did the study choose 1900 tokens as the optimal prompt length instead of the maximum 2048?

- Concept: Hallucination in language models and strategies to mitigate it.
  - Why needed here: The study observed that GPT-3 sometimes generated incorrect answers even with context, and introduced a control statement to reduce hallucination.
  - Quick check question: What control statement was added to the prompt to mitigate hallucination?

## Architecture Onboarding

- Component map: PDF conversion -> Chunking -> Embedding generation -> Similarity search -> Prompt construction -> Answer generation
- Critical path: Embedding query -> Retrieve top-k relevant chunks -> Concatenate to prompt -> Generate answer
- Design tradeoffs: Longer prompts increase context but risk exceeding token limits and diluting focus; shorter prompts risk missing the answer
- Failure signatures: Performance drops to no-context baseline if embeddings fail to retrieve relevant chunks; formatting changes can cause accuracy drops
- First 3 experiments:
  1. Test GPT-3 on questions without any context to establish baseline (already done: 82%)
  2. Vary prompt length (500, 1200, 1900 tokens) with context to find optimal length
  3. Compare performance with and without bullet points in context formatting to quantify formatting sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-3 change when using context from non-public or proprietary documents compared to publicly available documents like the California Driver's Handbook?
- Basis in paper: explicit
- Why unresolved: The paper only tested GPT-3 with context from the publicly available California Driver's Handbook. The model's performance with non-public or proprietary documents is unknown.
- What evidence would resolve it: Conducting experiments using GPT-3 with context from non-public or proprietary documents and comparing its performance to that with public documents.

### Open Question 2
- Question: What is the optimal prompt length for GPT-3 to achieve the best performance in question-answering tasks across different domains and types of questions?
- Basis in paper: explicit
- Why unresolved: The paper found that a 1900-word prompt length worked best for the California Driver's License test, but this may not be optimal for other domains or question types. The optimal prompt length may vary depending on the complexity and specificity of the questions.
- What evidence would resolve it: Conducting experiments with GPT-3 using different prompt lengths across various domains and question types to determine the optimal prompt length for each scenario.

### Open Question 3
- Question: How does the inclusion of visual elements, such as images and tables, in the context affect GPT-3's performance in question-answering tasks?
- Basis in paper: explicit
- Why unresolved: The paper manually excluded visual elements from the California Driver's Handbook when creating the tabular dataset. The impact of including these elements on GPT-3's performance is unknown.
- What evidence would resolve it: Conducting experiments with GPT-3 using context that includes visual elements and comparing its performance to that with text-only context.

## Limitations

- Limited generalizability due to evaluation on only one domain (California driving rules) and small sample size (50 questions)
- Lack of systematic exploration of formatting sensitivity - only tested bullet points vs comma-separated text
- Missing actual questions used, preventing independent verification or reproduction of exact results
- Reliance on single LLM model (GPT-3 text-davinci-003) and embedding model limits applicability to other model combinations

## Confidence

- **High Confidence**: Context retrieval via embeddings improves GPT-3's performance on domain-specific questions (from 82% to 96% passing score)
- **Medium Confidence**: Findings about prompt length optimization (1900 tokens optimal) and formatting sensitivity are based on systematic testing but underlying mechanisms remain somewhat speculative
- **Low Confidence**: Claim about hallucination mitigation through control statements is mentioned but not quantitatively evaluated or compared against baselines

## Next Checks

1. **Cross-domain replication**: Test the same retrieval-augmented approach on driving knowledge tests from different states or countries to assess whether the 96% accuracy generalizes beyond California's specific context.

2. **Ablation study on formatting**: Systematically test GPT-3's performance across multiple formatting variations (plain text, bullet points, numbered lists, bold text) to quantify the exact impact of each formatting element on accuracy.

3. **Human evaluation of context relevance**: Have human experts assess whether the retrieved context chunks actually contain the answers to the questions, independently verifying that the embedding-based retrieval is functioning correctly rather than just measuring end-to-end accuracy.