---
ver: rpa2
title: 'Generating with Confidence: Uncertainty Quantification for Black-box Large
  Language Models'
arxiv_id: '2305.19187'
source_url: https://arxiv.org/abs/2305.19187
tags:
- uncertainty
- coqa
- confidence
- trivia
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates uncertainty quantification (UQ) for black-box
  large language models (LLMs) in natural language generation (NLG). It introduces
  the distinction between uncertainty (a property of the input) and confidence (a
  property of a particular generated response).
---

# Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models

## Quick Facts
- arXiv ID: 2305.19187
- Source URL: https://arxiv.org/abs/2305.19187
- Reference count: 40
- Black-box LLM uncertainty quantification using pairwise response similarity is effective and practical

## Executive Summary
This paper addresses uncertainty quantification (UQ) for black-box large language models in natural language generation, introducing a clear distinction between uncertainty (a property of the input) and confidence (a property of a particular generated response). The authors propose simple, white-box-free metrics based on pairwise similarity of multiple generated responses to quantify these quantities. These metrics are evaluated on three datasets and three LLMs, using GPT-3.5-turbo as an automatic judge. Results show that simple metrics, particularly those using entailment-based similarity, can effectively identify difficult questions and predict response quality, significantly outperforming prior white-box baselines and demonstrating that black-box UQ is feasible for real-world LLM deployment.

## Method Summary
The method generates multiple responses per input using a black-box LLM, then computes pairwise similarities between responses using either Jaccard similarity or entailment-based similarity from a DeBERTa-large NLI model. These similarities are used to construct a graph where nodes represent responses and edges represent similarity scores. From this graph, various metrics are derived: uncertainty metrics quantify input dispersion (NumSet, EigV), while confidence metrics quantify response reliability (Deg, Ecc). The approach is evaluated on three datasets (CoQA, TriviaQA, Natural Questions) using AUROC for predicting accuracy and AUARC for evaluating uncertainty/confidence metrics against GPT-3.5-turbo as an automatic judge.

## Key Results
- Simple black-box metrics based on pairwise response similarity effectively quantify uncertainty and confidence
- Entailment-based similarity outperforms lexical (Jaccard) similarity for semantic understanding
- Best metrics significantly outperform prior white-box baselines, demonstrating feasibility of black-box UQ
- Degree and eccentricity metrics perform best for confidence estimation, while eigenvalue-based metrics work well for uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise similarity of generated responses can serve as a proxy for semantic dispersion, which in turn indicates uncertainty.
- Mechanism: Multiple responses are generated for the same input, and pairwise similarity scores (Jaccard or entailment-based) are computed. High dispersion (low similarity) among responses signals high uncertainty, while clustered responses indicate low uncertainty.
- Core assumption: Generated responses are conditionally independent given the input and model parameters.
- Evidence anchors:
  - [abstract] "propose several simple, white-box-free metrics to quantify these quantities, based on pairwise similarity of multiple generated responses"
  - [section 4.1] "we list methods to convert such similarities into uncertainty metrics"
  - [corpus] "COPU: Conformal Prediction for Uncertainty Quantification in Natural Language Generation" - similar focus on UQ for NLG using model-agnostic methods
- Break condition: If responses are conditionally dependent (e.g., due to prompt priming or decoding strategies that encourage diversity), the similarity-based dispersion may no longer reflect true uncertainty.

### Mechanism 2
- Claim: Graph Laplacian eigenvalues encode cluster structure of responses, allowing quantification of the number of semantic meanings.
- Mechanism: Responses are treated as nodes in a graph with edge weights from pairwise similarity. The spectrum of the normalized graph Laplacian reveals the number of connected components (semantic clusters). The sum of adjusted eigenvalues (1 - λk for small λk) estimates the effective number of semantic meanings.
- Core assumption: The semantic similarity graph is approximately block-diagonal when responses fall into distinct meaning groups.
- Evidence anchors:
  - [section 4.2] "continuous version of UNumSet by using λ1 < · · · < λm, the eigenvalues of L"
  - [section 4.2] "the multiplicity of the eigenvalue 0 of L is equal to the number of connected components"
  - [corpus] "CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs" - also uses semantic similarity for UQ
- Break condition: If the semantic similarity function is noisy or responses are continuous variations of the same meaning, the eigenvalue gap may not reliably indicate discrete clusters.

### Mechanism 3
- Claim: Node degree in the similarity graph reflects confidence: high-degree nodes are central to clusters and thus represent confident, consensus responses.
- Mechanism: The degree matrix D from the similarity graph provides a confidence score for each response: Dj,j/m. Responses well-connected to many others (high degree) are deemed more confident because they align with the majority semantic cluster.
- Core assumption: Confidence correlates with agreement among multiple generated responses.
- Evidence anchors:
  - [section 4.2] "D already contains relevant information: A node with high degree is well-connected to other nodes, suggesting that it lies in a confident region of the LLM"
  - [section 4.2] "UDeg(x) = trace(m − D)/m2 CDeg(x, sj) = Dj,j/m"
  - [corpus] "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees" - also addresses confidence estimation in LLMs
- Break condition: If the model consistently generates diverse responses without a clear consensus (e.g., due to high aleatoric uncertainty), high degree may not imply correctness or confidence.

## Foundational Learning

- Concept: Natural Language Inference (NLI) classifiers and entailment scores.
  - Why needed here: Used to measure semantic similarity between generated responses beyond simple lexical overlap.
  - Quick check question: How does the entailment score differ from Jaccard similarity when comparing "Olympia" and "Zeus was the patron god of Olympia, Greece"?
- Concept: Graph Laplacian and spectral clustering.
  - Why needed here: Provides a continuous measure of semantic dispersion and cluster count from pairwise similarities.
  - Quick check question: What does the multiplicity of the zero eigenvalue of the graph Laplacian represent in the context of response clustering?
- Concept: Confidence vs uncertainty distinction.
  - Why needed here: Uncertainty is input-dependent dispersion; confidence is response-dependent correctness likelihood. Mixing them leads to poor selective generation.
  - Quick check question: Why is it problematic to use input uncertainty to predict the correctness of a specific generated response?

## Architecture Onboarding

- Component map: Input → LLM API (black-box) → multiple responses → pairwise similarity (Jaccard or NLI) → similarity graph → metrics (NumSet, Deg, Ecc, EigV) → evaluation (AUROC/AUARC vs correctness)
- Critical path: Generation → Similarity → Graph construction → Metric computation → Evaluation
- Design tradeoffs: Jaccard is fast but ignores semantics; NLI is slower but captures meaning; more generations improve metrics but increase latency
- Failure signatures: Low AUROC/AUARC, high variance across runs, metrics not improving with more generations, NLI model failing to parse responses
- First 3 experiments:
  1. Generate 3 responses for a fixed question, compute Jaccard similarity, and plot pairwise similarity distribution.
  2. Build similarity graph and compute degree and eigenvalue-based metrics; compare values.
  3. Use GPT-3.5-turbo to judge correctness of each response and compute AUROC for Deg and Ecc metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a method to quantify uncertainty that works across multiple types of NLP tasks beyond question-answering?
- Basis in paper: [explicit] The authors note that current evaluation of uncertainty/confidence metrics is restricted to question-answering tasks and it's difficult to acquire labels on whether a response is "reliable" for open-ended conversations.
- Why unresolved: The paper focuses on question-answering datasets and notes this as a limitation, suggesting it's generally difficult to evaluate reliability in more open-ended contexts.
- What evidence would resolve it: A systematic study comparing uncertainty quantification methods across diverse NLP tasks (dialogue, summarization, code generation, etc.) with appropriate reliability labels for each task type.

### Open Question 2
- Question: Can we develop unified uncertainty and confidence measures that jointly predict generation quality rather than evaluating them separately?
- Basis in paper: [explicit] The authors state "we currently evaluate uncertainty and confidence separately. Methods that consider both have the potential to predict the quality of generations better."
- Why unresolved: The paper evaluates U(x) and C(x,s) independently despite noting that joint methods could be more effective.
- What evidence would resolve it: A proposed framework that combines uncertainty and confidence into a single metric, validated against the existing separate metrics on the same benchmarks.

### Open Question 3
- Question: How can we develop uncertainty quantification methods that can identify factual errors propagated from training data, not just uncertainty in the model's posterior?
- Basis in paper: [explicit] The authors note that "the uncertainty and confidence measures in this work...reflect those in the 'posterior' represented by the LLM. It may not, for example, identify factual errors propagated from the training corpus."
- Why unresolved: Current methods focus on the dispersion of possible outputs rather than factuality checking against external knowledge.
- What evidence would resolve it: A method that combines uncertainty quantification with knowledge base lookup or fact-checking capabilities, with empirical validation on datasets containing known factual errors.

## Limitations
- Reliance on multiple expensive generation calls for uncertainty estimation, which may not be practical for latency-sensitive applications
- Assumption that multiple responses are conditionally independent may not hold in practice, particularly with deterministic decoding strategies
- Evaluation relies on GPT-3.5-turbo as an automatic judge, introducing potential bias and questions about ground truth accuracy

## Confidence

*High confidence:* The core mechanism of using pairwise similarity to quantify uncertainty is well-established and mathematically sound. The distinction between uncertainty (input property) and confidence (response property) is conceptually clear and properly maintained throughout the paper.

*Medium confidence:* The effectiveness of specific metrics (particularly those using entailment-based similarity) on the three datasets tested. While results are promising, the datasets represent a limited scope of natural language generation tasks.

*Low confidence:* The generalizability of these metrics to domains outside of question answering, such as creative writing or open-ended dialogue, where semantic dispersion may not correlate with uncertainty in the same way.

## Next Checks

1. **Independence validation**: Conduct experiments varying decoding temperature and nucleus sampling parameters to test whether the conditional independence assumption holds. Measure how correlation between responses changes with different generation strategies.

2. **Cross-domain evaluation**: Apply the proposed metrics to non-QA tasks (e.g., summarization, dialogue generation) and compare performance against the QA results. This would validate the generalizability claim.

3. **Ground truth verification**: For a subset of responses, obtain human judgments of correctness and uncertainty to validate the GPT-3.5-turbo automatic judge. Compare AUROC/AUARC metrics using human vs. automatic judgments.