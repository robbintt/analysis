---
ver: rpa2
title: 'Align on the Fly: Adapting Chatbot Behavior to Established Norms'
arxiv_id: '2312.15907'
source_url: https://arxiv.org/abs/2312.15907
tags:
- school
- rules
- llms
- question
- moral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces On-the-fly Preference Optimization (OPO),
  a real-time alignment method for large language models (LLMs) that uses an external
  memory of established rules to guide model behavior without additional training.
  The approach retrieves relevant legal and moral rules based on input queries and
  constrains LLM outputs accordingly.
---

# Align on the Fly: Adapting Chatbot Behavior to Established Norms

## Quick Facts
- **arXiv ID**: 2312.15907
- **Source URL**: https://arxiv.org/abs/2312.15907
- **Reference count**: 40
- **One-line primary result**: OPO consistently improves alignment performance across 15 different LLMs, with some models showing accuracy gains of over 10%.

## Executive Summary
This paper introduces On-the-fly Preference Optimization (OPO), a real-time alignment method for large language models (LLMs) that uses an external memory of established rules to guide model behavior without additional training. The approach retrieves relevant legal and moral rules based on input queries and constrains LLM outputs accordingly. The method addresses challenges of aligning LLMs with dynamic, complex human values across different contexts and locations. Experiments on both human-annotated and auto-generated questions across legal and moral domains show that OPO consistently improves alignment performance across 15 different LLMs, with some models showing accuracy gains of over 10%.

## Method Summary
The On-the-fly Preference Optimization (OPO) method aligns LLMs with human values by retrieving relevant rules from an external corpus and using them to constrain model outputs. The system collects legal rules from national databases and moral rules from various sources, converts them into vector embeddings, and stores them in a retrieval database. When a query is received, it's embedded and used to retrieve the most similar rules, which are then provided as context to the LLM during response generation. The evaluation framework uses GPT-4 to automatically generate test questions based on the rules, with human annotators filtering low-quality outputs to ensure evaluation quality.

## Key Results
- OPO consistently improves alignment performance across 15 different LLMs
- Some models show accuracy gains of over 10% on alignment tasks
- The method demonstrates effectiveness for both legal and moral alignment domains
- Performance varies with context length, with optimal results found between 500-1000 tokens

## Why This Works (Mechanism)

### Mechanism 1: Externalized Alignment via Retrieval-Augmented Generation
- Claim: The proposed method aligns LLMs with human values by retrieving relevant rules from an external corpus rather than internalizing them through parameter updates.
- Mechanism: For each input query, the method embeds the query, retrieves top-k similar rules from a vector database, and passes both to the LLM to constrain its output generation.
- Core assumption: Relevant legal and moral rules can be retrieved with sufficient accuracy to guide LLM behavior without requiring fine-tuning.
- Evidence anchors:
  - [abstract] "It employs an external memory to store established rules for alignment, which can constrain LLMs' behaviors without further training"
  - [section 3.2] "We utilize OpenAI's text-embedding-ada-002 model...to obtain dense representations for the collected rules and create the vector database"
- Break condition: If the retrieval step fails to find relevant rules, the LLM will generate unconstrained responses, potentially violating alignment objectives.

### Mechanism 2: Dynamic Value Retrieval for Context-Sensitive Alignment
- Claim: The system can adapt to different contexts by retrieving location-specific legal rules based on query analysis.
- Mechanism: The method extracts location information from queries using regular expressions, then retrieves both national and location-specific laws from respective sub-databases.
- Core assumption: Query analysis can accurately identify relevant jurisdictional constraints that should apply to the response.
- Evidence anchors:
  - [section 4.1] "We adopt regular expressions to extract the location...Subsequently, we retrieve similar laws from the sub-database consisting of national laws, and local laws related to the extracted location"
  - [section 3.3] "We first collect a set of questions as the seed questions...we leverage our collected text corpus to provide relevant contexts for the generation of new questions"
- Break condition: If location extraction fails or the location-specific rules are incomplete, the system may provide inappropriate guidance for the specific jurisdiction.

### Mechanism 3: Scalable Evaluation via Automatic Question Generation
- Claim: The evaluation framework can expand test coverage by automatically generating questions based on unaligned rules.
- Mechanism: GPT-4 generates multi-choice questions based on randomly selected rules and seed questions, with human annotators filtering low-quality outputs.
- Core assumption: Generated questions maintain sufficient quality and coverage to effectively evaluate alignment performance.
- Evidence anchors:
  - [section 3.3] "we propose an evaluation module to automatically generate new legal questions and professional moral questions by utilizing GPT-4"
  - [section 4.2] "three well-educated human annotators are employed to review each question of the auto-generated datasets to further ensure the quality"
- Break condition: If generated questions have systematic biases or fail to cover edge cases, the evaluation may overestimate alignment performance.

## Foundational Learning

- **Vector embeddings and similarity search**
  - Why needed here: The system relies on dense vector representations to retrieve relevant rules from a large corpus
  - Quick check question: How does the cosine similarity between query and rule embeddings determine which rules are retrieved?

- **Zero-shot prompting**
  - Why needed here: The evaluation uses zero-shot settings to fairly compare LLMs without introducing in-context learning bias
  - Quick check question: What are the key differences between zero-shot and few-shot prompting, and why does this paper choose zero-shot?

- **Chain-of-thought reasoning**
  - Why needed here: Used in the question generation pipeline to improve the quality and logical consistency of generated questions
  - Quick check question: How does COT reasoning help GPT-4 generate more coherent and logically structured questions?

## Architecture Onboarding

- **Component map**: Rule Creation Module -> Alignment Module -> Evaluation Module -> LLM Interface
- **Critical path**: 
  1. Query embedding generation
  2. Rule retrieval from vector database
  3. LLM response generation with retrieved rules as context
  4. Response output to user
- **Design tradeoffs**:
  - Retrieval accuracy vs. computational efficiency: Larger k values improve alignment but increase latency
  - Context length vs. relevance: Longer retrieved contexts may include more noise
  - Human annotation vs. automatic generation: Human review ensures quality but limits scalability
- **Failure signatures**:
  - Retrieval returns irrelevant rules → LLM produces misaligned responses
  - Context window exceeded → System truncates important rule information
  - Generated questions have logical errors → Evaluation overestimates alignment capability
- **First 3 experiments**:
  1. Baseline: Measure LLM accuracy on legal/moral questions without OPO
  2. Retrieval length sweep: Test performance across different context lengths (200, 500, 1000, 1500, 2000 tokens)
  3. Location-specific alignment: Evaluate performance on jurisdiction-specific legal questions vs. general laws

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OPO scale with increasing size and diversity of the external memory corpus?
- Basis in paper: [inferred] The paper demonstrates OPO effectiveness with 957,778 rules, but does not explore performance scaling with larger or more diverse corpora.
- Why unresolved: The paper does not systematically evaluate OPO with varying corpus sizes or explore diminishing returns from excessive rule volume.
- What evidence would resolve it: Controlled experiments testing OPO performance with corpora of different sizes (e.g., 10k, 100k, 1M, 10M rules) and varying diversity metrics, measuring both accuracy gains and computational efficiency.

### Open Question 2
- Question: Can OPO be effectively adapted for continuous learning scenarios where new rules are added incrementally?
- Basis in paper: [explicit] The paper mentions OPO allows "convenient updates and customization of human values" but does not explore incremental learning.
- Why unresolved: The paper does not investigate how OPO handles dynamic rule updates, potential catastrophic forgetting, or efficiency of updating the vector database.
- What evidence would resolve it: Experiments tracking OPO performance as new rules are incrementally added over time, measuring adaptation speed, rule retrieval accuracy, and model alignment stability.

### Open Question 3
- Question: What is the optimal balance between retrieved rule context length and LLM performance across different model sizes?
- Basis in paper: [explicit] The paper explores context lengths from 200-2000 tokens but does not provide systematic analysis of optimal lengths per model size.
- Why unresolved: The paper shows performance varies with context length but does not determine if smaller models benefit from shorter contexts or if larger models can effectively utilize longer contexts.
- What evidence would resolve it: Comprehensive experiments testing multiple context lengths (50, 200, 500, 1000, 2000, 4000 tokens) across a wider range of model sizes, measuring both performance and computational efficiency trade-offs.

## Limitations
- Reliance on external rule corpora introduces potential gaps in coverage for rapidly evolving social norms and emerging legal contexts
- Evaluation framework depends heavily on auto-generated questions that may not fully capture edge cases or adversarial scenarios
- System performance is constrained by the quality and completeness of underlying rule databases

## Confidence
- **High Confidence**: The core retrieval-augmented alignment mechanism works as described, with measurable accuracy improvements across multiple LLMs
- **Medium Confidence**: The evaluation methodology provides valid comparative insights, though auto-generated questions may introduce subtle biases
- **Medium Confidence**: The system's effectiveness for location-specific legal alignment, as results depend on the completeness of jurisdictional databases

## Next Checks
1. Conduct adversarial testing with deliberately misleading queries to evaluate the system's robustness against attempts to bypass retrieved constraints
2. Perform ablation studies to quantify the contribution of different rule sources (legal vs. moral) to overall alignment performance
3. Test the system's temporal adaptation by evaluating performance on questions about recently changed laws or emerging social norms not present in the original rule corpus