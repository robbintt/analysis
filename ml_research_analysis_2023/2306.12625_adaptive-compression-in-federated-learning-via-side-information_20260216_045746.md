---
ver: rpa2
title: Adaptive Compression in Federated Learning via Side Information
arxiv_id: '2306.12625'
source_url: https://arxiv.org/abs/2306.12625
tags:
- server
- clients
- distribution
- ours
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high communication cost in federated learning
  (FL) by exploiting side information available at the server to compress model updates.
  The key idea is to use an importance sampling algorithm that communicates samples
  from a pre-data distribution (side information) instead of the post-data distribution
  (model updates), reducing the communication cost to the KL divergence between these
  distributions.
---

# Adaptive Compression in Federated Learning via Side Information

## Quick Facts
- arXiv ID: 2306.12625
- Source URL: https://arxiv.org/abs/2306.12625
- Reference count: 40
- One-line primary result: KLMS achieves up to 82x smaller bitrate compared to prior work, corresponding to 2,650x overall compression in federated learning

## Executive Summary
This paper introduces KLMS (KL Divergence Minimization with Side Information), a novel framework for reducing communication costs in federated learning by exploiting side information available at the server. The key insight is that model updates from clients and side information at the server can be modeled as probability distributions, and the communication cost can be reduced to the KL divergence between these distributions. By using importance sampling to communicate samples from a pre-data distribution instead of the post-data distribution, KLMS achieves significant compression gains while maintaining accuracy.

## Method Summary
KLMS integrates into existing stochastic federated learning frameworks by replacing direct model parameter communication with compressed representations. The server maintains a pre-data distribution based on historical updates, while clients compute post-data distributions from their local model updates. Using importance sampling, clients communicate indices of samples from the pre-data distribution rather than full model updates, with the number of bits determined by the KL divergence between distributions. The framework includes adaptive block selection to optimize bit allocation across model parameters and training rounds.

## Key Results
- Achieves up to 82 times smaller bitrate compared to prior work
- Provides 2,650 times overall compression improvement
- Maintains accuracy-bitrate tradeoffs across various FL settings
- Outperforms vanilla QSGD with 12x improvement in bitrate

## Why This Works (Mechanism)

### Mechanism 1
KLMS reduces communication cost to the KL divergence between pre-data and post-data distributions. Instead of sending a deterministic sample from the client's post-data distribution, KLMS communicates an index from a pre-computed set of samples drawn from the pre-data distribution. This leverages the server's side information to reduce the number of bits needed. Core assumption: The pre-data distribution at the server is close to the client's post-data distribution in KL divergence.

### Mechanism 2
Adaptive block selection optimizes bit allocation across model parameters and training rounds. The model is divided into blocks such that each block has the same KL divergence. This allows setting a uniform number of samples per block, reducing the need to manually tune block sizes. Core assumption: KL divergence varies significantly across model parameters and rounds, making fixed-size blocks suboptimal.

### Mechanism 3
Temporal correlation in FL allows the server to maintain an accurate pre-data distribution over time. The server updates the pre-data distribution using historical client updates, leveraging the temporal correlation of model updates across rounds. Core assumption: Model updates are temporally correlated, meaning recent updates are similar to past updates.

## Foundational Learning

- Concept: Importance sampling
  - Why needed here: KLMS relies on importance sampling to communicate samples from the post-data distribution using a pre-data distribution
  - Quick check question: What is the role of the importance weights in the importance sampling algorithm?

- Concept: Kullback-Leibler (KL) divergence
  - Why needed here: KL divergence quantifies the similarity between the pre-data and post-data distributions, which determines the communication cost
  - Quick check question: How does KL divergence relate to the number of bits needed to communicate a sample?

- Concept: Federated learning (FL)
  - Why needed here: KLMS is designed to improve communication efficiency in FL by exploiting the server's side information
  - Quick check question: What are the key challenges in FL that KLMS addresses?

## Architecture Onboarding

- Component map: Server -> Clients -> Communication Protocol -> Global Model Update
- Critical path:
  1. Server initializes pre-data distribution
  2. Clients compute local updates and apply KLMS
  3. Clients send compressed samples (indices) to server
  4. Server decodes samples and updates global model
  5. Server updates pre-data distribution using historical updates
- Design tradeoffs:
  - Block size vs. KL divergence: Smaller blocks allow finer adaptation but increase overhead
  - Number of samples per block (K) vs. accuracy: More samples reduce error but increase communication cost
  - Update frequency of pre-data distribution vs. accuracy: More frequent updates improve accuracy but increase overhead
- Failure signatures:
  - High KL divergence between pre-data and post-data distributions
  - Inaccurate pre-data distribution due to poor temporal correlation
  - Overhead from block location updates outweighing compression benefits
- First 3 experiments:
  1. Test KLMS with fixed block sizes on a simple model (e.g., CONV4 on MNIST) to verify communication savings
  2. Evaluate adaptive block selection by comparing fixed vs. adaptive block sizes on a more complex model (e.g., ResNet-18 on CIFAR-10)
  3. Test KLMS with different pre-data distributions (e.g., uniform vs. empirical) to assess the impact of side information quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of pre-data and post-data distributions affect the performance of KLMS in different federated learning frameworks beyond the four examples provided? The paper only demonstrates KLMS on four specific frameworks, leaving the question of how it performs on other stochastic FL frameworks unanswered.

### Open Question 2
What is the impact of statistical heterogeneity on the performance of KLMS in federated learning settings? While the paper provides some experimental results showing KLMS's performance under different levels of statistical heterogeneity, a more comprehensive analysis is needed to fully understand its impact.

### Open Question 3
How does the adaptive block selection strategy in KLMS affect its performance compared to fixed-size blocks? While the paper demonstrates the effectiveness of the adaptive block selection strategy in some experiments, a more comprehensive comparison with fixed-size blocks across different federated learning frameworks and settings is needed.

## Limitations
- Performance heavily depends on quality of side information available at the server
- Adaptive block selection requires careful tuning of target KL divergence parameter
- Temporal correlation assumption may break down with highly non-i.i.d. data distributions

## Confidence
- **High confidence**: Core mechanism of using KL divergence between pre-data and post-data distributions to determine communication cost
- **Medium confidence**: Adaptive block selection strategy and its claimed benefits
- **Medium confidence**: Temporal correlation assumption and its exploitation through historical update tracking

## Next Checks
1. Test KLMS performance across varying quality of pre-data distributions to understand practical limits when side information is imperfect
2. Evaluate KLMS performance under extreme non-i.i.d. data distributions and irregular client participation patterns
3. Quantify computational and memory overhead of maintaining pre-data distributions and performing adaptive block selection, comparing against communication savings