---
ver: rpa2
title: Prompting Disentangled Embeddings for Knowledge Graph Completion with Pre-trained
  Language Model
arxiv_id: '2312.01837'
source_url: https://arxiv.org/abs/2312.01837
tags:
- entity
- triple
- pdkgc
- graph
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of Knowledge Graph Completion (KGC),
  which aims to predict missing entities in incomplete triples using both graph structures
  and textual information. The proposed method, PDKGC, utilizes pre-trained language
  models (PLMs) through prompt-tuning, which is more efficient than fine-tuning.
---

# Prompting Disentangled Embeddings for Knowledge Graph Completion with Pre-trained Language Model

## Quick Facts
- arXiv ID: 2312.01837
- Source URL: https://arxiv.org/abs/2312.01837
- Reference count: 40
- The paper proposes PDKGC, which uses prompt-tuning on frozen PLMs with disentangled graph representations, achieving state-of-the-art performance on KGC datasets.

## Executive Summary
This paper addresses Knowledge Graph Completion (KGC) by proposing PDKGC, a method that leverages pre-trained language models (PLMs) through prompt-tuning rather than fine-tuning. The key innovation is the use of two specialized prompts: a hard task prompt that reformulates KGC as a masked token prediction task, and a disentangled structure prompt that learns semantic aspects of entities through relation-aware attention. The method combines textual and structural predictors to provide comprehensive entity predictions. Experiments on WN18RR and FB15K-237 demonstrate that PDKGC often outperforms state-of-the-art baselines while being more efficient than fine-tuning approaches.

## Method Summary
PDKGC reformulates KGC as a masked token prediction task to leverage PLM pre-training knowledge. It introduces a disentangled graph learner that uses relation-aware attention to distribute neighboring entities into K components, each encoding specific semantic aspects. The method employs a structure-aware text encoder that incorporates these disentangled embeddings through layer-wise prompts prepended to the PLM input. Entity prediction is performed by both a textual predictor (using PLM output at the MASK token) and a structural predictor (using ConvE on the structural prompts), with their scores combined through weighted sum fusion. The entire framework is trained using frozen PLMs with prompt-tuning, requiring significantly fewer parameters than fine-tuning.

## Key Results
- PDKGC achieves state-of-the-art or competitive performance on WN18RR and FB15K-237 datasets
- The method consistently outperforms strong KGC baselines including DistMult, ConvE, and RoBERTa-based approaches
- Ablation studies confirm the effectiveness of both the disentangled structure prompt and the hard task prompt components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PDKGC reformulates the KGC task as a masked token prediction task to leverage the pre-training knowledge of PLMs.
- Mechanism: The hard task prompt `[B]htext[S]rtext[S][MASK][S]` reformulates the missing tail entity prediction as predicting the `[MASK]` token, aligning with the PLM's masked language modeling (MLM) pre-training task.
- Core assumption: The pre-trained knowledge in PLMs, especially the ability to predict masked tokens, is transferable and beneficial for the KGC task.
- Evidence anchors:
  - [abstract] "The hard task prompt which is to adapt the KGC task to the PLM pre-training task of token prediction"
  - [section] "to meet the needs of bridging KGC to these pre-training tasks, to distill the pre-trained knowledge from PLMs without having to fine-tune, we propose an 'auto-completion' task prompt, to view the missing entity as a missing token"
- Break condition: If the PLM's pre-training objective is significantly different from masked token prediction or if the textual information is insufficient to provide context for predicting the missing entity.

### Mechanism 2
- Claim: PDKGC learns disentangled graph representations to capture different semantic aspects of entities and their relevant graph structures.
- Mechanism: The disentangled graph learner uses relation-aware attention to distribute neighboring entities into different components, each encoding a specific semantic aspect. For example, neighbors connected through "family" relations are encoded in one component, while neighbors connected through "career" relations are encoded in another.
- Core assumption: Entities in a KG have multiple semantic aspects, and their neighbors contribute differently to these aspects. Learning disentangled representations can capture these aspects and their relevant graph structures more effectively.
- Evidence anchors:
  - [abstract] "a disentangled structure prompt which learns disentangled graph representation so as to enable the PLM to combine more relevant structure knowledge with the text information"
  - [section] "we propose to learn a disentangled representation (embedding) for each entity. Such a representation contains multiple components, each of which encodes the features of a specific subset of neighbors that are highly relevant to this entity in a certain semantic aspect."
- Break condition: If the graph structure is too sparse or if the relations do not have clear semantic distinctions, making it difficult to learn meaningful disentangled representations.

### Mechanism 3
- Claim: PDKGC combines textual and structural predictors to provide more comprehensive entity predictions.
- Mechanism: The textual predictor uses the PLM's output at the `[MASK]` token to predict the missing entity based on the textual information and the incorporated structure knowledge. The structural predictor forwards the PLM outputs of the structural prompts to a KGE model to predict the missing entity based on the graph structure. The final prediction is made by combining the scores from both predictors.
- Core assumption: The textual information and the graph structure provide complementary information for entity prediction, and combining predictions from both sources can lead to better performance.
- Evidence anchors:
  - [abstract] "PDKGC builds a textual predictor and a structural predictor, respectively, and their combination leads to more comprehensive entity prediction."
  - [section] "Consequently, we can fuse them during inference by e.g., performing a weighted sum of the predicted scores."
- Break condition: If one of the predictors consistently underperforms or if the combination method does not effectively leverage the complementary information from both predictors.

## Foundational Learning

- Concept: Pre-trained Language Models (PLMs)
  - Why needed here: PDKGC leverages PLMs to encode textual information and incorporate structure knowledge. Understanding how PLMs work, especially their pre-training tasks and architectures, is crucial for understanding PDKGC.
  - Quick check question: What is the difference between encoder-only, encoder-decoder, and decoder-only PLMs, and how does this difference affect their usage in PDKGC?

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: PDKGC uses a KGE model (ConvE) as the structural predictor to predict missing entities based on the graph structure. Understanding how KGE models work and their different types (e.g., translation-based, semantic matching-based, neural network-based) is important for understanding PDKGC.
  - Quick check question: How does the ConvE model score a triple, and how does it differ from other KGE models like TransE or DistMult?

- Concept: Prompt Tuning
  - Why needed here: PDKGC uses prompt tuning to adapt the PLM to the KGC task without fine-tuning the entire model. Understanding how prompt tuning works and its advantages over fine-tuning is essential for understanding PDKGC.
  - Quick check question: What is the difference between hard prompts and soft prompts, and how are they used in PDKGC?

## Architecture Onboarding

- Component map: Disentangled Graph Learner -> Hard Task Prompt -> Structure-aware Text Encoder -> Textual Predictor + Structural Predictor -> Ensemble Module

- Critical path: For a given triple (h, r, ?), the critical path is: Disentangled Graph Learner -> Hard Task Prompt -> Structure-aware Text Encoder -> Textual Predictor + Structural Predictor -> Ensemble Module.

- Design tradeoffs:
  - Using frozen PLMs vs. fine-tuning: Frozen PLMs have fewer trainable parameters and are less prone to overfitting, but they may not adapt as well to the KGC task as fine-tuned PLMs.
  - Using disentangled vs. non-disentangled representations: Disentangled representations can capture different semantic aspects of entities more effectively, but they may require more complex learning mechanisms and may not be as effective for entities with simple or uniform semantics.

- Failure signatures:
  - Poor performance on entities with sparse or uninformative textual descriptions: If the textual information is insufficient to provide context for predicting the missing entity, the textual predictor may underperform.
  - Poor performance on entities with complex or ambiguous graph structures: If the graph structure is too complex or if the relations do not have clear semantic distinctions, the disentangled graph learner may struggle to learn meaningful disentangled representations.

- First 3 experiments:
  1. Ablation study: Remove the disentangled graph learner and use non-disentangled entity representations to evaluate the impact of the disentangled structure prompt.
  2. Ablation study: Remove the textual predictor and rely solely on the structural predictor to evaluate the impact of the hard task prompt and the textual information.
  3. Hyperparameter tuning: Tune the number of components (K) in the disentangled graph learner and the prompt length (n) to optimize the model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are decoder-only PLMs like LLaMA compared to encoder-only models like BERT for KGC tasks?
- Basis in paper: [explicit] The paper mentions they plan to explore decoder-only PLMs like LLaMA in future work, suggesting uncertainty about their effectiveness compared to BERT.
- Why unresolved: The paper only evaluates BERT and RoBERTa models, leaving the performance of decoder-only PLMs unexplored.
- What evidence would resolve it: Implementing and evaluating PDKGC with decoder-only PLMs like LLaMA on the same KGC datasets used in the paper.

### Open Question 2
- Question: Can the proposed disentangled structure prompt approach be extended to handle inductive KGC scenarios with new entities and relations?
- Basis in paper: [explicit] The authors mention extending PDKGC to inductive KGC as future work, indicating this capability is currently unresolved.
- Why unresolved: The current PDKGC implementation focuses on transductive KGC where all entities and relations are known during training.
- What evidence would resolve it: Adapting PDKGC to handle unseen entities and relations during inference and evaluating its performance on inductive KGC benchmarks.

### Open Question 3
- Question: What is the optimal number of disentangled components (K) for different KGC datasets and how does it impact performance?
- Basis in paper: [explicit] The paper mentions selecting K from a pool of {2, 4, 6} based on validation MRR, but doesn't provide a systematic analysis of how K affects performance across different datasets.
- Why unresolved: The impact of K on model performance and its optimal value for different datasets is not thoroughly investigated.
- What evidence would resolve it: Conducting extensive experiments varying K on multiple KGC datasets and analyzing the relationship between K and performance metrics.

## Limitations

- The paper uses frozen PLMs with prompt-tuning rather than fine-tuning, which may limit the model's ability to fully adapt to the KGC task compared to fine-tuned approaches.
- The choice of K components for disentangled representations (K=2 for WN18RR, K=4 for FB15K-237) appears arbitrary and may not be optimal for all datasets.
- The effectiveness of the disentangled representations in capturing semantic aspects is demonstrated through performance improvements but lacks detailed analysis of what these aspects actually represent.

## Confidence

**High confidence**: The overall experimental results showing PDKGC outperforming baseline methods on standard KGC benchmarks (WN18RR and FB15K-237). The methodology is clearly described, and the results are consistent across different metrics (MRR and Hits@).

**Medium confidence**: The effectiveness of the disentangled structure prompt in capturing different semantic aspects of entities and graph structures. While the paper provides theoretical justification and shows improved performance, the actual mechanism by which disentanglement improves KGC is not thoroughly validated or analyzed.

**Low confidence**: The claim that prompt-tuning with frozen PLMs is sufficient for effective KGC without fine-tuning. The paper does not provide comparisons with fine-tuned PLMs or ablation studies on the impact of freezing vs. fine-tuning the PLM parameters.

## Next Checks

1. **Disentanglement analysis**: Conduct a detailed analysis of the learned disentangled representations by visualizing the components for different entities and relations, and examine whether they indeed capture distinct semantic aspects. This could include clustering analysis of the components and correlation studies with relation types.

2. **Prompt sensitivity study**: Perform an ablation study varying the prompt length (n) and the prompt templates to determine the sensitivity of PDKGC to these hyperparameters. Compare different prompt designs and evaluate their impact on performance across different KGC datasets.

3. **Frozen vs. fine-tuned comparison**: Implement a version of PDKGC with fine-tuned PLM parameters and compare its performance against the frozen PLM version. This would directly test whether the limitation of using frozen PLMs significantly impacts the model's ability to perform KGC effectively.