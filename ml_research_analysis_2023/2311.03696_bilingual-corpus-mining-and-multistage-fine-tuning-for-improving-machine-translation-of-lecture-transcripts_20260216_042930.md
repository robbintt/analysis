---
ver: rpa2
title: Bilingual Corpus Mining and Multistage Fine-Tuning for Improving Machine Translation
  of Lecture Transcripts
arxiv_id: '2311.03696'
source_url: https://arxiv.org/abs/2311.03696
tags:
- english
- translation
- parallel
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a framework for extracting high-quality parallel\
  \ corpora from Coursera lecture transcripts and leveraging them to improve machine\
  \ translation (MT) systems for educational content. The approach uses an MT-based\
  \ sentence alignment algorithm that combines machine translation with cosine similarity\
  \ over sentence embeddings, achieving an F1 score of 96% on manually annotated data\u2014\
  outperforming methods based on BLEU, BERTScore, LASER, and sentBERT."
---

# Bilingual Corpus Mining and Multistage Fine-Tuning for Improving Machine Translation of Lecture Transcripts

## Quick Facts
- arXiv ID: 2311.03696
- Source URL: https://arxiv.org/abs/2311.03696
- Reference count: 20
- Primary result: F1 score of 96% for sentence alignment; up to 4.1 BLEU improvement via multistage fine-tuning

## Executive Summary
This paper presents a framework for improving machine translation of lecture transcripts by extracting high-quality parallel corpora from Coursera and leveraging them through multistage fine-tuning. The approach combines an MT-based sentence alignment algorithm with dynamic programming and cosine similarity over monolingual embeddings, achieving superior performance compared to methods using multilingual embeddings. Two parallel corpora (English–Japanese and English–Chinese) of approximately 50,000 lines each were created and used to demonstrate significant improvements in BLEU scores through curriculum-style fine-tuning.

## Method Summary
The method involves crawling Coursera lecture transcripts, cleaning and normalizing the text, and extracting parallel sentences using a dynamic programming-based alignment algorithm that combines machine translation with cosine similarity over monolingual embeddings. High-quality test and development sets are created through manual filtering. Machine translation models are then trained using multistage fine-tuning, where models are progressively adapted from out-of-domain to increasingly domain-similar datasets, culminating in fine-tuning on the extracted in-domain lecture corpora.

## Key Results
- Sentence alignment achieves 96% F1 score, outperforming BERTScore, LASER, and sentBERT methods
- Multistage fine-tuning improves BLEU scores by up to 4.1 points compared to standard two-stage fine-tuning
- The extracted parallel corpora contain approximately 50,000 lines each for English–Japanese and English–Chinese

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation combined with cosine similarity over monolingual embeddings outperforms multilingual embedding approaches for sentence alignment.
- Mechanism: Translating one document into the target language and then computing cosine similarity in a shared monolingual embedding space avoids the inaccuracies of cross-lingual embeddings, especially for distant language pairs.
- Core assumption: The initial MT system, even if trained on out-of-domain data, produces sufficient quality translations for alignment purposes.
- Evidence anchors:
  - [abstract] "The sentence alignment F1 score reaches 96%, which is higher than using the BERTScore, LASER, or sentBERT methods."
  - [section 3.2] "We employ MT combined with a cosine similarity of sentence embeddings to measure the similarity of two sentences in different languages, formulated as follows: SimEMB(si, s′j) = cos[emb(MT(si)), emb(s′j)]"
  - [corpus] No corpus evidence directly supports this; the claim is derived from experimental comparison.
- Break condition: If the initial MT system quality is too low (e.g., BLEU score below a threshold), cosine similarity in monolingual space will also degrade.

### Mechanism 2
- Claim: Dynamic programming (DP) is effective for extracting parallel sentences because lecture transcripts have a monotonic order.
- Mechanism: The DP algorithm leverages the fact that corresponding sentences in roughly aligned documents are in the same order, allowing it to efficiently find 1-to-1 and many-to-many alignments without comparing all possible sentence pairs.
- Core assumption: The monotonic nature of lecture transcripts holds across different languages and courses.
- Evidence anchors:
  - [section 3.2] "In our scenario, DP is effective because of the monotonic nature of the transcripts, as the corresponding sentences in each pair of documents are roughly in the same order."
  - [section 4.3.1] "There are 162 (84%) 1-to-1 alignment cases, 8 (4%) many-to-1 cases, 21 (11%) 1-to-many cases, and 1 (1%) many-to-many alignment cases, which requires the ability to combine multiple lines into one."
  - [corpus] No corpus evidence directly supports this; the claim is based on the structure of the crawled data.
- Break condition: If transcripts are not strictly monotonic (e.g., due to editing or reordering), DP alignment accuracy will suffer.

### Mechanism 3
- Claim: Multistage fine-tuning, where models are trained on increasingly domain-similar data, yields better translation quality than direct fine-tuning or mixing all data.
- Mechanism: Gradually introducing in-domain data through multiple stages prevents catastrophic forgetting and allows the model to adapt smoothly to the lecture domain.
- Core assumption: The datasets can be ordered by similarity to the target domain using language model likelihood.
- Evidence anchors:
  - [section 5.3] "We first sort all the datasets into a list by the similarity with the in-domain dataset in terms of average per-token log-likelihood from the smallest to the largest given by the in-domain LM"
  - [section 5.4] "Multistage fine-tuning shows more robust if not better performance than Mixed data, two-stage fine-tuning or mixed fine-tuning."
  - [corpus] No corpus evidence directly supports this; the claim is derived from experimental results.
- Break condition: If the ordering of datasets by similarity is incorrect, or if the in-domain dataset is too small, multistage fine-tuning may not improve performance.

## Foundational Learning

- Concept: Dynamic Programming for Sequence Alignment
  - Why needed here: To efficiently find the best alignment between sentences in two documents while respecting their monotonic order.
  - Quick check question: What is the time complexity of the DP algorithm for aligning two documents with m and n sentences, respectively?

- Concept: Cosine Similarity in Vector Space
  - Why needed here: To measure the similarity between sentence embeddings in a shared vector space, which is used as the scoring function in the DP algorithm.
  - Quick check question: How does cosine similarity differ from Euclidean distance when comparing sentence embeddings?

- Concept: Curriculum Learning / Multistage Fine-tuning
  - Why needed here: To leverage both large out-of-domain and small in-domain datasets effectively by gradually adapting the model to the target domain.
  - Quick check question: Why might directly fine-tuning a model on a small in-domain dataset lead to overfitting?

## Architecture Onboarding

- Component map: Data Crawler -> Text Cleaner -> Initial MT System -> Sentence Aligner -> Corpus Filter -> NMT Trainer
- Critical path: Data Crawler → Text Cleaner → Initial MT System → Sentence Aligner → Corpus Filter → NMT Trainer
- Design tradeoffs:
  - Using an out-of-domain MT system for alignment vs. training a new one on in-domain data.
  - Manual filtering of test/dev sets vs. relying solely on automatic metrics.
  - Multistage fine-tuning vs. direct fine-tuning or mixing all data.
- Failure signatures:
  - Low alignment F1 score: indicates issues with MT system quality or embedding space.
  - Overfitting on in-domain data: suggests the need for more regularization or data augmentation.
  - Poor translation quality: could be due to suboptimal fine-tuning schedule or dataset ordering.
- First 3 experiments:
  1. Train the initial MT system on TED Talks and evaluate BLEU on a held-out TED test set.
  2. Run the sentence alignment algorithm on a small set of manually annotated document pairs and compute F1 score.
  3. Train an NMT model on the created in-domain corpus and evaluate BLEU on the test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multistage fine-tuning approach compare to other advanced domain adaptation techniques, such as adversarial training or meta-learning, in improving machine translation performance for lecture transcripts?
- Basis in paper: [inferred] The paper suggests that the multistage fine-tuning method improves translation quality, but it does not compare its effectiveness to other domain adaptation techniques.
- Why unresolved: The paper focuses on the effectiveness of the proposed corpora and the multistage fine-tuning method but does not explore other advanced domain adaptation techniques.
- What evidence would resolve it: Comparative experiments using other domain adaptation techniques, such as adversarial training or meta-learning, would provide insights into the relative effectiveness of the proposed approach.

### Open Question 2
- Question: Can the proposed sentence alignment algorithm be extended to handle languages with different writing systems or script types, such as Arabic or Hindi, without significant modifications?
- Basis in paper: [explicit] The paper demonstrates the algorithm's effectiveness for English-Japanese and English-Chinese, but it does not explore its applicability to other language pairs with different writing systems.
- Why unresolved: The algorithm relies on cosine similarity of sentence embeddings, which may not be directly applicable to languages with different writing systems without additional preprocessing or embedding adjustments.
- What evidence would resolve it: Testing the algorithm on language pairs with different writing systems and comparing its performance to existing methods would determine its generalizability.

### Open Question 3
- Question: How does the quality of the automatically extracted parallel corpora compare to manually translated corpora in terms of translation accuracy and fluency for lecture transcripts?
- Basis in paper: [inferred] The paper focuses on the automatic extraction of parallel corpora but does not compare their quality to manually translated corpora.
- Why unresolved: The paper does not provide a direct comparison between automatically extracted and manually translated corpora, which would offer insights into the trade-offs between automation and quality.
- What evidence would resolve it: A side-by-side comparison of translation outputs using automatically extracted and manually translated corpora would reveal differences in accuracy and fluency.

## Limitations
- Reliance on out-of-domain MT systems for sentence alignment may not generalize well to all lecture content types
- Manual filtering process for test/development sets introduces potential subjectivity and limits reproducibility
- Corpus size (50,000 lines) remains relatively small compared to established parallel corpora

## Confidence
- Sentence alignment method superiority (96% F1): High
- Multistage fine-tuning effectiveness: Medium
- Monotonic alignment assumption: Medium

## Next Checks
1. Test alignment robustness: Evaluate the sentence alignment algorithm on lecture transcripts from different domains (e.g., humanities vs. STEM) to verify the monotonic assumption holds across varied content.

2. Assess corpus coverage: Analyze the lexical and syntactic diversity of the created parallel corpus compared to established MT evaluation benchmarks to identify potential coverage gaps.

3. Validate fine-tuning stability: Perform ablation studies on the multistage fine-tuning schedule, including testing different dataset orderings and fine-tuning durations to confirm the reported improvements are robust.