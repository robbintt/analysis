---
ver: rpa2
title: Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource
  Local Languages in Indonesia
arxiv_id: '2311.00998'
source_url: https://arxiv.org/abs/2311.00998
tags:
- data
- language
- training
- translation
- parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks neural machine translation (NMT) for four
  low-resource Indonesian local languages (Javanese, Sundanese, Minangkabau, Balinese)
  under replicable conditions using publicly available data and modest compute. The
  researchers trained 32 NMT systems across three training approaches (Scratch, Pre-trained
  Cross-Lingual, and Code-switched Pre-trained Cross-Lingual) and two training paradigms
  (unsupervised and semi-supervised), augmented with synthetic parallel data generated
  by gpt-3.5-turbo.
---

# Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia

## Quick Facts
- arXiv ID: 2311.00998
- Source URL: https://arxiv.org/abs/2311.00998
- Reference count: 21
- Key outcome: Code-switched Pre-trained Cross-Lingual approach achieved average spm200BLEU scores of 23.80 and 18.15 for x→id and id→x directions respectively

## Executive Summary
This study benchmarks neural machine translation for four low-resource Indonesian local languages (Javanese, Sundanese, Minangkabau, Balinese) using replicable conditions and publicly available data. The researchers trained 32 NMT systems across three training approaches and two training paradigms, augmented with synthetic parallel data generated by gpt-3.5-turbo. They found that the Code-switched Pre-trained Cross-Lingual approach consistently outperformed others, with synthetic data generation significantly improving translation quality. Some models matched or exceeded gpt-3.5-turbo's zero-shot translation performance using far fewer resources.

## Method Summary
The study trained 32 NMT systems using three training approaches (Scratch, Pre-trained Cross-Lingual, and Code-switched Pre-trained Cross-Lingual) and two training paradigms (unsupervised and semi-supervised). Models were based on XLM architecture with Masked Language Modeling for pre-training. Synthetic parallel data was generated using gpt-3.5-turbo and incorporated into semi-supervised training. The researchers evaluated translation quality using spm200BLEU scores on the publicly available FLORES-200 dataset, testing all possible translation directions between Indonesian and the four local languages.

## Key Results
- Code-switched Pre-trained Cross-Lingual approach achieved highest average spm200BLEU scores (23.80 x→id, 18.15 id→x)
- Synthetic data generation significantly improved translation quality across all language pairs
- Several NMT systems rivaled the translation quality of zero-shot gpt-3.5-turbo
- Denoising autoencoding objective played more prominent role than parallel data in extremely low-resource settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code-switched Pre-trained Cross-Lingual (CodeXL) approach consistently outperforms other training approaches.
- Mechanism: Code-switching during pre-training introduces stronger cross-lingual signals by mixing tokens from both source and target languages within training instances, creating a bilingual context that improves the model's ability to learn cross-lingual representations.
- Core assumption: The bilingual dictionary used for code-switching is accurate and comprehensive enough to capture meaningful word-level correspondences between languages.
- Evidence anchors:
  - [abstract]: "We find that the Code-switched Pre-trained Cross-Lingual approach consistently outperformed others, achieving average spm200BLEU scores of 23.80 and 18.15 for x→id and id→x directions respectively."
  - [section]: "Results obtained by Kuwanto et al. (2021) imply that this method helps the model by giving stronger cross-lingual signals, which helps translation tasks during fine-tuning."
- Break condition: If the bilingual dictionary is incomplete or contains incorrect translations, the code-switching process may introduce noise that degrades model performance.

### Mechanism 2
- Claim: Synthetic parallel data generated by GPT-3.5-turbo can improve NMT performance to approach or exceed the generative AI's zero-shot translation performance.
- Mechanism: The generative AI produces high-quality synthetic parallel data that supplements limited training data. When used in supervised training, this data provides additional examples that help the model learn better representations.
- Core assumption: The generative AI's translation quality is high enough that its generated translations are useful for training.
- Evidence anchors:
  - [abstract]: "synthetic data generation significantly improved translation quality, with some models matching or exceeding gpt-3.5-turbo's zero-shot translation performance using far fewer resources."
  - [section]: "we observe a trend in which generative AIs can help augment the training process by generating synthetic parallel data. In most cases... the parallel data generated by generative AI can impact the performance of NMT systems to approach or even outperform the performance of the generative AI's translation performance."
- Break condition: If the generative AI produces translations with significant errors or biases, the synthetic data could degrade model performance rather than improve it.

### Mechanism 3
- Claim: Denoising autoencoding objective plays a more prominent role than parallel data in extremely low-resource NMT.
- Mechanism: The denoising autoencoding task forces the model to learn robust representations by reconstructing corrupted input, which is particularly valuable when parallel data is scarce.
- Core assumption: The model can learn useful cross-lingual mappings through monolingual denoising alone when parallel data is extremely limited.
- Evidence anchors:
  - [abstract]: "several of our NMT systems achieve competitive performances, rivaling the translation quality of zero-shot gpt-3.5-turbo."
  - [section]: "unsup created better NMT systems than the semisup training paradigm... This indicates that denoising-autoencoding plays a more significant role in model performance than parallel data when the training parallel data is limited."
- Break condition: If the monolingual data is too limited or too different between languages, denoising alone may not provide sufficient cross-lingual signal.

## Foundational Learning

- Concept: Neural Machine Translation architecture
  - Why needed here: Understanding the basic Transformer architecture and how different components (encoder, decoder, attention mechanisms) work together is essential for implementing and modifying NMT systems.
  - Quick check question: What is the purpose of the attention mechanism in Transformer-based NMT models?

- Concept: Cross-lingual language modeling
  - Why needed here: The XLM architecture used here relies on cross-lingual pre-training to build shared representations across languages, which is fundamental to understanding how the CodeXL approach works.
  - Quick check question: How does masked language modeling help a model learn cross-lingual representations?

- Concept: Data augmentation techniques for low-resource languages
  - Why needed here: Understanding techniques like back-translation, code-switching, and synthetic data generation is crucial for implementing the various training approaches evaluated in this work.
  - Quick check question: What is the difference between back-translation and code-switching as data augmentation techniques?

## Architecture Onboarding

- Component map: Data preparation -> Pre-training (MLM) -> Fine-tuning (DAE/BT) -> Supervised training -> Evaluation
- Critical path:
  1. Data preparation (filtering, tokenization, vocabulary building)
  2. Pre-training with MLM objective (with or without code-switching)
  3. Fine-tuning with denoising autoencoding and back-translation
  4. Supervised training with available parallel data
  5. Evaluation on FLORES-200 test set
- Design tradeoffs:
  - Smaller models (1024 emb_dim, 6 layers) vs. larger models for better performance but higher resource requirements
  - Code-switching introduces additional computational overhead but potentially better cross-lingual representations
  - Synthetic data generation adds training time but can significantly improve performance with limited resources
- Failure signatures:
  - Poor translation quality in one direction but not the other suggests data imbalance or directionality issues
  - Code-switching degrades performance instead of improving it indicates poor bilingual dictionary quality
  - Synthetic data doesn't improve performance suggests the generative AI's translations are not reliable
- First 3 experiments:
  1. Train a basic Scratch-Unsup model to establish baseline performance without any pre-training or synthetic data
  2. Implement CodeXL approach with code-switching to test the impact of cross-lingual signals during pre-training
  3. Generate synthetic parallel data using GPT-3.5-turbo and train a Scratch-Semisup model to evaluate synthetic data effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the denoising autoencoding objectives contribute to cross-lingual language understanding in extremely low-resource NMT, beyond just improving model robustness?
- Basis in paper: [explicit] The authors note that "denoising-autoencoding plays a more significant role in model performance than parallel data when the training parallel data is limited" for the id-ban language pair, and suggest this might indicate a role in cross-lingual language understanding beyond robustness.
- Why unresolved: The paper does not provide detailed analysis of the mechanisms by which denoising autoencoding improves cross-lingual understanding in extremely low-resource settings, nor does it compare its effects to other objectives.
- What evidence would resolve it: Detailed ablation studies comparing denoising autoencoding with other objectives (like masked language modeling) on extremely low-resource NMT tasks, along with analysis of model internal representations to identify cross-lingual understanding mechanisms.

### Open Question 2
- Question: What is the optimal amount and quality of synthetic parallel data needed to significantly improve NMT performance in extremely low-resource settings, and how does this vary by language pair?
- Basis in paper: [explicit] The authors found that adding 5000 synthetic parallel sentences only marginally improved performance for id-ban, suggesting that the amount and quality of synthetic data may be insufficient or not optimally generated.
- Why unresolved: The paper only explores one synthetic data generation method (zero-shot prompting) and one amount of data (5000 sentences), without systematically varying these parameters or comparing different generation approaches.
- What evidence would resolve it: Systematic studies varying the amount of synthetic data (e.g., 1000, 5000, 10000 sentences) and comparing different generation methods (few-shot prompting, back-translation, retrieval-based methods) across multiple language pairs to identify optimal data characteristics.

### Open Question 3
- Question: Why does the CodeXL approach show superior performance over PreXL, and what specific aspects of code-switching pre-training contribute to this improvement?
- Basis in paper: [explicit] The authors observe that "CodeXL training approaches generally create NMT systems with better performances compared to Scratch and PreXL approaches" but do not provide detailed analysis of why this occurs or which aspects of code-switching are most beneficial.
- Why unresolved: The paper does not include detailed analysis of the internal representations or learning dynamics that differentiate CodeXL from PreXL, nor does it isolate specific components of the code-switching approach.
- What evidence would resolve it: Detailed analysis of model representations (e.g., through probing tasks or attention visualization) comparing CodeXL and PreXL models, along with ablation studies isolating different aspects of code-switching (e.g., amount of code-switching, dictionary quality, training schedule).

## Limitations

- Study relies heavily on synthetic data generated by GPT-3.5-turbo without independent quality verification
- Evaluation uses only spm200BLEU metric, which may not capture full translation quality nuances
- Bilingual dictionaries for code-switching are not described in detail, raising questions about comprehensiveness and accuracy

## Confidence

**High Confidence:**
- The Code-switched Pre-trained Cross-Lingual approach outperforms other training approaches
- Synthetic parallel data generation improves NMT performance
- The study provides reproducible benchmarking methodology for low-resource languages

**Medium Confidence:**
- Synthetic data can match or exceed GPT-3.5-turbo's zero-shot translation performance
- Denoising autoencoding plays a more prominent role than parallel data in extremely low-resource settings

**Low Confidence:**
- The exact mechanisms by which code-switching improves cross-lingual representations
- The long-term generalization of models trained with synthetic data

## Next Checks

1. **Synthetic Data Quality Validation**: Conduct human evaluation of synthetic parallel data generated by GPT-3.5-turbo to verify translation accuracy and identify potential systematic errors or biases that may affect model training.

2. **Code-switching Dictionary Audit**: Evaluate the completeness and accuracy of the bilingual dictionaries used for code-switching by measuring translation accuracy at the word level and identifying coverage gaps across different domains.

3. **Extended Evaluation Suite**: Test the trained models on additional evaluation datasets beyond FLORES-200, including domain-specific test sets (news, religious texts, conversational data) to assess robustness across different translation contexts.