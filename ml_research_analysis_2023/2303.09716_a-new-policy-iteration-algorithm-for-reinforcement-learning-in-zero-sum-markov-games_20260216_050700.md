---
ver: rpa2
title: A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov
  Games
arxiv_id: '2303.09716'
source_url: https://arxiv.org/abs/2303.09716
tags:
- policy
- algorithm
- games
- iteration
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the problem of computing optimal policies in
  zero-sum Markov games, a long-standing open problem in reinforcement learning. The
  authors propose a simple variant of the naive policy iteration algorithm that converges
  exponentially fast for all discounted-reward/cost zero-sum Markov games.
---

# A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games

## Quick Facts
- **arXiv ID**: 2303.09716
- **Source URL**: https://arxiv.org/abs/2303.09716
- **Reference count**: 32
- **Primary result**: Simple variant of naive policy iteration converges exponentially fast for all discounted-reward/cost zero-sum Markov games

## Executive Summary
This paper addresses the long-standing open problem of computing optimal policies in zero-sum Markov games by proposing a simple variant of the naive policy iteration algorithm. The key modification involves using lookahead policies in the policy improvement phase, which is appealing because lookahead is commonly used in practical RL algorithms. The authors show that this modification leads to exponential convergence for all discounted zero-sum Markov games and can be implemented efficiently in the function approximation setting of linear Markov games.

## Method Summary
The method builds upon naive policy iteration by replacing the policy improvement step with a lookahead version that iteratively applies the Bellman operator H-1 times before determining the greedy policy. In the function approximation setting, the algorithm exploits the linear structure of the problem to reduce computational complexity from depending on state and action space cardinalities to depending only on the dimension of feature vectors. The algorithm combines a learning phase that estimates transition probabilities from samples with a planning phase that uses generalized policy iteration to find the Nash equilibrium policy.

## Key Results
- The modified policy iteration algorithm converges exponentially fast for all discounted zero-sum Markov games
- Lookahead can be implemented efficiently in linear Markov games, reducing computational complexity
- The algorithm provides sample and time complexity bounds for model-based RL algorithms using generalized policy iteration for planning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm converges exponentially fast for all discounted-reward/cost zero-sum Markov games.
- Mechanism: The key modification is replacing the policy improvement step with a lookahead version that iteratively applies the Bellman operator H-1 times before determining the greedy policy.
- Core assumption: The lookahead depth H satisfies Assumption 1: α^(H-1) + 2(1 + α^m) α^(H-1)/(1-α) < 1.
- Evidence anchors:
  - [abstract] The only addition we propose to naive policy iteration is the use of lookahead policies, which are anyway used in practical algorithms.
  - [section] We present a simple modification of the well-studied naive policy iteration or the algorithm of Pollatschek and Avi-Itzhak Pollatschek and Avi-Itzhak (1969) which converges for all discounted, simultaneous-move Markov zero-sum games.
  - [corpus] The paper directly addresses convergence guarantees for the naive policy iteration algorithm, which is a well-known open problem in reinforcement learning.
- Break condition: If the lookahead depth H is too small (violating Assumption 1), the algorithm may not converge or may converge subexponentially.

### Mechanism 2
- Claim: Lookahead can be implemented efficiently in the function approximation setting of linear Markov games.
- Mechanism: The algorithm exploits the linear structure of the problem to reduce computational complexity from depending on state and action space cardinalities to depending only on the dimension of feature vectors.
- Core assumption: The model follows the linear MDP structure where rewards and transitions can be expressed as inner products with feature vectors.
- Evidence anchors:
  - [abstract] We further show that lookahead can be implemented efficiently in the function approximation setting of linear Markov games, which are the counterpart of the much-studied linear MDPs.
  - [section] We then extend the results to incorporate function approximation by studying convergence and scalability to lower dimension linear MDPs.
  - [section] The computations in Algorithm 3 do not depend on the cardinalities of the state and action spaces and rather depend on the dimension of the feature vectors used to represent the model.
- Break condition: If the linear MDP assumption is violated or the feature vectors are too high-dimensional, the computational benefits may be lost.

### Mechanism 3
- Claim: The algorithm provides sample and time complexity bounds for model-based RL algorithms that use generalized policy iteration for planning.
- Mechanism: By combining the learning phase from Zhang et al. (2020) with the generalized policy iteration algorithm, the paper provides complete characterization of sample complexity.
- Core assumption: The learning oracle provides accurate estimates of the transition matrix from samples.
- Evidence anchors:
  - [abstract] We illustrate the application of our algorithm by providing bounds for policy-based RL (reinforcement learning) algorithms.
  - [section] We then consider multi-agent reinforcement learning which uses our algorithm in the planning phases, and provide sample and time complexity bounds for such an algorithm.
  - [section] The algorithm assumes knowledge of the reward function (this setting is called the reward-aware setting) as well as access to a generator, which, at any iteration, for any state-actions tuple (s, a1, a2) can sample from the distribution P(·|s, a1, a2).
- Break condition: If the learning oracle provides poor estimates of the transition matrix, the policy iteration algorithm may converge to suboptimal policies.

## Foundational Learning

- Concept: Zero-sum Markov games and Nash equilibria
  - Why needed here: The paper studies computing optimal policies in zero-sum Markov games, which requires understanding the concept of Nash equilibria in game-theoretic settings.
  - Quick check question: What is the difference between a Nash equilibrium and a Stackelberg equilibrium in the context of Markov games?

- Concept: Policy iteration and value iteration algorithms
  - Why needed here: The paper builds upon and extends the policy iteration algorithm, so understanding how policy iteration works in standard MDPs is crucial.
  - Quick check question: How does policy iteration differ from value iteration in terms of convergence properties and computational requirements?

- Concept: Function approximation in reinforcement learning
  - Why needed here: The paper extends results to the function approximation setting of linear Markov games, which requires understanding how to represent value functions using feature vectors.
  - Quick check question: What are the advantages and disadvantages of using linear function approximation versus non-linear function approximation in reinforcement learning?

## Architecture Onboarding

- Component map: Learning phase -> Planning phase (Generalized Policy Iteration) -> Nash Equilibrium Policy
- Critical path: Learning phase → Planning phase (Generalized Policy Iteration) → Nash Equilibrium Policy
- Design tradeoffs:
  - Trade-off between lookahead depth H and computational complexity
  - Trade-off between sample complexity in learning phase and computational complexity in planning phase
  - Choice between model-based and model-free approaches
- Failure signatures:
  - Non-convergence of the policy iteration algorithm (may indicate insufficient lookahead depth)
  - Large error in estimated transition matrix (may indicate insufficient samples)
  - Suboptimal policies (may indicate violation of linear MDP assumption)
- First 3 experiments:
  1. Verify convergence of the algorithm on a small, synthetic zero-sum Markov game with known optimal policy
  2. Compare the performance of the algorithm with and without lookahead on a linear Markov game
  3. Test the algorithm's sample efficiency by varying the number of samples used in the learning phase and measuring the resulting policy quality

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the content, several important directions remain unexplored:

### Open Question 1
- Question: Under what conditions does the convergence rate of the lookahead policy iteration algorithm degrade in practice, particularly in large state-action spaces or with sparse rewards?
- Basis in paper: The paper proves exponential convergence under certain assumptions but doesn't explore practical convergence rates or conditions where performance might degrade.
- Why unresolved: The theoretical analysis assumes a known model and doesn't account for practical issues like function approximation errors, exploration-exploitation trade-offs, or computational constraints in large problems.
- What evidence would resolve it: Empirical studies comparing convergence rates across different problem scales, reward structures, and approximation methods would help identify practical limitations.

### Open Question 2
- Question: How can the lookahead policy iteration algorithm be extended to handle stochastic shortest path (SSP) problems without discount factors, and what modifications are needed to ensure convergence?
- Basis in paper: The authors mention SSP as a future direction but don't provide any analysis or proposed solutions for this setting.
- Why unresolved: SSP problems have fundamentally different characteristics (no discount factor, potential for infinite returns) that make standard policy iteration approaches problematic.
- What evidence would resolve it: A modified algorithm with convergence guarantees for SSP problems, along with empirical validation on benchmark problems.

### Open Question 3
- Question: What are the sample complexity and computational trade-offs when using the lookahead policy iteration algorithm in the function approximation setting, particularly with non-linear function approximators?
- Basis in paper: The paper provides bounds for linear function approximation but doesn't explore non-linear approximators or the trade-offs between sample complexity and computational requirements.
- Why unresolved: Non-linear function approximators are more practical for complex problems but introduce new challenges in convergence analysis and computational efficiency.
- What evidence would resolve it: Comparative studies of sample complexity and computational requirements across different function approximation methods, with theoretical bounds for non-linear settings.

## Limitations

- The algorithm requires knowledge of the reward function and access to a model generator, making it primarily model-based rather than model-free
- The lookahead depth H must satisfy specific technical conditions (Assumption 1) for guaranteed convergence
- The approach is currently limited to the linear MDP setting, restricting its applicability to problems where such structure exists

## Confidence

- **High Confidence**: The exponential convergence result for the modified policy iteration algorithm under the stated assumptions
- **Medium Confidence**: The sample complexity bounds for the combined learning and planning algorithm, as they depend on constants from other work
- **Medium Confidence**: The computational efficiency claims for the linear MDP setting, which assume the feature vectors are well-chosen

## Next Checks

1. **Convergence Verification**: Implement the algorithm on a small synthetic zero-sum Markov game with known optimal policy to empirically verify exponential convergence under Assumption 1
2. **Feature Sensitivity Analysis**: Test the algorithm's performance across different feature vector choices in linear MDPs to understand sensitivity to representation quality
3. **Assumption Robustness**: Experimentally evaluate how violations of Assumption 1 (insufficient lookahead depth) affect convergence rates and final policy quality