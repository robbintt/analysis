---
ver: rpa2
title: An Invitation to Deep Reinforcement Learning
arxiv_id: '2312.08365'
source_url: https://arxiv.org/abs/2312.08365
tags:
- learning
- policy
- reinforcement
- data
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an introductory tutorial on deep reinforcement
  learning (RL) that bridges RL concepts to supervised learning by framing RL as optimization
  of non-differentiable objectives. The key insight is that RL can optimize objectives
  like accuracy or execution speed that are not differentiable, which supervised learning
  cannot directly optimize due to zero gradients in step functions.
---

# An Invitation to Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.08365
- Source URL: https://arxiv.org/abs/2312.08365
- Authors: 
- Reference count: 40
- Primary result: RL can optimize non-differentiable objectives like accuracy or execution speed that supervised learning cannot directly optimize

## Executive Summary
This paper presents an introductory tutorial on deep reinforcement learning that bridges RL concepts to supervised learning by framing RL as optimization of non-differentiable objectives. The key insight is that RL can optimize objectives like accuracy or execution speed that are not differentiable, which supervised learning cannot directly optimize due to zero gradients in step functions. The tutorial introduces two main approaches: value learning (predicting rewards with Q-functions) and stochastic policy gradients (up-weighting action probabilities proportional to rewards). These concepts are first demonstrated on single-step problems like image classification, then extended to sequential decision-making. The paper connects RL to supervised learning by showing how cross-entropy loss can be viewed as a policy gradient method optimizing accuracy.

## Method Summary
The tutorial introduces reinforcement learning as optimization of non-differentiable objectives through two main approaches. Value learning uses Q-functions to predict rewards for each action, with the policy implicitly defined by choosing the action with highest predicted reward. This is trained using mean squared error loss between predicted and observed rewards. Stochastic policy gradients train a policy network that outputs probability distributions over actions, with the policy optimized to maximize expected reward by following the policy gradient that up-weights action probabilities proportional to rewards received. The tutorial extends these concepts to sequential decision-making using temporal difference learning and modern algorithms like Soft Actor-Critic (SAC) for off-policy learning and Proximal Policy Optimization (PPO) for on-policy learning.

## Key Results
- RL can optimize non-differentiable objectives like accuracy and execution speed that supervised learning cannot directly optimize
- Cross-entropy loss can be interpreted as a policy gradient method optimizing accuracy
- Q-learning and policy gradients can achieve comparable performance to supervised learning on single-step problems when optimizing accuracy directly
- Modern algorithms like PPO and SAC extend these concepts to complex sequential decision-making problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL can optimize non-differentiable objectives that supervised learning cannot directly optimize due to zero gradients in step functions.
- Mechanism: RL bypasses the need for gradients by directly predicting rewards and using them to reinforce actions, rather than trying to compute gradients through a non-differentiable objective.
- Core assumption: The reward function can be sampled or computed without requiring differentiability.
- Evidence anchors:
  - [abstract]: "A common workaround is to define differentiable surrogate losses, leading to suboptimal solutions with respect to the actual objective. Reinforcement learning (RL) has emerged as a promising alternative for optimizing deep neural networks to maximize non-differentiable objectives in recent years."
  - [section 2]: "In reinforcement learning, the scalar r is called the reward and the objective is to maximize the reward function R(s,a)→r. Reward functions R represent a generalization of loss functions L as they do not need to be differentiable and the optimal actions a⋆=h(s) does not need to be known."

### Mechanism 2
- Claim: Value learning (Q-learning) can optimize non-differentiable objectives by predicting rewards and implicitly defining a policy via argmax.
- Mechanism: A Q-function predicts the reward for each action given a state. The policy is implicitly defined by choosing the action with the highest predicted reward. The Q-function is trained to minimize the difference between predicted and observed rewards using MSE loss.
- Core assumption: The action space is discrete (or can be approximated as discrete for continuous actions).
- Evidence anchors:
  - [section 3.1]: "The most popular idea to maximize rewards without a gradient from the action a to the reward r is value learning... The policy π is implicitly defined by choosing the action for which the Q-function predicts the highest reward: π(s) = argmax_a Q(s,a)."
  - [section 3.1.3]: "Our goal is to train a ResNet50 (He et al., 2016) for image classification on CIFAR-10 (Krizhevsky et al., 2009), optimizing accuracy directly with Q-learning... The reward labels can therefore be naturally represented as one-hot vectors, just like in supervised learning. Thus, changing the loss function from cross-entropy to mean squared error is the only change necessary to switch from supervised learning to Q-learning."

### Mechanism 3
- Claim: Stochastic policy gradients can optimize non-differentiable objectives by up-weighting action probabilities proportional to rewards.
- Mechanism: A policy network predicts a probability distribution over actions. Actions are sampled from this distribution. The policy is optimized to maximize the expected reward by following the policy gradient, which up-weights action probabilities proportional to the rewards they receive.
- Core assumption: The policy output must be a probability distribution to avoid degenerate solutions.
- Evidence anchors:
  - [section 3.2]: "The second popular idea to maximize non-differentiable rewards in reinforcement learning is called stochastic policy gradients or policy gradients in short. Policy gradient methods train a policy network π(a|s) that predicts a probability distribution over actions. The central idea is to ignore the non-differentiable gap, and instead to use the rewards directly to change the action distribution."
  - [section 3.2.1]: "Consider again the example of classification, but this time with policy gradients... We see that accuracy maximization is another motivation for cross-entropy, besides the standard information theoretic derivation (Shore & Johnson, 1980) and maximum likelihood estimation (Bishop, 2006)."

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: RL is the main topic of the paper. It provides a framework for optimizing non-differentiable objectives by directly maximizing rewards.
  - Quick check question: What is the key difference between supervised learning and reinforcement learning in terms of the objective function?

- Concept: Value Learning (Q-Learning)
  - Why needed here: Value learning is one of the two main approaches to RL introduced in the paper. It provides a way to optimize non-differentiable objectives by predicting rewards and implicitly defining a policy via argmax.
  - Quick check question: How does Q-learning differ from supervised learning in terms of the loss function used?

- Concept: Stochastic Policy Gradients
  - Why needed here: Stochastic policy gradients is the other main approach to RL introduced in the paper. It provides a way to optimize non-differentiable objectives by up-weighting action probabilities proportional to rewards.
  - Quick check question: Why must the policy output a probability distribution in policy gradient methods?

## Architecture Onboarding

- Component map:
  - State -> Policy Network/Q-Function -> Action/Action Probabilities -> Environment -> Reward/Next State -> Policy/Q-Function Update

- Critical path:
  - For Q-learning: State → Q-Function → Argmax → Action → Environment → Reward → Q-Function Update
  - For Policy Gradients: State → Policy Network → Action → Environment → Reward → Policy Update

- Design tradeoffs:
  - Off-policy vs. On-policy: Off-policy learning (Q-learning) is more sample efficient but can suffer from the compounding error problem. On-policy learning (Policy Gradients) is less sample efficient but avoids compounding errors.
  - Value Learning vs. Policy Gradients: Value learning is simpler but can struggle with continuous action spaces. Policy gradients are more general but can have higher variance.

- Failure signatures:
  - Q-learning: Q-values collapse to a constant (self-overfitting), maximization bias, long time horizons.
  - Policy Gradients: High variance in gradients, difficulty with sparse rewards, compounding errors in sequential problems.

- First 3 experiments:
  1. Implement a simple Q-learning algorithm for a discrete action space problem (e.g., gridworld).
  2. Implement a policy gradient algorithm for a continuous action space problem (e.g., pendulum).
  3. Compare the performance of Q-learning and policy gradients on a benchmark problem (e.g., CartPole).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RL algorithms be effectively scaled to handle deep neural networks with more than a few layers?
- Basis in paper: [explicit] The paper mentions that "Off-policy Q-learning methods often struggle to optimize very deep networks" and notes this is "an active area of research"
- Why unresolved: Current Q-learning approaches show limited success with deep architectures, particularly in visual domains where they easily overfit. The paper identifies this as a significant challenge but doesn't provide a definitive solution.

### Open Question 2
- Question: What is the optimal balance between exploration and exploitation in RL for practical real-world applications?
- Basis in paper: [explicit] The paper discusses the "exploration-exploitation tradeoff" and mentions that "the best value of the discount factor depends on the environment and reward function"
- Why unresolved: The paper notes that while techniques like entropy bonuses and noise addition exist, there's no clear guide on optimal exploration strategies. The tradeoff appears to be problem-specific and context-dependent.

### Open Question 3
- Question: How can RL algorithms be made more sample-efficient for complex real-world problems?
- Basis in paper: [explicit] The paper identifies "sample efficiency" as a major drawback of both REINFORCE and on-policy learning methods, noting that "RL techniques require a lot of data even with off-policy methods"
- Why unresolved: While the paper discusses various techniques to improve sample efficiency (like prioritized experience replay and importance sampling), it acknowledges that reducing sample requirements remains "an active area of research".

## Limitations
- Q-learning struggles with very deep neural networks and visual domains where it easily overfits
- Policy gradients can have high variance in gradients and difficulty with sparse rewards
- Both approaches require significant sample efficiency improvements for complex real-world problems

## Confidence
- RL optimizing non-differentiable objectives: High
- Cross-entropy as policy gradient interpretation: Medium
- Treatment of sequential decision-making and modern algorithms: Medium

## Next Checks
1. Implement both cross-entropy and Q-learning on CIFAR-10 with identical architectures to verify the claim that MSE loss can match cross-entropy performance for accuracy maximization.
2. Apply policy gradient methods to a continuous control problem (e.g., MuJoCo tasks) to validate the theoretical framework extends beyond discrete classification.
3. Systematically measure the impact of the compounding error problem by training on different data collection strategies (on-policy vs. off-policy with replay buffers) and quantifying performance degradation over time horizons.