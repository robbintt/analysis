---
ver: rpa2
title: An Embedding-based Approach to Inconsistency-tolerant Reasoning with Inconsistent
  Ontologies
arxiv_id: '2304.01664'
source_url: https://arxiv.org/abs/2304.01664
tags:
- axioms
- ontology
- consistent
- semantic
- subsets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses inconsistency-tolerant reasoning with ontologies
  in description logics. The core idea is to leverage embedding models to compute
  semantic similarities between axioms and use this to score and select maximal consistent
  subsets (MCSs) for inference.
---

# An Embedding-based Approach to Inconsistency-tolerant Reasoning with Inconsistent Ontologies

## Quick Facts
- arXiv ID: 2304.01664
- Source URL: https://arxiv.org/abs/2304.01664
- Reference count: 10
- Primary result: Embedding-based method achieves up to 97.85% success rate on inconsistency-tolerant reasoning, outperforming existing MCS-based approaches

## Executive Summary
This paper introduces an embedding-based approach to inconsistency-tolerant reasoning in description logics ontologies. The method leverages semantic embeddings to represent axioms as vectors and uses similarity scores to select the most coherent maximal consistent subset (MCS) for inference. By transforming axioms into semantic vectors using sentence or knowledge graph embedding models, computing pairwise similarities, and aggregating these within each MCS, the approach selects the subset with highest semantic coherence. Experiments on six ontologies demonstrate significant performance improvements over traditional MCS-based methods, with success rates reaching 97.85%.

## Method Summary
The approach transforms DL axioms into semantic vectors using embedding models (sentence models like SBERT/RoBERTa or knowledge graph models like TransE/TransH), computes pairwise semantic similarities, aggregates these similarities within each MCS to score subsets, and selects the highest-scoring MCS for inference. The method is theoretically grounded through proof that the selection relation satisfies rationality properties (system P and R) via monotonic selection induced by the scoring function.

## Key Results
- Embedding-based method achieves success rates up to 97.85% on tested ontologies
- Outperforms existing MCS-based approaches across all six evaluated ontologies
- Demonstrates rational inference by satisfying system P and R logical properties
- Shows robustness across different embedding model choices (sentence and knowledge graph variants)

## Why This Works (Mechanism)

### Mechanism 1
Semantic embedding similarity captures logical relevance between axioms, enabling selection of coherent MCSs. Axioms are transformed into vectors using sentence or knowledge graph embedding models; similarity scores are computed pairwise, aggregated within each MCS, and used to rank and select the most semantically coherent subset. Core assumption: Embedding models preserve semantic relationships such that similar axioms in vector space correspond to logically related axioms in the ontology.

### Mechanism 2
Aggregation of axiom-level similarity scores within an MCS reflects its overall semantic coherence and reliability. For each axiom in an MCS, its similarity to all other axioms in that subset is averaged; these per-axiom scores are summed to produce a total MCS score, which is then used for ranking. Core assumption: The collective semantic cohesion of axioms in an MCS is a good proxy for the subset's usefulness in reasoning.

### Mechanism 3
Rational inference properties are preserved because the scoring function induces a monotonic selection relation. The axiom scoring function satisfies positivity (non-trivial axioms get positive scores), which ensures the induced selection relation is monotonic; monotonicity is proven equivalent to satisfying the system P and R logical properties. Core assumption: The monotonic selection relation defined via the embedding-based scoring function aligns with rational inference requirements.

## Foundational Learning

- Concept: Description Logic (DL) axioms and their consistency
  - Why needed here: The approach operates directly on DL ontologies, and MCS selection depends on understanding which axiom sets are consistent.
  - Quick check question: What distinguishes a terminological axiom from an assertional axiom in a DL ontology?

- Concept: Embedding models (sentence and knowledge graph)
  - Why needed here: The method relies on transforming axioms into vectors to compute semantic similarity; understanding how these models capture semantics is essential for interpreting results.
  - Quick check question: How does cosine similarity differ from Euclidean distance in measuring vector similarity, and which is used in this work?

- Concept: Logical properties of inference relations (system P and R)
  - Why needed here: The paper claims rationality by satisfying these properties; understanding them is necessary to evaluate the theoretical soundness.
  - Quick check question: What does the Cut property require of a preferential inference relation?

## Architecture Onboarding

- Component map: NaturalOWL preprocessing → Sentence/KG embedding models → Vector similarity computation → Aggregation within MCS → Scoring → MCS selection → Inference
- Critical path:
  1. Axiom encoding (embedding)
  2. Pairwise similarity calculation
  3. Aggregation to MCS score
  4. Selection of highest-scoring MCS
  5. Inference over selected MCS
- Design tradeoffs:
  - Embedding choice (sentence vs KG) affects semantic fidelity and computational cost
  - Similarity metric (cosine vs Euclidean) influences scoring sensitivity
  - Aggregation method (sum vs other) impacts MCS discrimination
- Failure signatures:
  - Low success rate despite high embedding quality → possible semantic misalignment
  - Inconsistent results across runs → embedding randomness or preprocessing variance
  - Long MCS selection time → scalability issue with large ontologies
- First 3 experiments:
  1. Verify that axioms with known semantic similarity yield high similarity scores after embedding
  2. Compare MCS scores before and after introducing controlled inconsistencies
  3. Test inference correctness on a small ontology with ground truth MCS selection

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the embedding-based method vary with the size and complexity of the ontology? The paper mentions experiments on six ontologies with varying sizes, but does not provide a detailed analysis of how performance scales with ontology size or complexity.

### Open Question 2
Can the embedding-based method be extended to handle ontologies with weighted axioms or uncertainty? The paper mentions future work on extending the method to weighted ontologies, but does not explore this direction.

### Open Question 3
How sensitive is the method to the choice of embedding model and similarity metric? While the paper presents results using various models and metrics, it does not systematically study the impact of these choices on the method's performance or provide guidance on selecting the best model and metric for a given ontology.

## Limitations
- Performance may not generalize to ontologies with axiom structures significantly different from the tested domains
- NaturalOWL preprocessing rules are not fully specified, creating reproducibility challenges
- Computational complexity for large ontologies with many MCSs is not thoroughly analyzed

## Confidence

- High confidence: Theoretical framework establishing rationality through monotonic selection relations and satisfaction of system P and R properties
- Medium confidence: Empirical performance claims given the specific ontology selection and embedding model choices may not generalize to all DL applications
- Low confidence: Scalability assessments as computational complexity for large ontologies is not thoroughly analyzed

## Next Checks

1. **Semantic fidelity test**: Verify that axioms with known logical relationships (synonyms, subsumptions, disjointness) maintain appropriate similarity scores across different embedding models before applying to full ontologies

2. **Cross-domain generalization**: Apply the method to ontologies from different domains (biomedical, geographic, bibliographic) to assess whether embedding-based scoring remains effective when axiom semantics differ substantially from the original test sets

3. **Ablation study on aggregation methods**: Compare the proposed aggregation approach against alternative scoring methods (e.g., max similarity, weighted sums) to determine if the specific aggregation mechanism contributes significantly to performance gains