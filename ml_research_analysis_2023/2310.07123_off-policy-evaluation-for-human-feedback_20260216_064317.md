---
ver: rpa2
title: Off-Policy Evaluation for Human Feedback
arxiv_id: '2310.07123'
source_url: https://arxiv.org/abs/2310.07123
tags:
- human
- returns
- policies
- opehf
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of evaluating policies using human
  feedback in reinforcement learning, especially when such feedback is sparse and
  conditioned on multiple factors. The authors propose an OPEHF framework that reconstructs
  immediate human rewards from end-of-episode returns, leveraging a latent space learned
  by a variational model to regularize the reconstruction.
---

# Off-Policy Evaluation for Human Feedback

## Quick Facts
- arXiv ID: 2310.07123
- Source URL: https://arxiv.org/abs/2310.07123
- Reference count: 40
- Key outcome: Introduces OPEHF framework that reconstructs immediate human rewards from end-of-episode returns using variational latent modeling, validated on real-world experiments showing improved accuracy over existing methods.

## Executive Summary
This paper addresses the challenge of off-policy evaluation (OPE) when human feedback is sparse and only provided at episode end. The authors propose a novel framework that reconstructs immediate human rewards from end-of-episode returns by leveraging a variational latent model to regularize the reconstruction process. This approach enables existing OPE methods to be applied to human feedback scenarios without requiring strong correlation between environmental and human rewards.

## Method Summary
The OPEHF framework introduces immediate human reward (IHR) reconstruction as a way to revive existing OPE methods for human feedback scenarios. The method uses a variational latent model with human returns (VLM-H) to encode environmental dynamics and human returns into a shared latent space. This latent space is then used to regularize the reconstruction of per-step IHRs from end-of-episode human returns. The reconstructed IHRs can be fed into standard OPE estimators (IS, DR, DICE, FQE) to estimate expected human returns under target policies.

## Key Results
- VLM-H with RILR regularization significantly improves IHR reconstruction accuracy compared to baseline methods
- OPEHF framework achieves lower mean absolute error and higher rank correlation in estimating human returns across multiple OPE estimators
- Method performs well even when environmental rewards and human feedback are weakly correlated
- Validated on two real-world experiments (adaptive deep brain stimulation and intelligent tutoring) and a visual Q&A simulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The IHR reconstruction objective, regularized by latent similarity, reduces variance in OPEHF compared to naive rescaling.
- Mechanism: By enforcing the cumulative reconstructed IHRs to match the sparse end-of-episode human returns, and regularizing per-step IHRs based on latent-space similarity, the method avoids extreme sparsity that inflates variance in standard OPE estimators.
- Core assumption: IHRs can be reasonably inferred from end-of-episode returns if constrained by latent dynamics and state-action similarity.
- Evidence anchors:
  - [abstract] "We introduce an OPE for HF (OPEHF) framework that revives existing OPE methods...through IHRs reconstruction."
  - [section 2.3] "we introduce the variational latent model with human returns (VLM-H)...to regularize the reconstructed IHRs."
  - [corpus] Weak - no direct corpus evidence found for this exact reconstruction objective.
- Break condition: If latent space does not meaningfully cluster state-action pairs with similar returns, the regularization term becomes ineffective and IHRs reconstruction fails.

### Mechanism 2
- Claim: The VLM-H captures both environmental and human returns in a shared latent space, enabling regularization without requiring strong correlation between the two.
- Mechanism: By encoding environmental rewards and human returns into the same latent space using separate decoders, the model learns representations that align state-action dynamics with return patterns, allowing IHRs to be inferred from environmental context even if human returns are sparse.
- Core assumption: Environmental dynamics and human feedback are correlated enough in latent space for meaningful regularization.
- Evidence anchors:
  - [abstract] "Our approach does not require the environmental rewards and the HF signals to be strongly correlated."
  - [section 2.3] "the VLM-H learns to reconstruct both by sampling from two independent distributions, pϕ(rt−1|zt) and pϕ(GH0:T |zT) respectively."
  - [corpus] Weak - no corpus evidence found for latent-space regularization in OPEHF.
- Break condition: If environmental and human return signals are uncorrelated in latent space, regularization fails and IHR reconstruction degrades.

### Mechanism 3
- Claim: The RILR objective preserves policy return structure across behavioral and target policies by leveraging behavioral policy coverage in the latent space.
- Mechanism: By regularizing IHRs reconstruction with K-nearest neighbor latent encodings from behavioral trajectories, the method ensures reconstructed IHRs respect the underlying return structure of the state-action space observed in offline data.
- Core assumption: Behavioral policy trajectories cover sufficient state-action space to provide meaningful neighbors in latent space.
- Evidence anchors:
  - [section 2.3] "J = {j0, ..., jK−1} are the indices of offline trajectories that correspond to the latent encodings...that are K-neighbours of the latent encoding z(i)t."
  - [corpus] Weak - no corpus evidence for K-NN regularization in OPEHF.
- Break condition: If behavioral policy coverage is insufficient or clusters poorly in latent space, K-NN regularization becomes unreliable and IHRs reconstruction fails.

## Foundational Learning

- Concept: Importance sampling and its variance reduction variants (PDIS, DR, DICE, FQE)
  - Why needed here: These are the downstream estimators that process reconstructed IHRs to estimate human returns.
  - Quick check question: What is the key difference between IS and PDIS in terms of variance reduction?

- Concept: Variational auto-encoders and latent space regularization
  - Why needed here: VLM-H uses VAE architecture to learn compact latent representations for both environmental dynamics and human returns, which regularize IHR reconstruction.
  - Quick check question: How does KL divergence regularization in VAEs affect the expressiveness of the learned latent space?

- Concept: K-nearest neighbor similarity in high-dimensional latent spaces
  - Why needed here: RILR uses K-NN in latent space to find similar state-action pairs for regularization, based on the assumption that similar pairs yield similar returns.
  - Quick check question: What distance metric is typically used for K-NN in latent spaces learned by VAEs?

## Architecture Onboarding

- Component map: VLM-H (encoder/decoder for environmental dynamics and human returns) → IHR reconstruction (bi-LSTM mapping end-of-episode returns to per-step rewards) → downstream OPE estimator (IS/DR/DICE/FQE)
- Critical path: Data → VLM-H training (ELBO optimization) → RILR objective optimization → Downstream OPE estimator → Performance metrics
- Design tradeoffs: Using latent regularization trades off model complexity and training time for improved estimation accuracy when human returns are sparse
- Failure signatures: High MAE/rank correlation indicates IHR reconstruction failed; check latent space clustering and behavioral policy coverage
- First 3 experiments:
  1. Train VLM-H on synthetic data with known latent structure to verify it learns meaningful representations
  2. Test IHR reconstruction with and without latent regularization to measure variance reduction
  3. Run OPEHF on a simple MDP with sparse human returns to verify the complete pipeline works

## Open Questions the Paper Calls Out

The paper mentions the possibility of extending OPEHF to handle conflicting feedback from multiple sources but does not provide specific strategies or results.

## Limitations

- Method's effectiveness depends heavily on the quality of the learned latent space and behavioral policy coverage
- Introduces additional model complexity and training time compared to naive OPE methods
- Performance may degrade if environmental and human returns are uncorrelated in latent space

## Confidence

- High confidence in the general framework design and mathematical formulation
- Medium confidence in the effectiveness of latent space regularization for IHR reconstruction
- Low confidence in the robustness across diverse real-world scenarios without extensive hyperparameter tuning

## Next Checks

1. **Latent Space Quality Analysis**: Evaluate whether the learned latent space meaningfully clusters state-action pairs with similar human returns across different behavioral policies and domains.

2. **Behavioral Policy Coverage Study**: Systematically vary the coverage and diversity of behavioral policies to quantify the impact on OPEHF performance and identify minimum coverage requirements.

3. **Correlation Threshold Investigation**: Test the method's performance when environmental and human rewards have varying degrees of correlation to determine the operational limits of the "no strong correlation required" claim.