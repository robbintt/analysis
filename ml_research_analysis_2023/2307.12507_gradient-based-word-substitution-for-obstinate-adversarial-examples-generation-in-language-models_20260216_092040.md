---
ver: rpa2
title: Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation
  in Language Models
arxiv_id: '2307.12507'
source_url: https://arxiv.org/abs/2307.12507
tags:
- language
- secret
- words
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the existence of "secret language" in language
  models, where models interpret semantically dissimilar inputs as equivalent. To
  address this, the authors propose a novel gradient-based method called SecretFinding
  to automatically discover such substitutions.
---

# Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models

## Quick Facts
- **arXiv ID**: 2307.12507
- **Source URL**: https://arxiv.org/abs/2307.12507
- **Reference count**: 40
- **Primary result**: SecretFinding generates high-accuracy adversarial examples by replacing semantically dissimilar words, with >95% success on multi-sentence tasks.

## Executive Summary
This paper investigates the phenomenon of "secret language" in language models, where models interpret semantically dissimilar inputs as equivalent. The authors propose SecretFinding, a gradient-based method that automatically discovers such substitutions by perturbing one-hot word representations until the model's prediction is preserved. Experiments on five NLP tasks and five models show that SecretFinding successfully generates adversarial examples that fool models with high accuracy, and these substitutions transfer across models and even to GPT-3 and ChatGPT.

## Method Summary
SecretFinding uses gradient descent to find word substitutions that preserve model output while being semantically dissimilar to humans. The method identifies important words using Integrated Gradients (IG), then optimizes a noise vector added to the one-hot representation of each word to minimize cross-entropy loss while maintaining the original prediction. A WordNet filter ensures semantic dissimilarity. The algorithm is evaluated on five models (Electra, ALBERT, Roberta, DistillBERT, CLIP) finetuned on four NLP benchmarks and MSCOCO, with transferability tests to GPT-3 and ChatGPT.

## Key Results
- SecretFinding achieves >95% accuracy when replacing 2 words in multi-sentence tasks.
- Secret languages transfer across models with high accuracy (e.g., 96% from ALBERT to other models).
- The phenomenon is most prominent in models finetuned on simpler tasks (e.g., GLUE/MRPC).
- Words with higher frequency in natural language are easier to find secret languages for when transferring to models like GPT-3 and ChatGPT.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SecretFinding exploits the fact that language models map semantically dissimilar words to the same internal representation when the output class is preserved.
- Mechanism: The algorithm perturbs the one-hot representation of a word via gradient descent on cross-entropy loss until the model's prediction matches the original label. Because the perturbation is optimized to maintain the output while altering the input token, it finds words that are functionally equivalent in the model's learned space, even if semantically different to humans.
- Core assumption: The model's decision boundary is locally flat in token embedding space for the current prediction, so small perturbations in the one-hot space can traverse to semantically different words without changing the output.
- Evidence anchors:
  - [abstract] "Our proposed GradObstinate generates more powerful obstinate adversarial examples, exhibiting a higher attack success rate compared to antonym-based methods."
  - [section 2.2] "we use gradient descent to update the noise vector z in order to find a replacement that satisfies our requirements... measure the distance between the outputs... using the cross entropy loss"
- Break condition: If the model's internal representations are highly sensitive to token changes, or if the loss surface is too steep, the optimization will fail to find semantically dissimilar substitutions that preserve the output.

### Mechanism 2
- Claim: The method's effectiveness is amplified when applied to words with high Integrated Gradients (IG) scores, because these words contribute most to the model's decision.
- Mechanism: IG identifies tokens that most influence the output. By targeting these high-impact words for substitution, SecretFinding maximizes the chance that changing them will not alter the model's classification, revealing hidden equivalences in the model's understanding.
- Core assumption: Words with high IG scores are indeed the most critical for the model's decision; replacing them with semantically different words will test whether the model truly "understands" them.
- Evidence anchors:
  - [section 2.1] "we use IG to decide the importance of each word in a sentence... By sorting the value of IG, we get the importance ranking of each word."
  - [section 3.2] "The accuracy for finding semantically dissimilar replacements is inversely proportional to the number of words to be replaced."
- Break condition: If IG misidentifies importance (e.g., due to attention-based noise), or if the model relies on low-scoring words for its decision, the attack may fail or miss vulnerabilities.

### Mechanism 3
- Claim: Secret languages discovered by SecretFinding are transferable across models because different models share overlapping learned representations for certain functional equivalences.
- Mechanism: Once a substitution is found that fools one model, the same pair often fools another model in a black-box setting, indicating that the semantic confusion is rooted in common training data or architectural biases rather than model-specific quirks.
- Core assumption: Different models learn similar "shortcuts" or biases from their training corpora, making them vulnerable to the same semantic substitutions.
- Evidence anchors:
  - [section 3.4] "we transfer the secret languages found on GPT-2 (Radford et al., 2019) and Roberta to GPT-3 (Brown et al., 2020) and ChatGPT (OpenAI, 2023)... the words with higher frequency seem to be easier to find secret languages in this transfer-based method"
  - [section 4] "the secret language may not only be independent of the specific context, but is also independent of the specific models as the secret language that we find on DistillBERT and Roberta is able to transfer to each other with a high accuracy"
- Break condition: If models are trained on very different data or with architectures that enforce diverse representations, transferability will drop.

## Foundational Learning

- Concept: Integrated Gradients (IG) attribution
  - Why needed here: IG is used to identify the most important words in a sentence whose replacement is most likely to reveal secret language without altering the model's output.
  - Quick check question: If a word has IG score 0.8 and another has 0.2, which one should be prioritized for substitution in SecretFinding and why?

- Concept: Gradient-based adversarial example generation
  - Why needed here: The core of SecretFinding is using gradients to find token substitutions that preserve model output, which is a variant of adversarial attack methodology.
  - Quick check question: What loss function is optimized in SecretFinding to ensure the perturbed word keeps the same model output, and how is it computed?

- Concept: Semantic similarity vs. functional equivalence in NLP models
  - Why needed here: The paper's key insight is that models may treat semantically different words as functionally equivalent; understanding this distinction is crucial for interpreting results.
  - Quick check question: Why does replacing "yoga" with "burg" preserve the model's output, even though humans see them as semantically unrelated?

## Architecture Onboarding

- Component map: Tokenizer -> IG module -> SecretFinding module -> WordNet filter -> Evaluation
- Critical path:
  1. Input sentence → Tokenizer → IG scores → Select high-scoring words.
  2. For each selected word: SecretFinding optimization → Candidate substitution.
  3. WordNet filter → Final secret language set.
  4. Evaluate on model(s) → Record accuracy.
- Design tradeoffs:
  - High IG word selection increases success but may miss vulnerabilities in low-scoring words.
  - Using cross-entropy loss vs. direct output equality as stopping criterion: CE provides smoother gradients but may require more steps.
  - WordNet filtering ensures semantic dissimilarity but may exclude rare but valid secret languages.
- Failure signatures:
  - Optimization stalls (no progress in loss over many steps).
  - All substitutions fail WordNet filter (no secret languages found).
  - Transferability accuracy is low (model-specific secret languages).
- First 3 experiments:
  1. Run SecretFinding on a single sentence with 1 high-IG word; verify output equality and WordNet filtering.
  2. Vary the number of words replaced (1 to 5) on a small dataset; plot accuracy vs. words replaced.
  3. Transfer a set of found secret languages from DistillBERT to RoBERTa; measure black-box accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the transferability of secret language vary across different NLP tasks (e.g., sentiment analysis, NLI, QA)?
- Basis in paper: [explicit] The paper demonstrates that secret language can transfer across models and tasks, but the success rate varies. For example, secret language found on ALBERT (GLUE/MRPC) has high transferability to other models, while secret language found on Roberta (SQuAD) has lower transferability.
- Why unresolved: The paper does not provide a systematic analysis of how transferability varies across different tasks or why certain tasks (like MRPC) are more susceptible to secret language transfer than others (like SQuAD).
- What evidence would resolve it: A comprehensive study comparing transferability success rates across multiple tasks, along with an analysis of task-specific features (e.g., complexity, dataset size) that influence transferability.

### Open Question 2
- Question: Can secret language be effectively mitigated by finetuning models on more challenging or diverse datasets?
- Basis in paper: [inferred] The paper suggests that models finetuned on simpler tasks (e.g., GLUE/MRPC) are more vulnerable to secret language than those finetuned on complex tasks (e.g., SQuAD, SNLI). However, it does not explore whether finetuning on more challenging datasets can eliminate secret language.
- Why unresolved: The paper only hints at the potential benefits of using challenging benchmarks but does not empirically test whether finetuning on such datasets reduces secret language susceptibility.
- What evidence would resolve it: Experiments comparing the robustness of models finetuned on simple vs. complex datasets, along with ablation studies to identify which aspects of challenging datasets contribute to mitigation.

### Open Question 3
- Question: How does the frequency of words in natural language influence the likelihood of discovering secret language for those words?
- Basis in paper: [explicit] The paper notes that words with higher frequency in human language (e.g., "amazon forest") are easier to find secret languages for when transferring to models like GPT-3 and ChatGPT. However, it does not explore this relationship systematically.
- Why unresolved: The paper only provides anecdotal evidence (e.g., "amazon forest" vs. "Turner") without quantifying how word frequency correlates with secret language discovery.
- What evidence would resolve it: A statistical analysis of word frequency in natural language corpora versus the success rate of finding secret languages for those words, along with experiments manipulating word frequency in training data.

### Open Question 4
- Question: Are there architectural differences in language models that make them more or less susceptible to secret language?
- Basis in paper: [inferred] The paper compares multiple models (e.g., ALBERT, DistillBERT, Roberta, Electra, CLIP) and observes varying levels of susceptibility to secret language. However, it does not investigate whether specific architectural features (e.g., pretraining objectives, model depth) influence this susceptibility.
- Why unresolved: The paper only provides comparative results across models without analyzing the underlying architectural factors that contribute to differences in secret language susceptibility.
- What evidence would resolve it: A controlled study comparing models with varying architectures (e.g., BERT vs. ELECTRA vs. ALBERT) to identify which architectural features correlate with higher or lower susceptibility to secret language.

## Limitations

- The method relies on WordNet for semantic dissimilarity filtering, which may have incomplete coverage of modern or rare words, potentially allowing semantically related words to pass as "secret languages."
- Transferability experiments to GPT-3 and ChatGPT are black-box and limited in scope, raising questions about generalizability beyond tested models and tasks.
- The method's dependence on high IG scores for word selection may systematically miss vulnerabilities in less important words, biasing results toward certain types of failures.

## Confidence

- **High confidence** in the experimental results showing SecretFinding's effectiveness on the tested models and tasks, given the detailed accuracy metrics and ablation studies.
- **Medium confidence** in the mechanism claims (gradient-based perturbation finding functional equivalence), as the core idea is plausible but the exact reasons for success across diverse models are not fully explained.
- **Low confidence** in the generalizability and practical significance of "secret languages" beyond the tested domains, due to limited model diversity and potential WordNet filtering issues.

## Next Checks

1. **WordNet Coverage Audit**: Manually verify a sample of the discovered secret language pairs to confirm they are truly semantically unrelated and not excluded by WordNet's limitations.
2. **IG-Independent Attack**: Run SecretFinding with random word selection (instead of IG) to assess whether the method's success depends on IG-based prioritization or can find vulnerabilities elsewhere.
3. **Broader Model Transferability**: Test the transferability of secret languages to a wider range of models (e.g., T5, GPT-4) and tasks (e.g., summarization, translation) to evaluate generalizability beyond the current scope.