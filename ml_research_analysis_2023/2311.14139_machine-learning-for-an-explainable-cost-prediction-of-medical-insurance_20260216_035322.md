---
ver: rpa2
title: Machine Learning For An Explainable Cost Prediction of Medical Insurance
arxiv_id: '2311.14139'
source_url: https://arxiv.org/abs/2311.14139
tags:
- insurance
- data
- https
- learning
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study deployed three ensemble ML models (XGBoost, GBM, RF)
  to predict medical insurance costs using 986 records from KAGGLE. XGBoost achieved
  the best performance with R2=86.47% and RMSE=2231.52, though it consumed more computational
  resources than RF and GBM.
---

# Machine Learning For An Explainable Cost Prediction of Medical Insurance

## Quick Facts
- arXiv ID: 2311.14139
- Source URL: https://arxiv.org/abs/2311.14139
- Reference count: 22
- Key outcome: XGBoost achieved R2=86.47% and RMSE=2231.52 for medical insurance cost prediction

## Executive Summary
This study deployed three ensemble ML models (XGBoost, GBM, RF) to predict medical insurance costs using 986 records from KAGGLE. XGBoost achieved the best performance with R2=86.47% and RMSE=2231.52, though it consumed more computational resources than RF and GBM. RF recorded the lowest MAE (1379.96) and MAPE (5.83%) while using fewer resources. SHAP and ICE XAi methods identified Age, BMI, Chronic Diseases, and Family Cancer History as key premium determinants. XGBoost and GBM showed minimal impact from Blood Pressure and Allergies, while RF indicated a BMI-diabetes interaction. ICE plots revealed more detailed feature interactions than SHAP. The study demonstrates ML's effectiveness in transparent insurance cost prediction and its potential for decision support in policy selection.

## Method Summary
The study used a medical insurance dataset from KAGGLE with 986 records and 11 attributes. Three ensemble models (XGBoost, GBM, RF) were trained and optimized using GridSearchCV with 20-fold cross-validation. SHAP and ICE plots were generated to analyze feature importance and interactions. Performance was evaluated using R2, MAE, RMSE, and MAPE metrics.

## Key Results
- XGBoost achieved the highest R2 (86.47%) and lowest RMSE (2231.52) but required more computational resources
- RF achieved the lowest MAE (1379.96) and MAPE (5.83%) with better computational efficiency
- Age, BMI, Chronic Diseases, and Family Cancer History were identified as key premium determinants
- ICE plots revealed more detailed feature interactions than SHAP analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** XGBoost's gradient boosting framework with regularization terms yields superior performance over Random Forest and GBM for medical insurance cost prediction.
- **Mechanism:** XGBoost uses sequential weak learner addition with gradient descent optimization and L1/L2 regularization to iteratively reduce prediction error. This contrasts with Random Forest's parallel tree construction and GBM's sequential addition without the same regularization strength.
- **Core assumption:** The dataset contains nonlinear interactions and complex feature dependencies that benefit from iterative boosting and regularization.
- **Evidence anchors:**
  - [abstract] "XGBoost achieved the best performance with R2=86.47% and RMSE=2231.52, though it consumed more computational resources than RF and GBM"
  - [section] "XGBoost aims to improve model performance and computational speed via the boosting approach"
  - [corpus] Weak - no direct corpus evidence found
- **Break condition:** If regularization parameters are poorly tuned, XGBoost may overfit despite theoretical advantages.

### Mechanism 2
- **Claim:** SHAP provides both global and local interpretability by computing feature contributions across all possible feature subsets.
- **Mechanism:** SHAP uses game theory to assign each feature a value representing its marginal contribution to prediction, considering all possible feature combinations. This creates consistent, additive explanations.
- **Core assumption:** Feature contributions can be meaningfully decomposed and averaged across all permutations.
- **Evidence anchors:**
  - [abstract] "SHAP and ICE XAi methods identified Age, BMI, Chronic Diseases, and Family Cancer History as key premium determinants"
  - [section] "SHAP considers the contribution of each determinant by computing all the different permutations of the explanatory variables"
  - [corpus] Weak - no direct corpus evidence found
- **Break condition:** If features are highly correlated, SHAP's assumption of feature independence breaks down, leading to unreliable attribution.

### Mechanism 3
- **Claim:** ICE plots reveal individual instance-level relationships that average-based methods like PDP might obscure.
- **Mechanism:** ICE plots disaggregate the partial dependence plot by showing the prediction function for each individual instance separately, revealing heterogeneous effects across the dataset.
- **Core assumption:** Individual instances exhibit different relationships between features and predictions that averaging would mask.
- **Evidence anchors:**
  - [abstract] "ICE plots revealed more detailed feature interactions than SHAP"
  - [section] "ICE plots originated from Friedman's Partial Dependence Plot (PDP), which highlights an average partial relationship between a set of predictors and the predicted response"
  - [corpus] Weak - no direct corpus evidence found
- **Break condition:** If the dataset is homogeneous with consistent relationships, ICE plots provide little additional insight over PDPs.

## Foundational Learning

- **Concept:** Gradient Boosting vs. Random Forest difference
  - **Why needed here:** Understanding why XGBoost outperforms RF despite higher computational cost requires grasping the fundamental algorithmic differences.
  - **Quick check question:** What is the key difference in how XGBoost and Random Forest construct their trees?

- **Concept:** SHAP vs. LIME explanation methods
  - **Why needed here:** Both XAI methods were used; understanding their theoretical foundations explains why ICE plots provided more detail than SHAP.
  - **Quick check question:** How does SHAP's approach to feature attribution differ from LIME's local approximation?

- **Concept:** Regularization in ensemble methods
  - **Why needed here:** XGBoost's regularization terms are crucial to its performance advantage; understanding this prevents overgeneralization of results.
  - **Quick check question:** What role do L1 and L2 regularization play in preventing overfitting in boosting algorithms?

## Architecture Onboarding

- **Component map:**
  - Data pipeline → Preprocessing → Model training (XGBoost, GBM, RF) → Hyperparameter tuning (GridSearchCV) → Evaluation → XAI interpretation (SHAP, ICE)

- **Critical path:**
  - Data preprocessing → Model training with GridSearchCV → Performance evaluation → XAI interpretation
  - This path determines the final model selection and interpretability analysis

- **Design tradeoffs:**
  - XGBoost vs. RF: Accuracy vs. computational efficiency
  - SHAP vs. ICE: Global vs. local interpretability and computational cost
  - 5-fold CV vs. 20-fold CV: Bias-variance tradeoff in hyperparameter selection

- **Failure signatures:**
  - XGBoost overfitting: High training accuracy but poor test performance
  - SHAP unreliable: Highly correlated features showing inconsistent attributions
  - ICE plot noise: Too few instances causing jagged, unreliable curves

- **First 3 experiments:**
  1. Train XGBoost with default parameters vs. tuned parameters to quantify hyperparameter impact
  2. Compare SHAP summary plots with ICE plots for a single feature to understand granularity differences
  3. Remove highly correlated features and retrain to test SHAP's independence assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the prediction accuracies of ML models vary across different healthcare systems (single-payer vs. multi-payer) given their distinct pricing structures and administrative complexities?
- Basis in paper: [inferred] The paper discusses US healthcare administrative costs and fee-for-service models as key drivers of high costs, suggesting system structure impacts pricing. The models were trained on a US dataset without comparison to other healthcare systems.
- Why unresolved: The study used a single US dataset and did not test model performance across different healthcare system structures or international datasets.
- What evidence would resolve it: Comparative analysis of model performance using datasets from countries with different healthcare system structures (e.g., UK NHS, German multi-payer system) would reveal how system characteristics affect prediction accuracy.

### Open Question 2
- Question: What is the optimal balance between model complexity and interpretability for real-world insurance underwriting applications, considering regulatory compliance and stakeholder needs?
- Basis in paper: [explicit] The paper notes that "Accuracy versus interpretability has remained a major issue at the intersection of ML and the explainability of models" and discusses trade-offs between XGBoost's performance and RF's interpretability.
- Why unresolved: The study demonstrates both high-performing models and explainability methods but does not evaluate which combination best serves actual insurance underwriting requirements or regulatory standards.
- What evidence would resolve it: Empirical studies comparing model adoption rates, regulatory approval processes, and stakeholder satisfaction across different complexity-interpretability trade-offs in real insurance settings would identify optimal approaches.

### Open Question 3
- Question: How do feature interactions identified by ICE plots translate into actionable underwriting rules compared to SHAP-based explanations in practice?
- Basis in paper: [explicit] The paper concludes that "ICE plots showed in more detail the interactions between each variable than the SHAP analysis which seemed to be more high-level" and demonstrates different insights from each method.
- Why unresolved: While the paper shows methodological differences, it does not evaluate how these different explanatory approaches affect actual underwriting decisions or policy design in real-world applications.
- What evidence would resolve it: Field studies comparing underwriting decisions and policy modifications derived from ICE-based versus SHAP-based explanations would reveal which approach provides more actionable insights for insurance practice.

## Limitations

- Small dataset size (986 records) may limit generalizability of findings
- Absence of statistical significance testing makes performance differences uncertain
- Lack of external validation dataset prevents assessment of model generalizability

## Confidence

- **High Confidence:** The comparative performance ranking (XGBoost > GBM > RF in R2 and RMSE) - supported by explicit metrics in the abstract.
- **Medium Confidence:** The feature importance findings (Age, BMI, Chronic Diseases as key determinants) - consistent with domain knowledge but lacking statistical validation.
- **Low Confidence:** The claim that ICE plots reveal "more detailed feature interactions than SHAP" - qualitative assessment without quantitative comparison metrics.

## Next Checks

1. Conduct statistical significance testing (paired t-tests or Wilcoxon signed-rank) on cross-validated model performances to verify XGBoost's superiority.
2. Validate SHAP feature importance stability using bootstrapping to assess sensitivity to data perturbations.
3. Test model performance on an independent medical insurance dataset to evaluate generalizability beyond the original 986-record sample.