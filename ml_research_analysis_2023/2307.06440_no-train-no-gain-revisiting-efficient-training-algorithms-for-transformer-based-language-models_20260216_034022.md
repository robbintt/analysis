---
ver: rpa2
title: 'No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based
  Language Models'
arxiv_id: '2307.06440'
source_url: https://arxiv.org/abs/2307.06440
tags:
- training
- arxiv
- learning
- layer
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits three categories of efficient training algorithms
  for Transformer-based language models: dynamic architectures (layer stacking, layer
  dropping), batch selection (selective backprop, RHO loss), and efficient optimizers
  (Lion, Sophia). The authors evaluate these methods under fixed computation budgets
  (6, 12, 24 hours) for pre-training BERT and T5 models.'
---

# No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models

## Quick Facts
- **arXiv ID**: 2307.06440
- **Source URL**: https://arxiv.org/abs/2307.06440
- **Reference count**: 40
- **Key outcome**: Evaluation of three categories of efficient training algorithms (dynamic architectures, batch selection, efficient optimizers) for Transformer-based language models under fixed computation budgets, finding none consistently outperform baseline training

## Executive Summary
This paper revisits three categories of efficient training algorithms for Transformer-based language models: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). The authors evaluate these methods under fixed computation budgets (6, 12, 24 hours) for pre-training BERT and T5 models. They propose a new evaluation protocol using "reference system time" to standardize timing across different hardware. The key finding is that none of the efficient training algorithms consistently improve downstream task performance compared to a baseline with fully-decayed learning rate within the given budgets.

## Method Summary
The authors evaluate three categories of efficient training algorithms for Transformer-based language models. Dynamic architectures include layer stacking (transferring parameters from smaller to larger models) and layer dropping (randomly skipping layers during training). Batch selection methods include selective backprop (removing low-loss samples) and RHO loss (dynamic batch sizing). Efficient optimizers include Lion and Sophia, which modify the optimization process. All methods are compared against a baseline with fully-decayed learning rate across fixed computation budgets of 6, 12, and 24 hours, measured using a standardized "reference system time" metric that normalizes wall-clock time across different hardware.

## Key Results
- Layer stacking is the only approach that consistently outperforms the baseline's training loss across budgets and models, but this advantage diminishes with longer training
- Batch selection methods (selective backprop, RHO loss) underperform the baseline on validation loss across three datasets
- Methods with lower per-iteration costs (dynamic architecture methods) can slightly improve downstream performance for lower budgets (6, 12 hours), but the improvement disappears with longer training
- Methods with higher per-iteration costs (batch selection methods and some efficient optimizers) are significantly worse than the baseline in some downstream tasks (GLUE, SNI) for all budgets

## Why This Works (Mechanism)

### Mechanism 1: RST as a Standardized Timing Metric
Reference System Time (RST) allows fair comparison of training algorithms across different hardware by normalizing wall-clock time. Convert arbitrary system timing to a reference machine by multiplying iterations run by the time per iteration on the reference system. Core assumption: Time per iteration is consistent enough across similar models to enable meaningful normalization.

### Mechanism 2: Layer Stacking Performance Gains from Parameter Transfer
Layer stacking achieves lower training loss by warm-starting stacked models with parameters transferred from smaller models. Replicate a smaller model's parameters to create a larger model, then continue training with the larger architecture. Core assumption: Attention distributions of bottom layers in smaller models are similar to those of top layers in larger models, indicating similar functionality.

### Mechanism 3: Layer Dropping's Regularization Effect at High Epoch Counts
Layer dropping prevents overfitting when training for many epochs by acting as a form of regularization. Randomly skip layers during training with increasing probability over time, reducing effective model capacity. Core assumption: Layer dropping's effect is similar to dropout, which is known to prevent overfitting in language model pre-training with repeated data.

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: All evaluated methods modify training procedures for Transformer-based models
  - Quick check question: What is the difference between self-attention and cross-attention in Transformers?

- **Concept**: Stochastic gradient descent and optimization algorithms
  - Why needed here: Efficient optimizers (Lion, Sophia) modify the optimization process
  - Quick check question: How does AdamW differ from standard SGD in terms of momentum and adaptive learning rates?

- **Concept**: Mixed precision training and numerical stability
  - Why needed here: The paper uses BF16 and FP16 for different optimizers, with stability considerations
  - Quick check question: Why might BF16 be more stable than FP16 for certain optimizers?

## Architecture Onboarding

- **Component map**: Reference timing measurement -> Efficient training algorithms (dynamic architectures, batch selection, efficient optimizers) -> Evaluation protocols -> Downstream task benchmarks
- **Critical path**: 1) Measure reference system timing, 2) Implement efficient training algorithms, 3) Run experiments with fixed RST budgets, 4) Evaluate training and downstream performance
- **Design tradeoffs**: RST provides hardware-agnostic timing but assumes consistent iteration times; layer stacking offers early performance gains but requires careful timing of stacking operations; batch selection methods have overhead costs that may outweigh benefits
- **Failure signatures**: NaN losses indicate numerical instability (especially with certain optimizers in FP16); worse downstream performance despite better training loss suggests overfitting or poor generalization
- **First 3 experiments**:
  1. Measure time per iteration on reference system for baseline model configuration
  2. Implement layer stacking with different stacking time intervals and compare training loss curves
  3. Test selective backprop with different Î² values on a small dataset to observe validation loss impact

## Open Questions the Paper Calls Out

1. Would efficient training algorithms provide better performance if evaluated over longer training budgets (e.g., 48+ hours)?
2. How would the results change if efficient training algorithms were tested on different model architectures beyond BERT and T5?
3. Do efficient training algorithms show different performance characteristics when training with multiple epochs versus single-epoch regimes?

## Limitations
- Limited model and task diversity - evaluation focuses on BERT and T5 models pre-trained on C4 data
- Short training budgets (6-24 hours) may not be representative of real-world training scenarios
- Potential hardware dependency of RST metric assumptions

## Confidence

**High confidence**: The observation that layer stacking achieves lower training loss consistently across budgets and models is well-supported by the experimental results.

**Medium confidence**: The conclusion that none of the efficient training algorithms consistently improve downstream performance compared to the baseline is reasonable given the evidence, but could vary with different hyperparameter tuning or longer training durations.

**Low confidence**: The explanation for layer stacking's success (attention distribution similarities between bottom and top layers) lacks strong empirical support and may not capture the full mechanism.

## Next Checks
1. Conduct experiments to validate the RST assumption by measuring time per iteration variation across different hardware configurations for the same model and training algorithm
2. Systematically vary key hyperparameters for each method to determine if performance gaps to baseline can be reduced through better tuning
3. Run experiments with longer fixed budgets (e.g., 48-72 hours) to test whether the relative performance of methods changes over longer training periods