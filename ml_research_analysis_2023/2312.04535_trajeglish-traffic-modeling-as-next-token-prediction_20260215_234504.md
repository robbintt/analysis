---
ver: rpa2
title: 'Trajeglish: Traffic Modeling as Next-Token Prediction'
arxiv_id: '2312.04535'
source_url: https://arxiv.org/abs/2312.04535
tags:
- agents
- agent
- trajeglish
- trajectory
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Trajeglish, a discrete sequence modeling approach
  for traffic simulation. The authors tokenize trajectories into discrete actions
  and use a transformer-based model to predict multi-agent interactions autoregressively.
---

# Trajeglish: Traffic Modeling as Next-Token Prediction

## Quick Facts
- arXiv ID: 2312.04535
- Source URL: https://arxiv.org/abs/2312.04535
- Reference count: 40
- Primary result: Trajeglish achieves state-of-the-art results on the Waymo Sim Agents Benchmark, improving realism by 3.3% and interaction by 9.9% over prior work.

## Executive Summary
Trajeglish presents a novel approach to traffic simulation by treating trajectories as discrete sequences of actions, similar to language modeling. The method tokenizes continuous trajectories into a small vocabulary of 384 discrete actions using a k-disks algorithm, then uses a transformer-based autoregressive model to predict multi-agent interactions. This approach achieves state-of-the-art performance on the Waymo Open Motion Dataset Sim Agents Benchmark, demonstrating that discrete sequence modeling can effectively capture complex traffic dynamics while enabling scalable training and inference.

## Method Summary
Trajeglish discretizes continuous trajectory data into a fixed vocabulary of 384 action tokens using a k-disks algorithm. A transformer encoder-decoder architecture then predicts these tokens autoregressively, conditioning on initial agent states, map information, and actions from other agents. The model uses a causal mask to ensure proper sequential prediction, and incorporates intra-timestep interaction modeling by conditioning on actions chosen by other agents within the same timestep. After prediction, tokens are rendered back to continuous trajectories for evaluation.

## Key Results
- Achieves 3.3% improvement on realism metric and 9.9% on interaction metric compared to prior work on WOMD Sim Agents Benchmark
- Effectively captures intra-timestep agent interactions while maintaining computational efficiency
- Successfully transfers to nuScenes dataset, demonstrating scalability across different traffic environments
- Shows strong performance across kinematic, interactive, and map-based metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenizing trajectories into discrete actions enables autoregressive modeling of multi-agent interactions.
- Mechanism: The k-disks algorithm discretizes continuous trajectory data into a fixed vocabulary of action tokens. Each token represents a change in position and heading in the local coordinate frame of the agent's current state. The autoregressive transformer then predicts the next action token for each agent sequentially, conditioning on previous actions from all agents and current timestep actions from other agents.
- Core assumption: The continuous distribution of multi-agent trajectories can be well-approximated by a discrete distribution over a fixed vocabulary of actions, and that intra-timestep interactions between agents are weak but important for realistic simulation.
- Evidence anchors:
  - [abstract] "Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary."
  - [section] "We iteratively find the token with minimum corner distance to the next state" and "Our tokenizer f and renderer r are defined by...dl,w(si, local(s0, s))"
- Break condition: If the vocabulary size is too small to capture the diversity of agent behaviors, or if intra-timestep interactions are strong (which the paper suggests they are weak in general).

### Mechanism 2
- Claim: The encoder-decoder transformer architecture can effectively model multi-agent interactions by processing agents in a fixed order and conditioning on map information.
- Mechanism: The encoder processes initial agent states and map information into embeddings. The decoder uses a causal mask to predict action tokens sequentially for each agent in a fixed order. The model conditions on map objects through a VectorNet encoder and uses positional embeddings to encode agent order and identity.
- Core assumption: A fixed agent order is sufficient to capture the sequential nature of multi-agent interactions, and that the transformer architecture can effectively model the dependencies between agents.
- Evidence anchors:
  - [section] "Our model follows an encoder-decoder structure very similar to those used for LLMs" and "we apply a layer of latent query attention that outputs a final encoding of the scene initialization"
  - [abstract] "Our model outputs a distribution over actions for agents one at a time which we show is ideal for interactive applications"
- Break condition: If the fixed agent order cannot capture important interaction patterns, or if the transformer cannot effectively model the long-range dependencies in traffic scenarios.

### Mechanism 3
- Claim: Intra-timestep interaction modeling provides a window into understanding when coordination between agents is important for traffic modeling.
- Mechanism: The model conditions on actions chosen by other agents within the same timestep when predicting an agent's next action. This allows the model to capture cases where agents coordinate their actions within a single timestep, even though such coordination is weak in general.
- Core assumption: While intra-timestep interactions are generally weak, they are important in rare cases and provide insight into the nature of multi-agent interactions in traffic scenarios.
- Evidence anchors:
  - [section] "While intra-timestep interaction between agents is weak in general, explicitly modeling this interaction provides a window into understanding cases when it is important to consider for the purposes of traffic modeling"
  - [abstract] "We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task"
- Break condition: If the computational cost of modeling intra-timestep interactions outweighs the benefits, or if the model cannot effectively capture these interactions.

## Foundational Learning

- Concept: Discrete sequence modeling and tokenization
  - Why needed here: The continuous nature of trajectory data makes it difficult to model with standard autoregressive approaches. Tokenization converts continuous trajectories into discrete sequences that can be modeled using established techniques from language modeling.
  - Quick check question: What is the expected discretization error when using a vocabulary size of 384 for vehicle trajectories?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The transformer's ability to model long-range dependencies and its parallelizability make it well-suited for modeling the complex interactions between multiple agents over time.
  - Quick check question: How does the causal mask in the decoder ensure that the model predicts actions in the correct order?

- Concept: Multi-agent interaction modeling
  - Why needed here: Realistic traffic simulation requires modeling how multiple agents influence each other's behavior. The factorization in Equation 2 captures this requirement for interactive applications.
  - Quick check question: What is the difference between the "marginal" baseline and the full Trajeglish model in terms of how they model multi-agent interactions?

## Architecture Onboarding

- Component map:
  - Tokenizer (k-disks algorithm) -> Encoder (VectorNet + agent embeddings) -> Decoder (causal transformer) -> Renderer (continuous trajectory reconstruction) -> Density estimator

- Critical path:
  1. Tokenize training trajectories using k-disks algorithm
  2. Encode initial scene with agent and map information
  3. Predict action tokens sequentially using decoder
  4. Render tokens back to continuous trajectories
  5. Evaluate on WOMD Sim Agents Benchmark or other metrics

- Design tradeoffs:
  - Vocabulary size vs. discretization error - larger vocabularies capture more detail but increase computational cost
  - Agent order vs. interaction modeling - fixed order simplifies implementation but may miss some interaction patterns
  - Intra-timestep modeling vs. computational efficiency - modeling intra-timestep interactions improves realism but increases computational cost

- Failure signatures:
  - High collision rates in generated scenarios - indicates poor modeling of agent interactions
  - Poor performance on kinematic metrics - suggests issues with discretization or rendering
  - Overfitting to training data - indicates need for more regularization or data augmentation

- First 3 experiments:
  1. Compare tokenization methods (k-disks vs. k-means vs. grid-based) on discretization error
  2. Evaluate the impact of intra-timestep interaction modeling on collision rates
  3. Test model scalability with respect to parameter count and dataset size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Trajeglish perform with longer rollout lengths beyond 8 seconds?
- Basis in paper: [explicit] The authors mention they only evaluated up to 8 seconds for the WOMD benchmark and that they "reserve a more rigorous study of how all hyperparameters of the training strategy affect sampling performance for future work."
- Why unresolved: The current evaluation only covers short rollouts, leaving the model's performance at longer horizons unknown.
- What evidence would resolve it: Testing Trajeglish on the WOMD benchmark with rollout lengths of 10-20 seconds and comparing metrics like realism, interaction, and collision rates to shorter rollouts.

### Open Question 2
- Question: What is the optimal vocabulary size for Trajeglish that balances discretization error and computational efficiency?
- Basis in paper: [explicit] The authors tested vocabulary sizes of 128, 256, 384, and 512, finding that 384 achieved good performance, but note that "a more rigorous study of how all hyperparameters of the training strategy affect sampling performance is reserved for future work."
- Why unresolved: While 384 seems effective, the relationship between vocabulary size, model performance, and computational cost across different datasets and scenarios is not fully explored.
- What evidence would resolve it: Systematic experiments varying vocabulary size across different dataset sizes and evaluating trade-offs between discretization error, model performance, and training/inference speed.

### Open Question 3
- Question: How does Trajeglish transfer to completely different driving environments, such as those with different traffic rules or vehicle types?
- Basis in paper: [explicit] The authors showed successful transfer to nuScenes but noted that "there are multiple arbitrary choices in the definition of map objects that may inhibit transfer of traffic models to different datasets."
- Why unresolved: The current transfer study was limited to nuScenes, which shares similarities with WOMD. Performance in radically different environments remains untested.
- What evidence would resolve it: Fine-tuning Trajeglish on datasets from countries with left-hand traffic, different vehicle types (e.g., motorbikes, rickshaws), or unique road structures, then evaluating rollout realism and interaction metrics.

## Limitations

- The k-disks tokenization algorithm lacks direct validation against established trajectory discretization methods, making it difficult to assess whether the chosen vocabulary size of 384 is optimal.
- The assumption that intra-timestep interactions are "weak in general" is not empirically verified across diverse traffic scenarios.
- The fixed agent order used in the decoder may not adequately capture complex multi-agent dynamics where interaction order is crucial.

## Confidence

**High Confidence (8/10)**: The core claim that Trajeglish achieves state-of-the-art results on the WOMD Sim Agents Benchmark is well-supported by the reported metrics (3.3% improvement on realism, 9.9% on interaction).

**Medium Confidence (6/10)**: The claim that intra-timestep interaction modeling provides valuable insights into traffic dynamics is plausible but not definitively proven.

**Low Confidence (4/10)**: The assertion that the k-disks algorithm is optimal for trajectory discretization lacks comparative analysis with alternative methods.

## Next Checks

1. **Discretization Error Analysis**: Conduct a systematic comparison of the k-disks tokenization against k-means clustering and grid-based discretization methods on the WOMD dataset. Measure corner distance error, collision rates in generated scenarios, and qualitative trajectory quality for each method to determine if 384 tokens is optimal.

2. **Intra-timestep Interaction Importance**: Design experiments that systematically vary the strength of intra-timestep conditioning in the model. Generate traffic scenarios with known coordination events (e.g., four-way stops, pedestrian crossings) and measure how well the model captures these interactions under different conditioning strengths.

3. **Agent Order Sensitivity**: Implement a version of Trajeglish that samples agent order during training rather than using a fixed order. Compare performance on interaction metrics and collision rates between fixed and randomized agent ordering to assess whether the current ordering assumption limits the model's ability to capture complex multi-agent dynamics.