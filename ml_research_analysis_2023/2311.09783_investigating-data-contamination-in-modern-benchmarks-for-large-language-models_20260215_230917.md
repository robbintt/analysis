---
ver: rpa2
title: Investigating Data Contamination in Modern Benchmarks for Large Language Models
arxiv_id: '2311.09783'
source_url: https://arxiv.org/abs/2311.09783
tags:
- data
- benchmark
- llms
- contamination
- truthfulqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates data contamination in modern LLM benchmarks
  by proposing two novel methods. First, they build an information retrieval system
  to detect overlaps between evaluation benchmarks and pretraining corpora.
---

# Investigating Data Contamination in Modern Benchmarks for Large Language Models

## Quick Facts
- arXiv ID: 2311.09783
- Source URL: https://arxiv.org/abs/2311.09783
- Reference count: 9
- Key outcome: Proposed novel methods (information retrieval and TS-Guessing protocol) reveal potential data contamination in modern LLM benchmarks, with commercial models like ChatGPT achieving 57% EM rate in guessing masked MMLU options

## Executive Summary
This paper investigates data contamination in modern LLM benchmarks through two complementary approaches. The authors build an information retrieval system using BM25 indexing to detect overlaps between evaluation benchmarks and pretraining corpora, while also introducing a novel "Testset Slot Guessing" (TS-Guessing) protocol to assess potential data leakage. Their findings reveal that commercial LLMs can surprisingly guess missing options in benchmarks like MMLU and TruthfulQA with high accuracy, raising significant concerns about the validity of current evaluation methodologies.

## Method Summary
The authors employ two novel methods to detect data contamination: (1) a retrieval-based system using Pyserini with BM25 indexing to search for overlaps between benchmark data and pretraining corpora (The Pile and C4), and (2) a TS-Guessing protocol that masks portions of test examples and prompts models to fill in the gaps. The TS-Guessing protocol operates in two modes - Question-based guessing (masking keywords in sentences) and Question-Multichoice guessing (masking incorrect answers in multiple-choice questions). They evaluate results using metrics like BM25 score, SacreBLEU, Rouge-L, BLEURT, GPTscore for retrieval-based detection, and Exact Match rate and Rouge-L F1 score for TS-Guessing.

## Key Results
- ChatGPT achieved a 57% Exact Match rate in guessing masked wrong options in MMLU
- Significant overlap detected between TruthfulQA benchmark and pretraining corpora
- Commercial LLMs showed remarkable performance on TruthfulQA when provided with metadata hints
- Open-source models (LLaMa 2-13B, Mistral-7B) performed notably worse than commercial models on TS-Guessing tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TS-Guessing protocol detects contamination by measuring whether LLMs can predict masked wrong answers in multiple-choice questions
- Mechanism: If a model has been exposed to benchmark data during training, it can guess the exact masked wrong option more accurately than random chance
- Core assumption: Masked wrong options are not trivially inferable from question content or common knowledge
- Evidence anchors:
  - ChatGPT demonstrated a 57% Exact Match (EM) rate in guessing missing options
  - Detailed filtering procedures eliminate instances where answer options correlate with question content
- Break condition: If guessing accuracy approaches random chance (25% for 4 options), contamination is unlikely

### Mechanism 2
- Claim: Retrieval-based detection identifies contamination through n-gram matching between pretraining corpora and benchmark data
- Mechanism: High overlap (via 13-gram tokenization) between pretraining documents and benchmark questions/answers suggests potential contamination
- Core assumption: N-gram overlap indicates the model likely encountered similar examples during training
- Evidence anchors:
  - 13-gram tokenization approach used to chunk documents and calculate similarity scores
  - Significant overlap observed between TruthfulQA and pretraining corpora
  - Varying contamination scores across different benchmarks
- Break condition: If n-gram matching scores are low and human evaluation confirms no meaningful overlap

### Mechanism 3
- Claim: Question-based TS-Guessing detects contamination by measuring if models can predict masked keywords in benchmark questions
- Mechanism: Accurate guessing of masked keywords (e.g., "fortune" in "Where did [MASK] cookies originate?") suggests exposure to similar question-answer pairs
- Core assumption: Keywords are specific enough that random guessing would yield low accuracy
- Evidence anchors:
  - Commercial LLMs achieved remarkable performance when provided with metadata
  - Models may disproportionately predict masked words if exposed to similar test data
- Break condition: If keyword guessing accuracy is no better than random selection from plausible alternatives

## Foundational Learning

- Concept: Information Retrieval and BM25 scoring
  - Why needed here: To efficiently search large pretraining corpora for potential benchmark overlaps
  - Quick check question: What does BM25 score measure in document retrieval?

- Concept: N-gram tokenization and matching
  - Why needed here: To identify text overlap between pretraining data and benchmarks at a granular level
  - Quick check question: Why use 13-gram tokenization instead of character-level matching?

- Concept: Exact Match (EM) rate calculation
  - Why needed here: To quantify how often models correctly guess masked options in TS-Guessing
  - Quick check question: How does EM rate differ from token-level accuracy in this context?

## Architecture Onboarding

- Component map:
  Data Ingestion -> Retrieval System -> TS-Guessing Engine -> Evaluation Metrics -> Filtering Pipeline

- Critical path:
  1. Load benchmark data and pretraining corpora
  2. Apply filtering to remove trivial cases
  3. Run TS-Guessing protocol on filtered data
  4. Calculate contamination scores using multiple metrics
  5. Compare results against random baseline

- Design tradeoffs:
  - Retrieval speed vs. recall: Higher k in BM25 retrieval increases disk usage and time
  - Filtering strictness vs. test coverage: Aggressive filtering may miss edge cases
  - Model choice vs. cost: GPT-4 vs. open-source models for TS-Guessing

- Failure signatures:
  - High EM rates across all benchmarks suggest universal contamination
  - Very low scores on all metrics indicate clean benchmarks
  - Disproportionate performance on metadata-augmented prompts suggests specific contamination

- First 3 experiments:
  1. Run TS-Guessing on TruthfulQA with and without metadata hints
  2. Compare BM25 retrieval scores between TruthfulQA and MMLU
  3. Test open-source models (LLaMa, Mistral) on MMLU with filtering applied

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different training methodologies (SFT, RLHF, MoE) affect susceptibility to data contamination?
- Basis in paper: The paper mentions these training techniques and notes that applying the same evaluation methods to different techniques could yield varying results regarding contamination.
- Why unresolved: The study focuses on contamination detection but doesn't explore how different training approaches might influence contamination vulnerability.
- What evidence would resolve it: Comparative studies testing the same contamination detection methods across models trained with different techniques, measuring correlation between training methodology and contamination detection scores.

### Open Question 2
- Question: What is the optimal filtering threshold for identifying potentially contaminated data points in multi-choice benchmarks?
- Basis in paper: The paper uses a 0.65 Rouge-L F1 score threshold for filtering options, noting this was chosen based on initial experiments.
- Why unresolved: The chosen threshold appears somewhat arbitrary, and the paper doesn't systematically explore how different thresholds affect contamination detection accuracy.
- What evidence would resolve it: Systematic analysis of contamination detection performance across various threshold values, comparing false positive/negative rates at each level.

### Open Question 3
- Question: How does the age and source novelty of training data affect contamination susceptibility in different benchmark domains?
- Basis in paper: The paper notes that TruthfulQA shows contamination despite its 2022 release date, and discusses the importance of source document novelty.
- Why unresolved: The study identifies contamination but doesn't explore how temporal factors or source diversity in training data might affect contamination patterns across different benchmark types.
- What evidence would resolve it: Longitudinal studies tracking contamination detection rates across models trained on data from different time periods and source distributions, stratified by benchmark domain.

## Limitations
- The TS-Guessing protocol may conflate contamination with genuine model capabilities or reasoning skills
- Neither method can definitively prove contamination - only suggest its likelihood through indirect evidence
- The study focuses on commercial models with opaque training methodologies, making it difficult to attribute high guessing accuracy to data contamination

## Confidence
- Medium confidence in core claims due to several key limitations
- Confidence is further tempered by the fact that the study focuses on commercial models which have opaque training methodologies

## Next Checks
1. **Cross-Model Consistency Test**: Apply the TS-Guessing protocol to open-source models (LLaMa, Mistral) with known, transparent training data to determine if high guessing accuracy correlates with documented exposure to benchmark-like content.

2. **Human Baseline Evaluation**: Have human subjects attempt the same TS-Guessing tasks to establish whether commercial LLMs' performance significantly exceeds human ability to guess masked options.

3. **Temporal Analysis**: Test models trained at different time periods (pre- and post-benchmark publication) to determine if guessing accuracy correlates with the model's training cutoff date relative to when benchmarks became publicly available.