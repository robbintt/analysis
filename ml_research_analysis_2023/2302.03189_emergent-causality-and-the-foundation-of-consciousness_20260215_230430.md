---
ver: rpa2
title: Emergent Causality and the Foundation of Consciousness
arxiv_id: '2302.03189'
source_url: https://arxiv.org/abs/2302.03189
tags:
- which
- intervention
- 'true'
- interventions
- nition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that accurate inductive inference in an interactive
  setting does not require presupposing an explicit representation of intervention.
  The author shows that the do operator can be represented by variables and that variables
  are abstractions which can emerge through induction.
---

# Emergent Causality and the Foundation of Consciousness

## Quick Facts
- arXiv ID: 2302.03189
- Source URL: https://arxiv.org/abs/2302.03189
- Reference count: 27
- Key outcome: This paper argues that accurate inductive inference in an interactive setting does not require presupposing an explicit representation of intervention. The author shows that the do operator can be represented by variables and that variables are abstractions which can emerge through induction. The formalism presented does not presuppose abstractions, so representations of causal interventions will emerge through induction. These emergent abstractions function as representations of one's self and of any other object, insofar as their interventions impact the satisfaction of goals. The author argues this explains how one might reason about one's own identity and intent, those of others, one's own as perceived by others, etc. This describes what it is to be aware and is a mechanistic explanation of aspects of consciousness.

## Executive Summary
This paper presents a novel formalism for causal reasoning that eliminates the need for explicit do-operators by showing how representations of interventions can emerge through inductive inference. The key insight is that the weakest model maximizing generalization probability will naturally distinguish between observation and intervention in interactive settings, causing causal abstractions to emerge as variables. These emergent abstractions serve as representations of self and other entities based on their interventions' impact on goal satisfaction, providing a mechanistic explanation for aspects of consciousness including self-awareness and theory of mind.

## Method Summary
The formalism operates on an implementable language structure ⟨H, V, L⟩ where H contains objective totalities, V is a finite vocabulary, and L is the set of statements. Tasks are defined as T = ⟨S, D, M⟩ where S represents situations, D represents correct decisions, and M represents valid models. The induction process computes the weakest model by maximizing weakness |Zl| as a proxy for generalization probability. The key mechanism is that in interactive settings where distinguishing intervention from observation is necessary for accurate inference, the weakest model will represent this distinction, causing causal representations to emerge through induction without presupposing explicit do-operators.

## Key Results
- Do operators can be represented as variables in an implementable language
- Representations of causal interventions emerge through induction in interactive settings
- Emergent abstractions function as representations of self and other entities based on their interventions' impact on goal satisfaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The do operator can be represented by a variable in an implementable language
- Mechanism: In the formalism, interventions are modeled as statements in a language where variables are abstractions that emerge through induction. The intervention "do[C = c]" can be represented as a variable A such that p(C = true|A = true) = 1, where A represents the causal intervention.
- Core assumption: The formalism does not presuppose explicit representations of intervention, allowing variables representing interventions to emerge through induction.
- Evidence anchors:
  - [abstract] "The author shows that the do operator can be represented by variables and that variables are abstractions which can emerge through induction."
  - [section] "The intervention can instead be represented by a variable A such that p(C = true|A = true) = 1 and p(C = false|A = false) = 1"
  - [corpus] No direct corpus evidence, but this is a core theoretical claim in the paper
- Break condition: If the formalism requires explicit representation of interventions from the start, or if variables cannot adequately capture causal interventions

### Mechanism 2
- Claim: Representations of causal interventions will emerge through induction in the absence of explicit do operators
- Mechanism: The formalism uses weakness as a proxy for maximizing generalization probability. In tasks where distinguishing intervention from observation is necessary for accurate inference, the weakest model will represent this distinction, causing representations of interventions to emerge.
- Core assumption: The formalism's use of weakness maximizes the probability that induction results in generalization (premises 1 and 2).
- Evidence anchors:
  - [abstract] "The formalism presented does not presuppose abstractions, so representations of causal interventions will emerge through induction."
  - [section] "From (prem. 1) and (prem. 2) we have formal proof that choosing the weakest model maximises the probability of generalisation."
  - [corpus] No direct corpus evidence, but this follows from the paper's premises about weakness and generalization
- Break condition: If weakness does not correlate with generalization in interactive settings, or if the formalism cannot represent the distinction between observation and intervention

### Mechanism 3
- Claim: Emergent abstractions function as representations of self and other objects based on their interventions' impact on goal satisfaction
- Mechanism: When an agent intervenes (a) and forces an outcome (c), the distinction a-c represents the agent's identity. Through induction, the agent can model others' interventions and infer their intent by constructing tasks and computing weakest models.
- Core assumption: The agent must distinguish its own interventions from observations to make accurate inferences (premise 3).
- Evidence anchors:
  - [abstract] "These emergent abstractions function as representations of one's self and of any other object, insofar as their interventions impact the satisfaction of goals."
  - [section] "If k ⊆ a-c can represent our identity as party undertaking interventions, it follows that j ⊆ v-c may represent Harvey's."
  - [corpus] No direct corpus evidence, but this extends the paper's theoretical framework to theory of mind
- Break condition: If the agent cannot distinguish its own interventions from observations, or if identity cannot be represented as a subset of interventions

## Foundational Learning
- Concept: Do calculus and causal inference
  - Why needed here: The paper builds on Pearl's do-calculus to show how interventions can be represented without explicit do operators
  - Quick check question: What is the key difference between p(R|C) and p(R|do(C)) in the raincoat example?
- Concept: Implementable language and statements
  - Why needed here: The formalism uses an implementable language where statements represent sensorimotor activity and are inferred rather than given
  - Quick check question: How does a statement differ from a variable in this context?
- Concept: Weakness and generalization
  - Why needed here: The paper uses weakness as a proxy for maximizing generalization probability in induction
  - Quick check question: Why does choosing the weakest model maximize the probability of generalization?

## Architecture Onboarding
- Component map:
  - Implementable language (H, V, L) -> Tasks (S, D, M) -> Induction mechanism -> Identity representation -> Intent modeling
- Critical path:
  1. Define implementable language with vocabulary V
  2. Create tasks (S, D, M) representing decision problems
  3. Run induction to compute weakest models
  4. Distinguish interventions from observations to form identity
  5. Model others' intent by observing their interventions
- Design tradeoffs:
  - Explicit vs. emergent representation of interventions
  - Vocabulary size vs. computational complexity
  - Memory requirements for modeling others' models of self
- Failure signatures:
  - Inability to distinguish intervention from observation
  - Poor generalization across tasks
  - Failure to construct valid models of others' intent
- First 3 experiments:
  1. Test whether introducing variables representing interventions improves task performance compared to explicit do operators
  2. Measure how well the system distinguishes between passive observation and intervention in controlled scenarios
  3. Evaluate the system's ability to infer others' intent based on observed interventions in multi-agent tasks

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the emergence of causal representations in interactive settings affect the design of artificial general intelligence systems?
- Basis in paper: [explicit] The paper discusses the emergence of causal representations through induction in interactive settings.
- Why unresolved: The paper presents a theoretical framework but does not provide concrete implementation details or empirical evidence of how this emergence occurs in practice.
- What evidence would resolve it: Experimental results showing the development of causal representations in AI systems through interaction and induction, or a detailed computational model demonstrating this process.

### Open Question 2
- Question: To what extent can the proposed formalism explain human-like awareness and consciousness in artificial systems?
- Basis in paper: [explicit] The paper argues that the formalism explains aspects of consciousness, including self-awareness and theory of mind.
- Why unresolved: The connection between the formalism and human consciousness is speculative and relies on philosophical arguments rather than empirical evidence.
- What evidence would resolve it: Empirical studies comparing the behavior and decision-making processes of systems implementing this formalism to those of humans in similar interactive scenarios.

### Open Question 3
- Question: How does the proposed approach to causal reasoning compare to existing methods in terms of performance and scalability?
- Basis in paper: [inferred] The paper claims the formalism is pareto optimal but does not provide comparative analysis with other approaches.
- Why unresolved: The paper does not present empirical benchmarks or theoretical comparisons with other causal reasoning methods.
- What evidence would resolve it: Comprehensive benchmarking studies comparing the performance, scalability, and accuracy of this approach to established causal reasoning methods in various interactive scenarios.

## Limitations
- The theoretical framework lacks empirical validation through concrete experiments
- Critical assumptions about the correlation between weakness and generalization in interactive settings remain unproven
- The extension to human-like consciousness is speculative without empirical evidence

## Confidence
- Mechanism 1 (do operator representation): High - The theoretical construction is mathematically rigorous
- Mechanism 2 (emergence through induction): Medium - Relies on the critical assumption that weakness correlates with generalization in interactive settings
- Mechanism 3 (identity and intent modeling): Low-Medium - Extends theoretical framework to consciousness aspects without empirical validation

## Next Checks
1. Create a controlled environment where agents must distinguish between observation and intervention to achieve goals, measuring whether intervention representations emerge through the induction process
2. Test the correlation between model weakness and generalization across a benchmark of tasks with varying complexity and intervention requirements
3. Implement multi-agent scenarios where agents must infer each other's intent through observed interventions, validating the theory of mind extension of the framework