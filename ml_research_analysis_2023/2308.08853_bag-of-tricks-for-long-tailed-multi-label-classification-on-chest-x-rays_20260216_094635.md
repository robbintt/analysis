---
ver: rpa2
title: Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays
arxiv_id: '2308.08853'
source_url: https://arxiv.org/abs/2308.08853
tags:
- learning
- image
- data
- chest
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a solution for long-tailed multi-label classification
  on chest X-rays in the ICCV CVAMD 2023 CXR-LT Competition. The authors address the
  challenges of class imbalance and label co-occurrence in medical image diagnosis.
---

# Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays

## Quick Facts
- arXiv ID: 2308.08853
- Source URL: https://arxiv.org/abs/2308.08853
- Reference count: 23
- Top-5 ranking with mAP of 0.349 on competition test set

## Executive Summary
This paper addresses the challenges of long-tailed multi-label classification on chest X-rays for the ICCV CVAMD 2023 CXR-LT Competition. The authors develop a comprehensive framework that integrates feature extraction, classifier design, loss reweighting, data augmentation, and test-time augmentation to handle class imbalance and label co-occurrence in medical image diagnosis. Their approach combines visual features from chest X-rays with semantic information from pretrained medical text encoders using transformer decoder layers, achieving competitive performance in the competition.

## Method Summary
The framework uses a two-encoder architecture where visual features are extracted from chest X-rays (using ResNet-50 or DenseNet-121) and semantic features are derived from pretrained medical text encoders (PubMedBERT or Clinical-T5). These features are combined through transformer decoder layers with label embeddings as queries, allowing the model to leverage label correlations. The approach employs separate classifiers for each class, loss function reweighting for hard classes identified on the development set, MixUp augmentation during training, and test-time augmentation with geometric mean merging. External datasets (ChestXRay14 and CheXpert) are incorporated by treating missing labels as absent.

## Key Results
- Achieved mAP of 0.349 on the competition test set, ranking in the top five
- Separate classifiers outperform shared classifier design for multi-label chest X-ray classification
- Loss reweighting (factors of 2 or 3) for nine hard classes improves tail class performance
- Incorporating external datasets (ChestXRay14 and CheXpert) provides performance benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label co-occurrence is effectively handled by combining visual and textual information through transformer decoder layers.
- Mechanism: The framework uses a two-encoder architecture where visual features are extracted from chest X-rays and semantic features are derived from pretrained medical text encoders. These are combined via transformer decoder layers with label embeddings as queries, image features as keys/values, allowing the model to leverage label correlations for better predictions.
- Core assumption: Medical text encoders pretrained on domain-specific corpora (like PubMedBERT or Clinical-T5) capture meaningful semantic relationships between diseases that can improve classification.
- Evidence anchors:
  - [abstract] "We also incorporate the pretrained text encoders to extract the class embedding that has incorporated the label correlation."
  - [section] "We also incorporate the pretrained text encoders to extract the class embedding that has incorporated the label correlation."
- Break condition: If the text encoders fail to capture meaningful label correlations or if the label relationships are too complex for the transformer decoder architecture to model effectively.

### Mechanism 2
- Claim: Class imbalance is addressed through a combination of loss function reweighting and separate classifier design.
- Mechanism: The loss function is reweighted to give more importance to classes that performed poorly on the development set, specifically upweighting nine tail classes by factors of 2 or 3. Additionally, separate classifiers are used for each class rather than a shared classifier, preserving prediction independence.
- Core assumption: The development set performance can reliably identify which classes need reweighting, and that separate classifiers will reduce interference between classes during training.
- Evidence anchors:
  - [abstract] "we reweight the loss of those classes that performed poorly on the development set."
  - [section] "we reweight the loss of those classes that performed poorly on the development set."
- Break condition: If the development set is not representative of the test distribution, or if the reweighting factors are not properly tuned.

### Mechanism 3
- Claim: External data supplementation improves model generalization across the long-tailed distribution.
- Mechanism: The framework incorporates additional CXR datasets (ChestXRay14 and CheXpert) by training on their official training sets and treating missing labels as absent (0), effectively increasing training data for common diseases while maintaining the long-tailed nature.
- Core assumption: Treating missing labels as absent is a reasonable approximation that provides useful additional training signal without introducing harmful noise.
- Evidence anchors:
  - [abstract] "We also train the model together with other CXR datasets like ChestXRay14 and CheXpert."
  - [section] "We choose the official training set of both datasets for model training and the label space of both datasets is 14."
- Break condition: If the external datasets have significantly different distributions or if the missing label assumption introduces systematic bias.

## Foundational Learning

- Concept: Multi-label classification with class imbalance
  - Why needed here: Chest X-rays naturally exhibit both characteristics - patients can have multiple conditions simultaneously, and disease frequencies follow a long-tailed distribution
  - Quick check question: What is the difference between multi-label and multi-class classification in the context of medical diagnosis?

- Concept: Transformer architectures for multimodal fusion
  - Why needed here: The framework needs to effectively combine visual features from X-rays with semantic information from medical texts, requiring sophisticated attention mechanisms
  - Quick check question: How do transformer decoder layers differ from encoder layers in their typical use case?

- Concept: Data augmentation and test-time augmentation
  - Why needed here: Small datasets and class imbalance require techniques to improve generalization and stability of predictions
  - Quick check question: What is the key difference between training-time augmentation (like MixUp) and test-time augmentation?

## Architecture Onboarding

- Component map:
  - Image → Visual features (before pooling) → Transformer decoder layers → MLP → Class predictions
  - Text → Semantic features for all labels → Transformer decoder layers
  - Visual + Semantic → Transformer decoder fusion → MLP → Class predictions

- Critical path:
  1. Image → Visual features (before pooling)
  2. Text → Semantic features for all labels
  3. Visual + Semantic → Transformer decoder fusion
  4. Fusion → MLP → Class predictions
  5. Predictions → Weighted loss computation

- Design tradeoffs:
  - Text encoder choice: PubMedBERT (knowledge graph fine-tuned) vs Clinical-T5 (MIMIC-based)
  - Feature extraction: ResNet-50 (more efficient) vs DenseNet-121 (potentially richer features)
  - Reweighting strategy: Global vs class-specific factors
  - Data supplementation: How to handle missing labels from external datasets

- Failure signatures:
  - Performance degradation on tail classes despite reweighting
  - Inconsistent predictions across similar test images
  - Poor transfer from external datasets
  - Overfitting to development set distribution

- First 3 experiments:
  1. Baseline comparison: General framework vs. framework with separate classifiers
  2. Ablation study: Impact of reweighting factor (2 vs 3) on tail class performance
  3. External data impact: Performance with vs without ChestXRay14/CheXpert supplementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of pretraining techniques vary when applied to different medical imaging tasks beyond chest X-rays, such as CT or MRI?
- Basis in paper: [explicit] The paper mentions the prevalence of pretraining techniques but lacks a systematic study on incorporating these new paradigms into the current framework.
- Why unresolved: The study is limited to chest X-rays and does not explore the applicability of pretraining techniques to other medical imaging modalities.
- What evidence would resolve it: Comparative studies evaluating the performance of pretraining techniques across various medical imaging tasks, including CT and MRI, would provide insights into their generalizability and effectiveness.

### Open Question 2
- Question: What is the impact of label co-occurrence on model performance in real-world clinical settings, and how can it be effectively modeled?
- Basis in paper: [explicit] The paper highlights the importance of addressing label co-occurrence in medical diagnosis, which is different from conventional multi-class settings.
- Why unresolved: The paper does not provide a detailed analysis of how label co-occurrence affects model performance in practical scenarios or propose specific methods to model it.
- What evidence would resolve it: Empirical studies demonstrating the impact of label co-occurrence on model accuracy and robustness in clinical settings, along with proposed methods to model and mitigate its effects, would address this question.

### Open Question 3
- Question: How can semi-supervised learning techniques be integrated with exogenous data replenishment to improve model performance on long-tailed datasets?
- Basis in paper: [explicit] The paper suggests that the use of exogenous data could be further improved from the perspective of semi-supervised learning.
- Why unresolved: The paper does not explore the integration of semi-supervised learning techniques with exogenous data replenishment in detail.
- What evidence would resolve it: Experiments comparing the performance of models using semi-supervised learning techniques with those using only supervised learning on long-tailed datasets would provide insights into the potential benefits of such integration.

### Open Question 4
- Question: What are the trade-offs between model complexity and performance in long-tailed multi-label classification tasks?
- Basis in paper: [explicit] The paper explores various advanced designs and techniques to improve performance, implying a consideration of model complexity.
- Why unresolved: The paper does not explicitly discuss the trade-offs between model complexity and performance, nor does it provide a framework for evaluating these trade-offs.
- What evidence would resolve it: Comparative studies analyzing the performance of models with varying levels of complexity on long-tailed multi-label classification tasks would help identify the optimal balance between complexity and performance.

## Limitations

- The framework's effectiveness relies heavily on the quality of medical text encoders, but no systematic comparison of their relative performance is provided
- The approach of treating missing labels from external datasets as absent (0) is a significant approximation that could introduce bias, yet no ablation study examines this impact
- The methodology for selecting hard classes for reweighting (based solely on development set performance) may not generalize to different dataset splits or distributions

## Confidence

- **High confidence**: The overall framework architecture combining visual and textual features through transformer decoders is technically sound and well-established in multimodal learning literature
- **Medium confidence**: The specific implementation details like reweighting factors (2 or 3) and choice of external datasets are reasonable but not thoroughly validated across different scenarios
- **Low confidence**: The claim about label correlation being effectively captured through pretrained text encoders lacks empirical validation - no comparison is provided against methods that don't use text information

## Next Checks

1. **Ablation study**: Compare framework performance with and without text encoder integration to quantify the actual contribution of label correlation modeling

2. **Reweighting sensitivity analysis**: Test different reweighting strategies (global vs class-specific, varying factors) to determine optimal configuration and robustness

3. **Missing label assumption validation**: Evaluate the impact of different missing label handling strategies (treating as absent vs. ignoring external data for those samples) on tail class performance