---
ver: rpa2
title: 'OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained
  from Scratch'
arxiv_id: '2309.10706'
source_url: https://arxiv.org/abs/2309.10706
tags:
- arxiv
- language
- training
- data
- openba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenBA is a 15B-parameter bilingual encoder-decoder language model
  pre-trained from scratch on 380B tokens of English, Chinese, and code data. It uses
  a shallow-encoder deep-decoder architecture and a three-stage training strategy
  (UL2 pre-training, length adaptation, and instruction tuning on a collected bilingual
  Flan dataset).
---

# OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch

## Quick Facts
- arXiv ID: 2309.10706
- Source URL: https://arxiv.org/abs/2309.10706
- Reference count: 31
- OpenBA is a 15B-parameter bilingual encoder-decoder language model pre-trained from scratch on 380B tokens of English, Chinese, and code data.

## Executive Summary
OpenBA is a 15B-parameter bilingual encoder-decoder language model pre-trained from scratch on 380B tokens of English, Chinese, and code data. It uses a shallow-encoder deep-decoder architecture and a three-stage training strategy (UL2 pre-training, length adaptation, and instruction tuning on a collected bilingual Flan dataset). OpenBA achieves strong performance across a wide range of tasks, including reading comprehension, generation, and reasoning, outperforming larger models on several benchmarks. For example, it surpasses LLaMA-70B on BELEBELE, BLOOM-176B on MMLU, and GLM-130B on C-Eval (hard).

## Method Summary
OpenBA uses a three-stage training strategy: (1) UL2 pre-training on a mixture of English, Chinese, and code tokens, (2) length adaptation to handle longer sequences, and (3) bilingual Flan instruction tuning. The model employs a shallow-encoder deep-decoder architecture with 12 encoder layers and 36 decoder layers, using techniques like sandwich layer normalization, rotary embedding, and SwiGLU activation. The pre-training corpus consists of 190B English tokens, 190B Chinese tokens, and 20B code tokens, balanced at the token level.

## Key Results
- Outperforms LLaMA-70B on BELEBELE
- Surpasses BLOOM-176B on MMLU
- Exceeds GLM-130B on C-Eval (hard)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-stage training pipeline (UL2 pre-training → length adaptation → bilingual Flan instruction tuning) improves performance on both understanding and generation tasks by gradually exposing the model to increasingly complex and task-oriented data distributions.
- Mechanism: Each stage incrementally shifts the model's focus—first from denoising and masked language modeling, to handling longer sequences, to instruction following—which builds competence across the full spectrum of tasks without overwhelming the model in a single stage.
- Core assumption: Task performance improves when training data complexity and instruction specificity are increased progressively rather than all at once.
- Evidence anchors:
  - [abstract] "adopt a three-stage training strategy to train the model from scratch"
  - [section 3.3] Describes each stage with distinct objectives: UL2 pre-training, length-adaptation, and Flan instruction tuning.
- Break condition: If stage transitions are too abrupt, or the data distributions overlap too heavily, the staged gains could collapse into instability or overfitting.

### Mechanism 2
- Claim: The shallow-encoder deep-decoder asymmetric architecture enables stronger generative capability while maintaining efficient comprehension, especially for tasks requiring long outputs or multi-turn dialogue.
- Mechanism: By allocating more layers to the decoder, the model can better model generation steps, while the shallower encoder reduces computational cost during inference when context is relatively static (e.g., dialogue history stored in decoder).
- Core assumption: Decoder depth is more critical for generation quality than encoder depth, and encoder efficiency is less impactful in inference-dominated scenarios.
- Evidence anchors:
  - [abstract] "we utilize another asymmetric model structure, i.e., shallow-encoder deep decoder to enhance the generation capability"
  - [section 5.1] "We explore three model structures... (1) a shallow encoder with a deep decoder... (2) a deep encoder with a shallow decoder... (3) the encoder and decoder with equal depth."
- Break condition: If the decoder becomes too deep relative to the encoder, the model may lose balanced comprehension, especially on tasks that require complex context integration.

### Mechanism 3
- Claim: Incorporating multilingual and code data in a balanced way during pre-training boosts performance on cross-lingual and code-related tasks without sacrificing monolingual benchmarks.
- Mechanism: Balanced exposure to high-quality English and Chinese tokens, along with code tokens, diversifies the learned representations, making the model more robust to multilingual input and generation.
- Core assumption: Quality and diversity of pre-training data are more important than sheer quantity; balanced bilingual data yields better cross-lingual generalization than skewed monolingual data.
- Evidence anchors:
  - [section 3.1.1] "our pre-training data consists of the same proportion of Chinese and English tokens"
  - [section 3.1.2] Describes balanced sampling from English Flan and Chinese Flan to form BiFlan dataset.
- Break condition: If data quality is inconsistent or code tokens are overrepresented, the multilingual gains could be offset by degradation in natural language understanding.

## Foundational Learning

- Concept: **Encoder-decoder architecture and seq2seq modeling**
  - Why needed here: Understanding how the shallow-encoder deep-decoder structure differs from standard T5 or BERT is critical for interpreting performance gains.
  - Quick check question: What is the main advantage of a deeper decoder in seq2seq models for generation tasks?

- Concept: **Stage-wise curriculum learning**
  - Why needed here: The three-stage training pipeline is central to the model's design; knowing how to implement and tune each stage is essential for replication.
  - Quick check question: Why might abruptly switching from UL2 pre-training to instruction tuning cause training instability?

- Concept: **Multilingual data balancing and token-level mixing**
  - Why needed here: The model's bilingual capability hinges on balanced data composition; understanding the rationale prevents over- or under-representation of one language.
  - Quick check question: What could happen if Chinese tokens are underrepresented during pre-training?

## Architecture Onboarding

- Component map:
  - Encoder: 12 layers, 40 attention heads, 4096 dmodel, 16384 dff
  - Decoder: 36 layers, 40 attention heads, 4096 dmodel, 16384 dff
  - Tokenizer: mT5 tokenizer (supports Chinese, English, other languages)
  - Embeddings: Shared between encoder and decoder

- Critical path:
  1. Load balanced pre-training corpus (190B English + 190B Chinese + 20B code tokens)
  2. Apply three-stage training pipeline in sequence
  3. Fine-tune with bilingual Flan dataset (66.7% English, 33.3% Chinese)

- Design tradeoffs:
  - Shallow encoder reduces memory/compute during inference but may limit comprehension depth
  - Deep decoder increases generation capability but also training/inference cost
  - Stage-wise training improves stability but requires careful data curation and transition

- Failure signatures:
  - Loss collapse or divergence in early UL2 stage → likely caused by too aggressive denoising or improper learning rate
  - Poor multilingual performance → likely imbalanced data mixing or insufficient Chinese token count
  - Instruction-following failure → likely insufficient fine-tuning on Flan or weak bilingual instruction coverage

- First 3 experiments:
  1. **Encoder-decoder depth swap test**: Train a model with deep encoder/shallow decoder and compare S-Denoising task loss/accuracy to baseline.
  2. **Stage ablation**: Skip length-adaptation stage and measure performance drop on long-sequence tasks.
  3. **Bilingual data ratio sweep**: Vary English/Chinese token ratio during pre-training and evaluate BELEBELE benchmark performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the asymmetric shallow-encoder deep-decoder architecture compare to balanced architectures in terms of task performance and computational efficiency?
- Basis in paper: [explicit] The paper describes using a shallow-encoder deep-decoder architecture motivated by empirical observations, but doesn't provide direct comparisons with balanced architectures.
- Why unresolved: The paper focuses on the effectiveness of their chosen architecture but doesn't include ablation studies comparing it to alternative architectural choices like balanced encoders/decoders or deep-encoder shallow-decoder setups.
- What evidence would resolve it: Direct ablation studies comparing different architectural configurations (shallow-deep, deep-shallow, balanced) on the same training setup and benchmarks.

### Open Question 2
- Question: What is the optimal balance between English and Chinese data in the pre-training mixture for maximizing bilingual performance?
- Basis in paper: [explicit] The paper uses a 50-50 token split between English and Chinese data, but notes this was based on GLM's approach and limited computational resources.
- Why unresolved: The paper doesn't explore different data mixture ratios or analyze the sensitivity of model performance to the English-Chinese balance in pre-training data.
- What evidence would resolve it: Systematic experiments varying the ratio of English to Chinese tokens in pre-training and measuring downstream performance on bilingual benchmarks.

### Open Question 3
- Question: How does the three-stage training strategy compare to end-to-end training in terms of final model performance and training efficiency?
- Basis in paper: [inferred] The paper describes a three-stage training process (UL2 pre-training, length adaptation, Flan training) but doesn't compare it to training with a single objective or different stage arrangements.
- Why unresolved: While the staged approach is described in detail, there's no comparison to alternative training strategies that might achieve similar or better results with fewer stages or different objectives.
- What evidence would resolve it: Comparative experiments training the same model with different stage arrangements or single-stage approaches, measuring both final performance and total training time/FLOPs.

## Limitations

- Data Quality and Composition: The specific filtration and deduplication strategies for privacy and quality are not fully detailed, which could impact reproducibility and model performance.
- Training Stability and Numerical Precision: The effectiveness of FP32 precision in maintaining stability across the three-stage training pipeline is not empirically validated.
- Stage Transition Dynamics: The exact criteria for transitioning between stages and the impact of abrupt or gradual transitions are not fully explored.

## Confidence

- **High Confidence**: The claim that OpenBA achieves strong performance across multiple benchmarks (e.g., BELEBELE, MMLU, C-Eval) is supported by direct comparisons with established models like LLaMA-70B and BLOOM-176B.
- **Medium Confidence**: The assertion that the shallow-encoder deep-decoder architecture enhances generation capability is plausible but not exhaustively validated across all task types, especially those requiring deep context integration.
- **Low Confidence**: The claim that balanced bilingual data yields better cross-lingual generalization is based on the model's design and data composition, but lacks direct experimental evidence comparing skewed vs. balanced data distributions.

## Next Checks

1. **Encoder-Decoder Depth Swap Test**: Train a model with a deep encoder and shallow decoder to compare performance on S-Denoising tasks against the baseline shallow-encoder deep-decoder model. This will validate the importance of decoder depth for generation tasks.

2. **Stage Ablation Study**: Skip the length-adaptation stage and evaluate performance on long-sequence tasks. This will determine the necessity and impact of each training stage on overall model capability.

3. **Bilingual Data Ratio Sweep**: Vary the ratio of English to Chinese tokens during pre-training and measure performance on the BELEBELE benchmark. This will quantify the impact of data balance on multilingual task performance.