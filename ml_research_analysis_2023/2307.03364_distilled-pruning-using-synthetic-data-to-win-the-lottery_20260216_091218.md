---
ver: rpa2
title: 'Distilled Pruning: Using Synthetic Data to Win the Lottery'
arxiv_id: '2307.03364'
source_url: https://arxiv.org/abs/2307.03364
tags:
- pruning
- distilled
- data
- sparsity
- lottery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of finding
  sparse, trainable subnetworks (lottery tickets) via Iterative Magnitude Pruning
  (IMP) by introducing Distilled Pruning. The core method uses distilled synthetic
  data to approximate the training process within IMP's inner loop, enabling faster
  mask generation while maintaining comparable accuracy.
---

# Distilled Pruning: Using Synthetic Data to Win the Lottery

## Quick Facts
- arXiv ID: 2307.03364
- Source URL: https://arxiv.org/abs/2307.03364
- Authors: 
- Reference count: 23
- One-line primary result: Distilled Pruning finds sparse, trainable subnetworks up to 5Ã— faster than Iterative Magnitude Pruning using synthetic data to approximate training in the pruning loop.

## Executive Summary
This paper addresses the computational inefficiency of finding sparse, trainable subnetworks (lottery tickets) via Iterative Magnitude Pruning (IMP) by introducing Distilled Pruning. The core method uses distilled synthetic data to approximate the training process within IMP's inner loop, enabling faster mask generation while maintaining comparable accuracy. Experimental results on CIFAR-10 (AlexNet) and CIFAR-100 (ConvNet) demonstrate that Distilled Pruning can find lottery tickets up to 5Ã— faster than standard IMP at similar sparsity levels.

## Method Summary
The method replaces the expensive real-data retraining in IMP's inner loop with training on distilled synthetic data. Using dataset distillation techniques, the authors generate synthetic datasets that capture essential training patterns. During each pruning iteration, instead of retraining with real data to identify which weights to prune, the network trains on the distilled dataset. This significantly reduces computation time per iteration (approximately 8Ã— faster). The sparsity mask identified through this process is then validated by retraining with real data. The approach offers a promising trade-off between efficiency and performance for rapid experimentation in pruning research and neural architecture search.

## Key Results
- Distilled Pruning finds lottery tickets up to 5Ã— faster than IMP at comparable sparsity on CIFAR-10
- Synthetic data training is approximately 8Ã— faster per iteration than real data training
- Distilled pruning-generated tickets exhibit initialization stability (robust to SGD noise even when rewound to initialization), unlike IMP-generated tickets which require early-training rewinding
- The method achieves 55 seconds per distilled training session on CIFAR-10 versus 7.25 minutes per training on real data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilled data can approximate trained weights well enough to identify sparsity masks comparable to IMP.
- Mechanism: Synthetic data generated through dataset distillation captures essential training patterns. When used in the inner loop of IMP, these patterns guide the pruning process to identify similar sparsity masks as would be found using real data, but much faster.
- Core assumption: The distilled dataset preserves sufficient information about the original training distribution to guide effective mask selection.
- Evidence anchors:
  - [abstract] "Our approach can find sparse, trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative Magnitude Pruning at comparable sparsity on CIFAR-10."
  - [section 2] "We employ a simple augmentation to the original IMP algorithm by replacing the training data, ð·train, needed to find the sparsity mask with distilled data, ð·syn"
  - [corpus] Weak evidence - related works show dataset distillation can approximate training trajectories but don't directly prove effectiveness for lottery ticket finding.

### Mechanism 2
- Claim: Lottery tickets found via distilled pruning exhibit different stability characteristics than IMP tickets.
- Mechanism: The synthetic training process creates a different optimization landscape. Tickets found through this process are stable even when rewound to initialization, unlike IMP tickets which typically require early-training rewinding.
- Core assumption: The optimization trajectory on distilled data leads to different weight initialization sensitivities compared to real data training.
- Evidence anchors:
  - [section 3.3] "In contrast, the lottery tickets identified through distilled pruning proved stable against SGD noise, even when rewound to the initialized weights."
  - [section 3.3] "We further discovered that subnetworks pruned using distilled data exhibited greater stability even at high sparsities."
  - [corpus] Moderate evidence - related works on dataset distillation suggest trajectory matching can produce stable models, but the lottery ticket context is novel.

### Mechanism 3
- Claim: The efficiency gain comes primarily from reduced retraining iterations in the inner loop.
- Mechanism: Each iteration of IMP requires full retraining to assess which weights to prune. By using distilled data that trains ~8x faster, the cumulative time savings across multiple pruning iterations becomes substantial.
- Core assumption: The speed advantage of distilled data training scales linearly with the number of pruning iterations.
- Evidence anchors:
  - [abstract] "The method achieves this speedup by replacing expensive real-data retraining with synthetic data training, which is approximately 8Ã— faster per iteration."
  - [section 3.2] "We achieve an average of 55 seconds per distilled training session on CIFAR-10, compared to 7.25 minutes per training on real data."
  - [corpus] Weak evidence - while dataset distillation is known to be faster, the specific 8x factor for lottery ticket finding isn't established in related work.

## Foundational Learning

- Concept: Lottery Ticket Hypothesis
  - Why needed here: This work builds directly on LTH by exploring whether distilled data can find equivalent lottery tickets more efficiently.
  - Quick check question: What is the key claim of the Lottery Ticket Hypothesis regarding sparse subnetworks?

- Concept: Dataset Distillation
  - Why needed here: The method replaces real training data with distilled synthetic data to accelerate the pruning process.
  - Quick check question: What is the primary goal of dataset distillation in the context of this work?

- Concept: Iterative Magnitude Pruning (IMP)
  - Why needed here: The proposed method modifies IMP by replacing its inner loop training with distilled data training.
  - Quick check question: What are the three main steps in each iteration of IMP?

## Architecture Onboarding

- Component map: Dataset Distillation Module -> Modified IMP Framework -> Real-Data Validation
- Critical path: 1) Generate distilled data for the target dataset, 2) Run modified IMP using distilled data for mask generation, 3) Validate mask quality by retraining with real data.
- Design tradeoffs: Speed vs. accuracy - using more distilled data per class (e.g., 50 vs 10 ipc) improves mask quality but reduces speedup. The method also trades the stability properties of IMP tickets for the initialization stability of distilled tickets.
- Failure signatures: Poor performance on distilled data (e.g., failing to match the dense model accuracy) indicates the distilled dataset doesn't capture essential patterns. Instability during the final real-data retraining suggests the mask quality is insufficient.
- First 3 experiments:
  1. Run distilled pruning with 50 ipc on AlexNet/CIFAR-10 and compare accuracy to IMP at various sparsity levels.
  2. Test stability by interpolating between two models trained with different SGD noise and measuring accuracy drop.
  3. Vary the number of distilled images per class (e.g., 10, 25, 50 ipc) to find the optimal tradeoff between speed and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does distilled pruning consistently find lottery tickets across different model architectures beyond ConvNets and AlexNet?
- Basis in paper: [inferred] The paper only demonstrates results on AlexNet (CIFAR-10) and a 128-width ConvNet (CIFAR-100), with the authors noting that current distillation methods may not scale to deeper networks
- Why unresolved: The experiments were limited to specific architectures, and the authors explicitly state that distillation methods may not generalize well to deeper networks
- What evidence would resolve it: Testing distilled pruning on ResNet, VGG, and transformer architectures across multiple datasets

### Open Question 2
- Question: What is the theoretical relationship between the quality of distilled data and the performance of lottery tickets found through distilled pruning?
- Basis in paper: [explicit] The authors note that MTT (the distillation method used) has one of the largest computational costs and that "any advancements in data distillation techniques can directly translate into speed improvements for distilled pruning"
- Why unresolved: The paper doesn't establish a formal relationship between distillation quality metrics and lottery ticket performance, only noting that better distillation should lead to better results
- What evidence would resolve it: A study correlating specific distillation quality metrics (e.g., KL divergence, feature matching loss) with lottery ticket accuracy and stability

### Open Question 3
- Question: Why do distilled pruning-generated lottery tickets exhibit stability to SGD noise even when rewound to initialization, unlike IMP-generated tickets?
- Basis in paper: [explicit] The authors observe this phenomenon in their instability analysis and state "we believe further research is necessary to fully understand why distilled data-generated tickets exhibit such stability"
- Why unresolved: The paper identifies the phenomenon but doesn't provide a theoretical explanation for the different stability characteristics
- What evidence would resolve it: Comparative analysis of the optimization landscapes, weight initialization distributions, and training dynamics between distilled and IMP-generated tickets

## Limitations
- Synthetic data fidelity: The distilled datasets are generated through MTT, but the paper doesn't provide ablations showing how sensitive mask quality is to the specific distillation method.
- Generalization to larger models: Experiments are limited to AlexNet and a 128-width ConvNet. The approach's effectiveness on modern architectures (ResNets, Transformers) remains unverified.
- Speedup variability: The 8Ã— faster claim for distilled data training is based on CIFAR-10 with AlexNet. This ratio may differ substantially for larger datasets or more complex models.

## Confidence
- High confidence: The efficiency improvement mechanism (replacing real data retraining with faster distilled data training) is well-established and experimentally validated.
- Medium confidence: The claim of finding comparable lottery tickets with distilled pruning is supported by CIFAR-10 results but needs verification on additional architectures and datasets.
- Low confidence: The assertion that distilled tickets are stable at initialization while IMP tickets require early-training rewinding needs more systematic investigation across multiple datasets and architectures.

## Next Checks
1. **Ablation study on distillation method**: Test whether alternative dataset distillation techniques (e.g., Meta-Dataset Distillation, DAFL) can find lottery tickets with comparable quality and speed to MTT.
2. **Scaling experiment**: Apply Distilled Pruning to ResNet-18 on CIFAR-10 and ImageNet-1k to verify if the speedup and accuracy claims hold for larger, more modern architectures.
3. **Stability analysis across initialization methods**: Systematically compare the stability of distilled vs. IMP tickets under different initialization schemes (Kaiming, Xavier) and learning rate schedules to better understand the initialization stability property.