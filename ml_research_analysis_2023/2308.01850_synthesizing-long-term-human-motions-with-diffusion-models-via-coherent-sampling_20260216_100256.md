---
ver: rpa2
title: Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling
arxiv_id: '2308.01850'
source_url: https://arxiv.org/abs/2308.01850
tags:
- motion
- motions
- sampling
- diffusion
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel diffusion model-based approach to\
  \ generate coherent long-term human motions from text streams. Unlike existing methods\
  \ that handle only short-term motions or require post-processing for long-term sequences,\
  \ the proposed method uses a past-conditioned diffusion model with two sampling\
  \ strategies\u2014Past Inpainting Sampling and Compositional Transition Sampling\u2014\
  to directly produce smooth, semantically aligned motions without alignment or interpolation."
---

# Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling

## Quick Facts
- arXiv ID: 2308.01850
- Source URL: https://arxiv.org/abs/2308.01850
- Reference count: 40
- This paper introduces a novel diffusion model-based approach to generate coherent long-term human motions from text streams.

## Executive Summary
This paper presents a novel approach for generating coherent long-term human motions from text streams using diffusion models. The method addresses the limitation of existing approaches that can only generate short-term motions or require post-processing for long-term sequences. By introducing Past Inpainting Sampling and Compositional Transition Sampling strategies, the proposed Past-Conditioned Human Motion Diffusion Model (PCMDM) can directly produce smooth, semantically aligned motion sequences without requiring alignment or interpolation. The approach achieves state-of-the-art performance on the BABEL dataset, demonstrating both higher quality and stronger coherence in long-term motion generation.

## Method Summary
The paper proposes PCMDM, a diffusion model for generating long-term coherent human motions from text streams. The model uses a past-conditioned encoder with two sampling strategies: Past Inpainting Sampling, which conditions future motion generation on the ending frames of the previous motion segment, and Compositional Transition Sampling, which models the transition region between adjacent actions as the product of their respective motion distributions. The method is trained on the BABEL dataset and evaluated using FID, R-Precision, Diversity, and Transition Distance metrics.

## Key Results
- Achieves FID of 5.242 compared to 7.312 for the best prior method
- Reduces Transition Distance to 0.014 from 0.122 in previous approaches
- Demonstrates superior coherence and quality in long-term motion generation
- Outperforms baseline methods including TEACH and MDM on all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
Past Inpainting Sampling enables seamless motion transitions by conditioning future motion generation on the ending frames of the previous motion segment. At each diffusion sampling step, the known ending frames of the previous motion are injected into the current motion tensor, effectively "inpainting" the transition. This prevents discontinuity by ensuring spatial and temporal coherence across segments. The core assumption is that the last few frames of a motion segment are sufficient to encode the state and momentum needed for smooth continuation.

### Mechanism 2
Compositional Transition Sampling models the transition region between adjacent actions as the product of their respective motion distributions. The transition frames are shared between two motions and their distributions are combined via weighted summation during each sampling step, allowing mutual influence rather than autoregressive dependence. The core assumption is that the transition region between two semantically different actions can be modeled as a weighted mixture of the two action distributions, capturing their interaction.

### Mechanism 3
The past-conditioned encoder allows the model to learn transition-aware representations by conditioning the current motion on the last â„ frames of the previous motion during training. The past encoder processes the tail of the previous motion and fuses it with the current timestep and text embedding, enabling the diffusion model to learn context-aware denoising that preserves continuity. The core assumption is that the encoder can effectively encode the relevant contextual information from the previous motion without losing pose-specific detail.

## Foundational Learning

- **Diffusion models**: Learn to reverse a noising process by predicting noise or clean data at each timestep. Why needed here: The core generation pipeline relies on denoising at each step to reconstruct coherent human motion from Gaussian noise. Quick check question: What is the purpose of the variance schedule {ğ›¼ğ‘¡} in the forward diffusion process?
- **Classifier-free guidance**: Allows conditional sampling without requiring a separate classifier. Why needed here: Enables flexible conditioning on both text prompts and previous motion segments during sampling. Quick check question: How does the guidance scale ğ‘  control the trade-off between fidelity to the condition and sample diversity?
- **Product of distributions**: In energy-based models allows combining multiple conditional influences. Why needed here: Used in Compositional Transition Sampling to blend the distributions of adjacent motions in the transition region. Quick check question: In what sense is the product of two distributions different from their mixture?

## Architecture Onboarding

- **Component map**: Text Encoder -> Past Encoder -> Transformer Encoder -> Motion Output
- **Critical path**:
  1. Encode current text prompt â†’ get text embedding
  2. Encode last â„ frames of previous motion â†’ get past embedding
  3. Combine with timestep embedding â†’ form token sequence
  4. Concatenate with noisy current motion â†’ feed into Transformer
  5. Predict denoised motion â†’ sample next step
- **Design tradeoffs**:
  - â„ vs coherence: Larger â„ improves continuity but may include irrelevant context
  - Transition length ğ¿ğ‘‡ ğ‘Ÿ vs smoothness: Longer transitions allow smoother blending but may blur action semantics
  - Guidance scale ğ‘  vs sample quality: Higher ğ‘  increases prompt adherence but may reduce diversity
- **Failure signatures**:
  - High transition distance: Indicates poor motion alignment between segments
  - Low FID: Suggests generated motions deviate from real motion distribution
  - Mode collapse in diversity: May indicate over-conditioning or insufficient noise
- **First 3 experiments**:
  1. Train PCMDM with â„=4 and compare transition distance to TEACH baseline
  2. Implement Past Inpainting Sampling and evaluate on transition coherence
  3. Test Compositional Transition Sampling with different ğ¿ğ‘‡ ğ‘Ÿ values to find optimal transition length

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method perform when extended to generate human motions from text streams containing more than two prompts, as demonstrated qualitatively in Figure 6? The paper mentions that the method is "easily extendable to generate longer coherent 3D human motions for streams with more than two text prompts" and provides qualitative results for such cases in Figure 6, but only provides qualitative results without quantitative evaluation.

### Open Question 2
How does the performance of the proposed method vary with different numbers of inpainting frames (â„) and transition frames (ğ¿ğ‘‡ ğ‘Ÿ) as explored in the ablation study? While the ablation study in Section 5.5 investigates the effect of varying â„ and ğ¿ğ‘‡ ğ‘Ÿ on performance, showing that smaller values tend to yield better results, it does not explore the full range of possible values nor explain the underlying reasons for the observed trends.

### Open Question 3
How does the proposed method handle text streams with overlapping or ambiguous action descriptions, and what is the impact on the coherence and quality of the generated motions? The paper focuses on generating coherent motions from text streams but does not explicitly address cases where action descriptions may overlap or be ambiguous, which are common in real-world scenarios.

## Limitations
- The Past Encoder architecture is underspecified, described only as "a small linear layer" without implementation details
- The Compositional Transition Sampling uses fixed 1/2 weighting regardless of transition position, which may not be optimal for all action pairs
- The MotionCLIP evaluation model's architecture and training procedure are not detailed, making metric reliability difficult to assess

## Confidence

- **High Confidence**: The fundamental diffusion model framework and training procedure are well-established and clearly described. The BABEL dataset usage and metric definitions (FID, R-Precision, Diversity, Transition Distance) are explicit.
- **Medium Confidence**: The theoretical justification for Compositional Transition Sampling as a product of distributions is sound, but the practical implementation details introduce uncertainty about real-world effectiveness.
- **Low Confidence**: The Past Inpainting Sampling mechanism's reliance on a fixed number of conditioning frames without position-dependent weighting could lead to suboptimal transitions, particularly for semantically distant action pairs.

## Next Checks

1. **Architectural Verification**: Implement and test multiple Past Encoder variants (linear vs. small Transformer) to determine if the simple linear layer is truly optimal for capturing transition context.
2. **Transition Weighting Analysis**: Replace the fixed 1/2 weighting in Compositional Transition Sampling with position-dependent weights and evaluate impact on Transition Distance across different action pair types.
3. **Cross-Dataset Generalization**: Evaluate the model on a held-out test set from BABEL not seen during training, and test on a different human motion dataset to assess generalization beyond the training distribution.