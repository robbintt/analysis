---
ver: rpa2
title: 'BAL: Balancing Diversity and Novelty for Active Learning'
arxiv_id: '2312.15944'
source_url: https://arxiv.org/abs/2312.15944
tags:
- learning
- cycle
- active
- data
- labeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses active learning, focusing on how to optimally
  select a small subset of data to label within a fixed budget. It introduces a method
  that uses self-supervised features to balance diversity and uncertainty in data
  selection.
---

# BAL: Balancing Diversity and Novelty for Active Learning

## Quick Facts
- **arXiv ID**: 2312.15944
- **Source URL**: https://arxiv.org/abs/2312.15944
- **Reference count**: 40
- **Key outcome**: Outperforms existing active learning methods on benchmarks by 1.20%, maintaining performance comparable to using full dataset even when labeling 80% of samples

## Executive Summary
This paper introduces BAL (Balancing Active Learning), a framework that addresses the core challenge in active learning: optimally selecting a small subset of data to label within a fixed budget. BAL uses self-supervised features to balance diversity and uncertainty in data selection, introducing a novel Cluster Distance Difference (CDD) metric and an adaptive sub-pool mechanism. The method demonstrates significant improvements over existing approaches while maintaining performance comparable to full dataset training even with substantial labeling reductions.

## Method Summary
BAL addresses active learning by constructing adaptive sub-pools to balance diverse and uncertain data. The method trains a self-supervised model on unlabeled data, extracts features, and applies K-means clustering. It then computes CDDs to measure diversity and sorts data accordingly. An adaptive sub-pool mechanism with a balancing factor β dynamically adjusts the size of each sub-pool based on early performance. Finally, an uncertainty-based sampler selects K samples for labeling in each cycle. The framework repeats this process until the labeling budget is met, training the main task model on the updated labeled pool.

## Key Results
- Outperforms existing active learning methods on benchmarks by 1.20%
- Maintains performance comparable to using full dataset even when labeling 80% of samples
- Demonstrates effectiveness across multiple datasets including CIFAR-10, SVHN, Caltech-101, and Tiny-ImageNet

## Why This Works (Mechanism)

### Mechanism 1: Cluster Distance Difference (CDD) as a Diversity Indicator
CDD provides a computationally efficient measure of clustering difficulty that outperforms pretext task loss in active learning. It computes the difference between distances to the two nearest cluster centers, reaching zero when a point lies on the decision boundary between clusters. Core assumption: Points with lower CDD values are more difficult to classify and thus more informative for model training.

### Mechanism 2: Adaptive Sub-pools with Balancing Factor
Dynamic adjustment of sub-pool sizes balances diversity and uncertainty more effectively than fixed-size sub-pools. The balancing factor β controls overlap between adjacent sub-pools, allowing more uncertain samples when β > 1 and more diverse samples when β < 1. Core assumption: The optimal balance between diversity and uncertainty varies depending on the labeling budget and dataset characteristics.

### Mechanism 3: Self-supervised Features for Active Learning
Features learned through self-supervised learning capture inter-sample relationships that benefit downstream active learning tasks. Self-supervised models are trained on unlabeled data, and their features are used for clustering and CDD computation. Core assumption: Self-supervised features contain information about the data distribution that is useful for active learning.

## Foundational Learning

- **Concept**: Self-supervised learning
  - Why needed here: To extract meaningful features from unlabeled data that capture the underlying data distribution
  - Quick check question: What is the difference between supervised and self-supervised learning, and why is self-supervised learning particularly useful in active learning?

- **Concept**: Clustering algorithms (K-means)
  - Why needed here: To group similar data points together and identify decision boundaries between clusters
  - Quick check question: How does K-means clustering work, and what are its limitations in high-dimensional spaces?

- **Concept**: Active learning strategies
  - Why needed here: To select the most informative data points for labeling, reducing the overall labeling cost
  - Quick check question: What are the main types of active learning strategies, and how do they differ in their approach to data selection?

## Architecture Onboarding

- **Component map**:
  Self-supervised model (Fss) -> K-means clustering -> CDD computation -> Adaptive sub-pool generation -> Uncertainty-based sampler -> Main task model (Fm)

- **Critical path**:
  1. Train self-supervised model on unlabeled data
  2. Extract features and perform K-means clustering
  3. Compute CDDs and sort data
  4. Determine balancing factor β based on early performance
  5. Generate adaptive sub-pools
  6. Select K samples using uncertainty-based sampler
  7. Label selected samples and update labeled pool
  8. Train main task model on updated labeled pool
  9. Repeat steps 4-8 until labeling budget is met

- **Design tradeoffs**:
  - Complexity of self-supervised task vs. performance gain
  - Number of clusters in K-means vs. computational cost
  - Balancing factor β vs. diversity-uncertainty tradeoff
  - Sampling method (e.g., confidence vs. entropy) vs. performance

- **Failure signatures**:
  - Poor clustering results leading to inaccurate CDD values
  - Sub-optimal balancing factor β resulting in imbalanced diversity-uncertainty tradeoff
  - Uncertainty-based sampler failing to select truly uncertain samples
  - Main task model not benefiting from the selected diverse and uncertain samples

- **First 3 experiments**:
  1. Implement CDD computation and compare its performance against pretext task loss on a small dataset
  2. Test adaptive sub-pools with different balancing factors on a medium-sized dataset to find the optimal β
  3. Evaluate the overall BAL framework on a large dataset and compare its performance against existing active learning methods

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of BAL compare when applied to pixel-level tasks like semantic segmentation, as opposed to image-level classification tasks? The authors mention that for pixel-level tasks like segmentation, they consider pixel-level reconstruction tasks like MAE and BEiT more suitable but do not further explore them in this paper.

### Open Question 2
What is the impact of varying the initial labeling budget on the performance of BAL? The paper discusses the performance of BAL under different labeling budgets but does not specifically analyze the impact of varying the initial labeling budget.

### Open Question 3
How does BAL perform in active learning scenarios with non-stationary data distributions, where the data distribution changes over time? The paper does not discuss the performance of BAL in scenarios with non-stationary data distributions.

## Limitations
- Weak corpus support for core mechanisms, particularly the Cluster Distance Difference (CDD) metric and the adaptive sub-pool framework
- Claims about CDD outperforming pretext task loss lack comparative analysis with established uncertainty measures in the literature
- The adaptive sub-pool mechanism's dynamic adjustment is not extensively validated across diverse dataset characteristics

## Confidence
- **High**: The general active learning framework and the use of self-supervised features for diversity-uncertainty balancing
- **Medium**: The effectiveness of CDD as a diversity indicator compared to alternative measures
- **Low**: The optimal configuration of the balancing factor β and its generalizability across different datasets

## Next Checks
1. Conduct ablation studies comparing CDD against established uncertainty measures (entropy, confidence) on multiple datasets to verify its claimed superiority
2. Test the adaptive sub-pool mechanism across datasets with varying complexity and size distributions to validate the balancing factor's robustness
3. Implement cross-dataset evaluation to assess whether the self-supervised features learned on one dataset generalize effectively to active learning tasks on different datasets