---
ver: rpa2
title: Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics
  with MASCHInE
arxiv_id: '2306.03659'
source_url: https://arxiv.org/abs/2306.03659
tags:
- embeddings
- entities
- knowledge
- entity
- protograph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MASCHInE, a schema-based approach to improve\
  \ knowledge graph embedding models (KGEMs) by leveraging ontology information to\
  \ build protographs\u2014abstracted versions of the KG that capture semantic constraints.\
  \ Two protograph construction strategies (P1 and P2) are proposed, with P2 incorporating\
  \ subclass relationships to generate more triples, albeit with some noise."
---

# Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE

## Quick Facts
- **arXiv ID**: 2306.03659
- **Source URL**: https://arxiv.org/abs/2306.03659
- **Reference count**: 40
- **Key outcome**: MASCHInE improves knowledge graph embedding versatility by leveraging schema information through protographs, showing substantial semantic validity gains while maintaining rank-based performance.

## Executive Summary
This paper introduces MASCHInE, a schema-based approach to improve knowledge graph embedding models (KGEMs) by leveraging ontology information to build protographs—abstracted versions of the KG that capture semantic constraints. Two protograph construction strategies (P1 and P2) are proposed, with P2 incorporating subclass relationships to generate more triples, albeit with some noise. The approach pre-trains embeddings on protographs and then transfers them to the original KG for fine-tuning. Experiments across three tasks—link prediction (LP), entity clustering (EC), and node classification (NC)—show that MASCHInE yields more versatile KGEs. While rank-based LP metrics remain largely unchanged, Sem@K (semantic validity) improves substantially, especially for ConvE and TuckER. For EC, ARI and NMI increase significantly, demonstrating better class separability. In NC, F1-scores improve notably for complex datasets, particularly when using P2. MASCHInE consistently enhances semantic awareness and multi-task utility of KGEs.

## Method Summary
MASCHInE introduces a schema-driven approach to knowledge graph embedding by first constructing protographs that capture semantic constraints from ontology information. The method uses two strategies: P1 builds protographs using domain and range axioms, while P2 additionally incorporates subclass relationships to generate more triples. KGEMs are trained on these protographs to learn semantically meaningful embeddings, which are then transferred to initialize the full KG's entities and relations. The embeddings are subsequently fine-tuned on the original KG. The approach is evaluated across five KGEMs (TransE, DistMult, ComplEx, ConvE, TuckER) and three tasks: link prediction (using MRR, Hits@K, and Sem@K), entity clustering (using ARI and NMI), and node classification (using F1-score). This two-stage training process aims to produce more versatile embeddings that perform well across multiple downstream tasks while improving semantic validity.

## Key Results
- MASCHInE substantially increases semantic validity (Sem@K) for link prediction while maintaining equivalent rank-based performance
- Entity clustering metrics (ARI and NMI) improve significantly, indicating better class separability in the embedding space
- Node classification F1-scores improve notably for complex datasets, especially when using P2 strategy
- The approach demonstrates consistent improvements in semantic awareness and multi-task utility across all tested KGEMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Schema-driven protographs create semantically meaningful training data that improves entity representation learning.
- Mechanism: The MASCHInE approach leverages ontological constraints (domain, range, subclass relationships) to generate protographs that encode general relational patterns. These protographs are used to pre-train embeddings, which are then transferred to the full KG, initializing entity and relation embeddings in a semantically informed manner.
- Core assumption: Ontological constraints capture generalizable relational semantics that transfer to the instance KG.
- Evidence anchors:
  - [abstract] "We devise heuristics for generating protographs – small, modified versions of a KG that leverage schema-based information. The learnt protograph-based embeddings are meant to encapsulate the semantics of a KG."
  - [section 3.1] "Generating a protograph using schema-based information to improve KGEM performance with respect to a given task is an under-explored avenue."
  - [corpus] "Do Similar Entities have Similar Embeddings?" suggests that semantic assumptions in KGEs are often questioned, supporting the need for schema-driven approaches.
- Break condition: If the KG's schema is sparse or inconsistent, protographs may not capture meaningful patterns, leading to degraded transfer performance.

### Mechanism 2
- Claim: Pre-training on protographs provides a better initialization that improves downstream task performance.
- Mechanism: Embeddings are first learned on the protograph (which encodes schema-level semantics), then transferred to the full KG entities and relations. This initialization provides a more semantically aware starting point for fine-tuning on the instance KG.
- Core assumption: Protograph embeddings capture transferable semantic patterns that improve learning on the full KG.
- Evidence anchors:
  - [abstract] "The protograph embeddings are then used to initialize the actual KGEs, and training starts on these pre-trained KGEs."
  - [section 3.2] "The proposed MASCHInE approach allows to generate pre-trained embeddings based on a protograph, to be further fine-tuned on the initial KG."
  - [corpus] "Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization" suggests that initialization strategies can significantly impact embedding quality.
- Break condition: If the protograph is too dissimilar from the instance KG, transfer may introduce noise rather than helpful semantic structure.

### Mechanism 3
- Claim: Schema-aware training improves semantic validity of predictions while maintaining rank-based performance.
- Mechanism: By incorporating schema constraints during protograph construction and pre-training, the resulting embeddings better reflect entity types and relation domains/ranges, leading to more semantically valid link predictions even when rank-based metrics remain stable.
- Core assumption: Semantic validity correlates with schema consistency in predictions.
- Evidence anchors:
  - [abstract] "For link prediction, using MASCHInE substantially increases the number of semantically valid predictions with equivalent rank-based performance."
  - [section 4.2] "Sem@K values are higher when embeddings have been trained with either P1 or P2, compared to V."
  - [corpus] "Sem@$K$: Is my knowledge graph embedding model semantic-aware?" directly addresses semantic validity metrics, supporting the importance of this measure.
- Break condition: If rank-based metrics are the primary evaluation criterion, semantic validity improvements may be undervalued in certain applications.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGEs)
  - Why needed here: MASCHInE builds on standard KGE frameworks and improves them through schema-driven pre-training.
  - Quick check question: What is the difference between TransE and ComplEx in how they model relations?

- Concept: Ontology and Schema Information
  - Why needed here: Protographs are built using domain, range, and subclass relationships from the KG schema.
  - Quick check question: How do domain and range constraints in an ontology define valid triples?

- Concept: Transfer Learning in Embedding Spaces
  - Why needed here: MASCHInE transfers embeddings from protographs to the full KG, requiring understanding of how embeddings can be initialized and fine-tuned.
  - Quick check question: What happens to an entity's embedding when it belongs to multiple classes in the protograph?

## Architecture Onboarding

- Component map:
  - Protograph Builder -> Protograph KGE Trainer -> Embedding Transfer Module -> Fine-tuner -> Evaluator

- Critical path:
  1. Build protograph from schema
  2. Train KGEM on protograph
  3. Transfer embeddings to KG
  4. Fine-tune on KG
  5. Evaluate across multiple tasks

- Design tradeoffs:
  - P1 vs P2: P1 is cleaner but smaller; P2 includes more triples with potential noise
  - Transfer strategy: Averaging vs. selecting most specific class embeddings
  - Fine-tuning: Whether to continue using protograph embeddings during fine-tuning or freeze them

- Failure signatures:
  - Low Sem@K despite good rank-based metrics: Indicates poor semantic encoding
  - Degradation in EC performance: Suggests transferred embeddings don't capture class structure
  - Similar results across V, P1, P2: May indicate weak schema or poor protograph construction

- First 3 experiments:
  1. Compare Sem@K on a small schema-rich KG between V, P1, and P2 settings
  2. Evaluate ARI/NMI for EC task using k-means clustering on entity embeddings
  3. Test F1-score for NC task on DLCC benchmark datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of MASCHInE vary if protographs were constructed using different heuristics beyond P1 and P2, such as incorporating subclass relationships transitively or leveraging disjointWith axioms?
- Basis in paper: [explicit] The authors state "Other possibilities exist and we leave them for future work" when discussing protograph construction heuristics, and suggest that P2 introduces noise by considering direct subclasses instead of transitive ones.
- Why unresolved: The paper only evaluates two protograph construction strategies (P1 and P2), leaving open the question of how alternative heuristics might affect performance.
- What evidence would resolve it: Experiments comparing MASCHInE performance across multiple protograph construction strategies, including transitive subclass relationships and disjointWith axioms, would clarify which heuristics yield the most versatile embeddings.

### Open Question 2
- Question: What is the impact of iterative co-training between protographs and knowledge graphs on the quality and versatility of knowledge graph embeddings?
- Basis in paper: [explicit] The authors mention in the conclusion that "While in the current form, the transfer from the protograph embeddings to the KG entity embeddings is done once, we will study alternatives for their interaction to facilitate iterative co-training."
- Why unresolved: The current MASCHInE approach uses a single transfer of protograph embeddings to initialize KG embeddings, without exploring iterative refinement.
- What evidence would resolve it: Comparative experiments measuring the performance of MASCHInE with and without iterative co-training would demonstrate whether repeated interactions between protograph and KG embeddings improve versatility across tasks.

### Open Question 3
- Question: How does the performance of MASCHInE vary across knowledge graphs with different levels of schema completeness and expressivity?
- Basis in paper: [inferred] The authors use datasets with varying schema characteristics (e.g., YAGO14K with 5 hierarchy levels vs. FB14K with 2 levels) and observe that Freebase's simple hierarchy may limit protograph usefulness, suggesting schema complexity affects outcomes.
- Why unresolved: While the paper tests MASCHInE on datasets with different schema depths, it doesn't systematically investigate how schema completeness or expressivity impacts performance across a broader range of knowledge graphs.
- What evidence would resolve it: Experiments applying MASCHInE to knowledge graphs with varying schema completeness (from minimal to highly expressive) and measuring performance across tasks would reveal the approach's sensitivity to schema characteristics.

## Limitations

- Dependence on schema quality: When KG ontologies are sparse or inconsistent, protograph construction may introduce noise rather than meaningful semantic structure
- Computational overhead: Training twice (on protograph then KG) may not scale well to massive KGs
- Semantic validity metrics: The Sem@K metrics used lack standardized benchmarks for comparison across different KGEMs

## Confidence

- High confidence: Schema-aware initialization provides better starting points for KGEM training (supported by Sem@K improvements)
- Medium confidence: MASCHInE improves semantic validity and multi-task versatility (demonstrated across LP, EC, and NC tasks)
- Low confidence: Whether the improvements justify the additional computational cost for large-scale applications

## Next Checks

1. Test MASCHInE on a KG with known schema inconsistencies to quantify the noise-tolerance threshold of P2 strategy
2. Conduct ablation studies comparing pre-training epochs on protographs versus direct training on KGs
3. Evaluate transfer performance when mapping protograph embeddings to KG entities with multiple type assignments