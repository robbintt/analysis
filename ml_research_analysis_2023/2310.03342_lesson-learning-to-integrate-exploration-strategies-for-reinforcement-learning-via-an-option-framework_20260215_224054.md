---
ver: rpa2
title: 'LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning
  via an Option Framework'
arxiv_id: '2310.03342'
source_url: https://arxiv.org/abs/2310.03342
tags:
- exploration
- learning
- option
- policy
- lesson
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LESSON, a unified framework for exploration
  in reinforcement learning (RL) based on an option-critic model. The framework learns
  to integrate multiple diverse exploration strategies so that the agent can adaptively
  select the most effective exploration strategy over time to realize a relevant exploration-exploitation
  trade-off for each given task.
---

# LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework

## Quick Facts
- arXiv ID: 2310.03342
- Source URL: https://arxiv.org/abs/2310.03342
- Reference count: 20
- One-line primary result: A unified framework for exploration in RL based on an option-critic model that learns to adaptively select the most effective exploration strategy over time.

## Executive Summary
This paper introduces LESSON, a unified framework for exploration in reinforcement learning based on an option-critic model. The framework learns to integrate multiple diverse exploration strategies so that the agent can adaptively select the most effective exploration strategy over time to realize a relevant exploration-exploitation trade-off for each given task. The framework was evaluated on MiniGrid and Atari environments and consistently outperformed existing exploration methods.

## Method Summary
LESSON employs an option-critic architecture with predefined intra-policies (greedy, random, temporal-difference-based random, and prediction-error-maximizing) and learned termination functions. The framework uses both extrinsic and intrinsic rewards (prediction error) to update the option-value function and termination functions. The behavior policy selects among the N intra-policies based on the learned option selection policy, while the target policy learns to maximize extrinsic rewards using a standard DQN architecture.

## Key Results
- LESSON consistently outperforms existing exploration methods on MiniGrid and Atari environments
- The framework demonstrates effective adaptive selection of exploration strategies over time
- Including the greedy policy as one of the intra-policies enables exploration-exploitation trade-off within the same behavioral policy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The adaptive selection of exploration strategies via the option-critic model enables the agent to perform better exploration-exploitation trade-offs over time.
- **Mechanism:** The option-critic model learns a selection policy πΩ over N intra-policies, each representing a different exploration strategy. The termination functions {βw} determine how long each intra-policy is used before switching. By training these components with an objective that balances extrinsic rewards (exploitation) and intrinsic rewards (exploration), the framework learns to dynamically select the most effective strategy at each learning phase.
- **Core assumption:** The intrinsic reward signal (e.g., prediction error) provides meaningful guidance for exploration that can be combined with extrinsic rewards to improve learning.
- **Evidence anchors:**
  - [abstract] "The framework learns to integrate multiple diverse exploration strategies so that the agent can adaptively select the most effective exploration strategy over time to realize a relevant exploration-exploitation trade-off for each given task."
  - [section] "By learning optimal selection of one intra-policy out of the N such intra-policies at each time step, we can realize an effective trade-off between exploitation by the greedy policy and exploration by N − 1 exploration policies."
- **Break condition:** If the intrinsic reward signal becomes unreliable or if the environment rewards are dense and immediate, the learned selection may over-explore or under-exploit.

### Mechanism 2
- **Claim:** Including the greedy policy as one of the intra-policies allows the framework to balance exploration and exploitation within the same behavioral policy.
- **Mechanism:** By making the greedy policy one of the options, the option-critic can choose between pure exploitation (greedy) and various exploration strategies. The termination functions determine how long each strategy is used, allowing the agent to shift from exploration to exploitation as learning progresses.
- **Core assumption:** The greedy policy, when included as an option, provides a stable baseline for exploitation that can be compared against exploration strategies.
- **Evidence anchors:**
  - [section] "The inclusion of the greedy policy as one of the intra-policies of the option model is crucial because this inclusion enables exploration-exploitation trade-off in sampling by allowing the behavior policy to visit not only new state-action pairs for exploration but also the state-action pairs generated by the greedy policy for exploitation."
- **Break condition:** If the greedy policy is too far from optimal early in training, it may mislead the option selection process, causing the agent to prematurely exploit suboptimal behavior.

### Mechanism 3
- **Claim:** The use of prediction-error-based intrinsic rewards enables sample history-aware exploration that complements random exploration strategies.
- **Mechanism:** The PEM intra-policy selects actions that maximize prediction error, encouraging the agent to visit novel states. This intrinsic reward is normalized and combined with extrinsic rewards in the option-critic objective, allowing the agent to balance curiosity-driven exploration with reward-seeking behavior.
- **Core assumption:** Prediction errors correlate with state novelty and can be used as a reliable intrinsic reward signal.
- **Evidence anchors:**
  - [section] "The PEM intra-policy focuses purely on exploration with ignoring the extrinsic reward... Our PEM intra-policy focuses purely on exploration with ignoring the extrinsic reward, which is a different point from the original RND policy (Burda et al., 2018)."
- **Break condition:** If the prediction model overfits or if the environment is deterministic, prediction errors may not reflect novelty, leading to ineffective exploration.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP)
  - Why needed here: LESSON operates within the MDP framework, where states, actions, rewards, and transitions define the learning problem. Understanding MDPs is essential to grasp how exploration strategies interact with the environment dynamics.
  - Quick check question: What are the five components of an MDP, and how does the discount factor γ influence the agent's behavior?

- **Concept:** Option Framework
  - Why needed here: LESSON is built on the option-critic architecture, which extends MDPs with temporally extended actions (options). Each option consists of an intra-policy, an initiation set, and a termination function.
  - Quick check question: How does the call-and-return execution model work, and why is it important for temporal abstraction?

- **Concept:** Intrinsic Motivation and Exploration Bonuses
  - Why needed here: LESSON uses intrinsic rewards (e.g., prediction error) to encourage exploration. Understanding how intrinsic motivation works is key to designing effective exploration strategies.
  - Quick check question: What is the difference between count-based and prediction-based intrinsic rewards, and how do they encourage exploration?

## Architecture Onboarding

- **Component map:**
  - Target policy: A standard DQN that learns to maximize extrinsic rewards
  - Behavior policy: An option-critic model with N intra-policies (greedy + exploration strategies) and N termination functions
  - Intra-policies: Predefined strategies (greedy, random, TE-random, PEM)
  - Termination functions: Learned probabilities for switching between intra-policies
  - Intrinsic reward: Prediction error from RND or pseudo-count from PixelCNN
  - Replay buffer: Stores (state, action, option, extrinsic reward, intrinsic reward, next state) tuples

- **Critical path:**
  1. Sample a trajectory using the option-based behavior policy
  2. Store the trajectory in the replay buffer
  3. Update the target Q-network using TD error
  4. Update the option-value function QΩ using TD error with both extrinsic and intrinsic rewards
  5. Update termination functions using the gradient of QΩ

- **Design tradeoffs:**
  - Fixed vs. learned α: Fixed α simplifies training but may not adapt well to different tasks; learned α adds complexity but can adapt
  - Predefined vs. learned intra-policies: Predefined policies simplify training but limit flexibility; learned policies are more adaptable but harder to train
  - Intrinsic reward choice: RND is simple and effective for sparse rewards; pseudo-count is better for image-based environments

- **Failure signatures:**
  - Poor exploration: If termination probabilities for exploration strategies remain low, the agent may under-explore
  - Over-exploration: If termination probabilities for the greedy policy remain high, the agent may fail to exploit learned knowledge
  - Instability: If intrinsic rewards are too large, they may dominate extrinsic rewards and destabilize learning

- **First 3 experiments:**
  1. **Empty-16x16 with center goal:** Verify that the greedy policy termination probability decreases over time, and that exploration strategies are used early
  2. **MiniGrid DoorKey-8x8:** Test whether the framework can learn to use PEM exploration to find the key and then switch to greedy exploitation to reach the goal
  3. **Atari Enduro:** Evaluate whether the framework can learn to balance exploration (avoiding collisions) and exploitation (overtaking cars) in a continuous control environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LESSON compare to other advanced exploration methods like Curiosity-driven Exploration by Self-supervised Prediction (CESP) or Disagreement-based Exploration (DBE) on the MiniGrid and Atari environments?
- Basis in paper: [inferred] The paper compares LESSON to several baselines including ϵ-greedy, ϵz-greedy, ϵr-greedy, RND-based DQN, and EWC. However, it does not compare LESSON to other advanced exploration methods like CESP or DBE.
- Why unresolved: The paper does not provide a comparison between LESSON and other advanced exploration methods.
- What evidence would resolve it: Experimental results comparing the performance of LESSON to CESP, DBE, and other advanced exploration methods on the MiniGrid and Atari environments.

### Open Question 2
- Question: How does the performance of LESSON vary with the number of intra-policies (N) in the option-critic model?
- Basis in paper: [inferred] The paper uses N = 4 intra-policies in the option-critic model for LESSON. However, it does not explore the impact of varying the number of intra-policies on the performance of LESSON.
- Why unresolved: The paper does not provide an ablation study on the number of intra-policies in the option-critic model.
- What evidence would resolve it: Experimental results showing the performance of LESSON with different values of N (e.g., N = 2, 3, 5, 6) on the MiniGrid and Atari environments.

### Open Question 3
- Question: How does the performance of LESSON vary with the coefficient α controlling the trade-off between extrinsic and intrinsic rewards in the objective function for the behavior policy?
- Basis in paper: [explicit] The paper investigates the impact of α on the performance of LESSON in Section 4.2. However, it does not provide a comprehensive study on the optimal value of α for different tasks.
- Why unresolved: The paper only provides a qualitative discussion on the impact of α on the performance of LESSON.
- What evidence would resolve it: Experimental results showing the performance of LESSON with different values of α on the MiniGrid and Atari environments, along with an analysis of the optimal value of α for each task.

## Limitations
- Performance depends critically on the diversity and quality of predefined exploration strategies
- Computational overhead of maintaining multiple intra-policies may limit scalability
- Effectiveness relies on intrinsic rewards remaining reliable across different environment types

## Confidence

- **High confidence**: The option-critic architecture enables adaptive selection between exploration and exploitation strategies, as demonstrated by consistent performance improvements over baselines
- **Medium confidence**: The use of prediction-error-based intrinsic rewards effectively encourages exploration, but the robustness of this mechanism across diverse environments needs further validation
- **Medium confidence**: The inclusion of the greedy policy as an intra-policy provides a stable baseline for exploitation, though its effectiveness depends on early learning progress

## Next Checks

1. **Ablation study on intra-policy diversity**: Remove one or more exploration strategies (e.g., PEM or TE-random) and evaluate whether LESSON still outperforms baselines. This will test the importance of strategy diversity.

2. **Termination function analysis**: Track the termination probabilities of each intra-policy over training steps to verify that exploration strategies are used early and exploitation strategies dominate later. This will validate the adaptive exploration-exploitation claim.

3. **Robustness to reward density**: Evaluate LESSON on environments with varying reward sparsity (e.g., MiniGrid with different goal placements) to test whether the intrinsic reward mechanism remains effective across tasks.