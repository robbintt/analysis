---
ver: rpa2
title: Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive
  Learning
arxiv_id: '2307.03486'
source_url: https://arxiv.org/abs/2307.03486
tags:
- achievement
- collect
- wood
- achievements
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses discovering hierarchical achievements in procedurally
  generated environments, which requires generalization and long-term reasoning. Prior
  model-based or hierarchical approaches demand excessive environment interactions
  or large model sizes.
---

# Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning

## Quick Facts
- arXiv ID: 2307.03486
- Source URL: https://arxiv.org/abs/2307.03486
- Reference count: 40
- Achieves 21.79% score on Crafter using only 4% of parameters compared to previous state-of-the-art

## Executive Summary
This paper addresses the challenge of discovering hierarchical achievements in procedurally generated environments, which requires generalization and long-term reasoning capabilities. The authors demonstrate that Proximal Policy Optimization (PPO), when optimized with recent implementation practices, outperforms previous model-based and hierarchical approaches. They introduce a novel contrastive learning method called achievement distillation that strengthens the agent's ability to predict the next achievement by distilling relevant information from episodes. Their approach achieves state-of-the-art performance on the Crafter environment while using significantly fewer parameters than competing methods.

## Method Summary
The method combines PPO with recent implementation improvements (larger network, layer normalization, value normalization) and augments it with a contrastive learning-based achievement distillation module. The agent collects episodes and identifies achievements as transitions between states, then uses contrastive objectives to learn representations that predict the next achievement. The approach employs both intra-trajectory prediction (learning to predict the next achievement from state-action pairs) and cross-trajectory matching (using optimal transport to align achievement sequences from different episodes). The contrastive objectives are trained alternately with the policy updates, creating representations that capture hierarchical achievement structure while maintaining policy performance.

## Key Results
- Achieves 21.79% geometric mean score on Crafter, state-of-the-art performance
- Uses only 1M parameters (4% of previous state-of-the-art model size)
- 73.6% achievement prediction accuracy, 28.7 percentage points higher than PPO baseline
- Superior performance compared to MuZero + SPR which uses pre-collected exploratory data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PPO with recent implementation practices outperforms prior model-based/hierarchical methods for hierarchical achievements
- **Mechanism:** Implementation optimizations (larger network, layer normalization, value normalization) significantly improve PPO's representation learning capacity for procedurally generated environments
- **Core assumption:** The improvements from implementation practices transfer effectively from single-environment RL to hierarchical achievement discovery
- **Evidence anchors:**
  - "PPO, a simple yet versatile model-free algorithm, outperforms previous methods when optimized with recent implementation practices"
  - "we modify the default ResNet architecture in IMPALA as follows: • Network size: we increase the channel size from [16, 32, 32] to [64, 128, 128] and the hidden dimension from 256 to 1024. • Layer normalization: we apply layer normalization before each dense or convolutional layer [3]. • Value normalization: we maintain the moving average of mean and standard deviation of value function targets and update the value network to predict the normalized targets."
- **Break condition:** If the implementation improvements don't generalize to procedural generation or if the gains come from overfitting to Crafter's specific structure

### Mechanism 2
- **Claim:** Contrastive learning between state-action pairs and next achievements improves prediction accuracy
- **Mechanism:** The intra-trajectory contrastive objective (Lpred) maximizes similarity between (st, at) representations and the next achievement representation, creating a learned embedding space where achievement progression is geometrically encoded
- **Core assumption:** Achievements can be represented as differences between consecutive state embeddings (νθ(g+) = ϕθ(st+1) - ϕθ(st))
- **Evidence anchors:**
  - "we regard g+ as the anchor and (st, at) as the positive... minimize the following contrastive loss to maximize the cosine similarity between the representations of the anchor and positive"
  - "Our method exhibits a strong capacity for discovering hierarchical achievements and shows state-of-the-art performance on the challenging Crafter environment"
  - "our method achieves a classification accuracy of 73.6%, which is a 28.7%p increase compared to PPO"
- **Break condition:** If the achievement representation as state differences fails to capture semantic meaning or if the contrastive objective creates degenerate solutions

### Mechanism 3
- **Claim:** Cross-trajectory matching via optimal transport creates generalizable achievement representations
- **Mechanism:** Partial optimal transport with entropic regularization finds soft matchings between achievement sequences from different episodes, then contrastive learning on matched pairs creates representations invariant to procedural generation variations
- **Core assumption:** The same achievement has similar representations across different procedurally generated environments, and optimal transport can discover these correspondences without supervision
- **Evidence anchors:**
  - "we leverage the common achievement structure shared across all episodes... compute a soft-matching T between them using partial optimal transport"
  - "we use contrastive representation learning to train the encoder to produce similar representations for the matched achievements"
  - "our method exhibits superior score performance compared to MuZero + SPR, which utilizes pre-collected exploratory data"
- **Break condition:** If optimal transport matching fails in early training or if the representations become too generic to distinguish achievements

## Foundational Learning

- **Concept: Contrastive representation learning**
  - Why needed here: To learn semantically meaningful representations of achievements without explicit labels
  - Quick check question: How does the temperature parameter λ in the contrastive loss affect the learned representations?

- **Concept: Optimal transport for sequence matching**
  - Why needed here: To match achievement sequences from different episodes without supervision
  - Quick check question: Why use partial optimal transport with total mass min{m,n} instead of full transport?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: Forms the base RL algorithm that the contrastive learning augments
  - Quick check question: What's the purpose of the clipped surrogate objective in PPO?

## Architecture Onboarding

- **Component map:** PPO backbone (policy/value networks with shared encoder) -> Achievement distillation module (contrastive objectives + optimal transport) -> Memory mechanism (previous achievement representations concatenated to state)

- **Critical path:** PPO training → achievement collection → contrastive updates → improved representations → better policy

- **Design tradeoffs:**
  - Model size vs performance (1M params still beats baselines)
  - Auxiliary training frequency vs sample efficiency
  - Hard vs soft matching for cross-trajectory alignment

- **Failure signatures:**
  - Performance plateaus early → check if contrastive objectives are being optimized
  - Achievement prediction accuracy stays low → verify representation similarity
  - Training instability → check regularizers and learning rates

- **First 3 experiments:**
  1. Test PPO baseline with implementation improvements only
  2. Add intra-trajectory contrastive learning without cross-trajectory matching
  3. Add cross-trajectory matching with random achievement representations (no contrastive learning)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the achievement distillation method perform when applied to other procedurally generated environments beyond Crafter?
- Basis in paper: [explicit] The authors demonstrate state-of-the-art performance on Crafter but do not evaluate transferability to other environments
- Why unresolved: The method is specifically designed for hierarchical achievement discovery, and its effectiveness on different environment types remains unknown
- What evidence would resolve it: Testing achievement distillation on other procedurally generated environments like Minigrid or DMLab would show if the method generalizes

### Open Question 2
- Question: What is the minimum amount of environment interactions needed for the contrastive learning objectives to be effective?
- Basis in paper: [inferred] The method uses a buffer of episodes collected during policy updates, but the relationship between buffer size and learning effectiveness is not explored
- Why unresolved: The paper does not conduct experiments varying the amount of collected data or the frequency of auxiliary training phases
- What evidence would resolve it: Systematic experiments varying buffer sizes and training frequencies would reveal the minimum data requirements

### Open Question 3
- Question: How does the optimal transport matching algorithm handle situations where the achievement sequences have significantly different lengths or structures?
- Basis in paper: [explicit] The authors mention using partial optimal transport with a hard threshold of 0.5, but do not analyze edge cases
- Why unresolved: The paper only shows examples with relatively similar achievement sequences and does not explore scenarios with divergent episode trajectories
- What evidence would resolve it: Testing the matching algorithm on episodes with varying achievement counts and structural differences would reveal its robustness limitations

### Open Question 4
- Question: What is the impact of different temperature parameters (λ) on the contrastive learning objectives?
- Basis in paper: [inferred] The authors use a fixed temperature parameter in their contrastive losses but do not explore its sensitivity
- Why unresolved: The paper does not conduct ablation studies varying the temperature parameter to understand its effect on learning performance
- What evidence would resolve it: Systematic experiments varying λ across different ranges would reveal its impact on achievement prediction accuracy and overall performance

## Limitations

- Limited evaluation to a single procedurally generated environment (Crafter)
- Reliance on optimal transport matching assumes achievement semantic consistency across procedural variations
- No analysis of performance degradation with increased achievement hierarchy complexity

## Confidence

- **High confidence**: Implementation improvements for PPO (layer normalization, value normalization, larger network)
- **Medium confidence**: Contrastive learning approach for achievement prediction
- **Low confidence**: Generalization beyond Crafter to other procedurally generated environments

## Next Checks

1. Test the method on environments with more varied procedural generation where achievement structures may not align perfectly across episodes
2. Evaluate performance with different representations of achievements (e.g., using action differences or custom feature embeddings instead of state differences)
3. Compare against alternative unsupervised achievement discovery methods that don't rely on optimal transport matching