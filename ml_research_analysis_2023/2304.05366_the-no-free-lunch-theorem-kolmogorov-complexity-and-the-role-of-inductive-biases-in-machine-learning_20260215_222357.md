---
ver: rpa2
title: The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive
  Biases in Machine Learning
arxiv_id: '2304.05366'
source_url: https://arxiv.org/abs/2304.05366
tags:
- complexity
- learning
- data
- free
- lunch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: No free lunch theorems state that no learner can solve all problems,
  seemingly necessitating specialized models for different tasks. This paper challenges
  that view by showing real-world data is highly compressible and shares low complexity
  across domains.
---

# The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning

## Quick Facts
- arXiv ID: 2304.05366
- Source URL: https://arxiv.org/abs/2304.05366
- Reference count: 40
- No free lunch theorems state that no learner can solve all problems, seemingly necessitating specialized models for different tasks

## Executive Summary
This paper challenges the conventional interpretation of No Free Lunch theorems by showing that real-world data is highly compressible and shares low complexity across domains. The authors demonstrate that neural networks, including randomly initialized ones, naturally prefer low-complexity solutions, which explains their strong generalization performance. A single learner with a soft simplicity bias can perform well across tasks and dataset sizes, eliminating the need for specialized models. The work provides PAC-Bayes bounds that explain cross-domain generalization without requiring problem-specific priors.

## Method Summary
The core method combines multiple architectures (small and large models like GoogLeNet and ViT) using a convex combination of their logits, with weight decay on the combination parameter to encourage the use of simpler solutions. This approach is tested on CIFAR-10, CIFAR-100, and ImageNet datasets, with CIFAR images resized to 224x224 for compatibility. Models are trained using SGD with momentum 0.9 and learning rate 0.1 with cosine annealing. The combined model is trained for 10 epochs with frozen model parameters, using SGD with momentum 0.9, learning rate 0.1 with cosine annealing, and weight decay on the combination parameter.

## Key Results
- Models like GPT-3 and CNNs generalize well on diverse data due to their low-complexity preference
- Combining models with simplicity penalties achieves strong performance on both small and large datasets
- PAC-Bayes bounds explain cross-domain generalization without requiring problem-specific priors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-world data shares low Kolmogorov complexity across domains, making universal learners possible
- Mechanism: The NFL theorem assumes uniform distribution over all possible datasets, which are exponentially unlikely to be compressible. Real-world datasets are highly structured and compressible, violating this assumption. Neural networks naturally prefer low-complexity solutions, aligning with the structure of real data.
- Core assumption: The structure shared by real-world data is sufficiently common to make a single learner effective across domains
- Evidence anchors:
  - [abstract] "real-world problems disproportionately generate low-complexity data"
  - [section] "we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains"
  - [corpus] "Found 25 related papers... Top related titles: Separable Power of Classical and Quantum Learning Protocols Through the Lens of No-Free-Lunch Theorem"
- Break condition: If real-world data distributions shift to be more uniformly random or if neural networks lose their simplicity bias

### Mechanism 2
- Claim: A single expressive model with soft simplicity bias can perform well across different dataset sizes
- Mechanism: Instead of selecting different models for small vs large datasets, combine multiple architectures with a preference for simpler solutions. This allows the model to automatically adapt - using simpler components when they suffice, but scaling up when needed.
- Core assumption: The complexity of the optimal solution correlates with dataset size in a predictable way
- Evidence anchors:
  - [abstract] "a single learner with a soft simplicity bias can perform well across tasks and dataset sizes"
  - [section] "combining them with a preference for simplicity achieves the best of both worlds" (referring to GoogLeNet and ViT)
  - [corpus] "No-Free-Lunch Theories for Tensor-Network Machine Learning Models"
- Break condition: If the relationship between dataset complexity and optimal model complexity becomes too irregular or unpredictable

### Mechanism 3
- Claim: PAC-Bayes generalization bounds can explain cross-domain generalization without problem-specific priors
- Mechanism: The universal prior based on Kolmogorov complexity assigns higher probability to compressible hypotheses. When applied to neural networks, this explains their generalization even on data far from their design domain (like CNNs on tabular data).
- Core assumption: Neural networks have a bias towards low-complexity solutions that transfers across domains
- Evidence anchors:
  - [abstract] "we use this property to compute cross-domain generalization bounds via Kolmogorov complexity"
  - [section] "Compression based generalization bounds... for CNNs on tabular data... indicate that even CNNs designed for computer vision have a generic inductive bias appropriate for a wide range of datasets"
  - [corpus] "A Survey on Universal Approximation Theorems"
- Break condition: If neural networks lose their inherent simplicity bias or if the relationship between compressibility and generalization breaks down

## Foundational Learning

- Kolmogorov Complexity
  - Why needed here: Provides the mathematical foundation for understanding data compressibility and its relationship to learnability
  - Quick check question: Why does the incompressibility of uniformly random data make learning impossible under the NFL theorem?

- PAC-Bayes Theory
  - Why needed here: Offers a framework for understanding generalization that doesn't require problem-specific priors and explains cross-domain generalization
  - Quick check question: How does the PAC-Bayes framework differ from traditional uniform convergence bounds in explaining neural network generalization?

- No Free Lunch Theorems
  - Why needed here: Understanding the assumptions behind NFL theorems reveals why they don't apply to real-world learning problems
  - Quick check question: What specific assumption in the NFL theorem is violated by real-world data distributions?

## Architecture Onboarding

- Component map:
  Base architectures (CNNs, Transformers, MLPs) -> Simplicity bias mechanism (regularization, architecture combination) -> Compression-based evaluation

- Critical path:
  1. Encode data into appropriate format for base architecture
  2. Train base architecture with simplicity bias regularization
  3. Measure compressibility and generalization bounds
  4. Combine architectures if needed with convex combination and complexity penalty

- Design tradeoffs:
  - Expressiveness vs simplicity bias strength
  - Computational cost of multiple architectures vs single architecture
  - Precision of complexity measurement vs computational feasibility

- Failure signatures:
  - Poor generalization despite high training accuracy (likely overfitting due to insufficient simplicity bias)
  - No improvement from combining architectures (likely need stronger regularization or different base architectures)
  - High compressibility but poor generalization (likely issue with PAC-Bayes bound assumptions)

- First 3 experiments:
  1. Test compressibility of a tabular dataset using a CNN architecture - verify cross-domain generalization
  2. Measure GPT-3's preference for low-complexity sequences using the expression tree language
  3. Combine a small CNN and Transformer on CIFAR-10 with simplicity bias - verify performance across dataset sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design models that are both flexible and simplicity-biased while remaining computationally efficient?
- Basis in paper: [inferred] The paper discusses the potential for designing models that combine flexibility with a soft simplicity bias, but it does not provide concrete solutions for achieving this efficiently.
- Why unresolved: Designing such models requires balancing model expressiveness with computational cost, which is a complex challenge that the paper acknowledges but does not fully address.
- What evidence would resolve it: Experimental results demonstrating the effectiveness and efficiency of models that successfully combine flexibility with a simplicity bias would provide evidence for this question.

### Open Question 2
- Question: How crucial are the implicit biases of optimization procedures in finding simple generalizable solutions?
- Basis in paper: [explicit] The paper discusses the role of implicit biases in optimization procedures and their impact on finding simple solutions, but it does not provide a definitive answer to how crucial these biases are.
- Why unresolved: The relationship between optimization biases and solution simplicity is complex and depends on various factors, making it difficult to determine the exact importance of these biases.
- What evidence would resolve it: Comparative studies analyzing the impact of different optimization procedures on the simplicity and generalization of solutions would provide insights into the importance of implicit biases.

### Open Question 3
- Question: Can a single model effectively handle both small and large training datasets without sacrificing performance?
- Basis in paper: [explicit] The paper suggests that a single model with a flexibility and simplicity bias can potentially work well with both small and large datasets, but it does not provide conclusive evidence for this claim.
- Why unresolved: While the paper presents theoretical arguments and some experimental results, more extensive empirical studies are needed to confirm the effectiveness of a single model approach across different dataset sizes.
- What evidence would resolve it: Extensive empirical studies comparing the performance of a single model approach to specialized models across a wide range of dataset sizes would provide evidence for this question.

## Limitations

- The central claim that real-world data shares low Kolmogorov complexity across domains remains empirically under-supported
- The assumption that neural networks maintain consistent simplicity bias across different domains and architectures requires more rigorous testing
- The relationship between optimization biases and solution simplicity is complex and not fully characterized

## Confidence

- High confidence: The theoretical analysis showing NFL theorems rely on uniform data distributions, which real-world data violates
- Medium confidence: The experimental results demonstrating cross-domain generalization with CNNs and Transformers
- Medium confidence: The PAC-Bayes bounds providing a theoretical explanation for observed generalization

## Next Checks

1. Cross-domain compressibility test: Measure Kolmogorov complexity of tabular datasets using vision architectures (CNNs, Vision Transformers) and compare with architecture-specific models to quantify the universality claim

2. Simplicity bias characterization: Systematically measure and compare the simplicity bias of different neural network architectures across domains using PAC-Bayes bounds and empirical generalization gaps

3. Dataset size scaling experiment: Vary training set sizes systematically and measure the relationship between optimal model complexity and dataset size to validate the proposed combination strategy beyond the current two-architecture setup