---
ver: rpa2
title: 'Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
  for Code Generation'
arxiv_id: '2310.18628'
source_url: https://arxiv.org/abs/2310.18628
tags:
- data
- student
- personalised
- distillation
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces personalised distillation, an approach that
  customizes the distillation process to a student model's abilities by having it
  attempt tasks first, then refining its mistakes using teacher feedback. The method
  contrasts with standard distillation, which trains all students on the same teacher-generated
  data.
---

# Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation

## Quick Facts
- arXiv ID: 2310.18628
- Source URL: https://arxiv.org/abs/2310.18628
- Reference count: 11
- Key outcome: Personalized distillation achieves 36.4% pass@1 on CodeGen-mono-16B and 45.8% pass@1 on StarCoder using only one-third the data of standard distillation

## Executive Summary
This paper introduces personalized distillation, an approach that customizes the distillation process to a student model's abilities by having it attempt tasks first, then refining its mistakes using teacher feedback. The method contrasts with standard distillation, which trains all students on the same teacher-generated data. Personalised distillation collects only one-third the data of standard distillation but achieves superior performance on code generation benchmarks (HumanEval and MBPP), boosting CodeGen-mono-16B by 7% to 36.4% pass@1 and StarCoder by 12.2% to 45.8% pass@1 pass@1. The approach supports both code generation and refinement tasks, benefits from multi-round distillation, and enables execution-feedback-driven self-correction.

## Method Summary
The personalized distillation method involves three key steps: (1) the student model generates code attempts for given tasks, (2) unit tests evaluate these attempts and identify failures, (3) the teacher model (ChatGPT) provides personalized refinements based on the student's specific attempt and execution feedback. The student is then finetuned on this personalized data. The approach includes variants like PERsD-ref (refinement-only), PERsD-gen (generation-only), and PERsD-combined (both tasks). Multi-round distillation further improves performance by collecting additional personalized data after initial finetuning.

## Key Results
- Personalized distillation with one-third the data outperforms standard distillation with full data on both CodeGen and StarCoder models
- PERsD-combined round-2 improves pass@1 by 7% (CodeGen-mono-16B: 36.4%) and 12.2% (StarCoder: 45.8%) over base models
- Personalized data proves significantly more effective than input-personalized distillation (INPD) and self-edit approaches
- Multi-round personalized distillation provides additional modest improvements over single-round approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personalized distillation outperforms standard distillation by adapting the teacher's feedback to the student's current solution attempts.
- Mechanism: The student first generates a solution attempt, then the teacher provides a refinement only when the student's attempt fails unit tests. This refinement is conditioned on the student's attempt, creating a personalized correction pathway.
- Core assumption: The teacher can effectively correct the student's specific mistakes rather than providing generic solutions.
- Evidence anchors: [abstract] "personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution" [section] "the teacher's refinement follows the student's attempt and improves upon it"

### Mechanism 2
- Claim: Collecting fewer personalized examples (one-third the data) achieves better performance than collecting more standard examples.
- Mechanism: The data collection process is more efficient because it only generates refinement data for tasks where the student makes mistakes, filtering out tasks the student can already solve correctly.
- Core assumption: Personalized data is higher quality per example than generic teacher-generated data.
- Evidence anchors: [abstract] "personalised distillation consistently outperforms standard distillation with only one third of the data" [section] "personalized data proves significantly more effective than ILF as we empirically investigate in ยง5"

### Mechanism 3
- Claim: Multi-round distillation provides additional improvements over single-round personalized distillation.
- Mechanism: After finetuning with personalized distillation data, the student model can attempt tasks again, potentially making different mistakes that can be refined in subsequent rounds.
- Core assumption: The student continues to make meaningful mistakes that can be corrected through further personalized feedback.
- Evidence anchors: [section] "PERsD-combined round-2 generally outperforms PERsD-combined round-1 by a modest margin" [abstract] "benefits from multi-round distillation"

## Foundational Learning

- Concept: Execution-based feedback evaluation
  - Why needed here: The approach relies on executing generated code against unit tests to determine whether the student's attempt is correct and what specific errors need correction.
  - Quick check question: How does the system determine if a student's code generation attempt is correct before asking for teacher refinement?

- Concept: Adaptive prompting and instruction-following
  - Why needed here: The teacher model needs to understand both the task and the student's specific attempt to provide meaningful refinement, requiring sophisticated prompt engineering.
  - Quick check question: What information must be included in the teacher's prompt to ensure the refinement is personalized to the student's attempt?

- Concept: Code refinement as a distinct task from code generation
  - Why needed here: The approach treats code refinement as a separate task type, requiring the model to learn how to modify existing code rather than generate from scratch.
  - Quick check question: How does the model learn the difference between generating new code and refining existing code?

## Architecture Onboarding

- Component map: Student model -> Unit test executor -> Teacher model (ChatGPT) -> Data pipeline -> Training loop
- Critical path: 1. Generate task instruction 2. Student generates attempt 3. Execute attempt against unit tests 4. If failed, request teacher refinement with student's attempt and feedback 5. Collect valid refinement data 6. Finetune student on collected data
- Design tradeoffs: Data efficiency vs. coverage: Personalized distillation collects fewer examples but may miss some task variations; Teacher dependency: Requires access to a capable teacher model for refinement; Execution overhead: Each attempt requires running unit tests, adding latency
- Failure signatures: Student makes no mistakes: No data collected, no improvement; Teacher cannot refine: Collection pipeline stalls; Execution errors: Pipeline fails if test cases cannot be parsed or executed
- First 3 experiments: 1. Verify that the student model can generate code attempts that fail unit tests 2. Test that the teacher model can provide meaningful refinements given student attempts and feedback 3. Confirm that the collected data format is correct for finetuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of personalized distillation scale with increasing data size?
- Basis in paper: [inferred] The paper mentions that personalized distillation has been proven to support more effective and efficient learning, and it is intriguing to investigate how well it scales with data size. They hypothesize that if personalized distillation data is scaled to 50K, PERsD methods would receive more performance gain compared to InpD and StanD with the scaling of data size.
- Why unresolved: The paper only conducted experiments based on the same 10K DSTAND data and the corresponding personalized data processed from DSTAND are of size 2-3K. They did not explore the performance of personalized distillation with larger data sizes.
- What evidence would resolve it: Conducting experiments with personalized distillation data scaled to 50K and comparing the performance gain of PERsD methods with InpD and StanD methods.

### Open Question 2
- Question: Can an online version of personalized distillation be developed to maximize its benefits?
- Basis in paper: [explicit] The paper mentions that after finetuning the student model with personalized distillation data, performing another round of personalized distillation continues to improve the model. This observation suggests the potential of an online version of personalized distillation, which collects a batch of personalized data on-the-fly with the teacher model, after each optimization step during finetuning.
- Why unresolved: The paper did not explore the development of an online version of personalized distillation and its potential benefits.
- What evidence would resolve it: Developing and testing an online version of personalized distillation that collects personalized data on-the-fly with the teacher model during finetuning, and comparing its performance with the current approach.

### Open Question 3
- Question: How does personalized distillation compare to other feedback-based code generation models like ILF and Self-Edit?
- Basis in paper: [explicit] The paper mentions that they compared PERsD-variants with ILF and Self-Edit models. They found that PERsD-variants significantly outperform ILF by 11.8% at pass@1 at a cost 1e-4 times lower than ILF. They also found that PERsD-models are more effective than Self-Edit style models.
- Why unresolved: The paper only compared PERsD-variants with ILF and Self-Edit models in specific settings. It is unclear how personalized distillation would perform in other settings or with different models.
- What evidence would resolve it: Conducting more comprehensive experiments comparing personalized distillation with other feedback-based code generation models in various settings and with different models.

## Limitations

- The paper lacks detailed information about ChatGPT prompt templates, making it difficult to verify the personalization mechanism
- Evaluation relies heavily on pass@k metrics which may not capture semantic correctness or code quality
- Comparison with standard distillation uses one-third the data, making it unclear whether improvements stem from personalization or more training examples
- Results are limited to specific open-source models (CodeGen-mono and StarCoder) and a single teacher model (ChatGPT)

## Confidence

**High confidence**: The paper clearly demonstrates that personalized distillation can improve code generation performance compared to standard distillation when using the same data volume, and that execution-based feedback provides meaningful signals for refinement.

**Medium confidence**: The claim that personalized distillation with one-third the data outperforms standard distillation with full data is supported by the experiments, but the mechanism by which personalization creates efficiency gains could benefit from more rigorous ablation studies.

**Low confidence**: The paper's claims about multi-round distillation providing consistent improvements are based on limited experimental evidence and lack statistical significance testing across multiple runs.

## Next Checks

1. **Prompt template validation**: Obtain and test the exact ChatGPT prompt templates used for collecting personalized refinement data to verify that the teacher's feedback truly adapts to student-specific mistakes rather than providing generic corrections.

2. **Data efficiency ablation**: Conduct experiments comparing personalized distillation against standard distillation using equal amounts of data to isolate whether improvements come from personalization or simply having more training examples.

3. **Semantic correctness evaluation**: Beyond pass@k metrics, implement semantic similarity scoring between student attempts and teacher refinements to verify that the personalization mechanism is capturing meaningful code improvements rather than superficial test-passing strategies.