---
ver: rpa2
title: 'AI planning in the imagination: High-level planning on learned abstract search
  spaces'
arxiv_id: '2308.08693'
source_url: https://arxiv.org/abs/2308.08693
tags:
- learning
- agent
- search
- planning
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PiZero introduces a method for agents to perform high-level planning
  in a learned abstract search space that is completely decoupled from the real environment.
  Unlike prior methods, this enables agents to reason in terms of compound or temporally-extended
  actions, which is useful in environments where many fine-grained actions are needed
  to perform relevant tasks.
---

# AI planning in the imagination: High-level planning on learned abstract search spaces

## Quick Facts
- arXiv ID: 2308.08693
- Source URL: https://arxiv.org/abs/2308.08693
- Authors: 
- Reference count: 13
- Primary result: PiZero enables agents to perform high-level planning in learned abstract search spaces, outperforming prior methods without requiring environment simulators

## Executive Summary
PiZero introduces a method for agents to perform high-level planning in a learned abstract search space that is completely decoupled from the real environment. Unlike prior methods, this enables agents to reason in terms of compound or temporally-extended actions, which is useful in environments where many fine-grained actions are needed to perform relevant tasks. The method augments a standard neural-network-based agent with a planning module that it can query before selecting actions in the real environment. The planning module uses a learned dynamics model and a learned prediction model to perform Monte Carlo Tree Search in the abstract environment. The agent's parameters are optimized using evolution strategies, which do not require gradients. Experiments on multiple domains, including the traveling salesman problem, Sokoban, and a collection problem, demonstrate that PiZero outperforms comparable prior methods without assuming access to an environment simulator.

## Method Summary
PiZero augments neural-network-based agents with a planning module that performs Monte Carlo Tree Search in a learned abstract search space. The abstract space is completely decoupled from the real environment, allowing compound actions at arbitrary timescales. The method uses a representation function to map real observations to abstract states, a dynamics model to predict transitions in the abstract space, and a prediction model to estimate values and action probabilities. These components work together to guide MCTS planning. Parameters are optimized using OpenAI-ES, which handles the nondifferentiable MCTS subroutine by estimating gradients through function evaluations.

## Key Results
- PiZero enables planning in abstract search spaces decoupled from real environments
- The method allows compound actions at arbitrary timescales, useful for environments requiring many fine-grained actions
- Outperforms comparable prior methods on traveling salesman, Sokoban, and collection problems without requiring environment simulators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The abstract search space decouples planning from the real environment, allowing compound actions at arbitrary timescales.
- Mechanism: By learning a dynamics model that maps abstract states and actions to rewards and new abstract states, the agent can reason in a space where each action can represent many real micro-actions. MCTS operates in this abstract space, enabling efficient high-level planning without being constrained by the granularity of the real environment.
- Core assumption: The learned dynamics model captures the essential relationships between abstract states and actions, making the abstract space useful for planning even if it's not a perfect replica of the real environment.
- Evidence anchors:
  - [abstract] "proposes a new method, called PiZero, that gives an agent the ability to plan in an abstract search space that the agent learns during training, which is completely decoupled from the real environment"
  - [section 3.1] "The states, actions, and rewards of the planning module are purely abstract and do not need to have any direct relation or correspondence with those of the real environment"
- Break condition: If the learned dynamics model fails to capture meaningful state transitions, MCTS in the abstract space will not provide useful guidance for real environment actions.

### Mechanism 2
- Claim: The agent uses OpenAI-ES for optimization, which works despite the nondifferentiable MCTS subroutine.
- Mechanism: OpenAI-ES estimates the gradient by evaluating the expected return at random perturbations of the parameters and averaging the results. This avoids the need to differentiate through the MCTS process, making it possible to optimize policies with nondifferentiable components.
- Core assumption: The gradient estimates provided by OpenAI-ES are sufficiently accurate to guide the learning process toward better policies.
- Evidence anchors:
  - [section 3.3] "We use OpenAI-ES... which computes an estimate of the smoothed gradient at θ as follows: g = 1/nσ ∑(i=1 to n) yiεi where εi ~ N(0, I), yi = F(θ + σεi)"
  - [section 3.3] "Evolution strategies have been shown to be a scalable alternative to standard reinforcement learning"
- Break condition: If the batch size is too small or the standard deviation is poorly chosen, the gradient estimates will have high variance and optimization will be ineffective.

### Mechanism 3
- Claim: The learned representation function and prediction model work together to guide MCTS in the abstract space.
- Mechanism: The representation function maps the real memory-observation input to an abstract state, which is then fed into the prediction model. The prediction model estimates both the value of the abstract state and the probabilities of taking each abstract action. These predictions guide the MCTS search by informing which branches to explore more deeply.
- Core assumption: The prediction model learns to accurately estimate both the value and action probabilities in the abstract space, making MCTS more efficient.
- Evidence anchors:
  - [section 3.2] "The prediction function maps a state to a predicted value and action logits... The planner runs an MCTS algorithm on the resulting abstract search space, using the prediction function to guide the search"
  - [section 3.2] "The representation function maps a memory-observation input to a root state"
- Break condition: If the prediction model's estimates are inaccurate, MCTS will waste simulations exploring unpromising branches or miss promising ones.

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS is the core search algorithm that explores the abstract state space and finds good action sequences. Understanding its mechanics (selection, expansion, simulation, backpropagation) is crucial for debugging and improving the planner.
  - Quick check question: How does MCTS balance exploration and exploitation during the selection phase?

- Concept: Evolution Strategies (ES) optimization
  - Why needed here: Since the policy contains a nondifferentiable MCTS subroutine, standard gradient-based methods won't work. ES provides a way to optimize such policies by estimating gradients through function evaluations.
  - Quick check question: What role does the standard deviation parameter play in the OpenAI-ES gradient estimator?

- Concept: Representation learning
  - Why needed here: The agent must learn to map complex, high-dimensional observations into a compact abstract state representation that is useful for planning. This requires understanding how neural networks can learn meaningful embeddings.
  - Quick check question: Why might a GRU be chosen over a simpler RNN for the representation function in partially observable environments?

## Architecture Onboarding

- Component map:
  Observation encoder -> Memory updater (GRU) -> Representation function -> Prediction model -> MCTS planner -> Actor network -> Real action

- Critical path: Observation → Encoder → Memory updater → Representation function → Prediction model → MCTS planner → Actor → Real action → Environment → Observation

- Design tradeoffs:
  - Abstract state dimensionality: Higher dimensions may capture more information but increase computational cost
  - Number of abstract actions: More actions allow finer-grained planning but make MCTS slower
  - MCTS simulation budget: More simulations improve plan quality but increase planning time per step
  - Evolution strategy parameters: Learning rate and standard deviation must balance exploration and exploitation in the parameter space

- Failure signatures:
  - If the agent fails to improve: Check if OpenAI-ES gradient estimates have high variance (increase batch size or adjust standard deviation)
  - If planning is too slow: Reduce MCTS simulation budget or abstract action space size
  - If the agent learns a poor policy: Verify that the representation function is capturing relevant information from observations and memory
  - If the agent overfits to specific instances: Increase diversity in training environments or add regularization

- First 3 experiments:
  1. Verify the abstract dynamics model can predict next states: Initialize a trained agent, take a step in the real environment, and check if the abstract dynamics model's prediction of the next abstract state is reasonable.
  2. Test MCTS planning quality: Run MCTS with a fixed abstract dynamics and prediction model, and visualize the search tree to see if it's exploring meaningful branches.
  3. Check OpenAI-ES gradient estimates: Run a small-scale optimization with a known test function (e.g., sphere function) to verify that the gradient estimates are accurate enough to find the optimum.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PiZero's performance compare to model-based RL methods that use supervised training against the real environment's values, rewards, and discounts?
- Basis in paper: [explicit] The paper states that PiZero completely decouples the dynamics and evaluation models from the real environment, unlike VPN which trains its predicted values, rewards, and discounts to match those of the real environment.
- Why unresolved: The paper only compares PiZero to prior methods like AlphaZero and MuZero, not to model-based RL methods with supervised training.
- What evidence would resolve it: Empirical experiments comparing PiZero's performance to model-based RL methods with supervised training on various environments.

### Open Question 2
- Question: How does the choice of black-box optimization algorithm impact PiZero's performance and sample efficiency?
- Basis in paper: [explicit] The paper uses OpenAI-ES for optimization, but mentions other black-box optimization algorithms like NES variants and those using more sophisticated queries of the black-box objective.
- Why unresolved: The paper only experiments with OpenAI-ES and does not explore the impact of different black-box optimization algorithms on performance.
- What evidence would resolve it: Empirical experiments comparing the performance and sample efficiency of PiZero using different black-box optimization algorithms on various environments.

### Open Question 3
- Question: How does PiZero scale to environments with complex action and observation spaces, such as high-dimensional arrays like images?
- Basis in paper: [inferred] The paper mentions that some environments may have complex action and observation spaces that require more sophisticated network architectures, but does not experimentally evaluate PiZero on such environments.
- Why unresolved: The experiments in the paper are limited to environments with relatively simple action and observation spaces.
- What evidence would resolve it: Empirical experiments evaluating PiZero's performance on environments with complex action and observation spaces, such as those involving high-dimensional arrays like images.

## Limitations

- Experiments were conducted primarily on relatively small, discrete environments rather than complex continuous control problems, raising questions about scalability to real-world domains
- The paper doesn't provide thorough analysis of what the learned abstract representations actually capture or whether they generalize beyond the training distribution
- Performance sensitivity to OpenAI-ES hyperparameters (population size, learning rate, standard deviation) is not well-characterized through ablation studies

## Confidence

- Planning in abstract spaces enables compound actions: High
- OpenAI-ES successfully optimizes the architecture: Medium
- Abstract representations generalize to unseen problems: Low
- Performance improvements are robust across domains: Medium

## Next Checks

1. Evaluate PiZero on continuous control benchmarks (e.g., DeepMind Control Suite) to test scalability beyond discrete action spaces
2. Perform ablation studies varying OpenAI-ES hyperparameters to quantify sensitivity to optimization settings
3. Analyze the learned abstract representations using techniques like t-SNE to understand what features they capture and whether they exhibit meaningful structure