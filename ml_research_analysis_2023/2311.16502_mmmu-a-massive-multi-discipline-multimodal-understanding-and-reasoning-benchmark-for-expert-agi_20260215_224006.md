---
ver: rpa2
title: 'MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark
  for Expert AGI'
arxiv_id: '2311.16502'
source_url: https://arxiv.org/abs/2311.16502
tags:
- error
- image
- back
- case
- gpt-4v
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMMU is a new benchmark designed to evaluate multimodal models
  on massive multi-discipline tasks requiring college-level subject knowledge and
  deliberate reasoning. It includes 11.5K carefully collected multimodal questions
  from college exams, quizzes, and textbooks, covering six core disciplines and 30
  subjects with 30 highly heterogeneous image types.
---

# MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI

## Quick Facts
- **arXiv ID**: 2311.16502
- **Source URL**: https://arxiv.org/abs/2311.16502
- **Reference count**: 40
- **Key outcome**: New benchmark designed to evaluate multimodal models on massive multi-discipline tasks requiring college-level subject knowledge and deliberate reasoning

## Executive Summary
MMMU is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks requiring college-level subject knowledge and deliberate reasoning. It includes 11.5K carefully collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines and 30 subjects with 30 highly heterogeneous image types. The evaluation of 14 open-source LMMs and proprietary GPT-4V and Gemini reveals significant challenges, with even the advanced GPT-4V achieving only 56% accuracy, indicating substantial room for improvement. MMMU aims to stimulate the development of next-generation multimodal foundation models towards expert artificial general intelligence.

## Method Summary
The MMMU benchmark evaluates multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. It includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering), 30 subjects, and 183 subfields with 30 highly heterogeneous image types. The evaluation metric is micro-averaged accuracy. The benchmark measures three essential skills in LMMs: perception, knowledge, and reasoning. The evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on the benchmark.

## Key Results
- MMMU includes 11.5K multimodal questions spanning 6 disciplines, 30 subjects, and 183 subfields
- Evaluation of 14 open-source LMMs and proprietary GPT-4V and Gemini shows significant challenges
- Even advanced GPT-4V achieves only 56% accuracy, indicating substantial room for improvement
- Models struggle particularly with domain-specific visual data like chemical structures and medical images

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MMMU's questions require deep integration of multimodal perception and subject-specific knowledge.
- **Mechanism**: The benchmark presents complex images (diagrams, charts, medical scans, etc.) alongside interleaved text, forcing models to jointly interpret visual and textual information using domain expertise rather than relying on surface-level visual cues or OCR alone.
- **Core assumption**: Advanced multimodal reasoning is needed when visual data is heterogeneous and requires specialized knowledge to decode.
- **Evidence anchors**:
  - [abstract]: "MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks... comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures."
  - [section]: "MMMU also presents two unique challenges absent in current benchmarks... Firstly, it covers diverse image formats, from visual scenes like photographs and paintings to diagrams and tables, testing the perceptual capabilities of LMMs."
- **Break condition**: If models could solve tasks using only basic visual understanding or external tools (OCR, captioning) without deep domain reasoning, this mechanism would fail.

### Mechanism 2
- **Claim**: MMMU's college-level subject coverage ensures evaluation of breadth and depth comparable to skilled adult expertise.
- **Mechanism**: By spanning 6 disciplines, 30 subjects, and 183 subfields, MMMU tests models across a wide range of specialized domains, requiring both broad knowledge and deep reasoning within each area, similar to what skilled adults demonstrate in their fields.
- **Core assumption**: Breadth and depth of subject knowledge are essential components of expert-level performance.
- **Evidence anchors**:
  - [abstract]: "These questions span 30 subjects and 183 subfields... many problems within MMMU require expert-level reasoning, such as applying 'Fourier Transform' or 'Equilibrium Theory' to derive the solution."
  - [section]: "Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts."
- **Break condition**: If models could achieve high scores by memorizing patterns or using shallow knowledge across domains without true understanding, this mechanism would fail.

### Mechanism 3
- **Claim**: MMMU's interleaved text-image format prevents simple visual recognition and demands integrated understanding.
- **Mechanism**: Unlike benchmarks with separate text and image inputs, MMMU's interleaved format requires models to process and relate information across modalities in context, preventing reliance on isolated visual or textual processing.
- **Core assumption**: Integrated multimodal processing is more challenging and representative of real-world expert tasks than separate modality processing.
- **Evidence anchors**:
  - [abstract]: "MMMU features interleaved text-image inputs. A model needs to jointly understand the images and text, which often requires recalling deep subject knowledge, and conducting complex reasoning based on the understanding and knowledge to reach a solution."
  - [section]: "Secondly, MMMU features interleaved text-image inputs. A model needs to jointly understand the images and text..."
- **Break condition**: If models could achieve similar performance with separate text and image processing pipelines, this mechanism would fail.

## Foundational Learning

- **Concept**: College-level subject knowledge and reasoning
  - Why needed here: MMMU tests expert-level understanding across diverse academic domains, requiring knowledge typically acquired through formal education in each subject area.
  - Quick check question: Can you explain how to apply Fourier Transform to solve a differential equation, or how Equilibrium Theory works in economics?

- **Concept**: Multimodal integration and joint perception
  - Why needed here: MMMU's interleaved text-image format demands models process and relate information across modalities simultaneously, not as separate streams.
  - Quick check question: Given a diagram of a chemical reaction with accompanying text describing the process, can you explain how the visual and textual information complement each other to fully understand the reaction mechanism?

- **Concept**: Advanced reasoning with domain-specific knowledge
  - Why needed here: Many MMMU questions require applying specialized knowledge to solve complex problems, not just recalling facts or performing basic inference.
  - Quick check question: If shown a medical image with a patient description, can you diagnose the condition by integrating visual findings with clinical knowledge and reasoning through possible causes?

## Architecture Onboarding

- **Component map**: Input processing pipeline → Multimodal fusion layer → Knowledge retrieval module → Reasoning engine → Output generator
- **Critical path**: Multimodal fusion layer → Knowledge retrieval module → Reasoning engine (this is where most of the challenge lies)
- **Design tradeoffs**: Balancing depth of domain knowledge vs. breadth of coverage, computational efficiency vs. reasoning capability, visual feature extraction vs. text understanding
- **Failure signatures**: Poor performance on questions requiring domain-specific knowledge, inability to integrate visual and textual information, reliance on simple pattern matching rather than true reasoning
- **First 3 experiments**:
  1. Test model performance on MMMU questions from a single discipline (e.g., Biology) to isolate domain knowledge requirements
  2. Compare performance on interleaved vs. separate text-image inputs to measure the impact of multimodal integration
  3. Evaluate the effectiveness of different knowledge retrieval methods (e.g., external databases vs. internal representation) on reasoning accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific design principles or methodologies should be prioritized to improve the perceptual capabilities of multimodal models when dealing with domain-specific visual data like chemical structures or medical images?
- Basis in paper: [inferred] The paper highlights that models struggle with domain-specific visual data types like chemical structures and medical images, with performance close to random guesses for these categories.
- Why unresolved: The paper identifies the challenge but doesn't propose specific solutions or methodologies for improving perceptual capabilities on these specialized image types.
- What evidence would resolve it: Comparative studies testing different architectural approaches (e.g., specialized visual encoders, domain adaptation techniques) on MMMU's chemical structures and medical images categories.

### Open Question 2
- Question: How does the inclusion of interleaved text and images in MMMU affect model performance compared to benchmarks where text and images are presented separately?
- Basis in paper: [explicit] The paper emphasizes that MMMU features interleaved text and images, requiring models to jointly understand both modalities, which is a unique challenge compared to existing benchmarks.
- Why unresolved: The paper doesn't provide direct comparisons between performance on interleaved vs. separate text-image presentations.
- What evidence would resolve it: Controlled experiments comparing model performance on MMMU questions with interleaved text/images versus equivalent questions with separated text and images.

### Open Question 3
- Question: What is the relationship between model performance on MMMU and actual expert-level capabilities in specific domains like medicine or engineering?
- Basis in paper: [explicit] The paper acknowledges that MMMU is not a sufficient test for Expert AGI as there's no direct mapping between MMMU performance and "90th percentile of skilled adults."
- Why unresolved: The paper identifies this limitation but doesn't explore how MMMU scores correlate with real-world expert performance.
- What evidence would resolve it: Empirical studies comparing MMMU performance of models with actual expert evaluations in specific domains covered by the benchmark.

## Limitations
- The benchmark's reliance on zero-shot evaluation may not fully capture models' potential when provided with appropriate context or fine-tuning
- The paper doesn't provide evidence that MMMU's specific design choices (heterogeneous image types, interleaved format) are optimal for measuring expert-level multimodal reasoning
- The extent to which MMMU's challenges reflect fundamental limitations versus evaluation design choices remains unclear

## Confidence
- **High Confidence**: The benchmark successfully collects diverse college-level multimodal questions requiring domain knowledge
- **Medium Confidence**: The claim that MMMU represents a significant advancement over existing benchmarks in measuring expert-level multimodal reasoning
- **Low Confidence**: The assertion that MMMU will effectively stimulate development of next-generation multimodal foundation models

## Next Checks
1. Conduct ablation studies comparing MMMU's interleaved format against separate text-image inputs to quantify the specific contribution of multimodal integration to overall difficulty
2. Test whether providing domain-specific context or fine-tuning on related tasks significantly improves model performance on MMMU questions
3. Evaluate whether simpler visual processing (e.g., OCR extraction followed by text-only reasoning) can achieve comparable results to full multimodal processing on a subset of MMMU questions