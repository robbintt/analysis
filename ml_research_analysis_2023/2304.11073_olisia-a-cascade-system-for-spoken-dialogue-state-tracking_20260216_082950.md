---
ver: rpa2
title: 'OLISIA: a Cascade System for Spoken Dialogue State Tracking'
arxiv_id: '2304.11073'
source_url: https://arxiv.org/abs/2304.11073
tags:
- spoken
- dialogue
- speech
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OLISIA, a cascade system for spoken dialogue
  state tracking (DST) that integrates an automatic speech recognition (ASR) model
  with a DST model. The authors address the challenge of robustness to spoken inputs
  by adapting both ASR outputs and DST inputs.
---

# OLISIA: a Cascade System for Spoken Dialogue State Tracking

## Quick Facts
- arXiv ID: 2304.11073
- Source URL: https://arxiv.org/abs/2304.11073
- Reference count: 16
- Primary result: OLISIA ranked first in DSTC11 Track 3 by integrating ASR normalization and data augmentation to improve spoken dialogue state tracking robustness.

## Executive Summary
This paper introduces OLISIA, a cascade system for spoken dialogue state tracking that addresses the robustness challenge of spoken inputs by combining ASR normalization with data augmentation techniques. The system integrates Whisper ASR with a T5-based generative DST model, applying time normalization and proper noun correction to ASR outputs while using value replacement, paraphrasing, and speech simulation for DST input adaptation. OLISIA achieved first place in DSTC11 Track 3, demonstrating that these techniques significantly reduce the performance gap between written and spoken conversations. The ensemble approach using different training sets further enhances robustness to spoken inputs.

## Method Summary
OLISIA employs a cascade architecture where user speech is first processed by Whisper ASR, then normalized through time normalization and proper noun correction, before being fed into a T5 generative DST model. The system uses data augmentation including value replacement (introducing new slot values), paraphrasing (with SG-GPT), and speech simulation (TTS + Whisper transcription) to bridge the written-spoken distribution gap. Multiple T5 models trained on different augmented datasets are ensembled through majority voting. The approach is evaluated on MultiWOZ 2.1 with TTS-synthesized and human-recorded speech data.

## Key Results
- OLISIA ranked first in DSTC11 Track 3, a benchmark for spoken dialogue state tracking
- Time normalization improved JGA by approximately 5% relative across all test sets
- Value replacement augmentation achieved 7-8% absolute JGA increase by alleviating memorization issues
- Proper noun correction provided 1.4-2.4% relative JGA improvement on noisier human-verbatim and human-paraphrased inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR normalization reduces transcription noise that directly harms DST performance.
- Mechanism: Time normalization and named entity correction map ASR outputs to the canonical formats and vocabulary expected by the DST model.
- Core assumption: The DST model's performance degrades linearly with the number of out-of-vocabulary tokens and format mismatches.
- Evidence anchors:
  - [abstract] "normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations."
  - [section] "Time normalization improves equally all three versions of the test set (around 5% relative JGA increase). Proper noun correction does not help much the clean TTS-verbatim (0.75% relative JGA increase), however, it seems to be much more valuable for the noisier Human-verbatim and Human-paraphrased versions (respectively 1.4% and 2.4% relative JGA increase)."
  - [corpus] Weak signal: corpus neighbors mention phonetic error augmentation and speech-text joint training, suggesting domain-specific normalization helps.
- Break condition: If the ASR model already outputs perfectly canonical formats, or if the DST model is robust to input variations (e.g., through training on noisy data).

### Mechanism 2
- Claim: Data augmentation bridges the distribution gap between written and spoken dialogue.
- Mechanism: Value replacement introduces unseen entities; speech simulation injects ASR noise; paraphrasing adds linguistic variation.
- Core assumption: The DST model can generalize from augmented data to real spoken inputs if the augmentations preserve semantic intent.
- Evidence anchors:
  - [abstract] "adapting the DST inputs through data augmentation... play an important role in reducing the performance discrepancy between written and spoken conversations."
  - [section] "introducing new values for the slots hotel-name, restaurant-name, train-departure and train-destination greatly alleviates the issue of memorization... with a 7% to 8% absolute increase in JGA."
  - [corpus] Weak signal: corpus neighbors discuss robustness testing and zero-shot DST, implying generalization from augmented data is valued.
- Break condition: If augmentation introduces label noise or breaks semantic consistency, harming model performance.

### Mechanism 3
- Claim: Larger pre-trained models improve robustness to spoken inputs.
- Mechanism: Increased parameter count enables better representation of subtle linguistic variations and error patterns.
- Core assumption: Model capacity is the bottleneck for handling ASR errors and spoken language phenomena.
- Evidence anchors:
  - [abstract] "increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations."
  - [section] "We observe that introducing new values... greatly alleviates the issue of memorization... and enables the model to generalize more... The speech simulation provides an additional slight improvement... paraphrasing... leads to an overall worse performance."
  - [corpus] Weak signal: corpus neighbors mention LLM-driven phonetic error augmentation, implying larger models can better handle spoken data.
- Break condition: If the task complexity is low enough that model capacity is not the limiting factor, or if training data is insufficient to leverage extra capacity.

## Foundational Learning

- Concept: Dialogue state tracking as sequence generation
  - Why needed here: The system uses a T5-based generative DST model that outputs <slot-name, slot-value> pairs directly, rather than extracting from a fixed ontology.
  - Quick check question: What is the difference between generative and extractive DST, and why might generative be more robust to spoken inputs?

- Concept: Data augmentation for robustness
  - Why needed here: The gap between written and spoken inputs is bridged by simulating speech phenomena and introducing novel entity values.
  - Quick check question: How do value replacement, speech simulation, and paraphrasing each contribute to reducing the written-spoken performance gap?

- Concept: ASR post-processing and its impact on downstream tasks
  - Why needed here: Whisper outputs vary in format and accuracy; normalization aligns them to the DST model's expectations.
  - Quick check question: Why is Word Error Rate (WER) not a good measure for ASR quality in DST, and what metric should be used instead?

## Architecture Onboarding

- Component map:
  - ASR module: Whisper transformer model with forced English decoding
  - ASR post-processing: Time normalization, named entity correction
  - DST module: T5 generative model fine-tuned on augmented MultiWOZ
  - Ensemble layer: Majority vote across multiple DST model variations

- Critical path:
  1. User speech → ASR transcription
  2. ASR output → time normalization
  3. ASR output → named entity correction
  4. Corrected text → DST model input (dialogue history)
  5. DST output → slot-value predictions
  6. Ensemble voting across models

- Design tradeoffs:
  - Cascade vs. end-to-end: Cascade leverages state-of-the-art ASR and DST separately but lacks joint optimization; end-to-end could be more compact but harder to scale.
  - Model size vs. efficiency: Larger models (Whisper-Large, T5-Large) improve robustness but increase compute cost and risk overfitting.
  - Augmentation complexity vs. benefit: More augmentation techniques improve robustness but risk label noise and semantic inconsistency.

- Failure signatures:
  - Low JGA despite low WER: ASR normalization may be misaligned with DST expectations or augmentation may be introducing noise.
  - High variance across model runs: Training instability or insufficient diversity in augmented data.
  - Degradation on paraphrased test set: Model overfitting to canonical forms or paraphrase augmentation being too noisy.

- First 3 experiments:
  1. Run baseline with Whisper-Small and T5-Small, measure JGA/WER; identify gap to challenge baseline.
  2. Add time normalization only; measure impact on TTS-verbatim and Human-verbatim separately.
  3. Apply value replacement augmentation; measure generalization by evaluating on unseen entity slots.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance differences between spoken and written dialogue systems change when using spontaneous speech instead of read speech?
- Basis in paper: [inferred] The authors mention that the dataset lacks spontaneity associated with actual speech and suggest that findings might not hold on spontaneous spoken dialogue.
- Why unresolved: The current dataset uses read speech, which doesn't capture the natural variations and disfluencies of spontaneous speech.
- What evidence would resolve it: Testing the system on a dataset with spontaneous speech would provide evidence of how performance differs between spoken and written dialogue systems.

### Open Question 2
- Question: What is the impact of model size on the performance of spoken dialogue systems when using end-to-end approaches instead of cascade approaches?
- Basis in paper: [inferred] The authors discuss the trade-off between model size and performance, but focus on cascade approaches.
- Why unresolved: The paper primarily focuses on cascade approaches, leaving the impact of model size on end-to-end approaches unexplored.
- What evidence would resolve it: Comparing the performance of end-to-end models of different sizes on spoken dialogue tasks would provide insights into the impact of model size.

### Open Question 3
- Question: How do different data augmentation techniques affect the robustness of spoken dialogue systems to various types of speech disfluencies?
- Basis in paper: [explicit] The authors use data augmentation techniques like value replacement, paraphrasing, and speech simulation to improve robustness to spoken inputs.
- Why unresolved: The paper does not explore the impact of these techniques on specific types of speech disfluencies, such as hesitations or repetitions.
- What evidence would resolve it: Evaluating the system's performance on datasets with controlled speech disfluencies would reveal the effectiveness of different data augmentation techniques.

## Limitations

- The system's heavy reliance on Whisper ASR introduces a potential bottleneck, as ASR errors are a primary source of DST degradation.
- The evaluation focuses primarily on the MultiWOZ dataset, which may not capture the full complexity of real-world spoken dialogue scenarios.
- The ensemble approach, while effective, adds computational overhead and complexity that may not be practical for all deployment scenarios.

## Confidence

**High Confidence**: The effectiveness of time normalization and proper noun correction for ASR post-processing is well-supported by the experimental results, showing consistent improvements across different test conditions (5% relative JGA increase for time normalization, 1.4-2.4% for proper noun correction on noisier inputs).

**Medium Confidence**: The impact of data augmentation techniques shows mixed results. While value replacement demonstrates clear benefits (7-8% absolute JGA increase), the effectiveness of paraphrasing is questionable, with the authors noting it "leads to an overall worse performance." The benefits of speech simulation appear marginal.

**Low Confidence**: The claim that increasing pre-trained model size is crucial for reducing performance discrepancy between written and spoken conversations is weakly supported. The paper mentions this as important but provides limited ablation studies isolating the impact of model size from other factors like augmentation techniques.

## Next Checks

1. **Ablation Study on Model Size**: Conduct experiments isolating the impact of model size by comparing T5-Small, T5-Base, and T5-Large models with identical augmentation and normalization strategies to determine if the performance gains are truly due to increased capacity or other factors.

2. **Cross-Dataset Evaluation**: Evaluate the system on additional dialogue datasets beyond MultiWOZ to assess generalization across different domains and dialogue styles, particularly focusing on how well the augmentation strategies transfer to new data distributions.

3. **Failure Mode Analysis**: Systematically analyze the types of errors that persist even after normalization and augmentation, categorizing them by error source (ASR vs. DST) and developing targeted mitigation strategies for the most common failure patterns.