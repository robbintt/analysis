---
ver: rpa2
title: Evaluating Neural Language Models as Cognitive Models of Language Acquisition
arxiv_id: '2310.20093'
source_url: https://arxiv.org/abs/2310.20093
tags:
- language
- word
- blimp
- does
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study critically evaluates neural language models (LMs) as
  cognitive models of language acquisition by testing them on existing syntactic benchmarks.
  It demonstrates that commonly used template-based benchmarks like BLiMP and Zorro
  lack structural diversity and can be solved by simple non-human-like models, such
  as n-gram models and hand-written linear rules, without requiring genuine grammatical
  knowledge.
---

# Evaluating Neural Language Models as Cognitive Models of Language Acquisition

## Quick Facts
- arXiv ID: 2310.20093
- Source URL: https://arxiv.org/abs/2310.20093
- Reference count: 20
- Key outcome: Template-based benchmarks like BLiMP and Zorro can be solved by simple models without genuine grammatical knowledge; gradient acceptability judgments provide more rigorous testing

## Executive Summary
This study critically examines whether neural language models can serve as valid cognitive models of language acquisition. Through systematic evaluation on syntactic benchmarks, the research demonstrates that commonly used template-based tests like BLiMP and Zorro lack structural diversity and can be solved by simple non-human-like models such as n-gram models and hand-written linear rules. When trained on child-directed speech data, LMs often underperform these baselines on many paradigms. The paper advocates for more rigorous evaluation methods, including carefully curated datasets with gradient acceptability judgments, to better probe whether LMs truly acquire human-like grammatical knowledge.

## Method Summary
The study evaluates neural language models on three syntactic benchmarks: BLiMP, Zorro, and LI-Adger. Researchers trained 5-gram models on AO-CHILDES corpus and developed hand-written linear rules for each paradigm. These simple baselines were compared against neural models like BabyBERTa on pseudo log-likelihood scores and forced-choice accuracy. The evaluation included analyzing performance on template-based benchmarks and gradient acceptability judgments, with particular attention to whether models showed human-like variability in their judgments across repeated evaluations.

## Key Results
- Template-based benchmarks like BLiMP and Zorro can be solved by simple n-gram models and hand-written linear rules without requiring genuine grammatical knowledge
- LMs trained on child-directed speech data are often outperformed by these simple baselines on many syntactic paradigms
- On the LI-Adger dataset with gradient acceptability judgments, LMs show inconsistent and non-human-like variability, with performance often comparable to or worse than naive models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral probing benchmarks can be solved without genuine grammatical knowledge
- Mechanism: Template-generated sentence pairs often have structural properties that can be captured by linear rules or n-gram models
- Core assumption: Structural diversity in test sentences is low
- Evidence anchors: Abstract mentions template benchmarks lack structural diversity; section notes many structures are simple
- Break condition: If benchmarks included structurally diverse sentences requiring hierarchical processing

### Mechanism 2
- Claim: Child-directed speech benchmarks overestimate LM performance
- Mechanism: Simpler lexical and structural properties in Zorro make it easier for models to succeed without deep grammatical knowledge
- Core assumption: Simpler properties translate to easier benchmarks
- Evidence anchors: Abstract notes Zorro's restricted vocabulary; section states Zorro is a weaker benchmark than BLiMP
- Break condition: If Zorro included more complex structures and diverse lexical items

### Mechanism 3
- Claim: Gradient acceptability judgments provide more rigorous testing
- Mechanism: Nuanced ratings reveal inconsistencies in LM behavior that binary tasks might miss
- Core assumption: Human language users exhibit gradient acceptability judgments
- Evidence anchors: Abstract mentions datasets evaluated for gradient acceptability; section notes acceptability judgments are consistent with theoretical literature
- Break condition: If binary tasks proved as effective as gradient judgments

## Foundational Learning

- Concept: Structural vs. rote learning in language acquisition
  - Why needed here: Understanding the distinction between learning vocabulary and grammar is crucial for evaluating whether LMs acquire human-like linguistic knowledge
  - Quick check question: Can you explain why vocabulary learning is considered rote while grammar learning is structural?

- Concept: Template-based vs. naturally occurring linguistic data
  - Why needed here: Recognizing limitations of template-generated data helps understand why such benchmarks might not effectively test genuine grammatical knowledge
  - Quick check question: How might the use of template-based data affect the evaluation of a model's linguistic knowledge?

- Concept: Gradient acceptability judgments
  - Why needed here: Understanding gradient judgments is important for appreciating why they provide a more nuanced test of grammatical knowledge than binary acceptability tasks
  - Quick check question: Why might gradient acceptability judgments be more informative than binary minimal pairs in evaluating grammatical knowledge?

## Architecture Onboarding

- Component map: Data preparation (BLiMP, Zorro, LI-Adger datasets) -> Model training (various LMs including BabyBERTa) -> Baseline creation (5-gram models, hand-written rules) -> Evaluation (accuracy metrics, variability analysis)
- Critical path: Data preparation → Model training → Baseline creation → Evaluation → Analysis of results
- Design tradeoffs: Template-based data allows easy generation of large datasets but lacks structural diversity; gradient judgments are more informative but require more complex data collection
- Failure signatures: High performance on template-based benchmarks without corresponding performance on gradient judgment tasks suggests reliance on non-human-like solutions
- First 3 experiments:
  1. Train 5-gram model on AO-CHILDES and evaluate on BLiMP and Zorro to establish baseline performance
  2. Develop hand-written linear rules for BLiMP and Zorro paradigms to demonstrate non-human-like solutions
  3. Evaluate LMs on LI-Adger dataset using gradient acceptability judgments to compare with human variability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective evaluation benchmarks that better capture the structural diversity of natural language and avoid being solvable by simple linear models?
- Basis in paper: Explicit
- Why unresolved: Paper demonstrates existing benchmarks can be solved by simple models but doesn't propose specific methods for creating new benchmarks
- What evidence would resolve it: Development and empirical testing of new benchmarks that show significantly better discrimination between human-like linguistic knowledge and simple pattern matching

### Open Question 2
- Question: How can we determine whether neural language models are actually learning human-like linguistic representations versus using non-human-like shortcuts to solve behavioral tasks?
- Basis in paper: Explicit
- Why unresolved: Paper shows models can perform well while showing high variability inconsistent with human judgments but doesn't provide definitive method for distinguishing between human-like and non-human-like learning
- What evidence would resolve it: Development of evaluation methods that can reliably distinguish between models using genuine grammatical knowledge versus simple pattern matching

### Open Question 3
- Question: What is the optimal approach for evaluating neural language models as cognitive models of language acquisition, given the limitations of behavioral probing alone?
- Basis in paper: Explicit
- Why unresolved: Paper concludes behavioral probing alone is insufficient but doesn't provide comprehensive framework for integrating multiple evaluation methods
- What evidence would resolve it: Empirical demonstration of evaluation framework combining behavioral probing with other methods to provide more reliable insights into cognitive relevance

## Limitations
- The critique of template-based benchmarks relies on the assumption that simple models can effectively capture regularities without requiring genuine grammatical knowledge
- The LI-Adger dataset analysis is based on a relatively small dataset (60 sentence pairs) compared to thousands in BLiMP and Zorro
- The generalizability of simple models to all syntactic structures remains uncertain

## Confidence

- High confidence: Demonstration that n-gram models and linear rules can solve many BLiMP and Zorro paradigms without requiring hierarchical syntactic knowledge
- Medium confidence: Claim that Zorro benchmarks overestimate LM performance due to lexical and structural simplicity
- Medium confidence: Assertion that gradient acceptability judgments provide more rigorous testing than binary minimal pairs

## Next Checks

1. Conduct systematic analysis of template-based benchmarks to identify structural patterns exploitable by simple models and quantify proportion of paradigms solvable without hierarchical knowledge
2. Expand LI-Adger dataset with additional sentence pairs and structures to better assess LM performance on gradient acceptability judgments
3. Develop and evaluate new benchmarks that combine scale of template-based datasets with structural diversity and gradient judgments of carefully curated datasets