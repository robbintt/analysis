---
ver: rpa2
title: 'R-LPIPS: An Adversarially Robust Perceptual Similarity Metric'
arxiv_id: '2307.15157'
source_url: https://arxiv.org/abs/2307.15157
tags:
- lpips
- adversarial
- r-lpips
- perceptual
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of robustness of the LPIPS perceptual
  similarity metric against adversarial examples. The authors propose R-LPIPS, a new
  metric that leverages adversarially trained deep features to achieve robustness.
---

# R-LPIPS: An Adversarially Robust Perceptual Similarity Metric

## Quick Facts
- arXiv ID: 2307.15157
- Source URL: https://arxiv.org/abs/2307.15157
- Reference count: 6
- One-line primary result: R-LPIPS outperforms LPIPS in robustness against adversarial attacks

## Executive Summary
This paper introduces R-LPIPS, a robust perceptual similarity metric designed to address the vulnerability of the original LPIPS metric to adversarial examples. By leveraging adversarially trained deep features, R-LPIPS demonstrates significant improvements in generalization and robustness against ℓ∞-PGD and ℓ2-PGD attacks across various data distortions. The authors also propose new perceptual attacks, R-PPGA and R-LPA, which are shown to be stronger than previously established attacks. Experimental results highlight the superiority of R-LPIPS in terms of robustness and performance under attack conditions.

## Method Summary
The authors develop R-LPIPS by applying adversarial training to the LPIPS weights (`w_j`) using the adversarial training scheme introduced by Madry et al. (2017). This involves introducing an adversarial perturbation δ at each step of the training on the input images. The resulting metric, R-LPIPS, is then evaluated against ℓ∞-PGD and ℓ2-PGD attacks on datasets like ImageNet-100 and CIFAR-10. The experiments demonstrate that R-LPIPS significantly improves robustness compared to the original LPIPS metric. Additionally, the authors introduce new perceptual attacks, R-PPGA and R-LPA, based on R-LPIPS, which are shown to be stronger than existing attacks.

## Key Results
- R-LPIPS outperforms LPIPS in robustness against ℓ∞-PGD and ℓ2-PGD attacks.
- Significant improvements in generalization across various data distortions.
- New perceptual attacks (R-PPGA and R-LPA) based on R-LPIPS are stronger than previously established attacks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: R-LPIPS improves robustness by replacing standard LPIPS deep features with adversarially trained deep features.
- Mechanism: Adversarial training during the tuning phase of the LPIPS weights (`w_j`) makes the feature extractor more resilient to small perturbations. By minimizing the adversarial loss during training, the model learns weights that maintain perceptual similarity even under small `ℓ_∞` or `ℓ_2` perturbations.
- Core assumption: The adversarial examples crafted during training are representative of the perturbations that will be encountered during evaluation.
- Evidence anchors:
  - [abstract] "R-LPIPS, a new metric that leverages adversarially trained deep features."
  - [section] "we leverage the adversarial training scheme introduced by Madry et al. (2017) and introduce an adversarial perturbation δ at each step of the training on x₀"
  - [corpus] Weak: corpus does not directly address adversarial training for perceptual metrics.
- Break condition: If adversarial training does not cover the full perturbation space or if the attacks used are stronger than those seen during training, robustness can break down.

### Mechanism 2
- Claim: R-LPIPS achieves better generalization across different types of image distortions compared to LPIPS.
- Mechanism: By incorporating adversarial training, R-LPIPS learns a more stable representation of perceptual similarity that is less sensitive to the specific noise patterns used in crafting adversarial examples. This stability translates to better performance on unseen distortions.
- Core assumption: Adversarial training on one set of distortions improves robustness to other types of distortions.
- Evidence anchors:
  - [abstract] "significant improvements in robustness and performance under attack."
  - [section] "Table 1 presents results for natural and under attack images with ℓ∞-PGD and ℓ₂-PGD... we observe a consistent increase in robustness of R-LPIPS compared to the original LPIPS metric."
  - [corpus] Weak: corpus does not provide evidence on generalization across distortions.
- Break condition: If the generalization assumption fails, R-LPIPS may not perform better than LPIPS on certain distortions.

### Mechanism 3
- Claim: R-LPIPS enables the creation of stronger perceptual attacks that can bypass defenses based on LPIPS.
- Mechanism: By using R-LPIPS as the perceptual similarity metric in attack optimization, attackers can craft perturbations that are less detectable by defenses relying on LPIPS. This is because R-LPIPS captures a more robust notion of perceptual similarity.
- Core assumption: The perceptual similarity captured by R-LPIPS is a better proxy for human perception under attack conditions.
- Evidence anchors:
  - [abstract] "we introduced new perceptual attacks, R-PPGA and R-LPA, based on R-LPIPS, which are shown to be stronger than previously established attacks."
  - [section] "we combined the PPGA and LPA attacks with our robust perceptual distance metric and developed attacks named R-PPGA and R-LPA."
  - [corpus] Weak: corpus does not discuss the strength of attacks based on R-LPIPS.
- Break condition: If R-LPIPS does not accurately reflect human perception under attack, the attacks based on it may not be stronger.

## Foundational Learning

- Concept: Adversarial Training
  - Why needed here: To make the perceptual similarity metric robust against adversarial perturbations by training on perturbed examples.
  - Quick check question: What is the primary difference between standard training and adversarial training?

- Concept: Perceptual Similarity Metrics
  - Why needed here: Understanding how LPIPS works and why it is susceptible to adversarial attacks is crucial for grasping the need for R-LPIPS.
  - Quick check question: How does LPIPS measure perceptual similarity between images?

- Concept: Projected Gradient Descent (PGD)
  - Why needed here: PGD is used both in crafting adversarial examples and in the adversarial training process to improve robustness.
  - Quick check question: What role does PGD play in the adversarial training of R-LPIPS?

## Architecture Onboarding

- Component map:
  - Input: Two images (reference and distorted)
  - Backbone: AlexNet (or similar) for feature extraction
  - Metric: R-LPIPS (adversarially trained LPIPS)
  - Output: Scalar value representing perceptual similarity

- Critical path:
  1. Extract deep features from both images using the backbone network.
  2. Apply channel-wise scaling to the features using the adversarially trained weights.
  3. Compute the L2 distance between the scaled features.
  4. Normalize the distance by the filter dimensions.
  5. Output the final R-LPIPS value.

- Design tradeoffs:
  - Using AlexNet as the backbone vs. more modern architectures for potentially better feature representation.
  - Trade-off between robustness and accuracy in the metric's alignment with human perception.

- Failure signatures:
  - High R-LPIPS values for visually similar images indicate a failure in capturing perceptual similarity.
  - Low robustness to new types of adversarial attacks suggests insufficient coverage in adversarial training.

- First 3 experiments:
  1. Compare R-LPIPS and LPIPS values on a set of adversarial examples crafted with PGD to quantify the improvement in robustness.
  2. Evaluate the 2AFC score of R-LPIPS under attack on the BAPPS dataset to measure generalization across different distortions.
  3. Use R-LPIPS in place of LPIPS in existing perceptual attacks (e.g., PPGD) to assess the increase in attack strength.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different training strategies for R-LPIPS (e.g., training on x0, x1, or both) affect its robustness against adversarial attacks?
- Basis in paper: [explicit] The paper mentions exploring different versions of R-LPIPS trained on x0, x1, or both, but does not provide experimental results comparing their robustness.
- Why unresolved: The authors acknowledge this as a potential direction for future work but do not conduct the experiments.
- What evidence would resolve it: Conducting adversarial training on different combinations of x0 and x1, and evaluating the robustness of each resulting R-LPIPS variant against various attacks.

### Open Question 2
- Question: Can R-LPIPS be effectively used as a defense mechanism in a perceptual adversarial training (PAT) scheme, and how does it compare to existing defenses?
- Basis in paper: [inferred] The paper suggests that R-LPIPS could be used to develop a more universal perceptual adversarial defense (R-PAT) but does not explore this experimentally.
- Why unresolved: The authors propose this as a future direction but do not implement or evaluate R-PAT.
- What evidence would resolve it: Implementing R-PAT using R-LPIPS and evaluating its performance against various perceptual attacks compared to existing PAT methods.

### Open Question 3
- Question: Is it possible to provide theoretical guarantees for the robustness of R-LPIPS, similar to the guarantees provided for ℓ∞-AT in standard adversarial training?
- Basis in paper: [explicit] The authors mention that R-LPIPS inherits the lack of theoretical guarantees from adversarial training and suggest developing such guarantees as a future direction.
- Why unresolved: Theoretical analysis of perceptual similarity metrics is challenging due to their non-linear nature and reliance on deep features.
- What evidence would resolve it: Developing a theoretical framework that provides robustness guarantees for R-LPIPS, possibly by leveraging techniques from robust optimization or Lipschitz continuity analysis.

## Limitations

- The adversarial training may not cover all possible perturbation spaces, potentially leaving vulnerabilities against stronger or novel attacks.
- The generalization claim across different image distortions relies on the assumption that adversarial training on one set of distortions transfers to others, which may not always hold.
- The introduction of new perceptual attacks (R-PPGA and R-LPA) is promising but their effectiveness against other defense mechanisms remains untested.

## Confidence

- High confidence: R-LPIPS outperforms LPIPS in robustness against ℓ∞-PGD and ℓ2-PGD attacks
- Medium confidence: R-LPIPS maintains or improves natural 2AFC scores
- Low confidence: R-LPIPS generalizes better across all types of image distortions

## Next Checks

1. Test R-LPIPS against a broader range of adversarial attacks, including those not seen during training, to verify its robustness limits.
2. Evaluate the performance of R-LPIPS on a wider variety of image distortions not used in the adversarial training process to confirm generalization claims.
3. Implement and test R-PPGA and R-LPA attacks against other perceptual similarity metrics and defense mechanisms to assess their relative strength.