---
ver: rpa2
title: Sentence Representations via Gaussian Embedding
arxiv_id: '2305.12990'
source_url: https://arxiv.org/abs/2305.12990
tags:
- sentence
- sentences
- entailment
- learning
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GaussCSE, a Gaussian distribution-based contrastive
  learning framework for sentence embedding that can handle asymmetric relationships
  between sentences, along with a similarity measure for identifying inclusion relations.
  The proposed method uses Gaussian distributions to represent sentence embeddings,
  allowing for capturing asymmetric relationships between sentences such as entailment
  and hierarchical relations.
---

# Sentence Representations via Gaussian Embedding

## Quick Facts
- arXiv ID: 2305.12990
- Source URL: https://arxiv.org/abs/2305.12990
- Reference count: 12
- Primary result: Gaussian embeddings with KL divergence achieve directional entailment detection while maintaining NLI accuracy

## Executive Summary
This paper introduces GaussCSE, a Gaussian distribution-based contrastive learning framework for sentence embedding that captures asymmetric relationships between sentences. By representing sentences as Gaussian distributions rather than point vectors, the method can distinguish between entailment directions (e.g., "A entails B" vs "B entails A"). The approach uses KL divergence to measure asymmetric similarity and incorporates reversed sentence pairs in training to improve directional prediction accuracy. Experiments on SNLI and SICK datasets show that GaussCSE achieves comparable NLI accuracy to previous methods while additionally estimating entailment direction.

## Method Summary
GaussCSE represents sentences as Gaussian distributions using mean and variance vectors obtained by fine-tuning BERT. The method employs contrastive learning with three types of example sets: entailment pairs (positive), contradiction pairs (negative), and reversed entailment pairs (additional negative). KL divergence serves as the asymmetric similarity measure, where larger variance in one sentence relative to another indicates potential entailment. The model is trained to increase variance for premise sentences and decrease variance for hypothesis sentences in entailment pairs. Training uses a combined SNLI and MNLI dataset, with hyperparameters selected via grid search on SNLI dev set AUC.

## Key Results
- Achieves 91.2% accuracy on SNLI test set (comparable to BERT-flow's 91.9%)
- Achieves 87.0% accuracy on SICK dataset
- Directional entailment prediction AUC of 0.837 on SNLI and 0.785 on SICK

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL divergence captures asymmetric sentence relationships better than cosine similarity.
- Mechanism: KL divergence measures information loss when approximating one Gaussian with another. It is inherently asymmetric: D_KL(N_i || N_j) ≠ D_KL(N_j || N_i). This property allows distinguishing between entailment directions.
- Core assumption: Asymmetric similarity is more appropriate for modeling entailment and hierarchical relations than symmetric measures like cosine similarity.
- Evidence anchors:
  - [abstract]: "Since the range of KL divergence is [0,∞), the range of sim(s_i||s_j) is (0, 1]. When the variance of N_i is greater than the variance of N_j, then D_KL(N_i||N_j) tends to be larger than D_KL(N_j||N_i), which means that sim(s_j||s_i) tends to be larger than sim(s_i||s_j)."
  - [section 2.3]: "Since the range of KL divergence is [0,∞), the range of sim(s_i||s_j) is (0, 1]. When the variance of N_i is greater than the variance of N_j, then D_KL(N_i||N_j) tends to be larger than D_KL(N_j||N_i), which means that sim(s_j||s_i) tends to be larger than sim(s_i||s_j)."
- Break condition: If sentence pairs are truly symmetric (e.g., paraphrases), asymmetric measures may introduce unnecessary complexity and reduce performance.

### Mechanism 2
- Claim: Variance in Gaussian embeddings encodes directional entailment strength.
- Mechanism: During training, GaussCSE increases the variance of premise sentences and decreases the variance of hypothesis sentences in entailment pairs. This creates embeddings where a sentence with larger variance can "contain" or "entail" sentences with smaller variance.
- Core assumption: Sentence meaning complexity (or uncertainty) is reflected in variance magnitude, and more complex sentences tend to entail simpler ones.
- Evidence anchors:
  - [section 2.3]: "In learning entailment relations, as with word representation by Gaussian embedding, GaussCSE performs learning so that the embedding of a sentence that entails the other sentence has greater variance than the embedding of the sentence to be entailed."
  - [section 3.3]: "On SNLI, since premise sentences tend to be longer than hypothesis sentences, premise sentences have larger variances."
- Break condition: If sentence length bias dominates (e.g., in SICK dataset), variance may reflect length rather than semantic complexity, reducing directional prediction accuracy.

### Mechanism 3
- Claim: Contrastive learning with reversed sentence pairs improves directional entailment detection.
- Mechanism: Including reversed entailment pairs (swapping premise and hypothesis) in the negative set forces the model to distinguish between correct and incorrect entailment directions.
- Core assumption: Training on reversed pairs provides explicit supervision for distinguishing entailment direction, beyond what positive and contradiction pairs alone can provide.
- Evidence anchors:
  - [section 2.3]: "The set of sentence pairs obtained by reversing each sentence pair in the 'entailment set.' The sentences whose entailment relation is reversed are used as negative examples in order to increase the variance of premise sentences and decrease the variance of the hypothesis."
  - [section 3.3]: "Adding reversed set improves the performance of both SNLI and SICK. This demonstrates that using reversed set is useful in reducing the effect of sentence length bias and capturing semantically valid sentence entailment."
- Break condition: If the dataset lacks sufficient entailment pairs or contains too many noisy examples, adding reversed pairs may degrade NLI classification performance.

## Foundational Learning

- Concept: Gaussian distribution parameterization (mean vector + variance vector)
  - Why needed here: GaussCSE represents sentences as Gaussian distributions using mean vectors (traditional embeddings) and variance vectors (diagonal covariance). This allows capturing uncertainty and asymmetric relationships.
  - Quick check question: How does GaussCSE represent a sentence's Gaussian embedding using BERT outputs?

- Concept: KL divergence for asymmetric similarity
  - Why needed here: KL divergence is used to measure similarity between Gaussian sentence embeddings. Its asymmetric nature enables directional entailment detection.
  - Quick check question: What is the range of KL divergence and how does it map to the similarity score in GaussCSE?

- Concept: Contrastive learning framework
  - Why needed here: GaussCSE uses contrastive learning to train the model by bringing positive examples closer and pushing negative examples apart in the Gaussian embedding space.
  - Quick check question: What are the three types of example sets used in GaussCSE's contrastive learning loss function?

## Architecture Onboarding

- Component map:
  - BERT base model (768 dim output) -> Two linear layers (mean vector μ, variance vector σ) -> KL divergence similarity function -> Contrastive loss function with entailment, contradiction, and reversed sets

- Critical path:
  1. Input sentence → BERT → [CLS] token embedding
  2. Embedding → Linear layers → μ and σ
  3. Compute Gaussian distribution N(μ, Σ)
  4. Calculate pairwise KL divergence similarities
  5. Compute contrastive loss
  6. Backpropagate and update BERT parameters

- Design tradeoffs:
  - Using only diagonal covariance matrix for efficiency vs full covariance for richer representation
  - Training with reversed pairs improves directional accuracy but may hurt NLI classification
  - Temperature hyperparameter τ in contrastive loss affects gradient scaling

- Failure signatures:
  - Low directional accuracy but high NLI accuracy: Variance may reflect sentence length rather than semantic complexity
  - High directional accuracy but low NLI accuracy: Model may overfit to reversed pairs at expense of general similarity
  - Unstable training: Learning rate or batch size may be suboptimal (see Table 3)

- First 3 experiments:
  1. Verify that variance correlates with sentence length on a small dataset
  2. Test directional accuracy on SNLI with and without reversed pairs
  3. Compare NLI accuracy using different temperature values (τ) in the loss function

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several areas remain unresolved based on the analysis:

1. How do sentence embeddings change over the course of training? Does the variance of a sentence increase more than its mean distance from semantically similar sentences, or vice versa?
2. Does the direction of entailment always correspond to the direction of higher variance? Are there cases where the entailment direction is opposite to the variance direction?
3. How does the proposed method perform on other NLI datasets, such as MultiNLI or SciTail? Does the performance degrade on these datasets compared to SNLI and SICK?

## Limitations

- The variance-based mechanism for capturing semantic complexity is confounded by sentence length bias, as evidenced by reduced directional accuracy on SICK where premise and hypothesis lengths are more similar
- Training with reversed pairs creates a tradeoff between directional accuracy and NLI classification performance, with no clear guidance on when to prioritize each
- The method has only been evaluated on English NLI datasets, limiting claims about generalization to other languages or semantic relationship types

## Confidence

**High confidence** in the technical implementation: The Gaussian embedding parameterization, KL divergence similarity calculation, and contrastive learning framework are well-defined and reproducible.

**Medium confidence** in the directional entailment claims: While results show improved directional accuracy, the variance-based mechanism for capturing semantic complexity needs more rigorous validation beyond sentence length correlations.

**Low confidence** in the generalization claims: The paper evaluates primarily on English NLI datasets and doesn't test the approach on other languages or domains where asymmetric relationships might manifest differently.

## Next Checks

1. **Length vs. Complexity Analysis**: Conduct a controlled experiment using sentence pairs where length is held constant but semantic complexity varies (e.g., "The cat sat" vs "The feline rested on the mat"). Verify that variance differences reflect semantic complexity rather than length.

2. **Ablation on Reversed Pairs**: Systematically vary the proportion of reversed pairs in training (0%, 25%, 50%, 100%) to quantify the exact tradeoff between directional accuracy and NLI classification performance.

3. **Cross-Domain Evaluation**: Test GaussCSE on a non-NLI dataset where asymmetric relationships are well-defined, such as hypernym-hyponym pairs from WordNet or hierarchical category relationships.