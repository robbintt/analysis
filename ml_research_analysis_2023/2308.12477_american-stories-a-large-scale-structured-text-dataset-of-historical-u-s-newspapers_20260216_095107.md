---
ver: rpa2
title: 'American Stories: A Large-Scale Structured Text Dataset of Historical U.S.
  Newspapers'
arxiv_id: '2308.12477'
source_url: https://arxiv.org/abs/2308.12477
tags:
- dataset
- data
- content
- article
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: American Stories is a novel pipeline for extracting structured
  text from historical U.S. newspaper scans.
---

# American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers

## Quick Facts
- arXiv ID: 2308.12477
- Source URL: https://arxiv.org/abs/2308.12477
- Reference count: 35
- Key outcome: 5.1% end-to-end character error rate on structured text extraction from 20M historical newspaper scans

## Executive Summary
American Stories is a novel pipeline for extracting structured text from historical U.S. newspaper scans. It combines layout detection, legibility classification, custom OCR, and article association using efficient mobile-oriented architectures. Applied to the Library of Congress's Chronicling America collection, the pipeline processes 20 million scans to produce a dataset of 1.14 billion content regions with high quality text transcriptions. Evaluation shows an end-to-end character error rate of 5.1% (4.4% with spellchecking) and strong performance on layout detection and OCR tasks. The resulting structured dataset enables new applications in language modeling, social science research, and multimodal analysis that are not possible with existing unstructured newspaper datasets.

## Method Summary
The American Stories pipeline processes historical newspaper scans through a modular architecture. It begins with YOLOv8 for layout detection, identifying content regions like articles, headlines, and advertisements. MobileNetv3 classifies text regions as legible, illegible, or borderline. EfficientOCR performs word-level recognition using contrastive learning and nearest neighbor retrieval against a rendered font index. Finally, rule-based methods associate content regions with articles. The entire pipeline is optimized for mobile deployment, achieving over an order of magnitude lower compute cost compared to larger models while maintaining near state-of-the-art accuracy.

## Key Results
- End-to-end character error rate of 5.1% (4.4% with spellchecking) on 1,027 test images
- Article association performance of 92.7% F1 score for headlines
- Dataset contains 1.14 billion content regions across 20 million scans
- EfficientOCR achieves 4.3% CER on ground truth layouts and lines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mobile-optimized deep learning architectures achieve near state-of-the-art accuracy at much lower compute cost.
- **Mechanism**: Using YOLOv8 and MobileNetv3 backbones instead of large vision transformers reduces deployment cost by over an order of magnitude with only modest accuracy loss.
- **Core assumption**: The accuracy hit from smaller models is acceptable for the scale of this application.
- **Evidence anchors**:
  - [abstract]: "built with efficient architectures designed for mobile phones" and "over an order of magnitude lower" cost.
  - [section 4]: "accuracy hit relative to much larger models was very modest and deployment costs were over an order of magnitude lower."
- **Break condition**: If the accuracy drop becomes too large for downstream applications, the cost savings are no longer worth it.

### Mechanism 2
- **Claim**: Modular deep learning pipelines allow for cheaper training and more flexible architecture updates.
- **Mechanism**: Separating layout detection, legibility classification, OCR, and content association into distinct modules reduces labeling costs and enables swapping in better components over time.
- **Core assumption**: Different tasks in the pipeline rely on different features of the image data.
- **Evidence anchors**:
  - [section 4]: "Theoretically, localization...and recognition...may rely on different features...suggesting modularity."
  - [section 4]: "There are vast differences in the number of labels required for training each component of the pipeline."
- **Break condition**: If the tasks are too interdependent for effective modularization, the pipeline performance may suffer.

### Mechanism 3
- **Claim**: Contrastive word-level image retrieval enables high-accuracy, low-cost OCR on noisy historical text.
- **Mechanism**: EfficientOCR uses vision encoders trained to match word crops to an offline index of rendered font embeddings, allowing parallel decoding and avoiding expensive sequence models.
- **Core assumption**: Historical newspaper words can be accurately represented by rendered fonts, and nearest neighbor search is sufficiently accurate.
- **Evidence anchors**:
  - [section 4]: "Character/word recognition is modeled as a character/word-level image retrieval problem, using a vision encoder contrastively trained on character/word crops."
  - [section 5]: "OCR Evaluation: Table 2 reports the CER from running OCR on ground truth layouts and lines. It is 0.043."
- **Break condition**: If the rendered font vocabulary is insufficient for historical vocabulary, retrieval accuracy will degrade.

## Foundational Learning

- **Concept**: Optical Character Recognition (OCR) and its limitations
  - **Why needed here**: Understanding how OCR works and fails is critical for appreciating the contribution of EfficientOCR and the need for layout detection and legibility filtering.
  - **Quick check question**: What are the main sources of error in traditional OCR on historical documents?

- **Concept**: Object detection and layout analysis
  - **Why needed here**: The pipeline relies on YOLOv8 to detect and classify content regions, so understanding how object detection works is essential.
  - **Quick check question**: How does object detection differ from image classification, and why is it needed for newspaper layout analysis?

- **Concept**: Contrastive learning and nearest neighbor retrieval
  - **Why needed here**: EfficientOCR uses contrastive learning to train word embeddings and nearest neighbor search for recognition, so familiarity with these concepts is key.
  - **Quick check question**: How does contrastive learning enable efficient word-level retrieval for OCR?

## Architecture Onboarding

- **Component map**: Input images → Layout detection (YOLOv8) → Legibility classification (MobileNetv3) → Line detection (YOLOv8) → OCR (EfficientOCR) → Content association (rule-based) → Output structured dataset
- **Critical path**: Layout detection → OCR → Content association. Errors in layout detection directly propagate to OCR and association.
- **Design tradeoffs**: Modularity and cost vs. end-to-end performance; smaller models vs. accuracy.
- **Failure signatures**: High CER indicates OCR issues; low mAP indicates layout detection problems; poor article association F1 indicates rule-based logic errors.
- **First 3 experiments**:
  1. Run layout detection on a small sample of scans and inspect predicted bounding boxes for accuracy.
  2. Evaluate OCR quality on ground truth text regions to isolate OCR performance from layout errors.
  3. Test content association rules on labeled article-headline pairs to validate rule logic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can American Stories be used to improve historical language model training beyond what is possible with existing datasets?
- Basis in paper: [explicit] The paper states that American Stories "could be used for pre-training a large language model to achieve better understanding of historical English and historical world knowledge."
- Why unresolved: The paper mentions the potential but does not provide specific examples or experimental results demonstrating the improvement in language model performance.
- What evidence would resolve it: Experiments comparing language model performance trained on American Stories versus other datasets, showing improvements in understanding historical English and world knowledge.

### Open Question 2
- Question: How effective are the current methods for detecting and removing potentially copyrighted content from American Stories?
- Basis in paper: [explicit] The paper discusses how the pipeline could help address copyright issues by detecting individual content regions like ads and comics.
- Why unresolved: The paper does not provide results on the effectiveness of these methods or their accuracy in identifying copyrighted content.
- What evidence would resolve it: Experiments evaluating the accuracy of the content detection methods in identifying copyrighted material, along with a comparison to manual annotation.

### Open Question 3
- Question: How can American Stories be used to study semantic change over time in a more comprehensive way than previous methods?
- Basis in paper: [explicit] The paper mentions that American Stories "provides extensive content for studying semantic change."
- Why unresolved: The paper does not provide specific examples or methodologies for how this study could be conducted using the dataset.
- What evidence would resolve it: Case studies or research papers utilizing American Stories to analyze semantic change in historical texts, demonstrating the advantages of the structured dataset over previous approaches.

## Limitations
- The 5.1% character error rate is measured on a subset of 1,027 images from 16 volumes, which may not be fully representative of the broader 20 million scan collection.
- The paper does not provide specific hyperparameter details for the YOLOv8 and MobileNetv3 models, nor does it specify the exact training data composition for the EfficientOCR component.
- The article association performance relies on heuristic rules whose robustness across diverse newspaper layouts is not extensively validated.

## Confidence
- **High confidence**: The modular pipeline architecture and use of efficient mobile-oriented models (YOLOv8, MobileNetv3) are clearly demonstrated and well-justified by the cost-performance tradeoff.
- **Medium confidence**: The 5.1% character error rate claim is supported by evaluation on a test set, but the representativeness of this test set relative to the full collection is unclear.
- **Medium confidence**: The article association performance (92.7% F1) is reported, but the methodology relies on heuristic rules whose robustness across diverse newspaper layouts is not extensively validated.

## Next Checks
1. **Dataset representativeness audit**: Sample and analyze a diverse set of newspaper pages from different time periods, geographic regions, and publication types (dailies vs. weeklies) to assess whether the 5.1% CER generalizes across the full collection.
2. **Cross-validation of article association**: Test the rule-based article association system on manually annotated articles from newspapers with varying layouts (e.g., narrow columns, broadsheets, tabloids) to identify edge cases where the heuristic rules fail.
3. **Component isolation experiment**: Run the pipeline on a controlled test set where ground truth layouts are provided, to isolate and quantify the contribution of each pipeline component (layout detection, legibility filtering, OCR, association) to the final error rate.