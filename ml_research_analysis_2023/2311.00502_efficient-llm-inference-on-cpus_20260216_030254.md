---
ver: rpa2
title: Efficient LLM Inference on CPUs
arxiv_id: '2311.00502'
source_url: https://arxiv.org/abs/2311.00502
tags:
- quantization
- arxiv
- int4
- fp32
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach for efficient LLM inference on
  CPUs through automatic INT4 weight-only quantization and a specialized LLM runtime.
  The key idea is to quantize model weights to INT4 while keeping activations in higher
  precision, and then accelerate inference using highly optimized CPU kernels.
---

# Efficient LLM Inference on CPUs

## Quick Facts
- arXiv ID: 2311.00502
- Source URL: https://arxiv.org/abs/2311.00502
- Reference count: 5
- This paper presents an approach for efficient LLM inference on CPUs through automatic INT4 weight-only quantization and a specialized LLM runtime.

## Executive Summary
This paper introduces a comprehensive framework for deploying large language models on CPUs by combining INT4 weight-only quantization with highly optimized CPU kernels. The approach achieves near-lossless accuracy (<1% drop from FP32) while delivering 1.3-1.6x speedup over existing solutions on 4th Gen Intel Xeon CPUs. The method addresses the challenge of CPU inference for LLMs, which traditionally requires significant memory and compute resources, by leveraging advanced quantization techniques and instruction set optimizations including AVX-512, AVX-512_VNNI, and AMX.

## Method Summary
The approach consists of automatic INT4 quantization using Intel Neural Compressor with various recipes (GPTQ, AWQ, TEQ, SignRound), optimized CPU tensor library kernels supporting modern instruction sets, and an efficient LLM runtime with memory management and KV cache optimizations. The framework quantizes model weights to INT4 while maintaining activations in higher precision (FP32/BF16) to preserve accuracy. The runtime includes pre-allocated KV cache with in-place updates and a thread scheduler optimized for CPU architectures. The method supports popular LLM architectures including Llama2, Llama, GPT-NeoX, Falcon, and GPT-J with parameter sizes ranging from 7B to 20B.

## Key Results
- Achieves <1% accuracy loss from FP32 baseline through INT4 weight-only quantization with FP32 activations
- Delivers 1.3-1.6x speedup over ggml-based solutions on 4th Gen Intel Xeon CPUs
- Enables token generation latency of 20-80ms, exceeding typical human reading speed of 200ms per token

## Why This Works (Mechanism)

### Mechanism 1
- INT4 weight-only quantization with FP32 activations enables near-lossless accuracy while significantly reducing memory and computation. By quantizing only weights to INT4 and keeping activations in FP32/BF16, the approach reduces memory footprint and bandwidth requirements while preserving numerical stability during inference.

### Mechanism 2
- CPU tensor library with AMX and VNNI instruction set support delivers 1.3-1.6x speedup over ggml-based solutions. The library implements INT4 matrix multiplication kernels optimized for x86 instruction sets, with AMX providing 8-bit integer matrix operations and VNNI accelerating INT8 operations.

### Mechanism 3
- Pre-allocated KV cache with in-place updates reduces memory allocation overhead during generation. Instead of reallocating memory for the full KV cache at each token generation step, the optimized approach pre-allocates sufficient memory and only updates the portion corresponding to the new token.

## Foundational Learning

- Concept: INT4 quantization and its trade-offs
  - Why needed here: Understanding how weight-only INT4 quantization preserves accuracy while reducing memory/compute is central to the paper's approach.
  - Quick check question: Why does weight-only INT4 quantization typically achieve better accuracy than INT8 quantization of both weights and activations?

- Concept: CPU instruction sets for deep learning (AVX-512, VNNI, AMX)
  - Why needed here: The performance gains depend on leveraging specific CPU instructions designed for matrix operations and low-precision arithmetic.
  - Quick check question: What is the primary advantage of AMX over AVX-512 for INT8/INT4 matrix multiplication?

- Concept: Transformer architecture and KV cache mechanics
  - Why needed here: The KV cache is identified as performance-critical, and understanding its role in self-attention is necessary to appreciate the optimization.
  - Quick check question: How does the KV cache enable efficient autoregressive generation in decoder-only Transformers?

## Architecture Onboarding

- Component map: Automatic INT4 quantization flow -> CPU tensor library with INT4 kernels -> LLM runtime with memory management and KV cache optimization
- Critical path: 1) Quantize FP32 model to INT4 using recipe tuning loop, 2) Load INT4 model into LLM runtime, 3) Pre-allocate memory for KV cache, 4) Execute inference with optimized INT4 kernels and thread scheduling
- Design tradeoffs: INT4 weights vs. INT8 (memory vs. quantization difficulty), pre-allocated KV cache vs. dynamic allocation (overhead vs. memory waste), group size tuning (performance vs. accuracy)
- Failure signatures: Accuracy degradation >1% from FP32 baseline indicates quantization recipe issues, performance matching or worse than ggml suggests kernel inefficiencies or memory bottlenecks, memory allocation errors during KV cache updates indicate incorrect pre-allocation sizing
- First 3 experiments: 1) Run quantization flow on a small FP32 model (1B parameters) and verify <1% accuracy loss, 2) Benchmark INT4 kernel performance on a known CPU (4th Gen Xeon) with different group sizes, 3) Profile memory usage during KV cache updates to validate pre-allocation strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the INT4 quantization flow perform when applied to other model architectures beyond the ones tested (GPT-J, Llama2, GPT-NeoX, Falcon)?
- Basis in paper: [explicit] The paper demonstrates results on popular LLMs ranging from 7B to 20B parameters but does not explore other architectures.
- Why unresolved: The evaluation is limited to specific models, and broader applicability remains untested.
- What evidence would resolve it: Testing the quantization flow on diverse architectures (e.g., OPT, BLOOM, T5) and comparing accuracy and performance metrics.

### Open Question 2
- Question: What is the impact of different group sizes on quantization quality and performance for larger models beyond 20B parameters?
- Basis in paper: [inferred] The paper tests group sizes of 32 and 128 but does not explore their effects on models larger than 20B parameters.
- Why unresolved: Larger models may have different quantization dynamics, and the optimal group size could vary.
- What evidence would resolve it: Conducting experiments on models >20B parameters with varying group sizes and analyzing accuracy and latency trade-offs.

### Open Question 3
- Question: How does the CPU tensor library perform on non-x86 architectures, such as ARM or RISC-V?
- Basis in paper: [explicit] The paper focuses on x86 CPUs and does not address other architectures.
- Why unresolved: The tensor library's optimizations are tailored to x86 instruction sets, and its performance on other architectures is unknown.
- What evidence would resolve it: Benchmarking the tensor library on ARM and RISC-V CPUs and comparing performance metrics with x86.

## Limitations
- The quantization recipe tuning process is not fully specified, making it difficult to reproduce the near-lossless accuracy results consistently.
- Performance comparison with ggml lacks detailed methodology and configuration information for independent verification.
- Memory usage and scalability analysis is incomplete, particularly for production deployments with varying sequence lengths and batch sizes.

## Confidence

**High Confidence**: The feasibility of CPU-based LLM inference with acceptable performance. The paper demonstrates working implementations and provides benchmark results showing practical token generation speeds (20-80ms per token), which exceeds typical human reading speeds.

**Medium Confidence**: The 1.3-1.6x speedup over ggml-based solutions. While the methodology appears sound and leverages appropriate hardware optimizations, the lack of detailed benchmarking methodology and ggml configuration details limits confidence in the generalizability of these results.

**Medium Confidence**: The <1% accuracy loss claim. The approach of weight-only INT4 quantization with FP32/BF16 activations is theoretically sound, but achieving near-lossless accuracy requires careful recipe tuning that isn't fully specified in the paper.

## Next Checks

1. **Quantization Recipe Validation**: Reproduce the INT4 quantization flow on a 1B parameter model using the described Intel Neural Compressor setup. Measure accuracy on multiple benchmarks (e.g., lambada, wikitext) and verify that <1% accuracy loss is consistently achievable across different models and datasets. Document the specific recipe parameters and tuning process.

2. **Independent Performance Benchmarking**: Set up a controlled benchmark comparing the proposed CPU runtime with ggml on identical hardware (4th Gen Intel Xeon) and models (e.g., Llama2-7B). Use standardized evaluation procedures and measure not just token generation latency but also memory bandwidth utilization, CPU utilization patterns, and cache efficiency to understand where performance gains originate.

3. **Memory Scaling Analysis**: Test the KV cache optimization strategy across a range of sequence lengths (128, 1024, 4096, 8192 tokens) and batch sizes (1, 8, 32). Measure peak memory usage, cache hit rates, and allocation overhead to validate that the pre-allocation strategy remains efficient as workload parameters scale. Identify any memory bottlenecks or scaling limitations.