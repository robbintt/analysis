---
ver: rpa2
title: 'ED-NeRF: Efficient Text-Guided Editing of 3D Scene with Latent Space NeRF'
arxiv_id: '2310.02712'
source_url: https://arxiv.org/abs/2310.02712
tags:
- nerf
- editing
- latent
- text
- ed-nerf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method, ED-NeRF, for text-guided editing
  of 3D scenes represented as neural radiance fields. The key idea is to optimize
  the NeRF model directly in the latent space of a pre-trained diffusion model, which
  reduces training time and improves editing performance compared to existing methods
  that operate in image space.
---

# ED-NeRF: Efficient Text-Guided Editing of 3D Scene with Latent Space NeRF

## Quick Facts
- arXiv ID: 2310.02712
- Source URL: https://arxiv.org/abs/2310.02712
- Reference count: 13
- Key outcome: ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models, as measured by CLIP directional score and user study.

## Executive Summary
This paper proposes a novel method, ED-NeRF, for text-guided editing of 3D scenes represented as neural radiance fields. The key idea is to optimize the NeRF model directly in the latent space of a pre-trained diffusion model, which reduces training time and improves editing performance compared to existing methods that operate in image space. Specifically, ED-NeRF introduces a refinement layer to address the lack of geometric consistency in the latent space, and extends the Delta Denoising Score (DDS) loss from 2D to 3D for more effective editing. Experiments show that ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models, as measured by CLIP directional score and user study.

## Method Summary
ED-NeRF optimizes NeRF directly in the latent space of a pre-trained diffusion model (Stable Diffusion) to reduce training time and improve editing performance. The method introduces a refinement layer to address geometric inconsistency in the latent space and extends Delta Denoising Score (DDS) loss from 2D to 3D for effective editing. ED-NeRF encodes training images into the latent space, trains a latent NeRF to reconstruct these features, and then applies DDS-based editing using source and target text prompts.

## Key Results
- ED-NeRF achieves faster editing speed compared to state-of-the-art 3D editing models
- Improved output quality as measured by CLIP directional score
- Better preservation of original object structure and background during editing, validated through user study

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing NeRF directly in the latent space of a diffusion model reduces computational cost and improves editing speed.
- Mechanism: By encoding all training images into the latent space of Stable Diffusion via a VAE, ED-NeRF trains to predict these low-dimensional latent features instead of high-resolution RGB images. This dimensionality reduction simplifies the optimization task and leverages the powerful prior of the diffusion model for editing.
- Core assumption: The latent space of the diffusion model captures sufficient semantic information for view synthesis and editing while being lower-dimensional than RGB space.
- Evidence anchors:
  - [abstract] "optimizing the NeRF model directly in the latent space of a pre-trained diffusion model, which reduces training time"
  - [section 3.1] "Optimizing NeRF to render the latent feature value of the latent diffusion model offers several advantages in text-guided 3D generation. These advantages include a reduced training burden due to the decreased dimensionality of the space"
  - [corpus] Weak evidence; no direct citations of latent space NeRF training benefits in related papers
- Break condition: If the latent space does not preserve enough geometric consistency or semantic detail for accurate view synthesis, the rendered results will be blurry or distorted.

### Mechanism 2
- Claim: The refinement layer restores geometric consistency lost in the latent space by modeling pixel interactions.
- Mechanism: The refinement layer, composed of ResNet blocks and self-attention layers, is added after the latent NeRF renders a feature map. This layer mimics the encoder/decoder interactions in the diffusion model, allowing neighboring pixels to influence each other and improve view synthesis quality.
- Core assumption: The degradation in latent NeRF view synthesis is due to the lack of pixel-to-pixel interactions, which are present in the diffusion model's encoder/decoder but absent in NeRF's independent ray sampling.
- Evidence anchors:
  - [section 3.2] "we analyzed the Encoder E and Decoder D of Stable Diffusion and discovered...pixel values exhibit interference between each other, primarily due to ResNet and self-attention layers"
  - [section 3.2] "we find that the reason for the deformed reconstruction performance of the latent NeRF lies in the inconsideration of the interactions mentioned above"
  - [corpus] Weak evidence; no direct citations of refinement layers for latent NeRF in related papers
- Break condition: If the refinement layer is too shallow or improperly initialized, it may not fully recover the lost interactions, leading to residual artifacts.

### Mechanism 3
- Claim: Delta Denoising Score (DDS) provides more effective text-guided editing than Score Distillation Sampling (SDS) by conditioning on both source and target text prompts.
- Mechanism: DDS computes the difference between denoising scores from the source and target text, guiding the NeRF update in the direction of the desired edit while preserving the original structure. This avoids the mode-seeking behavior and color saturation issues of SDS.
- Core assumption: The difference between source and target denoising scores provides a more targeted gradient for editing than the absolute denoising score conditioned only on the target.
- Evidence anchors:
  - [section 3.3] "DDS can be formed as a difference between two SDS scores: ∇θLDDS = ∇θLSDS(z, ysrc) − ∇θLSDS(ˆz, ytrg)"
  - [section 3.3] "DDS guides the optimized latent towards the target prompt from the source prompt without the influence of the pure noise component"
  - [corpus] Moderate evidence; DDS is cited in related papers (e.g., [250469]) as a contrastive denoising technique for 2D editing
- Break condition: If the source and target text prompts are too dissimilar, the difference in denoising scores may become noisy or unstable, leading to poor edits.

## Foundational Learning

- Concept: Neural Radiance Fields (NeRF)
  - Why needed here: ED-NeRF is built upon NeRF as the underlying 3D representation; understanding its architecture and training is essential.
  - Quick check question: What are the two outputs of the NeRF MLP for a given 3D point and view direction?

- Concept: Diffusion Models and Latent Diffusion
  - Why needed here: ED-NeRF leverages the latent space and denoising process of Stable Diffusion for semantic guidance and editing.
  - Quick check question: How does a VAE encoder map images into the latent space used by latent diffusion models?

- Concept: Score Distillation Sampling (SDS) and Delta Denoising Score (DDS)
  - Why needed here: These are the core loss functions used for text-guided editing in ED-NeRF; understanding their differences is key to grasping the method's advantages.
  - Quick check question: What is the key difference between the gradients computed by SDS and DDS?

## Architecture Onboarding

- Component map:
  Multi-view images -> VAE Encoder -> Latent NeRF -> Refinement Layer -> Latent Decoder -> Edited 3D Scene

- Critical path:
  1. Encode training images to latent space
  2. Train Latent NeRF + Refinement Layer to reconstruct latents
  3. For editing: Render target latents, apply DDS with source/target text, update NeRF and refinement

- Design tradeoffs:
  - Latent space vs. RGB space: Lower dimensionality and faster training vs. potential loss of detail
  - Refinement layer: Improved view synthesis vs. added complexity and training time
  - DDS vs. SDS: More targeted edits vs. increased complexity in loss computation

- Failure signatures:
  - Blurry or distorted novel views: Likely issue with latent NeRF or refinement layer
  - Poor text alignment: Likely issue with DDS loss or text conditioning
  - Unintended edits outside mask: Likely issue with masked reconstruction loss weighting

- First 3 experiments:
  1. Train Latent NeRF on a small synthetic dataset (e.g., a single object with 50 views) and evaluate novel view reconstruction quality.
  2. Add the refinement layer and compare view synthesis quality with and without it.
  3. Apply DDS-based editing on a trained Latent NeRF model and evaluate text alignment and preservation of unedited regions.

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions.

## Limitations
- The refinement layer's effectiveness relies on the assumption that pixel interactions are the primary cause of degraded latent NeRF reconstruction, but the exact mechanisms are not fully explored or validated against alternative explanations.
- While DDS is claimed to be superior to SDS, the paper lacks ablation studies comparing these losses directly in the 3D editing context, leaving the magnitude of improvement unclear.
- The paper assumes the availability of pre-trained Stable Diffusion and segmentation models, but does not address the computational overhead or licensing considerations of these dependencies.

## Confidence
- High confidence in the core mechanism of latent space optimization reducing dimensionality and training time, supported by established literature on latent diffusion models.
- Medium confidence in the refinement layer's ability to restore geometric consistency, as the evidence is primarily empirical without extensive ablation or theoretical analysis.
- Medium confidence in DDS's superiority over SDS for 3D editing, based on prior 2D results but lacking direct 3D ablation studies.

## Next Checks
1. Conduct ablation studies comparing DDS and SDS losses in the 3D editing context, measuring both editing quality and training stability.
2. Analyze the latent NeRF reconstruction quality with and without the refinement layer, isolating the contribution of different architectural components.
3. Evaluate the method's performance on datasets with varying levels of geometric complexity and text editability to assess its generalizability.