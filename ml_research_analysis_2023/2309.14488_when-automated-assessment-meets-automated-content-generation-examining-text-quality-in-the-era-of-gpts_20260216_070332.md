---
ver: rpa2
title: 'When Automated Assessment Meets Automated Content Generation: Examining Text
  Quality in the Era of GPTs'
arxiv_id: '2309.14488'
source_url: https://arxiv.org/abs/2309.14488
tags:
- text
- human
- scoring
- assessment
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how automated assessment models trained on
  human-generated text perform when assessing content generated by GPT models. We
  benchmark seven state-of-the-art machine learning models across two datasets containing
  18,460 essays, comparing their effectiveness in scoring human versus GPT-generated
  text.
---

# When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs

## Quick Facts
- arXiv ID: 2309.14488
- Source URL: https://arxiv.org/abs/2309.14488
- Reference count: 40
- This study evaluates how automated assessment models trained on human-generated text perform when assessing content generated by GPT models.

## Executive Summary
This study examines how automated essay scoring models trained on human text perform when evaluating GPT-generated content. Benchmarking seven state-of-the-art machine learning models across 18,460 essays, the research reveals that transformer-based language models (BERT and RoBERTa) score human essay quality more accurately than traditional deep learning and feature-based methods. Interestingly, these transformer models score GPT-generated text 10-15% higher than human-authored documents, while traditional models show the opposite pattern. The findings demonstrate that generative AI can significantly disrupt automated text scoring systems, with important implications for applications including search relevance, content recommendation, and credibility assessment.

## Method Summary
The study benchmarks seven machine learning models (BERT, RoBERTa, CNN, GRU, SVR, XGB, KNN) on two essay datasets containing 18,460 essays total: 12,977 essays from the ASAP dataset (7th-10th grade argumentative/response/narrative essays) and 2,460 essays from the CLC-FCE dataset (5 genres). An additional 3,023 GPT-generated essays (1,537 from GPT-3.5, 1,486 from GPT-4) were created using prompts similar to human prompts. Models were trained using 5-fold cross-validation on human essays, then applied to both human and GPT-generated essays. Statistical analysis (ANOVA with 3-way interactions) examined scoring differences, and content analysis explored attention patterns in transformer models.

## Key Results
- Transformer-based language models (BERT and RoBERTa) achieve 10-40% lower mean squared error than traditional methods when scoring human essay quality
- Transformer models score GPT-generated text 10-15% higher on average than human-authored documents
- Traditional deep learning and feature-based models score human text considerably higher than GPT-generated text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based PLMs (BERT, RoBERTa) are more effective at scoring human essay quality compared to traditional ML methods.
- Mechanism: Transformer models leverage contextualized embeddings and attention mechanisms to capture nuanced linguistic patterns in human text, outperforming CNNs, RNNs, and feature-based methods.
- Core assumption: The pre-training data used for BERT and RoBERTa includes a diverse corpus that overlaps with human essay content, enabling better generalization to unseen human text.
- Evidence anchors:
  - [abstract] "transformer-based language models (BERT and RoBERTa) more accurately score human essay quality as compared to CNN/RNN and feature-based ML methods, achieving 10-40% lower mean squared error."
  - [section] "Results of our benchmark analysis reveal that transformer pretrained language models (PLMs) more accurately score human essay quality as compared to CNN/RNN and feature-based ML methods, attaining scoring mean squared errors that are 10-40 percent lower."

### Mechanism 2
- Claim: Transformer models trained on human text score GPT-generated text higher than human-authored documents.
- Mechanism: Familiarity with language patterns from pre-training data (including Wikipedia and web text) may cause transformers to recognize and favor certain stylistic or structural elements common in GPT output.
- Core assumption: There is substantial overlap between the pre-training data of BERT/RoBERTa and the training data used for GPT models, leading to shared linguistic features.
- Evidence anchors:
  - [abstract] "Interestingly, we find that the transformer PLMs tend to score GPT-generated text 10-15% higher on average, relative to human-authored documents."
  - [section] "Further analysis reveals that although the transformer PLMs are exclusively fine-tuned on human text, they more prominently attend to certain tokens appearing only in GPT-generated text, possibly due to familiarity/overlap in pre-training."

### Mechanism 3
- Claim: Feature-based and CNN/RNN models score human text higher than GPT text.
- Mechanism: These models rely on predefined features or static embeddings that are more aligned with human linguistic patterns, making them less effective at recognizing GPT-generated content.
- Core assumption: The feature sets and embeddings used by these models are optimized for human language and may not capture the stylistic nuances of GPT-generated content.
- Evidence anchors:
  - [abstract] "Conversely, traditional deep learning and feature-based ML models score human text considerably higher."
  - [section] "This is interesting because the GPT data is out-of-sample for these models trained on human-only text. Conversely, the traditional deep learning and feature-based ML models score human-generated text considerably higher than GPT text."

## Foundational Learning

- Concept: Automated Essay Scoring (AES)
  - Why needed here: Understanding AES is crucial as it is the primary testbed for evaluating how ML models score human vs. GPT-generated text.
  - Quick check question: What are the key challenges in AES that make it a suitable testbed for studying the interplay between human and ML-generated text?

- Concept: Transformer-based Pre-trained Language Models (PLMs)
  - Why needed here: PLMs like BERT and RoBERTa are central to the study's findings, as they outperform other models in scoring human text and show unique behavior with GPT-generated content.
  - Quick check question: How do the attention mechanisms in PLMs contribute to their effectiveness in scoring text quality?

- Concept: Parallel Representations in Text Analysis
  - Why needed here: Parallel representations help analyze linguistic differences between human and GPT-generated text, providing insights into why certain models score them differently.
  - Quick check question: What are the key types of parallel representations used in the study, and how do they help differentiate human and GPT-generated text?

## Architecture Onboarding

- Component map: ASAP, CLC-FCE datasets -> 7 ML models (BERT, RoBERTa, CNN, GRU, SVR, XGB, KNN) -> Statistical analysis framework -> Evaluation metrics (MSE, MAE, QWK, PCC, SRC)

- Critical path:
  1. Preprocess and standardize datasets
  2. Train and evaluate ML models on human essays
  3. Apply trained models to both human and GPT-generated essays
  4. Conduct statistical analysis to compare scoring patterns
  5. Perform content analysis using parallel representations and attention weights

- Design tradeoffs:
  - Using multiple datasets (ASAP, CLC-FCE) provides diversity but may introduce inconsistencies in scoring standards.
  - Including both GPT-3.5 and GPT-4 allows robustness checks but increases complexity.
  - Fine-tuning PLMs on human text ensures relevance but may limit their ability to generalize to GPT-generated content.

- Failure signatures:
  - High variance in model performance across different essay genres.
  - Overfitting to specific linguistic patterns in human text, leading to poor generalization.
  - Inconsistent scoring between human and GPT-generated essays across different ML models.

- First 3 experiments:
  1. Benchmark the seven ML models on human essays using cross-validation to establish baseline performance.
  2. Apply the best-performing models (BERT, RoBERTa) to GPT-generated essays and compare scoring distributions.
  3. Conduct a statistical analysis to identify interactions between model type, essay genre, and author type (human vs. GPT).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of GPT-generated text vary across different prompt genres when evaluated by human raters versus automated assessment models?
- Basis in paper: [explicit] The paper shows that transformer models score GPT text higher than human text across certain genres (response, narrative, argumentative), while traditional models show the opposite pattern, but doesn't directly compare human versus automated assessments of GPT text quality.
- Why unresolved: The study focuses on automated assessment models' scores rather than human evaluations of GPT text quality, leaving the question of how humans would rate GPT-generated text across different genres unanswered.
- What evidence would resolve it: A study where human raters evaluate GPT-generated essays across different genres and compare these ratings with automated model scores to identify discrepancies.

### Open Question 2
- Question: What specific linguistic features in GPT-generated text cause transformer models to score them higher than human-written text?
- Basis in paper: [explicit] The paper identifies that transformer models attend more to certain tokens appearing only in GPT-generated text, but doesn't specify which linguistic features drive this preference.
- Why unresolved: While the paper identifies that there are differences in attention patterns, it doesn't conduct a detailed linguistic analysis of which specific features (syntactic structures, vocabulary choices, etc.) contribute to higher scores.
- What evidence would resolve it: A comprehensive linguistic analysis comparing GPT and human text across multiple dimensions (syntax, semantics, pragmatics) to identify which features correlate with higher transformer model scores.

### Open Question 3
- Question: How does the pre-training data overlap between GPT and transformer-based assessment models influence scoring patterns?
- Basis in paper: [explicit] The paper speculates that overlap in pre-training sources may contribute to transformer models scoring GPT text higher, but doesn't quantify this relationship.
- Why unresolved: The paper acknowledges potential familiarity effects but doesn't empirically measure the extent of pre-training data overlap or its impact on scoring patterns.
- What evidence would resolve it: An analysis measuring the degree of overlap between pre-training corpora and correlating this with scoring patterns across different model combinations.

## Limitations
- The exact nature and extent of pre-training data overlap between BERT/RoBERTa and GPT models remains speculative and unverified.
- Prompt formulations for GPT essay generation are not fully specified, limiting reproducibility.
- All analysis is conducted within the educational essay scoring domain, potentially limiting generalizability to other text types.

## Confidence

- High Confidence: The benchmark results showing transformer models achieving 10-40% lower MSE than traditional methods on human essays are well-supported by the cross-validation methodology and large dataset size.
- Medium Confidence: The finding that transformers score GPT text 10-15% higher than human text is statistically robust but the underlying mechanism (pre-training overlap) remains inferential rather than empirically verified.
- Low Confidence: Claims about specific token-level attention patterns favoring GPT text are based on qualitative analysis without systematic validation of which tokens drive the bias.

## Next Checks

1. Conduct a systematic comparison of BERT/RoBERTa pre-training corpora against GPT training data to identify specific linguistic patterns, vocabulary, or text structures that might explain the scoring bias.

2. Replicate the benchmark study using non-essay text types (product reviews, technical documentation, news articles) to test whether transformer scoring biases extend beyond educational writing.

3. Generate multiple sets of GPT essays using systematically varied prompts (different lengths, styles, constraints) to determine whether scoring biases correlate with specific prompt characteristics or remain consistent across prompt variations.