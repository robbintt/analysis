---
ver: rpa2
title: 'DocTER: Evaluating Document-based Knowledge Editing'
arxiv_id: '2308.09954'
source_url: https://arxiv.org/abs/2308.09954
tags:
- knowledge
- editing
- documents
- evaluation
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocTER, the first evaluation benchmark for
  document-based knowledge editing in large language models. Instead of relying on
  manually labeled factual triples, DocTER enables editing using raw documents containing
  counterfactual knowledge.
---

# DocTER: Evaluating Document-based Knowledge Editing

## Quick Facts
- arXiv ID: 2308.09954
- Source URL: https://arxiv.org/abs/2308.09954
- Authors: 
- Reference count: 12
- Key outcome: Document-based knowledge editing is significantly harder than triple-based methods, with a 10-point performance gap

## Executive Summary
This paper introduces DocTER, the first benchmark for evaluating document-based knowledge editing in large language models. Unlike previous work that relies on manually labeled factual triples, DocTER enables editing using raw documents containing counterfactual knowledge. The benchmark evaluates editing effectiveness across four perspectives: Edit Success, Locality, Reasoning, and Cross-lingual Transfer. Experiments demonstrate that document-based editing presents significantly greater challenges than triple-based methods, with even the best-performing approaches lagging by 10 points in editing success. The study identifies key challenges including difficulties in reasoning with altered knowledge and cross-lingual transfer, while finding that editing middle and feed-forward layers proves most effective.

## Method Summary
DocTER uses an Extract-then-Edit pipeline where documents are first processed to extract factual knowledge triples, which are then used for model editing through fine-tuning or parameter-efficient methods like LoRA. The editing targets specific model components including self-attention and feed-forward layers across different ranges (1-10, 11-20, 21-30). Evaluation employs the COUNTERFACT dataset with counterfactual knowledge queries across four perspectives: DKEE (Edit Success), UKRE (Locality), IKEE (Reasoning), and CKEE (Cross-lingual Transfer). The benchmark generates counterfactual documents using ChatGPT with specific prompts and filtering, then extracts triples for editing.

## Key Results
- Document-based editing shows 10-point lower performance compared to triple-based methods
- Editing middle layers (11-20) and feed-forward components yields the best results
- Cross-lingual transfer remains challenging with significant performance gaps
- Preserving unrelated knowledge continues to be problematic during editing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-based knowledge editing is significantly harder than triple-based methods
- Mechanism: The paper demonstrates that even the best-performing in-context editing approach lags behind by 10 points in editing success when using documents versus gold triples. This suggests that the raw document format introduces additional complexity that existing methods struggle to handle effectively.
- Core assumption: The difficulty stems from the complexity of extracting and applying knowledge from unstructured document text rather than structured triples.
- Evidence anchors:
  - [abstract] states "document-based editing is significantly harder than triple-based methods, with even the best-performing approaches lagging by 10 points in editing success"
  - [section] mentions "Experiments on popular knowledge editing methods demonstrate that editing with documents presents significantly greater challenges than using triples"
- Break condition: If future methods develop more sophisticated document processing pipelines that can extract knowledge as effectively as manual triple labeling, this advantage gap would diminish.

### Mechanism 2
- Claim: Editing middle and feed-forward layers proves most effective
- Mechanism: The paper finds that fine-tuning the middle layers (11-20) and feed-forward components of BLOOM models yields better performance than editing other components. This suggests these layers play a crucial role in knowledge storage and modification.
- Core assumption: The middle layers contain more relevant knowledge representations that can be effectively modified without disrupting overall model functionality.
- Evidence anchors:
  - [section] states "we observe that +LoRA(MLP11-20) outperforms +LoRA(MLP1-10) and +LoRA(MLP 21-30)" and "among all Self-attention layers, +LoRA(Self-attention 11-20) exhibits the best performance"
- Break condition: If future research identifies different layer architectures or knowledge storage patterns in newer model designs, this finding may not generalize.

### Mechanism 3
- Claim: Cross-lingual knowledge transfer remains challenging for existing editing methods
- Mechanism: The paper introduces cross-lingual evaluation and finds that CES scores are significantly lower than ES scores, indicating models struggle to apply edited knowledge across languages even when the editing was done on documents in a different language.
- Core assumption: Cross-lingual transfer requires deeper semantic understanding that current editing methods cannot achieve.
- Evidence anchors:
  - [abstract] mentions "Cross-lingual Transfer (using edited knowledge across languages)" as one evaluation perspective
  - [section] states "CES scores from the CKEE perspective are significantly lower than the ES scores" and "queries for CKEE, which are in a different language from the raw documents, pose a challenge"
- Break condition: If multilingual models develop stronger cross-lingual representations or if editing methods incorporate multilingual alignment strategies, this gap could close.

## Foundational Learning

- Concept: Knowledge editing in language models
  - Why needed here: Understanding how factual knowledge is stored and can be modified in neural networks is fundamental to this work
  - Quick check question: What are the main challenges in modifying specific factual knowledge in large language models without affecting unrelated knowledge?

- Concept: Evaluation metrics for knowledge editing
  - Why needed here: The paper introduces specific metrics (Efficacy Score, Neighborhood Score, etc.) that are essential for assessing editing success
  - Quick check question: How do Efficacy Score and Neighborhood Score differ in measuring knowledge editing effectiveness?

- Concept: Document processing and information extraction
  - Why needed here: The core innovation involves using raw documents instead of triples, requiring understanding of how to extract structured knowledge from unstructured text
  - Quick check question: What are the key challenges in converting document-based knowledge into a format suitable for editing?

## Architecture Onboarding

- Component map: Document generation pipeline -> Knowledge extraction module -> Editing methods (LoRA, fine-tuning) -> Four evaluation frameworks (DKEE, UKRE, IKEE, CKEE). The editing methods modify specific components (MLP layers, attention layers) of the transformer architecture.

- Critical path: Document generation → Knowledge extraction → Model editing → Four-perspective evaluation. The bottleneck appears to be the knowledge extraction and reasoning application steps.

- Design tradeoffs: Using documents provides more natural knowledge representation but introduces complexity in extraction. Parameter-efficient methods like LoRA preserve model capabilities but may be less effective than full fine-tuning. Cross-lingual evaluation adds realism but increases difficulty.

- Failure signatures: Poor DKEE scores indicate editing failure. Low UKRE scores suggest catastrophic forgetting. Poor IKEE results reveal inability to reason with altered knowledge. Low CKEE scores indicate weak cross-lingual transfer.

- First 3 experiments:
  1. Compare ES scores between document-based and triple-based editing on the same model
  2. Test editing different layer ranges (1-10, 11-20, 21-30) to confirm middle layers are most effective
  3. Evaluate cross-lingual transfer by editing with Chinese documents and testing on English queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal layer composition for knowledge editing in transformer models to balance editing effectiveness and preservation of unrelated knowledge?
- Basis in paper: [explicit] The paper finds that editing middle and feed-forward layers is most effective, but preserving unrelated knowledge remains problematic. Different layer combinations (1-10, 11-20, 21-30) show varying performance.
- Why unresolved: While the paper identifies that middle layers (11-20) show the best performance, it doesn't determine the optimal combination of self-attention and feed-forward layers or the exact layer boundaries for knowledge editing.
- What evidence would resolve it: Systematic experiments varying layer combinations and boundaries, measuring both editing success and unrelated knowledge retention across different model architectures.

### Open Question 2
- Question: How can knowledge editing methods be improved to better support cross-lingual knowledge transfer?
- Basis in paper: [explicit] The paper shows significant performance gaps in cross-lingual editing (CES scores much lower than ES scores) and identifies cross-lingual transfer as a major challenge.
- Why unresolved: Current methods struggle with cross-lingual transfer, and the paper doesn't provide solutions for improving this capability beyond identifying the problem.
- What evidence would resolve it: Development and testing of new editing methods specifically designed for cross-lingual scenarios, measuring improvements in cross-lingual efficacy scores.

### Open Question 3
- Question: What factors determine the quality of extracted triples from documents for knowledge editing?
- Basis in paper: [explicit] The paper mentions that document-based editing is harder than triple-based methods and identifies the quality of extracted triples as a key factor influencing performance.
- Why unresolved: The paper doesn't explore what makes some triple extractions more effective than others or how to optimize the extraction process.
- What evidence would resolve it: Analysis of extraction quality metrics, correlation studies between extraction quality and editing success, and development of improved extraction methods.

## Limitations
- Document-based editing shows 10-point performance gap compared to triple-based methods
- Cross-lingual transfer remains challenging with significant performance gaps
- Preserving unrelated knowledge during editing continues to be problematic

## Confidence

**High confidence**: The benchmark framework design and four-perspective evaluation methodology are well-specified and reproducible. The finding that document-based editing is harder than triple-based editing is clearly supported by experimental results with specific performance gaps.

**Medium confidence**: The identification of middle and feed-forward layers as most effective for editing has strong empirical support but lacks theoretical explanation for why these layers are superior. The cross-lingual transfer results are reliable but may be influenced by the specific language pairs tested.

**Low confidence**: The long-term implications of document-based editing difficulty and the generalizability of layer effectiveness findings to different model architectures remain uncertain. The ChatGPT-generated counterfactual documents may introduce evaluation biases that are difficult to quantify.

## Next Checks
1. **Cross-lingual generalization test**: Evaluate the same editing methods across additional language pairs beyond Chinese-English to determine if cross-lingual transfer challenges persist universally or are language-specific.

2. **Layer architecture analysis**: Conduct ablation studies to identify whether specific attention heads or MLP neurons within the effective layer ranges drive performance, rather than the entire layer range.

3. **Document quality impact study**: Systematically vary the quality and structure of input documents to measure how document characteristics affect editing success rates, isolating the document processing bottleneck.