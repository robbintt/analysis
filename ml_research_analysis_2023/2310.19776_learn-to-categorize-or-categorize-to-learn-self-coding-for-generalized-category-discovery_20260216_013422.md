---
ver: rpa2
title: Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category
  Discovery
arxiv_id: '2310.19776'
source_url: https://arxiv.org/abs/2310.19776
tags:
- category
- code
- categories
- tree
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for generalized category discovery,
  which aims to categorize test-time samples into both known and unknown categories.
  It addresses the limitations of traditional supervised models that are restricted
  to predefined categories.
---

# Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery

## Quick Facts
- arXiv ID: 2310.19776
- Source URL: https://arxiv.org/abs/2310.19776
- Reference count: 40
- Primary result: Proposes InfoSieve for generalized category discovery using self-supervised binary coding, achieving state-of-the-art performance on fine-grained datasets while maintaining competitive results on coarse-grained datasets.

## Executive Summary
This paper addresses the challenge of generalized category discovery (GCD), where models must categorize test-time samples into both known and unknown categories. Traditional supervised models struggle with this task due to their restriction to predefined categories. The proposed method, InfoSieve, conceptualizes categories as optimal solutions to a well-defined problem and uses self-supervised learning to extract minimum-length category codes. By combining contrastive learning at both input and code levels with binary mask optimization, InfoSieve discovers hierarchical category structures and achieves superior performance on fine-grained datasets while maintaining competitive results on coarse-grained ones.

## Method Summary
InfoSieve is a framework for generalized category discovery that uses self-supervised learning to extract category codes. It employs a dual-level contrastive learning approach: one on continuous input embeddings to capture broad category information, and another on binary code embeddings to encode fine-grained distinctions. The model learns binary masks to truncate codes to optimal length, controlling category granularity. A feature extractor (pre-trained ViT-B/16) generates embeddings, which are transformed into binary codes by a code generator. A code masker truncates these codes, and a categorizer maps them to ground truth categories using supervision. The method is trained with multiple contrastive and regularization losses over 200 epochs with batch size 128.

## Key Results
- Outperforms state-of-the-art methods on fine-grained datasets (CUB-200, Aircraft, SCars, Oxford-Pet, Herbarium-19)
- Maintains competitive performance on coarse-grained datasets (CIFAR-10/100, ImageNet-100)
- Demonstrates superior generalization to novel categories while preserving accuracy on known categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method finds an implicit binary tree that optimally encodes categories by minimizing both algorithmic mutual information and code length
- Mechanism: The model learns category codes that uniquely identify each sample while ensuring that samples from the same category share a common prefix. This is achieved by maximizing mutual information between input and code, and between category and code, while minimizing code length through binary mask optimization
- Core assumption: An underlying implicit hierarchical tree structure exists in the data that can be discovered through self-supervised learning
- Evidence anchors:
  - [abstract] "we conceptualize a category through the lens of optimization, viewing it as an optimal solution to a well-defined problem"
  - [section 3.1] "Optimizing for these two measures provides an encoding that satisfies the necessary conditions"
  - [corpus] Weak - no direct evidence in corpus papers about implicit tree discovery through self-supervision
- Break condition: If the data lacks hierarchical structure or contains noise that prevents consistent prefix sharing among category members

### Mechanism 2
- Claim: Contrastive learning on both input embeddings and binary codes enables the model to discover categories at different granularities
- Mechanism: Two levels of contrastive learning are used - one on continuous input embeddings to capture broad category information, and another on binary code embeddings to encode fine-grained distinctions. This dual approach allows the model to handle both coarse and fine-grained categories
- Core assumption: Different levels of contrastive learning can capture different aspects of category structure
- Evidence anchors:
  - [section 4.1] "One of the advantages of contrastive learning is to find a representation that maximizes the mutual information with the input"
  - [section 4.1] "Minimizing Contrastive Loss on the Codes... effectively decrease n to 2 while actively minimizing d"
  - [corpus] Moderate - several GCD papers use contrastive learning but not with this dual-level approach
- Break condition: If the input embeddings don't capture sufficient category-relevant information, or if binary codes become too coarse

### Mechanism 3
- Claim: Binary mask optimization controls code length to balance between category granularity and representation efficiency
- Mechanism: The model learns a binary mask that truncates the category code to optimal length. The mask is optimized to minimize the number of ones while ensuring they appear at the beginning of the sequence, effectively controlling how fine-grained the categorization should be
- Core assumption: There exists an optimal code length that balances information preservation and computational efficiency
- Evidence anchors:
  - [section 3.2] "To find the optimal code lengths li in equation 10, we have to minimize the total length of the latent code"
  - [section 3.2] "Consider a masked version of zi=zi1···ziL, which we will denote as ˜zi=˜zi1···˜ziL"
  - [corpus] Weak - no direct evidence in corpus papers about this specific mask-based length control mechanism
- Break condition: If the optimal code length varies significantly across categories, or if mask optimization becomes unstable

## Foundational Learning

- Concept: Shannon mutual information and its relationship to algorithmic mutual information
  - Why needed here: The paper uses Shannon mutual information as a computable approximation for the theoretically optimal but incomputable algorithmic mutual information
  - Quick check question: Why can we use Shannon mutual information instead of algorithmic mutual information for practical implementation?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Contrastive learning is the core mechanism for maximizing mutual information between inputs and learned representations at multiple levels
  - Quick check question: How does minimizing InfoNCE loss relate to maximizing mutual information between inputs and learned embeddings?

- Concept: Binary tree representations and hierarchical encoding
  - Why needed here: The method conceptualizes categories as paths in a binary tree, where each bit represents a decision at a node
  - Quick check question: Why does using binary codes naturally lead to hierarchical category representations?

## Architecture Onboarding

- Component map: FeatureExtractor -> CodeGenerator -> CodeMasker -> Categorizer
- Critical path: FeatureExtractor → CodeGenerator → CodeMasker → Categorizer
  - The contrastive losses flow through the first two components, while mask optimization affects the third, and supervision is applied at the final stage
- Design tradeoffs:
  - Using binary codes provides natural hierarchy but limits expressiveness compared to continuous embeddings
  - The dual contrastive learning approach captures both broad and fine-grained information but increases complexity
  - Mask-based length control is flexible but introduces hyperparameters that affect granularity
- Failure signatures:
  - Poor performance on known categories suggests insufficient information preservation in the code generation stage
  - Poor performance on novel categories suggests inadequate exploration of the code space or overly aggressive masking
  - Instability during training may indicate improper balance between the multiple loss terms
- First 3 experiments:
  1. Train with only the input contrastive loss (LC_in) to verify basic embedding quality
  2. Add code contrastive loss (LC_code) while keeping mask optimization disabled to test binary code learning
  3. Enable mask optimization with fixed mask length to evaluate code truncation effects before full training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the InfoSieve model perform on datasets with more than 200 categories, especially in the fine-grained category setting?
- Basis in paper: [inferred] The paper evaluates the model on datasets with up to 200 categories, but does not explore its performance on larger datasets.
- Why unresolved: The paper does not provide experimental results on datasets with more than 200 categories.
- What evidence would resolve it: Conducting experiments on datasets with more than 200 categories, especially in the fine-grained category setting, would provide evidence of the model's performance on larger datasets.

### Open Question 2
- Question: How does the InfoSieve model handle datasets with overlapping categories or categories with hierarchical relationships?
- Basis in paper: [inferred] The paper does not explicitly discuss the model's performance on datasets with overlapping categories or hierarchical relationships.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the model's ability to handle overlapping categories or hierarchical relationships.
- What evidence would resolve it: Conducting experiments on datasets with overlapping categories or hierarchical relationships would provide evidence of the model's ability to handle such scenarios.

### Open Question 3
- Question: How does the InfoSieve model perform on datasets with noisy or incomplete labels?
- Basis in paper: [explicit] The paper mentions the label inconsistency problem in conventional supervised learning models, but does not explore the model's performance on datasets with noisy or incomplete labels.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the model's robustness to noisy or incomplete labels.
- What evidence would resolve it: Conducting experiments on datasets with noisy or incomplete labels would provide evidence of the model's robustness to label noise and incompleteness.

## Limitations
- The theoretical foundation for binary code optimality relies heavily on algorithmic mutual information, which is incomputable in practice
- The binary tree assumption may not hold for all datasets, particularly those with non-hierarchical or overlapping category structures
- The dual contrastive learning approach increases complexity without clear ablation studies showing the necessity of both levels

## Confidence

- **High Confidence**: The empirical results on benchmark datasets (CIFAR-10/100, ImageNet-100) showing superior performance on fine-grained categories. The experimental methodology is clearly described and reproducible.
- **Medium Confidence**: The theoretical framework connecting binary codes to optimal category representations. While the proof structure is sound, the practical implications of using Shannon vs. algorithmic mutual information remain unclear.
- **Low Confidence**: The claim that InfoSieve naturally discovers hierarchical category structures through self-supervised learning. The binary mask optimization mechanism's effectiveness across diverse datasets needs more validation.

## Next Checks

1. **Ablation Study**: Systematically remove either the input contrastive loss or code contrastive loss to quantify their individual contributions to performance, particularly on fine-grained vs. coarse-grained datasets.

2. **Theoretical Validation**: Implement synthetic datasets with known hierarchical structures to empirically verify the binary tree discovery claims and test the limits of the Shannon mutual information approximation.

3. **Generalization Test**: Apply InfoSieve to datasets with explicitly non-hierarchical category relationships (e.g., disjoint feature spaces) to assess robustness beyond the assumed tree structure.