---
ver: rpa2
title: Rethinking Gauss-Newton for learning over-parameterized models
arxiv_id: '2302.02904'
source_url: https://arxiv.org/abs/2302.02904
tags:
- learning
- convergence
- gradient
- gauss-newton
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the continuous-time Gauss-Newton method for
  training over-parameterized neural networks in the mean-field regime. The authors
  prove that under suitable initialization conditions, the method converges globally
  at a linear rate faster than gradient descent.
---

# Rethinking Gauss-Newton for learning over-parameterized models

## Quick Facts
- arXiv ID: 2302.02904
- Source URL: https://arxiv.org/abs/2302.02904
- Reference count: 40
- This paper analyzes the continuous-time Gauss-Newton method for training over-parameterized neural networks in the mean-field regime.

## Executive Summary
This paper analyzes the continuous-time Gauss-Newton method for training over-parameterized neural networks in the mean-field regime. The authors prove that under suitable initialization conditions, the method converges globally at a linear rate faster than gradient descent. Empirically, they demonstrate that Gauss-Newton can operate in both kernel and feature learning regimes, but solutions found in the feature learning regime exhibit worse generalization compared to gradient descent, suggesting a trade-off between optimization speed and implicit bias.

## Method Summary
The study compares Gauss-Newton (GN) and Gradient Descent (GD) for training two-layer neural networks with SiLU activation on synthetic regression tasks. The setup uses M = 5000 hidden units with weights initialized from small Gaussian noise. GN employs damping ε(w) = max(ε₀, σ⋆(Aw)) and fixed learning rate λ = 10, while GD uses λ = 10. The experiments track training and test loss, along with the smallest singular value of the Neural Tangent Kernel matrix, across varying initialization scales τ₀ ∈ {0.5, 1, 5, 10, 100} and dataset sizes N ∈ {100, 500, 1000, 1500, 2000}.

## Key Results
- Gauss-Newton achieves faster convergence than gradient descent in both kernel and feature learning regimes
- Solutions found by Gauss-Newton in the feature learning regime exhibit worse generalization than gradient descent
- The optimization speed of Gauss-Newton can be detrimental to generalization in the feature learning regime
- Initialization scale controls whether the method operates in kernel or feature learning regime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gauss-Newton dynamics achieve faster convergence than gradient descent in the over-parameterized regime due to improved conditioning.
- Mechanism: The pre-conditioning matrix in Gauss-Newton adapts step sizes according to local curvature information, reducing the condition number of the optimization problem.
- Core assumption: The Hessian-related structure remains well-conditioned and the damping parameter appropriately balances convergence speed and stability.
- Evidence anchors:
  - [abstract]: "under suitable initialization conditions, the method converges globally at a linear rate faster than gradient descent"
  - [section]: "the convergence rate is controlled by the smallest singular value of the NTKAw which can get arbitrarily small"
  - [corpus]: Found 25 related papers discussing Gauss-Newton convergence properties, with 5 specifically addressing over-parameterized neural networks
- Break condition: The mechanism breaks when the Jacobian becomes singular or when the damping parameter is poorly chosen, leading to numerical instability.

### Mechanism 2
- Claim: Gauss-Newton can operate in both kernel and feature learning regimes depending on initialization scale.
- Mechanism: Large initial weight variance leads to kernel regime behavior where only the final layer is learned, while small initial variance enables feature learning where hidden layers adapt.
- Core assumption: The initialization scale directly controls which regime the optimization dynamics fall into.
- Evidence anchors:
  - [abstract]: "GN exhibits both a kernel regime where it generalizes as well as gradient flows, and a feature learning regime"
  - [section]: "the gradient ﬂow can run under two regimes which yield solutions that possess qualitatively different generalization properties depending on the variance of initial weights"
  - [corpus]: Corpus includes papers on kernel methods and feature learning, supporting the dual-regime claim
- Break condition: The mechanism breaks when the initialization scale is neither clearly large nor small, creating ambiguous regime behavior.

### Mechanism 3
- Claim: Gauss-Newton's faster convergence can be detrimental to generalization in the feature learning regime.
- Mechanism: The optimization speed of Gauss-Newton is too fast compared to gradient descent, preventing sufficient time for learning features with good generalization properties.
- Core assumption: There exists an implicit bias in gradient descent that requires more iterations to manifest optimal generalization properties.
- Evidence anchors:
  - [abstract]: "solutions found in the feature learning regime exhibit worse generalization compared to gradient descent"
  - [section]: "While GN is consistently faster than GD in finding a global optimum, the learned model generalizes well on test data when starting from random initial weights with a small variance and using a small step size to slow down convergence"
  - [corpus]: Corpus signals include papers on implicit bias and generalization, though evidence is limited
- Break condition: The mechanism breaks when the optimization speed is deliberately slowed down or when the implicit bias of gradient descent is not relevant to the problem.

## Foundational Learning

- Concept: Mean-field limit for neural networks
  - Why needed here: The paper analyzes convergence in the mean-field regime where the number of neurons tends to infinity with appropriate scaling
  - Quick check question: What distinguishes the mean-field regime from the neural tangent kernel regime in terms of feature learning capabilities?

- Concept: Implicit bias in optimization
  - Why needed here: The paper discusses how different optimization methods select different solutions among infinitely many global minima, affecting generalization
  - Quick check question: How does the implicit bias of gradient descent differ from that of Gauss-Newton in the feature learning regime?

- Concept: Wasserstein gradient flows
  - Why needed here: The paper mentions that in the mean-field limit, the dynamics of neural network weights are described by a Wasserstein gradient flow
  - Quick check question: What role does the Wasserstein geometry play in characterizing the convergence of over-parameterized networks?

## Architecture Onboarding

- Component map: Synthetic data generation -> Two-layer SiLU network with M=5000 units -> GD/GN training with damping -> Convergence and generalization evaluation
- Critical path:
  1. Initialize network weights with controlled variance
  2. Select optimization method (GD or GN)
  3. Monitor convergence of training objective
  4. Evaluate generalization performance on test data
  5. Analyze implicit bias effects on solution quality
- Design tradeoffs:
  - Optimization speed vs generalization: GN converges faster but may generalize worse in feature learning regime
  - Computational cost vs approximation quality: Exact GN steps vs approximate methods for large-scale problems
  - Initialization scale: Controls regime selection but affects convergence behavior
- Failure signatures:
  - Numerical instability when Jacobian becomes singular
  - Poor generalization in feature learning regime despite fast convergence
  - Regime ambiguity when initialization scale is intermediate
  - Slow convergence in kernel regime despite theoretical advantages
- First 3 experiments:
  1. Compare convergence rates of GD and GN on synthetic regression task with varying initialization scales
  2. Evaluate generalization performance in both kernel and feature learning regimes for different optimization methods
  3. Analyze the evolution of the smallest singular value of the Neural Tangent Kernel during training for both methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Gauss-Newton's faster convergence rate come at the cost of selecting solutions with worse generalization properties compared to gradient descent, even in the feature learning regime?
- Basis in paper: [explicit] The paper states that Gauss-Newton "induces an implicit bias for selecting global solutions that systematically under-performs those found by a gradient flow" and that "GN is faster to optimize the training objective, it is often too fast... to allow enough time for learning features with comparable statistical benefit."
- Why unresolved: While the paper demonstrates this empirically, it does not provide a theoretical explanation for why this trade-off exists or under what conditions it might be mitigated.
- What evidence would resolve it: A theoretical analysis proving that Gauss-Newton's optimization speed inherently limits its ability to learn optimal features, or experiments showing that modifying Gauss-Newton (e.g., adding regularization or slowing convergence) can improve its generalization without sacrificing speed.

### Open Question 2
- Question: Can the implicit bias of Gauss-Newton be characterized theoretically to understand why it selects solutions with worse generalization properties compared to gradient descent?
- Basis in paper: [inferred] The paper mentions that characterizing the implicit bias of Gauss-Newton is an important open question and suggests using approaches like those introduced in (Mulayoff et al., 2021) for gradient descent.
- Why unresolved: While the paper demonstrates empirically that Gauss-Newton's solutions generalize worse, it does not provide a theoretical framework for understanding the underlying mechanism.
- What evidence would resolve it: A theoretical analysis of Gauss-Newton's implicit bias, potentially building on existing work on gradient descent's implicit bias, or experiments showing that modifying Gauss-Newton's update rule can improve its generalization properties.

### Open Question 3
- Question: Is there a way to design a Gauss-Newton-like optimization method that trades off optimization speed and implicit bias to achieve both fast convergence and good generalization properties?
- Basis in paper: [explicit] The paper suggests that "new methods exploiting curvature information should trade off between optimization speed and implicit bias" and that "the study suggests the need to go beyond improving the computational cost of GN for over-parametrized models towards designing new methods that can trade off optimization speed and the quality of their implicit bias."
- Why unresolved: While the paper identifies the need for such methods, it does not propose any specific approaches or analyze their potential benefits and drawbacks.
- What evidence would resolve it: The development and empirical evaluation of a new optimization method that incorporates aspects of Gauss-Newton while also incorporating mechanisms to control its implicit bias, such as regularization or adaptive learning rates.

## Limitations

- Theoretical analysis is limited to the mean-field regime, which may not fully capture finite-width network behavior
- Empirical validation is restricted to synthetic regression tasks with specific network architectures
- The study does not explore deeper architectures or real-world datasets
- The analysis of implicit bias effects on generalization, while insightful, relies on indirect evidence

## Confidence

- Gauss-Newton convergence speed claims: High confidence in kernel regime, Medium confidence in feature learning regime
- Generalization trade-off claims: Medium confidence due to limited empirical scope
- Regime transition analysis: High confidence in theoretical framework, Medium confidence in practical implications

## Next Checks

1. Verify the implementation of the damping mechanism ε(w) = max(ε₀, σ⋆(Aw)) and its numerical stability during training
2. Test the regime transition behavior across a wider range of initialization scales to confirm the kernel/feature learning boundary
3. Evaluate whether the observed generalization trade-off persists when using different activation functions or network architectures