---
ver: rpa2
title: 'SALM: Speech-augmented Language Model with In-context Learning for Speech
  Recognition and Translation'
arxiv_id: '2310.09424'
source_url: https://arxiv.org/abs/2310.09424
tags:
- speech
- salm
- in-context
- learning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SALM combines a frozen LLM with a speech encoder and LoRA layers
  to perform multitask speech-language modeling. The unified model achieves performance
  on par with task-specific Conformer baselines for ASR (2.4 WER on LibriSpeech) and
  AST (30.7-16.8 BLEU on MuST-C) through instruction tuning.
---

# SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation

## Quick Facts
- arXiv ID: 2310.09424
- Source URL: https://arxiv.org/abs/2310.09424
- Reference count: 0
- SALM achieves performance on par with task-specific Conformer baselines for ASR (2.4 WER on LibriSpeech) and AST (30.7-16.8 BLEU on MuST-C) through instruction tuning

## Executive Summary
SALM introduces a novel speech-augmented language model that combines a frozen LLM with a speech encoder and LoRA layers to perform multitask speech-language modeling. The unified model achieves competitive performance on standard ASR and AST benchmarks while introducing the capability for zero-shot in-context learning, demonstrated through keyword-boosting tasks that improve recognition without external biasing graphs. The key innovation lies in bridging the gap between LLM pretraining and speech domain requirements through speech supervised in-context training.

## Method Summary
SALM combines a frozen text LLM (Megatron 2B) with an audio encoder (Fast Conformer-large) and modality adapter modules, along with LoRA layers to accommodate speech input and associated task instructions. The model is trained through multitask speech instruction tuning on paired speech and text data from ASR and AST public corpora. Speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks by augmenting training data with optional text context containing keywords. The model is open-sourced via the NeMo toolkit.

## Key Results
- Achieves 2.4 WER on LibriSpeech (clean/other) matching Conformer baselines
- Achieves 30.7-16.8 BLEU scores on MuST-C (en-de/en-ja) matching task-specific baselines
- Demonstrates zero-shot in-context learning for keyword boosting tasks, improving ASR/WER and AST/BLEU scores without external biasing graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SALM leverages in-context learning to improve keyword recognition without external biasing.
- Mechanism: The LLM component of SALM is prompted with keyword examples in the text context before processing speech, allowing it to learn the desired keywords on-the-fly during inference.
- Core assumption: The LLM can generalize keyword recognition patterns from the text prompt to the speech input domain.
- Evidence anchors:
  - [abstract] "SALM...exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST."
  - [section] "We define the in-context learning here as: learning the boosted words from the prompting text context, without back-propagation."
  - [corpus] "Weak evidence - related papers focus on LLM integration but do not directly test keyword boosting via in-context learning."

### Mechanism 2
- Claim: Speech supervised in-context training (Speech ICT) improves the transfer of textual knowledge to speech understanding.
- Mechanism: By augmenting training data with optional text context containing keywords, SALM learns to leverage text context for speech tasks, bridging the gap between LLM pretraining and speech domain requirements.
- Core assumption: Including text context during training teaches the model to use that context during inference for improved performance.
- Evidence anchors:
  - [abstract] "Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models."
  - [section] "In the speech instruction tuning stage, we augment the same supervised data by randomly sampling words...and including them as the optional text context for the utterance."
  - [corpus] "Weak evidence - while related work exists on LLM fine-tuning, direct evidence for speech-supervised in-context training is limited."

### Mechanism 3
- Claim: The multimodal adapter and LoRA layers enable effective integration of speech and text modalities within the frozen LLM.
- Mechanism: The adapter and LoRA layers transform speech features to match the LLM's input space and allow fine-tuning of the model for speech tasks without modifying the base LLM weights.
- Core assumption: The adapter layers can effectively bridge the modality gap between continuous speech features and discrete text embeddings.
- Evidence anchors:
  - [abstract] "SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions."
  - [section] "To guide LLM to condition on outputs from the audio encoder, we introduce modality adapter and LoRA layers...train these layers through multitask speech instruction tuning."
  - [corpus] "Moderate evidence - adapter-based multimodal integration is a common approach, though specific effectiveness for SALM's architecture needs further validation."

## Foundational Learning

- Concept: In-context learning
  - Why needed here: SALM relies on in-context learning to improve keyword recognition and other speech tasks without additional fine-tuning.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are its limitations for speech tasks?

- Concept: Multitask learning
  - Why needed here: SALM is trained on multiple speech tasks (ASR, AST) simultaneously to leverage shared representations and improve generalization.
  - Quick check question: What are the benefits and challenges of multitask learning for speech models, and how does SALM address them?

- Concept: Modality adaptation
  - Why needed here: SALM must bridge the gap between speech and text modalities to effectively use an LLM for speech tasks.
  - Quick check question: What are the key challenges in adapting speech features to match text embeddings, and how do SALM's adapter layers address them?

## Architecture Onboarding

- Component map: Speech input → Audio encoder (Fast Conformer) → Modality adapter layers → LoRA layers → Frozen LLM (Megatron) → Text output
- Critical path: Speech input → Audio encoder → Modality adapter → LLM inference
  - The audio encoder and adapter layers are critical for proper feature transformation
- Design tradeoffs:
  - Freezing the LLM limits adaptation but preserves pre-trained capabilities
  - Using adapters adds complexity but enables multimodal integration
  - In-context learning avoids fine-tuning but may have limitations for complex tasks
- Failure signatures:
  - Poor ASR/WER or AST/BLEU scores indicate issues with the audio encoder or adapter layers
  - Inability to improve with keyword boosting suggests problems with in-context learning or text context integration
  - Hallucinations or deletions in output may indicate limitations of the LLM or adapter layers
- First 3 experiments:
  1. Test the audio encoder and adapter layers separately to ensure proper feature transformation
  2. Evaluate the in-context learning ability using a simple keyword boosting task
  3. Compare the performance of SALM with and without speech supervised in-context training to validate its effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the hallucination and deletion problems in LLM-based SALM be effectively resolved?
- Basis in paper: [explicit] The paper mentions that SALM suffers from hallucination and long-form deletion problems compared to baseline models.
- Why unresolved: The paper acknowledges these issues but does not provide a concrete solution or methodology to address them.
- What evidence would resolve it: Experimental results demonstrating improved performance metrics (e.g., reduced hallucination and deletion rates) after implementing proposed solutions to these problems.

### Open Question 2
- Question: What is the optimal positive ratio for speech supervised in-context training (Speech ICT) to maximize in-context learning performance?
- Basis in paper: [explicit] The paper discusses the effect of positive ratio on inference precision and recall but does not identify an optimal value.
- Why unresolved: The paper only provides a comparison of different positive ratios without determining the best one for maximizing performance.
- What evidence would resolve it: Empirical results showing the performance of SALM with different positive ratios, identifying the ratio that yields the highest in-context learning performance.

### Open Question 3
- Question: How can SALM be improved to handle long contexts more effectively?
- Basis in paper: [explicit] The paper mentions that SALM suffers from issues when scaling up the number of boosted words, suggesting problems with handling long contexts.
- Why unresolved: The paper does not provide a detailed analysis or solution to improve SALM's ability to handle long contexts.
- What evidence would resolve it: Experimental results demonstrating improved performance metrics (e.g., precision and recall) when handling longer contexts after implementing proposed improvements.

## Limitations
- Performance advantages over Conformer baselines are modest and may not generalize beyond the tested benchmarks
- In-context learning capabilities are primarily demonstrated on keyword-boosting tasks from proprietary dataset, limiting external validation
- The model architecture's effectiveness depends on specific combination of frozen LLM, audio encoder, and adapter configuration

## Confidence

**High Confidence Claims:**
- SALM successfully integrates speech and text modalities through adapter layers to produce reasonable ASR and AST outputs (validated by competitive WER/BLEU scores on standard benchmarks)
- The model architecture can process speech inputs and generate text outputs for multiple tasks (demonstrated through LibriSpeech and MuST-C evaluations)

**Medium Confidence Claims:**
- SALM exhibits genuine in-context learning capabilities for keyword boosting (supported by F-score improvements on GTC dataset, though limited to specific task type)
- Speech supervised in-context training provides meaningful improvements to in-context learning ability (evidenced by ablation studies, but magnitude varies)

**Low Confidence Claims:**
- SALM achieves "performance on par with task-specific Conformer baselines" (claims are based on modest improvements that may not generalize)
- The in-context learning mechanism generalizes beyond keyword boosting to broader speech understanding tasks (not extensively validated beyond the primary demonstration)

## Next Checks

**Validation Check 1:** Conduct a systematic ablation study removing speech supervised in-context training to quantify its contribution across multiple languages and task types, including testing on public keyword datasets beyond the proprietary GTC corpus.

**Validation Check 2:** Evaluate SALM's in-context learning capabilities on a broader range of speech understanding tasks beyond keyword boosting, such as named entity recognition, semantic understanding, or speech-to-speech translation to assess generalization.

**Validation Check 3:** Test the model's robustness to prompt variations by systematically modifying instruction prompts and context lengths to determine the sensitivity of in-context learning performance to prompt engineering choices.