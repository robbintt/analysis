---
ver: rpa2
title: Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination
  in Vision-Language Models
arxiv_id: '2310.05338'
source_url: https://arxiv.org/abs/2310.05338
tags:
- object
- question
- negp
- image
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces NOPE, a benchmark for measuring object hallucination
  in vision-language models. The authors propose a scalable method using large language
  models to generate high-quality synthetic data where the correct answer is a negative
  indefinite pronoun, indicating the absence of an object.
---

# Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2310.05338
- **Source URL**: https://arxiv.org/abs/2310.05338
- **Reference count**: 40
- **Primary result**: All 10 evaluated VL models hallucinate objects frequently, with accuracies below 10% on negative object presence questions.

## Executive Summary
This paper introduces NOPE, a benchmark for measuring object hallucination in vision-language models. The authors develop a scalable method using large language models to generate high-quality synthetic data where the correct answer is a negative indefinite pronoun, indicating the absence of an object. By evaluating 10 state-of-the-art VL models on this data, they find that all models hallucinate objects frequently, with accuracies below 10% on negative object presence questions. The results reveal that lexical diversity, question type, and object-scene relevance significantly impact hallucination rates, with models more prone to hallucination on diverse questions, location/object/color queries, and scene-relevant objects.

## Method Summary
The paper proposes using instruction-tuned large language models with a "list-then-rewrite" prompting strategy to generate synthetic negative pronoun (NegP) VQA data from image captions. This approach decomposes the complex task of generating negative questions into listing objects not mentioned in captions and rewriting reference questions. The generated data is automatically validated using NLI and image-QA classification models, then combined with existing VQA datasets to create a balanced benchmark. The NOPE benchmark consists of ~30k development and ~36k test samples, maintaining near-balanced proportions of NegP and others data. The paper evaluates 10 state-of-the-art VL models on this benchmark using accuracy, METEOR, and NegP accuracy metrics.

## Key Results
- All 10 VL models evaluated achieve accuracies below 10% on negative object presence questions, demonstrating widespread object hallucination.
- Lexical diversity significantly impacts hallucination rates, with more diverse questions leading to higher hallucination errors.
- Question types with large scopes (location, object, color) show higher hallucination rates than other types.
- Scene-relevant objects are more likely to be hallucinated than loosely related or unrelated objects.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can generate high-quality synthetic data for evaluating object hallucination by leveraging instruction-tuned prompting strategies.
- Mechanism: The paper proposes using multi-turn prompting instruction-tuned large language models to generate synthetic negative pronoun (NegP) data, which allows for scalable and cost-effective creation of evaluation data that accurately captures object hallucination scenarios.
- Core assumption: Instruction-tuned LLMs can understand complex tasks like generating questions with negative indefinite pronouns as answers when properly prompted.
- Evidence anchors:
  - [abstract] "We propose a cost-effective and scalable approach utilizing large language models to generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE."
  - [section] "We utilize an image captioning dataset... We rely on ci to describe the objects and the relationship between objects depicted in vi."
  - [corpus] Weak - the corpus doesn't directly address this mechanism, but the paper mentions using existing image captioning datasets as input.
- Break condition: If the LLMs fail to understand the complex task of generating questions with negative indefinite pronouns, or if the prompting strategy is not effective in eliciting the desired behavior from the models.

### Mechanism 2
- Claim: Decomposing complex tasks into simpler subtasks improves the quality of generated data for evaluating object hallucination.
- Mechanism: The paper proposes a "list-then-rewrite" approach where the LLM first lists objects not mentioned in the image caption, then rewrites reference questions to include these objects, resulting in higher quality NegP questions compared to a single-step "generate-from-scratch" approach.
- Core assumption: Breaking down the task of generating NegP questions into listing objects and then rewriting questions improves the LLMs' ability to handle the complexity of the task.
- Evidence anchors:
  - [section] "From this human evaluation result, we can conjecture that the generate-from-scratch prompting method is not reliable and fails to elicit the LLMs' understanding of complex tasks such as question generation."
  - [section] "Using the list-then-rewrite method, we generate 29.5k NegP VQA data to build the NOPE dataset from OpenImagesV7 (Kuznetsova et al., 2020)."
  - [corpus] Weak - the corpus doesn't directly address this mechanism, but the paper mentions using OpenImagesV7 as the source for image captions.
- Break condition: If the LLM fails to accurately list objects not mentioned in the image caption, or if the reference question rewriting step introduces inconsistencies or errors.

### Mechanism 3
- Claim: Evaluating object hallucination in vision-language models requires a balanced dataset with diverse question types and object-scene relevance.
- Mechanism: The NOPE benchmark includes NegP questions with varying lexical diversity, question types (e.g., color, object, location), and object-scene relevance (related, loosely related, unrelated), providing a comprehensive evaluation of object hallucination across different scenarios.
- Core assumption: A balanced and diverse dataset is necessary to accurately assess the susceptibility of VL models to object hallucination under various conditions.
- Evidence anchors:
  - [abstract] "Furthermore, we uncover that lexically diverse visual questions, question types with large scopes, and scene-relevant objects capitalize the risk of object hallucination in VL models."
  - [section] "Table 1 describes the data distribution of the dev and test sets of the benchmark. Each set respectively comprises ~30k and ~36k data, maintaining near-balanced proportions of NegP and Others data."
  - [corpus] Weak - the corpus doesn't directly address this mechanism, but the paper mentions using various VQA datasets to construct the benchmark.
- Break condition: If the dataset is not sufficiently balanced or diverse, or if the evaluation fails to capture the nuances of object hallucination across different scenarios.

## Foundational Learning

- Concept: Visual Question Answering (VQA)
  - Why needed here: Understanding the basics of VQA is crucial for comprehending the task of evaluating object hallucination in VL models, as the paper proposes a benchmark based on VQA tasks.
  - Quick check question: What is the main goal of VQA, and how does it differ from other vision-language tasks like image captioning?

- Concept: Object Hallucination
  - Why needed here: Object hallucination is the central phenomenon being evaluated in the paper, so understanding its definition and implications is essential for grasping the significance of the NOPE benchmark.
  - Quick check question: How does object hallucination differ from incorrectness in the context of VL models, and why is it a critical issue to address?

- Concept: Instruction-tuned Large Language Models
  - Why needed here: The paper leverages instruction-tuned LLMs for generating synthetic data, so understanding their capabilities and limitations is important for evaluating the effectiveness of the proposed approach.
  - Quick check question: What are the key differences between instruction-tuned LLMs and other types of language models, and how do these differences impact their ability to generate high-quality data for evaluating object hallucination?

## Architecture Onboarding

- Component map: Image Captioning Dataset -> Large Language Models (List-then-Rewrite) -> Automatic Validation -> Human Evaluation -> NOPE Benchmark
- Critical path: Image Captioning Dataset → Large Language Models (List-then-Rewrite) → Automatic Validation → Human Evaluation → NOPE Benchmark
- Design tradeoffs:
  - Using instruction-tuned LLMs allows for scalable and cost-effective data generation but may introduce biases or limitations based on the model's training data and instruction tuning process.
  - The "list-then-rewrite" approach improves data quality compared to "generate-from-scratch" but requires more complex prompting and validation steps.
  - Balancing the dataset with diverse question types and object-scene relevance provides a comprehensive evaluation but may introduce additional complexity in data generation and analysis.
- Failure signatures:
  - Low validity of generated questions based on automatic or human evaluation metrics.
  - VL models performing well on the benchmark despite exhibiting object hallucination in real-world scenarios.
  - Imbalanced or insufficient data distribution across question types or object-scene relevance categories.
- First 3 experiments:
  1. Evaluate the quality of generated NegP questions using automatic validation methods (e.g., NLI and image-QA pair classification) and compare the results across different LLM models and prompting strategies.
  2. Conduct human evaluation of a subset of generated questions to assess the agreement between automatic validation and human judgment, and identify areas for improvement in the data generation process.
  3. Analyze the performance of various VL models on the NOPE benchmark, focusing on their accuracy on NegP questions and the impact of lexical diversity, question type, and object-scene relevance on their object hallucination rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does object-scene relevance specifically impact the performance of zero-shot models compared to fine-tuned models on NOPE?
- Basis in paper: [explicit] The paper states that zero-shot models show less pronounced differences in performance based on object-scene relevance compared to fine-tuned models, suggesting a different understanding of negative object presence.
- Why unresolved: The paper doesn't provide a detailed analysis of why this difference exists or how it might be leveraged to improve zero-shot models.
- What evidence would resolve it: Further experiments comparing zero-shot and fine-tuned models' performance across varying levels of object-scene relevance, with ablation studies to isolate the impact of pre-training data and fine-tuning strategies.

### Open Question 2
- Question: Can the proposed list-then-rewrite method be generalized to generate NegP data for other tasks beyond VQA, such as image captioning or visual reasoning?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of list-then-rewrite for generating high-quality NegP VQA data, suggesting its potential applicability to other vision-language tasks.
- Why unresolved: The paper doesn't explore the use of list-then-rewrite for other tasks or provide insights into the challenges and limitations of such an extension.
- What evidence would resolve it: Applying list-then-rewrite to generate NegP data for other vision-language tasks and evaluating the quality and impact of the generated data on model performance.

### Open Question 3
- Question: How does the performance of VL models on NOPE correlate with their performance on other VQA benchmarks, and what does this reveal about the relationship between object hallucination and general VQA capabilities?
- Basis in paper: [explicit] The paper shows that models perform significantly worse on NOPE compared to other VQA benchmarks, indicating a specific vulnerability to object hallucination.
- Why unresolved: The paper doesn't investigate the correlation between NOPE performance and other VQA metrics or explore the implications for model design and evaluation.
- What evidence would resolve it: Analyzing the correlation between NOPE scores and other VQA metrics across different models and datasets, and investigating whether specific model architectures or training strategies are more effective at addressing both object hallucination and general VQA challenges.

### Open Question 4
- Question: What are the underlying mechanisms that cause VL models to hallucinate objects, and how can these mechanisms be addressed through architectural or training modifications?
- Basis in paper: [explicit] The paper identifies lexical diversity, question type, and object-scene relevance as factors influencing object hallucination, but doesn't delve into the root causes of this phenomenon.
- Why unresolved: The paper focuses on measuring and analyzing object hallucination rather than explaining its underlying mechanisms or proposing solutions to mitigate it.
- What evidence would resolve it: Conducting in-depth analyses of model attention patterns, feature representations, and decision-making processes during object hallucination, and exploring the effectiveness of different architectural or training modifications in reducing hallucination rates.

## Limitations
- The benchmark relies on English language data and Western-centric imagery, potentially limiting generalizability to other cultural contexts.
- The paper doesn't address more complex hallucination phenomena like attribute confusion or compositional errors.
- The synthetic data generation process may introduce biases based on the LLM's training data and instruction tuning.

## Confidence
- High: Core claim that object hallucination is a widespread issue in VL models, supported by consistently low accuracies (<10%) across 10 different models.
- Medium: Synthetic data generation approach, as the paper demonstrates quality through automatic validation and human evaluation, but lacks extensive ablation studies on alternative generation methods.
- Low: Universality of findings, given the benchmark's reliance on specific image sources (Open Images V7) and language patterns that may not generalize to all visual domains.

## Next Checks
1. Test whether VL models that perform poorly on NOPE also exhibit higher object hallucination rates in real-world applications, establishing ecological validity.
2. Conduct cross-linguistic evaluation of NOPE to determine if hallucination patterns are consistent across different languages and cultural contexts.
3. Implement an ablation study comparing NOPE results with benchmarks using human-generated negative questions to quantify the impact of synthetic data generation on measurement accuracy.