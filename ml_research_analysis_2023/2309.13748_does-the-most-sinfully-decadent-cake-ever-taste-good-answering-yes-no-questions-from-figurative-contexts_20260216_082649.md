---
ver: rpa2
title: Does the "most sinfully decadent cake ever" taste good? Answering Yes/No Questions
  from Figurative Contexts
arxiv_id: '2309.13748'
source_url: https://arxiv.org/abs/2309.13748
tags:
- figurative
- contexts
- language
- chatgpt
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FigurativeQA, a dataset of 1000 yes/no questions
  with figurative and non-figurative contexts from restaurant and product reviews.
  The authors demonstrate that state-of-the-art QA models like RoBERTa perform significantly
  worse (up to 15% drop) on questions from figurative contexts compared to non-figurative
  ones.
---

# Does the "most sinfully decadent cake ever" taste good? Answering Yes/No Questions from Figurative Contexts

## Quick Facts
- **arXiv ID**: 2309.13748
- **Source URL**: https://arxiv.org/abs/2309.13748
- **Reference count**: 14
- **Primary result**: QA models perform significantly worse (up to 15% drop) on figurative contexts compared to non-figurative ones

## Executive Summary
This paper introduces FigurativeQA, a dataset of 1000 yes/no questions with figurative and non-figurative contexts from restaurant and product reviews. The authors demonstrate that state-of-the-art QA models like RoBERTa perform significantly worse (up to 15% drop) on questions from figurative contexts compared to non-figurative ones. They propose a method to automatically convert figurative contexts into literal ones using prompting with GPT-3 and ChatGPT, which leads to substantial performance gains. The best results are achieved using ChatGPT with chain-of-thought prompting to generate simplified contexts before answering. Additionally, they show that finetuning models on synthetic training data generated by ChatGPT further improves performance.

## Method Summary
The authors first establish a baseline by fine-tuning RoBERTa on BoolQ and evaluating it on FigurativeQA. They then use GPT-3 and ChatGPT to automatically convert figurative contexts to non-figurative ones while preserving meaning. ChatGPT with chain-of-thought prompting is used to first generate a simplified context, then answer the question. Finally, ChatGPT is prompted to generate synthetic training data (question-answer pairs) from figurative contexts, which is used to further finetune the QA models.

## Key Results
- RoBERTa performance drops by up to 15% on figurative contexts compared to non-figurative ones
- GPT-3 and ChatGPT can automatically simplify figurative contexts with 89% and 81% correctness respectively
- ChatGPT with chain-of-thought prompting achieves the best performance on FigurativeQA
- Finetuning on synthetic data generated by ChatGPT further improves model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large language models (LLMs) can automatically convert figurative language to literal language, improving QA performance.
- **Mechanism**: GPT-3 and ChatGPT models are prompted to rephrase figurative contexts into non-figurative ones while preserving the original meaning. This simplification makes the text easier for downstream QA models to process.
- **Core assumption**: Figurative language comprehension is harder than literal language comprehension, and simplification preserves semantic content.
- **Evidence anchors**:
  - [abstract] "we show that further performance gains can be achieved by automatically simplifying the figurative contexts into their non-figurative (literal) counterparts."
  - [section 5] "We observe that these models are pretty good at converting figurative language in FigurativeQA to literal, with nearly 89% and 81% of the outputs from GPT-3 judged to be correct in Amazon and Yelp, respectively."
  - [corpus] Weak - corpus only shows related papers, no direct evidence about LLM simplification effectiveness.
- **Break condition**: If the simplification process introduces semantic drift or fails to capture the intended meaning, the QA performance gains would disappear.

### Mechanism 2
- **Claim**: Chain-of-thought prompting with ChatGPT enables better reasoning about figurative language.
- **Mechanism**: By first generating a simplified version of the context and then answering the question, ChatGPT can break down the reasoning process into steps, mimicking human comprehension strategies.
- **Core assumption**: Complex reasoning tasks benefit from intermediate reasoning steps rather than direct answer generation.
- **Evidence anchors**:
  - [abstract] "We find that the best overall model is ChatGPT with chain-of-thought prompting to generate non-figurative contexts."
  - [section 7] "Since understanding figurative language often requires implicit reasoning, we investigate the effect of applying chain-of-thought prompting for FigurativeQA using ChatGPT."
  - [corpus] Weak - corpus neighbors don't directly support this mechanism.
- **Break condition**: If the chain-of-thought process introduces errors or the intermediate steps don't actually improve understanding, performance gains would be lost.

### Mechanism 3
- **Claim**: Finetuning QA models on synthetic data generated by LLMs improves performance on figurative questions.
- **Mechanism**: ChatGPT is prompted to generate domain-specific question-answer pairs from figurative contexts, creating training data that helps models learn the mapping between figurative expressions and their literal meanings.
- **Core assumption**: Synthetic training data can effectively teach models to recognize and interpret figurative language patterns.
- **Evidence anchors**:
  - [abstract] "we show that finetuning models on synthetic training data generated by ChatGPT further improves performance."
  - [section 8] "We find that further finetuning RoBERTa-finetuned-on-BoolQ on synthetic QA data generated from ChatGPT yields the best performance on the figurative split of both Amazon and Yelp."
  - [corpus] Weak - corpus doesn't contain evidence about synthetic data effectiveness.
- **Break condition**: If the synthetic data doesn't accurately represent the complexity of real figurative language or contains errors, finetuning could harm rather than help performance.

## Foundational Learning

- **Concept**: Figurative language understanding
  - **Why needed here**: The entire paper addresses the challenge of QA models handling figurative expressions like similes, metaphors, and hyperbole.
  - **Quick check question**: Can you identify which text fragments in "The cake was sinfully decadent" are figurative and what they literally mean?

- **Concept**: Chain-of-thought prompting
  - **Why needed here**: This technique is shown to be the most effective method for improving QA performance on figurative contexts.
  - **Quick check question**: How would you break down the reasoning for answering "Did the cake taste good?" given the context "The cake was sinfully decadent"?

- **Concept**: Synthetic data generation
  - **Why needed here**: The paper uses LLM-generated synthetic data to augment training, addressing the lack of figurative QA training data.
  - **Quick check question**: What are the risks of using synthetic data for training, and how might they affect model performance?

## Architecture Onboarding

- **Component map**: Figurative contexts → GPT-3/ChatGPT simplification → QA Model (RoBERTa/ChatGPT) → Yes/No answers
- **Critical path**: Context → Simplification (if figurative) → QA Model → Answer
  The most critical path is the context simplification step, as errors here propagate to the final answer.
- **Design tradeoffs**: 
  - Manual vs. automatic context simplification: Manual is more accurate but expensive; automatic is scalable but less reliable.
  - Finetuning vs. prompting: Finetuning gives better performance but requires more resources; prompting is faster but may be less effective.
- **Failure signatures**: 
  - Performance drops when encountering complex figurative expressions that don't simplify well
  - Synthetic data contains factual errors or doesn't cover the full range of figurative language
  - Chain-of-thought reasoning goes off-track and produces incorrect intermediate steps
- **First 3 experiments**:
  1. Run the baseline RoBERTa model on both figurative and non-figurative splits to establish performance drop.
  2. Use provided prompts to convert figurative contexts to non-figurative using GPT-3 (da-vinci-003) or ChatGPT (gpt-3.5-turbo). Evaluate correctness of generated non-figurative contexts.
  3. Implement chain-of-thought prompting with ChatGPT on a small test set and compare accuracy to direct prompting.

## Open Questions the Paper Calls Out
No open questions are explicitly called out in the paper.

## Limitations
- The quality of automatically generated non-figurative contexts is only partially verified (89%/81% correctness rates), with no error analysis of the simplification process.
- The paper focuses primarily on accuracy metrics without deeper analysis of whether models truly understand figurative language or simply map patterns to answers.
- The synthetic data generation process could introduce biases or errors that aren't captured in the reported performance gains.

## Confidence
- **High confidence**: The baseline finding that FigurativeQA is harder than BoolQ and that RoBERTa performance drops significantly on figurative contexts (15% drop).
- **Medium confidence**: The effectiveness of automatic context simplification using GPT-3/ChatGPT. While improvements are shown, only partial manual verification was performed.
- **Medium confidence**: The finetuning on synthetic data improves performance. The improvement is demonstrated, but the quality and representativeness of the synthetic data relative to real figurative language remains uncertain.

## Next Checks
1. **Manual evaluation of simplification quality**: Select 100 figurative contexts from the dataset, run them through GPT-3/ChatGPT simplification, and have human annotators rate whether the meaning is preserved. Calculate precision, recall, and F1 of semantic preservation to quantify the reliability of the simplification pipeline.

2. **Error analysis on challenging figurative expressions**: Identify specific types of figurative language (complex metaphors, cultural idioms, mixed figurative-non-figurative contexts) where the simplification and QA pipeline fails. Test whether these errors stem from the simplification step, the QA model, or both.

3. **Generalization test across domains**: Apply the best-performing model (ChatGPT with chain-of-thought + finetuning) to a different figurative language dataset (e.g., SQuAD with figurative contexts, or manually constructed test cases) to verify that improvements transfer beyond the FigurativeQA dataset.