---
ver: rpa2
title: There Is a Digital Art History
arxiv_id: '2308.07464'
source_url: https://arxiv.org/abs/2308.07464
tags:
- digital
- history
- visual
- images
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors revisit Drucker\u2019s question on digital art history\
  \ in light of recent advances in multimodal vision models like CLIP. They argue\
  \ that these models encode a nuanced visual-cultural repertoire beyond simple object\
  \ recognition, allowing extraction of non-figurative and non-perspectival logics."
---

# There Is a Digital Art History

## Quick Facts
- arXiv ID: 2308.07464
- Source URL: https://arxiv.org/abs/2308.07464
- Reference count: 0
- Authors: Johanna Drucker, Lev Manovich
- Key outcome: Digital art history must expand to include critical analysis of model ideologies, integrating insights from media studies and computational humanities

## Executive Summary
The authors revisit Drucker's question on digital art history in light of recent advances in multimodal vision models like CLIP. They argue that these models encode a nuanced visual-cultural repertoire beyond simple object recognition, allowing extraction of non-figurative and non-perspectival logics. Two case studies illustrate this: mapping spatial associations of "Paris" across Google Street View images, and visualizing the entanglement of "naked" vs. "nude" in an art collection. Both reveal the epistemic entanglement between model and data—understanding images now requires critiquing the model's cultural biases. The authors conclude that digital art history must expand to include critical analysis of model ideologies, integrating insights from media studies and computational humanities.

## Method Summary
The paper employs CLIP (Contrastive Language-Image Pre-training) for zero-shot image retrieval and embedding extraction on two datasets: Google Street View images of Paris (10,000 images sampled at regular intervals) and 1,000 images from the Art Institute of Chicago collection. The methodology involves computing CLIP embeddings for each image and measuring similarity scores for specific prompts ("a photo of Paris", "a photo of Los Angeles", "naked", "nude"). Results are visualized as spatial heatmaps and 2D scatter plots using browser-based tools like 2D CLIP and CLIP-MAP, revealing how the model's cultural encoding shapes its interpretation of visual concepts.

## Key Results
- CLIP encodes culturally situated visual concepts beyond object recognition through multimodal training on image-text pairs
- Model-data entanglement requires critical methodology in digital art history, as analysis simultaneously reveals both dataset biases and model ideologies
- Art history is uniquely positioned to analyze multimodal foundation models due to its expertise in visual culture and historical contextualization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP encodes culturally situated visual concepts beyond object recognition
- Mechanism: Multimodal training on image-text pairs allows CLIP to learn associations between linguistic and visual representations, capturing non-object-specific cultural meanings (e.g., "Paris" → Eiffel Tower, boulevards)
- Core assumption: Large-scale image-text training data reflects diverse cultural representations of visual concepts
- Evidence anchors:
  - [abstract] "large-scale vision models have 'seen' large parts of the Western visual canon mediated by Net visual culture"
  - [section 3] "CLIP, through its natural language interface, allows for retrieval based on visual concepts of arbitrary complexity"
  - [corpus] Weak - corpus neighbors are mostly about computer vision/AI, not about multimodal cultural encoding
- Break condition: If training data lacks cultural diversity or multimodal correlations, CLIP's concept space becomes limited to literal object recognition

### Mechanism 2
- Claim: Model-data entanglement requires critical methodology in digital art history
- Mechanism: Analysis of image corpora through CLIP simultaneously reveals the model's visual ideology and the dataset's cultural biases, making them mutually interpretable
- Core assumption: Visual models encode implicit cultural assumptions that become visible when applied to real datasets
- Evidence anchors:
  - [abstract] "the visual ideologies of research datasets and training datasets become entangled"
  - [section 5] "we can measure anything we can name across a dataset of images" and "CLIP's own way of seeing"
  - [corpus] Weak - corpus neighbors don't directly address model-data entanglement methodology
- Break condition: If models are treated as neutral tools rather than objects of critique, the entanglement remains invisible and biases propagate unchecked

### Mechanism 3
- Claim: Art history is uniquely positioned to analyze multimodal foundation models
- Mechanism: Art history's expertise in visual culture and historical contextualization makes it better equipped than other disciplines to analyze the cultural-ideological content of vision models
- Core assumption: Art historical methods of visual analysis can be extended to analyze machine learning model representations
- Evidence anchors:
  - [section 6] "art history is uniquely well prepared for what is to come" and "historically and analytically situating complex objects of visual culture"
  - [section 6] "digital art history needs to open up to influences... from media studies" and "computational literary studies"
  - [corpus] Weak - corpus neighbors focus on technical applications, not disciplinary positioning
- Break condition: If art history doesn't develop critical methodologies for model analysis, other disciplines will dominate this analytical space

## Foundational Learning

- Concept: Multimodal foundation models
  - Why needed here: The paper's argument depends on understanding how CLIP differs from traditional object recognition models
  - Quick check question: What key difference between CLIP and traditional computer vision models enables analysis of "non-figurative, non-perspectival, non-representational logics"?

- Concept: Visual cultural encoding
  - Why needed here: Understanding how models encode cultural meanings beyond literal object detection is central to the paper's methodology
  - Quick check question: How does CLIP's training on image-text pairs create a "mental image" of concepts like "Paris" that reflects cultural rather than literal representations?

- Concept: Model-data entanglement
  - Why needed here: The paper argues this entanglement creates a new methodological requirement for digital art history
  - Quick check question: What does it mean that "looking at data with multimodal models means looking at multimodal models with data"?

## Architecture Onboarding

- Component map: CLIP model (image encoder + text encoder) -> Training data (LAION-400M/5B image-text pairs) -> Visualization tools (2D CLIP, CLIP-MAP) -> Analysis framework (critical methodology combining art history and media studies)

- Critical path:
  1. Load image corpus
  2. Generate CLIP embeddings for images
  3. Generate CLIP embeddings for text prompts
  4. Calculate similarity scores between images and prompts
  5. Create visualizations mapping conceptual relationships
  6. Interpret results through critical lens of model and data ideologies

- Design tradeoffs:
  - Scale vs. specificity: Large models capture broad cultural patterns but may miss local variations
  - Transparency vs. capability: More capable models are harder to interpret critically
  - Technical vs. critical focus: Balancing computational methodology with humanities critique

- Failure signatures:
  - Over-reliance on model outputs without critical analysis
  - Treating CLIP as neutral tool rather than cultural artifact
  - Ignoring dataset biases and their impact on results
  - Technical failures: out-of-memory errors, slow processing on large corpora

- First 3 experiments:
  1. Replicate the "Paris" spatial mapping to understand CLIP's urban concept space
  2. Use 2D CLIP to map a small art collection along two conceptual axes (e.g., "modern" vs "traditional")
  3. Compare CLIP results with traditional metadata-based retrieval on the same corpus to reveal model biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can digital art history develop methodologies to critically analyze the ideological biases embedded in large-scale multimodal vision models?
- Basis in paper: [explicit] The authors argue that digital art history must expand to include critical analysis of model ideologies and that any study using these models must also be a study of the models themselves.
- Why unresolved: While the paper identifies the need for critical methodology, it does not provide specific frameworks or methods for conducting such analysis.
- What evidence would resolve it: Development and demonstration of concrete analytical frameworks that successfully identify and contextualize model biases in specific art historical research contexts.

### Open Question 2
- Question: To what extent can multimodal foundation models like CLIP provide meaningful insights into historically contingent concepts like "nude" vs "naked" that require cultural and historical contextualization?
- Basis in paper: [explicit] The authors demonstrate CLIP's ability to distinguish between "naked" and "nude" but note this requires interpretation against art historical scholarship.
- Why unresolved: The paper shows CLIP can operationalize these concepts but doesn't determine whether this operationalization captures the full historical and cultural complexity.
- What evidence would resolve it: Comparative studies showing how CLIP's conceptual distinctions align or diverge from established art historical interpretations across different time periods and cultural contexts.

### Open Question 3
- Question: How do the training datasets of multimodal models shape their "mental images" of cultural concepts, and how can researchers systematically map these relationships?
- Basis in paper: [explicit] The authors discuss how CLIP's concept of "Paris" reflects tourism imagery and national identity, suggesting a need to understand how training data shapes model outputs.
- Why unresolved: The paper provides examples but doesn't offer systematic methods for mapping the relationship between training data composition and model outputs.
- What evidence would resolve it: Methodological frameworks that trace specific visual concepts in model outputs back to their origins in training data, with validation across multiple cultural concepts.

## Limitations
- The case studies primarily demonstrate retrieval of culturally conventional associations rather than truly non-representational concepts
- The technical methodology relies heavily on zero-shot capabilities without exploring how fine-tuning or prompting strategies might affect results
- The paper doesn't address potential limitations of CLIP's predominantly Western and English-language training data

## Confidence

- **High confidence**: The claim that digital art history must incorporate critical analysis of model ideologies is well-supported by the paper's methodology and aligns with broader trends in critical data studies.
- **Medium confidence**: The assertion that CLIP can capture "nuanced visual-cultural repertoire" beyond object recognition is supported by case studies but could benefit from more rigorous validation.
- **Low confidence**: The claim that art history is "uniquely well prepared" to analyze multimodal foundation models compared to other humanities disciplines lacks direct evidence.

## Next Checks

1. **Cross-cultural validation**: Test CLIP's retrieval of cultural concepts using image corpora from non-Western art traditions (e.g., Japanese ukiyo-e, Islamic geometric patterns) to assess whether the model's cultural encoding is truly universal or predominantly Western-centric.

2. **Ground truth comparison**: Compare CLIP's spatial and conceptual mappings against established art historical taxonomies and geographical-cultural knowledge bases to quantify how well the model's associations align with expert knowledge.

3. **Methodological triangulation**: Replicate the case studies using alternative multimodal models (e.g., ALIGN, OpenCLIP) and traditional computer vision approaches to determine whether CLIP's results are unique or represent a general capability of large-scale multimodal training.