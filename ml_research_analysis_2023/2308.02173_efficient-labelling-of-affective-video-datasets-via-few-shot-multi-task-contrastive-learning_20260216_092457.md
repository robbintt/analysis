---
ver: rpa2
title: Efficient Labelling of Affective Video Datasets via Few-Shot & Multi-Task Contrastive
  Learning
arxiv_id: '2308.02173'
source_url: https://arxiv.org/abs/2308.02173
tags:
- learning
- mt-clar
- video
- emotion
- facial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of efficiently annotating affective
  video datasets, which is typically a time-consuming and error-prone task. We propose
  Multi-Task Contrastive Learning for Affect Representation (MT-CLAR), a novel framework
  that combines multi-task learning with a Siamese network trained via contrastive
  learning to infer dynamic valence and arousal in videos.
---

# Efficient Labelling of Affective Video Datasets via Few-Shot & Multi-Task Contrastive Learning

## Quick Facts
- **arXiv ID:** 2308.02173
- **Source URL:** https://arxiv.org/abs/2308.02173
- **Reference count:** 40
- **Primary result:** State-of-the-art performance on AFEW-VA with as little as 6% of video frames labeled

## Executive Summary
This study addresses the challenge of efficiently annotating affective video datasets by proposing MT-CLAR, a novel framework combining multi-task learning with Siamese networks trained via contrastive learning. The framework learns robust affective representations by estimating similarity between facial expressions and differences in valence and arousal levels. The authors extend MT-CLAR for few-shot learning, enabling automated video labeling using only a small set of labeled frames. Extensive experiments demonstrate that MT-CLAR achieves state-of-the-art performance with as little as 6% of video frames labeled, significantly reducing the annotation burden for affective video datasets.

## Method Summary
The MT-CLAR framework uses a Siamese network with shared encoders to generate embeddings for image pairs, which are then passed through three separate projectors for similarity classification and valence/arousal differential prediction. The model is trained using a cumulative loss function with shake-shake regularization to prevent any single task from dominating. For few-shot learning, the framework uses a small support-set of labeled video frames as anchors, predicting valence/arousal for query frames by computing differences from these anchors. The approach is further extended by combining MT-CLAR with supervised learning (MT-CLAR + SL) to predict both categorical and dimensional emotion labels for singleton images.

## Key Results
- Achieves state-of-the-art performance on AFEW-VA dataset with only 6% of video frames labeled
- Improves similarity accuracy by 1% through multi-task learning with valence/arousal differential prediction
- Competitive performance on AffectNet and AFEW-VA datasets when combining MT-CLAR with supervised learning for singleton image emotion prediction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-task contrastive learning improves emotion similarity classification by jointly optimizing similarity, valence difference, and arousal difference tasks.
- **Mechanism:** Siamese network with shared encoders generates embeddings for image pairs, passed through three projectors (similarity, valence diff, arousal diff) with cumulative loss and shake-shake regularization.
- **Core assumption:** Task-relatedness between similarity classification and valence/arousal differential prediction leads to improved representations for all tasks.
- **Evidence anchors:** Abstract mentions combining multi-task learning with Siamese network for similarity and emotion differential tasks; section notes 1% improvement in similarity accuracy when adding valence/arousal prediction.
- **Break condition:** If task-relatedness assumption is incorrect, multi-task approach could degrade performance compared to single-task learning.

### Mechanism 2
- **Claim:** Few-shot learning with anchor frames enables efficient video emotion annotation with minimal labeled data.
- **Mechanism:** Uses small support-set of labeled video frames (anchors); predicts valence/arousal for query frames by computing differences from anchor frames using learned MT-CLAR model.
- **Core assumption:** Learned representations can generalize to estimate emotion differentials between frames, even with anchors from different subjects or videos.
- **Evidence anchors:** Abstract describes automated video labeling using support-set of labeled frames; section claims first use of FSL for dynamic facial valence/arousal labeling; results show better than SOTA with 5.96% labeled anchor frames.
- **Break condition:** Performance degrades significantly when anchor frame is from different subject, suggesting model relies on identity cues not present in test data.

### Mechanism 3
- **Claim:** Combining MT-CLAR with supervised learning enables state-of-the-art singleton image emotion prediction.
- **Mechanism:** Learned embeddings from Siamese network used as features for additional MLP predicting categorical emotion labels and continuous valence/arousal values.
- **Core assumption:** Contrastive learning embeddings are high-quality representations that transfer well to supervised emotion prediction tasks.
- **Evidence anchors:** Abstract mentions extending MT-CLAR via supervised learning for singleton image prediction; section describes combining MT-CLAR with supervised learning architecture; results show high competitiveness compared to other models.
- **Break condition:** If contrastive learning embeddings are not generalizable or transferable, MT-CLAR + SL would not outperform models trained directly with supervised learning from scratch.

## Foundational Learning

- **Concept:** Siamese Networks
  - Why needed here: Essential for comparing pairs of facial images and learning similarity metrics, forming basis for both contrastive learning and few-shot video annotation approach.
  - Quick check question: How does a Siamese network differ from a standard CNN when processing pairs of inputs?

- **Concept:** Contrastive Learning
  - Why needed here: Helps model learn to distinguish between similar and dissimilar facial expressions, crucial for building robust emotion representations that can generalize to new data.
  - Quick check question: What is the primary objective of contrastive loss in metric learning?

- **Concept:** Multi-Task Learning
  - Why needed here: By jointly optimizing multiple related tasks (similarity, valence difference, arousal difference), model can leverage shared representations and task-relatedness to improve overall performance.
  - Quick check question: How does multi-task learning typically affect performance of individual tasks compared to single-task learning?

## Architecture Onboarding

- **Component map:** Input preprocessing (288×288 → 256×256 with random affine transforms/flips) -> EmoFAN encoder (256-dim embeddings) -> Siamese network (shared weights) -> Three projectors (2048→1024→512→128 neurons for similarity, valence diff, arousal diff) -> Cumulative loss function with shake-shake regularization
- **Critical path:** For video annotation - (1) Load anchor frame(s) and query frame, (2) Generate embeddings through shared encoders, (3) Compute valence/arousal differentials, (4) Apply to anchor labels to get query predictions
- **Design tradeoffs:** Siamese network enables pair-wise comparison but requires careful data sampling; multi-task learning can improve generalization but adds complexity to hyperparameter tuning; few-shot approach reduces labeling requirements but depends heavily on anchor frame selection
- **Failure signatures:** Performance degradation with cross-identity anchors indicates reliance on identity cues; poor similarity classification suggests contrastive loss or data sampling issues; inconsistent valence/arousal predictions across similar frames indicates embedding instability
- **First 3 experiments:**
  1. **Baseline similarity classification:** Train MT-CLAR with only similarity task (cross-entropy loss) and evaluate on AffectNet to establish baseline accuracy
  2. **Multi-task vs single-task:** Compare similarity accuracy with and without valence/arousal differential prediction tasks to verify the 1% improvement claim
  3. **Anchor frame impact:** Test all anchor configurations (first frame, random frame, recurring frames) on small AFEW-VA subset to identify which provides best performance with minimal labeled data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of anchor frames impact the precision of valence and arousal predictions in few-shot learning for video emotion annotation?
- **Basis in paper:** [explicit] The paper discusses different anchor set configurations and their impact on prediction accuracy.
- **Why unresolved:** While showing choice of anchor frames significantly affects prediction accuracy, paper does not provide definitive answer on optimal configuration for all scenarios or how to determine best configuration for given dataset.
- **What evidence would resolve it:** Systematic experiments comparing different anchor configurations across various datasets and conditions, including analysis of when each configuration performs best.

### Open Question 2
- **Question:** Are current performance metrics (RMSE, PCC, CCC, SAGR) sufficient for evaluating precision of continuous affect predictions in dynamic emotion inference?
- **Basis in paper:** [explicit] The paper acknowledges current metrics are "rather coarse-grained" and do not imply generation of precise estimates, suggesting need for alternate performance metrics.
- **Why unresolved:** While identifying limitation of current metrics, paper does not propose or test alternative metrics that could better capture precision of affect predictions.
- **What evidence would resolve it:** Development and validation of new metrics specifically designed to evaluate precision of continuous affect predictions, followed by empirical comparison with existing metrics.

### Open Question 3
- **Question:** How does MT-CLAR's performance generalize across different video datasets and real-world applications beyond the AFEW-VA dataset?
- **Basis in paper:** [inferred] The paper demonstrates MT-CLAR's effectiveness on AFEW-VA dataset but does not extensively test performance on other datasets or in diverse real-world scenarios.
- **Why unresolved:** Experiments are limited to specific datasets (AFEW-VA and AffectNet), with no discussion of how model would perform in different contexts or with varying data characteristics.
- **What evidence would resolve it:** Extensive testing of MT-CLAR on multiple diverse video datasets and real-world applications, including analysis of performance across different domains and conditions.

## Limitations

- Reliance on anchor frames from same subject for few-shot learning raises concerns about generalizability across identities
- Multi-task mechanism showing 1% improvement represents relatively modest gain that may not justify added complexity in all applications
- Implementation details for Mikels' Wheel-based data sampling and dynamic weight functions remain underspecified, potentially affecting reproducibility

## Confidence

- **Multi-task contrastive learning mechanism:** Medium - Theoretical framework is sound but 1% improvement claim needs independent verification and mechanism's robustness across different affect datasets remains untested
- **Few-shot video annotation capability:** High - Experimental results show consistent improvements with anchor frames, though identity dependency is notable limitation
- **MT-CLAR + SL for singleton images:** Medium - Competitive performance demonstrated but comparison with newer supervised models trained directly on target datasets would strengthen claim

## Next Checks

1. **Cross-identity validation:** Test few-shot performance using anchor frames from different subjects to quantify identity dependency and explore mitigation strategies
2. **Task ablation study:** Systematically compare single-task vs multi-task performance across multiple affect datasets to verify robustness of 1% improvement claim
3. **Anchor frame optimization:** Conduct comprehensive evaluation of anchor selection strategies (temporal spacing, distribution across video duration, number of anchors) to identify optimal configurations for different video characteristics