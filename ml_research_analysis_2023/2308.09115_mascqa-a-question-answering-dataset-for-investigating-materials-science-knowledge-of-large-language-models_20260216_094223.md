---
ver: rpa2
title: 'MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge
  of Large Language Models'
arxiv_id: '2308.09115'
source_url: https://arxiv.org/abs/2308.09115
tags:
- questions
- materials
- llms
- performance
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MaScQA, a new question-answering dataset
  of 650 challenging materials science questions designed to evaluate the knowledge
  of large language models (LLMs). The dataset covers 14 materials science subdomains
  and includes four types of questions: multiple choice, matching, numerical with
  options, and numerical.'
---

# MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models

## Quick Facts
- arXiv ID: 2308.09115
- Source URL: https://arxiv.org/abs/2308.09115
- Reference count: 0
- Primary result: GPT-4 achieves 62% accuracy on materials science questions, with conceptual errors (~64%) dominating performance issues

## Executive Summary
This paper introduces MaScQA, a novel dataset of 650 challenging materials science questions designed to evaluate the domain knowledge of large language models. The dataset covers 14 materials science subdomains and includes four question types. The authors evaluate GPT-3.5 and GPT-4 using zero-shot and chain-of-thought prompting, finding that GPT-4 significantly outperforms GPT-3.5 but chain-of-thought prompting provides no significant benefit. Error analysis reveals that conceptual errors account for the majority of mistakes, suggesting LLMs lack sufficient domain-specific conceptual grounding in materials science.

## Method Summary
The authors created MaScQA from GATE exam questions across 14 materials science subdomains, classifying questions into four types: multiple choice, matching, numerical with options, and numerical. They evaluated GPT-3.5 and GPT-4 using OpenAI's API with two prompting strategies - direct answers and step-by-step reasoning. Model responses were compared against official answer keys to calculate accuracy, followed by manual error analysis to categorize mistakes as conceptual, computational, or grounding errors.

## Key Results
- GPT-4 achieves 62% accuracy compared to GPT-3.5's 38% on the full dataset
- Chain-of-thought prompting does not significantly improve performance, contrary to general observations
- Questions on materials' mechanical and electrical behavior show the highest error rates (~60%)
- Conceptual errors account for ~64% of mistakes versus ~36% computational errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MaScQA dataset exposes fundamental gaps in LLM conceptual understanding of materials science.
- Mechanism: By requiring undergraduate-level reasoning across 14 materials science subdomains, the dataset forces LLMs to retrieve and apply complex domain concepts rather than surface-level pattern matching.
- Core assumption: LLMs trained on general text lack sufficient domain-specific conceptual grounding in materials science.
- Evidence anchors:
  - [abstract] "conceptual errors (~64%) as the major contributor compared to computational errors (~36%) towards the reduced performance of LLMs"
  - [section] "It is observed that questions related to materials' mechanical and electrical behavior have the most percentage of incorrectly answered questions (~60%)."
  - [corpus] Weak - only 5 related papers found, suggesting this domain gap hasn't been extensively studied yet
- Break condition: If LLM performance improves significantly with domain-specific pretraining or finetuning, this mechanism would need revision.

### Mechanism 2
- Claim: Chain-of-thought prompting does not significantly improve performance on materials science questions.
- Mechanism: The dataset's questions require retrieval of correct concepts first, then application. CoT helps with computational steps but not conceptual retrieval, which is the dominant error type.
- Core assumption: Conceptual errors dominate because LLMs cannot retrieve correct domain knowledge, not because they cannot apply it.
- Evidence anchors:
  - [abstract] "Interestingly, in contrast to the general observation, no significant improvement in accuracy is observed with the chain of thought prompting."
  - [section] "The CoT prompting cannot significantly improve the LLM performance as the mistakes are mainly conceptual."
  - [corpus] Weak - no corpus papers specifically address why CoT fails for conceptual retrieval
- Break condition: If future work shows CoT variants can improve conceptual retrieval, this mechanism would need revision.

### Mechanism 3
- Claim: MaScQA reveals specific materials science subdomains where LLMs struggle most.
- Mechanism: By categorizing questions across 14 subdomains, the dataset allows granular analysis of LLM weaknesses in specific areas like thermodynamics, atomic structure, and mechanical behavior.
- Core assumption: Materials science subdomains have distinct conceptual demands that LLMs handle differently.
- Evidence anchors:
  - [section] "questions related to materials' mechanical and electrical behavior have the most percentage of incorrectly answered questions (~60%)."
  - [section] "The category of atomic structure has ~42% incorrectly answered, mostly related to questions on the analysis of X-Ray diffraction studies to identify the crystal structure of the materials."
  - [corpus] Weak - only 5 related papers found, suggesting subdomain analysis hasn't been extensively studied
- Break condition: If future datasets show different subdomain patterns, this mechanism would need revision.

## Foundational Learning

- Concept: Question classification by structure (MCQ, MATCH, MCQN, NUM)
  - Why needed here: Different question structures test different reasoning abilities and have different error patterns
  - Quick check question: How would you classify a question asking to match materials properties with their applications?

- Concept: Domain-specific error analysis
  - Why needed here: Understanding whether errors are conceptual vs computational requires domain expertise to categorize mistakes correctly
  - Quick check question: What distinguishes a conceptual error from a computational error in materials science problem solving?

- Concept: Chain-of-thought prompting limitations
  - Why needed here: Understanding when CoT helps vs hurts requires knowing the error profile of the dataset
  - Quick check question: Why might CoT improve computational accuracy but not conceptual retrieval?

## Architecture Onboarding

- Component map: Dataset creation → LLM evaluation (zero-shot + CoT) → Error analysis → Domain-specific insights
- Critical path: Create question bank → Classify by structure/domain → Evaluate with GPT-3.5/GPT-4 → Analyze error types → Draw conclusions
- Design tradeoffs: Comprehensive coverage vs dataset size; manual classification vs automated; proprietary models vs open-source
- Failure signatures: High conceptual error rates indicate domain knowledge gaps; CoT not helping indicates retrieval issues; subdomain patterns reveal specific weaknesses
- First 3 experiments:
  1. Replicate GPT-4 vs GPT-3.5 performance comparison on full dataset
  2. Test additional prompting strategies (e.g., retrieval-augmented prompting) on conceptual error questions
  3. Create a subset of questions with known correct answers to validate manual error classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-specific LLMs significantly outperform general-purpose models like GPT-4 on materials science tasks?
- Basis in paper: Explicit - Authors note that conceptual errors (~64%) are the major contributor to LLM failures and suggest that domain-specific LLMs are needed to advance materials science applications.
- Why unresolved: The paper only tests general-purpose models (GPT-3.5 and GPT-4) and does not explore domain-specific alternatives.
- What evidence would resolve it: Training and benchmarking a materials science-specific LLM on MaScQA and comparing its performance to GPT-4.

### Open Question 2
- Question: Are there prompting strategies that can reduce conceptual errors in LLM responses for materials science questions?
- Basis in paper: Explicit - Authors observe that chain-of-thought prompting does not significantly improve performance and suggest that improved prompting strategies are needed.
- Why unresolved: The paper only tests zero-shot and chain-of-thought prompting, without exploring other prompting techniques.
- What evidence would resolve it: Testing various prompting strategies (e.g., few-shot, role-playing, meta-prompting) on MaScQA and analyzing their impact on conceptual error rates.

### Open Question 3
- Question: How do grounding errors in materials science LLMs compare to conceptual and computational errors?
- Basis in paper: Explicit - Authors categorize errors into conceptual, grounding, and computational, noting that grounding errors are virtually eliminated by chain-of-thought prompting but do not provide a comprehensive analysis of their prevalence.
- Why unresolved: The error analysis focuses primarily on conceptual vs. computational errors, with limited discussion of grounding errors.
- What evidence would resolve it: Conducting a comprehensive error analysis on a larger sample of questions to quantify the frequency and impact of grounding errors compared to other error types.

## Limitations
- Error analysis relies on manual categorization without reported inter-rater reliability measures
- Dataset coverage limited to GATE exam questions, potentially missing real-world materials science applications
- Model comparison restricted to two proprietary models from the same provider

## Confidence

- **High confidence**: The finding that GPT-4 outperforms GPT-3.5 on materials science questions (62% vs 38% accuracy) is well-supported by direct comparisons across the full dataset.
- **Medium confidence**: The observation that chain-of-thought prompting does not improve performance is supported but could be model-specific rather than a general LLM limitation.
- **Medium confidence**: The claim that conceptual errors (~64%) dominate over computational errors is supported by the error analysis but depends on the reliability of the manual classification process.

## Next Checks

1. **Error Classification Validation**: Conduct inter-rater reliability testing on a subset of 50-100 questions to establish kappa scores for the conceptual vs computational error classification, ensuring the ~64% conceptual error rate is robust.

2. **Model Diversity Testing**: Evaluate additional models including open-source alternatives (Llama 3, Mistral) and domain-specific materials science models to determine if the performance patterns are generalizable beyond GPT-3.5/4.

3. **Cross-Dataset Validation**: Test the same models on complementary materials science datasets (e.g., OPENXRD, KGQA4MAT) to verify that subdomain performance patterns (mechanical/electrical behavior ~60% error) hold across different question sources and formats.