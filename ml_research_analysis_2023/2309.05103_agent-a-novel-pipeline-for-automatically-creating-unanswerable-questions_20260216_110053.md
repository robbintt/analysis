---
ver: rpa2
title: 'AGent: A Novel Pipeline for Automatically Creating Unanswerable Questions'
arxiv_id: '2309.05103'
source_url: https://arxiv.org/abs/2309.05103
tags:
- questions
- unanswerable
- squad
- agent
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGent, a pipeline for automatically creating
  unanswerable questions in extractive QA by re-matching answerable questions with
  contexts that lack the information needed to answer them. AGent applies TF-IDF retrieval
  to find relevant but insufficient contexts, uses adversarial filtering to identify
  challenging unanswerable candidates, and applies a filtering model to remove false
  positives.
---

# AGent: A Novel Pipeline for Automatically Creating Unanswerable Questions

## Quick Facts
- arXiv ID: 2309.05103
- Source URL: https://arxiv.org/abs/2309.05103
- Authors: 
- Reference count: 19
- Key outcome: AGent pipeline creates unanswerable questions with 6% error rate for SQuAD and 5% for HotpotQA, achieving comparable performance to SQuAD 2.0 across multiple QA benchmarks

## Executive Summary
This paper introduces AGent, a three-step pipeline that automatically generates unanswerable questions for extractive QA by re-matching answerable questions with contexts that lack the information needed to answer them. The pipeline employs TF-IDF retrieval to find topically relevant but insufficient contexts, adversarial filtering to identify challenging unanswerable candidates, and a filtering model to remove false positives. Human evaluation shows low error rates (6% for SQuAD, 5% for HotpotQA), and models fine-tuned on these automatically created unanswerable questions perform comparably to those trained on SQuAD 2.0 across multiple benchmarks.

## Method Summary
AGent automatically creates unanswerable questions through a three-step pipeline. First, TF-IDF retrieval matches answerable questions with topically relevant contexts that lack the specific information needed for answering. Second, adversarial filtering uses multiple fine-tuned models to identify challenging unanswerable candidates that at least two models incorrectly predict as answerable. Third, a filtering model evaluates confidence scores from multiple models to remove false positives, ensuring high precision. The pipeline was evaluated on SQuAD 1.1 and HotpotQA datasets, with human review confirming low error rates and strong model performance across benchmarks.

## Key Results
- Human evaluation shows error rates of 6% for SQuAD and 5% for HotpotQA AGent datasets
- Models fine-tuned on automatically created unanswerable questions perform comparably to those trained on SQuAD 2.0 across multiple QA benchmarks
- AGent-generated questions are challenging for models trained on human-annotated unanswerable data
- TextBugger adversarial attacks reveal robustness differences between models fine-tuned on AGent versus SQuAD 2.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TF-IDF retrieval finds relevant but information-lacking contexts
- Mechanism: By using bigram TF-IDF with the question as query, the system retrieves paragraphs that are topically related but do not contain the specific information needed to answer the question
- Core assumption: High TF-IDF similarity indicates topical relevance even when the answer is absent
- Evidence anchors:
  - [section] "We employ the term frequency–inverse document frequency (TF-IDF) method to retrieve the k most relevant paragraphs from the large corpus containing all contexts from the original dataset"
  - [section] "The outcome of this step is a set of unanswerable candidates"
- Break condition: If TF-IDF similarity breaks down for certain question types (e.g., highly specific factual questions), the retrieved contexts may be irrelevant

### Mechanism 2
- Claim: Adversarial filtering identifies challenging unanswerable questions
- Mechanism: Models trained on mixed answerable/unanswerable data are used to filter out easy examples, keeping only questions that at least two models predict as answerable (indicating difficulty)
- Core assumption: If multiple models incorrectly predict an unanswerable question as answerable, it is genuinely challenging
- Evidence anchors:
  - [section] "We employ the concept of adversarial filtering where the adversarial model(s) is applied to filter out easy examples"
  - [section] "If at least two of the six models predict that a given question is answerable, we consider it to be a challenging unanswerable question"
- Break condition: If models become too similar in their predictions, the filtering may not effectively distinguish challenging questions

### Mechanism 3
- Claim: Filtering model ensures fidelity by removing answerable questions
- Mechanism: Uses model confidence scores to calculate a value V(q) for each question, removing those below a threshold determined by answerable questions
- Core assumption: The calculated value V(q) can effectively distinguish answerable from unanswerable questions
- Evidence anchors:
  - [section] "The filtering models must be developed independently for different datasets"
  - [section] "The filtering threshold is established by identifying the minimum value V(qa) where qa represents an answerable question"
- Break condition: If the filtering model overfits to the annotated samples, it may fail on unseen data

## Foundational Learning

- Concept: TF-IDF and information retrieval
  - Why needed here: Understanding how TF-IDF retrieves topically relevant contexts is crucial for grasping the first step of the pipeline
  - Quick check question: How does TF-IDF balance term frequency with document frequency to rank context relevance?

- Concept: Adversarial filtering and model-based filtering
  - Why needed here: The pipeline uses models themselves as adversaries to filter out easy examples, requiring understanding of this technique
  - Quick check question: Why does having multiple models predict a question as answerable indicate it's challenging rather than easy?

- Concept: Confidence score aggregation and thresholding
  - Why needed here: The filtering model combines confidence scores from multiple models to make final decisions
  - Quick check question: How does the filtering model use both cumulative confidence scores and the number of models making predictions?

## Architecture Onboarding

- Component map:
  TF-IDF Retriever → Adversarial Models (BERT, RoBERTa, SpanBERT base/large) → Filtering Model → Final Unanswerable Dataset
  Human Review (Phase 1 and 2) for quality validation

- Critical path:
  TF-IDF retrieval → Adversarial filtering → Filtering model → Human validation
  (each step builds on the previous to ensure relevance, difficulty, and fidelity)

- Design tradeoffs:
  - Using multiple models increases robustness but adds computational cost
  - The filtering model prioritizes precision over recall (tosses some unanswerable questions to ensure low error rate)
  - Manual annotation is used sparingly for threshold tuning rather than full dataset creation

- Failure signatures:
  - High error rate in human review indicates filtering model threshold is too permissive
  - Low diversity in challenging candidates suggests adversarial models are too similar
  - Poor performance on certain test sets may indicate domain mismatch

- First 3 experiments:
  1. Run TF-IDF retrieval on a small answerable dataset and manually verify retrieved contexts are topically relevant but unanswerable
  2. Train the six adversarial models on a mixed dataset and test their ability to identify challenging examples
  3. Apply the filtering model to challenging candidates and check precision on a small annotated sample

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the AGent pipeline perform when applied to low-resource languages with limited training data?
- Basis in paper: [inferred] The paper acknowledges that the AGent pipeline was primarily evaluated using multiple pre-trained transformer-based models in English, which can be prohibitively expensive to create, especially for languages with limited resources. The authors suggest that potential future research could investigate the effectiveness of implementing the AGent pipeline in other languages.
- Why unresolved: The paper does not provide any experimental results or analysis of the AGent pipeline's performance on low-resource languages. It only mentions this as a potential future research direction.
- What evidence would resolve it: Conducting experiments to apply the AGent pipeline on low-resource languages and comparing its performance with high-resource languages like English would provide evidence of its effectiveness and limitations in such settings.

### Open Question 2
- Question: How robust are models fine-tuned on AGent datasets against various types of adversarial attacks beyond TextBugger?
- Basis in paper: [explicit] The authors mention that their analysis does not encompass a comprehensive examination of the models' robustness against various types of adversarial attacks in EQA when fine-tuned on AGent datasets. They state that such an analysis is crucial in determining the effectiveness of the AGent pipeline in real-world applications and its absence deserves further research.
- Why unresolved: The paper only focuses on the TextBugger adversarial attack and does not provide any results or insights into the models' robustness against other types of adversarial attacks commonly used in EQA.
- What evidence would resolve it: Conducting experiments to evaluate the robustness of models fine-tuned on AGent datasets against a wide range of adversarial attacks, such as word substitution, paraphrasing, and distractor injection, would provide a comprehensive understanding of their resilience and potential vulnerabilities.

### Open Question 3
- Question: What are the underlying factors that contribute to the observed phenomenon where models fine-tuned on SQuAD AGent are less robust against TextBugger attacks compared to models fine-tuned on SQuAD 2.0?
- Basis in paper: [explicit] The authors acknowledge that their study has not discussed the underlying factors for the observed phenomenon where a model fine-tuned on SQuAD AGent is less robust against TextBugger attack than its peer model fine-tuned on SQuAD 2.0. They mention that investigating this direction requires remarkably intricate investigation, which they deem beyond the scope of their present research.
- Why unresolved: The paper does not provide any hypotheses or explanations for the observed difference in robustness between models fine-tuned on SQuAD AGent and SQuAD 2.0 against TextBugger attacks.
- What evidence would resolve it: Conducting a detailed analysis of the characteristics of questions and contexts in SQuAD AGent and SQuAD 2.0 datasets, as well as the models' behavior during the TextBugger attack, would help identify the underlying factors contributing to the difference in robustness. This could involve analyzing the types of errors introduced by TextBugger and their impact on the models' ability to distinguish between answerable and unanswerable questions.

## Limitations

- The pipeline assumes TF-IDF retrieval effectively finds topically relevant but information-lacking contexts for all question types, which may not hold for highly specific factual questions or questions requiring cross-paragraph reasoning
- Evaluation focuses primarily on SQuAD 1.1 and HotpotQA datasets, with untested performance on other QA benchmarks with different characteristics
- The filtering model's performance heavily depends on manually determined threshold values, which could significantly impact precision and recall if threshold values are suboptimal

## Confidence

**High Confidence**: The core three-step pipeline architecture (TF-IDF retrieval → Adversarial filtering → Filtering model) is clearly described and implemented. The human evaluation error rates (6% for SQuAD, 5% for HotpotQA) are directly reported and verifiable.

**Medium Confidence**: Claims about model performance comparability to SQuAD 2.0 across multiple benchmarks are supported by results but may depend on specific implementation details not fully disclosed. The assertion that AGent-generated questions are challenging for models trained on human-annotated data is demonstrated but with limited test cases.

**Low Confidence**: The robustness claims regarding TextBugger adversarial attacks are mentioned but not thoroughly evaluated. The specific threshold values and exact implementation details for the filtering model remain unspecified, making precise replication challenging.

## Next Checks

1. **Cross-Dataset Generalization Test**: Apply AGent to a different QA dataset (e.g., Natural Questions or TriviaQA) and evaluate the error rates and model performance to assess generalizability beyond SQuAD and HotpotQA.

2. **Threshold Sensitivity Analysis**: Systematically vary the filtering model thresholds and measure their impact on both precision (error rate) and recall (fraction of true unanswerable questions retained) to understand the robustness of the filtering approach.

3. **Adversarial Robustness Evaluation**: Conduct comprehensive testing of model performance when fine-tuned on AGent-generated questions against various adversarial attacks beyond TextBugger to validate the robustness claims across different attack vectors.