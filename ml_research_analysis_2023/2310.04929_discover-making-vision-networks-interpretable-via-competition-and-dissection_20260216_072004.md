---
ver: rpa2
title: 'DISCOVER: Making Vision Networks Interpretable via Competition and Dissection'
arxiv_id: '2310.04929'
source_url: https://arxiv.org/abs/2310.04929
tags:
- neuron
- each
- neurons
- clip-dissect
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of interpretability in modern deep
  vision networks, which are highly complex and difficult to audit for reliability
  standards. The core method idea is to use competition-based activations imposed
  upon otherwise linear projection units, forming Stochastic Local Winner-Takes-All
  (LWTA) layers, which lead to extremely high activation sparsity and data-driven
  pattern of neuron functionality.
---

# DISCOVER: Making Vision Networks Interpretable via Competition and Dissection

## Quick Facts
- arXiv ID: 2310.04929
- Source URL: https://arxiv.org/abs/2310.04929
- Reference count: 40
- Primary result: CVNs achieve up to 2% improvement in neuron identification accuracy compared to conventional networks while maintaining or improving classification performance

## Executive Summary
This paper addresses the interpretability challenge in modern deep vision networks by introducing Competition-based Vision Networks (CVNs) that use Stochastic Local Winner-Takes-All (LWTA) layers. These layers impose competition between multiple units within each block, resulting in sparse activations where only a small subset of neurons fire for any given input. The method achieves classification performance on par with or better than conventional networks on ImageNet and Places365 datasets while providing a principled framework for generating human-understandable descriptions of neuronal representations.

## Method Summary
The method replaces conventional activation functions (GELU/ReLU) with stochastic LWTA layers that create competition between multiple units within each block. During training, a winner is selected stochastically from each block based on unit activations using Gumbel-Softmax reparameterization, enabling differentiable training. The resulting sparse activation patterns (4-50% of neurons active) allow for efficient post-hoc network dissection using CLIP-Dissect, which generates text descriptions for active neurons and matches them to ground truth concepts. The approach is evaluated on DeiT models for ImageNet and ResNet-18 for Places365, showing improved or comparable classification accuracy with enhanced interpretability.

## Key Results
- CVNs achieve up to 2% improvement in neuron identification accuracy compared to conventional networks
- Classification performance on ImageNet and Places365 remains on par or improves with CVNs
- Extremely high activation sparsity (as low as 4%) enables tractable per-example analysis
- Concept-to-neuron identification accuracy improves by 1.7-2.0% for CLIP-ResNet-18 on Places365

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic local competition in LWTA layers leads to sparse activations that improve interpretability.
- Mechanism: Each LWTA block contains multiple competing units, but only one wins per input, passing its response while others output zero. This creates sparse activation patterns where only a small subset of neurons fire for any given input.
- Core assumption: The stochastic nature of winner selection allows neurons to specialize in specific input characteristics, leading to more diverse and interpretable functionality.
- Evidence anchors:
  - [abstract] "only a small subset of layer neurons are activated for a given input, leading to extremely high activation sparsity (as low as only ≈ 4%)"
  - [section] "the higher the response of a particular unit in a block (relative to the others), the higher its probability of being the winner; however, the final decision remains stochastic"
- Break condition: If competition becomes deterministic or if all units in a block win equally often, the sparsity benefit disappears and interpretability degrades.

### Mechanism 2
- Claim: Sparse activations enable per-example analysis of only active neurons, simplifying interpretation.
- Mechanism: Since only ~4-50% of neurons are active per example, the dissection process can focus exclusively on these winning neurons rather than analyzing all neurons. This dramatically reduces the complexity of identifying what concepts each neuron represents.
- Core assumption: The active neurons are sufficient to capture the essential features needed for classification, making the inactive neurons less relevant for interpretation.
- Evidence anchors:
  - [abstract] "human understandable descriptions are generated only for the very few active neurons, thus facilitating the direct investigation of the network's decision process"
  - [section] "After neuron identification, i.e., matching neurons to concepts, and since only a small subset of neurons is active for each example, it becomes practically tractable to perform a per-example analysis on that particular small subset of 'winner' (active) neurons"
- Break condition: If the proportion of active neurons becomes too high (>50%), the benefit of focusing on a small subset diminishes.

### Mechanism 3
- Claim: Competition-based representations diversify neuron functionality, improving identification accuracy.
- Mechanism: The competition mechanism forces neurons within each block to develop distinct specializations to increase their chances of winning. This leads to more diverse and interpretable representations compared to conventional networks where neurons might learn redundant features.
- Core assumption: The competition pressure encourages neurons to specialize in complementary features rather than overlapping ones, leading to more interpretable and distinct neuron identities.
- Evidence anchors:
  - [abstract] "the neurons to activate/specialize to inputs with specific characteristics, diversifying their individual functionality"
  - [section] "the emerging summary vectors will be sparse; only the winner of each LWTA block will retain its linear computation and pass it as a non-zero activation for each example X n; the rest units in the block pass zero activations"
- Break condition: If neurons within blocks learn to respond similarly, the competition becomes meaningless and diversity benefits disappear.

## Foundational Learning

- Concept: Stochastic variational inference with Gumbel-Softmax reparameterization
  - Why needed here: The competition mechanism introduces discrete latent variables (winner indicators) that require specialized training approaches since they're not differentiable
  - Quick check question: How does the Gumbel-Softmax trick enable gradient-based training of models with discrete latent variables?

- Concept: Multimodal contrastive learning (CLIP architecture)
  - Why needed here: CLIP provides the text-image matching capability that enables mapping neuron activations to textual descriptions without requiring predefined concept labels
  - Quick check question: What is the key insight behind using CLIP for neuron dissection compared to traditional segmentation-based methods?

- Concept: Network dissection and neuron identification metrics
  - Why needed here: Evaluating the interpretability benefits requires measuring how well generated neuron descriptions match ground truth concepts, using metrics like cosine similarity and identification accuracy
  - Quick check question: How does the neuron identification accuracy metric differ from traditional classification accuracy in evaluating interpretability?

## Architecture Onboarding

- Component map:
  Input layer → Stochastic LWTA blocks (grouped competing units) → Output layer
- Critical path: Input → Linear computation in each competing unit → Softmax to get probabilities → Categorical sampling for winner → Winner's response passes through → Next layer processing
- Design tradeoffs:
  - More competitors per block → Higher sparsity but potentially more specialized neurons
  - Fewer competitors → Less sparsity but easier training and more robust performance
  - Tradeoff between interpretability benefits and potential accuracy loss
- Failure signatures:
  - All units in blocks win with similar frequency → Competition not working, no sparsity
  - Similar activation patterns across different neurons → Poor specialization, redundancy
  - Low neuron identification accuracy despite sparsity → Competition not leading to interpretable features
- First 3 experiments:
  1. Replace ReLU with stochastic LWTA (U=2) in a small CNN and verify increased sparsity in activations
  2. Compare neuron identification accuracy between conventional and competitive networks using CLIP-Dissect
  3. Test different numbers of competitors (U=2,4,8,16) to find optimal sparsity-accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CVNs compare to conventional networks when trained with optimal hyperparameters and augmentations?
- Basis in paper: [inferred] The paper notes that CVNs yield on par or improved performance compared to conventional networks without hyperparameter tuning, suggesting potential for further improvements with optimization.
- Why unresolved: The paper focuses on interpretability rather than performance optimization, so optimal hyperparameter settings for CVNs were not explored.
- What evidence would resolve it: Conducting a comprehensive hyperparameter search and augmentation strategy for CVNs and comparing their performance to optimally tuned conventional networks.

### Open Question 2
- Question: Can the spatial information in CVNs be leveraged to identify lower-level concepts and patterns?
- Basis in paper: [inferred] The paper mentions that CLIP-Dissect disregards spatial information, which could allow for identifying lower-level concepts. It suggests that CVNs could potentially accommodate such functionality via block-level and unit-level descriptions.
- Why unresolved: The current implementation of CVNs and CLIP-Dissect does not utilize spatial information for neuron identification.
- What evidence would resolve it: Developing a method to incorporate spatial information into the neuron identification process and evaluating its effectiveness in identifying lower-level concepts and patterns in CVNs.

### Open Question 3
- Question: How does the interpretability of CVNs compare to ante-hoc interpretable models like Concept Bottleneck Models (CBMs)?
- Basis in paper: [explicit] The paper discusses the trade-offs between ante-hoc and post-hoc interpretability methods, mentioning that ante-hoc models often suffer from performance drops and require additional data.
- Why unresolved: The paper focuses on post-hoc interpretability using CVNs and does not directly compare their interpretability to ante-hoc models like CBMs.
- What evidence would resolve it: Conducting a comparative study of the interpretability of CVNs and CBMs, evaluating factors such as the quality of generated descriptions, ease of concept interrogation, and overall transparency of the decision-making process.

## Limitations

- The evidence for competition-based diversification of neuron functionality relies heavily on weak corpus support and lacks direct measurement of feature diversity within LWTA blocks
- Optimal number of competitors per block remains unclear, with only U=4 tested for ResNet-18, leaving the sparsity-accuracy tradeoff unexplored across architectures
- The generalizability of CVNs across different network architectures and the long-term stability of learned competitive dynamics are not thoroughly examined

## Confidence

- **High Confidence**: The technical implementation of stochastic LWTA layers and their ability to produce sparse activations is well-supported by empirical results (4-50% activation sparsity). The classification performance claims are directly verifiable through reported metrics.
- **Medium Confidence**: The interpretability improvements through competition-based diversification are plausible given the mechanism, but weak corpus evidence and lack of direct measurement of feature diversity create uncertainty. The neuron identification accuracy improvements (up to 2%) are promising but need independent verification.
- **Low Confidence**: The generalizability of the approach across different network architectures and the long-term stability of the learned competitive dynamics remain unclear from current evidence.

## Next Checks

1. Measure and report the actual diversity of features learned within LWTA blocks (e.g., correlation between unit activations, distinctiveness of receptive fields) to directly validate the competition-based specialization hypothesis.
2. Systematically test different numbers of competitors (U=2,4,8,16) across multiple architectures to establish the optimal sparsity-accuracy tradeoff and identify failure modes when competition doesn't lead to specialization.
3. Conduct ablation studies removing the stochastic component (making competition deterministic) to quantify how much of the interpretability benefit comes specifically from the stochastic nature versus the sparse activation structure itself.