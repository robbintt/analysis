---
ver: rpa2
title: 'MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models'
arxiv_id: '2309.12284'
source_url: https://arxiv.org/abs/2309.12284
tags:
- question
- reasoning
- data
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new method to improve the mathematical reasoning
  abilities of open-source LLMs. The key idea is to bootstrap mathematical questions
  by rewriting them from multiple perspectives, resulting in a new dataset called
  MetaMathQA.
---

# MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models

## Quick Facts
- arXiv ID: 2309.12284
- Source URL: https://arxiv.org/abs/2309.12284
- Reference count: 40
- Primary result: MetaMath-7B achieves 66.4% on GSM8K and 19.4% on MATH, exceeding state-of-the-art models of the same size by 11.5% and 8.7%, respectively

## Executive Summary
This paper introduces MetaMath, a method to enhance mathematical reasoning in open-source LLMs through question bootstrapping. The approach generates diverse mathematical questions by applying forward reasoning, backward reasoning, and LLM rephrasing to existing datasets. By fine-tuning LLaMA-2 models on the resulting MetaMathQA dataset, the authors create MetaMath, which demonstrates state-of-the-art performance among open-source models on mathematical reasoning benchmarks GSM8K and MATH.

## Method Summary
The method involves creating a bootstrapped dataset (MetaMathQA) by applying multiple augmentation strategies to existing mathematical datasets like GSM8K and MATH. These strategies include answer augmentation (generating multiple reasoning paths for the same question), rephrasing (using LLM to rewrite questions), self-verification (generating questions from answers), and FOBAR (If-Then reasoning). The augmented dataset is then used to fine-tune LLaMA-2 models through supervised learning, with the objective of maximizing the log likelihood of reasoning paths conditioned on questions.

## Key Results
- MetaMath-7B achieves 66.4% accuracy on GSM8K, outperforming state-of-the-art models of the same size by 11.5%
- MetaMath-7B achieves 19.4% accuracy on MATH, exceeding comparable models by 8.7%
- MetaMath-70B demonstrates strong performance while using QLoRA for efficient fine-tuning
- The approach shows consistent improvements across multiple model sizes (7B, 13B, 70B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MetaMathQA improves reasoning by exposing models to multiple perspectives of the same problem.
- Mechanism: The model is trained on both forward reasoning paths (solving the problem step-by-step) and backward reasoning paths (deducing intermediate steps from the answer). This multi-view approach increases the diversity of reasoning patterns the model encounters.
- Core assumption: Mathematical problems can be decomposed into interchangeable forward and backward reasoning views without loss of semantic content.

### Mechanism 2
- Claim: Question diversity increases generalization to unseen mathematical scenarios.
- Mechanism: By rephrasing and bootstrapping questions, the training distribution becomes richer, covering more permutations of language and problem structure than the original dataset.
- Core assumption: Increased diversity in training examples correlates with improved out-of-distribution performance.

### Mechanism 3
- Claim: Intermediate reasoning steps are as valuable as final answers for training.
- Mechanism: The model learns from the reasoning path itself, not just the correct answer, enabling it to internalize procedural knowledge.
- Core assumption: Supervision on intermediate steps improves learning efficiency and robustness over answer-only supervision.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Enables generation of intermediate reasoning steps that serve as training supervision.
  - Quick check question: What is the difference between zero-shot and few-shot CoT prompting?

- Concept: Data diversity and generalization
  - Why needed here: Explains why bootstrapping improves performance beyond simple answer augmentation.
  - Quick check question: How does diversity gain differ from dataset size increase?

- Concept: Backward reasoning and problem inversion
  - Why needed here: Central to the backward reasoning augmentation strategy.
  - Quick check question: What is the logical difference between a forward and a backward reasoning path for a given problem?

## Architecture Onboarding

- Component map:
  - Prompt templates for rephrasing, self-verification, and FOBAR -> GPT-3.5-Turbo generator -> Answer filtering pipeline -> LLaMA-2 base model (full fine-tuning or QLoRA) -> MetaMath model

- Critical path:
  - Generate reasoning paths → filter by correctness → augment dataset → fine-tune base model

- Design tradeoffs:
  - Full fine-tuning vs. QLoRA: trade-off between performance and compute cost
  - Temperature sampling: trade-off between diversity and correctness
  - Number of bootstrapped samples per original question: trade-off between dataset size and redundancy

- Failure signatures:
  - Accuracy plateaus despite increased dataset size (diversity saturation)
  - Perplexity on validation set increases after fine-tuning (overfitting)
  - Backward reasoning accuracy lags far behind forward reasoning accuracy (semantic misalignment)

- First 3 experiments:
  1. Run ablation study comparing AnsAug-only vs. AnsAug + Rephrasing vs. all augmentations.
  2. Measure diversity gain vs. accuracy improvement across different dataset sizes.
  3. Compare perplexity on MetaMathQA vs. original GSM8K CoT to validate ease-of-learning hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between forward and backward reasoning augmentation for maximizing mathematical reasoning performance in LLMs?
- Basis in paper: [explicit] The paper compares the effectiveness of different augmentation strategies but the optimal balance is not specified.
- Why unresolved: The paper does not provide a systematic study of different ratios of forward vs. backward reasoning questions.
- What evidence would resolve it: An ablation study varying the proportions of each augmentation type in the training data and measuring the impact on benchmark performance.

### Open Question 2
- Question: How does the diversity gain metric correlate with mathematical reasoning performance across different types of mathematical problems?
- Basis in paper: [explicit] The paper introduces diversity gain as a metric and shows a positive correlation with accuracy, but only for GSM8K data.
- Why unresolved: The paper only demonstrates this correlation for one dataset and doesn't explore different mathematical domains.
- What evidence would resolve it: Experiments measuring diversity gain and performance across multiple mathematical reasoning datasets with varying difficulty levels and domains.

### Open Question 3
- Question: What is the mechanism by which question bootstrapping improves mathematical reasoning in LLMs?
- Basis in paper: [inferred] The paper suggests that question bootstrapping provides multi-view augmentation of meta-knowledge, but the specific mechanism is not fully explained.
- Why unresolved: While the paper shows that bootstrapping improves performance, it doesn't provide a detailed analysis of how different types of questions contribute to learning.
- What evidence would resolve it: Detailed analysis of model behavior on different question types before and after training with bootstrapped data, including attention patterns and intermediate reasoning steps.

## Limitations

- Evaluation scope is limited to GSM8K and MATH benchmarks without validation on diverse mathematical domains
- Critical implementation details are missing, including exact prompt templates and sampling parameters
- Quality control for bootstrapping process is not rigorously validated, raising concerns about semantic fidelity

## Confidence

**High Confidence Claims**:
- MetaMath models achieve state-of-the-art performance among open-source LLMs on GSM8K and MATH benchmarks
- The MetaMathQA dataset creation methodology is technically sound and implementable
- Fine-tuning LLaMA-2 on MetaMathQA produces measurable performance improvements

**Medium Confidence Claims**:
- Question diversity from bootstrapping improves generalization to unseen mathematical scenarios
- Intermediate reasoning steps provide valuable supervision beyond final answers
- Backward reasoning augmentation contributes meaningfully to performance gains

**Low Confidence Claims**:
- The exact contribution of each bootstrapping component to overall performance
- Whether the diversity metrics used actually correlate with mathematical reasoning capability
- The scalability of the approach to other mathematical domains or reasoning tasks

## Next Checks

1. **Ablation Study Replication**: Implement and compare all combinations of augmentation types (AnsAug-only, AnsAug + Rephrasing, complete pipeline) to isolate which components drive performance improvements.

2. **Out-of-Distribution Testing**: Evaluate MetaMath models on mathematical reasoning benchmarks outside GSM8K and MATH (such as AMPS, SVAMP, or competition mathematics problems) to test the generalization claims.

3. **Backward Reasoning Validation**: Conduct a detailed analysis comparing forward and backward reasoning accuracy on held-out questions to verify that the backward reasoning paths maintain semantic fidelity.