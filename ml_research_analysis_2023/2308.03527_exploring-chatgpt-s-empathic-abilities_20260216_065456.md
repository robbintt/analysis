---
ver: rpa2
title: Exploring ChatGPT's Empathic Abilities
arxiv_id: '2308.03527'
source_url: https://arxiv.org/abs/2308.03527
tags:
- chatgpt
- empathy
- emotion
- emotional
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the empathic abilities of ChatGPT by analyzing
  its understanding and expression of emotions, parallel emotional response, and empathic
  personality. The authors evaluated ChatGPT's performance on various empathy aspects
  and compared it with human behavior using five standardized questionnaires covering
  different aspects of empathy.
---

# Exploring ChatGPT's Empathic Abilities

## Quick Facts
- arXiv ID: 2308.03527
- Source URL: https://arxiv.org/abs/2308.03527
- Reference count: 39
- Key outcome: ChatGPT can correctly identify emotions and produce appropriate answers in 91.7% of cases, with parallel emotional responses at 70.7% accuracy, though scores remain below healthy human averages.

## Executive Summary
This paper investigates ChatGPT's empathic capabilities through three lenses: emotional understanding and expression, parallel emotional response generation, and empathic personality assessment. The authors employ standardized questionnaires and emotion classification techniques to evaluate ChatGPT against human baselines. Results show that while ChatGPT demonstrates considerable empathic abilities, its performance still falls short of typical human empathy, though it exceeds that of individuals diagnosed with Asperger syndrome or high-functioning autism.

## Method Summary
The study employs a multi-pronged approach to evaluate ChatGPT's empathy. First, researchers manually annotate 60 neutral sentences rephrased into six emotions (joy, anger, fear, love, sadness, surprise) with majority voting across three annotators. Second, they use 20,300+ empathetic dialogues from Facebook Research's EmpatheticDialogues dataset to prompt ChatGPT and classify responses using a BERT-based emotion classifier fine-tuned on the CARER dataset. Third, five standardized empathy questionnaires (IRI, EQ, TEQ, PES, AQ) are administered to ChatGPT, with responses manually matched to questionnaire options using majority voting across three annotators.

## Key Results
- ChatGPT correctly identifies emotions and produces appropriate responses in 91.7% of cases
- ChatGPT generates parallel emotional responses with 70.7% accuracy
- ChatGPT's empathy scores exceed those of individuals with Asperger syndrome/high-functioning autism but remain below healthy human averages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can correctly identify emotions and produce appropriate answers in 91.7% of the cases.
- Mechanism: The model leverages its fine-tuning on large conversational datasets to map input prompts to emotionally relevant responses.
- Core assumption: The emotion classification system used for evaluation (BERT-based on CARER dataset) accurately captures the emotional intent of both the input and ChatGPT's output.
- Evidence anchors:
  - [abstract] "in 91.7% of the cases, ChatGPT was able to correctly identify emotions and produces appropriate answers"
  - [section III-B] "The results of the experiment are illustrated in Figure 2. The green line indicates the average classification accuracy over our six emotion categories. We see that when it comes to expressing emotions, ChatGPT can express the desired emotion with an accuracy of 91.7%."
  - [corpus] Weak—corpus does not provide direct validation of emotion classification accuracy.
- Break condition: If the evaluation dataset's emotion labels do not align with the actual emotional intent ChatGPT conveys, accuracy estimates become unreliable.

### Mechanism 2
- Claim: ChatGPT reacts with a parallel emotion in 70.7% of cases.
- Mechanism: ChatGPT's internal representations of emotional states are sufficiently aligned with human emotion categories to produce matching responses in most cases.
- Core assumption: The emotion classifier's mapping between categories is symmetric; that is, if the classifier says ChatGPT matches the input emotion, it truly does in human terms.
- Evidence anchors:
  - [abstract] "In conversations, ChatGPT reacted with a parallel emotion in 70.7% of cases."
  - [section IV-B] "Overall, in 70.7% (20,237 responses), ChatGPT responds with the same emotion category as the initial prompt."
  - [corpus] Weak—corpus lacks direct human ratings of ChatGPT's emotional responses.
- Break condition: If ChatGPT's output consistently biases toward certain emotions (e.g., joy) regardless of input, the parallel emotion rate is inflated.

### Mechanism 3
- Claim: ChatGPT's empathy scores are worse than healthy humans but better than those diagnosed with Asperger syndrome/high-functioning autism.
- Mechanism: The model's training data contains diverse social interactions, enabling it to capture some empathy-related patterns but not to the depth of typical human development.
- Core assumption: Standardized empathy questionnaires can be meaningfully answered by a language model without lived human experience.
- Evidence anchors:
  - [abstract] "the scores of ChatGPT are still worse than the average of healthy humans, although better than people diagnosed with Asperger syndrome/high-functioning autism."
  - [section V-E] "ChatGPT achieved a total score of 19... which is only slightly higher than the mean scores of healthy males (17.8, SD = 6.8)... and significantly higher than the mean scores of healthy females (15.4, SD = 5.7)."
  - [corpus] Weak—corpus does not provide human empathy test results for direct comparison.
- Break condition: If the mapping from ChatGPT's free-text answers to questionnaire options is inaccurate, the empathy scores are unreliable.

## Foundational Learning

- Concept: Emotion classification
  - Why needed here: Accurate evaluation of ChatGPT's emotional understanding depends on reliable classification of both input prompts and model outputs.
  - Quick check question: What is the accuracy of the BERT-based emotion classifier on the test set used for evaluation?

- Concept: Parallel emotional response
  - Why needed here: The core empathy metric relies on measuring whether ChatGPT's response matches the emotional tone of the input.
  - Quick check question: How is "parallel emotional response" defined in the study, and what percentage of responses matched?

- Concept: Empathy questionnaires
  - Why needed here: Multiple validated scales are used to assess ChatGPT's empathy across cognitive, affective, and social dimensions.
  - Quick check question: Which five standardized questionnaires were administered to ChatGPT, and what aspects of empathy does each cover?

## Architecture Onboarding

- Component map: Data pipeline (input prompts → ChatGPT → emotion classifier → human annotation), evaluation metrics (accuracy, parallel response rate, questionnaire scores), comparison framework (human baselines, clinical groups)
- Critical path: 1) Collect emotionally labeled prompts. 2) Generate ChatGPT responses. 3) Classify response emotions. 4) Annotate for accuracy. 5) Administer empathy questionnaires. 6) Compare scores to human baselines
- Design tradeoffs: High automation (classifier) vs. accuracy (human annotation). Using existing datasets (CARER) vs. potential domain mismatch. Comparing to clinical groups vs. healthy norms
- Failure signatures: Over-reliance on automated classification can inflate empathy scores. ChatGPT's tendency to bias responses toward joy can distort parallel response metrics. Questionnaire administration without lived experience may produce unreliable scores
- First 3 experiments:
  1. Re-run emotion rephrasing task with a held-out test set to verify 91.7% accuracy
  2. Conduct a human evaluation study on a subset of ChatGPT's parallel emotional responses to validate the 70.7% rate
  3. Apply an alternative emotion classification model to the EmpatheticDialogues responses to check for consistency in the parallel response analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the empathic abilities of ChatGPT compare to other large language models beyond GPT-3.5, such as GPT-4 or other contemporary chatbots?
- Basis in paper: [explicit] The paper focuses on ChatGPT based on GPT-3.5 and does not compare it with other models or chatbots.
- Why unresolved: The study does not include a comparative analysis with other language models or chatbots, limiting the understanding of ChatGPT's relative empathic performance.
- What evidence would resolve it: Conducting comparative studies using the same standardized questionnaires and tasks on other models like GPT-4 or other chatbots would provide a clearer picture of ChatGPT's relative empathic abilities.

### Open Question 2
- Question: Can ChatGPT's empathic responses be improved through fine-tuning with emotionally rich datasets, and if so, to what extent?
- Basis in paper: [inferred] The paper evaluates ChatGPT's current empathic capabilities but does not explore the potential for improvement through additional training or fine-tuning.
- Why unresolved: The study does not investigate the impact of further training on ChatGPT's ability to understand and express emotions or generate empathic responses.
- What evidence would resolve it: Fine-tuning ChatGPT with datasets specifically designed to enhance emotional understanding and empathy, followed by re-evaluation using the same metrics, would demonstrate the potential for improvement.

### Open Question 3
- Question: How does the empathic interaction of ChatGPT differ when engaging with users from diverse cultural backgrounds or with varying emotional expressions?
- Basis in paper: [inferred] The paper does not address the cultural or individual differences in emotional expression and how ChatGPT adapts to these variations.
- Why unresolved: The study does not include an analysis of ChatGPT's performance across different cultural contexts or with users who express emotions differently.
- What evidence would resolve it: Testing ChatGPT's empathic responses with users from diverse cultural backgrounds and analyzing the accuracy and appropriateness of its emotional understanding and expression would provide insights into its adaptability.

## Limitations
- The study's reliance on automated emotion classification and questionnaire mapping introduces significant uncertainty about accuracy
- The comparison to clinical groups (Asperger syndrome/high-functioning autism) relies on published literature values rather than direct measurement
- The study does not address potential cultural biases in emotion expression or ChatGPT's tendency to default to certain emotions regardless of context

## Confidence
- **High confidence**: ChatGPT's ability to rephrase neutral sentences into emotionally expressive statements (based on direct manual annotation with majority voting)
- **Medium confidence**: Parallel emotional response accuracy (70.7%), as this relies on automated classification that may not perfectly capture emotional nuance
- **Medium confidence**: Questionnaire score comparisons, given the untested assumption that language models can meaningfully respond to human empathy assessments

## Next Checks
1. **Emotion Classifier Validation**: Re-run the emotion rephrasing task with a held-out test set to verify the 91.7% accuracy and report the classifier's performance on the CARER test set.

2. **Human Evaluation of Emotional Responses**: Conduct a comprehensive human evaluation study on a larger sample of ChatGPT's parallel emotional responses to validate the 70.7% rate and assess potential bias toward certain emotions.

3. **Alternative Questionnaire Scoring Method**: Implement an alternative method for mapping ChatGPT's free-text responses to questionnaire options (e.g., using semantic similarity or human-in-the-loop validation) to verify the reported empathy scores.