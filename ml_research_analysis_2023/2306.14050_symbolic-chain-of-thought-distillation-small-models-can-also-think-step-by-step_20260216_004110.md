---
ver: rpa2
title: 'Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step'
arxiv_id: '2306.14050'
source_url: https://arxiv.org/abs/2306.14050
tags:
- scotd
- student
- language
- teacher
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem that chain-of-thought prompting,
  which helps large language models reason step-by-step, is only effective for models
  with over 50 billion parameters. To make chain-of-thought reasoning accessible to
  much smaller models, the authors propose Symbolic Chain-of-Thought Distillation
  (SCoTD).
---

# Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step

## Quick Facts
- arXiv ID: 2306.14050
- Source URL: https://arxiv.org/abs/2306.14050
- Authors: 
- Reference count: 10
- Primary result: Small models (125M-1.3B parameters) can learn step-by-step reasoning through symbolic distillation from large teacher models

## Executive Summary
This paper addresses the challenge that chain-of-thought prompting, while effective for large language models, fails to work for models with fewer than 50 billion parameters. The authors propose Symbolic Chain-of-Thought Distillation (SCoTD), a method that enables smaller models to learn step-by-step reasoning by training on diverse chain-of-thought rationales generated by a large teacher model. Through systematic experiments on commonsense reasoning benchmarks, the approach demonstrates significant performance improvements for student models ranging from 125 million to 1.3 billion parameters.

## Method Summary
Symbolic Chain-of-Thought Distillation works by first sampling multiple diverse chain-of-thought rationales from a large teacher model (GPT-3) for each training instance. These rationales are then used to train a smaller student model (OPT) to predict both the rationale and the corresponding label. During inference, self-consistency voting is applied where multiple reasoning paths are sampled from the student model and a majority vote determines the final answer. The approach specifically targets the gap where traditional chain-of-thought prompting only works for models with over 50 billion parameters.

## Key Results
- SCoTD significantly improves student model performance on commonsense reasoning benchmarks, with accuracy gains of up to 18% on contrast sets
- Sampling 30 diverse rationales per instance from the teacher model is crucial for successful distillation
- After distillation, student-generated chain-of-thoughts are judged by humans as comparable to the teacher's despite being orders of magnitude smaller
- Self-consistency voting effectively improves student performance when trained on multiple unfiltered rationales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sampling multiple diverse rationales per example from the teacher model is critical for successful distillation.
- Mechanism: The teacher model's probability mass is distributed across many valid reasoning paths for a given input. By exposing the student to multiple diverse samples, it learns the full distribution of valid reasoning chains rather than overfitting to a single path.
- Core assumption: Chain-of-thought reasoning involves multiple valid paths, and the teacher model's probability distribution captures this diversity.
- Evidence anchors:
  - [abstract]: "sampling many reasoning chains per instance from the teacher is paramount"
  - [section]: "sampling a relatively large number of rationales per example from the teacher model (e.g., 30 rationales/example)"
  - [corpus]: The corpus contains 30 rationales per instance, supporting the claim about volume being important
- Break condition: If the teacher model only produces a single valid chain-of-thought per instance, or if the reasoning task has a deterministic single solution path, then sampling multiple rationales would provide diminishing returns.

### Mechanism 2
- Claim: Symbolic Chain-of-Thought Distillation works by training the student to predict both the rationale and the label from the teacher's outputs.
- Mechanism: The student model learns to generate step-by-step reasoning by being trained on the teacher's chain-of-thoughts paired with their corresponding predictions, effectively learning the reasoning process itself.
- Core assumption: The teacher's chain-of-thoughts contain the reasoning process that leads to correct predictions, and this process can be learned by the student.
- Evidence anchors:
  - [section]: "train a smaller student model to predict the sampled rationale and sampled label"
  - [section]: "We decode both a chain-of-thought and a predicted label from the student"
  - [corpus]: The corpus format includes both rationales and predictions from the teacher
- Break condition: If the teacher's chain-of-thoughts are not actually the reasoning process that leads to predictions, but rather post-hoc rationalizations, then the student would learn incorrect reasoning patterns.

### Mechanism 3
- Claim: Self-consistency voting improves student performance when trained on multiple unfiltered rationales.
- Mechanism: By sampling multiple reasoning paths from the student and taking a majority vote, the model can overcome individual reasoning errors and select the most consistent answer across diverse reasoning paths.
- Core assumption: Different reasoning paths may lead to different intermediate conclusions but the same correct final answer, and aggregating across paths can improve robustness.
- Evidence anchors:
  - [section]: "taking a majority vote over a large set of sample of predicted labels (resulting from a diverse range of CoTs) can improve performance"
  - [section]: "self-consistency can be effectively applied after distillation"
  - [corpus]: The corpus contains multiple rationales per instance, enabling self-consistency voting
- Break condition: If the student's diverse reasoning paths tend to make the same errors, or if the reasoning task requires strict sequential logic where different paths cannot lead to the same answer, self-consistency would not help.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The paper builds on symbolic knowledge distillation where a smaller student model learns from a larger teacher model's outputs rather than from ground truth labels.
  - Quick check question: What is the key difference between traditional knowledge distillation and symbolic knowledge distillation?

- Concept: Chain-of-Thought Reasoning
  - Why needed here: Understanding how chain-of-thought prompting works and why it benefits larger models is essential to grasp why the distillation approach is needed for smaller models.
  - Quick check question: Why does chain-of-thought prompting typically only work for models with more than 50B parameters?

- Concept: Contrast Sets
  - Why needed here: The paper evaluates on contrast sets to test generalization, so understanding what contrast sets are and their purpose is important for interpreting the results.
  - Quick check question: How do contrast sets differ from standard test sets in evaluating model performance?

## Architecture Onboarding

- Component map: Teacher model (GPT-3) → Corpus sampler → Student model (OPT) → Evaluation pipeline. The key components are the teacher model for generating rationales, the sampling mechanism to create the training corpus, the student model training process, and the evaluation system for measuring performance.

- Critical path: The most critical path is: sample rationales from teacher → filter and prepare corpus → train student model on corpus → evaluate student performance. Any failure in rationale quality or corpus preparation will directly impact student performance.

- Design tradeoffs: Sampling more rationales per instance improves performance but increases computational cost and storage requirements. Using diversity filters may improve quality but could reduce the volume of training data. Self-consistency voting improves robustness but requires multiple forward passes at inference time.

- Failure signatures: If the student model doesn't improve after distillation, check if the teacher is generating high-quality rationales, if enough rationales are being sampled per instance, or if the corpus filtering is too aggressive. If the student performs well on training tasks but poorly on contrast sets, the distilled reasoning may not be generalizable.

- First 3 experiments:
  1. Verify that sampling N=30 rationales per instance from the teacher produces a diverse corpus by measuring rationale diversity metrics.
  2. Train the student model on a small subset of the corpus (e.g., 1000 instances with 30 rationales each) and measure performance improvement on a validation set.
  3. Test the impact of self-consistency voting by comparing student performance with and without majority voting on a held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of chain-of-thought samples to generate from the teacher model for each training instance to maximize student model performance?
- Basis in paper: [explicit] The paper states that sampling many chain-of-thoughts per instance from the teacher is crucial for success, but does not specify an optimal number.
- Why unresolved: The paper only experiments with 30 rationales per instance and briefly mentions that increasing to 50 rationales did not improve performance, but does not systematically explore the relationship between the number of rationales and performance.
- What evidence would resolve it: A systematic study varying the number of rationales per instance and measuring the resulting student model performance would clarify the optimal number.

### Open Question 2
- Question: How does the quality of the teacher model impact the effectiveness of symbolic chain-of-thought distillation?
- Basis in paper: [inferred] The paper uses GPT-3 as the teacher model and OPT as the student model, but does not explore how the teacher model's size or quality affects distillation performance.
- Why unresolved: The paper only uses a single teacher model (GPT-3) and does not compare its performance to other potential teacher models.
- What evidence would resolve it: Experiments comparing the performance of SCoTD using different teacher models of varying sizes and qualities would elucidate the impact of teacher model quality.

### Open Question 3
- Question: Can symbolic chain-of-thought distillation be applied to generative tasks, or is it limited to classification tasks?
- Basis in paper: [explicit] The paper states that "Future work would be well suited to consider if chain-of-thought prompting can be useful for generative tasks" and only evaluates on classification tasks.
- Why unresolved: The paper only evaluates SCoTD on classification tasks and does not explore its applicability to generative tasks.
- What evidence would resolve it: Applying SCoTD to generative tasks and evaluating its effectiveness would determine its broader applicability.

## Limitations

- The approach depends heavily on the quality and diversity of chain-of-thought rationales generated by the teacher model
- Evaluation is primarily limited to commonsense reasoning tasks, leaving uncertainty about generalization to other domains
- The computational overhead of sampling 30 rationales per instance during training is substantial and not thoroughly analyzed

## Confidence

**High Confidence:** The core finding that symbolic chain-of-thought distillation improves small model performance on commonsense reasoning tasks. This is supported by consistent improvements across multiple benchmarks and student model sizes, with statistically significant results on contrast sets.

**Medium Confidence:** The claim that sampling many diverse rationales per instance is crucial for successful distillation. While the paper shows improved performance with N=30 rationales, the ablation studies could be more comprehensive to definitively establish this as the optimal sampling strategy.

**Medium Confidence:** The human evaluation showing student-generated chain-of-thoughts are judged as comparable to the teacher's. While the methodology appears sound, the evaluation only captures perceived plausibility rather than actual reasoning quality or correctness.

## Next Checks

1. **Rationale Quality Investigation:** Systematically evaluate how teacher rationale quality affects student performance by comparing students trained on high-quality versus low-quality teacher rationales, using both automatic metrics and human evaluation of reasoning correctness.

2. **Domain Generalization Test:** Apply the SCoTD approach to a different reasoning domain (e.g., mathematical word problems or symbolic logic) to assess whether the method generalizes beyond commonsense reasoning tasks.

3. **Sampling Efficiency Analysis:** Conduct a detailed cost-benefit analysis comparing different sampling strategies (varying N from 1 to 50 rationales per instance) to determine the optimal trade-off between performance improvement and computational overhead.