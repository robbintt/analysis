---
ver: rpa2
title: Data augmentation and explainability for bias discovery and mitigation in deep
  learning
arxiv_id: '2308.09464'
source_url: https://arxiv.org/abs/2308.09464
tags:
- bias
- data
- images
- attribution
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis presents a comprehensive approach to identifying and
  mitigating bias in deep learning models. It proposes a semi-automated method using
  global explainability techniques to detect biases in datasets, such as artifacts
  correlated with skin lesion types in dermoscopy images.
---

# Data augmentation and explainability for bias discovery and mitigation in deep learning

## Quick Facts
- **arXiv ID**: 2308.09464
- **Source URL**: https://arxiv.org/abs/2308.09464
- **Reference count**: 0
- **Primary result**: Proposes semi-automated bias detection using global explainability and three mitigation methods: Style Transfer Data Augmentation, Targeted Data Augmentation, and Attribution Feedback.

## Executive Summary
This thesis presents a comprehensive framework for identifying and mitigating bias in deep learning models through a combination of explainability techniques and targeted data augmentation. The approach focuses on detecting biases in datasets (such as artifacts correlated with skin lesion types in dermoscopy images) and implementing three distinct mitigation strategies. The methods are designed to make models more robust and accurate by addressing spurious correlations and teaching models to focus on correct features rather than irrelevant patterns.

## Method Summary
The thesis introduces a semi-automated bias detection system using global explainability techniques to identify potential biases in datasets, particularly focusing on dermoscopy images where artifacts like hair, frames, or ruler marks may correlate with lesion types. To mitigate identified biases, three methods are proposed: Style Transfer Data Augmentation (mixing shapes and textures from different classes), Targeted Data Augmentation (randomly inserting biases during training), and Attribution Feedback (fine-tuning models to focus on correct features via attribution loss). The methods are evaluated using performance metrics and counterfactual bias insertion measures.

## Key Results
- Targeted Data Augmentation effectively breaks spurious correlations by randomly inserting biases during training
- Attribution Feedback successfully guides models to focus on correct features and ignore insignificant input parts
- Style Transfer Data Augmentation mitigates shape and texture bias by creating conflicting shape-texture combinations

## Why This Works (Mechanism)

### Mechanism 1: Targeted Data Augmentation (TDA)
- Claim: TDA mitigates bias by randomly inserting biases into training data, breaking spurious correlations.
- Mechanism: Randomly inserts identified biases into all training images, making bias presence less correlated with specific classes and forcing the model to ignore bias features.
- Core assumption: Model can learn to ignore randomly inserted features not causally linked to class labels.
- Evidence anchors: Abstract statement about random bias insertion; section description of bias insertion approach.
- Break condition: If inserted bias is too strong or randomness insufficient, model may still rely on bias.

### Mechanism 2: Attribution Feedback
- Claim: Training with attribution feedback guides model to focus on correct features and ignore insignificant input parts.
- Mechanism: Fine-tunes model with end-to-end trainable attribution module using classification and attribution losses to generate attribution maps highlighting essential input parts.
- Core assumption: Attribution maps can be effectively guided to highlight correct features through combined loss functions.
- Evidence anchors: Abstract description of attribution feedback; section explanation of fine-tuning approach.
- Break condition: If attribution maps are too noisy or desired maps poorly defined, model may not learn correct feature focus.

### Mechanism 3: Neural Style Transfer Data Augmentation
- Claim: NST data augmentation mitigates shape and texture bias by mixing conflicting shapes and textures from different classes.
- Mechanism: Transfers style from one class to content of another, creating images with conflicting shape-texture pairs that force model to learn features less dependent on either alone.
- Core assumption: Model can learn features consistent across different shapes and textures rather than relying on shape or texture alone.
- Evidence anchors: Abstract description of style transfer approach; section explanation of shape-texture mixing.
- Break condition: If generated images unrealistic or conflicts too extreme, model may not learn effectively.

## Foundational Learning

- Concept: Bias in machine learning pipeline
  - Why needed here: Understanding how bias enters at different pipeline stages is crucial for identification and mitigation.
  - Quick check question: What are the six main stages of the machine learning pipeline, and how can bias be introduced at each stage?

- Concept: Explainable Artificial Intelligence (XAI)
  - Why needed here: XAI methods are essential for understanding model predictions and identifying potential biases.
  - Quick check question: What are the two main categories of XAI methods, and how do they differ in their approach to explaining model predictions?

- Concept: Data augmentation
  - Why needed here: Augmentation techniques can increase training data diversity to help mitigate bias and improve robustness.
  - Quick check question: What are different types of data augmentation methods, and how can they be used to mitigate bias in specific scenarios?

## Architecture Onboarding

- Component map: Bias Detection (XAI methods) -> Bias Mitigation (TDA, Attribution Feedback, NST) -> Evaluation (Performance metrics, Counterfactual bias insertion)
- Critical path: 1) Identify potential biases using XAI methods. 2) Design and implement bias mitigation techniques. 3) Evaluate effectiveness using performance and counterfactual bias insertion metrics. 4) Iterate on identification and mitigation as needed.
- Design tradeoffs: Complexity of bias identification/mitigation techniques vs. effectiveness. More complex techniques may be more effective but computationally expensive and difficult to implement.
- Failure signatures: Poor performance on biased data, high sensitivity to inserted biases, attribution maps not focusing on correct features indicate ineffective bias identification or mitigation.
- First 3 experiments:
  1. Implement TDA on biased dataset and evaluate effectiveness in reducing bias influence.
  2. Implement attribution feedback on biased model and evaluate effectiveness in guiding correct feature focus.
  3. Implement NST data augmentation on dataset with shape-texture bias and evaluate mitigation effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively detect and mitigate bias in deep learning models beyond skin lesion classification?
- Basis in paper: [explicit] Paper discusses bias impact in deep learning and presents skin lesion classification methods, but bias is prevalent across various domains.
- Why unresolved: Methods may not generalize equally well to other types of bias or data domains beyond dermoscopic images.
- What evidence would resolve it: Conducting experiments and case studies in different domains (NLP, general CV) to evaluate effectiveness of proposed methods beyond skin lesion classification.

### Open Question 2
- Question: How can we improve interpretability and explainability of deep learning models to enhance bias detection and mitigation?
- Basis in paper: [explicit] Paper emphasizes importance of explainable AI techniques in justifying predictions and controlling/improving models.
- Why unresolved: While methods for bias detection/mitigation are presented, there's room for improvement in model interpretability and explainability.
- What evidence would resolve it: Developing novel techniques or improving existing ones to enhance model interpretability, evaluating impact on bias detection and mitigation.

### Open Question 3
- Question: How can we ensure fairness and ethical use of deep learning models in real-world applications?
- Basis in paper: [inferred] Paper acknowledges importance of addressing bias in applications like healthcare, legal systems, and finance requiring clear reasoning and correct decisions.
- Why unresolved: Beyond bias detection/mitigation, ensuring fairness and ethical use requires broader considerations including algorithmic fairness, discrimination prevention, and societal impact.
- What evidence would resolve it: Conducting interdisciplinary research involving ethicists, policymakers, and domain experts to develop guidelines and frameworks for fair and ethical use.

## Limitations

- Empirical validation without theoretical guarantees for bias mitigation effectiveness
- Attribution feedback success contingent on quality of desired attribution maps which are not fully specified
- NST augmentation assumes conflicting shape-texture combinations will force robust feature learning, but generated images may not be sufficiently realistic

## Confidence

- **High confidence**: Overall framework structure and general approach of using explainability for bias discovery
- **Medium confidence**: Specific implementations of bias mitigation methods (TDA, attribution feedback, NST)
- **Low confidence**: Transferability of results across different domains and bias types

## Next Checks

1. Conduct ablation studies on each bias mitigation method to isolate contribution of individual components and verify proposed mechanisms are responsible for observed improvements.

2. Test robustness of methods across different architectures (beyond DenseNet121) and different types of bias (beyond dermoscopic artifacts) to assess generalizability.

3. Implement theoretical analysis of Targeted Data Augmentation method to establish conditions under which random bias insertion successfully breaks spurious correlations, providing mathematical guarantees for when method will or won't work.