---
ver: rpa2
title: Towards Better Query Classification with Multi-Expert Knowledge Condensation
  in JD Ads Search
arxiv_id: '2308.01098'
source_url: https://arxiv.org/abs/2308.01098
tags:
- data
- online
- knowledge
- bert
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of improving search query classification\
  \ in e-commerce systems, where FastText models are widely used due to their low\
  \ latency but suffer from poor generalization, especially on rare or tail queries.\
  \ To address this, the authors propose knowledge condensation (KC), a teacher-student\
  \ framework that leverages BERT\u2019s strong semantic generalization to enrich\
  \ FastText\u2019s training data."
---

# Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search

## Quick Facts
- **arXiv ID**: 2308.01098
- **Source URL**: https://arxiv.org/abs/2308.01098
- **Reference count**: 40
- **Primary result**: Knowledge condensation framework improves FastText query classification by 1.38% PV, 1.33% CLICK, and 1.99% PAY gains without adding latency

## Executive Summary
This paper addresses the challenge of improving search query classification in e-commerce systems where FastText models are widely used for their low latency but struggle with rare or tail queries. The authors propose a knowledge condensation framework that leverages BERT's semantic generalization to enrich FastText's training data with additional relevant labels for infrequent queries. A multi-expert strategy further enhances performance by training separate BERT models for different query frequency distributions. The approach demonstrates significant improvements in both offline metrics and online A/B tests while maintaining the low latency requirement of production systems.

## Method Summary
The knowledge condensation framework trains a BERT model on historical click-through data, then uses it to infer and retrieve additional relevant labels for infrequent queries. These augmented labels enrich the FastText training data, improving its ability to classify tail queries. The multi-expert strategy trains three separate BERT models on forward-weighted (long-tailed), uniform, and inverse-weighted data distributions, each specializing in high, middle, or low-frequency queries respectively. The enriched data is then used to train the final FastText model for online inference.

## Key Results
- +1.38% PV gain in online A/B testing
- +1.33% CLICK gain in online A/B testing  
- +1.99% PAY gain in online A/B testing
- Maintained low latency (TP99) during online deployment
- Significant improvements in P@5, R@5, and accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FastText models excel at remembering queries that appear in historical data, but generalize poorly on unseen or rare queries.
- Mechanism: FastText's shallow architecture enables efficient memorization of frequent patterns, while BERT's deeper architecture captures semantic representations that generalize to unseen queries.
- Core assumption: The historical click-through data distribution is long-tailed, with most queries appearing infrequently.
- Evidence anchors:
  - [abstract]: "FastText models are widely used due to their low latency but suffer from poor generalization, especially on rare or tail queries."
  - [section 4.2]: "the FastText model can do better for some user queries that have been exposed in the historical click-through data. On the contrary, for some unseen or low-frequency search queries, BERT can perform better benefited from its powerful semantic representation."
  - [corpus]: Weak evidence - no direct comparison of FastText vs BERT generalization in neighbors.

### Mechanism 2
- Claim: Knowledge condensation leverages BERT's semantic generalization to augment FastText's training data with additional relevant labels for infrequent queries.
- Mechanism: BERT predicts additional relevant labels for queries with sparse historical data, expanding the training set and improving FastText's ability to classify tail queries.
- Core assumption: BERT's semantic representation captures query intent beyond exact term matching.
- Evidence anchors:
  - [abstract]: "BERT is first trained on historical click-through data, then used to infer and retrieve additional relevant labels for infrequent queries."
  - [section 4.3]: "we train an offline BERT model to retrieve more potentially relevant data. Benefiting from its powerful semantic representation, more relevant labels not exposed in the historical data will be added into the training set for better FastText model training."
  - [corpus]: Weak evidence - neighbors focus on retrieval systems but not on knowledge distillation for query classification.

### Mechanism 3
- Claim: Multi-expert learning improves retrieval of relevant labels across different query frequency distributions by training separate BERT models on differently weighted data.
- Mechanism: Three BERT experts are trained on forward-weighted (long-tailed), uniform, and inverse-weighted data distributions, each specializing in high, middle, or low-frequency queries respectively.
- Core assumption: Different data distributions expose different semantic patterns that specialized models can better capture.
- Evidence anchors:
  - [abstract]: "A distribution-diverse multi-expert strategy further enhances retrieval by training separate BERT models for high, middle, and low-frequency queries."
  - [section 4.4]: "we first offline train three teacher models on the long-tailed, uniform, and inverse long-tailed data distributions respectively. And these learned models can perform better at high, middle, and low-frequency search queries respectively."
  - [corpus]: Weak evidence - neighbors discuss retrieval but not multi-expert approaches for frequency-specific performance.

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: The task involves assigning multiple relevant categories to each search query.
  - Quick check question: How would you modify the loss function if a query could belong to at most one category?

- Concept: Knowledge distillation
  - Why needed here: The framework transfers semantic knowledge from BERT (teacher) to FastText (student) without direct architectural similarity.
  - Quick check question: What would happen if we applied standard temperature-based KD instead of the proposed "condensation" approach?

- Concept: Long-tail distribution
  - Why needed here: Historical click data follows a long-tail distribution, with most queries appearing infrequently.
  - Quick check question: How would the model performance change if the query distribution were uniform instead of long-tailed?

## Architecture Onboarding

- Component map: Historical data preprocessing -> BERT teacher training -> BERT inference for label augmentation -> FastText student training -> Online inference
- Critical path: 1. Historical click data collection and preprocessing 2. BERT model training on historical data 3. BERT inference to generate additional labels 4. FastText model training on augmented data 5. Model deployment for online inference
- Design tradeoffs:
  - Speed vs accuracy: FastText chosen for online inference speed despite lower representation capacity
  - Offline computation vs online latency: Heavy BERT computation moved offline to preserve low latency
  - Label augmentation vs noise: Tradeoff between adding relevant labels and introducing irrelevant ones
- Failure signatures:
  - Online performance degradation despite offline improvements
  - High variance in performance across different query frequency ranges
  - Significant mismatch between offline metrics and online results
- First 3 experiments:
  1. Baseline: Train FastText only on historical data, measure performance on tail queries
  2. Single expert: Apply knowledge condensation with one BERT model, compare to baseline
  3. Multi-expert: Implement distribution-diverse multi-expert strategy, measure improvements across frequency ranges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-expert strategy scale when the number of query frequency categories increases beyond three (high, middle, low)?
- Basis in paper: [explicit] The authors describe a three-expert setup (forward, uniform, backward) but do not explore scalability to more frequency bands or alternative binning strategies.
- Why unresolved: The paper focuses on a fixed three-expert architecture without analyzing performance degradation or gains from additional experts.
- What evidence would resolve it: Empirical comparisons of performance and latency across setups with 2, 3, 4, and 5 experts, along with analysis of query frequency distribution impacts.

### Open Question 2
- Question: What is the long-term stability of the knowledge condensation framework as the underlying query distribution shifts over time?
- Basis in paper: [inferred] The approach is validated on datasets from specific time windows (T+0, T+1), but no temporal drift analysis is provided.
- Why unresolved: Real-world query distributions evolve, and the framework's robustness to such shifts is not quantified.
- What evidence would resolve it: Longitudinal studies tracking model performance across multiple months or seasons, including retraining frequency recommendations.

### Open Question 3
- Question: How does the choice of hyper-parameter M (size of supplementary set) affect the trade-off between relevance and coverage?
- Basis in paper: [explicit] The paper mentions M as a hyper-parameter in the PV allocation formula but does not explore its sensitivity or optimal tuning strategies.
- Why unresolved: Different values of M could significantly impact the balance between retrieving more relevant labels versus avoiding noise.
- What evidence would resolve it: Systematic ablation studies varying M and measuring its impact on precision, recall, and online metrics like PV and CLICK gains.

### Open Question 4
- Question: Could the knowledge condensation framework be extended to other types of shallow models beyond FastText?
- Basis in paper: [explicit] The paper focuses specifically on FastText but mentions other shallow models (e.g., TextCNN) in the context of industry usage.
- Why unresolved: The framework's applicability to different model architectures and their specific limitations is not explored.
- What evidence would resolve it: Comparative experiments applying the framework to TextCNN, logistic regression, or other shallow models, measuring performance gains and latency impacts.

## Limitations

- Limited transparency in multi-expert strategy implementation details, particularly regarding inverse BCE loss computation and data distribution normalization
- Weak corpus evidence supporting the multi-expert approach, with neighboring work focusing on retrieval rather than knowledge distillation for query classification
- Online A/B test results lack statistical significance measures and detailed breakdown across different query frequency ranges

## Confidence

- **High confidence**: The core knowledge condensation mechanism (BERT augmenting FastText training data) is well-supported by the paper's empirical results and aligns with established knowledge distillation principles.
- **Medium confidence**: The multi-expert strategy's effectiveness is demonstrated but lacks detailed implementation specifications that would enable precise replication.
- **Low confidence**: The claim that the multi-expert approach outperforms single-expert knowledge condensation requires more granular analysis, as the paper doesn't provide frequency-range-specific performance breakdowns.

## Next Checks

1. **Statistical validation**: Conduct t-tests or confidence interval analysis on the online A/B test results to confirm the reported +1.38% PV, +1.33% CLICK, and +1.99% PAY gains are statistically significant.

2. **Frequency-range ablation**: Implement the single-expert baseline and measure its performance across high, middle, and low-frequency query ranges to validate whether the multi-expert approach genuinely improves performance for each category.

3. **Noise sensitivity analysis**: Systematically vary the label augmentation ratio from BERT predictions and measure FastText performance degradation to quantify the tradeoff between additional relevant labels and potential noise introduction.