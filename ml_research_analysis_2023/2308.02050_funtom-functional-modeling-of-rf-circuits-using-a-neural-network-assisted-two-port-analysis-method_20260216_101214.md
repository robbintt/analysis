---
ver: rpa2
title: 'FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted
  Two-Port Analysis Method'
arxiv_id: '2308.02050'
source_url: https://arxiv.org/abs/2308.02050
tags:
- circuit
- modeling
- training
- funtom
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FuNToM is a functional modeling method for RF circuits that uses
  a neural network assisted two-port analysis to reduce the training data and time
  required for post-layout modeling. The method leverages the two-port analysis method
  to model multiple circuit topologies using a single main dataset and multiple small
  datasets.
---

# FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method

## Quick Facts
- arXiv ID: 2308.02050
- Source URL: https://arxiv.org/abs/2308.02050
- Reference count: 23
- FuNToM reduces required training data by 2.8x - 10.9x and time for collecting training set in post-layout modeling by 176.8x - 188.6x while maintaining accuracy

## Executive Summary
FuNToM presents a novel approach for functional modeling of RF circuits that significantly reduces the training data and time required for post-layout modeling. The method combines two-port network analysis with neural networks to create modular models where circuits are divided into Electrical-networks (E-networks). Each E-network is modeled separately and their S-parameters serve as inputs to a main neural network that predicts performance of interests. The approach achieves 2.8x-10.9x reduction in training data requirements and 176.8x-188.6x speedup in post-layout training data collection while maintaining the same accuracy as conventional methods.

## Method Summary
FuNToM divides RF circuits into multiple E-networks and uses neural networks to model their behavior. Each E-network is characterized by S-parameters, which are computed from design parameters using SPICE simulations. These S-parameters, along with parameters not belonging to any E-network, are fed into a main neural network (using either Circuit-Connectivity-Inspired-NN or Fully-Connected architecture) to predict performance of interests. The method leverages transfer learning by using schematic-level data to bootstrap post-layout modeling, requiring significantly less new data collection. TensorFlow is used for training with Adam optimizer, learning rate of 0.001, mean absolute error loss, and RELU activation.

## Key Results
- 2.8x-10.9x reduction in required training data compared to state-of-the-art methods
- 176.8x-188.6x reduction in time for collecting training set in post-layout modeling
- Maintained R2-score of ~0.95 average accuracy on tested phase shifters and two-stage LNAs at both schematic and post-layout levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modularizing RF circuits into Electrical-networks (E-networks) reduces the dimensionality of the neural network training space.
- Mechanism: By breaking a circuit into E-networks, each sub-network deals with fewer design parameters, leading to smaller, simpler neural networks that require less training data.
- Core assumption: The functional behavior of each E-network can be adequately captured by its S-parameter matrix.
- Evidence anchors:
  - [abstract] "FuNToM leverages the two-port analysis method for modeling multiple topologies using a single main dataset and multiple small datasets."
  - [section III-A] "In our approach, we have gone further and we break down the circuit into multiple E-networks... each determines the S-parameters of the associated E-network using the circuit design parameters within such E-network."
  - [corpus] Weak evidence: No direct corpus reference to E-network modularity reducing training dimensionality.
- Break condition: If the E-network boundaries are chosen poorly, leading to complex internal interactions that S-parameters cannot capture, the model's accuracy will degrade.

### Mechanism 2
- Claim: Using S-parameters as inputs to the main neural network enables the model to generalize across multiple circuit topologies.
- Mechanism: S-parameters act as a "wrapper" around E-networks, encoding their behavior in a standardized format. This allows the main neural network to learn relationships between general S-parameter patterns and performance of interests (PoIs), independent of the specific topology.
- Core assumption: S-parameters fully characterize the input-output behavior of each E-network for the frequency range of interest.
- Evidence anchors:
  - [abstract] "FuNToM leverages the two-port analysis method for modeling multiple topologies using a single main dataset and multiple small datasets."
  - [section II-B] "Using two-port analysis, an S-parameter matrix is derived which is used for characterizing circuits."
  - [corpus] Weak evidence: No direct corpus reference to S-parameter generalization across topologies.
- Break condition: If the frequency range of interest is too broad or if non-linear effects dominate, S-parameters may not fully capture the E-network behavior.

### Mechanism 3
- Claim: The Circuit-Connectivity-Inspired-NN (CCI-NN) structure reduces the required training data compared to fully connected networks.
- Mechanism: CCI-NN breaks a large neural network into smaller, sequential chunks that mirror the circuit's connectivity. This structure enforces physical constraints and reduces the number of free parameters, leading to better generalization with less data.
- Core assumption: The sequential flow of signals through the circuit can be mirrored in the neural network architecture.
- Evidence anchors:
  - [abstract] "It also leverages neural networks which have shown promising results in predicting the behavior of circuits."
  - [section III-A] "As stated by Hassanpourghadi et al. [2], CCI-NN structure requires less training data in comparison with the Fully-Connected NNs (FC-NNs)."
  - [corpus] Weak evidence: No direct corpus reference to CCI-NN's data efficiency.
- Break condition: If the circuit topology does not have a clear sequential signal flow, the CCI-NN structure may not provide significant advantages.

## Foundational Learning

- Concept: Two-port network analysis and S-parameters
  - Why needed here: S-parameters are the key to modularizing the circuit and enabling generalization across topologies.
  - Quick check question: What are the four S-parameters (S11, S12, S21, S22) and how are they defined in terms of incident and reflected waves?

- Concept: Neural network architectures (FC-NN vs CCI-NN)
  - Why needed here: Understanding the difference between fully connected and circuit-inspired neural networks is crucial for appreciating FuNToM's design choices.
  - Quick check question: How does the CCI-NN structure differ from a standard FC-NN, and why might this lead to better generalization with less data?

- Concept: Transfer learning in the context of post-layout modeling
  - Why needed here: FuNToM leverages transfer learning to reduce the training data needed when moving from schematic to post-layout models.
  - Quick check question: How can knowledge gained from schematic-level modeling be transferred to improve post-layout modeling efficiency?

## Architecture Onboarding

- Component map: E-networks -> Sub-neural networks -> Main neural network -> Performance of Interests
- Critical path:
  1. Partition circuit into E-networks
  2. For each E-network, generate training data (design parameters -> S-parameters) using SPICE simulations
  3. Train sub-neural networks on E-network data
  4. Generate training data for main neural network (S-parameters of all E-networks + PoIs) using SPICE simulations
  5. Train main neural network (CCI-NN or FC-NN) on combined data
  6. For inference, use sub-networks to get S-parameters, then main network to get PoIs

- Design tradeoffs:
  - Number and size of E-networks: More, smaller E-networks lead to simpler sub-networks but more of them. Fewer, larger E-networks lead to more complex sub-networks but fewer of them.
  - CCI-NN vs FC-NN for main network: CCI-NN may require less training data but is more complex to implement.

- Failure signatures:
  - Poor generalization across topologies: Likely due to inadequate S-parameter representation or overly simplistic circuit partitioning.
  - Overfitting on training data: Likely due to too few training samples or overly complex neural network architecture.
  - Slow inference: Likely due to inefficient implementation or overly complex neural network architecture.

- First 3 experiments:
  1. Implement a simple two-port E-network (e.g., a series RLC circuit) and verify that the sub-neural network can accurately predict its S-parameters from design parameters.
  2. Combine two simple E-networks and verify that the main neural network (CCI-NN) can accurately predict a PoI (e.g., insertion loss) from the combined S-parameters.
  3. Test the full FuNToM pipeline on a simple RF circuit (e.g., a single-stage amplifier) and compare its performance (accuracy and training time) to a conventional neural network approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to automatically partition circuits into E-networks for maximum efficiency in FuNToM?
- Basis in paper: [inferred] The paper mentions that circuit partitioning is currently done manually and suggests the need for an automated approach.
- Why unresolved: The paper does not provide a detailed method for automatic circuit partitioning, only stating that different scenarios need to be tested.
- What evidence would resolve it: A detailed algorithm or methodology for automatic circuit partitioning that demonstrates improved efficiency over manual partitioning.

### Open Question 2
- Question: How does the performance of FuNToM scale with increasingly complex and larger RF circuits?
- Basis in paper: [inferred] The paper suggests that the method is particularly effective for large RF circuits with many design parameters, but does not provide empirical data on scaling.
- Why unresolved: The paper does not include performance data for larger and more complex circuits beyond the tested examples.
- What evidence would resolve it: Empirical results showing the performance and efficiency of FuNToM on larger and more complex RF circuits.

### Open Question 3
- Question: Can FuNToM be extended to handle digital circuits or mixed-signal circuits effectively?
- Basis in paper: [explicit] The paper focuses on RF circuits and does not mention the applicability to digital or mixed-signal circuits.
- Why unresolved: The paper does not explore or discuss the potential extension of FuNToM to other types of circuits.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of FuNToM on digital or mixed-signal circuits.

## Limitations

- Manual circuit partitioning process limits scalability to complex circuits and requires expert knowledge
- Effectiveness highly dependent on optimal E-network partitioning choices not fully disclosed in the paper
- Limited empirical validation across diverse circuit types and complexity levels

## Confidence

- High confidence: The core two-port analysis methodology and neural network training framework are well-established and correctly implemented
- Medium confidence: The data reduction claims are supported by results but depend on specific circuit partitioning choices that aren't fully disclosed
- Medium confidence: The generalization across topologies using S-parameters is theoretically sound but lacks extensive empirical validation across diverse circuit types

## Next Checks

1. Test FuNToM on a more complex RF circuit (e.g., multi-stage LNA with feedback) to evaluate scalability and determine if manual partitioning remains practical
2. Implement automated circuit partitioning based on S-parameter sensitivity analysis and compare results to manual partitioning on the same circuits
3. Validate the CCI-NN architecture's data efficiency advantage by training equivalent FC-NNs with varying amounts of training data and comparing convergence rates and final accuracy