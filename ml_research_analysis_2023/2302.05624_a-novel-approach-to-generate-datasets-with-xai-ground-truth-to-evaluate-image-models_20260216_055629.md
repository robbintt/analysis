---
ver: rpa2
title: A novel approach to generate datasets with XAI ground truth to evaluate image
  models
arxiv_id: '2302.05624'
source_url: https://arxiv.org/abs/2302.05624
tags:
- function
- these
- used
- proposed
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel methodology to generate synthetic
  datasets with ground truth (GT) for evaluating explainable artificial intelligence
  (XAI) methods on image models. The method defines a function that maps visual patterns
  in images to numerical importance values, providing pixel-level GT explanations.
---

# A novel approach to generate datasets with XAI ground truth to evaluate image models

## Quick Facts
- arXiv ID: 2302.05624
- Source URL: https://arxiv.org/abs/2302.05624
- Authors: 
- Reference count: 17
- Introduces methodology to generate synthetic datasets with ground truth for XAI evaluation on image models

## Executive Summary
This paper presents a novel methodology for generating synthetic datasets with ground truth explanations to evaluate explainable AI (XAI) methods for image models. The approach defines attribution functions that map visual patterns in images to numerical importance values, creating pixel-level ground truth explanations. The method is demonstrated through two synthetic datasets (AIXI-Shape and AIXI-Color) and three different attribution functions for both regression and classification tasks. Experimental results using LIME as the XAI method show excellent fidelity metrics (MAE and MSE close to zero) when compared to the generated ground truth, confirming the methodology's effectiveness for objective evaluation of XAI methods.

## Method Summary
The methodology generates synthetic datasets by creating images with controlled visual patterns and defining attribution functions that map these patterns to importance values. For each pixel, a pattern extraction function g(pi,I) extracts information about specific patterns present in the image. The attribution function F then computes the output as a weighted sum of these extracted pattern values. The method supports both regression tasks (direct output of F) and classification tasks (using a step function on F's output). Ground truth explanations are generated by applying the pattern extraction function to the images and computing the corresponding importance values through F.

## Key Results
- Two synthetic datasets (AIXI-Shape and AIXI-Color) created with 50,000 training images and 2,000 validation images each
- Three attribution functions (ssin, suum, class) successfully implemented for both regression and classification tasks
- LIME explanations achieve MAE and MSE close to zero when compared to ground truth
- Methodology successfully validates the correctness of XAI explanations through direct comparison with ground truth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed methodology generates pixel-level ground truth by defining an attribution function F that maps visual patterns to importance values.
- Mechanism: The system constructs synthetic images with known patterns (e.g., circles, squares, colored shapes) and assigns each pattern a weight. The attribution function F computes the output as a weighted sum of these patterns, creating a direct mapping from input pixels to importance scores.
- Core assumption: The attribution function F is known and can be expressed as a deterministic combination of pattern-based features.
- Evidence anchors:
  - [abstract] "This method defines a function that maps visual patterns in images to numerical importance values, providing pixel-level GT explanations."
  - [section] "To define our methodology, we use the formalization of SAB proposed by Mamalakis et al. We set three different elements: the input data X, attribution function F, and output data Y, the latter is the result of applying F to X."
- Break condition: If the attribution function F cannot be expressed as a combination of identifiable patterns in the image, or if pattern recognition fails to accurately extract the pattern information.

### Mechanism 2
- Claim: The synthetic datasets allow direct measurement of XAI method fidelity without ad-hoc solutions by providing exact ground truth explanations.
- Mechanism: By generating datasets where the true importance of each pixel is known through the attribution function, any XAI method can be evaluated by comparing its output directly to the ground truth using standard metrics (MAE, MSE).
- Core assumption: The ground truth generated by the attribution function is accurate and represents the true causal relationship in the model.
- Evidence anchors:
  - [abstract] "Experiments using LIME as the XAI method show excellent fidelity metrics (MAE and MSE close to zero) compared to the GT, confirming the correctness of the proposed methodology."
  - [section] "We conducted a set of experiments that compared our GT with real model explanations and obtained excellent results confirming that our proposed method is correct."
- Break condition: If the XAI method introduces significant errors or biases that cannot be captured by comparing to ground truth, or if the ground truth itself is inaccurate.

### Mechanism 3
- Claim: The methodology works for both regression and classification tasks by appropriately defining the attribution function and output transformation.
- Mechanism: For regression tasks, the attribution function directly outputs continuous values. For classification tasks, a threshold function (step function) is applied to the output of the attribution function to create binary classes while maintaining the ground truth for pixel importance.
- Core assumption: The same pattern extraction function g(pi,I) can be used across different types of attribution functions and tasks.
- Evidence anchors:
  - [abstract] "Two datasets (AIXI-Shape and AIXI-Color) and three functions (ssin, suum, class) are proposed for regression and classification tasks."
  - [section] "To adapt this function to a binary classification task, it must be altered. The output must have only two possible values. This behaviour was accomplished using the step function, Fclassification(I) = F(I) â‰¤ 0."
- Break condition: If the step function introduces discontinuities that cannot be handled by the XAI method, or if the pattern extraction function g(pi,I) is not robust across different function types.

## Foundational Learning

- Concept: Synthetic attribution benchmark (SAB) formalization
  - Why needed here: Understanding the SAB framework is essential to grasp how the methodology creates ground truth for XAI evaluation
  - Quick check question: What are the three components that define a synthetic attribution benchmark according to Mamalakis et al.?

- Concept: Pattern-based feature extraction
  - Why needed here: The methodology relies on extracting numerical information from visual patterns in images to create the ground truth
  - Quick check question: In the context of this paper, what does the function g(pi,I) represent and how is it used?

- Concept: Fidelity metrics for XAI evaluation
  - Why needed here: Understanding how to measure the quality of explanations is crucial for validating the methodology
  - Quick check question: What are the two metrics used in this paper to compare XAI explanations with ground truth, and what is the key difference between them?

## Architecture Onboarding

- Component map:
  - Pattern Generator -> Attribution Function F -> Pattern Extractor g(pi,I) -> XAI Method (e.g., LIME) -> Evaluation Metrics (MAE, MSE)

- Critical path:
  1. Generate synthetic dataset with known patterns
  2. Define attribution function F based on patterns
  3. Create ground truth explanations using F and pattern extractor
  4. Train predictive model to approximate F
  5. Apply XAI method to predictive model
  6. Compare XAI explanations with ground truth using MAE/MSE

- Design tradeoffs:
  - Simple vs. complex patterns: Simpler patterns are easier to recognize but may not represent real-world complexity
  - Regression vs. classification functions: Different functions test different aspects of XAI methods
  - Image size and resolution: Larger images provide more detail but increase computational cost

- Failure signatures:
  - High MAE/MSE values indicate XAI method inaccuracies or ground truth errors
  - Inconsistent results across different functions suggest limitations in the methodology
  - Pattern recognition failures lead to incorrect ground truth generation

- First 3 experiments:
  1. Generate AIXI-Shape dataset and apply ssin function to create ground truth, then verify with LIME
  2. Generate AIXI-Color dataset and apply suum function to create ground truth, then verify with LIME
  3. Use class function with both datasets to test classification task ground truth generation and verification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed methodology be extended to evaluate XAI methods for tasks beyond image classification and regression, such as object detection or segmentation?
- Basis in paper: [explicit] The paper focuses on image classification and regression tasks, but does not explore other common computer vision tasks.
- Why unresolved: The paper does not provide any insights or experiments related to extending the methodology to other tasks.
- What evidence would resolve it: Experiments demonstrating the application of the proposed methodology to evaluate XAI methods for object detection or segmentation tasks.

### Open Question 2
- Question: How does the performance of the proposed methodology compare to existing methods for evaluating XAI methods, such as perturbation-based approaches or proxy models?
- Basis in paper: [explicit] The paper mentions that existing methods have limitations, such as being difficult to calculate or not considering the out-of-domain problem, but does not directly compare the proposed methodology to these approaches.
- Why unresolved: The paper does not provide a comprehensive comparison between the proposed methodology and existing methods.
- What evidence would resolve it: A detailed comparison study between the proposed methodology and existing methods, including quantitative metrics and qualitative assessments.

### Open Question 3
- Question: How can the proposed methodology be adapted to handle more complex and diverse image datasets, such as those with multiple objects, occlusions, or varying backgrounds?
- Basis in paper: [inferred] The paper uses simple synthetic datasets with limited complexity, which may not fully represent real-world scenarios.
- Why unresolved: The paper does not address the challenges of applying the methodology to more complex and diverse datasets.
- What evidence would resolve it: Experiments demonstrating the application of the proposed methodology to evaluate XAI methods on more complex and diverse image datasets, along with insights into the challenges and potential solutions.

## Limitations
- Methodology relies on synthetic datasets with controlled patterns that may not fully represent real-world image complexity
- Attribution functions must be expressible as combinations of identifiable visual patterns, limiting applicability to certain types of models
- Ground truth generation assumes the attribution function accurately represents true importance, which may not hold in practical scenarios

## Confidence

**High Confidence**: The methodology's ability to generate synthetic datasets with ground truth for simple geometric patterns (circles, squares, colors) is well-supported by the experimental results showing MAE and MSE close to zero.

**Medium Confidence**: The approach's scalability to more complex patterns and real-world images remains uncertain, as the paper focuses primarily on simple geometric shapes.

**Medium Confidence**: The claim that this methodology enables objective evaluation of XAI methods is supported, but the extent to which it captures all relevant aspects of XAI performance needs further investigation.

## Next Checks
1. **Scalability Test**: Apply the methodology to generate ground truth for images containing more complex, overlapping patterns (e.g., textures, natural scenes) and evaluate XAI method performance.
2. **Robustness Evaluation**: Test the attribution functions with different levels of image noise and distortion to assess the methodology's resilience to imperfect inputs.
3. **Cross-Dataset Comparison**: Use the generated ground truth to evaluate multiple XAI methods (beyond LIME) across both AIXI-Shape and AIXI-Color datasets to ensure consistent performance assessment.