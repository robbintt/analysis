---
ver: rpa2
title: Learn or Recall? Revisiting Incremental Learning with Pre-trained Language
  Models
arxiv_id: '2312.07887'
source_url: https://arxiv.org/abs/2312.07887
tags:
- class
- probing
- linear
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits incremental learning with pre-trained language
  models and challenges the assumption that catastrophic forgetting is the primary
  obstacle to achieving superior incremental learning performance. Through extensive
  experiments on various tasks and settings, the authors find that this assumption
  is problematic and that most existing methods severely underestimate the inherent
  anti-forgetting ability of pre-trained language models.
---

# Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models

## Quick Facts
- arXiv ID: 2312.07887
- Source URL: https://arxiv.org/abs/2312.07887
- Reference count: 40
- This paper challenges the assumption that catastrophic forgetting is the primary obstacle in incremental learning with pre-trained language models, showing most existing methods underestimate PLMs' inherent anti-forgetting ability.

## Executive Summary
This paper revisits the problem of incremental learning with pre-trained language models (PLMs) and challenges the conventional wisdom that catastrophic forgetting is the main obstacle to performance. Through extensive experiments on multiple tasks and settings, the authors demonstrate that PLMs possess significant anti-forgetting abilities that most existing incremental learning methods fail to recognize. The study reveals that performance degradation during sequential fine-tuning is primarily caused by classifier dynamics rather than forgetting in the PLM itself. Based on these findings, the authors propose a simple method called SEQ* that achieves competitive or superior performance compared to state-of-the-art incremental learning methods while requiring fewer trainable parameters and less training time.

## Method Summary
The authors propose SEQ* (Sequential learning with frozen PLM and proper classifiers), a frustratingly easy method for incremental learning with PLMs. The approach involves warming up the PLM on the first task, then freezing it completely. For each subsequent task, old classifiers are frozen while only training new classifiers. The method uses either linear classifiers (when storing samples) or cosine linear classifiers (when not storing samples). An optional pre-allocation strategy can be used to enhance forward compatibility. The key insight is that by freezing the PLM and carefully managing classifier updates, the method leverages the PLM's inherent anti-forgetting ability while avoiding classifier drift that causes performance degradation on previous tasks.

## Key Results
- SEQ* achieves competitive or superior performance compared to state-of-the-art incremental learning methods
- The method requires considerably fewer trainable parameters and less training time than existing approaches
- Probing studies show that PLMs maintain high knowledge preservation even under sequential fine-tuning, contrary to previous assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained language models (PLMs) inherently preserve knowledge during incremental learning due to their architecture and pre-training.
- Mechanism: The Transformer architecture and pre-training create representations that maintain high linear probing performance even under sequential fine-tuning (SEQ).
- Core assumption: The features extracted by PLMs fall in a narrow cone space and have high cosine similarity, making them robust to incremental task learning.
- Evidence anchors:
  - [abstract] "most of them severely underestimate the inherent anti-forgetting ability of PLMs"
  - [section] "The architecture of the Transformer is also a key factor in the high linear probing accuracy during SEQ"
  - [corpus] Weak - corpus neighbors don't directly address this mechanism
- Break condition: If the pre-training task distribution is too different from downstream tasks, the probing performance may degrade.

### Mechanism 2
- Claim: Catastrophic forgetting in SEQ is primarily caused by classifier dynamics, not PLM forgetting.
- Mechanism: Class embeddings drift away from their optimal position relative to class feature centers, causing performance degradation despite PLMs maintaining knowledge.
- Core assumption: The moving distance of class embeddings can be used as a metric to quantify classifier forgetting.
- Evidence anchors:
  - [abstract] "the changes of relative position between the class embeddings in classifiers and the features extracted by PLMs lead to poor performance on old tasks"
  - [section] "the forgetting happens because the old class embeddings are pushed away from their initial and optimal position"
  - [corpus] Weak - corpus neighbors don't directly address this mechanism
- Break condition: If class embeddings are frozen after warm-up, the moving distance should remain low.

### Mechanism 3
- Claim: Using proper classifier strategies (freezing PLMs, freezing old classifiers, using linear classifiers) can close the gap between probing and observed performance.
- Mechanism: By preserving the relative position of class embeddings and using optimal classifiers, SEQ* achieves competitive performance with SOTA methods.
- Core assumption: The probing performance is the upper bound when classifiers are optimal.
- Evidence anchors:
  - [abstract] "we propose a frustratingly easy method called SEQ* for IL with PLMs"
  - [section] "The results show that SEQ* has competitive or superior performance compared to state-of-the-art (SOTA) IL methods"
  - [corpus] Weak - corpus neighbors don't directly address this mechanism
- Break condition: If the PLM architecture or pre-training is insufficient for the task, even SEQ* may not achieve optimal performance.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding the problem being addressed by the proposed method
  - Quick check question: What is catastrophic forgetting in neural networks?

- Concept: Probing study
  - Why needed here: The method uses probing to measure knowledge preservation in PLMs
  - Quick check question: How does a probing study differ from traditional evaluation?

- Concept: Linear probing
  - Why needed here: The primary metric used to measure PLM knowledge preservation
  - Quick check question: What is linear probing and why is it used in this context?

## Architecture Onboarding

- Component map:
  PLM backbone -> Linear classifiers (one per task) -> Probing classifiers (for evaluation)

- Critical path:
  1. Warm-up PLM on first task
  2. Freeze PLM after warm-up
  3. Freeze old classifiers when learning new tasks
  4. Use linear classifiers for CIL with stored samples
  5. Use cosine linear classifiers for CIL without stored samples

- Design tradeoffs:
  - Freezing PLM vs. fine-tuning all parameters
  - Using linear vs. cosine linear classifiers
  - Storing samples vs. not storing samples

- Failure signatures:
  - High moving distance of class embeddings
  - Significant drop in observed performance compared to probing performance
  - Poor performance on tasks with large pre-training to downstream gap

- First 3 experiments:
  1. Compare probing performance of frozen vs. fine-tuned PLM on Clinic150
  2. Measure moving distance of class embeddings during SEQ on FewRel
  3. Evaluate SEQ* vs. SOTA methods on OntoNotes5 with bert-base-cased

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental mechanism by which PLMs maintain anti-forgetting abilities during incremental learning?
- Basis in paper: [explicit] The paper states that PLMs achieve high probing performance even under sequential fine-tuning, indicating an inherent anti-forgetting ability. However, the exact mechanism is not fully understood.
- Why unresolved: The study reveals that the Transformer architecture and pre-training both contribute to this ability, but a detailed understanding of the internal mechanisms is lacking.
- What evidence would resolve it: Detailed analysis of attention head weights and feature space dynamics during incremental learning, combined with ablation studies on different architectural components, could elucidate the underlying mechanisms.

### Open Question 2
- Question: How does the choice of probing metric affect the evaluation of forgetting in PLMs?
- Basis in paper: [explicit] The paper compares four probing metrics (linear, cosine linear, prototype, and cosine prototype) and finds that linear probing performs best, but the reasons for this are explored in depth.
- Why unresolved: While the paper explains why linear probing is optimal, the impact of different probing metrics on evaluating forgetting across various tasks and settings is not fully explored.
- What evidence would resolve it: Systematic experiments comparing probing metrics across diverse tasks, model architectures, and incremental learning scenarios would clarify their relative effectiveness.

### Open Question 3
- Question: What are the limitations of SEQ* when applied to more complex or diverse incremental learning tasks?
- Basis in paper: [explicit] The paper acknowledges that SEQ* may not perform best when PLMs need to absorb new knowledge or when there are overlaps between new and old tasks, such as in Few-NERD.
- Why unresolved: The paper provides some examples but does not extensively explore the limitations of SEQ* in more complex or varied scenarios.
- What evidence would resolve it: Extensive testing of SEQ* on a wider range of tasks, including those with complex class boundaries or significant task overlap, would identify its limitations and potential areas for improvement.

### Open Question 4
- Question: How can the forward compatibility of classifiers be further enhanced in incremental learning scenarios?
- Basis in paper: [explicit] The paper mentions that pre-allocating future classifiers enhances forward compatibility, but the underlying reasons and potential for further enhancement are not fully explored.
- Why unresolved: While the strategy is effective, the paper does not delve into the mechanisms by which it improves forward compatibility or how it could be optimized further.
- What evidence would resolve it: Experiments varying the number and configuration of pre-allocated classifiers, along with analysis of their impact on forward compatibility, would provide insights into optimization strategies.

### Open Question 5
- Question: What are the implications of PLM anti-forgetting for the design of future incremental learning algorithms?
- Basis in paper: [explicit] The paper suggests that the anti-forgetting ability of PLMs challenges the assumption that catastrophic forgetting is the primary obstacle in incremental learning.
- Why unresolved: The study highlights the need to revisit IL algorithms but does not provide a comprehensive framework for integrating PLM anti-forgetting into future algorithm designs.
- What evidence would resolve it: Developing and evaluating new IL algorithms that leverage PLM anti-forgetting properties, along with comparative studies against existing methods, would clarify the implications for future research.

## Limitations

- The probing methodology may overestimate knowledge preservation as it assumes linear separability of features
- The study focuses primarily on classification tasks and may not generalize to generation or structured prediction tasks
- The moving distance metric for classifier forgetting is a novel concept that needs further validation across diverse PLM architectures and tasks

## Confidence

- **High confidence**: The observation that existing IL methods underestimate PLM's anti-forgetting ability is well-supported by empirical evidence across multiple datasets and tasks.
- **Medium confidence**: The claim that classifier dynamics (not PLM forgetting) drive performance degradation is plausible but requires more theoretical grounding to explain why classifier drift occurs.
- **Medium confidence**: The effectiveness of SEQ* as a simple yet competitive method is demonstrated empirically, though ablation studies could strengthen the case for each component's contribution.

## Next Checks

1. **Cross-task probing validation**: Test the probing methodology on non-classification tasks (e.g., sequence labeling or generation) to verify if linear probing consistently reflects PLM knowledge preservation across task types.

2. **Architecture sensitivity analysis**: Evaluate whether the findings hold across different PLM architectures (e.g., encoder-decoder models, vision-language models) to determine if the results are architecture-dependent.

3. **Long-term stability test**: Assess the moving distance of class embeddings over extended task sequences to determine if classifier drift accumulates or stabilizes over time.