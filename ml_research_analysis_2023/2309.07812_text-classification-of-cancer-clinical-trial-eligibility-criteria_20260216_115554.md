---
ver: rpa2
title: Text Classification of Cancer Clinical Trial Eligibility Criteria
arxiv_id: '2309.07812'
source_url: https://arxiv.org/abs/2309.07812
tags:
- criteria
- trial
- trials
- clinical
- eligibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of automatically identifying clinical
  trials for which a patient is eligible, given that trial eligibility criteria are
  stated in natural language. The authors focus on seven common exclusion criteria
  in cancer trials and develop a method using text classification with domain-specific
  BERT models.
---

# Text Classification of Cancer Clinical Trial Eligibility Criteria

## Quick Facts
- arXiv ID: 2309.07812
- Source URL: https://arxiv.org/abs/2309.07812
- Reference count: 40
- Key outcome: BERT-based models can reliably classify common exclusion criteria in cancer trial eligibility text, with ClinicalTrialBERT achieving highest average performance

## Executive Summary
This paper addresses the challenge of automatically identifying clinical trials for which patients are eligible by classifying exclusion criteria stated in natural language. The authors focus on seven common exclusion criteria in cancer trials and develop a text classification approach using domain-specific BERT models. They introduce ClinicalTrialBERT, a BERT model pre-trained on eligibility criteria sections from ClinicalTrials.gov, and demonstrate its effectiveness compared to other pre-trained models. The method shows promise for streamlining clinical trial recruitment by enabling faster and more accurate identification of relevant trials for patients.

## Method Summary
The method uses BERT-based text classification to detect seven exclusion criteria in cancer clinical trial eligibility text. The authors fine-tune five pre-trained BERT models (BioBERT, ClinicalBERT, BlueBERT, PubMedBERT, SciBERT) plus their newly developed ClinicalTrialBERT on 764 manually annotated Phase III cancer trials. Keyword filtering is applied first to select relevant criteria from eligibility sections, then the selected text is classified by the BERT models. The models are evaluated using 5-fold cross-validation with precision, recall, and F1 scores at both criterion and trial levels.

## Key Results
- ClinicalTrialBERT, pre-trained on eligibility criteria sections, yields the highest average performance across all seven exclusion criteria
- Text classification using domain-specific BERT models can reliably detect exclusion criteria from eligibility text
- Keyword filtering effectively captures relevant criteria with high recall (>0.98) across most exclusions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text classification using domain-specific BERT models can reliably detect exclusion criteria from eligibility text.
- Mechanism: Pre-trained BERT variants are fine-tuned on labeled eligibility criteria, leveraging their masked language model pretraining to capture contextual patterns in clinical trial language.
- Core assumption: The labeled dataset sufficiently represents the language patterns of the targeted exclusion criteria.
- Evidence anchors:
  - [abstract] "Our results demonstrate the feasibility of automatically classifying common exclusion criteria."
  - [section] "Our results indicate that, at the criterion-level evaluation, there are no differences among all models for HBV and Auto."
  - [corpus] Weak: corpus neighbors are all preprints, no citations yet.

### Mechanism 2
- Claim: Pre-training a BERT model on clinical trial eligibility criteria improves classification performance over general biomedical models.
- Mechanism: ClinicalTrialBERT is initialized from ClinicalBERT and further pre-trained on 442,370 eligibility criteria sections, capturing domain-specific terminology and phrasing.
- Core assumption: Eligibility criteria have unique linguistic features not captured by general biomedical pretraining.
- Evidence anchors:
  - [abstract] "we demonstrate the value of a pre-trained language model specifically for clinical trials, which yields the highest average performance across all criteria."
  - [section] "Our pre-trained ClinicalTrialBERT model performed equally or better than other models for three out of five remaining exclusions (Prior, HIV, HCV)."
  - [corpus] Weak: no citation evidence yet; neighbor papers focus on other models.

### Mechanism 3
- Claim: Keyword filtering before BERT classification improves precision and recall by focusing on relevant criteria.
- Mechanism: Keywords are used to select candidate criteria from eligibility sections, ensuring only criteria containing exclusion-related terms are passed to the classifier.
- Core assumption: The keyword lists are both sensitive (high recall) and specific enough to avoid excessive false positives.
- Evidence anchors:
  - [section] "We believe that our keyword lists are suitable for capturing the criteria with the considered exclusions, with some consideration for precision but the primary focus being on recall."
  - [section] Table 4 shows high recall (>0.98) across most exclusions after keyword filtering.
  - [corpus] Weak: no external validation of keyword selection strategy.

## Foundational Learning

- Concept: Transformer-based language models (BERT)
  - Why needed here: BERT's bidirectional context modeling is crucial for understanding nuanced eligibility criteria text.
  - Quick check question: What is the difference between BERT's masked language model objective and autoregressive language modeling?

- Concept: Tokenization and sequence length limits
  - Why needed here: Clinical trial eligibility sections often exceed BERT's 512 token limit, requiring splitting into individual criteria.
  - Quick check question: Why does splitting eligibility sections into individual criteria help with BERT's sequence length constraint?

- Concept: Evaluation metrics for classification
  - Why needed here: Precision, recall, and F1 are needed to assess classifier performance at both criterion and trial levels.
  - Quick check question: How does trial-level evaluation differ from criterion-level evaluation in this context?

## Architecture Onboarding

- Component map: Data ingestion → Keyword filtering → BERT fine-tuning → Cross-validation → Performance evaluation
- Critical path: Data preparation (annotation + keyword filtering) → Model training/fine-tuning → Cross-validation → Result aggregation
- Design tradeoffs: Keyword filtering trades recall for computational efficiency; model selection balances performance vs. complexity.
- Failure signatures: Low recall in keyword filtering → poor downstream classifier performance; overfitting in fine-tuning → poor cross-validation generalization.
- First 3 experiments:
  1. Train ClinicalTrialBERT on the annotated dataset and compare to BioBERT baseline.
  2. Test keyword filtering recall by holding out trials and checking coverage.
  3. Evaluate trial-level vs criterion-level performance differences to understand practical impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we differentiate between criteria that require specific facts extracted and those that are semantically common but lexically diverse?
- Basis in paper: [explicit] The authors mention the need for hybrid solutions that differentiate between criteria requiring specific facts extracted and those that are semantically common yet lexically diverse.
- Why unresolved: The paper does not provide a clear methodology for differentiating between these two types of criteria.
- What evidence would resolve it: A study comparing the performance of different approaches (e.g., information extraction vs. text classification) on a diverse set of eligibility criteria, along with a clear methodology for determining which approach is more suitable for each type of criterion.

### Open Question 2
- Question: How can we extend the text classification framework to include information from protocols or publications?
- Basis in paper: [explicit] The authors mention that many trials did not disclose certain key criteria on ClinicalTrials.gov but only mentioned them in protocols or publications.
- Why unresolved: The paper does not provide a clear methodology for incorporating information from protocols or publications into the text classification framework.
- What evidence would resolve it: A study demonstrating the effectiveness of incorporating information from protocols or publications into the text classification framework, along with a clear methodology for doing so.

### Open Question 3
- Question: How can we evaluate the generalizability of the results to non-oncology trials?
- Basis in paper: [explicit] The authors mention that their study is limited to phase III cancer trials and that the results may not generalize well to other types of trials.
- Why unresolved: The paper does not provide a clear methodology for evaluating the generalizability of the results to non-oncology trials.
- What evidence would resolve it: A study evaluating the performance of the text classification framework on a diverse set of non-oncology trials, along with a clear methodology for determining the generalizability of the results.

## Limitations

- The study is limited to Phase III cancer trials, potentially limiting generalizability to other trial phases or cancer types
- Keyword filtering relies on manually curated lists that prioritize recall over precision, creating potential for false positives
- The relatively small dataset (764 trials) may not capture the full diversity of eligibility criteria language

## Confidence

- **High Confidence**: The feasibility of using BERT-based models for text classification of exclusion criteria is well-supported by the consistent performance across multiple models and evaluation metrics.
- **Medium Confidence**: The superiority of ClinicalTrialBERT over general biomedical models is demonstrated but requires external validation on different datasets to confirm robustness.
- **Medium Confidence**: The keyword filtering strategy effectively captures relevant criteria, though precision tradeoffs and potential missed criteria due to vocabulary limitations are acknowledged concerns.

## Next Checks

1. **External Dataset Validation**: Test the trained ClinicalTrialBERT model on an independent dataset of clinical trial eligibility criteria to assess generalization beyond the PROTECTOR1 dataset.

2. **Statistical Significance Testing**: Conduct paired t-tests or Wilcoxon signed-rank tests to determine if performance differences between models are statistically significant rather than due to random variation.

3. **Precision-Recall Tradeoff Analysis**: Systematically evaluate the impact of different keyword list configurations on both precision and recall to identify optimal filtering strategies that balance computational efficiency with classification accuracy.