---
ver: rpa2
title: Reconstructive Neuron Pruning for Backdoor Defense
arxiv_id: '2305.14876'
source_url: https://arxiv.org/abs/2305.14876
tags:
- backdoor
- defense
- attacks
- neurons
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Reconstructive Neuron Pruning (RNP), a novel
  defense against backdoor attacks in deep neural networks. RNP employs an asymmetric
  unlearning-recovering process: it first unlearns clean neurons using neuron-level
  gradient ascent on clean samples, then recovers the model at the filter level to
  expose backdoor neurons.'
---

# Reconstructive Neuron Pruning for Backdoor Defense

## Quick Facts
- arXiv ID: 2305.14876
- Source URL: https://arxiv.org/abs/2305.14876
- Authors: Boyuan Li, Yiwen Hu, Han Yu, Chaowei Xiao, Yihan He, Qi Alfred Chen, Bo Li
- Reference count: 29
- One-line primary result: Reduces backdoor attack success rates to under 5% while maintaining clean accuracy with less than 2% degradation

## Executive Summary
This paper introduces Reconstructive Neuron Pruning (RNP), a novel defense against backdoor attacks in deep neural networks. RNP employs an asymmetric unlearning-recovering process: it first unlearns clean neurons using neuron-level gradient ascent on clean samples, then recovers the model at the filter level to expose backdoor neurons. The method achieves state-of-the-art results, reducing attack success rates to under 5% across 12 advanced attacks while maintaining clean accuracy with less than 2% degradation. Notably, RNP only needs to prune 41 neurons to reduce BadNets attack success from 100% to 0.20%. The intermediate unlearned model also improves other backdoor defense tasks including trigger recovery, backdoor label detection, and backdoor sample detection. The method demonstrates effectiveness across multiple datasets and model architectures.

## Method Summary
RNP is a defense method against backdoor attacks that operates through an asymmetric unlearning-recovering process. It first unlearns the model using neuron-level gradient ascent to maximize error on clean samples, then recovers the model at the filter level using gradient descent on the same data. The asymmetric granularity (neuron-level unlearning, filter-level recovering) exposes backdoor neurons, which are then pruned using dynamic thresholding. The method requires only 500 clean samples as defense data and achieves significant backdoor removal while preserving clean accuracy.

## Key Results
- Reduces attack success rates to under 5% across 12 advanced backdoor attacks
- Maintains clean accuracy with less than 2% degradation
- Only needs to prune 41 neurons to reduce BadNets attack success from 100% to 0.20%
- Improves performance on other backdoor defense tasks including trigger recovery, backdoor label detection, and backdoor sample detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuron-level unlearning followed by filter-level recovering creates an asymmetric learning procedure that exposes backdoor neurons.
- Mechanism: The unlearning step maximizes the model's error on clean samples at the neuron level, which removes clean neurons more effectively than backdoor neurons. The subsequent filter-level recovering step has coarser granularity, forcing the model to repurpose backdoor neurons to compensate for the loss of clean features. This asymmetric process makes backdoor neurons more identifiable for pruning.
- Core assumption: Backdoor neurons are more resilient to unlearning on clean data than clean neurons, and filter-level recovery provides insufficient capacity to fully restore clean features.
- Evidence anchors:
  - [abstract] "RNP first unlearns the neurons by maximizing the model's error on a small subset of clean samples and then recovers the neurons by minimizing the model's error on the same data. In RNP, unlearning is operated at the neuron level while recovering is operated at the filter level"
  - [section] "Interestingly, we find that if the unlearning is performed at the neuron level while the recovering is performed at the filter level, then the network tends to relocate the backdoor neurons to compensate for the loss of clean features caused by the unlearning."
  - [corpus] Weak - no direct evidence in neighbor papers about asymmetric unlearning-recovering mechanisms.
- Break condition: If backdoor neurons share significant activation patterns with clean neurons, the unlearning step may also affect backdoor neurons, reducing the effectiveness of exposure.

### Mechanism 2
- Claim: The asymmetric unlearning-recovering process reduces the activation strength of backdoor features while preserving or enhancing clean features.
- Mechanism: During unlearning, clean neurons are deactivated or removed, causing the model to rely more on backdoor neurons to maintain performance. During filter-level recovery, the coarser granularity limits the model's ability to fully restore clean features, forcing backdoor neurons to take on more responsibility. This shift in feature importance makes backdoor neurons more prominent and easier to identify through mask values.
- Core assumption: The model will naturally repurpose backdoor neurons when clean neurons are unavailable during recovery.
- Evidence anchors:
  - [section] "Figure 2 visualizes the feature maps... in the recovered model, the activations related to the trigger pattern are greatly decreased (mask value decreases to almost zero) while those of the clean features are significantly boosted"
  - [section] "The backdoor neurons, i.e., neurons associated with the backdoor features, are largely preserved in the unlearned model"
  - [corpus] Missing - neighbor papers focus on pruning magnitude or gradient-based methods rather than feature repurposing.
- Break condition: If the backdoor trigger is highly correlated with clean features, the model may not be able to distinguish between them during recovery.

### Mechanism 3
- Claim: The unlearned model at the intermediate step improves performance on other backdoor defense tasks like trigger recovery and backdoor label detection.
- Mechanism: The unlearning process removes clean neurons, leaving backdoor neurons more prominent. This makes the model's predictions more biased toward the backdoor class, which directly reveals the backdoor label. For trigger recovery, the reduced clean feature space makes it easier to isolate and identify the trigger pattern. For backdoor sample detection, the model's output becomes more sensitive to backdoor triggers.
- Core assumption: Removing clean neurons will make backdoor neurons more dominant in the model's decision-making process.
- Evidence anchors:
  - [abstract] "the unlearned model at the intermediate step of our RNP can be directly used to improve other backdoor defense tasks including backdoor removal, trigger recovery, backdoor label detection, and backdoor sample detection"
  - [section] "When applying the trigger recovery and backdoor detection method Neural Cleanse (NC) on the unlearned model... one can expose the potential backdoor target more easily"
  - [corpus] Weak - neighbor papers don't discuss the utility of intermediate models for other defense tasks.
- Break condition: If backdoor neurons are distributed across multiple layers or classes, the unlearned model may not provide clear signals for other defense tasks.

## Foundational Learning

- Concept: Backdoor attacks in deep neural networks
  - Why needed here: Understanding how backdoor attacks work is essential to grasp why RNP's defense mechanism is effective. The paper assumes readers know that backdoor attacks inject triggers that activate specific neurons during inference.
  - Quick check question: What is the difference between input-space and feature-space backdoor attacks, and how do they affect the model differently?

- Concept: Neuron pruning and filter-level operations
  - Why needed here: RNP operates at both neuron and filter levels, so understanding these concepts is crucial for following the algorithm. The paper assumes familiarity with how neural networks are structured and how pruning works.
  - Quick check question: How does pruning at the filter level differ from pruning at the neuron level in terms of granularity and impact on the model?

- Concept: Unlearning and adversarial training
  - Why needed here: RNP uses unlearning as a core mechanism, which involves maximizing loss rather than minimizing it. Understanding this concept is necessary to follow the algorithm's logic.
  - Quick check question: What is the difference between traditional model training and unlearning, and how does unlearning help expose backdoor neurons?

## Architecture Onboarding

- Component map:
  - Backdoored model -> Unlearning module (neuron-level gradient ascent) -> Filter mask learning (filter-level gradient descent) -> Pruning module (dynamic thresholding) -> Clean model

- Critical path:
  1. Unlearn the model using neuron-level gradient ascent on defense data
  2. Learn filter mask using filter-level gradient descent on the same defense data
  3. Prune neurons based on the learned mask using dynamic thresholding
  4. Evaluate the pruned model on clean and backdoored test sets

- Design tradeoffs:
  - Neuron-level unlearning vs. filter-level unlearning: Neuron-level provides finer control but may affect more neurons, while filter-level is coarser but may miss some backdoor neurons
  - Number of unlearning epochs: More epochs may better expose backdoor neurons but risk damaging clean neurons
  - Dynamic thresholding vs. fixed fraction pruning: Dynamic thresholding adapts to the attack but requires additional computation

- Failure signatures:
  - High attack success rate after pruning: Indicates that backdoor neurons were not properly identified or removed
  - Significant drop in clean accuracy: Suggests that too many clean neurons were pruned along with backdoor neurons
  - Unlearned model with high clean accuracy: Indicates that the unlearning step was not aggressive enough to expose backdoor neurons

- First 3 experiments:
  1. Test RNP on a simple BadNets attack with a single trigger pattern to verify the basic mechanism works
  2. Test RNP on a feature-space attack (FC) to verify it can handle more complex backdoor injection methods
  3. Test RNP with different numbers of defense samples (100, 500, 1000) to determine the minimum effective defense data size

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- The paper lacks rigorous theoretical justification for why the asymmetric unlearning-recovering mechanism specifically exposes backdoor neurons
- Effectiveness on lightweight models like EfficientNet may be limited due to insufficient neuron redundancy
- The mechanism's generalization to non-CNN architectures (transformers, graph neural networks) is not explored

## Confidence
- High: Claims about RNP's effectiveness in reducing attack success rates below 5% across 12 attack types are well-supported by quantitative results
- Medium: Claims about RNP's ability to improve other backdoor defense tasks through the unlearned intermediate model have some evidence but limited ablation studies
- Medium: Claims about the asymmetric unlearning-recovering mechanism exposing backdoor neurons are supported by visualizations but lack rigorous theoretical grounding

## Next Checks
1. Conduct ablation studies comparing RNP with neuron-level only pruning and filter-level only pruning to isolate the contribution of the asymmetric mechanism
2. Test RNP's effectiveness on backdoor attacks with multiple trigger patterns simultaneously to assess scalability
3. Evaluate whether the unlearned intermediate model consistently improves other defense methods across different backdoor attack scenarios