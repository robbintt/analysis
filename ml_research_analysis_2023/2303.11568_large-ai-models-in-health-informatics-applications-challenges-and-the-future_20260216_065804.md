---
ver: rpa2
title: 'Large AI Models in Health Informatics: Applications, Challenges, and the Future'
arxiv_id: '2303.11568'
source_url: https://arxiv.org/abs/2303.11568
tags:
- medical
- arxiv
- lams
- learning
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper reviews the rapidly evolving landscape of Large AI\
  \ Models (LAMs), such as ChatGPT, and their emerging applications in health informatics.\
  \ It categorizes LAMs into four types\u2014large language models (LLMs), large vision\
  \ models (LVMs), large audio models (LAudiMs), and large multi-modal models (LMMs)\u2014\
  and discusses their roles across seven key sectors: molecular biology and drug discovery,\
  \ medical diagnosis and decision-making, medical imaging and vision, medical informatics,\
  \ medical education, public health, and medical robotics."
---

# Large AI Models in Health Informatics: Applications, Challenges, and the Future

## Quick Facts
- arXiv ID: 2303.11568
- Source URL: https://arxiv.org/abs/2303.11568
- Reference count: 40
- One-line primary result: Comprehensive review of large AI models' applications, challenges, and future directions in health informatics across seven key sectors

## Executive Summary
This paper provides a systematic review of Large AI Models (LAMs) and their emerging applications in health informatics. The authors categorize LAMs into four types—large language models (LLMs), large vision models (LVMs), large audio models (LAudiMs), and large multi-modal models (LMMs)—and examine their roles across molecular biology, medical diagnosis, medical imaging, medical informatics, medical education, public health, and medical robotics. The review highlights how LAMs leverage self-supervised learning and vast multimodal biomedical data to address longstanding challenges such as data annotation costs and model generalization. While demonstrating significant achievements including AlphaFold's breakthroughs in protein structure prediction and improved medical imaging segmentation, the authors emphasize critical challenges including data privacy, ethical risks, adversarial vulnerabilities, and the need for robust evaluation metrics and regulations.

## Method Summary
This paper presents a comprehensive literature review of large AI models in health informatics. The authors systematically categorize LAMs into four types and examine their applications across seven key sectors in healthcare. The review methodology involves synthesizing existing research from 40 references to identify current achievements, challenges, and future directions. The paper draws on recent developments in deep learning, transformer architectures, and multimodal learning to provide a holistic view of LAMs' potential in transforming healthcare delivery and medical discovery. While not conducting original empirical research, the review provides theoretical frameworks and identifies critical research gaps requiring further investigation.

## Key Results
- LAMs demonstrate transformative potential across seven health informatics sectors through self-supervised learning on multimodal biomedical data
- AlphaFold2 represents a breakthrough in protein structure prediction, while transformer-based models improve medical imaging segmentation
- Critical challenges include data privacy, ethical risks, adversarial vulnerabilities, and the need for robust evaluation metrics and regulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised learning enables large AI models to leverage unlabeled biomedical data at scale, overcoming the annotation bottleneck in healthcare
- Mechanism: By training on massive amounts of unlabeled data (e.g., protein sequences, medical images) without requiring expert annotations, these models learn rich representations that can be transferred to various downstream tasks
- Core assumption: The inherent structure and patterns in unlabeled biomedical data contain sufficient information for the model to learn useful representations without explicit labels
- Evidence anchors:
  - [abstract] "However, this might not be a bottleneck for LAMs, as they can leverage self-supervision and reinforcement learning in training, relieving the annotation burden and workload of creating large-scale annotated datasets"
  - [section] "However, such annotations have to be conducted by domain experts, which is often expensive and time-consuming. This causes the curation of large-scale medical and clinical data with high-quality annotations to be challenging. However, this might not be a bottleneck for LAMs, as they can leverage self-supervision..."
  - [corpus] Weak - corpus does not contain direct evidence about self-supervision in biomedical contexts
- Break condition: If biomedical data lacks sufficient inherent structure for self-supervised learning to extract meaningful representations

### Mechanism 2
- Claim: Large-scale pretraining with diverse multimodal biomedical data enables LAMs to develop generalizable representations across different health informatics domains
- Mechanism: Models pretrained on massive heterogeneous datasets (text, images, sequences, signals) learn cross-modal relationships and domain-agnostic features that transfer effectively to specialized tasks
- Core assumption: Biomedical data across different modalities share underlying patterns that can be captured by a single model architecture
- Evidence anchors:
  - [abstract] "The scale of multimodality data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas"
  - [section] "Fig. 3 shows common biomedical and healthcare data modalities. The multi-modal nature of biomedical and health data provides the natural and promising ground for fostering LAMs in this field"
  - [corpus] Missing - corpus neighbors do not discuss multimodal pretraining approaches
- Break condition: If domain-specific features cannot be captured by general multimodal representations, requiring specialized architectures

### Mechanism 3
- Claim: Transfer learning from large general-domain models to biomedical tasks enables rapid development of specialized health informatics applications
- Mechanism: Pre-trained general models (LLMs, LVMs) can be fine-tuned on biomedical data to achieve competitive performance with significantly less task-specific training data
- Core assumption: Knowledge learned from general domains transfers effectively to specialized biomedical contexts
- Evidence anchors:
  - [abstract] "Once pretrained, large AI models demonstrate impressive performance in various downstream tasks"
  - [section] "BioBERT [191], a seminal BioLLM which outperformed previous state-of-the-art on various biomedical text mining tasks such as biomedical named entity recognition, many different BioLLMs that stem from their general LLM counterparts have been proposed"
  - [corpus] Weak - corpus mentions foundation models but not specific transfer learning examples in biomedicine
- Break condition: If domain-specific knowledge is too specialized to transfer effectively from general models

## Foundational Learning

- Concept: Self-supervised learning techniques
  - Why needed here: Enables training on massive unlabeled biomedical datasets without expensive expert annotations
  - Quick check question: Can you explain the difference between masked language modeling and contrastive learning in the context of biomedical data?

- Concept: Multimodal representation learning
  - Why needed here: Biomedical data exists in diverse modalities (text, images, sequences, signals) that need to be integrated
  - Quick check question: How would you design a unified representation space for protein sequences, medical images, and EHR data?

- Concept: Transfer learning and fine-tuning
  - Why needed here: Allows leveraging large general models for specialized biomedical applications with limited task-specific data
  - Quick check question: What factors determine whether knowledge transfers effectively from general to biomedical domains?

## Architecture Onboarding

- Component map: Pretraining layer -> Fine-tuning layer -> Inference layer -> Safety layer
- Critical path: Pretraining → Fine-tuning → Deployment → Monitoring
  Focus on establishing robust pretraining pipelines first, as downstream performance depends heavily on quality of learned representations
- Design tradeoffs:
  - Model size vs. inference efficiency: Larger models offer better performance but require more computational resources
  - Pretraining data diversity vs. domain specificity: Broader data improves generalization but may dilute domain-specific features
  - Self-supervision objectives: Different tasks (masked prediction, contrastive learning) may work better for different biomedical modalities
- Failure signatures:
  - Poor transfer from general to biomedical domains (indicated by low fine-tuning performance)
  - Overfitting to specific datasets (poor generalization to new institutions or patient populations)
  - Adversarial vulnerabilities in clinical decision-making scenarios
- First 3 experiments:
  1. Implement masked language modeling on PubMed abstracts to establish baseline biomedical text representations
  2. Test contrastive learning on paired medical images and reports to learn cross-modal embeddings
  3. Fine-tune pretrained general model on MIMIC-III EHR data for disease prediction task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large AI models completely replace experimental methods in molecular biology, such as protein structure determination?
- Basis in paper: [explicit] The paper discusses that while LAMs like AlphaFold2 have made significant progress in protein structure prediction, they still rely heavily on databases and may not handle rare or unseen data well. It raises the question of whether AI can fully replace experimental techniques like Cryo-EM
- Why unresolved: The paper highlights the mutual and complementary benefits between LAMs and experimental methods, but it does not provide a definitive answer on whether AI can entirely replace these methods. The quality assessment of model predictions for unknown structures remains unclear
- What evidence would resolve it: Comparative studies demonstrating the accuracy and reliability of LAM predictions versus experimental methods across a wide range of proteins, including rare and novel structures, would help resolve this question

### Open Question 2
- Question: How can we ensure the robustness and fairness of large AI models in biomedical and healthcare applications?
- Basis in paper: [explicit] The paper discusses the challenges of data inequality and bias in current LAM datasets, which are often skewed against resource-poor countries and rare diseases. It also mentions the vulnerability of LAMs to adversarial attacks
- Why unresolved: The paper emphasizes the need for continuous monitoring and efforts to ensure LAMs do not develop biases, but it does not provide specific solutions or strategies to achieve this. The lack of robust evaluation metrics for fairness and security is also highlighted
- What evidence would resolve it: Development and validation of robust evaluation metrics for fairness and security in LAMs, along with successful implementations of bias mitigation strategies, would help address this question

### Open Question 3
- Question: How can we improve the interpretability and explainability of large AI models' outputs in healthcare?
- Basis in paper: [explicit] The paper mentions the transparency challenges of LAMs, including the lack of access to training data and the models themselves. It also discusses the need for explicit evaluation metrics that can measure the scientific and factual grounding of LAMs
- Why unresolved: The paper highlights the importance of interpretability and explainability but does not provide concrete methods or frameworks to achieve this. The current lack of transparency in LAM development and deployment is a significant barrier
- What evidence would resolve it: Development of interpretable LAM architectures, along with successful implementations of explainability techniques that provide clear and understandable insights into model decisions, would help resolve this question

## Limitations

- The review relies heavily on theoretical frameworks without systematic empirical validation of LAM performance across the seven sectors discussed
- Specific implementation details and comparative performance metrics for individual LAMs are often lacking
- The paper acknowledges significant challenges around data privacy, ethical risks, and adversarial vulnerabilities but does not provide quantitative assessments or mitigation strategies

## Confidence

- High confidence: The classification of LAM types and their general capabilities
- Medium confidence: Applications across the seven sectors based on existing literature
- Low confidence: Quantitative impact assessments and risk mitigation strategies

## Next Checks

1. Conduct systematic benchmarking of LAM performance across different biomedical modalities to verify claimed transfer learning capabilities
2. Implement controlled experiments testing adversarial robustness in clinical decision-making scenarios to quantify security vulnerabilities
3. Develop standardized evaluation frameworks for LAM interpretability and bias detection specific to healthcare applications