---
ver: rpa2
title: '$DA^3$: A Distribution-Aware Adversarial Attack against Language Models'
arxiv_id: '2311.08598'
source_url: https://arxiv.org/abs/2311.08598
tags:
- adversarial
- examples
- original
- attack
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical vulnerability in adversarial
  attacks on language models: generated adversarial examples exhibit distribution
  shifts from original examples, making them easily detectable via simple OOD detection
  methods like Maximum Softmax Probability (MSP) and Mahalanobis Distance (MD), thereby
  reducing attack effectiveness. To address this, the authors propose DALA (Distribution-Aware
  LoRA-based Adversarial Attack), a two-phase method.'
---

# $DA^3$: A Distribution-Aware Adversarial Attack against Language Models

## Quick Facts
- **arXiv ID**: 2311.08598
- **Source URL**: https://arxiv.org/abs/2311.08598
- **Reference count**: 40
- **Key outcome**: Adversarial examples generated by previous methods exhibit detectable distribution shifts, making them vulnerable to simple OOD detection methods like MSP and MD.

## Executive Summary
This paper addresses a critical vulnerability in adversarial attacks on language models: generated adversarial examples exhibit distribution shifts from original examples, making them easily detectable via simple OOD detection methods like Maximum Softmax Probability (MSP) and Mahalanobis Distance (MD), thereby reducing attack effectiveness. To address this, the authors propose DALA (Distribution-Aware LoRA-based Adversarial Attack), a two-phase method that first fine-tunes a LoRA-based PLM using Data Alignment Loss to generate distribution-aligned examples, then generates adversarial examples through masked language modeling with minimal perturbation. The method introduces a novel evaluation metric, Non-detectable Attack Success Rate (NASR), which combines Attack Success Rate with OOD detection to provide a more realistic assessment of attack effectiveness.

## Method Summary
The proposed method addresses the distribution shift problem in adversarial examples by employing a two-phase approach. First, a LoRA-based PLM is fine-tuned using Data Alignment Loss (combining MSP and MD losses) to generate examples that closely match the original data distribution. This fine-tuning process involves masking tokens and optimizing the model to generate examples with similar MSP and MD values to the originals. During inference, the fine-tuned model generates adversarial examples through a masked language modeling task, iteratively replacing important tokens with minimal perturbation until the attack succeeds or a termination condition is met. The method is evaluated on four datasets (SST-2, CoLA, RTE, MRPC) using both white-box models (BERT-BASE, RoBERTa-BASE) and black-box LLaMA2-7b, with results showing competitive Attack Success Rate (ASR) and improved Non-detectable Attack Success Rate (NASR) compared to baselines like TextFooler, TextBugger, DeepWordBug, and BERT-Attack.

## Key Results
- DALA achieves competitive ASR and NASR on BERT-BASE and RoBERTa-BASE compared to baseline methods
- DALA demonstrates superior transferability to black-box LLaMA2-7b model compared to baselines
- The ablation study confirms the effectiveness of both MSP and MD loss components in improving attack undetectability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversarial examples generated by previous attack methods exhibit distribution shifts that make them easily detectable via simple OOD detection methods.
- Mechanism: The proposed method addresses this by aligning the distribution of adversarial examples with original examples through Data Alignment Loss, which combines Maximum Softmax Probability (MSP) Loss and Mahalanobis Distance (MD) Loss during fine-tuning.
- Core assumption: Distribution shifts in adversarial examples are the primary reason for their detectability, and aligning these distributions will improve attack effectiveness.
- Evidence anchors:
  - [abstract]: "generated adversarial examples have a different data distribution compared with the original examples, making them easily detectable via simple OOD detection methods like Maximum Softmax Probability (MSP) and Mahalanobis Distance (MD)"
  - [section 3]: "we find disparities in distribution manifest between adversarial examples and original examples, which implies original examples are in-distribution examples while adversarial examples are Out-of-Distribution (OOD) examples"
  - [corpus]: Weak evidence - corpus lacks direct studies on distribution alignment for adversarial attacks
- Break condition: If the distribution alignment fails to capture the full complexity of what makes adversarial examples detectable, or if detection methods evolve beyond simple MSP/MD metrics.

### Mechanism 2
- Claim: The two-phase approach (fine-tuning + inference) enables generation of adversarial examples with minimal perturbation while maintaining distribution alignment.
- Mechanism: During fine-tuning, a LoRA-based PLM is trained to generate examples that match original examples in MSP and MD. During inference, this fine-tuned model generates adversarial examples through masked language modeling with minimal token changes.
- Core assumption: The LoRA-based fine-tuning can effectively learn to generate distribution-aligned adversarial examples that still successfully attack the target model.
- Evidence anchors:
  - [section 4.2]: "DALA consists of two phases: fine-tuning and inference" and describes how the LoRA-based PLM is fine-tuned and then used for inference
  - [section 4.3]: Describes the Data Alignment Loss combining MSP and MD components
  - [corpus]: No direct corpus evidence for this specific two-phase LoRA-based approach
- Break condition: If the fine-tuned LoRA-based model fails to generalize to new examples, or if minimal perturbation during inference compromises attack success.

### Mechanism 3
- Claim: The novel evaluation metric NASR effectively measures both attack success and detectability, providing a more realistic assessment of attack effectiveness.
- Mechanism: NASR combines Attack Success Rate (ASR) with OOD detection metrics (MSP and MD detection), requiring adversarial examples to both fool the model and evade detection.
- Core assumption: Traditional ASR alone is insufficient because detectable adversarial examples have limited practical value, and NASR better reflects real-world attack effectiveness.
- Evidence anchors:
  - [section 5]: "NASR posits that the indicative criterion for a successful attack resides in the capacity of an adversarial example to cause failure in the victim model while concurrently eluding OOD detection methods"
  - [section 6]: Describes how NASR is formulated and calculated
  - [corpus]: Weak evidence - corpus lacks studies on combined attack success/detectability metrics
- Break condition: If NASR fails to capture other important aspects of attack effectiveness, or if detection methods become more sophisticated than simple MSP/MD metrics.

## Foundational Learning

- Concept: Distribution shifts and out-of-distribution (OOD) detection
  - Why needed here: Understanding why adversarial examples are detectable and how to measure distribution alignment is fundamental to the proposed method
  - Quick check question: What are the key differences between in-distribution and out-of-distribution examples in terms of confidence scores and distance metrics?

- Concept: Masked Language Modeling (MLM) and fine-tuning techniques
  - Why needed here: The method relies on fine-tuning a LoRA-based PLM using MLM tasks to learn distribution-aligned adversarial generation
  - Quick check question: How does fine-tuning with Data Alignment Loss differ from standard MLM fine-tuning objectives?

- Concept: LoRA (Low-Rank Adaptation) for efficient model adaptation
  - Why needed here: The proposed method uses LoRA to efficiently fine-tune PLMs for adversarial example generation without full model retraining
  - Quick check question: What are the advantages of using LoRA over full fine-tuning for this adversarial attack task?

## Architecture Onboarding

- Component map:
  - Original sentence and label -> LoRA-based PLM fine-tuning with Data Alignment Loss -> Token importance ranking -> Masked Language Modeling with MLM -> Adversarial sentence output

- Critical path:
  1. Fine-tune LoRA-based PLM using Data Alignment Loss on training data
  2. During inference, rank tokens by importance
  3. Iteratively mask and replace tokens using MLM
  4. Stop when attack succeeds or termination condition met

- Design tradeoffs:
  - LoRA vs full fine-tuning: LoRA is more efficient but may have limited adaptation capacity
  - MSP vs MD focus: Balancing these two aspects of distribution alignment
  - Token selection strategy: Tradeoff between perturbation size and attack success rate

- Failure signatures:
  - High Detection Rate (DR) despite good ASR indicates distribution misalignment
  - Poor transferability to black-box models suggests overfitting to white-box model
  - Slow inference or high computational cost may indicate inefficient token selection

- First 3 experiments:
  1. Verify distribution shifts exist: Compare MSP and MD of original vs adversarial examples generated by baseline methods
  2. Test fine-tuning effectiveness: Measure how well fine-tuned LoRA-based model generates distribution-aligned examples
  3. Evaluate NASR metric: Compare NASR scores of proposed method vs baselines to validate effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the trade-off between MSP loss and MD loss affect the overall performance of DALA in terms of attack success rate and detection rate?
- Basis in paper: [explicit] The paper discusses the trade-off relationship between MSP loss and MD loss, noting that they often exhibit opposite trends at the individual step level during fine-tuning.
- Why unresolved: The paper does not provide a detailed analysis of how balancing these two losses impacts the overall effectiveness of the attack, especially in terms of maintaining high attack success rates while minimizing detection.
- What evidence would resolve it: Conducting experiments that systematically vary the weights of MSP and MD losses to observe their impact on ASR and NASR would provide insights into the optimal balance.

### Open Question 2
- Question: How does the performance of DALA compare when attacking different types of language models, such as those trained on specialized datasets or models with different architectures?
- Basis in paper: [inferred] The paper tests DALA on BERT-BASE and LLaMA2-7b, but does not explore its effectiveness against other architectures or specialized models.
- Why unresolved: The generalizability of DALA's effectiveness across diverse model architectures and specialized datasets remains untested, limiting understanding of its broader applicability.
- What evidence would resolve it: Testing DALA against a variety of models, including those trained on niche datasets or using different architectures, would clarify its versatility and robustness.

### Open Question 3
- Question: What are the computational costs associated with DALA compared to other adversarial attack methods, and how do these costs impact its practical deployment?
- Basis in paper: [inferred] While the paper outlines the methodology and effectiveness of DALA, it does not discuss the computational resources required for its implementation.
- Why unresolved: Understanding the computational efficiency of DALA is crucial for assessing its practicality in real-world applications, especially in resource-constrained environments.
- What evidence would resolve it: Benchmarking the computational time and resource usage of DALA against other methods would provide a clearer picture of its practical deployment feasibility.

## Limitations
- The method relies on relatively simple OOD detection methods (MSP and MD) that may not capture all aspects of distribution shifts
- The two-phase approach requires careful hyperparameter tuning that isn't fully specified in the paper
- The LoRA-based fine-tuning may have limited capacity to capture complex distribution alignments

## Confidence
- **High Confidence**: The core observation that adversarial examples exhibit detectable distribution shifts from original examples is well-supported by the experimental evidence across multiple datasets and detection methods.
- **Medium Confidence**: The effectiveness of the two-phase LoRA-based approach in generating distribution-aligned adversarial examples is demonstrated, but the specific implementation details and hyperparameter sensitivity are not fully transparent.
- **Low Confidence**: The claim that DALA achieves "superior transferability" to black-box LLaMA2-7b compared to baselines is based on a single black-box model evaluation.

## Next Checks
1. **Robustness to Advanced Detection Methods**: Test whether DALA-generated adversarial examples remain undetectable when evaluated against more sophisticated OOD detection approaches beyond MSP and MD, such as energy-based models or contrastive learning-based detectors.

2. **Hyperparameter Sensitivity Analysis**: Conduct a systematic ablation study varying key hyperparameters in the fine-tuning phase (learning rate, masking percentage, LoRA rank) to quantify their impact on both attack success and detectability, providing clearer guidance for practical implementation.

3. **Cross-Model Transferability Validation**: Evaluate the transferability of DALA-generated adversarial examples across a broader range of black-box models including different architectures (T5, GPT variants) and sizes to verify whether the claimed transferability advantages are architecture-agnostic or specific to LLaMA2-7b.