---
ver: rpa2
title: 'Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training'
arxiv_id: '2305.14342'
source_url: https://arxiv.org/abs/2305.14342
tags:
- hessian
- sophia
- loss
- learning
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sophia is a second-order optimizer that estimates the diagonal
  Hessian of the loss as a pre-conditioner and applies per-coordinate clipping to
  control the worst-case update size. This allows Sophia to adapt more efficiently
  to heterogeneous curvatures across parameter dimensions, achieving a 2x speedup
  over AdamW in terms of steps, total compute, and wall-clock time on GPT-2 pre-training.
---

# Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training

## Quick Facts
- arXiv ID: 2305.14342
- Source URL: https://arxiv.org/abs/2305.14342
- Authors: 
- Reference count: 40
- Sophia achieves 2x speedup over AdamW in terms of steps, total compute, and wall-clock time on GPT-2 pre-training

## Executive Summary
Sophia is a second-order optimizer that estimates the diagonal Hessian of the loss as a pre-conditioner and applies per-coordinate clipping to control the worst-case update size. This allows Sophia to adapt more efficiently to heterogeneous curvatures across parameter dimensions. The speed-up comes from the Hessian-based pre-conditioner, which automatically adjusts the update size according to the curvature, and the clipping mechanism, which safeguards against inaccurate Hessian estimates and non-convexity. Sophia only estimates the diagonal Hessian every few steps, which has negligible overhead.

## Method Summary
Sophia estimates diagonal Hessian every k steps (k=10) and applies element-wise clipping to prevent large, destabilizing updates. The optimizer uses a biased Gauss-Newton-Bartlett estimator to ensure positive semi-definite diagonal Hessian, providing descent directions. EMA smoothing reduces noise in gradient and Hessian estimates. Theoretical analysis shows Sophia's runtime bound does not depend on the condition number of the loss, demonstrating its advantage in adapting to heterogeneous curvatures. The method was tested on GPT-2 pre-training across multiple model sizes (30M, 125M, 355M, 540M, 770M parameters) on OpenWebText.

## Key Results
- 2x speedup over AdamW in steps, total compute, and wall-clock time
- Larger models show greater improvement: 540M Sophia-H outperforms 770M AdamW after same training steps
- Hessian estimation overhead is negligible, averaging 2.5% of total runtime
- Fewer gradient clipping events compared to AdamW and Lion (less than 10% of steps)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-coordinate clipping combined with diagonal Hessian pre-conditioning adapts learning rates to heterogeneous curvatures.
- Mechanism: The diagonal Hessian estimates curvature along each parameter dimension, automatically adjusting update sizes inversely to curvature magnitude. Clipping caps maximum updates, preventing instability from inaccurate Hessian estimates or negative curvature.
- Core assumption: Hessian estimates are sufficiently accurate and updated frequently enough (every 10 steps) to track curvature changes.
- Evidence anchors:
  - [abstract] "Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead."
  - [section 2.1] "The clipping mechanism kicks in and the optimizer defaults to SignGD...which is sub-optimal for benign situations."
  - [corpus] Weak evidence; neighboring papers focus on second-order methods but don't specifically validate per-coordinate clipping.

### Mechanism 2
- Claim: Estimating diagonal Hessian every k steps with EMA denoising balances computational overhead against adaptation speed.
- Mechanism: Computing full diagonal Hessian every step would be expensive. By estimating every k steps and applying EMA, the optimizer captures curvature trends while keeping per-step cost low.
- Core assumption: Curvature changes slowly enough that k=10 is sufficient to track relevant changes without introducing instability.
- Evidence anchors:
  - [abstract] "Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead."
  - [section 2.2] "Similar to the EMA of moments of gradients in Adam, we also denoise the diagonal Hessian estimates with EMA across iterations."
  - [corpus] No direct evidence; neighboring papers discuss second-order methods but not EMA-based Hessian estimation.

### Mechanism 3
- Claim: Using biased Gauss-Newton-Bartlett estimator ensures positive semi-definite diagonal Hessian, providing descent directions.
- Mechanism: The GNB estimator constructs an unbiased estimate of the Gauss-Newton matrix, which is positive semi-definite. This guarantees that the pre-conditioned update is always a descent direction, avoiding ascent in non-convex regions.
- Core assumption: The Gauss-Newton approximation to the Hessian is sufficiently accurate for effective pre-conditioning.
- Evidence anchors:
  - [section 2.3] "the Gauss-Newton term...is often relatively smaller than the first term...and used as pre-conditioners in second-order optimizers."
  - [section 2.3] "the PSDness ensures that the pre-conditioned update is always a descent direction."
  - [corpus] Weak evidence; neighboring papers discuss second-order methods but don't specifically validate the GNB estimator.

## Foundational Learning

- Concept: Diagonal Hessian estimation
  - Why needed here: Provides per-dimension curvature information for adaptive learning rates, crucial for handling heterogeneous curvatures in LLM training.
  - Quick check question: What's the difference between estimating the full Hessian vs. the diagonal Hessian in terms of computational complexity and memory usage?

- Concept: Moving average (EMA) smoothing
  - Why needed here: Reduces noise in gradient and Hessian estimates, stabilizing updates and preventing overfitting to mini-batch fluctuations.
  - Quick check question: How does the choice of β1 and β2 affect the trade-off between responsiveness and stability in the EMA?

- Concept: Per-coordinate clipping
  - Why needed here: Prevents large, destabilizing updates when Hessian estimates are inaccurate or when encountering negative curvature, while allowing larger updates in flat directions.
  - Quick check question: What's the effect of the clipping threshold ρ on the balance between stability and convergence speed?

## Architecture Onboarding

- Component map: Forward pass → loss computation → gradient → (periodic) Hessian estimate → EMA updates → clipped update → parameter update
- Critical path: Forward pass → loss computation → gradient → (periodic) Hessian estimate → EMA updates → clipped update → parameter update
- Design tradeoffs:
  - k (Hessian estimation frequency) vs. adaptation speed and computational overhead
  - ρ (clipping threshold) vs. stability and convergence speed
  - Estimator choice (Hutchinson vs. GNB) vs. bias, variance, and computational requirements
- Failure signatures:
  - Excessive clipping (ρ too low or Hessian estimates too noisy)
  - Stale Hessian estimates (k too high or curvature changes too rapidly)
  - Poor convergence (GNB estimator inaccurate in highly non-convex regions)
- First 3 experiments:
  1. Verify per-coordinate clipping works by testing on a simple 2D quadratic with heterogeneous curvatures.
  2. Compare Hutchinson vs. GNB estimators on a small model to measure bias, variance, and computational overhead.
  3. Tune k and ρ on a 30M parameter model to find optimal balance between stability and convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact scaling relationship between the number of parameters in a model and the improvement in training speed when using Sophia compared to AdamW?
- Basis in paper: The authors state that the gap between Sophia and AdamW grows as the model size increases, and that a 540M parameter model trained with Sophia-H has a smaller loss than a 770M parameter model trained with AdamW after the same number of steps.
- Why unresolved: The paper provides a general trend but does not give a precise mathematical relationship or functional form for how the improvement scales with model size.
- What evidence would resolve it: Additional experiments across a wider range of model sizes and a detailed analysis of the scaling laws governing the performance difference between Sophia and AdamW.

### Open Question 2
- Question: How does the performance of Sophia compare to other second-order optimization methods beyond the ones mentioned in the paper (e.g., K-FAC, Shampoo)?
- Basis in paper: The authors mention that previous second-order optimizers have not achieved a speedup on large language models in wall-clock time or total compute, but do not provide a direct comparison with other methods.
- Why unresolved: The paper focuses on comparing Sophia to AdamW and Lion, leaving the question of how it fares against other second-order methods open.
- What evidence would resolve it: A comprehensive benchmarking study comparing Sophia to a range of other second-order optimization methods on large language model pre-training tasks.

### Open Question 3
- Question: What is the theoretical justification for the choice of the clipping threshold ρ in Sophia, and how sensitive is the algorithm's performance to this hyperparameter?
- Basis in paper: The authors provide theoretical analysis showing that Sophia's runtime bound does not depend on the condition number of the loss, but they do not explain the theoretical basis for choosing ρ or analyze its sensitivity.
- Why unresolved: The paper mentions that ρ is chosen based on grid search and that the hyperparameters are transferable across model sizes, but does not provide a theoretical justification or sensitivity analysis.
- What evidence would resolve it: A rigorous theoretical analysis of the role of ρ in Sophia's convergence and stability, along with empirical studies on the sensitivity of the algorithm's performance to different values of ρ.

## Limitations
- The effectiveness of per-coordinate clipping heavily depends on the choice of clipping threshold ρ, which is treated as a hyperparameter without systematic sensitivity analysis
- While the paper claims Hessian estimation every 10 steps has "negligible overhead," the actual computational cost scaling with model size and sequence length is not fully quantified
- The comparison against AdamW and Lion is primarily on GPT-2; generalizability to other architectures (BERT, decoder-only variants) remains untested

## Confidence
- High confidence: Sophia achieves 2x speedup over AdamW in terms of steps, total compute, and wall-clock time on GPT-2 pre-training
- Medium confidence: Per-coordinate clipping combined with diagonal Hessian pre-conditioning adapts learning rates to heterogeneous curvatures
- Medium confidence: Estimating diagonal Hessian every k steps with EMA denoising balances computational overhead against adaptation speed
- Low confidence: Using biased Gauss-Newton-Bartlett estimator ensures positive semi-definite diagonal Hessian providing descent directions (weak empirical validation)

## Next Checks
1. Systematically vary the clipping threshold ρ across multiple orders of magnitude (10^-3 to 10^3) on GPT-2 small to quantify its impact on training stability and convergence speed
2. Measure and report the actual per-step overhead of diagonal Hessian estimation at different model scales (30M, 355M, 770M) to validate the "negligible overhead" claim with concrete numbers
3. Test Sophia on a BERT-style encoder model pre-trained on GLUE to assess cross-architecture generalization beyond autoregressive models