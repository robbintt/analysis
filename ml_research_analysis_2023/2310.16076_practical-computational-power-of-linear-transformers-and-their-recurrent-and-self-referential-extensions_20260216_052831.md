---
ver: rpa2
title: Practical Computational Power of Linear Transformers and Their Recurrent and
  Self-Referential Extensions
arxiv_id: '2310.16076'
source_url: https://arxiv.org/abs/2310.16076
tags:
- recurrent
- neural
- learning
- proc
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies linear Transformers (LTs) and their computational
  power compared to standard Transformers and RNNs. The authors prove that several
  existing results on Transformers directly transfer to LTs.
---

# Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions

## Quick Facts
- arXiv ID: 2310.16076
- Source URL: https://arxiv.org/abs/2310.16076
- Authors: 
- Reference count: 34
- Linear Transformers (LTs) can be equivalently expressed as RNN-like processors with fixed-size state

## Executive Summary
This paper studies linear Transformers (LTs) and their computational power compared to standard Transformers and RNNs. The authors prove that several existing results on Transformers directly transfer to LTs. They then empirically evaluate LTs on formal language recognition tasks, showing that they struggle with certain regular languages like parity, similar to Transformers. However, they show that recurrent and self-referential extensions to LTs can overcome these limitations, successfully learning to recognize parity and other regular languages.

## Method Summary
The paper evaluates linear Transformers and their extensions on formal language recognition tasks. The core LT architecture maintains a state matrix W_t updated additively at each time step through outer products of value and key vectors. The authors test standard LTs, delta-rule extensions (introducing explicit forgetting), recurrent extensions (adding feedback connections), and self-referential extensions (allowing the model to modify its own parameters). They train on 8 formal language tasks using pre-generated datasets and compare against baseline LSTM/e-LSTM/Transformer results, measuring sequence-level accuracy on short (≤50) and medium (51-100) length sequences.

## Key Results
- Standard LTs struggle with parity and (aa)* tasks but succeed on counter languages
- Recurrent and self-referential extensions successfully solve parity, (aa)*, and reset Dyck-1 tasks
- Delta-rule extension enables explicit forgetting mechanism crucial for reset Dyck-1
- Self-referential weight matrices enhance computational power but introduce training complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear Transformers (LTs) can be equivalently expressed as RNN-like sequence processors with fixed-size state
- Mechanism: The key insight is that LTs maintain a state matrix W_t that gets updated additively at each time step through outer products of value and key vectors. This state update is element-wise and bounded, making LTs function similarly to RNNs with constant-size memory.
- Core assumption: The equivalence holds when the attention mechanism is linear (non-normalized) and the state updates are purely additive
- Evidence anchors:
  - [abstract] "LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state"
  - [section 2] "An FWP... transforms an input x_t ∈ R^d_in to an output y_t ∈ R^d_out as follows: q_t, k_t, v_t = W_slow x_t (1) W_t = W_{t-1} + v_t ⊗ ϕ(k_t) (2) y_t = W_t ϕ(q_t) (3)"
  - [corpus] Weak evidence - corpus neighbors focus on different aspects of LTs rather than this specific equivalence claim
- Break condition: The equivalence breaks if the attention mechanism includes complex non-linearities beyond the simple activation function ϕ, or if the state update involves multiplicative interactions between previous states

### Mechanism 2
- Claim: Recurrent and self-referential extensions to LTs overcome their limitations in recognizing certain regular languages
- Mechanism: The extensions introduce mechanisms that allow the model to track and manipulate memory states in ways that standard LTs cannot. Recurrent connections add proper recurrence by feeding back previous outputs, while self-referential weight matrices allow the model to modify its own parameters based on current context.
- Core assumption: The limitations of standard LTs stem from their inability to implement certain counting and memory manipulation operations that are essential for recognizing specific formal languages
- Evidence anchors:
  - [abstract] "Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT"
  - [section 4.2] "In contrast, both the recurrent (Recurrent Delta) and self-referential (SRWM) extensions successfully solve and generalise on these tasks"
  - [corpus] Weak evidence - corpus neighbors don't discuss formal language recognition capabilities
- Break condition: The extensions would fail if the additional computational mechanisms they introduce don't provide sufficient expressive power to implement the required language recognition operations, or if training instabilities prevent proper learning of these mechanisms

### Mechanism 3
- Claim: The delta-rule extension introduces an explicit forget mechanism to LTs
- Mechanism: By replacing purely additive updates with delta-rule updates that include a dynamic learning rate and error correction term, the model gains the ability to both add and subtract from its state matrix. This allows it to "forget" previous information when necessary, which is crucial for tasks like reset Dyck-1.
- Core assumption: The ability to subtract from the state matrix (not just add) is essential for implementing forgetting operations in formal language recognition
- Evidence anchors:
  - [section 2] "Wt = W_{t-1} + σ(β_t)(v_t - v̄_t) ⊗ ϕ(k_t) (8)" where the term (v_t - v̄_t) can be negative
  - [section 4.2] "We also confirm that all LT variants can learn representative counter languages that the original Transformer can learn" and "Table 2: Accuracies of single-layer models" showing DeltaNet succeeding on reset Dyck-1
  - [corpus] No direct evidence in corpus neighbors about delta-rule's forgetting mechanism
- Break condition: The mechanism would fail if the dynamic learning rate σ(β_t) cannot effectively control when to add versus subtract, or if the subtraction operation introduces numerical instabilities that prevent proper learning

## Foundational Learning

- Concept: Formal language theory and the Chomsky hierarchy
  - Why needed here: The paper's evaluation framework relies on understanding which computational models can recognize which classes of formal languages (regular, context-free, context-sensitive)
  - Quick check question: Can you explain why a finite automaton cannot recognize the language {a^n b^n | n ≥ 0} but a pushdown automaton can?

- Concept: Attention mechanisms and their linear variants
  - Why needed here: The paper compares standard attention with linear attention variants and their computational properties
  - Quick check question: What is the computational complexity difference between standard softmax attention and linear attention, and why does this matter for practical applications?

- Concept: RNN architectures and their computational power
  - Why needed here: The paper positions LTs within the broader context of RNN computational hierarchies and their practical limitations
  - Quick check question: What is the key difference between LSTM and GRU that gives LSTM additional computational power according to the paper's references?

## Architecture Onboarding

- Component map: Input → Key/Query/Value generation → State update → Output computation → (for extensions) additional feedback or self-modification steps

- Critical path: Input → Key/Query/Value generation → State update → Output computation → (for extensions) additional feedback or self-modification steps

- Design tradeoffs:
  - Memory vs. computation: LTs use fixed-size state but may need more parameters than standard attention
  - Expressiveness vs. stability: Extensions add power but may introduce training instabilities
  - Generalization vs. overfitting: More complex models may memorize training data rather than learn general patterns

- Failure signatures:
  - Training instability: NaN values or exploding gradients, especially in self-referential models
  - Poor generalization: High accuracy on training data but poor performance on longer sequences
  - Mode collapse: Model converges to trivial solutions that don't solve the task

- First 3 experiments:
  1. Implement and test a basic LT on the parity task to verify the core equivalence with RNNs
  2. Add recurrent connections to the LT and test on the same parity task to verify the improvement
  3. Implement the delta-rule extension and test on reset Dyck-1 to verify the forgetting mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational power of linear Transformers be formally compared to standard Transformers?
- Basis in paper: [explicit] The authors note that while their study provides insights into the capabilities and limitations of LTs, they cannot definitively compare their expressivity to standard Transformers based solely on the presented results.
- Why unresolved: The study focuses on specific formal language recognition tasks and does not provide a comprehensive analysis of the computational power of LTs compared to standard Transformers.
- What evidence would resolve it: A rigorous mathematical proof comparing the computational power of LTs and standard Transformers would resolve this question.

### Open Question 2
- Question: What is the exact mechanism by which self-referential weight matrices enhance the computational power of linear Transformers?
- Basis in paper: [explicit] The authors demonstrate that self-referential weight matrices (SRWMs) successfully overcome certain limitations of linear Transformers, but they note that future work should provide more insights into the power of self-reference.
- Why unresolved: The paper shows empirical results but does not provide a theoretical explanation for why SRWMs enhance the computational power of LTs.
- What evidence would resolve it: A theoretical analysis explaining the mechanism by which SRWMs enhance the computational power of LTs would resolve this question.

### Open Question 3
- Question: How do self-modifying automata (SMAs) and self-modifying finite automata (SMFAs) relate to self-referential weight matrices (SRWMs)?
- Basis in paper: [explicit] The authors discuss the potential connection between SMAs/SMFAs and SRWMs, suggesting that it may be interesting to extract SMFAs from SRWMs trained on certain meta-linear/counter languages.
- Why unresolved: The paper only briefly mentions the potential connection between SMAs/SMFAs and SRWMs without providing a detailed analysis or experimental results.
- What evidence would resolve it: Experimental results demonstrating the extraction of SMFAs from SRWMs trained on meta-linear/counter languages would resolve this question.

## Limitations
- The evaluation focuses on relatively short sequences (up to 100 tokens), limiting insights into practical computational power
- The theoretical analysis assumes specific conditions that may not hold in practical implementations with numerical precision constraints
- Self-referential extensions introduce significant training complexity that could limit scalability to larger models or datasets

## Confidence
**High Confidence**: The core claim that LTs are equivalent to RNN-like processors with fixed-size state - supported by direct mathematical formulation and multiple equation references.

**Medium Confidence**: The claim that recurrent and self-referential extensions overcome LT limitations - while empirical results are provided, the theoretical justification for why these specific extensions work could be more rigorous.

**Low Confidence**: The paper's broader claims about the practical computational power of these models for real-world sequence processing tasks, as the evaluation is restricted to synthetic formal language recognition problems.

## Next Checks
1. **Scaling Experiment**: Test the recurrent and self-referential extensions on longer sequences (1000+ tokens) to verify that the improvements observed on short sequences scale to practical problem sizes.

2. **Robustness Analysis**: Evaluate model performance across different random seeds and initialization schemes, particularly for the self-referential extension, to quantify training stability and variance in results.

3. **Ablation Study**: Systematically remove components of the recurrent and self-referential extensions (e.g., test recurrent LT without the delta-rule) to isolate which mechanisms are essential for solving each formal language task.