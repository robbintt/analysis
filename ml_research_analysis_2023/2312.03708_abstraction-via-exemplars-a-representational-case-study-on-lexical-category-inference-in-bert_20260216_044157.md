---
ver: rpa2
title: Abstraction via exemplars? A representational case study on lexical category
  inference in BERT
arxiv_id: '2312.03708'
source_url: https://arxiv.org/abs/2312.03708
tags:
- category
- novel
- exemplars
- bert
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether lexical category abstractions in
  language models can emerge from the encoding of exemplars rather than explicit abstract
  rules. It adapts experiments where BERT learns novel tokens' grammatical categories
  from a single context and then tests generalization.
---

# Abstraction via exemplars? A representational case study on lexical category inference in BERT

## Quick Facts
- arXiv ID: 2312.03708
- Source URL: https://arxiv.org/abs/2312.03708
- Reference count: 7
- Novel tokens trained on single context achieve 0.70-0.93 classification accuracy across category pairs

## Executive Summary
This paper investigates whether lexical category abstractions in language models can emerge from encoding exemplars rather than explicit abstract rules. The study adapts experiments where BERT learns novel tokens' grammatical categories from a single context and tests generalization. By analyzing BERT's internal representations during training, the research finds that novel token embeddings move toward regions occupied by known category exemplars in 2D PCA space. Further experiments demonstrate that initializing novel tokens with embeddings sampled from exemplar regions yields high classification accuracy without additional training, providing empirical support that abstraction-like generalization can arise from exemplar encoding.

## Method Summary
The paper adapts Kim and Smolensky's experiments to test BERT's ability to learn lexical categories from minimal exposure. BERT is trained on fill-in-the-blank stimuli containing novel tokens (e.g., "wug", "dax") with disambiguating context pairs that establish grammatical categories. The model's novel token embeddings are updated while all other parameters remain frozen. The study tracks embedding movement in 2D PCA space during training and measures classification accuracy on a test set. Additional experiments sample embeddings from exemplar regions in PCA space to initialize novel tokens and evaluate performance without training.

## Key Results
- Novel token embeddings move toward regions of known category exemplars in 2D PCA space during training
- Initializing novel tokens with embeddings sampled from exemplar regions achieves 0.70-0.93 classification accuracy without training
- Single context exposure enables accurate lexical category inference in BERT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical category generalization emerges from representational alignment between novel tokens and known exemplar regions in embedding space.
- Mechanism: During training, novel token embeddings move in PCA space toward centroids of known category exemplars, and this geometric proximity enables accurate predictions without explicit rule encoding.
- Core assumption: Category generalization can be approximated by nearest-neighbor similarity in learned embedding space rather than symbolic abstraction.
- Evidence anchors:
  - [abstract] "BERT's capacity to generalize to unseen expressions involving the use of these novel tokens constitutes the movement of novel token representations towards regions of known category exemplars in two-dimensional space."
  - [section] "Figure 2 suggests that the final states of the embeddings of the novel tokens move closer in two-dimensional space to that of known, unambiguous category exemplars (N=500 per category)."
- Break condition: If novel token embeddings fail to converge toward exemplar regions or if exemplar regions are not sufficiently discriminative, generalization accuracy would drop below chance levels.

### Mechanism 2
- Claim: Random initialization of novel tokens followed by category exposure leads to category-appropriate embedding updates via contextual prediction objectives.
- Mechanism: BERT's fill-in-the-blank training objective causes gradients to update novel token embeddings such that they become contextually appropriate for their observed category usage.
- Core assumption: The word prediction loss gradient propagates sufficient signal to align novel embeddings with their true grammatical category despite minimal exposure.
- Evidence anchors:
  - [abstract] "We adapt the experiments of Kim and Smolensky [6]... trained the model's embeddings of the novel tokens only (initialized with random-values), keeping all other parameters constant."
  - [section] "BERT's capacity to generalize to unseen expressions... from exposure to a single observation"
- Break condition: If the contextual prediction objective cannot distinguish category-appropriate contexts with only one training example, novel embeddings would remain randomly distributed and fail to generalize.

### Mechanism 3
- Claim: Sampling novel token embeddings from exemplar region distributions bypasses training while preserving generalization ability.
- Mechanism: By initializing novel tokens with values randomly sampled from PCA-projected exemplar regions, the model achieves high accuracy without gradient updates.
- Core assumption: Category-appropriate behavior can be achieved purely through geometric placement in embedding space without learning.
- Evidence anchors:
  - [abstract] "Further experiments show that initializing novel tokens with embeddings sampled from exemplar regions yields high classification accuracy (0.70-0.93 across category pairs) without additional training."
  - [section] "We take multiple novel tokens (N=20 per category) and assign them values randomly sampled from regions in two-dimensional space populated by category exemplars."
- Break condition: If exemplar regions overlap significantly or if the 2D PCA projection loses critical discriminative information, sampled embeddings would produce chance-level performance.

## Foundational Learning

- Concept: Principal Component Analysis (PCA) dimensionality reduction
  - Why needed here: The study uses 2D PCA to visualize and analyze movement of embeddings toward exemplar regions
  - Quick check question: What does each principal component represent in the context of token embeddings?

- Concept: Fill-in-the-blank (FITB) language modeling objective
  - Why needed here: BERT is trained to predict missing words, which provides the training signal for novel token embedding updates
  - Quick check question: How does the FITB objective differ from standard next-word prediction in terms of context availability?

- Concept: Lexical category disambiguation through context
  - Why needed here: The experimental design relies on pairs of contexts that disambiguate between different grammatical categories
  - Quick check question: Why is a single context sufficient to determine the grammatical category of a novel token?

## Architecture Onboarding

- Component map: BERT-base model with frozen parameters except for novel token embeddings; PCA visualization layer for analysis; sampling module for exemplar region initialization
- Critical path: Initialize novel token → Train on single context pair → Update embedding via gradient descent → Measure accuracy on test set → Visualize embedding movement via PCA
- Design tradeoffs: Using only embedding updates (not full fine-tuning) preserves pre-trained knowledge but may limit learning capacity; 2D PCA visualization may lose information compared to higher dimensions
- Failure signatures: Novel tokens not moving toward exemplar regions in PCA space; accuracy dropping below 0.5 on category disambiguation; sampled embeddings producing chance-level performance
- First 3 experiments:
  1. Run PCA on known category exemplars and visualize their distribution in 2D space
  2. Train novel tokens on single context pairs and track embedding movement toward exemplar regions
  3. Sample embeddings from exemplar regions and test classification accuracy without training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does BERT's success on lexical category inference tasks transfer to other linguistic abstractions beyond part-of-speech categories?
- Basis in paper: [inferred] The paper demonstrates BERT can generalize novel token categories through exemplar encoding, but only tests POS categories. The authors suggest this supports broader abstraction-via-exemplars claims.
- Why unresolved: The study focuses specifically on noun/verb/adjective/adverb categories. While the methodology could be extended to test other linguistic abstractions (syntactic structures, semantic relations, etc.), this wasn't done.
- What evidence would resolve it: Testing BERT's ability to learn and generalize other linguistic abstractions (e.g., syntactic movement rules, semantic role mappings) using the same exemplar-encoding approach would show whether this phenomenon extends beyond POS categories.

### Open Question 2
- Question: How does the dimensionality of the exemplar space affect abstraction-like generalization in language models?
- Basis in paper: [explicit] The paper analyzes exemplar regions in 2D PCA space and finds that sampling from these regions yields good generalization. However, it doesn't explore whether this holds across different dimensionalities.
- Why unresolved: The analysis only examines 2D PCA projections. While this captures some structure, it's unclear if the observed effects are specific to 2D or would hold in higher-dimensional spaces that might better represent the true structure of linguistic categories.
- What evidence would resolve it: Systematically varying the dimensionality of the exemplar space (1D, 3D, 10D, etc.) and measuring how it affects generalization accuracy would reveal whether the 2D finding is robust or contingent on dimensionality.

### Open Question 3
- Question: Do other neural language models exhibit similar exemplar-based abstraction patterns as BERT?
- Basis in paper: [inferred] The paper focuses exclusively on BERT, a specific transformer-based model. While it mentions neural LMs generally, it doesn't test whether other architectures show the same exemplar-encoding behavior.
- Why unresolved: The study uses BERT specifically, leaving open whether this is a property of this particular model or a more general feature of neural language models trained on similar objectives.
- What evidence would resolve it: Replicating the experiments with other neural LMs (GPT, RoBERTa, T5, etc.) using the same methodology would reveal whether exemplar-based abstraction is specific to BERT or a broader phenomenon in neural language modeling.

## Limitations
- The core evidence relies on 2D PCA projections which may distort high-dimensional relationships
- Single context exposure may not capture the full complexity of lexical category acquisition
- The sampling procedure from exemplar regions requires unclear mapping between 2D PCA and full embedding space
- No established benchmarks exist for validating the exemplar-driven generalization mechanism

## Confidence
- High Confidence: The empirical finding that novel token embeddings can be successfully initialized from exemplar regions to achieve high classification accuracy (0.70-0.93) without training
- Medium Confidence: The interpretation that embedding movement toward exemplar regions in PCA space demonstrates exemplar-based rather than rule-based abstraction
- Low Confidence: The generalizability of the exemplar-based mechanism to natural language processing tasks beyond the controlled experimental setting

## Next Checks
1. Replicate the exemplar sampling experiment using the full embedding space or higher-dimensional projections (e.g., 10-50 dimensions) rather than 2D PCA to verify that the observed generalization is not an artifact of dimensionality reduction.

2. Systematically vary the number of training contexts (1, 2, 5, 10) to determine whether the exemplar-based mechanism scales with additional exposure or whether single-context generalization is a special case.

3. Compare exemplar-region sampling against other initialization strategies (random initialization, category centroid sampling, nearest-neighbor exemplar assignment) to isolate the specific contribution of the exemplar-region approach.