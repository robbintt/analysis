---
ver: rpa2
title: $Q_{bias}$ -- A Dataset on Media Bias in Search Queries and Query Suggestions
arxiv_id: '2311.17780'
source_url: https://arxiv.org/abs/2311.17780
tags:
- search
- bias
- news
- queries
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces \U0001D444\U0001D44F\U0001D456\U0001D44E\
  \U0001D460, a dataset of search queries and query suggestions from Google and Bing\
  \ for U.S. political news topics."
---

# $Q_{bias}$ -- A Dataset on Media Bias in Search Queries and Query Suggestions

## Quick Facts
- arXiv ID: 2311.17780
- Source URL: https://arxiv.org/abs/2311.17780
- Reference count: 40
- Introduces ùëÑùëèùëñùëéùë† dataset of search queries and query suggestions for investigating bias in search engine interactions

## Executive Summary
This paper introduces ùëÑùëèùëñùëéùë†, a dataset of search queries and query suggestions from Google and Bing for U.S. political news topics. The dataset enables investigation of bias in search engine interactions. The authors also provide a tool to scrape biased news articles from AllSides and fine-tuned DistilBERT models to generate biased text. Their evaluation shows models trained on headlines produce the most biased results with the lowest nonsensical predictions. The authors argue their work raises awareness of how easily AI systems can reproduce bias.

## Method Summary
The authors fine-tune DistilBERT on the AllSides Biased News Dataset using three configurations: headlines only, text only, and both headlines and text. They compare padding and concatenation for chunk size consistency and evaluate the effect of intentional overfitting by increasing training epochs to 20. Models are evaluated using Rank Biased Overlap (RBO) and manual assessment for nonsensical and biased predictions.

## Key Results
- Headline-only fine-tuning produces the most biased results with lowest nonsensical predictions
- Concatenation of text produces better results than padding for most configurations
- Intentional overfitting increases nonsensical predictions without improving bias generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning DistilBERT with headline-only data produces the most biased and lowest nonsensical predictions due to the condensed, attention-grabbing nature of headlines.
- Mechanism: Headlines contain high density of opinionated phrasing and quotes, providing stronger lexical bias signals than full articles. DistilBERT learns these patterns more effectively, generating politically charged continuations when prompted with ambiguous political queries.
- Core assumption: Headlines reflect the core bias of the article without diluting it with neutral or contradictory content.
- Evidence anchors:
  - [section] "models fine-tuned with headlines show the best (lowest) RBO scores, as well as the lowest ùëÉùëõùëúùëõùë† scores... We assume that the high amount of quotes in news texts...might have induced noise in fine-tuning the models."
  - [corpus] Weak evidence: No direct corpus comparison of headline vs full text bias density provided.
- Break condition: If headlines are rewritten to remove bias (e.g., editorial standards enforce neutrality), the mechanism fails.

### Mechanism 2
- Claim: Using padded vs concatenated input affects the model's ability to learn coherent bias patterns across documents.
- Mechanism: Concatenation preserves sentence boundaries and topic coherence, allowing the model to learn longer-range bias dependencies. Padding truncates documents arbitrarily, disrupting bias signal continuity.
- Core assumption: Bias is a distributed signal that spans multiple sentences or clauses.
- Evidence anchors:
  - [section] "Concatenating texts produces on average better results than padding, although for fine-tuning with only headlines, the effect is minimal."
  - [corpus] Weak evidence: No corpus-level analysis of bias continuity in headlines vs full articles.
- Break condition: If bias is primarily localized to individual sentences, padding would not degrade performance.

### Mechanism 3
- Claim: Overfitting via extended epochs increases nonsensical predictions, harming practical usability.
- Mechanism: Excessive training causes the model to memorize idiosyncratic phrases and hallucinate unlikely word combinations, especially when prompted with out-of-domain queries.
- Core assumption: The fine-tuning corpus is small enough that overfitting is a real risk.
- Evidence anchors:
  - [section] "Intentional overfitting by raising the amount of training epochs seems to worsen the RBO scores...The percentage of nonsensical predictions does not differ much between different fine-tuning setups."
  - [corpus] No corpus evidence on overfitting effects in similar bias fine-tuning tasks.
- Break condition: If the corpus were much larger, overfitting would be less likely and the negative effect would diminish.

## Foundational Learning

- Concept: Political bias detection via lexical cues
  - Why needed here: The dataset and models rely on identifying partisan language to label news and generate biased text.
  - Quick check question: What linguistic features distinguish left- vs right-leaning political texts?

- Concept: Search query suggestion dynamics
  - Why needed here: The dataset models real user search behavior through autocomplete suggestions, requiring understanding of how engines predict user intent.
  - Quick check question: How do query suggestion algorithms balance popularity vs novelty in predictions?

- Concept: Transformer fine-tuning mechanics
  - Why needed here: The bias-induction approach depends on customizing pre-trained language models for a specific domain and task.
  - Quick check question: What is the difference between token-level and sequence-level fine-tuning objectives in transformer models?

## Architecture Onboarding

- Component map: AllSides Scraper -> News Article Corpus -> DistilBERT Fine-tuning -> Biased Language Models -> Query Masking & Generation -> RBO Evaluation
- Critical path:
  1. Scrape AllSides headlines and articles.
  2. Preprocess and split into left/right biased datasets.
  3. Fine-tune DistilBERT on headline-only, text-only, or combined data.
  4. Evaluate models using RBO on masked queries and manual nonsense/bias labeling.
- Design tradeoffs:
  - Headline-only: higher bias density, lower context, faster training.
  - Full text: more context, potential noise from quotes, slower training.
  - Padding: uniform chunk size, possible truncation of bias signals.
  - Concatenation: preserves context, risk of exceeding max sequence length.
- Failure signatures:
  - High nonsense rate: overfitting or inappropriate corpus.
  - Low RBO between left/right models: insufficient bias differentiation.
  - Low bias label rate: corpus not representative of target bias types.
- First 3 experiments:
  1. Compare RBO scores for headline-only vs full-text fine-tuning on a small subset.
  2. Test padding vs concatenation on fixed chunk sizes (128 vs 256 tokens).
  3. Vary epochs (6 vs 20) on headline-only data and measure nonsense rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fine-tuning approaches for language models compare in terms of their ability to generate biased text across various domains beyond politics?
- Basis in paper: [explicit] The paper evaluates different fine-tuning settings for generating biased text using political news headlines and articles, but does not explore other domains.
- Why unresolved: The evaluation focuses solely on the political news domain, and the authors acknowledge that future research should investigate the findings with other language models and compare performance.
- What evidence would resolve it: Empirical studies applying the same fine-tuning approaches to datasets from diverse domains (e.g., healthcare, finance, entertainment) and comparing the resulting bias generation capabilities.

### Open Question 2
- Question: What is the relationship between the type and intensity of bias in search queries and the resulting bias in search engine results?
- Basis in paper: [explicit] The authors state that the true effect of biased search queries on search results has not been sufficiently investigated and plan to use their datasets to explore this relationship.
- Why unresolved: Previous studies have not had access to large datasets of real search queries and query suggestions, making it difficult to establish a clear correlation.
- What evidence would resolve it: Analysis of search results obtained using biased and unbiased queries from the ùëÑùëèùëñùëéùë† dataset, examining the presence and degree of bias in the results.

### Open Question 3
- Question: How do user behaviors and properties influence the likelihood of encountering bias in online information search?
- Basis in paper: [explicit] The authors plan to conduct a study using a simulation approach to investigate how different user behaviors and properties affect exposure to bias in search results.
- Why unresolved: The impact of user-specific factors on bias exposure is not well understood, and existing research has not employed simulation methods to isolate these effects.
- What evidence would resolve it: Results from the planned simulation study, identifying which user behaviors (e.g., query formulation, interaction with suggestions) and properties (e.g., prior knowledge, search goals) increase or decrease the risk of encountering bias.

## Limitations

- The paper lacks direct empirical validation for hypothesized mechanisms, particularly the claim that headline-only fine-tuning produces optimal results
- No corpus-level analysis of bias density or continuity to support the padding vs concatenation findings
- Limited exploration of bias generation beyond the political domain

## Confidence

- High Confidence: Dataset construction methodology and general DistilBERT fine-tuning approach are well-specified and reproducible
- Medium Confidence: Headline-only fine-tuning producing optimal results is supported by metrics but lacks direct corpus-level evidence
- Low Confidence: Mechanism by which padding vs concatenation affects bias learning is weakly supported with minimal observed differences

## Next Checks

1. **Corpus Bias Density Analysis**: Quantitatively compare bias density (partisan language frequency, sentiment polarization) between headlines and full articles in the AllSides corpus to validate the assumption that headlines contain stronger bias signals.

2. **Bias Continuity Testing**: Analyze how bias manifests across sentence boundaries in full articles versus headlines, measuring whether bias is distributed or localized to support the padding vs concatenation findings.

3. **Overfitting Validation**: Conduct ablation studies with varying corpus sizes and fine-tuning durations to establish whether the observed nonsensical predictions are truly due to overfitting or other factors like corpus quality or model architecture limitations.