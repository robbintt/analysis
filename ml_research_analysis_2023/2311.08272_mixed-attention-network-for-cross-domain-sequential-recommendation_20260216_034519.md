---
ver: rpa2
title: Mixed Attention Network for Cross-domain Sequential Recommendation
arxiv_id: '2311.08272'
source_url: https://arxiv.org/abs/2311.08272
tags:
- item
- attention
- sequential
- cross-domain
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of cross-domain sequential recommendation
  without assuming overlapped users across domains, which is a practical challenge
  in real-world recommender systems. The authors propose a Mixed Attention Network
  (MAN) that captures domain-specific and cross-domain information using local and
  global attention modules.
---

# Mixed Attention Network for Cross-domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2311.08272
- Source URL: https://arxiv.org/abs/2311.08272
- Reference count: 40
- Primary result: Proposed MAN model achieves up to 8.25% improvement in AUC on Amazon dataset for cross-domain sequential recommendation without requiring overlapped users.

## Executive Summary
This paper addresses cross-domain sequential recommendation without assuming overlapped users, a practical challenge in real-world recommender systems. The authors propose a Mixed Attention Network (MAN) that captures domain-specific and cross-domain information using three attention modules: item similarity attention, sequence-fusion attention, and group-prototype attention. MAN includes separate local and global encoding layers to process domain-specific and shared item embeddings, enabling transfer of sequential patterns across domains. Experiments on Micro Video and Amazon datasets demonstrate significant improvements over state-of-the-art models, achieving up to 8.25% AUC improvement on Amazon while being model-agnostic and effective even with limited user or item overlap.

## Method Summary
MAN addresses cross-domain sequential recommendation without overlapped users by employing a mixed attention architecture with dual embedding spaces. The method creates both local (domain-specific) and global (cross-domain) item embeddings, processed through separate local and global encoders. Three attention modules then operate at different levels: item similarity attention weights historical items based on similarity to target items, sequence-fusion attention combines local and global sequence representations, and group-prototype attention transfers preferences at the group level using multiple prototypes with disentanglement regularization. The model is trained end-to-end on sequential data from two domains, with evaluation using standard recommendation metrics (AUC, GAUC, MRR, NDCG@10).

## Key Results
- MAN achieves up to 8.25% improvement in AUC on Amazon dataset compared to state-of-the-art models
- Outperforms baseline methods on Micro Video dataset with 7.18% to 8.37% user overlap
- Demonstrates effectiveness even with limited overlapped users (0.27% to 0.04% on Amazon dataset)
- Shows model-agnostic property when tested with different backbone models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAN transfers user preferences across domains without overlapped users using group-prototype attention at the group level
- Mechanism: Group-prototype attention identifies shared user groups across domains by pooling sequence representations into group relevance scores, aggregating with group prototypes, and applying disentanglement regularization for distinct prototypes
- Core assumption: Users can be meaningfully grouped into prototypes sharing preferences across domains, discoverable through unsupervised pooling
- Evidence anchors:
  - [abstract]: "we propose a group-prototype attention module, which utilizes multiple group prototypes to transfer the information at the group level."
  - [section]: "we propose a group-prototype attention module, which utilizes multiple group prototypes to transfer the information at the group level."
  - [corpus]: Weak evidence - only one corpus paper ("FedHCDR") mentions group-level mechanisms without prototype-based transfer without user overlap
- Break condition: If user groups aren't shared across domains (e.g., domains target different demographics), group-prototype attention fails to capture meaningful cross-domain patterns

### Mechanism 2
- Claim: MAN captures domain-specific and cross-domain sequential patterns using separate local and global encoding layers with different item embeddings
- Mechanism: Local embeddings capture domain-specific item characteristics while global embeddings capture shared characteristics; separate encoders process sequences with local/global embeddings to extract domain-specific and cross-domain sequential patterns, fused via sequence-fusion attention
- Core assumption: Items can be represented in both domain-specific and shared latent spaces, with sequential patterns disentangled into local and global components
- Evidence anchors:
  - [abstract]: "Firstly, we propose a local/global encoding layer to capture the domain-specific/cross-domain sequential pattern."
  - [section]: "We build both domain-specific local and cross-domain global embeddings for items. We further look up item embeddings and encode them with local and global encoders at the sequence level."
  - [corpus]: Weak evidence - most related works focus on single embedding spaces rather than dual local/global representations
- Break condition: If items have fundamentally different characteristics across domains with no shared space, global embeddings become meaningless and hurt performance

### Mechanism 3
- Claim: Item similarity attention allows MAN to weight similar historical items more heavily when predicting next item, improving recommendation accuracy
- Mechanism: MAN computes similarity scores between historical items (in both local and global spaces) and target item, then uses softmax weighting to refine item embeddings based on similarity
- Core assumption: Similar items in both domain-specific and shared spaces are informative for predicting next item, with similarity effectively computed via learned attention
- Evidence anchors:
  - [abstract]: "item similarity attention to capture the local/global item similarity"
  - [section]: "To capture the similarity between local/global item embeddings and target item embedding, we first fuse the item embedding from local space and global space together."
  - [corpus]: No direct evidence - this is a standard attention mechanism applied in a novel context
- Break condition: If item similarity isn't predictive of user preferences (e.g., in random or novelty-driven domains), this attention mechanism adds noise rather than signal

## Foundational Learning

- Concept: Cross-domain sequential recommendation
  - Why needed here: The paper addresses transferring sequential recommendation knowledge across domains without assuming overlapped users, a more realistic scenario than previous approaches
  - Quick check question: What are the key challenges in cross-domain sequential recommendation when users don't overlap across domains?

- Concept: Attention mechanisms in sequential modeling
  - Why needed here: MAN uses three types of attention (item similarity, sequence-fusion, and group-prototype) to capture different levels of relationships in data, critical for its performance
  - Quick check question: How does attention differ from traditional RNN-based sequential modeling in handling long-term dependencies?

- Concept: Representation learning with dual embedding spaces
  - Why needed here: MAN creates both local (domain-specific) and global (cross-domain) item embeddings, requiring understanding of how to learn and utilize multiple embedding spaces simultaneously
  - Quick check question: What are the benefits and challenges of maintaining separate local and global embedding spaces for the same items?

## Architecture Onboarding

- Component map: Input -> Local/Global Item Embeddings -> Local/Global Encoders -> Mixed Attention Layer (Item Similarity + Sequence-Fusion + Group-Prototype) -> Local/Global Prediction Layers -> Output
- Critical path: Item embeddings -> encoders -> attention fusion -> prediction
- Design tradeoffs: Local vs. global representations (complexity vs. expressiveness), number of group prototypes (capacity vs. overfitting), attention mechanisms (computational cost vs. performance)
- Failure signatures: Poor AUC/GAUC scores indicate attention mechanisms aren't capturing useful patterns; high variance across domains suggests domain-specific vs. cross-domain balance is off; slow convergence may indicate overly complex architecture
- First 3 experiments:
  1. Test local-only vs. global-only vs. combined MAN on small dataset to verify dual representation helps
  2. Vary number of group prototypes (1, 5, 10, 20) to find optimal group granularity
  3. Compare different backbone models (SASRec, SURGE, GRU4REC) to verify model-agnostic property

## Open Questions the Paper Calls Out

- Question: How does the performance of MAN vary when applied to datasets with different degrees of user overlap?
  - Basis in paper: [inferred] The paper states MAN is designed to work without assuming overlapped users, but performance is evaluated on datasets with varying degrees of user overlap (7.18% to 8.37% for Micro Video and 0.27% to 0.04% for Amazon)
  - Why unresolved: The paper doesn't provide systematic analysis of MAN's performance across datasets with different levels of user overlap
  - What evidence would resolve it: Empirical results comparing MAN's performance on datasets with varying degrees of user overlap

- Question: What is the impact of the number of group prototypes on MAN's performance, and is there an optimal number?
  - Basis in paper: [explicit] The paper mentions the number of group prototypes is a hyperparameter and provides a study in Appendix A.2, but doesn't explore the impact in detail
  - Why unresolved: The paper doesn't provide comprehensive analysis of how the number of group prototypes affects MAN's performance
  - What evidence would resolve it: Detailed analysis of MAN's performance with different numbers of group prototypes

- Question: How does MAN's performance compare to other state-of-the-art cross-domain sequential recommendation models that do not rely on user overlap?
  - Basis in paper: [inferred] The paper compares MAN to several cross-domain sequential recommendation models, but doesn't explicitly compare it to models that don't rely on user overlap
  - Why unresolved: The paper doesn't provide direct comparison between MAN and other models addressing the same problem
  - What evidence would resolve it: Empirical results comparing MAN's performance to other models that don't rely on user overlap

## Limitations

- Dataset accessibility: The Micro Video dataset used in experiments is not publicly available, limiting reproducibility and independent verification
- Group prototype assumption: Effectiveness hinges on assumption that user groups meaningfully overlap across domains, which may fail if domains target fundamentally different user segments
- Limited domain scalability: All experiments involve only two domains, with no validation of MAN's performance in multi-domain scenarios (3+ domains)

## Confidence

- High Confidence: The paper's core architecture and its components (local/global embeddings, item similarity attention, sequence-fusion attention) are well-defined and technically sound. The model-agnostic claim is reasonable given the modular design.
- Medium Confidence: Experimental results show strong performance on two datasets, but limited scope (two domains, one public dataset) reduces confidence in broader applicability.
- Low Confidence: The group-prototype attention's effectiveness in scenarios with no user overlap is assumed but not rigorously tested with negative cases (e.g., completely disjoint user groups).

## Next Checks

1. **Domain Disjoint Test**: Evaluate MAN on domains with provably disjoint user groups (e.g., gaming vs. cooking) to test the limits of group-prototype attention.

2. **Multi-Domain Scalability**: Test MAN on a three-or four-domain setup to verify whether performance scales or degrades with additional domains.

3. **Ablation of Global Embeddings**: Train a variant of MAN with only local embeddings (no global space) to quantify the exact contribution of cross-domain knowledge transfer.