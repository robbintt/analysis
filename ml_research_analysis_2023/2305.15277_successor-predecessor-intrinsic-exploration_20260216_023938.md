---
ver: rpa2
title: Successor-Predecessor Intrinsic Exploration
arxiv_id: '2305.15277'
source_url: https://arxiv.org/abs/2305.15277
tags:
- exploration
- intrinsic
- state
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Successor-Predecessor Intrinsic Exploration
  (SPIE), a method that augments standard intrinsic rewards with retrospective information
  to guide exploration in sparse-reward reinforcement learning. SPIE combines prospective
  and retrospective state occupancy information via the successor and predecessor
  representations.
---

# Successor-Predecessor Intrinsic Exploration

## Quick Facts
- arXiv ID: 2305.15277
- Source URL: https://arxiv.org/abs/2305.15277
- Authors: [Not specified in source]
- Reference count: 22
- Key outcome: SPIE combines successor and predecessor representations to guide exploration toward bottleneck states, achieving faster state coverage and better sample efficiency than count-based methods in both discrete and continuous environments.

## Executive Summary
Successor-Predecessor Intrinsic Exploration (SPIE) is a novel intrinsic reward method that augments standard exploration with retrospective information from predecessor representations. The method encourages exploration of bottleneck states—critical transition points between different regions of the state space—by penalizing transitions to states that are frequently reached from many other states. SPIE demonstrates superior performance compared to existing intrinsic exploration methods on discrete grid worlds, continuous MountainCar, and sparse-reward Atari games. The approach maintains exploration motivation even when the transition model is known, addressing a key limitation of count-based methods.

## Method Summary
SPIE computes intrinsic rewards using the difference between successor representation (SR) and the sum of predecessor representation (PR) columns. In discrete MDPs, the reward is defined as rSR-R(s,a) = M[s,s'] - ||M[:,s']||1, where M is the SR matrix. This formulation combines prospective state occupancy information with retrospective accessibility information to identify and explore bottleneck states. The method extends to continuous state spaces using successor and predecessor features with reciprocal norms. SPIE can be implemented with tabular methods for discrete environments or neural networks with separate heads for deep RL applications.

## Key Results
- SPIE achieves faster state coverage than SARSA-SR, RND, and other intrinsic reward methods in discrete grid worlds
- In MountainCar, SPIE finds the optimal policy significantly faster than pure count-based exploration
- On sparse-reward Atari games including Montezuma's Revenge, DQN-SF-PF (SPIE's deep RL variant) achieves higher scores than RND and count-based baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPIE uses the difference between successor and predecessor representations to guide exploration toward bottleneck states.
- Mechanism: The intrinsic reward is defined as rSR-R(s,a) = M[s,s'] - ||M[:,s']||1, where the first term is the forward-looking state occupancy and the second term is the sum of backward-looking accessibilities from all other states. This penalizes transitions to states that are frequently reached from many other states, encouraging the agent to visit states that are harder to reach in general (e.g., bottlenecks).
- Core assumption: The environment has distinguishable bottleneck states that connect different clusters of the state space.
- Evidence anchors:
  - [abstract] "SPIE combines prospective and retrospective state occupancy information via the successor and predecessor representations."
  - [section] "rSR-R(s) can be interpreted as penalising transitions leading into states s′ that are frequently reached from many states other than s, hence providing an intrinsic motivation for guiding the agent towards states that are harder to reach in general, e.g, boundary states and bottleneck states."
  - [corpus] Weak - the corpus contains general exploration papers but no direct evidence for successor-predecessor combinations.
- Break condition: If the state space has no clear bottleneck structure or if transitions are stochastic such that backward accessibility doesn't correlate with bottleneck importance.

### Mechanism 2
- Claim: SPIE maintains exploration motivation even when the transition model is known, unlike pure count-based methods.
- Mechanism: The intrinsic reward includes a retrospective component ||M[:,s']||1 that depends on the sum of successor values from all states. This sum does not converge to a uniform value as the SR matrix converges, unlike the row norm of the SR alone. Therefore, even with a known transition model, SPIE continues to provide non-trivial intrinsic rewards that encourage exploration for learning rewards.
- Core assumption: The extrinsic reward function is initially unknown and needs exploration to be learned.
- Evidence anchors:
  - [section] "rSR-R contains the sum of columns of the SR matrix, the asymptotic uniformity in rSR no longer holds, yielding non-trivial intrinsic exploration even when the SR matrix is known and fixed a priori, allowing continual exploration for learning the reward structure despite sparse extrinsic reinforcement."
  - [abstract] "SPIE yields more efficient and ethologically plausible exploratory behaviour in environments with sparse rewards and bottleneck states than competing methods."
  - [corpus] Weak - no corpus evidence directly addresses this property.
- Break condition: If the reward function is known a priori or if the environment has dense rewards.

### Mechanism 3
- Claim: SPIE generalizes to continuous state spaces using successor and predecessor features.
- Mechanism: In continuous spaces, successor features (SF) replace the successor representation and predecessor features (PF) replace the predecessor representation. The intrinsic reward is defined as rSF-PF = 1/||ξ(s')||1 - 1/||ψ(s,a)||1, where ξ is the PF and ψ is the SF. This maintains the same logic as the discrete case while being tractable in continuous spaces.
- Core assumption: There exists a feature representation ϕ such that rewards can be expressed as linear combinations of features.
- Evidence anchors:
  - [section] "We propose the 'Predecessor Feature' (PF) that generalises PR... Contrary to how we define rSR-R as the difference between the SR and the column sum of the SR in discrete MDPs, we find that the resulting agent yields better empirical performance by setting the intrinsic reward as the difference between the reciprocal of the norms of the SF and the PF."
  - [abstract] "We also implement SPIE in deep reinforcement learning agents, and show that the resulting agent achieves stronger empirical performance than existing methods on sparse-reward Atari games."
  - [corpus] Weak - no corpus evidence directly addresses successor-predecessor feature generalization.
- Break condition: If the feature representation is inadequate to capture the relevant state information or if the reciprocal relationship between SR and PR doesn't hold in the continuous setting.

## Foundational Learning

- Concept: Successor Representation (SR)
  - Why needed here: The SR provides the prospective component of the intrinsic reward by encoding expected future state occupancy.
  - Quick check question: What does the i-th row of the SR matrix represent in terms of state visitation?

- Concept: Predecessor Representation (PR)
  - Why needed here: The PR provides the retrospective component by measuring how often a state is accessed from other states.
  - Quick check question: How does the PR differ from the SR in terms of the direction of information flow?

- Concept: Temporal Difference Learning
  - Why needed here: Both SR and PR can be learned online using TD learning, which is essential for the practical implementation of SPIE.
  - Quick check question: What is the update rule for learning the SR matrix online?

## Architecture Onboarding

- Component map: Q-value table + SR matrix + PR matrix (discrete) → Neural network with SF/PF heads + Q-value head (continuous)
- Critical path: 1) Learn SR/PR (or SF/PF) via TD learning 2) Compute intrinsic reward using the difference formula 3) Combine with extrinsic reward 4) Update Q-values using the augmented reward
- Design tradeoffs:
  - Discrete vs continuous: Tabular methods are exact but scale poorly; neural methods generalize but require careful architecture design
  - Online vs fixed SR: Online learning captures changing policies but adds complexity; fixed SR simplifies computation but may be less adaptive
  - Feature choice: Random Fourier features work for simple continuous tasks; learned representations needed for complex domains like Atari
- Failure signatures:
  - Poor exploration: Intrinsic reward too small relative to extrinsic reward (β too low)
  - Unstable training: Learning rates for SR/PR too high causing divergence
  - No improvement over baselines: Feature representation inadequate or environment lacks bottleneck structure
- First 3 experiments:
  1. Implement SPIE on a simple grid world with clear bottleneck states and compare state coverage against SARSA-SR
  2. Test SPIE with fixed SR matrix on RiverSwim to verify exploration continues despite known transitions
  3. Implement DQN-SF-PF on Montezuma's Revenge with sparse rewards to validate deep RL extension

## Open Questions the Paper Calls Out

- Question: What is the theoretical foundation for why the combination of successor and predecessor representations leads to more efficient exploration than either representation alone?
  - Basis in paper: [explicit] The paper states that "the retrospective information contains useful signals about the connectivity structure of the environment" and that SPIE "facilitates more efficient targeted exploration between sub-regions of state space given structure awareness." However, it does not provide a formal theoretical analysis of why this combination is superior.
  - Why unresolved: The paper focuses on empirical demonstrations rather than theoretical analysis. While the authors suggest that the predecessor representation captures "connectivity structure," they do not formally define or prove this property, nor do they show mathematically why combining it with the successor representation yields superior exploration.
  - What evidence would resolve it: A formal proof showing that the successor-predecessor intrinsic reward function satisfies certain theoretical properties (e.g., bounds on exploration efficiency, guarantees on reaching bottleneck states) would resolve this question.

- Question: How does SPIE perform in environments with continuous action spaces or partial observability?
  - Basis in paper: [inferred] The paper only evaluates SPIE in environments with discrete action spaces and full observability. While the authors mention that "SPIE is a general framework that can be implemented with other formulations," they do not provide any empirical results or theoretical analysis for these more complex settings.
  - Why unresolved: Extending SPIE to continuous action spaces or partially observable environments would require significant modifications to the algorithm, and the paper does not address these challenges or explore the potential performance of SPIE in such settings.
  - What evidence would resolve it: Empirical evaluations of SPIE in environments with continuous action spaces or partial observability would provide evidence for its effectiveness in these more challenging settings.

- Question: How sensitive is SPIE to the choice of hyperparameters, particularly the balance between the successor and predecessor components of the intrinsic reward?
  - Basis in paper: [inferred] The paper mentions that "the intrinsic reward operates by motivating the agent to move into under-explored parts of the state space," but it does not provide a detailed analysis of how the relative weighting of the successor and predecessor components affects the exploration behavior or performance of the agent.
  - Why unresolved: The choice of hyperparameters can significantly impact the performance of reinforcement learning algorithms, and the paper does not explore the sensitivity of SPIE to these choices. Understanding the impact of hyperparameter tuning would be important for practical applications of SPIE.
  - What evidence would resolve it: A systematic study of the sensitivity of SPIE to the balance between the successor and predecessor components of the intrinsic reward, as well as other hyperparameters, would provide insights into the robustness of the algorithm and guide hyperparameter tuning in practice.

## Limitations
- Performance relies on environments having clear bottleneck structure, which may not generalize to all domains
- The continuous state space extension depends heavily on feature representation quality, which is not thoroughly validated
- The retrospective component's effectiveness assumes that states frequently accessed from many states are true bottlenecks, which may not hold in stochastic environments
- Lacks ablation studies to quantify the relative contributions of prospective vs retrospective components

## Confidence

- High confidence: The discrete MDP theoretical analysis and the core mechanism of combining SR and PR for exploration are sound and well-justified
- Medium confidence: The continuous state space extension and the specific choice of reciprocal reward formulation in the deep RL case
- Low confidence: The Atari results, particularly on Montezuma's Revenge, given the complexity of that domain and limited baseline comparison

## Next Checks

1. Implement ablation studies comparing SPIE with only SR-based rewards, only PR-based rewards, and the full combination on the discrete grid world tasks to quantify each component's contribution
2. Test SPIE on a modified grid world where bottleneck states are not the only hard-to-reach states (e.g., add isolated regions) to check if the retrospective component correctly identifies true bottlenecks
3. Validate the continuous extension by testing DQN-SF-PF on a simple continuous control task with known bottleneck structure (e.g., a continuous version of the grid world) before moving to complex Atari environments