---
ver: rpa2
title: Monte Carlo guided Diffusion for Bayesian linear inverse problems
arxiv_id: '2308.07983'
source_url: https://arxiv.org/abs/2308.07983
tags:
- inverse
- problems
- where
- posterior
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MCGDiff, a novel Monte Carlo method for solving
  Bayesian linear inverse problems with denoising diffusion model priors. The core
  idea is to exploit the structure of both the linear inverse problem and the denoising
  diffusion generative model to design an efficient Sequential Monte Carlo (SMC) sampler.
---

# Monte Carlo guided Diffusion for Bayesian linear inverse problems

## Quick Facts
- arXiv ID: 2308.07983
- Source URL: https://arxiv.org/abs/2308.07983
- Reference count: 40
- Key outcome: Introduces MCGDiff, an SMC sampler that achieves 0.4-2.3 sliced Wasserstein distance on GMM problems vs 4.3-9.9 for baselines

## Executive Summary
This paper introduces MCGDiff, a novel Monte Carlo method for solving Bayesian linear inverse problems with denoising diffusion model priors. The method leverages the structure of both the linear inverse problem and the denoising diffusion generative model to design an efficient Sequential Monte Carlo (SMC) sampler. By iteratively updating particles to approximate the posterior distribution using carefully designed transition kernels and weight functions, MCGDiff balances data fidelity with the denoising diffusion prior. Theoretical analysis establishes consistency under mild assumptions, while numerical simulations demonstrate superior performance compared to competing baselines in terms of accuracy and diversity of samples.

## Method Summary
MCGDiff is a Sequential Monte Carlo (SMC) sampler that approximates the posterior distribution of linear inverse problems with denoising diffusion model priors. The algorithm iteratively updates particles through proposal kernels and weight functions designed to balance data fidelity with the denoising diffusion prior. It extends to general linear Gaussian observation models via SVD decomposition of the forward operator, transforming the problem into an independent scalar basis where SMC can be run efficiently.

## Key Results
- Achieves sliced Wasserstein distances of 0.4-2.3 on Gaussian mixture model problems (vs 4.3-9.9 for other methods)
- Demonstrates effectiveness on image inpainting tasks with realistic and coherent reconstructions
- Shows consistency under mild assumptions through theoretical analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCGDiff achieves consistency by designing transition kernels that balance data fidelity with denoising diffusion priors
- Mechanism: At each iteration, MCGDiff selects particles using a weight function incorporating both backward diffusion kernel and Gaussian conditioned on measurement
- Core assumption: Backward diffusion kernel from Song et al. (2021a) is reversible with respect to the prior
- Break condition: If diffusion kernel is not well-aligned with prior, weight function will not correctly target posterior

### Mechanism 2
- Claim: MCGDiff avoids consistency issues when forward/backward reversal fails by using forward diffused observation to guide observed state
- Mechanism: Instead of directly replacing observed coordinates, MCGDiff weights backward process by density of forward process conditioned on measurement
- Core assumption: Forward diffused observation at time τ approximates measurement distribution
- Break condition: If forward diffusion does not approximate measurement distribution well, guiding potentials become inaccurate

### Mechanism 3
- Claim: MCGDiff extends to general linear Gaussian observation models via SVD decomposition
- Mechanism: SVD transformation decouples measurement model into independent scalar problems; MCGDiff runs SMC in V-basis
- Core assumption: Prior is isotropic in V-basis (p0(Vx) = p0(x))
- Break condition: If prior is not isotropic, V-basis decomposition breaks down

## Foundational Learning

- Concept: Sequential Monte Carlo (SMC) sampling
  - Why needed here: MCGDiff builds particle approximation of posterior through iterative updates
  - Quick check question: In SMC, what is the purpose of weight function at each iteration?

- Concept: Score-based generative models and denoising diffusion models
  - Why needed here: Prior in MCGDiff is defined by denoising diffusion model; backward kernel derived from this prior
  - Quick check question: What is relationship between learned score function and noise predictor?

- Concept: Feynman-Kac formalism
  - Why needed here: SMC algorithm targets sequence of distributions forming Feynman-Kac model
  - Quick check question: In Feynman-Kac model, what role does potential function play?

## Architecture Onboarding

- Component map: Forward noising process -> Backward denoising process -> SMC sampler -> Weight function
- Critical path: 1) Sample particles from prior at t=n, 2) For t=n-1,...,0: compute weight, sample ancestors, propagate, 3) At t=0, sample final particles
- Design tradeoffs: More particles N increases accuracy but costs O(N²) per iteration; including timestep τ trades exactness for practicality
- Failure signatures: Particle degeneracy, divergence from posterior, high variance weights
- First 3 experiments: 1) Implement noiseless inpainting (dy=1, dx=8) and compare SW distance, 2) Vary particles N and measure convergence rate, 3) Test noisy case with σy=0.1 and observe particle evolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions on denoising diffusion model architecture and training data does MCGDiff maintain consistency?
- Basis in paper: Paper proves consistency under general assumptions but doesn't characterize conditions on specific model choices
- Why unresolved: Proves consistency under general assumptions (A1) but doesn't analyze how specific choices affect validity
- What evidence would resolve it: Formal characterization of conditions on denoising diffusion model ensuring required assumptions hold

### Open Question 2
- Question: How does MCGDiff performance scale with dimensionality and ill-posedness of inverse problem?
- Basis in paper: Demonstrates experiments on synthetic data with varying dimensions but lacks comprehensive scaling analysis
- Why unresolved: Focuses on specific synthetic examples without exploring asymptotic behavior or providing theoretical guarantees for high-dimensional settings
- What evidence would resolve it: Theoretical analysis of convergence rate and sample complexity as function of problem dimensions and condition number

### Open Question 3
- Question: Can MCGDiff be extended to handle more general observation models beyond linear Gaussian noise?
- Basis in paper: Presents MCGDiff for linear Gaussian observations and mentions potential extensions but doesn't explore other observation models
- Why unresolved: Focuses on specific case of linear Gaussian observations without covering other realistic scenarios
- What evidence would resolve it: Formal derivation for other observation models with numerical experiments demonstrating effectiveness

## Limitations
- Theoretical consistency proven but lacks rigorous analysis of convergence rates or practical performance bounds
- Empirical validation limited to synthetic GMM and qualitative image inpainting results without comprehensive comparison on real-world datasets
- Extension to non-isotropic priors remains largely theoretical without empirical validation

## Confidence

- High Confidence: Core SMC framework and particle update equations are mathematically sound following established methodology
- Medium Confidence: Claimed performance improvements supported by numerical experiments but could benefit from more extensive validation
- Low Confidence: Practical implementation details for SVD extension are outlined but not empirically verified, particularly for non-isotropic priors

## Next Checks

1. Implement and validate SVD-based extension for non-isotropic priors on simple linear inverse problem with known ground truth, comparing against traditional MCMC methods
2. Conduct systematic ablation studies varying number of particles N, diffusion timesteps, and noise levels σy to characterize convergence behavior
3. Apply MCGDiff to real-world inverse problem from medical imaging or geophysics, comparing against established Bayesian inversion techniques like variational inference or Hamiltonian Monte Carlo