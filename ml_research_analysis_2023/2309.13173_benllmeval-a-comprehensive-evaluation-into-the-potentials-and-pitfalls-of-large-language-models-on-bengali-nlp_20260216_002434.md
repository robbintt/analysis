---
ver: rpa2
title: 'BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of
  Large Language Models on Bengali NLP'
arxiv_id: '2309.13173'
source_url: https://arxiv.org/abs/2309.13173
tags:
- input
- sentiment
- bangla
- language
- please
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on six Bengali
  NLP tasks using zero-shot prompting. Experiments on abstractive summarization, question
  answering, paraphrasing, natural language inference, text classification, and sentiment
  analysis show that while LLMs like ChatGPT occasionally match or exceed state-of-the-art
  fine-tuned models on some tasks, their overall performance is significantly lower.
---

# BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP

## Quick Facts
- arXiv ID: 2309.13173
- Source URL: https://arxiv.org/abs/2309.13173
- Reference count: 0
- Zero-shot prompting with LLMs underperforms SOTA fine-tuned models on Bengali NLP tasks.

## Executive Summary
This paper evaluates large language models (LLMs) on six Bengali NLP tasks using zero-shot prompting. Experiments on abstractive summarization, question answering, paraphrasing, natural language inference, text classification, and sentiment analysis show that while LLMs like ChatGPT occasionally match or exceed state-of-the-art fine-tuned models on some tasks, their overall performance is significantly lower. For example, ChatGPT achieves 52.71% accuracy on Bengali NLI and only 1.06% on news article classification, compared to SOTA accuracies of 82.8% and 87.60% respectively. These results highlight the current limitations of LLMs for low-resource languages like Bengali and call for further research into developing models better suited for such languages.

## Method Summary
The study evaluates three LLMs (ChatGPT, LLaMA-2-13b-chat, Claude-2) on six Bengali NLP tasks using zero-shot prompting. Task-specific prompts are constructed for each dataset including XL-Sum, SQuAD_Bangla, IndicParaphrase, BNLI, Soham News Article Classification, and IndicSentiment/SentNoB. Performance is compared against state-of-the-art fine-tuned models using task-specific metrics such as ROUGE for summarization, EM/F1 for QA, and accuracy for classification.

## Key Results
- ChatGPT achieves 52.71% accuracy on Bengali NLI versus 82.8% for SOTA models
- ChatGPT achieves only 1.06% accuracy on news article classification versus 87.60% for SOTA models
- LLMs exhibit bias toward expressing polarity in NLI tasks, misclassifying neutral examples
- LLMs frequently generate out-of-range responses in classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Zero-shot prompting with LLMs underperforms supervised fine-tuned models on Bengali NLP tasks due to lack of task-specific adaptation.
- **Mechanism**: The LLM relies on general multilingual knowledge and in-context learning from the prompt, but without task-specific training data in Bengali, it cannot align its reasoning patterns to the linguistic and cultural nuances of the language.
- **Core assumption**: Prompts are well-crafted and representative of task requirements, but the model's pre-training data distribution and fine-tuning objectives are not optimized for low-resource languages like Bengali.
- **Evidence anchors**:
  - [abstract]: "Our experimental results demonstrate an inferior performance of LLMs for different Bangla NLP tasks... calling for further effort to develop better understanding of LLMs in low-resource languages like Bangla."
  - [section]: "ChatGPT Response: 2 (Neutral)" in NLI task shows incorrect classification even with clear logical entailment.
  - [corpus]: Weak—no direct citations comparing zero-shot vs fine-tuned performance, only related benchmark studies.
- **Break condition**: If the LLM is pre-trained with balanced multilingual data and fine-tuned on domain-specific Bengali datasets, the performance gap narrows or closes.

### Mechanism 2
- **Claim**: LLMs exhibit bias toward expressing polarity (entailment/contradiction) in NLI tasks, failing to recognize neutral relationships in Bengali.
- **Mechanism**: The auto-regressive training objective favors decisive outputs, and the model's learned priors from multilingual data reinforce sentiment polarity over neutrality in logical reasoning.
- **Core assumption**: The confusion matrix shows high accuracy for contradiction and entailment but high misclassification for neutral labels.
- **Evidence anchors**:
  - [section]: "Approximately 49% of the misclassifications arise when attempting to predict the Neutral class... ChatGPT often exhibits bias toward expressing a particular opinion polarity."
  - [abstract]: "ChatGPT achieves 52.71% accuracy on Bengali NLI" versus higher accuracy for other tasks indicates structural bias.
  - [corpus]: Weak—no corpus evidence on bias patterns, only the paper's empirical findings.
- **Break condition**: If the training objective includes explicit neutrality balancing or contrastive learning on neutral samples, bias toward polarity diminishes.

### Mechanism 3
- **Claim**: LLMs generate out-of-range responses in classification tasks when prompted with target classes, violating instruction-following.
- **Mechanism**: The model's decoder overgeneralizes from the prompt context and outputs tokens outside the allowed set due to insufficient instruction-tuning on constrained output formats.
- **Core assumption**: Prompts specify target classes explicitly, yet outputs like "Development" (not in class list) occur frequently.
- **Evidence anchors**:
  - [section]: "ChatGPT produced 12 classes that are outside the target class range, thus demonstrating the model's inability to comply with the specified instructions."
  - [abstract]: "ChatGPT misclassified the category kolkata the most... failing to generate the correct response in 503 of the test set's 569 examples."
  - [corpus]: Weak—no external studies on out-of-range generation in constrained classification, only internal error analysis.
- **Break condition**: If the model undergoes fine-tuning with constrained output supervision or post-processing filtering, out-of-range generation reduces.

## Foundational Learning

- **Concept**: Zero-shot vs Few-shot learning distinction
  - Why needed here: The study evaluates zero-shot prompting only; understanding the performance gap informs whether few-shot learning would help.
  - Quick check question: What is the main difference between zero-shot and few-shot prompting in terms of model adaptation?

- **Concept**: Cross-lingual transfer limitations
  - Why needed here: LLMs are multilingual but Bengali is low-resource; understanding transfer gaps explains performance drops.
  - Quick check question: Why might a model trained on high-resource languages struggle with low-resource language tasks despite multilingual training?

- **Concept**: Prompt engineering for classification
  - Why needed here: Classification tasks failed when target classes were not explicitly constrained; prompt design directly impacts output correctness.
  - Quick check question: How does specifying target classes in the prompt affect model output adherence?

## Architecture Onboarding

- **Component map**: Prompt constructor → LLM API (ChatGPT/LLaMA/Claude) → Output parser → Evaluation metrics (accuracy, F1, BLEU, etc.)
- **Critical path**: 1. Load test dataset. 2. Generate prompt per sample. 3. Send to LLM API. 4. Parse response. 5. Compare to gold label. 6. Aggregate metrics.
- **Design tradeoffs**: Zero-shot avoids fine-tuning cost but suffers from performance; fine-tuning would require Bengali labeled data. Using multiple LLMs allows comparison but increases cost and complexity. Manual review needed for QA due to generative nature of outputs.
- **Failure signatures**: Out-of-range outputs in classification (e.g., "Development" instead of category). Misclassification bias in NLI (skewed toward contradiction/entailment). Paraphrase generation that changes meaning. Summarization that over-generates or misses key data.
- **First 3 experiments**: 1. Run zero-shot prompting on BNLI with neutral examples only to measure bias magnitude. 2. Test constrained output prompting on Soham News Classification to verify reduction in out-of-range responses. 3. Compare zero-shot vs few-shot (2 examples) on SQuAD_Bangla to assess adaptation benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of Bangla linguistic features (morphology, syntax, idiomatic expressions) do LLMs struggle with most, and how do these challenges vary across different NLP tasks?
- Basis in paper: [explicit] The paper shows LLMs perform poorly on Bangla NLI and text classification tasks, with specific examples of failure to capture nuanced sentiment and logical relationships in Bangla text.
- Why unresolved: The paper identifies performance gaps but doesn't analyze which linguistic features (e.g., honorifics, agglutination, word order) contribute most to these failures across tasks.
- What evidence would resolve it: Detailed error analysis showing which linguistic phenomena cause most errors in each task, comparing performance on sentences with/without these features.

### Open Question 2
- Question: How does the performance gap between English and Bangla in LLMs correlate with dataset size and linguistic distance from English?
- Basis in paper: [explicit] The paper notes LLMs perform well in English but poorly in Bangla, suggesting a relationship between language resources and performance.
- Why unresolved: While the paper observes the performance gap, it doesn't quantify how much dataset size vs. linguistic distance contributes to the difference.
- What evidence would resolve it: Comparative analysis of LLM performance across multiple languages with varying dataset sizes and linguistic distances from English, controlling for other factors.

### Open Question 3
- Question: What architectural modifications or training strategies could improve zero-shot performance of LLMs in low-resource languages like Bangla?
- Basis in paper: [inferred] The paper suggests LLMs should be trained on larger low-resource language datasets or that task-specific models may be needed, but doesn't explore specific solutions.
- Why unresolved: The paper identifies the problem but only suggests general directions (more data, task-specific models) without exploring concrete architectural or training innovations.
- What evidence would resolve it: Experimental results comparing different architectural modifications (e.g., adapter layers, multilingual training strategies) and their impact on zero-shot performance across Bangla NLP tasks.

## Limitations
- Prompt engineering dependency: Performance differences may be due to suboptimal prompting rather than inherent model limitations
- Limited evaluation granularity: No detailed error analysis beyond surface-level observations
- Cross-model consistency issues: Differences in model architecture and training data not normalized

## Confidence
- **High Confidence**: Claims about overall inferior LLM performance compared to SOTA fine-tuned models on Bengali tasks
- **Medium Confidence**: Claims about LLM bias toward polarity in NLI and out-of-range generation in classification
- **Low Confidence**: Claims about the mechanism of failure (e.g., lack of task-specific adaptation or instruction-following inability)

## Next Checks
1. Probe prompt sensitivity: Systematically vary prompts for the most failed tasks to determine if performance improves with prompt engineering
2. Test few-shot learning: Re-run evaluation with 2-3 examples in the prompt to quantify the gap between zero-shot and few-shot performance
3. Analyze cross-lingual transfer: Compare LLM performance on Bengali tasks against equivalent tasks in high-resource languages using the same prompts