---
ver: rpa2
title: Foundation Models for Generalist Geospatial Artificial Intelligence
arxiv_id: '2310.18660'
source_url: https://arxiv.org/abs/2310.18660
tags:
- data
- prithvi
- training
- learning
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Prithvi, a geospatial foundation model pre-trained\
  \ on over 1TB of multispectral satellite imagery from NASA\u2019s Harmonized Landsat\
  \ Sentinel-2 dataset. Leveraging a novel framework, the model is fine-tuned for\
  \ downstream tasks including multi-temporal cloud gap imputation, flood mapping,\
  \ wildfire scar segmentation, and multi-temporal crop segmentation."
---

# Foundation Models for Generalist Geospatial Artificial Intelligence

## Quick Facts
- arXiv ID: 2310.18660
- Source URL: https://arxiv.org/abs/2310.18660
- Reference count: 40
- One-line primary result: Pre-trained geospatial foundation model accelerates fine-tuning and improves data efficiency on downstream Earth observation tasks.

## Executive Summary
This paper introduces Prithvi, a geospatial foundation model pre-trained on over 1TB of multispectral satellite imagery from NASA's Harmonized Landsat Sentinel-2 dataset. Leveraging a novel framework, the model is fine-tuned for downstream tasks including multi-temporal cloud gap imputation, flood mapping, wildfire scar segmentation, and multi-temporal crop segmentation. Results show Prithvi accelerates fine-tuning compared to randomly initialized models, achieving strong performance in multiple tasks. For example, Prithvi outperforms a conditional GAN in cloud imputation by up to 5pp in structural similarity index. The model is also highly data-efficient, maintaining accuracy even with significantly reduced labeled data. The pre-trained 100 million parameter model and fine-tuning workflows are publicly available.

## Method Summary
The paper proposes a geospatial foundation model trained using a masked autoencoder (MAE) approach on over 1TB of multispectral satellite imagery from NASA's Harmonized Landsat Sentinel-2 (HLS) dataset. The model uses 3D patch embeddings and positional encodings to handle spatiotemporal data, treating time as a third dimension. After pretraining on diverse, stratified sampled data, the model is fine-tuned on four downstream tasks with varying amounts of labeled data to evaluate performance and data efficiency. The architecture builds upon the Vision Transformer (ViT) with modifications to process 6 spectral bands across 3 temporal snapshots, and employs Zarr for efficient data loading during training.

## Key Results
- Prithvi accelerates fine-tuning compared to randomly initialized models, reducing the number of epochs needed to reach target accuracy on downstream tasks.
- The model achieves strong performance across multiple tasks, including outperforming a conditional GAN in cloud imputation by up to 5pp in structural similarity index.
- Prithvi demonstrates high data efficiency, maintaining accuracy even when labeled data is reduced to one fourth of the original dataset size.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining accelerates downstream fine-tuning by transferring generalizable features learned from large-scale unlabeled data.
- Mechanism: The MAE-style pretraining on multispectral time-series imagery allows the model to learn robust spatiotemporal representations. When fine-tuning on a small labeled dataset, these representations provide a strong starting point, reducing the number of epochs and data needed to reach high performance.
- Core assumption: Learned representations from HLS data are useful for tasks involving different resolutions, regions, and domains.
- Evidence anchors:
  - [abstract]: "Our experiments show that the pre-trained model accelerates the fine-tuning process compared to leveraging randomly initialized weights."
  - [section]: "After 50 epochs, we observe an IoU on the water class of 81.26, surpassing the performance of off-the-shelf vision transformer architectures (i.e., standard implementations of ViT, Swin) both with random and pre-trained weights."
  - [corpus]: Weak - corpus lacks explicit fine-tuning speed comparisons, but related works cite foundation model advantages generally.
- Break condition: If pretraining data distribution is too narrow or differs too much from target tasks, the transferred features may not be useful, leading to minimal or no speedup.

### Mechanism 2
- Claim: Data efficiency is achieved by leveraging pretrained weights, allowing accurate performance with fewer labeled samples.
- Mechanism: The pretrained encoder already encodes rich multispectral and temporal patterns. During fine-tuning, fewer gradient updates are needed to adapt to a specific task, preserving accuracy even when labeled data is scarce.
- Core assumption: The diversity of the pretraining dataset (stratified sampling across temperature/precipitation regions) provides a good coverage of Earth observation patterns.
- Evidence anchors:
  - [abstract]: "due to the limited availability of labeled data in the field of Earth observation, we gradually reduce the quantity of available labeled data for refining the model to evaluate data efficiency and demonstrate that data can be decreased significantly without affecting the model's accuracy."
  - [section]: "we reduce the number of labeled images during fine-tuning to one forth of the original dataset size (i.e., from 540 images to 252 images to 135 images). The performances are comparable to runs on the full dataset, two-thirds of the dataset, and half of the dataset."
  - [corpus]: Weak - no direct quantitative data efficiency comparisons in corpus; assumption based on general foundation model literature.
- Break condition: If the task domain is highly specialized or outside the pretraining distribution, reduced labeled data may lead to significant performance degradation.

### Mechanism 3
- Claim: The 3D positional encoding and 3D patch embedding enable the model to effectively process spatiotemporal satellite data.
- Mechanism: Unlike standard ViT, the model treats time as a third dimension, allowing attention to span across both spatial and temporal patches. This captures temporal dynamics and spatial relationships in multispectral imagery, which is essential for Earth observation tasks.
- Core assumption: Temporal patterns in satellite imagery are sufficiently correlated to benefit from joint spatiotemporal encoding rather than treating each timestep independently.
- Evidence anchors:
  - [section]: "Our main modifications to the ViT architecture are the 3D positional embedding and the 3D patch embedding, which are required to deal with the spatiotemporal data."
  - [section]: "The model successfully predicts a change in the color of the shape in t2. This visually confirms the temporal abilities of reconstruction."
  - [corpus]: Weak - corpus lacks detailed description of spatiotemporal encoding; assumption based on internal methodological claim.
- Break condition: If temporal correlations are weak or irregular, the 3D encoding may introduce noise without benefit, and simpler temporal aggregation could perform better.

## Foundational Learning

- Concept: Masked Autoencoder (MAE) pretraining
  - Why needed here: Pretraining on large unlabeled multispectral time-series data allows the model to learn rich, generalizable representations before fine-tuning on small labeled datasets.
  - Quick check question: How does MAE differ from standard supervised pretraining, and why is it more suitable for Earth observation?

- Concept: Stratified sampling for data diversity
  - Why needed here: Ensures the pretraining dataset is representative across different Earth regions and climates, avoiding biases toward common landscapes.
  - Quick check question: What are the risks of random sampling in satellite imagery pretraining, and how does stratification mitigate them?

- Concept: 3D positional and patch embeddings
  - Why needed here: Captures both spatial and temporal relationships in satellite data, enabling the model to understand how features evolve over time.
  - Quick check question: Why can't standard 2D ViT positional encodings handle time-series satellite imagery effectively?

## Architecture Onboarding

- Component map: Input -> 3D patch embedding -> MAE encoder (ViT backbone) -> 3D positional encoding -> Masked reconstruction -> Pretrained weights export. Fine-tuning: Pretrained encoder + lightweight decoder head -> Task-specific output.
- Critical path: Efficient data loading (Zarr) -> Pretraining convergence (MAE) -> Fine-tuning with task head -> Evaluation on downstream tasks.
- Design tradeoffs: Larger backbone (ViT-large) improves accuracy but increases compute; smaller (ViT-base) is faster but less accurate. Masking ratio (e.g., 75%) balances reconstruction difficulty and efficiency.
- Failure signatures: Slow convergence in fine-tuning may indicate poor pretraining or domain mismatch; high validation loss in pretraining suggests data quality or masking issues.
- First 3 experiments:
  1. Verify 3D positional encoding by comparing reconstruction accuracy with and without time dimension.
  2. Test fine-tuning speedup by comparing epochs to reach target accuracy with pretrained vs. random weights.
  3. Evaluate data efficiency by fine-tuning on progressively smaller labeled subsets and measuring performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of pretraining on global HLS data versus just US data for downstream task performance?
- Basis in paper: [explicit] The authors state "we expect an additional increase in performance by pretraining on global HLS data" and mention their current model is trained only on US data
- Why unresolved: The paper focuses on a model pretrained on US data and evaluates its generalization to global tasks, but doesn't compare this to a model pretrained on global data
- What evidence would resolve it: Direct comparison of downstream task performance between a model pretrained on US-only HLS data versus one pretrained on global HLS data

### Open Question 2
- Question: How does the performance of Prithvi compare to other foundation models when applied to high-resolution satellite imagery (less than 10m resolution)?
- Basis in paper: [explicit] The authors state "we are interested in the performance of our models on high-resolution satellite imagery (i.e., less than 10m resolution)" as a future direction
- Why unresolved: The paper evaluates Prithvi on 10m and 30m resolution data but doesn't test it on higher resolution imagery or compare it to other foundation models on such data
- What evidence would resolve it: Performance metrics of Prithvi and other foundation models on tasks using sub-10m resolution satellite imagery

### Open Question 3
- Question: What is the optimal masking ratio for MAE pretraining on multispectral and temporal satellite imagery?
- Basis in paper: [inferred] The authors use a 75% masking ratio following the original MAE paper, but note that "we observe that the model successfully reconstructs RGB bands and infrared bands across time and for different masking ratios" in their experiments
- Why unresolved: The paper uses 75% masking as a default but doesn't systematically explore how different masking ratios affect downstream task performance
- What evidence would resolve it: Comparative results showing downstream task performance using different masking ratios during pretraining

### Open Question 4
- Question: How does Prithvi's performance on flood mapping compare to dedicated models like U-Net when evaluated on geographically diverse test data?
- Basis in paper: [explicit] The authors cite [63] which found that while Prithvi was outperformed by 2.5% IoU by a U-Net on Sen1Floods11, it "strongly outperforms the U-Net on data from unseen geographical regions (i.e., 8.6% in the IoU on the water class)"
- Why unresolved: The paper provides limited comparison data and the cited study only evaluates on one dataset, leaving open questions about performance across diverse geographic regions
- What evidence would resolve it: Systematic evaluation of Prithvi versus dedicated models like U-Net across multiple geographically diverse flood mapping datasets

## Limitations

- Limited empirical validation of pretraining speedup: While the paper claims pretrained models accelerate fine-tuning, direct comparisons of epochs to convergence between pretrained and randomly initialized models are not provided.
- Data efficiency claims lack statistical rigor: Performance across different labeled dataset sizes is shown, but without reporting statistical significance or variance across runs.
- Spatiotemporal encoding contributions are not isolated: The paper describes 3D positional encoding but lacks ablation studies comparing it to simpler temporal aggregation methods.

## Confidence

- **High Confidence**: The general premise that foundation models can be effective for Earth observation tasks is well-supported by both this work and the broader literature. The architectural modifications (3D patch and positional embeddings) are logically necessary for spatiotemporal data and align with established practices in multimodal learning.
- **Medium Confidence**: The claims about accelerated fine-tuning and data efficiency are plausible and supported by qualitative observations, but lack rigorous empirical backing. The performance improvements over baselines (e.g., ViT, Swin) are demonstrated, but the comparison to random initialization is incomplete, and no statistical tests are reported.
- **Low Confidence**: The specific contributions of the 3D spatiotemporal encoding to performance are not empirically isolated. Without ablation studies or comparisons to 2D + temporal aggregation baselines, it's unclear whether the added complexity is justified or whether simpler approaches could achieve similar results.

## Next Checks

1. **Fine-tuning Speed Validation**: Conduct controlled experiments comparing the number of epochs required to reach target accuracy for both pretrained and randomly initialized models across all downstream tasks. Report learning curves with standard deviations from multiple runs to assess statistical significance and reproducibility.
2. **Data Efficiency Robustness**: Perform k-fold cross-validation on the labeled datasets to quantify variance in performance as labeled data is reduced. Test whether the claimed data efficiency holds across different tasks and regions, and report confidence intervals for each reduction level.
3. **Spatiotemporal Encoding Ablation**: Design an experiment comparing the current 3D positional encoding approach against a 2D ViT baseline with temporal aggregation (e.g., per-timestep processing followed by late fusion). Measure whether the 3D encoding provides measurable gains in reconstruction quality and downstream task performance, and whether it justifies the increased model complexity.