---
ver: rpa2
title: Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient
  Selection
arxiv_id: '2310.20046'
source_url: https://arxiv.org/abs/2310.20046
tags:
- examples
- adaicl
- daicl
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting which examples to
  annotate for in-context learning (ICL) with large language models under a limited
  budget. The proposed method, ADAICL, combines uncertainty sampling with semantic
  diversity-based selection to identify informative examples that help the model learn
  new information while being budget-efficient.
---

# Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection

## Quick Facts
- **arXiv ID:** 2310.20046
- **Source URL:** https://arxiv.org/abs/2310.20046
- **Reference count:** 40
- **Primary result:** ADAICL improves performance by 4.4% accuracy points over SOTA with 2x fewer examples, and is up to 3x more budget-efficient than random annotation.

## Executive Summary
This paper addresses the critical challenge of selecting which examples to annotate for in-context learning (ICL) with large language models (LLMs) under budget constraints. The proposed ADAICL method combines uncertainty sampling with semantic diversity-based selection to identify informative examples that help the model learn new information while being budget-efficient. By framing the selection problem as a Maximum Coverage problem and dynamically adapting based on the model's feedback, ADAICL significantly outperforms random annotation and state-of-the-art baselines across nine datasets and seven different LLMs.

## Method Summary
ADAICL identifies hard examples using the LLM's uncertainty scores, then constructs semantic regions around these hard examples in a similarity graph. It solves a Maximum Coverage problem to select representative examples that cover the most hard examples while avoiding redundancy. The algorithm uses a greedy approach for approximate solutions, with an enhanced version (ADAICL+) that dynamically re-weights examples to prioritize dense regions over sparse outliers. The method requires an unlabeled dataset, a fixed annotation budget, a semantic similarity space (e.g., SBERT embeddings), and an LLM to generate initial predictions and uncertainty scores.

## Key Results
- ADAICL improves performance by 4.4% accuracy points over state-of-the-art methods
- Achieves 7.7% relative improvement in effectiveness compared to random selection
- Demonstrates up to 3x better budget efficiency than random annotation with 2x fewer ICL examples needed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ADAICL improves performance by focusing on examples where the LLM is uncertain, thereby reducing wasted annotation on examples the model already knows.
- **Mechanism:** The algorithm first uses the LLM's confidence scores to identify "hard" examples (those with low probability predictions), then constructs semantic regions around these hard examples and selects the most representative examples from dense regions.
- **Core assumption:** Examples that the model is uncertain about are the most informative for learning, and selecting representative examples from dense hard regions will generalize well to similar hard examples.
- **Evidence anchors:** [abstract] "Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information"
- **Break condition:** If the model's confidence scores are poorly calibrated or if the semantic space doesn't capture task-relevant similarities, the hard example identification may be ineffective.

### Mechanism 2
- **Claim:** ADAICL's maximum coverage formulation ensures that selected examples provide new information rather than redundant coverage.
- **Mechanism:** The algorithm constructs egonets around hard examples in a similarity graph and solves a maximum coverage problem to select examples that cover the most hard examples while avoiding redundancy.
- **Core assumption:** The semantic regions defined by egonets capture meaningful clusters of similar hard examples, and covering more hard examples with fewer representatives is more informative than redundant coverage.
- **Evidence anchors:** [section 4.2] "M AXCOVER aims to select regions that cover the hardest examples, giving importance to denser regions and disregarding regions already covered"
- **Break condition:** If the semantic similarity space is poorly aligned with task difficulty or if hard examples are uniformly distributed, the coverage formulation may not provide meaningful selection.

### Mechanism 3
- **Claim:** ADAICL+ dynamically re-weights examples to prioritize dense regions over sparse outliers.
- **Mechanism:** When an example is covered, its weight is reduced instead of being marked as covered, allowing dense regions to accumulate higher total weight than sparse regions with single hard examples.
- **Core assumption:** Dense regions of hard examples are more valuable than sparse regions, and re-weighting can effectively shift selection toward these dense regions.
- **Evidence anchors:** [section 4.3] "ADAICL+ tackles this pitfall by a re-weighting schema for the M AXCOVER problem. Whenever a hard example is covered, instead of being marked as covered, ADAICL+ reduces its weight"
- **Break condition:** If the initial weight distribution is poor or if the re-weighting doesn't effectively distinguish dense from sparse regions, the dynamic adjustment may not improve selection.

## Foundational Learning

- **Concept:** In-context learning (ICL) as non-parametric kernel regression
  - Why needed here: Understanding that ICL predictions depend on similarity-weighted averaging of example labels helps explain why selecting semantically diverse examples matters
  - Quick check question: How does the similarity between test examples and ICL examples affect the final prediction in the kernel regression view of ICL?

- **Concept:** Active learning uncertainty sampling
  - Why needed here: ADAICL builds on uncertainty sampling principles but adapts them for the unique characteristics of ICL with frozen LLMs
  - Quick check question: What's the difference between traditional active learning (with model updates) and ADAICL's approach to uncertainty sampling?

- **Concept:** Maximum coverage problem and greedy approximation
  - Why needed here: The core selection mechanism relies on solving a maximum coverage problem, and understanding the greedy approximation is crucial for implementation
  - Quick check question: Why does the greedy algorithm provide a good approximation for the maximum coverage problem, and what's the theoretical guarantee?

## Architecture Onboarding

- **Component map:** Unlabeled dataset -> LLM confidence scores -> Hard example identification -> Semantic region construction -> Maximum coverage optimization -> Selected examples for annotation

- **Critical path:**
  1. Compute LLM confidence scores for all unlabeled examples
  2. Identify hard examples based on uncertainty threshold
  3. Build similarity graph and construct egonets around hard examples
  4. Solve maximum coverage problem to select B examples
  5. Return selected examples for annotation

- **Design tradeoffs:**
  - Computational cost vs. selection quality: Multiple iterations of LLM inference increase cost but improve selection
  - Graph construction parameters (m, hop count) affect both runtime and selection quality
  - Uncertainty threshold determines how conservative the hard example selection is

- **Failure signatures:**
  - Poor performance despite high budget: Likely issues with semantic space quality or uncertainty calibration
  - Overfitting to training distribution: May occur if similarity space is too aligned with training data
  - Random selection performs similarly: Suggests the selection mechanism isn't capturing task-relevant information

- **First 3 experiments:**
  1. Compare ADAICL against random selection on a single dataset with one LLM to verify basic effectiveness
  2. Test sensitivity to the uncertainty threshold parameter to find optimal values
  3. Evaluate different similarity space embeddings (e.g., SBERT vs. BERT) to assess robustness to embedding choices

## Open Questions the Paper Calls Out

- **Question:** How does ADAICL's performance scale with larger budgets beyond the tested range?
  - **Basis in paper:** [explicit] The paper states ADAICL "is up to 3× more budget-efficient than random annotation" and outperforms SOTA "with 2× fewer ICL examples," but testing was limited to specific budget sizes.
  - **Why unresolved:** The experiments focused on budgets up to 20 annotations. Scaling to much larger budgets (e.g., 100+ examples) would require different computational resources and potentially new algorithmic adaptations.
  - **What evidence would resolve it:** Systematic experiments testing ADAICL with progressively larger budgets (e.g., 20, 50, 100, 200) on multiple datasets, comparing both absolute accuracy gains and relative efficiency against baselines.

- **Question:** Can ADAICL be adapted to handle adversarial examples or poisoned datasets?
  - **Basis in paper:** [inferred] The paper mentions limitations including that "adversarial examples are injected into the pool in order to degrade performance," suggesting this is an unaddressed vulnerability.
  - **Why unresolved:** The current algorithm assumes examples are benign and relies on semantic similarity, which could be exploited by carefully crafted adversarial examples designed to mislead the uncertainty estimation or diversity selection.
  - **What evidence would resolve it:** Testing ADAICL on datasets with injected adversarial examples, measuring performance degradation, and proposing modifications (e.g., adversarial training, robust similarity metrics) to maintain effectiveness.

- **Question:** How does ADAICL perform in the presence of label noise or inconsistent annotations?
  - **Basis in paper:** [inferred] While not explicitly discussed, label noise is a common issue in real-world datasets that could affect both the uncertainty estimation (if noisy labels influence the model's confidence) and the semantic diversity (if noisy labels create spurious clusters).
  - **Why unresolved:** The experiments used datasets with presumably clean labels. Introducing varying levels of label noise would test the robustness of ADAICL's uncertainty sampling and diversity-based selection.
  - **What evidence would resolve it:** Experiments with synthetic label noise at different rates (e.g., 5%, 10%, 20%) applied to the datasets, measuring how ADAICL's performance degrades compared to baselines, and analyzing whether it becomes more or less sensitive to noise than other methods.

## Limitations
- Relies on semantic similarity spaces (SBERT embeddings) that may not perfectly capture task-relevant distinctions
- Maximum coverage formulation is a greedy approximation that may miss optimal selections
- Doesn't extensively explore different uncertainty metrics beyond confidence scores
- Assumes examples are benign and doesn't address potential biases in LLM's initial predictions

## Confidence
- **High Confidence Claims:** ADAICL improves performance over random annotation (4.4% accuracy gain), the combination of uncertainty sampling and diversity-based selection is effective, budget efficiency improvements (up to 3x) are reproducible across datasets
- **Medium Confidence Claims:** ADAICL+ with re-weighting provides consistent improvements over standard ADAICL, the method scales to different LLMs and task types, the greedy maximum coverage approximation is sufficient for practical use
- **Low Confidence Claims:** Performance on extremely long or structured inputs (tables, code), robustness to semantic space quality degradation, generalization to domains with weak semantic structure

## Next Checks
1. **Semantic Space Robustness Test:** Evaluate ADAICL using multiple different embedding methods (e.g., SBERT, BERT, task-specific embeddings) to determine how sensitive the selection quality is to the choice of semantic space.

2. **Uncertainty Metric Ablation:** Systematically compare different uncertainty measures (entropy, margin, confidence) and their impact on selection quality to clarify whether confidence-based uncertainty is optimal.

3. **Coverage vs. Random with Similarity Constraint:** Design an experiment comparing ADAICL against a random selection that is explicitly constrained to match the semantic diversity of ADAICL's selection to isolate whether gains come from coverage optimization or semantic diversity alone.