---
ver: rpa2
title: Hallucination Detection for Grounded Instruction Generation
arxiv_id: '2310.15319'
source_url: https://arxiv.org/abs/2310.15319
tags:
- arxiv
- hallucination
- instruction
- room
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucination detection in grounded instruction
  generation, where models generate instructions with references inconsistent with
  the described path. The proposed method fine-tunes a pre-trained vision-language
  model using contrastive learning with synthetic data containing labeled hallucinations.
---

# Hallucination Detection for Grounded Instruction Generation

## Quick Facts
- arXiv ID: 2310.15319
- Source URL: https://arxiv.org/abs/2310.15319
- Authors: 
- Reference count: 13
- Key outcome: Proposed hallucination detection model achieves F1 score of 44.9, outperforming LSTM (38.7) and T5 (33.9) baselines on human-annotated test data

## Executive Summary
This paper addresses hallucination detection in grounded instruction generation, where models generate instructions with references inconsistent with the described path. The authors propose a method that fine-tunes a pre-trained vision-language model using contrastive learning with synthetic data containing labeled hallucinations. Experiments on human-annotated test data show the approach outperforms baselines including LSTM and T5 models, achieving an F1 score of 44.9 versus 38.7 and 33.9 respectively. Ablation studies confirm the importance of self-supervised pre-training and contrastive fine-tuning for performance.

## Method Summary
The proposed method fine-tunes a pre-trained Airbert vision-language model using contrastive learning on synthetic data containing synthesized hallucinations. The model uses dual Transformers (one for vision, one for language) to encode trajectory observations and instructions, with element-wise multiplication to capture cross-modal interactions. Training involves creating positive and negative pairs by replacing directional words or rooms in correct instructions to generate hallucinated versions. The model learns to distinguish between matching trajectory-instruction pairs and those containing hallucinations through a contrastive loss objective.

## Key Results
- Achieves F1 score of 44.9 on human-annotated test data
- Outperforms LSTM baseline (38.7 F1) and T5 baseline (33.9 F1)
- Shows better performance on intrinsic hallucinations (directional words) than extrinsic hallucinations (rooms/objects)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning with synthetic hallucinated examples creates a discriminative boundary between hallucinated and non-hallucinated instruction words.
- Mechanism: The model learns to produce similar representations for trajectory-instruction pairs where the instruction matches the trajectory (positive examples), and dissimilar representations when the instruction contains hallucinated words (negative examples). This forces the model to encode trajectory-relevant information that can detect inconsistencies.
- Core assumption: Synthetic hallucinations created by replacing directional words or rooms with alternatives effectively mimic real hallucinations in the speaker model's output.
- Evidence anchors:
  - [abstract] "fine-tuning it with a contrastive loss that separates correct instructions from instructions containing synthesized hallucinations"
  - [section 4.2] "We then train the model to recognize the positive example by minimizing the cross entropy between ˆp and p⋆ = (1, 0)"
  - [corpus] Weak evidence - related papers discuss hallucination detection but don't directly validate this contrastive approach
- Break condition: If synthetic hallucinations don't match the distribution of real hallucinations, the learned boundary may not generalize to actual model-generated hallucinations.

### Mechanism 2
- Claim: Pre-training on large vision-language corpora provides transferable representations that capture multimodal correspondences relevant for hallucination detection.
- Mechanism: The Airbert model, pre-trained on 1.4M images and 0.7M captions, learns general visual-linguistic relationships. This pre-training provides a strong starting point that captures relevant semantic and visual features before fine-tuning on the hallucination detection task.
- Core assumption: Representations learned from general image-caption pairs transfer effectively to the specific task of matching trajectories to instructions.
- Evidence anchors:
  - [abstract] "adopting a model pre-trained on a large corpus of image-text pairs"
  - [section 4.2] "Instead of learning from scratch, we fine-tune a pre-trained checkpoint of the Airbert model"
  - [section 5] "Ablation studies confirm the importance of the proposed self-supervised pre-training"
- Break condition: If the pre-training data distribution differs significantly from the navigation domain, transfer may be ineffective.

### Mechanism 3
- Claim: The specific architectural design with dual Transformers (one for vision, one for language) enables effective multimodal reasoning for hallucination detection.
- Mechanism: The language Transformer processes the instruction with the target word highlighted, while the vision Transformer processes the trajectory observations. Element-wise multiplication of their outputs allows the model to focus on the interaction between specific instruction words and trajectory elements.
- Core assumption: The cross-modal attention in dual Transformers effectively captures the correspondence between instruction words and trajectory observations.
- Evidence anchors:
  - [section 4.1] "An overview of the model is given in Figure 1. It implements two Transformers: one encodes the instruction u and the other encodes the trajectory r."
  - [section 4.1] "The model computes a score function s(x) = s(r, u, i) = w⊤(hlang ⊙ hvision)"
  - [corpus] Weak evidence - related papers don't specifically analyze this dual-Transformer architecture for hallucination detection
- Break condition: If the cross-modal attention mechanism doesn't effectively capture relevant correspondences, the model won't learn meaningful hallucination detection boundaries.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Enables learning discriminative representations without requiring labeled hallucination examples during pre-training
  - Quick check question: What is the objective function used to separate positive and negative examples in contrastive learning?

- Concept: Multimodal representation learning
  - Why needed here: The task requires understanding relationships between visual trajectories and textual instructions
  - Quick check question: How does the Airbert model represent trajectory observations versus textual instructions?

- Concept: Synthetic data generation
  - Why needed here: Enables creating training data with hallucination labels without expensive human annotation
  - Quick check question: What are the two main types of hallucinations created synthetically in this work?

## Architecture Onboarding

- Component map: Trajectory observations → vision Transformer → visual embeddings → hvision; Instruction with highlighted word → language Transformer → hlang; hlang ⊙ hvision → linear layer → score
- Critical path: Vision encoder processes 36 panoramic images per location plus action vectors; language encoder processes instruction with target word highlighted; element-wise multiplication captures cross-modal interactions; linear classifier produces hallucination score
- Design tradeoffs: Dual Transformer architecture vs single multimodal Transformer; element-wise multiplication vs concatenation; synthetic data generation vs human annotation
- Failure signatures: Poor performance on room/object hallucinations suggests limited visual grounding; low recall indicates threshold selection issues; poor performance on synthetic vs real data suggests domain shift
- First 3 experiments:
  1. Train with pre-training disabled (random initialization) to measure transfer benefit
  2. Train with maximum likelihood fine-tuning instead of contrastive learning to compare objectives
  3. Evaluate on development set with different decision thresholds to find optimal operating point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of hallucinations (intrinsic vs extrinsic) impact the overall performance of hallucination detection models, and are there specific detection strategies that are more effective for one type over the other?
- Basis in paper: [explicit] The paper explicitly defines and differentiates between intrinsic and extrinsic hallucinations, noting that the model struggles more with detecting room and object hallucinations.
- Why unresolved: The paper provides a breakdown of performance by word type but does not analyze the differential impact of intrinsic versus extrinsic hallucinations on model performance or suggest targeted strategies for each type.
- What evidence would resolve it: Detailed analysis of model performance metrics (precision, recall, F1) for intrinsic vs extrinsic hallucinations separately, along with experiments testing specialized detection strategies for each type.

### Open Question 2
- Question: What is the scalability of the proposed hallucination detection approach when applied to larger, more diverse datasets beyond the Room-to-Room dataset, particularly in environments with more complex visual and linguistic features?
- Basis in paper: [inferred] The paper mentions the use of the Room-to-Room dataset and synthetic data augmentation, but does not explore the model's performance on larger or more diverse datasets.
- Why unresolved: The paper's experiments are limited to a specific dataset, and there is no discussion of how the model would perform in more varied or complex environments.
- What evidence would resolve it: Experiments applying the hallucination detection model to multiple, larger datasets with diverse environments and linguistic structures, comparing performance metrics across these datasets.

### Open Question 3
- Question: How does the model's hallucination detection performance vary with the quality of the input instructions, and can the model be adapted to provide feedback or corrections to improve instruction generation in real-time?
- Basis in paper: [explicit] The paper mentions using high-quality model-generated instructions as positive examples in training and discusses the model's ability to detect hallucinations, but does not explore real-time feedback or correction mechanisms.
- Why unresolved: The paper focuses on detection rather than correction, and there is no discussion of how the model's performance might vary with instruction quality or how it could be integrated into a real-time feedback system.
- What evidence would resolve it: Studies measuring detection accuracy across a range of instruction qualities and experiments implementing a feedback loop where the model suggests corrections to improve instruction generation in real-time scenarios.

## Limitations

- Domain Generalization Gap: The synthetic hallucination generation method may not capture the full complexity of real hallucinations produced by instruction generation models.
- Data Dependency: The approach relies heavily on the quality and diversity of synthetic training data, with only 14,871 synthetic examples.
- Evaluation Scope: The human-annotated test set contains only 136 instructions, which may not be representative of the full distribution of hallucination types and frequencies.

## Confidence

**High Confidence (Likelihood > 80%)**:
- The contrastive learning approach with synthetic data improves hallucination detection over baseline models
- The Airbert pre-trained model provides useful representations for the task
- The proposed method achieves better F1 scores than LSTM and T5 baselines on the human-annotated test set

**Medium Confidence (Likelihood 50-80%)**:
- The synthetic hallucination generation method effectively captures real hallucination patterns
- The dual Transformer architecture is critical for the model's performance
- The performance gap between intrinsic and extrinsic hallucination detection reflects fundamental differences in visual grounding difficulty

**Low Confidence (Likelihood < 50%)**:
- The method will generalize to other instruction generation models beyond the speaker model used in the evaluation
- The model will maintain performance when scaled to larger, more diverse datasets
- The learned representations capture semantically meaningful distinctions beyond the specific hallucination types in the training data

## Next Checks

1. **Cross-Model Validation**: Evaluate the hallucination detection model on instructions generated by different instruction generation models (beyond the speaker model used in the current evaluation) to assess generalizability across different generation architectures and training approaches.

2. **Real vs Synthetic Distribution Analysis**: Conduct a detailed comparison of word distributions, hallucination patterns, and semantic content between the synthetic training data and real model-generated instructions to quantify the domain gap and identify specific types of hallucinations that may be missing from the synthetic data.

3. **Navigation Impact Study**: Measure the correlation between hallucination detection scores and actual navigation performance by integrating the detection model into a navigation pipeline and measuring success rates when filtering out instructions with detected hallucinations versus unfiltered instructions.