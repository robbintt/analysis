---
ver: rpa2
title: 'Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective
  on Demonstration Selection'
arxiv_id: '2312.07476'
source_url: https://arxiv.org/abs/2312.07476
tags:
- demonstration
- demonstrations
- bias
- llms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Comparable Demonstrations (CDs) to mitigate
  demonstration bias in In-Context Learning (ICL) for large language models (LLMs).
  The authors argue that traditional demonstration selection methods can lead to bias,
  as LLMs may misunderstand the task's essence due to limited demonstration numbers.
---

# Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection

## Quick Facts
- **arXiv ID:** 2312.07476
- **Source URL:** https://arxiv.org/abs/2312.07476
- **Reference count:** 0
- **Primary result:** Introduces Comparable Demonstrations (CDs) to reduce demonstration bias in ICL by creating label-flipped text pairs that highlight task essence.

## Executive Summary
This paper addresses the critical issue of demonstration bias in In-Context Learning (ICL) for large language models. The authors propose Comparable Demonstrations (CDs), constructed by minimally editing texts to flip corresponding labels, creating strong inter-demonstration comparisons. This approach aims to highlight the task's essence and eliminate spurious correlations that can mislead LLMs during instruction induction. Extensive experiments show that CDs significantly reduce demonstration bias and outperform semantic similarity-based methods, particularly in out-of-distribution scenarios.

## Method Summary
The method involves constructing Comparable Demonstrations by minimally editing texts to flip corresponding labels, inspired by Counterfactually-Augmented Data. The authors compare CDs against random selection and semantic similarity-based strategies across sentiment analysis and NLI tasks using GPT-3.5-turbo. They evaluate performance on both in-distribution and out-of-distribution datasets, measuring accuracy and instruction induction quality. The approach specifically targets the demonstration bias problem where LLMs may misunderstand task essence due to limited demonstration numbers.

## Key Results
- CDs significantly reduce demonstration bias by creating strong inter-demonstration comparisons that highlight task essence
- CDs outperform semantic similarity-based methods, especially in out-of-distribution scenarios and simple tasks
- CDs show robustness to demonstration number variations, while semantic similarity methods are more sensitive to the number of demonstrations used

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CDs reduce demonstration bias by creating strong inter-demonstration comparisons that highlight task essence while eliminating spurious correlations
- **Mechanism:** Minimal text edits that flip labels create pairs where only task-relevant features differ, forcing the model to focus on essential differences
- **Core assumption:** Edited portions involve essential task properties critical for understanding input-label mapping
- **Evidence anchors:** [abstract], [section] on CAD inspiration
- **Break condition:** If minimal edits don't capture essential task features or if model ignores edited portions

### Mechanism 2
- **Claim:** CDs improve OOD performance by helping models focus on task essence rather than dataset-specific information
- **Mechanism:** Demonstrations emphasize core task logic through comparison, helping models learn general principles over dataset-specific patterns
- **Core assumption:** Dataset-specific information interferes with generalization and CDs can suppress this interference
- **Evidence anchors:** [abstract], [section] on dataset-specific information
- **Break condition:** If model already has strong generalization or task lacks dataset-specific confounders

### Mechanism 3
- **Claim:** CDs are more robust to demonstration number variations than semantic similarity-based methods
- **Mechanism:** Explicit comparisons provide consistent task boundary information regardless of demonstration count
- **Core assumption:** Quality and consistency of demonstration information is more important than quantity
- **Evidence anchors:** [abstract], [section] on robustness to demonstration number
- **Break condition:** If task requires minimum demonstrations or model learns better from larger diverse sets

## Foundational Learning

- **Concept:** In-Context Learning (ICL) paradigm
  - **Why needed here:** Understanding how LLMs use demonstrations without parameter updates is fundamental to why demonstration selection matters
  - **Quick check question:** How does an LLM use K input-label pairs to predict a new input's label without updating parameters?

- **Concept:** Demonstration bias
  - **Why needed here:** The core problem CDs address is that limited demonstrations can lead to incorrect input-label mappings
  - **Quick check question:** What happens when an LLM induces an input-label mapping that satisfies demonstrations but doesn't match actual task requirements?

- **Concept:** Counterfactual augmentation
  - **Why needed here:** CDs build on minimally editing examples to flip labels, which helps identify task-relevant features
  - **Quick check question:** How does minimally editing an example to flip its label help identify essential task features?

## Architecture Onboarding

- **Component map:** Text embeddings -> Demonstration selection module -> LLM inference engine -> Evaluation pipeline
- **Critical path:** 1) Load dataset and CAD examples, 2) Generate text embeddings, 3) Select demonstrations, 4) Format ICL prompt, 5) Send to LLM, 6) Evaluate performance
- **Design tradeoffs:** Manual editing vs automated generation (expensive but ensures quality), demonstration number (more may reduce bias but increase cost), task complexity (simple tasks benefit more)
- **Failure signatures:** Poor OOD performance despite good ID results suggests overfitting, inconsistent performance across demonstration numbers suggests sensitivity, similar performance between random and CDs suggests ineffective editing
- **First 3 experiments:** 1) Compare instruction induction quality across random, nearest, and CDs random on IMDb, 2) Test 4-shot ICL performance on IMDb vs Amazon/Yelp using different strategies, 3) Vary demonstration number (4, 8, 12 shots) to assess robustness

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific aspects of text are most commonly edited to create CDs, and how do these edits relate to task's essence?
  - **Basis in paper:** [explicit] Paper mentions edited parts involve essential properties but doesn't specify which aspects are most commonly edited
  - **Why unresolved:** Paper lacks detailed analysis of specific edits and their relationship to task essence
  - **What evidence would resolve it:** Detailed breakdown of edit types with analysis of how they relate to task essence

- **Open Question 2:** How does CD performance vary across different task complexities, and what factors contribute to this variation?
  - **Basis in paper:** [inferred] Paper notes larger performance differences on simple tasks vs complex tasks
  - **Why unresolved:** Paper doesn't analyze factors contributing to performance variation across task complexities
  - **What evidence would resolve it:** Comprehensive analysis across task complexities with investigation of contributing factors

- **Open Question 3:** Can CD effectiveness be improved by incorporating additional inter-demonstration relationships like many-to-many relationships?
  - **Basis in paper:** [explicit] Paper mentions CDs only consider one-to-one relationships without many-to-many relationships
  - **Why unresolved:** Paper doesn't explore potential benefits of incorporating additional inter-demonstration relationships
  - **What evidence would resolve it:** Experiments comparing CDs with and without additional inter-demonstration relationships

## Limitations

- **Resource constraint:** Creating CDs requires manual annotation work, limiting scalability and practical applicability
- **Generalization gap:** Effectiveness for task types beyond sentiment analysis and NLI remains untested
- **Mechanism dependency:** Effectiveness heavily depends on quality of minimal edits, which isn't fully validated across tasks

## Confidence

- **High confidence:** Demonstration bias problem is well-established in ICL literature, experimental methodology is sound
- **Medium confidence:** Mechanism by which CDs reduce bias is plausible but relies on assumptions about minimal edits not fully validated
- **Low confidence:** Claim about robustness to demonstration number variations lacks strong empirical support

## Next Checks

1. **Ablation Study on Edit Quality:** Systematically vary degree of text editing (very minimal to substantial changes) and measure effects on instruction induction quality and downstream ICL performance

2. **Cross-Domain Transfer Test:** Evaluate CDs on completely different task domain (e.g., mathematical reasoning or code generation) to test mechanism generalization

3. **Human Evaluation of Edit Semantics:** Have human annotators rate whether minimal edits capture task-essential features versus introducing noise, correlating judgments with model performance