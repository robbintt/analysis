---
ver: rpa2
title: 'Query2GMM: Learning Representation with Gaussian Mixture Model for Reasoning
  over Knowledge Graphs'
arxiv_id: '2306.10367'
source_url: https://arxiv.org/abs/2306.10367
tags:
- query
- distribution
- embedding
- queries
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Query2GMM, a novel approach for answering
  complex logical queries over knowledge graphs by embedding queries and entities
  into a shared space using a univariate Gaussian Mixture Model (GMM). The method
  addresses the challenge of modeling diversified answer entities by accurately quantifying
  each subset through cardinality, semantic center, and dispersion degree.
---

# Query2GMM: Learning Representation with Gaussian Mixture Model for Reasoning over Knowledge Graphs

## Quick Facts
- arXiv ID: 2306.10367
- Source URL: https://arxiv.org/abs/2306.10367
- Reference count: 40
- Key outcome: Achieves 6.35% average absolute improvement in MRR over state-of-the-art baselines for complex logical query answering

## Executive Summary
Query2GMM introduces a novel approach for answering complex logical queries over knowledge graphs by embedding queries and entities into a shared space using a univariate Gaussian Mixture Model (GMM). The method addresses the challenge of modeling diversified answer entities by accurately quantifying each subset through cardinality, semantic center, and dispersion degree. By employing a new similarity measure based on mixed Wasserstein distance, Query2GMM effectively computes relationships between entities and multiple answer subsets, significantly outperforming state-of-the-art baselines.

## Method Summary
Query2GMM represents each query using a univariate GMM embedding that captures the cardinality, semantic center, and dispersion degree of each subset of answer entities. The model employs specific neural networks for projection, intersection, negation, and union operators, with a co-attention network designed for the intersection operator to capture both inter-query and inter-subset level information. The similarity between entities and queries is measured using a mixed Wasserstein distance that combines Wasserstein distance and the upper bound of KL divergence. The model is trained using cross-entropy loss on three standard datasets (FB15k, FB237, NELL).

## Key Results
- Achieves an average absolute improvement of 6.35% in MRR over state-of-the-art baselines
- Outperforms baselines on 12 out of 14 query types across all datasets
- Shows consistent improvements across different metrics (MRR, Hit@3, Hit@10)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The GMM embedding accurately captures the cardinality, semantic center, and dispersion degree of each subset of answer entities.
- **Mechanism**: Each subset is encoded by its cardinality (normalized probability), semantic center (mean value), and dispersion degree (standard deviation) using a univariate Gaussian mixture model. The Cartesian product of these parameters for each dimension allows independent learning of different information.
- **Core assumption**: Multi-modal distributions can better represent queries with diversified answers than uni-modal distributions, as queries may contain multiple disjoint answer subsets due to the compositional nature of multi-hop queries and the varying latent semantics of relations.
- **Evidence anchors**:
  - [abstract]: "Research along this line suggests that using multi-modal distribution to represent answer entities is more suitable than uni-modal distribution, as a single query may contain multiple disjoint answer subsets due to the compositional nature of multi-hop queries and the varying latent semantics of relations."
  - [section 1]: "A few recent studies aim to model the diversity of answer entities by designing appropriate embedding backbones that encode multiple disjoint subsets of answer entities."
  - [corpus]: Weak - the corpus contains related papers but lacks direct evidence for the specific mechanism of GMM embedding.
- **Break condition**: If the answer entities do not form multiple disjoint subsets, the GMM embedding's advantage over uni-modal distributions diminishes.

### Mechanism 2
- **Claim**: The mixed Wasserstein distance effectively measures the relationships between entities and multiple answer subsets of a query.
- **Mechanism**: The mixed Wasserstein distance combines the Wasserstein distance and the upper bound of KL divergence to compute the similarity between a Gaussian distribution (entity) and a Gaussian mixture distribution (query). This allows for bidirectional adjustments and works even when distributions do not intersect.
- **Core assumption**: There is no existing feasible solution to compute the similarity between a Gaussian distribution and a Gaussian mixture distribution directly.
- **Evidence anchors**:
  - [section 3.6]: "Actually, there is no existing feasible solution to compute the similarity between the Gaussian distribution and the Gaussian mixture distribution..."
  - [section 3.6]: "To solve these issues, we define a new similarity function (named mixed Wasserstein distance), based on the Wasserstein distance [22] and the upper bound of KL divergence..."
  - [corpus]: Weak - the corpus does not provide direct evidence for the effectiveness of the mixed Wasserstein distance.
- **Break condition**: If the upper bound of KL divergence is not tight enough, the accuracy of the similarity measurement may be compromised.

### Mechanism 3
- **Claim**: The co-attention network effectively captures both inter-query and inter-subset level information for the intersection operator.
- **Mechanism**: The co-attention network uses a cross-attention network for inter-query level information and a self-attention network for inter-subset level information. Attention pooling is then used to adaptively generate the output.
- **Core assumption**: The complexity of intersection operations increases k-fold in the multi-modal distribution context, requiring both inter-query and inter-subset level information.
- **Evidence anchors**:
  - [section 3.3]: "To overcome the above drawbacks, we design a co-attention model to directly derive the m âˆ’ intersected areas of m input entity sets."
  - [section 4.3]: "Furthermore, we replace our neural model with self-attention and random sampling used in Q2P [6] for the intersection operator, called Query2GMMinter. The performance falls by up to 12.2%..."
  - [corpus]: Weak - the corpus does not provide direct evidence for the effectiveness of the co-attention network.
- **Break condition**: If the number of input entity sets (m) is small, the benefit of the co-attention network may not be significant.

## Foundational Learning

- **Concept**: First-Order Logic (FOL) queries and their Disjunctive Normal Form (DNF) representation
  - Why needed here: The paper aims to answer FOL queries over knowledge graphs, and DNF is used to represent these queries for effective computation.
  - Quick check question: What are the logical operations involved in FOL queries, and how are they represented in DNF?

- **Concept**: Gaussian Mixture Models (GMMs) and their parameters (normalized probability, mean value, standard deviation)
  - Why needed here: The paper proposes using GMM embeddings to represent queries, where each subset of a query is encoded by its cardinality, semantic center, and dispersion degree.
  - Quick check question: How do the parameters of a GMM correspond to the cardinality, semantic center, and dispersion degree of a subset?

- **Concept**: Wasserstein distance and Kullback-Leibler (KL) divergence
  - Why needed here: The paper defines a new similarity measure called mixed Wasserstein distance, which combines the Wasserstein distance and the upper bound of KL divergence.
  - Quick check question: What are the properties of Wasserstein distance and KL divergence, and how are they used in the mixed Wasserstein distance?

## Architecture Onboarding

- **Component map**: Entity/Relation Embeddings -> GMM Query Embedding -> Logical Operators (Projection, Intersection, Negation, Union) -> Mixed Wasserstein Distance -> Answer Selection

- **Critical path**: The critical path for answering a query involves:
  1. Constructing the computation graph from the query
  2. Executing the logical operators in sequence using the neural networks
  3. Computing the similarity between entities and the final query embedding using the mixed Wasserstein distance
  4. Selecting the top-k entities as answers

- **Design tradeoffs**:
  - Using univariate Gaussian distributions with Cartesian product instead of multivariate Gaussian distributions to maintain linear complexity and avoid expensive computations like matrix inversion.
  - Employing a co-attention network for the intersection operator to capture both inter-query and inter-subset level information, which increases model complexity but improves performance.

- **Failure signatures**:
  - If the model fails to accurately capture the cardinality of subsets, it may include too many false positives or negatives in the answer set.
  - If the mixed Wasserstein distance is not effective, the similarity measurement between entities and queries may be inaccurate, leading to incorrect answers.

- **First 3 experiments**:
  1. Verify the effectiveness of the GMM embedding by comparing the performance of Query2GMM with and without cardinality and dispersion degree parameters on a small dataset.
  2. Test the co-attention network for the intersection operator by comparing its performance with a simpler approach like random sampling on a query with multiple input entity sets.
  3. Evaluate the mixed Wasserstein distance by comparing its performance with other similarity measures like L1 distance on a query with multiple answer subsets.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Query2GMM perform on datasets with significantly different graph structures or densities compared to the tested knowledge graphs (FB15k, FB237, NELL)?
  - Basis in paper: [inferred] The experiments were conducted on three standard datasets with similar graph structures.
  - Why unresolved: The paper does not provide experimental results on datasets with significantly different graph structures or densities.
  - What evidence would resolve it: Conducting experiments on datasets with varying graph structures and densities to compare Query2GMM's performance.

- **Open Question 2**: What is the impact of varying the embedding dimension (d) on Query2GMM's performance?
  - Basis in paper: [explicit] The paper states that the embedding dimension d was set to 400.
  - Why unresolved: The paper does not explore the impact of different embedding dimensions on the model's performance.
  - What evidence would resolve it: Conducting experiments with different embedding dimensions and analyzing the effect on Query2GMM's performance.

- **Open Question 3**: How does Query2GMM handle queries with a large number of answer subsets, and what is the computational complexity in such cases?
  - Basis in paper: [inferred] The paper mentions that Query2GMM uses GMM embedding to represent each query, but does not discuss its performance with a large number of answer subsets.
  - Why unresolved: The paper does not provide information on the model's performance or computational complexity when dealing with queries having a large number of answer subsets.
  - What evidence would resolve it: Conducting experiments with queries containing a varying number of answer subsets and analyzing the computational complexity and performance of Query2GMM.

## Limitations
- The approach assumes queries have multiple disjoint answer subsets, which may not hold for all logical query patterns.
- The mixed Wasserstein distance relies on an upper bound approximation that may not be tight in all cases.
- The model complexity increases with the number of answer subsets due to the co-attention network for intersection operations.

## Confidence
- **High Confidence**: The overall framework design and experimental methodology are sound, with proper evaluation on standard benchmarks and clear improvement over baselines.
- **Medium Confidence**: The mechanism of using GMM embeddings to capture multi-modal answer distributions is well-motivated theoretically and shows consistent improvements in experiments.
- **Medium Confidence**: The mixed Wasserstein distance formulation is novel and addresses a real technical gap, though the approximation of KL divergence upper bounds may introduce inaccuracies.

## Next Checks
1. **Ablation Study**: Remove cardinality and dispersion degree parameters from the GMM embedding and measure performance degradation on queries with known multi-modal answer distributions.
2. **Wasserstein Distance Validation**: Compare the mixed Wasserstein distance performance against exact computation methods on small synthetic examples where ground truth similarity can be computed.
3. **Operator Robustness**: Test the co-attention network's performance degradation when reducing the number of input entity sets in intersection queries to verify if the added complexity is justified.