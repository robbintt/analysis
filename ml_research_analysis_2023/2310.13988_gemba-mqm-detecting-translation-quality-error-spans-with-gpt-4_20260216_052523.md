---
ver: rpa2
title: 'GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4'
arxiv_id: '2310.13988'
source_url: https://arxiv.org/abs/2310.13988
tags:
- translation
- metrics
- error
- machine
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEMBA-MQM, a GPT-4 based evaluation metric
  for detecting translation quality errors without requiring human reference translations.
  The method uses a fixed three-shot prompting technique with language-agnostic prompts,
  enabling evaluation across different language pairs without manual prompt preparation.
---

# GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4

## Quick Facts
- arXiv ID: 2310.13988
- Source URL: https://arxiv.org/abs/2310.13988
- Reference count: 12
- Primary result: Achieves 96.5% system-level accuracy on WMT2023 blind test set, outperforming established metrics

## Executive Summary
This paper introduces GEMBA-MQM, a novel evaluation metric for detecting translation quality errors without requiring human reference translations. The method leverages GPT-4's few-shot learning capabilities through language-agnostic prompts to identify and classify errors using the MQM framework. Tested on WMT2023, MQM22, and internal Microsoft test sets, GEMBA-MQM demonstrates state-of-the-art performance for system-level ranking while avoiding the need for manual prompt preparation across different language pairs.

## Method Summary
GEMBA-MQM employs a fixed three-shot prompting technique using GPT-4 to mark translation quality error spans according to the MQM framework. The method uses language-agnostic prompts with three predetermined examples, enabling evaluation across different language pairs without manual prompt preparation. For each source-target segment pair, the system queries GPT-4 to identify errors, classify them into MQM categories (accuracy, fluency, locale convention, style, terminology, non-translation, other, or no-error), and assign severity levels (critical, major, minor). The method then aggregates these error annotations into quality scores for segment and system-level evaluation.

## Key Results
- Achieves 96.5% system-level pairwise accuracy on WMT2023 blind test set
- Outperforms established metrics like COMET and BLEURT-20 in system ranking tasks
- Maintains consistent performance across high-resource languages (15 highest resource languages tested)
- Successfully handles segment-level evaluation when removing problematic locale convention error class

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can accurately identify and classify translation quality errors using MQM framework without reference translations
- Mechanism: The large language model leverages its pretraining on diverse multilingual text to understand translation quality issues and apply the MQM error classification schema through few-shot prompting
- Core assumption: GPT-4 has sufficient multilingual understanding and error pattern recognition from its training to function as an effective translation quality evaluator
- Evidence anchors:
  - [abstract] "GEMBA-MQM employs a fixed three-shot prompting technique, querying the GPT-4 model to mark quality error spans"
  - [section 2] "Our technique adopts few-shot learning with the GPT-4 model (OpenAI, 2023), prompting the model to mark quality error spans using the MQM framework"
  - [corpus] Weak evidence - corpus shows related work but no direct validation of GPT-4's error detection capability
- Break condition: If GPT-4 lacks sufficient multilingual coverage or the error patterns are too domain-specific for its pretraining distribution

### Mechanism 2
- Claim: Language-agnostic prompts enable evaluation across different language pairs without manual prompt preparation
- Mechanism: By using fixed three-shot examples that don't rely on language-specific knowledge, the same prompt template can be applied universally across language pairs
- Core assumption: The error classification categories and examples are general enough to transfer across languages without needing language-specific customization
- Evidence anchors:
  - [abstract] "our method has language-agnostic prompts, thus avoiding the need for manual prompt preparation for new languages"
  - [section 2] "our prompts are universally applicable across languages, avoiding the need for manual prompt preparation for each language pair"
  - [section 4.3] "When we look at the distribution of the error classes over the fifteen highest resource languages... we observe that 32% of all errors for GEMBA-locale-MQM are marked as a locale convention"
- Break condition: If certain languages have error types not covered by the fixed examples or require language-specific cultural knowledge

### Mechanism 3
- Claim: System-level pairwise accuracy effectively measures translation quality metric performance
- Mechanism: By comparing how well metrics rank system pairs relative to human rankings, we get a robust measure of metric utility for the primary use case of system comparison
- Core assumption: The pairwise comparison approach captures the most important metric usage scenario and provides stable evaluation results
- Evidence anchors:
  - [section 3.2] "The main use case of automatic metrics is system ranking... we focus on a method that specifically measures this target: system-level pairwise accuracy"
  - [section 3.2] "Accuracy = |sign(metric∆) == sign(human∆)| |all system pairs|"
  - [section 3.2] "Although it is desirable to have an automatic metric that correlates highly with human annotation behaviour and which is useful for segment-level evaluation, more research is needed"
- Break condition: If the pairwise accuracy metric doesn't generalize well to other evaluation scenarios or becomes unstable with small sample sizes

## Foundational Learning

- Concept: Few-shot learning with large language models
  - Why needed here: Enables the model to understand the task and error classification schema without requiring extensive fine-tuning
  - Quick check question: How many examples are used in the few-shot prompting approach, and why is this number significant?

- Concept: MQM (Multidimensional Quality Metrics) framework
  - Why needed here: Provides the standardized error classification schema that GPT-4 uses to identify and categorize translation errors
  - Quick check question: What are the main categories of errors in the MQM framework, and how are they weighted for severity?

- Concept: System-level pairwise accuracy evaluation
  - Why needed here: Measures how well the metric performs its primary use case of comparing translation systems
  - Quick check question: How is pairwise accuracy calculated, and what does it tell us about metric performance?

## Architecture Onboarding

- Component map:
  - Source text -> Translated text -> Prompt template with three-shot examples -> GPT-4 model -> Error spans with MQM categories and severity levels -> Aggregated quality scores

- Critical path:
  1. Prepare source and target text segments
  2. Format according to prompt template
  3. Send to GPT-4 with three-shot examples
  4. Parse LLM response for error spans
  5. Classify and weight errors according to MQM
  6. Aggregate to segment and system-level scores

- Design tradeoffs:
  - Fixed prompts vs. language-specific customization (reduced complexity vs. potential performance)
  - Three-shot vs. more examples (efficiency vs. task understanding)
  - GPT-4 dependency vs. open models (performance vs. accessibility)

- Failure signatures:
  - High proportion of "locale convention" errors indicating prompt misinterpretation
  - Inconsistent error classification across similar segments
  - System-level scores that don't align with human rankings

- First 3 experiments:
  1. Test on a small bilingual corpus with known errors to verify error detection accuracy
  2. Compare results using different numbers of shot examples (1-shot, 3-shot, 5-shot)
  3. Evaluate performance on low-resource language pairs to test language-agnostic approach limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GEMBA-MQM's performance vary across different language families and resource levels, particularly for low-resource languages not covered in the current evaluation?
- Basis in paper: [inferred] The paper acknowledges that WMT evaluations primarily focus on high-resource languages and cannot conclude if the method will perform well on low-resource languages
- Why unresolved: The current evaluation only covers 15 high-resource languages paired with English. The performance characteristics, error detection accuracy, and robustness of the method on truly low-resource languages or those from different language families remain unknown
- What evidence would resolve it: Comprehensive evaluation on a diverse set of low-resource languages across different families, comparing performance metrics and error detection accuracy against established benchmarks

### Open Question 2
- Question: What is the long-term stability and consistency of GEMBA-MQM's performance given the potential for GPT model updates and fluctuations over time?
- Basis in paper: [explicit] The paper explicitly warns about GPT model updates potentially affecting results, citing Chen et al. (2023) showing performance fluctuations throughout 2023
- Why unresolved: The authors cannot guarantee model availability or consistency across updates, and there's no established methodology for tracking or compensating for these changes
- What evidence would resolve it: Longitudinal studies tracking performance metrics across multiple GPT model versions over extended periods, with documented changes and their impact on evaluation consistency

### Open Question 3
- Question: How would GEMBA-MQM perform when implemented with open-source LLMs like Llama 2 compared to proprietary models, and would this address the concerns about academic usage?
- Basis in paper: [explicit] The authors propose exploring Llama 2 as a more open alternative and acknowledge this could enable broader academic usage if superior performance is confirmed
- Why unresolved: No comparative evaluation has been conducted between GPT-based and Llama 2-based implementations of the same methodology
- What evidence would resolve it: Direct performance comparison studies between GEMBA-MQM implementations using GPT-4 versus Llama 2 across multiple test sets, measuring both accuracy and stability metrics

## Limitations
- Heavy reliance on proprietary GPT-4 model with uncertain future availability and potential performance changes
- Limited evaluation scope restricted to high-resource language pairs (15 highest resource languages)
- Potential bias from GPT-4's training data affecting error classification consistency

## Confidence
- High: Core claim that GPT-4 can effectively detect translation errors using few-shot prompting with MQM classification
- Medium: Language-agnostic approach generalization across diverse language families
- Low: Long-term stability claims given proprietary model dependencies

## Next Checks
1. Test GEMBA-MQM on a diverse set of low-resource language pairs to verify the language-agnostic approach holds beyond high-resource languages
2. Re-run the evaluation using different GPT-4 model versions or checkpoints to assess consistency over time
3. Compare error detection accuracy between GPT-4 and open-source alternatives with similar few-shot prompting to quantify the proprietary advantage