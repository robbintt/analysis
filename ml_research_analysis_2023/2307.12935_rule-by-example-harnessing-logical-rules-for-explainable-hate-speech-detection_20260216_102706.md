---
ver: rpa2
title: 'Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection'
arxiv_id: '2307.12935'
source_url: https://arxiv.org/abs/2307.12935
tags:
- rule
- rules
- ruleset
- hate
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of hate speech detection, addressing
  the trade-off between rule-based methods (interpretable but fragile) and deep learning
  (powerful but opaque). The proposed Rule By Example (RBE) framework uses a dual-encoder
  architecture with contrastive learning to combine logical rules and exemplars, enabling
  rule-grounded predictions.
---

# Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection

## Quick Facts
- arXiv ID: 2307.12935
- Source URL: https://arxiv.org/abs/2307.12935
- Reference count: 9
- Primary result: RBE framework combines logical rules and exemplars with contrastive learning to achieve up to 4% higher F1-score than deep learning baselines in hate speech detection

## Executive Summary
This paper addresses the trade-off between rule-based and deep learning approaches for hate speech detection by proposing Rule By Example (RBE), a dual-encoder architecture that uses contrastive learning to align rule embeddings with semantically similar text embeddings. RBE learns rule and text representations that maximize similarity for matching labels while minimizing it for mismatches, enabling both superior performance and rule-grounded interpretability. The framework achieves up to 4% F1-score improvement over state-of-the-art deep learning classifiers while providing transparent, customizable predictions through exemplar-based rule grounding.

## Method Summary
RBE employs a dual-encoder architecture with separate Rule Encoder and Text Encoder components that learn representations through contrastive learning. The framework encodes concatenated exemplars for each rule and input texts into fixed-size embeddings, then applies a contrastive loss to maximize similarity between matching rule-text pairs while minimizing it for mismatches. At inference, RBE efficiently retrieves the nearest rule embeddings and provides explanations by showing the originating rule and its exemplars. The approach enables rule-grounded predictions that combine the interpretability of logical rules with the power of learned representations.

## Key Results
- Achieves up to 4% higher F1-score than state-of-the-art deep learning classifiers on three hate speech datasets
- Outperforms baselines in both supervised and unsupervised settings
- Provides rule-grounded predictions that enable interpretability and customization
- Demonstrates efficient inference through pre-indexed rule embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning aligns rule embeddings with semantically similar text embeddings while pushing apart dissimilar pairs.
- Mechanism: The dual-encoder architecture encodes rules and texts separately; a contrastive loss maximizes similarity for matching labels and minimizes it for mismatches.
- Core assumption: Cosine similarity in the embedding space meaningfully captures semantic equivalence between rules and texts.
- Evidence anchors:
  - [abstract] "Through the use of contrastive learning, our framework uses a semantic similarity objective that pairs hateful examples with clusters of rule exemplars that govern it."
  - [section] "We employ a contrastive loss (Hadsell et al., 2006) to learn the embedding representations for our rule and text encoder. Contrastive learning encourages the model to maximize the representation similarity between same-label examples and to minimize it for different-label examples."
  - [corpus] Weak - corpus does not directly validate the effectiveness of contrastive learning for rule grounding.
- Break condition: If embeddings do not preserve semantic relationships, the contrastive objective will fail to align rules with correct texts, leading to poor generalization.

### Mechanism 2
- Claim: Rule-grounding provides interpretability by tracing predictions to the originating rule and its exemplars.
- Mechanism: After prediction, the system searches for rules that fire on the input and retrieves nearest exemplars to explain the decision.
- Core assumption: Exemplar-based rule-grounding is sufficient for human users to understand model decisions.
- Evidence anchors:
  - [abstract] "RBE is capable of providing rule-grounded predictions, allowing for more explainable and customizable predictions compared to typical deep learning-based approaches."
  - [section] "By taking an embeddings-based approach to learning representations, RBE enables what we define as rule-grounding. Rule-grounding enables us to trace our model predictions back to the explainable ruleset accompanied by the exemplars that define each rule."
  - [corpus] Weak - corpus does not provide evidence that rule-grounding improves user trust or understanding.
- Break condition: If the retrieved exemplars are not representative or the rule explanations are too abstract, users may not find the grounding helpful.

### Mechanism 3
- Claim: Dual-encoder pre-indexing allows efficient inference by avoiding re-encoding rules for each input.
- Mechanism: Rules and exemplars are encoded once during training and stored; at inference, only the input text is encoded and compared via cosine similarity.
- Core assumption: Pre-encoding rules is sufficient for capturing all necessary semantic nuances without re-encoding per input.
- Evidence anchors:
  - [section] "Our architecture consists of a Rule Encoder Θr and a Text Encoder Θt... This Dual Encoder architecture enables pre-indexing of exemplars allowing for faster inference at runtime after training."
  - [section] "Using the form xe = [CLS] e1_1, ..., e1_m, [SEP], en_1, ...., en_k, we then use rule encoder Θr to encode xe into hidden states..."
  - [corpus] Weak - corpus does not provide latency or efficiency benchmarks.
- Break condition: If rule semantics drift significantly with new inputs, pre-encoded representations may become outdated, reducing accuracy.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To align rule and text embeddings based on semantic similarity rather than supervised labels alone.
  - Quick check question: What loss function encourages similar representations for matching labels and dissimilar for non-matching?
- Concept: Embedding-based similarity
  - Why needed here: To compare rule and text representations without requiring explicit feature engineering.
  - Quick check question: Which distance metric is used in the paper to measure similarity between embeddings?
- Concept: Dual-encoder architecture
  - Why needed here: To allow separate encoding of rules and texts, enabling pre-indexing and efficient retrieval.
  - Quick check question: What is the advantage of encoding rules once versus encoding them per input?

## Architecture Onboarding

- Component map:
  Rule Encoder (Θr) -> Text Encoder (Θt) -> Contrastive Loss Layer -> Rule-Grounding Module
- Critical path:
  1. Concatenate exemplars for each rule.
  2. Encode concatenated exemplars via Rule Encoder.
  3. Encode input text via Text Encoder.
  4. Compute cosine similarity between rule and text embeddings.
  5. Apply contrastive loss to update encoders.
  6. For inference, match input to nearest rule embeddings and retrieve corresponding exemplars.
- Design tradeoffs:
  - Using a single exemplar per rule vs. multiple exemplars: Simpler but may reduce robustness to rule ambiguity.
  - Pre-training sentence encoders vs. training from scratch: Faster convergence but may inherit bias.
  - Rule elimination (Distance strategy) vs. representation learning (Mean/Concat strategies): Trade-off between rule pruning and rule adaptation.
- Failure signatures:
  - High precision but low recall: Rules too restrictive; may need exemplar expansion.
  - Low precision but high recall: Rules too broad; may need rule elimination or stricter thresholds.
  - Slow inference: Rule encoder may need optimization or caching.
- First 3 experiments:
  1. Test contrastive loss with synthetic rule-text pairs to verify embedding alignment.
  2. Evaluate rule-grounding by manually inspecting traced predictions for interpretability.
  3. Benchmark inference latency with and without pre-indexing to confirm efficiency gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of rules and exemplars affect the performance of the derived Dual Encoder model?
- Basis in paper: [explicit] The paper mentions that the quality of rules and exemplars could affect downstream Dual Encoder model quality, especially in the unsupervised setting.
- Why unresolved: The paper acknowledges this potential limitation but does not provide a systematic study on the effect of rule and exemplar quality on model performance.
- What evidence would resolve it: Experiments varying the quality of rules and exemplars (e.g., using expert vs. non-expert rules, high-quality vs. low-quality exemplars) and measuring the impact on Dual Encoder model performance.

### Open Question 2
- Question: Can the Rule by Example method be extended to handle multi-label classification tasks?
- Basis in paper: [inferred] The current framework is designed for binary classification (hateful vs. non-hateful). Hate speech often involves multiple types or categories.
- Why unresolved: The paper focuses on binary classification and does not explore multi-label scenarios.
- What evidence would resolve it: Adapting the framework to handle multiple labels and evaluating its performance on multi-label hate speech datasets.

### Open Question 3
- Question: How does the Rule by Example method perform on languages other than English?
- Basis in paper: [inferred] The paper only evaluates the method on English datasets.
- Why unresolved: The paper does not provide any cross-lingual experiments or analysis.
- What evidence would resolve it: Evaluating the method on multilingual hate speech datasets and comparing its performance across different languages.

## Limitations
- Heavy reliance on high-quality rulesets that are difficult to construct and validate
- Performance improvements depend on ruleset quality and may not generalize to new datasets
- Interpretability claims lack empirical validation through user studies
- No ablation studies to isolate the contribution of contrastive learning versus simpler similarity measures

## Confidence
- **High confidence**: The dual-encoder architecture and basic contrastive learning framework are well-established techniques that work as described
- **Medium confidence**: The reported performance improvements over baselines, as these depend heavily on ruleset quality which varies
- **Low confidence**: The interpretability benefits, as no user study validates whether rule-grounding meaningfully helps human understanding

## Next Checks
1. **Ruleset robustness test**: Systematically degrade ruleset quality (e.g., remove 25%, 50% of rules) and measure performance degradation to quantify dependence on ruleset completeness
2. **Cross-dataset transfer**: Train on one hate speech dataset with its ruleset, then evaluate on another dataset without retraining to assess generalization beyond the training distribution
3. **Human evaluation study**: Recruit human annotators to assess whether rule-grounded explanations (with exemplars) improve their understanding and trust compared to standard deep learning predictions without explanations