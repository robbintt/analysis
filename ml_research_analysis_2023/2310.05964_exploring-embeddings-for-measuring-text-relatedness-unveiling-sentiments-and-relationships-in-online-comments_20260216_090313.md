---
ver: rpa2
title: 'Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments
  and Relationships in Online Comments'
arxiv_id: '2310.05964'
source_url: https://arxiv.org/abs/2310.05964
tags:
- sentiment
- comments
- embeddings
- platforms
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed sentiment and semantic relationships across
  social media platforms using word embeddings. It employed BERT-based models to process
  text from YouTube, Reddit, Twitter, and Amazon reviews, aiming to uncover public
  opinion trends.
---

# Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments and Relationships in Online Comments

## Quick Facts
- arXiv ID: 2310.05964
- Source URL: https://arxiv.org/abs/2310.05964
- Reference count: 32
- Primary result: SiEBERT achieved 99.37% sentiment classification accuracy and a 0.83 silhouette score in clustering

## Executive Summary
This study analyzes sentiment and semantic relationships across social media platforms using word embeddings. The research employs BERT-based models to process text from YouTube, Reddit, Twitter, and Amazon reviews, aiming to uncover public opinion trends. SiEBERT demonstrated exceptional performance with 99.37% sentiment classification accuracy and 0.83 silhouette score in clustering. K-means clustering revealed distinct but limited interconnection among online viewpoints. KL-Divergence showed a 0.87 divergence in Amazon review language between 2004-2008 and 2009-2012, indicating evolving sentiment expression. The findings suggest the internet exhibits balanced sentiment but limited semantic cohesion, with potential applications for analyzing political sentiment and cross-platform biases.

## Method Summary
The study collected over 2.6 million data points from YouTube, Reddit, Twitter, and Amazon reviews, then applied comprehensive data cleaning to remove noise like emojis, hashtags, and URLs while normalizing text. BERT-based models including SiEBERT, ALBERT, and RoBERTa were used to generate text embeddings for sentiment analysis. K-means clustering was performed on these embeddings to reveal semantic relationships, while KL-Divergence measured semantic changes across different time periods within the Amazon review dataset. The research evaluated sentiment classification accuracy, clustering quality through silhouette scores, and semantic divergence patterns.

## Key Results
- SiEBERT achieved 99.37% sentiment classification accuracy across diverse social media platforms
- K-means clustering achieved a 0.83 silhouette score, revealing distinct but limited interconnection among online viewpoints
- KL-Divergence showed 0.87 divergence in Amazon review language between 2004-2008 and 2009-2012, indicating evolving sentiment expression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT-based embeddings capture context-dependent sentiment with high accuracy, enabling reliable sentiment classification across diverse social media platforms.
- Mechanism: BERT's bidirectional masked language modeling learns deep contextual relationships between words in a sentence, allowing it to generate embeddings that reflect nuanced sentiment information. This is superior to models like FastText, which rely on simpler n-gram statistics and struggle with complex language.
- Core assumption: Context is critical for accurate sentiment classification, and models that capture this context will outperform those that do not.
- Evidence anchors:
  - [abstract] "We made use of popular natural language processing models like Bidirectional Encoder Representations from Transformers (BERT) to analyze sentiments and explore relationships between comment embeddings."
  - [section] "BERT is unique in its use of bidirectional context analysis, with a novel approach of Masked Language Model- ing (MLM) that allows BERT to predict any missing word in a sentence."
- Break condition: If the context captured by BERT embeddings is not sufficiently relevant to the specific sentiment being analyzed, or if the dataset contains too much noise or slang that BERT's pre-training did not cover, the accuracy will degrade.

### Mechanism 2
- Claim: K-means clustering of embeddings reveals semantic relationships and distinct online viewpoints, indicating limited semantic cohesion across the internet.
- Mechanism: By representing comments as vectors in a high-dimensional space, K-means can group similar comments into clusters. The number of clusters and their quality (measured by silhouette score) reflect the diversity of viewpoints and semantic relationships in the data. A low number of well-separated clusters suggests limited semantic cohesion.
- Core assumption: Comments with similar meanings and sentiments will be close together in the embedding space, allowing K-means to effectively group them.
- Evidence anchors:
  - [abstract] "K-means clustering revealed distinct but limited interconnection among online viewpoints."
  - [section] "Clustering was utilized in order to place each comment in a specific category or 'cluster'...The number of clusters generated reveals how the comments varied..."
- Break condition: If the embeddings do not adequately capture semantic relationships, or if the data is too diverse or noisy, K-means may produce overlapping or meaningless clusters, leading to incorrect conclusions about semantic cohesion.

### Mechanism 3
- Claim: KL-Divergence quantifies the semantic and sentiment divergence between different time periods or platforms, revealing evolving language patterns.
- Mechanism: KL-Divergence measures the difference between two probability distributions. By treating the distribution of words or sentiments in different time periods or platforms as probability distributions, KL-Divergence can quantify how much they have diverged. A high KL-Divergence indicates significant changes in language use or sentiment expression.
- Core assumption: The distribution of words or sentiments in text data can be modeled as a probability distribution, and changes in this distribution reflect changes in language or sentiment.
- Evidence anchors:
  - [abstract] "KL-Divergence showed a 0.87 divergence in Amazon review language between 2004-2008 and 2009-2012, indicating evolving sentiment expression."
  - [section] "Finally, we measured the relative entropy, or Kl-Divergence of two probability distributions...Experiments involving Kl-Divergence revealed themes in terms of sentiment and semantics across specific time periods."
- Break condition: If the assumptions about modeling text as probability distributions are violated, or if the time periods or platforms being compared are not sufficiently different, KL-Divergence may not accurately reflect semantic or sentiment divergence.

## Foundational Learning

- Concept: Natural Language Processing (NLP) and Word Embeddings
  - Why needed here: NLP techniques and word embeddings are fundamental to understanding and analyzing text data. The paper relies on these concepts to process and represent social media comments in a way that allows for sentiment analysis and relationship measurement.
  - Quick check question: What is the difference between word embeddings and sentence embeddings, and why might you choose one over the other for a particular task?

- Concept: Clustering and Dimensionality Reduction
  - Why needed here: Clustering is used to group similar comments together and reveal semantic relationships, while dimensionality reduction (e.g., PCA) is used to simplify the high-dimensional embedding space and extract the most important features. These techniques are essential for understanding the structure and diversity of online viewpoints.
  - Quick check question: How does the elbow method help determine the optimal number of clusters in K-means, and what does the silhouette score tell you about the quality of the clusters?

- Concept: Probability Distributions and Information Theory
  - Why needed here: KL-Divergence is based on the concept of probability distributions and information theory. Understanding these concepts is crucial for interpreting the results of KL-Divergence and drawing conclusions about semantic and sentiment divergence over time or across platforms.
  - Quick check question: What does a KL-Divergence of 0 indicate about two probability distributions, and how does this relate to the semantic or sentiment similarity of two sets of text data?

## Architecture Onboarding

- Component map: Data Collection -> Data Cleaning -> Embedding Generation -> Sentiment Analysis/Clustering/KL-Divergence -> Visualization and Analysis
- Critical path: Data Collection → Data Cleaning → Embedding Generation → Sentiment Analysis/Clustering/KL-Divergence → Visualization and Analysis
- Design tradeoffs:
  - Model selection: BERT-based models (SiEBERT) offer high accuracy but are computationally expensive, while simpler models (FastText) are faster but less accurate.
  - Clustering granularity: More clusters provide finer-grained distinctions but may be harder to interpret, while fewer clusters offer a broader overview but may miss nuanced relationships.
  - KL-Divergence time periods: Comparing longer time periods may reveal more significant changes but may also introduce more noise, while shorter periods may be less stable.
- Failure signatures:
  - Low sentiment analysis accuracy: May indicate issues with the embedding model or the complexity of the language in the data.
  - Poor clustering quality (low silhouette score): May suggest that the embeddings do not adequately capture semantic relationships or that the data is too diverse or noisy.
  - High KL-Divergence without clear interpretation: May indicate that the assumptions about modeling text as probability distributions are violated or that the time periods/platforms being compared are not sufficiently different.
- First 3 experiments:
  1. Run sentiment analysis on a small, labeled dataset using SiEBERT and FastText to compare their accuracy and understand the tradeoff between complexity and performance.
  2. Perform K-means clustering on a subset of the data using SiEBERT embeddings and experiment with different numbers of clusters to find the optimal balance between granularity and interpretability.
  3. Calculate KL-Divergence between two short time periods of Amazon reviews using SiEBERT embeddings to understand how the distribution of words or sentiments changes over time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does sentiment evolve across different platforms over time, and what factors drive these changes?
- Basis in paper: [explicit] The study used KL-Divergence to show a 0.87 divergence in Amazon review language between 2004-2008 and 2009-2012, indicating evolving sentiment expression.
- Why unresolved: The paper only analyzed Amazon reviews across two time periods. It did not compare sentiment evolution across multiple platforms or investigate the underlying factors driving changes.
- What evidence would resolve it: Longitudinal analysis of sentiment across multiple platforms, coupled with metadata on external events, platform updates, or user demographics that could explain shifts.

### Open Question 2
- Question: Do highly rated comments on social media influence the sentiment of subsequent comments, creating feedback loops?
- Basis in paper: [inferred] The paper mentions analyzing comment sections of individual videos to study divergence or convergence trends over time, but does not explore the impact of top comments.
- Why unresolved: The paper did not investigate the causal relationship between highly visible comments and the tone of subsequent user responses.
- What evidence would resolve it: Experiments comparing sentiment trajectories in comment threads with and without highly rated initial comments, controlling for topic and user base.

### Open Question 3
- Question: How do regional and cultural differences impact the effectiveness of sentiment analysis models like SiEBERT?
- Basis in paper: [explicit] The paper suggests extending the study to explore sentiment variations across global regions, particularly for analyzing political sentiment.
- Why unresolved: The current analysis did not account for linguistic or cultural nuances that could affect model performance in different regions.
- What evidence would resolve it: Comparative evaluation of SiEBERT's accuracy on sentiment classification across datasets from diverse linguistic and cultural contexts, with error analysis to identify failure modes.

## Limitations

- The paper's claims about sentiment balance across platforms rely on dataset composition assumptions that are not fully disclosed, making it difficult to assess whether observed results reflect true public opinion or sampling artifacts.
- The KL-Divergence results showing semantic divergence over time depend heavily on the quality and representativeness of the Amazon review datasets used, and may not generalize to other platforms or time periods.
- The clustering analysis indicates distinct but limited interconnection among viewpoints, but does not explore whether this reflects actual semantic isolation or simply the limitations of the K-means algorithm with high-dimensional embeddings.

## Confidence

- **High Confidence**: The technical methodology for sentiment classification using BERT-based models is well-established, and the reported 99.37% accuracy is plausible given the strong performance of SiEBERT on similar tasks.
- **Medium Confidence**: The clustering analysis and its interpretation of limited semantic cohesion across the internet are reasonable but depend on the quality of the embeddings and the appropriateness of K-means for this high-dimensional data.
- **Low Confidence**: The broader claims about "the internet" having balanced sentiment and limited semantic cohesion are overreaching, as the study's sample of social media platforms and Amazon reviews may not be representative of the entire internet.

## Next Checks

1. Reconstruct the dataset composition and time periods for each platform to verify that the samples are representative and free from temporal or demographic biases that could affect sentiment and semantic analyses.

2. Evaluate the quality of the SiEBERT embeddings by performing downstream tasks (e.g., semantic similarity, analogy completion) to ensure that they adequately capture the nuances of social media language and slang.

3. Repeat the clustering analysis using alternative methods (e.g., DBSCAN, hierarchical clustering) and dimensionality reduction techniques (e.g., t-SNE, UMAP) to confirm the robustness of the observed semantic relationships and the validity of the "limited interconnection" conclusion.