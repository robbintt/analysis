---
ver: rpa2
title: 'CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and
  Infant Care'
arxiv_id: '2307.01458'
source_url: https://arxiv.org/abs/2307.01458
tags:
- https
- chinese
- generation
- question
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CARE-MI, a Chinese benchmark for evaluating
  misinformation in long-form generation by large language models on the sensitive
  topic of maternity and infant care. The authors construct the benchmark using a
  combination of knowledge graph triplets, medical exam QA pairs, and a synthetic
  data generation pipeline involving true/false statement generation and question
  generation.
---

# CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care

## Quick Facts
- arXiv ID: 2307.01458
- Source URL: https://arxiv.org/abs/2307.01458
- Authors: 
- Reference count: 40
- Primary result: Introduces CARE-MI, a Chinese benchmark for evaluating misinformation in long-form generation by LLMs on maternity and infant care

## Executive Summary
This paper introduces CARE-MI, a Chinese benchmark for evaluating misinformation in long-form generation by large language models (LLMs) on the sensitive topic of maternity and infant care. The authors construct the benchmark using a combination of knowledge graph triplets, medical exam QA pairs, and a synthetic data generation pipeline involving true/false statement generation and question generation. Experiments on multiple Chinese LLMs show that current models struggle with this domain, with GPT-4 achieving the highest correctness (0.867) and interpretability (0.928) scores among evaluated models, but still falling short of human expert performance (0.938, 0.973). The authors also develop automated judgment models for efficient evaluation, with a LLaMA-13B-T based model achieving 0.898 accuracy on correctness and 0.835 on interpretability.

## Method Summary
The CARE-MI benchmark is constructed through a multi-step process: knowledge graph triplets and medical exam QA pairs are transformed into true statements, which are then converted into true/false and open-ended questions. Expert annotators review the generated samples, and a final benchmark of 1,612 questions with human-selected references is created. Multiple Chinese LLMs are evaluated under zero-shot settings using correctness and interpretability metrics, and automated judgment models are trained using synthetic positive/negative answers and human evaluation results to enable efficient evaluation.

## Key Results
- GPT-4 achieves the highest correctness (0.867) and interpretability (0.928) scores among evaluated models
- Human experts outperform all models with scores of 0.938 (correctness) and 0.973 (interpretability)
- LLaMA-13B-T based judgment model achieves 0.898 accuracy on correctness and 0.835 on interpretability
- Models struggle particularly with open-ended questions compared to true/false questions
- Even the best models show significant performance gaps compared to human experts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graph triplets combined with MCQA datasets provide a strong foundation for generating diverse and domain-specific true statements.
- Mechanism: Knowledge graphs offer structured, factual triplets that can be transformed into declarative statements, while MCQA datasets contain more complex, nuanced questions that can be converted into true statements via QA2D transformations.
- Core assumption: The selected knowledge sources (BIOS, CPubMed, MLEC-QA, MEDQA) are sufficiently comprehensive and accurate for maternity and infant care domain knowledge.
- Evidence anchors:
  - [section] "We utilize two knowledge graph (KG) datasets and two MCQA datasets as our data sources... BIOS triplets are collected in English and later translated into Chinese [Luo et al., 2021]; translation quality is ensured by applying back-translation and filtering out samples with low confidence."
  - [section] "For samples from KG datasets, we build the true statements heuristically from the triplets with rule-based methods. For samples from MCQA datasets, we formalize the generation of true statements as a QA2D task where the model is required to perform sentence transformations to combine the question and answer into a declarative sentence [Demszky et al., 2018]."
  - [corpus] Weak - the corpus provides related papers but no direct evidence about the KG-MCQA combination effectiveness.
- Break condition: If the knowledge sources contain significant factual errors or if the QA2D transformation fails to preserve meaning, the true statement generation would produce unreliable data.

### Mechanism 2
- Claim: Expert annotation ensures benchmark quality by filtering out samples that don't meet specific criteria.
- Mechanism: Three medical-domain experts independently evaluate each generated sample against criteria like single correct answer, factual accuracy, and sufficient evidence, with a meta-annotator resolving disagreements.
- Core assumption: Expert annotators can reliably distinguish high-quality samples from low-quality ones using the provided guidelines.
- Evidence anchors:
  - [section] "We hire two medical-domain experts as annotators to review the generated samples and a third meta-annotator to arbitrate their disagreement... We ask annotators to evaluate 5,779 synthetic samples, and we discard those that receive two or more negative evaluations."
  - [section] "The annotators are instructed to decide whether a question is fluent, factually correct, only having a single correct answer, and containing enough retrieved knowledge, among other criteria."
  - [corpus] Weak - no direct corpus evidence about expert annotation effectiveness.
- Break condition: If annotators are inconsistent or if the annotation guidelines are ambiguous, the quality filtering would be unreliable.

### Mechanism 3
- Claim: Automated judgment models trained on synthetic positive/negative answers plus human evaluations can accurately assess LLM output correctness and interpretability.
- Mechanism: Judgment models are fine-tuned using generated true/false statements as synthetic answers and human evaluation results as labels, learning to mimic human judgment on correctness and interpretability metrics.
- Core assumption: The synthetic positive/negative answers capture the key features that distinguish correct from incorrect responses.
- Evidence anchors:
  - [section] "We use 1) both generated true and false statements as synthetic positive and negative answers, and 2) the human evaluation results, to train all judgment models."
  - [section] "We observe that BERT-Large is not able to learn anything useful... Larger models tend to perform better... For the final judgment model, we select LLaMA-13B-T as the backbone as it outperforms all other models and we train the model again but with all available data."
  - [corpus] Weak - corpus provides related papers but no direct evidence about judgment model effectiveness.
- Break condition: If the synthetic answers don't adequately represent real LLM outputs, or if human evaluation labels are noisy, the judgment models would fail to generalize.

## Foundational Learning

- Concept: Knowledge Graph (KG) representation and reasoning
  - Why needed here: KG triplets form the backbone of true statement generation from BIOS and CPubMed datasets
  - Quick check question: Given a triplet (head, relation, tail), how would you transform it into a declarative sentence about maternity care?

- Concept: QA to Declarative (QA2D) transformation
  - Why needed here: MCQA datasets require converting question-answer pairs into declarative statements for true statement generation
  - Quick check question: Convert the question "What is the only immunoglobulin that can pass through the placenta?" with answer "IgG" into a declarative statement

- Concept: Prompt engineering for LLM-controlled generation
  - Why needed here: Both question generation and statement generation rely on carefully crafted prompts to control LLM outputs
  - Quick check question: What prompt modifications would you make to ensure a LLM generates only true/false questions from a given statement?

## Architecture Onboarding

- Component map: Data sources (BIOS, CPubMed, MLEC-QA, MEDQA) -> True statement generation -> Question generation -> Knowledge retrieval -> Expert annotation -> Benchmark -> LLM evaluation -> Judgment model training -> Automated evaluation
- Critical path: Data collection -> True statement generation -> Question generation -> Expert annotation (quality control) -> Benchmark creation -> LLM evaluation -> Judgment model development
- Design tradeoffs: Using expert annotation ensures quality but is expensive and slow; using synthetic data speeds up process but may introduce noise; judgment models provide automation but require careful training
- Failure signatures: Low correlation between automated metrics and human evaluation; poor performance on OE questions compared to TF questions; significant performance gap between models and human experts
- First 3 experiments:
  1. Evaluate different prompt strategies for true statement generation from KG triplets
  2. Compare different LLM models (GPT-3.5-turbo, ChatGLM-6B, ChatYuan) for question generation quality
  3. Test judgment model performance with different architectures (BERT, GPT variants, LLaMA) on synthetic evaluation data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of LLMs on complex reasoning tasks be improved in knowledge-intensive domains like maternity and infant care?
- Basis in paper: [explicit] The paper found that current models struggle with more complicated reasoning, particularly on open-ended questions, and that even the best models (GPT-4) are not comparable to human expert performance.
- Why unresolved: The paper identifies the problem but does not provide solutions for improving model performance on complex reasoning tasks in knowledge-intensive domains.
- What evidence would resolve it: Experiments comparing different approaches to improve LLM performance on complex reasoning tasks in knowledge-intensive domains, such as fine-tuning on more domain-specific data, using larger models, or incorporating external knowledge bases.

### Open Question 2
- Question: How can automated metrics for evaluating LLM-generated text be made more accurate and transferable across tasks and languages?
- Basis in paper: [explicit] The paper discusses the limitations of traditional automated metrics like BLEU and ROUGE, and explores using judgment models as a proxy for human evaluation, but notes that trained models cannot transfer across different tasks and languages.
- Why unresolved: The paper does not provide a definitive solution for creating automated metrics that are both accurate and transferable across tasks and languages.
- What evidence would resolve it: Experiments comparing the performance of different automated metrics and judgment models across multiple tasks and languages, and identifying the factors that contribute to their transferability.

### Open Question 3
- Question: How can the accuracy of information in benchmarks like CARE-MI be maintained over time as medical knowledge and practices evolve?
- Basis in paper: [inferred] The paper mentions that the accuracy of information in the benchmark may decrease over time as medical knowledge and practices evolve, but does not provide a solution for maintaining its accuracy.
- Why unresolved: The paper does not address how to update the benchmark to reflect changes in medical knowledge and practices over time.
- What evidence would resolve it: A framework for regularly updating benchmarks like CARE-MI to reflect changes in medical knowledge and practices, and experiments evaluating the effectiveness of this framework in maintaining the accuracy of the benchmark over time.

## Limitations

- Benchmark generalizability to other sensitive domains beyond maternity and infant care remains untested
- Synthetic data generation pipeline relies heavily on LLMs with unknown error rates in QA2D transformation
- Judgment models show promising performance but were only evaluated against human judgments on a subset of data

## Confidence

- **High confidence**: The benchmark construction methodology and data collection process are well-documented and follow established practices for creating evaluation benchmarks.
- **Medium confidence**: The experimental results showing current LLMs' struggles with the domain are reliable, though the absolute performance differences between models may be influenced by evaluation methodology choices.
- **Low confidence**: Claims about the judgment models' transferability to other domains and low-resourced languages are speculative without empirical validation.

## Next Checks

1. **Cross-domain validation**: Test the judgment models on misinformation evaluation tasks in completely different domains (e.g., financial advice, legal information) to verify claimed transferability.

2. **Error analysis**: Conduct detailed error analysis on the judgment model failures to identify systematic biases or limitations in the synthetic training data approach.

3. **Human evaluation scalability**: Measure the correlation between automated judgments and human evaluations across different subsets of the benchmark to quantify the reliability of full automation for large-scale evaluation.