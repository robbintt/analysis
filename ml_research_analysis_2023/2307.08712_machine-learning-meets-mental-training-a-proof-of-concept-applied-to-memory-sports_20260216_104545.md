---
ver: rpa2
title: Machine Learning Meets Mental Training -- A Proof of Concept Applied to Memory
  Sports
arxiv_id: '2307.08712'
source_url: https://arxiv.org/abs/2307.08712
tags:
- data
- figure
- training
- memory
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study applied machine learning to optimize speed-accuracy\
  \ balance in competitive memory sports. Two programs were developed: (1) a linear\
  \ regression model predicting ideal memorization quantity from past scores and correct\
  \ digits in IAM competitions (133 data points, R\xB2 = 0.93, MAE = 16), and (2)\
  \ hyperbola-based regression modeling individual Memory League performance curves\
  \ (304 data points, MAE = 1.35)."
---

# Machine Learning Meets Mental Training -- A Proof of Concept Applied to Memory Sports

## Quick Facts
- arXiv ID: 2307.08712
- Source URL: https://arxiv.org/abs/2307.08712
- Reference count: 0
- Two machine learning programs successfully optimized speed-accuracy balance in memory sports with R² = 0.93 and MAE = 1.35

## Executive Summary
This study demonstrates the application of machine learning to optimize performance in competitive memory sports. Two programs were developed: one predicting ideal memorization quantity for IAM competitions using linear regression (133 data points, R² = 0.93), and another modeling individual Memory League performance curves using hyperbola-based regression (304 data points, MAE = 1.35). The work validates ML's potential for guiding training strategies and competition planning, though limitations include data sparsity for extreme performance ranges and current accessibility only to the researcher.

## Method Summary
The study employed two distinct machine learning approaches. Task I used multiple linear regression to predict perfect scores from IAM competition data by fitting a 3D plane to score, correct digits, and perfect score data. Task II applied hyperbola-based regression with median absolute error loss to model individual Memory League athlete performance curves. Data collection involved manual selection from athlete training logs and web scraping of match results, followed by cleaning, feature selection, and cross-validation to ensure robust performance estimates.

## Key Results
- Linear regression model achieved R² = 0.93 with MAE = 16 on IAM competition data
- Hyperbola-based regression model achieved MAE = 1.35 on Memory League individual performance curves
- Cross-validation confirmed model validity for small datasets
- Models successfully captured athlete-specific performance trends and enabled competitor comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The machine learning model can accurately predict the ideal memorization quantity for IAM competitions by capturing the linear relationship between score, correct digits, and perfect score.
- Mechanism: A plane in 3D space is fitted to the data points using multiple linear regression, allowing prediction of the perfect score given an imperfect attempt's score and correct digits.
- Core assumption: The relationship between score, correct digits, and perfect score is linear and consistent across different athletes.
- Evidence anchors:
  - [abstract] "Two programs were developed: (1) a linear regression model predicting ideal memorization quantity from past scores and correct digits in IAM competitions (133 data points, R² = 0.93, MAE = 16)"
  - [section] "The model selected was a multiple linear regression. The idea, in essence, was to fit a plane to the data (rather than a simple line), to be able to predict the perfect score based on the two input features of the score and the correct data of another attempt at the discipline."
  - [corpus] Weak evidence for broader ML applications in memory sports; this appears to be a unique application.
- Break condition: If the relationship between score, correct digits, and perfect score becomes non-linear or varies significantly between athletes, the model's accuracy will degrade.

### Mechanism 2
- Claim: Individual athlete performance curves in Memory League can be accurately modeled using hyperbola-based regression to predict score based on time taken.
- Mechanism: The hyperbola equation is fitted to the data points, capturing the asymptotic behavior of the performance curve as time approaches zero and the maximum score of 80.
- Core assumption: Athlete performance follows a hyperbolic curve, with accuracy declining as time taken decreases.
- Evidence anchors:
  - [abstract] "The second program, refined with median absolute error loss, accurately captured athlete-specific performance trends and enabled comparison between competitors."
  - [section] "The ultimate solution, in the end, came from realizing a particular feature was missing in all the functions currently used, and it was something the manual curvature tuning was actually trying to artificially replicate: an asymptote."
  - [corpus] Limited evidence for hyperbola-based regression in this specific domain; more common in other fields.
- Break condition: If athlete performance does not follow a hyperbolic curve or if the asymptote is not at 80, the model will not accurately predict scores.

### Mechanism 3
- Claim: Cross-validation provides a more reliable estimate of model performance when dealing with small datasets in memory sports applications.
- Mechanism: The dataset is partitioned into k folds, with each fold used as a validation set while the remaining folds are used for training. The performance is then averaged across all folds.
- Core assumption: Cross-validation reduces the dependency on a specific split of the data and provides a more robust estimate of the model's generalization performance.
- Evidence anchors:
  - [abstract] "Results validated the approach's potential to guide training strategies and competition planning, though limitations include data sparsity for extreme performance ranges"
  - [section] "Cross-validation appeared as a useful additional method to apply to check the validity of the results obtained. As anticipated in the previous chapter, in fact, this technique helps avoid common issues associated with small datasets by providing a more robust estimate of the model's performance, as it makes the most efficient use of the available data."
  - [corpus] Weak evidence for cross-validation in memory sports applications; more common in general machine learning.
- Break condition: If the dataset is too small or the performance measures do not stabilize with increasing k, cross-validation may not provide a reliable estimate.

## Foundational Learning

- Concept: Linear regression
  - Why needed here: To model the linear relationship between score, correct digits, and perfect score in IAM competitions.
  - Quick check question: What is the equation of a line in 3D space, and how does it relate to the multiple linear regression model used in this study?

- Concept: Hyperbola-based regression
  - Why needed here: To model the asymptotic behavior of athlete performance curves in Memory League as time approaches zero and the maximum score of 80.
  - Quick check question: What is the general form of a hyperbola equation, and how does it capture the diminishing returns of speed in memory sports?

- Concept: Cross-validation
  - Why needed here: To provide a more reliable estimate of model performance when dealing with small datasets in memory sports applications.
  - Quick check question: How does k-fold cross-validation work, and why is it particularly useful when the dataset is small?

## Architecture Onboarding

- Component map: Data collection -> Data preparation -> Model selection and training -> Model testing and validation
- Critical path:
  1. Collect and prepare data from IAM competitions and Memory League matches
  2. Select and train appropriate machine learning models based on the data characteristics
  3. Evaluate and compare model performance using various metrics and cross-validation
  4. Interpret and apply the results to guide training strategies and competition planning
- Design tradeoffs:
  - Simplicity vs. accuracy: Linear regression models are simpler but may not capture complex relationships, while more complex models like random forests may overfit
  - Universality vs. specificity: Universal models can be applied to multiple athletes but may not capture individual differences, while athlete-specific models are more accurate but less generalizable
  - Data quantity vs. quality: Collecting more data can improve model performance but may introduce noise or bias if the data is not carefully selected and cleaned
- Failure signatures:
  - Poor performance measures (low R-squared, high MSE, MAE, MDAE, RMSE) indicate the model is not accurately capturing the underlying relationships in the data
  - Overfitting, characterized by low training error but high validation error, suggests the model is too complex and capturing noise in the training data
  - Underfitting, characterized by high training and validation error, suggests the model is too simple and not capturing the true relationships in the data
- First 3 experiments:
  1. Train a linear regression model on the IAM data and evaluate its performance using cross-validation and various metrics
  2. Train a hyperbola-based regression model on a single athlete's Memory League data and visualize the resulting performance curve
  3. Compare the performance of different regression models (linear, polynomial, logarithmic, hyperbola-based) on a subset of the Memory League data and select the best-performing model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the hyperbola-based regression model compare to more complex machine learning models (e.g., neural networks) for predicting individual Memory League athlete performance curves?
- Basis in paper: [inferred] The paper tested several models (linear regression, polynomial regression, decision trees, random forests, XGBoost, logarithmic regression, and hyperbola-based regression) and found the hyperbola-based regression to perform best among those tested. However, neural networks were not explored.
- Why unresolved: The paper did not test neural networks, which could potentially capture more complex patterns in the data.
- What evidence would resolve it: Conducting experiments comparing the hyperbola-based regression model to neural network models on the same dataset, using appropriate performance metrics.

### Open Question 2
- Question: How would the performance of the machine learning models change if data from solo training sessions was included in addition to rated head-to-head matches?
- Basis in paper: [explicit] The paper mentions that data was collected solely from rated head-to-head matches due to concerns about data availability and representativeness. It also notes that including solo training data could address limitations related to capturing extreme performance ranges.
- Why unresolved: The paper did not include solo training data in the analysis, so the impact of this data on model performance is unknown.
- What evidence would resolve it: Collecting and analyzing data from both solo training sessions and rated head-to-head matches, then comparing the performance of machine learning models trained on each dataset.

### Open Question 3
- Question: How effective are the proposed applications of machine learning to memory sports (e.g., optimizing memory palaces, analyzing systems, generating training plans) in practice, and what are the specific challenges in implementing them?
- Basis in paper: [explicit] The paper discusses potential applications of machine learning to memory sports, including optimizing memory palaces, analyzing systems, generating training plans, and providing real-time feedback during competitions. However, these applications are presented as ideas for future research and have not been implemented or tested.
- Why unresolved: The paper does not provide evidence of the effectiveness or feasibility of these applications in real-world scenarios.
- What evidence would resolve it: Developing and testing prototypes of these applications, gathering feedback from memory athletes, and conducting user studies to evaluate their impact on training and performance.

## Limitations
- IAM competition dataset contains only 133 records, creating potential overfitting risks despite R² = 0.93 performance
- Memory League data sparsity at performance extremes limits model reliability for outlier cases
- Hyperbola-based regression approach lacks extensive validation across diverse athlete populations

## Confidence

- Task I linear regression predictions: Medium - Strong in-sample performance but limited external validation
- Task II hyperbola-based modeling: Medium - Promising individual athlete fits but unproven generalizability
- Cross-validation methodology: High - Well-established statistical practice appropriate for small datasets

## Next Checks

1. **External validation**: Test both models on independent competition datasets from different time periods or geographic regions to assess true generalization capability.

2. **Performance stability analysis**: Systematically evaluate model performance across varying k-fold values in cross-validation to confirm stability and identify potential overfitting patterns.

3. **Athlete-specific calibration**: Collect additional data points for extreme performers (both novice and elite) to validate model performance at the boundaries where current predictions are least reliable.