---
ver: rpa2
title: 'Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex
  Visual Reasoning Tasks'
arxiv_id: '2307.16395'
source_url: https://arxiv.org/abs/2307.16395
tags:
- image
- language
- reasoning
- such
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of "bridge-architectures"
  - multi-modal models that project image embeddings into the text space of Large
  Language Models (LLMs) - on complex visual reasoning tasks. The authors focus on
  the NLVR2 dataset, which requires fine-grained analysis of image pairs and their
  captions.
---

# Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks

## Quick Facts
- arXiv ID: 2307.16395
- Source URL: https://arxiv.org/abs/2307.16395
- Reference count: 16
- Primary result: Bridge-architectures struggle with complex visual reasoning tasks like NLVR2, and object-level features don't improve performance

## Executive Summary
This paper investigates the performance of bridge-architectures - multimodal models that project image embeddings into the text space of Large Language Models (LLMs) - on complex visual reasoning tasks. The authors focus on the NLVR2 dataset, which requires fine-grained analysis of image pairs and their captions. Through systematic experiments, they find that adding object-level features from segmentation models does not significantly improve performance, and that pre-training on multi-modal data is crucial for good results on complex reasoning tasks like NLVR2. The paper also presents initial results on LLaVA in a zero-shot setting and analyzes its performance characteristics.

## Method Summary
The authors evaluate bridge-architectures by combining image encoders (CLIP, MAE) with object features from SAM segmentation model, projecting these features into LLM text space via linear mapping, and fine-tuning on NLVR2 dataset. They test different LLM configurations (FLAN-T5, OPT) and evaluate zero-shot performance of LLaVA with and without Chain-of-Thought prompting. The methodology includes comparing global features alone versus global plus object-level features, and analyzing the impact of instruction finetuning on LLM reasoning capabilities.

## Key Results
- Adding object-level features from SAM to bridge architectures does not improve NLVR2 performance
- Pre-training on multimodal data is essential for bridge architectures to perform well on complex reasoning tasks
- Instruction finetuning of LLMs significantly improves zero-shot performance through better chain-of-thought reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding object-level features from segmentation models (SAM) to bridge architectures does not significantly improve NLVR2 performance
- Mechanism: NLVR2 requires complex reasoning over global image properties and fine-grained object relationships, but the object features from SAM lack semantic understanding of object categories and relationships necessary for the task
- Core assumption: Object-level features need to be semantically meaningful and properly aligned with text for effective reasoning
- Evidence anchors:
  - [abstract]: "Our analysis shows that adding object level features to bridge architectures does not help"
  - [section]: "Since NLVR2 requires extremely complex reasoning across multiple images and objects, we find the current bridge architectures lacking since they all use a global feature vector to represent the image, which would often be insufficient for NLVR2"
  - [corpus]: Weak evidence - related papers focus on different aspects of multimodal reasoning without directly testing object-level features in bridge architectures
- Break condition: If object features are semantically enriched through language supervision or better alignment mechanisms

### Mechanism 2
- Claim: Pretraining on multimodal data is crucial for bridge architectures to perform well on complex reasoning tasks
- Mechanism: Bridge architectures rely on frozen image and text encoders, and without multimodal pretraining, the linear projection cannot effectively align visual features with the text space of LLMs for complex reasoning
- Core assumption: Multimodal pretraining creates better feature representations that can be linearly mapped to text space
- Evidence anchors:
  - [abstract]: "our analysis shows that adding object level features to bridge architectures does not help, and that pre-training on multi-modal data is key for good performance on complex reasoning tasks such as NLVR2"
  - [section]: "we notice that using the instruction finetuned FLAN-T5 gives slightly better results than OPT although the improvement is not significant. This is expected as we see that our further analysis indicates that instruction finetuning helps"
  - [corpus]: Moderate evidence - related papers discuss importance of pretraining but don't specifically analyze bridge architectures
- Break condition: If alternative alignment methods (e.g., adapters, contrastive learning) can compensate for lack of pretraining

### Mechanism 3
- Claim: Multimodal instruction finetuning of LLMs is important for eliciting chain-of-thought reasoning in bridge architectures
- Mechanism: Instruction finetuned LLMs like FLAN-T5 have better zero-shot reasoning capabilities and can leverage chain-of-thought prompting to perform complex multimodal reasoning tasks
- Core assumption: LLMs need instruction finetuning to effectively use chain-of-thought prompting for multimodal tasks
- Evidence anchors:
  - [abstract]: "Our experiments with LLaV A demonstrate that instruction finetuning is very important for leveraging the Chain of Thought (CoT) reasoning of the LLM which helps the zero-shot performance quite a lot"
  - [section]: "Using a larger token length allows the model to build stronger reasoning about the images and hence choose the answer more accurately"
  - [corpus]: Weak evidence - related papers discuss instruction finetuning but not specifically for bridge architectures with CoT reasoning
- Break condition: If non-instruction finetuned LLMs can achieve similar performance with different prompting strategies

## Foundational Learning

- Concept: Linear projection as bridge between vision and language spaces
  - Why needed here: Bridge architectures use a single linear projection matrix to map image embeddings to the text space of LLMs, enabling zero-shot multimodal reasoning
  - Quick check question: What is the mathematical operation that transforms image features to match LLM input dimensions?

- Concept: Chain-of-thought prompting in LLMs
  - Why needed here: LLaVA and other instruction finetuned models can generate intermediate reasoning steps when prompted, improving performance on complex tasks like NLVR2
  - Quick check question: How does chain-of-thought prompting differ from direct answer generation in terms of intermediate outputs?

- Concept: Object detection and segmentation fundamentals
  - Why needed here: Understanding how models like SAM extract object-level features and their limitations in semantic understanding is crucial for interpreting why they don't help in bridge architectures
  - Quick check question: What information do object features typically contain, and what key information might they lack for complex reasoning?

## Architecture Onboarding

- Component map: Image encoder (CLIP, MAE, etc.) → Linear projection → LLM (FLAN-T5, OPT, etc.) → Output generation
- Critical path: Image encoding → Linear projection → LLM processing → Answer generation
- Design tradeoffs:
  - Single linear projection vs. more complex alignment methods (simpler but less expressive)
  - Frozen encoders vs. end-to-end training (more efficient but potentially less capable)
  - Global features vs. object features (simpler but may miss fine-grained details)
- Failure signatures:
  - Performance close to random guessing (poor alignment between vision and language spaces)
  - Systematic errors on object count or position questions (lack of fine-grained visual information)
  - Inconsistent performance across different question types (limited reasoning capabilities)
- First 3 experiments:
  1. Test bridge architecture with different image encoders (CLIP vs MAE) while keeping all other components fixed
  2. Evaluate impact of chain-of-thought prompting on zero-shot performance across different LLMs
  3. Compare performance of instruction finetuned vs non-finetuned LLMs on the same bridge architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does multimodal instruction finetuning of image encoders impact the performance of bridge architectures on complex visual reasoning tasks?
- Basis in paper: [explicit] The authors hypothesize that the lack of multimodal pretraining, especially for the image encoder, is a key limitation of bridge architectures, and suggest that pretraining the image encoder on multimodal instruction data could improve performance.
- Why unresolved: The authors did not have the resources to test this hypothesis by pretraining the image encoder of LLaVA on multimodal instruction data.
- What evidence would resolve it: Experiments comparing the performance of bridge architectures with and without multimodal instruction finetuning of the image encoder on complex visual reasoning tasks like NLVR2.

### Open Question 2
- Question: How does the choice of image encoder (e.g., CLIP vs. MAE) affect the performance of bridge architectures on complex visual reasoning tasks?
- Basis in paper: [explicit] The authors experimented with different image encoders (CLIP and MAE) and observed some differences in performance, suggesting that the choice of image encoder may impact the model's ability to capture the necessary visual information.
- Why unresolved: The authors did not perform an extensive comparison of different image encoders, and the impact of the choice of image encoder on complex visual reasoning tasks remains unclear.
- What evidence would resolve it: Systematic experiments comparing the performance of bridge architectures with different image encoders (e.g., CLIP, MAE, ViT) on complex visual reasoning tasks.

### Open Question 3
- Question: How does the fusion of multiple images into a single input affect the performance of bridge architectures on tasks like NLVR2?
- Basis in paper: [explicit] The authors note that LLaVA is designed to take a single image as input, and they fused the two images in NLVR2 into a single input. They suggest that this limitation might be a reason for some of LLaVA's failure cases.
- Why unresolved: The authors did not experiment with alternative methods of handling multiple images as input, such as concatenating the images or using a different model architecture.
- What evidence would resolve it: Experiments comparing the performance of bridge architectures with different methods of handling multiple images as input (e.g., fusion, concatenation, separate inputs) on tasks like NLVR2.

## Limitations
- Analysis is limited to NLVR2 dataset, which may not generalize to other complex reasoning tasks
- Object features from SAM lack semantic understanding and relationships between objects
- Only zero-shot performance of LLaVA is evaluated, without fine-tuning experiments

## Confidence
- High Confidence: Instruction finetuning improves zero-shot reasoning through chain-of-thought prompting
- Medium Confidence: Adding object features doesn't improve performance due to semantic limitations
- Medium Confidence: Multimodal pretraining is crucial but effect needs more direct isolation

## Next Checks
1. Implement semantically enriched object features with category labels and spatial relationships, comparing performance against current SAM-based features
2. Evaluate additional bridge-architecture variants including LLaVA with fine-tuning, OpenFlamingo, and VLMO on NLVR2
3. Test the best-performing bridge architecture on other visual reasoning datasets (GQA, VQA-CP, OK-VQA) to assess generalization beyond NLVR2