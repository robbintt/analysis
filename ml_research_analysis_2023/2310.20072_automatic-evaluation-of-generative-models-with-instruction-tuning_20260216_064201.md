---
ver: rpa2
title: Automatic Evaluation of Generative Models with Instruction Tuning
arxiv_id: '2310.20072'
source_url: https://arxiv.org/abs/2310.20072
tags:
- tasks
- task
- evaluation
- association
- hate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically evaluating
  natural language generation (NLG) systems, which has been difficult to achieve due
  to the limitations of existing reference-based metrics. The authors propose a novel
  approach using instruction tuning to train learned metrics that can assess NLG outputs
  across diverse tasks and evaluation criteria without relying on reference texts.
---

# Automatic Evaluation of Generative Models with Instruction Tuning

## Quick Facts
- arXiv ID: 2310.20072
- Source URL: https://arxiv.org/abs/2310.20072
- Reference count: 11
- Key outcome: Instruction tuning significantly improves automatic evaluation of NLG systems across diverse tasks

## Executive Summary
This paper addresses the challenge of automatically evaluating natural language generation (NLG) systems by proposing instruction-tuned metrics that can assess outputs across diverse tasks and criteria without reference texts. The authors create HEAP, a dataset of human judgments across 8 NLG tasks and 22 evaluation criteria, converted into pairwise comparisons. They fine-tune BART models using natural language instructions to predict scores, exploring single-task, multi-task, and cross-task training setups. Results show that instruction tuning substantially improves performance over base models, with multi-tasking yielding the best results.

## Method Summary
The authors create HEAP, a dataset of human judgments across 8 NLG tasks and 22 evaluation criteria, converted into pairwise comparisons. They fine-tune BART-base models using natural language instructions prepended to inputs, training the model to predict scores for generated outputs. The approach explores three training setups: single-task (fine-tuning on individual tasks), multi-task (joint training on multiple tasks), and cross-task (zero-shot evaluation on unseen tasks). Models are trained to maximize ranking accuracy using pairwise comparisons.

## Key Results
- Instruction tuning significantly improves performance over base models, with multi-task training yielding the best results
- Most evaluation criteria are learnable, though complex ones like answer validity are more challenging
- Cross-task performance varies significantly, with better results when training on similar tasks
- The proposed method achieves good correlation with human judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning improves generalization across NLG tasks by learning task-agnostic evaluation patterns.
- Mechanism: By prepending natural language instructions to inputs, the model learns to extract evaluation criteria from linguistic cues rather than task-specific features.
- Core assumption: Evaluation criteria like grammaticality or informativeness can be identified from task instructions alone, without task-specific training.
- Evidence anchors:
  - [abstract]: "Inspired by the generalization ability of instruction-tuned models..."
  - [section]: "Instruction tuning involves presenting the model with natural language instructions..."
  - [corpus]: Found 25 related papers; average neighbor FMR=0.484. Limited direct evidence for generalization via instructions alone.
- Break condition: If instructions are ambiguous or task-specific patterns dominate, generalization fails.

### Mechanism 2
- Claim: Multi-task training enhances performance by enabling transfer learning between related evaluation criteria.
- Mechanism: Joint training on multiple tasks allows the model to learn shared representations for evaluation, improving performance on tasks with limited data.
- Core assumption: Evaluation tasks share underlying patterns that can be learned jointly, even if they target different NLG tasks.
- Evidence anchors:
  - [section]: "Multi-tasking is beneficial... fine-tuning on multiple tasks... yields additional performance improvements."
  - [corpus]: Limited direct evidence; neighbor papers focus on task evaluation rather than multi-task learning.
- Break condition: If tasks are too dissimilar, transfer learning degrades performance.

### Mechanism 3
- Claim: Pairwise comparison training aligns model preferences with human judgments.
- Mechanism: Training on comparative pairs (good vs. bad outputs) teaches the model to rank outputs based on evaluation criteria.
- Core assumption: Human preferences for NLG outputs can be captured through relative comparisons rather than absolute scores.
- Evidence anchors:
  - [section]: "We decided to go with the comparative setup based on the findings of Askell et al. (2021)..."
  - [corpus]: Neighbor papers focus on evaluation but not pairwise comparison methodology.
- Break condition: If comparisons are ambiguous or criteria are subjective, alignment with human judgments weakens.

## Foundational Learning

- Concept: Natural Language Instructions
  - Why needed here: Instructions guide the model to understand evaluation criteria without task-specific training.
  - Quick check question: Can the model infer the correct evaluation from a simple instruction like "Check if this text is grammatical"?

- Concept: Pairwise Ranking Loss
  - Why needed here: The model must learn to prefer better outputs over worse ones.
  - Quick check question: Does the model output a higher score for grammatically correct text over incorrect text?

- Concept: Transfer Learning
  - Why needed here: Multi-task training leverages shared evaluation patterns across tasks.
  - Quick check question: Does training on grammaticality improve performance on other linguistic quality tasks?

## Architecture Onboarding

- Component map: Input text → BART encoder → BART decoder → Linear layer → Score prediction
- Critical path: Input → BART encoding → Linear layer → Score prediction → Loss computation
- Design tradeoffs:
  - Single-task vs. multi-task training: Multi-task improves generalization but increases training complexity
  - Instruction complexity: More detailed instructions may improve accuracy but require more data
- Failure signatures:
  - Random baseline performance: Indicates fine-tuning is essential
  - Poor cross-task performance: Suggests tasks are too dissimilar for transfer
- First 3 experiments:
  1. Fine-tune on a single task with clear instructions (e.g., grammaticality)
  2. Train multi-task on 2-3 related tasks to test transfer learning
  3. Evaluate cross-task zero-shot performance on a new task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of the instruction-tuned metric be improved for complex evaluation criteria like answer validity?
- Basis in paper: [explicit] The paper mentions that more nuanced or complex criteria, such as answer validity, are more difficult to learn than others, like grammaticality.
- Why unresolved: The paper does not provide a clear solution for improving the performance of the instruction-tuned metric on complex criteria. It only highlights the difficulty in learning these criteria.
- What evidence would resolve it: Evidence that demonstrates the effectiveness of additional training data, more sophisticated fine-tuning techniques, or the incorporation of domain-specific knowledge in improving the performance of the instruction-tuned metric on complex criteria would help resolve this question.

### Open Question 2
- Question: What are the potential biases in the HEAP dataset that may affect the performance of the instruction-tuned metric?
- Basis in paper: [inferred] The paper mentions that the HEAP dataset is based on annotators' judgements of model-generated outputs along various dimensions, and it's possible that some tasks involve inherent subjectivity, thus creating variance in the quality and consistency of the data for different tasks.
- Why unresolved: The paper does not provide a detailed analysis of the potential biases in the HEAP dataset or their impact on the performance of the instruction-tuned metric.
- What evidence would resolve it: Evidence that identifies and quantifies the potential biases in the HEAP dataset, as well as studies that investigate the impact of these biases on the performance of the instruction-tuned metric, would help resolve this question.

### Open Question 3
- Question: How can the cross-task setup be improved to achieve better generalization across diverse tasks and evaluation criteria?
- Basis in paper: [explicit] The paper mentions that the cross-task setup performs substantially worse than the multi-task setup, which is expected since the target task training data is excluded.
- Why unresolved: The paper does not provide a clear solution for improving the cross-task setup to achieve better generalization across diverse tasks and evaluation criteria.
- What evidence would resolve it: Evidence that demonstrates the effectiveness of advanced transfer learning techniques, such as meta-learning or domain adaptation, in improving the generalization capabilities of the instruction-tuned metric across diverse tasks and evaluation criteria would help resolve this question.

## Limitations

- Dataset covers only 8 NLG tasks and 22 evaluation criteria, limiting universal applicability claims
- Cross-task performance varies significantly, with some criteria showing minimal transferability
- Reliance on pairwise comparisons may not capture all aspects of human judgment
- Complex evaluation criteria like answer validity remain challenging to learn

## Confidence

- **High confidence**: Instruction tuning improves over baseline performance; multi-task training benefits most evaluation criteria
- **Medium confidence**: Cross-task generalization works for similar tasks; more complex criteria (answer validity) are harder to learn
- **Low confidence**: Claims about universal applicability across all NLG tasks; absolute performance metrics without baseline comparisons

## Next Checks

1. Test cross-task performance on a held-out task with minimal semantic overlap to HEAP tasks
2. Compare pairwise instruction-tuned metrics against absolute scoring approaches on the same dataset
3. Evaluate performance degradation when instructions are simplified or ambiguous