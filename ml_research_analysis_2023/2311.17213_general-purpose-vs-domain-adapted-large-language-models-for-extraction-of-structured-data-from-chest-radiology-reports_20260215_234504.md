---
ver: rpa2
title: General-Purpose vs. Domain-Adapted Large Language Models for Extraction of
  Structured Data from Chest Radiology Reports
arxiv_id: '2311.17213'
source_url: https://arxiv.org/abs/2311.17213
tags:
- system
- radling
- radiology
- gpt-4
- cdes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares a radiology-adapted language model (RadLing)
  with a general-purpose LLM (GPT-4) for extracting structured data from chest radiology
  reports. The RadLing system achieved an F1 score of 95% compared to GPT-4's 82%,
  with significantly higher sensitivity (94% vs 70%).
---

# General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Structured Data from Chest Radiology Reports

## Quick Facts
- **arXiv ID**: 2311.17213
- **Source URL**: https://arxiv.org/abs/2311.17213
- **Reference count**: 30
- **Primary result**: Domain-adapted model (RadLing) achieved 95% F1 vs 82% for general-purpose LLM (GPT-4) on chest radiology report extraction

## Executive Summary
This study compares a domain-adapted language model (RadLing) with a general-purpose LLM (GPT-4) for extracting structured data from chest radiology reports. RadLing, with its radiology-specific embeddings and lightweight architecture, achieved significantly higher sensitivity (94% vs 70%) and overall F1 score (95% vs 82%) compared to GPT-4. The domain-adapted model also demonstrated superior performance in differentiating between absent and unspecified findings, while offering operational advantages including local deployment capability and reduced runtime costs. These results demonstrate that specialized domain adaptation can outperform larger general-purpose models for structured data extraction from radiology reports.

## Method Summary
The study evaluated two systems on 1,300 anonymized chest radiology reports (900 training, 400 test) using 21 pre-selected CDEs from RadElement.org. The RadLing system employed a domain-adapted ELECTRA-based model (335M parameters) with radiology-specific embeddings, while the GPT-4 system used OpenAI's general-purpose embeddings and GPT-4 model. Both systems used cosine similarity for CDE identification and value mapping, with three radiologists annotating the test set of 2,741 sentences for evaluation.

## Key Results
- RadLing achieved 95% F1 score compared to GPT-4's 82% for structured data extraction
- RadLing demonstrated significantly higher sensitivity (94% vs 70%) in identifying relevant CDEs
- RadLing's lightweight value mapper achieved comparable precision to GPT-4 (95.4% vs 95.0%) while offering local deployment advantages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Domain-adapted embeddings outperform general-purpose embeddings in identifying relevant CDEs.
- **Mechanism**: RadLing's embeddings are trained on radiology-specific corpora, capturing nuanced terminology and contextual relationships unique to radiology reports.
- **Core assumption**: Radiology reports contain domain-specific language patterns and terminology that general-purpose embeddings cannot adequately capture.
- **Evidence anchors**:
  - [abstract]: "RadLing's domain-adapted embeddings were more effective in identifying relevant CDEs (95% vs 71% sensitivity)"
  - [section]: "RadLing system's domain-adapted embeddings were more sensitive than OpenAI's general-purpose text-embedding-ada-002 (95% vs 71%)"
- **Break condition**: If the radiology domain doesn't have sufficiently unique linguistic patterns.

### Mechanism 2
- **Claim**: Lightweight value mapping achieves comparable precision to GPT-4's complex reasoning.
- **Mechanism**: RadLing uses dependency tree parsing and semantic similarity techniques for value assignment, avoiding computational overhead of GPT-4's full reasoning pipeline.
- **Core assumption**: Structured radiology reports have predictable patterns that can be captured through syntactic analysis rather than deep reasoning.
- **Evidence anchors**:
  - [abstract]: "RadLing's lightweight value mapper achieved comparable precision to GPT-4 (95.4% vs 95.0%)"
  - [section]: "For the step of mapping identified CDEs to respective values... both systems exhibited high precision—95.4% and 95.0% respectively"
- **Break condition**: If radiology reports contain highly variable or complex value expressions.

### Mechanism 3
- **Claim**: Local deployment provides operational advantages over cloud-based LLMs.
- **Mechanism**: RadLing's smaller model size enables deployment on local CPUs, reducing costs and improving data privacy compared to GPT-4's 1.7T parameters requiring cloud infrastructure.
- **Core assumption**: The performance gains from RadLing's domain adaptation outweigh the benefits of larger, more general models when considering operational constraints.
- **Evidence anchors**:
  - [abstract]: "RadLing system offers operational advantages including local deployment and reduced runtime costs"
  - [section]: "RadLing system offers deployment flexibility and cost-effectiveness... It can run on local CPUs, enhancing data privacy"
- **Break condition**: If performance gap narrows, making cloud deployment more attractive.

## Foundational Learning

- **Concept**: Cosine similarity for embedding comparison
  - Why needed here: The system uses cosine similarity between sentence embeddings and CDE embeddings to identify relevant CDEs
  - Quick check question: How would you compute the similarity between two sentence embeddings to determine if they refer to the same clinical finding?

- **Concept**: Named Entity Recognition (NER) in radiology
  - Why needed here: RadLing uses NER to extract core concepts from sentences before embedding comparison
  - Quick check question: What types of entities would you expect to extract from a sentence like "3mm nodule in left lung base"?

- **Concept**: Data augmentation for imbalanced datasets
  - Why needed here: The study augmented underrepresented CDEs with template-based examples to balance the training data
  - Quick check question: How would you generate synthetic examples for a rarely occurring finding like "chronic rib fracture"?

## Architecture Onboarding

- **Component map**: Sentence → Embedding generation → CDE identification → Value mapping → Output
- **Critical path**: Sentence → Embedding generation → CDE identification → Value mapping → Output
- **Design tradeoffs**: RadLing sacrifices some coverage (higher precision but lower recall than GPT-4) for operational efficiency and data privacy
- **Failure signatures**: Low recall indicates poor embedding quality; low precision indicates incorrect value mapping; slow inference indicates suboptimal deployment configuration
- **First 3 experiments**:
  1. Test embedding quality by comparing similarity scores for known matching vs non-matching sentence pairs
  2. Validate value mapping accuracy on sentences with straightforward value expressions
  3. Benchmark inference speed on target deployment hardware (CPU vs GPU)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does RadLing's performance compare when tested on radiology reports from different hospitals with varying reporting styles and terminology?
- **Basis in paper**: [inferred] The study acknowledges its limitation of focusing on chest radiography and CT reports in English from specific sites.
- **Why unresolved**: The study only evaluated RadLing on reports from two data sites under data-use agreements.
- **What evidence would resolve it**: Comparative performance metrics of RadLing across radiology reports from multiple hospitals with diverse reporting styles and terminologies.

### Open Question 2
- **Question**: What is the impact of RadLing's domain-adapted embeddings on performance for other radiology subspecialties beyond chest imaging?
- **Basis in paper**: [inferred] The paper demonstrates RadLing's effectiveness for chest radiology reports but doesn't explore its performance in other subspecialties.
- **Why unresolved**: The study specifically selected 21 CDEs relevant to chest radiology.
- **What evidence would resolve it**: Testing RadLing on radiology reports from different subspecialties and measuring its F1 scores for relevant CDEs.

### Open Question 3
- **Question**: How does RadLing's performance degrade when handling complex sentences with multiple findings or ambiguous language that could reference different CDEs?
- **Basis in paper**: [explicit] The paper notes that RadLing mapped "left lower lobe bronchial wall thickening and peribronchovascular opacities suspicious for infection/aspiration" to pleural effusion due to high co-occurrence in training data.
- **Why unresolved**: The study provides limited analysis of how the model handles complex sentences with multiple findings or ambiguous language.
- **What evidence would resolve it**: Detailed error analysis and performance metrics on reports containing complex sentences with multiple findings, contradictory statements, or ambiguous language.

## Limitations

- Evaluation limited to a single institution's radiology reports, potentially limiting generalizability to other clinical settings
- Comparison focused on a specific set of 21 CDEs from RadElement.org, leaving questions about broader clinical documentation needs
- While RadLing shows superior precision, its recall remains lower than GPT-4 (85% vs 95%), potentially missing some clinically relevant findings

## Confidence

- **High confidence**: Domain adaptation provides measurable performance benefits for CDE identification (95% vs 71% sensitivity). The statistical significance of performance differences is well-established with proper test selection.
- **Medium confidence**: The operational advantages of local deployment are theoretically sound but not empirically validated with cost comparisons.
- **Low confidence**: The claim about RadLing's superiority in distinguishing absent vs unspecified findings is based on a limited set of examples.

## Next Checks

1. **Generalizability test**: Evaluate both systems on radiology reports from multiple institutions with varying reporting styles to assess cross-site performance consistency.

2. **Edge case analysis**: Systematically test reports containing ambiguous or complex value expressions (e.g., "possible consolidation" vs "likely consolidation") to identify breaking points for both systems.

3. **Cost-benefit analysis**: Measure actual deployment costs including model training, inference time, and maintenance for both local RadLing and cloud-based GPT-4 implementations across different usage volumes.