---
ver: rpa2
title: 'Textbooks Are All You Need II: phi-1.5 technical report'
arxiv_id: '2309.05463'
source_url: https://arxiv.org/abs/2309.05463
tags:
- phi-1
- data
- arxiv
- language
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce phi-1.5, a 1.3B parameter language model
  trained on synthetic "textbook-like" data, demonstrating strong performance on natural
  language tasks. Unlike previous models that relied heavily on web data, phi-1.5
  is trained primarily on carefully curated synthetic data, which helps mitigate toxicity
  and bias issues.
---

# Textbooks Are All You Need II: phi-1.5 technical report

## Quick Facts
- **arXiv ID:** 2309.05463
- **Source URL:** https://arxiv.org/abs/2309.05463
- **Reference count:** 11
- **Primary result:** phi-1.5 is a 1.3B parameter model trained on synthetic textbook-like data that achieves results comparable to models 5x larger on reasoning tasks while showing reduced toxicity.

## Executive Summary
phi-1.5 demonstrates that carefully curated synthetic "textbook-like" data can enable smaller language models to match or exceed the performance of much larger models on reasoning tasks. The 1.3B parameter model is trained primarily on synthetic data rather than web data, which helps mitigate toxicity and bias issues while maintaining strong performance across common sense reasoning, language understanding, and multi-step reasoning tasks like math and coding.

The key insight is that data quality and organization matter more than scale for certain types of reasoning tasks. By using structured, educational-style synthetic data, phi-1.5 achieves comparable results to models 5x its size on benchmarks like WinoGrande, ARC, and coding tasks, while showing superior performance on multi-step reasoning compared to larger web-trained models.

## Method Summary
phi-1.5 is trained using a Transformer architecture with 24 layers, 32 heads, and 64 dimensions per head, implemented with flash-attention and the Codegen tokenizer. The model is trained on 150B tokens using Adam optimizer with learning rate 2e-4, weight decay 0.1, and momentum 0.9/0.98, optimized with DeepSpeed ZeRO Stage 2 in fp16. The training data consists of 7B tokens from phi-1's training data combined with approximately 20B tokens of newly created synthetic "textbook-like" data covering common sense reasoning and general knowledge topics.

## Key Results
- Achieves results comparable to models 5x larger on common sense reasoning benchmarks (WinoGrande, ARC, BoolQ)
- Surpasses larger models on multi-step reasoning tasks including math (GSM8K) and coding (HumanEval, MBPP)
- Demonstrates reduced toxicity compared to web-trained models of similar size
- Shows efficient knowledge storage and retrieval despite smaller parameter count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic "textbook-quality" data enables smaller models to match larger ones on reasoning tasks
- Mechanism: Carefully curated synthetic data focuses on structured knowledge and reasoning patterns, reducing noise from web data
- Core assumption: Quality of training data matters more than scale for reasoning tasks
- Evidence anchors: Abstract mentions carefully curated synthetic data mitigates toxicity; section 2.2 details the 20B tokens of synthetic data; corpus shows weak evidence with FMR score of 0.616

### Mechanism 2
- Claim: Removing web data reduces toxicity and bias in model outputs
- Mechanism: Synthetic textbook data eliminates exposure to toxic web content while maintaining educational value
- Core assumption: Web data is the primary source of toxicity in language models
- Evidence anchors: Abstract notes improvement thanks to absence of web data; section 4 discusses attenuating effect on toxic content generation

### Mechanism 3
- Claim: Specialized synthetic data enables efficient knowledge storage and retrieval
- Mechanism: Textbook-style data structures knowledge in ways that smaller models can more effectively encode and access
- Core assumption: Knowledge organization affects model efficiency
- Evidence anchors: Section 5 shows model retains performance on mixed tasks; section 4 highlights efficient knowledge storage and access

## Foundational Learning

- **Concept: Synthetic data generation and curation**
  - Why needed here: The paper relies entirely on synthetic data for training, making understanding data generation crucial
  - Quick check question: What are the key differences between synthetic textbook data and web data in terms of structure and content?

- **Concept: Chain-of-thought prompting**
  - Why needed here: The model demonstrates chain-of-thought capabilities in examples, showing how prompting techniques can enhance reasoning
  - Quick check question: How does chain-of-thought prompting improve performance on multi-step reasoning tasks?

- **Concept: Toxicity measurement and bias evaluation**
  - Why needed here: The paper emphasizes toxicity reduction as a key benefit of synthetic data
  - Quick check question: What metrics and methods are used to evaluate toxicity in language models?

## Architecture Onboarding

- **Component map:** Transformer (24 layers, 32 heads, 64 head dimension) -> Flash-attention -> Codegen tokenizer -> DeepSpeed ZeRO Stage 2 optimization
- **Critical path:** 1) Data preparation (synthetic textbook generation) -> 2) Model initialization and training -> 3) Evaluation on benchmarks -> 4) Toxicity and bias assessment
- **Design tradeoffs:** Smaller parameter count (1.3B) vs. performance; synthetic data vs. web data quality; training efficiency vs. comprehensive coverage
- **Failure signatures:** Poor performance on reasoning tasks indicates synthetic data inadequacy; high toxicity scores suggest synthetic data generation issues; training instability may indicate architecture limitations
- **First 3 experiments:** 1) Compare performance on reasoning tasks with varying amounts of synthetic vs. web data; 2) Test toxicity levels with different synthetic data generation strategies; 3) Evaluate knowledge retrieval efficiency across different knowledge domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of phi-1.5 compare to larger models when fine-tuned for specific tasks beyond the general benchmarks tested?
- Basis in paper: The paper states phi-1.5 achieves comparable results to models 5x larger on general benchmarks, but does not explore fine-tuning for specific tasks.
- Why unresolved: The paper focuses on general performance across multiple benchmarks but does not investigate task-specific fine-tuning.
- What evidence would resolve it: Comparative studies of phi-1.5 vs larger models on task-specific fine-tuning tasks like legal document analysis or medical diagnosis.

### Open Question 2
- Question: What is the long-term impact of training exclusively on synthetic data on the model's ability to handle real-world scenarios?
- Basis in paper: The paper highlights that phi-1.5 is trained almost exclusively on synthetic data, raising questions about its real-world applicability.
- Why unresolved: While synthetic data improves certain aspects like toxicity, it may limit exposure to diverse real-world scenarios.
- What evidence would resolve it: Longitudinal studies comparing phi-1.5's performance on real-world tasks over time versus models trained on mixed data.

### Open Question 3
- Question: How does the model's performance degrade or improve when exposed to adversarial or out-of-distribution data?
- Basis in paper: The paper mentions the model's robustness but does not explicitly test adversarial or out-of-distribution scenarios.
- Why unresolved: The paper focuses on standard benchmarks and does not explore edge cases or adversarial testing.
- What evidence would resolve it: Experiments testing phi-1.5's responses to adversarial prompts or data outside its training distribution.

## Limitations

- **Synthetic data quality uncertainty**: The specific quality metrics and comprehensive coverage validation for the 20B tokens of synthetic data remain unclear
- **Generalization concerns**: Strong performance on controlled benchmarks doesn't guarantee broad real-world applicability
- **Limited toxicity evaluation**: Claims of reduced toxicity are based on observations rather than systematic measurement across diverse scenarios

## Confidence

- **High confidence**: Claims about phi-1.5's architecture specifications and training methodology are well-documented and reproducible
- **Medium confidence**: Claims regarding performance improvements over larger models and toxicity reduction are supported by benchmark results but need independent validation
- **Medium confidence**: Claims about data quality being more important than scale are plausible but require broader empirical validation

## Next Checks

1. **Synthetic Data Validation Study**: Conduct comprehensive analysis of the synthetic dataset's coverage, coherence, and potential biases by having independent reviewers assess samples across all topic categories and reasoning domains

2. **Long-term Generalization Test**: Evaluate phi-1.5's performance on out-of-distribution tasks and real-world applications not included in the original benchmark suite, particularly focusing on temporal generalization

3. **Toxicity Measurement Protocol**: Implement standardized toxicity evaluation framework using multiple metrics across diverse prompt categories to systematically quantify the claimed toxicity reduction compared to web-trained models of similar size