---
ver: rpa2
title: Encouraging Divergent Thinking in Large Language Models through Multi-Agent
  Debate
arxiv_id: '2305.19118'
source_url: https://arxiv.org/abs/2305.19118
tags:
- debate
- translation
- which
- llms
- circle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of Degeneration-of-Thought (DoT)
  in large language models (LLMs), where models become overly confident in their initial
  incorrect answers and fail to generate novel solutions through self-reflection.
  To address this, the authors propose a Multi-Agent Debate (MAD) framework where
  multiple agents debate in a "tit for tat" manner under the supervision of a judge
  to obtain the final solution.
---

# Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate

## Quick Facts
- arXiv ID: 2305.19118
- Source URL: https://arxiv.org/abs/2305.19118
- Reference count: 13
- Multi-agent debate framework outperforms baselines on challenging reasoning tasks

## Executive Summary
This paper addresses Degeneration-of-Thought (DoT) in large language models, where models become overconfident in incorrect answers and fail to generate novel solutions. The authors propose Multi-Agent Debate (MAD), a framework where multiple agents debate under judge supervision to obtain final solutions. MAD encourages divergent thinking and is less susceptible to DoT by leveraging external feedback and multiple perspectives. Experiments on commonsense machine translation and counter-intuitive arithmetic reasoning show MAD outperforms baseline methods including GPT-4 in some cases.

## Method Summary
The paper introduces MAD framework with three components: meta prompts for initialization, multiple debating agents, and a judge that manages the debate process. Agents engage in "tit for tat" argumentation while the judge monitors debate quality and extracts the final answer. The framework uses adaptive break strategies and operates under the premise that disagreement and external feedback can prevent the degeneration-of-thought problem in LLMs.

## Key Results
- MAD outperforms GPT-4 and other baselines on commonsense machine translation tasks
- MAD shows improved accuracy on counter-intuitive arithmetic reasoning compared to self-reflection methods
- Adaptive break strategy and moderate disagreement levels are optimal for MAD performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-Agent Debate (MAD) reduces Degeneration-of-Thought (DoT) by introducing external disagreement signals.
- Mechanism: When an LLM agent becomes overconfident in an incorrect answer, other agents provide opposing viewpoints that challenge the flawed reasoning path, preventing the agent from settling into a suboptimal solution.
- Core assumption: Agents will generate meaningful counterarguments when prompted to "tit for tat."
- Evidence anchors:
  - [abstract]: "MAD framework, in which multiple agents express their arguments in the state of 'tit for tat' and a judge manages the debate process to obtain a final solution."
  - [section]: "Our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation."
  - [corpus]: Found 25 related papers; average neighbor FMR=0.352 indicates moderate relatedness to debate-based LLM approaches.
- Break condition: If agents fail to generate substantive counterarguments, the system reverts to single-agent self-reflection behavior.

### Mechanism 2
- Claim: The judge component provides adaptive stopping criteria that prevents over-debate.
- Mechanism: The judge evaluates whether a correct solution has emerged and can terminate the debate early when consensus is reached, avoiding degradation from excessive iterations.
- Core assumption: The judge can accurately identify when a correct solution is presented.
- Evidence anchors:
  - [abstract]: "a judge manages the debate process to obtain a final solution."
  - [section]: "Extensive analyses suggest that the adaptive break strategy of debate and the modest level of 'tit for tat' state are required for MAD to obtain good performance."
  - [corpus]: The paper explicitly discusses "adaptive break strategy" as a performance requirement.
- Break condition: If the judge incorrectly identifies incorrect answers as correct, the system may terminate prematurely.

### Mechanism 3
- Claim: MAD exploits complementary reasoning strengths across multiple agents.
- Mechanism: Different agents may approach problems from different angles, and the debate process surfaces these diverse perspectives, leading to more robust solutions than any single agent could achieve.
- Core assumption: Different LLM agents will generate meaningfully different reasoning paths.
- Evidence anchors:
  - [abstract]: "MAD encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation."
  - [section]: "Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework."
  - [corpus]: Related work on multi-agent debate systems suggests this is an active research area.
- Break condition: If all agents share the same biases or reasoning patterns, MAD provides no advantage over single-agent approaches.

## Foundational Learning

- Concept: Degeneration-of-Thought (DoT) in LLMs
  - Why needed here: Understanding DoT is crucial for appreciating why MAD is necessary and how it addresses a fundamental limitation in LLM reasoning.
  - Quick check question: What happens to an LLM's reasoning ability after it becomes confident in an incorrect answer?

- Concept: Chain-of-Thought prompting
  - Why needed here: MAD builds on CoT by adding multiple reasoning paths through debate, so understanding basic CoT is essential for grasping MAD's enhancements.
  - Quick check question: How does standard CoT prompting differ from the debate-based reasoning in MAD?

- Concept: Multi-agent collaboration patterns
  - Why needed here: MAD relies on agents working together through debate rather than simple aggregation, requiring understanding of collaborative AI systems.
  - Quick check question: What distinguishes debate-based collaboration from other multi-agent approaches like voting or consensus?

## Architecture Onboarding

- Component map:
  - Meta Prompts -> Debaters (N agents) -> Judge -> Final Answer
  - Debate History (tracks all arguments)

- Critical path:
  1. Initialize agents with meta prompts
  2. First debater generates initial argument
  3. Judge evaluates for correctness
  4. If incorrect, next debater responds
  5. Repeat until judge approves or iteration limit reached
  6. Judge extracts final answer from debate history

- Design tradeoffs:
  - More agents increase diversity but also computational cost
  - Higher iteration limits allow more thorough debate but risk diminishing returns
  - Judge complexity affects evaluation accuracy but increases implementation difficulty

- Failure signatures:
  - All agents converge on incorrect answer despite disagreement
  - Judge fails to identify correct answers, causing premature termination
  - Debate becomes circular without progress toward solution

- First 3 experiments:
  1. Run MAD with 2 agents on a simple arithmetic problem where the correct answer is non-intuitive
  2. Test adaptive break strategy by comparing fixed vs. adaptive iteration limits
  3. Vary the "tit for tat" intensity to find optimal disagreement level for performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MAD scale with an increasing number of agents/debaters beyond the default of two?
- Basis in paper: [inferred] The paper mentions that future works may include "scheduling more agents in the debate," suggesting that this is an open question.
- Why unresolved: The paper only experiments with two debaters and one judge, so the effect of adding more agents is unknown.
- What evidence would resolve it: Experiments comparing MAD performance with varying numbers of agents (e.g., 2, 4, 8) on the same tasks would show if additional agents improve or degrade results.

### Open Question 2
- Question: What specific biases or cognitive limitations in LLMs lead to the Degeneration-of-Thought (DoT) problem, and can these be mitigated through architectural changes rather than just debate?
- Basis in paper: [explicit] The paper outlines three factors contributing to DoT: bias and distorted perception, rigidity and resistance to change, and limited external feedback. However, it doesn't explore architectural solutions.
- Why unresolved: The paper proposes MAD as a workaround but doesn't investigate whether the underlying LLM architecture can be modified to reduce DoT.
- What evidence would resolve it: Comparative studies between MAD and architectural modifications (e.g., different training objectives, attention mechanisms) on tasks prone to DoT would reveal which approach is more effective.

### Open Question 3
- Question: How does the choice of judge LLM affect the fairness and outcome of MAD, and what criteria should be used to select an optimal judge?
- Basis in paper: [explicit] The paper finds that "LLMs might not be a fair judge if different LLMs are used for agents," with the judge showing preference for agents with the same backbone model.
- Why unresolved: The paper observes this bias but doesn't explore methods to mitigate it or determine what makes an effective judge.
- What evidence would resolve it: Experiments varying judge models and measuring outcome consistency, along with analysis of judge decision patterns, would identify characteristics of fair and effective judges.

### Open Question 4
- Question: Can MAD be extended to handle tasks beyond reasoning and translation, such as creative writing or code generation, and what modifications would be needed?
- Basis in paper: [inferred] The paper demonstrates MAD on reasoning and translation tasks, implying potential for broader application, but doesn't explore other domains.
- Why unresolved: The effectiveness of MAD in tasks requiring different cognitive processes (e.g., creativity, long-term planning) is unknown.
- What evidence would resolve it: Applying MAD to diverse tasks like story continuation, program synthesis, or strategic game playing and comparing results to baselines would show its versatility and necessary adaptations.

## Limitations

- Experimental validation is limited to only two specific task domains (commonsense translation and arithmetic reasoning)
- Implementation details for critical components like judge evaluation criteria and final answer extraction are underspecified
- Limited quantitative evidence comparing DoT rates between MAD and baseline methods
- Paper doesn't establish generalizability of MAD to other task domains beyond tested ones

## Confidence

- **High Confidence**: The experimental results showing MAD outperforming baselines on the two tested datasets are reproducible based on the provided methodology. The concept of Degeneration-of-Thought as a phenomenon affecting LLM reasoning is well-supported by the evidence presented.

- **Medium Confidence**: The claim that MAD encourages "divergent thinking" is supported by qualitative observations but lacks systematic measurement of divergence metrics. The assertion that adaptive break strategies and moderate disagreement levels are optimal is based on ablation studies but the specific mechanisms remain underspecified.

- **Low Confidence**: The generalizability of MAD to other task domains beyond the two tested datasets is not established. The paper's claim that MAD is a general framework for combating DoT across diverse LLM applications lacks sufficient empirical support.

## Next Checks

1. **Cross-domain generalization test**: Apply MAD to a third, distinctly different task domain (e.g., logical reasoning puzzles or creative writing) to assess whether the debate framework generalizes beyond the tested domains of translation and arithmetic reasoning.

2. **DoT rate quantification**: Implement systematic measurement of how frequently and severely DoT occurs in both MAD and baseline single-agent approaches across multiple problem instances, providing quantitative comparison of DoT susceptibility.

3. **Judge bias analysis**: Conduct experiments varying the judge's evaluation criteria and measure how often the judge selects answers from different agents when all agents share the same LLM backbone, to determine whether the judge introduces bias that could affect MAD's apparent performance advantages.