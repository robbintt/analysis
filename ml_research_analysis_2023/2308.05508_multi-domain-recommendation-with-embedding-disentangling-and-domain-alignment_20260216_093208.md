---
ver: rpa2
title: Multi-domain Recommendation with Embedding Disentangling and Domain Alignment
arxiv_id: '2308.05508'
source_url: https://arxiv.org/abs/2308.05508
tags:
- domain
- domains
- knowledge
- recommendation
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EDDA addresses multi-domain recommendation challenges by disentangling
  inter-domain and intra-domain knowledge at both model and embedding levels, using
  separate embeddings and models for each. It enhances knowledge transfer through
  a random walk-based domain alignment strategy that identifies similar user/item
  pairs across domains and encourages their embeddings to be similar.
---

# Multi-domain Recommendation with Embedding Disentangling and Domain Alignment

## Quick Facts
- arXiv ID: 2308.05508
- Source URL: https://arxiv.org/abs/2308.05508
- Reference count: 40
- Primary result: EDDA achieves up to 7.6% and 41.8% improvements in AUC and recall, respectively, on 3 real datasets

## Executive Summary
This paper introduces EDDA, a novel approach for multi-domain recommendation that addresses the challenge of disentangling inter-domain (general) and intra-domain (domain-specific) knowledge. EDDA uses separate embeddings and models for each knowledge type, eliminating gradient conflicts during training. It enhances knowledge transfer through a random walk-based domain alignment strategy that identifies similar users/items across domains and encourages their embeddings to be similar. Extensive experiments on three real-world datasets demonstrate that EDDA consistently outperforms 12 state-of-the-art baselines, with particularly significant benefits for smaller domains with less data.

## Method Summary
EDDA addresses multi-domain recommendation challenges through embedding disentangling and domain alignment. The approach separates both the model and embedding for inter-domain (general) and intra-domain (domain-specific) knowledge, effectively avoiding gradient conflict problems. For domain alignment, EDDA leverages random walks from graph processing to identify similar user/item pairs across different domains based on their connectivity to common overlapping nodes. The framework uses Graph-based Recommender (GRec) models for both inter-domain and intra-domain knowledge learning, with a combined loss function that includes BPR loss for recommendations and alignment loss for domain alignment. The method is evaluated on three real-world datasets (AliCCP, Amazon, AliAd) using AUC and Recall metrics.

## Key Results
- EDDA achieves up to 7.6% and 41.8% improvements in AUC and recall, respectively, compared to state-of-the-art baselines
- The approach particularly benefits smaller domains with less data, where traditional single-domain methods struggle
- EDDA consistently outperforms all 12 compared methods across different domain overlap ratios and dataset sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating inter-domain and intra-domain knowledge into distinct embeddings eliminates gradient conflict during training.
- Mechanism: By using separate embeddings for shared and domain-specific knowledge, gradient updates from different domains no longer interfere with each other, preventing cancellation of opposing gradient directions.
- Core assumption: Inter-domain and intra-domain knowledge have different optimal gradient directions and should be updated independently.
- Evidence anchors:
  - [abstract]: "the embedding disentangling recommender separates both the model and embedding for the inter-domain part and the intra-domain part"
  - [section]: "Unlike model disentangling, ED recommender explicitly separates inter-domain and intra-domain knowledge into distinct embeddings, effectively avoiding the gradient conflict problem"
  - [corpus]: Weak - no direct evidence in related papers about gradient conflict in MDR, but related to multi-task learning literature

### Mechanism 2
- Claim: Random walk-based domain alignment identifies semantically similar users/items across domains by measuring their connectivity to common anchors.
- Mechanism: Random walks from nodes in different domains are performed until they reach common overlapping nodes. The similarity between nodes is measured by how similarly they reach these anchors, implying similar behavior patterns.
- Core assumption: Nodes that are likely to reach the same set of common nodes exhibit similar behavior patterns or properties across domains.
- Evidence anchors:
  - [abstract]: "The domain alignment leverages random walks from graph processing to identify similar user/item pairs from different domains"
  - [section]: "The rationale is that two users/items should exhibit similar behavior patterns or properties if they are likely to reach the same set of anchors"
  - [corpus]: Weak - related papers mention domain alignment but don't specifically discuss random walk-based approaches for MDR

### Mechanism 3
- Claim: Knowledge transfer benefits small domains more because they are more susceptible to overfitting and have limited training data.
- Mechanism: EDDA leverages knowledge from larger domains to improve recommendations in smaller domains through both embedding disentanglement and domain alignment, compensating for data sparsity.
- Core assumption: Smaller domains benefit disproportionately from cross-domain knowledge transfer compared to larger domains.
- Evidence anchors:
  - [abstract]: "particularly benefiting smaller domains with less data" and "achieving up to 7.6% and 41.8% improvements in AUC and recall, respectively"
  - [section]: "The improvements of EDDA over the best-performing baseline in AUC and recall are up to 7.6% and 41.8%, respectively" and analysis showing gains increase with smaller domain size
  - [corpus]: Weak - no direct evidence in related papers about size-dependent benefits of MDR approaches

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and neighbor aggregation
  - Why needed here: The paper uses GRec, a GNN-based model, for both inter-domain and intra-domain knowledge learning through iterative neighbor aggregation
  - Quick check question: What is the difference between GNNs and traditional matrix factorization approaches in capturing user-item relationships?

- Concept: Multi-task learning and knowledge sharing
  - Why needed here: MDR is fundamentally a multi-task learning problem where different domains are treated as different tasks, requiring mechanisms to share knowledge while avoiding negative transfer
  - Quick check question: How does multi-task learning differ from multi-domain recommendation in terms of data distribution and label spaces?

- Concept: Embedding disentanglement and gradient flow
  - Why needed here: The core innovation relies on separating embeddings to prevent gradient conflicts, which requires understanding how gradients flow through neural networks and affect embedding updates
  - Quick check question: Why would having a single embedding for multiple domains cause gradient conflicts during training?

## Architecture Onboarding

- Component map: Inter-domain embeddings -> Inter-domain GRec -> Alignment layer -> Concatenated embeddings -> Recommendation output
- Critical path: Embedding initialization → Model training with BPR loss → Domain alignment similarity calculation → Alignment loss incorporation → Final recommendation scoring
- Design tradeoffs:
  - Memory vs. performance: Separate embeddings increase memory usage but improve disentanglement
  - Computational cost vs. alignment quality: Random walk-based alignment is expensive but provides meaningful similarity measures
  - Model complexity vs. overfitting: GRec with multiple layers captures complex patterns but may overfit on small domains
- Failure signatures:
  - Poor performance on domains with high overlap: May indicate over-alignment or gradient conflicts
  - Degradation when adding more domains: Could suggest scalability issues with the random walk approach
  - Inconsistent results across different runs: May indicate sensitivity to embedding initialization
- First 3 experiments:
  1. Validate gradient separation: Compare gradient norms and directions for inter-domain vs intra-domain embeddings during training
  2. Test alignment effectiveness: Measure recommendation accuracy with and without domain alignment on datasets with varying overlap ratios
  3. Scale sensitivity: Evaluate performance degradation as the number of domains increases to identify scalability limits

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several implicit questions remain:
- How does the effectiveness of EDDA's domain alignment strategy scale with increasing domain overlap sparsity?
- Can the embedding disentangling architecture be theoretically proven to prevent gradient conflict better than model-level disentangling?
- How does the performance of EDDA change when using different single-domain recommendation models as building blocks within the ED architecture?

## Limitations
- The paper assumes inter-domain and intra-domain knowledge have conflicting gradient directions without providing empirical validation of this claim
- The random walk-based domain alignment's computational efficiency and scalability for larger graphs remains unverified
- The exact impact of different domain overlap ratios on alignment effectiveness is not thoroughly analyzed

## Confidence
- **High**: EDDA outperforms baselines on the tested datasets (AUC and Recall improvements)
- **Medium**: Disentangled embeddings prevent gradient conflicts (mechanism proposed but not empirically verified)
- **Low**: Small domains benefit proportionally more from knowledge transfer (evidence is correlational, not causal)

## Next Checks
1. Measure and compare gradient norms and directions for inter-domain vs intra-domain embeddings during training to empirically verify the gradient conflict prevention claim
2. Conduct controlled experiments varying domain overlap ratios to quantify the impact on alignment effectiveness and recommendation accuracy
3. Analyze the computational complexity of the random walk-based alignment strategy and test its scalability on larger graphs with more domains