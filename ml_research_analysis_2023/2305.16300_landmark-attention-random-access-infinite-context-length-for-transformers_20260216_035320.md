---
ver: rpa2
title: 'Landmark Attention: Random-Access Infinite Context Length for Transformers'
arxiv_id: '2305.16300'
source_url: https://arxiv.org/abs/2305.16300
tags:
- tokens
- attention
- blocks
- token
- landmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for handling arbitrarily long contexts
  in transformers by introducing a special landmark token for each block of tokens
  in the input. The attention mechanism is trained to use the landmark token to retrieve
  relevant blocks from memory, enabling direct access to any token in the entire context.
---

# Landmark Attention: Random-Access Infinite Context Length for Transformers

## Quick Facts
- arXiv ID: 2305.16300
- Source URL: https://arxiv.org/abs/2305.16300
- Reference count: 40
- Key outcome: Method achieves comparable performance to Transformer-XL while retrieving 50x fewer tokens, extending LLaMA 7B context to over 32k tokens

## Executive Summary
This paper introduces landmark attention, a method for handling arbitrarily long contexts in transformers by introducing special landmark tokens that represent blocks of input. The approach enables random-access retrieval of relevant context blocks through a trained attention mechanism, achieving performance comparable to Transformer-XL while significantly reducing computational overhead. The method is demonstrated by fine-tuning LLaMA 7B to handle contexts exceeding 32k tokens.

## Method Summary
The method breaks input into fixed-length blocks and introduces a special landmark token for each block. During attention computation, grouped softmax forces the model to trade off between attending to local context versus retrieving entire blocks through their landmark tokens. A "stingy position mapping" scheme approximates positional encoding for retrieved blocks to enable extrapolation beyond training context lengths. The approach is validated through language modeling on PG-19 and arXiv math datasets, and by fine-tuning LLaMA 7B for extended context length.

## Key Results
- Achieves comparable perplexity to Transformer-XL on PG-19 and arXiv math datasets while retrieving 50x fewer tokens
- Successfully extends LLaMA 7B context length capacity to over 32k tokens
- Ablation study shows 4% perplexity increase when landmark tokens are removed (pg-19 dataset)

## Why This Works (Mechanism)

### Mechanism 1: Landmark Token Semantic Gates
Landmark tokens act as semantic gates that control retrieval of entire blocks of context. Each block gets a landmark token that represents the block, and the attention mechanism learns to assign high attention scores to landmark tokens corresponding to relevant blocks. This gates access to all tokens in that block through grouped softmax. The model learns to associate landmark tokens with the semantic content of their corresponding blocks, enabling meaningful retrieval decisions.

### Mechanism 2: Grouped Softmax for Efficient Retrieval
Grouped softmax enables efficient retrieval by forcing attention decisions between blocks and local context. Instead of standard softmax, grouped softmax applies softmax separately within groups. Each block's regular tokens form their own group, while landmark tokens for other blocks are grouped with the current token. This creates a trade-off where the model must choose between attending to other blocks versus current tokens, forcing meaningful retrieval decisions.

### Mechanism 3: Stingy Position Mapping for Extrapolation
Positional encoding can be approximated through "stingy position mapping" to enable extrapolation beyond training context length. Instead of using actual token positions, a pre-allocated segment maps retrieved blocks to specific positions. Recent blocks are mapped to the end of this segment while older blocks map to the beginning, preserving relative ordering while avoiding position extrapolation issues.

## Foundational Learning

- Concept: Self-attention mechanism
  - Why needed here: Landmark attention builds directly on standard self-attention but modifies it with grouped softmax and landmark gating
  - Quick check question: How does the attention score computation change when using grouped softmax versus standard softmax?

- Concept: Positional encoding in transformers
  - Why needed here: Understanding positional encoding is crucial for the stingy position mapping approach and why standard positional encoding fails for long contexts
  - Quick check question: What happens to attention scores when tokens are positioned beyond the training context length?

- Concept: Gradient accumulation and mixed-precision training
  - Why needed here: The paper mentions using gradient accumulation and bfloat16 to train effectively with limited GPU resources
  - Quick check question: How does gradient accumulation affect the effective batch size and training stability?

## Architecture Onboarding

- Component map: Input → Tokenization → Landmark insertion → Transformer layers with grouped softmax → Cache management → Output generation
- Critical path: Token generation requires attention computation to landmark tokens, retrieval decision, block loading from cache, and final attention computation
- Design tradeoffs: 
  - More retrieved blocks → better performance but higher memory/computation
  - Fewer landmark tokens → less overhead but coarser retrieval granularity
  - Complex positional encoding → better extrapolation but harder training
- Failure signatures:
  - Poor perplexity → likely issues with landmark learning or retrieval quality
  - Memory errors → cache management or block loading problems
  - Slow inference → inefficient retrieval or too many blocks being retrieved
- First 3 experiments:
  1. Verify landmark tokens are being inserted correctly by checking token counts and landmark positions
  2. Test grouped softmax implementation by comparing attention distributions with standard softmax
  3. Validate cache management by measuring memory usage and ensuring retrieved blocks match expected content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance change when using different block sizes for landmark tokens?
- Basis in paper: The paper mentions using blocks of 50 tokens in their experiments and notes that this translates to a 50x reduction in computation, but doesn't explore the impact of different block sizes on performance.
- Why unresolved: The paper only tests one block size, so the optimal block size for different tasks or datasets is unknown.
- What evidence would resolve it: Experiments comparing the model's performance and computational efficiency with different block sizes (e.g., 25, 50, 100 tokens) on various datasets.

### Open Question 2
- Question: How does the landmark attention mechanism compare to other long-context methods like retrieval-augmented models in terms of accuracy and computational efficiency?
- Basis in paper: The paper introduces landmark attention as an alternative to recurrent memory and retrieval-based augmentation methods, but doesn't directly compare the performance of landmark attention with these other methods.
- Why unresolved: The paper focuses on the strengths of landmark attention but doesn't provide a comprehensive comparison with other state-of-the-art methods for handling long contexts.
- What evidence would resolve it: Experiments comparing the accuracy and computational efficiency of landmark attention with other long-context methods on the same tasks and datasets.

### Open Question 3
- Question: How does the model's performance change when using different positional encoding schemes for long contexts?
- Basis in paper: The paper mentions that Transformers have difficulty extrapolating to contexts longer than those seen during training and proposes a "stingy position mapping" scheme, but doesn't extensively explore its impact on performance.
- Why unresolved: The paper only tests one positional encoding scheme and briefly mentions an alternative, so the optimal positional encoding for different long-context scenarios is unknown.
- What evidence would resolve it: Experiments comparing the model's performance and ability to handle long contexts with different positional encoding schemes on various datasets and context lengths.

## Limitations
- Limited theoretical analysis of why landmark tokens work as semantic gates
- Single data point for extended context length demonstration on LLaMA 7B
- No comparison with other long-context methods like retrieval-augmented models

## Confidence

**High Confidence**: The core claim that landmark attention can achieve comparable perplexity to Transformer-XL while retrieving significantly fewer tokens is well-supported by experimental results on standard benchmarks.

**Medium Confidence**: The claim about extending LLaMA 7B to 32k+ context length is supported but represents a single data point, making generalization uncertain.

**Low Confidence**: The assertion that stingy position mapping is necessary for extrapolation is weakly supported with limited comparison to alternative position encoding schemes.

## Next Checks

1. **Ablation Study on Landmark Placement**: Systematically vary the placement strategy of landmark tokens within blocks (beginning, middle, end, random) and measure the impact on retrieval quality and overall performance.

2. **Cross-Domain Generalization Test**: Apply the landmark attention mechanism to a non-language modeling task such as time series prediction or protein sequence analysis to test generalization beyond language-specific evaluation.

3. **Position Encoding Comparison**: Implement and compare alternative position encoding strategies for long contexts while keeping the landmark attention mechanism constant to isolate the contribution of the position encoding scheme.