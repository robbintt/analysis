---
ver: rpa2
title: Large language models converge toward human-like concept organization
arxiv_id: '2308.15047'
source_url: https://arxiv.org/abs/2308.15047
tags:
- language
- vector
- knowledge
- average
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that large language models develop conceptual
  representations that increasingly align with human-like knowledge organization as
  they scale in size. The authors compare 21 language models (OPT, GPT-2, Pythia,
  BERT, and GPT-3) with vector spaces derived from knowledge graph embeddings (BigGraph,
  TransE, ComplEx) by aligning them through Procrustes analysis and ridge regression.
---

# Large language models converge toward human-like concept organization

## Quick Facts
- arXiv ID: 2308.15047
- Source URL: https://arxiv.org/abs/2308.15047
- Reference count: 40
- Key outcome: Language models increasingly align with human-like knowledge organization as they scale

## Executive Summary
This study demonstrates that large language models develop conceptual representations that increasingly mirror human-like knowledge organization as model size grows. By comparing 21 language models with knowledge graph embeddings through Procrustes analysis and ridge regression, the authors find consistent convergence patterns. The largest models (GPT-3 and OPT-6.7B) achieve precision@50 scores exceeding 60% in aligning with human conceptual structures. This convergence is most pronounced for common and monosemous words, suggesting that scaling enables models to develop inferential semantics rather than relying solely on pattern matching.

## Method Summary
The authors evaluated 21 transformer-based language models (including OPT, GPT-2, Pythia, BERT, and GPT-3) by comparing their embeddings to knowledge graph embeddings (BigGraph, TransE, ComplEx) from Wikidata. They aligned vector spaces using Procrustes analysis and ridge regression, then measured alignment precision (P@k for k âˆˆ {1, 10, 20, 50}) and representational similarity through cosine similarity of representational dissimilarity matrices. The study used a 20,000-word vocabulary from Google's 10k list, with models generating embeddings for each word. Results were analyzed across model families and scales to identify convergence trends toward human-like conceptual organization.

## Key Results
- Larger models show stronger alignment with knowledge graph embeddings, with GPT-3 achieving P@50 beyond 60%
- Convergence is most pronounced for monosemous and common words
- Representational similarity analysis confirms geometric convergence between model spaces and knowledge bases
- Consistent improvement across all model families (OPT, GPT-2, Pythia, BERT) as size increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models induce more isomorphic vector spaces to knowledge graph embeddings
- Mechanism: Scaling increases model capacity to encode semantic relationships, leading to geometric convergence with structured knowledge representations
- Core assumption: Semantic relationships are learnable from text and reflected in embedding geometry
- Evidence anchors:
  - [abstract]: "bigger and better models exhibit more human-like concept organization"
  - [section]: Figure 2 shows consistent upward trend in P@50 across model families as size increases
  - [corpus]: Neighbor papers discuss "privileged and convergent bases" suggesting geometric convergence is a general phenomenon
- Break condition: If semantic relationships require multimodal grounding not present in text, scaling alone won't help

### Mechanism 2
- Claim: Procrustes alignment works because model embeddings preserve relative distances when scaled
- Mechanism: Larger models maintain consistent relative positioning of concepts while scaling magnitude, enabling linear transformation to knowledge base space
- Core assumption: The internal geometry of embeddings is stable across model scales
- Evidence anchors:
  - [section]: "linear projections are found to be significantly more precise" for larger models
  - [section]: PCA used to reduce dimensionality for Procrustes, suggesting stable subspace structure
  - [corpus]: "Concept-Enhanced Pre-Training" implies structured relationships can be learned
- Break condition: If embeddings become too high-dimensional for linear methods to capture relationships

### Mechanism 3
- Claim: Analogies task success indicates learned relational reasoning
- Mechanism: Language models develop vector arithmetic properties that mirror knowledge base relationships
- Core assumption: Analogies represent fundamental semantic relations that can be learned from distributional patterns
- Evidence anchors:
  - [section]: "analogies play a central role in human commonsense reasoning"
  - [section]: Figure 4 shows consistent improvement in analogy solving across model scales
  - [corpus]: "meaning without reference" paper suggests analogies can be learned without external grounding
- Break condition: If analogies require world knowledge beyond text patterns

## Foundational Learning

- Representational Similarity Analysis
  - Why needed here: RSA quantifies geometric similarity between model spaces and knowledge bases
  - Quick check question: What does an RDM measure and why is cosine similarity between RDMs meaningful?

- Procrustes Analysis
  - Why needed here: Enables alignment of vector spaces with different dimensionalities
  - Quick check question: How does Procrustes find the optimal rotation matrix between two point clouds?

- Ridge Regression for Vector Alignment
  - Why needed here: Provides alternative to Procrustes when embeddings have different dimensionalities
  - Quick check question: Why might ridge regression outperform Procrustes for high-dimensional embeddings?

## Architecture Onboarding

- Component map:
  Language model embeddings -> Vector space
  Knowledge graph embeddings -> Reference space
  Procrustes/Ridge regression -> Alignment transformation
  Retrieval evaluation -> Precision@k metric
  RSA -> Geometric similarity measurement

- Critical path:
  1. Load language model and generate embeddings for 20K vocabulary
  2. Load corresponding knowledge graph entities
  3. Apply alignment method (Procrustes or ridge regression)
  4. Evaluate retrieval performance with precision@k
  5. Analyze convergence trends across model scales

- Design tradeoffs:
  - Procrustes requires PCA dimensionality reduction vs ridge regression works directly
  - Higher k in precision@k gives more lenient evaluation but may mask poor local structure
  - RSA provides global similarity but may miss local geometric differences

- Failure signatures:
  - Poor alignment precision indicates embeddings don't capture similar semantic relationships
  - Negative convergence trends suggest scaling introduces noise or changes semantic encoding
  - Low RSA scores indicate fundamentally different geometric organizations

- First 3 experiments:
  1. Verify Procrustes alignment works on small toy datasets with known transformations
  2. Test retrieval performance on held-out vocabulary subsets to validate alignment quality
  3. Compare RSA scores between random embeddings and knowledge graph embeddings as baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors determine which semantic categories are better represented in large language models?
- Basis in paper: [explicit] The authors observe that places (world cities) show higher positive rates in convergence compared to other categories like anthroponyms
- Why unresolved: The paper identifies this pattern but doesn't explain the underlying mechanisms that make certain semantic categories more amenable to language model representation
- What evidence would resolve it: Systematic analysis comparing multiple semantic categories across different languages and knowledge domains, identifying which linguistic features correlate with better representation

### Open Question 2
- Question: Does the convergence toward human-like concept organization continue indefinitely with larger models, or does it plateau at some point?
- Basis in paper: [inferred] The paper shows increasing convergence with model size up to the largest models tested (GPT-3, OPT-6.7B, Pythia-6.9B) but doesn't explore beyond these sizes
- Why unresolved: The experiments only test up to a certain model scale, leaving open whether the trend continues or saturates
- What evidence would resolve it: Testing convergence patterns with future, larger language models beyond current scales

### Open Question 3
- Question: How do morphological and syntactic properties of different languages affect the alignment between language models and knowledge graph embeddings?
- Basis in paper: [explicit] The authors explicitly acknowledge this as a major limitation, noting all experiments were limited to English
- Why unresolved: Cross-linguistic variation in how concepts are expressed and organized could significantly impact the convergence patterns observed
- What evidence would resolve it: Replicating the experiments across multiple languages with varying morphological complexity and syntactic structures to identify universal vs language-specific patterns

## Limitations
- Analysis limited to 20,000 common words, which may not represent full conceptual complexity
- Knowledge graph embeddings (Wikidata-based) represent one specific conceptualization of semantic relationships
- Procrustes alignment assumes linear transformations are sufficient for all semantic dimensions
- Experiments restricted to English language models, limiting cross-linguistic generalizability

## Confidence
**High confidence**: The convergence trend across model scales is robust and consistently observed across multiple evaluation metrics (P@k, RSA, analogies). The methodology for alignment and evaluation is well-established in the literature.

**Medium confidence**: The interpretation that convergence represents "human-like" conceptual organization assumes knowledge graph embeddings accurately reflect human conceptual structures. This assumption is reasonable but not definitively proven.

**Low confidence**: Claims about the specific mechanisms driving convergence (e.g., "privileged bases" emergence) remain speculative without direct causal evidence. The study shows correlation between model size and alignment but cannot definitively establish causation.

## Next Checks
1. **Vocabulary size sensitivity analysis**: Repeat the alignment analysis using progressively larger vocabularies (50K, 100K words) to determine if convergence trends hold across broader semantic domains and whether any scaling relationships change.

2. **Cross-linguistic validation**: Apply the same methodology to multilingual models and knowledge graphs in languages with different typological properties (e.g., Mandarin, Arabic, Finnish) to test whether convergence is language-dependent or represents a universal property of large language models.

3. **Alternative alignment methods**: Implement and compare non-linear alignment approaches (e.g., neural network-based mappings, optimal transport) to assess whether the observed convergence persists under different geometric transformations between model spaces and knowledge bases.