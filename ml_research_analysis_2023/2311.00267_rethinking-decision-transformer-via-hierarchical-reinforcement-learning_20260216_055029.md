---
ver: rpa2
title: Rethinking Decision Transformer via Hierarchical Reinforcement Learning
arxiv_id: '2311.00267'
source_url: https://arxiv.org/abs/2311.00267
tags:
- policy
- learning
- training
- steps
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Autotuned Decision Transformer (ADT), a hierarchical
  reinforcement learning framework that improves upon the original Decision Transformer
  by enabling better stitching of suboptimal trajectories. The key innovation is a
  two-level policy architecture where a high-level policy generates adaptive prompts
  (either scalar values or sub-goals) that guide a low-level policy to make decisions.
---

# Rethinking Decision Transformer via Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.00267
- Source URL: https://arxiv.org/abs/2311.00267
- Reference count: 40
- This paper introduces ADT, a hierarchical RL framework that significantly outperforms vanilla DT on offline RL benchmarks.

## Executive Summary
This paper addresses the limitations of Decision Transformer (DT) in handling suboptimal trajectory data by introducing Autotuned Decision Transformer (ADT), a hierarchical reinforcement learning framework. ADT uses a two-level policy architecture where a high-level policy generates adaptive prompts (scalar values or subgoals) that guide a low-level policy's decisions. The high-level policy is trained to predict in-sample optimal values or k-step subgoals, while the low-level policy uses advantage-weighted regression conditioned on these prompts. Empirical results on multiple offline RL benchmarks demonstrate that ADT significantly outperforms prior transformer-based methods like DT and QLDT, achieving comparable performance to state-of-the-art offline RL algorithms.

## Method Summary
ADT introduces a hierarchical policy architecture with a high-level policy that generates adaptive prompts and a low-level transformer-based policy that conditions actions on these prompts. The high-level policy is trained using IQL (for V-ADT) or HIQL (for G-ADT) to generate either scalar values or k-step subgoals. The low-level policy is optimized using advantage-weighted regression with the concatenated state-prompt representation. Both policies are jointly optimized to improve stitching of suboptimal trajectories, addressing DT's limitations with generalization and suboptimal data.

## Key Results
- ADT significantly outperforms vanilla DT and QLDT on multiple offline RL benchmarks (MuJoCo, AntMaze, FrankaKitchen)
- ADT achieves comparable performance to state-of-the-art offline RL algorithms
- The two-level hierarchical architecture enables better stitching of suboptimal trajectories compared to flat architectures
- Extensive ablation studies validate the importance of auto-tuned prompts and the proposed policy optimization framework

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ADT's two-level hierarchy enables effective stitching of suboptimal trajectories by generating adaptive prompts instead of relying on fixed return-to-go values.
- **Mechanism**: The high-level policy generates context-specific prompts that guide the low-level policy's decisions, allowing information about optimal paths to propagate backward through the trajectory.
- **Core assumption**: The high-level policy can be trained to generate useful prompts that improve the low-level policy's performance beyond what fixed prompts provide.
- **Evidence anchors**:
  - [abstract] "The high-level policy is trained to predict in-sample optimal values or k-step subgoals, while the low-level policy is optimized using advantage-weighted regression conditioned on these prompts."
  - [section 3.1] "There are several issues. First, the oracle return R* is not known. The initial return-to-go prompt of DT is picked by hand and might not be consistent with the one observed in the dataset."
- **Break condition**: If the high-level policy cannot generate meaningful prompts or the low-level policy cannot effectively use them, the hierarchical structure provides no advantage over flat architectures.

### Mechanism 2
- **Claim**: ADT uses advantage-weighted regression for the low-level policy, which better handles the distribution mismatch between optimal and suboptimal trajectories.
- **Mechanism**: The low-level policy is trained using an objective that weights transitions by their advantage (Q(s,a) - V(s)), allowing the policy to focus on actions that provide higher value while ignoring less useful actions from the dataset.
- **Core assumption**: Advantage weighting provides better signal for learning from suboptimal data than standard maximum likelihood training.
- **Evidence anchors**:
  - [abstract] "The low-level policy is optimized using advantage-weighted regression conditioned on these prompts."
  - [section 3.2.1] "Directly training the model to predict the trajectory, as done in DT, is not suitable for our approach... We use advantage-weighted regression to learn the low-level policy."
- **Break condition**: If the advantage estimates are poor or the weighting becomes too extreme, the policy may fail to learn effectively from the dataset.

### Mechanism 3
- **Claim**: The joint optimization of high-level and low-level policies enables better generalization than training them separately.
- **Mechanism**: By optimizing both policies together with a shared objective, the high-level prompts become tailored to the low-level policy's capabilities, creating a more coherent hierarchical decision-making system.
- **Core assumption**: The high-level and low-level policies are sufficiently coupled that joint optimization provides benefits over independent training.
- **Evidence anchors**:
  - [abstract] "This joint optimization approach addresses the limitations of vanilla DT, which struggles with generalization and stitching ability when learning from suboptimal data."
  - [section 3.2.2] "G-ADT shares the same low-level policy objective as V-ADT" with joint optimization.
- **Break condition**: If the policies become too specialized to each other, the system may lose robustness to variations in the environment or dataset.

## Foundational Learning

- **Concept: Hierarchical Reinforcement Learning**
  - Why needed here: ADT builds on HRL principles by using a high-level policy to generate prompts that guide a low-level policy, creating temporal abstraction in decision-making.
  - Quick check question: In ADT, what role does the high-level policy play in the decision-making process?
- **Concept: Transformer architectures and in-context learning**
  - Why needed here: ADT leverages transformers' ability to generate sequences conditioned on context, using the high-level prompts as part of the context for the low-level policy.
  - Quick check question: How does ADT's use of transformer architecture differ from the original Decision Transformer?
- **Concept: Offline reinforcement learning and value functions**
  - Why needed here: ADT relies on learning value functions from offline data to generate meaningful prompts (in-sample optimal values) for the low-level policy.
  - Quick check question: What is the significance of using "in-sample optimal values" rather than return-to-go in ADT?

## Architecture Onboarding

- **Component map**: High-level policy → Prompt generation → State+prompt concatenation → Low-level policy → Action prediction
- **Critical path**: High-level policy generates adaptive prompts → prompts concatenated with states → low-level transformer policy generates actions
- **Design tradeoffs**: 
  - Joint optimization vs. independent training of policies
  - Scalar value prompts vs. subgoal prompts (V-ADT vs. G-ADT)
  - Tokenization strategy: concatenated state+prompt vs. separate tokens
- **Failure signatures**:
  - High-level policy generates nonsensical or unhelpful prompts
  - Low-level policy ignores prompts and behaves randomly
  - Advantage-weighted regression causes unstable learning
  - Tokenization breaks the attention mechanism in the transformer
- **First 3 experiments**:
  1. Implement ADT with the original Decision Transformer's tokenization strategy to verify the ablation study findings
  2. Test ADT with fixed prompts (like DT) to confirm the need for adaptive prompting
  3. Implement the advantage-weighted regression objective without the hierarchical structure to isolate its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ADT be extended to incorporate skills and options as latent actions in the hierarchical RL framework?
- Basis in paper: [explicit] The paper mentions that "other options for latent actions in hierarchical RL encompass skills [Ajay et al., 2020] and options [Sutton et al., 1999]" and suggests investigating potential extensions of ADT by incorporating skills and options.
- Why unresolved: While the paper discusses the potential for extending ADT to incorporate skills and options, it does not provide concrete implementation details or empirical results for these extensions.
- What evidence would resolve it: Empirical results comparing the performance of ADT with skills and options as latent actions to the current implementation using values and sub-goals, along with a detailed description of the implementation process.

### Open Question 2
- Question: Can a unified framework be established that bridges value-prompted ADT and goal-prompted ADT?
- Basis in paper: [explicit] The paper states that "according to the reward hypothesis, goals can be conceptualized as the maximization of expected value through the cumulative sum of a reward signal" and suggests investigating the potential for establishing a unified framework that bridges value-prompted ADT and goal-prompted ADT.
- Why unresolved: The paper does not provide a concrete approach or implementation details for creating a unified framework that combines value-prompted and goal-prompted ADT.
- What evidence would resolve it: A detailed description of the unified framework, along with empirical results comparing its performance to the separate value-prompted and goal-prompted ADT implementations.

### Open Question 3
- Question: What is the optimal tokenization strategy for ADT to achieve the best performance across various domains?
- Basis in paper: [explicit] The paper discusses the importance of tokenization strategies in ADT and presents an ablation study comparing different tokenization methods. It suggests that "our tokenization method contributes to superior performance both for V-ADT and G-ADT."
- Why unresolved: While the paper provides an ablation study on tokenization strategies, it does not explore the optimal tokenization strategy for ADT across different domains or provide a comprehensive analysis of the impact of tokenization on performance.
- What evidence would resolve it: A systematic study comparing the performance of ADT with various tokenization strategies across different domains, along with an analysis of the factors that influence the effectiveness of each tokenization method.

## Limitations

- The tokenization strategy for concatenated prompt-state pairs is underspecified, potentially affecting reproducibility
- Scaling factors for prompts across different domains are mentioned as important but not explicitly defined
- Advantage-weighted regression implementation details are sparse, making it difficult to determine if the weighting scheme matches the theoretical formulation

## Confidence

- **High confidence**: The hierarchical architecture concept and the general problem statement (offline RL with suboptimal data) are clearly defined and reproducible
- **Medium confidence**: The joint optimization framework and V-ADT/G-ADT variants are described with sufficient detail for implementation, though some hyperparameter choices remain unclear
- **Low confidence**: The exact tokenization scheme and advantage-weighted regression implementation details are underspecified, potentially affecting reproducibility

## Next Checks

1. **Tokenization validation**: Implement ADT with both the described concatenated state+prompt tokenization and the original DT's separate token approach to empirically verify the ablation study findings about tokenization impact
2. **Prompt scaling analysis**: Systematically test different prompt scaling strategies across domains (Mujoco, AntMaze, Kitchen) to identify optimal scaling factors and validate the claim that proper scaling is critical for performance
3. **Advantage-weighted regression isolation**: Implement the advantage-weighted regression objective in a flat (non-hierarchical) transformer policy to isolate its contribution from the hierarchical structure, testing whether the weighting alone provides benefits