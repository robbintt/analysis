---
ver: rpa2
title: Simple GNNs with Low Rank Non-parametric Aggregators
arxiv_id: '2310.05250'
source_url: https://arxiv.org/abs/2310.05250
tags:
- kernel
- performance
- graph
- spectral
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses semi-supervised node classification (SSNC)
  in graph neural networks (GNNs) and proposes a simpler approach using low-rank non-parametric
  aggregators. The authors revisit spectral GNN approaches and argue that current
  SOTA architectures may be over-engineered.
---

# Simple GNNs with Low Rank Non-parametric Aggregators

## Quick Facts
- arXiv ID: 2310.05250
- Source URL: https://arxiv.org/abs/2310.05250
- Reference count: 10
- Simple spectral GNN approach using low-rank non-parametric aggregators achieves SOTA or near-SOTA performance on semi-supervised node classification benchmarks

## Executive Summary
This paper proposes a simpler approach to semi-supervised node classification (SSNC) in graph neural networks by replacing feature aggregation with non-parametric regression in the spectral domain. The authors argue that current state-of-the-art architectures may be over-engineered and demonstrate that kernel-based spectral reshaping can achieve competitive performance without complex regularization or optimization layers. Their method uses kernel functions to reshape the spectrum of the graph adjacency matrix, allowing for better incorporation of network structure into the learning process while maintaining simplicity and interpretability.

## Method Summary
The proposed method applies kernel functions to the singular values of the graph adjacency matrix to reshape its spectrum, creating a propagation matrix that captures relevant structural information. This propagation matrix is optionally truncated to a low rank for computational efficiency. Node features are then propagated through this kernel-transformed matrix and passed through a linear layer for classification. The approach eliminates the need for complex architectures with dropout, batch normalization, or per-parameter optimizers, instead relying on the spectral properties of the graph structure and simple linear regression. Different kernel choices (identity, linear, compact Sobolev, unbounded Sobolev, Gaussian RBF) allow the model to adapt to different graph characteristics.

## Key Results
- Achieves state-of-the-art or near-SOTA performance on various SSNC benchmarks including Cora, Citeseer, Pubmed, Chameleon, and Squirrel
- Shows improvements of up to 20% compared to other spectral methods on directed networks like Chameleon and Squirrel
- Demonstrates that recent changes in evaluation conventions (particularly the balanced split) may have contributed to rising performances over time
- Ablation studies show robustness to hyperparameter choices including kernel selection, matrix representation, and truncation factor

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kernel reshaping of the graph spectrum allows the model to learn meaningful feature aggregations without deep architectures.
- Mechanism: By applying a kernel function K to the singular values of the adjacency matrix A, the model effectively remaps the spectrum into a space where low-rank approximation captures the most relevant propagation modes for node classification.
- Core assumption: The graph's community or class structure can be represented in a low-dimensional spectral subspace that is well-approximated by a kernel transformation.
- Evidence anchors:
  - [abstract] "replace feature aggregation with a non-parametric learner we are able to streamline the GNN design process"
  - [section] "we can achieve a reshaping of the spectrum h(σi) by a general function h through an appropriate choice of the kernel function"
  - [corpus] Weak evidence; neighbor papers focus on GNN efficiency but not spectral kernel reshaping specifically.
- Break condition: If the graph lacks a clear spectral community structure (e.g., random graphs), the kernel-based reshaping will not yield meaningful propagation patterns.

### Mechanism 2
- Claim: Low-rank truncation of the kernel-transformed spectrum accelerates computation while retaining most predictive power.
- Mechanism: Truncating the kernel-expanded spectrum to the top r singular values reduces the propagation matrix to a smaller, dense form that can be applied faster than the full-rank version, with only gradual performance loss.
- Core assumption: Most of the relevant information for node classification lies in the top eigenvectors of the kernel-transformed adjacency.
- Evidence anchors:
  - [section] "Low-rank truncation can homogenize response outcomes for different kernel choices" and "performance declines gradually with increases in the truncation factor"
  - [section] "In special cases like Squirrel and Chameleon, performance is even seen to increase at larger truncation values"
  - [corpus] No direct corpus evidence; assumed from experimental ablation.
- Break condition: If the node labels depend heavily on fine-grained spectral details (e.g., highly overlapping classes), truncation may remove discriminative information.

### Mechanism 3
- Claim: Simplifying model design (no dropout, batchnorm, or per-parameter optimizers) improves interpretability without sacrificing performance on many benchmarks.
- Mechanism: Removing complex regularization and optimization layers reduces overfitting risk on small, sparse graphs while maintaining generalization, especially when the spectral features already encode the signal.
- Core assumption: The dataset's node features and graph structure already provide sufficient inductive bias for classification; extra layers mostly add noise.
- Evidence anchors:
  - [abstract] "Our methods make no use of post-model augmentations such as dropout or batchnorm allowing for clean theoretical analysis"
  - [section] "absence of any post-model augmentations distinguishes our approach from other competing SOTA spectral methods"
  - [corpus] Neighbor paper "Efficient Low-Rank GNN Defense Against Structural Attacks" also emphasizes low-rank methods but focuses on robustness, not simplicity.
- Break condition: On datasets where node features dominate the label signal (e.g., Actor, Cornell), simple spectral models may underperform compared to feature-only MLPs.

## Foundational Learning

- Concept: Graph Fourier Transform
  - Why needed here: The model relies on spectral decomposition of the adjacency matrix to identify and manipulate propagation modes.
  - Quick check question: What is the difference between the Laplacian and adjacency matrix in terms of their eigenvalues for community detection?

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: The kernel function applied to singular values defines an RKHS of continuous functions, allowing the model to learn a smooth spectral filter.
  - Quick check question: Why does the choice of kernel bandwidth affect the smoothness of the learned spectral filter?

- Concept: Low-rank Matrix Approximation
  - Why needed here: Truncating the kernel-transformed spectrum to a low rank speeds up computation and reduces overfitting risk.
  - Quick check question: What happens to the approximation error as the truncation rank decreases?

## Architecture Onboarding

- Component map: X, A -> SVD of A -> kernel reshaping -> (optional truncation) -> propagation -> linear classification
- Critical path: X → A SVD → kernel reshaping → (optional truncation) → propagation → linear classification
- Design tradeoffs:
  - Full-rank kernel: More expressive but slower; truncation: faster but potentially less accurate
  - Kernel choice: Affects smoothness of spectral filter; some kernels work better on directed vs undirected graphs
  - Truncation rank r: Balances speed vs. performance; higher r = better accuracy, lower r = faster
- Failure signatures:
  - Performance drops dramatically with high truncation → graph has fine-grained spectral structure
  - Performance similar to linear model → node features dominate; graph structure adds little
  - High variance across random seeds → kernel hyperparameters not well-tuned for dataset
- First 3 experiments:
  1. Run full-rank kernel model with identity kernel on a small directed graph (e.g., Chameleon) to verify basic functionality.
  2. Compare performance of different kernels (linear, RBF, compact Sobolev) on a benchmark (e.g., Cora) to identify best kernel family.
  3. Measure runtime and accuracy trade-off by varying truncation rank r from 10% to 90% on a medium graph (e.g., Squirrel).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the relevant graph statistics that describe the homogenization discrepancy between directed and undirected datasets when using low-rank spectral truncation?
- Basis in paper: [inferred] The paper notes that the LR K ERNEL model partially alleviates performance degradation from kernel choice, but this homogenization effect is not observed for undirected datasets.
- Why unresolved: The paper mentions this as something left for future work without providing specific graph statistics that would explain this discrepancy.
- What evidence would resolve it: Analysis of common graph statistics (e.g., degree distribution, clustering coefficient, diameter) for both directed and undirected datasets showing correlations with performance homogenization effects.

### Open Question 2
- Question: How can we balance propagation and identity terms (P and I) in the regularization form P' = P + βI to prevent performance degradation when node features are highly informative?
- Basis in paper: [explicit] The paper suggests this form as a potential solution for X-dominated datasets but notes preliminary implementations haven't shown competitive results.
- Why unresolved: The paper only mentions this as a preliminary idea without exploring different regularization forms or optimization strategies.
- What evidence would resolve it: Experiments comparing different regularization forms and optimization strategies for P' = P + βI across various datasets showing improved performance on X-dominated graphs.

### Open Question 3
- Question: What is the optimal evaluation convention (sparse, dense, or balanced split) for benchmarking SSNC methods, and how do these conventions affect model comparison across different architectures?
- Basis in paper: [explicit] The paper highlights recent changes in evaluation conventions and their outsized effect on performance, particularly noting the balanced split's impact.
- Why unresolved: The paper demonstrates the impact of different splits but doesn't provide guidance on which convention is most appropriate for fair model comparison.
- What evidence would resolve it: Systematic comparison of SSNC methods across all three split conventions, including analysis of variance in performance and sensitivity to label rates.

## Limitations
- Heavy dependence on spectral assumptions about graph structure, with degraded performance on graphs lacking clear community structure
- Spectral decomposition step has O(n³) complexity, making it impractical for very large graphs despite low-rank truncation benefits
- Limited analysis of what the spectral representations actually capture or how to interpret them in practice

## Confidence
- **High Confidence**: Claims about simplicity improvements over SOTA spectral methods (no dropout, batchnorm, or complex optimizers). Supported by explicit methodological statements and consistent ablation results.
- **Medium Confidence**: Claims about kernel-based spectral reshaping improving performance. While ablation studies show kernel choice matters, the mechanism for why certain kernels work better on directed vs undirected graphs isn't fully explained.
- **Low Confidence**: Claims about interpretability improvements. The paper states the approach is more interpretable, but provides limited analysis of what the spectral representations actually capture or how to interpret them in practice.

## Next Checks
1. Test model performance on synthetic graphs with controlled spectral properties (random graphs, planted partition graphs) to verify claims about kernel reshaping effectiveness.
2. Implement a small-scale spectral analysis comparing learned spectral filters across different kernels to validate the mechanism claims about spectrum reshaping.
3. Conduct runtime benchmarking comparing full-rank vs truncated versions across graph sizes to quantify the claimed computational efficiency benefits.