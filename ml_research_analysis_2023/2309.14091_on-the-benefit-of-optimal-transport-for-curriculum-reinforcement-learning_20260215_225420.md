---
ver: rpa2
title: On the Benefit of Optimal Transport for Curriculum Reinforcement Learning
arxiv_id: '2309.14091'
source_url: https://arxiv.org/abs/2309.14091
tags:
- learning
- currot
- distribution
- performance
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating effective curricula
  for reinforcement learning (RL) agents by framing the problem as an interpolation
  between task distributions. The authors propose two methods, CURROT and GRADIENT,
  that use optimal transport to generate curricula between initial and target task
  distributions.
---

# On the Benefit of Optimal Transport for Curriculum Reinforcement Learning

## Quick Facts
- arXiv ID: 2309.14091
- Source URL: https://arxiv.org/abs/2309.14091
- Reference count: 40
- Primary result: CURROT and GRADIENT methods using optimal transport outperform existing curriculum RL approaches

## Executive Summary
This paper addresses curriculum generation for reinforcement learning by framing it as interpolation between task distributions using optimal transport. The authors propose two methods: CURROT, which constrains task distributions to contexts meeting performance thresholds, and GRADIENT, which uses predetermined barycentric interpolation. Both methods demonstrate superior performance compared to existing curriculum RL approaches across various environments, with CURROT consistently achieving the highest performance. The key insight is that optimal transport provides geometry-aware task similarity measures that avoid the limitations of KL divergence-based methods.

## Method Summary
The paper presents two curriculum generation methods for reinforcement learning. CURROT uses constrained optimal transport to generate task distributions that focus on contexts at the edge of agent competence by restricting support to contexts meeting performance thresholds. GRADIENT uses predetermined barycentric interpolation between initial and target task distributions, avoiding expected performance constraints by fixing the interpolation path and letting agent performance control step size. Both methods use Wasserstein distances to compare and interpolate between task distributions in a geometry-aware manner, addressing the limitations of KL divergence that can cause "jumps" in task similarity.

## Key Results
- CURROT consistently achieves highest performance across all tested environments
- Both methods outperform existing curriculum RL approaches (SPRL, ALP-GMM, GOALGAN, PLR, VDS, ACL)
- Methods show robustness to high-dimensional context spaces and handle both Euclidean and non-Euclidean distance metrics effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CURROT uses constrained optimal transport to generate task distributions that focus on contexts at the edge of agent competence.
- Mechanism: The algorithm restricts the support of the task distribution p(c) to the set V(π, δ) = {c ∈ C | J(π, c) ≥ δ}, ensuring all sampled tasks satisfy a performance threshold. This creates a curriculum that gradually increases task difficulty by concentrating probability density on the boundary of achievable performance.
- Core assumption: The performance threshold δ accurately captures the agent's current capability frontier.
- Evidence anchors:
  - [abstract] "CURROT restricts the support of p(c) to those contexts c ∈ C that fulfill the performance constraint J(π, c)≥δ"
  - [section 4.2] "CURROT restricts the support of p(c) to those contexts c ∈ C that fulfill the performance constraint J(π, c)≥δ"
- Break condition: If the performance estimator J(π, c) is unreliable, CURROT may sample tasks that are too easy or too hard, degrading curriculum quality.

### Mechanism 2
- Claim: GRADIENT uses predetermined barycentric interpolation between initial and target task distributions, avoiding the pitfalls of expected performance constraints.
- Mechanism: The algorithm fixes the interpolation path between p0(c) and µ(c) as a Wasserstein barycenter with interpolation parameter α. Agent performance only controls how fast α advances, not the shape of the curriculum.
- Core assumption: The distance metric d(c1, c2) properly encodes task similarity, making the barycentric interpolation meaningful.
- Evidence anchors:
  - [abstract] "GRADIENT uses a predetermined barycentric interpolation"
  - [section 4.2] "GRADIENT restricts the interpolation to the barycentric interpolation between the initial- and target context distribution"
- Break condition: If the metric d(c1, c2) poorly represents task similarity, the predetermined path may skip over useful intermediate tasks or include irrelevant ones.

### Mechanism 3
- Claim: Optimal transport provides a geometry-aware measure of similarity between task distributions that avoids the limitations of KL divergence.
- Mechanism: Wasserstein distances explicitly incorporate the ground metric d(c1, c2) when comparing distributions, ensuring interpolations respect the underlying geometry of the context space. This prevents the "jumps" in task similarity that occur with KL-based methods.
- Core assumption: The ground metric d(c1, c2) is meaningful for the task space.
- Evidence anchors:
  - [section 4.1] "Wasserstein distances allow us to define what is referred to as Wasserstein barycenters... This explicit notion of task similarity allows to generate interpolations that are stable across changes in the parameterization of context distributions"
  - [section 3.3] "Wasserstein distances... compare two distributions under a given metric, allowing... for the analysis of probabilistic inference algorithms"
- Break condition: If d(c1, c2) is misspecified or computationally intractable, the geometry-aware benefits of optimal transport are lost.

## Foundational Learning

- Concept: Optimal transport and Wasserstein distances
  - Why needed here: CURROT and GRADIENT rely on Wasserstein distances to compare and interpolate between task distributions in a geometry-aware manner. Understanding how Wasserstein distances work is essential to grasp why these methods avoid the pitfalls of KL divergence.
  - Quick check question: What is the key difference between how KL divergence and Wasserstein distance measure similarity between distributions?

- Concept: Curriculum reinforcement learning objectives
  - Why needed here: Both CURROT and GRADIENT are framed as constrained optimization problems. Understanding the general structure of CRL objectives helps explain why the specific constraints and formulations matter.
  - Quick check question: How does the expected performance constraint in traditional CRL methods differ from the support constraint used in CURROT?

- Concept: Distance metrics and their role in interpolation
  - Why needed here: The choice of distance metric d(c1, c2) is crucial for both algorithms. Understanding how different metrics affect interpolation quality helps explain the empirical results.
  - Quick check question: Why does the Euclidean distance sometimes fail to produce meaningful curricula, while shortest-path distance succeeds?

## Architecture Onboarding

- Component map:
  - Task space C with distance metric d(c1, c2) -> Initial distribution p0(c) and target distribution µ(c) -> Performance estimator J(π, c) -> Buffer management (D+ and D- for CURROT) -> OT solver for computing Wasserstein barycenters -> RL agent training loop with context sampling

- Critical path:
  1. Initialize p0(c) and µ(c)
  2. Sample contexts from current distribution
  3. Train RL agent and collect returns
  4. Update performance estimates and buffers
  5. Compute new context distribution via OT
  6. Repeat until µ(c) is reached

- Design tradeoffs:
  - CURROT: Adaptive curriculum but requires performance estimation and buffer management
  - GRADIENT: Simpler but less adaptive; predetermined path may be suboptimal
  - Particle count: More particles improve distribution approximation but increase computational cost
  - Trust region size: Larger values allow faster progression but risk sampling poor tasks

- Failure signatures:
  - CURROT: Curriculum collapses to single context (trust region too small) or samples irrelevant tasks (performance estimator poor)
  - GRADIENT: Agent never reaches threshold (predetermined path skips useful tasks) or progresses too slowly (step size too conservative)
  - Both: Poor performance on µ(c) (metric d(c1, c2) poorly chosen)

- First 3 experiments:
  1. E-Maze environment with Euclidean distance: Verify that both methods fail with dE but succeed with dS, demonstrating the importance of metric choice
  2. Point-mass environment with Gaussian mixture target: Test that both methods can handle multi-modal targets while SPRL cannot
  3. Sparse goal-reaching with high-precision targets: Demonstrate CURROT's ability to focus on high-precision tasks while other methods sample low-precision ones

## Open Questions the Paper Calls Out
- Question: How do learned distance metrics between tasks compare to hand-designed metrics in terms of curriculum performance and robustness across different environments?
  - Basis in paper: [explicit] The authors mention that "distances learned from experience, which encode a form of intrinsic motivation, will significantly advance these methods by merging the strong empirical results of intrinsic motivation in open-ended learning scenarios [13] with the targeted learning achieved by CURROT and GRADIENT."
  - Why unresolved: The paper only briefly mentions the potential of learned distance metrics but does not provide any experimental results or theoretical analysis of their effectiveness compared to hand-designed metrics.
  - What evidence would resolve it: A systematic comparison of curriculum performance using both hand-designed and learned distance metrics across multiple environments, including an analysis of robustness to changes in task distributions and generalization to new tasks.

## Limitations
- Performance estimation quality directly impacts CURROT's effectiveness
- Particle-based approximation introduces sampling variance not fully characterized
- Experiments focus on continuous control tasks, limiting generalization to more complex domains

## Confidence
- Mechanism claims: Medium
- Empirical results: Medium
- Theoretical guarantees: Low

## Next Checks
1. Test CURROT with intentionally noisy performance estimates to quantify robustness to estimator errors
2. Compare curricula generated with different distance metrics on the same task to verify geometry-aware benefits
3. Scale experiments to higher-dimensional task spaces to test particle approximation limits