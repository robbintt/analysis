---
ver: rpa2
title: A Formalism and Approach for Improving Robustness of Large Language Models
  Using Risk-Adjusted Confidence Scores
arxiv_id: '2310.03283'
source_url: https://arxiv.org/abs/2310.03283
tags:
- risk
- decision
- rule
- language
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a formalism and approach for improving robustness
  of large language models using risk-adjusted confidence scores. The key contributions
  are: 1) Formalizing two types of risk - decision risk and composite risk - and proposing
  novel metrics to measure these risks in both in-domain and out-of-domain settings.'
---

# A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores

## Quick Facts
- arXiv ID: 2310.03283
- Source URL: https://arxiv.org/abs/2310.03283
- Reference count: 13
- This paper introduces a formalism and approach for improving robustness of large language models using risk-adjusted confidence scores.

## Executive Summary
This paper addresses the critical challenge of improving robustness in large language models (LLMs) by introducing a novel formalism and approach centered on risk-adjusted confidence scores. The authors identify two types of risk—decision risk and composite risk—and propose metrics to measure these risks in both in-domain and out-of-domain settings. They introduce a risk-adjusted calibration method called DwD that uses risk injection functions to train a binary classifier to adjust raw confidence scores of LLMs. Through extensive experiments on four NLI benchmarks, the authors demonstrate that DwD significantly reduces both decision and composite risk compared to baselines, enabling LLMs to better handle uncertain predictions by abstaining when appropriate.

## Method Summary
The proposed method involves training a binary classifier (DwD) that adjusts raw confidence scores from LLMs using features like prompt length, predicted answer length, confidence standard deviation, and embedding similarities. Risk Injection Functions (RIFs) such as No-Question, Wrong-Question, and No-Right-Answer are applied to unambiguous instances to create ambiguous training data. The decision rule is trained on both original and risk-injected instances, learning to identify risky situations. During inference, the DwD classifier outputs adjusted confidence scores that are used to decide whether to make a prediction or abstain, with the goal of minimizing decision and composite risk.

## Key Results
- DwD reduces additional composite risk of ChatGPT by 17.9% compared to baselines
- DwD helps RoBERTa skip 19.8% of high-risk tasks it would have answered incorrectly
- Across all benchmarks, DwD-guided RoBERTa Ensemble outperformed confidence threshold and random baselines by margins of 31.1% and 15.3% respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Risk-adjusted confidence scores reduce both decision and composite risk by enabling LLMs to abstain from uncertain predictions.
- Mechanism: The DwD method learns a binary classifier that adjusts raw confidence scores using features such as prompt length, predicted answer length, standard deviation of confidences, and embedding similarities. This adjusted confidence is then used to decide whether to make a prediction or abstain.
- Core assumption: Raw LLM confidence scores are poorly calibrated and do not accurately reflect the true uncertainty of the model's predictions.
- Evidence anchors:
  - [abstract] "proposing a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture."
  - [section] "DwD builds upon this foundation by refining the calibration. We use several features: the prompt length (in terms of the number of characters), the length of the predicted answer generated by the LLM, the standard deviation of the confidences across candidate choices per instance, the sentence embedding similarity between the prompt and each candidate choice, and the standard deviation of these embedding similarities."
- Break condition: If the features used by DwD are not predictive of the actual uncertainty in LLM predictions, the risk-adjusted confidence scores will not effectively reduce decision and composite risk.

### Mechanism 2
- Claim: Training on risk-injected instances (using RIFs) improves the model's ability to handle out-of-domain decision risk.
- Mechanism: Risk Injection Functions (RIFs) such as No-Question, Wrong-Question, and No-Right-Answer are applied to unambiguous instances to create ambiguous instances. The decision rule is trained on both original and risk-injected instances, learning to identify risky situations.
- Core assumption: Exposure to artificially created risky scenarios during training generalizes to real-world, out-of-domain risky situations.
- Evidence anchors:
  - [section] "We designed three reasonable RIFs that can be used to inject risk into any instance i = (q, Y) from an NLI benchmark I."
  - [section] "Table 1 (second value in each cell) illustrates that RoBERTa Ensemble, utilizing a DwD-generated decision rule trained with either WQ or NRA function, achieved the best performance with an average accuracy of 0.738. This outperformed other baselines by a substantial 18.9% margin."
- Break condition: If the RIFs do not accurately represent the types of risks encountered in real-world out-of-domain scenarios, the model will not generalize well.

### Mechanism 3
- Claim: Using a binary classifier for risk-adjusted calibration is more effective than using raw confidence scores directly.
- Mechanism: The DwD method uses a random forest classifier to predict whether an instance is risky (0) or risk-free (1) based on features derived from the LLM's output and the prompt. This classifier is trained to minimize decision and composite risk.
- Core assumption: A learned decision rule based on multiple features is better at distinguishing risky from risk-free instances than a simple threshold on raw confidence.
- Evidence anchors:
  - [section] "We use several features: the prompt length (in terms of the number of characters), the length of the predicted answer generated by the LLM, the standard deviation of the confidences across candidate choices per instance, the sentence embedding similarity between the prompt and each candidate choice, and the standard deviation of these embedding similarities."
  - [section] "In evaluating ID decision risk, results in Table 1 (first value of each cell) show that, across all benchmarks and settings, the accuracy of RoBERTa Ensemble, when guided by the proposed DwD and Calibrator methods, outperformed its accuracy in conjunction with ConfStd and Random, by margins of 31.1% and 15.3%, respectively."
- Break condition: If the binary classifier is not well-calibrated or if the features are not informative, the risk-adjusted calibration will not outperform simple confidence thresholding.

## Foundational Learning

- Concept: Calibration of confidence scores
  - Why needed here: The paper argues that raw confidence scores from LLMs are poorly calibrated and need to be adjusted to reflect true uncertainty.
  - Quick check question: Why might raw confidence scores from LLMs be poorly calibrated?

- Concept: Risk injection and its role in training
  - Why needed here: Risk Injection Functions (RIFs) are used to create training data that exposes the model to risky scenarios, improving its ability to handle uncertainty.
  - Quick check question: How do Risk Injection Functions (RIFs) contribute to the training of the DwD method?

- Concept: Decision and composite risk
  - Why needed here: The paper formalizes two types of risk (decision and composite) and proposes metrics to evaluate them, which are central to the proposed framework.
  - Quick check question: What is the difference between decision risk and composite risk as defined in the paper?

## Architecture Onboarding

- Component map:
  LLM (RoBERTa or ChatGPT) -> Confidence score generation -> DwD binary classifier -> Risk-adjusted confidence score -> Decision rule -> Abstain or predict -> Selection rule -> Final prediction

- Critical path:
  1. Input prompt to LLM
  2. LLM generates confidence scores for each candidate choice
  3. DwD binary classifier takes confidence scores and additional features as input
  4. DwD outputs adjusted confidence score
  5. Decision rule uses adjusted confidence to decide whether to abstain or predict
  6. If predicting, selection rule chooses the highest confidence candidate

- Design tradeoffs:
  - Using a binary classifier adds complexity but allows for more nuanced risk assessment compared to simple thresholding.
  - Training on risk-injected instances may improve out-of-domain performance but could also lead to overfitting if not done carefully.
  - The choice of features for the DwD classifier (e.g., prompt length, embedding similarities) can impact its effectiveness.

- Failure signatures:
  - If DwD does not improve over baselines, it may indicate that the features are not informative or that the classifier is not well-calibrated.
  - If out-of-domain performance is poor, it may suggest that the RIFs used for training do not generalize well to real-world scenarios.
  - If the decision rule leads to too much abstention, it may indicate that the risk threshold is set too conservatively.

- First 3 experiments:
  1. Evaluate the performance of DwD on in-domain decision risk using the NQ RIF.
  2. Evaluate the performance of DwD on out-of-domain decision risk using the WQ and NRA RIFs.
  3. Compare the sensitivity, specificity, and relative risk ratios of DwD against baseline methods on the original NLI benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different risk injection functions (RIFs) affect the performance of decision rules in out-of-domain settings, and is there a way to optimize the choice of RIF for specific applications?
- Basis in paper: [explicit] The paper discusses the use of different RIFs (No-Question, Wrong-Question, and No-Right-Answer) and evaluates their performance in both in-domain and out-of-domain settings.
- Why unresolved: The paper shows that different RIFs have varying impacts on decision rule performance, especially in out-of-domain evaluations, but does not explore the possibility of optimizing RIF selection for specific use cases or applications.
- What evidence would resolve it: Experimental results comparing the performance of decision rules trained with different RIFs across a wide range of applications, and an analysis of how the choice of RIF affects robustness in real-world scenarios.

### Open Question 2
- Question: Can the DwD method be extended to handle more complex or ambiguous scenarios beyond natural language inference (NLI), such as multi-modal tasks or tasks requiring domain-specific knowledge?
- Basis in paper: [inferred] The DwD method is introduced as a general risk-adjusted calibration approach, but its evaluation is limited to NLI tasks. The paper mentions the potential for generalization but does not explore it in detail.
- Why unresolved: The paper does not provide evidence of DwD's performance on tasks beyond NLI, such as multi-modal tasks or domain-specific applications, leaving its broader applicability unclear.
- What evidence would resolve it: Experiments demonstrating DwD's effectiveness on diverse tasks, such as image-based inference, medical diagnosis, or legal reasoning, along with a comparison to task-specific baselines.

### Open Question 3
- Question: How does the DwD method perform when integrated with other calibration techniques, such as temperature scaling or entropy-based methods, and can these techniques be combined to further improve robustness?
- Basis in paper: [inferred] The paper introduces DwD as a standalone risk-adjusted calibration method but does not explore its integration with other calibration techniques or the potential benefits of combining multiple approaches.
- Why unresolved: The paper focuses on DwD's performance in isolation, without investigating how it interacts with or complements other calibration methods.
- What evidence would resolve it: Comparative experiments showing the performance of DwD when combined with other calibration techniques, such as temperature scaling or entropy-based methods, on both decision and composite risk metrics.

## Limitations

- Generalizability: The evaluation is limited to NLI benchmarks, and the effectiveness of DwD on other NLP tasks or domains remains unclear.
- Feature Selection Sensitivity: The paper does not provide sensitivity analysis showing how performance changes with different feature sets or when some features are removed.
- RIF Realism: The synthetically created Risk Injection Functions may not accurately capture the full spectrum of real-world uncertainties that LLMs encounter.

## Confidence

- High Confidence: The formalism for defining decision risk and composite risk is well-grounded and clearly articulated. The experimental methodology for measuring these risks follows logically from the definitions.
- Medium Confidence: The claim that DwD reduces both decision and composite risk is supported by the experimental results, but the improvements are measured against relatively simple baseline methods. The extent to which these improvements would hold against more sophisticated calibration methods is uncertain.
- Low Confidence: The paper claims that the features used by DwD (prompt length, answer length, etc.) are predictive of actual uncertainty in LLM predictions. This core assumption driving the mechanism's effectiveness lacks empirical validation beyond their use in the DwD model.

## Next Checks

1. **Ablation Study**: Remove each feature from the DwD classifier one at a time and measure the impact on decision and composite risk reduction. This would validate whether all features contribute meaningfully to the method's effectiveness.

2. **Cross-Domain Evaluation**: Apply the DwD method to a non-NLI task (such as question answering or summarization) and evaluate whether the same risk reduction benefits are observed. This would test the generalizability of the approach.

3. **Baseline Comparison**: Compare DwD against state-of-the-art confidence calibration methods for LLMs (such as temperature scaling or Bayesian approaches) rather than just the simple baselines used in the current experiments. This would better contextualize the claimed improvements.