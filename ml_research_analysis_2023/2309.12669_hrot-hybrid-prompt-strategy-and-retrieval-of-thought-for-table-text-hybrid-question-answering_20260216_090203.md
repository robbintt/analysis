---
ver: rpa2
title: 'HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid
  Question Answering'
arxiv_id: '2309.12669'
source_url: https://arxiv.org/abs/2309.12669
tags:
- table
- reasoning
- question
- hybrid
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HRoT, a novel prompting strategy for answering
  numerical questions over hybrid table and text data. The method addresses the challenge
  of Chain-of-Thought prompting's poor performance on long, complex hybrid data by
  introducing retrieval thinking and a hybrid prompt strategy.
---

# HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering

## Quick Facts
- arXiv ID: 2309.12669
- Source URL: https://arxiv.org/abs/2309.12669
- Reference count: 26
- Primary result: HRoT achieves 46.17 EM and 46.91 F1 scores on MultiHiertt in few-shot setting, outperforming previous SOTA

## Executive Summary
HRoT introduces a novel prompting strategy for answering numerical questions over hybrid table and text data. The method addresses the challenge of Chain-of-Thought prompting's poor performance on long, complex hybrid data by introducing retrieval thinking and a hybrid prompt strategy. Specifically, HRoT reconstructs tables based on retrieved evidence and guides the model to retrieve relevant information before reasoning. Experiments on the MultiHiertt dataset show that HRoT outperforms previous state-of-the-art methods in few-shot settings.

## Method Summary
HRoT combines separate text and table retrievers using DeBERTa models with DSCLoss optimization, Type-Aware Table Reconstruction to preserve hierarchical structure, and a novel HRoT prompt that guides the model to retrieve evidence before reasoning. The method processes hybrid documents by first classifying question types, then retrieving relevant evidence from both text and tables, reconstructing tables when arithmetic operations are needed, and finally using few-shot prompting with GPT-3.5 to generate answers. The approach specifically addresses the limitation of standard Chain-of-Thought prompting on long hybrid documents by ensuring the model uses correct evidence through explicit retrieval guidance.

## Key Results
- HRoT achieves 46.17 EM and 46.91 F1 scores on MultiHiertt in few-shot setting
- Outperforms fully-supervised SOTA methods in few-shot configurations (0-4 shots)
- DeBERTa retriever shows 19.8% improvement in Table Recall and 10.34% improvement in Text Recall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval of Thought (RoT) improves Chain-of-Thought prompting by first guiding the model to retrieve relevant evidence before reasoning.
- Mechanism: HRoT prompts the model to "retrieve above text and table step by step" before engaging in reasoning, ensuring the model uses correct evidence for subsequent steps.
- Core assumption: LLMs are more likely to use irrelevant information for reasoning when long, complex hybrid data is provided without explicit retrieval guidance.
- Evidence anchors:
  - [abstract] "Through In-Context Learning, we prompt the model to develop the ability of retrieval thinking when dealing with hybrid data."
  - [section 3.4] "To address the problem of LLM selecting incorrect evidence for reasoning, we introduce HRoT, which adopts a retrieval-based approach to gradually retrieve the evidence required for the question and generate the answer based on these evidence."
- Break condition: If the retrieved evidence is still noisy or irrelevant, the reasoning chain may still fail despite retrieval guidance.

### Mechanism 2
- Claim: Type-Aware Table Reconstruction preserves hierarchical table structure by reconstructing sub-tables based on retrieved evidence.
- Mechanism: The algorithm partitions multi-hierarchical tables into spans and reconstructs a sub-table containing only relevant headers and evidence rows/columns based on the question type.
- Core assumption: Multi-hierarchical tables lose critical spatial information when flattened into table descriptions, leading to incorrect reasoning outcomes.
- Evidence anchors:
  - [abstract] "Our method achieves superior performance compared to the fully-supervised SOTA on the MultiHiertt dataset in the few-shot setting."
  - [section 3.3] "Performing arithmetic operations on multi-hierarchical tables can present challenges when certain spatial information is not explicitly included in table descriptions."
- Break condition: If the table partitioning or evidence mapping is incorrect, the reconstructed sub-table may still lack necessary information for accurate reasoning.

### Mechanism 3
- Claim: Hybrid prompt strategy with DeBERTa retriever improves evidence selection quality for hybrid data.
- Mechanism: Separate DeBERTa models for text and table descriptions with DSCLoss optimize F1 score directly and address imbalanced positive/negative samples through resampling.
- Core assumption: Joint text-table retrieval models may not effectively distinguish relevant from irrelevant evidence in long hybrid documents.
- Evidence anchors:
  - [section 3.2] "During training, only a small portion of the given text contains the evidence required to answer a question. To address the problem of imbalanced positive and negative samples, we use resampling..."
  - [section 3.2] "Our loss function is defined as: Loss = CrossEntropy (y, ˆy) + λ · DSCLoss (y, ˆy)"
  - [section 4.4] "DeBERTa achieves an improvement of 10.34% in the Text Recall, and 19.8% in the Table Recall."
- Break condition: If the retrieval threshold is too strict or too lenient, relevant evidence may be missed or irrelevant evidence may be included.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Understanding why standard CoT fails on long hybrid data and how RoT improves upon it.
  - Quick check question: What is the key difference between CoT and RoT prompting strategies?

- Concept: Table reconstruction algorithms
  - Why needed here: Understanding how hierarchical table structures are preserved through selective sub-table reconstruction.
  - Quick check question: How does Type-Aware Table Reconstruction determine which rows and columns to retain?

- Concept: Retrieval-augmented generation
  - Why needed here: Understanding how evidence retrieval quality impacts downstream reasoning performance.
  - Quick check question: Why does HRoT use separate retrievers for text and table descriptions rather than a joint model?

## Architecture Onboarding

- Component map: Question classifier -> Dual retrievers (DeBERTa) -> Type-Aware Table Reconstruction (if arithmetic) -> HRoT prompt generator -> LLM reasoning engine (GPT-3.5) -> Answer extraction

- Critical path: Question → Classifier → Dual retrievers → Table reconstruction (if arithmetic) → HRoT prompt generation → LLM reasoning → Answer extraction

- Design tradeoffs: Separate text/table retrievers vs joint model; table reconstruction vs table descriptions; HRoT vs standard CoT

- Failure signatures: Low recall on retrieved evidence, incorrect table reconstruction, LLM generating wrong answer format, temperature setting causing inconsistent outputs

- First 3 experiments:
  1. Verify classifier accuracy on question type distribution in development set
  2. Test retriever performance with different top-k settings and measure evidence recall
  3. Compare HRoT with and without table reconstruction on a small subset of questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HRoT method's performance scale with increasing numbers of demonstrations in few-shot settings?
- Basis in paper: [explicit] The paper compares HRoT performance across 0-4 shot settings, showing improvement with more demonstrations.
- Why unresolved: The paper only tests up to 4 shots, leaving open the question of whether this trend continues with more demonstrations.
- What evidence would resolve it: Experiments testing HRoT with 5+ shots, plotting performance against number of demonstrations.

### Open Question 2
- Question: How does the Type-Aware Table Reconstruction algorithm handle tables with even more complex hierarchical structures?
- Basis in paper: [explicit] The paper mentions reconstructing multi-hierarchical tables but only demonstrates on a specific dataset.
- Why unresolved: The paper doesn't explore the limits of the reconstruction algorithm's effectiveness on tables with varying degrees of complexity.
- What evidence would resolve it: Testing the algorithm on tables with more hierarchical levels or different types of hierarchical structures.

### Open Question 3
- Question: How does the performance of HRoT compare to other state-of-the-art methods on datasets beyond MultiHiertt?
- Basis in paper: [explicit] The paper only tests on the MultiHiertt dataset, comparing to other methods within that dataset.
- Why unresolved: The paper doesn't explore how HRoT generalizes to other TextTableQA datasets with different characteristics.
- What evidence would resolve it: Applying HRoT to other TextTableQA datasets and comparing performance to current state-of-the-art methods on those datasets.

## Limitations

- Corpus gap limitation: The paper lacks direct citations supporting the effectiveness of Retrieval of Thought prompting and Type-Aware Table Reconstruction algorithms.
- Implementation detail uncertainty: Critical hyperparameters for the retriever (learning rate, batch size, top-k retrieval parameters) are not specified.
- Performance ceiling concern: While HRoT achieves 46.17 EM and 46.91 F1 in few-shot settings, these scores are still significantly below the fully-supervised state-of-the-art.

## Confidence

**High confidence**: The core architecture combining separate text/table retrievers with table reconstruction and HRoT prompting is clearly specified and internally consistent. The ablation study results showing 19.8% improvement in Table Recall and 10.34% improvement in Text Recall provide strong empirical support for the retrieval components.

**Medium confidence**: The claim that Retrieval of Thought improves upon standard Chain-of-Thought prompting is supported by the experimental results on MultiHiertt, but the underlying mechanism needs more direct validation through controlled experiments.

**Low confidence**: The assertion that Type-Aware Table Reconstruction preserves hierarchical information critical for reasoning is primarily justified through qualitative reasoning rather than quantitative ablation studies.

## Next Checks

1. **Ablation of retrieval guidance**: Run HRoT without the "retrieve above text and table step by step" prompt component to quantify the exact contribution of Retrieval of Thought versus standard Chain-of-Thought prompting with the same evidence.

2. **Table reconstruction fidelity measurement**: Implement a metric to measure information loss when converting multi-hierarchical tables to reconstructed sub-tables, comparing reasoning accuracy on original vs reconstructed tables for arithmetic questions.

3. **Cross-dataset generalization**: Test HRoT on a different hybrid table-text QA dataset (e.g., HybridQA or TAT-QA) to validate whether the method's improvements transfer beyond the MultiHiertt domain, particularly examining performance on questions requiring cross-modal reasoning.