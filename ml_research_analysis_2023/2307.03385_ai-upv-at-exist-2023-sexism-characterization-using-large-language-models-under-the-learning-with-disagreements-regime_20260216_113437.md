---
ver: rpa2
title: AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models
  Under The Learning with Disagreements Regime
arxiv_id: '2307.03385'
source_url: https://arxiv.org/abs/2307.03385
tags:
- sexism
- task
- ensemble
- ai-upv
- exist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an ensemble-based approach using large language
  models (mBERT and XLM-RoBERTa) for sexism identification and characterization under
  the learning with disagreements paradigm. The proposed system addresses three tasks:
  sexism identification, source intention classification, and sexism categorization,
  using both soft and hard label evaluations.'
---

# AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime

## Quick Facts
- arXiv ID: 2307.03385
- Source URL: https://arxiv.org/abs/2307.03385
- Reference count: 40
- Key outcome: Ensemble of mBERT and XLM-RoBERTa achieved first place in Task 3 (ICM-Soft of -2.32 and normalized ICM-Soft of 0.79) and fourth place in Task 2

## Executive Summary
This paper presents an ensemble-based approach using large language models (mBERT and XLM-RoBERTa) for sexism identification and characterization under the learning with disagreements paradigm. The proposed system addresses three tasks: sexism identification, source intention classification, and sexism categorization, using both soft and hard label evaluations. The ensemble approach outperformed individual models, achieving first place in Task 3 and demonstrating the effectiveness of ensemble methods in handling disagreement among annotators while maintaining strong performance across different granularity levels of sexism detection.

## Method Summary
The approach uses an ensemble of mBERT and XLM-RoBERTa transformer models trained directly on soft labels from multiple annotators without aggregation. After fine-tuning each model on the EXIST dataset, predictions are combined through mean averaging, then adjusted to match the number of annotators per sample. The system evaluates using both ICM-Soft (comparing against annotator distributions) and ICM-Hard (comparing against aggregated labels) metrics across three tasks of increasing complexity in sexism characterization.

## Key Results
- First place in Task 3 (ICM-Soft of -2.32 and normalized ICM-Soft of 0.79)
- Fourth place in Task 2
- Ensemble approach outperformed individual large language models in both soft and hard label evaluation modes
- Learning directly from disagreement data without aggregated labels proved effective for sexism characterization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble averaging of multilingual transformer models improves sexism detection performance by reducing model-specific variance and capturing complementary linguistic features across languages.
- Mechanism: The system trains mBERT and XLM-RoBERTa independently on the EXIST dataset, then combines their soft probability outputs through mean aggregation. This approach leverages the strengths of both models—mBERT's bidirectional context and multilingual pretraining, and XLM-RoBERTa's cross-lingual transfer capabilities—to produce more robust predictions that better handle the disagreement among annotators.
- Core assumption: Individual transformer models capture different aspects of sexism-related linguistic patterns, and their errors are not perfectly correlated.
- Evidence anchors:
  - [abstract] "The ensemble approach outperformed individual large language models obtaining the best performances both adopting a soft and a hard label evaluation."
  - [section] "Inspired by the state-of-the-art results, we propose an ensemble method to combine two transformer-based models: namely mBERT and XLM-RoBERTa."
- Break condition: If the individual models are highly correlated in their errors or if one model consistently dominates the other, the ensemble benefit may diminish or become negligible.

### Mechanism 2
- Claim: Learning directly from disagreement data without aggregated labels preserves the nuanced perspectives of annotators and improves model calibration.
- Mechanism: Instead of creating hard labels through majority voting, the system trains on the full set of soft labels provided by annotators. This approach captures the inherent subjectivity in sexism detection and allows the model to learn the distribution of annotator opinions rather than a single consensus view.
- Core assumption: The variation in annotator labels reflects genuine uncertainty or perspective differences rather than random noise, and this information is valuable for the model.
- Evidence anchors:
  - [abstract] "The proposed approach aims at addressing the task of sexism identification and characterization under the learning with disagreements paradigm by training directly from the data with disagreements, without using any aggregated label."
  - [section] "The training phase has the main goal of learning directly with soft labels."
- Break condition: If annotator disagreements are primarily due to random noise, incompetence, or misunderstanding of the task rather than genuine perspective differences, learning from disagreements could harm performance.

### Mechanism 3
- Claim: Adjusting ensemble predictions to match the number of annotators ensures probability distributions are feasible and improves evaluation alignment.
- Mechanism: After computing the mean probabilities from mBERT and XLM-RoBERTa, the system adjusts these predictions by selecting the most similar feasible distribution according to cosine similarity, constrained by the actual number of annotators for each sample. This ensures the model's output respects the discrete nature of annotator counts.
- Core assumption: The evaluation metrics (ICM-Soft) are designed to compare against distributions that respect the number of annotators, and unadjusted predictions may violate these constraints.
- Evidence anchors:
  - [section] "To ensure that the predicted probabilities are compliant with respect to the number of annotators, we performed the following operation: given the probability distribution of a model, we selected the most similar distribution according to cosine similarity with the feasible distributions."
- Break condition: If the adjustment operation introduces significant bias or if the feasible distribution space is too restrictive relative to the true distribution of annotator opinions.

## Foundational Learning

- Concept: Learning with disagreements paradigm
  - Why needed here: Sexism detection is inherently subjective, with different annotators potentially having different perspectives on what constitutes sexist content. The EXIST challenge explicitly provides data with multiple annotator labels to capture this subjectivity.
  - Quick check question: What is the key difference between traditional supervised learning and learning with disagreements when handling multi-annotator data?

- Concept: Soft vs. hard label evaluation
  - Why needed here: The EXIST challenge evaluates systems using both soft evaluation (comparing predicted probabilities against annotator distributions) and hard evaluation (comparing predicted hard labels against aggregated annotator labels). Understanding both evaluation modes is crucial for interpreting results and optimizing the system.
  - Quick check question: How does ICM-Soft evaluation differ from traditional accuracy metrics in multi-annotator settings?

- Concept: Transformer-based multilingual models
  - Why needed here: The system uses mBERT and XLM-RoBERTa, which are pretrained multilingual transformers. Understanding their architecture, pretraining objectives, and limitations is essential for effective fine-tuning and ensemble design.
  - Quick check question: What are the key architectural differences between mBERT and XLM-RoBERTa that might affect their performance on sexism detection tasks?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training pipeline -> Ensemble module -> Adjustment module -> Evaluation module
- Critical path: Data → Model fine-tuning → Ensemble prediction → Adjustment → Evaluation
- Design tradeoffs:
  - Ensemble averaging vs. more complex combination methods (weighted averaging, stacking)
  - Adjustment operation complexity vs. potential evaluation metric improvement
  - Single vs. multiple fine-tuning epochs (balancing performance and computational cost)
- Failure signatures:
  - Low correlation between ensemble predictions and individual model predictions (suggests models capture similar patterns)
  - Large discrepancy between soft and hard evaluation results (indicates calibration issues)
  - Performance degradation after adjustment operation (suggests constraint violation)
- First 3 experiments:
  1. Train individual mBERT and XLM-RoBERTa models with 1-10 epochs and evaluate on validation set to identify optimal hyperparameters
  2. Implement basic ensemble without adjustment and compare ICM-Soft scores against individual models
  3. Add adjustment operation and measure impact on both soft and hard evaluation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ensemble strategies perform when incorporating additional multilingual transformer models like LLaMA or ELECTRA?
- Basis in paper: [explicit] The authors explicitly mention that "the proposed ensemble model can easily be extended to include also other Large Language Models that have shown promising performance on similar tasks (e.g., LLaMA [39] or ELECTRA [40])" and suggest this as future work.
- Why unresolved: The current study only uses mBERT and XLM-RoBERTa in their ensemble approach. The potential performance gains from adding other transformer models remain untested.
- What evidence would resolve it: Experimental results comparing ensemble performance with different combinations of transformer models (mBERT, XLM-RoBERTa, LLaMA, ELECTRA) on the EXIST tasks, showing whether additional models improve performance beyond the current two-model ensemble.

### Open Question 2
- Question: How do sentiment information and lexical features (e.g., uppercase usage, emojis) impact sexism detection performance in the learning with disagreements paradigm?
- Basis in paper: [explicit] The authors state "Another possible improvement of the proposed approach refers to the inclusion of additional features. While the proposed approach only considers the text within the tweet, sentiment information and lexical characteristics (e.g., the usage of uppercase or emoji) have been shown to be important clues for hate-related tasks."
- Why unresolved: The current approach uses only the raw text of tweets without incorporating additional linguistic features that could provide useful context for detection.
- What evidence would resolve it: Comparative experiments showing performance differences between the current text-only approach and approaches that incorporate sentiment scores and lexical features, demonstrating whether these additions improve detection accuracy under the learning with disagreements framework.

### Open Question 3
- Question: How would author profiling and annotator characteristics improve sexism detection performance when this information becomes available for the test dataset?
- Basis in paper: [explicit] The authors note "despite previous works showing the importance of author profiling and demonstrating the utility of exploiting annotators’ characteristics in disagreement detection, this information for the test dataset has not yet been released – making those strategies unfeasible for the participation in the challenge – but is a line of work that we plan to investigate in the near future."
- Why unresolved: The test dataset lacks information about tweet authors and annotator demographics, preventing exploration of how these factors might improve model performance.
- What evidence would resolve it: Experimental results comparing sexism detection models with and without author profiling features and annotator characteristics when such information becomes available, demonstrating whether these additional data points improve prediction accuracy in the learning with disagreements context.

## Limitations
- Performance evaluated only on EXIST 2023 dataset, limiting generalizability to other sexism detection tasks or domains
- Ensemble method assumes mBERT and XLM-RoBERTa capture complementary information, but this correlation is not empirically validated
- Probability adjustment operation that constrains predictions to match annotator counts is described but not thoroughly evaluated for its impact on performance

## Confidence
**High Confidence Claims:**
- The ensemble approach of mBERT and XLM-RoBERTa outperforms individual models on the EXIST 2023 dataset, as evidenced by the reported ICM-Soft and normalized ICM-Soft scores that achieved first place in Task 3 and fourth place in Task 2.
- Learning directly from soft labels without aggregation preserves annotator disagreement information and is effective for sexism characterization, supported by the paper's explicit methodology description and evaluation approach.

**Medium Confidence Claims:**
- The adjustment operation that aligns probability distributions with the number of annotators improves evaluation alignment with ICM-Soft metrics. While the method is described, the quantitative impact of this adjustment on performance is not clearly demonstrated.
- The learning with disagreements paradigm is particularly suited for sexism detection due to its subjective nature. This claim is reasonable but lacks direct empirical comparison against traditional hard label aggregation methods.

**Low Confidence Claims:**
- The specific reasons why mBERT and XLM-RoBERTa complement each other in capturing sexism-related linguistic patterns are not empirically validated. The claim about complementary information is plausible but untested.
- The approach will generalize well to other languages or domains beyond English and Spanish social media text. This remains speculative without cross-dataset validation.

## Next Checks
1. **Error Correlation Analysis**: Compute the correlation between mBERT and XLM-RoBERTa prediction errors on a validation set to empirically verify whether their errors are sufficiently uncorrelated to justify ensemble averaging. This would validate the fundamental assumption behind the ensemble approach.

2. **Ablation Study on Adjustment Operation**: Remove the probability adjustment step that constrains predictions to match annotator counts and measure the impact on both soft and hard evaluation metrics. This would quantify whether the adjustment provides meaningful benefits or introduces unnecessary complexity.

3. **Cross-Dataset Generalization Test**: Evaluate the trained ensemble model on a different sexism detection dataset (such as HateXplain or another publicly available corpus) to assess how well the approach generalizes beyond the EXIST 2023 data. This would address concerns about dataset-specific overfitting.