---
ver: rpa2
title: Histopathological Image Classification and Vulnerability Analysis using Federated
  Learning
arxiv_id: '2310.07380'
source_url: https://arxiv.org/abs/2310.07380
tags:
- data
- learning
- privacy
- accuracy
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores the application of federated learning (FL) in
  histopathological image classification, focusing on skin cancer diagnosis. The proposed
  approach enables privacy-preserving machine learning by training local models on
  decentralized data without data exchange.
---

# Histopathological Image Classification and Vulnerability Analysis using Federated Learning

## Quick Facts
- arXiv ID: 2310.07380
- Source URL: https://arxiv.org/abs/2310.07380
- Reference count: 23
- Primary result: Federated learning for skin cancer diagnosis is vulnerable to label-flipping attacks that degrade model accuracy

## Executive Summary
This study explores federated learning (FL) for histopathological image classification with a focus on skin cancer diagnosis. The proposed approach enables privacy-preserving machine learning by training local models on decentralized data without data exchange. The research demonstrates that FL models are vulnerable to data poisoning attacks, specifically label flipping, where a malicious client introduces corrupted labels that reduce global model accuracy. The findings highlight critical security concerns in FL-based healthcare diagnostics and emphasize the need for robust defenses against adversarial attacks.

## Method Summary
The study uses the HAM10000 dataset containing 10,015 dermoscopic images across 7 skin lesion classes distributed across 10 clients/hospitals with IID data. Federated Averaging (FedAvg) algorithm is implemented with 100 communication rounds, using an MLP model with 3 hidden layers (200 units each), SGD optimizer (learning rate 0.01, momentum 0.9), and categorical crossentropy loss with batch size 32. The methodology includes baseline evaluation and systematic introduction of label flipping attacks where one malicious client randomly flips 2-20% of labels to measure impact on global model performance.

## Key Results
- FL model achieved baseline accuracy of approximately 67% for skin lesion classification
- Label flipping attacks by malicious clients caused progressive accuracy degradation as attack percentage increased
- Minority classes (0th and 5th) showed complete precision/recall/F1-score loss under attack conditions
- Model performance was most vulnerable when poisoned samples dominated learning signals for underrepresented classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label flipping attacks degrade global model accuracy by introducing adversarial samples that mislead the federated averaging step
- Mechanism: A malicious client alters its local labels before training. During averaging, these poisoned updates shift the global model weights toward misclassifying certain classes, especially underrepresented ones
- Core assumption: The global model aggregates weights linearly, so adversarial updates can shift the consensus if not detected
- Evidence anchors:
  - [abstract] "Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accuracy of the global model."
  - [section] "In SGD, a batch of samples B is selected from the dataset D, and the corresponding gradient gB is computed. The model parameters θ are then updated in the direction of the negative gradient... Each h[i] updates the model parameters θr,i and sends them to the aggregator."
  - [corpus] Weak: Corpus neighbors focus on model poisoning and adversarial attacks but not explicitly label flipping in histopathology

### Mechanism 2
- Claim: Federated averaging is vulnerable to model drift when one client's local data distribution is adversarial
- Mechanism: When a client flips labels, its local model learns incorrect decision boundaries. During FedAvg, these misaligned gradients influence the global model's convergence, causing performance drops on clean data
- Core assumption: The averaging step assumes all clients are honest and their updates are in-distribution
- Evidence anchors:
  - [abstract] "As the percentage of label flipping increases, there is a noticeable decrease in accuracy."
  - [section] "A hospital h[i] may adopt the following steps to alter the outcome of the global model AI: 1) Select k samples from the labeled dataset... 2) For each k replace the skin-lesion class with an alternate random skin-lesion class..."
  - [corpus] Weak: No direct discussion of label flipping in corpus; only related to poisoning or adversarial attacks

### Mechanism 3
- Claim: Small class sizes amplify poisoning effects because poisoned samples dominate the learning signal for rare classes
- Mechanism: In the HAM10000 dataset, some skin lesion classes have few samples. Flipping labels in these classes can disproportionately skew the model's decision boundary, causing near-zero precision/recall for those classes
- Core assumption: Performance degradation is more visible on minority classes due to insufficient counteracting samples
- Evidence anchors:
  - [section] "The model does not perform well on the 0th as well as the 5th class due to a smaller number of samples for both of these classes."
  - [abstract] "The 0th,1st,2nd,3rd, and 5th classes had a precision, recall, and f1-score of 0.00..."
  - [corpus] Weak: No direct mention of class imbalance effects in corpus; inferred from performance results

## Foundational Learning

- Concept: Federated learning (FL)
  - Why needed here: Enables privacy-preserving training across distributed hospitals without centralizing sensitive skin lesion images
  - Quick check question: What distinguishes FL from traditional centralized training in terms of data flow?

- Concept: Data poisoning and label flipping
  - Why needed here: Central to understanding how malicious clients can degrade global model accuracy by corrupting local training data
  - Quick check question: How does flipping labels on a small subset of data impact the global model after averaging?

- Concept: Stochastic gradient descent (SGD) and FedAvg
  - Why needed here: These are the core optimization mechanisms used for local and global model updates in the federated setting
  - Quick check question: In FedAvg, how are local model updates combined on the server?

## Architecture Onboarding

- Component map: Clients (hospitals) -> Local training -> Model weight upload -> Aggregator -> Global model update -> Model redistribution
- Critical path:
  1. Global model initialization and distribution to clients
  2. Local training with (possibly poisoned) data
  3. Weight upload to aggregator
  4. FedAvg aggregation
  5. Global model update and redistribution
- Design tradeoffs:
  - Privacy vs. vulnerability: FL preserves privacy but opens door to poisoning
  - Model simplicity vs. robustness: Simple MLP is easy to train but more vulnerable than robust architectures
  - Communication efficiency vs. security: Frequent updates increase attack surface
- Failure signatures:
  - Sudden drops in accuracy on minority classes
  - High variance in model updates from a single client
  - Convergence stalls or oscillations during training
- First 3 experiments:
  1. Run FL with all honest clients; verify accuracy baseline (~67%)
  2. Introduce a malicious client flipping 5% of labels; measure accuracy drop
  3. Vary the percentage of flipped labels (2% to 20%) and plot accuracy degradation curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal defense mechanisms against data poisoning attacks in federated learning for histopathological image classification?
- Basis in paper: [explicit] The paper explicitly mentions the vulnerability of the FL model to data poisoning attacks and the need to address these vulnerabilities
- Why unresolved: The paper demonstrates the effectiveness of data poisoning attacks but does not propose or evaluate specific defense mechanisms
- What evidence would resolve it: Experimental results comparing the effectiveness of various defense strategies (e.g., anomaly detection, robust aggregation) against data poisoning attacks in the same FL setup

### Open Question 2
- Question: How does the performance of federated learning for skin lesion classification compare to centralized learning approaches when considering data privacy?
- Basis in paper: [explicit] The paper compares the accuracy of the FL model to a centralized learning setup using the same hyperparameters
- Why unresolved: The paper does not explore the trade-offs between model performance and data privacy in different learning paradigms
- What evidence would resolve it: A comprehensive study comparing the accuracy, privacy guarantees, and computational efficiency of FL and centralized learning for skin lesion classification

### Open Question 3
- Question: What is the impact of varying the percentage of label flipping on the overall accuracy and robustness of the federated learning model?
- Basis in paper: [explicit] The paper demonstrates that increasing the percentage of label flipping leads to a decrease in accuracy
- Why unresolved: The paper only explores a limited range of label flipping percentages and does not analyze the model's robustness to different attack intensities
- What evidence would resolve it: A systematic analysis of the model's performance under varying degrees of label flipping, including extreme cases and potential saturation points

## Limitations

- The study uses a simple MLP architecture that may not reflect vulnerabilities in more complex, modern federated learning systems
- Only label flipping attacks are explored, without testing other poisoning strategies or multi-client collusion scenarios
- No defensive mechanisms (e.g., anomaly detection, robust aggregation) are implemented or evaluated to mitigate poisoning attacks

## Confidence

- Claim: FL models are vulnerable to label-flipping attacks that degrade accuracy
  - Confidence: Medium
  - Basis: Clear demonstration of accuracy degradation with increasing attack severity, but limited testing of defense mechanisms
- Claim: Minority classes suffer complete performance loss under attack conditions
  - Confidence: High
  - Basis: Reproducible experimental results showing consistent zero scores for affected classes

## Next Checks

1. Implement and test robust aggregation methods (e.g., coordinate-wise median, trimmed mean) to assess their effectiveness against label-flipping attacks
2. Evaluate the attack's impact on non-IID data distributions, as real-world medical data is often heterogeneous across institutions
3. Measure attack transferability—whether a model poisoned on one dataset maintains its compromised behavior when fine-tuned on clean data from a different distribution