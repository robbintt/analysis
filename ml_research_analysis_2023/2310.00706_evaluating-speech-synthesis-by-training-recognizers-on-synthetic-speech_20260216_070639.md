---
ver: rpa2
title: Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech
arxiv_id: '2310.00706'
source_url: https://arxiv.org/abs/2310.00706
tags:
- speech
- synthetic
- real
- data
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new evaluation method for synthetic speech
  by training an ASR model on synthetic speech and evaluating its performance on real
  speech. The authors argue that this approach better captures the distributional
  shift between synthetic and real speech distributions compared to existing metrics
  like WER, SpeechLMScore, and MOSNet.
---

# Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech

## Quick Facts
- arXiv ID: 2310.00706
- Source URL: https://arxiv.org/abs/2310.00706
- Reference count: 0
- This paper proposes a new evaluation method for synthetic speech by training an ASR model on synthetic speech and evaluating its performance on real speech.

## Executive Summary
This paper introduces a novel evaluation method for synthetic speech quality by training an ASR model on synthetic speech and testing it on real speech. The authors argue that this approach better captures the distributional shift between synthetic and real speech distributions compared to existing metrics like WER, SpeechLMScore, and MOSNet. Experiments on three TTS systems (StyleTTS, MQTTS, YourTTS) show that their metric correlates well with human Mean Opinion Scores for naturalness and intelligibility, while other metrics do not. The method requires only small amounts of synthetic speech to train the ASR model.

## Method Summary
The method involves generating 10 hours of synthetic speech from TTS systems, fine-tuning a Whisper-medium ASR model using CTC loss on the synthetic speech, and then evaluating the fine-tuned model on real speech from test sets. The Word Error Rate (WER) of this evaluation serves as the proposed metric. The paper compares this metric against traditional approaches including pre-trained ASR WER, SpeechLMScore, MOSNet, and human Mean Opinion Scores (MOS-N for naturalness and MOS-I for intelligibility).

## Key Results
- The proposed metric shows positive correlation with human MOS scores for naturalness and intelligibility across three TTS systems
- Other metrics (pre-trained ASR WER, SpeechLMScore, MOSNet) do not correlate well with human evaluations
- The method requires only 10 hours of synthetic speech to train the ASR model effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training ASR on synthetic speech and evaluating on real speech captures distributional shift better than pre-trained ASR evaluation.
- Mechanism: The ASR model trained on synthetic data models the synthetic speech distribution. Its ability to recognize real speech reflects how well the synthetic distribution matches the real distribution.
- Core assumption: ASR models approximate the underlying data distribution, and classification accuracy differences between training and test distributions reflect distributional shift.
- Evidence anchors:
  - [abstract] "Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions"
  - [section] "We argue that on the other hand, an ASR model trained on the synthetic speech and evaluated on real speech better captures the statistical difference between the two"
  - [corpus] Weak evidence - no direct corpus citations about ASR distribution modeling
- Break condition: If synthetic speech distribution is too narrow or lacks diversity, ASR may overfit and fail to capture meaningful distributional differences.

### Mechanism 2
- Claim: ASR models trained on synthetic speech show higher WER on real speech when distributions differ significantly.
- Mechanism: Synthetic speech typically has lower variance than real speech. When ASR is trained on low-variance synthetic data, it struggles with high-variance real data, revealing distributional differences.
- Core assumption: Synthetic speech distributions have lower variance than real speech distributions.
- Evidence anchors:
  - [section] "Note that the real data has more variance than the synthetic data (which is true for the real and synthetic speech)"
  - [section] "The higher the difference in the joint distributions in the real and synthetic distributions, the greater the range of errors in the real data"
  - [corpus] Weak evidence - no corpus citations about variance differences in speech
- Break condition: If synthetic speech generation produces high-variance outputs that closely match real speech distribution, this mechanism may fail.

### Mechanism 3
- Claim: Small amounts of synthetic speech (10 hours) are sufficient to train an effective ASR model for evaluation.
- Mechanism: ASR models can learn the synthetic speech distribution effectively with limited data, as the synthetic distribution is typically more constrained and regular than real speech.
- Core assumption: Synthetic speech distributions are more regular and easier to model than real speech distributions.
- Evidence anchors:
  - [abstract] "The method requires only small amounts of synthetic speech to train the ASR model"
  - [section] "Experiments using 3 public open-source speech synthesis systems show that our model correlates positively with subjective human Mean Opinion Scores"
  - [corpus] Weak evidence - no corpus citations about minimum training data requirements
- Break condition: If synthetic speech distribution is too complex or varied, more training data may be required than anticipated.

## Foundational Learning

- Concept: Distributional shift and divergence metrics
  - Why needed here: The core idea is measuring how different synthetic and real speech distributions are using ASR performance
  - Quick check question: What's the difference between KL divergence and the proposed asymmetric divergence metric?

- Concept: Automatic Speech Recognition (ASR) systems
  - Why needed here: The evaluation method relies on training and testing ASR models on different speech types
  - Quick check question: How does Whisper-medium differ from other ASR architectures?

- Concept: Mean Opinion Score (MOS) evaluation
  - Why needed here: The proposed metric is validated against MOS for naturalness and intelligibility
  - Quick check question: Why is MOS considered the gold standard for speech quality evaluation?

## Architecture Onboarding

- Component map:
  TTS Systems (StyleTTS, MQTTS, YourTTS) -> Synthetic Speech Generation -> Whisper-medium ASR Fine-tuning -> WER Evaluation on Real Speech -> Correlation with MOS Scores

- Critical path:
  1. Generate synthetic speech using TTS systems
  2. Fine-tune Whisper-medium on synthetic speech
  3. Evaluate fine-tuned model on real speech
  4. Calculate WER and correlate with MOS scores

- Design tradeoffs:
  - Small training data (10h) vs. model performance
  - Synthetic speech quality vs. evaluation reliability
  - Model complexity vs. training time

- Failure signatures:
  - Low correlation between proposed metric and MOS scores
  - High variance in WER across different TTS systems
  - ASR overfitting to synthetic speech distribution

- First 3 experiments:
  1. Train ASR on synthetic speech and evaluate on real speech with varying amounts of training data (5h, 10h, 20h)
  2. Compare WER correlation with MOS scores across different TTS systems
  3. Test the effect of synthetic speech quality on evaluation reliability by using TTS systems with known quality differences

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The correlation between ASR WER on real speech and human MOS scores lacks statistical significance testing details and confidence intervals
- The claim that 10 hours of synthetic speech is sufficient for reliable evaluation is not rigorously validated across different TTS architectures or speech domains
- No analysis of how domain-specific characteristics (accent, speaking rate, background noise) affect the proposed metric's reliability

## Confidence
- **High confidence**: The core methodology of training ASR on synthetic speech and evaluating on real speech is technically sound and well-established
- **Medium confidence**: The claim that this approach better captures distributional shift than pre-trained ASR evaluation, based on limited comparative experiments
- **Low confidence**: The assertion that this method requires only "small amounts" of synthetic speech without systematic validation of minimum effective training data

## Next Checks
1. Conduct statistical significance testing (p-values, confidence intervals) on the correlation between proposed metric and MOS scores across all TTS systems
2. Systematically vary synthetic speech training data volume (1h, 5h, 10h, 20h) to empirically determine minimum effective training requirements
3. Test the metric's reliability across different speech domains (conversational speech, accented speech, noisy environments) beyond the LibriTTS corpus