---
ver: rpa2
title: 'Assessing Logical Puzzle Solving in Large Language Models: Insights from a
  Minesweeper Case Study'
arxiv_id: '2311.07387'
source_url: https://arxiv.org/abs/2311.07387
tags:
- cell
- action
- cells
- reasoning
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) show promising performance on many
  tasks but struggle with logical puzzle-solving that requires multi-step reasoning
  and planning. This paper evaluates LLMs on Minesweeper, a logic puzzle game designed
  to test reasoning capabilities unfamiliar to their training data.
---

# Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study

## Quick Facts
- arXiv ID: 2311.07387
- Source URL: https://arxiv.org/abs/2311.07387
- Reference count: 40
- Key outcome: LLMs show promise on many tasks but struggle with multi-step logical reasoning in unfamiliar puzzle contexts

## Executive Summary
This paper evaluates large language models' (LLMs) logical reasoning capabilities using Minesweeper, a logic puzzle designed to be unfamiliar to their training data. Experiments with GPT-3.5 and GPT-4 variants reveal that while LLMs can understand basic game mechanics, they fail to integrate these skills into coherent multi-step logical reasoning processes. GPT-3.5 models particularly struggle with generating valid actions and maintaining logical reasoning chains, often repeating previous actions. GPT-4 shows some improvement in simpler scenarios but still faces challenges with hallucinations and inconsistent context awareness. The study highlights the need for further research to develop more sophisticated AI reasoning and planning models that can handle complex logical puzzles beyond pattern recognition and knowledge retrieval.

## Method Summary
The study evaluates LLMs on 5x5 Minesweeper boards with 4 hidden mines, presented in both table and coordinate text representations. Using GPT-3.5 and GPT-4 models with prompt engineering techniques including chain-of-thought reasoning and few-shot in-context learning, the researchers systematically test model performance across varying difficulty levels. The experimental setup includes 100 random board configurations with initial clicks that reveal at least 10 cells, measuring performance through valid action counts, correctly identified mines, completed games, and logical coherence of reasoning chains.

## Key Results
- GPT-3.5 models demonstrate significant deficiencies in identifying cell states and maintaining logical reasoning chains, failing approximately one-third of the time in board navigation tasks
- All GPT-3.5 variants exhibit repetitive action patterns during board solving, persisting even with explicit instructions to avoid repetition
- GPT-4 shows improved performance in simple scenarios but still struggles with hallucinations and inconsistent context awareness when faced with complex reasoning requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform basic symbolic reasoning tasks when the input format aligns with their training distribution.
- Mechanism: Models leverage learned pattern matching to map symbols to actions and perform simple arithmetic when the task structure is familiar.
- Core assumption: The model's pre-training included sufficient examples of symbol manipulation and spatial reasoning tasks.
- Evidence anchors:
  - [abstract]: "GPT-3.5 models particularly struggle with generating valid actions and maintaining logical reasoning chains, often repeating previous actions"
  - [section 4.1]: "GPT-3.5-16k demonstrated a notable deficiency in accurately identifying the cell state, failing approximately one-third of the time in board navigation tasks"
  - [corpus]: Weak evidence - the corpus papers focus on puzzle-solving but don't specifically address Minesweeper-style reasoning
- Break condition: When the input format deviates from training distribution (e.g., table representations with backtick quotes) or requires multi-step planning beyond simple pattern matching.

### Mechanism 2
- Claim: LLMs can learn to solve simple Minesweeper scenarios through example-based reasoning when given explicit demonstrations.
- Mechanism: Models imitate reasoning patterns from provided examples rather than deriving logical rules from first principles.
- Core assumption: The model has sufficient capacity to memorize and apply reasoning patterns from examples to similar situations.
- Evidence anchors:
  - [abstract]: "GPT-4 shows some improvement in simpler scenarios but still faces challenges with hallucinations and inconsistent context awareness"
  - [section 4.2]: "GPT-4 can logically reason if the number of neighboring unopened cells corresponds to the remaining unflagged mines near a numbered cell, specifically in simple scenarios like `1' and `2'"
  - [corpus]: Explicit evidence - "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles" suggests example-based learning works for similar tasks
- Break condition: When scenarios become more complex or require reasoning beyond what's demonstrated in examples.

### Mechanism 3
- Claim: LLMs struggle with maintaining coherent multi-step reasoning chains due to limited attention to historical context.
- Mechanism: Models focus on immediate board states while losing track of previous deductions and action constraints, leading to repetitive or contradictory actions.
- Core assumption: Transformer-based architectures prioritize recent context over historical consistency when processing sequential tasks.
- Evidence anchors:
  - [abstract]: "GPT-3.5 models particularly struggle with generating valid actions and maintaining logical reasoning chains, often repeating previous actions"
  - [section 4.2]: "All GPT-3.5 models present a noticeable pattern of repetitive actions during board solving, persisting even with direct instructions to avoid repetition"
  - [corpus]: Strong evidence - "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving" directly addresses this limitation in similar reasoning tasks
- Break condition: When the task requires maintaining consistency across multiple reasoning steps or when the action space becomes too large for the model to track.

## Foundational Learning

- Concept: Spatial reasoning and neighbor counting
  - Why needed here: Minesweeper requires understanding cell relationships and counting adjacent mines
  - Quick check question: Can you explain how to count the number of neighboring mines for a given cell?

- Concept: Logical deduction and constraint satisfaction
  - Why needed here: Players must deduce mine locations based on numerical clues and eliminate impossible configurations
  - Quick check question: If a cell shows '2' and has 3 unopened neighbors, what can you conclude about those neighbors?

- Concept: Sequential planning and state tracking
  - Why needed here: Successful gameplay requires maintaining a coherent plan across multiple moves while updating beliefs based on new information
  - Quick check question: How would you update your strategy after revealing a cell that doesn't contain a mine?

## Architecture Onboarding

- Component map: Input parser -> State tracker -> Reasoning engine -> Output formatter
- Critical path:
  1. Parse input board representation
  2. Update internal state with new information
  3. Generate reasoning chain for next action
  4. Validate action against game rules
  5. Format and output action
- Design tradeoffs:
  - Table vs coordinate representation: Table is more human-readable but harder for LLMs to parse; coordinate is less intuitive but more effective
  - Natural conversation vs compact history: NC provides context but risks exceeding token limits; CH is efficient but may lose important reasoning context
  - Action validation: Strict validation prevents invalid moves but may frustrate the model; lenient validation allows exploration but risks game failure
- Failure signatures:
  - Repetitive actions: Model fails to track previous moves or update state correctly
  - Invalid coordinates: Parsing errors or boundary condition issues
  - Inconsistent reasoning: Model loses track of previous deductions or makes contradictory statements
  - Context truncation: Important historical information gets cut off due to token limits
- First 3 experiments:
  1. Board understanding validation: Test model's ability to correctly identify cell states and count neighbors in both table and coordinate formats
  2. Single-step reasoning: Evaluate model's performance on simple Minesweeper scenarios that require only one logical deduction
  3. Multi-step planning: Test model's ability to maintain coherent reasoning chains across 3-5 moves in increasingly complex scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different board representations (table vs coordinate) affect LLM performance in Minesweeper?
- Basis in paper: [explicit] The paper explicitly compares table and coordinate representations and finds coordinate representation superior
- Why unresolved: While the paper identifies the superior performance of coordinate representation, it doesn't explore why this difference exists or investigate other potential board representation methods
- What evidence would resolve it: Experiments testing alternative table formats (HTML, Markdown), image-based approaches, or systematic analysis of why coordinate representation works better

### Open Question 2
- Question: Can fine-tuning LLMs with specific reasoning capabilities improve their Minesweeper performance?
- Basis in paper: [inferred] The paper mentions this as a limitation and potential future research direction, noting they didn't conduct fine-tuning experiments due to computational constraints
- Why unresolved: The paper only tested zero-shot performance and didn't explore whether targeted fine-tuning could develop genuine reasoning capabilities
- What evidence would resolve it: Experiments comparing fine-tuned models against original models on matching vs. varying symbol usage, board sizes, and representations

### Open Question 3
- Question: What is the extent of GPT-4's reasoning capabilities versus pattern matching in Minesweeper?
- Basis in paper: [explicit] The paper discusses GPT-4's ability to learn from examples and apply knowledge to similar situations, but questions whether this stems from genuine reasoning or pattern matching
- Why unresolved: The paper observes that removing examples dramatically reduces GPT-4's coherent reasoning, suggesting example-based learning rather than true rule comprehension
- What evidence would resolve it: Systematic experiments varying example quantity and content, testing GPT-4's ability to generalize rules to novel scenarios without example support

## Limitations
- Evaluation focuses on a single puzzle game (Minesweeper), limiting generalizability to other logical reasoning domains
- Only tested zero-shot performance without exploring whether fine-tuning could develop genuine reasoning capabilities
- Limited exploration of alternative board representations beyond table and coordinate formats

## Confidence
- High Confidence: LLMs can understand basic game mechanics and perform simple symbolic reasoning tasks when input formats align with training distributions
- Medium Confidence: GPT-4 shows improvement in simpler scenarios but still faces challenges with hallucinations and inconsistent context awareness
- Low Confidence: The claim that current LLMs cannot perform multi-step logical reasoning beyond pattern recognition and knowledge retrieval

## Next Checks
1. **Cross-domain reasoning validation**: Test the same LLMs on other unfamiliar logical puzzles (e.g., Sudoku, Slitherlink) to assess whether the observed limitations generalize beyond Minesweeper.
2. **Attention mechanism analysis**: Conduct detailed analysis of model attention patterns during multi-step reasoning tasks to identify whether the repetitive behavior stems from attention collapse or state-tracking failures.
3. **Alternative architectural evaluation**: Compare performance using models with enhanced context windows and memory mechanisms to determine if the reasoning limitations are architecture-specific or fundamental to current transformer-based approaches.