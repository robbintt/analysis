---
ver: rpa2
title: Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification
arxiv_id: '2305.14827'
source_url: https://arxiv.org/abs/2305.14827
tags:
- intent
- utterances
- pre-training
- names
- intents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel pre-training method for text encoders
  using contrastive learning with pseudo-intent labels to improve zero- and few-shot
  intent classification performance. The method involves training an Intent Role Labeling
  (IRL) tagger to extract key phrases from utterances that are crucial for interpreting
  intents, and using these extracted phrases to create pseudo-intent names for pre-training
  a text encoder.
---

# Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification

## Quick Facts
- arXiv ID: 2305.14827
- Source URL: https://arxiv.org/abs/2305.14827
- Authors: 
- Reference count: 5
- Key outcome: Up to 5.4% and 4.0% higher accuracy than previous state-of-the-art for N-way zero- and one-shot intent classification

## Executive Summary
This paper introduces a novel pre-training method for text encoders using contrastive learning with pseudo-intent labels to improve zero- and few-shot intent classification performance. The method involves training an Intent Role Labeling (IRL) tagger to extract key phrases from utterances that are crucial for interpreting intents, and using these extracted phrases to create pseudo-intent names for pre-training a text encoder. The pre-trained intent-aware encoder (PIE) model is then optimized by pulling the gold utterance, gold intent, and pseudo intent close to the input utterance in the embedding space. Experiments on four intent classification datasets demonstrate that the proposed model achieves up to 5.4% and 4.0% higher accuracy than the previous state-of-the-art pre-trained sentence encoder for the N-way zero- and one-shot settings, respectively.

## Method Summary
The method trains an Intent Role Labeling (IRL) tagger to extract key phrases (Action, Argument, Request, Query, Slot, Problem) from utterances. These spans are concatenated to form pseudo-intent names, which are used in contrastive learning with a MPNet-based encoder. The pre-training objective combines three loss terms: Lgold_intent (utterance-gold intent pairs), Lgold_utterance (utterance-gold utterance pairs), and Lpseudo (utterance-pseudo intent pairs). The pre-trained PIE model is then fine-tuned on downstream intent classification datasets using Prototypical Networks with intent names as support examples.

## Key Results
- PIE achieves 88.5% average accuracy in one-shot setting, surpassing SBERTParaphrase by 2.8%
- PIE shows 5.4% and 4.0% higher accuracy than previous state-of-the-art in zero- and one-shot settings respectively
- Performance gains are consistent across four different intent classification datasets (Banking77, HWU64, Liu54, Clinc150)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-intent names generated by the IRL tagger provide informative contrastive supervision that aligns utterance and intent embeddings.
- Mechanism: The IRL tagger extracts intent-relevant spans (Action, Argument, Query) from utterances. These spans are concatenated to form pseudo intent names, which are then used as positive pairs in contrastive learning. The InfoNCE loss pulls these pseudo intents close to their source utterances in embedding space.
- Core assumption: Extracted spans from IRL tagger contain sufficient semantic information to approximate true intent names for pre-training purposes.
- Evidence anchors:
  - [abstract] "we first train a tagger to identify key phrases within utterances that are crucial for interpreting intents. We then use these extracted phrases to create examples for pre-training a text encoder in a contrastive manner."
  - [section 3.3] "To generate pseudo intent names, we simply concatenate all spans that have been predicted as IRL labels in each utterance."
  - [corpus] Weak - only 5 related papers found, none directly addressing pseudo-intent generation methodology.

### Mechanism 2
- Claim: Including gold utterances as additional positive pairs improves encoder's ability to distinguish between similar intents.
- Mechanism: Beyond utterance-intent and utterance-pseudo-intent pairs, the model also treats randomly sampled utterances with the same gold intent as positive pairs. This creates a three-way alignment: utterance ↔ gold intent, utterance ↔ gold utterance, utterance ↔ pseudo intent.
- Core assumption: Utterances sharing the same gold intent have similar semantic representations that can serve as useful positive examples.
- Evidence anchors:
  - [section 4] "The second supervised positive pair is between the input utterances and their gold utterances. We define gold utterances as randomly sampled utterances that share the same gold intent names as the input utterances."
  - [section 4] "Our final loss is a combination of these three losses..."
  - [corpus] Missing - no direct evidence in related papers about using gold utterances as contrastive pairs.

### Mechanism 3
- Claim: Pre-training with pseudo-intents creates embeddings that generalize better to unseen intents than standard sentence encoders.
- Mechanism: The PIE model is pre-trained on multiple dialogue datasets using the intent-aware contrastive loss. This forces the encoder to learn representations where utterances and their corresponding (gold or pseudo) intents are close in embedding space, even for intents not seen during downstream fine-tuning.
- Core assumption: The semantic structure learned during pre-training transfers to downstream intent classification tasks, especially in low-data regimes.
- Evidence anchors:
  - [abstract] "our PIE model achieves up to 5.4% and 4.0% higher accuracy than the previous state-of-the-art pre-trained sentence encoder for the N-way zero- and one-shot settings on four IC datasets."
  - [section 5.5] "The results demonstrate that PIE achieves an average accuracy of 88.5%, surpassing SBERTParaphrase... by 2.8% in the one-shot setting."
  - [corpus] Weak - related papers focus on different aspects (image retrieval, music retrieval) without directly addressing intent classification pre-training.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: The core training objective relies on pulling positive pairs (utterance-intent, utterance-utterance, utterance-pseudo-intent) together while pushing apart negative pairs in embedding space.
  - Quick check question: What is the difference between supervised and semi-supervised positive pairs in this context?

- Concept: Intent Role Labeling (IRL)
  - Why needed here: IRL provides the mechanism for extracting intent-relevant spans that are used to generate pseudo-intent names for pre-training.
  - Quick check question: How does IRL differ from standard semantic role labeling in terms of the labels it predicts?

- Concept: Prototypical Networks for Few-shot Learning
  - Why needed here: The downstream evaluation uses Prototypical Networks, which require good utterance embeddings to compute class prototypes and make predictions.
  - Quick check question: In Prototypical Networks, how are class prototypes computed from support examples?

## Architecture Onboarding

- Component map: Pre-training corpus utterances -> IRL Tagger -> Pseudo intent names -> PIE Encoder -> Contrastive Loss -> Downstream Classifier (Prototypical Networks)
- Critical path:
  1. Pre-training corpus utterances → IRL Tagger → Pseudo intent names
  2. Encoder processes (utterance, gold intent, pseudo intent, gold utterance) triples
  3. Contrastive loss optimizes embeddings to align these pairs
  4. Downstream: Support examples + intent names → PIE embeddings → Prototypes → Query classification
- Design tradeoffs:
  - Using pseudo-intents vs. requiring expensive gold intent annotations
  - Including gold utterances as positives vs. simpler utterance-intent pairs only
  - Three-way contrastive loss vs. simpler two-way approaches
  - Pre-training on dialogue data vs. general text corpora
- Failure signatures:
  - Poor IRL tagger performance (low F1 on Action/Argument labels) → noisy pseudo-intents
  - Overfitting to pre-training corpus → poor downstream generalization
  - Loss imbalance → one term dominates training
  - Insufficient negative pairs in batch → weak contrastive signal
- First 3 experiments:
  1. Train PIE with only Lgold_intent (remove Lgold_utterance and Lpseudo) to measure contribution of each loss term
  2. Replace pseudo-intents with random strings to confirm they provide meaningful signal
  3. Evaluate PIE on a dataset with overlapping intents between pre-training and downstream to measure transfer effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PIE compare to other state-of-the-art methods when fine-tuned on task-specific data?
- Basis in paper: [explicit] The authors compare PIE to SBERTParaphrase and BERTTAPT in zero- and one-shot settings without fine-tuning on task-specific data.
- Why unresolved: The paper does not provide results for PIE when fine-tuned on task-specific data, which could reveal if it has a higher ceiling than other methods.
- What evidence would resolve it: Fine-tuning PIE on task-specific data and comparing its performance to other state-of-the-art methods in zero- and one-shot settings.

### Open Question 2
- Question: How does the performance of PIE vary with different types of pre-training data, such as non-dialogue datasets?
- Basis in paper: [explicit] The authors perform an ablation study on different dialogue datasets for pre-training PIE.
- Why unresolved: The paper does not explore the impact of using non-dialogue datasets for pre-training PIE, which could reveal if it can leverage diverse text sources.
- What evidence would resolve it: Pre-training PIE on non-dialogue datasets and comparing its performance to the current approach using dialogue datasets.

### Open Question 3
- Question: How does the performance of PIE vary with different IRL tagger configurations, such as using different labels or weighting them differently?
- Basis in paper: [explicit] The authors use a specific set of IRL labels and treat them equally when constructing pseudo-intents.
- Why unresolved: The paper does not explore the impact of using different IRL tagger configurations, which could reveal if certain labels or weightings are more effective for pre-training PIE.
- What evidence would resolve it: Experimenting with different IRL tagger configurations, such as using different labels or weighting them differently, and comparing the performance of PIE.

## Limitations
- The paper doesn't provide quantitative evidence showing how IRL tagger performance correlates with downstream intent classification accuracy
- The individual contributions of the three contrastive loss terms are not clearly isolated through ablations
- The paper doesn't explore whether simpler pseudo-intent generation methods would achieve similar results

## Confidence
- High confidence: The overall methodology of using contrastive learning for pre-training intent-aware encoders is sound and well-motivated by the literature.
- Medium confidence: The specific approach of using IRL-extracted spans for pseudo-intent generation improves performance, though the evidence is primarily comparative rather than mechanistic.
- Low confidence: The individual contributions of the three contrastive loss terms are not clearly isolated through ablations.

## Next Checks
1. **IRL Tagger Quality Analysis**: Run a controlled experiment measuring the correlation between IRL tagger F1 scores (particularly on Action and Argument labels) and downstream intent classification accuracy across different datasets. This would quantify whether pseudo-intent quality directly impacts performance.

2. **Ablation of Gold Utterance Component**: Train PIE models with only Lgold_intent and Lpseudo (removing Lgold_utterance), and separately with only Lgold_intent and Lgold_utterance (removing Lpseudo). Compare these to the full model to isolate the contribution of each positive pair type.

3. **Pseudo-intent Generation Comparison**: Replace IRL-based pseudo-intents with simpler methods (e.g., extracting top TF-IDF keywords, using n-grams, or randomly sampled spans) while keeping the same contrastive learning framework. This would test whether the complexity of IRL tagging is necessary or if simpler approaches achieve similar gains.