---
ver: rpa2
title: 'Neural networks for insurance pricing with frequency and severity data: a
  benchmark study from data preprocessing to technical tariff'
arxiv_id: '2310.12671'
source_url: https://arxiv.org/abs/2310.12671
tags:
- data
- neural
- frequency
- cann
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares deep learning architectures against standard
  actuarial models for frequency-severity insurance pricing. It evaluates a feed-forward
  neural network (FFNN), combined actuarial neural network (CANN), generalized linear
  model (GLM), and gradient-boosted trees (GBM) on four real-world insurance datasets.
---

# Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff

## Quick Facts
- arXiv ID: 2310.12671
- Source URL: https://arxiv.org/abs/2310.12671
- Reference count: 9
- Key outcome: Deep learning models (FFNN, CANN) match or slightly outperform GLM/GBM benchmarks in insurance frequency-severity pricing, with autoencoder embeddings providing notable gains

## Executive Summary
This paper benchmarks deep learning architectures against standard actuarial models for insurance pricing using frequency and severity data. The authors evaluate feed-forward neural networks (FFNN), combined actuarial neural networks (CANN), generalized linear models (GLM), and gradient-boosted trees (GBM) across four real-world insurance datasets. They develop an autoencoder-based embedding technique for categorical variables and introduce a flexible CANN variant with trainable skip connection weights. Results show that CANN with GBM input and flexible output consistently achieves the lowest out-of-sample deviance for frequency modeling, while severity modeling shows comparable performance across all approaches. The paper also demonstrates how surrogate GLM models can translate neural network insights into interpretable tariff structures.

## Method Summary
The study preprocesses four insurance datasets by normalizing continuous variables, one-hot encoding categoricals, and using autoencoders to create compact embeddings. Four model architectures are implemented: FFNN with autoencoder embeddings, CANN combining GLM/GBM baselines with neural network corrections, GLM on binned data, and GBM. Hyperparameter tuning employs random grid search with 6×5-fold cross-validation. Models are evaluated using Poisson deviance for frequency and gamma deviance for severity. The authors also construct surrogate GLMs to interpret neural network predictions and compare technical tariff structures using Lorenz curves.

## Key Results
- CANN with GBM input and flexible output layer achieves lowest frequency deviance across all datasets
- Autoencoder embedding notably improves FFNN performance for both frequency and severity tasks
- All models perform comparably on severity modeling with no single approach dominating
- Surrogate GLMs outperform benchmark GLMs in both predictive accuracy and risk differentiation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The autoencoder embedding outperforms one-hot encoding for frequency and severity modeling, especially when data volume is limited.
- Mechanism: Autoencoders learn compact, continuous representations of categorical variables in an unsupervised manner, capturing interactions between categories and reducing dimensionality before supervised training.
- Core assumption: The embedding dimension d is sufficiently small to avoid overfitting but large enough to preserve information from the categorical variables.
- Evidence anchors:
  - [abstract]: "The autoencoder embedding notably improves FFNN performance for both frequency and severity tasks."
  - [section 3.2]: Describes autoencoder structure, softmax transformation, and scaling process.
  - [corpus]: Weak evidence; neighboring papers do not discuss autoencoders specifically for insurance categorical variables.
- Break condition: If the categorical variable cardinality is very low, one-hot encoding may perform as well as embeddings without added complexity.

### Mechanism 2
- Claim: The flexible CANN (with trainable output weights) consistently outperforms the fixed CANN for frequency modeling.
- Mechanism: Allowing trainable weights in the skip connection lets the network dynamically adjust the influence of the initial model (GBM/GLM) versus the neural network adjustment, improving the log-link fit.
- Core assumption: The initial model predictions contain useful baseline information that can be improved via learnable adjustments.
- Evidence anchors:
  - [abstract]: "flexible CANN variant that allows trainable weights in the skip connection, improving predictive accuracy."
  - [section 3.1]: Shows output equation with trainable wNN and wIN.
  - [corpus]: No direct evidence; neighboring papers do not mention CANN variants with trainable weights.
- Break condition: If the initial model is already optimal, flexible weights may overfit or add noise.

### Mechanism 3
- Claim: Combining GBM and neural network in a CANN structure yields the lowest frequency deviance across datasets.
- Mechanism: GBM provides strong, non-linear baseline predictions; the neural network adds fine-grained corrections on top of this baseline via skip connection, capturing residual patterns.
- Core assumption: The GBM baseline is stronger than GLM, and the neural network can learn meaningful corrections.
- Evidence anchors:
  - [abstract]: "For frequency modeling, the CANN with GBM input and flexible output layer consistently achieves the lowest out-of-sample deviance across datasets."
  - [section 3.1]: Describes CANN with GBM input and flexible output layer.
  - [corpus]: Weak; neighboring papers focus on transformers or tabular models, not GBM+NN combinations.
- Break condition: If the dataset is small or has few features, the neural network may not have enough data to learn meaningful corrections.

## Foundational Learning

- Concept: Tabular data preprocessing for deep learning
  - Why needed here: Neural networks require numerical inputs; categorical variables must be embedded and continuous variables normalized.
  - Quick check question: What happens if you skip normalization for continuous features before feeding them into the FFNN?

- Concept: Embedding dimension selection
  - Why needed here: Too large → overfitting; too small → information loss. Cross-validation is used to select the smallest d with acceptable reconstruction loss.
  - Quick check question: How does the autoencoder's reconstruction loss threshold (0.001) relate to embedding quality?

- Concept: Hyperparameter tuning with random grid search
  - Why needed here: Neural network performance is sensitive to architecture choices; random grid search balances exploration and computational cost.
  - Quick check question: Why use 3 repetitions per parameter set instead of 1?

## Architecture Onboarding

- Component map: Continuous variables → normalization → Input layer; Categorical variables → one-hot encoding → Autoencoder → scaled encoder → Input layer; Hidden layers (configurable depth/width, dropout, activation) → Output layer (exponential activation for frequency/severity predictions); CANN variant: Skip connection from GLM/GBM baseline to output

- Critical path:
  1. Preprocess continuous variables (normalize)
  2. One-hot encode categoricals
  3. Train autoencoder on one-hot encodings → scaled encoder
  4. Build FFNN/CANN with encoder integrated
  5. Hyperparameter tuning via random grid + cross-validation
  6. Train final model on full training set with best params
  7. Evaluate on held-out test set

- Design tradeoffs:
  - Encoder dimension vs. overfitting
  - Number of hidden layers vs. computational cost
  - Fixed vs. flexible CANN output weights
  - Batch size vs. memory constraints

- Failure signatures:
  - High validation loss → check embedding quality, overfitting
  - Deviance worse than GLM → encoder may be too small or network too shallow
  - Unstable predictions → check normalization, batch size, dropout

- First 3 experiments:
  1. Run baseline GLM and GBM for frequency/severity on all datasets
  2. Train autoencoder with d=5,10,15; select lowest d with reconstruction loss < 0.001
  3. Train FFNN and CANN GBM flex; compare Poisson/gamma deviances against benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CANN models compare to other deep learning architectures like transformers or attention-based models on insurance frequency-severity data?
- Basis in paper: [explicit] The paper compares FFNN, CANN, GLM, and GBM but does not explore more advanced architectures like transformers or attention mechanisms that have shown promise in other domains.
- Why unresolved: The paper's benchmark focuses on traditional deep learning approaches (FFNN, CANN) against classical actuarial models, leaving questions about whether newer architectures could provide further improvements.
- What evidence would resolve it: Direct comparison of CANN against transformer-based or attention-based models on the same insurance datasets, using identical preprocessing and evaluation protocols.

### Open Question 2
- Question: What is the optimal embedding dimension for autoencoder-based categorical variable encoding in insurance frequency-severity models?
- Basis in paper: [explicit] The paper mentions tuning the embedding dimension across {5, 10, 15} but does not provide systematic analysis of how different dimensions affect performance or whether there are diminishing returns.
- Why unresolved: The paper only briefly mentions the embedding dimension selection process without exploring the relationship between dimension size and predictive accuracy or model interpretability.
- What evidence would resolve it: Comprehensive study varying embedding dimensions from very small to very large, analyzing the trade-off between model performance, interpretability, and computational efficiency.

### Open Question 3
- Question: How do CANN models perform on high-dimensional insurance datasets with complex feature types like images, text, or time series data?
- Basis in paper: [inferred] The paper acknowledges that future research could explore datasets with high-dimensional feature sets including images, text, or time series, and that deep learning might provide more value in such contexts.
- Why unresolved: The current benchmark uses traditional tabular data with numerical, categorical, and spatial variables, but the authors suggest that more complex data types might benefit more from deep learning approaches.
- What evidence would resolve it: Benchmark study comparing CANN and other deep learning models against traditional approaches on insurance datasets containing telematics time series, accident scene images, or policy text descriptions.

## Limitations

- Claims about autoencoder superiority over all alternatives rely on limited evidence from a single study
- Flexible CANN mechanism is not fully validated across different dataset sizes or distributions
- Surrogate GLM interpretation may oversimplify neural network decision boundaries, potentially losing important non-linear patterns

## Confidence

- High Confidence: GLM and GBM benchmark results, autoencoder reconstruction quality thresholds, basic CANN architecture
- Medium Confidence: Performance rankings across datasets, the general benefit of embeddings, CANN architecture with flexible weights
- Low Confidence: Claims about autoencoder superiority over all alternatives, specific hyperparameter choices, transferability of results to non-insurance domains

## Next Checks

1. Test the autoencoder embedding against entity embeddings and feature hashing on the same datasets to verify claimed superiority
2. Conduct ablation studies removing the skip connection in CANN to quantify its contribution to performance gains
3. Validate surrogate GLM interpretability by comparing its risk differentiation against SHAP values from the neural network on held-out data