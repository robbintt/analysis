---
ver: rpa2
title: Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov
  Decision Processes
arxiv_id: '2307.15931'
source_url: https://arxiv.org/abs/2307.15931
tags:
- network
- information
- action
- observation
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops robust deep reinforcement learning algorithms
  to address the challenge of non-static disturbances in partially observable Markov
  decision processes (POMDPs). The core method extends the Twin Delayed Deep Deterministic
  policy gradient (TD3) algorithm with Long Short-Term Memory (LSTM) networks, incorporating
  action sequences as input to improve robustness.
---

# Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov Decision Processes

## Quick Facts
- arXiv ID: 2307.15931
- Source URL: https://arxiv.org/abs/2307.15931
- Reference count: 35
- Key outcome: This study develops robust deep reinforcement learning algorithms to address the challenge of non-static disturbances in partially observable Markov decision processes (POMDPs).

## Executive Summary
This study develops robust deep reinforcement learning algorithms to address the challenge of non-static disturbances in partially observable Markov decision processes (POMDPs). The core method extends the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm with Long Short-Term Memory (LSTM) networks, incorporating action sequences as input to improve robustness. Several network structures are proposed, including modified versions of LSTM-TD3 with single input channels and a novel hidden-state-based TD3 (H-TD3) algorithm that shares internal state representations between actor and critic networks. The algorithms are evaluated in a simulated pendulum environment with various disturbance types, including temporal sinusoidal waves, random sinusoidal waves, noise, and hidden states. Results show that including action sequences significantly improves robustness, with the proposed LSTM-TD3 variants and H-TD3 achieving better performance than the original TD3 and LSTM-TD3 without action sequences. H-TD3 offers reduced computational time by sharing hidden states between networks. The study demonstrates that the choice of information and network structure significantly impacts robustness in POMDP environments with dynamic disturbances.

## Method Summary
This study develops robust deep reinforcement learning algorithms for POMDPs with non-static disturbances by extending the TD3 algorithm with LSTM networks. The method incorporates action sequences as input to capture temporal correlations in disturbances, evaluating several network structures including modified LSTM-TD3 variants and a novel H-TD3 algorithm. The algorithms are trained in a simulated pendulum environment with various disturbance types (temporal bias, temporal sinusoidal wave, random sinusoidal wave, noise, hidden states) using a replay buffer that stores hidden and cell states alongside observations and actions. The core innovation involves processing sequential information through LSTM layers to reconstruct internal state representations that capture both system dynamics and disturbance patterns.

## Key Results
- Including action sequences in LSTM-TD3 significantly improves robustness against temporally correlated disturbances compared to the original TD3 and LSTM-TD3 without action sequences.
- The H-TD3 algorithm achieves comparable performance to LSTM-TD3 while reducing computational time by sharing hidden states between actor and critic networks.
- The choice of network structure (single vs. double input channels) affects performance, with single-channel structures treating complete sequences as one input proving more effective.
- Performance degrades in noise environments due to the lack of dynamic models in purely random disturbances, highlighting limitations of the approach.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including action sequences in LSTM-TD3 improves robustness by capturing causality between actions and observations.
- Mechanism: Actions serve as sufficient statistics for the hidden state in POMDP environments. By including past action sequences as input to LSTM layers, the network can reconstruct the internal state representation s* that captures the underlying dynamics of both the system and disturbances.
- Core assumption: The disturbances have temporal correlation that can be identified through action-observation sequences.
- Evidence anchors:
  - [abstract] "This study shows the benefit of action sequence inclusion in order to solve Partially Observable Markov Decision Process"
  - [section 3.5] "identification of the dynamic transition model relies not only on the historical information of observation, but also that of action, since both have causal connections on the current observation"
  - [corpus] Weak - related papers discuss POMDP solutions but don't specifically address action sequence inclusion
- Break condition: If disturbances are purely random with no temporal correlation, action sequences provide no additional information beyond observations.

### Mechanism 2
- Claim: Processing past and current information as one sequence is more effective than separate channels.
- Mechanism: The LSTM layer can learn to weigh the importance of different time steps through its gating mechanisms. By treating the complete sequence I_t-l:t as one input, the network can naturally prioritize relevant information rather than being constrained by predefined channel structures.
- Core assumption: The LSTM gating mechanisms are sufficient to identify and prioritize relevant information across the sequence.
- Evidence anchors:
  - [section 5.2] "it might not be necessary to have a double-headed structure" and "all data in a sequence should be treated in the same path"
  - [section 4.3] Results showing LSTM-TD3 with action sequences outperforming the original version
  - [corpus] Weak - related papers focus on RNN usage but don't discuss channel structure tradeoffs
- Break condition: If the current observation is significantly more informative than past observations, separate processing might be beneficial.

### Mechanism 3
- Claim: Sharing hidden states between actor and critic networks reduces computational cost while maintaining performance.
- Mechanism: The hidden state s* generated by the actor network during exploration can initialize the LSTM in critic networks, eliminating the need to reprocess the same sequence multiple times. This is particularly valuable in TD3 which has two critic networks.
- Core assumption: The hidden state representation learned by the actor is sufficiently informative for the critic's value estimation task.
- Evidence anchors:
  - [section 5.3] Introduction of H-TD3 algorithm with "shared hidden states seen in H-TD3 results"
  - [section 6.2] "H-TD3 offers a much shorter time per iteration" compared to other LSTM-based algorithms
  - [section 6.2] "H-TD3 showed a slower learning trajectory" but "offered comparable result with LSTM-TD3 with action case"
  - [corpus] Weak - no related papers discuss hidden state sharing between actor and critic
- Break condition: If the actor's hidden state representation doesn't capture information relevant to value estimation, the critic performance will degrade.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDP)
  - Why needed here: The entire paper addresses robustness in environments where observations don't fully reveal the true state due to disturbances
  - Quick check question: What makes POMDP different from MDP, and why can't standard RL algorithms directly handle POMDP environments?

- Concept: Recurrent Neural Networks and LSTM mechanisms
  - Why needed here: LSTM layers are the core mechanism for processing sequential information and maintaining internal state representations
  - Quick check question: How do LSTM gates (input, forget, output) enable the network to selectively process sequential information?

- Concept: Actor-Critic architecture and TD3 algorithm
  - Why needed here: The proposed methods extend TD3 with LSTM layers, requiring understanding of how actor and critic networks interact
  - Quick check question: In TD3, what problem does having twin critics solve, and how does the target network update work?

## Architecture Onboarding

- Component map: Environment -> Observation with disturbance -> LSTM layers -> Action -> Actor network; Observation with disturbance -> LSTM layers (initialized with actor hidden state) -> Value estimate -> Critic networks
- Critical path: Environment generates observation with disturbance → Actor network processes sequence (observation, action) through LSTM to generate action → Hidden and cell states stored in replay buffer → During training, critic networks initialize LSTM with stored states and process current observation → TD3 update rules applied with twin critics and target networks
- Design tradeoffs:
  - Sequence length vs. computational cost: Longer sequences capture more temporal information but increase training time
  - Input structure: Separate channels prioritize current info but may lose temporal context; single channel preserves sequence but requires LSTM to learn prioritization
  - Shared vs. independent states: State sharing reduces computation but couples actor and critic learning
- Failure signatures:
  - Degradation with increased noise variance: Network overfitting to specific disturbance patterns
  - Performance plateau in hidden state scenario: Insufficient sequence length to recover missing information
  - H-TD3 poor performance in noise case: Missing action information at critic input prevents effective noise filtering
- First 3 experiments:
  1. Compare original TD3 vs. LSTM-TD3 (with and without action sequences) on temporal sinusoidal wave disturbance
  2. Test different sequence lengths (l=1, 3, 6, 10, 20) on random sinusoidal wave environment
  3. Evaluate H-TD3 vs. LSTM-TD3 variants on computational time and final performance across all disturbance types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of action sequences in LSTM-TD3 algorithms affect robustness across different types of disturbances in POMDP environments?
- Basis in paper: [explicit] The paper discusses the benefit of including action sequences to solve POMDP and evaluates the effect of action sequence inclusion with LSTM-TD3 algorithms in different disturbance scenarios.
- Why unresolved: The paper shows improved performance with action sequences but does not provide a comprehensive analysis of how this improvement varies across different disturbance types.
- What evidence would resolve it: Comparative performance analysis of LSTM-TD3 algorithms with and without action sequences across various disturbance types would provide clearer insights into the effectiveness of action sequence inclusion.

### Open Question 2
- Question: How does the choice of network structure impact the robustness of deep reinforcement learning algorithms in POMDP environments?
- Basis in paper: [explicit] The paper proposes and evaluates several network structures, including modified LSTM-TD3 and H-TD3, to improve robustness in POMDP environments.
- Why unresolved: While the paper presents results showing improved performance with different network structures, it does not provide a detailed analysis of why certain structures are more effective than others.
- What evidence would resolve it: A thorough analysis of the impact of network structure on robustness, including comparisons between different structures and explanations of their effectiveness, would help resolve this question.

### Open Question 3
- Question: How does the length of the action sequence impact the performance of LSTM-TD3 algorithms in POMDP environments?
- Basis in paper: [explicit] The paper investigates the effect of the length of the action sequence on the performance of LSTM-TD3 algorithms in different disturbance scenarios.
- Why unresolved: The paper provides some insights into the impact of sequence length but does not offer a comprehensive analysis of the optimal length for different disturbance types.
- What evidence would resolve it: A systematic evaluation of LSTM-TD3 algorithms with varying sequence lengths across different disturbance types would provide a clearer understanding of the optimal sequence length for robust performance.

## Limitations
- Evaluation is restricted to a single pendulum environment, making generalizability to complex, real-world POMDP problems uncertain.
- Computational complexity of LSTM-TD3 variants increases significantly with sequence length, creating practical scalability concerns.
- Paper lacks detailed ablation studies on relative importance of architectural components and investigation of performance across different disturbance frequencies and amplitudes.

## Confidence
- High: The benefit of including action sequences in LSTM-TD3 for improving robustness against temporally correlated disturbances is well-supported by experimental results across multiple disturbance types.
- Medium: The computational efficiency gains of H-TD3 through shared hidden states are demonstrated, but the tradeoff with learning trajectory speed requires further investigation across different environments.
- Low: The generalizability of these findings to complex, real-world POMDP problems remains uncertain due to the limited evaluation scope and lack of comparison with other state-of-the-art POMDP methods.

## Next Checks
1. Evaluate the proposed algorithms on continuous control benchmarks (e.g., MuJoCo tasks) with varying observation dimensions to assess scalability and performance in higher-dimensional POMDPs.
2. Conduct systematic ablation studies varying LSTM architecture parameters (sequence length, hidden state size, number of layers) to identify optimal configurations for different disturbance types.
3. Compare the LSTM-TD3 variants against attention-based POMDP approaches and recurrent variants of other actor-critic algorithms to establish relative performance in diverse disturbance scenarios.