---
ver: rpa2
title: A Distribution-Based Threshold for Determining Sentence Similarity
arxiv_id: '2311.16675'
source_url: https://arxiv.org/abs/2311.16675
tags:
- distance
- similarity
- distributions
- matches
- threshold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to determine when two sentences are
  similar by using a siamese neural network to create distributions of distances between
  similar and dissimilar sentence pairs. The distributions are used to find a "threshold"
  that distinguishes vector distances of similar pairs from dissimilar pairs.
---

# A Distribution-Based Threshold for Determining Sentence Similarity

## Quick Facts
- arXiv ID: 2311.16675
- Source URL: https://arxiv.org/abs/2311.16675
- Reference count: 7
- Primary result: 86.6% validation accuracy on STS benchmark using threshold-based sentence similarity scoring

## Executive Summary
This paper proposes a method to determine when two sentences are similar by using a siamese neural network to create distributions of distances between similar and dissimilar sentence pairs. The distributions are used to find a "threshold" that distinguishes vector distances of similar pairs from dissimilar pairs. An accuracy scoring system is also developed that combines attributes from the distributions and the distance function. The method is tested on a benchmark STS dataset and achieves 86.6% validation accuracy, with a threshold-based similarity score that can be applied to new sentence pairs. The results show the approach can be generalized to a wider range of domains.

## Method Summary
The method uses a siamese neural network with pre-trained multilingual Universal Sentence Encoder (USE) to generate vector representations of sentence pairs. The model processes two sentences through identical networks with tied weights, producing fixed-dimensional vectors whose distance serves as a quantitative similarity measure. Distance values from similar and dissimilar pairs are fitted into histograms to find a threshold at the crossing point of the two distributions. An accuracy scoring system combines proximity to optimal distance values and distribution peak likelihoods to provide calibrated accuracy estimates for each prediction.

## Key Results
- Achieved 86.6% validation accuracy on benchmark STS dataset
- Threshold-based similarity scoring successfully distinguishes similar from dissimilar pairs
- Accuracy scoring system provides calibrated confidence estimates for predictions
- Method generalizes across different domains including registry and payment datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a siamese neural network with pre-trained multilingual Universal Sentence Encoder (USE) can create vector representations that capture the distinguishing features between similar and dissimilar sentence pairs.
- Mechanism: The siamese architecture processes two sentences through identical networks with tied weights, converting them into fixed-dimensional vectors. The distance between these vectors serves as a quantitative measure of similarity.
- Core assumption: The USE embeddings retain enough semantic information to differentiate between similar and dissimilar pairs, even when the distinguishing features are highly specific (e.g., names, addresses, identification codes).
- Break condition: If the distinguishing features between sentence pairs are too subtle or the semantic content is too sparse, the USE embeddings may fail to capture meaningful differences, causing the distributions to overlap significantly and the threshold to become unreliable.

### Mechanism 2
- Claim: The intersection point of the distance distributions from similar and dissimilar pairs serves as a robust threshold for classifying new sentence pairs.
- Mechanism: By fitting the distance values from the model into histograms for each class, the point where the two distributions cross defines a boundary. Pairs with distances below this threshold are classified as similar; those above are dissimilar.
- Core assumption: The distributions of distances from each class are sufficiently separated so that their intersection provides a meaningful and stable decision boundary.
- Break condition: If the data contains many ambiguous pairs (e.g., pairs with a relatedness score of 2-3 in SICK), the distributions may not be well-separated, causing a high rate of misclassification and making the threshold ineffective.

### Mechanism 3
- Claim: The scoring system, which combines closeness to the optimal distance value and the distribution peak, provides a calibrated accuracy estimate for each prediction.
- Mechanism: For each prediction, the system calculates two scales: a "closeness scale" based on proximity to the ideal distance (0 for similar, 1 for dissimilar) and a "distribution scale" based on the likelihood of the distance value occurring in the training data. These scales are combined and normalized to yield an accuracy score between 0 and 100.
- Core assumption: The shape and peak of the training distributions are stable and representative of the underlying data distribution, so new predictions near the peak of the correct class distribution should be considered more accurate.
- Break condition: If the training data is too small or not representative of the full distribution of real-world sentence pairs, the accuracy scoring system may assign misleading scores, especially for pairs that fall into regions of the distance space not well-represented in the training set.

## Foundational Learning

- Concept: Siamese neural networks with tied weights
  - Why needed here: Allows the model to process two sentences in parallel using the same transformation, ensuring that the distance metric is symmetric and meaningful.
  - Quick check question: If the weights were not tied, would the model still be able to fairly compare two sentences?

- Concept: Pre-trained sentence embeddings (e.g., USE)
  - Why needed here: Provides a rich, context-aware representation of sentences without requiring a large labeled dataset for training from scratch.
  - Quick check question: What would happen to the model's performance if you used randomly initialized embeddings instead of pre-trained ones?

- Concept: Distance metrics (Euclidean, Manhattan, Minkowski)
  - Why needed here: Converts the vector representations into a scalar similarity score that can be thresholded and analyzed statistically.
  - Quick check question: How does changing the order of the Minkowski distance (p-value) affect the shape of the distributions?

## Architecture Onboarding

- Component map: Sentence → USE → Dense (50 units, ReLU) → Distance (Minkowski, p=3) → Threshold → Classification
- Critical path: Sentence → USE → Dense → Distance → Threshold → Classification
- Design tradeoffs:
  - Using USE vs. training embeddings from scratch: USE is faster to deploy and works well with limited data but may not capture domain-specific nuances as well.
  - Freezing USE weights vs. fine-tuning: Freezing speeds up training and reduces overfitting risk but may limit performance gains from domain adaptation.
  - Minkowski vs. Euclidean/Manhattan: Minkowski with p=3 gave slightly better separation in the paper but is more computationally intensive.
- Failure signatures:
  - High overlap between distributions → unreliable threshold
  - Large percentage of misclassified pairs → poor feature extraction or inappropriate distance metric
  - Accuracy scores that do not correlate with true performance → inaccurate distribution modeling or calibration issues
- First 3 experiments:
  1. Train the model on a small subset of labeled data and visualize the distance distributions for each class.
  2. Test different distance metrics (Euclidean, Manhattan, Minkowski) and measure the percentage of misclassified pairs.
  3. Apply the accuracy scoring system to a held-out validation set and compare the predicted accuracy scores with actual classification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of the threshold-based similarity scoring system be further improved?
- Basis in paper: [explicit] The paper states that the current validation accuracy is 86.6% and suggests that different architectures and hyperparameter optimization could lead to improvements.
- Why unresolved: The paper does not explore alternative architectures or hyperparameter optimization techniques beyond what was tested.
- What evidence would resolve it: Experiments comparing the current model with other architectures and hyperparameter settings, showing increased validation accuracy.

### Open Question 2
- Question: What are the effects of using different pre-trained models instead of the Universal Sentence Encoder?
- Basis in paper: [explicit] The paper mentions that the Universal Sentence Encoder was chosen for its higher accuracy, but other models could potentially yield better results.
- Why unresolved: The paper does not test other pre-trained models for comparison.
- What evidence would resolve it: Results from experiments using different pre-trained models, demonstrating improved performance or validation accuracy.

### Open Question 3
- Question: How does the threshold-based similarity scoring system perform on datasets with varying levels of sentence similarity?
- Basis in paper: [explicit] The paper generalizes the results to the SICK dataset, but only tests a binary distinction between similar and not similar sentences.
- Why unresolved: The paper does not explore the system's performance on datasets with more nuanced levels of similarity.
- What evidence would resolve it: Experiments using datasets with varying levels of similarity, showing the system's ability to accurately score and classify sentences across different similarity ranges.

## Limitations

- Datasets used in the study are not publicly available, making independent validation difficult
- Method assumes pre-trained embeddings capture sufficient semantic nuance for highly specific domain text
- Threshold-crossing mechanism may break down when distributions overlap significantly
- Accuracy scoring system relies on stable distribution shapes that may not be representative of real-world variation

## Confidence

- **High confidence**: The core methodology of using a siamese neural network with pre-trained embeddings to generate distance distributions is well-established and technically sound.
- **Medium confidence**: The threshold-crossing approach for binary classification works well when distributions are well-separated but may fail in more ambiguous cases.
- **Low confidence**: The accuracy scoring system's calibration and generalizability across different domains have not been extensively validated.

## Next Checks

1. **Distribution Analysis**: Test the model on a small held-out validation set and visualize the distance distributions for each class to verify separation and identify overlap regions.

2. **Threshold Stability**: Apply the method to multiple random train/test splits of the same dataset and measure variation in the threshold value and classification accuracy.

3. **Cross-Domain Generalization**: Train the model on one domain (e.g., registry) and test on another (e.g., payments) to assess whether the threshold and accuracy scoring system transfer effectively.