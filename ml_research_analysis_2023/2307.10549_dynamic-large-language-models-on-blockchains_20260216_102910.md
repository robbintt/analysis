---
ver: rpa2
title: Dynamic Large Language Models on Blockchains
arxiv_id: '2307.10549'
source_url: https://arxiv.org/abs/2307.10549
tags:
- more
- language
- gong
- data
- blockchain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes training and deploying dynamic large language
  models on blockchains to address computational cost and static nature issues. The
  key idea is to leverage blockchain's distributed computing power for training and
  enable continuous learning from user input.
---

# Dynamic Large Language Models on Blockchains

## Quick Facts
- arXiv ID: 2307.10549
- Source URL: https://arxiv.org/abs/2307.10549
- Authors: 
- Reference count: 40
- Key outcome: Proposes training and deploying dynamic large language models on blockchains to address computational cost and static nature issues, leveraging distributed computing and continuous learning from user input.

## Executive Summary
This paper introduces a novel approach to deploying large language models (LLMs) on blockchain networks to overcome the computational expense and static nature of traditional models. By leveraging blockchain's distributed computing power and enabling continuous learning from user input, the proposed system aims to create a more efficient and adaptive AI infrastructure. The approach incorporates a dynamic pricing model based on task complexity, ensuring a win-win economic model for both users and developers while maintaining transparency and decentralization.

## Method Summary
The paper proposes training and deploying dynamic large language models (DLLMs) on blockchain networks. DLLMs are designed to continuously learn from user input after the initial training process, allowing them to evolve and improve over time. The system leverages blockchain's distributed computing power to reduce training costs and implements a dynamic pricing model based on task complexity. The method involves setting up a blockchain network with nodes capable of distributed computation, implementing DLLM architecture with dynamic parameter weights and modular design, and deploying DLLMs on the blockchain network for continuous learning from user input.

## Key Results
- Dynamic LLMs can continuously learn from user input after training, evolving beyond static models.
- Blockchain's distributed computing power reduces training costs by leveraging idle computational resources across nodes.
- Dynamic pricing model aligns computational resource allocation with task complexity and model capability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Blockchain-based distributed computing reduces training cost for large language models by leveraging idle computational resources across nodes.
- Mechanism: Blockchain networks provide a decentralized pool of computational power that can be accessed for training tasks, avoiding the need for expensive centralized GPU clusters.
- Core assumption: Network participants have sufficient and consistent idle computing resources to contribute meaningfully to LLM training workloads.
- Evidence anchors:
  - [abstract] "train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers"
  - [section III.A] "Blockchains offer a decentralized and transparent platform for storing and processing large amounts of data"
- Break condition: If network nodes have insufficient computational power or become unavailable, training throughput would degrade below acceptable thresholds.

### Mechanism 2
- Claim: Continuous user input on blockchain enables dynamic model evolution beyond initial training.
- Mechanism: User interactions are recorded on the blockchain, allowing developers to continuously update model parameters based on real-world usage patterns.
- Core assumption: User input data can be validated and integrated into the model training process without introducing significant noise or malicious content.
- Evidence anchors:
  - [abstract] "The dynamic large language models can continuously learn from the user input after the training process"
  - [section II.B] "the neural network parameters can be continuously updated during their usage after the training process"
- Break condition: If user input contains too much noise or malicious data, the model's performance may degrade rather than improve.

### Mechanism 3
- Claim: Dynamic pricing model aligns computational resource allocation with task complexity and model capability.
- Mechanism: Different tasks incur different computational costs, creating a pricing structure where complex models handle complex tasks at higher prices, while simpler models handle simpler tasks at lower prices.
- Core assumption: Users are willing to pay differential prices based on task complexity and model performance requirements.
- Evidence anchors:
  - [section III.B] "Unlike previous static LLM, DLLM take dynamic input and are dynamic with different tasks. Therefore, it takes different cost and price for easy and difficult tasks"
  - [section III.D] "Users get access to the model at a lower price and developers can develop their model at a lower price"
- Break condition: If users are unwilling to pay differential pricing, the economic model may fail to incentivize proper resource allocation.

## Foundational Learning

- Concept: Blockchain consensus mechanisms
  - Why needed here: Understanding how blockchain networks validate and execute computations is crucial for implementing distributed LLM training
  - Quick check question: What consensus algorithm would be most appropriate for validating LLM training updates on a blockchain network?

- Concept: Federated learning
  - Why needed here: The dynamic updating of models based on user input resembles federated learning approaches, but requires adaptation for blockchain environments
  - Quick check question: How would you modify federated learning techniques to work within a blockchain's transaction and consensus framework?

- Concept: Dynamic neural network architectures
  - Why needed here: The proposed system requires models that can adapt their architecture and parameters based on task complexity and usage patterns
  - Quick check question: What architectural changes would allow a model to dynamically adjust its depth and complexity based on input requirements?

## Architecture Onboarding

- Component map:
  - User Interface Layer -> Blockchain Transaction Layer -> Computational Resource Pool -> Dynamic Model Manager -> Economic Engine

- Critical path:
  1. User submits query/task
  2. Query is evaluated for complexity and cost
  3. Blockchain records transaction and initiates computational task
  4. Available nodes process the task
  5. Results are validated and returned to user
  6. User feedback is recorded for future model updates

- Design tradeoffs:
  - Decentralization vs. performance: More nodes increase resilience but may slow computation
  - Transparency vs. privacy: Full transparency aids trust but may expose sensitive data
  - Dynamic pricing vs. user adoption: Complex pricing may deter casual users

- Failure signatures:
  - Network congestion: Slow transaction processing times
  - Node failure: Inconsistent model updates or degraded performance
  - Economic imbalance: Either users stop using the system or developers stop updating models

- First 3 experiments:
  1. Deploy a simple static model on a test blockchain network to validate computational feasibility
  2. Implement a basic dynamic pricing model with predefined task complexity levels
  3. Create a prototype of the continuous learning mechanism with synthetic user data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed dynamic pricing model based on task complexity perform in practice, and what metrics can be used to quantify the relationship between task difficulty and computational cost?
- Basis in paper: [explicit] The paper introduces a dynamic pricing model based on task complexity, stating "The more difficult the task, the more computation is required and thus the price is higher for its usage."
- Why unresolved: The paper does not provide any performance metrics or empirical evidence to support the effectiveness of this dynamic pricing model.
- What evidence would resolve it: Experimental results demonstrating the correlation between task complexity, computational cost, and pricing in the proposed system.

### Open Question 2
- Question: What specific mechanisms ensure the transparency and availability of all input data for researchers in the proposed blockchain-based LLM system?
- Basis in paper: [explicit] The paper claims "all the input data must be transparent and available to all researchers" in the proposed DLLM, contrasting with proprietary data used in traditional LLMs.
- Why unresolved: The paper does not detail the technical implementation or specific protocols that would guarantee this transparency and accessibility.
- What evidence would resolve it: A description of the blockchain architecture and data management protocols that ensure input data transparency and accessibility for all researchers.

### Open Question 3
- Question: How does the proposed system address the potential issue of bias amplification in the training data, especially given the dynamic nature of the LLM and its continuous learning from user input?
- Basis in paper: [inferred] While the paper discusses the potential for bias in traditional LLMs, it does not explicitly address how the proposed dynamic system would mitigate or monitor bias over time.
- Why unresolved: The paper does not provide any mechanisms or strategies for detecting, measuring, or mitigating bias in the dynamic LLM system.
- What evidence would resolve it: A framework or algorithm for continuous bias detection and mitigation in the dynamic LLM, along with experimental results demonstrating its effectiveness.

### Open Question 4
- Question: What are the specific performance gains (if any) of the proposed dynamic LLM architecture compared to traditional static LLMs, particularly in terms of task completion time and accuracy?
- Basis in paper: [inferred] The paper claims that the dynamic LLM can continuously evolve and improve with user input, but does not provide any quantitative comparisons with traditional LLMs.
- Why unresolved: The paper lacks empirical data or benchmarks comparing the performance of the proposed dynamic LLM to existing static LLMs.
- What evidence would resolve it: Experimental results showing improvements in task completion time, accuracy, or other relevant metrics when using the dynamic LLM compared to traditional static LLMs.

## Limitations
- No empirical validation or performance benchmarks provided for the proposed system.
- Technical details on consensus mechanisms for model updates and dynamic pricing algorithms are unspecified.
- Potential security vulnerabilities in continuous learning from untrusted user inputs are not addressed.

## Confidence
- High Confidence: The core insight that blockchain's distributed computing could theoretically reduce training costs is sound and well-established in the broader literature on decentralized computing.
- Medium Confidence: The concept of continuous learning from user input is feasible and aligns with existing federated learning approaches, though blockchain-specific implementation challenges remain unaddressed.
- Low Confidence: The dynamic pricing model and economic sustainability claims lack supporting evidence or mathematical formulation, making these assertions speculative.

## Next Checks
1. **Technical Feasibility Assessment**: Implement a proof-of-concept with a small-scale blockchain network (5-10 nodes) running a basic transformer model to measure computational overhead and throughput compared to traditional GPU training.

2. **Security Analysis**: Design and execute penetration tests to evaluate vulnerabilities in the continuous learning mechanism, specifically testing for data poisoning attacks and model manipulation through malicious user inputs.

3. **Economic Viability Modeling**: Develop a mathematical model of the proposed dynamic pricing system with realistic assumptions about user behavior, computational costs, and developer incentives, then simulate various scenarios to test sustainability under different market conditions.