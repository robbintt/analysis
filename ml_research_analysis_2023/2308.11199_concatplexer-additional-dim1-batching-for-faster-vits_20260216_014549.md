---
ver: rpa2
title: 'ConcatPlexer: Additional Dim1 Batching for Faster ViTs'
arxiv_id: '2308.11199'
source_url: https://arxiv.org/abs/2308.11199
tags:
- image
- concatplexer
- multiplexer
- vision
- nmux
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ConcatPlexer, a method for efficient visual
  recognition that employs additional dim1 batching (i.e., concatenation) to greatly
  improve throughput with little compromise in accuracy. The method is inspired by
  DataMUX, which was originally proposed for language models.
---

# ConcatPlexer: Additional Dim1 Batching for Faster ViTs

## Quick Facts
- arXiv ID: 2308.11199
- Source URL: https://arxiv.org/abs/2308.11199
- Reference count: 24
- Primary result: Achieves 23.5% less GFLOPs than ViT-B/16 with 69.5% ImageNet1K and 83.4% CIFAR100 accuracy

## Executive Summary
ConcatPlexer introduces an efficient visual recognition method that leverages additional dim1 batching through concatenation to significantly improve throughput with minimal accuracy compromise. Inspired by DataMUX from language models, this approach extracts high-level feature tokens via transformer encoder layers and reduces their length using learned convolution before concatenating multiple images for simultaneous processing. The method achieves 23.5% less GFLOPs than baseline ViT-B/16 while maintaining competitive accuracy on ImageNet1K (69.5%) and CIFAR100 (83.4%). It also shows promising results on multimodal retrieval tasks through its multimodal multiplexing (MMP) approach.

## Method Summary
ConcatPlexer employs a C-Multiplexer that reduces token length via convolution, enabling simultaneous processing of multiple images without losing essential information. The method processes high-level feature tokens from the transformer encoder patchifier, applies a 1D convolution to compress the sequence length from L to L/NMUX, then concatenates the compressed tokens from NMUX images into a single sequence. This concatenated sequence is processed by the backbone as if it were a batch of NMUX times larger, significantly improving computational efficiency while maintaining accuracy through careful token reduction.

## Key Results
- Achieves 23.5% reduction in GFLOPs compared to ViT-B/16 baseline
- Maintains 69.5% validation accuracy on ImageNet1K dataset
- Achieves 83.4% validation accuracy on CIFAR100 dataset
- Demonstrates promising results on multimodal retrieval tasks with MMP approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C-Multiplexer reduces token length via convolution, enabling simultaneous processing of multiple images without losing essential information.
- Mechanism: The C-Multiplexer applies a 1D convolution to high-level feature tokens from the Transformer encoder, reducing the sequence length from L to L/NMUX. The compressed tokens from NMUX images are then concatenated into a single sequence of length L, which the backbone processes as if it were a batch of NMUX times larger.
- Core assumption: High-level feature tokens retain enough discriminative information after length reduction for accurate classification.
- Evidence anchors:
  - [abstract] "The ConcatPlexer extracts high-level feature tokens via Transformer encoder layers and reduces the length of input tokens using a learned convolution and concatenates them for simultaneous processing."
  - [section] "Using the conv1d layer with the output channel dim, the dimension of (bs, L, dim) tokens from TrE tokenizer becomes (bs, L/NMUX, dim)...From this operation, the input of dimension (bs, L/NMUX, dim) becomes (bs/NMUX, L, dim) thereby enables the model to process NMUX times larger batch."
- Break condition: If the convolution collapses unique discriminative features, classification accuracy drops significantly.

### Mechanism 2
- Claim: Multimodal multiplexing (MMP) enables joint image-text representation in a single transformer backbone, improving cross-modal retrieval.
- Mechanism: MMP processes images and text through a single transformer without modality-specific branches, leveraging the transformer's attention mechanism to align features in a shared embedding space.
- Core assumption: The transformer's attention heads can effectively learn cross-modal alignment without explicit modality separation.
- Evidence anchors:
  - [abstract] "MMP: It is a single ViT-architected model that represents images and texts in a modality-agnostic manner... MMP achieves promising preliminary results on challenging multimodal retrieval task..."
  - [section] "Comparison with original DataMUX: Original DataMUX and its descendent [14, 15] demonstrated its effectiveness on GLUE benchmark [21]. However, an expectation of random chance of CIFAR100 and ImageNet1K is much lower..."
- Break condition: If cross-modal alignment is poor, retrieval accuracy will degrade sharply.

### Mechanism 3
- Claim: Token reduction via convolution preserves semantic information while drastically reducing computational cost.
- Mechanism: By reducing token length before concatenation, the method ensures the backbone processes fewer tokens overall, cutting FLOPs while maintaining accuracy.
- Core assumption: Convolved tokens retain enough semantic content for the downstream classification task.
- Evidence anchors:
  - [abstract] "ConcatPlexer extracts high-level featured tokens using the transformer encoder patchifier and concatenates multiple images to process them at once."
  - [section] "Using the conv1d layer with the output channel dim, the dimension of (bs, L, dim) tokens from TrE tokenizer becomes (bs, L/NMUX, dim) where NMUX is number of sample to multiplex."
- Break condition: If semantic loss from token reduction is too high, accuracy will drop below acceptable thresholds.

## Foundational Learning

- Concept: Transformer encoder patchifier
  - Why needed here: Extracts high-level feature tokens from raw images, reducing redundancy and preparing tokens for multiplexing.
  - Quick check question: What is the role of the CNN layer in the Transformer encoder patchifier?

- Concept: Data multiplexing (DataMUX)
  - Why needed here: Core idea of projecting multiple inputs into a single representation to improve throughput.
  - Quick check question: How does DataMUX originally handle token multiplexing in NLP?

- Concept: Token reduction via convolution
  - Why needed here: Enables simultaneous processing of multiple images by compressing token sequences before concatenation.
  - Quick check question: Why does reducing token length help computational efficiency in transformers?

## Architecture Onboarding

- Component map: Input → Transformer Encoder Patchifier → C-Multiplexer → Backbone → Demultiplexer → Output
- Critical path: Input → Patchifier → C-Multiplexer → Backbone → Demultiplexer → Output
- Design tradeoffs: Increasing NMUX reduces FLOPs but may degrade accuracy; more projection layers improve accuracy but increase computation.
- Failure signatures: Sharp accuracy drop when NMUX is too high; degraded performance with inadequate projection layers.
- First 3 experiments:
  1. Baseline test: Compare ConcatPlexer with ViT-B/16 on ImageNet1K without multiplexing.
  2. Ablation study: Vary NMUX (2, 4, 8) and observe accuracy vs. FLOPs trade-off.
  3. Multimodal test: Evaluate MMP on cross-modal retrieval benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between the number of multiplexed images (NMUX) and model performance across different vision tasks?
- Basis in paper: [explicit] The paper demonstrates that increasing NMUX improves computational efficiency but degrades accuracy, with specific performance metrics given for different NMUX values on ImageNet1K and CIFAR100.
- Why unresolved: The paper only tests a limited range of NMUX values (2-4) and shows a general trend rather than identifying an optimal point. Different vision tasks may have different optimal NMUX values.
- What evidence would resolve it: Systematic experiments varying NMUX across a wider range (1-8) on multiple vision benchmarks (object detection, segmentation, retrieval) to identify task-specific optimal points.

### Open Question 2
- Question: How does the ConcatPlexer's performance compare to other efficient transformer architectures like Swin Transformers or ConvNeXt when applied to large-scale vision tasks?
- Basis in paper: [inferred] The paper only compares ConcatPlexer to ViT-based models and baseline Image Multiplexer, not to other state-of-the-art efficient architectures that use different approaches (hierarchical structures, local attention, etc.).
- Why unresolved: The paper focuses on demonstrating the feasibility of data multiplexing in vision but doesn't benchmark against other established efficient architectures.
- What evidence would resolve it: Direct performance and efficiency comparisons between ConcatPlexer, Swin Transformer, ConvNeXt, and other efficient architectures on ImageNet1K and downstream tasks.

### Open Question 3
- Question: Can the data multiplexing concept be extended beyond simple image classification to more complex vision tasks like object detection or semantic segmentation?
- Basis in paper: [explicit] The paper mentions the possibility of extending to multimodal tasks (Vision&Language) and suggests future work could apply similar concepts to other vision tasks.
- Why unresolved: The paper only demonstrates multiplexing for image classification tasks and provides only a proof-of-concept for multimodal retrieval, without exploring more complex vision tasks.
- What evidence would resolve it: Successful implementation and evaluation of ConcatPlexer-style multiplexing on object detection (e.g., DETR-based models) and semantic segmentation tasks, with performance metrics compared to standard approaches.

## Limitations
- Token compression fidelity is not empirically quantified, leaving uncertainty about information loss during compression
- Multimodal alignment robustness lacks ablation studies on diverse datasets
- Evaluation limited to ImageNet1K and CIFAR100, with untested performance on larger-scale datasets

## Confidence
- **Throughput Improvement (23.5% GFLOPs reduction)**: High confidence
- **Accuracy Retention on ImageNet1K (69.5%) and CIFAR100 (83.4%)**: Medium confidence
- **Multimodal Retrieval Effectiveness**: Low confidence

## Next Checks
1. **Token Compression Ablation**: Conduct experiments varying the convolution kernel size and depth in the C-Multiplexer to quantify the trade-off between token compression and accuracy retention. Measure the semantic similarity of compressed vs. uncompressed tokens using metrics like CKA (Centered Kernel Alignment).

2. **Cross-Modal Retrieval Benchmark**: Evaluate MMP on established multimodal retrieval benchmarks (e.g., Flickr30k, COCO Captions) to assess its robustness and compare against modality-specific architectures.

3. **Scaling Study**: Test ConcatPlexer on larger-scale datasets (e.g., ImageNet-22K) and higher-resolution images to validate its scalability and efficiency gains in more demanding scenarios.