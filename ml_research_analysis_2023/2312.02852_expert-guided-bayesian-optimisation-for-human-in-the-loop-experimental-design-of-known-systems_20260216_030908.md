---
ver: rpa2
title: Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design
  of Known Systems
arxiv_id: '2312.02852'
source_url: https://arxiv.org/abs/2312.02852
tags:
- best
- expert
- regret
- optimisation
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a human-in-the-loop Bayesian optimisation framework
  that enables domain experts to influence experimental design by selecting from a
  set of high-quality alternative solutions. The method solves a multi-objective optimisation
  problem that balances utility maximisation with solution diversity, using the knee
  point of the Pareto front to select a set of alternatives.
---

# Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems

## Quick Facts
- **arXiv ID:** 2312.02852
- **Source URL:** https://arxiv.org/abs/2312.02852
- **Reference count:** 40
- **Primary result:** Human-in-the-loop Bayesian optimisation enables experts to improve convergence by selecting from high-quality alternative solutions, with benefits strongest in early iterations.

## Executive Summary
This paper introduces a human-in-the-loop Bayesian optimisation framework that leverages expert domain knowledge by allowing practitioners to select from a set of high-quality alternative solutions at each iteration. The method solves a multi-objective optimization problem balancing utility maximization with solution diversity, using the knee point of the Pareto front to select alternatives. Experimental results across 1D, 2D, and 5D problems demonstrate that even partially informed practitioners (selecting the best solution 25-75% of the time) achieve regret performance comparable to standard Bayesian optimisation, while expert practitioners show improved early convergence.

## Method Summary
The method employs a multi-objective optimization framework that balances two objectives: maximizing the sum of utility values and maximizing the determinant of the covariance matrix of candidate solutions. This produces a Pareto front from which the knee point is selected to provide 4 alternative solutions per iteration. A Gaussian Process model with Matérn 5/2 kernel predicts objective function values, and NSGA-II optimizes the multi-objective problem. The expert selects from alternatives, with simulated behaviors including "Expert" (always best), "Adversarial" (always worst), and probabilistic selection strategies. The framework is evaluated using simple regret and average regret metrics over 50 simulated functions for each problem dimension.

## Key Results
- Partially informed practitioners (25-75% best solution selection) recover standard Bayesian optimisation regret performance
- Expert practitioners achieve improved convergence, particularly in early iterations
- Method demonstrates robustness across 1D, 2D, and 5D problem dimensions
- Benefits of expert input diminish in later iterations as automated optimization becomes sufficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-objective formulation with utility maximization and determinant-based diversity ensures high-quality, distinct alternatives that mitigate gradient calculation by experts.
- Mechanism: By maximizing both the sum of utility values and the determinant of the covariance matrix of the augmented solution set, the method creates a Pareto front where solutions are both high-performing and well-distributed. The knee point selection provides a balanced trade-off between exploitation and exploration.
- Core assumption: Human experts are better at making discrete choices than continuous ones, and the determinant of the covariance matrix effectively captures solution diversity.
- Evidence anchors:
  - [abstract] "By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct"
  - [section 3] "maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability"
  - [corpus] Weak evidence - related papers discuss Bayesian optimization with expert input but don't specifically address the multi-objective diversity mechanism
- Break condition: If the expert consistently selects the worst solution (adversarial behavior), performance degrades as shown in regret plots.

### Mechanism 2
- Claim: Expert selection from a discrete set of alternatives enables internal Bayesian reasoning that incorporates domain knowledge without requiring explicit prior specification.
- Mechanism: The expert conditions their prior knowledge on the utility values and predictive distributions provided for each alternative, effectively performing discrete Bayesian reasoning. This avoids the need for static expert-defined priors that may become outdated.
- Core assumption: Experts can effectively integrate mathematical information with their domain knowledge to make better decisions than automated selection alone.
- Evidence anchors:
  - [abstract] "The practitioner is then allowed to distinguish between these similar solutions in a way the computer cannot through their prior domain knowledge"
  - [section 3] "the practitioner effectively performs an internal discrete Bayesian reasoning, conditioning previous prior information and expert opinion with the mathematical quantities provided"
  - [corpus] Weak evidence - while related work mentions expert integration, the specific claim about discrete Bayesian reasoning by experts isn't directly supported in cited papers
- Break condition: If the expert has no domain knowledge (random selection), the method recovers standard Bayesian optimization performance.

### Mechanism 3
- Claim: Early expert intervention provides significant convergence benefits that diminish in later iterations as automated optimization becomes sufficient.
- Mechanism: Domain experts can identify promising regions early in the optimization when the search space is less constrained, providing information gain that automated methods may miss. This effect is strongest in early iterations.
- Core assumption: Expert knowledge is most valuable in early stages when the optimization landscape is less understood.
- Evidence anchors:
  - [abstract] "enables experts to influence critical early decisions" and "improved convergence (depending on the ability of the domain expert)"
  - [section 4] "benefits diminishing throughout the later stages, where the standard automated approach recovers the 'Expert' average simple regret, confirming previous observations regarding the importance of human input throughout earlier iterations"
  - [corpus] Moderate evidence - Kanarik et al. (2023) is cited showing experts improve initial convergence in Bayesian optimization
- Break condition: In later iterations, the expert's influence becomes less critical as the optimization converges toward the optimum.

## Foundational Learning

- Concept: Pareto optimality and multi-objective optimization
  - Why needed here: The method solves a multi-objective problem to balance utility maximization with solution diversity
  - Quick check question: What is the difference between a Pareto optimal solution and a Pareto optimal set?

- Concept: Gaussian Process regression and acquisition functions
  - Why needed here: The method uses GP models to predict objective function values and compute utility functions like UCB
  - Quick check question: How does the Matérn 5/2 kernel differ from the squared exponential kernel in GP regression?

- Concept: Regret analysis in Bayesian optimization
  - Why needed here: The paper evaluates performance using simple regret and average regret metrics
  - Quick check question: What is the difference between simple regret and cumulative regret in optimization?

## Architecture Onboarding

- Component map: Gaussian Process model → Multi-objective optimizer (NSGA-II) → Knee point selector → Expert interface → Dataset manager → Repeat
- Critical path: GP model → Multi-objective optimization → Knee point selection → Expert selection → Dataset update → Repeat
- Design tradeoffs: More alternatives (higher p) provide more choices but increase computational cost; deterministic expert selection vs. probabilistic models
- Failure signatures: (1) All alternatives converge to the same point, (2) Expert consistently selects suboptimal solutions, (3) Computational cost becomes prohibitive for high-dimensional problems
- First 3 experiments:
  1. Implement the 1D case study with a simple sinusoidal function to verify the basic workflow
  2. Test with a known benchmark function (e.g., Ackley) to validate regret performance
  3. Simulate different expert behaviors (expert, adversarial, random) to verify the method's robustness to different selection strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the expert-guided Bayesian optimisation approach scale with the number of alternative solutions provided to the expert (p > 4)?
- Basis in paper: [explicit] The paper mentions providing 4 alternative solutions in their experiments but does not explore how the number of alternatives affects performance.
- Why unresolved: The paper only tests with 4 alternatives and does not systematically vary this parameter to determine optimal values or scaling behavior.
- What evidence would resolve it: A systematic study varying p (e.g., 2, 4, 8, 16 alternatives) across different problem dimensions and function types, measuring regret and computational overhead.

### Open Question 2
- Question: How does the expert-guided approach perform when the utility function differs from the true objective function being optimized?
- Basis in paper: [inferred] The paper uses UCB as the utility function but does not explore scenarios where the acquisition function and true objective are misaligned or when multiple competing objectives exist.
- Why unresolved: All experiments use a single, known utility function that presumably aligns with the true objective, leaving open questions about robustness to utility-objective misalignment.
- What evidence would resolve it: Experiments where the utility function (acquisition function) is intentionally misaligned with the true objective, or multi-objective problems where different trade-offs exist.

### Open Question 3
- Question: What is the impact of expert fatigue or cognitive load when selecting from increasingly large sets of alternatives over many iterations?
- Basis in paper: [explicit] The paper discusses providing multiple alternatives but does not address the practical limitations of human decision-making over extended optimization campaigns.
- Why unresolved: The paper assumes perfect expert selection but does not model or test how expert performance degrades with iteration count, number of alternatives, or optimization duration.
- What evidence would resolve it: Longitudinal studies with human subjects performing selection tasks over extended periods, measuring selection accuracy, decision time, and subjective workload as iteration count increases.

## Limitations
- Core assumption that experts can perform discrete Bayesian reasoning lacks strong empirical validation
- Performance highly dependent on expert quality, with adversarial selection leading to degraded results
- Simulated expert behaviors may not accurately represent real-world expert decision-making

## Confidence
- **High confidence:** The multi-objective optimization framework and Pareto front selection methodology
- **Medium confidence:** The regret analysis methodology and comparison with standard Bayesian optimization
- **Low confidence:** The claims about human experts performing discrete Bayesian reasoning and the transferability of simulated expert behaviors to real practitioners

## Next Checks
1. Conduct user studies with domain experts to validate whether they can effectively use the provided utility information for selection, and measure the gap between simulated and real expert performance
2. Test the method on real experimental systems (not just simulated functions) to assess practical utility and identify potential real-world limitations
3. Evaluate the computational overhead of the multi-objective optimization compared to standard Bayesian optimization, particularly for higher-dimensional problems where the determinant-based diversity objective becomes more complex