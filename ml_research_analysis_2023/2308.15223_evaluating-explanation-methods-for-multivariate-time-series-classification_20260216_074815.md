---
ver: rpa2
title: Evaluating Explanation Methods for Multivariate Time Series Classification
arxiv_id: '2308.15223'
source_url: https://arxiv.org/abs/2308.15223
tags:
- time
- explanation
- series
- multivariate
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates explanation methods for multivariate time
  series classification (MTSC). The authors focus on saliency-based explanation methods
  that can highlight the most relevant channels and time points for classification
  decisions.
---

# Evaluating Explanation Methods for Multivariate Time Series Classification

## Quick Facts
- arXiv ID: 2308.15223
- Source URL: https://arxiv.org/abs/2308.15223
- Reference count: 33
- Key outcome: Flattening multivariate time series by concatenating channels works as well as using multivariate classifiers directly for explanation quality

## Executive Summary
This paper evaluates explanation methods for multivariate time series classification (MTSC), focusing on saliency-based approaches that can highlight relevant channels and time points. The authors analyze two popular classifiers (ROCKET and dResNet) along with two explanation methods (SHAP and dCAM) across three synthetic and two real-world datasets. The study reveals that commonly used synthetic benchmarks are inadequate for testing true time series analysis capabilities, as simple linear models can achieve perfect accuracy. For real-world datasets, ROCKET with SHAP concatenated explanations emerges as the best-performing combination, though SHAP's computational expense remains a limitation.

## Method Summary
The paper evaluates MTSC explanation methods using a framework that includes classification accuracy and explanation quality metrics. The approach involves training classifiers (ROCKET, dResNet, Ridge) on both multivariate and concatenated (flattened) versions of datasets, generating explanations using SHAP (both concatenated and channel-by-channel) and dCAM, then evaluating these explanations using metrics like AMEE, precision, recall, and F1-score. The study compares performance across three synthetic datasets (PseudoPeriodic, Gaussian, AutoRegressive) and two real-world datasets (CMJ, Military Press), with evaluation focusing on both classification accuracy and the quality of generated explanations.

## Key Results
- Flattening multivariate datasets by concatenating channels performs as well as using multivariate classifiers directly
- SHAP adaptations for MTSC work effectively, with SHAP concatenated outperforming dCAM on the Military Press dataset
- Popular synthetic benchmarks are inadequate for time series analysis as simple classifiers like Ridge achieve perfect accuracy on all three synthetic datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Flattening multivariate time series by concatenating channels performs as well as using multivariate classifiers directly.
- Mechanism: Channel concatenation transforms the multivariate classification problem into a univariate one without losing discriminative information because the model can still learn channel-specific patterns from the concatenated feature vector.
- Core assumption: The discriminative information is not inherently tied to the channel structure but can be recovered from the concatenated representation.
- Evidence anchors:
  - [abstract] states "flattening the multivariate datasets by concatenating the channels works as well as using multivariate classifiers directly"
  - [section] shows in Table 2 that dResNet and ROCKET achieve comparable accuracy on concatenated datasets versus multivariate datasets
  - [corpus] provides no direct evidence; the related work focuses on explanation methods rather than classification performance
- Break condition: If the classification problem truly requires modeling inter-channel dependencies that cannot be captured in a flat feature space, or if the channel order information is critical for the classification decision.

### Mechanism 2
- Claim: SHAP adaptations for multivariate time series classification work effectively.
- Mechanism: SHAP's model-agnostic approach can be applied to any classifier by treating the concatenated time series as features, and the resulting importance scores can be reshaped to highlight relevant channels and time points.
- Core assumption: The model's behavior is stable enough that SHAP's feature importance estimates remain valid after reshaping the concatenated explanation back to a 2D channel-time matrix.
- Evidence anchors:
  - [abstract] states "adaptations of SHAP for MTSC work quite well"
  - [section] shows SHAP concatenated outperforms dCAM on the Military Press dataset according to AMEE evaluation (Table 5)
  - [corpus] provides no direct evidence; the related work discusses SHAP in general but not specific MTSC adaptations
- Break condition: If the classifier's internal representation changes significantly when processing concatenated versus multivariate inputs, or if SHAP's sampling-based approximations become unreliable for the transformed feature space.

### Mechanism 3
- Claim: Popular synthetic MTSC benchmarks are not suitable for time series analysis because simple classifiers like Ridge can achieve perfect accuracy.
- Mechanism: The synthetic datasets are constructed by adding/subtracting constant values to consecutive time points, creating patterns that are easily captured by linear models without requiring sophisticated time series analysis.
- Core assumption: The benchmark generation process creates linearly separable patterns that do not require temporal modeling capabilities.
- Evidence anchors:
  - [abstract] states "commonly used synthetic benchmarks are not suitable for time series analysis as simple classifiers like Ridge can achieve perfect accuracy"
  - [section] shows RidgeCV achieves perfect accuracy on all three synthetic datasets (Table 2) and discusses that these benchmarks are created by "adding or subtracting a single value to consecutive time points"
  - [corpus] provides no direct evidence; the related work focuses on explanation methods rather than benchmark quality
- Break condition: If the synthetic datasets are modified to include more complex temporal dependencies or non-linear patterns that require actual time series modeling capabilities.

## Foundational Learning

- Concept: Multivariate time series representation
  - Why needed here: Understanding that MTSC involves d × L matrices where d channels represent different sensors/measurements and L time points represent temporal evolution
  - Quick check question: If you have a smartwatch recording acceleration and orientation, how would you represent this data mathematically for classification?

- Concept: Saliency maps and feature importance
  - Why needed here: Explanation methods produce 2D saliency maps highlighting which channels and time points are most important for classification decisions
  - Quick check question: Given a 3 × 100 time series, what would be the shape of its saliency map and what would each element represent?

- Concept: Model-agnostic vs model-specific explanation methods
  - Why needed here: Understanding the difference between SHAP (model-agnostic) and dCAM (model-specific for deep learning with GAP layers) and their respective trade-offs
  - Quick check question: Why can't dCAM be used with ROCKET, and what does this limitation imply about the relationship between model architecture and explanation methods?

## Architecture Onboarding

- Component map: Data preprocessing → Model training → Prediction → Explanation generation → Explanation evaluation → Results analysis
- Critical path: Data preprocessing → Model training → Prediction → Explanation generation → Explanation evaluation → Results analysis
- Design tradeoffs: Concatenation simplifies explanation but may lose channel structure; model-specific explanations (dCAM) are more tailored but less flexible; SHAP is flexible but computationally expensive.
- Failure signatures: Poor classification accuracy indicates model choice issues; explanation methods ranking near random suggest benchmark problems or explanation quality issues; high computational time for SHAP indicates scalability concerns.
- First 3 experiments:
  1. Train ROCKET and dResNet on concatenated CMJ dataset, compare accuracy to multivariate versions
  2. Generate explanations using SHAP concatenated and dCAM on the same CMJ test samples
  3. Evaluate both explanations using AMEE framework on the CMJ dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What makes a multivariate time series dataset truly multivariate, as opposed to one that can be effectively flattened into univariate data?
- Basis in paper: [explicit] The paper discusses how the Military Press dataset is truly multivariate while CMJ can be effectively flattened, based on accuracy comparisons.
- Why unresolved: The criteria for determining multivariate nature are not fully formalized or explored.
- What evidence would resolve it: Further analysis of accuracy degradation when flattening datasets, and identification of specific characteristics that indicate multivariate nature.

### Open Question 2
- Question: How can the computational complexity of SHAP be reduced for multivariate time series classification without sacrificing explanation quality?
- Basis in paper: [explicit] The paper highlights SHAP's high computational requirements as a limitation.
- Why unresolved: The paper identifies the issue but does not propose solutions or explore potential optimizations.
- What evidence would resolve it: Comparative studies of approximation techniques, sampling methods, or alternative implementations of SHAP for MTSC.

### Open Question 3
- Question: Are there better synthetic benchmarks for multivariate time series classification that can effectively evaluate both classifiers and explanation methods?
- Basis in paper: [explicit] The paper finds that commonly used synthetic benchmarks are not suitable for time series analysis.
- Why unresolved: The paper identifies the problem but does not propose alternatives or investigate what characteristics a good benchmark should have.
- What evidence would resolve it: Development and testing of new synthetic datasets that better capture the complexities of real-world MTSC problems.

## Limitations

- The study is limited by the relatively small number of datasets evaluated (5 total, with only 2 real-world datasets)
- The computational expense of SHAP explanations remains a significant practical limitation despite its superior performance
- The findings from synthetic benchmarks may not generalize to more complex real-world scenarios due to their inadequacy for testing true time series analysis capabilities

## Confidence

- **High Confidence**: The finding that flattening multivariate datasets through channel concatenation performs as well as multivariate classifiers directly
- **Medium Confidence**: The effectiveness of SHAP adaptations for MTSC, limited by computational cost and evidence primarily from a small number of datasets
- **Medium Confidence**: The inadequacy of synthetic benchmarks for time series analysis, though generalizability to other benchmark designs is uncertain

## Next Checks

1. Test the concatenation approach on additional real-world multivariate time series datasets with known complex inter-channel dependencies to verify the claim that flattening preserves discriminative information.

2. Evaluate SHAP's computational scalability on larger datasets (e.g., >10,000 samples) and compare against alternative approximation methods to determine practical feasibility limits.

3. Investigate whether more sophisticated synthetic benchmark designs can be created that require actual time series modeling capabilities while maintaining controlled experimental conditions.