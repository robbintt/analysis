---
ver: rpa2
title: Supervised Manifold Learning via Random Forest Geometry-Preserving Proximities
arxiv_id: '2307.01077'
source_url: https://arxiv.org/abs/2307.01077
tags:
- random
- supervised
- learning
- forest
- proximities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of supervised manifold learning
  by proposing a new method using random forest geometry-preserving proximities (RF-GAP)
  as an initialization for manifold learning methods. Existing supervised extensions
  of manifold learning algorithms, such as Isomap, UMAP, t-SNE, Diffusion Map, and
  Laplacian Eigenmaps, use class-conditional distances that artificially exaggerate
  the separation of points of opposing classes, distorting the global data structure.
---

# Supervised Manifold Learning via Random Forest Geometry-Preserving Proximities

## Quick Facts
- **arXiv ID**: 2307.01077
- **Source URL**: https://arxiv.org/abs/2307.01077
- **Reference count**: 27
- **Key outcome**: RF-GAP proximities improve supervised manifold learning by avoiding global structure distortion and better preserving variable importance compared to class-conditional methods

## Executive Summary
This paper introduces Random Forest Geometry-Preserving Proximities (RF-GAP) as a novel approach for supervised manifold learning. Unlike existing supervised extensions that use class-conditional distances to artificially separate classes, RF-GAP leverages the random forest's recursive splitting process to create proximity measures that maintain both local similarity and partial global structure. The method is evaluated across multiple UCI datasets using k-NN accuracy and variable importance correlation metrics, demonstrating superior performance in preserving the random forest's learning while avoiding the global structure distortion common in class-conditional approaches.

## Method Summary
The RF-GAP method trains a random forest on labeled data, then computes proximity measures based on how often observations share terminal nodes across trees, weighted by in-bag observation counts. These proximities are normalized, symmetrized, and adjusted to create a proper kernel matrix. The kernel is then used as initialization for various manifold learning algorithms (Isomap, UMAP, t-SNE, Diffusion Map, etc.). Performance is evaluated by comparing k-NN classifier accuracy on the low-dimensional embeddings against models trained on full data, and by measuring correlation between variable importance scores from the embeddings versus the original data.

## Key Results
- RF-GAP-based embeddings produce k-NN classification accuracies more consistent with models trained on full datasets compared to class-conditional methods
- RF-GAP embeddings better preserve variable importance scores than both unsupervised and class-conditional supervised approaches
- Diffusion-based RF-GAP embeddings particularly excel at retaining random forest learning structure in low dimensions

## Why This Works (Mechanism)

### Mechanism 1
RF-GAP proximities preserve local similarity while retaining partial global information through the random forest's recursive splitting process. When two observations share terminal nodes across multiple trees, this indicates they follow similar decision paths, encoding both local neighborhood structure and some global context from the tree hierarchy.

### Mechanism 2
RF-GAP proximities avoid the global structure distortion caused by class-conditional distances in existing supervised manifold learning methods. Class-conditional methods artificially inflate distances between classes, breaking the natural manifold structure. RF-GAP proximities instead measure similarity based on how the random forest actually learned to separate classes through data-driven splits.

### Mechanism 3
RF-GAP-based embeddings better preserve variable importance and random forest learning in low dimensions compared to both unsupervised and class-conditional supervised methods. The proximity measure inherently weights observations by their similarity in the random forest's feature space, which already accounts for variable importance through the splitting process.

## Foundational Learning

- **Random Forest Proximity Measures**: Understanding how RF-GAP proximities differ from traditional proximity measures is crucial for grasping why they work better for supervised manifold learning.
  - *Quick check*: What is the key difference between RF-GAP proximities and the original Breiman proximity measure?

- **Manifold Learning Kernels**: The paper uses RF-GAP proximities as a kernel for manifold learning algorithms. Understanding how kernels encode similarity and how different manifold methods use these kernels is essential.
  - *Quick check*: How does the choice of kernel affect the resulting manifold embedding?

- **Supervised vs Unsupervised Dimensionality Reduction**: The paper contrasts RF-GAP with existing supervised methods that use class-conditional distances. Understanding the tradeoffs between supervised and unsupervised approaches is key.
  - *Quick check*: What are the potential drawbacks of using class-conditional distances in supervised manifold learning?

## Architecture Onboarding

- **Component map**: Random Forest → Proximity Calculation → Kernel Matrix → Manifold Learning Algorithm → Low-dimensional Embedding
- **Critical path**: The proximity calculation step is critical - errors here propagate through the entire pipeline. The manifold learning algorithm choice also significantly impacts results.
- **Design tradeoffs**: RF-GAP provides better supervised structure preservation but adds computational overhead for proximity calculation. Class-conditional methods are simpler but distort global structure.
- **Failure signatures**: Overfitting in the random forest leading to artificial proximity values, or manifold learning algorithms that don't properly handle the RF-GAP kernel structure.
- **First 3 experiments**:
  1. Compare RF-GAP proximities with original Breiman proximities on a simple dataset to verify improved structure preservation
  2. Test different manifold learning algorithms (Isomap, UMAP, t-SNE) with RF-GAP kernel to identify which works best
  3. Evaluate variable importance preservation by comparing importance scores before and after embedding on a known dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of kernel function (e.g., Gaussian kernel) in manifold learning methods affect the preservation of local and global structure when using RF-GAP proximities?

### Open Question 2
How does the number of trees in a random forest affect the quality of RF-GAP proximities and subsequent manifold learning embeddings?

### Open Question 3
How do RF-GAP proximities perform in high-dimensional data settings compared to traditional distance-based methods?

## Limitations

- Claims about global structure preservation through recursive splitting are based on theoretical reasoning rather than direct empirical validation
- Evidence for variable importance preservation is correlational rather than mechanistic
- Comparison with class-conditional methods assumes artificial class separation is always detrimental

## Confidence

- **High confidence**: RF-GAP proximities improve k-NN accuracy consistency compared to class-conditional methods
- **Medium confidence**: RF-GAP avoids global structure distortion seen in class-conditional approaches
- **Low confidence**: RF-GAP inherently preserves variable importance through the random forest splitting process

## Next Checks

1. Conduct ablation studies isolating the impact of proximity normalization and symmetrization procedures on embedding quality
2. Test RF-GAP performance on datasets where class separation is genuinely important (e.g., medical diagnosis) to verify the claim about avoiding artificial separation
3. Implement visualization tools to directly compare manifold distortion between RF-GAP and class-conditional approaches on simple synthetic datasets