---
ver: rpa2
title: Extremal Domain Translation with Neural Optimal Transport
arxiv_id: '2301.12874'
source_url: https://arxiv.org/abs/2301.12874
tags:
- transport
- cost
- optimal
- neural
- supp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the extremal transport (ET) problem to formalize
  the theoretically optimal unpaired domain translation under a given similarity function.
  The authors derive a scalable algorithm to approximate ET maps by solving a sequence
  of incomplete transport problems, leveraging recent advances in neural optimal transport.
---

# Extremal Domain Translation with Neural Optimal Transport

## Quick Facts
- **arXiv ID**: 2301.12874
- **Source URL**: https://arxiv.org/abs/2301.12874
- **Reference count**: 36
- **Key outcome**: Proposed extremal transport (ET) problem for optimal unpaired domain translation, with scalable algorithm using neural networks and incomplete transport, showing higher input-output similarity with increasing weight parameter w.

## Executive Summary
This paper introduces the extremal transport (ET) problem as a theoretical framework for optimal unpaired domain translation under a given similarity function. The authors develop a scalable neural algorithm that approximates ET maps by solving a sequence of incomplete transport problems, leveraging recent advances in neural optimal transport. The method is evaluated on both toy 2D examples and unpaired image-to-image translation tasks, demonstrating that increasing the parameter w yields translations with higher input-output similarity while preserving visual quality.

## Method Summary
The method solves the incomplete transport (IT) problem by approximating the transport map and potential with neural networks (T_θ and f_ψ), training them using stochastic gradient ascent-descent. The algorithm maximizes the incomplete transport objective, which balances transporting input probability mass to the target domain with preserving input-output similarity. The weight parameter w controls the fraction of the target measure to which probability mass is mapped, with higher values yielding more similar translations. The neural networks are trained using Adam optimizer with learning rate 10^-4, and w is gradually increased during training.

## Key Results
- The algorithm successfully approximates extremal transport maps by solving incomplete transport problems with increasing weight w
- On image translation tasks, increasing w improves input-output similarity (measured by ℓ2 distance) while maintaining visual quality
- The method outperforms CycleGAN in terms of transport cost on several unpaired translation tasks
- The approach scales to reasonable image sizes (64×64 and 128×128) while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Increasing w yields translations with higher input-output similarity while preserving visual quality.
- **Mechanism**: Higher w values allow the algorithm to map more of input probability mass P to regions of target Q that are closer to P in terms of the chosen cost function.
- **Core assumption**: The target domain can be meaningfully partitioned and the cost function accurately captures desired similarity.
- **Evidence anchors**: Abstract demonstrates increasing w yields higher input-output similarity; Section 5.2 shows ℓ2 similarity increases with w.

### Mechanism 2
- **Claim**: ET maps can be recovered as limits of incomplete transport maps by increasing w to infinity.
- **Mechanism**: The algorithm approximates optimal translation by solving a sequence of partial optimal transport problems.
- **Core assumption**: IT problem converges to ET problem as w approaches infinity.
- **Evidence anchors**: Abstract mentions deriving algorithm to approximate ET maps; Section 3.3 discusses using IT plans for sufficiently large finite w.

### Mechanism 3
- **Claim**: The algorithm solves incomplete transport by training neural networks via saddle point optimization.
- **Mechanism**: Learns transport map T_θ and potential f_ψ by solving a saddle point optimization problem derived from the dual formulation.
- **Core assumption**: Neural networks can accurately approximate optimal transport map and potential.
- **Evidence anchors**: Section 3.4 describes solving optimization by approximating map and potential with neural networks; Section 5 mentions PyTorch implementation.

## Foundational Learning

- **Concept: Optimal Transport (OT)**
  - Why needed here: Provides theoretical foundation for extremal transport problem and incomplete transport problem.
  - Quick check question: What is the Kantorovich relaxation of the Monge OT problem, and how does it relate to the extremal transport problem?

- **Concept: Neural Networks**
  - Why needed here: Used to approximate transport map and potential in the proposed algorithm.
  - Quick check question: How does the choice of neural network architecture (e.g., UNet, ResNet) affect performance of transport map approximation?

- **Concept: Saddle Point Optimization**
  - Why needed here: Algorithm solves saddle point optimization problem to learn transport map and potential.
  - Quick check question: What are potential issues with saddle point optimization, and how can they be mitigated in learning transport maps?

## Architecture Onboarding

- **Component map**: Input Distribution P -> Transport Map T_θ -> Output Distribution T_♯P ∩ Q, with Potential f_ψ enforcing constraints
- **Critical path**: Sample batches from P and Q → Compute loss for T_θ and f_ψ → Update networks using stochastic gradient ascent-descent (T_θ updated more frequently)
- **Design tradeoffs**: Cost function choice (ℓ2 vs perceptual) affects similarity; weight w controls trade-off between transport to target and input-output similarity
- **Failure signatures**: Poor visual translations indicate cost function or w issues; unstable training suggests saddle point optimization problems
- **First 3 experiments**:
  1. Train on simple 2D toy example with different w values to observe transport map changes
  2. Train on small unpaired image task (celeba to anime) with w=1, compare to CycleGAN
  3. Train on larger unpaired task (handbag to shoes) with different w values to observe similarity changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do neural extremal transport maps behave in high-dimensional domains (e.g., natural images beyond 128×128)?
- Basis in paper: [inferred] Authors test on 64×64 and 128×128 images, noting uncertainty about scaling to high dimensions.
- Why unresolved: Experiments limited to small image sizes; authors explicitly note scaling to high dimensions remains open.
- What evidence would resolve it: Experimental results on larger images demonstrating performance and scalability.

### Open Question 2
- Question: Can extremal transport formulation be extended to more general cost functions beyond ℓ2 that better capture perceptual similarity?
- Basis in paper: [explicit] Authors note theoretical results hold for any continuous cost function but experiments only use ℓ2.
- Why unresolved: While theory allows general costs, experiments only use ℓ2, and authors suggest perceptual costs as promising future direction.
- What evidence would resolve it: Empirical comparison of maps learned with different cost functions on image translation tasks.

### Open Question 3
- Question: What is theoretical convergence rate of neural algorithm for approximating extremal transport maps as w increases?
- Basis in paper: [inferred] Authors prove convergence but don't provide quantitative rates.
- Why unresolved: Theoretical results establish convergence but don't characterize how fast approximation improves with increasing w.
- What evidence would resolve it: Analytical bounds on convergence rate or empirical studies showing scaling with w and optimization time.

### Open Question 4
- Question: How can algorithm be modified to handle cases where supports of P and Q intersect, avoiding degenerate solutions?
- Basis in paper: [explicit] Authors discuss this limitation where ET plan doesn't move mass in intersecting support.
- Why unresolved: Authors identify issue but don't propose solutions or modifications.
- What evidence would resolve it: Algorithmic modifications with experimental validation on intersecting support cases.

### Open Question 5
- Question: How can quality metrics for partial generative models be developed to better evaluate extremal transport maps?
- Basis in paper: [explicit] Authors state existing unpaired metrics are not suitable for their setup where learned maps capture only part of target distribution.
- Why unresolved: Lack of appropriate metrics for evaluating methods that capture subsets rather than matching target distribution exactly.
- What evidence would resolve it: Development and validation of new metrics specifically designed for partial generative models.

## Limitations
- Scalability concerns: Experiments limited to relatively small datasets (64×64 or 128×128 images)
- Hyperparameter sensitivity: Performance appears sensitive to weight w and learning rate without comprehensive analysis
- Limited domain diversity: Evaluation focuses on relatively similar domains, untested on more diverse or challenging pairs

## Confidence
- **High confidence**: Theoretical framework and core mechanism of using incomplete transport to approximate extremal transport are well-established and empirically validated
- **Medium confidence**: Empirical results on image-to-image translation tasks are promising but limited in scope and dataset size
- **Low confidence**: Algorithm's performance on high-resolution images and more diverse domain pairs is speculative

## Next Checks
1. Scale-up experiment: Test algorithm on high-resolution (256×256 or 512×512) unpaired image-to-image translation tasks to validate computational efficiency claims
2. Domain diversity test: Apply method to more diverse domain pairs (paintings to photographs, sketches to real images) to assess generalization capability
3. Ablation study: Conduct comprehensive ablation varying w, learning rates, and neural network architectures to understand impact on performance and provide practical guidance