---
ver: rpa2
title: Hierarchically Gated Recurrent Neural Network for Sequence Modeling
arxiv_id: '2311.04823'
source_url: https://arxiv.org/abs/2311.04823
tags:
- forget
- lower
- modeling
- hgrn
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchically Gated Recurrent Neural Network
  (HGRN), a gated linear RNN model designed to address the challenges of slow sequential
  training and limited long-term dependency modeling in traditional RNNs. The core
  idea of HGRN is to incorporate forget gates with lower bounds that increase monotonically
  across layers, enabling upper layers to model long-term dependencies and lower layers
  to handle short-term dependencies.
---

# Hierarchically Gated Recurrent Neural Network for Sequence Modeling

## Quick Facts
- arXiv ID: 2311.04823
- Source URL: https://arxiv.org/abs/2311.04823
- Authors: 
- Reference count: 40
- This paper introduces Hierarchically Gated Recurrent Neural Network (HGRN), a gated linear RNN model designed to address the challenges of slow sequential training and limited long-term dependency modeling in traditional RNNs.

## Executive Summary
This paper introduces Hierarchically Gated Recurrent Neural Network (HGRN), a gated linear RNN model designed to address the challenges of slow sequential training and limited long-term dependency modeling in traditional RNNs. The core idea of HGRN is to incorporate forget gates with lower bounds that increase monotonically across layers, enabling upper layers to model long-term dependencies and lower layers to handle short-term dependencies. The proposed model achieves competitive performance on language modeling, image classification, and long-range arena benchmarks while maintaining linear computational complexity.

## Method Summary
HGRN uses a novel gating mechanism where forget gates have learnable lower bounds that increase monotonically across layers. This hierarchical structure allows upper layers to retain more historical information while lower layers can selectively forget. The model employs linear recurrence operations instead of full matrix multiplications, enabling parallel training and maintaining linear computational complexity. Additionally, HGRN extends to complex-valued representations with learnable phase angles to encode relative positional information, similar to RoPE encoding.

## Key Results
- Achieves perplexity of 24.14 on Wikitext-103 and 4.14 on the Pile dataset for a 1B parameter model
- Demonstrates superior or comparable results to state-of-the-art methods in autoregressive language modeling tasks
- Shows robustness in computer vision tasks and efficiency in processing long sequences while maintaining linear computational complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower-bounded forget gates in upper layers enable effective long-term dependency modeling while lower layers handle short-term dependencies.
- Mechanism: The monotonically increasing lower bounds push forget gate values away from saturation, maintaining gradient flow. Upper layers with high lower bounds keep more historical information, while lower layers with smaller bounds allow forgetting of irrelevant data.
- Core assumption: The additive lower bound structure effectively separates temporal scales across layers without requiring complex initialization or training tricks.
- Evidence anchors:
  - [abstract] "includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers."
  - [section 3.2] "we add an additive learnable value, referred to as the lower bound, to the original forget gate value... This ensures that the forget gate values in the lower layers remain relatively small, enabling the necessary forgetting of past information for modeling short-term dependencies."
  - [corpus] Weak: No direct citations in corpus supporting this specific hierarchical gating mechanism.
- Break condition: If the monotonic increase becomes too steep, upper layers may become unable to forget irrelevant information, causing memory bloat and degraded performance.

### Mechanism 2
- Claim: Complex-valued recurrence with learnable phase angles provides superior expressiveness compared to real-valued recurrence.
- Mechanism: By parameterizing recurrence in the complex domain (ht = λt ⊙ exp(iθ) ⊙ ht−1 + (1-λt) ⊙ ct), the model can encode oscillatory behaviors and relative positional information through the phase component θ.
- Core assumption: The complex domain provides sufficient additional degrees of freedom to capture richer sequential patterns without increasing computational complexity.
- Evidence anchors:
  - [section 3.2] "we extend our model to consider ht, it, ct ∈ C1×d as complex values... We find that parameterizing θt in a data-independent manner is preferable, as it allows for a clear interpretation of encoding relative position information."
  - [section 3.3] "Θ directly corresponds to RoPE" and "the matrix Λ will no longer be a Toeplitz matrix, thus unable to capture relative position information" if θ is data-dependent.
  - [corpus] Weak: No corpus evidence directly supporting this specific complex-valued formulation.
- Break condition: If θ becomes too sensitive to initialization or training dynamics, the model may fail to learn meaningful positional encodings.

### Mechanism 3
- Claim: Gating mechanisms in the output of linear recurrence layers provide performance gains by selectively filtering information.
- Mechanism: The output gate (gt = Sigmoid(Wgxt + bg)) acts as a learned filter before projection, allowing the model to suppress irrelevant information from the recurrent state.
- Core assumption: The additional gating layer provides complementary information selection beyond the forget and input gates in the recurrence.
- Evidence anchors:
  - [section 3.2] "The addition of gates to the output of the recurrence layer has been shown to be effective in state-space models" with citation [11, 46, 49, 82].
  - [section 3.2] "we incorporate an output gate before performing the output projection as follows and get HGRU"
  - [corpus] Moderate: The corpus mentions related works (RWKV, S4, DSS) that use gating but doesn't specifically validate output gating.
- Break condition: If the output gate becomes saturated (values near 0 or 1), it may prevent necessary information flow and degrade performance.

## Foundational Learning

- Concept: Linear recurrence and element-wise operations
  - Why needed here: HGRN relies on efficient element-wise recurrence rather than full matrix multiplication for scalability
  - Quick check question: Why does element-wise multiplication enable parallel training while matrix multiplication doesn't?

- Concept: Gating mechanisms in RNNs
  - Why needed here: The forget gate with lower bounds is the core innovation that enables HGRN's performance
  - Quick check question: How does adding a lower bound to the forget gate prevent gradient vanishing while maintaining the ability to forget?

- Concept: Complex number representations
  - Why needed here: HGRN uses complex-valued recurrence to encode oscillatory behaviors and positional information
  - Quick check question: What advantage does complex-valued recurrence provide over real-valued recurrence in terms of expressiveness?

## Architecture Onboarding

- Component map: Input → Linear projection → Forget gate with lower bound → Input gate → Complex-valued recurrence → Output gate → LayerNorm → Output
- Critical path: Input → Linear projection → Forget gate with lower bound → Input gate → Complex-valued recurrence → Output gate → LayerNorm → Output
- Design tradeoffs: Linear recurrence enables parallel training but requires careful gating to avoid losing expressiveness; complex values add representational power but increase parameter count.
- Failure signatures: Gradient vanishing (if forget gates saturate), memory bloat (if forget gates don't forget enough), or poor positional encoding (if complex phase angles are poorly initialized).
- First 3 experiments:
  1. Compare HGRN with and without lower bounds on a simple language modeling task to verify the hierarchical gating effect
  2. Test complex vs real-valued recurrence with the same gating structure to isolate the benefit of complex numbers
  3. Evaluate the impact of output gating by removing it from the HGRU and measuring performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HGRN scale with increasing model size beyond 1 billion parameters on the Pile dataset?
- Basis in paper: [explicit] The paper trains a 1 billion parameter HGRN model on the Pile dataset and compares it to other models of similar size, but does not explore larger scales.
- Why unresolved: The paper focuses on demonstrating HGRN's effectiveness at moderate scales, leaving questions about its behavior at larger scales unanswered.
- What evidence would resolve it: Training and evaluating HGRN models with 2 billion, 5 billion, and 10 billion parameters on the Pile dataset and comparing their perplexity scores to similarly sized transformer models.

### Open Question 2
- Question: How does the hierarchical gating mechanism in HGRN compare to other long-range dependency modeling techniques in terms of computational efficiency and accuracy?
- Basis in paper: [inferred] The paper claims HGRN's hierarchical gating allows upper layers to model long-term dependencies while lower layers handle short-term dependencies, but does not compare this approach to alternatives like sparse attention or convolutional methods.
- Why unresolved: The paper focuses on HGRN's internal mechanisms rather than comparing its gating strategy to other approaches for handling long sequences.
- What evidence would resolve it: Benchmarking HGRN against models using sparse attention, convolutional methods, and other gating techniques on tasks requiring long-range dependencies, measuring both accuracy and computational resources.

### Open Question 3
- Question: What is the impact of the lower bound parameterization on HGRN's ability to learn task-specific representations?
- Basis in paper: [explicit] The paper describes the monotonically increasing lower bounds but does not explore how different parameterizations affect performance on specific tasks.
- Why unresolved: The paper demonstrates the effectiveness of the current parameterization but does not investigate whether alternative formulations could improve task-specific performance.
- What evidence would resolve it: Ablation studies varying the lower bound parameterization (e.g., different activation functions, learning rate schedules) and measuring the impact on task-specific metrics across multiple domains.

### Open Question 4
- Question: How does HGRN's performance degrade when processing sequences significantly longer than those seen during training?
- Basis in paper: [inferred] The paper shows HGRN can extrapolate to longer sequences but does not quantify performance degradation beyond the tested ranges.
- Why unresolved: The paper demonstrates extrapolation capability but does not explore the limits of this ability or the nature of performance degradation at extreme sequence lengths.
- What evidence would resolve it: Systematic testing of HGRN on sequences 2x, 5x, and 10x longer than training sequences, measuring accuracy and computational requirements to identify scaling limits.

### Open Question 5
- Question: What is the relationship between the number of layers in HGRN and its ability to capture hierarchical patterns in data?
- Basis in paper: [inferred] The paper mentions hierarchical gating but does not investigate how the number of layers affects the model's ability to learn hierarchical representations.
- Why unresolved: The paper demonstrates the effectiveness of multiple layers with hierarchical gating but does not explore how the depth affects the learning of hierarchical patterns.
- What evidence would resolve it: Experiments varying the number of layers (e.g., 2, 4, 8, 16) and analyzing the learned representations for hierarchical structure using techniques like probing classifiers or visualization methods.

## Limitations

- Implementation Complexity: The paper claims linear computational complexity and parallel training capabilities, but the CUDA-based sequential scan implementation details are not provided. This critical component affects both reproducibility and the claimed efficiency benefits.
- Complex-Valued Extension Validation: While the complex-valued formulation is presented as a key innovation, the empirical validation is limited to ablation studies without direct comparison to real-valued alternatives under identical conditions.
- Generalization Across Domains: The model shows strong performance in language modeling but limited validation in other domains beyond the LRA benchmark, raising questions about its effectiveness across diverse sequence modeling tasks.

## Confidence

**High Confidence:** The core mechanism of hierarchical forget gates with monotonically increasing lower bounds is well-supported by both theoretical analysis and empirical results. The ablation studies clearly demonstrate the importance of this design choice.

**Medium Confidence:** The computational complexity claims and parallel training benefits are supported by theoretical analysis, but lack detailed implementation specifics that would enable full verification.

**Low Confidence:** The advantages of the complex-valued formulation are theoretically justified but lack comprehensive empirical validation, particularly in direct comparisons with real-valued alternatives.

## Next Checks

1. **Independent Implementation Verification:** Reimplement HGRN from scratch focusing on the forget gate lower bound mechanism and verify the reported perplexity scores on Wikitext-103 with a 1B parameter model.

2. **Complex vs Real-Valued Comparison:** Train identical architectures with and without complex-valued recurrence while keeping all other components constant to isolate the contribution of complex numbers to performance.

3. **Parallel Training Benchmark:** Implement the parallel scan algorithm and measure actual training throughput improvements compared to standard sequential RNN training on identical hardware, verifying the claimed linear complexity benefits.