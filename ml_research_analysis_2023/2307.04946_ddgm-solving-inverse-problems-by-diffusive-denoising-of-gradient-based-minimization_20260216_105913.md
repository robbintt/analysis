---
ver: rpa2
title: 'DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization'
arxiv_id: '2307.04946'
source_url: https://arxiv.org/abs/2307.04946
tags:
- diffusion
- noise
- reconstruction
- denoising
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new method, DDGM, for solving inverse problems
  by combining gradient-based minimization of reconstruction error with denoising.
  The method iteratively adds noise, denoises the image using a pre-trained denoising
  network, and updates the image via gradient descent on the reconstruction error.
---

# DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization

## Quick Facts
- arXiv ID: 2307.04946
- Source URL: https://arxiv.org/abs/2307.04946
- Reference count: 23
- Solves inverse problems by combining gradient descent with denoising using exponentially decaying noise schedule

## Executive Summary
This paper introduces DDGM, a method for solving inverse problems by iteratively combining gradient-based minimization of reconstruction error with denoising using a pre-trained network. The approach adds Gaussian noise to the current estimate before denoising, with both noise level and denoising step size decaying exponentially over iterations. The method is demonstrated on tomographic reconstruction from electron micrographs, where it achieves high accuracy with as few as 50 denoising steps, outperforming competing diffusion methods DDRM and DPS.

## Method Summary
DDGM solves inverse problems by alternating between gradient descent on reconstruction error and denoising steps. Starting from an initial estimate (typically zero), the algorithm performs K gradient descent steps on the reconstruction error, adds Gaussian noise with level σ, applies a pre-trained denoising network, and repeats. Both the noise level σ and denoising step size decay exponentially from initial values σ₁ and λ to final values σ_N and λ_N. The denoising network is trained to predict noise from corrupted images using a standard U-Net architecture. For large images, the method uses patch-based denoising with smooth blending.

## Key Results
- DDGM achieves high accuracy in tomographic reconstruction with as few as 50 denoising steps
- Outperforms competing diffusion methods DDRM and DPS on tomography benchmarks using MSE and SSIM metrics
- Demonstrates effective extension to arbitrarily large images through patch-based denoising with smooth blending

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding noise before denoising enables iterative improvement because it decorrelates the gradient-descent artifact from the true image structure.
- Mechanism: The gradient descent on reconstruction error alone produces a blurry, artifact-laden estimate. By injecting Gaussian noise, the denoising network is forced to rely on learned priors rather than low-level reconstruction errors, gradually sharpening and denoising the estimate.
- Core assumption: The denoising network has learned a prior that correlates with the true image distribution, and noise injection at each step ensures the denoiser operates on corrupted-but-structure-preserving inputs.
- Evidence anchors:
  - [abstract] "Noise is also added at each step, so the iterative dynamics resembles a Langevin or diffusion process."
  - [section] "The trick is to add Gaussian noise to x before applying the denoising net. If we add enough noise, our denoiser appears to improve x in some ways, at the expense of making it blurry."
  - [corpus] No direct match in neighbor papers; assumption based on empirical claim.
- Break condition: If the denoising network is overfit to clean images and cannot generalize to noisy inputs, the iterative process will degrade rather than improve the reconstruction.

### Mechanism 2
- Claim: Exponential decay of noise and denoising step size balances data fidelity and prior strength across iterations.
- Mechanism: Early iterations use high noise to allow the prior to dominate, gradually shifting to lower noise where the reconstruction error term dominates, enabling fine-tuning of details.
- Core assumption: The annealing schedule can be tuned so that the prior and data terms are balanced appropriately at each stage of reconstruction.
- Evidence anchors:
  - [abstract] "Both the level of added noise and the size of the denoising step decay exponentially with time."
  - [section] "We keep this value λ = 9e − 5 for all experiments. Future work may explore tuning this parameter as well."
  - [corpus] Neighbor paper on non-convex energy minimization suggests similar annealing concepts but no direct DDGM match.
- Break condition: If the decay schedule is too aggressive, the prior may dominate too early, causing the reconstruction to ignore important data-driven corrections.

### Mechanism 3
- Claim: The number of gradient steps per denoising iteration (K) determines the balance between algebraic reconstruction and prior-based refinement.
- Mechanism: Fewer gradient steps per denoising iteration means more frequent denoising, allowing the prior to correct reconstruction artifacts sooner; more gradient steps allow the reconstruction to move further before correction.
- Core assumption: The optimal K depends on the ill-conditioning of the forward operator and the strength of the learned prior.
- Evidence anchors:
  - [section] "We found that K = 25... When the number of denoiser evaluations N = 150, we found the optimal value of K = 15..."
  - [section] "Interestingly these values are less than the optimal value of K = 100 when doing simple algebraic reconstruction."
  - [corpus] No direct neighbor evidence; empirical tuning result.
- Break condition: If K is too large, the reconstruction may diverge from the true solution before denoising can correct it; if too small, denoising may overly constrain the solution.

## Foundational Learning

- Concept: Ill-posed inverse problems and the role of regularization
  - Why needed here: The tomography problem is highly ill-conditioned; without regularization, gradient descent amplifies noise and produces poor reconstructions.
  - Quick check question: What happens to the solution of an ill-posed inverse problem if you perform unlimited gradient descent on the reconstruction error without regularization?

- Concept: Denoising as a learned prior
  - Why needed here: The denoising network encodes image statistics learned from clean data, serving as a prior when solving the inverse problem.
  - Quick check question: How does a network trained to denoise images implicitly learn a probability distribution over natural images?

- Concept: Diffusion processes in generative modeling
  - Why needed here: The iterative noise-addition and denoising steps resemble a diffusion process, borrowing ideas from score-based generative models.
  - Quick check question: In score-based generative models, what role does the noise schedule play in the sampling process?

## Architecture Onboarding

- Component map:
  Forward operator A -> Gradient descent module -> Noise injection module -> Denoising network -> Output

- Critical path:
  1. Initialize x = 0
  2. For each denoising iteration:
     a. Perform K gradient descent steps on reconstruction error
     b. Add Gaussian noise with current noise level σn
     c. Apply denoising network
  3. Output final x

- Design tradeoffs:
  - More denoising steps (N) vs. more gradient steps per denoising (K): higher N allows finer-grained prior application but increases runtime; higher K allows larger reconstruction moves but risks divergence.
  - Noise schedule aggressiveness: faster decay gives more weight to data early but may underutilize the prior; slower decay allows more prior influence but may underfit the data.

- Failure signatures:
  - Blurry outputs: too much denoising or insufficient gradient steps
  - Noisy/high-frequency artifacts: too few denoising steps or aggressive gradient steps
  - Systematic bias: denoising network not well-matched to image domain

- First 3 experiments:
  1. Run with K=1, N=10, σ1=3.0, σN=0.03: observe trade-off between reconstruction fidelity and denoising influence
  2. Compare with K=25, N=10, same noise schedule: test if more gradient steps per denoising improves results
  3. Test with σ1=30.0 (unconditional generation level): see if higher initial noise improves or degrades reconstruction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical justification for the effectiveness of adding noise before denoising in the DDGM method?
- Basis in paper: [explicit] The paper mentions that adding noise before denoising is a key aspect of DDGM, but does not provide a theoretical explanation for why this improves results.
- Why unresolved: The paper does not provide a rigorous theoretical analysis of why adding noise before denoising is beneficial for solving inverse problems.
- What evidence would resolve it: A theoretical analysis proving that adding noise before denoising helps in solving inverse problems by providing a mathematical framework for understanding the diffusion process and its impact on the solution.

### Open Question 2
- Question: How does the choice of annealing schedule for the noise level and gradient step size impact the performance of DDGM?
- Basis in paper: [inferred] The paper mentions that the noise level and gradient step size decay exponentially over iterations, but does not explore the impact of different annealing schedules on the method's performance.
- Why unresolved: The paper does not systematically investigate the impact of different annealing schedules on the reconstruction quality and computational efficiency of DDGM.
- What evidence would resolve it: A comprehensive study comparing the performance of DDGM with different annealing schedules for the noise level and gradient step size, using both synthetic and real-world datasets.

### Open Question 3
- Question: Can DDGM be extended to handle non-linear inverse problems effectively?
- Basis in paper: [inferred] The paper focuses on linear inverse problems, but mentions that extending DDGM to non-linear problems is a potential direction for future work.
- Why unresolved: The paper does not provide any empirical or theoretical results on the application of DDGM to non-linear inverse problems, which are more common in practice.
- What evidence would resolve it: Empirical results demonstrating the effectiveness of DDGM on a range of non-linear inverse problems, along with a discussion of any modifications or adaptations required for handling non-linearities.

## Limitations

- Method performance depends heavily on the quality and domain match of the pre-trained denoising network
- Optimal hyperparameters (K, N, noise schedule) were tuned on a single dataset and may not generalize to other inverse problems
- The extension to large images using patch-based denoising lacks detailed implementation specifications for the blending function

## Confidence

- Mechanism 1 (Noise injection enabling iterative improvement): Medium - supported by empirical observations but lacks rigorous theoretical justification
- Mechanism 2 (Exponential decay schedule): Medium - the annealing concept is well-established, but optimal schedule parameters are problem-specific
- Mechanism 3 (K parameter balancing reconstruction vs. denoising): Low - this is primarily empirical, with limited theoretical grounding

## Next Checks

1. Test DDGM on a different inverse problem (e.g., compressed sensing MRI) to assess hyperparameter transferability and method robustness
2. Compare DDGM against plug-and-play priors using the same denoising network to isolate the contribution of the gradient descent component
3. Analyze reconstruction trajectories (intermediate outputs) to empirically verify whether the noise injection genuinely decorrelates gradient descent artifacts as hypothesized