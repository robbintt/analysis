---
ver: rpa2
title: 'The Devil is in the Errors: Leveraging Large Language Models for Fine-grained
  Machine Translation Evaluation'
arxiv_id: '2308.07286'
source_url: https://arxiv.org/abs/2308.07286
tags:
- score
- translation
- errors
- palm-2
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large language models (LLMs) for
  fine-grained machine translation evaluation by moving beyond single scalar scores
  to identify and categorize translation errors using the Multidimensional Quality
  Metrics (MQM) framework. The authors propose AutoMQM, a prompting technique that
  asks LLMs to identify error spans and classify them by severity and category, then
  derive a quality score automatically.
---

# The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation

## Quick Facts
- arXiv ID: 2308.07286
- Source URL: https://arxiv.org/abs/2308.07286
- Reference count: 40
- Primary result: Finetuning LLMs significantly improves segment-level correlation with human judgment for machine translation evaluation, with AutoMQM outperforming simple score prediction prompting

## Executive Summary
This paper investigates using large language models (LLMs) for fine-grained machine translation evaluation by moving beyond single scalar scores to identify and categorize translation errors using the Multidimensional Quality Metrics (MQM) framework. The authors propose AutoMQM, a prompting technique that asks LLMs to identify error spans and classify them by severity and category, then derive a quality score automatically. Experiments with PaLM and PaLM-2 models show that finetuning improves segment-level correlation with human judgment, particularly for smaller models. AutoMQM outperforms simple score prediction prompting, especially for larger models, while providing interpretable error annotations.

## Method Summary
The paper evaluates LLMs for machine translation evaluation using two prompting approaches: score prediction (GEMBA-SQM) and AutoMQM, which asks models to identify and classify translation errors. The authors test zero-shot, in-context learning, and finetuned settings across PaLM and PaLM-2 model families on WMT Metrics Shared Task data. Finetuning uses regression loss to match human scores, while AutoMQM leverages chain-of-thought prompting to elicit structured reasoning about errors. The system derives quality scores algorithmically from identified errors using the MQM framework.

## Key Results
- PaLM-2 models achieve 90.1% system-level accuracy but only 0.247 Pearson correlation at segment level for score prediction
- Finetuning significantly improves segment-level correlation, with PaLM-2 BISON reaching 0.511 Pearson (reference-based)
- AutoMQM improves performance over score prediction prompting, with PaLM-2 UNICORN achieving 0.432 Pearson versus 0.401
- PaLM-2 models correctly identify over 50% of major errors and achieve comparable span-level quality to supervised word-level evaluators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompting LLMs for score prediction alone leads to high system-level accuracy but low segment-level correlation with human judgment.
- **Mechanism:** LLMs can effectively rank entire systems by average quality due to their strong reasoning over full translation outputs, but struggle to consistently assess individual segments due to output distribution collapse (predicting only a limited set of discrete scores).
- **Core assumption:** System-level evaluation benefits from averaging out segment-level noise, while segment-level evaluation requires fine-grained score distribution matching.
- **Evidence anchors:**
  - [abstract] states LLMs are zero-shot state-of-the-art system-level evaluators but show low segment-level correlation compared to learned metrics.
  - [section 6.2.1] shows PaLM-540B achieves 90.1% system-level accuracy but only 0.247 Pearson at segment level.
  - [corpus] includes "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models" which discusses limitations of simple score prediction.

### Mechanism 2
- **Claim:** Finetuning LLMs on human judgment data significantly improves segment-level correlation, particularly for smaller models.
- **Mechanism:** By exposing LLMs to the distribution of human scores during finetuning, the model learns to map translation features to the appropriate score range, overcoming the zero-shot output collapse.
- **Core assumption:** The finetuning data contains sufficient diversity in score distributions to teach the model the full range of quality assessments.
- **Evidence anchors:**
  - [section 6.2.1] shows finetuned PaLM-2 BISON achieves 0.511 Pearson (reference-based) compared to 0.394 zero-shot.
  - [section 6.2.1] notes smaller models benefit more from finetuning, with S and M models showing competitive performance after finetuning.
  - [section 4.2] describes using regression loss between model outputs and human scores during finetuning.

### Mechanism 3
- **Claim:** AUTO MQM prompting improves LLM performance over simple score prediction by leveraging chain-of-thought reasoning to identify and classify errors according to MQM framework.
- **Mechanism:** By explicitly asking the model to identify error spans and classify them, the model performs structured reasoning that leads to more accurate quality assessments, with the final score derived algorithmically from identified errors.
- **Core assumption:** The model's reasoning capabilities are sufficient to accurately identify and classify translation errors when properly prompted.
- **Evidence anchors:**
  - [abstract] states AutoMQM improves performance compared to score prediction prompting, especially for larger models.
  - [section 5] describes the AutoMQM prompt structure that asks for error identification and classification.
  - [section 6.3.1] shows PaLM-2 UNICORN achieves 0.432 Pearson with AutoMQM versus 0.401 with score prediction.
  - [corpus] includes "xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection" which aligns with the fine-grained error detection approach.

## Foundational Learning

- **Concept:** MQM (Multidimensional Quality Metrics) framework
  - Why needed here: AutoMQM relies on the MQM framework's error severity and category system to structure the model's reasoning and derive quality scores.
  - Quick check question: What are the two severity levels in MQM and how do they contribute to the final score?

- **Concept:** Chain-of-thought prompting
  - Why needed here: AutoMQM uses chain-of-thought prompting to elicit structured reasoning from the LLM, asking it to identify errors before deriving a score.
  - Quick check question: How does chain-of-thought prompting differ from direct score prediction prompting in terms of model output?

- **Concept:** In-context learning and its limitations
  - Why needed here: The paper explores in-context learning for both score prediction and AutoMQM, finding it generally ineffective for score prediction but beneficial for AutoMQM.
  - Quick check question: What is the key difference in how in-context learning affects AutoMQM versus score prediction performance?

## Architecture Onboarding

- **Component map:** Prompt → LLM → Error identification/score prediction → Score derivation → Correlation calculation with human judgments
- **Critical path:** Prompt → LLM → Error identification/score prediction → Score derivation → Correlation calculation with human judgments
- **Design tradeoffs:** Using LLMs provides strong system-level evaluation without training data but requires finetuning for segment-level accuracy; AutoMQM provides interpretability but requires in-context examples; larger models perform better but are more expensive.
- **Failure signatures:** Output score distribution collapse (predicting only 0, 50, 90, 95); poor segment-level correlation despite good system-level accuracy; in-context learning degrading rather than improving performance.
- **First 3 experiments:**
  1. Test zero-shot score prediction with PaLM-2 BISON on WMT22 en-de to establish baseline system-level and segment-level performance.
  2. Implement AutoMQM prompt with 1-2 in-context examples on same data to measure improvement in both correlation and error identification.
  3. Compare finetuned PaLM-2 BISON (regression objective) against zero-shot version on segment-level correlation to quantify finetuning benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of LLMs on low-resource language pairs compare to high-resource language pairs when using AutoMQM?
- **Basis in paper:** [explicit] The paper evaluates AutoMQM on low-resource language pairs (en↔gu and en↔kk) and finds that performance lags behind state-of-the-art learned metrics.
- **Why unresolved:** The paper provides limited analysis on why this performance gap exists and whether it can be mitigated through additional techniques like finetuning or larger model sizes.
- **What evidence would resolve it:** A comprehensive study comparing AutoMQM performance across a wider range of low-resource and high-resource language pairs, including the impact of finetuning and model scaling.

### Open Question 2
- **Question:** What is the optimal number of in-context examples for AutoMQM to achieve the best performance?
- **Basis in paper:** [explicit] The paper explores the impact of increasing the number of in-context examples on AutoMQM performance and finds that performance plateaus after 4 examples.
- **Why unresolved:** The paper does not investigate the impact of different in-context example sampling strategies or the quality of examples on AutoMQM performance.
- **What evidence would resolve it:** An ablation study comparing the performance of AutoMQM with different numbers and qualities of in-context examples, as well as different sampling strategies.

### Open Question 3
- **Question:** How does the performance of AutoMQM compare to other fine-grained evaluation methods like COMET-WL?
- **Basis in paper:** [explicit] The paper compares AutoMQM to COMET-WL in terms of span-level quality and finds that AutoMQM predicts spans of comparable quality but with lower span precision.
- **Why unresolved:** The paper does not provide a detailed analysis of the strengths and weaknesses of each method or investigate how they can be combined for better performance.
- **What evidence would resolve it:** A comprehensive comparison of AutoMQM and COMET-WL on a wide range of language pairs and evaluation metrics, as well as an investigation into potential synergies between the two methods.

## Limitations

- Limited generalization to low-resource language pairs, with performance lagging behind state-of-the-art learned metrics
- No comprehensive analysis of in-context learning effectiveness or guidance on when to use it versus finetuning
- Uncertainty about real-world reliability of LLM-identified errors beyond controlled evaluation datasets

## Confidence

**High confidence:** The mechanism that prompting alone leads to output distribution collapse and low segment-level correlation is well-supported by experimental results showing discrete score predictions (0, 50, 90, 95) and consistently lower Pearson correlations compared to finetuned models.

**Medium confidence:** The claim that AutoMQM improves performance over simple score prediction, particularly for larger models, is supported by correlation improvements but relies on specific prompt engineering that may not generalize across different LLM architectures or MQM variants.

**Medium confidence:** The finding that smaller models benefit more from finetuning is supported by experimental data but the paper does not provide a theoretical explanation for this phenomenon or investigate the optimal training set size for different model scales.

## Next Checks

1. **Cross-domain validation:** Evaluate AutoMQM on domain-specific translation datasets (medical, legal, technical) to assess generalization beyond news translation and verify if error identification patterns hold across different content types.

2. **Human evaluation of LLM-identified errors:** Conduct systematic human assessment of LLM-identified error spans to validate precision and recall metrics, particularly focusing on false positives and false negatives to understand model limitations.

3. **Ablation study on in-context learning:** Systematically vary the number and selection criteria of in-context examples to determine optimal configurations for different model sizes and tasks, providing clearer guidance on when in-context learning is beneficial.