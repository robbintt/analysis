---
ver: rpa2
title: Active and Passive Causal Inference Learning
arxiv_id: '2308.09248'
source_url: https://arxiv.org/abs/2308.09248
tags:
- causal
- data
- outcome
- action
- potential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive overview of causal inference
  methods, categorizing them into active and passive approaches. Active methods, such
  as randomized controlled trials (RCTs) and bandit algorithms, involve actively collecting
  data to estimate causal effects.
---

# Active and Passive Causal Inference Learning

## Quick Facts
- arXiv ID: 2308.09248
- Source URL: https://arxiv.org/abs/2308.09248
- Authors: 
- Reference count: 40
- Key outcome: This paper provides a comprehensive overview of causal inference methods, categorizing them into active and passive approaches. Active methods, such as randomized controlled trials (RCTs) and bandit algorithms, involve actively collecting data to estimate causal effects. Passive methods, like matching, inverse probability weighting, and doubly robust estimators, work with existing observational data. The paper also discusses recent deep learning-based approaches that aim to improve causal inference by learning latent representations of confounders. While these methods show promise, the paper highlights challenges such as collider bias and distribution shifts. Overall, the paper serves as a valuable resource for researchers and practitioners seeking to understand and apply causal inference techniques.

## Executive Summary
This paper provides a comprehensive overview of causal inference methods, categorizing them into active and passive approaches. Active methods, such as randomized controlled trials (RCTs) and bandit algorithms, involve actively collecting data to estimate causal effects. Passive methods, like matching, inverse probability weighting, and doubly robust estimators, work with existing observational data. The paper also discusses recent deep learning-based approaches that aim to improve causal inference by learning latent representations of confounders. While these methods show promise, the paper highlights challenges such as collider bias and distribution shifts. Overall, the paper serves as a valuable resource for researchers and practitioners seeking to understand and apply causal inference techniques.

## Method Summary
The paper synthesizes various causal inference methods, distinguishing between active approaches (RCTs, bandits) that actively collect interventional data and passive approaches (matching, IPW, doubly robust) that work with observational data. It also explores deep learning-based methods for causal inference, which aim to learn latent representations of confounders from high-dimensional data. The methods are evaluated based on their ability to estimate average treatment effects (ATE) and individual treatment effects (ITE) under various assumptions and data scenarios.

## Key Results
- Active methods like RCTs and bandit algorithms provide better identifiability of causal effects but may be more expensive or unethical to implement
- Passive methods are more practical but require stronger assumptions and may introduce bias due to unmeasured confounding or violation of the overlap assumption
- Deep learning-based methods show promise in learning compact representations of confounders but face challenges in generalization and handling distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active causal inference methods like RCTs and bandit algorithms allow direct estimation of intervention probabilities by actively collecting data, bypassing the need for complex adjustments required in passive methods.
- Mechanism: By randomly assigning actions independent of covariates, RCTs ensure exchangeability and positivity assumptions are met, allowing conditional probabilities to equal intervention probabilities. This satisfies the identifiability conditions needed for causal inference.
- Core assumption: The data collection process can be controlled to ensure randomization or near-randomization of treatment assignment.
- Evidence anchors:
  - [abstract]: "Active methods, such as randomized controlled trials (RCTs) and bandit algorithms, involve actively collecting data to estimate causal effects."
  - [section]: "A set of points{(xt,at,yt)}T t=0 collected by RCT automatically satisfy the exchangeability assumption."
- Break condition: When randomization is impossible or unethical, such as in medical treatment studies where patients cannot be randomly assigned to treatments.

### Mechanism 2
- Claim: Deep learning-based causal inference methods can learn compact representations of confounders from high-dimensional data, enabling better estimation of causal effects than traditional methods.
- Mechanism: Deep neural networks learn non-linear transformations of covariates that capture intrinsic similarities underlying the data, allowing them to estimate potential outcomes even with complex, high-dimensional input like medical images.
- Core assumption: The underlying causal relationships can be captured by a function representable by a deep neural network.
- Evidence anchors:
  - [abstract]: "The paper also discusses recent deep learning-based approaches that aim to improve causal inference by learning latent representations of confounders."
  - [section]: "Deep learning is applied to CI in order to infer causal effects by learning the hidden/unknown confounder representations from complicated data and causal graph relationships."
- Break condition: When the causal relationships are fundamentally non-linear or involve interactions that cannot be captured by the chosen neural network architecture.

### Mechanism 3
- Claim: Combining active and passive causal inference methods can mitigate biases from non-randomized data while maintaining efficiency in data collection.
- Mechanism: Active methods like bandits can be used to collect additional interventional data on underrepresented covariate-action combinations, while passive methods like IPW and matching can be applied to the existing observational dataset.
- Core assumption: There is sufficient overlap between the covariate distributions of the observational and interventional datasets.
- Evidence anchors:
  - [section]: "It is however often necessary to combine active and passive approaches in order to mitigate the bias arising from non-randomized data."
  - [corpus]: "Found 25 related papers... Top related titles: Passive learning of active causal strategies in agents and language models, Privacy Against Agnostic Inference Attacks in Vertical Federated Learning, Active-Passive Federated Learning for Vertically Partitioned Multi-view Data."
- Break condition: When the observational and interventional datasets have very different covariate distributions, making it difficult to combine them effectively.

## Foundational Learning

- Concept: Exchangeability/Ignorability
  - Why needed here: This assumption is crucial for converting statistical quantities (conditional probabilities) into causal quantities (intervention probabilities) in both active and passive methods.
  - Quick check question: If treatment assignment is independent of potential outcomes, can we use observational data to estimate causal effects?

- Concept: Positivity/Overlap
  - Why needed here: This ensures that for every covariate value, there is a non-zero probability of receiving each possible treatment, which is necessary for estimating all potential outcomes.
  - Quick check question: If a particular treatment is never observed for a specific subgroup, can we still estimate the causal effect for that subgroup?

- Concept: Confounder vs. Collider
  - Why needed here: Understanding the difference between confounders (variables that affect both treatment and outcome) and colliders (variables caused by both treatment and outcome) is crucial for proper causal identification and avoiding bias.
  - Quick check question: If we condition on a collider variable, what effect does this have on our causal estimates?

## Architecture Onboarding

- Component map: Data collection layer (active vs. passive methods) -> Causal graph specification -> Estimand definition (ATE, CATE, etc.) -> Estimation method (matching, IPW, doubly robust, deep learning) -> Assumption checking and validation

- Critical path:
  1. Specify causal graph and identify confounders
  2. Choose appropriate data collection method (active vs. passive)
  3. Apply estimation method appropriate to data type and assumptions
  4. Validate assumptions and check for potential biases

- Design tradeoffs:
  - Active methods (RCTs, bandits) vs. passive methods (matching, IPW)
    - Active methods provide better identifiability but may be more expensive or unethical
    - Passive methods are more practical but require stronger assumptions and may introduce bias
  - Simple parametric models vs. complex deep learning models
    - Simple models are more interpretable but may not capture complex relationships
    - Deep learning models can capture complex relationships but may overfit or be less interpretable

- Failure signatures:
  - Violation of positivity/overlap assumption: High variance in treatment effect estimates, particularly for subgroups with little or no representation in the data
  - Unmeasured confounding: Systematic bias in treatment effect estimates, often revealed by sensitivity analyses
  - Model misspecification in deep learning approaches: Poor generalization to out-of-sample data, particularly for counterfactual predictions

- First 3 experiments:
  1. Apply matching methods to a simulated dataset with known confounding structure to verify the method can recover the true causal effect
  2. Implement an epsilon-greedy bandit algorithm to collect interventional data and compare ATE estimates to those from an RCT
  3. Train a deep latent variable model (e.g., CEVAE) on a high-dimensional dataset and evaluate its ability to estimate individual treatment effects compared to traditional methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the overlap assumption be relaxed or mitigated in practice when dealing with high-dimensional covariate spaces?
- Basis in paper: [explicit] The paper discusses the overlap assumption and its challenges, particularly in high-dimensional spaces, suggesting that generalization might help but also noting the potential for extrapolation errors.
- Why unresolved: The paper mentions that generalization through parametric or non-parametric models can help, but it does not provide a concrete method or framework for ensuring reliable generalization, especially in high-dimensional settings where extrapolation is risky.
- What evidence would resolve it: A detailed study or framework that demonstrates how to effectively use generalization techniques (e.g., deep learning models) to relax the overlap assumption without introducing significant bias, along with empirical results showing improved performance in high-dimensional settings.

### Open Question 2
- Question: What are the most effective ways to detect and correct for collider bias in causal inference, especially in complex causal graphs?
- Basis in paper: [explicit] The paper mentions collider bias as a limitation and provides an example of how it can occur, but it does not discuss methods for detecting or correcting it.
- Why unresolved: Collider bias is a subtle form of bias that can easily go unnoticed, and the paper does not provide any guidance on how to identify or address it in practice.
- What evidence would resolve it: A set of diagnostic tools or statistical tests that can reliably detect the presence of collider bias in observational data, along with methods for adjusting causal estimates to account for it.

### Open Question 3
- Question: How can deep latent variable models be made more robust to distribution shifts between training and inference, particularly when the treatment assignment mechanism changes?
- Basis in paper: [explicit] The paper discusses the issue of distribution shift in the context of deep latent variable models like CEVAE and UTVAE, noting that using a uniform treatment distribution can help but does not fully resolve the issue.
- Why unresolved: While the paper suggests using importance weighting and uniform treatment distributions, it does not provide a comprehensive solution for handling distribution shifts, especially in cases where the shift is significant or the treatment assignment mechanism is unknown.
- What evidence would resolve it: A robust framework for training deep latent variable models that can handle distribution shifts, along with empirical results showing improved performance in scenarios with varying treatment assignment mechanisms.

## Limitations
- The effectiveness of active methods is limited by ethical and practical constraints, such as the inability to randomize treatment in certain settings
- Passive methods rely on strong assumptions (exchangeability, positivity, no unmeasured confounding) that may not hold in practice
- Deep learning-based methods face challenges in generalization and handling distribution shifts, particularly when the treatment assignment mechanism changes

## Confidence
- Claim: Active methods like RCTs provide better identifiability of causal effects than passive methods
  - Confidence: Medium
- Claim: Deep learning-based methods can learn compact representations of confounders from high-dimensional data
  - Confidence: Medium
- Claim: Combining active and passive methods can mitigate biases from non-randomized data
  - Confidence: Low

## Next Checks
1. Implement a simple RCT simulation and verify that the ATE estimate recovers the true effect when exchangeability holds, and deviates when it is violated.
2. Apply matching methods to a benchmark dataset (e.g., IHDP) and assess the sensitivity of ATE estimates to the choice of matching algorithm and distance metric.
3. Train a CEVAE model on a high-dimensional dataset and evaluate its ability to estimate ITEs compared to traditional methods like BART or causal forests.