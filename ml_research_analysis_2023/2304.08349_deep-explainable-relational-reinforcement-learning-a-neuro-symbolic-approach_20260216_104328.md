---
ver: rpa2
title: 'Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach'
arxiv_id: '2304.08349'
source_url: https://arxiv.org/abs/2304.08349
tags:
- learning
- rule
- rules
- derrl
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DERRL, a neuro-symbolic framework for learning
  interpretable policies in deep reinforcement learning. The key idea is to use neural
  networks to generate logical rules in First-Order Logic (FOL) that explain how actions
  are selected.
---

# Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach

## Quick Facts
- arXiv ID: 2304.08349
- Source URL: https://arxiv.org/abs/2304.08349
- Authors: 
- Reference count: 40
- Key outcome: DERRL learns interpretable FOL policies that generalize across environmental configurations, outperforming NLRL in efficiency and accuracy

## Executive Summary
DERRL introduces a neuro-symbolic framework that combines deep learning with First-Order Logic to generate interpretable policies in reinforcement learning. The system uses neural networks to produce logical rules that explain action selection decisions, enabling policies that generalize to different environmental configurations. Through experiments across multiple domains including Countdown Game, Blocks World, Gridworld, and Traffic, DERRL demonstrates superior computational efficiency, policy accuracy, and semantic constraint enforcement compared to existing baselines.

## Method Summary
DERRL implements a neuro-symbolic approach where a neural network generates logical rules in First-Order Logic to explain action selection. The framework maps each action to a rule vector and weight vector, using fuzzy conjunction operators (Lukasiewicz t-norm) to compute action probabilities based on how well the current state satisfies each rule. The system is trained end-to-end using the REINFORCE algorithm with semantic loss constraints to prevent redundant rules. This approach reduces learnable parameters from O(m choose 2) to O(2m), achieving approximately 10x computational efficiency improvement over NLRL.

## Key Results
- DERRL achieves higher average rewards across multiple tasks compared to NLRL, GCN, MLP, and Random baselines
- Computational efficiency improves by a factor of approximately 10 compared to NLRL due to atom-level weight learning
- Learned policies successfully generalize to different configurations and contexts, demonstrating environmental modification adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DERRL learns interpretable policies by generating First-Order Logic (FOL) rules that explain action selection decisions
- Mechanism: Neural network maps actions to rule vectors and weight vectors, using fuzzy conjunction to compute action probabilities
- Core assumption: Interpretable rules are sufficient to represent good policies
- Evidence anchors: Abstract confirms FOL rule generation for action explanation; section 4.2 details rule vector probability mapping
- Break condition: Insufficient hypothesis space or overly complex rules diminish interpretability

### Mechanism 2
- Claim: DERRL achieves generalization through variable-based rules applicable to different configurations
- Mechanism: Rules use variables instead of constants, with semantic loss penalizing redundancy for compactness
- Core assumption: Learned rules capture essential relational structure for transferability
- Evidence anchors: Abstract confirms generalization to environmental modifications; section 4.3 describes semantic refinement with supervised loss
- Break condition: Insufficient background knowledge leads to overfitting to training configurations

### Mechanism 3
- Claim: DERRL improves computational efficiency by learning atom weights instead of complete rule weights
- Mechanism: Reduces parameters from O(m choose 2) to O(2m) by assigning weights to individual atoms
- Core assumption: Optimal policies can be represented through atom combination selection
- Evidence anchors: Section 6.3 compares NLRL's rule weights to DERRL's atom weights, showing 10x efficiency gain
- Break condition: Complex atom interactions require complete rule learning, sacrificing efficiency for expressiveness

## Foundational Learning

- Concept: First-Order Logic (FOL) rules and predicates
  - Why needed here: DERRL represents policies as FOL rules, essential for interpreting learned policies
  - Quick check question: What is the difference between an extensional predicate and an intensional (target) predicate in DERRL?

- Concept: Relational Markov Decision Processes (RMDPs)
  - Why needed here: DERRL operates on RMDPs with relational state representations
  - Quick check question: How does the state encoding process work in DERRL for relational states?

- Concept: Fuzzy logic and t-norms
  - Why needed here: DERRL uses fuzzy conjunction operators for differentiable inference
  - Quick check question: What properties must a t-norm satisfy for fuzzy conjunction in DERRL?

## Architecture Onboarding

- Component map: State Encoder -> Rule Generator -> Inference Engine -> Semantic Loss Module -> RL Trainer
- Critical path: 1) Encode state to vector, 2) Generate rule/weight vectors per action, 3) Compute action probabilities via fuzzy logic, 4) Select action and execute, 5) Update parameters via REINFORCE
- Design tradeoffs: Rule expressiveness vs. computational efficiency; number of rules vs. policy clarity; semantic constraint strictness vs. learning flexibility
- Failure signatures: Overly complex/redundant rules, poor generalization, slow convergence, inability to handle certain relational structures
- First 3 experiments: 1) Implement rule generation for Countdown Game and verify interpretability, 2) Test generalization from small to large configurations, 3) Compare computational efficiency with NLRL on scalable domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DERRL scale to more complex domains with larger numbers of objects and relations?
- Basis in paper: Inferred - paper mentions efficiency gains but doesn't explore scaling to larger environments
- Why unresolved: Only tested on relatively small, simple domains
- What evidence would resolve it: Experiments on larger domains like real-world robotics tasks or complex games

### Open Question 2
- Question: How does DERRL compare to other state-of-the-art RL methods on complex domains?
- Basis in paper: Inferred - limited comparison to NLRL and few baselines
- Why unresolved: Narrow comparison scope
- What evidence would resolve it: Direct comparison with other state-of-the-art RL methods on complex domains

### Open Question 3
- Question: How can DERRL be extended to handle complex reasoning tasks like planning under uncertainty?
- Basis in paper: Inferred - focuses on on-policy, model-free settings
- Why unresolved: Only addresses specific reasoning task type
- What evidence would resolve it: Experiments on planning and decision-making under uncertainty tasks

## Limitations
- Computational efficiency comparison lacks detailed benchmarking methodology
- No comprehensive ablation studies to isolate individual component impacts
- Generalization claims primarily evaluated on simple domains, raising scalability questions

## Confidence
- High Confidence: Core mechanism of neural networks generating FOL rules is well-established
- Medium Confidence: Efficiency improvements demonstrated but lack methodological transparency
- Low Confidence: Generalization claims supported by limited experimental evidence

## Next Checks
1. Implement ablation studies removing semantic loss and fuzzy inference to quantify individual contributions
2. Test DERRL on more complex domains with larger state spaces and longer time horizons
3. Conduct detailed computational profiling to verify the claimed 10x efficiency improvement across different problem sizes