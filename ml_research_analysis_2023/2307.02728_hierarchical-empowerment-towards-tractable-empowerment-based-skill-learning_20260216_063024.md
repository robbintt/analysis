---
ver: rpa2
title: 'Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill Learning'
arxiv_id: '2307.02728'
source_url: https://arxiv.org/abs/2307.02728
tags:
- goal
- space
- level
- goal-conditioned
- empowerment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Hierarchical Empowerment, a framework that
  uses reinforcement learning to compute the long-term empowerment of states. A key
  contribution is Goal-Conditioned Empowerment, a new variational lower bound on mutual
  information that learns a space of goal states using an automated curriculum of
  goal-conditioned RL problems.
---

# Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill Learning

## Quick Facts
- arXiv ID: 2307.02728
- Source URL: https://arxiv.org/abs/2307.02728
- Reference count: 32
- Key outcome: Hierarchical Empowerment framework computes long-term empowerment using a hierarchical architecture with Goal-Conditioned Empowerment, outperforming DIAYN on robotic navigation tasks.

## Executive Summary
This paper introduces Hierarchical Empowerment, a framework for computing long-term empowerment through a hierarchical architecture that integrates Goal-Conditioned Empowerment at multiple levels. The key insight is that empowerment can be scaled to exponentially longer time horizons by decomposing the problem into nested goal-conditioned reinforcement learning tasks. The framework learns a space of goal states using an automated curriculum, allowing it to compute empowerment over both short and long horizons in a tractable manner.

## Method Summary
The framework consists of two main components: Goal-Conditioned Empowerment and a hierarchical architecture. Goal-Conditioned Empowerment uses a variational lower bound on mutual information to learn a space of goal states through an automated curriculum of goal-conditioned RL problems. The hierarchical architecture integrates multiple levels of goal-conditioned policies, each with its own learned goal space, allowing empowerment to be computed at exponentially longer time scales. The method uses Soft Actor-Critic to train policies and requires access to transition dynamics models for efficient parallel training.

## Key Results
- Goal-Conditioned Empowerment outperforms DIAYN in learning diverse skills that cover the reachable state space
- The hierarchical architecture is necessary to compute empowerment at longer time horizons
- Agents with more levels of hierarchy achieve better performance on downstream tasks
- The learned goal spaces effectively capture the reachable state space in robotic navigation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Goal-Conditioned Empowerment explicitly encourages skills to specialize and target specific states by using a fixed variance Gaussian distribution as the variational distribution, combined with a learned goal space.
- Mechanism: The reward function in Goal-Conditioned Empowerment encourages skills to differentiate and achieve distinct states, even when there is significant overlap among the skills in the initial stages of training.
- Core assumption: The fixed variance Gaussian variational distribution provides a stable baseline for comparing the effectiveness of different skills in achieving distinct states.
- Evidence anchors:
  - [abstract] "Goal-Conditioned Empowerment, a new variational lower bound on mutual information that learns a space of goal states using an automated curriculum of goal-conditioned RL problems."
  - [section] "This reward function does not encourage skills to differentiate and target distinct regions of the state space (i.e., increase mutual information) when the skills are currently overlapping."
  - [corpus] Weak: Limited direct evidence of the fixed variance Gaussian distribution's effectiveness in encouraging skill specialization. Further investigation needed.
- Break condition: If the learned goal space becomes too large or too small relative to the reachable state space, the objective may not effectively encourage skill specialization.

### Mechanism 2
- Claim: Hierarchical Empowerment scales Goal-Conditioned Empowerment to longer time horizons by integrating multiple levels of goal-conditioned policies, each with its own learned goal space.
- Mechanism: The hierarchical architecture allows each level to compute empowerment at an exponentially increasing time scale, while each policy only needs to learn a short sequence of decisions, making optimization with RL more tractable.
- Core assumption: Each level's goal-conditioned policy can effectively learn to achieve goals within the goal space of the level below, and the learned goal spaces can be composed to cover a larger area of the state space.
- Evidence anchors:
  - [abstract] "The second contribution is a hierarchical architecture for computing empowerment over exponentially longer time scales."
  - [section] "With this nested structure inspired by GCHRL, the time horizon of the computed empowerment can grow exponentially with k, and no goal-conditioned policy is required to learn a long sequence of actions, making optimizing Goal-Conditioned Empowerment with RL more tractable."
  - [corpus] Weak: Limited evidence of the hierarchical architecture's effectiveness in scaling to longer time horizons. Further investigation needed.
- Break condition: If the learned goal spaces at each level do not compose well or if the optimization of the goal-conditioned policies becomes too difficult, the hierarchical architecture may not effectively scale to longer time horizons.

### Mechanism 3
- Claim: The reparameterization trick allows for efficient optimization of the goal space policy by transforming the original expectation into an expectation with respect to exogenous noise.
- Mechanism: By reparameterizing the goal space distribution as a function of exogenous noise and the current state, the optimization problem becomes a maximum entropy bandit problem, where the goal space is rewarded for being large and containing achievable goals.
- Core assumption: The reparameterization trick can be effectively applied to the goal space distribution, and the resulting maximum entropy bandit problem can be efficiently optimized.
- Evidence anchors:
  - [section] "Applying the reparameterization trick to the expectation term in Equation 6 and inserting the fixed variance Gaussian variational distribution used by [Choi et al. 2021], the variational lower bound objective becomes..."
  - [corpus] Weak: Limited evidence of the reparameterization trick's effectiveness in optimizing the goal space policy. Further investigation needed.
- Break condition: If the reparameterization trick does not lead to a well-behaved optimization problem or if the maximum entropy bandit problem is too difficult to solve, the goal space policy may not be effectively optimized.

## Foundational Learning

- Concept: Mutual Information and Empowerment
  - Why needed here: Empowerment is defined as the maximum mutual information between skills and states, and understanding mutual information is crucial for grasping the motivation behind Goal-Conditioned Empowerment and Hierarchical Empowerment.
  - Quick check question: What is the relationship between mutual information and empowerment, and why is empowerment an appealing objective for learning large collections of distinct skills?

- Concept: Goal-Conditioned Reinforcement Learning
  - Why needed here: Goal-Conditioned Empowerment extends the idea of goal-conditioned RL by learning the distribution of goal states, and understanding goal-conditioned RL is essential for understanding the proposed framework.
  - Quick check question: How does goal-conditioned RL differ from standard RL, and what are the key challenges in optimizing goal-conditioned RL objectives?

- Concept: Hierarchical Reinforcement Learning
  - Why needed here: Hierarchical Empowerment builds upon the idea of hierarchical RL by integrating multiple levels of goal-conditioned policies, and understanding hierarchical RL is crucial for understanding the proposed framework's scalability to longer time horizons.
  - Quick check question: What are the key benefits and challenges of using hierarchical RL to learn temporally extended policies, and how does Hierarchical Empowerment address these challenges?

## Architecture Onboarding

- Component map:
  - Goal-Conditioned Actor-Critic -> learns a goal-conditioned policy to achieve goals within and nearby the current goal space
  - Goal Space Actor-Critic -> learns a goal space policy to find the largest space of achievable goals
  - Hierarchical Architecture -> integrates multiple levels of goal-conditioned and goal space policies, with each level's action space set to the learned goal space of the level below
  - Transition Model -> used to simulate temporally extended subgoal actions and run episodes in parallel for efficient training

- Critical path:
  1. Initialize the goal-conditioned and goal space actor-critics for each level.
  2. For each level, update the goal-conditioned actor-critic to improve its ability to achieve goals within and nearby the current goal space.
  3. For each level, update the goal space actor-critic to expand the goal space to include more achievable goals.
  4. Repeat steps 2-3 until convergence or a stopping criterion is met.

- Design tradeoffs:
  - The hierarchical architecture trades off the complexity of learning long-horizon policies for the tractability of learning short-horizon policies at each level.
  - The use of a fixed variance Gaussian variational distribution provides stability but may limit the flexibility of the learned goal space.
  - The requirement for a transition model enables efficient training but limits the applicability of the framework to domains with known dynamics.

- Failure signatures:
  - If the learned goal spaces do not compose well or if the optimization of the goal-conditioned policies becomes too difficult, the hierarchical architecture may fail to scale to longer time horizons.
  - If the goal space policy fails to expand the goal space to include achievable goals, the learned skills may be too limited in scope.
  - If the transition model is inaccurate or if the training episodes are not sufficiently diverse, the learned policies may not generalize well to new states or goals.

- First 3 experiments:
  1. Implement Goal-Conditioned Empowerment on a simple navigation task with a small state space to verify that it can effectively learn a diverse set of skills that cover the reachable state space.
  2. Extend the previous experiment to a larger state space and compare the performance of Goal-Conditioned Empowerment with a baseline method that does not learn the goal space.
  3. Implement Hierarchical Empowerment on a multi-level navigation task and compare its performance with a flat implementation of Goal-Conditioned Empowerment on the same task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the hierarchical architecture scale to domains with more than 4 levels of hierarchy? What is the practical limit of the number of levels?
- Basis in paper: [explicit] The paper compares agents with 1, 2, 3, and 4 levels, but does not test if more levels would continue to improve performance.
- Why unresolved: The paper only tests up to 4 levels, so the scalability of the approach is unknown.
- What evidence would resolve it: Experiments testing agents with 5, 6, or more levels of hierarchy in various domains to determine if performance continues to improve or if there is a point of diminishing returns.

### Open Question 2
- Question: How does the performance of Hierarchical Empowerment compare to other hierarchical reinforcement learning methods like HIRO or FeUdal Networks in similar domains?
- Basis in paper: [inferred] The paper introduces Hierarchical Empowerment as a new hierarchical approach, but does not compare it to other hierarchical RL methods.
- Why unresolved: The paper focuses on comparing Hierarchical Empowerment to non-hierarchical methods like DIAYN, so its performance relative to other hierarchical methods is unknown.
- What evidence would resolve it: Experiments comparing Hierarchical Empowerment to other hierarchical RL methods like HIRO or FeUdal Networks in the same or similar domains.

### Open Question 3
- Question: Can the goal space distribution p(z|s0) be learned to be non-uniform, potentially improving performance in certain domains?
- Basis in paper: [explicit] The paper assumes p(z|s0) is a uniform distribution, but mentions that it could be a location-scale distribution.
- Why unresolved: The paper only tests the uniform distribution, so the impact of learning a non-uniform goal space distribution is unknown.
- What evidence would resolve it: Experiments testing Hierarchical Empowerment with different distributions for p(z|s0), such as Gaussian or learned distributions, to see if they improve performance in certain domains.

## Limitations
- The framework requires access to transition dynamics models, limiting its applicability to domains with known dynamics
- The effectiveness of the fixed variance Gaussian variational distribution in encouraging skill specialization lacks strong empirical validation
- The hierarchical architecture's ability to scale to very long horizons (more than 4 levels) remains untested

## Confidence
- Goal-Conditioned Empowerment's ability to learn diverse skills: Medium
- Hierarchical architecture's scalability to long horizons: Low
- Effectiveness of reparameterization trick in practice: Low

## Next Checks
1. Conduct ablation studies to quantify the impact of the fixed variance Gaussian variational distribution on skill differentiation in Goal-Conditioned Empowerment.
2. Design experiments that specifically test the hierarchical architecture's ability to compose learned goal spaces effectively across multiple levels.
3. Implement a controlled experiment comparing the reparameterization-based optimization of goal spaces against alternative approaches to isolate its contribution to performance.