---
ver: rpa2
title: Psychometric Predictive Power of Large Language Models
arxiv_id: '2311.07484'
source_url: https://arxiv.org/abs/2311.07484
tags:
- llama-2
- sentence
- human
- please
- falcon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how well instruction-tuned large language
  models (IT-LLMs) simulate human reading behavior compared to base LLMs. Results
  show that IT-LLMs exhibit worse psychometric predictive power (PPP) than base LLMs
  with equivalent perplexities, indicating that instruction tuning does not necessarily
  align LLMs with human language processing.
---

# Psychometric Predictive Power of Large Language Models

## Quick Facts
- **arXiv ID:** 2311.07484
- **Source URL:** https://arxiv.org/abs/2311.07484
- **Reference count:** 37
- **Primary result:** Instruction-tuned large language models (IT-LLMs) exhibit worse psychometric predictive power (PPP) for human reading behavior than base LLMs with equivalent perplexities.

## Executive Summary
This study investigates how well instruction-tuned large language models (IT-LLMs) simulate human reading behavior compared to base LLMs. Results show that IT-LLMs exhibit worse psychometric predictive power (PPP) than base LLMs with equivalent perplexities, indicating that instruction tuning does not necessarily align LLMs with human language processing. Further experiments with prompting and metalinguistic methods reveal that while certain prompts improve PPP, they still underperform compared to base LLMs. Direct probability measurements from base LLMs remain the most effective method for simulating human reading behavior, highlighting the limited impact of instruction tuning and prompting in cognitive modeling.

## Method Summary
The study computes surprisal, Shannon entropy, and Rényi entropy (α=0.5) for words in context using various base and instruction-tuned LLMs. Reading time data from the Dundee Corpus and Natural Stories Corpus are used to train regression models predicting reading times from these information-theoretic measures. PPP is calculated as the increase in log-likelihood due to the surprisal/entropy factor. The study compares PPP across different models, prompts, and corpora, examining how instruction tuning, prompting strategies, and metalinguistic approaches affect the ability to simulate human reading behavior.

## Key Results
- Instruction-tuned LLMs consistently show worse PPP than base LLMs with equivalent perplexities
- Prompts encouraging simple grammar and vocabulary improve PPP for IT-LLMs but remain below base LLM performance
- Metalinguistic prompting yields near-zero correlations with reading times, while direct probability measurements show moderate correlations (0.16-0.19)
- There is an inverse scaling relationship between perplexity and PPP for instruction-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Base LLMs retain statistical properties from natural corpora that align with human reading behavior, while instruction-tuned LLMs lose this alignment.
- Mechanism: Instruction tuning modifies the underlying probability distributions of LLMs to favor human-preferred responses, but this modification disrupts the statistical regularities that correlate with human reading patterns.
- Core assumption: Human reading behavior is primarily driven by next-word predictability based on natural language statistics, not by instruction-following capability.
- Evidence anchors:
  - [abstract] "instruction-tuned large language models (LLMs) yield worse psychometric predictive power (PPP) for human reading behavior than base LLMs with equivalent perplexities"
  - [section] "instruction tuning, which helps LLMs provide human-preferred responses, does not always make them human-like from the computational psycholinguistics perspective"
  - [corpus] Limited evidence; no direct corpus-based analysis of how instruction tuning alters statistical properties

### Mechanism 2
- Claim: Prompting can improve the psychometric predictive power of instruction-tuned LLMs by guiding them toward simpler linguistic structures.
- Mechanism: Certain prompts that encourage simpler grammar and vocabulary lead instruction-tuned LLMs to generate text that better matches human expectations during reading.
- Core assumption: Human reading behavior is biased toward simpler linguistic structures than what LLMs naturally produce.
- Evidence anchors:
  - [abstract] "prompts reflecting a particular linguistic hypothesis lead LLMs to exhibit better PPP but are still worse than base LLMs"
  - [section] "instructions to use simple grammar/vocabulary work better than those to use complex grammar/vocabulary"
  - [corpus] Limited evidence; no direct corpus-based analysis of how different prompts affect the statistical properties of generated text

### Mechanism 3
- Claim: Metalinguistic prompting, where models are asked to directly estimate cognitive load, is less effective than direct probability measurements for simulating human reading behavior.
- Mechanism: Models are not well-calibrated to introspect about their own probability distributions when asked metalinguistically, leading to poor estimates of cognitive load.
- Core assumption: Models can estimate their own probability distributions accurately when asked directly, but struggle to translate this into metalinguistic estimates of cognitive load.
- Evidence anchors:
  - [abstract] "metalinguistic prompting to be inferior to direct probability measurements in PPP"
  - [section] "we find metalinguistic prompting to be inferior to direct probability measurements in PPP"
  - [corpus] Limited evidence; no direct corpus-based analysis of the relationship between metalinguistic estimates and actual probability distributions

## Foundational Learning

- **Concept:** Psychometric Predictive Power (PPP)
  - Why needed here: PPP is the key metric used to evaluate how well language models simulate human reading behavior. Understanding PPP is essential for interpreting the results of the experiments.
  - Quick check question: What does a high PPP score indicate about a language model's ability to simulate human reading behavior?

- **Concept:** Surprisal and Entropy
  - Why needed here: Surprisal and entropy are the information-theoretic measures used to quantify the predictability of words in context, which are then compared to human reading times.
  - Quick check question: How does surprisal relate to the predictability of a word in context?

- **Concept:** Instruction Tuning
  - Why needed here: Instruction tuning is the process of fine-tuning language models to follow human instructions, which is the focus of the study's investigation into its effects on simulating human reading behavior.
  - Quick check question: What is the primary goal of instruction tuning in language models?

## Architecture Onboarding

- **Component map:** Dundee Corpus -> Base LLMs (GPT-2, GPT-3, LLaMA-2, Falcon, OPT) -> Surprisal computation -> Regression model -> PPP calculation; Instruction-tuned LLMs (GPT-3.5, LLaMA-2, Falcon) -> Similar pipeline

- **Critical path:**
  1. Compute surprisal/entropy values for words in context using different language models
  2. Train regression models to predict reading times from surprisal/entropy values and other factors
  3. Calculate PPP as the increase in log-likelihood due to the surprisal/entropy factor
  4. Compare PPP values across different models, prompts, and corpora

- **Design tradeoffs:**
  - Using instruction-tuned LLMs vs. base LLMs: Instruction-tuned LLMs may be better at following instructions but worse at simulating human reading behavior
  - Using different prompts: Some prompts may improve PPP but others may not
  - Using metalinguistic prompting vs. direct probability measurements: Direct probability measurements are more effective at simulating human reading behavior

- **Failure signatures:**
  - Low PPP scores: The language model is not well-aligned with human reading behavior
  - No improvement in PPP with prompting: The prompts are not effective at guiding the model toward human-like behavior
  - Low correlations between metalinguistic estimates and reading times: The model is not well-calibrated to introspect about its own probability distributions

- **First 3 experiments:**
  1. Compute surprisal values for words in the Dundee Corpus using a base LLM and an instruction-tuned LLM with equivalent perplexities. Train regression models to predict reading times and calculate PPP for each model.
  2. Use the instruction-tuned LLM with different prompts (e.g., simple grammar, complex grammar) to compute surprisal values. Compare PPP values across prompts.
  3. Use the instruction-tuned LLM with metalinguistic prompts to estimate cognitive load. Compare correlations between these estimates and reading times to correlations between direct surprisal values and reading times.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does instruction tuning degrade psychometric predictive power compared to base models?
- Basis in paper: [explicit] The paper demonstrates that IT-LLMs consistently show worse PPP than base LLMs with equivalent perplexities, despite their goal of human alignment.
- Why unresolved: The paper suggests two hypotheses (corruption of natural language statistics vs. misaligned objectives) but does not experimentally verify which is correct.
- What evidence would resolve it: Controlled ablation studies comparing different instruction-tuning datasets and objectives, or analysis of the language model's internal representations before and after instruction tuning.

### Open Question 2
- Question: What specific aspects of human reading behavior are better captured by base LLMs' direct probability measurements compared to metalinguistic prompting?
- Basis in paper: [explicit] The paper shows that metalinguistic prompting yields near-zero correlations with reading times, while direct probability measurements show moderate correlations (0.16-0.19).
- Why unresolved: The paper doesn't investigate which linguistic features or cognitive processes are better captured by each method.
- What evidence would resolve it: Detailed analysis comparing which word types (e.g., syntactic vs. semantic features) are better predicted by each method, or systematic testing across different linguistic phenomena.

### Open Question 3
- Question: How do different prompt formulations affect the linguistic complexity of generated sentences and their alignment with human reading behavior?
- Basis in paper: [explicit] The paper shows that prompts requesting simple vocabulary/grammar improve PPP compared to complex requests, but doesn't explore the full space of prompt variations.
- Why unresolved: The study only tested a limited set of prompt types and didn't analyze the relationship between generated sentence properties and PPP scores.
- What evidence would resolve it: Systematic exploration of prompt variations measuring both generated sentence complexity metrics and resulting PPP scores, potentially revealing optimal prompt formulations.

## Limitations

- The study does not provide direct evidence about which specific aspects of the training data distribution are altered by instruction tuning
- The linguistic prompts used are limited in scope and may not represent the full space of potentially effective strategies
- Achieving true equivalence in perplexities across diverse model architectures and sizes presents practical challenges

## Confidence

- **High Confidence:** The core finding that instruction-tuned LLMs exhibit worse PPP than base LLMs with equivalent perplexities is well-supported by the data and consistent across multiple model families and corpora.
- **Medium Confidence:** The interpretation that instruction tuning disrupts natural statistical properties that align with human reading behavior is plausible but requires additional corpus-level analysis to confirm the mechanism.
- **Medium Confidence:** The conclusion that direct probability measurements outperform metalinguistic prompting is supported, though the space of possible prompting strategies remains largely unexplored.

## Next Checks

1. Conduct corpus-level analysis comparing the statistical properties (n-gram distributions, entropy measures) of text generated by base vs. instruction-tuned models to identify specific distributional differences that correlate with PPP degradation.

2. Systematically explore a broader space of linguistic prompts beyond simple grammar/vocabulary complexity, including prompts that explicitly reference human cognitive processes or reading patterns, to determine if more sophisticated prompting can close the PPP gap.

3. Investigate whether instruction-tuning on psycholinguistics-specific datasets or with explicit PPP objectives could preserve both instruction-following capability and alignment with human reading behavior, potentially through targeted fine-tuning approaches.