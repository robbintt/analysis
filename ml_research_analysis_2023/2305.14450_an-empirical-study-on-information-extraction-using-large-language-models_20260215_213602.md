---
ver: rpa2
title: An Empirical Study on Information Extraction using Large Language Models
arxiv_id: '2305.14450'
source_url: https://arxiv.org/abs/2305.14450
tags:
- soft
- chatgpt
- types
- hard
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models like ChatGPT show promise for information
  extraction but currently lag behind specialized methods. This study benchmarks ChatGPT
  across 14 IE sub-tasks, finding a large performance gap versus state-of-the-art.
---

# An Empirical Study on Information Extraction using Large Language Models

## Quick Facts
- arXiv ID: 2305.14450
- Source URL: https://arxiv.org/abs/2305.14450
- Reference count: 40
- Primary result: ChatGPT shows promise for IE but lags behind specialized methods across 14 sub-tasks and 17 datasets

## Executive Summary
This empirical study benchmarks ChatGPT across 14 information extraction sub-tasks spanning 17 datasets, comparing zero-shot, few-shot, and chain-of-thought prompting strategies. The authors find a substantial performance gap versus state-of-the-art specialized methods, with ChatGPT's best performance still trailing by significant margins. To address ChatGPT's tendency to generate longer spans than annotations, they propose a soft-matching evaluation strategy that accounts for semantic equivalence. The study also reveals robustness issues with irrelevant context and long-tail target types, while error analysis shows unannotated spans are the dominant error category.

## Method Summary
The study evaluates ChatGPT on 17 datasets covering NER, RE, EE, and ABSA tasks using OpenAI's API. For each dataset, prompts were constructed for zero-shot, few-shot ICL, and few-shot COT settings, with 5 variants per sample. Responses were evaluated using both hard matching (exact span offset matching) and soft matching (similarity threshold-based matching). The evaluation measured F1 scores across tasks and analyzed robustness to irrelevant context, performance on head vs. tail types, and categorized errors into seven types. Standard deviations were computed across prompt variants.

## Key Results
- ChatGPT's performance lags significantly behind state-of-the-art specialized IE methods across all 14 sub-tasks
- Soft-matching evaluation strategy shows consistent performance gains over hard matching by accounting for ChatGPT's tendency to generate longer spans
- Performance drops up to 48% when irrelevant context is added, demonstrating high sensitivity to input quality
- Long-tail target types show significantly worse performance (up to 75.9% of head type performance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's human-like response style leads to performance overestimation under strict span matching
- Mechanism: ChatGPT tends to generate longer spans that include qualifiers (e.g., "The University of Michigan" vs. "University of Michigan"). Strict evaluation only matches exact offsets, missing valid matches
- Core assumption: Longer spans are semantically equivalent to annotated spans if they contain the same core entity
- Evidence anchors:
  - [section]: "ChatGPT tends to identify longer spans than the annotated ones, to get closer to humans... The annotated spans usually do not contain qualifiers such as quantifiers, articles, adjectives, time, place, etc. While the spans predicted by ChatGPT usually contain these qualifier parts, which are also correct target information."
  - [corpus]: Weak—corpus neighbors discuss extraction tasks but not span matching issues
- Break condition: If qualifier inclusion changes the entity's semantic meaning (e.g., "old building" vs. "building")

### Mechanism 2
- Claim: Irrelevant context significantly degrades ChatGPT's IE performance
- Mechanism: ChatGPT lacks robust context filtering, so adding unrelated text before/after the target sentence confuses the model and reduces extraction accuracy
- Core assumption: ChatGPT processes input sequentially without explicit irrelevant context removal
- Evidence anchors:
  - [section]: "The performance of most sub-tasks decreases significantly, up to 48.0%, when adding irrelevant context randomly... ChatGPT is very sensitive to the irrelevant context, which can significantly degrade performance on IE tasks."
  - [corpus]: Weak—corpus neighbors discuss extraction efficiency but not context sensitivity
- Break condition: If the model learns to ignore irrelevant context via prompt engineering or fine-tuning

### Mechanism 3
- Claim: ChatGPT struggles with long-tail target types due to frequency bias
- Mechanism: ChatGPT's training data likely overrepresents common entity/relation types, leading to poor generalization on rare types
- Core assumption: Training data distribution influences zero-shot generalization on infrequent types
- Evidence anchors:
  - [section]: "The performance of tail types is significantly worse than head types, only up to 75.9% performance of head types... ChatGPT also suffers from the long-tail problem."
  - [corpus]: Weak—corpus neighbors do not explicitly discuss long-tail issues in IE tasks
- Break condition: If few-shot examples compensate for the imbalance by providing tail-type demonstrations

## Foundational Learning

- Concept: Zero-shot vs. few-shot vs. chain-of-thought prompting
  - Why needed here: The study evaluates ChatGPT across these three prompting strategies to measure performance gaps and improvement potential
  - Quick check question: What is the key difference between few-shot ICL and few-shot COT prompts in this context?
- Concept: Soft-matching evaluation strategy
  - Why needed here: Standard hard matching fails to account for ChatGPT's tendency to generate longer spans; soft matching uses similarity thresholds to allow partial matches
  - Quick check question: How does the soft-matching algorithm determine if a predicted span matches an annotated span?
- Concept: Long-tail distribution in target types
  - Why needed here: IE tasks often have skewed type frequencies; ChatGPT's performance drops on rare types, highlighting a generalization gap
  - Quick check question: What threshold (K) is used to classify head vs. tail types in the study?

## Architecture Onboarding

- Component map: Input sentences -> Prompt generator -> ChatGPT API -> Evaluation module (hard/soft matching) -> Analysis module
- Critical path: 1) Load dataset and split into sentences 2) Generate prompts for each sentence under all three settings 3) Send prompts to ChatGPT API and collect responses 4) Evaluate responses using both matching strategies 5) Analyze errors and robustness across scenarios
- Design tradeoffs: Hard matching is simple but underestimates performance due to span length differences; soft matching is more accurate but requires similarity threshold tuning and may introduce false positives; using few-shot examples improves performance but increases cost and prompt sensitivity
- Failure signatures: Invalid output (unexpected format or irrelevant content); performance drop (adding irrelevant context or encountering rare target types); error dominance (high proportion of "Unannotated spans" suggests data quality issues)
- First 3 experiments: 1) Test hard vs. soft matching on a small dataset to quantify performance gains 2) Add irrelevant context to sentences and measure performance degradation 3) Compare performance on head vs. tail types to confirm long-tail sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific evaluation metric should be used to accurately measure ChatGPT's information extraction performance, considering its tendency to generate longer spans than annotations?
- Basis in paper: [explicit] The authors propose a soft-matching strategy to account for ChatGPT's tendency to generate longer spans than annotations, and show it delivers consistent and significant performance gains
- Why unresolved: The paper only evaluates one similarity threshold (γ = 0.5) for the soft-matching strategy. It's unclear if this is the optimal threshold, or if different thresholds would be more appropriate for different tasks or datasets
- What evidence would resolve it: Systematic evaluation of the soft-matching strategy with a range of similarity thresholds (e.g. 0.3, 0.5, 0.7, 0.9) to determine the threshold that maximizes performance for each task and dataset

### Open Question 2
- Question: How can ChatGPT's sensitivity to irrelevant context be mitigated to improve its information extraction performance?
- Basis in paper: [explicit] The authors find that irrelevant context significantly degrades ChatGPT's performance on IE tasks, with performance drops up to 48%
- Why unresolved: The paper only investigates the negative impact of irrelevant context, but does not propose or evaluate methods to mitigate this issue. It's unclear how to make ChatGPT more robust to irrelevant context
- What evidence would resolve it: Development and evaluation of prompt engineering techniques or fine-tuning approaches to improve ChatGPT's ability to ignore irrelevant context and focus on the target information

### Open Question 3
- Question: What is the optimal number of demonstration examples to include in few-shot prompts to maximize ChatGPT's information extraction performance?
- Basis in paper: [inferred] The authors observe that using few-shot ICL prompts leads to significant improvements compared to zero-shot, but still lags behind SOTA results. They also note that Wadhwa et al. (2023) use more demonstration examples (around 20) than they do (5 examples) to achieve SOTA-level performance
- Why unresolved: The paper only evaluates few-shot prompts with 5 demonstration examples. It's unclear if using more examples would further improve performance, or if there is a point of diminishing returns
- What evidence would resolve it: Systematic evaluation of few-shot prompts with varying numbers of demonstration examples (e.g. 1, 5, 10, 20, 50) to determine the optimal number for each task and dataset

## Limitations
- Proprietary nature of ChatGPT prevents understanding of why certain error patterns emerge
- Evaluation relies on annotated datasets that may contain their own labeling inconsistencies
- Span-based F1 scores may not fully capture practical utility in downstream applications
- Performance comparisons to specialized methods lack direct head-to-head experimental validation

## Confidence

**High Confidence**: The observation that ChatGPT struggles with long-tail target types and shows performance degradation with irrelevant context is well-supported by quantitative results across multiple datasets. The error type analysis showing "Unannotated spans" as the dominant error category is robust across experiments.

**Medium Confidence**: The claim that soft-matching evaluation better captures ChatGPT's true performance relies on the assumption that longer spans containing qualifiers are semantically equivalent to shorter annotated spans. While supported by examples, this equivalence may not hold universally across all entity types and contexts.

**Low Confidence**: The comparison of ChatGPT's performance to specialized IE methods is limited by the lack of direct experimental comparison. The study reports performance gaps but does not conduct head-to-head evaluations under identical conditions, making quantitative comparisons less reliable.

## Next Checks

1. **Error Boundary Validation**: Conduct human evaluation of soft-matched spans to verify semantic equivalence between predicted and annotated entities, particularly for cases where qualifiers significantly alter meaning

2. **Context Sensitivity Experiment**: Systematically vary the amount and type of irrelevant context added to inputs to quantify the relationship between context length and performance degradation across different IE sub-tasks

3. **Few-Shot Optimization Study**: Experiment with different strategies for selecting few-shot demonstrations (e.g., random sampling vs. similarity-based selection) to determine whether demonstration quality or quantity drives the observed performance improvements