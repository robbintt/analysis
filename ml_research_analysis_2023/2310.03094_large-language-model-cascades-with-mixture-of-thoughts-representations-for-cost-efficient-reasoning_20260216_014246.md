---
ver: rpa2
title: Large Language Model Cascades with Mixture of Thoughts Representations for
  Cost-efficient Reasoning
arxiv_id: '2310.03094'
source_url: https://arxiv.org/abs/2310.03094
tags:
- cost
- answer
- task
- weaker
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing the cost of using
  large language models (LLMs) for reasoning tasks while maintaining performance.
  The proposed approach involves creating an LLM cascade that uses a weaker, more
  affordable LLM for simpler questions and a stronger, more expensive LLM for complex
  ones.
---

# Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning

## Quick Facts
- **arXiv ID**: 2310.03094
- **Source URL**: https://arxiv.org/abs/2310.03094
- **Reference count**: 40
- **Primary result**: LLM cascades achieve accuracy comparable to GPT-4 alone while reducing cost to approximately 40% by routing questions based on weaker LLM answer consistency.

## Executive Summary
This paper proposes a cost-efficient approach for using large language models (LLMs) for reasoning tasks through LLM cascades. The method routes simpler questions to a weaker, more affordable LLM (GPT-3.5-turbo) and complex ones to a stronger, more expensive LLM (GPT-4). The routing decision is based on "answer consistency" - measuring agreement among multiple answers sampled from the weaker LLM. The authors introduce a mixture of Chain-of-Thought and Program-of-Thought representations to generate diverse answer samples, improving the reliability of difficulty detection. Experiments on six reasoning benchmark datasets demonstrate that this approach can maintain performance comparable to using GPT-4 exclusively while requiring only 40% of the cost.

## Method Summary
The method involves creating an LLM cascade where a weaker LLM (GPT-3.5-turbo) handles simpler questions and a stronger LLM (GPT-4) handles complex ones. For each question, the weaker LLM generates K answer samples using different temperatures and prompt settings (CoT and PoT). These samples are evaluated for consistency using either vote-based or verification-based methods. If the consistency score exceeds a threshold (or passes verification), the weaker LLM's answer is accepted; otherwise, the question is routed to the stronger LLM. The approach includes a mixture of thought representations that samples from both Chain-of-Thought and Program-of-Thought prompts to generate more diverse opinions for better difficulty detection.

## Key Results
- LLM cascades achieve accuracy comparable to GPT-4 alone while reducing cost to approximately 40%
- The proposed MoT (Mixture of Thoughts) approach outperforms single-representation methods
- Verification-based decision-making is more reliable than vote-based methods for routing decisions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sampling multiple answers from the weaker LLM and measuring their consistency reveals the question's difficulty level.
- **Mechanism**: When the weaker LLM samples highly consistent answers for a given question, it reveals high confidence in solving the question. When answers are inconsistent, it indicates uncertainty or difficulty.
- **Core assumption**: The weaker LLM's consistency in multiple sampling paths correlates with the question's difficulty.
- **Evidence anchors**:
  - [abstract] "We consider the 'answer consistency' of the weaker LLM as a signal of the question difficulty"
  - [section 2.2] "When the weaker LLM samples highly consistent answers for a given question, it reveals a high 'confidence' in solving this question and its most consistent answer is likely to be correct"
- **Break condition**: If the weaker LLM has systematic biases or hallucinations that produce consistent but incorrect answers, this mechanism would fail to detect difficult questions.

### Mechanism 2
- **Claim**: Using a mixture of Chain-of-Thought and Program-of-Thought representations provides diverse "opinions" that improve difficulty detection.
- **Mechanism**: Different thought representations can bring in more diverse perspectives on the same input question, resembling how experts with diverse perspectives contribute to more effective results in collaborative work.
- **Core assumption**: CoT and PoT promptings elicit different reasoning patterns that can complement each other in detecting question difficulty.
- **Evidence anchors**:
  - [abstract] "we propose to leverage a 'mixture of thought (MoT) representations', which samples answers from both Chain-of-Thought (Wei et al., 2022) and Program-of-Thought (Chen et al., 2022; Gao et al., 2023) prompts"
- **Break condition**: If the task domain is such that one representation (CoT or PoT) is consistently superior, mixing them might dilute the signal rather than enhance it.

### Mechanism 3
- **Claim**: Verification-based decision-making using answers from different prompt settings is more reliable than vote-based methods.
- **Mechanism**: The verification method compares the most consistent answers produced by each prompt setting. Only when these answers match does the weaker LLM's answer get accepted, providing a higher confidence threshold.
- **Core assumption**: When the weaker LLM produces consistent answers across different prompt settings, it indicates true confidence rather than random agreement.
- **Evidence anchors**:
  - [abstract] "a verification-based method that checks if the majority-voted answers sampled from different prompts are consistent"
- **Break condition**: If the different prompt settings introduce systematic differences rather than true diversity, the verification might reject correct answers unnecessarily.

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) prompting
  - **Why needed here**: The method uses CoT as one of the thought representations for sampling answers
  - **Quick check question**: What is the difference between CoT and standard prompting in LLMs?

- **Concept**: Program-of-Thought (PoT) prompting
  - **Why needed here**: The method uses PoT as the second thought representation for sampling answers
  - **Quick check question**: How does PoT differ from CoT in terms of output format and execution?

- **Concept**: Answer consistency measurement
  - **Why needed here**: The core mechanism relies on measuring consistency across multiple answer samples
  - **Quick check question**: What are different ways to measure consistency between multiple generated answers?

## Architecture Onboarding

- **Component map**: Question → Weaker LLM → Answer sampler → Consistency checker → Decision maker → (Optional) Stronger LLM → Final answer

- **Critical path**: Question → Weaker LLM → Answer sampling → Consistency checking → Decision → (Optional) Stronger LLM → Final answer

- **Design tradeoffs**:
  - Higher sample size K improves consistency detection but increases cost
  - Higher temperature increases diversity but may reduce consistency
  - Vote-based methods are simpler but verification-based methods are more reliable

- **Failure signatures**:
  - High routing rate to stronger LLM indicates poor consistency detection
  - Low accuracy despite low cost indicates over-aggressive routing
  - Inconsistent performance across datasets suggests prompt sensitivity

- **First 3 experiments**:
  1. Test basic vote-based method with single CoT prompt on GSM8k dataset
  2. Compare CoT vs PoT performance on same dataset
  3. Implement MoT-1D-Vote and measure improvement over single-representation methods

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the abstract or conclusion sections.

## Limitations

- Limited evaluation scope to primarily mathematical and reasoning tasks
- Unknown exact cost configurations and token pricing ratios used in experiments
- Verification method sensitivity to exact string matching may be overly strict

## Confidence

**High confidence**: The core mechanism of using answer consistency as a proxy for question difficulty is well-supported by experimental results.

**Medium confidence**: The claim that MoT representations improve performance over single representations is supported but could be more rigorously tested.

**Low confidence**: The superiority of verification-based methods over vote-based methods is claimed but the evidence is mixed across datasets.

## Next Checks

1. **Cost sensitivity analysis**: Re-run experiments varying the relative token costs between GPT-3.5-turbo and GPT-4 across a wider range to test the robustness of the 40% cost reduction claim under different pricing scenarios.

2. **Consistency threshold calibration**: Systematically test how different consistency thresholds affect the accuracy-cost tradeoff, particularly focusing on the knee point where additional consistency requirements stop providing meaningful accuracy gains.

3. **Cross-domain generalization**: Evaluate the cascade approach on non-mathematical reasoning tasks such as code generation, commonsense reasoning, or domain-specific knowledge tasks to assess generalizability beyond the current evaluation suite.