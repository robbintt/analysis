---
ver: rpa2
title: A Positive-Unlabeled Metric Learning Framework for Document-Level Relation
  Extraction with Incomplete Labeling
arxiv_id: '2306.14806'
source_url: https://arxiv.org/abs/2306.14806
tags:
- learning
- atlop
- relation
- positive
- document-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incomplete labeling in document-level
  relation extraction (RE), where many positive entity pairs are not annotated. The
  authors propose a positive-unlabeled metric learning framework (P3M) that combines
  positive-unlabeled learning with metric learning.
---

# A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling

## Quick Facts
- arXiv ID: 2306.14806
- Source URL: https://arxiv.org/abs/2306.14806
- Reference count: 28
- Key outcome: Achieves state-of-the-art F1 scores, improving by 4-10 points on document-level RE datasets with incomplete labeling.

## Executive Summary
This paper addresses the challenge of incomplete labeling in document-level relation extraction (RE), where many positive entity pairs are not annotated. The authors propose a positive-unlabeled metric learning framework (P3M) that combines positive-unlabeled learning with metric learning. P3M uses dropout to augment positive samples and introduces a positive-none-class mixup method to improve model generalization. Experiments show P3M achieves state-of-the-art results, improving F1 scores by 4-10 points on various document-level RE datasets with incomplete labeling, and also performs well in fully labeled scenarios. The method demonstrates robustness to prior estimation bias.

## Method Summary
The P3M framework transforms document-level RE with none-class relation into a proxy-based metric learning task, setting an anchor for each positive relation and none-class relation. It uses dropout to augment positive samples and mixup to interpolate between positive entity pair embeddings and none-class relation embeddings. The framework consists of an encoding model (ATLOP) that generates entity pair embeddings, relation embeddings for each predefined relation plus a none-class relation, a metric learning loss function adapted for PU learning, dropout augmentation for positive samples, and mixup interpolation between positives and none-class relation.

## Key Results
- Achieves 4-10 point F1 score improvements on DocRED and ChemDisGene datasets
- Demonstrates robustness to prior estimation bias (πi)
- Performs well in both incomplete labeling and fully labeled scenarios
- Shows effectiveness of dropout augmentation and positive-none-class mixup

## Why This Works (Mechanism)

### Mechanism 1
The metric learning formulation effectively pulls positive entity pair embeddings toward their relation embeddings while pushing them away from the none-class relation embedding, compensating for missing negative labels in PU learning. The embedding space is structured such that relation embeddings serve as effective anchors for both positive and negative classes in the PU setting.

### Mechanism 2
Dropout augmentation of positive samples expands the distribution of labeled positive samples, mitigating bias from incomplete labeling where labeled positives are only a subset of true positives. The model's inherent dropout noise creates perturbed versions of positive samples during training, effectively augmenting the positive sample distribution.

### Mechanism 3
Positive-none-class mixup improves model generalization by interpolating between positive samples and the none-class relation embedding, providing pseudo-negative samples in the PU setting. The none-class relation embedding serves as an effective pseudo-negative sample for mixup, and the interpolation preserves meaningful information about both positive and negative classes.

## Foundational Learning

- Concept: Positive-Unlabeled (PU) Learning
  - Why needed here: Document-level RE datasets have incomplete labeling where many positive entity pairs are not annotated, creating a PU learning scenario with labeled positives and unlabeled data containing both positives and negatives.
  - Quick check question: What is the key difference between PU learning and semi-supervised learning when dealing with document-level relation extraction?

- Concept: Metric Learning and Proxy-Based Losses
  - Why needed here: Traditional classification methods struggle with PU learning due to missing negative labels; metric learning provides an alternative framework where distances between embeddings can be optimized without explicit negative labels.
  - Quick check question: How does proxy-based metric learning differ from pair-based metric learning in handling large-scale relation extraction?

- Concept: Data Augmentation and Mixup
  - Why needed here: Limited labeled positive samples and the PU setting require techniques to expand the positive sample distribution and improve generalization without introducing noise from unlabeled positives.
  - Quick check question: Why is using the none-class relation embedding as a pseudo-negative sample in mixup preferable to sampling from unlabeled data in PU learning?

## Architecture Onboarding

- Component map: Entity pair -> ATLOP encoding -> metric learning loss (with PU adaptation) -> dropout augmentation -> mixup -> final loss optimization -> model update
- Critical path: Entity pair → ATLOP encoding → metric learning loss (with PU adaptation) → dropout augmentation → mixup → final loss optimization → model update
- Design tradeoffs: The framework balances between expanding positive sample distribution (dropout augmentation) and improving generalization (mixup) while maintaining the PU learning structure; using dropout is computationally efficient but may introduce noise, while mixup with none-class relation avoids false negatives but requires careful interpolation.
- Failure signatures: Performance degradation when dropout rate is too high (excessive noise), when mixup strength is too strong (over-smoothing), when prior estimates are severely incorrect, or when relation embeddings fail to capture semantic distinctions.
- First 3 experiments:
  1. Baseline comparison: Run PM (positive-unlabeled metric learning) without augmentation or mixup to establish the core PU metric learning performance.
  2. Ablation study: Compare P2M (with dropout augmentation only) versus P3M (with both dropout and mixup) to isolate the contribution of each augmentation technique.
  3. Prior sensitivity test: Vary the prior estimation πi across different values (e.g., πlabeled,i, 2πlabeled,i, 3πlabeled,i) to validate the framework's robustness to prior estimation errors.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Reliance on accurate prior estimation (πi) for PU learning, though authors claim robustness to estimation bias
- Experimental validation primarily focuses on document-level RE datasets with limited testing across diverse domains
- Computational overhead of the complete P3M framework versus simpler baselines is not extensively discussed

## Confidence

- **High confidence**: The core metric learning formulation for PU learning - supported by clear mathematical formulation and experimental results.
- **Medium confidence**: The effectiveness of dropout augmentation for expanding positive distributions - while the approach is described clearly, its specific benefits in PU contexts require further validation.
- **Medium confidence**: The positive-none-class mixup approach - the conceptual framework is sound, but the specific implementation details and hyperparameters significantly impact performance.

## Next Checks

1. **Prior Sensitivity Analysis**: Systematically vary the prior estimation πi across a wider range (e.g., 0.5πlabeled,i to 3πlabeled,i) and measure performance degradation to quantify the framework's actual robustness to prior estimation errors.

2. **Cross-Domain Generalization**: Test P3M on non-document-level RE datasets or completely different PU learning tasks (e.g., image classification with incomplete labels) to validate whether the dropout augmentation and mixup strategies transfer effectively.

3. **Ablation Study with Alternative Mixup Strategies**: Replace the none-class relation embedding mixup with alternative approaches (e.g., mixup with labeled negative samples where available, or different interpolation strategies) to determine if the current mixup design is optimal or simply effective.