---
ver: rpa2
title: 'GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
  Correction with Supervised Fine-Tuning'
arxiv_id: '2307.13923'
source_url: https://arxiv.org/abs/2307.13923
tags:
- data
- grammatical
- chinese
- errors
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GrammarGPT explores open-source LLMs for native Chinese grammatical
  error correction using a hybrid dataset combining ChatGPT-generated and human-annotated
  data. The method uses heuristic prompts to generate ungrammatical sentences for
  errors with clues, manual annotation for errors without clues, and error-invariant
  data augmentation through named entity substitution.
---

# GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning

## Quick Facts
- **arXiv ID:** 2307.13923
- **Source URL:** https://arxiv.org/abs/2307.13923
- **Reference count:** 30
- **Primary result:** 32.56 and 35.84 F0.5 scores on word-level and char-level metrics respectively, using 1200x less data than SOTA

## Executive Summary
GrammarGPT explores the potential of open-source LLMs for native Chinese grammatical error correction using supervised fine-tuning with minimal data. The method combines ChatGPT-generated and human-annotated data through a hybrid approach, employs error-invariant augmentation via named entity substitution, and applies instruction tuning to fine-tune open-source LLMs (Phoenix) on approximately 1k parallel data samples. Despite using only 1200x less data than state-of-the-art systems, GrammarGPT achieves competitive performance and ranks 3rd in the NLPCC2023 SharedTask1 competition, demonstrating the viability of open-source LLMs for this task.

## Method Summary
The method involves three key components: hybrid dataset construction, error-invariant data augmentation, and instruction tuning. For data collection, ChatGPT generates ungrammatical sentences for errors with explicit linguistic clues (e.g., redundant components), while humans annotate more subtle errors without clear indicators. Named entity substitution is applied to create error-invariant training examples by replacing named entities while preserving grammatical error structure. Finally, instruction tuning converts parallel data into instruction format (task description, input, output) to fine-tune open-source LLMs (Phoenix) on approximately 1k samples using AdamW optimizer with learning rate 2e-5 for 3 epochs.

## Key Results
- Achieved 32.56 F0.5 score on word-level and 35.84 F0.5 score on char-level metrics
- Outperformed SOTA systems despite using 1200x less data (1k vs 1.2M samples)
- Ranked 3rd in NLPCC2023 SharedTask1 competition

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid dataset construction using both ChatGPT-generated and human-annotated data improves performance by covering complementary error types.
- **Mechanism:** ChatGPT generates ungrammatical sentences for errors with explicit linguistic clues, while humans annotate subtle errors without clear indicators.
- **Core assumption:** Errors with linguistic clues can be reliably generated by prompting ChatGPT, and these generated errors complement human-annotated subtle errors.
- **Evidence anchors:**
  - [abstract] "For grammatical errors with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical sentences by providing those clues."
  - [section 3.1] "We can construct the ungrammatical sentences by inserting these cues into grammatical sentences."
  - [corpus] Weak - only 5 related papers, no direct citation evidence for hybrid dataset effectiveness.
- **Break condition:** If ChatGPT-generated errors do not accurately reflect native speaker patterns, or if human annotation quality is inconsistent.

### Mechanism 2
- **Claim:** Error-invariant data augmentation through named entity substitution enhances model robustness to native Chinese grammatical errors.
- **Mechanism:** By replacing named entities in parallel data with similar ones while preserving the grammatical error structure, the model learns to focus on grammatical patterns rather than specific vocabulary.
- **Core assumption:** Grammatical errors in native Chinese are typically independent of named entity positions, so substituting entities preserves the error's nature.
- **Evidence anchors:**
  - [abstract] "we employed an error-invariant augmentation method to enhance the ability of the model to correct native Chinese grammatical errors."
  - [section 3.2] "We adopt a strategy of substituting the named entities in the parallel data with similar ones."
  - [corpus] Weak - no direct evidence from related papers about named entity substitution for CGEC.
- **Break condition:** If grammatical errors in Chinese are contextually tied to named entities, augmentation could distort the error patterns.

### Mechanism 3
- **Claim:** Instruction tuning with carefully structured prompts enables effective fine-tuning of open-source LLMs for grammatical error correction.
- **Mechanism:** Converting parallel data into instruction format (task description, input, output) allows the LLM to learn the correction task explicitly.
- **Core assumption:** Open-source LLMs can effectively learn grammatical error correction patterns when provided with instruction-formatted data, even with limited training samples.
- **Evidence anchors:**
  - [abstract] "We ultimately constructed about 1k parallel data and utilized these data to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese University of Hong Kong, Shenzhen) with instruction tuning."
  - [section 3.3] "Instruction tuning[21,16] has emerged as the mainstream approach for fine-tuning LLMs by providing explicit instructions to enhance model comprehension."
  - [section 4.4] "our GrammarGPT exhibited substantial improvement with only about 1k data samples for fine-tuning."
  - [corpus] Moderate - related work on instruction tuning for LLMs exists but not specifically for CGEC.
- **Break condition:** If the instruction format doesn't adequately capture the complexity of grammatical error correction, or if the LLM's pre-training doesn't transfer well to this specific task.

## Foundational Learning

- **Concept:** Grammatical Error Correction (GEC) paradigms
  - **Why needed here:** Understanding the difference between Seq2edit and Seq2seq approaches helps contextualize why GrammarGPT uses the Seq2seq paradigm with instruction tuning.
  - **Quick check question:** What are the key differences between Seq2edit and Seq2seq approaches in GEC, and why might Seq2seq be more suitable for LLM-based methods?

- **Concept:** Named Entity Recognition and Substitution
  - **Why needed here:** The error-invariant augmentation method relies on identifying and substituting named entities, so understanding NER is crucial for implementing this technique.
  - **Quick check question:** How do you identify named entities in Chinese text, and what are the challenges in finding "similar" named entities for substitution?

- **Concept:** Instruction Tuning Methodology
  - **Why needed here:** The entire fine-tuning approach depends on converting data to instruction format, requiring understanding of how instruction tuning works and its effects on model behavior.
  - **Quick check question:** What are the key components of an effective instruction for fine-tuning, and how does the task suffix influence the model's behavior?

## Architecture Onboarding

- **Component map:** Data Collection → Data Augmentation → Instruction Formatter → LLM Fine-tuning → Evaluation
- **Critical path:** Data Collection → Data Augmentation → Instruction Formatting → LLM Fine-tuning → Evaluation
- **Design tradeoffs:**
  - Using 1k training samples vs. 1.2M in SOTA baseline (20x more parameters but 1200x less data)
  - ChatGPT generation vs. manual annotation (cost vs. quality)
  - Named entity substitution (robustness vs. potential context loss)
- **Failure signatures:**
  - Poor performance on errors without clues despite good performance on clue-based errors
  - Degradation when evaluating on data with different named entity distributions
  - Overfitting to specific instruction formats or error types
- **First 3 experiments:**
  1. Train on only ChatGPT-generated data (no human annotation) to measure the contribution of each data source
  2. Train without error-invariant augmentation to isolate its impact on performance
  3. Vary the amount of training data (e.g., 100, 500, 1000 samples) to understand data efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal ratio of ChatGPT-generated to human-annotated data for fine-tuning open-source LLMs on native Chinese grammatical error correction?
- **Basis in paper:** [explicit] The authors mention using a hybrid dataset of ChatGPT-generated and human-annotated data, with roughly 65% ChatGPT-generated and 35% human-annotated.
- **Why unresolved:** The paper does not explore the impact of varying the ratio of ChatGPT-generated to human-annotated data on the model's performance.
- **What evidence would resolve it:** Experiments varying the ratio of ChatGPT-generated to human-annotated data and measuring the impact on the model's performance would provide insights into the optimal ratio.

### Open Question 2
- **Question:** How does the performance of open-source LLMs for native Chinese grammatical error correction scale with the size of the fine-tuning dataset?
- **Basis in paper:** [inferred] The authors use approximately 1k parallel data samples for fine-tuning and achieve significant performance improvements compared to the SOTA baseline. However, the paper does not explore the impact of scaling the dataset size on performance.
- **Why unresolved:** The paper does not provide experiments or analysis on how the performance of open-source LLMs scales with the size of the fine-tuning dataset.
- **What evidence would resolve it:** Experiments varying the size of the fine-tuning dataset and measuring the impact on the model's performance would provide insights into the scaling behavior of open-source LLMs for native Chinese grammatical error correction.

### Open Question 3
- **Question:** How does the performance of open-source LLMs for native Chinese grammatical error correction compare to closed-source LLMs like ChatGPT?
- **Basis in paper:** [explicit] The authors mention that closed-source LLMs like ChatGPT have demonstrated excellent capabilities in grammatical error correction, but the potential of open-source LLMs remains unexplored.
- **Why unresolved:** The paper does not provide a direct comparison between the performance of open-source LLMs and closed-source LLMs like ChatGPT on native Chinese grammatical error correction.
- **What evidence would resolve it:** Experiments comparing the performance of open-source LLMs and closed-source LLMs like ChatGPT on native Chinese grammatical error correction would provide insights into the relative strengths and weaknesses of each approach.

## Limitations

- **Unknown prompt templates:** The specific prompt templates used to guide ChatGPT for generating ungrammatical sentences are not provided, which is critical for reproducibility.
- **Limited evaluation scope:** Performance is only evaluated on the NLPCC2023 SharedTask1 validation set (500 samples) without comprehensive testing on other benchmarks.
- **Unclear manual annotation process:** The exact sources and collection process for ungrammatical sentences from public websites requiring manual correction are not detailed.

## Confidence

- **High Confidence:** The core finding that instruction tuning with ~1k samples can achieve competitive results (32.56/35.84 F0.5) compared to systems using 1.2M samples is well-supported by the reported metrics and competition ranking.
- **Medium Confidence:** The effectiveness of the hybrid dataset construction (ChatGPT + human annotation) is plausible based on the results, but the lack of ablation studies comparing each data source independently limits confidence in the exact contribution of each component.
- **Low Confidence:** The generalizability of the approach beyond the specific dataset and error types covered in the competition is uncertain without testing on diverse Chinese grammatical error correction benchmarks.

## Next Checks

1. **Ablation Study on Data Sources:** Train separate models using only ChatGPT-generated data, only human-annotated data, and only augmented data to quantify the individual contribution of each component to the final performance.

2. **Cross-Dataset Evaluation:** Evaluate the trained model on at least two additional Chinese grammatical error correction datasets (e.g., CGED, etc.) to assess generalization beyond the NLPCC2023 SharedTask1 validation set.

3. **Prompt Template Analysis:** Conduct controlled experiments varying the prompt templates used for ChatGPT generation to identify which prompt characteristics most strongly influence the quality and diversity of generated grammatical errors.