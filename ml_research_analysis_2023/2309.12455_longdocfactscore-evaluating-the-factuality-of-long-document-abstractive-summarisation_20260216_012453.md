---
ver: rpa2
title: 'LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation'
arxiv_id: '2309.12455'
source_url: https://arxiv.org/abs/2309.12455
tags:
- document
- data
- long
- metrics
- longdocfactscore
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating factual consistency
  in long document abstractive summarization, where existing metrics struggle due
  to token limits. The authors propose LongDocFACTScore, a framework that extends
  existing metrics by comparing each summary sentence with the most relevant sections
  of the source document using sentence embeddings and cosine similarity.
---

# LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation

## Quick Facts
- arXiv ID: 2309.12455
- Source URL: https://arxiv.org/abs/2309.12455
- Authors: 
- Reference count: 17
- Key outcome: LongDocFACTScore improves correlation with human factuality judgments on long document summarization by comparing each summary sentence with relevant document sections rather than truncating documents.

## Executive Summary
This paper addresses the challenge of evaluating factual consistency in long document abstractive summarization, where existing metrics struggle due to token limits. The authors propose LongDocFACTScore, a framework that extends existing metrics by comparing each summary sentence with the most relevant sections of the source document using sentence embeddings and cosine similarity. This approach allows efficient processing of long documents without truncation. The framework outperforms state-of-the-art metrics in correlating with human annotations of factuality on long document datasets, achieving Kendall's tau correlations of 0.61 on both PubMed and ArXiv data. It also performs comparably to existing metrics on short document datasets.

## Method Summary
LongDocFACTScore is a framework that extends existing factual consistency metrics to handle long documents without truncation. It works by splitting both the summary and source document into sentences, generating sentence embeddings, and using cosine similarity to identify the K most relevant document sentences for each summary sentence. The framework then calculates the base metric score (e.g., BARTScore) between each summary sentence and its corresponding relevant document sections, averaging the results. This approach allows metrics to process long documents efficiently while maintaining factual accuracy by considering the full document context through similarity-based section selection.

## Key Results
- LongDocFACTScore achieves Kendall's tau correlations of 0.61 with human factuality judgments on both PubMed and ArXiv long document datasets
- The framework outperforms existing state-of-the-art metrics (ROUGE, BERTScore, FactCC, QuestEval) on long document factuality evaluation
- LongDocFACTScore performs comparably to baseline metrics on short document datasets, showing it doesn't degrade performance where truncation isn't needed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LongDocFACTScore improves correlation with human factuality judgments by comparing each summary sentence to the most relevant document sections rather than truncating the document.
- Mechanism: The framework uses sentence embeddings to identify the K most similar source document sentences for each summary sentence, then calculates the metric score between these relevant sections and the summary sentence. This captures factual relationships that would be lost through truncation.
- Core assumption: The most similar source document sentences contain the relevant factual information needed to verify the summary sentence.
- Evidence anchors:
  - [abstract] "This framework allows efficient processing of long documents without truncation"
  - [section] "LongDocFACTScore considers sections of text from the full length of the source document in its calculation (using sentence embeddings to select the most relevant from across the document) whereas other metrics truncate the source document"
- Break condition: If the sentence embedding similarity does not correlate with factual relevance, or if relevant facts are distributed across distant document sections that aren't captured by the K nearest neighbors.

### Mechanism 2
- Claim: LongDocFACTScore achieves computational efficiency by calculating metric scores on short text segments rather than entire truncated documents.
- Mechanism: Instead of processing the full truncated document through a transformer, the framework processes one summary sentence against a few short document snippets at a time, reducing the maximum sequence length per metric calculation.
- Core assumption: Factuality can be assessed sentence-by-sentence rather than requiring full-document context.
- Evidence anchors:
  - [abstract] "The framework allows metrics to be efficiently extended to any length document"
  - [section] "The second significant difference is that LongDocFACTScore calculates a metric score on short sections of text at one time, comparing one sentence in the predicted summary to a short section of the source document, rather than long, truncated sections"
- Break condition: If factuality assessment requires full-document context to detect contradictions or if sentence-level evaluation misses document-wide factual inconsistencies.

### Mechanism 3
- Claim: LongDocFACTScore outperforms existing metrics because it captures information that truncation-based metrics miss.
- Mechanism: By considering the full document through similarity-based section selection, the framework can verify facts from any part of the document, including those in the latter portions that would be truncated in other methods.
- Core assumption: Facts from any document section can be relevant to any summary sentence, and truncation disproportionately affects longer documents.
- Evidence anchors:
  - [abstract] "This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of factuality when used to evaluate long document summarisation data sets"
  - [section] "if a generated summary includes content from the latter part of a long document, it will be ignored, which is a problem when assessing factual consistency of long document summarisation"
- Break condition: If truncation doesn't significantly impact factuality assessment, or if the similarity-based section selection misses relevant document sections.

## Foundational Learning

- Sentence embeddings and cosine similarity
  - Why needed here: To efficiently identify the most relevant source document sections for each summary sentence without processing the entire document through the transformer model
  - Quick check question: How would you explain the relationship between sentence embedding similarity and factual relevance?

- Transformer model token limits
  - Why needed here: Understanding why existing metrics must truncate documents and how LongDocFACTScore avoids this limitation
  - Quick check question: What happens to a transformer's attention mechanism when input exceeds maximum token limits?

- Kendall's tau correlation coefficient
  - Why needed here: The primary metric used to evaluate how well automatic metrics correlate with human factuality judgments
  - Quick check question: Why might Kendall's tau be preferred over Spearman correlation for this evaluation with smaller sample sizes?

## Architecture Onboarding

- Component map: Sentence splitter (nltk) → Sentence embedding generator (sentence-transformers) → Cosine similarity calculator → K nearest neighbor selector → Text snippet constructor → Metric calculator (e.g., BARTScore) → Score aggregator

- Critical path: Summary sentence → Embedding generation → Similarity calculation → Top K selection → Text snippet creation → Metric scoring → Average aggregation

- Design tradeoffs:
  - Sentence-level vs document-level evaluation: Faster but may miss cross-sentence factual dependencies
  - K nearest neighbors parameter: Higher K improves coverage but reduces efficiency
  - Embedding model choice: BERT-based embeddings balance speed and quality for this task

- Failure signatures:
  - Low correlation with human judgments despite high similarity scores: Sentence embeddings may not capture factual relevance
  - Slow performance on very long documents: K parameter may need adjustment or parallel processing
  - Inconsistent results across different document types: Embedding model may not generalize well to all domains

- First 3 experiments:
  1. Verify that sentence embedding similarity correlates with human-identified relevant document sections
  2. Test different K values (1, 3, 5, all sentences) to find optimal balance between accuracy and efficiency
  3. Compare correlation with human judgments when using different base metrics (BARTScore, FactCC, QuestEval) extended with LongDocFACTScore framework

## Open Questions the Paper Calls Out
The paper mentions in its conclusion that it hopes to apply LongDocFACTScore to extend other automatic metrics for measuring factual consistency to long document settings and evaluate their performance. The authors also note that their human evaluation used domain-specific annotators for scientific documents, acknowledging this limitation for generalizability.

## Limitations
- Evaluation relies on a relatively small human-annotated dataset (270 summaries), which may not capture full diversity of long document scenarios
- Specific focus on medical and scientific papers may not generalize to other long document types
- Assumes sentence embedding similarity reliably identifies factually relevant document sections, which may not always hold true

## Confidence
- **High confidence** in the framework's ability to handle long documents without truncation and achieve improved correlation with human judgments on tested datasets
- **Medium confidence** in generalizability of results to other domains beyond medical and scientific papers
- **Low confidence** in specific claims about computational efficiency without direct timing benchmarks against baseline methods

## Next Checks
1. Test LongDocFACTScore on long document summarization datasets from different domains (legal, news, technical documentation) to assess generalization beyond medical and scientific papers
2. Systematically vary the K parameter (1, 2, 3, 5, all sentences) across different document types and metric combinations to identify optimal settings and parameter stability
3. Measure and compare processing times for LongDocFACTScore versus traditional truncated approaches on documents of varying lengths to verify efficiency claims with concrete metrics