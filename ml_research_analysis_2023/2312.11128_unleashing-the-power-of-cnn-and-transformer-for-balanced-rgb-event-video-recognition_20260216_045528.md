---
ver: rpa2
title: Unleashing the Power of CNN and Transformer for Balanced RGB-Event Video Recognition
arxiv_id: '2312.11128'
source_url: https://arxiv.org/abs/2312.11128
tags:
- recognition
- event
- ieee
- features
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TSCFormer, a novel RGB-Event based pattern
  recognition framework that achieves a better tradeoff between model parameters and
  recognition performance. The core idea is to encode RGB and Event data using temporal
  shift CNN blocks and boost the interactions of multimodal features with global BridgeFormer
  layers.
---

# Unleashing the Power of CNN and Transformer for Balanced RGB-Event Video Recognition

## Quick Facts
- arXiv ID: 2312.11128
- Source URL: https://arxiv.org/abs/2312.11128
- Reference count: 40
- Primary result: 57.70% top-1 accuracy on PokerEvent, 53.04% on HARDVS

## Executive Summary
This paper introduces TSCFormer, a novel framework for RGB-Event video recognition that achieves state-of-the-art performance while maintaining computational efficiency. The approach combines temporal shift CNN blocks for efficient temporal modeling with a global BridgeFormer layer for multimodal feature fusion. By leveraging the complementary strengths of CNNs for local feature extraction and Transformers for global reasoning, TSCFormer effectively processes both RGB frames and event streams to achieve superior recognition accuracy on two large-scale benchmark datasets.

## Method Summary
TSCFormer integrates temporal shift operations within a ResNet50 backbone to efficiently capture temporal dependencies, followed by a BridgeFormer module that performs cross-attention between randomly initialized global tokens and local CNN features. The enhanced global features are then projected back into RGB and Event CNN blocks through F2E and F2V modules, creating an interactive fusion pipeline. The model is trained end-to-end using SGD with MultiScaleCrop preprocessing for 30 epochs, achieving strong performance on both PokerEvent and HARDVS datasets.

## Key Results
- Achieves 57.70% top-1 accuracy on PokerEvent benchmark
- Achieves 53.04% top-1 accuracy on HARDVS benchmark
- Surpasses previous state-of-the-art methods while maintaining lightweight architecture
- Demonstrates effective RGB-Event fusion with cross-attention mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BridgeFormer enables effective fusion of global and local features from RGB and Event modalities
- Mechanism: Uses cross-attention to combine randomly initialized global tokens with local CNN features, then applies self-attention and FFN layers
- Core assumption: Random global tokens can capture long-range dependencies when fused with local features through attention mechanisms
- Evidence anchors: abstract mentions "global BridgeFormer layers", section describes random token initialization and cross-attention fusion
- Break condition: If random tokens fail to capture meaningful global patterns or cross-attention cannot effectively combine features

### Mechanism 2
- Claim: TSM enables efficient 3D-like temporal modeling while maintaining 2D CNN complexity
- Mechanism: Shifts channels along temporal dimension to facilitate information exchange between neighboring frames without 3D convolutions
- Core assumption: Temporal information can be captured through channel shifting without explicit 3D convolutions
- Evidence anchors: section describes temporal shift operation for information exchange, abstract confirms 2D complexity maintenance
- Break condition: If temporal relationships are too complex for channel shifting or temporal resolution is too low

### Mechanism 3
- Claim: Interactive fusion using F2E and F2V modules optimizes information flow between local and global representations
- Mechanism: Enhanced global features are projected and fused back into RGB and Event CNN blocks through fully connected layers
- Core assumption: Bidirectional information flow between local CNN features and global Transformer representations improves feature quality
- Evidence anchors: abstract describes interactive manner of fusion, section explains concatenation after feature transformation
- Break condition: If feature transformation degrades information or bidirectional flow creates instability

## Foundational Learning

- Concept: Cross-attention mechanisms
  - Why needed here: To fuse heterogeneous RGB and Event features by allowing one modality to attend to the other
  - Quick check question: How does cross-attention differ from self-attention in terms of query, key, and value relationships?

- Concept: Temporal shift operations
  - Why needed here: To capture temporal dependencies between video frames without computational cost of 3D convolutions
  - Quick check question: What is the computational complexity difference between TSM and standard 3D convolutions?

- Concept: Multi-head attention
  - Why needed here: To learn different types of global relationships in feature space through parallel attention heads
  - Quick check question: How does number of attention heads affect model's ability to capture diverse global patterns?

## Architecture Onboarding

- Component map: Input → Stem → Temporal Shift CNN → BridgeFormer (with global tokens) → F2E/F2V fusion → CNN blocks → Classification head
- Critical path: RGB/Event frames → temporal shift encoding → BridgeFormer fusion → interactive feature projection → classification
- Design tradeoffs: Balance between local CNN feature extraction and global Transformer reasoning, model complexity vs. accuracy
- Failure signatures: Poor performance on temporal patterns (TSM issues), failure to fuse modalities (BridgeFormer issues), overfitting with too many parameters
- First 3 experiments:
  1. Test TSM effectiveness by comparing with 3D CNN baseline on temporal modeling
  2. Validate BridgeFormer fusion by comparing with simple concatenation baseline
  3. Test interactive fusion by comparing with one-way fusion from local to global only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the temporal shift operation's performance vary with different shift amounts and input frame rates?
- Basis in paper: [explicit] The paper mentions using 8 divisions for the temporal shift operation but doesn't explore the impact of different shift amounts or input frame rates.
- Why unresolved: The paper doesn't provide an ablation study on the effect of varying the temporal shift amount or input frame rates on recognition performance.
- What evidence would resolve it: An ablation study varying the temporal shift amount and input frame rates, comparing the resulting recognition accuracy on benchmark datasets.

### Open Question 2
- Question: How does the proposed TSCFormer framework perform on other multi-modal datasets beyond RGB-Event data?
- Basis in paper: [inferred] The paper focuses on RGB-Event data but doesn't explore the framework's applicability to other multi-modal datasets.
- Why unresolved: The paper doesn't provide experiments on other multi-modal datasets to validate the framework's generalizability.
- What evidence would resolve it: Experiments on other multi-modal datasets, such as RGB-Depth or RGB-Thermal, comparing the TSCFormer's performance to other state-of-the-art methods.

### Open Question 3
- Question: How does the BridgeFormer module's performance change with different attention mechanisms or fusion strategies?
- Basis in paper: [explicit] The paper uses cross-attention and self-attention mechanisms in the BridgeFormer module but doesn't explore other attention mechanisms or fusion strategies.
- Why unresolved: The paper doesn't provide an ablation study on the effect of using different attention mechanisms or fusion strategies in the BridgeFormer module.
- What evidence would resolve it: An ablation study comparing the performance of different attention mechanisms (e.g., additive attention, dot-product attention) and fusion strategies (e.g., addition, gating) in the BridgeFormer module.

## Limitations

- BridgeFormer fusion mechanism relies on random initialization of global tokens without theoretical justification
- Interactive fusion through F2E and F2V modules adds complexity without clear evidence of benefits over unidirectional approaches
- Temporal shift operation may not capture complex temporal dependencies in event streams requiring more sophisticated modeling

## Confidence

**High Confidence:** The temporal shift CNN implementation and basic TSM mechanism, as this builds directly on well-established video recognition techniques with proven effectiveness.

**Medium Confidence:** The overall architecture design combining CNN and Transformer for multimodal fusion, as this follows established multimodal learning principles, though specific implementation details of BridgeFormer remain partially unclear.

**Low Confidence:** The effectiveness of random global token initialization and the specific interactive fusion strategy using F2E/F2V modules, as these components lack comprehensive ablation studies and theoretical grounding in the provided evidence.

## Next Checks

1. **Ablation study of global token initialization:** Compare BridgeFormer performance using random initialization versus learned initialization of global tokens to isolate the impact of initialization strategy on multimodal fusion quality.

2. **Unidirectional vs bidirectional fusion comparison:** Implement and evaluate a variant where enhanced features flow only from global to local (not vice versa) to determine if interactive fusion provides measurable benefits over simpler unidirectional approaches.

3. **Event stream modeling comparison:** Compare the temporal shift-based event modeling against a baseline using 3D convolutions or more sophisticated temporal modeling to assess whether the computational efficiency gain comes at the cost of temporal pattern recognition capability.