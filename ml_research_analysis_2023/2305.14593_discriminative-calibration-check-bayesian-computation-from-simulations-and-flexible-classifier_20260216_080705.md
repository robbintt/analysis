---
ver: rpa2
title: 'Discriminative calibration: Check Bayesian computation from simulations and
  flexible classifier'
arxiv_id: '2305.14593'
source_url: https://arxiv.org/abs/2305.14593
tags:
- divergence
- classifier
- test
- classification
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces discriminative calibration, a method to assess
  the accuracy of Bayesian computation using flexible classifiers rather than rank-based
  simulation-based calibration (SBC). The approach learns test statistics from data
  by training a classifier to distinguish between true and approximate posterior samples,
  yielding a computable divergence metric and a valid hypothesis test p-value.
---

# Discriminative calibration: Check Bayesian computation from simulations and flexible classifier

## Quick Facts
- arXiv ID: 2305.14593
- Source URL: https://arxiv.org/abs/2305.14593
- Reference count: 40
- One-line primary result: Classification-based calibration detects miscalibration more powerfully than rank-based SBC while providing interpretable divergence estimates

## Executive Summary
This paper introduces discriminative calibration, a method to assess the accuracy of Bayesian computation using flexible classifiers rather than rank-based simulation-based calibration (SBC). The approach learns test statistics from data by training a classifier to distinguish between true and approximate posterior samples, yielding a computable divergence metric and a valid hypothesis test p-value. Experiments show the classifier-based approach typically has higher statistical power than SBC rank tests and provides interpretable divergence estimates.

## Method Summary
Discriminative calibration trains a classifier to distinguish between true posterior samples (from exact inference) and approximate posterior samples (from the inference method being evaluated). The method generates a simulation table with draws from the prior and posterior, creates classification examples through a label mapping, and uses permutation testing to compute valid p-values. The expected log predictive density of the classifier provides an interpretable divergence measure between the true and approximate posteriors. The approach addresses limitations of SBC including ad-hoc test statistics, difficulty examining interactions, multiple testing challenges, and lack of interpretable divergence measures.

## Key Results
- Classifier-based approach typically achieves higher statistical power than SBC rank tests for detecting miscalibration
- The method provides interpretable divergence estimates computed from classification accuracy
- Works across various inference methods including MCMC, variational inference, and likelihood-free inference
- Can incorporate statistically-informed features like ranks, likelihood, and log densities as classifier inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classification-based divergence estimation provides a principled lower bound to the true posterior divergence
- Mechanism: The method trains a classifier to distinguish between true posterior samples and approximate posterior samples. When the classifier achieves high accuracy, it indicates a significant divergence between the distributions. The expected log predictive density (ELPD) of this classifier, adjusted by the entropy of label distribution, provides a computable divergence metric.
- Core assumption: The label mapping creates classification examples where labels are conditionally independent of features given the true posterior equality, ensuring the divergence estimate is valid
- Evidence anchors:
  - [abstract] "this measure typically has a higher statistical power than the SBC test and returns an interpretable divergence measure of miscalibration, computed from classification accuracy"
  - [section] "If the classifier c is optimal...then the bound in the above equation is tight...ELPD1 − h(w) = D1(p, q)"
  - [corpus] Weak - related papers focus on different calibration approaches but don't directly address this classifier-based divergence estimation mechanism
- Break condition: If the label mapping violates the conditional independence assumption, the divergence estimate becomes invalid

### Mechanism 2
- Claim: The classifier learns optimal test statistics from data rather than relying on pre-specified rank statistics
- Mechanism: Instead of using ad-hoc rank statistics as in traditional SBC, the classifier automatically learns which features best distinguish between true and approximate posteriors. This learned test statistic adapts to the specific characteristics of the miscalibration present in the data.
- Core assumption: The classifier family is sufficiently flexible to capture the optimal test statistics for detecting the specific type of miscalibration present
- Evidence anchors:
  - [abstract] "replace the marginal rank test with a flexible classification approach that learns test statistics from data"
  - [section] "We define a 'label mapping' Φ that maps one simulation run...into a K-class classification example-set...the prediction abilityD(p, q, Φ, c) is a lower bound to the corresponding divergence"
  - [corpus] Weak - related papers discuss different diagnostic approaches but don't demonstrate this specific adaptive test statistic learning mechanism
- Break condition: If the classifier architecture is too restrictive to capture the true divergence patterns, the learned test statistics will be suboptimal

### Mechanism 3
- Claim: Permutation testing provides exact frequentist validity regardless of classifier optimality
- Mechanism: The method uses permutation testing where labels are randomly shuffled within batches while keeping features fixed. This creates the null distribution under the hypothesis that true and approximate posteriors are identical, yielding exact p-values even with finite samples.
- Core assumption: The classification examples within each batch are exchangeable under the null hypothesis
- Evidence anchors:
  - [abstract] "we adopt the permutation test: We train the classifier c once on the training set and keep it fixed, and evaluate the validation set log predictive density"
  - [section] "Theorem 4 (Finite sample frequentist test)...the permutation test is valid as the p-value computed above is uniformly distributed on [0, 1]"
  - [corpus] Weak - related papers mention different testing approaches but don't validate this specific permutation testing mechanism for calibration
- Break condition: If batches contain information that correlates with labels beyond what the null hypothesis predicts, the permutation test validity breaks down

## Foundational Learning

- Concept: Simulation-based calibration (SBC)
  - Why needed here: The method builds upon SBC principles but replaces rank-based testing with classification-based testing
  - Quick check question: What is the key limitation of traditional SBC that this method addresses?
  - Answer: Traditional SBC uses ad-hoc rank statistics that don't diagnose joint distributions or interactions well

- Concept: Kullback-Leibler (KL) divergence and Jensen-Shannon divergence
  - Why needed here: The method estimates divergences between true and approximate posteriors using classification accuracy
  - Quick check question: How does the method relate classification accuracy to divergence between distributions?
  - Answer: The ELPD of the optimal classifier minus the label entropy equals the Jensen-Shannon divergence between the distributions

- Concept: Permutation testing and multiple hypothesis testing
  - Why needed here: The method uses permutation testing to provide valid p-values while avoiding multiple testing corrections
  - Quick check question: Why does the permutation testing approach avoid the multiple testing problem present in traditional SBC?
  - Answer: Because it tests all dimensions simultaneously rather than testing each dimension separately

## Architecture Onboarding

- Component map:
  - Simulation engine: Generates (θ, y) pairs from prior and data model
  - Inference engine: Produces approximate posterior samples q(θ|y)
  - Label mapping module: Transforms simulation results into classification examples
  - Classifier trainer: Trains neural network to distinguish true vs approximate samples
  - Permutation test module: Validates results through label shuffling
  - Divergence calculator: Computes ELPD and converts to divergence estimate

- Critical path:
  1. Generate simulation table with S draws of (θ, y) and M posterior samples each
  2. Apply label mapping to create classification examples
  3. Split into training/validation sets (batch-wise)
  4. Train classifier on training set
  5. Evaluate ELPD on validation set
  6. Perform permutation test for p-value
  7. Bootstrap for confidence intervals

- Design tradeoffs:
  - Binary vs multiclass classification: Binary is simpler but multiclass converges to true KL divergence
  - Feature engineering: Including log densities improves accuracy but requires more information
  - Batch size vs number of simulations: Larger batches improve permutation test validity but reduce flexibility
  - Classifier complexity: More complex networks can capture subtler divergences but risk overfitting

- Failure signatures:
  - Uniform p-value distribution under null indicates valid testing
  - Divergence estimates approaching zero indicate good calibration
  - Classifier predictions near 0.5 indicate distributions are similar
  - Large confidence intervals suggest insufficient simulations

- First 3 experiments:
  1. Test with exact inference (q=p) to verify p-values are uniform under null
  2. Test with biased inference (shifted mean) to verify method detects miscalibration
  3. Test with varying M (number of posterior draws) to verify multiclass convergence to KL divergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between the classifier-based calibration divergence and other divergence measures like Wasserstein distance or total variation?
- Basis in paper: [inferred] The paper establishes connections between the classification divergence and Jensen-Shannon divergence, and shows convergence to KL divergence in the limit, but doesn't explore other divergence metrics.
- Why unresolved: The paper focuses on classification accuracy as the divergence measure but doesn't compare or relate it to other common divergence metrics used in Bayesian computation.
- What evidence would resolve it: Mathematical proofs establishing bounds or relationships between the classification divergence and other divergence measures, or empirical comparisons showing how they rank different inference methods.

### Open Question 2
- Question: How does the choice of classifier architecture (e.g., MLP vs CNN vs transformer) affect the statistical power and calibration accuracy of the method?
- Basis in paper: [explicit] The paper mentions using "flexible parametric family such as a multilayer perceptron (MLP) network" but doesn't systematically compare different architectures.
- Why unresolved: The paper uses MLPs as examples but doesn't explore whether certain architectures are better suited for different types of posterior distributions or inference methods.
- What evidence would resolve it: Systematic experiments comparing different classifier architectures across various inference problems, including both synthetic and real-world examples.

### Open Question 3
- Question: What is the optimal strategy for selecting the number of posterior draws M in the multiclass classification approach?
- Basis in paper: [explicit] The paper shows that D4 converges to KL divergence as M → ∞ but doesn't provide guidance on choosing M for practical applications.
- Why unresolved: The paper demonstrates theoretical convergence but doesn't address the trade-off between computational cost and accuracy when selecting M for finite samples.
- What evidence would resolve it: Empirical studies showing the relationship between M, computational cost, and calibration accuracy across different inference methods and problem dimensions.

## Limitations
- Theoretical guarantees rely on classifier flexibility that may not be achievable with practical neural network architectures
- Permutation test validity depends critically on exchangeability assumptions within batches
- Scalability to very high-dimensional parameter spaces remains untested and potentially problematic
- Method requires exact inference for true posterior samples, limiting applicability to complex models

## Confidence
- High confidence in theoretical framework connecting classification accuracy to divergence bounds
- Medium confidence in practical performance across diverse inference methods, as experimental validation is limited to specific examples
- Low confidence in scalability to very high-dimensional parameter spaces where classifier training may become unstable

## Next Checks
1. **Null calibration verification**: Run experiments with exact inference (q=p) across multiple model families to confirm p-values follow the expected uniform distribution under the null hypothesis

2. **Dimensionality stress test**: Evaluate performance as parameter dimensionality increases, measuring both power and computational cost, to establish practical limits of the method

3. **Cross-inference method comparison**: Apply discriminative calibration to multiple inference methods (MCMC, VI, ABC) on the same models to assess consistency and identify method-specific failure modes