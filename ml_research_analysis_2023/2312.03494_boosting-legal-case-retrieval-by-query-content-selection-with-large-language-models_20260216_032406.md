---
ver: rpa2
title: Boosting legal case retrieval by query content selection with large language
  models
arxiv_id: '2312.03494'
source_url: https://arxiv.org/abs/2312.03494
tags:
- retrieval
- query
- legal
- content
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving legal case retrieval
  by leveraging the salient content in long legal case queries. The authors propose
  a method to reformulate queries using large language models (LLMs) to extract or
  summarize salient content, which is then incorporated into retrieval models.
---

# Boosting legal case retrieval by query content selection with large language models

## Quick Facts
- **arXiv ID**: 2312.03494
- **Source URL**: https://arxiv.org/abs/2312.03494
- **Reference count**: 38
- **Primary result**: LLM reformulation of legal queries with salient content significantly improves both sparse and dense retrieval model performance.

## Executive Summary
This paper addresses the challenge of improving legal case retrieval by leveraging salient content in long legal case queries. The authors propose a method to reformulate queries using large language models (LLMs) to extract or summarize salient content, which is then incorporated into retrieval models. The study first manually annotates salient content in queries from the LeCaRD dataset and analyzes how traditional sparse and dense retrieval models attend to this content. The results show that while both types of models can perceive salient content, their attention does not align well with human annotations. The authors then experiment with various query content selection methods using LLMs, such as keyword extraction, key sentence extraction, and summarization. Experimental results demonstrate that reformulating queries with LLMs significantly improves the performance of both sparse and dense retrieval models. The best performance is achieved using annotated salient content as queries, highlighting the effectiveness of properly selecting and utilizing salient content in legal case retrieval.

## Method Summary
The paper proposes a method for improving legal case retrieval by reformulating long legal queries using large language models to extract salient content. The approach involves three main steps: (1) manual annotation of salient content in queries based on criminal elements theory, (2) analysis of how traditional sparse (BM25, TF-IDF, QL) and dense (BERT-CLS, BERT-PLI) retrieval models attend to this content, and (3) reformulation of queries using LLMs (keyword extraction, key sentence extraction, summarization) and incorporation into retrieval models. The method is evaluated on the LeCaRD dataset using standard retrieval metrics (P@5, P@10, MAP, NDCG).

## Key Results
- LLM reformulations improve both sparse and dense retrieval models, with the best performance achieved using annotated salient content as queries.
- Summarized queries with appropriate information density and fluent natural language format provide optimal balance for both lexical and semantic retrieval models.
- BM25 attends to high-frequency words with high IDF values while BERT attention favors rare words common across documents, neither aligning well with human salience judgments.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating long legal queries with LLM-extracted salient content improves retrieval model performance by reducing noise and highlighting relevance signals.
- Mechanism: Legal queries contain extensive redundant text where only a small fraction is legally salient (e.g., criminal elements). LLMs extract or summarize this salient content, creating shorter, focused queries that better align with retrieval models' matching mechanisms.
- Core assumption: The compressed salient content retains sufficient information for legal relevance judgment while eliminating noise that confuses retrieval models.
- Evidence anchors:
  - [abstract]: "reformulating queries with LLMs improves the performance of both sparse and dense models in legal case retrieval"
  - [section]: "we observe that nearly all retrieval models are improved... The summarized queries stably improve all retrieval models"
  - [corpus]: Weak evidence - no direct corpus support found for LLM effectiveness in legal query reformulation

### Mechanism 2
- Claim: Different retrieval models (sparse vs dense) have distinct attention patterns to query content, with neither matching human salience judgments well.
- Mechanism: BM25 attends to high-frequency words with high IDF values, while BERT attention favors rare words common across documents. Neither aligns with lawyer-annotated salient content, creating opportunity for query reformulation.
- Core assumption: The mismatch between model attention and human salience indicates room for improvement through content selection.
- Evidence anchors:
  - [abstract]: "both types of models can perceive salient content, their attention does not align well with human annotations"
  - [section]: "BM25 scores prefer words with high TF and IDF values while BERT attention prefer words with low TF and IDF values"
  - [corpus]: Weak evidence - no corpus data showing model attention patterns in legal domain

### Mechanism 3
- Claim: Query reformulation methods (keyword extraction, key sentence extraction, summarization) have varying effectiveness based on information density and fluency.
- Mechanism: Summarized queries with appropriate information density and fluent natural language format provide optimal balance for both lexical and semantic retrieval models.
- Core assumption: There exists an optimal information density for reformulated queries that maximizes retrieval performance across different model types.
- Evidence anchors:
  - [abstract]: "The best performance is achieved using annotated salient content as queries"
  - [section]: "The summarized queries stably improve all retrieval models... the summary generated by LLMs preserve valuable information and in fluent nature language format"
  - [corpus]: Weak evidence - no corpus support for optimal information density in query reformulation

## Foundational Learning

- Concept: Legal relevance judgment based on statutory elements (subject, object, conduct, mental state)
  - Why needed here: Understanding what makes content "salient" in legal queries is crucial for effective content selection
  - Quick check question: What are the four criminal elements in Chinese legal theory that determine case relevance?
- Concept: Sparse vs dense retrieval model mechanisms
  - Why needed here: Different models attend to query content differently, affecting how reformulation impacts performance
  - Quick check question: How do BM25 and BERT attention patterns differ in their focus on query words?
- Concept: Information density and query length effects on retrieval
  - Why needed here: Finding optimal balance between query compression and retention of relevance signals
  - Quick check question: What is the average compression rate of salient content in legal queries according to the paper?

## Architecture Onboarding

- Component map: Data layer (LeCaRD dataset with annotated salient content) -> LLM layer (GPT-3.5-turbo for query reformulation) -> Retrieval layer (BM25, BERT-CLS, BERT-PLI models) -> Evaluation layer (P@5, P@10, MAP, NDCG metrics)
- Critical path: Annotate queries → Analyze model attention → Reformulate queries with LLM → Evaluate retrieval performance
- Design tradeoffs:
  - Query length vs. information retention: Shorter queries may lose context
  - Extraction vs. summarization: Keywords provide precision but lack fluency
  - Model compatibility: Different reformulation methods work better for different retrieval models
- Failure signatures:
  - Performance degradation when using reformulated queries
  - Hallucination of legal elements not present in original queries
  - Over-compression removing critical context
- First 3 experiments:
  1. Compare retrieval performance using original queries vs. annotated salient content as queries
  2. Test different LLM prompt strategies for keyword extraction on sample queries
  3. Evaluate the impact of query length reduction on precision vs. ranking metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do sparse and dense retrieval models differ in their perception of salient content in legal case queries?
- Basis in paper: [explicit] The authors analyze how BM25 (sparse) and BERT-CLS (dense) attend to salient content in queries, finding that they perceive different aspects of the content.
- Why unresolved: The paper provides initial insights but does not fully explore the underlying reasons for these differences or how they might be leveraged to improve retrieval performance.
- What evidence would resolve it: Further experiments comparing additional sparse and dense models, and analysis of their attention mechanisms on a larger dataset of annotated legal queries.

### Open Question 2
- Question: Can query reformulation methods using LLMs consistently improve dense retrieval models for legal case retrieval?
- Basis in paper: [explicit] The authors experiment with various query reformulation methods using LLMs, showing improvements for some dense models but not all.
- Why unresolved: The paper demonstrates that query reformulation can improve retrieval performance, but the results are not consistent across all models and query types. The reasons for this inconsistency are not fully explored.
- What evidence would resolve it: Further experiments testing different query reformulation methods and models on a larger, more diverse set of legal queries, and analysis of the characteristics of queries that benefit most from reformulation.

### Open Question 3
- Question: How can the effectiveness of query reformulation methods using LLMs be improved for legal case retrieval?
- Basis in paper: [explicit] The authors discuss the potential of query reformulation methods using LLMs, but note that they may not be good at extraction tasks and may not satisfy all user prompts.
- Why unresolved: The paper identifies limitations of current query reformulation methods using LLMs, but does not propose solutions or explore ways to overcome these limitations.
- What evidence would resolve it: Development and testing of improved prompt engineering techniques for LLMs, or the use of alternative methods such as fine-tuning LLMs on legal query data.

## Limitations

- The study relies heavily on manual annotation of salient content by legal experts, which may introduce subjectivity and limit generalizability across different legal systems.
- The LLM-based query reformulation uses GPT-3.5-turbo, and results may vary with different model versions or prompts.
- The experiments focus on a single Chinese legal case retrieval dataset (LeCaRD), which may not represent legal systems with different statutory structures.

## Confidence

- **High confidence**: The observation that both sparse and dense retrieval models can perceive salient content but don't align well with human annotations. This is directly supported by experimental analysis in the paper.
- **Medium confidence**: The claim that LLM reformulation improves retrieval performance across different model types. While supported by experimental results, the optimal reformulation strategy varies by retrieval model type.
- **Low confidence**: The assertion that summarized queries achieve the best performance due to "appropriate information density and fluent natural language format." The paper provides limited analysis of why summarization outperforms other methods.

## Next Checks

1. **Cross-system validation**: Test the query reformulation approach on legal datasets from different jurisdictions (e.g., common law systems) to assess generalizability of the salient content selection methodology.

2. **Hallucination audit**: Conduct a systematic evaluation of LLM-generated queries for hallucinated legal elements by comparing them against original query content and annotated salient content to quantify accuracy rates.

3. **Cost-benefit analysis**: Measure the computational overhead of LLM-based query reformulation against performance gains across different retrieval model types to determine practical scalability for production legal search systems.