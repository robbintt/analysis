---
ver: rpa2
title: The Training Process of Many Deep Networks Explores the Same Low-Dimensional
  Manifold
arxiv_id: '2305.01604'
source_url: https://arxiv.org/abs/2305.01604
tags:
- none
- adam
- simple
- erent
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training trajectories of deep neural networks
  by treating their predictions as high-dimensional probabilistic models. Using information-geometric
  techniques, the authors embed these models into lower-dimensional spaces using intensive
  principal component analysis (InPCA), which preserves pairwise distances between
  probability distributions.
---

# The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold

## Quick Facts
- **arXiv ID**: 2305.01604
- **Source URL**: https://arxiv.org/abs/2305.01604
- **Reference count**: 40
- **Primary result**: Training processes across diverse deep learning configurations explore effectively low-dimensional manifolds in prediction space, with top 3 dimensions capturing 70-90% of explained stress.

## Executive Summary
This paper reveals that despite the high dimensionality of deep neural network weight spaces, training processes explore effectively low-dimensional manifolds in the prediction space. By treating networks as probabilistic models and applying intensive principal component analysis (InPCA), the authors demonstrate that diverse architectures, optimization methods, and training configurations follow trajectories on shared manifolds. The key insight is that network architectures primarily distinguish trajectories, while other factors have minimal influence. This finding suggests deep learning optimization may have lower computational complexity than previously thought, with potential implications for improving generalization in smaller networks.

## Method Summary
The authors analyze deep neural networks by treating their predictions as high-dimensional probabilistic models. They record training trajectories across diverse configurations (architectures, optimization methods, regularization techniques, and weight initializations) on CIFAR-10 and ImageNet. Using intensive PCA (InPCA), they embed these high-dimensional models into lower-dimensional spaces while preserving pairwise distances between probability distributions via Bhattacharyya distance. The explained stress measures how well the embedding preserves these distances, revealing the effective dimensionality of the manifold. The study also introduces a "progress toward truth" metric (P*) based on geodesic distances in probability space to quantify learning dynamics.

## Key Results
- Training processes across diverse architectures and configurations explore effectively low-dimensional manifolds in prediction space
- Top three InPCA dimensions capture 70-90% of explained stress despite prediction spaces having hundreds of thousands to millions of dimensions
- Network architectures primarily distinguish training trajectories, while optimization methods, regularization schemes, and weight initializations have minimal influence
- Larger networks train along similar manifolds as smaller ones but progress faster toward the truth

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The training process explores an effectively low-dimensional manifold in the prediction space.
- **Mechanism**: InPCA preserves pairwise distances between probability distributions during training, revealing that despite high-dimensional weight space, models' predictions lie on a low-dimensional manifold.
- **Core assumption**: Training trajectories of diverse configurations explore similar regions of prediction space.
- **Evidence anchors**: The paper demonstrates this through InPCA embedding with 70-90% explained stress; weak related evidence from "An Analytical Characterization of Sloppiness in Neural Networks."
- **Break condition**: If trajectories diverge significantly or explained stress is low, the manifold hypothesis would be invalid.

### Mechanism 2
- **Claim**: Networks with different architectures follow distinguishable trajectories while other factors have minimal influence.
- **Mechanism**: InPCA embedding and hierarchical clustering reveal architecture as the primary distinguishing factor, with optimization methods, regularization, and initialization having minimal impact.
- **Core assumption**: Prediction space is rich enough to capture architectural differences, and InPCA is sensitive to these differences.
- **Evidence anchors**: The paper shows architecture distinguishes trajectories via hierarchical clustering; no direct support in related literature.
- **Break condition**: If InPCA fails to distinguish architectures or other factors significantly impact trajectories, the claim would be invalid.

### Mechanism 3
- **Claim**: Larger networks train along similar manifolds as smaller ones but progress faster toward the truth.
- **Mechanism**: InPCA embedding and progress toward truth (P*) analysis show larger networks follow similar trajectories but make more progress for the same gradient updates.
- **Core assumption**: Prediction space captures differences in progress between network sizes, and InPCA distinguishes these differences.
- **Evidence anchors**: Strong correlation (R² = 0.95 for train error, R² = 0.88 for test error) between progress and network size; no direct related support.
- **Break condition**: If InPCA fails to distinguish network sizes or progress isn't correlated with size, the claim would be invalid.

## Foundational Learning

- **Concept**: Principal Component Analysis (PCA) and its variants (e.g., InPCA)
  - **Why needed here**: To reduce dimensionality of prediction space and visualize training trajectories
  - **Quick check question**: What is the difference between standard PCA and InPCA, and why is InPCA more suitable for analyzing high-dimensional probabilistic models?

- **Concept**: Information Geometry and distances between probability distributions (e.g., Bhattacharyya distance, KL divergence)
  - **Why needed here**: To measure distances between models in prediction space and analyze manifold structure
  - **Quick check question**: How do different distance measures capture different aspects of similarity between probability distributions, and why is Bhattacharyya distance particularly suitable?

- **Concept**: Geodesics and geodesics in probability spaces
  - **Why needed here**: To measure progress toward truth (P*) and compare trajectories in prediction space
  - **Quick check question**: What is a geodesic in a probability space, and how does it relate to the shortest path between two probability distributions?

## Architecture Onboarding

- **Component map**: CIFAR-10, ImageNet datasets → Fully connected networks, ConvNets (AllCNN, ConvMixer), ResNets, ViTs → SGD, Adam optimization → Various regularization techniques → InPCA, hierarchical clustering analysis

- **Critical path**:
  1. Train models on datasets with various configurations
  2. Record training trajectories and compute predictions on train and test data
  3. Calculate pairwise distances between models using Bhattacharyya distance
  4. Perform InPCA to embed models into lower-dimensional space
  5. Analyze manifold structure and trajectories

- **Design tradeoffs**:
  - Computational complexity vs. accuracy: Using subset of samples for InPCA vs. all samples
  - Distance measure: Bhattacharyya distance vs. KL divergence, Hellinger distance
  - Dimensionality reduction: InPCA vs. t-SNE, UMAP

- **Failure signatures**:
  - Low explained stress in InPCA embedding
  - Divergence of trajectories for different configurations
  - Poor correlation between progress and error

- **First 3 experiments**:
  1. Train a small and large ResNet on CIFAR-10 and compare their training trajectories in InPCA embedding
  2. Train a fully connected network and a ConvMixer on CIFAR-10 with different optimization methods and regularization techniques, and analyze their trajectories in InPCA embedding
  3. Train a ViT on ImageNet and compare its trajectory with that of a ResNet in InPCA embedding

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the precise relationship between the low-dimensionality of the prediction space manifold and the optimization landscape of deep networks?
- **Basis in paper**: The paper demonstrates training trajectories lie on low-dimensional manifolds but doesn't explain why this occurs
- **Why unresolved**: Empirical evidence exists but theoretical explanation for how loss landscape geometry, initialization, and data structure constrain optimization is missing
- **What evidence would resolve it**: Theoretical framework explaining how Hessian spectrum or information-theoretic bounds constrain optimization to low-dimensional manifolds

### Open Question 2
- **Question**: Can the harmonic mean ensemble method be systematically applied to improve generalization across different architectures and datasets?
- **Basis in paper**: Harmonic mean ensembles achieve slightly better test error than arithmetic mean ensembles on CIFAR-10
- **Why unresolved**: Only demonstrated on single architecture (all-CNN) and dataset (CIFAR-10)
- **What evidence would resolve it**: Systematic experiments across diverse architectures, datasets, and tasks to identify when harmonic mean outperforms arithmetic mean

### Open Question 3
- **Question**: How does the low-dimensionality of the prediction space manifold affect theoretical understanding of generalization in deep learning?
- **Basis in paper**: Findings suggest optimization complexity might be lower than thought, potentially impacting generalization theory
- **Why unresolved**: Paper demonstrates empirical phenomena but doesn't explore theoretical consequences for generalization bounds
- **What evidence would resolve it**: New generalization bounds accounting for low-dimensional manifold structure and analysis of how this affects effective network capacity

## Limitations
- Analysis restricted to classification datasets (CIFAR-10, ImageNet) and final-layer predictions
- InPCA methodology requires substantial computational resources for large networks
- Study focuses on supervised learning and doesn't address unsupervised or reinforcement learning contexts
- "Progress toward truth" metric relies on geodesic distances that may not capture all learning dynamics

## Confidence
- **High confidence**: Core finding of low-dimensional manifolds (70-90% explained stress)
- **Medium confidence**: Architecture as primary distinguishing factor (sensitive to embedding choices)
- **Medium confidence**: Larger networks progress faster (depends on specific progress metrics)

## Next Checks
1. **Intermediate Layer Analysis**: Apply InPCA methodology to hidden layer activations rather than just final predictions to determine if low-dimensional manifold structure exists throughout the network

2. **Cross-Task Transferability**: Test whether networks trained on different classification tasks (e.g., CIFAR-100, Places365) exhibit similar manifold properties and whether these manifolds are task-specific or task-agnostic

3. **Architecture Ablation Study**: Systematically vary architectural components (depth, width, attention mechanisms) while holding other factors constant to quantify exactly how each contributes to trajectory differentiation in prediction space