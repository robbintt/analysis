---
ver: rpa2
title: Privacy-Preserving Parameter-Efficient Fine-Tuning for Large Language Model
  Services
arxiv_id: '2305.06212'
source_url: https://arxiv.org/abs/2305.06212
tags:
- privacy
- tuning
- prompt
- privatization
- rapt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAPT, a framework for privacy-preserving
  prompt tuning of large language models (LLMs). The core idea is to apply local differential
  privacy to user data before prompt tuning, combined with a novel privatized token
  reconstruction task to maintain performance.
---

# Privacy-Preserving Parameter-Efficient Fine-Tuning for Large Language Model Services

## Quick Facts
- arXiv ID: 2305.06212
- Source URL: https://arxiv.org/abs/2305.06212
- Reference count: 15
- This paper introduces RAPT, a framework for privacy-preserving prompt tuning of large language models (LLMs)

## Executive Summary
RAPT introduces a framework for privacy-preserving parameter-efficient fine-tuning of large language models by applying local differential privacy to user data before prompt tuning. The core innovation is a novel privatized token reconstruction task that is jointly trained with the downstream task, which significantly improves performance when training on privatized data. Experiments on three NLP tasks (SST, QQP, TP-UK) with BERTBASE and T5BASE models show that RAPT achieves competitive accuracy (76-91%) while providing strong privacy protection against embedding inversion and attribute inference attacks.

## Method Summary
RAPT applies local differential privacy to user data through a text-to-text privatization mechanism before prompt tuning. Users locally privatize their data using an embedding model with dX-privacy and L2 distance, then upload the privatized text to service providers. To address the performance degradation caused by data privatization, RAPT introduces a privatized token reconstruction task that is jointly trained with the downstream task. The framework uses plain tokens prepended to inputs as reconstruction targets, allowing the model to learn better task-dependent representations despite data perturbation. During inference, only the prompt tuning parameters are used without the reconstruction head.

## Key Results
- RAPT achieves competitive accuracy (76-91%) across three NLP tasks while providing strong privacy protection
- Privatized token reconstruction task significantly improves performance when training on privatized data
- RAPT consistently outperforms baselines including direct prompt tuning on privatized data and existing privacy-preserving fine-tuning methods
- Privacy evaluations show RAPT provides strong protection against embedding inversion and attribute inference attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local differential privacy applied through text-to-text privatization protects user data while maintaining utility
- Mechanism: Users apply text-to-text privatization locally before uploading data to service providers. This uses dX-privacy with L2 distance and noise injection to create privatized text while preserving semantic meaning
- Core assumption: Text-to-text privatization effectively prevents reconstruction of original sensitive content while maintaining sufficient semantic similarity for downstream task performance
- Evidence anchors:
  - [abstract] "RAPT adopts a local privacy approach, enabling users to privatize their data locally using a text-to-text local differential privacy mechanism"
  - [section 3.1] "For applying text-to-text privatization, we first need an embedding model to map words to vectors... M(xt) = xt + z"
  - [corpus] Weak evidence - corpus papers focus on privacy in fine-tuning but not specifically on text-to-text privatization mechanisms
- Break condition: If the embedding model mapping is not robust or if the distance function fails to adequately bound privacy leakage, the protection mechanism fails

### Mechanism 2
- Claim: Privatized token reconstruction task improves performance when training on privatized data
- Mechanism: Joint training on both downstream task and token reconstruction task allows the model to learn better task-dependent representations despite data perturbation from privatization
- Core assumption: Masked language modeling objectives help learn separable deep representations that transfer to improved downstream performance
- Evidence anchors:
  - [abstract] "Since PEFT performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task"
  - [section 3.2.2] "Recent studies on language model representations found that the masked language modeling objective is helpful in learning separable deep representations"
  - [section 4.3] "By introducing the privatized token reconstruction task, the performance of two prompt tuning methods is significantly improved across tasks and LLMs"
- Break condition: If the privatized token reconstruction becomes trivial (e.g., vocabulary too small) or the perturbation is too severe, the reconstruction objective cannot provide meaningful representation learning

### Mechanism 3
- Claim: Plain tokens added before privatization provide stable reconstruction targets
- Mechanism: Arbitrary tokens prepended to inputs serve as reconstruction targets, avoiding privacy leakage while providing consistent training signals
- Core assumption: Plain tokens can be arbitrarily chosen since they're safe to send to service providers, and the reconstruction task doesn't need to recover original sensitive content
- Evidence anchors:
  - [section 3.2.2] "We prepend a sequence of tokens, which we refer to as 'plain tokens', to each input in the training data... can be safely sent to the service provider without worrying about privacy leakage"
  - [section 4.5.2] "We found that using one plain token during privatized token reconstruction suffices to improve the performance"
  - [section 4.5.2] "The performances with different plain tokens of the same length are comparable"
- Break condition: If the plain tokens are not sufficiently diverse or if their length is too short to provide meaningful reconstruction challenges

## Foundational Learning

- Concept: Local differential privacy (LDP) and its guarantees
  - Why needed here: Understanding how local privacy differs from central privacy and why it's appropriate for LLM service scenarios
  - Quick check question: What's the key difference between local and central differential privacy, and why does RAPT use the local setting?

- Concept: Parameter-efficient fine-tuning methods (Prompt Tuning, Prefix Tuning)
  - Why needed here: RAPT builds on existing PEFT methods, so understanding their architecture and limitations is crucial
  - Quick check question: How do Prompt Tuning and Prefix Tuning differ in terms of parameters and approach?

- Concept: Masked language modeling objectives and their role in representation learning
  - Why needed here: The privatized token reconstruction task is inspired by masked LM objectives
  - Quick check question: Why do masked language modeling objectives help in learning better representations according to the paper?

## Architecture Onboarding

- Component map:
  - User side: Text-to-text privatization module (embedding model + noise injection + token replacement)
  - Service provider side: Prompt tuning framework + privatized token reconstruction head
  - Communication: Privatized text + plain tokens
  - Training loop: Joint optimization of downstream loss + reconstruction loss

- Critical path:
  1. User privatizes input text locally using embedding model and dX-privacy
  2. Plain tokens prepended to privatized text
  3. LLM processes sequence with prepended prompt
  4. Reconstruction head predicts plain tokens from early activations
  5. Downstream head predicts task labels from later activations
  6. Joint loss backpropagated to update prompt parameters

- Design tradeoffs:
  - Privacy vs utility: Stronger privacy (smaller η) reduces performance; privatized token reconstruction helps mitigate this
  - Reconstruction head complexity: Larger hidden size and vocabulary improve performance but add parameters
  - Plain token length: Longer tokens improve performance but increase computational cost

- Failure signatures:
  - Performance drops significantly with privatization: Check if reconstruction task is too easy or hard
  - Privacy attacks succeed: Verify η value and text-to-text privatization implementation
  - Training instability: Check learning rates and prompt initialization

- First 3 experiments:
  1. Implement text-to-text privatization with BERT embeddings and verify basic functionality on sample text
  2. Train PROMPT TUNING baseline without privacy to establish performance floor
  3. Add privatized token reconstruction with fixed plain tokens and measure improvement on SST-2 task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum size of LLMs for which RAPT remains effective?
- Basis in paper: [inferred] The authors state "Due to limited computational resources, we have yet to verify the effectiveness of our method on very large language models like GPT-3."
- Why unresolved: The paper only tests RAPT on BERTBASE, T5BASE, and T53B models, which are relatively small compared to state-of-the-art LLMs like GPT-3.
- What evidence would resolve it: Experiments demonstrating RAPT's performance on larger models like GPT-3, GPT-4, or other LLMs with hundreds of billions of parameters.

### Open Question 2
- Question: How does RAPT perform on natural language generation (NLG) tasks compared to natural language understanding (NLU) tasks?
- Basis in paper: [inferred] The authors note "We mainly conduct experiments on natural language understanding tasks" but do not evaluate RAPT on NLG tasks.
- Why unresolved: The paper focuses exclusively on NLU tasks (SST, QQP, TP-UK), leaving NLG performance unexplored.
- What evidence would resolve it: Experiments applying RAPT to NLG tasks such as text summarization, machine translation, or story generation, with comparison to baseline methods.

### Open Question 3
- Question: Can RAPT achieve comparable privacy guarantees against membership inference attacks as it does against embedding inversion and attribute inference attacks?
- Basis in paper: [explicit] The authors state "it remains to be explored whether RAPT can achieve comparable performance with other privacy-preserving methods in membership inference attack scenario."
- Why unresolved: The paper only evaluates RAPT against embedding inversion and attribute inference attacks, not membership inference attacks.
- What evidence would resolve it: Experiments testing RAPT's performance against membership inference attacks, comparing its effectiveness to other privacy-preserving methods like CAPE, DPNR, and fine-tuning approaches.

## Limitations

- Privacy guarantees are limited to specific attack types (embedding inversion and attribute inference), with membership inference attacks not evaluated
- Experiments focus on three specific NLP tasks with relatively small models, limiting generalizability to more complex tasks or larger models
- Implementation complexity of text-to-text privatization mechanism is significant, with key details like dX-privacy implementation not fully specified

## Confidence

- **High Confidence**: The core mechanism of using local differential privacy for data protection and the basic framework of joint training with privatized token reconstruction are well-established concepts
- **Medium Confidence**: The claim that "RAPT achieves competitive accuracy while providing strong privacy protection" is supported by experimental results but limited by the scope of evaluation
- **Low Confidence**: The claim that "RAPT is the first to study privacy-preserving PEFT from a user-centric perspective" is difficult to verify given the rapidly evolving field

## Next Checks

1. **Privacy Robustness Test**: Implement and evaluate RAPT against membership inference attacks specifically targeting the prompt tuning mechanism. This would validate whether the local differential privacy applied to input data is sufficient to protect against attacks that exploit the learned prompt parameters.

2. **Cross-Domain Generalization**: Test RAPT on a more diverse set of tasks including multi-hop reasoning, code generation, or medical text classification. This would validate whether the privatized token reconstruction mechanism generalizes beyond sentiment analysis and question-pair tasks.

3. **Implementation Fidelity Check**: Replicate the text-to-text privatization mechanism with different embedding models (e.g., RoBERTa, DeBERTa) and verify that the dX-privacy implementation matches the theoretical guarantees. Compare the performance impact of different distance metrics (L1 vs L2) and noise distributions.