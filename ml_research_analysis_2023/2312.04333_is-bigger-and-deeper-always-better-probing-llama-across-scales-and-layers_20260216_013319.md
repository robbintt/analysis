---
ver: rpa2
title: Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers
arxiv_id: '2312.04333'
source_url: https://arxiv.org/abs/2312.04333
tags:
- llama
- tasks
- layers
- reasoning
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a detailed layer-by-layer and model-scale analysis
  of LLaMA language models, using carefully designed multiple-choice probing tasks
  to evaluate intrinsic abilities in calculation, math problem solving, logical reasoning,
  and knowledge representation. The results reveal that increasing model size primarily
  improves reasoning abilities and reduces hallucinations only beyond certain size
  thresholds, while having minimal effect on computational or factual knowledge abilities.
---

# Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers

## Quick Facts
- arXiv ID: 2312.04333
- Source URL: https://arxiv.org/abs/2312.04333
- Reference count: 18
- Key outcome: Layer-wise and scale analysis reveals that model size thresholds trigger reasoning improvements while computational and factual knowledge abilities are layer-specialized, with penultimate layers often optimal for reasoning tasks.

## Executive Summary
This paper investigates whether increasing model size and depth in LLaMA language models automatically improves all capabilities through systematic layer-by-layer probing across 7B, 13B, and 70B parameter scales. The analysis reveals that model size thresholds primarily enhance reasoning abilities beyond certain parameter counts, while having minimal impact on computational and factual knowledge capabilities. Layer-wise examination shows clear functional specialization, with upper layers housing computational power and factual knowledge, while lower layers focus on abstract reasoning and multilingual processing.

## Method Summary
The study employs multiple-choice probing tasks to evaluate LLaMA models across different scales and layers, using few-shot prompting with 4-6 examples per task. Models tested include LLaMA 2-7B, 13B, and 70B, evaluated on arithmetic, math problem solving (MPS), logical reasoning (Reclor), factual knowledge (LAMA), and truthfulness (TruthfulQA) tasks. Layer-wise analysis is performed by separately evaluating each layer's output, while model-scale comparisons examine how performance changes with increasing parameters. The methodology relies on a GitHub repository containing models, datasets, and evaluation scripts.

## Key Results
- Model size increases trigger reasoning improvements only beyond specific thresholds, with minimal impact on computational or factual knowledge abilities
- Upper layers predominantly house computational power and factual knowledge, while lower layers specialize in abstract reasoning and multilingual features
- Penultimate layers often achieve optimal performance for reasoning tasks, while factual knowledge peaks at the final layer
- Performance improvements across tasks consistently emerge from mid-layer networks onward

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model size thresholds trigger emergent reasoning abilities beyond computational improvements
- Mechanism: Increasing model size creates architectural capacity that enables qualitative reasoning improvements only after surpassing specific parameter thresholds
- Core assumption: Same pre-training corpus across all model sizes means raw knowledge content remains constant
- Evidence anchors: Abstract states enlarging models enhances reasoning abilities only beyond size thresholds; Section 4 confirms increasing size doesn't significantly boost computational or fundamental abilities
- Break condition: If models are trained on different corpus sizes or qualities with scale, threshold effects may be confounded by data differences

### Mechanism 2
- Claim: Layer depth creates functional specialization where upper layers encode factual knowledge while lower layers focus on abstract reasoning
- Mechanism: Information propagation through transformer layers creates specialization where lower layers develop abstract reasoning and upper layers store factual knowledge
- Core assumption: Layer-wise specialization emerges from self-supervised training objectives rather than explicit constraints
- Evidence anchors: Abstract describes lower layers lacking substantial arithmetic knowledge but showcasing logical thinking; Section 5 states computational ability primarily exists in upper layers and models embed rich factual knowledge in top layers
- Break condition: If layer-wise specialization is reversed or absent in other architectures, claimed pattern may be LLaMA-specific

### Mechanism 3
- Claim: Penultimate layers achieve optimal reasoning performance while final layers specialize in factual knowledge storage
- Mechanism: Final layer specialization in factual knowledge may interfere with abstract reasoning representations that peak in penultimate layers
- Core assumption: Layer representations can be overwritten or modified by subsequent layers during inference
- Evidence anchors: Abstract notes optimal performance often found in penultimate rather than final layers for reasoning; Section 5 observes last layer doesn't necessarily represent best computational proficiency while LLaMA's ultimate layer harbors greatest factual knowledge
- Break condition: If layer outputs are concatenated rather than overwritten, penultimate-layer optimization may not hold

## Foundational Learning

- Concept: Transformer layer architecture and information flow
  - Why needed here: Understanding information flow through layers is critical for interpreting layer-wise specialization claims
  - Quick check question: What is the primary function of residual connections in transformer layers?

- Concept: Self-supervised pre-training objectives and their impact on learned representations
  - Why needed here: Study assumes pre-training objectives create observed layer-wise specialization patterns
  - Quick check question: How does next-token prediction objective influence what information gets stored in different layers?

- Concept: Multilingual model behavior and cross-lingual transfer
  - Why needed here: xMPS experiments require understanding how multilingual capabilities are distributed across layers
  - Quick check question: What architectural features enable transformers to handle multiple languages effectively?

## Architecture Onboarding

- Component map: Input text → embedding layer → transformer layers (32/40/80) → final layer output → task-specific head for multiple-choice classification
- Critical path: Text input flows through embedding layer into transformer layers, with each layer containing self-attention and feed-forward sublayers with residual connections, ultimately producing final layer output for task classification
- Design tradeoffs: Larger models have more parameters but similar computational patterns; layer-wise specialization suggests diminishing returns for adding layers beyond certain points for specific tasks
- Failure signatures: Poor performance on reasoning tasks despite large size indicates threshold effects not being met; uniform performance across layers suggests lack of specialization
- First 3 experiments:
  1. Layer-wise ablation study: Remove upper vs lower layers to quantify their contribution to different task types
  2. Cross-lingual probing: Test multilingual reasoning performance across different language pairs and layer depths
  3. Threshold scaling: Systematically test reasoning performance across finer-grained model size increments to pinpoint emergence points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do penultimate layers outperform final layer in mathematical computation tasks?
- Basis in paper: Explicit
- Why unresolved: Paper notes pattern but lacks direct evidence about underlying mechanism
- What evidence would resolve it: Comparative analysis of pre-training data distribution across layers or ablation studies removing specific layer types

### Open Question 2
- Question: Why does penultimate layer perform significantly better than last layer in cross-lingual math problem solving?
- Basis in paper: Explicit
- Why unresolved: Paper observes phenomenon but doesn't explain mechanism behind it
- What evidence would resolve it: Layer-wise attention analysis during multilingual reasoning or comparative studies with different multilingual strategies

### Open Question 3
- Question: What causes significant performance improvements starting from mid-layer networks across all tasks?
- Basis in paper: Explicit
- Why unresolved: Paper observes consistent pattern but doesn't investigate architectural or training factors
- What evidence would resolve it: Architectural analysis of mid-layer transformer components or training dynamics studies

### Open Question 4
- Question: Why do lower layers across different model scales exhibit nearly identical performance despite architectural differences?
- Basis in paper: Explicit
- Why unresolved: Paper observes similarity but doesn't explain why parameter differences don't translate to performance differences
- What evidence would resolve it: Comparative analysis of activation patterns and feature representations across model sizes in lower layers

### Open Question 5
- Question: Why do floating-point arithmetic operations consistently show lower performance than integer operations?
- Basis in paper: Explicit
- Why unresolved: Paper observes pattern but doesn't investigate whether due to training data distribution, architectural limitations, or inherent complexity differences
- What evidence would resolve it: Analysis of numerical precision handling in transformer architectures or controlled experiments varying decimal precision

## Limitations
- Findings may be specific to LLaMA architecture and training procedure rather than universal across LLM families
- Probing task methodology relies heavily on multiple-choice formats that may not capture full spectrum of capabilities
- Study focuses on English-centric tasks, limiting conclusions about multilingual capabilities despite including xMPS experiments

## Confidence
- High Confidence: Computational abilities housed in upper layers; factual knowledge peaks at final layer
- Medium Confidence: Model size thresholds triggering emergent reasoning abilities; penultimate layer optimization for reasoning tasks
- Low Confidence: Claims about uniform pre-training data across scales; multilingual layer specialization across limited language pairs

## Next Checks
1. **Threshold Refinement Study**: Conduct experiments with intermediate model sizes (e.g., 10B, 20B parameters) to precisely identify reasoning ability emergence points and determine whether threshold effects are continuous or discrete

2. **Cross-Architecture Replication**: Apply same layer-wise probing methodology to other LLM architectures (GPT, Mistral, or open-source alternatives) to test whether observed patterns are architecture-specific or general phenomena

3. **Task Format Diversity Test**: Replicate key findings using alternative task formats beyond multiple-choice (e.g., direct answer generation, structured reasoning tasks) to validate that observed patterns are not artifacts of chosen evaluation methodology