---
ver: rpa2
title: 'SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced Classification
  in Pathology'
arxiv_id: '2303.13405'
source_url: https://arxiv.org/abs/2303.13405
tags:
- learning
- imbalance
- contrastive
- instance
- sc-mil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called SC-MIL for addressing label
  imbalance in multiple instance learning problems in pathology. The key idea is to
  integrate supervised contrastive learning into the MIL framework, allowing for better
  learning of bag-level representations in imbalanced settings.
---

# SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced Classification in Pathology

## Quick Facts
- arXiv ID: 2303.13405
- Source URL: https://arxiv.org/abs/2303.13405
- Authors: [Not specified in input]
- Reference count: 31
- Primary result: SC-MIL outperforms state-of-the-art techniques for handling label imbalance in pathology classification, especially at higher imbalance ratios and for minority classes

## Executive Summary
This paper proposes SC-MIL, a method that integrates supervised contrastive learning into multiple instance learning (MIL) to address label imbalance in pathology classification. The key innovation is decoupling feature learning from classifier learning through a progressive curriculum that transitions from contrastive loss to cross-entropy loss during training. This approach enables better learning of bag-level representations in imbalanced settings by first creating balanced feature spaces before optimizing the classifier. Experiments on NSCLC and RCC subtyping datasets demonstrate that SC-MIL outperforms other state-of-the-art techniques for handling label imbalance, with improvements especially pronounced for minority classes and at higher imbalance ratios.

## Method Summary
SC-MIL addresses label imbalance in pathology MIL by applying supervised contrastive learning at the bag level rather than the instance level. The method uses a joint training objective combining supervised contrastive loss and cross-entropy loss, with a progressive weighting scheme (βt) that transitions from feature learning to classifier learning during training. Patches are extracted from whole slide images, embedded using a pre-trained ShuffleNet, and aggregated into bag representations via an attention mechanism. The bag embeddings are then projected for contrastive learning and classified for prediction. This curriculum-based approach prevents early overfitting to majority classes while creating balanced feature representations.

## Key Results
- SC-MIL achieves significantly higher macro-averaged F1 scores than baseline methods across all imbalance ratios (1, 5, 10) on both NSCLC and RCC datasets
- On minority classes, SC-MIL shows improvements of up to 10% in F1 score compared to the best baseline method
- Out-of-distribution validation demonstrates that SC-MIL maintains performance advantages even on data from different labs, indicating robust generalization
- The benefits of SC-MIL are most pronounced at higher imbalance ratios (10:1), where traditional methods show severe degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SC-MIL decouples feature learning from classifier learning, leading to more balanced decision boundaries in imbalanced settings.
- Mechanism: By progressively transitioning from contrastive loss (feature learning) to cross-entropy loss (classifier learning), the model first learns a balanced feature space before optimizing the classifier. This two-stage approach prevents the classifier from overfitting to majority classes during early training.
- Core assumption: Balanced feature spaces lead to better performance on minority classes than directly training a classifier on imbalanced data.
- Evidence anchors:
  - [abstract]: "We leverage the idea that decoupling feature and classifier learning can lead to improved decision boundaries for label imbalanced datasets."
  - [section]: "Curriculum-based feature and classifier learning using both contrastive and cross entropy losses has been shown to be effective in long-tailed image classification [13]."
  - [corpus]: Weak - corpus papers focus on MIL variations but not explicitly on decoupling feature/classifier learning for imbalance.
- Break condition: If the contrastive loss fails to create balanced features (e.g., due to poor temperature choice), the decoupling benefit disappears.

### Mechanism 2
- Claim: Applying supervised contrastive loss at the bag level (not instance level) avoids label noise in MIL.
- Mechanism: Since individual patches within a bag may not reflect the bag label (e.g., normal patches in a malignant bag), using bag-level representations for contrastive learning prevents incorrect label assignments to instances.
- Core assumption: Individual patches in a bag cannot be reliably labeled with the bag's class label without introducing noise.
- Evidence anchors:
  - [section]: "Applying SCL to patch features assumes assigning a bag-label to individual patches. However, a single patch might not have any information about the WSI label... This motivates our bag-level formulation of SC-MIL where contrastive loss is applied to the bag features."
  - [corpus]: Weak - corpus focuses on MIL architectures but not specifically on the bag-level vs instance-level contrastive learning distinction for label noise.
- Break condition: If bag representations are poorly constructed (e.g., ineffective attention mechanism), bag-level contrastive learning provides no benefit over instance-level.

### Mechanism 3
- Claim: SC-MIL's smooth curriculum transition from feature to classifier learning creates more discriminative latent spaces for imbalanced classification.
- Mechanism: The progressive weighting scheme (βt decaying from 1 to 0) allows the model to first establish good feature representations before fine-tuning the classifier, avoiding early overfitting to majority classes.
- Core assumption: Gradual transition from feature to classifier learning is superior to abrupt or two-stage training for imbalanced MIL problems.
- Evidence anchors:
  - [section]: "We apply the same approach to the MIL setting at a bag level... with a linear curriculum as described in Section 2.2, with βt = 1 at the start of training."
  - [corpus]: Weak - corpus papers don't discuss curriculum learning specifically in the context of SC-MIL or imbalanced MIL.
- Break condition: If the curriculum schedule is poorly chosen (e.g., too fast or too slow transition), the model may either underfit or overfit to the imbalance.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: The paper addresses pathology classification where only slide-level labels are available, not individual patch labels. MIL provides the framework to learn from bags of patches.
  - Quick check question: In MIL, if a bag contains both malignant and normal patches, how is the bag labeled and why?

- Concept: Supervised Contrastive Learning (SCL)
  - Why needed here: SCL helps create balanced feature representations by pulling same-class instances together and pushing different-class instances apart, which is crucial for handling label imbalance.
  - Quick check question: How does supervised contrastive loss differ from standard contrastive loss in terms of positive sample selection?

- Concept: Curriculum Learning
  - Why needed here: The progressive transition from contrastive to classification loss allows the model to first learn good features before optimizing the classifier, which is particularly beneficial in imbalanced settings.
  - Quick check question: What is the purpose of decaying βt from 1 to 0 during training in SC-MIL?

## Architecture Onboarding

- Component map: WSI -> patches (224x224) -> ShuffleNet embeddings -> AttentionMIL aggregation -> bag embedding -> projector -> contrastive loss + classifier -> predictions
- Critical path:
  1. Extract patches from WSI tissue regions
  2. Generate patch embeddings with ShuffleNet
  3. Compute bag embedding via attention aggregation
  4. Project bag embedding for contrastive loss
  5. Apply classifier to bag embedding for prediction
  6. Compute joint loss with progressive weighting
- Design tradeoffs:
  - Bag-level vs instance-level contrastive learning: Bag-level avoids label noise but may lose fine-grained information
  - Random vs class-balanced sampling: Random sampling preserves diversity; class-balanced may oversample minority classes
  - Temperature τ: Affects uniformity vs tolerance in feature space; must be tuned for 3-class problem
- Failure signatures:
  - Poor minority class performance: Likely issues with feature learning or curriculum schedule
  - Overfitting to majority class: Contrastive loss not creating balanced features
  - High variance across runs: Insufficient batch size or unstable attention mechanism
- First 3 experiments:
  1. Baseline ERM-RS (random sampling MIL) to establish performance floor
  2. SC-MIL with random sampling to test decoupling mechanism
  3. SC-MIL with class-balanced sampling to evaluate sampling strategy impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SC-MIL perform on pathology datasets with more than 3 classes?
- Basis in paper: [inferred] The paper only tests SC-MIL on datasets with 2-3 classes (NSCLC and RCC subtyping). The authors note that model performance is less sensitive to temperature changes for problems with fewer classes.
- Why unresolved: The paper does not explore the performance of SC-MIL on datasets with more classes, which is common in pathology applications like cancer subtyping.
- What evidence would resolve it: Testing SC-MIL on a pathology dataset with more than 3 classes and comparing its performance to other methods would provide insights into its scalability and effectiveness in more complex classification scenarios.

### Open Question 2
- Question: What is the impact of different patch sizes on SC-MIL's performance in pathology?
- Basis in paper: [inferred] The paper uses a fixed patch size of 224x224 pixels for all experiments. Different patch sizes might capture different levels of detail in pathology images.
- Why unresolved: The paper does not investigate the effect of varying patch sizes on SC-MIL's performance, which could be crucial for capturing relevant features in pathology images.
- What evidence would resolve it: Conducting experiments with different patch sizes and analyzing their impact on SC-MIL's performance would help determine the optimal patch size for various pathology applications.

### Open Question 3
- Question: How does SC-MIL handle extremely rare diseases in pathology with very few positive cases?
- Basis in paper: [inferred] The paper tests SC-MIL on datasets with imbalance ratios up to 10, but it does not explore scenarios with extremely rare diseases where the positive cases might be very scarce.
- Why unresolved: The paper does not address the performance of SC-MIL in cases where the positive class is extremely rare, which is a common challenge in pathology.
- What evidence would resolve it: Testing SC-MIL on datasets with very high imbalance ratios (e.g., 100 or 1000) and comparing its performance to other methods would provide insights into its effectiveness in handling extremely rare diseases in pathology.

## Limitations
- Evaluation scope limited to two specific pathology datasets (NSCLC and RCC subtyping) from TCGA, with uncertain generalizability to other pathology domains
- Temperature parameter τ = 1 is fixed without systematic ablation studies to determine optimal values for different imbalance ratios
- Curriculum transition schedule is heuristic and may not be optimal for all imbalance scenarios

## Confidence
- **High confidence**: The mechanism of decoupling feature learning from classifier learning is well-supported by prior work in long-tailed classification and the experimental results demonstrate consistent improvements across multiple imbalance ratios.
- **Medium confidence**: The bag-level contrastive learning approach is theoretically sound, but the empirical advantage over instance-level alternatives is not thoroughly explored, and the assumption about label noise in MIL patches, while intuitive, lacks quantitative validation.
- **Low confidence**: The claim that SC-MIL "significantly" outperforms baselines on minority classes - while statistically supported, the absolute performance gains for the smallest classes (especially KICH in RCC) remain relatively modest even with SC-MIL.

## Next Checks
1. **Ablation on temperature τ**: Systematically vary the temperature parameter in supervised contrastive loss across {0.1, 0.5, 1.0, 2.0} to determine optimal values for different imbalance ratios and validate the sensitivity of SC-MIL to this hyperparameter.
2. **Instance-level contrastive baseline**: Implement and compare against an instance-level supervised contrastive MIL approach to quantify the actual benefit of the bag-level formulation and validate the label noise assumption.
3. **Cross-domain validation**: Test SC-MIL on pathology datasets from different cancer types or organs (e.g., breast, prostate) and compare performance degradation to baseline methods to assess generalizability beyond NSCLC and RCC.