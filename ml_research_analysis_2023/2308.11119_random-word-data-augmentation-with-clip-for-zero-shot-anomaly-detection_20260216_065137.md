---
ver: rpa2
title: Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection
arxiv_id: '2308.11119'
source_url: https://arxiv.org/abs/2308.11119
tags:
- clip
- anomaly
- samples
- normal
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses zero-shot anomaly detection, where models must
  detect anomalies without prior knowledge of the object categories. The core idea
  is to leverage CLIP as a data source by generating diverse text embeddings with
  random word data augmentation, which are then used to train a feed-forward neural
  network (FNN) to classify normal and anomalous samples.
---

# Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection

## Quick Facts
- arXiv ID: 2308.11119
- Source URL: https://arxiv.org/abs/2308.11119
- Reference count: 40
- Primary result: Achieves 91.0% AUROC on MVTec-AD in zero-shot unknown-object setup

## Executive Summary
This paper introduces a novel approach to zero-shot anomaly detection that leverages CLIP as a data source through random word data augmentation. By inserting randomly generated words into prompts, the method creates diverse text embeddings that train a feed-forward neural network (FNN) to classify normal and anomalous samples without requiring target object information. The approach achieves state-of-the-art performance on two benchmark datasets, particularly excelling in the challenging zero-shot unknown-object scenario.

The key innovation lies in using random word augmentation to generate diverse training samples from CLIP's text encoder, which are then used to train an FNN classifier. During inference, the trained FNN can process image embeddings directly, enabling anomaly detection on previously unseen object categories. The method demonstrates that combining CLIP's semantic understanding with learned FNN patterns provides complementary information for improved detection performance.

## Method Summary
The method generates diverse text embeddings by inserting random words into prompt templates, then trains an FNN classifier on these embeddings to distinguish normal from anomalous samples. During inference, the trained FNN processes image embeddings directly, enabling zero-shot anomaly detection on unknown object categories. The approach leverages CLIP's pre-trained capabilities without requiring any target object information during training or inference.

## Key Results
- Achieves 91.0% AUROC on MVTec-AD in zero-shot unknown-object setup
- Reaches 78.1% AUROC on VisA dataset, outperforming CLIP and WinCLIP
- Demonstrates effectiveness on SewerML dataset with diverse industrial defects

## Why This Works (Mechanism)

### Mechanism 1
Random word data augmentation increases diversity of CLIP-generated embeddings, enabling better anomaly detection. Inserting randomly generated words into prompts causes CLIP's text encoder to produce perturbed embeddings that cover a wider semantic space, creating diverse training samples. The diversity in embeddings improves the FNN's ability to learn distinguishing features between normal and anomalous samples.

### Mechanism 2
Training an FNN on CLIP embeddings without target object information produces a category-agnostic anomaly detector. The FNN learns to classify embeddings based on normality/anomaly rather than object category features, making it applicable to unknown object categories. CLIP embeddings contain enough semantic information to distinguish normal from anomalous samples regardless of object category.

### Mechanism 3
Combining CLIP's prompt-guided scores with FNN scores improves overall performance. CLIP provides semantic understanding while FNN captures learned patterns from diverse embeddings, and their combination leverages both strengths. The two scoring methods capture complementary information about anomalies.

## Foundational Learning

- Concept: Contrastive learning in CLIP
  - Why needed here: Understanding how CLIP learns to align image and text embeddings is crucial for leveraging it as a data source
  - Quick check question: How does CLIP's training objective differ from traditional supervised classification?

- Concept: Anomaly detection vs. classification
  - Why needed here: The paper addresses zero-shot anomaly detection where models must detect anomalies without prior knowledge of object categories
  - Quick check question: What distinguishes anomaly detection from object classification in terms of training data requirements?

- Concept: Data augmentation techniques
  - Why needed here: The core contribution relies on random word data augmentation to generate diverse training samples
  - Quick check question: How does random word augmentation differ from traditional image-based data augmentation methods?

## Architecture Onboarding

- Component map: CLIP text encoder -> Random word prompt generator -> FNN classifier -> CLIP image encoder (inference)
- Critical path:
  1. Generate augmented prompts with random words
  2. Encode prompts to text embeddings using CLIP
  3. Train FNN on text embedding pairs
  4. During inference, encode input images to image embeddings
  5. Apply trained FNN to image embeddings for anomaly scoring
- Design tradeoffs:
  - Random word length (lmin=5, lmax=10) affects embedding diversity
  - Number of prompt pairs (Np=10,000) balances training coverage vs. overfitting
  - Using FNN alone vs. combining with CLIP scores for final decision
- Failure signatures:
  - Performance degrades with too few or too many training prompt pairs
  - Inappropriate word choices for normal/anomaly reduce effectiveness
  - Embeddings from augmented prompts don't sufficiently differ from natural prompts
- First 3 experiments:
  1. Test performance with varying numbers of prompt pairs (100, 1000, 10000, 50000)
  2. Evaluate different word choices for normal/anomaly in prompts
  3. Compare performance with and without multi-crop data augmentation during inference

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method's performance scale with the diversity of the anomalous samples in real-world industrial applications? The paper mentions the potential application in detecting ambiguous anomalous samples and evaluates on the SewerML dataset, which contains diverse defects. However, the evaluation is limited to a single dataset.

### Open Question 2
What is the impact of the choice of the visual-language model (e.g., CLIP) on the proposed method's performance, and how does it compare to other models? The paper uses CLIP as the visual-language model for generating text embeddings. However, it does not explore the use of other models or compare the performance of CLIP with other visual-language models.

### Open Question 3
How does the proposed method handle the trade-off between the number of random words and the performance of the anomaly detector? The paper mentions that the minimum and maximum word lengths for random words are set to 5 and 10, respectively, and that 10,000 pairs of normal and anomalous samples are generated by default. However, it does not explore the impact of varying these parameters on the performance of the anomaly detector.

## Limitations

- Performance drops on VisA dataset (78.1% AUROC) compared to MVTec-AD (91.0% AUROC), suggesting limited generalization across all object categories
- Computational efficiency concerns with inference involving both CLIP encoding and FNN processing
- Random word generation may introduce artifacts if generated words inadvertently align with object-specific features

## Confidence

**High Confidence**: The claim that random word data augmentation increases embedding diversity is well-supported by experimental results showing consistent performance improvements. The mechanism by which FNN training on CLIP embeddings enables category-agnostic anomaly detection is clearly demonstrated.

**Medium Confidence**: The assertion that combining CLIP and FNN scores provides complementary information is supported by improved performance, but lacks detailed analysis of specific contributions. The optimal number of training prompt pairs (10,000) appears well-justified through ablation studies.

**Low Confidence**: The claim that this approach represents a fundamental advance over prompt ensembling methods lacks comparative analysis with alternative augmentation strategies. The assertion that random words provide meaningful diversity without degrading embedding quality needs more rigorous validation.

## Next Checks

1. **Robustness Testing**: Evaluate the method's performance across a broader range of object categories and anomaly types, particularly focusing on edge cases where CLIP's understanding may be limited.

2. **Ablation on Random Word Generation**: Conduct systematic experiments varying the random word generation parameters (length, character distribution, semantic relevance) to determine the optimal configuration for maintaining embedding diversity while avoiding semantic artifacts.

3. **Computational Efficiency Analysis**: Benchmark the inference time and computational requirements against simpler anomaly detection methods, particularly focusing on real-world deployment scenarios where speed and resource constraints are critical.