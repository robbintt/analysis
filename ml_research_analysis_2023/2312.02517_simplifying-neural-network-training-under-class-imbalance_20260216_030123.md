---
ver: rpa2
title: Simplifying Neural Network Training Under Class Imbalance
arxiv_id: '2312.02517'
source_url: https://arxiv.org/abs/2312.02517
tags:
- training
- data
- class
- imbalanced
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of training neural networks on\
  \ class-imbalanced datasets, where some classes have far fewer samples than others.\
  \ It shows that instead of relying on specialized loss functions or sampling methods,\
  \ careful tuning of standard deep learning components\u2014such as smaller batch\
  \ sizes, data augmentation, model architecture, self-supervised learning, and label\
  \ smoothing\u2014can achieve state-of-the-art results on both vision and tabular\
  \ benchmarks."
---

# Simplifying Neural Network Training Under Class Imbalance

## Quick Facts
- arXiv ID: 2312.02517
- Source URL: https://arxiv.org/abs/2312.02517
- Reference count: 40
- Key outcome: Careful tuning of standard deep learning components (batch size, augmentation, architecture, self-supervised learning, label smoothing) achieves state-of-the-art results on class-imbalanced datasets without specialized loss functions or sampling methods.

## Executive Summary
This paper demonstrates that class-imbalanced training can be effectively addressed through careful tuning of standard deep learning components rather than relying on specialized loss functions or sampling techniques. The authors show that smaller batch sizes, strong data augmentation, moderate model architectures, self-supervised learning (Joint-SSL), and label smoothing can significantly improve minority class performance while maintaining majority class accuracy. Their empirical findings reveal that imbalanced datasets are particularly sensitive to batch size, benefit more from strong augmentations, and are prone to overfitting when using overly large architectures. The study also highlights that methods effective on web-scraped benchmarks often underperform on real-world imbalanced datasets, emphasizing the need for diverse evaluation benchmarks.

## Method Summary
The method focuses on tuning standard deep learning components for class-imbalanced training rather than developing specialized algorithms. The approach involves using smaller batch sizes (typically smaller than the standard 128), strong data augmentation strategies like AutoAugment, moderate-sized model architectures (avoiding overly large models that overfit minority classes), self-supervised learning through Joint-SSL integration during from-scratch training, and label smoothing with class-conditional smoothing parameters. The training procedure combines supervised learning with an additional self-supervised loss function, bypassing the need for pre-training. Asymmetric-SAM is implemented by increasing the ascent step size in SAM's inner loop for minority classes, pulling decision boundaries away from minority samples to create larger margins that prevent overfitting.

## Key Results
- Smaller batch sizes improve minority class performance through regularization effects that prevent overfitting to majority classes, despite often not containing minority samples
- Joint-SSL with Asymmetric-SAM and label smoothing improves minority class accuracy while maintaining majority class performance
- Class-imbalanced datasets are more sensitive to batch size and benefit more from strong augmentations compared to balanced settings
- Larger architectures overfit minority classes in imbalanced settings, making moderate-sized models preferable
- Methods effective on web-scraped benchmarks (CIFAR-10/100) often underperform on real-world imbalanced datasets (SIIM-ISIC Melanoma, APTOS 2019)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller batch sizes improve minority class performance by acting as a regularizer that prevents overfitting to majority classes.
- Mechanism: In imbalanced settings, smaller batches increase gradient noise and reduce the dominance of majority-class gradients, forcing the model to learn more generalizable features that benefit minority classes.
- Core assumption: Gradient noise from smaller batches serves as implicit regularization that counteracts the tendency of larger architectures to overfit minority classes.
- Evidence anchors:
  - [abstract]: "data with a high degree of class imbalance tends to benefit from smaller batch sizes, even though small batches often do not contain minority samples, possibly due to the regularization effects that help mitigate overfitting to the majority classes."
  - [section]: "Our analysis reveals that batch size has a much greater impact in highly imbalanced settings and very little impact in balanced settings."
- Break condition: If the model architecture is too small to represent the data complexity, small batch sizes may cause underfitting rather than beneficial regularization.

### Mechanism 2
- Claim: Self-supervised learning (Joint-SSL) improves generalization by learning feature representations that are independent of class-imbalanced labels.
- Mechanism: By adding an SSL loss during training, the model learns invariant features that capture underlying data structure rather than memorizing class-specific patterns that are biased by imbalance.
- Core assumption: The SSL objective is insensitive to class imbalance because it doesn't depend on label information.
- Evidence anchors:
  - [abstract]: "Adding a self-supervised loss during training can improve feature representations, leading to performance boosts on class-imbalanced problems."
  - [section]: "We instead integrate supervised learning with an additional self-supervised loss function during from-scratch training, bypassing the need for pre-training."
- Break condition: If the SSL objective conflicts with the supervised task (e.g., contrastive learning objectives that separate similar classes), it may degrade performance.

### Mechanism 3
- Claim: Asymmetric-SAM improves minority class accuracy by expanding decision margins specifically around minority samples.
- Mechanism: By increasing the ascent step size in SAM's inner loop for minority classes, the method pulls decision boundaries away from minority samples, creating larger margins that prevent overfitting to these samples.
- Core assumption: Flat minima with wide margins correspond to better generalization, and minority samples particularly benefit from larger margins.
- Evidence anchors:
  - [abstract]: "A small modification of Sharpness-Aware Minimization (SAM) [19] pulls decision bound-aries away from minority samples and significantly improves minority-group accuracy."
  - [section]: "SAM-A expands these margins, preventing the model from overfitting to the minority samples."
- Break condition: If the asymmetric margin expansion becomes too aggressive, it may underfit minority classes by pushing decision boundaries too far away.

## Foundational Learning

- Concept: Regularization and overfitting prevention
  - Why needed here: Class-imbalanced datasets are particularly prone to overfitting minority samples, as the model can easily memorize the few available examples. Understanding regularization techniques is crucial for preventing this overfitting.
  - Quick check question: How does increasing gradient noise through smaller batch sizes act as a regularizer in imbalanced settings?

- Concept: Decision boundary analysis and margin theory
  - Why needed here: The paper's analysis of decision boundaries and margins provides insight into why certain methods succeed or fail. Understanding how margins affect generalization is key to grasping the paper's findings.
  - Quick check question: Why does expanding margins around minority samples help prevent overfitting in imbalanced datasets?

- Concept: Neural collapse phenomenon
  - Why needed here: The paper discusses how neural collapse (features concentrating around class means) relates to overfitting in imbalanced settings. Understanding this concept is essential for interpreting the paper's analysis of method effectiveness.
  - Quick check question: How does neural collapse manifest differently in balanced vs. imbalanced training, and why is this problematic for minority classes?

## Architecture Onboarding

- Component map: WideResNet-28×10 -> SGD optimizer -> cosine annealing learning rate schedule -> weight decay -> Joint-SSL loss -> Asymmetric-SAM -> label smoothing
- Critical path: The critical path is the integration of Joint-SSL with Asymmetric-SAM and label smoothing. These components work synergistically to prevent overfitting while maintaining generalization.
- Design tradeoffs: Larger architectures overfit minority classes in imbalanced settings, so the design favors moderate-sized models. The tradeoff is between model capacity and regularization strength.
- Failure signatures: Poor minority class performance despite good overall accuracy indicates overfitting to majority classes. If SAM-A is too aggressive, it may underfit minority classes by creating margins that are too large.
- First 3 experiments:
  1. Train baseline WideResNet-28×10 with standard ERM on CIFAR-100 with 0.01 training ratio to establish baseline performance.
  2. Add Joint-SSL to the baseline and measure improvement in minority class accuracy.
  3. Implement Asymmetric-SAM and compare its effect on decision margins and minority class performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of neural networks trained under class imbalance generalize to real-world datasets that are not web-scraped benchmarks?
- Basis in paper: [explicit] The paper states that methods effective on web-scraped benchmarks like CIFAR-10 and CIFAR-100 underperform on real-world datasets such as SIIM-ISIC Melanoma and APTOS 2019 Blindness Detection, highlighting a low correlation between performance on different datasets.
- Why unresolved: The paper only tests a few real-world datasets and does not provide a comprehensive benchmark suite for class-imbalanced training, leaving the generalizability of methods uncertain.
- What evidence would resolve it: A diverse and extensive benchmark suite of real-world class-imbalanced datasets that allows for a thorough evaluation of training methods across different domains and application areas.

### Open Question 2
- Question: What is the impact of class imbalance on the design and optimization of neural network architectures?
- Basis in paper: [explicit] The paper shows that architectures optimized for class-balanced benchmarks like ImageNet are highly suboptimal under class imbalance, and that larger networks overfit on imbalanced data.
- Why unresolved: The paper does not explore the development of architectures specifically optimized for class imbalance or provide a framework for adapting existing architectures to imbalanced settings.
- What evidence would resolve it: New architectures or architectural modifications that are explicitly designed to handle class imbalance, along with empirical studies comparing their performance to standard architectures on imbalanced datasets.

### Open Question 3
- Question: How can the insights from class-imbalanced training be applied to improve the performance of language models on infrequent tokens?
- Basis in paper: [inferred] The paper discusses the importance of regularization and overfitting prevention in class-imbalanced training, which could be relevant to language models where some tokens occur much less frequently than others.
- Why unresolved: The paper focuses on image and tabular data, and does not investigate the application of class-imbalanced training techniques to language models or token-level classification tasks.
- What evidence would resolve it: Experiments applying class-imbalanced training techniques to language models, such as adjusting the training routine or incorporating self-supervision, and evaluating their impact on the performance of infrequent tokens.

## Limitations

- The paper's core findings rely heavily on empirical observations without strong theoretical grounding for several mechanisms
- The analysis of batch size effects lacks rigorous explanation for why smaller batches help minority classes despite often not containing minority samples
- The claim that SSL is insensitive to class imbalance needs more theoretical justification, as certain SSL objectives could potentially exacerbate imbalance issues
- Asymmetric-SAM modification lacks ablation studies to isolate the specific contribution of the asymmetric component versus standard SAM

## Confidence

- **High Confidence**: The empirical observations about overfitting to minority classes with large architectures, the general effectiveness of regularization techniques (batch size, augmentation, label smoothing), and the finding that web-scraped benchmarks don't generalize to real-world imbalanced datasets.
- **Medium Confidence**: The specific mechanisms proposed for why smaller batch sizes help, the effectiveness of Joint-SSL integration, and the asymmetric margin expansion concept in SAM-A.
- **Low Confidence**: The theoretical explanations for these mechanisms and the claim that SSL is inherently insensitive to class imbalance.

## Next Checks

1. **Mechanism Validation**: Conduct controlled experiments varying batch size while keeping other factors constant to isolate the regularization effect on minority class learning, specifically testing whether minority samples in a batch are necessary for the benefit.

2. **SSL Ablation Study**: Compare Joint-SSL against other SSL variants (e.g., contrastive learning, masked autoencoders) to determine if the benefits are specific to the Joint-SSL approach or if they generalize to other SSL methods.

3. **SAM-A Component Analysis**: Perform an ablation study comparing Asymmetric-SAM against standard SAM with different hyperparameter settings to quantify the marginal benefit of the asymmetric modification versus other SAM components.