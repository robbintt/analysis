---
ver: rpa2
title: Self-Assessment Tests are Unreliable Measures of LLM Personality
arxiv_id: '2309.08163'
source_url: https://arxiv.org/abs/2309.08163
tags:
- personality
- tests
- these
- llms
- self-assessment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of self-assessment personality
  tests for large language models (LLMs). It evaluates whether personality scores
  obtained from such tests are consistent and robust to variations in prompt templates
  and option order.
---

# Self-Assessment Tests are Unreliable Measures of LLM Personality

## Quick Facts
- **arXiv ID**: 2309.08163
- **Source URL**: https://arxiv.org/abs/2309.08163
- **Reference count**: 6
- **Primary result**: Self-assessment personality tests produce inconsistent scores for LLMs when prompt templates or option order are varied

## Executive Summary
This study investigates the reliability of self-assessment personality tests for large language models (LLMs) by examining whether personality scores remain consistent across different prompt templates and option orders. The researchers conducted two experiments using the IPIP-120 dataset and found that both ChatGPT and Llama2 models produce significantly different personality scores when the same questions are presented with semantically equivalent but syntactically different prompts, or when multiple-choice options are reordered. These findings demonstrate that self-assessment tests are unreliable measures of LLM personality due to their sensitivity to subjective administration choices, and highlight the need for more robust evaluation methods.

## Method Summary
The researchers used the IPIP-120 dataset (120 questions covering the Big-Five personality traits) and three semantically equivalent prompt templates from prior work. They tested three Llama2 model sizes (7B, 13B, 70B) and ChatGPT using zero-shot prompting with temperature=0.01 and top_p=1. For each question, they generated responses using all three prompts and both original and reversed option orders. Personality scores were computed from the responses and compared statistically to identify significant differences across conditions.

## Key Results
- Personality scores vary significantly across different but semantically equivalent prompt templates for both ChatGPT and Llama2 models
- Score distributions change when multiple-choice option orders are reversed, indicating sensitivity to option position
- The magnitude of variation differs across personality traits and model sizes, but the sensitivity pattern is consistent
- No single prompt template or option order produces consistently more "accurate" results, as there is no ground truth for LLM personality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-assessment personality tests for LLMs are unreliable because their scores are highly sensitive to minor variations in prompt templates.
- Mechanism: When the same personality test question is presented in semantically equivalent but syntactically different prompts, LLMs produce significantly different scores. This occurs because LLMs do not have a ground truth internal "personality" to draw from, so their responses depend on how the question is framed.
- Core assumption: LLMs do not possess an internal personality trait state analogous to humans, so test responses are determined by prompt interpretation rather than introspection.
- Evidence anchors:
  - [abstract] "Results show that personality scores vary significantly across prompts and option orders for both ChatGPT and Llama2 models, indicating that self-assessment tests are sensitive to subjective choices in their administration."
  - [section] "We find that all three prompts lead to very different personality scores, a difference that is statistically significant for all traits in a large majority of scenarios."
  - [corpus] Weak - no corpus neighbors directly test prompt variation effects on LLM personality.
- Break condition: If a future LLM architecture were shown to have a stable internal personality trait state independent of prompt framing, this mechanism would no longer hold.

### Mechanism 2
- Claim: Self-assessment personality test scores for LLMs are unreliable because they are sensitive to the order in which multiple-choice options are presented.
- Mechanism: LLMs exhibit a preference for certain option positions regardless of the semantic content, leading to different scores when the same question is presented with reordered options. This violates the assumption that personality tests should be robust to option order.
- Evidence anchors:
  - [abstract] "Since most of the self-assessment tests exist in the form of multiple choice question (MCQ) questions, we argue that the scores should also be robust to not just the prompt template but also the order in which the options are presented."
  - [section] "This test unsurprisingly reveals that the self-assessment test scores are not robust to the order of the options."
  - [corpus] Weak - no corpus neighbors directly test option order effects on LLM personality.
- Break condition: If LLMs were demonstrated to be insensitive to option order in personality assessments, this mechanism would be invalidated.

### Mechanism 3
- Claim: Self-assessment personality tests for LLMs are unreliable because LLMs cannot perform the introspection step required for self-assessment.
- Mechanism: Human personality tests assume the test-taker can introspect about their own tendencies and project them onto a Likert scale. LLMs lack this capability because they do not have experiential memories or subjective states to introspect upon.
- Evidence anchors:
  - [abstract] "Answering self-assessment questions is a non-trivial task and requires many different skills... requires the subject to introspect and self-reflect... This is usually based on analyzing the one's own reaction in similar situations in the past."
  - [section] "The final step in this test taking process is to project the obtained answer on the Likert scale... We argue that to even consider using these tests to measure LLM behavior, we must first evaluate the validity of these self-assessment tests for personality measurement of LLMs."
  - [corpus] Weak - no corpus neighbors directly test LLM introspection capabilities for personality assessment.
- Break condition: If LLMs were shown to have a form of introspection or self-modeling capability that allows them to answer personality questions consistently, this mechanism would be challenged.

## Foundational Learning

- Concept: Prompt sensitivity and its impact on LLM outputs
  - Why needed here: Understanding how minor prompt variations can lead to dramatically different LLM responses is crucial for interpreting the reliability of any LLM-based measurement system
  - Quick check question: If two semantically equivalent prompts produce different LLM outputs, what does this suggest about the reliability of using such outputs as measurements?

- Concept: Option order effects in multiple-choice questions
  - Why needed here: Many personality assessments use multiple-choice formats, and understanding how LLMs process option order helps explain measurement inconsistencies
  - Quick check question: Why might an LLM consistently prefer certain option positions regardless of the semantic content?

- Concept: The role of introspection in self-assessment tests
  - Why needed here: Recognizing that self-assessment tests assume the test-taker can introspect about their own tendencies helps explain why such tests may not transfer well to LLMs
  - Quick check question: What cognitive capabilities must a test-taker have to reliably complete a self-assessment personality test?

## Architecture Onboarding

- Component map:
  - Test administration layer -> LLM inference engine -> Score aggregation module -> Statistical analysis component

- Critical path:
  1. Load personality test questions (IPIP-120 dataset)
  2. Generate three semantically equivalent prompts for each question
  3. For each prompt, generate responses with both original and reversed option orders
  4. Calculate Big-Five personality scores for each condition
  5. Compare score distributions across conditions using statistical tests

- Design tradeoffs:
  - Using temperature=0.01 and top_p=1 ensures reproducibility but may not capture the full distribution of possible responses
  - Testing only chat models limits generalizability to non-conversational LLMs
  - Focusing on the Big-Five model excludes other personality frameworks that might behave differently

- Failure signatures:
  - Low variance in scores across prompt variations suggests the model is not engaging with the questions meaningfully
  - Perfect consistency in option order effects (e.g., always choosing the first option) indicates the model is not processing the semantic content
  - High standard deviations in scores even with controlled parameters suggest fundamental instability in the measurement approach

- First 3 experiments:
  1. Test the same LLM with three semantically equivalent prompts and compare variance in personality scores
  2. Test the same LLM with reversed option orders for each question and measure score changes
  3. Test multiple LLMs (different sizes) to determine if model scale affects sensitivity to prompt and option order variations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the ground truth for LLM personality traits, if any, and how can we establish it?
- Basis in paper: [explicit] The paper explicitly states that there is no ground truth for LLM personality scores, making it impossible to validate one prompt or method over another.
- Why unresolved: Without a ground truth, it is impossible to determine which method of administering personality tests is more accurate or reliable for LLMs.
- What evidence would resolve it: Developing a standardized, objective method to measure LLM personality that can be validated against human personality assessments or other reliable benchmarks.

### Open Question 2
- Question: Are LLMs capable of introspection and self-reflection, and how does this impact their ability to respond to self-assessment tests?
- Basis in paper: [explicit] The paper questions whether LLMs can introspect and understand their own behavioral tendencies, which is necessary for answering self-assessment questions.
- Why unresolved: The paper argues that introspection is a key component of self-assessment tests, but it is unclear if LLMs possess this capability.
- What evidence would resolve it: Conducting experiments to test LLMs' ability to introspect and predict their own behavior in various scenarios, and comparing the results with human introspection capabilities.

### Open Question 3
- Question: How do different prompt templates and option orders affect the consistency and reliability of LLM personality scores?
- Basis in paper: [explicit] The paper demonstrates that different prompts and option orders lead to varying personality scores, indicating sensitivity to these factors.
- Why unresolved: The paper shows that personality scores are sensitive to prompt templates and option orders, but it does not provide a solution to standardize these factors.
- What evidence would resolve it: Developing a standardized protocol for administering self-assessment tests to LLMs that minimizes the impact of prompt templates and option orders on the results.

## Limitations

- The study only tests two LLM families (ChatGPT and Llama2) and three prompt variants, which may not capture the full spectrum of potential variability
- The statistical analysis focuses on mean and standard deviation comparisons without exploring more sophisticated methods
- The study relies on zero-shot prompting without any fine-tuning or adaptation of LLMs to personality assessment tasks

## Confidence

- High confidence: Finding that prompt variations affect personality scores - directly demonstrated through statistical comparisons
- Medium confidence: Broader claim that self-assessment tests are unreliable for LLMs - supported but extrapolates from specific experimental conditions
- Low confidence: Generalizability to all LLM architectures and personality frameworks - limited to specific models and Big-Five framework

## Next Checks

1. **Cross-Model Validation**: Test the same personality assessment protocol across a broader range of LLM architectures (including smaller and specialized models) to determine if prompt sensitivity is universal or model-specific.

2. **Multiple Statistical Methods**: Apply additional statistical analyses (e.g., ANOVA, Bayesian methods) to verify that the observed differences are robust and not artifacts of the chosen analytical approach.

3. **Human-Likert Correlation**: Compare LLM personality scores across prompt variations with human responses to the same questions to determine if LLMs exhibit similar variability or if their behavior is fundamentally different from human test-takers.