---
ver: rpa2
title: Understanding Compositional Data Augmentation in Typologically Diverse Morphological
  Inflection
arxiv_id: '2305.13658'
source_url: https://arxiv.org/abs/2305.13658
tags:
- data
- stem
- compositional
- train
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a theoretical analysis and empirical evaluation
  of the StemCorrupt data augmentation method for low-resource morphological inflection.
  The core idea is that StemCorrupt improves compositional generalization by eliminating
  spurious correlations between morphemes.
---

# Understanding Compositional Data Augmentation in Typological Diverse Morphological Inflection

## Quick Facts
- **arXiv ID**: 2305.13658
- **Source URL**: https://arxiv.org/abs/2305.13658
- **Reference count**: 13
- **Primary result**: Selecting synthetic data with high diversity and predictive uncertainty significantly enhances StemCorrupt's data efficiency for morphological inflection

## Executive Summary
This paper presents a theoretical analysis and empirical evaluation of StemCorrupt, a data augmentation method for improving compositional generalization in low-resource morphological inflection tasks. The method works by randomly substituting stem characters in existing examples, which theoretically factorizes the data distribution into independent stem and affix generation components. Empirical results across 7 typologically diverse languages demonstrate that combining uniform morphological template sampling with uncertainty-based selection yields the best performance, though languages with non-concatenative patterns like vowel harmony benefit less from high-uncertainty examples due to phonotactic violations.

## Method Summary
The method involves generating synthetic training data by corrupting stem characters in existing examples using StemCorrupt (with substitution probability θ = 0.5), then selecting subsets of this synthetic data based on both diversity and predictive uncertainty. A Transformer model is first trained on gold-standard data, then uncertainty scores are computed for synthetic examples using negative log-likelihood. Subset selection strategies include random sampling, uniform morphological template (UMT) sampling, and combinations with uncertainty-based selection (UMT+Loss, UME+Loss). The selected synthetic data is then used to augment the training process.

## Key Results
- StemCorrupt with uncertainty-aware subset selection significantly improves data efficiency compared to random sampling
- The UMT+Loss hybrid strategy (combining morphological template diversity with uncertainty) performs best overall
- Languages with non-concatenative morphology like vowel harmony show reduced benefit from high-uncertainty examples due to phonotactic violations
- Results hold across 7 typologically diverse languages with only 100 gold-standard examples each

## Why This Works (Mechanism)

### Mechanism 1
- Claim: StemCorrupt eliminates spurious correlations between stem and affixes by randomizing stem characters, thereby factorizing the data distribution into stem and affix generation components
- Mechanism: By substituting stem characters with random ones from the language's alphabet, StemCorrupt breaks the statistical dependencies between the stem and the morphological features (MSDs), effectively decoupling stem generation from affix generation
- Core assumption: The stem is a non-empty aligned subsequence between lemma and inflected form, and the substitution probability θ is high enough (ideally θ=1) to fully randomize stem characters
- Evidence anchors: [abstract] "our theoretical analysis further leads us to study the sample efficiency with which StemCorrupt reduces these spurious correlations"; [section 3] "By the definition of Y = YstemYaf f ix, we have that P (Y|X, T) ≡ P (Ystem, Yaf f ix|X, T). Then, by the chain rule of probability, we have P (Yaf f ix|Ystem, X, T)P (Ystem|X, T)"
- Break condition: Languages with non-concatenative morphology like vowel harmony or reduplication, where stem-affix dependencies are linguistically necessary rather than spurious

### Mechanism 2
- Claim: The data distribution becomes factorized into P(Ystem|Xstem) and P(Yaf f ix|Xaf f ix, T) as the amount of augmented data increases, simplifying the learning problem
- Mechanism: As |DSyn train| → ∞, the mutual information between stem and affix variables approaches zero (Proposition 1), allowing the conditional probability to decompose into independent stem and affix generation factors
- Core assumption: The augmented dataset grows large enough that the synthetic examples dominate the training distribution, and the stem substitution is sufficiently random to eliminate dependencies
- Evidence anchors: [section 3] "Then, by the chain rule of probability, we have P (Yaf f ix|Ystem, X, T)P (Ystem|X, T). We first deconstruct the second factor. By proposition 1, we have that the second factor P (Ystem|X, T) = P (Ystem|X)"; [section 3.1] "Proof of Theorem 1. By the definition of Y = YstemYaf f ix, we have that P (Y|X, T) ≡ P (Ystem, Yaf f ix|X, T)"
- Break condition: When θ < 1 (partial corruption) or when the language has strong morphophonological constraints that create necessary stem-affix dependencies

### Mechanism 3
- Claim: Selecting synthetic examples with both high diversity and high predictive uncertainty improves sample efficiency compared to random sampling
- Mechanism: Uncertainty sampling identifies examples the model finds difficult, while uniform morphological template sampling ensures coverage across all MSDs, together providing more informative training examples per data point
- Core assumption: The initial model trained on gold-standard data can provide meaningful uncertainty estimates, and the synthetic dataset contains sufficient diversity to benefit from selective sampling
- Evidence anchors: [abstract] "we demonstrate that selecting a subset of datapoints with both high diversity and high predictive uncertainty significantly enhances the data-efficiency of StemCorrupt"; [section 4.2] "We quantify the uncertainty of a synthetic datapoint in DSyn train by computing the negative log-likelihood (averaged over all tokens in the target Y) for each synthetic datapoint in DSyn train"; [section 5.1] "Our analysis reveals statistically significant differences between the subset selection strategies, highlighting the effectiveness of the hybrid approaches (UMT/UME+L OSS) that consider both diversity and predictive uncertainty"
- Break condition: Languages where StemCorrupt induces phonotactic violations (like vowel harmony violations) that increase uncertainty without providing useful learning signals

## Foundational Learning

- Concept: Mutual information and its role in measuring statistical dependencies
  - Why needed here: The theoretical analysis relies on showing that mutual information between stem and affix variables approaches zero as augmented data increases, which is the mathematical foundation for the factorization claim
  - Quick check question: If I(X;Y) = 0, what does this tell us about the relationship between random variables X and Y?

- Concept: Information-theoretic characterization of data distributions
  - Why needed here: The paper uses information theory to formally characterize how StemCorrupt changes the underlying data distribution from a complex joint distribution to a simpler factorized form
  - Quick check question: How does the convexity of mutual information with respect to conditional distributions enable the proof that mutual information approaches zero?

- Concept: Active learning and uncertainty sampling principles
  - Why needed here: The empirical evaluation uses uncertainty sampling to select high-value synthetic examples, requiring understanding of how predictive uncertainty relates to learning efficiency
  - Quick check question: Why might selecting only high-uncertainty examples without considering diversity lead to suboptimal performance?

## Architecture Onboarding

- Component map: Transformer model trained on gold data + subsets of StemCorrupt-augmented data. Synthetic data generated by corrupting stems, selected via strategies (random, uniform morphological template, high loss/uncertainty, hybrid approaches)
- Critical path: 1) Generate large synthetic dataset via StemCorrupt, 2) Train initial model on gold-standard data, 3) Compute uncertainty scores for synthetic examples, 4) Select subset combining diversity and uncertainty, 5) Fine-tune or train final model on combined dataset, 6) Evaluate on compositional generalization test set
- Design tradeoffs: StemCorrupt introduces phonotactic violations in languages with non-concatenative morphology (vowel harmony, reduplication) which increases uncertainty but may not provide useful learning signals. The trade-off is between the generalization benefits of factorization versus the linguistic accuracy of maintaining necessary dependencies
- Failure signatures: Poor performance on languages with vowel harmony or reduplication, overfitting to common MSDs when using uncertainty-only selection, failure to improve beyond gold-standard performance when synthetic data distribution poorly matches target language's morphophonology
- First 3 experiments:
  1. Generate synthetic dataset with θ=1 and evaluate whether mutual information between stem and affix variables decreases as dataset size increases
  2. Compare UMT+L OSS vs RANDOM selection strategies across multiple languages and dataset sizes to verify the diversity+uncertainty hypothesis
  3. Analyze the correlation between uncertainty scores and phonotactic violations in languages with vowel harmony to understand when high-uncertainty examples are beneficial vs harmful

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the degree of corruption in StemCorrupt (controlled by parameter θ) affect the tradeoff between compositional generalization improvement and phonotactic violations in languages with non-concatenative morphology? The paper uses θ = 0.5 but theoretical analysis assumes θ = 1, and doesn't explore intermediate values that might balance benefits with reduced violations.

- **Open Question 2**: What specific linguistic features determine whether high-uncertainty synthetic examples are beneficial for compositional generalization, beyond the general distinction between concatenative and non-concatenative languages? The paper identifies vowel harmony as problematic but doesn't systematically analyze what features predict this effect.

- **Open Question 3**: How can uncertainty estimation be adapted for synthetic data selection in morphologically complex languages to avoid selecting phonotactically invalid examples while maintaining diversity benefits? The paper uses standard negative log-likelihood but doesn't investigate incorporating linguistic constraints into uncertainty estimation.

- **Open Question 4**: Does the effectiveness of the UMT+Loss strategy generalize to larger training datasets beyond the 100-example low-resource setting studied in the paper? The paper acknowledges this limitation and notes that data augmentation may provide more limited improvement in higher-resourced settings.

## Limitations

- Theoretical analysis assumes infinite synthetic data and complete corruption (θ = 1), which may not hold in practical finite-data scenarios
- Effectiveness of uncertainty-based selection depends on the quality of initial model predictions and maintenance of linguistic validity
- Phonotactic violations in non-concatenative languages (vowel harmony, reduplication) reduce the reliability of uncertainty signals for synthetic data selection
- Experiments limited to extremely low-resource setting (100 gold examples) without testing scalability to higher-resource scenarios

## Confidence

- **High confidence**: The empirical finding that combining diversity and uncertainty yields better sample efficiency than random sampling
- **Medium confidence**: The theoretical claim about distribution factorization as augmented data increases, pending validation with finite datasets
- **Medium confidence**: The mechanism by which StemCorrupt breaks spurious correlations, though this may not generalize to languages with necessary morphophonological dependencies

## Next Checks

1. Test whether the mutual information between stem and affix variables decreases empirically as synthetic dataset size increases, rather than just theoretically
2. Evaluate whether uncertainty-based selection remains beneficial when using a model trained on combined gold and synthetic data, rather than just gold data
3. Compare StemCorrupt performance against other data augmentation methods that preserve morphophonological constraints in languages with vowel harmony