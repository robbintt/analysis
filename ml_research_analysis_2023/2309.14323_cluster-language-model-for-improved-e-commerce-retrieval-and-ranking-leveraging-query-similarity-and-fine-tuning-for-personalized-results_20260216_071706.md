---
ver: rpa2
title: 'Cluster Language Model for Improved E-Commerce Retrieval and Ranking: Leveraging
  Query Similarity and Fine-Tuning for Personalized Results'
arxiv_id: '2309.14323'
source_url: https://arxiv.org/abs/2309.14323
tags:
- cluster
- query
- language
- retrieval
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cluster-based language model to enhance
  e-commerce product retrieval accuracy while minimizing additional training overhead.
  The approach fine-tunes a pre-trained language model within query clusters generated
  by K-Means, allowing the model to learn local manifolds and capture query-specific
  nuances.
---

# Cluster Language Model for Improved E-Commerce Retrieval and Ranking: Leveraging Query Similarity and Fine-Tuning for Personalized Results

## Quick Facts
- **arXiv ID**: 2309.14323
- **Source URL**: https://arxiv.org/abs/2309.14323
- **Reference count**: 11
- **Primary result**: Fine-tuning a pre-trained language model within query clusters improves e-commerce product retrieval accuracy, with significant gains in recall@8 and NDCG across all thresholds.

## Executive Summary
This paper introduces a cluster-based language model approach to enhance e-commerce product retrieval accuracy while minimizing additional training overhead. The method clusters queries using K-Means on baseline embeddings, then fine-tunes a pre-trained sentence transformer within each cluster to capture local semantic nuances. By assigning incoming queries to their appropriate clusters and using the corresponding cluster-specific model for retrieval, the approach achieves significant improvements in both recall (up to recall@8) and ranking performance (NDCG) compared to a baseline bi-encoder model.

## Method Summary
The method involves three main stages: first, training a baseline DistilBERT-based sentence transformer on query-product pairs using contrastive learning; second, clustering the training queries with K-Means on the baseline embeddings and recursively fine-tuning the baseline model for each cluster; and third, using the cluster assignment at inference time to select the appropriate cluster language model for retrieval. The relabeling strategy marks products in the Top Product Set as 1 if purchased and 0 if unpurchased before the last purchased item, creating a more discriminative training signal. The approach balances model specialization with computational efficiency by using a moderate number of clusters (N=29) and 5 epochs of fine-tuning per cluster.

## Key Results
- Significant improvement in recall up to retrieval threshold@8 compared to baseline bi-encoder model
- Consistently better ranking performance (NDCG) across all thresholds (k ∈ {1, 2, 4, 8, 12, 24, 100})
- Superior performance in clusters with higher frequency of testing purchased products appearing in training data
- Minimal additional training overhead while achieving substantial accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a global language model within query clusters captures query-specific nuances better than a single global model. By clustering queries and fine-tuning the model on each cluster's subset of data, the model parameters adapt to the local manifold in the feature space, capturing cluster-specific semantic patterns. Core assumption: Queries within the same cluster share similar semantic structures, allowing localized fine-tuning to improve relevance.

### Mechanism 2
Relabeling top products in each cluster using purchased vs. unpurchased status helps the model suppress irrelevant but semantically similar products. The relabeling strategy marks products in the Top Product Set as 1 if purchased and 0 if unpurchased (before the last purchased item). This creates a more discriminative training signal that emphasizes relevance over similarity. Core assumption: Purchased products are better relevance signals than impressed or added-to-cart products for fine-tuning.

### Mechanism 3
K-Means clustering of query embeddings based on the baseline model creates semantically coherent clusters that improve model adaptation. The baseline model generates query embeddings, which are then clustered using K-Means. Each cluster's language model is fine-tuned on queries from that cluster, allowing the model to specialize for that semantic space. Core assumption: The baseline model's embeddings capture sufficient semantic information to form meaningful clusters for downstream fine-tuning.

## Foundational Learning

- **Concept**: Contrastive learning for fine-tuning sentence transformers
  - Why needed here: The baseline and cluster models are fine-tuned using contrastive loss to map similar query-product pairs close and dissimilar pairs far apart in embedding space.
  - Quick check question: What is the role of the margin parameter in the contrastive loss function?

- **Concept**: K-Means clustering for query grouping
  - Why needed here: Clustering queries based on their embeddings allows the model to specialize for different semantic query types, improving retrieval accuracy for diverse query patterns.
  - Quick check question: How does the number of clusters (N) affect the balance between model specialization and computational overhead?

- **Concept**: Normalized Discounted Cumulative Gain (NDCG) for ranking evaluation
  - Why needed here: NDCG measures how well the model ranks relevant products higher in the retrieval list, which is crucial for e-commerce search quality.
  - Quick check question: Why does NDCG discount relevance scores logarithmically based on rank position?

## Architecture Onboarding

- **Component map**: Query → K-Means clustering → Cluster Language Models → ANN search → Ranking
- **Critical path**: Query → K-Means cluster assignment → Load corresponding Cluster LM → Generate embeddings → ANN search → Ranking
- **Design tradeoffs**: 
  - Number of clusters (N): More clusters = better specialization but higher memory/compute cost
  - Fine-tuning epochs: More epochs = better adaptation but risk of overfitting
  - Cluster size threshold: Prevents ineffective fine-tuning on very small or very large clusters
- **Failure signatures**:
  - Poor cluster separation: Check Calinski-Harabasz score or visualize cluster centers
  - Inconsistent performance across clusters: Investigate cluster size distribution and purchased product frequency
  - Slow inference: Monitor time spent in cluster assignment vs. model inference
- **First 3 experiments**:
  1. Test different values of N (number of clusters) and measure impact on recall@N and NDCG@K
  2. Compare relabeling strategy (purchased vs. all positive) against baseline fine-tuning
  3. Evaluate cluster-level performance to identify which clusters benefit most from specialization

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of clusters (N) for the K-Means clustering algorithm to balance performance and memory usage in the Cluster Language Model? The paper discusses the trade-off between a large number of clusters (N = 100) leading to memory issues and a smaller number (N = 29) resulting in 30 clusters with a mean cluster size of 513K, but does not provide a definitive answer on the optimal N.

### Open Question 2
How does the proposed Cluster Language Model perform compared to other state-of-the-art retrieval and ranking models in e-commerce, such as cross-encoder models or ensemble methods? The paper mentions that the proposed method offers an alternative to the popular bi-encoder and cross-encoder combination in semantic search, but does not provide a direct comparison with other state-of-the-art models.

### Open Question 3
How does the Cluster Language Model handle long-tail queries or rare product categories that may not have sufficient training data in their respective clusters? The paper does not explicitly discuss the handling of long-tail queries or rare product categories, though it mentions that the method performs well in clusters with a higher frequency of testing purchased products appearing in training data.

## Limitations
- Memory constraints during inference when using many clusters (N=100+), limiting scalability
- Missing details about exact relabeling rules for top products, particularly the cutoff point for "last purchased product"
- Unclear sensitivity to cluster size distribution and whether N=29 clusters is optimal or specific to the dataset

## Confidence

**Confidence: Low** on the specific relabeling strategy effectiveness due to missing details about the exact cutoff point for "last purchased product" and the number of top products retained per query.

**Confidence: Medium** on the cluster fine-tuning approach due to the assumption that K-Means clustering on baseline embeddings creates semantically coherent clusters, though sensitivity to cluster size distribution is not thoroughly explored.

**Confidence: Medium** on computational efficiency claims. The paper states minimal additional training overhead, but fine-tuning N separate models represents significant computational investment.

## Next Checks

1. **Cluster size sensitivity analysis**: Systematically vary the number of clusters (N ∈ {10, 29, 50, 100}) and measure the trade-off between recall@N and NDCG@K versus computational overhead.

2. **Relabeling strategy ablation study**: Compare three relabeling approaches - (a) baseline fine-tuning on all positive products, (b) purchased-only relabeling as described, and (c) a hybrid approach using purchased products plus top-impressed products.

3. **Cluster-level performance breakdown**: Analyze per-cluster performance metrics to identify which cluster sizes and query types benefit most from fine-tuning.