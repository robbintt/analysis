---
ver: rpa2
title: 'Meta Co-Training: Two Views are Better than One'
arxiv_id: '2311.18083'
source_url: https://arxiv.org/abs/2311.18083
tags:
- co-training
- learning
- views
- training
- meta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Meta Co-Training, a semi-supervised learning
  method that improves upon traditional co-training by addressing the limitations
  of pseudo-labeling. The authors propose using pre-trained models to construct multiple
  views of the data, which are then leveraged in a co-training framework.
---

# Meta Co-Training: Two Views are Better than One

## Quick Facts
- arXiv ID: 2311.18083
- Source URL: https://arxiv.org/abs/2311.18083
- Reference count: 40
- Key outcome: Meta Co-Training achieves state-of-the-art results on ImageNet-10%, reducing error rate by ~4.7% compared to previous approaches

## Executive Summary
Meta Co-Training introduces a novel semi-supervised learning method that improves upon traditional co-training by using pre-trained foundation models to construct multiple views of the data. The approach addresses the limitations of pseudo-labeling by employing a meta pseudo-labeling strategy that trains models to provide better pseudo-labels to each other. This method achieves significant performance improvements on both ImageNet-10% and several fine-grained image classification datasets.

## Method Summary
The method constructs views using pre-trained models (MAE, DINOv2, SwAV, EsViT, CLIP) and applies a co-training framework where two models are trained on different views. Meta Co-Training uses a meta pseudo-labeling approach where each model is trained to provide pseudo-labels that improve the other model's performance. The final predictions are made by combining the outputs of both models. Training uses Adam optimizer with learning rate 1e-4, batch size 4096, and ReduceOnPlateau schedule.

## Key Results
- Achieves state-of-the-art results on ImageNet-10%, reducing error rate by ~4.7% compared to previous approaches
- Outperforms existing methods on multiple fine-grained image classification datasets (Flowers102, Food101, FGVCAircraft, iNaturalist)
- Demonstrates effectiveness of meta pseudo-labeling in improving pseudo-label quality

## Why This Works (Mechanism)

### Mechanism 1
Pre-trained foundation models provide independent and sufficient representations for co-training. These models generate embeddings from different pretext tasks that act as distinct views of the same image. The embeddings capture different aspects of image data and each is sufficient for classification when used with a simple classifier.

### Mechanism 2
Meta pseudo-labeling improves upon standard co-training by training models to provide better pseudo-labels to each other. Instead of directly using pseudo-labels from one model to train the other, each model is optimized to provide pseudo-labels that will improve the other model's performance on the labeled set.

### Mechanism 3
Combining predictions from two independent views using a re-normalized product leverages complementary information. By multiplying predictions from two sufficiently independent views and re-normalizing, the method captures information that individual views might miss, leading to better overall predictions.

## Foundational Learning

- **Semi-supervised learning**: Using unlabeled data to improve classification performance when labeled data is scarce. Quick check: What is the main difference between supervised and semi-supervised learning?
- **Co-training**: Framework combining information from two different views of the data. Quick check: What are the two key assumptions required for co-training to work effectively?
- **Pretext tasks and foundation models**: Training models on pretext tasks to generate views for co-training. Quick check: How do pretext tasks contribute to the effectiveness of foundation models in SSL?

## Architecture Onboarding

- **Component map**: Pre-trained models -> Embeddings -> MLP classifiers -> Meta pseudo-labels -> Combined predictions
- **Critical path**: 1) Generate embeddings from pre-trained models, 2) Train two models on different views using co-training, 3) Use meta pseudo-labels to improve pseudo-label quality, 4) Combine predictions from both models
- **Design tradeoffs**: Using pre-trained models eliminates need for training new models but relies on embedding quality; Meta Co-Training adds complexity but improves performance when views are imbalanced
- **Failure signatures**: Poor performance if views aren't independent or one view is insufficient; degradation if meta-learning step is unstable; overfitting if models train too long without regularization
- **First 3 experiments**: 1) Verify view sufficiency by training MLP on individual views, 2) Compare standard co-training vs meta co-training on small ImageNet subset, 3) Test different pre-trained model combinations as views

## Open Questions the Paper Calls Out

1. How does performance scale with increasing number of views beyond two?
2. How does fine-tuning pre-trained foundation models affect Meta Co-Training performance compared to frozen embeddings?
3. How robust is Meta Co-Training to noisy or incorrect pseudo-labels?

## Limitations

- Paper doesn't provide direct empirical evidence for sufficiency and independence assumptions of views
- Limited comparison with other state-of-the-art semi-supervised learning methods that don't use co-training
- Experiments focus only on two-view co-training, scalability to more views remains unexplored

## Confidence

- **High Confidence**: Clear descriptions of Meta Co-Training method and components; effective experimental results on ImageNet-10% and fine-grained datasets
- **Medium Confidence**: Claims about view sufficiency and independence lack specific empirical evidence
- **Low Confidence**: Limited comparison with non-co-training SSL methods restricts understanding of relative effectiveness

## Next Checks

1. Conduct thorough analysis of view sufficiency and independence by training simple classifiers on individual views and measuring pairwise translation performance
2. Compare Meta Co-Training performance with state-of-the-art semi-supervised learning methods that don't use co-training (FixMatch, UDA)
3. Perform ablation study to evaluate impact of different pre-trained model combinations on Meta Co-Training performance