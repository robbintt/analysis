---
ver: rpa2
title: Investigating the Edge of Stability Phenomenon in Reinforcement Learning
arxiv_id: '2307.04210'
source_url: https://arxiv.org/abs/2307.04210
tags:
- edge
- stability
- learning
- agent
- threshold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the edge of stability phenomenon in reinforcement
  learning (RL), specifically in off-policy Q-learning algorithms across different
  data regimes. The authors conduct experiments using DQN and C51 on the MinAtar environment,
  examining both full-batch and mini-batch settings with gradient descent and momentum.
---

# Investigating the Edge of Stability Phenomenon in Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.04210
- Source URL: https://arxiv.org/abs/2307.04210
- Reference count: 40
- Primary result: DQN exhibits edge of stability in offline RL with Hessian eigenvalues oscillating around quadratic divergence threshold, while C51 shows different dynamics due to cross-entropy loss structure.

## Executive Summary
This paper investigates the edge of stability (EoS) phenomenon in reinforcement learning, specifically examining how different Q-learning algorithms (DQN and C51) exhibit EoS across various data regimes. The authors conduct extensive experiments on the MinAtar environment, analyzing Hessian eigenvalue dynamics during training. Their key finding is that DQN with Huber loss shows strong EoS behavior in offline RL settings where the leading Hessian eigenvalue oscillates around the quadratic divergence threshold, while C51 with cross-entropy loss exhibits different dynamics. The study reveals that the presence and characteristics of EoS in RL depend significantly on both the specific algorithm and loss function used, rather than being a universal phenomenon across all deep RL methods.

## Method Summary
The authors conduct experiments using DQN and C51 algorithms on the MinAtar environment, examining both full-batch and mini-batch settings with gradient descent and momentum. They create three types of offline replay buffers (greedy pre-trained agent, ε-greedy pre-trained agent, and last 10^6 transitions from online pre-trained agent) and compare these with online RL settings. The key metric analyzed is the leading eigenvalue of the Hessian (λ₁) and its relationship to the quadratic divergence threshold. Training procedures use learning rates of 0.01 for GD and 0.01 with β=0.8 for momentum, with batch sizes of 512 for mini-batch and 10^4 for full-batch settings. The authors log λ₁ and loss metrics at regular intervals to track EoS behavior.

## Key Results
- DQN with Huber loss exhibits strong edge of stability in offline RL, with λ₁ rising to and oscillating around the quadratic divergence threshold while loss remains unstable but decreasing
- C51 with cross-entropy loss does not consistently show edge of stability, with λ₁ growing early then decreasing regardless of data regime
- In online RL settings, DQN shows λ₁ plateauing below threshold while C51 can have λ₁ exceed threshold initially before decreasing
- The presence of EoS in RL is highly dependent on both the specific algorithm and loss function, rather than being a universal phenomenon

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DQN with Huber loss exhibits edge of stability in offline RL but not in online RL.
- Mechanism: In offline RL, the static replay buffer allows λ₁ to reach the quadratic divergence threshold and oscillate around it, causing loss instability while still decreasing overall. In online RL, non-stationarity from the agent's own experience prevents λ₁ from reaching the threshold.
- Core assumption: Static replay buffers in offline RL enable Hessian dynamics that push λ₁ to the threshold, while online RL's shifting data distribution dampens this effect.
- Evidence anchors:
  - [abstract] "DQN — using a Huber loss — showing a strong edge of stability effect that we do not observe with C51 — using a cross entropy loss."
  - [section 3.1] "Offline RL: agent trained using experience generated from a greedy pre-trained policy... Figure 1a shows a clear edge of stability effect... Consistent with results using the mean squared error in supervised learning."
- Break condition: If the replay buffer lacks diversity or if the learning rate is too low, λ₁ may never reach the threshold, preventing the edge of stability.

### Mechanism 2
- Claim: C51 with cross-entropy loss does not consistently exhibit edge of stability, but shows early λ₁ growth followed by decrease.
- Mechanism: The cross-entropy loss structure causes λ₁ to grow initially but then decline as training progresses, regardless of data regime. In online RL, λ₁ can exceed the threshold initially before decreasing.
- Core assumption: Cross-entropy loss dynamics inherently dampen large eigenvalues over time, unlike MSE-type losses.
- Evidence anchors:
  - [abstract] "C51 — using a cross entropy loss... does not consistently show the edge of stability behavior, but still exhibits a connection between large leading Hessian eigenvalues and training challenges."
  - [section 3.2] "we observe that λ₁ grows early in training, after which it consistently decreases... In online learning... λ₁ is consistently significantly above the quadratic divergence threshold."
- Break condition: If the learning rate is too high, λ₁ may remain above threshold without the subsequent decrease.

### Mechanism 3
- Claim: Bootstrapping and non-stationary data in RL modify the edge of stability dynamics compared to supervised learning.
- Mechanism: The use of target networks and the shifting data distribution in online RL prevent λ₁ from reaching the threshold, while the bootstrapping error can amplify instabilities when λ₁ is near the threshold.
- Core assumption: Target networks stabilize training but also dampen the sharpness growth that would otherwise push λ₁ to the threshold.
- Evidence anchors:
  - [section 2] "Off-policy deep RL algorithms like DQN and C51 differ from supervised learning both through their objectives—which use bootstrapping—and the data present in the replay buffer R used for learning the agent, which can be non-stationary..."
  - [section 3.1] "we note that while changing the target network can have a short term effect on λ₁, it does not drastically affect it's long term trajectory and the edge of stability phenomenon."
- Break condition: If target network updates are disabled or too frequent, λ₁ may grow unchecked toward the threshold.

## Foundational Learning

- Concept: Hessian eigenvalue analysis and quadratic divergence threshold.
  - Why needed here: The edge of stability phenomenon is defined by λ₁ approaching and oscillating around the threshold 1/(η(2+2β)).
  - Quick check question: Given η=0.01 and β=0.8, what is the quadratic divergence threshold? (Answer: 360)

- Concept: Replay buffer data regimes (offline vs online RL).
  - Why needed here: The paper contrasts different data regimes to isolate the effect of data distribution on optimization dynamics.
  - Quick check question: How does using a replay buffer from a pre-trained agent differ from using the agent's own experience? (Answer: Static vs dynamic data distribution)

- Concept: Loss function types (Huber vs cross-entropy) and their impact on Hessian spectra.
  - Why needed here: Different losses lead to different λ₁ trajectories and edge of stability presence.
  - Quick check question: Why might cross-entropy loss cause λ₁ to decrease over time, unlike MSE? (Answer: Different curvature properties and regularization effects)

## Architecture Onboarding

- Component map:
  DQN/C51 agent with neural network Q-function approximator -> Replay buffer (static for offline, dynamic for online) -> Target network (updated periodically) -> Optimizer (SGD with/without momentum) -> Logging system for loss, λ₁, and return

- Critical path:
  1. Collect/store transitions in replay buffer
  2. Sample minibatch and compute loss
  3. Backpropagate and update Q-network
  4. Periodically update target network
  5. Log λ₁ (via Hessian approximation) and performance metrics

- Design tradeoffs:
  - Full-batch vs mini-batch: Full-batch gives cleaner Hessian estimates but is computationally heavy; mini-batch is noisier but scalable
  - Target network update frequency: More frequent updates stabilize training but may dampen edge of stability effects
  - Replay buffer size: Larger buffers improve sample efficiency but may include stale data in offline settings

- Failure signatures:
  - λ₁ stays far below threshold → No edge of stability observed (e.g., C51 offline)
  - λ₁ spikes above threshold and stays high → Training instability, especially with cross-entropy
  - Loss oscillates wildly without decreasing → Optimization diverging, possibly due to too high learning rate

- First 3 experiments:
  1. Run DQN offline with greedy pre-trained replay buffer; verify λ₁ rises to threshold and oscillates
  2. Run C51 online; check if λ₁ exceeds threshold initially then decreases
  3. Disable target network updates; observe if λ₁ growth is more pronounced and unstable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does DQN exhibit the edge of stability phenomenon in offline RL but not in online RL, while C51 shows the opposite trend?
- Basis in paper: [explicit] The paper notes that DQN shows a strong edge of stability effect in offline RL but not in online RL, while C51 does not consistently show the edge of stability behavior but has large leading Hessian eigenvalues in online RL.
- Why unresolved: The paper does not provide a detailed explanation for these contrasting behaviors between DQN and C51 across different data regimes.
- What evidence would resolve it: Comparative experiments analyzing the Hessian eigenvalues, loss functions, and training dynamics of DQN and C51 in both offline and online settings, with a focus on identifying the factors contributing to these differences.

### Open Question 2
- Question: How does the choice of loss function (Huber vs. cross-entropy) influence the presence and characteristics of the edge of stability phenomenon in RL?
- Basis in paper: [explicit] The paper observes that DQN, using a Huber loss, shows a strong edge of stability effect, while C51, using a cross-entropy loss, does not consistently exhibit this behavior.
- Why unresolved: The paper does not provide a detailed analysis of how different loss functions affect the edge of stability phenomenon in RL.
- What evidence would resolve it: Systematic experiments comparing the edge of stability phenomenon across various loss functions (e.g., Huber, MSE, cross-entropy) in both supervised learning and RL settings.

### Open Question 3
- Question: What is the relationship between the leading eigenvalue of the Hessian and the agent's performance in RL?
- Basis in paper: [inferred] The paper mentions the need to connect the leading eigenvalue of the Hessian to the agent's performance, not only the loss, as has been done in supervised learning with generalization.
- Why unresolved: The paper does not provide a direct analysis of the relationship between the leading Hessian eigenvalue and the agent's performance in RL.
- What evidence would resolve it: Empirical studies correlating the leading Hessian eigenvalue with various performance metrics (e.g., reward, success rate) across different RL algorithms and environments.

## Limitations

- The empirical findings are based on a narrow set of environments (MinAtar's Breakout and Space Invaders) and specific algorithm configurations, limiting generalizability
- The computational cost of Hessian eigenvalue estimation may limit practical applicability to larger-scale RL problems
- The paper does not explore how hyperparameters like learning rate, momentum coefficient, and target network update frequency quantitatively affect the edge of stability dynamics

## Confidence

- High confidence in the observation of edge of stability in offline DQN: Clear and consistent experimental evidence across multiple replay buffer configurations
- Medium confidence in the claim that C51 does not exhibit edge of stability consistently: Less clearly established connection between large leading eigenvalues and training challenges
- Low confidence in the claim that online RL does not exhibit edge of stability: Empirical evidence is less conclusive with limited discussion of underlying mechanisms

## Next Checks

1. Conduct ablation studies varying the learning rate and momentum coefficient to quantify their effect on the edge of stability dynamics, particularly the threshold at which λ₁ oscillates and the frequency of oscillation

2. Implement a scalable approximation method for computing the leading Hessian eigenvalue (e.g., power iteration or randomized SVD) to enable analysis of larger neural networks and more complex environments

3. Compare the edge of stability behavior across a wider range of RL algorithms (e.g., PPO, SAC, Rainbow) and loss functions (e.g., MSE, L2 loss) to assess generalizability and identify common patterns