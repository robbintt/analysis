---
ver: rpa2
title: Multi-perspective Feedback-attention Coupling Model for Continuous-time Dynamic
  Graphs
arxiv_id: '2312.07983'
source_url: https://arxiv.org/abs/2312.07983
tags:
- uni00000013
- uni00000014
- uni00000024
- uni00000026
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Multi-Perspective Feedback-Attention Coupling
  (MPFA) model for continuous-time dynamic graphs. The model addresses the limitations
  of existing methods by incorporating both evolving and raw perspectives to capture
  the interleaved dynamics of graph processes.
---

# Multi-perspective Feedback-attention Coupling Model for Continuous-time Dynamic Graphs

## Quick Facts
- arXiv ID: 2312.07983
- Source URL: https://arxiv.org/abs/2312.07983
- Reference count: 40
- Key outcome: Proposes MPFA model achieving state-of-the-art performance on dynamic link prediction and node classification tasks

## Executive Summary
This paper introduces a Multi-Perspective Feedback-Attention Coupling (MPFA) model for continuous-time dynamic graphs. The model addresses limitations of existing methods by incorporating both evolving and raw perspectives to capture interleaved dynamics of graph processes. Through temporal self-attention and feedback attention mechanisms, MPFA effectively aggregates information from continuously evolving temporal neighbors and raw neighborhood information. Experimental results on seven public datasets demonstrate the model's effectiveness, achieving significant performance improvements over baseline methods.

## Method Summary
MPFA combines two perspectives: an evolving perspective that uses temporal self-attention to aggregate information from continuously evolving temporal neighbors, and a raw perspective that utilizes feedback attention with growth characteristic coefficients to aggregate raw neighborhood information. The model maintains both perspectives through dynamic updates after each event and couples them through attention mechanisms. A two-layer feedforward network fuses outputs from both perspectives, followed by an MLP decoder for final predictions. The model is trained using Binary Cross Entropy loss and evaluated on dynamic link prediction and node classification tasks.

## Key Results
- MPFA achieves state-of-the-art performance on seven public datasets
- On Wikipedia dataset, achieves AUC of 90.67, surpassing second-best method by 2.9 points
- Ablation studies validate importance of dual perspectives and feedback mechanism
- Model demonstrates significant performance improvements across AP, AUC, and ACC metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-perspective approach enables better capture of both long-term and short-term dependencies by preserving both evolving and raw state information.
- Mechanism: The model maintains two separate information streams - evolving information that dynamically updates after each event, and raw information that preserves the original interaction state. These streams are coupled through attention mechanisms to provide complementary views.
- Core assumption: Both the evolving state (current network dynamics) and raw state (original interaction context) contain complementary information necessary for accurate predictions.
- Evidence anchors:
  - [abstract] "MPFA incorporates information from both evolving and raw perspectives"
  - [section] "The evolving perspective employs temporal self-attention... Meanwhile, the raw perspective utilizes a feedback attention module"

### Mechanism 2
- Claim: Temporal self-attention in the evolving perspective allows selective aggregation of relevant temporal neighbors for information processing.
- Mechanism: The model uses query-key-value attention where the query is the target node's evolving information, keys are temporal neighbors' information with edge features, and values are the neighbor embeddings. This allows the model to weigh neighbors differently based on their relevance.
- Core assumption: Not all temporal neighbors contribute equally to understanding the current state; some are more relevant than others.
- Evidence anchors:
  - [section] "For the temporal self-attention, the inputs are hi(t−), the current timepoint t, [h1(t−), ..., hj(t−)], and the edge features"
  - [section] "The query Q(t) = hi(t−)||Φ(0) is the relevant information of the target node"

### Mechanism 3
- Claim: Feedback attention coefficients with growth characteristics enable the model to learn from historical interactions with time-weighted importance.
- Mechanism: The model computes feedback coefficients that grow over time, giving more weight to recent interactions while still considering historical ones. These coefficients are used to aggregate raw neighborhood information.
- Core assumption: The importance of historical interactions should be weighted by both their recency and the model's growing understanding of the network over time.
- Evidence anchors:
  - [section] "We compute feedback coefficients with growth properties... These coefficients are then utilized to aggregate the raw neighborhood information"
  - [section] "ai,j(tj → t) = exp(Ii,j(tj → t))P j′ ∈Ni(t), exp(Ii,j′ (tj → t))"

## Foundational Learning

- Concept: Temporal graph representation learning
  - Why needed here: The paper addresses continuous-time dynamic graphs where nodes and edges evolve over time, requiring specialized techniques beyond static graph methods
  - Quick check question: What is the key difference between discrete-time and continuous-time dynamic graphs?

- Concept: Attention mechanisms in graph neural networks
  - Why needed here: The model uses temporal self-attention and feedback attention to selectively aggregate information from neighbors based on their relevance
  - Quick check question: How does temporal self-attention differ from standard self-attention in static graphs?

- Concept: Dual-perspective modeling
  - Why needed here: The paper introduces both evolving and raw perspectives to capture different aspects of network dynamics, requiring understanding of how to maintain and fuse multiple information streams
  - Quick check question: Why might maintaining both evolving and raw perspectives be beneficial compared to using only one?

## Architecture Onboarding

- Component map:
  - Event occurrence -> Update evolving information -> Compute attention coefficients -> Aggregate information from both perspectives -> Attention coupling -> Prediction

- Critical path: Event occurrence → Update evolving information → Compute attention coefficients → Aggregate information from both perspectives → Attention coupling → Prediction

- Design tradeoffs: The dual-perspective approach increases model complexity but provides better performance; dynamic updates require more computation but enable capturing long-term dependencies with fewer neighbors

- Failure signatures: Poor performance on datasets with very frequent interactions between same node pairs; instability when batch size is too large; underperformance on networks with very few nodes but many edges

- First 3 experiments:
  1. Ablation study comparing MPFA with and without raw perspective on MOOC dataset
  2. Link prediction performance comparison across different numbers of temporal neighbors (1-30)
  3. Hyperparameter sensitivity analysis for batch size, embedding dimension, and number of temporal neighbors on USLegis dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MPFA model perform on networks with a small number of nodes (a few hundred) but millions of edges compared to contemporary models?
- Basis in paper: [explicit] The paper mentions that the model faces challenges when handling networks with a small number of nodes but millions of edges, and its performance tends to be slightly lower than some contemporary models.
- Why unresolved: The paper does not provide specific performance metrics or comparisons for this scenario, leaving the extent of the performance difference unclear.
- What evidence would resolve it: Conducting experiments on networks with a small number of nodes and millions of edges, and comparing the MPFA model's performance to that of contemporary models using appropriate evaluation metrics.

### Open Question 2
- Question: What is the impact of increasing the batch size to a large value on the performance of the MPFA model?
- Basis in paper: [explicit] The paper states that increasing the batch size to a large value results in a decrease in model performance.
- Why unresolved: The paper does not provide specific details on the extent of the performance decrease or the threshold batch size at which this occurs.
- What evidence would resolve it: Conducting experiments with varying batch sizes, including large values, and analyzing the model's performance to determine the relationship between batch size and performance.

### Open Question 3
- Question: How does the evolving perspective's performance compare to the raw perspective in different application scenarios?
- Basis in paper: [inferred] The ablation experiments show that the model with only the evolving perspective demonstrates the second-best performance on the MOOC dataset, while on the USLegis dataset, the model with only the raw perspective achieves the second-best performance.
- Why unresolved: The paper does not provide a comprehensive analysis of the performance differences between the evolving and raw perspectives across various application scenarios.
- What evidence would resolve it: Conducting extensive experiments on diverse datasets and application scenarios, comparing the performance of models with only the evolving perspective, only the raw perspective, and both perspectives combined.

## Limitations
- The dual-perspective approach increases computational complexity, potentially limiting scalability to massive graphs
- The model's performance degrades when handling networks with few nodes but millions of edges
- Large batch sizes lead to decreased model performance, indicating sensitivity to hyperparameter choices

## Confidence

- **High Confidence**: The effectiveness of temporal self-attention for neighbor aggregation and the overall framework's superiority over baseline methods
- **Medium Confidence**: The specific design choices for coupling mechanisms between perspectives and the claimed interpretability benefits
- **Low Confidence**: The generalizability of the growth characteristic coefficients across diverse temporal patterns and the model's scalability to massive graphs

## Next Checks

1. **Temporal Pattern Robustness Test**: Evaluate MPFA on synthetic datasets with controlled temporal patterns (periodic, bursty, decaying) to verify whether the growth characteristic coefficients adapt appropriately across different temporal dynamics.

2. **Scalability Benchmark**: Implement MPFA on graphs with 10× more nodes/edges than current datasets to measure performance degradation and identify bottlenecks in the dual-perspective computation pipeline.

3. **Cross-Domain Transferability**: Train MPFA on one domain (e.g., social networks) and test zero-shot on a structurally similar but semantically different domain (e.g., citation networks) to assess whether the dual-perspective mechanism provides domain-agnostic benefits.