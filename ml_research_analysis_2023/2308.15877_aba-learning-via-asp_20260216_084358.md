---
ver: rpa2
title: ABA Learning via ASP
arxiv_id: '2308.15877'
source_url: https://arxiv.org/abs/2308.15877
tags:
- learning
- guilty
- innocent
- rule
- mary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an ASP-based method for implementing ABA Learning,
  a symbolic machine learning approach for generating Assumption-Based Argumentation
  frameworks from background knowledge and positive/negative examples. The core method,
  called ASP-ABAlearn, combines transformation rules (Rote Learning, Folding, Assumption
  Introduction, and Subsumption) with Answer Set Programming to guide both Rote Learning
  and generalisation in ABA Learning.
---

# ABA Learning via ASP

## Quick Facts
- arXiv ID: 2308.15877
- Source URL: https://arxiv.org/abs/2308.15877
- Reference count: 21
- Primary result: Presents an ASP-based method for implementing ABA Learning to generate Assumption-Based Argumentation frameworks from background knowledge and examples

## Executive Summary
This paper introduces ASP-ABAlearn, a novel method for implementing ABA Learning using Answer Set Programming (ASP) to guide both Rote Learning and generalization in generating Assumption-Based Argumentation frameworks. The approach combines transformation rules (Rote Learning, Folding, Assumption Introduction, and Subsumption) with ASP encodings to automate entailment checking and generate new facts for contraries of assumptions. The method focuses on learning intensional solutions and leverages the correspondence between stable extensions in logic programming ABA frameworks and answer set programs. A proof-of-concept implementation using SWI-Prolog and Clingo is under development.

## Method Summary
The ASP-ABAlearn strategy implements ABA Learning by using Answer Set Programming to guide Rote Learning and generalization transformations. The method transforms the ABA framework and examples into ASP rules, then uses ASP solvers to compute cautious consequences and check entailment conditions automatically. Key transformation rules include Rote Learning (adding suitable facts), Folding (inverse resolution to generalize rules), Assumption Introduction (adding defeasible rules with exceptions), and Subsumption (removing redundant rules). The approach specifically targets flat ABA frameworks and aims to produce intensional solutions containing intentional rules rather than just facts.

## Key Results
- Proposes a novel ASP-based method for ABA Learning that combines transformation rules with automated entailment checking
- Demonstrates the method on illustrative examples showing how folding and assumption introduction create intensional solutions
- Establishes correspondence between stable extensions in logic programming ABA frameworks and answer set programs for flat frameworks
- Outlines a proof-of-concept implementation using SWI-Prolog and Clingo that is currently under development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ASP encoding of ABA frameworks enables automated entailment checking and generalization.
- Mechanism: By transforming the ABA framework and examples into Answer Set Programming rules, the system can use ASP solvers to compute cautious consequences and check entailment conditions automatically.
- Core assumption: The correspondence between stable extensions in logic programming ABA frameworks and answer set programs holds for the restricted class of flat ABA frameworks considered.
- Evidence anchors:
  - [abstract]: "We propose a novel method for implementing ABA Learning using Answer Set Programming as a way to help guide Rote Learning and generalisation in ABA Learning."
  - [section 4]: "By computing C (ASP+(⟨R, A , ⟩,⟨E +, E−⟩, K )) we generate facts, if at all possible, for contraries of assumptions in K that, when added to R, enable the entailment of the examples in ⟨E +, E−⟩."
  - [corpus]: Weak evidence - the paper mentions ASP is used but doesn't provide experimental results showing effectiveness.
- Break condition: The mechanism fails if the ASP encoding doesn't preserve the semantics of the original ABA framework, or if the ASP solver cannot handle the generated programs efficiently.

### Mechanism 2
- Claim: The folding transformation rule generalizes non-intensional rules into intensional ones.
- Mechanism: Folding replaces some atoms in a rule body with their "consequences" using other rules, effectively performing inverse resolution to create more general rule schemata.
- Core assumption: The background knowledge contains rules that can be used to replace specific atoms with more general patterns.
- Evidence anchors:
  - [section 4]: "Folding is a form of inverse resolution [9], which generalises a rule by replacing some atoms in its body with their 'consequence' using a rule in R."
  - [example 4]: Shows how folding transforms innocent(X)← X = bob using away(X)← X = bob to get innocent(X)← away(X).
  - [corpus]: Weak evidence - only one example is provided, and no quantitative analysis of generalization effectiveness.
- Break condition: The mechanism fails if folding creates rules that no longer entail the required examples, or if no suitable rules exist in the background knowledge for the replacement.

### Mechanism 3
- Claim: Assumption introduction combined with rote learning creates defeasible rules that capture exceptions.
- Mechanism: Assumption introduction adds new assumptions to rules, then rote learning generates facts for the contraries of these assumptions to handle exceptions, creating a complete defeasible rule with exceptions.
- Core assumption: The examples contain both positive cases and exceptions that can be captured by adding assumptions and their contraries.
- Evidence anchors:
  - [section 4]: "Assumption Introduction replaces a rule ρ1 : H← B in R by ρ2 : H← B, α(X), where X = vars(H)∪ vars(B) and α(X) is a new assumption with contrary c α(X)"
  - [example 5]: Demonstrates adding assumption a(X,Y) to guilty(X)← witness con(X,Y), person(Y) and generating c a(mary, alex) to handle the exception.
  - [corpus]: Weak evidence - the example is illustrative but doesn't show how well this captures complex exception patterns.
- Break condition: The mechanism fails if the generated assumptions and contraries don't correctly capture the exceptions in the data, or if the resulting framework becomes too complex to manage.

## Foundational Learning

- Concept: Stable extensions in argumentation frameworks
  - Why needed here: The learning method relies on stable extensions to determine which arguments are acceptable and should be entailed by the learned framework.
  - Quick check question: Can you explain why a stable extension must attack all arguments it doesn't contain?

- Concept: Cautious reasoning vs. credulous reasoning
  - Why needed here: The paper specifically uses cautious reasoning to determine consequences, meaning an atom is a consequence only if it appears in ALL stable extensions.
  - Quick check question: What's the difference between cautious and credulous consequences in argumentation frameworks?

- Concept: Inverse resolution in logic program transformation
  - Why needed here: The folding transformation rule is described as a form of inverse resolution, which generalizes rules by replacing specific atoms with more general ones using other rules.
  - Quick check question: How does inverse resolution differ from standard resolution in logic programming?

## Architecture Onboarding

- Component map: Background ABA framework (R, A, ⟩) -> Input positive/negative examples E+, E- -> ASP-ABAlearn strategy (RoLe + GEN procedures) -> Transformations (Rote Learning, Folding, Assumption Introduction, Subsumption) -> Output: Learned intensional ABA framework (R', A', ⟩')

- Critical path: RoLe → Initial fact generation → GEN → Folding → Assumption Introduction + Rote Learning → Subsumption → Intensional solution

- Design tradeoffs:
  - Using ASP enables automated entailment checking but adds solver overhead
  - Restricting to flat ABA frameworks simplifies the encoding but limits expressiveness
  - Focusing on intensional solutions promotes generalization but may miss specific facts needed for some examples
  - Non-deterministic folding requires exploration strategies that may affect completeness

- Failure signatures:
  - Unsatisfiable ASP programs indicate no solution exists with current transformations
  - Failure to entail positive examples after transformations suggests folding/assumption introduction isn't working correctly
  - Entailing negative examples indicates the learned framework is too general
  - Excessive rule generation without convergence suggests the search strategy needs refinement

- First 3 experiments:
  1. Run ASP-ABAlearn on the innocent/guilty example (Example 2-7) and verify it produces the expected intensional solution with rules for innocent(X)← away(X) and guilty(X)← witness con(X,Y), person(Y), a(X,Y).
  2. Test the RoLe procedure alone on a simple example to verify it correctly generates the initial facts needed to entail positive examples.
  3. Test the folding transformation on a simple background knowledge with obvious generalization opportunities to verify it correctly creates more general rule schemata.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions is ASP-ABAlearn complete, i.e., guaranteed to terminate and find a solution if one exists?
- Basis in paper: [explicit] The authors explicitly state "further work is needed to investigate conditions under which ASP-ABAlearn is complete"
- Why unresolved: The paper does not provide theoretical analysis of termination conditions or completeness guarantees for the transformation strategy
- What evidence would resolve it: Formal proofs showing when termination is guaranteed, or counterexamples demonstrating cases where the method may fail to find existing solutions

### Open Question 2
- Question: How does the non-determinism in Folding affect solution quality and computational efficiency?
- Basis in paper: [explicit] The authors note "the application of Folding is non-deterministic" and "Currently, we simply make use of a bound to limit the number of alternatives"
- Why unresolved: The paper mentions using a bound but doesn't analyze how different choices affect outcomes or whether smarter selection strategies could improve performance
- What evidence would resolve it: Empirical studies comparing different Folding selection strategies and their impact on solution quality and runtime

### Open Question 3
- Question: How does ASP-ABAlearn compare empirically to non-monotonic ILP systems like ILASP and Fold?
- Basis in paper: [explicit] The authors state "we also plan to perform an experimental comparison to non-monotonic ILP systems (such as Fold [17] and ILASP [8])"
- Why unresolved: This is acknowledged as future work; no experimental results are provided
- What evidence would resolve it: Comprehensive benchmark experiments measuring accuracy, generalization, and efficiency across multiple learning tasks

## Limitations

- The restriction to flat ABA frameworks significantly constrains expressiveness and may limit real-world applicability
- The non-deterministic nature of folding transformations introduces potential completeness issues without clear strategies for handling alternatives
- Complete absence of experimental validation with quantitative results, relying only on illustrative examples

## Confidence

- High: The correspondence between stable extensions and answer sets for flat ABA frameworks
- Medium: The effectiveness of the transformation rules (Rote Learning, Folding, Assumption Introduction) based on single illustrative examples
- Low: The overall learning strategy's ability to scale and find solutions for complex examples

## Next Checks

1. Implement the complete proof-of-concept with SWI-Prolog and Clingo and run it on the innocent/guilty example to verify it produces the expected intensional solution with two rules as shown in the paper.
2. Test the ASP encodings (ASP, ASP+, ASP*) independently to verify they correctly compute cautious consequences and generate contraries for assumptions, using small test ABA frameworks.
3. Evaluate the learning method on a benchmark set of ABA framework learning problems with varying complexity to assess scalability and solution quality compared to baseline approaches.