---
ver: rpa2
title: Optimizing PatchCore for Few/many-shot Anomaly Detection
arxiv_id: '2307.10792'
source_url: https://arxiv.org/abs/2307.10792
tags:
- performance
- feature
- data
- few-shot
- mvtec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work optimizes PatchCore, the current state-of-the-art full-shot
  anomaly detection method, for the few/many-shot settings. The authors hypothesize
  that performance improvements can be achieved by optimizing hyperparameters and
  transferring techniques from supervised few-shot learning.
---

# Optimizing PatchCore for Few/many-shot Anomaly Detection

## Quick Facts
- arXiv ID: 2307.10792
- Source URL: https://arxiv.org/abs/2307.10792
- Reference count: 40
- Primary result: State-of-the-art few-shot anomaly detection results on VisA achieved by optimizing PatchCore hyperparameters

## Executive Summary
This work adapts PatchCore, the current state-of-the-art full-shot anomaly detection method, for few/many-shot settings by optimizing hyperparameters and transferring techniques from supervised few-shot learning. Through systematic experiments on VisA and MVTec AD datasets, the authors demonstrate that careful tuning of feature extractors, input resolution, anti-aliasing, and data augmentation can significantly improve few-shot AD performance. The results show that anti-aliased WideResNet50 performs best overall, and that image-level augmentations must be carefully tuned per dataset to avoid performance degradation. This work establishes new state-of-the-art results for few-shot anomaly detection and identifies future research directions in feature extractors with strong inductive biases.

## Method Summary
The authors optimize PatchCore for few/many-shot anomaly detection by systematically evaluating different feature extractors (WideResNet50, EfficientNetB4, ConvNext, and anti-aliased variants), input resolutions (0.5x to 2.0x), and data augmentation strategies on MVTec AD and VisA datasets. They fix the number of nearest neighbors to 1 and use 5 random seeds per category for statistical significance. The method involves extracting patch features from normal training images using pre-trained feature extractors, constructing a memory bank through coreset subsampling, and computing anomaly scores via k-NN search. Performance is evaluated using AUROC, PR curves, HPROC, and AUHPROC metrics across k-shot settings (1, 2, 4, 5, 10, 25, 50 samples).

## Key Results
- Anti-aliased WideResNet50 achieves best average performance across few-shot settings
- Input resolution optimization (typically 1.5x-2.0x) consistently improves performance
- Dataset-specific data augmentation can improve performance but requires careful tuning
- State-of-the-art few-shot anomaly detection results achieved on VisA benchmark
- Significant performance improvements over default full-shot PatchCore configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing PatchCore hyperparameters for the few-shot setting yields better AD performance than using default full-shot configurations.
- Mechanism: Full-shot configurations overfit to large normal datasets; few-shot settings require adjusted feature extractors, higher resolution, and careful augmentation tuning to generalize from minimal normal samples.
- Core assumption: Performance gains from full-shot tuning do not transfer directly to few-shot regimes due to distributional differences.
- Evidence anchors:
  - [abstract] "We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain."
  - [section] "Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) significant performance improvements can be realized by optimizing hyperparameters such as the underlying feature extractor..."
  - [corpus] Weak signal: 0 citations, no direct overlap with hyperparameter tuning studies in few-shot AD.
- Break condition: If feature extractor choice and resolution tuning do not improve AUROC in few-shot regimes, indicating that full-shot defaults are adequate.

### Mechanism 2
- Claim: Anti-aliasing improves few-shot AD performance by enforcing translation equivariance, which compensates for the lack of abundant normal data.
- Mechanism: Anti-aliased CNNs restore translation equivariance, providing stronger inductive bias that helps generalize with limited training samples.
- Core assumption: Stronger inductive bias reduces the amount of data needed to learn meaningful representations.
- Evidence anchors:
  - [abstract] "Increasing the inductive bias of convolutional neural networks (CNNs) by enforcing translation equivariance improves performance, revealing a potential avenue for future research."
  - [section] "Anti-aliased WideResNet50 performs best on average compared to the other backbones... one should perform an in-depth comparison of more constrained feature extractors here, and furthermore include rotation-equivariant feature extractors..."
  - [corpus] Weak signal: 0 citations, no direct overlap with anti-aliasing in few-shot AD.
- Break condition: If anti-aliased models underperform compared to standard CNNs in few-shot regimes, suggesting the inductive bias is too restrictive.

### Mechanism 3
- Claim: Data augmentation improves few-shot AD performance but must be carefully tuned per dataset to avoid performance degradation.
- Mechanism: Augmentation introduces variability that mimics a larger dataset, but inappropriate augmentations (e.g., flipping for orientation-sensitive categories) can harm performance.
- Core assumption: Augmentation benefits outweigh potential negative impacts when tuned appropriately.
- Evidence anchors:
  - [abstract] "Image-level augmentations can, but are not guaranteed, to improve performance."
  - [section] "Table 3 shows that data augmentation can actually decrease performance on MVTec AD... it seems that there is a benefit to be gained by using augmentations in the few to many-shot AD/AS settings."
  - [corpus] Weak signal: 0 citations, no direct overlap with dataset-specific augmentation studies in few-shot AD.
- Break condition: If any augmentation combination consistently degrades performance across datasets, indicating that data augmentation is not beneficial for this task.

## Foundational Learning

- Concept: Nearest Neighbor (NN) search in feature space
  - Why needed here: PatchCore uses k-NN search to compare patch features against a memory bank of normal samples.
  - Quick check question: How does the choice of k affect anomaly detection sensitivity and specificity?

- Concept: Translation equivariance in CNNs
  - Why needed here: Anti-aliasing restores translation equivariance, which is crucial for handling variations in object positioning with limited training data.
  - Quick check question: What is the effect of breaking translation equivariance on the feature representation of shifted objects?

- Concept: Coreset subsampling
  - Why needed here: Coreset selection reduces memory bank size while maintaining performance, especially important when combining with data augmentation.
  - Quick check question: How does the coreset size affect the trade-off between inference speed and anomaly detection accuracy?

## Architecture Onboarding

- Component map:
  - Pre-trained Feature Extractor -> Memory Bank -> Coreset Selector -> Query Processor -> Anomaly Scorer

- Critical path:
  1. Extract patch features from normal training images using pre-trained feature extractor
  2. Apply data augmentation to training images
  3. Construct memory bank using coreset subsampling
  4. For each query image, extract patch features and compute distance to memory bank
  5. Aggregate patch-level scores to image-level anomaly score

- Design tradeoffs:
  - Feature Extractor Complexity vs. Generalization: More complex models may overfit with few samples
  - Input Resolution vs. Computational Cost: Higher resolution improves detail capture but increases computation
  - Augmentation Strength vs. Dataset Specificity: Strong augmentations may harm performance if not dataset-specific
  - Coreset Size vs. Performance: Smaller coreset speeds inference but may lose discriminative power

- Failure signatures:
  - Degraded performance with higher input resolution: Suggests feature extractor cannot leverage fine details
  - Performance drop with augmentation: Indicates inappropriate augmentations for dataset characteristics
  - Inconsistent performance across seeds: Points to instability in coreset selection or augmentation

- First 3 experiments:
  1. Evaluate PatchCore with default hyperparameters on few-shot setting to establish baseline
  2. Test different feature extractors (Anti-aliased WideResNet50, ConvNext, EfficientNet) at original resolution
  3. Apply dataset-specific augmentations to few-shot training set and evaluate performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the trend that architectures with better ImageNet performance transfer better to few-shot AD/AS tasks hold true for all types of anomalies (structural/textual vs. semantic/logical)?
- Basis in paper: [explicit] The authors state that the trend is not confirmed in the few and many-shot AD regimes, but they do not perform a root-cause analysis to identify the underlying reasons.
- Why unresolved: The authors only tested a limited set of feature extractors and did not investigate the performance across different types of anomalies.
- What evidence would resolve it: A comprehensive study comparing the performance of various feature extractors on datasets containing different types of anomalies would help determine if the trend holds true for all anomaly types.

### Open Question 2
- Question: Can image-level augmentations improve few-shot AD/AS performance across all datasets, or is their effectiveness dataset-specific?
- Basis in paper: [explicit] The authors found that image-level augmentations can improve few-shot performance overall, but must be carefully tuned to do so, and the performance boosts vary strongly between datasets.
- Why unresolved: The authors only tested a limited set of augmentations and did not investigate their effectiveness across a wide range of datasets.
- What evidence would resolve it: A study evaluating the performance of various image-level augmentations across multiple datasets would help determine if their effectiveness is dataset-specific or if they can improve performance across all datasets.

### Open Question 3
- Question: Does enforcing translation equivariance in CNNs improve few-shot AD/AS performance across all feature extractors, or is its effectiveness feature extractor-specific?
- Basis in paper: [explicit] The authors found that enforcing translation equivariance improves performance, but they only tested it on a limited set of feature extractors.
- Why unresolved: The authors did not investigate the effectiveness of translation equivariance across a wide range of feature extractors.
- What evidence would resolve it: A study evaluating the performance of translation-equivariant feature extractors across multiple datasets and feature extractors would help determine if its effectiveness is feature extractor-specific or if it can improve performance across all feature extractors.

## Limitations

- Limited comparison with state-of-the-art few-shot meta-learning approaches, only simple augmentation techniques were tested
- Anti-aliasing effectiveness not evaluated on non-convolutional architectures like Vision Transformers
- The investigation of rotation-equivariant networks was suggested but not implemented as part of the study

## Confidence

- High confidence for hyperparameter optimization benefits and anti-aliasing effectiveness
- Medium confidence for augmentation findings due to dataset-specific variability and limited ablation studies
- Low confidence for generalization to non-convolutional architectures and rotation-equivariant networks

## Next Checks

1. Test PatchCore with rotation-equivariant feature extractors as suggested by the authors to validate the potential of stronger inductive biases
2. Evaluate augmentation strategies on a larger benchmark of few-shot anomaly detection datasets to confirm dataset-specific effects
3. Compare against state-of-the-art few-shot meta-learning approaches to establish relative performance gains