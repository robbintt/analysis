---
ver: rpa2
title: 'Towards Few-Annotation Learning for Object Detection: Are Transformer-based
  Models More Efficient ?'
arxiv_id: '2310.19936'
source_url: https://arxiv.org/abs/2310.19936
tags:
- teacher
- labeled
- object
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of transformer-based
  object detectors in few-annotation learning (FAL) settings for object detection.
  The authors observe that while transformer-based detectors outperform traditional
  convolution-based models in few-shot learning (FSL), they fail to converge when
  used with existing semi-supervised object detection (SSOD) methods.
---

# Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?

## Quick Facts
- arXiv ID: 2310.19936
- Source URL: https://arxiv.org/abs/2310.19936
- Reference count: 39
- Primary result: Proposes MT-DETR, an SSOD approach for transformer-based detectors using raw soft pseudo-labels and cosine EMA, achieving SotA performance in FAL settings on COCO and VOC

## Executive Summary
This paper investigates the effectiveness of transformer-based object detectors in few-annotation learning (FAL) settings. The authors observe that while transformer-based detectors like Deformable DETR outperform traditional CNN models in few-shot learning, they fail to converge when combined with existing semi-supervised object detection methods. To address this, they propose Momentum Teaching DETR (MT-DETR), a student-teacher framework that uses raw soft pseudo-labels without post-processing heuristics and employs cosine-scheduled exponential moving average for teacher updates. Their method achieves state-of-the-art performance on COCO and Pascal VOC datasets, particularly when annotations are scarce.

## Method Summary
MT-DETR is a semi-supervised object detection approach designed specifically for transformer-based architectures. It employs a student-teacher framework where both models are initialized from a Deformable DETR backbone fine-tuned on limited labeled data. The teacher model is updated via cosine-scheduled exponential moving average of the student's weights. Unlike previous methods, MT-DETR generates pseudo-labels from raw softmax outputs without non-maximum suppression or confidence thresholding. The training process involves supervised loss on labeled data and unsupervised consistency loss on unlabeled data, using strong and weak augmentations respectively. The method is evaluated on COCO and Pascal VOC datasets under various FAL settings.

## Key Results
- Transformer-based detectors (Deformable DETR) outperform traditional CNN models (Faster R-CNN) in few-shot learning settings
- MT-DETR achieves state-of-the-art performance in few-annotation learning, especially with scarce annotations
- Raw soft pseudo-labels without post-processing heuristics outperform thresholded pseudo-labels in transformer-based SSOD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Raw soft pseudo-labels outperform hard labels and confidence-thresholded pseudo-labels in FAL settings for transformer-based detectors
- Mechanism: Soft pseudo-labels retain the full predicted class probability distribution, allowing the student model to learn nuanced class relationships without being biased toward high-confidence predictions
- Core assumption: The model benefits from learning from uncertain or low-confidence predictions in low-data regimes
- Evidence anchors:
  - Abstract: "Unlike previous methods, MT-DETR avoids using post-processing heuristics like non-maximum suppression and confidence thresholding, instead relying on raw soft pseudo-labels"
  - Section: "Using the full distributions makes the model less prone to focus on being highly confident in their predictions, and forces the model to take into account the relations between classes"
- Break condition: If the teacher model produces extremely noisy predictions, soft labels may introduce harmful noise

### Mechanism 2
- Claim: Cosine-scheduled EMA stabilizes the teacher model more effectively than constant EMA in FAL
- Mechanism: A cosine schedule gradually reduces the teacher's reliance on the student, allowing early adaptation while preventing late-stage divergence
- Core assumption: Teacher stability is critical when few labeled samples are available
- Evidence anchors:
  - Section: "Inspired by the Self-supervised learning literature... we update α following a cosine scheduling... This scheduling stabilizes the teacher model, especially in the last training iterations, to make it converge at the end of training"
  - Section: Ablation shows 0.7 p.p. improvement with cosine vs. constant scheduling
- Break condition: If the initial EMA rate is too high, convergence may be delayed

### Mechanism 3
- Claim: Deformable DETR outperforms Faster R-CNN in FSL, making it a better base for FAL
- Mechanism: Deformable DETR's sparse attention reduces computational cost and improves convergence speed, while avoiding handcrafted heuristics aligns naturally with raw pseudo-label use
- Core assumption: Transformer-based architectures generalize better from few samples
- Evidence anchors:
  - Section: "We can see that Deformable DETR... achieves consistently better performance than the most popular two-stage method in FSL"
  - Table: Quantitative comparison showing ~3 p.p. improvement of Def. DETR over FRCNN across COCO and VOC datasets
- Break condition: If the number of object queries is insufficient for complex scenes, performance may degrade

## Foundational Learning

- Concept: Semi-supervised object detection via teacher-student consistency
  - Why needed here: FAL combines limited labeled data with large unlabeled sets; teacher-student allows knowledge transfer without expensive annotations
  - Quick check question: What is the role of the teacher model in consistency regularization?

- Concept: Hungarian matching for bipartite assignment between predictions and ground truth/pseudo-labels
  - Why needed here: Transformer-based detectors like DETR do not rely on anchors or NMS; Hungarian matching ensures correct alignment for loss computation
  - Quick check question: How does the matching cost combine class and box components?

- Concept: Softmax probability distributions as supervision targets
  - Why needed here: Enables learning from uncertain predictions and multi-label confidences rather than single "best guess" labels
  - Quick check question: What advantage does a soft pseudo-label have over a hard pseudo-label in few-shot scenarios?

## Architecture Onboarding

- Component map: ResNet-50 backbone -> Deformable DETR encoder/decoder -> Hungarian matching -> Losses (Focal, ℓ1, GIoU, cross-entropy)
- Critical path:
  1. Forward labeled batch → supervised loss (Hungarian matching)
  2. Forward unlabeled batch (weak aug) → teacher soft pseudo-labels
  3. Forward unlabeled batch (strong aug) → student predictions
  4. Hungarian matching pseudo-labels ↔ student
  5. Backward step: weighted sum of supervised + unsupervised loss
  6. EMA update of teacher
- Design tradeoffs:
  - Soft pseudo-labels increase diversity but may introduce noise
  - No confidence threshold avoids early filtering but risks noisy supervision
  - Cosine EMA requires tuning of start/end rates; constant EMA is simpler but less stable
- Failure signatures:
  - Divergence after initial improvement: likely teacher instability or too aggressive EMA
  - Low performance despite convergence: teacher pseudo-labels may be too noisy
  - Overfitting to labeled data: too high λu or insufficient augmentation
- First 3 experiments:
  1. Reproduce FSL baseline (Def. DETR on 1% COCO) to verify improvement margin
  2. Run MT-DETR with raw vs. thresholded pseudo-labels on 1% COCO to confirm ablation
  3. Test constant vs. cosine EMA on 1% COCO to measure stability impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of transformer-based object detectors scale with increasing amounts of labeled data in semi-supervised settings?
- Basis in paper: [explicit] The authors observe that transformer-based detectors outperform traditional convolution-based models in few-shot learning, but fail to converge when used with existing semi-supervised object detection methods
- Why unresolved: The paper focuses on the few-annotation learning setting and does not explore how transformer-based detectors perform as the amount of labeled data increases
- What evidence would resolve it: Experiments comparing the performance of transformer-based detectors with traditional models across various levels of labeled data in semi-supervised settings

### Open Question 2
- Question: What are the specific architectural or algorithmic modifications needed to make transformer-based object detectors compatible with semi-supervised learning methods?
- Basis in paper: [explicit] The authors propose Momentum Teaching DETR (MT-DETR) to address the issue of transformer-based detectors failing to converge in semi-supervised settings
- Why unresolved: The paper presents a solution but does not provide a comprehensive analysis of the underlying reasons for the incompatibility or explore other potential solutions
- What evidence would resolve it: Detailed analysis of the differences between transformer-based and traditional detectors in the context of semi-supervised learning, along with a comparison of various proposed solutions

### Open Question 3
- Question: How does the performance of transformer-based object detectors in semi-supervised settings compare to that of traditional models when using different types of data augmentation?
- Basis in paper: [inferred] The authors use various data augmentation techniques in their experiments, suggesting that the choice of augmentation could impact the performance of different detector architectures
- Why unresolved: The paper does not explicitly compare the effects of different augmentation strategies on transformer-based versus traditional detectors in semi-supervised settings
- What evidence would resolve it: Experiments comparing the performance of both types of detectors using a range of data augmentation techniques in semi-supervised learning scenarios

## Limitations

- The claimed superiority of raw soft pseudo-labels over thresholded pseudo-labels is based on internal ablation studies without external validation
- The 0.7 p.p. improvement from cosine EMA scheduling is measured only within their own ablation framework, with no comparison to other scheduling strategies
- The avoidance of post-processing heuristics could lead to degraded performance on datasets with high class overlap or small objects where NMS traditionally helps

## Confidence

- High confidence in the overall effectiveness of MT-DETR in FAL settings, supported by quantitative results on COCO and VOC
- Medium confidence in the specific mechanisms (soft labels, cosine EMA) due to limited ablation scope and lack of comparison with alternative approaches
- Low confidence in the generalizability of the "transformers need raw soft labels" hypothesis without testing on more diverse datasets and detector architectures

## Next Checks

1. Test MT-DETR with confidence-thresholded pseudo-labels and NMS on 1% COCO to quantify the claimed benefit of raw soft labels
2. Compare cosine EMA against other scheduling strategies (linear, step decay) on 1% COCO to isolate the scheduling effect
3. Validate the FSL advantage of Deformable DETR over Faster R-CNN across multiple FSL ratios (0.5%, 2%, 5%) on COCO to confirm consistency