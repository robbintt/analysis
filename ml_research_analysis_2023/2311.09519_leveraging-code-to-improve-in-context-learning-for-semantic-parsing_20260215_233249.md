---
ver: rpa2
title: Leveraging Code to Improve In-context Learning for Semantic Parsing
arxiv_id: '2311.09519'
source_url: https://arxiv.org/abs/2311.09519
tags:
- python
- list
- state
- name
- returns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving in-context learning
  (ICL) for semantic parsing, where LLMs need to learn to parse rare domain-specific
  languages (DSLs) from just a few demonstrations. The core method involves using
  general-purpose programming languages (like Python) instead of DSLs and augmenting
  prompts with structured domain descriptions (DDs) that include available classes,
  functions, and their signatures.
---

# Leveraging Code to Improve In-context Learning for Semantic Parsing

## Quick Facts
- arXiv ID: 2311.09519
- Source URL: https://arxiv.org/abs/2311.09519
- Reference count: 40
- Primary result: Python prompts with domain descriptions achieve up to 66.5% execution accuracy on compositional splits, dramatically outperforming DSL-based approaches

## Executive Summary
This paper addresses the challenge of in-context learning (ICL) for semantic parsing, where large language models must learn to parse rare domain-specific languages (DSLs) from just a few demonstrations. The authors propose using general-purpose programming languages (like Python) instead of DSLs and augmenting prompts with structured domain descriptions that include available classes, functions, and their signatures. This approach leverages LLMs' existing coding knowledge and provides explicit domain-specific information. The results show significant improvements in execution accuracy across three datasets (GeoQuery, Overnight, SMCalFlow), with particularly dramatic gains on compositional splits where traditional DSL approaches struggle.

## Method Summary
The method involves constructing prompts that include a domain description (DD) outlining available operators and type signatures, followed by demonstrations showing how to map natural language utterances to executable programs. Instead of using the target DSL, the approach uses general-purpose programming languages like Python or JavaScript. The domain description provides structured information about available classes, methods, and their signatures, helping the model navigate the solution space. The generated programs are executed in an environment that evaluates their correctness against the original natural language query. The approach is evaluated across three semantic parsing datasets with different complexity levels and program characteristics.

## Key Results
- Python prompts with full domain descriptions achieve 66.5% execution accuracy on SMCalFlow compositional split (vs 7.9% for DSLs)
- Across all datasets, Python prompts with Full DD consistently outperform DSL-based prompts
- The approach nearly closes the performance gap between i.i.d. and compositional splits
- Python prompts require fewer demonstrations to achieve high accuracy compared to DSL approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Programming languages provide structural familiarity that reduces the learning burden during in-context learning.
- Mechanism: LLMs have extensive pre-training exposure to general-purpose code syntax, patterns, and idioms. When prompted with Python/JavaScript instead of domain-specific languages, the model can leverage existing parametric knowledge rather than learning new syntax from scratch.
- Core assumption: The LLM's pre-training corpus contains sufficient general-purpose code examples to provide meaningful prior knowledge for semantic parsing tasks.
- Evidence anchors: [abstract]: "learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs"; [section 4]: "PLs are prevalent in pre-training corpora; by prompting LLMs to generate PLs rather than DSLs, LLMs can leverage their existing coding knowledge"

### Mechanism 2
- Claim: Domain descriptions provide explicit type information and available operators, reducing ambiguity in the output space.
- Mechanism: By providing structured information about available classes, methods, and their signatures, the model can more efficiently navigate the solution space without trial-and-error exploration.
- Core assumption: The domain description is complete and accurate enough to cover all necessary operators for the task.
- Evidence anchors: [section 4]: "We propose an intuitive solution: providing the model with a Domain Description (DD) outlining the available operators"; [section 6.1]: "DDs are indeed utilized more effectively with PLs than with DSLs"

### Mechanism 3
- Claim: Programming languages allow intermediate variable assignment, enabling step-by-step problem decomposition.
- Mechanism: The ability to break down programs into multiple lines with intermediate results reduces cognitive load and allows more complex reasoning within the model's context window.
- Core assumption: The LLM's architecture can effectively reason across multiple lines of code rather than requiring single-pass generation.
- Evidence anchors: [section 6.2.3]: "A key distinction between the PLs and the DSLs evaluated in this work lies in the fact that PLs allow breaking down the programs into multiple steps"; [section 6.2.3]: "breaking down code into intermediate steps indeed contributes to higher performance in most cases"

## Foundational Learning

- Concept: In-context learning mechanics
  - Why needed here: Understanding how LLMs learn from demonstrations without parameter updates is crucial for optimizing prompt design
  - Quick check question: How does the model balance information from demonstrations versus its pre-trained knowledge when both are present?

- Concept: Type systems and static analysis
  - Why needed here: Domain descriptions include type signatures that guide the model's output generation
  - Quick check question: What information does a type signature provide that helps constrain the model's search space?

- Concept: Programming language semantics
  - Why needed here: Different languages have different idioms and patterns that affect how models generate code
  - Quick check question: How does the prevalence of a programming language in pre-training data affect its suitability for semantic parsing tasks?

## Architecture Onboarding

- Component map: Natural language utterances → Prompt construction (DD + demonstrations) → LLM generation → Code execution → Result evaluation

- Critical path: Natural language → Prompt construction → Model generation → Code execution → Result evaluation

- Design tradeoffs:
  - Prompt length vs. demonstration quality
  - Domain description detail vs. context window constraints
  - Python specificity vs. general PL applicability
  - Execution accuracy vs. exact match requirements

- Failure signatures:
  - Model generates syntactically valid but semantically incorrect code
  - Model fails to use operators listed in domain description
  - Generated code executes but produces wrong results
  - Model ignores domain description entirely

- First 3 experiments:
  1. Compare Python vs DSL with identical demonstrations and no domain description
  2. Add domain description to both Python and DSL prompts
  3. Test single-line vs multi-line program generation for complex examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the resemblance of a DSL to general-purpose programming languages affect the performance of LLMs in semantic parsing tasks, especially when combined with detailed domain descriptions?
- Basis in paper: [explicit] The paper concludes that "the resemblance of the target parse language to general-purpose code is a more important factor than the language's popularity in pre-training corpora."
- Why unresolved: The paper does not quantify the specific elements of PLs that should be adopted in DSLs to achieve comparable performance gains.
- What evidence would resolve it: A systematic study comparing various DSLs with different levels of resemblance to PLs, all equipped with detailed domain descriptions, and their impact on LLM performance.

### Open Question 2
- Question: Does the ability to break down programs into intermediate steps significantly contribute to the improved performance of general-purpose programming languages over domain-specific languages in semantic parsing?
- Basis in paper: [explicit] The paper mentions that "A key distinction between the PLs and the DSLs evaluated in this work lies in the fact that PLs allow breaking down the programs into multiple steps and assigning intermediate results to variables."
- Why unresolved: The paper only briefly mentions this aspect and does not explore it in depth or provide quantitative analysis.
- What evidence would resolve it: An experiment isolating the effect of intermediate steps by comparing single-line and multi-line program demonstrations in both PLs and DSLs, controlling for other variables.

### Open Question 3
- Question: What is the optimal trade-off between the detail of domain descriptions and the number of demonstrations in the prompt for maximizing semantic parsing accuracy across different datasets?
- Basis in paper: [inferred] The paper mentions that "Having the Full DD is preferred whenever it can fit" and that the impact of DDs depends on the dataset and the domain.
- Why unresolved: The paper does not provide a systematic analysis of how the optimal balance between DD detail and demonstration count varies across datasets and domains.
- What evidence would resolve it: A comprehensive study measuring accuracy as a function of both DD detail and number of demonstrations for each dataset, identifying the optimal balance for each.

## Limitations

- The study focuses on only three specific datasets (GeoQuery, Overnight, SMCalFlow), limiting generalizability to other domains
- The relative contribution of each component (Python vs DSL, domain descriptions, multi-line programs) remains unclear without proper ablation studies
- The execution accuracy metric may not fully capture semantic equivalence, as exact match accuracy could be lower

## Confidence

**High confidence**: The core finding that Python prompts with domain descriptions significantly outperform DSL prompts is well-supported by the experimental results across all three datasets. The improvement on the compositional split (from 7.9% to 66.5% on SMCalFlow) is dramatic and consistently observed.

**Medium confidence**: The claim that domain descriptions are more effective with programming languages than with DSLs is supported by the data but could benefit from more detailed analysis of why this difference exists. The paper provides some evidence through case studies but doesn't fully explain the underlying mechanism.

**Low confidence**: The assertion that breaking down code into intermediate steps is a key differentiator between PLs and DSLs is based on limited evidence. While the paper shows some cases where multi-line programs help, it doesn't systematically analyze when and why step-by-step decomposition matters.

## Next Checks

1. **Ablation study design**: Conduct experiments isolating each component (Python vs DSL, with vs without domain descriptions, single-line vs multi-line) to quantify their individual contributions to performance improvements.

2. **Generalization testing**: Apply the approach to additional DSLs and programming languages beyond the three studied datasets to assess whether the findings extend to broader domains and language choices.

3. **Semantic equivalence analysis**: Perform detailed comparison between execution accuracy and exact match accuracy to understand where the approach produces semantically equivalent but syntactically different programs, and whether these differences matter for downstream applications.