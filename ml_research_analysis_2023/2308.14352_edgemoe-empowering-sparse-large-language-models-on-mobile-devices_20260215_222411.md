---
ver: rpa2
title: 'EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices'
arxiv_id: '2308.14352'
source_url: https://arxiv.org/abs/2308.14352
tags:
- edgemoe
- expert
- experts
- memory
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EdgeMoE is an inference engine designed to enable fast, memory-efficient
  deployment of sparse Mixture-of-Experts (MoE) large language models (LLMs) on resource-constrained
  edge devices. It addresses the challenge of deploying MoE LLMs, which have massive
  parameter sizes but sparse activation, by partitioning the model across the storage
  hierarchy: non-expert weights are held in device memory, while expert weights are
  stored externally and fetched on-demand.'
---

# EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices

## Quick Facts
- **arXiv ID**: 2308.14352
- **Source URL**: https://arxiv.org/abs/2308.14352
- **Reference count**: 40
- **One-line primary result**: EdgeMoE enables 1.05-1.18× memory savings and 1.11-2.78× speedup for MoE LLM inference on edge devices

## Executive Summary
EdgeMoE is an inference engine designed to enable fast, memory-efficient deployment of sparse Mixture-of-Experts (MoE) large language models (LLMs) on resource-constrained edge devices. It addresses the challenge of deploying MoE LLMs, which have massive parameter sizes but sparse activation, by partitioning the model across the storage hierarchy: non-expert weights are held in device memory, while expert weights are stored externally and fetched on-demand. To reduce I/O overhead, EdgeMoE incorporates expert-wise bitwidth adaptation, quantizing experts to mixed precision based on their importance to model accuracy, and in-memory expert management, which predicts and preloads activated experts to overlap computation with I/O. On popular MoE LLMs (up to 26B parameters) and edge devices (Jetson TX2, Raspberry Pi 4B), EdgeMoE achieves 1.05-1.18× memory savings over holding the full model in memory and 1.11-2.78× speedup over memory-optimized baselines, enabling real-time inference with tolerable accuracy loss.

## Method Summary
EdgeMoE implements a memory partitioning strategy that leverages the sparse activation characteristic of MoE models, storing frequently accessed non-expert weights in device memory while keeping expert weights on external storage. The system employs expert-wise bitwidth adaptation to quantize different experts to varying precisions based on their importance, reducing storage requirements while maintaining accuracy. In-memory expert management uses predictive preloading to overlap I/O operations with computation, building a statistical model of expert activation patterns to preload experts before they are needed. The approach combines these techniques to enable efficient deployment of large MoE models on edge devices with limited memory and computational resources.

## Key Results
- Achieves 1.05-1.18× memory savings compared to holding full model in device memory
- Provides 1.11-2.78× speedup over memory-optimized baselines
- Maintains tolerable accuracy loss (5% threshold) while enabling real-time inference on edge devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EdgeMoE achieves memory savings by partitioning model weights into "hot" non-expert weights stored in device memory and "cold" expert weights stored on external storage and loaded on-demand.
- Mechanism: The system leverages the observation that most computation occurs in non-expert weights while expert weights are bulky but infrequently accessed due to sparse activation. This naturally fits the storage hierarchy where faster RAM holds frequently accessed weights and larger disk holds rarely accessed ones.
- Core assumption: The expert activation patterns are sufficiently sparse that most experts can remain on disk without causing excessive I/O overhead.
- Evidence anchors:
  - [abstract] "EdgeMoE achieves both memory- and compute-efficiency by partitioning the model into the storage hierarchy: non-expert weights are held in device memory, while expert weights are held on external storage and fetched to memory only when activated."
  - [section] "EdgeMoE differentiates the positions of experts and non-experts in storage hierarchy. Specifically, it permanently hosts all hot weights in memory since they are used per-token inference; while the rest of the memory budget is used as an expert buffer for the cold expert weights."
- Break condition: If expert activation becomes dense (many experts activated per token), the I/O overhead would overwhelm the system, negating the memory savings.

### Mechanism 2
- Claim: Expert-wise bitwidth adaptation reduces model size by quantizing experts to different precisions based on their importance to model accuracy.
- Mechanism: EdgeMoE profiles expert importance by measuring accuracy loss when each expert is quantized to low precision (INT2). Less important experts are quantized to lower bitwidths while important ones retain higher precision, achieving a balance between accuracy and storage size.
- Core assumption: Experts exhibit different sensitivity to quantization, allowing some to be compressed significantly without major accuracy degradation.
- Evidence anchors:
  - [abstract] "EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth adaptation that reduces the expert sizes with tolerable accuracy loss"
  - [section] "EdgeMoE employs a fine-grained, expert-wise bitwidth adaptation to fully leverage the model redundancy. At offline, EdgeMoE progressively lower the bitwidth of a few experts that are most robust to quantization, till the accuracy degradation meets a tolerable threshold specified by users."
- Break condition: If all experts are equally sensitive to quantization, the approach would provide minimal compression benefit.

### Mechanism 3
- Claim: In-memory expert management with prediction and pipelining hides I/O latency by preloading experts before they are needed.
- Mechanism: EdgeMoE builds a statistical model of expert activation paths during offline phase, then uses this model at runtime to predict which experts will be activated in upcoming layers. This allows preloading experts while current layers compute, overlapping I/O with computation.
- Core assumption: Expert activation paths follow predictable patterns (power law distribution) that can be statistically modeled and used for prediction.
- Evidence anchors:
  - [abstract] "(2) expert preloading that predicts the activated experts ahead of time and preloads it with the compute-I/O pipeline."
  - [section] "EdgeMoE builds a statistic model to estimate the probability of expert activation in the current layer based on the activations of previous layers. In online inference, EdgeMoE queries this model and preloads the most possible expert ahead of activation for I/O-compute pipeline."
- Break condition: If expert activation becomes truly random with no correlation across layers, prediction accuracy would drop to chance level.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: EdgeMoE is specifically designed for MoE LLMs, which have sparse activation patterns where only a subset of experts are activated per token.
  - Quick check question: In a MoE layer with 32 experts and top-1 routing, how many experts are activated for each token?

- Concept: Storage hierarchy and memory hierarchy
  - Why needed here: EdgeMoE relies on the natural fit between MoE characteristics and the device's storage hierarchy (fast RAM vs slower external storage).
  - Quick check question: Why is it more efficient to store frequently accessed weights in faster memory versus storing all weights in slower memory?

- Concept: Quantization and mixed-precision models
  - Why needed here: Expert-wise bitwidth adaptation uses quantization to reduce model size while maintaining accuracy by varying precision across different experts.
  - Quick check question: What is the trade-off between bitwidth reduction and accuracy loss in quantization?

## Architecture Onboarding

- Component map: User Application -> EdgeMoE Engine -> Device Memory (non-expert weights) + External Storage (expert weights) + Statistical Prediction Model + Quantization Modules
- Critical path: Token generation involves (1) routing to select experts, (2) preloading predicted experts via pipeline, (3) computation with loaded experts, (4) updating predictions for next token. The preloading and computation overlap to hide I/O latency.
- Design tradeoffs: Memory budget vs. performance - larger expert buffer reduces I/O but increases memory usage. Accuracy tolerance vs. compression - lower accuracy tolerance allows more aggressive quantization. Prediction accuracy vs. pipeline complexity - more sophisticated prediction models may provide better overlap but add complexity.
- Failure signatures: High cache miss rates indicate poor expert prediction accuracy. Excessive I/O latency suggests either too small expert buffer or overly aggressive quantization. Memory pressure indicates budget misconfiguration.
- First 3 experiments:
  1. Measure baseline MoE inference latency and memory usage on target device without EdgeMoE to establish performance targets.
  2. Profile expert activation patterns on representative dataset to validate power law distribution assumption.
  3. Test expert-wise quantization on small subset of experts to measure accuracy sensitivity and determine quantization thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of EdgeMoE scale with increasing model size and expert number, particularly beyond the evaluated 26B parameter models?
- Basis in paper: [inferred] The paper evaluates models up to 26B parameters but does not explore scaling effects beyond this point or the limits of EdgeMoE's effectiveness.
- Why unresolved: The paper focuses on demonstrating feasibility and performance gains within a specific parameter range, but does not investigate the theoretical or practical limits of scaling EdgeMoE to larger models.
- What evidence would resolve it: Experimental results showing performance, memory usage, and accuracy for models significantly larger than 26B parameters, including analysis of diminishing returns or performance bottlenecks.

### Open Question 2
- Question: What is the impact of different storage technologies (e.g., NVMe SSD, cloud storage) on EdgeMoE's performance, and how does this affect deployment scenarios?
- Basis in paper: [explicit] The paper compares SSD and HDD performance on Jetson TX2 but does not explore other storage technologies or their impact on different deployment scenarios (e.g., edge vs. cloud-edge hybrid).
- Why unresolved: Storage technology significantly affects I/O latency, which is critical for EdgeMoE's performance, but the paper only provides a limited comparison.
- What evidence would resolve it: Performance benchmarks of EdgeMoE using various storage technologies (NVMe, cloud storage, etc.) across different deployment scenarios, including analysis of cost-performance tradeoffs.

### Open Question 3
- Question: How robust is EdgeMoE's expert activation prediction model to distribution shifts in real-world data compared to the training datasets used for profiling?
- Basis in paper: [inferred] The paper describes building a statistical profile for expert activation prediction but does not evaluate its robustness to distribution shifts or out-of-distribution data.
- Why unresolved: Real-world data may have different characteristics than the datasets used for profiling, potentially degrading the accuracy of the prediction model and affecting performance.
- What evidence would resolve it: Experiments evaluating EdgeMoE's performance on datasets with different distributions from the training data, including analysis of prediction accuracy degradation and potential mitigation strategies.

### Open Question 4
- Question: What is the energy consumption profile of EdgeMoE compared to baselines, and how does this impact battery life on mobile devices?
- Basis in paper: [inferred] The paper focuses on latency and memory usage but does not provide energy consumption measurements or analysis of battery life impact.
- Why unresolved: Energy efficiency is crucial for mobile devices, but the paper does not quantify this aspect of EdgeMoE's performance.
- What evidence would resolve it: Power measurements and energy consumption analysis of EdgeMoE and baselines during inference, including battery life predictions for typical usage scenarios.

## Limitations
- Expert activation prediction accuracy may degrade with different input distributions or MoE architectures
- Aggressive quantization can lead to accuracy degradation beyond the tolerable threshold
- The approach's effectiveness depends on sufficient sparsity in expert activation patterns

## Confidence

- **High confidence**: The memory partitioning mechanism between device memory and external storage is well-established and the reported memory savings (1.05-1.18×) are directly measurable. The basic premise that MoE's sparse activation allows for efficient storage hierarchy utilization is sound.
- **Medium confidence**: The expert-wise bitwidth adaptation approach is reasonable but the actual compression ratios achieved and their relationship to accuracy loss depend heavily on the specific expert importance profiling methodology, which is not fully detailed. The 1.11-2.78× speedup claim depends on the quality of the implementation and the baseline used.
- **Low confidence**: The predictive preloading mechanism's effectiveness is difficult to evaluate without access to the statistical model details. The claim of successful I/O-compute overlap depends critically on the accuracy of expert activation predictions, which may degrade significantly with different input distributions or MoE architectures.

## Next Checks

1. **Expert activation prediction accuracy**: Measure the cache hit ratio and prediction accuracy across different datasets and input sequences to validate that the statistical model generalizes beyond the tested scenarios. Compare predicted vs actual activated experts on a token-by-token basis.

2. **Quantization sensitivity analysis**: Systematically test the bitwidth adaptation approach by varying the accuracy tolerance threshold and measuring the resulting compression ratios and accuracy degradation. Identify whether the expert importance profiling correctly identifies which experts can be aggressively quantized.

3. **Scalability testing**: Evaluate EdgeMoE's performance on MoE models with different expert counts (beyond the 32-expert models tested) and on additional edge hardware platforms to assess the approach's generalizability across the MoE design space.