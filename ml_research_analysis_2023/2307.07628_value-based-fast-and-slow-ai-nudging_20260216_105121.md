---
ver: rpa2
title: Value-based Fast and Slow AI Nudging
arxiv_id: '2307.07628'
source_url: https://arxiv.org/abs/2307.07628
tags:
- human
- decision
- machine
- nudging
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a value-based AI-human collaborative framework
  for decision-making, where AI systems nudge humans by proposing recommendations.
  The framework employs three nudging modalities: fast thinking (System 1), slow thinking
  (System 2), and meta-cognition.'
---

# Value-based Fast and Slow AI Nudging

## Quick Facts
- arXiv ID: 2307.07628
- Source URL: https://arxiv.org/abs/2307.07628
- Authors: 
- Reference count: 9
- Primary result: Proposed value-based AI-human collaborative framework using fast/slow thinking and metacognitive nudges

## Executive Summary
This paper introduces a value-based framework for AI-human collaborative decision-making where AI systems use nudging techniques to influence human choices. The framework employs three nudging modalities (fast thinking, slow thinking, and metacognition) that are activated based on machine confidence, human-machine performance comparisons, and prioritized values like decision quality, speed, human upskilling, agency, and privacy. The authors present FASCAI, a specific instance of the framework prioritizing human agency, upskilling, and decision quality. The paper raises important questions about the effectiveness of AI nudges, human acceptance, and the embedding of values in AI systems.

## Method Summary
The framework is parameterized by values relevant to specific decision scenarios and uses a decision controller that allocates three interaction modalities based on machine confidence and human vs machine performance comparisons. The system employs System 1 nudges (immediate recommendations leveraging anchoring bias), System 2 nudges (delayed recommendations encouraging deliberation), and metacognitive nudges (prompting humans to assess their own confidence and choose their approach). The architecture includes machine decision engines using fast/slow solvers and metacognitive agents, with performance tracking to monitor outcomes.

## Key Results
- Framework preserves human agency while improving decision outcomes through selective AI intervention
- Three nudging modalities align with dual-process theories of cognition (System 1, System 2, metacognition)
- Value-based parameterization allows adaptation to different decision environments while maintaining human agency and upskilling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework improves decision quality by leveraging cognitive theories of System 1 and System 2 thinking
- Mechanism: Uses three nudging modalities (fast thinking, slow thinking, and metacognition) to present AI recommendations that align with human cognitive processes. System 1 nudges exploit anchoring bias when AI is highly confident, while System 2 nudges encourage careful deliberation when confidence is low or human performance is better
- Core assumption: Human cognitive biases like anchoring effect and automation bias can be reliably triggered by AI recommendations in predictable ways
- Evidence anchors: [abstract] "Three different nudging modalities... are intended to stimulate human fast thinking, slow thinking, or meta-cognition"; [section] "The first nudging strategy... leverages the anchoring bias effect..."
- Break condition: Anchoring effect is significantly weaker for AI-generated recommendations compared to human-generated ones, or humans develop resistance to AI nudges over time

### Mechanism 2
- Claim: Value-based parameterization allows the framework to adapt to different decision environments while maintaining human agency and upskilling
- Mechanism: Treats values (decision quality, speed, human upskilling, agency, privacy) as parameters that can be prioritized differently based on decision context. The nudging controller allocates interaction modalities across confidence-performance scenarios according to value priorities
- Core assumption: Values can be meaningfully quantified and prioritized in a way that directly influences allocation of nudging modalities to produce desired outcomes
- Evidence anchors: [abstract] "Values that are relevant to a specific decision scenario are used to decide when and how to use each of these nudging modalities"; [section] "We are building the FASCAI architecture in a way that can be parameterized by the values..."
- Break condition: Value priorities cannot be effectively quantified or lead to contradictory allocations that degrade system performance or user experience

### Mechanism 3
- Claim: The framework preserves human autonomy while improving decision outcomes through selective AI intervention
- Mechanism: Includes scenarios where humans decide autonomously when their past performance exceeds AI's capabilities, and scenarios where AI decides autonomously when highly confident and human performance is poor. This selective intervention preserves agency while ensuring optimal decisions in critical situations
- Core assumption: Humans can reliably assess their own competence and knowledge gaps when prompted through metacognitive nudges, and will appropriately choose when to seek AI assistance
- Evidence anchors: [abstract] "The framework aims to support human-machine trust and well-being while ensuring system performance"; [section] "The third type of machine nudging takes place when the machine encourages the human to use her own metacognition..."
- Break condition: Humans consistently misjudge their own competence or fail to recognize when AI assistance would be beneficial, leading to suboptimal decisions

## Foundational Learning

- Concept: System 1 vs System 2 thinking
  - Why needed here: The framework explicitly leverages these dual-process theories to design appropriate nudging strategies that align with human cognitive architecture
  - Quick check question: Can you describe the key differences between System 1 and System 2 thinking in terms of speed, effort, and accuracy?

- Concept: Anchoring bias and automation bias
  - Why needed here: These cognitive biases are central to how the framework expects AI nudges to influence human decision-making, particularly for System 1 nudges
  - Quick check question: How does anchoring bias affect human decision-making when presented with immediate AI recommendations?

- Concept: Metacognition
  - Why needed here: The framework includes a third nudging modality that specifically targets metacognitive processes, requiring humans to assess their own confidence and knowledge gaps
  - Quick check question: What role does metacognition play in the framework's design for when and how humans choose to use AI recommendations?

## Architecture Onboarding

- Component map: Problem instance -> Nudging Controller -> Machine Decision Engine -> Human Decision Interface -> Final Decision -> Performance Tracking -> Value Parameter Update (feedback loop)

- Critical path: Problem instance → Nudging Controller → Machine Decision Engine → Human Decision Interface → Final Decision → Performance Tracking → Value Parameter Update (feedback loop)

- Design tradeoffs:
  - Human autonomy vs decision quality: More AI intervention improves quality but reduces human agency
  - Cognitive load vs learning: System 2 and metacognitive nudges increase learning but require more effort
  - Speed vs accuracy: System 1 nudges are faster but potentially less accurate
  - Transparency vs effectiveness: Full disclosure of AI confidence may reduce anchoring effect but improve trust

- Failure signatures:
  - Humans consistently reject AI recommendations regardless of confidence levels
  - Performance degradation when humans are over-nudged with System 1 recommendations
  - Users fail to develop metacognitive awareness despite repeated metacognitive nudges
  - Value prioritization leads to allocation conflicts (e.g., prioritizing speed conflicts with upskilling)

- First 3 experiments:
  1. A/B test comparing decision quality when using different value priority configurations (agency-focused vs quality-focused)
  2. Longitudinal study measuring human performance improvement over repeated interactions with the framework
  3. User acceptance study comparing reactions to System 1 vs System 2 AI nudges across different user demographics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are AI nudges generating a significant anchoring effect?
- Basis in paper: [explicit] The paper states that one of the main assumptions in their AI nudging approach is that AI nudging creates an anchoring effect, similar to human nudges
- Why unresolved: The paper acknowledges this assumption but does not provide empirical evidence or experiments to confirm whether AI nudges indeed generate a significant anchoring effect comparable to human nudges
- What evidence would resolve it: Conducting experiments that measure the anchoring effect of AI nudges and comparing it with the anchoring effect of human nudges would provide evidence to resolve this question

### Open Question 2
- Question: Are AI nudges accepted by humans as a human-machine collaboration mechanism?
- Basis in paper: [explicit] The paper mentions that there might be a tendency for humans to reject AI suggestions due to skepticism about AI's reliability and trustworthiness
- Why unresolved: The paper raises the question but does not present any studies or experiments that investigate human acceptance of AI nudges
- What evidence would resolve it: Conducting user studies or surveys to measure human acceptance and attitudes towards AI nudges would provide evidence to resolve this question

### Open Question 3
- Question: In the FASCAI instance, are humans learning over time, more than if they are asked to make decisions by themselves?
- Basis in paper: [explicit] The paper proposes that one of the values supported by the FASCAI framework is human upskilling
- Why unresolved: The paper raises the question but does not present any studies or experiments that measure human learning and upskilling in the FASCAI instance
- What evidence would resolve it: Conducting longitudinal studies that track human performance and learning over time in both the FASCAI instance and a control group making decisions without AI assistance would provide evidence to resolve this question

## Limitations
- The framework is presented conceptually rather than as a fully implemented system, limiting evaluation of practical effectiveness
- Key assumptions about human cognitive biases being triggered reliably by AI recommendations lack empirical validation
- The mechanism for translating values into specific parameter allocations is described at a high level but lacks concrete implementation details

## Confidence
- **High confidence**: The conceptual framework architecture and the general approach of using dual-process theories for AI-human collaboration is well-grounded in existing cognitive science literature
- **Medium confidence**: The value-based parameterization approach is theoretically sound but untested in practice, with significant uncertainties about how to quantify and prioritize values in real decision environments
- **Low confidence**: The specific assumptions about human responses to different nudging modalities (particularly the strength and consistency of anchoring effects with AI recommendations) lack empirical validation

## Next Checks
1. Conduct controlled experiments measuring actual human decision quality and agency preservation when exposed to different AI nudging modalities, comparing System 1 vs System 2 recommendations across multiple decision domains
2. Implement and test the value-to-parameter translation mechanism with real users to validate whether prioritized values actually produce the intended allocation of nudging modalities and desired outcomes
3. Design longitudinal studies to measure whether humans develop metacognitive awareness over time when exposed to metacognitive nudges, and whether this awareness translates to improved decision-making autonomy