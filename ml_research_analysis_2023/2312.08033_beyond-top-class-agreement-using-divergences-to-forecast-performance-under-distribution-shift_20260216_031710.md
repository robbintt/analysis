---
ver: rpa2
title: 'Beyond Top-Class Agreement: Using Divergences to Forecast Performance under
  Distribution Shift'
arxiv_id: '2312.08033'
source_url: https://arxiv.org/abs/2312.08033
tags:
- disagreement
- error
- test
- top-1
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies model disagreement notions that consider the
  full predictive distribution to forecast model performance under distribution shift.
  It investigates divergence-based disagreement using Hellinger distance, Jensen-Shannon
  divergence, and Kullback-Leibler divergence.
---

# Beyond Top-Class Agreement: Using Divergences to Forecast Performance under Distribution Shift

## Quick Facts
- arXiv ID: 2312.08033
- Source URL: https://arxiv.org/abs/2312.08033
- Reference count: 27
- One-line primary result: Divergence-based disagreement measures (Hellinger distance, Jensen-Shannon divergence, Kullback-Leibler divergence) provide better OOD performance estimation and detection compared to top-1 disagreement

## Executive Summary
This paper investigates model disagreement measures that consider the full predictive distribution to forecast model performance under distribution shift. Traditional top-1 disagreement is limited as it only considers predicted class agreement, missing nuanced uncertainty differences between models. The authors propose using divergence-based disagreement measures including Hellinger distance, Jensen-Shannon divergence, and Kullback-Leibler divergence, which capture full distributional differences between model predictions. Through extensive experiments on CIFAR-10/100 with synthetic corruptions, they demonstrate that divergence-based measures provide stronger correlations with test error under distribution shift and better OOD detection capabilities compared to top-1 disagreement baselines.

## Method Summary
The study uses pre-trained deep vision models from CIFAR-10/100 datasets and their corrupted versions (CIFAR-10C/100C with 18 types of synthetic corruptions). The authors compute pairwise divergence disagreements (HD, JSD, KLD) between model pairs and between each vision model and a CLIP model finetuned on ID data. They implement the ALineD method to estimate OOD test error by exploiting the linear relationship between ID vs OOD disagreement and ID vs OOD test error. For OOD detection, they use divergence-based disagreement measures as OOD scores and compare with baselines (MSP, MaxLogit) using ROC-AUC metrics. The performance is evaluated using mean absolute percentage error (MAPE) for OOD error estimation on shifts with strong ID vs OOD disagreement correlation (R² > 0.95).

## Key Results
- Divergence-based disagreement measures correlate more strongly with test error than top-1 disagreement under distribution shift
- Hellinger distance disagreement provides the best OOD test error estimates on high-correlation datasets (MAPE best in 17 out of 22 shifts)
- Divergence disagreement measures detect OOD samples better than top-1, MSP, and MaxLogit baselines with higher ROC-AUC scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Divergence-based disagreement measures correlate more strongly with test error than top-1 disagreement under distribution shift.
- Mechanism: By considering the full predictive distribution, divergence-based measures (HD, JSD, KLD) capture model uncertainty differences that top-1 misses, leading to stronger agreement-on-the-line and accuracy-on-the-line correlations.
- Core assumption: The link between ID vs OOD disagreement and ID vs OOD test error is stronger when considering the full predictive distribution.
- Evidence anchors:
  - [abstract] "divergence-based scores provide better test error estimates and detection rates on out-of-distribution data compared to their top-1 counterparts"
  - [section] "We observe stronger correlations for divergence notions compared to the standard top-1 approach for both on-the-line phenomena"
  - [corpus] "Average neighbor FMR=0.341, average citations=0.0" - weak external validation
- Break condition: When models are poorly calibrated, all disagreement measures lose correlation strength.

### Mechanism 2
- Claim: Hellinger distance disagreement provides the best OOD performance estimation on high correlation datasets.
- Mechanism: HD's bounded [0,1] range and symmetric properties make it robust for estimating test error via ALineD regression on corrupted datasets with strong ID vs OOD disagreement correlation.
- Core assumption: The ID vs OOD agreement-on-the-line relationship holds for high correlation corruption types.
- Evidence anchors:
  - [abstract] "Hellinger distance performs best overall in OOD performance estimation"
  - [section] "Hellinger disagreement provides the best OOD test error estimates on high correlation datasets reporting the smallest mean absolute percentage error (MAPE) in 17 out of 22 shifts"
  - [corpus] "Scalable Utility-Aware Multiclass Calibration" - external calibration work suggests miscalibration affects performance
- Break condition: When calibration error increases, HD correlation drops faster than top-1 on datasets with many labels.

### Mechanism 3
- Claim: Divergence disagreement measures detect OOD samples better than top-1, MSP, and MaxLogit baselines.
- Mechanism: Distribution shifts affect all output dimensions equally, which divergence measures (especially KLD) capture better than top-1 which only considers predicted class.
- Core assumption: Distribution shifts perturb model outputs across all classes uniformly.
- Evidence anchors:
  - [abstract] "divergence disagreement also provides better OOD detection capabilities"
  - [section] "Tab. 2 seems to confirm our hypothesis. It reports ROC-AUC for separating OOD from ID samples. Divergence disagreement performs best capturing information about the shift better than top-1, MSP and MaxLogit"
  - [corpus] "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning" - shows multi-agent disagreement applications
- Break condition: When distribution shifts are subtle or affect only specific classes, divergence measures may not outperform simpler baselines.

## Foundational Learning

- Divergence measures (HD, JSD, KLD):
  - Why needed here: They provide a more nuanced view of model disagreement than top-1 by considering the full predictive distribution.
  - Quick check question: What is the key difference between Hellinger distance and KL divergence in terms of their mathematical properties?

- Calibration error and its impact:
  - Why needed here: Miscalibration reduces the correlation strength between disagreement measures and test error under distribution shift.
  - Quick check question: How does increasing calibration error affect the agreement-on-the-line correlation for divergence-based disagreement?

- OOD detection metrics (ROC-AUC):
  - Why needed here: They provide a way to evaluate how well disagreement measures can distinguish between ID and OOD samples.
  - Quick check question: What does a higher ROC-AUC score indicate about a disagreement measure's ability to detect OOD samples?

## Architecture Onboarding

- Component map: Input data -> Pre-trained models -> Compute divergence disagreements (HD, JSD, KLD) -> Calculate test errors -> ALineD regression -> OOD performance estimation and detection

- Critical path:
  1. Load pre-trained models and compute their predictions on ID and OOD test sets
  2. Calculate divergence disagreement measures and top-1 disagreement for each model pair
  3. Compute test error and calibration error for each model
  4. Analyze correlations between disagreement measures and test error
  5. Perform OOD performance estimation and detection

- Design tradeoffs:
  - Using divergence measures provides more nuanced disagreement information but requires computing full predictive distributions
  - Top-1 disagreement is computationally simpler but may miss important uncertainty differences
  - Calibration error affects the validity of disagreement-error correlations

- Failure signatures:
  - Poor correlations between disagreement measures and test error
  - High calibration error across models
  - OOD performance estimation fails when ID vs OOD agreement-on-the-line correlation is weak

- First 3 experiments:
  1. Verify that divergence disagreement measures correlate more strongly with test error than top-1 on CIFAR-10C corruptions with R² > 0.95
  2. Compare OOD performance estimation accuracy using ALineD regression for HD, JSD, KLD, and top-1 on high correlation datasets
  3. Evaluate OOD detection ROC-AUC for divergence disagreement measures vs top-1, MSP, and MaxLogit baselines on CIFAR-10C and CIFAR-100C

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does miscalibration specifically affect the correlation between divergence-based disagreement and test error under distribution shift?
- Basis in paper: [explicit] The paper states "divergence disagreement correlation drops faster with increasing calibration error" and discusses calibration error's impact on on-the-line phenomena
- Why unresolved: While the paper observes this effect, it doesn't provide a detailed analysis of the specific mechanisms by which miscalibration affects each divergence type differently, or quantify the threshold at which calibration error begins to significantly degrade the correlation
- What evidence would resolve it: A systematic study varying calibration error levels across different divergence types and quantifying the exact correlation degradation patterns for each divergence measure

### Open Question 2
- Question: Why does Kullback-Leibler divergence perform best at OOD detection but worst at OOD performance estimation?
- Basis in paper: [explicit] The paper notes "KLD performs best on this task, but worst on OOD error estimation indicating it picks up information about the distribution shift that could not be exploited for error estimation"
- Why unresolved: The paper observes this paradoxical behavior but doesn't explain the underlying reason for why KLD captures different aspects of distribution shift for detection versus performance estimation
- What evidence would resolve it: An analysis decomposing what specific distributional properties KLD is sensitive to in OOD detection versus what properties are relevant for estimating error rates

### Open Question 3
- Question: Under what conditions do divergence-based disagreement measures outperform top-1 disagreement for OOD performance estimation?
- Basis in paper: [explicit] The paper finds that Hellinger distance performs best overall on high-correlation datasets but notes that "top-1 seems more robust on shifts with weak ID vs OOD disagreement"
- Why unresolved: The paper doesn't provide a clear characterization of when divergence measures are superior, only noting some empirical observations about dataset size and correlation strength
- What evidence would resolve it: A comprehensive study identifying specific distributional characteristics (e.g., type of shift, severity, dataset size) that predict when divergence measures will outperform top-1 disagreement

### Open Question 4
- Question: How do different types of distribution shifts affect the relative performance of divergence-based disagreement measures?
- Basis in paper: [inferred] The paper tests multiple corruption types (brightness, contrast, fog, etc.) but doesn't systematically analyze how each shift type affects the different divergence measures
- Why unresolved: While the paper shows overall performance across corruption types, it doesn't investigate which types of shifts benefit most from each divergence measure
- What evidence would resolve it: A detailed analysis of performance per corruption type for each divergence measure, identifying which shifts benefit most from divergence-based approaches versus top-1

## Limitations
- Results are primarily based on CIFAR datasets and synthetic corruptions; real-world distribution shifts remain unverified
- Experiments use vision models only; findings may not generalize to other domains like NLP or tabular data
- The study assumes ID vs OOD disagreement correlation holds, but this relationship can break down with severe miscalibration or class-specific shifts

## Confidence
- High Confidence: Comparative performance of divergence measures vs top-1 disagreement for OOD detection (ROC-AUC results)
- Medium Confidence: ALineD method for OOD performance estimation works well on high-correlation datasets but may have limited applicability on low-correlation shifts
- Low Confidence: Generalization of findings to non-vision domains and real-world distribution shifts without synthetic corruption

## Next Checks
1. Test divergence disagreement measures on NLP datasets (e.g., IMDb reviews with sentiment shift) to verify domain generalization
2. Apply the methodology to real-world distribution shifts (e.g., medical imaging datasets with domain adaptation) rather than synthetic corruptions
3. Systematically vary model calibration error and measure its impact on the agreement-on-the-line correlation for each divergence measure