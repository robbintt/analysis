---
ver: rpa2
title: 'ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations'
arxiv_id: '2303.01261'
source_url: https://arxiv.org/abs/2303.01261
tags:
- speech
- speaker
- parrottts
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ParrotTTS, a novel text-to-speech (TTS) system
  that leverages self-supervised speech representations. The core idea is to first
  train a speech-to-speech model on unlabeled data to learn discrete speech representations,
  then train a text-to-embedding model using paired text-speech data.
---

# ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations

## Quick Facts
- arXiv ID: 2303.01261
- Source URL: https://arxiv.org/abs/2303.01261
- Reference count: 12
- One-line primary result: Achieves competitive TTS performance with significantly reduced transcribed data and enables speaker adaptation from untranscribed audio.

## Executive Summary
ParrotTTS introduces a novel two-stage approach to text-to-speech synthesis that leverages self-supervised speech representations. The system first trains a speech-to-speech model on unlabeled data to learn discrete speech representations, then trains a text-to-embedding model using paired text-speech data. This design enables competitive or superior performance compared to state-of-the-art TTS models while significantly reducing the need for transcribed data and enabling speaker adaptation from untranscribed audio.

The core innovation lies in separating the learning of acoustic variability from phonetic-to-acoustic mapping, which reduces the complexity of each stage and enables faster training and inference times compared to traditional TTS systems.

## Method Summary
ParrotTTS employs a two-step approach: first, a self-supervised model (HuBERT) learns discrete speech representations from unlabeled audio; second, a text-to-embedding model maps text to these representations, followed by a decoder that converts embeddings to waveform. The system uses LJSpeech (24 hours, single speaker) and VCTK (44 hours, 108 speakers) datasets. Evaluation metrics include Mean Opinion Score (MOS) for naturalness, Word Error Rate (WER) for intelligibility, and Equal Error Rate (EER) for speaker adaptability.

## Key Results
- Achieves competitive speech quality with minimal transcribed data by leveraging self-supervised speech representations.
- Enables speaker adaptation without transcribed audio by fine-tuning the decoder on untranscribed speech from new speakers.
- Demonstrates at least eight-fold faster convergence in training iterations compared to traditional TTS models like FastSpeech2 and Tacotron2.

## Why This Works (Mechanism)

### Mechanism 1
ParrotTTS achieves competitive speech quality with minimal transcribed data by first learning discrete speech representations from unlabeled audio (via HuBERT) and then learning to map text to those representations. The two-stage learning separates the task of learning acoustic variability from the task of learning phonetic-to-acoustic mapping, reducing the complexity of each stage. The core assumption is that discrete HuBERT units capture sufficient phonetic and prosodic information to reconstruct intelligible speech when decoded.

### Mechanism 2
Speaker adaptation without transcribed audio is enabled because the ETS decoder can be fine-tuned on untranscribed speech from a new speaker, conditioned only on speaker identity embeddings. STE disentangles speaker identity from content, so speaker-specific tuning of ETS does not require text annotations. The core assumption is that speaker embeddings derived from one-hot encoding sufficiently capture speaker characteristics for adaptation.

### Mechanism 3
Faster training convergence for TTE compared to traditional TTS arises because TTE maps to discrete units rather than continuous Mel spectrograms, reducing output variance. Lower-dimensional discrete targets have less variability, making the learning problem easier and more stable. The core assumption is that discrete HuBERT units have lower variance than Mel spectrograms and still preserve sufficient linguistic content.

## Foundational Learning

- **Self-supervised learning via masked prediction (HuBERT)**: Enables learning of rich speech representations from unlabeled audio, avoiding reliance on expensive transcribed data. *Quick check: What is the role of clustering in HuBERT's pre-training, and how does it produce discrete units?*

- **Sequence-to-sequence modeling with attention**: TTE must map variable-length phoneme sequences to variable-length discrete embeddings; attention allows flexible alignment. *Quick check: How does the guided-attention loss enforce monotonic alignment between phonemes and embeddings?*

- **Speaker embedding conditioning**: Allows a single ETS decoder to produce speech for multiple speakers by conditioning on identity vectors. *Quick check: What are the trade-offs between one-hot speaker IDs vs. learned speaker embeddings in multi-speaker TTS?*

## Architecture Onboarding

- **Component map**: STE (HuBERT) -> TTE (text-to-embedding) -> ETS (HiFiGAN decoder) -> Waveform
- **Critical path**: Text → TTE → ETS → Waveform (for TTS inference)
- **Design tradeoffs**:
  - Discrete vs. continuous outputs: Discrete units reduce variance but may lose fine-grained prosody
  - AR vs. NAR TTE: AR may yield higher quality but slower inference; NAR is faster but may need extra duration modeling
  - Speaker conditioning: One-hot IDs are simple but less flexible than learned embeddings for unseen speakers
- **Failure signatures**:
  - TTE produces poor alignment → garbled speech or skipped words
  - ETS decoder fails to condition on speaker ID → speaker identity is lost or mixed
  - HuBERT units too coarse → unintelligible or robotic speech
- **First 3 experiments**:
  1. Train STE on LibriSpeech, extract units for a held-out set, verify clustering quality
  2. Train TTE on LJSpeech, check attention alignment and embedding quality on validation
  3. Train ETS on LJSpeech, synthesize sample utterances, measure MOS or WER

## Open Questions the Paper Calls Out

The paper identifies several limitations and open questions:

1. **Performance with noisy or low-quality audio inputs**: The model does not work well with noisy samples, and further research is needed to understand how well ParrotTTS can handle such inputs.

2. **Handling multiple languages beyond English**: Experiments are limited to a single language, and deeper studies with multiple languages are left for future work.

3. **Prosody and emotional variations in speech**: The pre-trained HuBERT model skips prosody information, and the model has no levers to control these prosodic variations.

4. **Impact on people with speech impediments or atypical speech patterns**: The dataset and results are not representative of the population, including people with speech impediments.

5. **Comparison to other self-supervised learning approaches**: While the paper compares ParrotTTS to supervised models, it does not provide a direct comparison with other self-supervised TTS approaches.

## Limitations

- **Training procedure gaps**: Lack of detailed specifications for training the HuBERT model, including key hyperparameters like learning rate, batch size, and number of epochs.
- **Architecture specifications**: Specific layer configurations, optimization strategies, and architectural choices are not clearly defined.
- **Evaluation metrics**: Experimental setup, dataset splits, and evaluation protocols are not fully detailed, raising questions about comparability with existing TTS systems.

## Confidence

- **High Confidence**: The core concept of using self-supervised speech representations to reduce transcribed data dependency is well-supported by prior work (e.g., HuBERT).
- **Medium Confidence**: The two-stage approach (STE → TTE → ETS) is logically sound, but the paper lacks sufficient experimental validation to confirm its superiority over existing methods.
- **Low Confidence**: Claims about faster training convergence and speaker adaptation effectiveness are based on limited evidence and require further experimental scrutiny.

## Next Checks

1. **HuBERT Unit Quality**: Train the STE component on LibriSpeech and evaluate the quality of discrete units by clustering analysis and reconstruction experiments. Verify that the units preserve phonetic and prosodic information.

2. **TTE Alignment and Embedding Quality**: Train the TTE model on LJSpeech and analyze attention alignments and embedding outputs. Check for issues like skipped words or garbled speech, which would indicate poor alignment or insufficient training.

3. **ETS Speaker Adaptation**: Fine-tune the ETS decoder on untranscribed audio from a new speaker and evaluate speaker identity preservation using EER or speaker verification metrics. Compare results with traditional speaker adaptation methods.