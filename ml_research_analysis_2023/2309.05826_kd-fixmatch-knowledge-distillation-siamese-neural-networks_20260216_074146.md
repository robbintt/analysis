---
ver: rpa2
title: 'KD-FixMatch: Knowledge Distillation Siamese Neural Networks'
arxiv_id: '2309.05826'
source_url: https://arxiv.org/abs/2309.05826
tags:
- data
- labels
- fixmatch
- pseudo
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KD-FixMatch, a novel semi-supervised learning
  (SSL) algorithm that improves upon FixMatch by incorporating knowledge distillation.
  The key idea is to use an outer SNN to generate pseudo labels for unlabeled data,
  which are then filtered using high-confidence sampling and deep embedding clustering
  to create a trusted subset.
---

# KD-FixMatch: Knowledge Distillation Siamese Neural Networks

## Quick Facts
- arXiv ID: 2309.05826
- Source URL: https://arxiv.org/abs/2309.05826
- Reference count: 0
- This paper proposes KD-FixMatch, a novel semi-supervised learning (SSL) algorithm that improves upon FixMatch by incorporating knowledge distillation.

## Executive Summary
KD-FixMatch is a semi-supervised learning algorithm that enhances FixMatch by using knowledge distillation through a two-stage Siamese neural network training process. The method first trains an outer SNN to generate pseudo labels, then applies high-confidence sampling and deep embedding clustering to create a trusted subset of pseudo-labeled data. An inner SNN is subsequently trained with labeled data, all unlabeled data, and the trusted subset, resulting in improved performance over FixMatch across multiple datasets.

## Method Summary
The method involves sequential training of two Siamese neural networks. First, an outer SNN is trained with both labeled and unlabeled data to generate pseudo labels for the unlabeled samples. These pseudo labels are then filtered through high-confidence sampling (threshold τ_select=0.80) and refined using deep embedding clustering to create a trusted subset. The inner SNN is then trained using labeled data, all unlabeled data, and the trusted subset, with the option to use either standard cross-entropy or symmetric cross-entropy loss to handle potential label noise.

## Key Results
- KD-FixMatch achieves higher test accuracy than FixMatch by 2.44%, 5.81%, 1.99%, and 3.88% on CIFAR10, SVHN, CIFAR100, and FOOD101, respectively, when using 4,000 labeled examples.
- The proposed method demonstrates consistent improvements across all four tested datasets.
- KD-FixMatch shows effectiveness in reducing early-stage label noise compared to FixMatch's simultaneous training approach.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KD-FixMatch reduces early-stage label noise by using a pre-trained outer SNN as teacher.
- Mechanism: Outer SNN is trained first with both labeled and unlabeled data. Once trained, it generates pseudo labels for unlabeled data, which are then filtered using high-confidence sampling and deep embedding clustering to form a trusted subset. This trusted subset has lower noise than pseudo labels generated during simultaneous training in FixMatch.
- Core assumption: A well-trained teacher network produces pseudo labels with significantly lower error rate than an immature teacher trained simultaneously.
- Evidence anchors:
  - [abstract] "an outer SNN will be first trained with labeled and unlabeled data and thus the percentage of label noise in the pseudo labels generated by the network of the well-trained outer SNN would be lower than that of the teacher in FixMatch, especially in the early stage."
  - [section] "However, it trains the teacher and the student in an SNN simultaneously and since the pseudo labels generated by the teacher in the early stage may not be correct, using them directly may introduce large amount of label noise."
- Break condition: If the outer SNN underfits or overfits, the pseudo labels will not be sufficiently trustworthy, negating the benefit.

### Mechanism 2
- Claim: Deep embedding clustering improves pseudo label trustworthiness by grouping semantically similar samples.
- Mechanism: After high-confidence sampling, the latent representations of the remaining unlabeled samples are clustered. Only samples whose cluster assignment matches their predicted class are retained in the trusted subset.
- Core assumption: Samples within a cluster are semantically similar, so consistent cluster-class assignment indicates correct labeling.
- Evidence anchors:
  - [section] "we extract the latent representations of the unlabeled data which has passed the τ select selection. We do a deep embedding clustering [11, 12] on the representations and choose the unlabeled data whose clustering results are consistent with their predicted class."
- Break condition: If the clustering is unstable or poorly aligned with class boundaries, incorrect samples may still be included.

### Mechanism 3
- Claim: Robust loss functions (e.g., symmetric cross-entropy) mitigate the impact of residual noisy labels.
- Mechanism: Inner SNN training uses SCE instead of standard CE, which is theoretically proven to be less sensitive to label noise.
- Core assumption: The inner SNN still encounters some noisy pseudo labels; robust loss functions reduce their negative impact on convergence.
- Evidence anchors:
  - [section] "the widely used loss functions in robust learning is symmetric loss functions [13, 14] which has been proven to be robust to noisy labels."
- Break condition: If noise rate is too high, even robust losses may not fully prevent model degradation.

## Foundational Learning

- Concept: Semi-supervised learning (SSL) basics
  - Why needed here: Understanding how unlabeled data can be leveraged to improve model generalization.
  - Quick check question: What is the key difference between supervised and semi-supervised learning in terms of data usage?

- Concept: Siamese neural networks (SNNs)
  - Why needed here: KD-FixMatch uses two identical weight-sharing networks (teacher and student) in both outer and inner SNNs.
  - Quick check question: In FixMatch, how are the teacher and student networks related in terms of weights?

- Concept: Knowledge distillation
  - Why needed here: KD-FixMatch applies knowledge distillation by transferring knowledge from the outer SNN (teacher) to the inner SNN (student).
  - Quick check question: What is the primary purpose of knowledge distillation in model training?

## Architecture Onboarding

- Component map:
  - Outer SNN -> Pseudo label generation -> High-confidence selector -> Deep embedding clustering -> Trusted subset -> Inner SNN

- Critical path:
  1. Train outer SNN (labeled + unlabeled).
  2. Generate pseudo labels on unlabeled data.
  3. Filter by confidence threshold.
  4. Apply deep embedding clustering to further filter.
  5. Initialize inner SNN.
  6. Train inner SNN (labeled + unlabeled + trusted subset) with chosen loss.
  7. Return inner SNN as final model.

- Design tradeoffs:
  - Sequential vs. simultaneous training: Sequential training reduces early noise but increases total training time.
  - Clustering vs. pure confidence filtering: Clustering improves precision at the cost of added computation.
  - Robust loss vs. standard CE: Robust loss mitigates noise but may converge slower.

- Failure signatures:
  - Poor outer SNN performance → Low-quality pseudo labels → Degraded inner SNN performance.
  - Clustering instability → Erroneous trusted subset → Inner SNN overfitting to noise.
  - Over-aggressive confidence threshold → Too few trusted samples → Insufficient regularization.

- First 3 experiments:
  1. Train outer SNN on SVHN with 4k labels, evaluate pseudo label quality (accuracy, noise rate).
  2. Train inner SNN with only high-confidence pseudo labels (no clustering), compare vs. full KD-FixMatch.
  3. Replace SCE with CE in inner SNN training, measure impact on test accuracy under noisy labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different deep embedding clustering algorithms compare in terms of performance for trusted pseudo label selection in KD-FixMatch?
- Basis in paper: [explicit] The paper mentions using deep embedding clustering to select trusted pseudo labels but does not explore different clustering algorithms.
- Why unresolved: The paper only uses one specific clustering method without comparing alternatives, leaving uncertainty about whether other clustering approaches could yield better results.
- What evidence would resolve it: Systematic comparison of KD-FixMatch performance using different clustering algorithms (e.g., K-means, DBSCAN, spectral clustering) on the same datasets would clarify the impact of clustering choice.

### Open Question 2
- Question: What is the optimal threshold combination (τ_select, τ_inner) for maximizing KD-FixMatch performance across different datasets?
- Basis in paper: [explicit] The paper sets τ_select=0.80 and τ_inner=0.95 without exploring other combinations or using grid search.
- Why unresolved: The thresholds were chosen heuristically rather than optimized, and different datasets may have different optimal values.
- What evidence would resolve it: Extensive hyperparameter search across various threshold combinations on multiple datasets would identify optimal values and their sensitivity.

### Open Question 3
- Question: How does KD-FixMatch scale with different amounts of labeled data beyond the tested ranges?
- Basis in paper: [inferred] The paper tests with 400, 1000, 2000, and 4000 labeled examples but doesn't explore the full spectrum from very few to many labels.
- Why unresolved: The performance trend with increasing labeled data is not fully characterized, particularly at extremes.
- What evidence would resolve it: Experiments with a wider range of labeled data amounts (e.g., 50, 100, 8000, 16000) would show the scalability and limitations of KD-FixMatch.

### Open Question 4
- Question: How sensitive is KD-FixMatch to the choice of base model architecture?
- Basis in paper: [explicit] The paper only uses EfficientNet-B0 pre-trained on ImageNet.
- Why unresolved: Using only one architecture limits understanding of how KD-FixMatch generalizes to different model types and sizes.
- What evidence would resolve it: Testing KD-FixMatch with various architectures (e.g., ResNet, Vision Transformers, smaller/larger EfficientNets) would reveal architecture dependence.

## Limitations
- The paper does not provide implementation details for the deep embedding clustering method, which is critical for reproducing the trusted pseudo-label selection process.
- Specific parameters for RandAugment and EMA configurations are unspecified, potentially affecting performance replication.
- The performance gains, while consistent, are moderate (2.44-5.81%) and may not justify the added complexity for all applications.

## Confidence
- **High Confidence**: The sequential training approach and performance improvements over FixMatch (2.44-5.81% accuracy gains) are well-supported by experimental results across four datasets.
- **Medium Confidence**: The theoretical justification for using symmetric cross-entropy loss and deep embedding clustering is sound, but the empirical impact of these components is not isolated in ablation studies.
- **Low Confidence**: The exact implementation details of the trusted subset selection process (high-confidence threshold and clustering consistency) are unclear, making faithful reproduction challenging.

## Next Checks
1. **Ablation Study**: Run KD-FixMatch with and without deep embedding clustering to quantify its contribution to performance gains.
2. **Noise Sensitivity Analysis**: Measure inner SNN performance using CE vs. SCE under varying noise levels to validate the robustness claim.
3. **Cluster Consistency Validation**: Evaluate the agreement between cluster assignments and pseudo labels to confirm the effectiveness of the clustering refinement step.