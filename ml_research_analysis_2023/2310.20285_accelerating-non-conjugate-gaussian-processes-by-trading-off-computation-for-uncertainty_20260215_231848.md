---
ver: rpa2
title: Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation For
  Uncertainty
arxiv_id: '2310.20285'
source_url: https://arxiv.org/abs/2310.20285
tags:
- data
- algorithm
- regression
- pfiq
- newton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IterGLM, a scalable family of algorithms for
  approximate inference in generalized linear models (GLMs) with log-concave likelihoods.
  The key idea is to reinterpret Newton's method for Laplace approximation as a sequence
  of GP regression problems, and solve each regression problem approximately using
  iterative solvers.
---

# Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation For Uncertainty

## Quick Facts
- arXiv ID: 2310.20285
- Source URL: https://arxiv.org/abs/2310.20285
- Authors: 
- Reference count: 40
- Key outcome: This paper proposes IterGLM, a scalable family of algorithms for approximate inference in generalized linear models (GLMs) with log-concave likelihoods.

## Executive Summary
This paper introduces IterGLM, a novel approach for scalable approximate inference in generalized linear models with log-concave likelihoods. The method reinterprets Newton's method for Laplace approximation as a sequence of GP regression problems, solving each approximately using iterative solvers. By trading off computation for uncertainty quantification, IterGLM achieves significant speedups while maintaining good predictive performance. The approach scales to large-scale GP classification problems with over 100,000 data points.

## Method Summary
IterGLM reformulates Newton's method for Laplace approximation as sequential GP regression problems with pseudo-targets. Instead of solving each linear system exactly, it uses iterative solvers that build low-rank approximations of the precision matrix while maintaining uncertainty quantification. The method recycles calculations between Newton steps and compresses information to reduce memory usage. Action policies (like conjugate gradient) determine which directions to explore in the solution space, and buffer compression controls memory while maximizing uncertainty reduction.

## Key Results
- Achieves significant speedups compared to baselines while maintaining good predictive performance
- Scales to large-scale GP classification problems with over 100,000 data points
- Maintains uncertainty quantification through iterative solver approximations
- Demonstrates effective recycling and compression mechanisms across Newton steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Newton's method for Laplace approximation can be reformulated as a sequence of GP regression problems with pseudo-targets.
- Mechanism: Each Newton step involves solving a linear system with matrix $\hat{K}(f_i) = K + W(f_i)^{-1}$. By defining pseudo-targets $\hat{y}(f_i) = f_i + W(f_i)^{-1}\nabla\log p(y|f_i)$, the Newton update becomes equivalent to computing the mean of a GP posterior with these pseudo-targets and noise precision $W(f_i)^{-1}$.
- Core assumption: The negative Hessian of the log-likelihood $W(f_i)$ is invertible (or has a well-defined pseudo-inverse for classification).
- Evidence anchors:
  - [abstract] "The key idea is to reinterpret Newton's method for Laplace approximation as a sequence of GP regression problems"
  - [section] "With Proposition 1 (i.e. assuming that $W(f_i)^{-1}$ exists), we can rewrite Eq. (8) as $m_{i,*}(\cdot) = m(\cdot) + K(\cdot, X)\hat{K}(f_i)^{-1}(\hat{y}(f_i) - m) = m(\cdot) + K(\cdot, X)\hat{K}(f_i)^{-1}(\hat{y}(f_i) - m)$"
  - [corpus] Weak - related papers focus on different approximation schemes
- Break condition: When $W(f_i)$ is not invertible and the pseudo-inverse cannot be computed efficiently, or when the GP prior is misspecified relative to the data.

### Mechanism 2
- Claim: Iterative solvers with carefully chosen action policies can efficiently solve each GP regression subproblem while maintaining uncertainty quantification.
- Mechanism: Instead of solving each linear system exactly, IterGP uses a probabilistic linear solver that iteratively builds a low-rank approximation of the inverse precision matrix. The policy determines which directions (actions) to explore in the solution space. Conjugate gradient-like policies target directions with highest residual error.
- Core assumption: The GP regression problems across Newton steps are sufficiently similar that information from previous steps can be reused.
- Evidence anchors:
  - [abstract] "solve each regression problem approximately using iterative solvers"
  - [section] "This policy is equivalent to simply using a subset X_{1:j} of the data and performing exact GP regression (e.g. via a Cholesky decomposition) in each Newton iteration"
  - [section] "For example, using the current residual s_j = r_{j-1} : = \hat{y}(f_i) - m - \hat{K}(f_i)v_{j-1}, approximately targets those data most, where the posterior mean prediction is far off"
  - [corpus] Weak - related papers don't directly address iterative solver policies
- Break condition: When the GP problems across Newton steps differ significantly, making recycled information misleading, or when the policy fails to explore critical directions in the solution space.

### Mechanism 3
- Claim: Computation can be recycled and compressed across Newton steps to reduce runtime and memory costs while preserving uncertainty quantification.
- Mechanism: Between Newton steps, the algorithm maintains buffers of previous action vectors and their products with K. These can be used to construct an initial approximation to the precision matrix for the next step via a virtual solver run. Eigenvalue truncation compresses this information to control memory usage while maximizing uncertainty reduction.
- Core assumption: The covariance structure of the GP problems remains similar across Newton iterations, making past computations relevant for future steps.
- Evidence anchors:
  - [abstract] "Computationally, the method recycles calculations between Newton steps and compresses information to reduce memory usage"
  - [section] "From a probabilistic perspective, through Eq. (13), we can quantify the effect of C_0 on the total marginal uncertainty of the predictions at the training data"
  - [section] "The second term Tr(M) describes the reduction of the prior uncertainty due to C_0. It can be maximized (which is our goal) when S contains those eigenvectors of \hat{K} with the largest eigenvalues"
  - [corpus] Weak - related papers focus on different approximation strategies
- Break condition: When the Newton iterates move far from previous iterates, making recycled information irrelevant, or when compression removes too much information, degrading solution quality.

## Foundational Learning

- Concept: Gaussian Process Regression and Laplace Approximation
  - Why needed here: The algorithm builds on understanding how GP regression problems arise from Laplace approximation in GLMs, and how the posterior mean and covariance can be computed via linear system solves.
  - Quick check question: In a GP regression problem with observations y observed with Gaussian noise N(0, σ²I), what is the posterior mean in terms of the prior mean m, kernel K, and observations y?

- Concept: Iterative Linear Solvers and Probabilistic Numerics
  - Why needed here: The algorithm uses iterative solvers not just for computational efficiency but to maintain uncertainty quantification about the solution. Understanding how these solvers work and how they quantify uncertainty is crucial.
  - Quick check question: In the context of solving Ax = b where A is symmetric positive definite, what does the conjugate gradient method do in each iteration, and how does it build up information about the solution?

- Concept: Low-rank Matrix Approximations and Compression
  - Why needed here: The algorithm uses low-rank approximations of the precision matrix and compresses information across iterations. Understanding how these approximations work and their trade-offs is essential.
  - Quick check question: If a symmetric positive definite matrix A has eigenvalue decomposition A = UΛU^T, what is the best rank-r approximation to A in terms of the Frobenius norm, and how is it constructed?

## Architecture Onboarding

- Component map:
  - Outer loop (Algorithm 2): Manages Newton iterations, updates f_i, and coordinates inner loop solves
  - Inner loop (Algorithm 3): Implements IterGP with virtual solver run for each GP regression subproblem
  - Virtual solver run (Algorithm 1): Reuses and compresses computations across Newton steps
  - Policy module: Determines action vectors for iterative solver (e.g., unit vectors, residuals, or custom policies)
  - Kernel module: Provides matrix-vector products with K and handles kernel-specific optimizations
  - Likelihood module: Computes gradients and Hessians of log-likelihood, handles W^-1 computations

- Critical path:
  1. Newton iteration begins with current f_i
  2. Compute pseudo-targets \hat{y}(f_i) and observation noise W(f_i)^-1
  3. Run inner loop (IterGP) to solve GP regression problem approximately
  4. Update f_{i+1} using approximate solution
  5. Recycle and compress information for next iteration
  6. Check convergence and repeat until stopping criterion met

- Design tradeoffs:
  - Solver iterations vs. Newton steps: More solver iterations per step vs. more steps with fewer iterations per step
  - Policy choice: Unit vectors (data subsampling) vs. residuals (conjugate gradient) vs. custom policies
  - Compression level: Memory usage vs. information preservation and solution quality
  - Exact vs. approximate linear solves: Computational cost vs. uncertainty quantification

- Failure signatures:
  - Divergence: Newton iterates move away from mode, possibly due to poor initial guess or ill-conditioned problems
  - Slow convergence: Too few solver iterations or ineffective policy choices
  - Memory overflow: Insufficient compression or too many solver iterations accumulating buffers
  - Poor uncertainty quantification: Over-compression or policy that misses important solution directions

- First 3 experiments:
  1. Binary classification on 1D synthetic data: Verify basic functionality, visualize policy effects, and test recycling mechanism
  2. Poisson regression: Test algorithm on non-conjugate likelihood, explore solver iteration vs. Newton step tradeoff
  3. Large-scale multi-class classification: Demonstrate scalability, compare with data subsampling baseline, test compression effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the IterGLM method to the choice of policy for selecting actions within the inner loop, and what are the optimal policy choices for different types of likelihood functions?
- Basis in paper: [explicit] The paper discusses different policies like unit vector and conjugate gradient, but does not provide a comprehensive analysis of policy effectiveness across different likelihood functions.
- Why unresolved: The paper mentions the policy choice but does not experimentally compare multiple policies or provide guidelines for selecting optimal policies for specific likelihood functions.
- What evidence would resolve it: Experiments comparing IterGLM performance with different policies (unit vector, conjugate gradient, random selection, etc.) across various likelihood functions (logistic, probit, softmax, etc.) would provide evidence.

### Open Question 2
- Question: What is the impact of the buffer size R on the accuracy and computational efficiency of IterGLM in practice, and how does this vary with problem size and complexity?
- Basis in paper: [explicit] The paper introduces buffer compression to control memory usage but only briefly discusses its impact through a single experiment in Fig. 4.
- Why unresolved: While the paper demonstrates buffer compression conceptually, it doesn't provide a systematic analysis of how buffer size affects performance across different problem scales or data distributions.
- What evidence would resolve it: Systematic experiments varying buffer size R across different problem sizes (N from 100 to 100,000) and data complexities would show the relationship between buffer size, accuracy, and efficiency.

### Open Question 3
- Question: How does IterGLM compare to other scalable inference methods for GLMs in terms of both predictive performance and uncertainty quantification?
- Basis in paper: [inferred] The paper demonstrates IterGLM's performance on specific datasets but doesn't compare it against other scalable inference methods like variational inference, EP, or subsampling-based approaches.
- Why unresolved: The paper focuses on demonstrating IterGLM's unique features but doesn't benchmark it against existing scalable methods that also trade computation for uncertainty.
- What evidence would resolve it: Head-to-head comparisons of IterGLM against state-of-the-art scalable methods (variational inference, expectation propagation, stochastic gradient MCMC) on standard benchmark datasets would provide this evidence.

### Open Question 4
- Question: How does the performance of IterGLM scale with the number of classes C in multi-class classification problems, and what are the computational bottlenecks as C increases?
- Basis in paper: [explicit] The paper demonstrates IterGLM on a 10-class problem but doesn't analyze how performance scales with increasing C.
- Why unresolved: While the paper shows IterGLM works for multi-class problems, it doesn't investigate how the computational complexity or accuracy changes as the number of classes grows.
- What evidence would resolve it: Experiments varying the number of classes from 2 to 100 on datasets of fixed size would reveal how IterGLM's runtime, memory usage, and accuracy scale with C.

## Limitations
- Limited empirical validation of uncertainty quantification claims
- Recycling mechanisms effectiveness depends on Newton iterates staying in local region
- Comparison with baselines limited to synthetic data rather than real-world datasets

## Confidence

- **Mechanism Claims**: Medium confidence - The mathematical derivation is sound, but empirical validation of the core mechanisms is limited.
- **Scalability Claims**: High confidence - The method demonstrably scales to 100K+ data points with significant speedups.
- **Uncertainty Quantification**: Low confidence - The paper claims to maintain uncertainty quantification but provides limited empirical evidence of this.

## Next Checks

1. **Uncertainty Calibration**: Test the method on problems where well-calibrated uncertainty estimates are critical (e.g., active learning or Bayesian optimization) and compare the quality of uncertainty estimates against exact inference methods.

2. **Policy Robustness**: Evaluate the method's performance across diverse datasets and problem structures to test whether the CG policy consistently outperforms data subsampling or whether the advantage depends on specific data characteristics.

3. **Compression Sensitivity**: Systematically vary the compression level and measure its impact on both computational efficiency and predictive performance to establish the optimal trade-off curve for different problem scales.