---
ver: rpa2
title: Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors
arxiv_id: '2312.04963'
source_url: https://arxiv.org/abs/2312.04963
tags:
- diffusion
- generation
- priors
- multi-view
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bidirectional Diffusion (BiDiff) combines 2D and 3D diffusion models
  to generate high-quality 3D objects efficiently. Unlike prior methods relying solely
  on 2D priors (causing geometry issues) or 3D priors (causing texture limitations),
  BiDiff employs a unified framework with bidirectional guidance.
---

# Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors

## Quick Facts
- **arXiv ID**: 2312.04963
- **Source URL**: https://arxiv.org/abs/2312.04963
- **Reference count**: 40
- **Primary result**: Bidirectional Diffusion (BiDiff) achieves CLIP R-precision of 0.79 for sampling (40s) and 0.85 with refinement (20min), outperforming prior optimization methods while reducing optimization time from 3.4 hours to 20 minutes.

## Executive Summary
Bidirectional Diffusion (BiDiff) is a unified framework that combines 2D and 3D diffusion models to generate high-quality 3D objects from text prompts. Unlike prior methods that rely solely on 2D priors (causing geometry issues) or 3D priors (causing texture limitations), BiDiff employs bidirectional guidance between the two processes to enable separate control of texture and geometry generation. The method shows significant improvements in both quality and efficiency, achieving state-of-the-art results on standard benchmarks while reducing generation time from hours to minutes.

## Method Summary
BiDiff combines a 3D diffusion model (based on SparseConv U-Net) that generates SDF geometry with a 2D diffusion model (based on DeepFloyd IF) that generates multi-view textures. During each denoising step, the 3D output is rendered into 2D images and used to guide the 2D diffusion process, while the 2D output is back-projected to 3D to guide the 3D diffusion model. This bidirectional guidance ensures consistency between texture and geometry generation. The framework also includes a prior enhancement strategy that allows independent control of the strength of 2D and 3D priors, enabling separate manipulation of texture and geometry. The model is trained on synthetic datasets like ShapeNet-Chair and Objaverse, using CLIP for text conditioning and optimization refinement.

## Key Results
- Achieves CLIP R-precision of 0.79 for sampling (40s) and 0.85 with refinement (20min)
- Reduces optimization time from 3.4 hours to 20 minutes while improving geometry quality
- Outperforms prior optimization methods on standard benchmarks
- Enables separate control of texture and geometry through independent prior enhancement

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Guidance
The bidirectional guidance between 2D and 3D diffusion processes prevents inconsistent generation directions. During each denoising step, the 3D diffusion output is rendered into 2D images and used as guidance for the 2D diffusion model, while the 2D diffusion output is back-projected to 3D to guide the 3D diffusion model. This mutual regularization ensures the two processes generate consistent texture and geometry.

### Mechanism 2: Separate Control of Texture and Geometry
The decoupled 2D and 3D diffusion processes enable independent control of texture and geometry generation. The 2D diffusion model focuses on texture generation while the 3D diffusion model focuses on geometry, allowing independent manipulation of each aspect through the prior enhancement strategy.

### Mechanism 3: Compensation for Limited 3D Dataset Diversity
The 2D diffusion model, pretrained on much larger datasets than 3D datasets, provides rich texture priors while the 3D diffusion model provides geometry priors. This combination compensates for the limited texture diversity in 3D datasets, producing more realistic textures for generated objects.

## Foundational Learning

- **Concept**: Score Distillation Sampling (SDS) loss
  - Why needed here: SDS is the optimization framework used to generate 3D objects from 2D diffusion models, which BiDiff improves upon.
  - Quick check question: What is the fundamental difference between SDS loss and the bidirectional diffusion approach in BiDiff?

- **Concept**: Signed Distance Field (SDF) representation
  - Why needed here: SDF is used as the 3D representation in BiDiff, enabling efficient 3D geometry learning and rendering.
  - Quick check question: How does the SDF representation enable efficient 3D rendering compared to voxel representations?

- **Concept**: ControlNet architecture
  - Why needed here: ControlNet is used to condition the 2D diffusion model on 3D-consistent features from the 3D diffusion process.
  - Quick check question: How does ControlNet enable conditioning of a pretrained 2D diffusion model on additional guidance signals?

## Architecture Onboarding

- **Component map**: 3D noisy SDF -> 3D denoising (with 2D guidance) -> rendered 2D images -> 2D denoising (with 3D guidance) -> refined 3D SDF
- **Critical path**: 3D noisy SDF → 3D denoising (with 2D guidance) → rendered 2D images → 2D denoising (with 3D guidance) → refined 3D SDF
- **Design tradeoffs**: Tradeoff between texture quality (favoring 2D priors) and geometry quality (favoring 3D priors); tradeoff between generation speed (sampling) and quality (optimization); tradeoff between diversity (lower guidance scales) and faithfulness (higher guidance scales)
- **Failure signatures**: Texture-geometry inconsistency (textures don't align properly with 3D geometry); multi-view inconsistency (rendered views show different objects); Janus problem (multiple faces on one object); texture blurriness (textures lack detail and sharpness)
- **First 3 experiments**: 1) Train BiDiff with only 3D priors (no 2D priors) to verify geometry quality; 2) Train BiDiff with only 2D priors (no 3D priors) to verify texture quality; 3) Train BiDiff with both priors but without bidirectional guidance to test its necessity

## Open Questions the Paper Calls Out

### Open Question 1
How does the BiDiff model perform on more complex 3D objects beyond chairs, such as highly detailed organic shapes or large-scale architectural structures? The paper primarily evaluates on ShapeNet-Chair and Objaverse datasets, leaving performance on complex objects untested.

### Open Question 2
What is the impact of varying the noise levels during the bidirectional diffusion process on the quality and diversity of the generated 3D objects? The paper mentions noise levels but doesn't analyze how varying them affects the final output.

### Open Question 3
How does the BiDiff model handle text prompts that are ambiguous or contradictory, and what strategies are employed to resolve such ambiguities? The paper doesn't discuss handling of ambiguous or contradictory text prompts.

## Limitations

- Performance on more diverse, real-world datasets beyond synthetic objects remains untested
- Scalability to higher resolution outputs without quality degradation is unproven
- The 20-minute optimization time may not generalize across different object complexities

## Confidence

**High confidence** claims:
- The fundamental approach of combining 2D and 3D diffusion models is sound
- The bidirectional guidance concept is theoretically valid
- Performance improvements over pure 2D methods are well-demonstrated

**Medium confidence** claims:
- The separate control of texture and geometry is effectively achieved
- The 20-minute optimization time is consistently achievable
- CLIP R-precision improvements represent meaningful quality gains

**Low confidence** claims:
- Generalization to complex real-world objects beyond synthetic datasets
- Robustness across different object categories and text prompts
- Scalability to higher resolution outputs without quality degradation

## Next Checks

1. **Cross-dataset validation**: Test BiDiff on more diverse datasets like CO3D or real-world 3D scans to verify if the 0.85 CLIP R-precision holds beyond synthetic objects.

2. **Ablation on guidance mechanisms**: Systematically disable different components of the bidirectional guidance (e.g., remove back-projection, vary guidance scales) to quantify each component's contribution to the final quality.

3. **Long-term consistency test**: Generate multiple views of the same object with different seeds and measure multi-view consistency using a geometric consistency metric.