---
ver: rpa2
title: Bootstrap Your Own Variance
arxiv_id: '2312.03213'
source_url: https://arxiv.org/abs/2312.03213
tags:
- learning
- bayesian
- posterior
- figure
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Bootstrap Your Own Variance (BYOV) combines Bootstrap Your Own\
  \ Latent (BYOL) with Bayes by Backprop (BBB) to enable uncertainty estimation in\
  \ self-supervised learning (SSL). BYOV learns an approximate posterior over network\
  \ parameters and uses the student\u2019s maximum a posteriori (MAP) parameters to\
  \ update the teacher parameters, unlike standard BYOL which uses deterministic parameters."
---

# Bootstrap Your Own Variance

## Quick Facts
- arXiv ID: 2312.03213
- Source URL: https://arxiv.org/abs/2312.03213
- Reference count: 20
- One-line primary result: BYOV improves ImageNet-1k ECE by 2.83% and Brier score by 1.03% over deterministic BYOL while enabling uncertainty-aware pruning

## Executive Summary
Bootstrap Your Own Variance (BYOV) extends Bootstrap Your Own Latent (BYOL) with Bayes by Backprop (BBB) to enable uncertainty estimation in self-supervised learning. The method learns an approximate posterior over network parameters using variational inference, then uses the student's maximum a posteriori (MAP) parameters to update the teacher in the BYOL framework. Experiments on ImageNet-1k show BYOV achieves better calibration than deterministic BYOL while maintaining competitive accuracy. The learned posterior enables signal-to-noise ratio (SNR) pruning that preserves up to 12% higher accuracy than magnitude-based pruning at 25% sparsity.

## Method Summary
BYOV implements BBB within the BYOL framework by parameterizing network weights with Gaussian posteriors (mean and variance). During training, weights are sampled from this posterior for the student network, while the teacher uses MAP parameters. The loss combines the standard BYOL similarity objective with a KL divergence term to a chosen prior (N(0,I), N(μT,I), or N(μT,ΣT)). The KL term is annealed from 0 to 1 during training. At inference, predictions use the teacher's MAP parameters, while uncertainty estimates come from Monte Carlo sampling the posterior. SNR pruning uses the ratio of posterior mean to standard deviation to rank parameters for removal.

## Key Results
- BYOV improves ImageNet-1k test ECE by 2.83% and Brier score by 1.03% over deterministic BYOL
- SNR pruning preserves up to 12% higher accuracy than magnitude-based pruning with 25% sparser models
- Predictive standard deviation of BYOV correlates with supervised BBB uncertainty, suggesting meaningful epistemic uncertainty capture
- Different prior choices affect posterior variance trajectories but not SNR-based pruning performance due to scale invariance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learned posterior over network parameters in BYOV captures meaningful epistemic uncertainty that aligns with supervised Bayesian models.
- Mechanism: BYOV learns a parameter posterior through Bayes by Backprop applied to a self-supervised loss, then uses the MAP parameters for teacher updates. The resulting predictive standard deviation correlates with that of a supervised BBB model, suggesting the posterior encodes useful uncertainty.
- Core assumption: The generalized posterior learned in SSL (without explicit likelihood) still captures the same uncertainty structure as a supervised posterior.
- Evidence anchors:
  - [abstract] "The predictive standard deviation of BYOV aligns with a Gaussian distribution and correlates with uncertainty estimates from a supervised BBB model, suggesting the learned posterior captures meaningful model uncertainty useful for downstream tasks."
  - [section 4.2] "In Figure 1b we look at the relationship between uncertainty of the BYOV predictive distribution, and uncertainty under a supervised BBB model. We observe that the relationship between the predictive standard deviation of both models can be suitably captured using a Gaussian, which gives credence to using BYOV as a proxy for the uncertainty of a supervised BBB model."
  - [corpus] Weak evidence: neighbors include BYOL-based works but no direct SSL+BBB uncertainty alignment studies.
- Break condition: If the SSL objective does not preserve the epistemic structure of the posterior, the alignment would break.

### Mechanism 2
- Claim: SNR-based pruning preserves higher accuracy than magnitude-based pruning in SSL models because SNR reflects parameter importance given uncertainty.
- Mechanism: BYOV learns a posterior variance for each parameter. The SNR (|μ|/σ) is used to rank parameters for pruning. Higher SNR parameters are kept, yielding better downstream accuracy (up to 12% better with 25% sparsity).
- Core assumption: In SSL, parameter posterior variance is a better indicator of downstream utility than raw magnitude.
- Evidence anchors:
  - [abstract] "Layerwise signal-to-noise ratio (SNR) analysis reveals that SNR pruning preserves up to 12% higher accuracy than magnitude-based pruning with a 25% sparser model."
  - [section 4.4] "A high posterior variance indicates that the model lacks confidence in a weight's value. We can use this to prune the network, removing weights where the network lacks confidence."
  - [corpus] No direct pruning comparison in neighbors; evidence is internal.
- Break condition: If the SSL posterior variance does not correlate with true parameter importance, SNR pruning would underperform.

### Mechanism 3
- Claim: Prior choice affects posterior variance trajectories but not SNR-based pruning performance, because network outputs are invariant to weight scale.
- Mechanism: Three priors (N(0,I), N(μT,I), N(μT,ΣT)) yield different posterior standard deviations across layers, but SNR evolution is similar. This explains why N(0,I) and N(μT,I) perform similarly in ECE and reliability despite different variances.
- Core assumption: Performance is invariant to constant shifts in weight scale, so SNR (normalized by variance) is the key metric.
- Evidence anchors:
  - [section 4.2] "We observe that the choice of prior makes a large difference on the learned layer-wise standard deviations. However, if we look at the posterior SNR (Figure 3), we see more similarity across priors, particularly between N(0,I) and N(μT,I), supporting the idea that performance is relatively invariant to rescaling."
  - [corpus] Weak evidence: no corpus neighbor directly addresses prior invariance in SSL+BBB.
- Break condition: If downstream tasks are sensitive to absolute weight scale, SNR-based pruning would fail.

## Foundational Learning

- Concept: Bayes by Backprop (BBB) as a variational inference method for learning approximate posterior over network weights.
  - Why needed here: BYOV extends BBB to SSL to enable uncertainty estimation without labels.
  - Quick check question: How does BBB approximate the posterior, and what role does the KL term play?

- Concept: Exponential Moving Average (EMA) teacher in BYOL and its adaptation in BYOV.
  - Why needed here: BYOV uses the student's MAP parameters to update the teacher EMA, maintaining consistency with BYOL while incorporating Bayesian updates.
  - Quick check question: What is the purpose of the EMA teacher in BYOL, and how is it modified in BYOV?

- Concept: Self-supervised learning via contrastive similarity without negative pairs.
  - Why needed here: BYOV builds on BYOL's negative-free framework, so understanding the loss and augmentation pipeline is essential.
  - Quick check question: How does BYOL minimize similarity between two views, and why is the predictor network necessary?

## Architecture Onboarding

- Component map:
  Student (Gaussian posterior weights) -> BYOL loss (cosine similarity) -> KL divergence -> Posterior update
  Teacher (EMA of student MAP) <- Student MAP update
  Inference: Teacher encoder + MC sampling from posterior

- Critical path:
  1. Sample weights from posterior for student forward pass
  2. Compute BYOL similarity loss between student and teacher views
  3. Update student posterior parameters via gradient descent on ELBO
  4. Update teacher EMA with student MAP parameters
  5. At inference, use teacher encoder (or sampled student) for downstream tasks

- Design tradeoffs:
  - Full Bayesian vs partial: Full BBB doubles parameters but minimal memory overhead due to reparameterization
  - Prior choice: N(0,I) vs N(μT,I) affects variance but not SNR-based pruning
  - MC sampling: More samples improve uncertainty estimate but increase cost

- Failure signatures:
  - Training instability: Often due to dynamic prior N(μT,ΣT) or improper KL annealing
  - Poor calibration: May indicate mismatch between SSL posterior and true epistemic uncertainty
  - Degraded SSL performance: Likely from weight decay or KL term too strong

- First 3 experiments:
  1. Train BYOV with N(0,I) prior and KL β schedule 0→1; verify ECE improvement over deterministic BYOL
  2. Compare SNR vs magnitude pruning on a held-out subset; measure accuracy vs sparsity
  3. Analyze layerwise SNR trajectories under different β schedules; confirm invariance to prior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of prior distribution (e.g., N(0,I), N(μT,I), N(μT,ΣT)) affect the quality of uncertainty estimates in BYOV for downstream tasks?
- Basis in paper: [explicit] The paper explores the impact of different prior choices on the Bootstrap Your Own Variance (BYOV) posterior and demonstrates that the resulting uncertainty estimates are distributionally aligned with outputs from a Bayesian supervised model.
- Why unresolved: The paper provides a comparison of different priors but does not fully explore how these choices impact the quality of uncertainty estimates for specific downstream tasks.
- What evidence would resolve it: Conduct experiments comparing the performance of BYOV with different priors on a variety of downstream tasks, such as image classification and object detection, and analyze the impact on uncertainty estimation accuracy.

### Open Question 2
- Question: Can BYOV be effectively applied to other types of self-supervised learning algorithms beyond BYOL, and what modifications might be necessary?
- Basis in paper: [inferred] The paper focuses on applying Bayes by Backprop (BBB) to Bootstrap Your Own Latent (BYOL), but does not explore its applicability to other SSL algorithms.
- Why unresolved: The paper does not investigate the potential for BYOV to be adapted to other SSL frameworks, leaving open the question of its generalizability.
- What evidence would resolve it: Implement BYOV with other SSL algorithms (e.g., SimCLR, MoCo) and compare the results in terms of uncertainty estimation and downstream task performance.

### Open Question 3
- Question: How does the signal-to-noise ratio (SNR) pruning strategy compare to other pruning methods in terms of computational efficiency and model compression for SSL models?
- Basis in paper: [explicit] The paper shows that SNR pruning preserves better performance than magnitude-based pruning in SSL models, with a sparser model.
- Why unresolved: The paper does not provide a comprehensive comparison of SNR pruning with other advanced pruning techniques, such as structured pruning or iterative pruning, in terms of computational efficiency and model compression.
- What evidence would resolve it: Conduct experiments comparing SNR pruning with other pruning methods, including computational cost analysis and model size reduction metrics, across various SSL models and tasks.

## Limitations
- The alignment between SSL-posterior uncertainty and supervised-BBB uncertainty lacks theoretical justification for why SSL preserves epistemic structure
- SNR-based pruning improvement relies on internal comparisons without ablation on variance vs magnitude contribution
- Prior invariance claims are observational without rigorous analysis of when scale invariance breaks

## Confidence
- Mechanism 1 (SSL uncertainty alignment): Medium - supported by empirical correlation but no theoretical justification
- Mechanism 2 (SNR pruning): Medium - improvement shown but lacks ablation on variance vs magnitude contribution
- Mechanism 3 (Prior invariance): Medium - empirical observation of SNR similarity but no theoretical analysis

## Next Checks
1. Perform ablation study isolating the contribution of posterior variance vs magnitude in SNR pruning by comparing against magnitude-only pruning using learned variances as weights
2. Test whether BYOV uncertainty degrades when SSL objective is modified (e.g., adding negatives or changing augmentation strength) to probe sensitivity to epistemic structure preservation
3. Evaluate BYOV calibration and pruning performance on out-of-distribution datasets to test whether learned posterior generalizes beyond the ImageNet training distribution