---
ver: rpa2
title: 'To token or not to token: A Comparative Study of Text Representations for
  Cross-Lingual Transfer'
arxiv_id: '2310.08078'
source_url: https://arxiv.org/abs/2310.08078
tags:
- languages
- language
- latin
- indo-european
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares tokenization-based (BERT, mBERT) and tokenization-free
  (CANINE, PIXEL) language models for cross-lingual transfer tasks. A novel Language
  Quotient (LQ) metric is introduced to evaluate model performance across 19 source
  and 133 target languages in POS tagging, dependency parsing, and NER tasks.
---

# To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer

## Quick Facts
- arXiv ID: 2310.08078
- Source URL: https://arxiv.org/abs/2310.08078
- Reference count: 18
- Image-based models (PIXEL) excel when source and target languages share visually similar scripts

## Executive Summary
This paper presents a comprehensive comparative study of tokenization-based (BERT, mBERT) and tokenization-free (CANINE, PIXEL) language models for cross-lingual transfer across 19 source and 133 target languages. The authors introduce a novel Language Quotient (LQ) metric to evaluate model performance on POS tagging, dependency parsing, and NER tasks. Through extensive experimentation, the study reveals that model effectiveness depends critically on the interplay between task requirements and language characteristics, with image-based models excelling for visually similar scripts, character-level models dominating in syntactic relationship tasks, and segmentation-based models proving superior for word-meaning-dependent tasks.

## Method Summary
The study employs a systematic experimental framework comparing four pre-trained language models (BERT, mBERT, CANINE, PIXEL) across 19 source languages and 133 target languages. Models are fine-tuned on source languages and evaluated using zero-shot and few-shot scenarios, with results aggregated into a composite Language Quotient (LQ) score. The evaluation covers three NLP tasks using Universal Dependencies and MasakhaNER datasets, with specific focus on how different tokenization approaches affect cross-lingual transfer performance across diverse language pairs.

## Key Results
- PIXEL excels when source and target languages share visually similar scripts
- Character-level models (CANINE) outperform others in dependency parsing
- Segmentation-based models (mBERT) prove superior for POS tagging and NER tasks

## Why This Works (Mechanism)

### Mechanism 1
PIXEL model performance correlates strongly with visual script similarity between source and target languages. The mechanism leverages visual elements (pixels) rather than traditional tokens, capturing script-based visual patterns. This works when target languages use visually similar scripts to the source, but degrades significantly with dissimilar scripts.

### Mechanism 2
Character-level models (CANINE) excel at dependency parsing due to focus on word relationships. CANINE processes text at character level, capturing finer-grained patterns in syntactic relationships. This approach benefits tasks where word relationships are crucial rather than individual word meanings.

### Mechanism 3
Segmentation-based models (mBERT) excel at POS tagging and NER due to word meaning focus. mBERT operates on token-level representations with predefined vocabulary, enabling better association with word meanings. This approach works well for tasks heavily relying on understanding individual word meanings.

## Foundational Learning

- **Cross-lingual transfer**: The paper compares models for transferring knowledge from high-resource to low-resource languages. Quick check: What is the primary goal of cross-lingual transfer in NLP?

- **Tokenization schemes**: Different models use different tokenization approaches (BERT, mBERT, CANINE, PIXEL). Quick check: How does tokenization affect language model performance across different languages?

- **Evaluation metrics (LQ score)**: The paper introduces a novel metric to evaluate model performance across multiple languages. Quick check: How does the Language Quotient (LQ) metric combine zero-shot and few-shot evaluation?

## Architecture Onboarding

- **Component map**: BERT/mBERT (tokenization-based) -> CANINE (character-level) -> PIXEL (image-based)
- **Critical path**: Model selection → Fine-tuning on source languages → Evaluation on target languages → LQ score calculation
- **Design tradeoffs**: Trade-off between visual similarity (PIXEL) vs semantic understanding (mBERT) vs syntactic relationships (CANINE)
- **Failure signatures**: PIXEL fails with dissimilar scripts, CANINE struggles with semantic tasks, mBERT may underperform with rare words
- **First 3 experiments**:
  1. Replicate POS tagging results with English as source and various European languages as targets
  2. Test dependency parsing with Hindi as source and Marathi/Urdu as targets
  3. Compare NER performance across all models with African languages as both sources and targets

## Open Questions the Paper Calls Out

### Open Question 1
How do tokenization-free models like PIXEL and CANINE perform on low-resource languages with non-Latin scripts that share no visual or linguistic similarities with the source language? The study focuses on comparing model performance based on visual and linguistic similarities but does not investigate cases where both are absent.

### Open Question 2
How does the performance of tokenization-based models like mBERT and BERT change when the source language is not part of their pre-training set? The study mentions BERT outperforming mBERT with Coptic as source but does not explore this systematically.

### Open Question 3
How do tokenization-free models perform on high-resource languages compared to tokenization-based models? The paper recommends using advanced models for high-resource languages but lacks empirical evidence comparing tokenization-free and tokenization-based models in this context.

## Limitations

- LQ metric's weighting between zero-shot and few-shot evaluations may disproportionately influence model rankings
- Limited scope to specific language families and three NLP tasks raises generalizability concerns
- Does not account for dataset quality variations or training data size impacts across languages

## Confidence

**High Confidence**: PIXEL excels with visually similar scripts (validated by French outperforming Russian for English → European transfers)
**Medium Confidence**: CANINE's advantage in dependency parsing relies on assumptions about character-level processing benefits
**Low Confidence**: mBERT's predefined vocabulary "expedites learning" lacks direct empirical support

## Next Checks

1. Conduct controlled experiments varying script similarity while holding other linguistic features constant
2. Perform ablation experiments comparing CANINE against other character-level architectures on dependency parsing
3. Design experiments systematically varying vocabulary size in mBERT to quantify its impact on learning efficiency