---
ver: rpa2
title: My Science Tutor (MyST) -- A Large Corpus of Children's Conversational Speech
arxiv_id: '2309.13347'
source_url: https://arxiv.org/abs/2309.13347
tags:
- corpus
- speech
- science
- children
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the MyST corpus, a large collection of children's
  conversational speech for automatic speech recognition research. The corpus comprises
  approximately 400 hours of speech from 1.3K third to fifth grade students across
  230K utterances in 10.5K virtual tutoring sessions.
---

# My Science Tutor (MyST) -- A Large Corpus of Children's Conversational Speech

## Quick Facts
- arXiv ID: 2309.13347
- Source URL: https://arxiv.org/abs/2309.13347
- Reference count: 0
- Key outcome: Large corpus of children's conversational speech for ASR research, with 400 hours of speech from 1.3K students across 10.5K sessions

## Executive Summary
This paper introduces the MyST corpus, a large collection of children's conversational speech for automatic speech recognition (ASR) research. The corpus comprises approximately 400 hours of speech from 1.3K third to fifth grade students across 230K utterances in 10.5K virtual tutoring sessions. It includes 100K transcribed utterances and is available for non-commercial use under a Creative Commons license, with commercial licensing also available. The corpus was developed as part of a 13-year project investigating the use of a conversational virtual tutor to improve elementary school science learning.

## Method Summary
The MyST corpus was collected through virtual tutoring sessions where children interacted with an AI-powered science tutor. The data was collected in two phases covering eight science modules aligned with classroom content. The corpus was partitioned into training, development, and test sets using stratified sampling to ensure proportional representation of each science module. The authors used an end-to-end transformer model fine-tuned from LibreSpeech using the SpeechBrain toolkit, training on utterances under 30 seconds due to memory limitations.

## Key Results
- Approximately 400 hours of children's conversational speech data collected
- 100K transcribed utterances available for training and evaluation
- Ten organizations have licensed the corpus for commercial use
- Approximately 40 university and not-for-profit research groups have downloaded it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MyST corpus enables ASR algorithms to improve by providing a large-scale, age-specific dataset for training and evaluation.
- Mechanism: The corpus provides approximately 400 hours of children's conversational speech across 10.5K sessions, allowing ASR models to learn patterns unique to children's speech characteristics, such as higher pitch, pronunciation variations, and conversational turn-taking.
- Core assumption: Children's speech patterns differ significantly from adults' and require dedicated training data for effective ASR.
- Evidence anchors:
  - [abstract]: "It is our hope that the corpus can be used to improve automatic speech recognition algorithms"
  - [section 4.4]: "Improvements in automatic transcription of children's speech–especially spontaneous conversations–can open doors to transformational applications in various areas"
- Break condition: If the ASR algorithms do not show significant improvement in WER on the MyST test set compared to models trained only on adult speech data.

### Mechanism 2
- Claim: The corpus structure with stratified partitions enables fair and replicable evaluation of ASR systems.
- Mechanism: By partitioning the data into training, development, and test sets that proportionately represent each science module and phase, the corpus allows for consistent benchmarking across different ASR architectures.
- Core assumption: Proper data partitioning is crucial for reliable model evaluation and comparison.
- Evidence anchors:
  - [section 4.1]: "These partitions were generated using stratified sampling strategy thus ensuring that they reasonably represent each of the science module in MyST"
  - [section 4.4]: "It is important that the ASR community report consistent and comparable WER on the MyST corpus to enable fair comparison across improved architectures"
- Break condition: If researchers fail to use the specified partitions or if the partitions are found to be unbalanced after extensive use.

### Mechanism 3
- Claim: The inclusion of untranscribed data enables semi-supervised learning approaches to further improve ASR performance.
- Mechanism: By including untranscribed sessions in all partitions, the corpus allows researchers to apply semi-supervised techniques, potentially leveraging pseudo-labels to augment the training data.
- Core assumption: Semi-supervised learning can effectively utilize untranscribed data to improve model performance.
- Evidence anchors:
  - [section 4.1]: "We also included untranscribed data in all partitions in order to be able to allow limited semi-supervised training data augmentation using the untranscribed portions of the data"
- Break condition: If semi-supervised approaches do not yield significant improvements in WER or if the untranscribed data is found to be too noisy for effective use.

## Foundational Learning

- Concept: Automatic Speech Recognition (ASR) fundamentals
  - Why needed here: Understanding ASR basics is crucial for effectively using and evaluating the MyST corpus for speech recognition research.
  - Quick check question: What are the key components of an ASR system, and how do they interact to transcribe speech to text?

- Concept: Children's speech characteristics
  - Why needed here: Recognizing the unique features of children's speech is essential for developing effective ASR models using the MyST corpus.
  - Quick check question: How does children's speech differ from adults' in terms of pitch, pronunciation, and language use?

- Concept: Data preprocessing and cleaning
  - Why needed here: Proper preprocessing of speech data is critical for building accurate ASR models, as demonstrated by the extensive cleanup procedures described in the MyST corpus documentation.
  - Quick check question: What are common audio quality issues in speech datasets, and how can they be addressed during preprocessing?

## Architecture Onboarding

- Component map:
  Data ingestion -> Audio preprocessing -> Feature extraction -> ASR model training -> Evaluation

- Critical path:
  1. Download and verify the corpus structure
  2. Preprocess audio files (quality checks, silence trimming)
  3. Extract features from audio data
  4. Train ASR model on the training set
  5. Fine-tune on the development set
  6. Evaluate on the test set

- Design tradeoffs:
  - Using transformer models vs. traditional HMM-DNN approaches
  - Balancing model size and computational requirements
  - Deciding between word-level and subword-level tokenization

- Failure signatures:
  - High WER on the test set despite low WER on the training set (overfitting)
  - Consistent errors on specific phonetic patterns (model bias)
  - Poor performance on certain science modules (data imbalance)

- First 3 experiments:
  1. Baseline: Train a simple ASR model on the MyST training set and evaluate on the test set
  2. Semi-supervised: Use untranscribed data to augment training and compare performance
  3. Domain adaptation: Fine-tune a pre-trained adult speech model on the MyST corpus and evaluate improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term impact of children's interactions with the MyST virtual tutor on their science learning outcomes?
- Basis in paper: [explicit] The paper mentions that surveys indicated over 70% of students were more excited about studying science after interacting with the tutor, but does not report on long-term learning outcomes.
- Why unresolved: The paper focuses on corpus development rather than longitudinal studies of student learning.
- What evidence would resolve it: Longitudinal studies tracking student science achievement and attitudes over multiple years following MyST interactions.

### Open Question 2
- Question: How does the performance of automatic speech recognition systems on the MyST corpus compare to other children's speech corpora?
- Basis in paper: [explicit] The paper mentions related work on children's speech corpora but does not provide direct comparisons of ASR performance.
- Why unresolved: The paper only reports WER on the MyST corpus itself, not comparative evaluations.
- What evidence would resolve it: Systematic comparison of ASR systems trained and evaluated on MyST versus other children's speech corpora.

### Open Question 3
- Question: What are the specific acoustic and linguistic features that make children's speech more challenging for ASR systems compared to adult speech?
- Basis in paper: [inferred] The paper focuses on children's speech but does not analyze specific challenging features.
- Why unresolved: The paper is primarily a corpus description rather than an analysis of speech characteristics.
- What evidence would resolve it: Detailed acoustic and linguistic analysis of the MyST corpus, identifying specific features that impact ASR performance.

## Limitations

- The exact transformer model architecture and hyperparameters used in experiments are not fully specified
- The process for identifying and removing "suspicious cases" from training data is unclear
- Only 100K out of 230K utterances are transcribed, limiting semi-supervised learning potential

## Confidence

**High Confidence Claims:**
- The MyST corpus exists and contains the specified amount of data (400 hours, 1.3K students, 230K utterances)
- The corpus has been licensed by multiple organizations and research groups
- The data collection methodology (8 science modules, 2 phases) is well-documented
- The stratified partitioning approach is technically sound

**Medium Confidence Claims:**
- The corpus will significantly improve ASR algorithms for children's speech
- The inclusion of untranscribed data will enable effective semi-supervised learning
- The specific WER improvements achievable with the corpus

**Low Confidence Claims:**
- Specific commercial applications enabled by the corpus (though theoretically plausible)
- Exact impact on educational outcomes (beyond the initial research project scope)

## Next Checks

1. **Partition Balance Verification**: Conduct an independent analysis of the train/dev/test partitions to verify that they maintain the claimed 10-10-80 split and that each partition represents all science modules proportionately. This should include statistical tests for representation across different student demographics.

2. **Baseline ASR Performance Benchmark**: Implement and evaluate a standard ASR model (e.g., SpeechBrain transformer) on the MyST test set to establish baseline WER. Compare results with other children's speech corpora to quantify the relative value of the MyST corpus for improving children's speech recognition.

3. **Semi-supervised Learning Validation**: Design and execute experiments using the untranscribed portions of the MyST corpus to test the claimed benefit of semi-supervised approaches. Compare WER improvements when using different proportions of untranscribed data and different pseudo-labeling strategies.