---
ver: rpa2
title: Deep Similarity Learning Loss Functions in Data Transformation for Class Imbalance
arxiv_id: '2312.10556'
source_url: https://arxiv.org/abs/2312.10556
tags:
- loss
- data
- examples
- class
- imbalanced
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces deep similarity learning using triplet loss
  functions to transform tabular multi-class imbalanced data into a more separable
  embedded representation without altering class sizes. It extends standard triplet
  loss by adding class imbalance weighting and local neighborhood-aware variants (safeness
  weights, cutoff, mean distances) to improve minority class separation.
---

# Deep Similarity Learning Loss Functions in Data Transformation for Class Imbalance

## Quick Facts
- arXiv ID: 2312.10556
- Source URL: https://arxiv.org/abs/2312.10556
- Reference count: 38
- Key outcome: Weighted triplet representation consistently outperforms popular resampling methods across multiple classifiers on 17 imbalanced datasets

## Executive Summary
This paper introduces deep similarity learning using triplet loss functions to transform multi-class imbalanced tabular data into more separable embedded representations without altering class sizes. The method extends standard triplet loss by adding class imbalance weighting and local neighborhood-aware variants to improve minority class separation. Experiments show consistent performance improvements over popular resampling methods across multiple classifiers and datasets.

## Method Summary
The method trains a feedforward neural network to learn an embedding space where similar examples (same class) are closer together and dissimilar examples (different classes) are farther apart, using modified triplet loss functions. The simplest variant weights the loss by inverse class cardinality to give higher importance to minority class examples. More advanced variants incorporate local neighborhood information, using k-nearest neighbors to create "safe" representations where minority examples are surrounded by same-class neighbors. The learned embedding is then used as input for standard classifiers like KNN, Decision Trees, or LDA.

## Key Results
- Weighted triplet representation consistently outperforms Global-CS, Static-SMOTE, and MDO resampling methods
- The Mean Dists variant achieved the best overall performance, particularly on harder datasets
- Statistically significant F1-score improvements were observed across multiple datasets and classifiers
- The approach avoids synthetic data generation while achieving superior performance

## Why This Works (Mechanism)

### Mechanism 1
Weighted triplet loss improves minority class separation by assigning higher loss weight to anchor examples from minority classes. The loss is multiplied by inverse class cardinality normalized to 1, so examples from minority classes contribute larger gradients during training, pulling them closer to positive examples and pushing them away from negatives.

### Mechanism 2
Local neighborhood-aware loss functions create "safe" representations by considering the distribution of same-class examples within k-nearest neighbors. Loss is minimized when distances to same-class neighbors are small and distances to different-class neighbors are large, with dynamic margin based on farthest same-class neighbor distance.

### Mechanism 3
Using mean distances instead of sums in the loss function provides more robust optimization by reducing sensitivity to outliers. This replaces sum of distances with mean distances, normalizing by the number of neighbors in each class, making the loss less sensitive to varying neighborhood sizes and outliers.

## Foundational Learning

- Concept: Triplet loss and contrastive learning
  - Why needed here: The paper builds on triplet loss to create separable embeddings for imbalanced data
  - Quick check question: What is the constraint enforced by triplet loss in terms of anchor, positive, and negative example distances?

- Concept: Class imbalance metrics (F1-score, G-mean)
  - Why needed here: These metrics are used to evaluate the proposed method against baselines
  - Quick check question: How do F1-score and G-mean differ from accuracy in evaluating imbalanced classification performance?

- Concept: Neural network training with custom loss functions
  - Why needed here: The paper proposes new loss functions that require custom training procedures
  - Quick check question: What modifications are needed to standard neural network training loops to implement the proposed weighted and neighborhood-aware triplet losses?

## Architecture Onboarding

- Component map: Input layer -> Hidden layers with PReLU activations -> Embedding layer (last layer) -> Loss computation (weighted triplet, basic triplet, safe neighborhood variants) -> Optimizer (Adam)
- Critical path: Data preprocessing -> Triplet selection strategy -> Embedding learning via custom loss -> Classifier training on embeddings
- Design tradeoffs: More complex loss functions may provide better representations but require more computation; simpler weighted triplet may be faster but less effective on difficult datasets
- Failure signatures: Poor separation in PCA visualizations, no improvement over baselines, unstable training with exploding gradients in neighborhood-based losses
- First 3 experiments:
  1. Implement and train with weighted triplet loss on a small imbalanced dataset, compare F1-score to baseline
  2. Implement basic safe neighborhood loss with k=20, compare to weighted triplet
  3. Implement mean distances variant and compare all three on a difficult multi-class imbalanced dataset

## Open Questions the Paper Calls Out
- How do the proposed triplet loss variants perform on binary imbalanced datasets compared to specialized binary methods?
- How sensitive are the triplet loss variants to the choice of k in k-nearest neighbors and what is the optimal k value across different dataset characteristics?
- What is the computational complexity of the proposed triplet loss variants compared to standard resampling methods and how does this scale with dataset size?

## Limitations
- The method requires careful hyperparameter tuning that may not generalize well across diverse datasets
- Performance gains are primarily demonstrated on medium-sized tabular datasets; scalability to high-dimensional or very large datasets remains untested
- The approach assumes sufficient minority class examples to form meaningful triplets, which may not hold for extremely imbalanced datasets

## Confidence
- High confidence in the core mechanism: Weighted triplet loss for class imbalance is well-grounded in contrastive learning theory
- Medium confidence in neighborhood-aware variants: Effectiveness depends heavily on k-NN search quality in embedding space
- Medium confidence in empirical results: Method shows consistent improvements but lacks some implementation details for reproducibility

## Next Checks
1. Reproduce with controlled experiments: Implement weighted triplet loss on a small imbalanced dataset with fixed random seeds and compare results against baseline values
2. Test neighborhood robustness: Systematically vary the k parameter in safe neighborhood variants and measure impact on minority class separation using t-SNE/PCA visualizations
3. Extreme imbalance stress test: Evaluate the method on datasets with higher imbalance ratios (>100:1) to identify practical limits of the approach