---
ver: rpa2
title: 'Node-aware Bi-smoothing: Certified Robustness against Graph Injection Attacks'
arxiv_id: '2312.03979'
source_url: https://arxiv.org/abs/2312.03979
tags:
- graph
- certified
- node
- robustness
- smoothing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing certified robustness
  for node classification tasks against graph injection attacks (GIAs), where malicious
  nodes are injected into the graph to degrade model performance. The authors propose
  a novel node-aware bi-smoothing framework that provides the first certifiably robust
  approach for general node classification tasks against GIAs.
---

# Node-aware Bi-smoothing: Certified Robustness against Graph Injection Attacks

## Quick Facts
- arXiv ID: 2312.03979
- Source URL: https://arxiv.org/abs/2312.03979
- Reference count: 40
- Key outcome: Proposes node-aware bi-smoothing for certified robustness against graph injection attacks, achieving 35-55% certified accuracy compared to 0% for baselines

## Executive Summary
This paper introduces the first certifiably robust approach for node classification tasks against graph injection attacks (GIAs). The authors propose node-aware bi-smoothing, a novel framework that combines edge-level and node-level smoothing to isolate injected nodes. The method is model-agnostic and applicable to both evasion and poisoning attacks. Through rigorous theoretical analysis, they establish certifiable conditions for their smoothing scheme, demonstrating significant improvements in certified accuracy compared to baseline methods across multiple datasets.

## Method Summary
The proposed node-aware bi-smoothing framework combines edge deletion (removing injected edges) and node deletion (removing entire injected nodes) with specified probabilities pe and pn. The method uses Monte Carlo sampling to generate randomized graphs, then applies statistical testing (Clopper-Pearson intervals, binomial tests) to estimate classification probabilities. For certification, it solves a linear program to find the worst-case classifier assignment that minimizes the classification margin. A node-aware-exclude variant improves poisoning attack certification by excluding isolated nodes from predictions.

## Key Results
- Node-aware bi-smoothing achieves 35% certified accuracy when injecting 10 nodes with up to 5 edges per node
- Node-aware-exclude variant achieves 55% certified accuracy under same conditions
- Both approaches significantly outperform sparsity-aware baseline (0% certified accuracy)
- Method applicable to both evasion and poisoning attacks with GCN and recommender models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Node-aware bi-smoothing increases the probability of isolating injected nodes compared to edge-deletion-only smoothing
- Mechanism: By combining edge deletion (removing injected edges) and node deletion (removing entire injected nodes with all their edges), the method creates two pathways to isolate injected nodes. Since attackers are constrained to connect each injected node to at most τ edges, both deletion strategies contribute to removing these edges
- Core assumption: Each injected node connects to at most τ edges, making node deletion particularly effective since it removes all edges incident to that node simultaneously
- Evidence anchors:
  - [abstract] "Our solution to this is a nontrivial generalization of the sparsity-aware certificate [12] to certify against node injection perturbation. Furthermore, to increase the sample overlap probability under GIA (limitation 2 ), our bi-smoothing scheme will randomly deletes nodes and edges simultaneously."
  - [section] "To increase the sample overlap probability under GIA (limitation 2 ), our bi-smoothing scheme will randomly deletes nodes and edges simultaneously."
- Break condition: If attackers can inject nodes with degrees exceeding τ, the effectiveness of node deletion smoothing diminishes significantly

### Mechanism 2
- Claim: The node-aware-exclude variant improves certified accuracy for poisoning attacks by excluding isolated nodes from voting
- Mechanism: During poisoning attacks, the model is trained on randomized graphs where isolated nodes don't affect other nodes' predictions. By excluding these isolated nodes from both training and testing phases, the model avoids making predictions on nodes it wasn't trained on, improving confidence in pA (probability of correct top prediction)
- Core assumption: Isolated nodes don't impact the classification results of other nodes, which holds for most graph models including message-passing GNNs and common recommendation models
- Evidence anchors:
  - [abstract] "Nevertheless, to enhance the certification performance specifically for poisoning attacks, we introduce a variant called node-aware-exclude. This variant excludes isolated nodes from the prediction process after randomization, thereby improving the overall prediction quality."
  - [section] "To avoid the impact of isolated nodes on the model parameter, we propose two different strategies termed node-aware-include and node-aware-exclude."
- Break condition: If the base model's predictions are affected by isolated nodes (contrary to the assumption), excluding them would reduce accuracy

### Mechanism 3
- Claim: The theoretical guarantee relies on the overlap probability being sufficiently high (specifically > 1/2) to maintain certification
- Mechanism: The certification condition requires that the probability of all injected edges being removed (either through edge deletion or node deletion) must be greater than 1/2. This ensures that under the worst-case classifier assignment, the classification margin remains positive
- Core assumption: The Neyman-Pearson lemma can be applied to determine the likelihood ratio regions for the randomized smoothing scheme
- Evidence anchors:
  - [abstract] "Through rigorous theoretical analysis, we establish the certifiable conditions of our smoothing scheme."
  - [section] "Based on this randomization, denoted as ϕ(·), it constructs a smoothed classifier... Leveraging the Neyman-Pearson lemma [33], these schemes offer verifiable classification margins when dealing with perturbed data."
- Break condition: If the overlap probability falls below 1/2, the certification fails regardless of the classifier's confidence

## Foundational Learning

- Concept: Randomized smoothing for adversarial robustness
  - Why needed here: The entire certification framework builds on randomized smoothing, extending it from graph modification attacks to graph injection attacks
  - Quick check question: What is the key difference between sparsity-aware smoothing and node-aware bi-smoothing in terms of their randomization strategy?

- Concept: Linear programming for worst-case analysis
  - Why needed here: The certification verification problem is formulated as a linear program to find the worst-case classifier that minimizes the classification margin
  - Quick check question: How does the solution to the LP problem determine which regions get assigned to the top and runner-up classes?

- Concept: Graph injection attack threat model
  - Why needed here: Understanding that attackers can inject nodes with features and edges, but with degree constraints (at most τ edges per node), is crucial for designing effective defenses
  - Quick check question: Why is graph injection considered more powerful than graph modification attacks in terms of attacker requirements?

## Architecture Onboarding

- Component map: Base classifier (GCN or recommender model) -> Smoothing distribution (edge deletion with probability pe, node deletion with probability pn) -> Monte Carlo sampling engine (generates N randomized graphs) -> Statistical testing module (Clopper-Pearson intervals, binomial tests) -> Certification verifier (solves LP problem, checks margin conditions)
- Critical path: Training → Randomization → Classification → Statistical aggregation → Certification verification
- Design tradeoffs:
  - Higher pe and pn increase certification but reduce clean accuracy
  - More Monte Carlo samples (N) improve confidence but increase computation time
  - Node-aware-exclude improves certification for poisoning but may reduce empirical robustness
- Failure signatures:
  - ABSTAIN output indicates insufficient confidence in predictions
  - Zero certified accuracy despite high clean accuracy suggests poor smoothing parameters
  - High abstain rate with large N indicates fundamental limitations of the approach
- First 3 experiments:
  1. Test certification on Cora-ML with ρ=5, τ=5 using pe=0.9, pn=0.9 and compare against sparsity-aware baseline
  2. Evaluate node-aware-exclude variant on Citeseer for poisoning attacks with ρ=10, τ=5
  3. Measure clean accuracy degradation as pe and pn increase from 0.0 to 0.9 on both datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the node-aware bi-smoothing approach perform against more sophisticated GIA attackers that can strategically select which nodes to inject and how to connect them?
- Basis in paper: [explicit] The paper mentions that their approach is tested against HAOGIA [25], a representative GIA attacker, but does not explore the effectiveness against more advanced attackers
- Why unresolved: The paper focuses on a specific GIA attacker (HAOGIA) and does not evaluate the robustness against a broader range of attack strategies
- What evidence would resolve it: Empirical results comparing the performance of node-aware bi-smoothing against various GIA attack strategies, including those that adaptively choose injection points and connections

### Open Question 2
- Question: What is the impact of the node-aware bi-smoothing approach on the computational efficiency of the model, especially when dealing with large graphs?
- Basis in paper: [inferred] The paper mentions the trade-off between prediction quality and certified accuracy, which implies computational considerations, but does not provide detailed analysis of computational efficiency
- Why unresolved: The paper focuses on the theoretical guarantees and effectiveness of the approach but does not address the practical computational implications
- What evidence would resolve it: Detailed analysis of the computational time and resources required for training and inference with the node-aware bi-smoothing approach on large-scale graphs

### Open Question 3
- Question: How does the node-aware bi-smoothing approach perform in scenarios where the injected nodes are not isolated but are connected to existing nodes in the graph?
- Basis in paper: [explicit] The paper assumes that injected nodes are isolated from existing nodes, but does not explore the case where injected nodes are connected to the graph
- Why unresolved: The assumption of isolated injected nodes simplifies the analysis but may not reflect real-world scenarios where attackers can strategically connect injected nodes to the graph
- What evidence would resolve it: Empirical results evaluating the performance of node-aware bi-smoothing against GIA attackers that can connect injected nodes to the existing graph

## Limitations
- Requires very high smoothing parameters (pe ≈ 0.9, pn ≈ 0.9) to achieve reasonable certified accuracy, significantly degrading clean accuracy
- Certification framework specifically designed for graph injection attacks with bounded node degrees (τ edges per node)
- Exclusion strategy for poisoning attacks assumes isolated nodes don't affect other nodes' predictions, which may not generalize to all base models

## Confidence
- **High confidence**: The theoretical foundation based on randomized smoothing and Neyman-Pearson lemma is well-established and correctly applied
- **Medium confidence**: Experimental results showing improvements over baselines are promising but rely on a single attack method (HAOGIA)
- **Medium confidence**: Claim that node-aware-exclude improves poisoning attack certification is supported by evidence but requires strong assumptions

## Next Checks
1. Replicate certified accuracy results on Cora-ML with ρ=10, τ=5 using sparsity-aware baseline and node-aware bi-smoothing variants to verify the claimed 35% vs 55% certified accuracy gap
2. Systematically vary pe and pn from 0.5 to 0.95 in 0.05 increments to quantify the trade-off between certified accuracy and clean accuracy degradation across all three datasets
3. Test the robustness of certified guarantees against adaptive attackers who can inject nodes with degrees exceeding τ or use alternative injection strategies not captured by the current threat model