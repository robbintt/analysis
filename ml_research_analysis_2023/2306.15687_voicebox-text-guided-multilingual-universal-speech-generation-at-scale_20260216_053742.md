---
ver: rpa2
title: 'Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale'
arxiv_id: '2306.15687'
source_url: https://arxiv.org/abs/2306.15687
tags:
- speech
- audio
- oicebox
- duration
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Voicebox is a text-guided generative model for speech that can
  perform many tasks through in-context learning, including text-to-speech synthesis,
  speech denoising, and diverse speech sampling. It is a non-autoregressive flow-matching
  model trained on a large scale multilingual dataset.
---

# Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale

## Quick Facts
- arXiv ID: 2306.15687
- Source URL: https://arxiv.org/abs/2306.15687
- Reference count: 34
- Voicebox outperforms VALL-E on intelligibility (5.9% vs 1.9% WER) and audio similarity (0.580 vs 0.681) while being up to 20x faster

## Executive Summary
Voicebox is a large-scale text-guided generative model for speech that can perform multiple tasks through in-context learning. It uses a non-autoregressive flow-matching approach trained on text-guided speech infilling, allowing it to generate speech from text, edit noisy speech, and sample diverse speech. The model achieves state-of-the-art performance on zero-shot text-to-speech synthesis while being significantly faster than previous approaches.

## Method Summary
Voicebox is a non-autoregressive continuous normalizing flow (CNF) model trained on text-guided speech infilling. It learns to generate masked speech segments given surrounding audio context and text transcript. The model uses a flow-matching training objective with optimal transport paths, enabling efficient training and parallel generation. It decouples duration and audio modeling, using separate components for phoneme duration prediction and audio generation. The architecture consists of two transformers (duration and audio models) trained on 60K hours of English and 50K hours of multilingual speech data.

## Key Results
- Outperforms VALL-E on zero-shot TTS: 5.9% vs 1.9% WER and 0.580 vs 0.681 similarity score
- Generates speech up to 20x faster than VALL-E through non-autoregressive parallel generation
- Achieves 50% relative WER improvement when used to generate synthetic training data for speech recognition systems
- Demonstrates strong performance on speech denoising and editing tasks through in-context learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Voicebox generalizes to new tasks through text-guided speech infilling rather than task-specific conditioning.
- **Mechanism:** The model learns p(x_mis | y, x_ctx) where x_mis is masked speech, y is transcript, and x_ctx is context. This formulation allows the model to fill missing segments in any context (past/future) given text, effectively enabling many tasks via in-context learning.
- **Core assumption:** A deterministic mapping from text and context to target speech is unrealistic for long segments; a probabilistic model is needed.
- **Evidence anchors:**
  - [abstract]: "Voicebox is trained on a text-guided speech infilling task, where the goal is to generate masked speech given its surrounding audio and text transcript."
  - [section 2]: "Voicebox is a text-guided infilling model, but it leverages the CNF model that can parameterize any distribution."
- **Break condition:** If the infilling task cannot capture long-range dependencies or if context is insufficient for the model to infer style.

### Mechanism 2
- **Claim:** Non-autoregressive flow-matching with optimal transport path enables faster and higher quality speech generation than autoregressive models.
- **Mechanism:** The model uses a conditional vector field vt(w, x_ctx, z; θ) trained via flow matching. It can generate speech in parallel (NAR) and achieve high quality with few ODE steps (NFE < 10), trading off speed and quality.
- **Core assumption:** Flow matching with OT path converges faster and generates higher quality samples than diffusion or AR models.
- **Evidence anchors:**
  - [abstract]: "Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster."
  - [section 3.1]: "the OT path leads to faster training, faster generation, and better performance compared to diffusion paths."
- **Break condition:** If the ODE solver fails to converge or if the flow matching objective does not capture the data distribution well.

### Mechanism 3
- **Claim:** Decoupling duration and audio modeling allows finer alignment control and improves performance.
- **Mechanism:** The model uses two components: an audio model q(x_mis | z, x_ctx) and a duration model q(l_mis | y, l_ctx). The duration model predicts phoneme durations given context, which are then used to construct frame-level phone transcripts for the audio model.
- **Core assumption:** Aligning text to speech at the phoneme level with explicit duration modeling improves intelligibility and similarity compared to implicit alignment.
- **Evidence anchors:**
  - [section 3.3]: "We decouple Voicebox into two components: an audio model and a duration model."
  - [section 5.2]: "VALL-E requires one AR and seven NAR steps. Voicebox decouples duration and audio modeling, enabling finer grained alignment control."
- **Break condition:** If the duration model predictions are inaccurate, leading to misalignment and poor intelligibility.

## Foundational Learning

- **Concept: Flow matching and continuous normalizing flows**
  - **Why needed here:** Enables efficient training of generative models that can sample from complex distributions (speech) via a simple prior, crucial for high-quality speech generation.
  - **Quick check question:** What is the key difference between flow matching and diffusion models in terms of the training objective?

- **Concept: Masked language modeling / infilling**
  - **Why needed here:** The training task of predicting masked speech segments given context and transcript is analogous to masked language modeling, allowing the model to learn rich representations.
  - **Quick check question:** How does the infilling task enable Voicebox to perform tasks it wasn't explicitly trained on?

- **Concept: Classifier-free guidance**
  - **Why needed here:** Allows trading off diversity and fidelity post-training by interpolating between conditional and unconditional generations, crucial for controlling speech style and quality.
  - **Quick check question:** What is the effect of increasing the classifier-free guidance strength α during inference?

## Architecture Onboarding

- **Component map:** Text → Phonemizer → Forced aligner → Duration model → Audio model → ODE solver → Vocoder → Audio
- **Critical path:** Text → Phonemizer → Forced aligner → Duration model → Audio model → ODE solver → Vocoder → Audio
- **Design tradeoffs:**
  - Narrower vs wider audio model (24 vs 12 layers): Wider model has more parameters (330M) but may overfit; narrower is faster to train.
  - Flow matching vs regression for duration: Flow matching captures duration distribution better but regression is simpler.
  - Masked vs all-frame loss: Masked loss focuses on generating the missing segment but may miss global coherence.
- **Failure signatures:**
  - High WER but low SIM: Audio style is transferred but content is incorrect (misalignment or poor text conditioning).
  - Low WER but low SIM: Content is correct but audio style is not transferred (poor context conditioning).
  - High FSD: Generated speech distribution differs from training data (poor diversity or quality).
- **First 3 experiments:**
  1. Train audio model with regression loss (replace flow matching) and compare WER/SIM on zero-shot TTS.
  2. Remove duration conditioning (unconditional duration model) and evaluate impact on intelligibility and similarity.
  3. Vary classifier-free guidance strength α during inference and measure tradeoff between WER and SIM.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does Voicebox perform on conversational speech or speech with non-verbal sounds like laughter and back-channeling?
  - Basis in paper: [inferred] The paper mentions that Voicebox models are trained on read speech from audiobooks and may not transfer well to conversational speech. It also states that they plan to scale the training data to incorporate more diverse speech, including non-verbal sounds.
  - Why unresolved: The paper does not provide any experimental results or analysis on Voicebox's performance on conversational speech or speech with non-verbal sounds.
  - What evidence would resolve it: Conducting experiments to evaluate Voicebox's performance on conversational speech datasets and speech with non-verbal sounds, comparing it to baselines and reporting metrics like WER, FSD, and subjective quality scores.

- **Open Question 2**
  - Question: Can Voicebox achieve disentangled control of audio attributes like voice, emotion, and speaking style through prompting or text description?
  - Basis in paper: [explicit] The paper states that while Voicebox yields impressive results on transferring audio style, it does not allow independent control of each attribute. It mentions that one cannot ask the model to generate speech that resembles the voice of one sample while resembling the emotion of another sample.
  - Why unresolved: The paper does not explore or provide any methods for achieving disentangled control of audio attributes in Voicebox.
  - What evidence would resolve it: Developing and evaluating methods for achieving disentangled control of audio attributes in Voicebox, such as conditioning on separate prompts or text descriptions for each attribute, and assessing the quality and controllability of the generated speech.

- **Open Question 3**
  - Question: How does Voicebox's performance compare to other large-scale generative models like GPT and DALL-E in terms of task generalization and zero-shot capabilities?
  - Basis in paper: [explicit] The paper mentions that Voicebox is a large-scale generative model that can perform many tasks through in-context learning, similar to GPT and DALL-E. It also states that Voicebox achieves state-of-the-art performance on various speech generation tasks.
  - Why unresolved: The paper does not directly compare Voicebox's performance to other large-scale generative models like GPT and DALL-E in terms of task generalization and zero-shot capabilities.
  - What evidence would resolve it: Conducting a comprehensive comparison of Voicebox's task generalization and zero-shot capabilities with other large-scale generative models like GPT and DALL-E, evaluating their performance on a wide range of tasks and assessing their ability to solve tasks not explicitly trained on.

## Limitations

- Dataset composition and quality control are not fully specified, which affects claims about noisy speech editing improvements
- Architecture details are sparse, making exact replication challenging
- Claims about task generalization are primarily demonstrated on TTS, denoising, and speech editing, with uncertain scope for truly novel tasks

## Confidence

- **High Confidence:** The core mechanism of text-guided speech infilling and the superiority of flow-matching over autoregressive approaches for speech generation
- **Medium Confidence:** The multilingual capabilities and zero-shot performance, which depend heavily on dataset quality and diversity
- **Low Confidence:** Claims about the model's ability to generate speech for training ASR systems based on limited evaluation

## Next Checks

1. Implement a minimal version of Voicebox with specified layers and dimensions, then verify the flow-matching training procedure converges and generates coherent speech samples.

2. Test the model's performance across the six claimed languages using standardized benchmarks to verify multilingual claims beyond reported WER improvements.

3. Systematically evaluate the model on tasks not mentioned in the paper (e.g., speech-to-speech translation, emotion transfer) to assess the true extent of its in-context learning capabilities.