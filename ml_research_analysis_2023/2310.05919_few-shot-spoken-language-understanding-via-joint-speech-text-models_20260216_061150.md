---
ver: rpa2
title: Few-Shot Spoken Language Understanding via Joint Speech-Text Models
arxiv_id: '2310.05919'
source_url: https://arxiv.org/abs/2310.05919
tags:
- speech
- text
- data
- speech-text
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of few-shot spoken language
  understanding, where limited labeled speech data is available. The core method idea
  is to leverage speech-text joint pre-trained models, which encode speech and text
  into a shared representation space.
---

# Few-Shot Spoken Language Understanding via Joint Speech-Text Models

## Quick Facts
- arXiv ID: 2310.05919
- Source URL: https://arxiv.org/abs/2310.05919
- Authors: 
- Reference count: 0
- With as little as 1 hour of labeled speech data, achieves close performance to previous work fine-tuned on full speech data

## Executive Summary
This paper addresses the challenge of few-shot spoken language understanding by leveraging speech-text joint pre-trained models that encode both modalities into a shared representation space. The authors fine-tune these models using limited labeled speech data combined with labeled text data, demonstrating that with only 1-3 hours of speech data, they achieve performance comparable to previous methods using 10x more speech data. The key insight is that the bottom layers of speech-text models are largely task-agnostic and align speech and text representations, while the top layers are more task-specific, enabling effective cross-modal transfer.

## Method Summary
The approach uses speech-text joint pre-trained models (SpeechLM-P, SpeechLM-H, and SpeechUT) that were pre-trained on LibriSpeech (960 hrs) and LibriSpeech LM corpus (40M text sentences). The method involves fine-tuning these models on the SLUE benchmark using a combination of labeled text data and limited labeled speech data (1-3 hours). The authors employ a specific fine-tuning strategy where bottom layers are frozen to preserve cross-modal alignment while top layers are updated for task-specific learning. Performance is evaluated on sentiment analysis and named entity recognition tasks using F1 scores.

## Key Results
- With 1 hour of labeled speech data, the approach achieves performance within 2 F1 points of previous methods using full speech data
- Freezing bottom layers during fine-tuning improves zero-shot performance while maintaining similar full-data performance
- The bottom layers of speech-text models successfully align speech and text representations before task-specific prediction occurs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speech-text models achieve cross-modal transferability from text to speech due to aligned representations in the bottom layers.
- Mechanism: The models encode speech and text into a shared representation space in the first few layers, allowing task-specific knowledge learned from text to be directly applicable to speech inputs.
- Core assumption: The bottom layers of the model successfully align speech and text representations before task-specific prediction occurs.
- Evidence anchors:
  - [abstract] "We find that the bottom layers of speech-text models are largely task-agnostic and align speech and text representations into a shared space"
  - [section] "they encode speech and text in a shared representation space in the first few layers, before making predictions in the remaining layers"

### Mechanism 2
- Claim: Freezing bottom layers during fine-tuning preserves the cross-modal alignment while allowing task-specific adaptation in top layers.
- Mechanism: By freezing the bottom layers responsible for alignment and only updating the top layers for task-specific learning, the model maintains its ability to transfer knowledge across modalities.
- Core assumption: The bottom layers contain primarily alignment information that shouldn't be modified during task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "we design a fine-tuning strategy by freezing the bottom layers and only updating the top layers, which improves the zero-shot performance"
  - [section] "freezing the bottom layers (i.e., the multilingual encoder) during fine-tuning in general only leads to a slight drop in same-language performance but potentially improves the cross-lingual ability"

### Mechanism 3
- Claim: Speech-text models follow a "first-align-then-predict" pattern similar to multilingual text models, enabling effective zero-shot transfer.
- Mechanism: The model architecture separates representation alignment (bottom layers) from task prediction (top layers), allowing the aligned representations to serve as a foundation for various downstream tasks.
- Core assumption: The model architecture inherently supports this separation of concerns between alignment and prediction.
- Evidence anchors:
  - [abstract] "speech-text models follow the 'first-align-then-predict' pattern observed in multi-lingual text models"
  - [section] "the bottom layers of speech-text models are largely task-agnostic and align speech and text representations into a shared space, while the top layers are more task-specific"

## Foundational Learning

- Concept: Self-supervised speech representation learning
  - Why needed here: Understanding how speech models learn from unlabeled data provides context for why joint speech-text models can leverage both modalities
  - Quick check question: What are the main pretext tasks used in self-supervised speech models like HuBERT and Wav2Vec 2.0?

- Concept: Cross-modal representation alignment
  - Why needed here: The core mechanism relies on creating a shared space where speech and text representations can be meaningfully compared
  - Quick check question: How does the Average Neuron-Wise Correlation (ANC) measure alignment between speech and text representations?

- Concept: Few-shot learning principles
  - Why needed here: The paper's main contribution is achieving comparable performance with minimal labeled speech data by leveraging text data
  - Quick check question: What is the key difference between zero-shot, few-shot, and full-data training settings in this context?

## Architecture Onboarding

- Component map: Speech input → Speech tokenizer → Bottom layers (6-layer transformer) → Shared layers (6-layer transformer) → Top layers → Task-specific head → Output
- Critical path: Input → Tokenization → Bottom layers (alignment) → Top layers (task-specific) → Prediction
- Design tradeoffs:
  - Discrete vs continuous representations: Discrete units provide better alignment but require additional tokenization models
  - Layer freezing: Preserves alignment but may limit task-specific adaptation
  - Text vs speech data ratio: More text data helps in low-resource settings but may cause interference
- Failure signatures:
  - Poor cross-modal performance: Indicates alignment in bottom layers is insufficient
  - Catastrophic forgetting: Suggests top layers are overwriting useful alignment information
  - Mode collapse: Implies the model is ignoring one modality during training
- First 3 experiments:
  1. Verify ANC scores between speech and text representations in pre-trained model to confirm alignment exists
  2. Test zero-shot performance on sentiment analysis to validate cross-modal transfer
  3. Experiment with different numbers of frozen layers to find optimal fine-tuning configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of speech-text models scale when using more labeled speech data in combination with text data?
- Basis in paper: [explicit] The paper mentions that the approach can be directly scaled up when more labeled speech/text data is available, but questions whether the model continues to benefit from text supervision when more speech data is available.
- Why unresolved: The experiments only explore limited amounts of speech data (up to 3 hours) and do not investigate the impact of increasing speech data on the effectiveness of text supervision.
- What evidence would resolve it: Additional experiments with larger amounts of labeled speech data, comparing performance with and without text data, would clarify whether text supervision continues to be beneficial as more speech data becomes available.

### Open Question 2
- Question: How do multilingual speech-text models perform compared to monolingual models in zero-shot and few-shot settings across multiple languages?
- Basis in paper: [explicit] The paper suggests exploring multilingual speech-text models to study how spoken and written forms of different languages can be integrated, given the similarity observed between speech-text models and multi-lingual text models.
- Why unresolved: The paper only explores monolingual English models and does not investigate the performance of multilingual models in cross-lingual settings.
- What evidence would resolve it: Experiments with multilingual speech-text models on tasks involving multiple languages would demonstrate their ability to transfer knowledge across languages in zero-shot and few-shot settings.

### Open Question 3
- Question: What is the impact of different fine-tuning strategies (e.g., freezing different layers) on the performance of speech-text models in zero-shot and few-shot settings?
- Basis in paper: [explicit] The paper explores freezing bottom layers during fine-tuning and finds that it improves zero-shot performance, but does not systematically investigate the impact of different fine-tuning strategies on performance.
- Why unresolved: The paper only explores one fine-tuning strategy (freezing bottom layers) and does not compare it to other possible strategies, such as freezing different combinations of layers or using different learning rates.
- What evidence would resolve it: A comprehensive comparison of different fine-tuning strategies, including freezing different layers and using different learning rates, would reveal the optimal approach for maximizing performance in zero-shot and few-shot settings.

## Limitations

- The approach requires substantial pre-training on large-scale speech and text corpora, which may limit accessibility
- Results are validated only on the SLUE benchmark and English language, limiting generalizability
- The optimal fine-tuning configuration (number of frozen layers, learning rates) is not fully explored

## Confidence

**High Confidence**:
- Speech-text joint models can leverage text data to improve few-shot spoken language understanding performance
- The bottom layers of speech-text models do show cross-modal alignment capabilities
- Freezing bottom layers during fine-tuning provides a reasonable trade-off between task performance and cross-modal transfer

**Medium Confidence**:
- The "first-align-then-predict" pattern is a generalizable property of speech-text joint models
- The specific performance improvements (F1 scores within 2 points of full-data baselines) will hold across different tasks and datasets
- The approach will scale effectively to languages beyond English

**Low Confidence**:
- The approach will work equally well for all types of spoken language understanding tasks
- The computational efficiency gains will offset the pre-training costs in practical applications
- The specific architectural details are optimal

## Next Checks

1. **Cross-Architecture Validation**: Test the proposed approach with alternative speech-text joint models beyond SpeechLM and SpeechUT to verify whether the alignment and fine-tuning patterns are architecture-independent.

2. **Cross-Lingual Transfer Evaluation**: Conduct systematic experiments on low-resource languages beyond English to quantify the performance gaps and identify whether specific language families affect cross-modal transfer effectiveness.

3. **Fine-tuning Configuration Sweep**: Perform a comprehensive ablation study varying the number of frozen layers, the ratio of speech to text data during fine-tuning, and the learning rates for different model components to identify the optimal configuration across multiple tasks and data regimes.