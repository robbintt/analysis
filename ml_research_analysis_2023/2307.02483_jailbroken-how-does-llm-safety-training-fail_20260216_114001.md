---
ver: rpa2
title: 'Jailbroken: How Does LLM Safety Training Fail?'
arxiv_id: '2307.02483'
source_url: https://arxiv.org/abs/2307.02483
tags:
- prompt
- attack
- attacks
- prompts
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Jailbreak attacks succeed on state-of-the-art language models
  by exploiting two failure modes: competing objectives between safety and capability
  training, and mismatched generalization where safety training does not extend to
  all model capabilities. The paper designs new jailbreak attacks based on these principles
  and evaluates them against GPT-4 and Claude v1.3.'
---

# Jailbroken: How Does LLM Safety Training Fail?

## Quick Facts
- arXiv ID: 2307.02483
- Source URL: https://arxiv.org/abs/2307.02483
- Reference count: 40
- Primary result: New jailbreak attacks succeed on over 96% of harmful prompts against state-of-the-art safety-trained LLMs

## Executive Summary
This paper systematically evaluates jailbreak attacks against state-of-the-art language models like GPT-4 and Claude v1.3, identifying fundamental failure modes in safety training. The authors demonstrate that despite extensive safety training, these models remain vulnerable to simple attacks, with the strongest combinations succeeding on 100% of curated red-teaming prompts. The research reveals that scaling alone cannot resolve these issues, arguing instead for safety mechanisms that match the sophistication of the underlying model capabilities.

## Method Summary
The authors evaluate 30 jailbreak methods on safety-trained language models using two datasets: 32 curated harmful prompts from red-teaming evaluations and 317 synthetic prompts generated via GPT-4 few-shot prompting. They test individual simple attacks (prefix injection, refusal suppression, Base64 encoding) and combinations thereof on GPT-4, Claude v1.3, and GPT-3.5 Turbo with temperature 0 sampling, measuring success rates based on whether models produce harmful responses.

## Key Results
- Combined jailbreak attacks succeed on over 96% of harmful prompts against GPT-4 and Claude v1.3
- The strongest attack combinations achieve 100% success on curated red-teaming prompts
- Competing objectives and mismatched generalization represent fundamental vulnerabilities that scaling alone cannot resolve

## Why This Works (Mechanism)

### Mechanism 1: Competing Objectives
- **Claim**: Competing objectives between safety and capability training create vulnerabilities when the model must choose between refusing harmful content or following other training objectives.
- **Mechanism**: The model is trained on multiple objectives (language modeling, instruction following, and safety) that can conflict. When a prompt is crafted to make refusal unlikely in the pretraining distribution while following instruction-following training, the model may choose to continue rather than refuse.
- **Core assumption**: Safety training does not fully override the model's other training objectives, particularly when refusal would be unusual in the pretraining distribution.
- **Evidence anchors**: [abstract]: "Competing objectives arise when a model's capabilities and safety goals conflict"; [section 3.1]: "This training can be exploited by crafting prompts that force a choice between either a restricted behavior or a response that is heavily penalized by the pretraining and instruction following objectives"
- **Break condition**: If safety training can effectively override all other training objectives, or if the model is trained to always prioritize safety over other objectives.

### Mechanism 2: Mismatched Generalization
- **Claim**: Mismatched generalization occurs when safety training fails to generalize to domains where the model has capabilities from broader pretraining.
- **Mechanism**: Pretraining uses a larger and more diverse dataset than safety training, so the model has capabilities not covered by safety training. Prompts can be constructed where pretraining and instruction following generalize, but safety training does not, leading to unsafe responses.
- **Core assumption**: Safety training does not cover all domains where the model has capabilities from pretraining, creating blind spots.
- **Evidence anchors**: [abstract]: "mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist"; [section 3.2]: "this mismatch can be exploited for jailbreaks by constructing prompts on which pretraining and instruction following generalize, but the model's safety training does not"
- **Break condition**: If safety training can be extended to cover all domains where the model has capabilities, or if the model's capabilities are constrained to match the scope of safety training.

### Mechanism 3: Combined Attack Synergies
- **Claim**: The combination of multiple simple attacks is more effective than individual attacks due to the model's inability to defend against all failure modes simultaneously.
- **Mechanism**: Simple attacks exploit individual failure modes, but combining them creates synergistic effects that are harder to defend against. The model may successfully defend against individual attacks but fail when multiple mechanisms are combined.
- **Core assumption**: The model's safety mechanisms are not sophisticated enough to detect and defend against combinations of failure modes.
- **Evidence anchors**: [abstract]: "Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests"; [section 4.1]: "We also test combinations of these basic attack techniques: combination_1 composes prefix injection, refusal suppression, and the Base64 attack, combination_2 adds style injection, and combination_3 adds generating website content and formatting constraints"
- **Break condition**: If safety mechanisms can be made sophisticated enough to detect and defend against combinations of simple attacks, or if the model can be trained to be robust to multiple failure modes simultaneously.

## Foundational Learning

- **Concept**: Language model training objectives and their interactions
  - Why needed here: Understanding how competing objectives create vulnerabilities requires knowledge of how LLMs are trained on multiple objectives and how these objectives can conflict.
  - Quick check question: Can you explain how a model trained for both instruction following and safety might behave when these objectives conflict?

- **Concept**: Generalization in machine learning
  - Why needed here: The mismatched generalization mechanism relies on understanding how models generalize from training data to new inputs, and how this generalization can be uneven across different capabilities.
  - Quick check question: Why might a model that generalizes well to Base64-encoded inputs fail to generalize safety training to the same inputs?

- **Concept**: Adversarial attack surface analysis
  - Why needed here: Understanding the attack surface helps identify where vulnerabilities exist and how different failure modes can be exploited, which is crucial for both attack and defense.
  - Quick check question: How does the concept of "attack surface" relate to the effectiveness of combined jailbreak attacks?

## Architecture Onboarding

- **Component map**: Pretrained language model -> Safety training (RLHF) -> Red teaming -> Inference-time filtering -> Model response
- **Critical path**: Attack → Model inference → Safety mechanism → Response. The vulnerabilities occur when safety mechanisms fail to intercept harmful responses in the inference stage.
- **Design tradeoffs**: More extensive safety training vs. maintaining model capabilities, simplicity of safety mechanisms vs. their effectiveness, and the cost of red teaming vs. coverage of potential vulnerabilities.
- **Failure signatures**: High BAD BOT rates for specific attack types, success of combined attacks over individual ones, and vulnerability to prompts that are out-of-distribution for safety training.
- **First 3 experiments**:
  1. Test individual simple attacks (prefix injection, refusal suppression, Base64) to understand basic failure modes.
  2. Combine two simple attacks to test for synergistic effects.
  3. Test attacks on prompts from different domains (PII, hallucination) to assess generalization of vulnerabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can scaling alone resolve the competing objectives failure mode in safety-trained LLMs?
- **Basis in paper**: Explicit. The paper states "Scaling up will not resolve competing objectives, as the issue lies with the optimization objective."
- **Why unresolved**: The paper argues against scaling as a solution but does not empirically test whether larger models exhibit reduced vulnerability to competing objectives.
- **What evidence would resolve it**: Comparative experiments on models of increasing scale (e.g., GPT-3.5, GPT-4, GPT-5) showing whether vulnerability to competing objectives decreases with scale.

### Open Question 2
- **Question**: Does mismatched generalization worsen with scale if safety training is not extended to broader domains?
- **Basis in paper**: Inferred. The paper suggests "scaling may lead to a combinatorially growing attack surface of capabilities to defend" but does not test this empirically.
- **Why unresolved**: The paper hypothesizes that scale without expanded safety training could increase mismatched generalization, but does not provide experimental evidence.
- **What evidence would resolve it**: Experiments comparing safety training coverage vs. model capability growth across model sizes, measuring mismatched generalization rates.

### Open Question 3
- **Question**: Is "safety-capability parity" necessary for robust defense against adversarial use?
- **Basis in paper**: Explicit. The paper argues "safety mechanisms should be as sophisticated as the underlying model" but does not test this hypothesis.
- **Why unresolved**: The paper proposes safety-capability parity as a defense principle but does not experimentally validate whether equally sophisticated safety mechanisms are required.
- **What evidence would resolve it**: Comparative experiments testing defense effectiveness of safety mechanisms with varying sophistication levels against models of corresponding capability.

## Limitations

- The evaluation relies on a relatively small curated dataset of 32 red-teaming prompts and 317 synthetic prompts, which may not represent the full space of harmful queries.
- Results may not generalize to other safety-trained models or different safety training approaches beyond GPT-4 and Claude v1.3.
- Several combination attacks lack precise specification, making exact reproduction difficult.

## Confidence

- **High confidence**: The identification of competing objectives as a fundamental tension in safety training is well-supported by the observed success rates across multiple attack types.
- **Medium confidence**: The mismatched generalization mechanism is theoretically sound and supported by empirical results, but lacks quantitative analysis of pretraining vs. safety training dataset distributions.
- **Low confidence**: The claim that scaling alone cannot resolve these issues is stated but not empirically tested.

## Next Checks

1. Test the same jailbreak attacks on a systematically expanded dataset of harmful prompts covering diverse domains to verify whether the 96% success rate holds across broader prompt distributions.

2. Evaluate the identified failure modes on a broader range of models including open-source safety-trained models and smaller models to determine whether these are fundamental architectural vulnerabilities.

3. Conduct quantitative analysis comparing the distribution of pretraining data versus safety training data across different domains to directly measure the extent of mismatched generalization.