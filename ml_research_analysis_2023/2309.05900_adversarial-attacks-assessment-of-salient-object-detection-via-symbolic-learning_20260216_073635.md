---
ver: rpa2
title: Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning
arxiv_id: '2309.05900'
source_url: https://arxiv.org/abs/2309.05900
tags:
- images
- networks
- database
- attacks
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the vulnerability of deep learning models
  to adversarial attacks in salient object detection (SOD). The authors propose using
  brain programming (BP), a symbolic learning approach, as a more robust alternative
  to deep learning for SOD tasks.
---

# Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning

## Quick Facts
- arXiv ID: 2309.05900
- Source URL: https://arxiv.org/abs/2309.05900
- Reference count: 34
- Key outcome: Brain Programming (BP), a symbolic learning approach, demonstrates superior robustness to adversarial attacks in salient object detection compared to five deep learning models, achieving 50.22% Fβ score on DUTS dataset versus 82.43% for best deep learning model (BASNet)

## Executive Summary
This study evaluates the robustness of deep learning models versus symbolic learning approaches for salient object detection (SOD) under adversarial attacks. The authors introduce Brain Programming (BP), a symbolic learning paradigm based on evolutionary computation, and compare it against five deep learning models using standard datasets and various attack methods. The results demonstrate that BP maintains high performance and robustness against all tested attacks, while deep learning models suffer significant performance losses. Notably, BP achieves consistent results across clean and adversarial images, suggesting symbolic learning may offer a more reliable foundation for computer vision systems in adversarial environments.

## Method Summary
The study compares Brain Programming (BP), a symbolic learning approach based on evolutionary computation, against five deep learning models (BASNet, PICANet, EDN, LeNo, DHSNet) for salient object detection. BP evolves mathematical and computational functions optimized for the SOD task, while deep learning models rely on gradient-based optimization. The evaluation uses five datasets (FT, ImgSal, PASCAL-S, DUTS, SNPL) with ground truth annotations, testing both clean and adversarially perturbed images. Adversarial attacks include FGSM with various epsilon values, adversarial patches, multipixel attacks, and noise perturbations (Gaussian, Salt & Pepper, Speckle). Performance is measured using the Fβ metric (β=0.3).

## Key Results
- BP achieved an average Fβ score of 50.22% on the DUTS dataset compared to 82.43% for the best deep learning model (BASNet)
- BP maintained high performance and robustness against all tested adversarial attacks, while deep learning models showed significant performance degradation
- On the real-world SNPL dataset, BP demonstrated superior robustness compared to deep learning models, particularly important for bird conservation applications
- The study proves that symbolic learning robustness is crucial for designing reliable visual attention systems that can withstand intense perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic learning approaches like Brain Programming (BP) are inherently more robust to adversarial perturbations than deep learning models because they do not rely on gradient-based optimization that can be exploited by adversarial examples.
- Mechanism: BP uses an evolutionary computation paradigm loosely based on the visual cortex, specifically the dorsal stream, to create computational models of SOD. It evolves a set of mathematical and computational functions optimized for the task, rather than learning fixed weight parameters.
- Core assumption: The symbolic nature of BP, which involves discovering critical parts of the algorithm through artificial evolution, inherently provides resistance to adversarial perturbations that target the gradient-based learning of neural networks.
- Evidence anchors: [abstract] "We compare our methodology with five different deep learning approaches, proving that they do not match the symbolic paradigm regarding robustness." [section] "The BP methodology was defined to detect conspicuous objects by involving a process similar to that of the artificial visual cortex in humans. After the experiments, BP proved highly robust, supporting even the highest-disturbance images."
- Break condition: If the evolved symbolic functions in BP can be reverse-engineered to identify vulnerabilities that can be exploited by adversarial perturbations, then its robustness advantage could be compromised.

### Mechanism 2
- Claim: The evolutionary optimization process in BP leads to solutions that are more generalizable and less sensitive to small input perturbations compared to the highly tuned parameters of deep neural networks.
- Mechanism: BP evolves individuals consisting of templates describing the visual attention model. This process explores a broader solution space and finds robust features that are less likely to be affected by small input changes.
- Core assumption: The evolutionary search in BP explores a diverse set of solutions that are inherently more robust to perturbations, while deep learning models converge to highly specific solutions that are vulnerable to adversarial attacks.
- Evidence anchors: [abstract] "Brain programming is a kind of symbolic learning in the vein of good old-fashioned artificial intelligence. This work provides evidence that symbolic learning robustness is crucial in designing reliable visual attention systems since it can withstand even the most intense perturbations." [section] "In the case of BP, we have f(I, F, T, a), such that P* ∈ S: Z(P*, G) ≥ Z (P, G) which maximizes the overlap between P and G from the solution space S."
- Break condition: If the evolutionary process in BP converges to solutions that are highly specific to the training data, then it could become vulnerable to adversarial perturbations similar to deep learning models.

### Mechanism 3
- Claim: The symbolic representation of features in BP makes it less susceptible to gradient-based adversarial attacks that exploit the linearity of neural networks.
- Mechanism: BP uses a set of predefined functions and terminals from the feature extraction process, which are combined symbolically to create the visual attention model. This avoids the linear behavior of neural networks that can be exploited by adversarial examples.
- Core assumption: The symbolic combination of features in BP does not exhibit the same linear behavior as neural networks, making it resistant to gradient-based adversarial attacks.
- Evidence anchors: [abstract] "Adversarial examples are maliciously-constructed inputs that fool machine learning models by usually degrading the output performance of image classification." [section] "AEs find input Iρ in the subspace I', such that Iρ ∈ I' and f(I) ≠ f(Iρ). Nevertheless we denote robustness in terms of function continuity."
- Break condition: If adversarial attacks can be designed to exploit the specific symbolic representations used in BP, then its resistance to gradient-based attacks could be compromised.

## Foundational Learning

- Concept: Adversarial attacks and their impact on deep learning models
  - Why needed here: Understanding how adversarial attacks work is crucial to appreciate the significance of BP's robustness and the limitations of deep learning models.
  - Quick check question: What are the two main types of adversarial attacks, and how do they differ in terms of the attacker's knowledge of the model?

- Concept: Evolutionary computation and symbolic learning
  - Why needed here: BP is based on evolutionary computation and symbolic learning paradigms, which are key to understanding its robustness and how it differs from deep learning approaches.
  - Quick check question: How does the evolutionary optimization process in BP differ from the gradient-based optimization used in deep learning models?

- Concept: Salient object detection (SOD) and its applications
  - Why needed here: SOD is the specific task being addressed in the paper, and understanding its importance and challenges is crucial to appreciate the significance of BP's robustness in real-world scenarios.
  - Quick check question: What are some real-world applications of SOD, and why is robustness against adversarial attacks particularly important in these applications?

## Architecture Onboarding

- Component map: Input Images -> Feature Extraction -> Evolutionary Optimization -> Saliency Map Generation -> Evaluation Metrics
- Critical path:
  1. Preprocessing: Load and prepare input images
  2. Feature extraction: Apply symbolic functions and terminals to extract features
  3. Evolutionary optimization: Evolve individuals (templates) to optimize the visual attention model
  4. Saliency map generation: Use the evolved model to generate saliency maps
  5. Evaluation: Compare performance with deep learning models using standard metrics and adversarial attacks
- Design tradeoffs:
  - Accuracy vs. robustness: BP may have slightly lower accuracy on clean images compared to deep learning models, but it offers significantly higher robustness against adversarial attacks.
  - Computational efficiency: BP's evolutionary optimization process may be computationally more expensive than training deep learning models, but it offers better long-term robustness.
  - Interpretability: BP's symbolic representation of features may be more interpretable than the black-box nature of deep learning models, but it may require more domain expertise to design effective symbolic functions.
- Failure signatures:
  - Low performance on clean images: If BP's performance on clean images is significantly lower than deep learning models, it may indicate issues with the symbolic functions or evolutionary optimization process.
  - Sensitivity to adversarial attacks: If BP's performance degrades significantly under adversarial attacks, it may indicate vulnerabilities in the evolved models or the symbolic representation of features.
  - Computational inefficiency: If BP's evolutionary optimization process is excessively slow or resource-intensive, it may limit its practical applicability.
- First 3 experiments:
  1. Compare BP's performance on clean images with deep learning models using standard SOD datasets (e.g., FT, ImgSal, PASCAL-S).
  2. Evaluate BP's robustness against FGSM and other adversarial attacks on the same datasets.
  3. Test BP's performance on a real-world dataset (e.g., SNPL) with challenging conditions and compare it with deep learning models.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the findings that warrant further investigation regarding the generalizability and limitations of symbolic learning approaches in adversarial contexts.

## Limitations
- The study only tests BP against five specific deep learning models, limiting generalizability to other architectures
- Evaluation is constrained to specific attack types without testing more sophisticated adaptive attacks
- The performance gap between BP and deep learning on clean images (82.43% vs 50.22% Fβ) suggests potential accuracy limitations
- Lack of implementation details for BP makes exact reproduction difficult

## Confidence
- BP's robustness advantage under tested attacks: **High**
- BP's superior overall performance: **Medium** (limited by lower accuracy on clean images)
- Generalization to other attack types: **Low** (only tested on specific attacks)

## Next Checks
1. Test BP against more sophisticated adaptive attacks that specifically target symbolic representations
2. Evaluate computational efficiency and training time compared to deep learning baselines
3. Validate results across additional SOD datasets and tasks beyond visual attention