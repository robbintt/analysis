---
ver: rpa2
title: On The Coherence of Quantitative Evaluation of Visual Explanations
arxiv_id: '2302.10764'
source_url: https://arxiv.org/abs/2302.10764
tags:
- methods
- explanations
- explanation
- evaluation
- smoothgrad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates several commonly used visual explanation
  methods (Grad-CAM, Integrated Gradients, LRP, occlusion, RISE, smoothgrad) using
  three metrics: average drop %, insertion/deletion curves, and pointing game. The
  methods are evaluated on a subset of ImageNet-1k using VGG16 and ResNet-50 networks.'
---

# On The Coherence of Quantitative Evaluation of Visual Explanations

## Quick Facts
- arXiv ID: 2302.10764
- Source URL: https://arxiv.org/abs/2302.10764
- Reference count: 40
- Primary result: No single metric can reliably rank explanation methods; choice of metric depends on desired explanation properties

## Executive Summary
This paper systematically evaluates the coherence of quantitative metrics used to assess visual explanation methods. The authors examine six popular explanation methods (Grad-CAM, Integrated Gradients, LRP, occlusion, RISE, smoothgrad) using three different evaluation metrics (average drop %, insertion/deletion curves, pointing game) on a subset of ImageNet-1k with VGG16 and ResNet-50 architectures. The key finding is that different metrics produce contradictory rankings of explanation methods, suggesting that the choice of evaluation metric significantly impacts which methods appear superior. This incoherence stems from fundamental differences in what each metric measures - some focus on prediction preservation while others emphasize localization accuracy.

## Method Summary
The study evaluates explanation methods on a subset of 100 classes from ImageNet-1k validation set (5,000 images total) using pre-trained VGG16 and ResNet-50 models. For each image, the methods generate visual explanations (heatmaps) that highlight input regions relevant to the model's prediction. These explanations are then evaluated using three metrics: average drop % (prediction change when masking high-scoring regions), insertion/deletion curves (gradual insertion/removal of pixels based on relevance scores), and pointing game (localization accuracy by checking if the highest-scoring pixel falls within object bounding boxes). The authors also test the impact of explanation sparsity by applying Gaussian blur and examine different metric configurations (pixel vs region-based, real-valued vs binarized).

## Key Results
- Gradient-based methods produce sparse explanations while RISE and Grad-CAM generate coarse explanations
- Insertion and deletion metrics show opposite trends - methods scoring high on insertion score low on deletion
- Pointing game metric has very low correlation with other metrics
- Sparsity significantly impacts evaluation scores, with blurring improving insertion scores for sparse methods
- Different metric configurations (pixel vs region-based) have limited impact on method rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluation metrics based on pixel-level perturbation are sensitive to explanation sparsity
- Mechanism: Sparse explanations assign high relevance to few pixels, causing large prediction drops when those pixels are perturbed, leading to poor insertion scores but good deletion scores
- Core assumption: Pixel-level perturbation is the primary operation mode of insertion/deletion metrics
- Evidence anchors:
  - [section 4.4]: "Gradient-based methods such as IG and Smoothgrad generate very sparse explanations" and blurring them improved insertion scores
  - [section 4.2]: Region-based insertion/deletion shows different behavior for gradient-based methods
  - [corpus]: Weak corpus evidence - only 1 paper with FMR score >0.5
- Break Condition: If evaluation uses region-based perturbation instead of pixel-level

### Mechanism 2
- Claim: Coarse explanations perform better on insertion metrics due to preserved neighborhood information
- Mechanism: Upsampling from small feature maps creates smooth relevance distributions where neighboring pixels have similar values, so inserting high-scoring regions minimally impacts prediction
- Core assumption: Upsampling preserves relevance patterns across pixel neighborhoods
- Evidence anchors:
  - [section 4.1]: "methods that produce a coarse saliency map (RISE and Grad-CAM) perform better on the insertion metric"
  - [section 4.4]: Gaussian blur (inducing coarseness) improved insertion scores for gradient methods
  - [corpus]: No direct corpus support found
- Break Condition: If evaluation uses strict pixel-level replacement instead of neighborhood-based

### Mechanism 3
- Claim: Pointing game metric has low correlation with other metrics because it measures localization, not explanation quality
- Mechanism: Pointing game only checks if highest-scoring pixel is inside bounding box, ignoring relevance distribution and prediction preservation aspects measured by other metrics
- Core assumption: Localization and explanation quality are distinct objectives
- Evidence anchors:
  - [section 4.3]: "the pointing game metric has very low correlation scores with any of the other metrics"
  - [section 4.1]: "no real connection between the pointing game scores and any other metrics"
  - [corpus]: No corpus evidence for this specific claim
- Break Condition: If ground truth annotations perfectly align with model-relevant features

## Foundational Learning

- Concept: Evaluation metrics for visual explanations
  - Why needed here: Understanding different metrics (insertion, deletion, average drop, pointing game) is crucial for interpreting experimental results
  - Quick check question: What is the key difference between insertion and deletion metrics in terms of pixel manipulation?

- Concept: Sparsity in explanations
  - Why needed here: Sparsity affects how different metrics evaluate explanations, as shown by the blurring experiments
  - Quick check question: How does applying Gaussian blur to sparse explanations affect insertion and deletion scores?

- Concept: Region-based vs pixel-based evaluation
  - Why needed here: Different configurations of insertion/deletion metrics show varying sensitivity to explanation characteristics
  - Quick check question: Why might region-based evaluation be more appropriate for coarse explanations?

## Architecture Onboarding

- Component map: Data pipeline (ImageNet subset → preprocessing → model inference) → Explanation methods (Grad-CAM, IG, LRP, occlusion, RISE, smoothgrad) → Evaluation metrics (average drop, insertion/deletion curves, pointing game) → Sanity checks (internal consistency, inter-method reliability) → Visualization (KDE plots, explanation heatmaps, insertion/deletion curves)

- Critical path: Explanation generation → metric computation → correlation analysis → sanity check validation

- Design tradeoffs:
  - Pixel-level vs region-level perturbation (precision vs robustness to sparsity)
  - Real-valued vs binarized explanations (sensitivity vs noise reduction)
  - Different uninformative baselines (zero, blurred, random noise)

- Failure signatures:
  - Low inter-method reliability between metrics suggests contradictory evaluation criteria
  - High correlation between different metric configurations suggests robustness
  - Significant score changes after blurring indicate sparsity issues

- First 3 experiments:
  1. Generate explanations for all methods on sample images and verify visual differences
  2. Compute insertion/deletion curves for a single method to understand metric behavior
  3. Apply Gaussian blur to sparse explanations and measure metric score changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different levels of binarization in the average drop % metric affect the ranking of explanation methods, and what is the optimal threshold for binarization?
- Basis in paper: [explicit] The paper discusses the impact of binarizing explanations on the average drop % metric and shows that binarization can change the evaluation scores.
- Why unresolved: The paper provides results for different binarization thresholds but does not determine an optimal threshold or explain how it affects the ranking of explanation methods.
- What evidence would resolve it: A study comparing the rankings of explanation methods across different binarization thresholds and determining the threshold that provides the most consistent and meaningful rankings.

### Open Question 2
- Question: What is the relationship between the pointing game metric and other evaluation metrics, and can a unified evaluation framework be developed that incorporates both localization and explanation quality?
- Basis in paper: [explicit] The paper shows that the pointing game metric has very low correlation with other metrics, suggesting it measures a different aspect of explanation quality.
- Why unresolved: The paper does not explore why pointing game is uncorrelated with other metrics or propose a unified framework that combines localization and explanation quality.
- What evidence would resolve it: Research investigating the theoretical basis for the pointing game's low correlation and developing a framework that integrates localization accuracy with explanation quality measures.

### Open Question 3
- Question: How do the characteristics of explanation methods (e.g., sparsity, coarseness) impact their performance on different evaluation metrics, and can these characteristics be quantified and controlled?
- Basis in paper: [explicit] The paper discusses how sparsity and coarseness of explanations affect evaluation scores and suggests that these characteristics are fundamental to the performance of different methods.
- Why unresolved: The paper identifies the impact of sparsity and coarseness but does not provide a systematic way to quantify or control these characteristics across methods.
- What evidence would resolve it: A framework that quantifies the sparsity and coarseness of explanations and studies how these metrics correlate with performance on different evaluation methods.

## Limitations

- The study is limited to a subset of 100 ImageNet classes (5,000 images), which may not capture full dataset diversity
- Results may differ for other architectures beyond VGG16 and ResNet-50
- The paper examines only six explanation methods, potentially missing important approaches
- Pointing game's low correlation with other metrics is identified but not fully explored for alternative evaluation frameworks

## Confidence

- High confidence: Claims about sparsity affecting insertion/deletion scores (supported by controlled blurring experiments)
- Medium confidence: General rankings of explanation methods across metrics (method-specific behaviors may vary with architecture)
- Medium confidence: Pointing game correlation findings (limited to studied methods and dataset)

## Next Checks

1. Test whether results generalize to other datasets (e.g., COCO, CIFAR-10) to assess dataset dependency
2. Evaluate additional explanation methods (e.g., LIME, SHAP) to determine if findings apply beyond studied methods
3. Examine correlation patterns on different architectures (e.g., EfficientNet, Vision Transformers) to assess architecture dependency