---
ver: rpa2
title: Neural approximation of Wasserstein distance via a universal architecture for
  symmetric and factorwise group invariant functions
arxiv_id: '2308.00273'
source_url: https://arxiv.org/abs/2308.00273
tags:
- point
- neural
- network
- sets
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a general neural network architecture for approximating
  symmetric and factor-wise group invariant (SFGI) product functions. The main contribution
  combines this general framework with a sketching idea to develop an efficient neural
  network that can approximate the p-th Wasserstein distance between point sets, with
  model complexity independent of input point set sizes.
---

# Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions

## Quick Facts
- arXiv ID: 2308.00273
- Source URL: https://arxiv.org/abs/2308.00273
- Reference count: 28
- One-line primary result: NProductNet approximates Wasserstein distance with model complexity independent of input point set sizes

## Executive Summary
This paper presents NProductNet, a neural network architecture for approximating the p-th Wasserstein distance between point sets. The key innovation combines a universal approximation framework for symmetric and factor-wise group invariant (SFGI) functions with a sketching approach, enabling the network to handle point sets of varying sizes with model complexity independent of input size. The architecture achieves comparable or better Wasserstein approximations than state-of-the-art methods while generalizing significantly better to unseen point set sizes and training much faster.

## Method Summary
NProductNet uses a three-component architecture: hθ1 maps individual points to a fixed-dimensional embedding space, sum-pooling aggregates these embeddings, ϕθ2 transforms the pooled representation, and ρθ3 combines outputs from both point sets. The network leverages sketching ideas where point sets are encoded into a fixed-dimensional space via sum-pooling of element-wise embeddings, then decoded back to approximate the original set. This encoding/decoding process can be approximated by MLPs whose complexity depends only on the additive approximation error, not the input size.

## Key Results
- NProductNet achieves Wasserstein approximation with model complexity independent of input point set size
- The network generalizes significantly better to input point sets of sizes unseen during training
- NProductNet trains much faster than existing methods while maintaining or improving approximation quality

## Why This Works (Mechanism)

### Mechanism 1
The network achieves size-independent complexity through sketching, where point sets are encoded into a fixed-dimensional space via sum-pooling. The encoding function h: X → Rᵃ can be approximated by MLPs whose complexity depends only on the covering number of the original metric space, not the maximum input size.

### Mechanism 2
NProductNet universally approximates SFGI product functions by decomposing them into single-factor group invariant functions. The sketching framework reduces the problem of approximating SFGI functions over product spaces to approximating single-factor group invariant functions combined with symmetric pooling.

### Mechanism 3
The network generalizes better to unseen point set sizes because its fixed-dimensional embedding is size-agnostic. Unlike architectures whose complexity scales with input size, NProductNet learns a representation independent of point set size, enabling better generalization.

## Foundational Learning

- Concept: Group invariance and group actions
  - Why needed here: The network handles functions invariant to permutations of points within each set and symmetric across the pair of point sets
  - Quick check question: What is the difference between a function being permutation invariant to points within a set versus symmetric across multiple sets?

- Concept: Universal approximation theorems for neural networks
  - Why needed here: The proof that NProductNet can approximate any continuous SFGI function relies on showing each component can be approximated by neural networks
  - Quick check question: What is the classic result about MLPs approximating continuous functions, and what norm does it use?

- Concept: Wasserstein distance and its properties
  - Why needed here: The target function being approximated has specific properties used in the sketching construction
  - Quick check question: How is the p-th Wasserstein distance between two weighted point sets formally defined?

## Architecture Onboarding

- Component map: Input point sets → hθ1 → Sum-pooling → ϕθ2 → ρθ3 → Output
- Critical path: Input two point sets A and B → Apply hθ1 to each point in A, sum results → Apply hθ1 to each point in B, sum results → Apply ϕθ2 to both summed embeddings → Apply ρθ3 to sum of two transformed embeddings → Output
- Design tradeoffs: Fixed embedding dimension vs. information loss, sum-pooling vs. other pooling operations, depth vs. width of MLPs
- Failure signatures: Poor approximation on seen sizes (embedding too small), good training but poor generalization (overfitting to size patterns), training instability (inappropriate architecture or hyperparameters)
- First 3 experiments: 1) Verify permutation invariance by testing output consistency under point permutations, 2) Test size generalization by training on small sets and testing on larger ones, 3) Ablation study on embedding dimension to find minimum size maintaining quality

## Open Questions the Paper Calls Out
1. Can the sketching framework extend to other geometric optimization problems beyond k-means and Hausdorff distance?
2. What is the theoretical lower bound on approximation error achievable by any neural network for Wasserstein distance?
3. How does NProductNet's performance scale with the dimensionality of the underlying metric space?
4. Can the framework handle more complex group actions beyond permutations, such as rigid transformations?

## Limitations
- The theoretical framework assumes existence of a (δ, a, G)-sketch without explicit construction
- Experimental validation focuses primarily on synthetic datasets with limited real-world testing
- The covering number arguments may not hold for all metric spaces, particularly high-dimensional ones

## Confidence
**High confidence**: Architectural design and basic implementation details are well-specified
**Medium confidence**: Theoretical claims about universal approximation are logically sound given sketching assumptions
**Low confidence**: Generalization claims to unseen point set sizes primarily validated on synthetic datasets

## Next Checks
1. Explicitly construct and verify the (δ, a, G)-sketch claimed in the theory for Euclidean metric spaces
2. Test NProductNet on real-world point cloud datasets with naturally varying sizes to verify size-agnostic claims
3. Systematically vary the fixed embedding dimension to measure the trade-off between approximation quality and model complexity