---
ver: rpa2
title: Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting
arxiv_id: '2302.04813'
source_url: https://arxiv.org/abs/2302.04813
tags:
- explanations
- answer
- seed
- combinations
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimizing explanation-infused
  prompts for chain-of-thought reasoning in large language models. The core method
  generates candidate explanations via a leave-one-out prompting scheme, then uses
  two proxy metrics (one-shot silver accuracy and one-shot log likelihood) to efficiently
  prioritize combinations of explanations.
---

# Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting

## Quick Facts
- arXiv ID: 2302.04813
- Source URL: https://arxiv.org/abs/2302.04813
- Reference count: 16
- Key outcome: Improves accuracy by 4% on average across four reasoning tasks compared to crowdworker annotations

## Executive Summary
This paper addresses the challenge of optimizing explanation-infused prompts for chain-of-thought reasoning in large language models. The authors propose a two-stage framework that generates candidate explanations using a leave-one-out sampling scheme and then efficiently selects the best combinations using proxy metrics evaluated on a silver-labeled development set. Across four textual reasoning tasks (GSM8K, ECQA, E-SNLI, StrategyQA), the approach consistently outperforms crowdworker annotations, achieving an average 4% accuracy improvement.

## Method Summary
The method generates candidate explanations for each prompt example using a leave-one-out scheme, where the model generates explanations while holding out one example from the context. These candidates are then evaluated using two proxy metrics: one-shot silver accuracy and one-shot log likelihood, which approximate the performance of explanation combinations. The best combinations are validated against a silver-labeled development set created through pseudo-labeling with majority-voted predictions. The framework operates within a fixed search budget to balance computational efficiency with solution quality.

## Key Results
- 4% average accuracy improvement over crowdworker annotations across four reasoning tasks
- Oracle accuracy of 0.85 demonstrates strong correlation between proxy metrics and actual performance
- Effectiveness verified through both greedy decoding and self-consistency methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leave-one-out sampling creates diverse, correct explanations that improve downstream accuracy when combined.
- Mechanism: By removing one example and generating explanations for the remaining K-1, the model produces alternative reasoning paths that still arrive at the correct answer, enriching the candidate pool.
- Core assumption: The model can generate multiple valid reasoning paths for the same problem when context is varied.
- Evidence anchors:
  - "We use the exemplar set T and the seed sets ˜E excluding (qi, ˜ei,a i) to prompt the LLM and draw N (40 in our implementation) samples for ˆEi"
  - "We first generate sets of candidate explanations for each example in the prompt using a leave-one-out scheme"
- Break condition: If the model cannot generate diverse reasoning paths or consistently produces incorrect answers for the held-out example.

### Mechanism 2
- Claim: One-shot silver accuracy and one-shot log likelihood serve as effective proxies for full-combination performance.
- Mechanism: Individual explanations that perform well when used alone are likely to contribute positively to combinations, allowing efficient search over the combinatorial space.
- Core assumption: The performance of a combination can be approximated by aggregating the performance of its constituent explanations when used individually.
- Evidence anchors:
  - "We hypothesize that the prediction of a combination can be approximated with the prediction of each explanation used one-shot"
  - "we expect p(a|{ (qi,e i,a i)}i=1:K,q ;θ) to be higher when ∑i=1:Kp(a| (qi,e i,a i),q ;θ) is higher"
- Break condition: When explanations exhibit strong negative interactions or the one-shot performance poorly correlates with combination performance.

### Mechanism 3
- Claim: Pseudo-labeling unlabeled data with majority-voted predictions enables effective evaluation of explanation combinations without ground truth labels.
- Mechanism: By sampling multiple random combinations and using the most frequent prediction as the pseudo-label, we create a silver standard for measuring explanation quality.
- Core assumption: The majority prediction from diverse combinations is more likely to be correct than individual predictions.
- Evidence anchors:
  - "We use a pseudo-labeling approach to derive labels forV following past work (Wang et al., 2022c)"
  - "given q∈V, we sample random combinations of explanations to get predictions and use the majority-voted answer as the pseudo label ˆa"
- Break condition: When pseudo-labels are systematically incorrect or when the unlabeled set is too small to yield reliable majority votes.

## Foundational Learning

- Concept: Chain-of-thought prompting and explanation-infused prompts
  - Why needed here: The paper builds on the premise that explanations improve reasoning performance, so understanding this foundation is critical for grasping the optimization problem.
  - Quick check question: What is the primary difference between standard prompting and chain-of-thought prompting in terms of model output?

- Concept: Leave-one-out sampling strategy
  - Why needed here: This is the core method for generating candidate explanations, so understanding the mechanics is essential for implementation.
  - Quick check question: How does removing one example from the prompt affect the diversity of generated explanations?

- Concept: Proxy metrics and their relationship to true performance
  - Why needed here: The efficiency of the search algorithm depends on reliable proxy metrics, so understanding their correlation with actual accuracy is crucial.
  - Quick check question: Why might one-shot accuracy correlate with full-combination accuracy even though the latter involves more complex interactions?

## Architecture Onboarding

- Component map: Candidate generation -> Proxy scoring -> Silver labeling -> Combination search -> Evaluation
- Critical path: Candidate generation → Proxy scoring → Silver labeling → Combination search → Evaluation
- Design tradeoffs:
  - Search budget vs. explanation quality: Higher budgets allow more thorough search but increase computational cost
  - Number of candidates per example: More candidates improve diversity but increase search space
  - Pseudo-label quality vs. data quantity: More unlabeled examples improve silver label reliability but increase computation
- Failure signatures:
  - Proxy metrics poorly correlate with actual performance (indicated by low oracle accuracy)
  - Silver labels systematically incorrect (indicated by poor performance even on seed explanations)
  - Search converges to poor solutions (indicated by minimal improvement over random selection)
- First 3 experiments:
  1. Verify leave-one-out sampling produces correct explanations by checking answer accuracy on held-out examples
  2. Test correlation between one-shot proxy metrics and full-combination performance using a small labeled validation set
  3. Evaluate pseudo-label quality by comparing silver labels against ground truth on a subset of unlabeled data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the two proxy metrics (SOSAcc and SOSLL) perform when evaluated on other reasoning tasks beyond the four datasets studied?
- Basis in paper: explicit
- Why unresolved: The paper only evaluates these metrics on four specific datasets (GSM, ECQA, E-SNLI, StrategyQA), leaving open the question of generalizability to other reasoning tasks.
- What evidence would resolve it: Systematic evaluation of SOSAcc and SOSLL on a diverse set of reasoning tasks, including but not limited to logical reasoning, commonsense reasoning, and scientific reasoning.

### Open Question 2
- Question: What is the impact of using different decoding strategies (e.g., greedy decoding, self-consistency decoding, beam search) on the effectiveness of the explanation optimization framework?
- Basis in paper: explicit
- Why unresolved: The paper primarily uses greedy decoding and self-consistency decoding, but does not explore other decoding strategies that might further improve the performance of the optimized explanations.
- What evidence would resolve it: Comparative analysis of the framework's performance using various decoding strategies, including beam search and nucleus sampling, across multiple datasets.

### Open Question 3
- Question: How does the performance of the explanation optimization framework scale with the size of the unlabeled development set?
- Basis in paper: inferred
- Why unresolved: The paper uses a fixed-size unlabeled development set for each dataset, but does not investigate how the performance of the framework changes with varying sizes of the development set.
- What evidence would resolve it: Experimental results showing the performance of the framework as a function of the size of the unlabeled development set, across multiple datasets.

## Limitations
- Effectiveness depends critically on the quality of silver-labeled development set, which may not hold for tasks with inherent ambiguity
- Leave-one-out sampling assumes the model can generate diverse yet correct explanations when context is varied, which may not generalize across different model architectures
- Search budget constraint (evaluating only 100 combinations) may miss optimal solutions, particularly when the candidate space is large

## Confidence
- High confidence in the general framework and methodology
- Medium confidence in the empirical improvements reported (4% average gain)
- Medium confidence in the scalability and generalizability

## Next Checks
1. **Proxy metric validation**: Measure the correlation between one-shot silver accuracy/log likelihood and actual combination performance on a small labeled validation set to quantify the reliability of the proxy metrics.

2. **Silver label quality assessment**: Compare pseudo-labels against ground truth on a subset of the unlabeled development set to quantify the accuracy of the silver standard and its impact on explanation selection.

3. **Search budget sensitivity analysis**: Systematically vary the search budget (e.g., 50, 100, 200 combinations) to determine the point of diminishing returns and the computational cost of finding optimal explanation combinations.