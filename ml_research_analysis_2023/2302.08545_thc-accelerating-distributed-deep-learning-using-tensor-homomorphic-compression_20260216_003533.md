---
ver: rpa2
title: 'THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression'
arxiv_id: '2302.08545'
source_url: https://arxiv.org/abs/2302.08545
tags:
- compression
- workers
- training
- table
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: THC introduces Tensor Homomorphic Compression to address communication
  bottlenecks in distributed deep learning. Unlike traditional bidirectional compression
  that decompresses and recompresses at the parameter server, THC enables direct aggregation
  of compressed gradients using a homomorphic property, eliminating PS-side computational
  overhead and reducing quantization error.
---

# THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression

## Quick Facts
- arXiv ID: 2302.08545
- Source URL: https://arxiv.org/abs/2302.08545
- Reference count: 40
- Primary result: THC improves time-to-target accuracy by up to 1.51× and throughput by up to 1.58× compared to baselines.

## Executive Summary
THC addresses communication bottlenecks in distributed deep learning by introducing Tensor Homomorphic Compression, which enables direct aggregation of compressed gradients at the parameter server without decompression overhead. The framework uses stochastic quantization with optimized lookup tables and Randomized Hadamard Transform preprocessing to improve accuracy-bandwidth tradeoffs. Evaluation demonstrates significant improvements in time-to-target accuracy and throughput, with strong scalability and resilience to packet loss and stragglers.

## Method Summary
THC implements homomorphic compression where workers compress gradients using stochastic quantization and RHT preprocessing, then send compressed values directly to the parameter server. The PS aggregates these compressed gradients using lookup tables without decompression, then broadcasts the aggregated compressed gradient back to workers. Workers decompress and update their models. The framework includes error feedback to compensate for quantization bias and supports both software PS and programmable switch implementations.

## Key Results
- Improves time-to-target accuracy by up to 1.51× compared to baselines
- Increases training throughput by up to 1.58×
- Demonstrates strong scalability up to 30 workers with maintained accuracy
- Shows resilience to packet loss and straggler workers through synchronization schemes

## Why This Works (Mechanism)

### Mechanism 1
THC enables direct aggregation of compressed gradients without decompression at the parameter server, eliminating PS-side computational overhead. THC uses homomorphic compression to allow the PS to sum compressed gradient indices and values directly, avoiding the need to decompress each gradient individually before aggregation.

### Mechanism 2
THC reduces quantization error by preprocessing gradients with Randomized Hadamard Transform and bounding the support. RHT decorrelates gradient coordinates and reduces their dynamic range, allowing more accurate quantization within a smaller interval. Clamping extreme values further reduces range.

### Mechanism 3
THC scales to many workers and tolerates packet loss and stragglers through synchronization and partial aggregation schemes. THC's homomorphic property ensures that even with dropped packets or delayed workers, the aggregated gradient remains unbiased. Synchronization rounds and waiting for top N% of workers mitigate these effects.

## Foundational Learning

- **Stochastic quantization and unbiasedness property**: Needed because THC relies on stochastic quantization to ensure expected value of compressed gradient equals original for unbiased aggregation. Quick check: If a coordinate value of 0.7 is quantized to 0 with probability 0.3 and to 1 with probability 0.7, what is the expected quantized value?

- **Randomized Hadamard Transform (RHT) and gradient distribution effects**: Needed because RHT is used to preprocess gradients to reduce their dynamic range and make distribution more amenable to quantization. Quick check: How does applying RHT to a vector affect the expected maximum absolute value of its coordinates?

- **Lookup table optimization for non-uniform quantization**: Needed because THC uses optimized lookup tables to minimize quantization error when mapping compressed indices to values that can be directly aggregated. Quick check: If you have 4 bits per coordinate and a granularity of 30, how many distinct quantization values can you represent?

## Architecture Onboarding

- **Component map**: Worker compression module -> Worker communication module -> THC Parameter Server -> Lookup table generator

- **Critical path**:
  1. Worker computes gradient and adds error feedback
  2. Worker runs RHT and clamps values
  3. Worker quantizes and maps to table indices
  4. Worker sends compressed gradient to PS
  5. PS performs table lookup and sums values
  6. PS broadcasts aggregated compressed gradient to workers
  7. Workers decompress and update models

- **Design tradeoffs**:
  - Bit budget vs. compression ratio: Higher bit budget reduces quantization error but increases communication volume
  - Granularity vs. overflow risk: Larger granularity allows finer quantization but increases risk of overflow when summing across many workers
  - p-fraction vs. bias: Smaller p-fraction reduces range but increases bias from truncation

- **Failure signatures**:
  - High NMSE: Indicates quantization error is too large relative to signal
  - Training accuracy stalls: Suggests bias from truncation or insufficient compression
  - Packet loss causing model divergence: Indicates need for synchronization or error correction

- **First 3 experiments**:
  1. Run THC with 4 workers, ResNet50, ImageNet, measure TTA vs. uncompressed baseline
  2. Vary bit budget (2, 3, 4 bits) and measure NMSE and TTA tradeoff
  3. Simulate 1% packet loss and measure accuracy degradation with/without synchronization

## Open Questions the Paper Calls Out

- **Scalability beyond 30 workers**: The paper demonstrates scalability up to 30 workers but doesn't explore performance limits at larger scales or identify thresholds where accuracy degradation might become significant.

- **Optimal parameter balance**: The relationship between granularity, bit budget, and p-fraction remains unexplored across different network conditions and model architectures, with fixed parameters used in main experiments.

- **Long-term error-feedback stability**: The paper doesn't investigate whether error-feedback prevents gradient drift or maintains training stability over extended periods in highly unstable network conditions.

## Limitations

- Homomorphic property lacks strong empirical validation in corpus literature
- RHT preprocessing analysis relies on probabilistic bounds that may not hold for all gradient distributions
- Packet loss and straggler resilience claims are largely theoretical with minimal corpus evidence

## Confidence

- **High confidence**: THC's core communication reduction mechanism and basic stochastic quantization properties
- **Medium confidence**: RHT preprocessing improvement and lookup table optimization show theoretical promise
- **Low confidence**: Packet loss and straggler resilience claims, and specific homomorphic aggregation property

## Next Checks

1. **Validate the homomorphic property**: Implement a small-scale test where compressed gradients are summed directly at the PS, then measure if the aggregated result matches the expected uncompressed sum within acceptable error bounds.

2. **Test RHT preprocessing impact**: Compare THC with and without RHT preprocessing on the same gradient distributions, measuring quantization error and final training accuracy to quantify the claimed improvement.

3. **Stress test packet loss resilience**: Simulate varying levels of packet loss (0.1% to 5%) and measure accuracy degradation with and without the proposed synchronization scheme, verifying the claimed resilience holds in practice.