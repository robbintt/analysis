---
ver: rpa2
title: 'In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning'
arxiv_id: '2308.04275'
source_url: https://arxiv.org/abs/2308.04275
tags:
- alignment
- vanilla
- in-context
- language
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: In this work, we propose in-context alignment, a method to make
  vanilla pretrained language models capable of following chat-style instructions
  without any fine-tuning. We retrieve an average of 9 demonstration alignment examples
  from a candidate pool and use them as in-context examples when prompting Llama-2
  before any fine-tuning.
---

# In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning

## Quick Facts
- arXiv ID: 2308.04275
- Source URL: https://arxiv.org/abs/2308.04275
- Reference count: 1
- Vanilla Llama-2 achieves 78.4% win-rate against OpenAI's text-davinci-003 using in-context alignment without fine-tuning

## Executive Summary
This work introduces in-context alignment, a method to make vanilla pretrained language models follow chat-style instructions without any fine-tuning. The approach retrieves demonstration alignment examples from a candidate pool and uses them as in-context examples when prompting Llama-2. This technique results in a 7x increase in win-rate compared to direct prompting, making the vanilla model comparable to strong baselines with alignment fine-tuning. The method demonstrates that inference-time alignment through in-context learning can be an effective alternative to fine-tuning, with potential advantages in efficiency and interpretability.

## Method Summary
The method involves retrieving an average of 9 demonstration alignment examples from a candidate pool and using them as in-context examples when prompting Llama-2 models. The retriever (Contriever) indexes prompts from alignment data (OASST1) and retrieves the most relevant prompts to the input prompt during inference. These retrieved prompts, along with their paired responses, are used as in-context examples to guide the model's response. The vanilla Llama-2 model then generates responses based on these demonstrations without any fine-tuning.

## Key Results
- 13B Llama-2-vanilla achieves 78.4% win-rate vs. text-davinci-003 (up from 11.4% with direct prompting)
- 7x improvement in win-rate through in-context alignment without model weight changes
- Comparable performance to strong baselines with alignment fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retriever finds semantically similar demonstrations that guide response generation
- Mechanism: Contriever indexes OASST1 prompts and retrieves most relevant ones to input prompt, using retrieved prompts and responses as in-context examples
- Core assumption: Retriever can effectively find demonstrations relevant to test prompt
- Evidence anchors: Section describes Contriever implementation; weak corpus evidence found 25 related papers

### Mechanism 2
- Claim: Vanilla language model learns to follow instructions through in-context learning
- Mechanism: Model presented with demonstration examples showing how to respond to similar prompts, then generates response based on these demonstrations
- Core assumption: Vanilla language model has capacity to learn from in-context examples and generalize
- Evidence anchors: Abstract shows 7x win rate improvement; section reports 78.4% win rate from 11.4% baseline

### Mechanism 3
- Claim: Order of demonstration examples matters, with more relevant examples closer to test prompt
- Mechanism: Demonstrations ordered from less relevant to more relevant, with most relevant closest to test prompt
- Core assumption: Model can effectively use ordering to guide response
- Evidence anchors: Section describes demonstration template ordering; weak corpus evidence

## Foundational Learning

- **Concept: Retrieval-based in-context learning**
  - Why needed here: Retriever finds relevant demonstration examples to guide model's response
  - Quick check question: How does the retriever determine relevance of demonstration to test prompt?

- **Concept: Language model fine-tuning vs. in-context learning**
  - Why needed here: Paper compares vanilla model performance with and without in-context learning
  - Quick check question: What are advantages and disadvantages of in-context learning vs. fine-tuning?

- **Concept: Alignment of language models**
  - Why needed here: Paper focuses on aligning vanilla model to follow chat-style instructions
  - Quick check question: What are challenges in aligning language models to follow instructions?

## Architecture Onboarding

- **Component map:** Retriever (Contriever) -> Alignment data (OASST1) -> Vanilla language model (Llama-2) -> Test prompts (LIMA)
- **Critical path:** 1) Index alignment data prompts with retriever, 2) Retrieve relevant demonstrations for each test prompt, 3) Concatenate demonstrations and test prompt, 4) Generate response using vanilla model
- **Design tradeoffs:** Number of demonstrations (more may improve performance but increase computational cost), retriever quality (better retriever may find more relevant examples but be more expensive), alignment data size (larger data provides more diversity but harder to index)
- **Failure signatures:** Retriever fails to find relevant examples, demonstrations ineffective at guiding response, model fails to generalize from demonstrations
- **First 3 experiments:** 1) Evaluate performance with different numbers of demonstration examples, 2) Compare different retrievers (Contriever, BM25), 3) Evaluate impact of demonstration ordering on performance

## Open Questions the Paper Calls Out
1. Can in-context alignment be extended to support multi-turn dialogs or instructions with long contexts?
2. How can in-context alignment be improved to handle more fine-grained demonstration examples and avoid inconsistent responses?
3. Can in-context alignment be built upon or used as a foundation for reinforcement learning from human feedback (RLHF)?

## Limitations
- Dependence on single retriever architecture (Contriever) and alignment dataset (OASST1)
- Evaluation relies entirely on GPT-4-based automatic judgment rather than human evaluation
- Limited testing to single-turn instructions with 3K token context limit

## Confidence
- **High Confidence:** In-context alignment improves Llama-2-vanilla's instruction following vs. direct prompting, retrieval approach selects relevant demonstrations, template ordering strategy is effective
- **Medium Confidence:** 7x improvement represents general advantage, method makes vanilla models comparable to fine-tuned models, efficiency benefits are practically significant
- **Low Confidence:** 9 demonstration examples is optimal across configurations, Contriever is best retriever choice, results generalize to other models beyond Llama-2

## Next Checks
1. Ablation study on retriever choice: Replace Contriever with BM25/other retrievers and measure impact on win-rate
2. Human evaluation validation: Conduct human preference studies on LIMA prompts to verify GPT-4 judgments
3. Dataset diversity testing: Apply same approach to alternative alignment datasets (Anthropic's HH-RLHF, other open datasets)