---
ver: rpa2
title: Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under
  Unawareness setting
arxiv_id: '2302.08204'
source_url: https://arxiv.org/abs/2302.08204
tags:
- sensitive
- features
- counterfactual
- classi
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to detect bias in machine learning
  models that have been trained without sensitive features, i.e., in the Fairness
  Under Unawareness setting. The approach leverages counterfactual reasoning to assess
  whether the model's decisions exhibit discriminatory behavior.
---

# Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under Unawareness setting

## Quick Facts
- arXiv ID: 2302.08204
- Source URL: https://arxiv.org/abs/2302.08204
- Reference count: 22
- Primary result: Proposes counterfactual flips metric to detect bias in models trained without sensitive features, showing discrimination persists even under fairness under unawareness

## Executive Summary
This paper introduces a counterfactual reasoning approach to detect bias in machine learning models trained without access to sensitive features, addressing the fairness under unawareness setting. The method generates counterfactual samples that achieve positive outcomes and uses a sensitive-feature classifier to determine if these require changes in sensitive group membership. Experiments on three benchmark datasets demonstrate that models can still exhibit discriminatory behavior even without sensitive features in training, and the proposed Counterfactual Flips (CFlips) metric effectively uncovers these biases. The approach also enables identification of proxy features through correlation analysis between feature changes and sensitive feature flips.

## Method Summary
The method trains a Decision Maker classifier without sensitive features, then generates counterfactual samples for negative predictions to achieve positive outcomes. These counterfactuals are classified using a sensitive-feature classifier to detect if achieving positive outcomes requires changing sensitive group membership. The Counterfactual Flips metric measures the percentage of such flips, with higher values indicating stronger bias. The approach also identifies proxy features through correlation analysis between feature changes and sensitive flips. Experiments use three datasets (Adult, Crime, German) with various classifiers and debiasing algorithms.

## Key Results
- Models trained without sensitive features still exhibit discriminatory behavior, detected through counterfactual analysis
- Counterfactual Flips metric effectively measures bias levels, with lower values indicating less discrimination
- Correlation analysis between feature changes and sensitive flips identifies proxy features that encode sensitive information
- KDtree counterfactual generation strategy outperforms Genetic strategy in most cases
- CFlips becomes stable after generating at least 20 counterfactuals per sample

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual reasoning can reveal hidden bias even when sensitive features are removed from training data.
- Mechanism: By generating counterfactual samples that achieve positive outcomes and then classifying them with a sensitive-feature classifier, the method detects if achieving positive outcomes requires changes in sensitive group membership.
- Core assumption: Proxy features exist that are correlated with sensitive features, allowing the sensitive-feature classifier to predict sensitive attributes without direct access to them.
- Evidence anchors: [abstract] "even without sensitive features in the training set, algorithms can persist in discrimination"; [section] "the same counterfactual samples feed an external classifier (that targets a sensitive feature) that reveals whether the modifications to the user characteristics needed for a positive outcome moved the individual to the non-discriminated group"

### Mechanism 2
- Claim: The Counterfactual Flips metric quantifies discrimination by measuring changes in sensitive group membership when outcomes change.
- Mechanism: For each negative decision, counterfactual samples are generated to achieve positive outcomes. The percentage of these counterfactuals that change sensitive group membership indicates bias level.
- Core assumption: The sensitive-feature classifier can accurately predict sensitive attributes from non-sensitive features.
- Evidence anchors: [section] "Counterfactual Flip indicates the percentage of counterfactual samples belonging to another demographic group"; [section] "The bigger the CFlips value is, the stronger the biases and the discrimination the model suffers from"

### Mechanism 3
- Claim: Correlation analysis between feature changes and sensitive flips identifies proxy features.
- Mechanism: For each feature, compute the Pearson correlation between the magnitude of change in that feature across counterfactuals and the probability of sensitive group membership change. High correlation indicates proxy feature status.
- Core assumption: Proxy features have systematic relationships with sensitive features that manifest in counterfactual space.
- Evidence anchors: [section] "we leverage the deviation of counterfactuals from the original sample to determine which features are proxies of specific sensitive information"; [section] "Let be Î´ the difference between the posterior conditional probability of predicting a counterfactual sample and the original sample as belonging to the privileged group"

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: The method fundamentally relies on generating counterfactual examples to test bias hypotheses
  - Quick check question: What distinguishes a counterfactual example from a regular perturbed sample in this context?

- Concept: Proxy features and their detection
  - Why needed here: Understanding how non-sensitive features can encode sensitive information is crucial for interpreting results
  - Quick check question: How might a feature like "hours per week" act as a proxy for gender discrimination?

- Concept: Fairness metrics (DEO, DSP, DAO)
  - Why needed here: These metrics provide the baseline comparison for evaluating whether counterfactual analysis adds value
  - Quick check question: What does Difference in Equal Opportunity (DEO) measure, and why might it miss certain types of bias?

## Architecture Onboarding

- Component map:
  - Decision Maker -> Counterfactual Generator -> Sensitive-Feature Classifier -> Metric Calculator

- Critical path:
  1. Generate counterfactuals for negative decisions
  2. Classify counterfactuals with sensitive-feature classifier
  3. Compute CFlips metric
  4. Analyze feature correlations with sensitive flips

- Design tradeoffs:
  - Number of counterfactuals vs. computational cost
  - Sensitive-feature classifier accuracy vs. bias detection reliability
  - Generation strategy (Genetic vs KDtree) affects exploration vs. exploitation

- Failure signatures:
  - CFlips near zero despite known bias in Decision Maker
  - Sensitive-feature classifier accuracy below 70%
  - Unstable CFlips across different counterfactual generation strategies

- First 3 experiments:
  1. Run with minimal counterfactuals (10 per sample) to establish baseline behavior
  2. Test with different sensitive-feature classifiers (RF, MLP, XGB) to assess robustness
  3. Compare CFlips on debiased vs. non-debiased Decision Makers to validate method sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically identify proxy features in datasets where sensitive features have been removed?
- Basis in paper: [explicit] The paper mentions identifying proxy features through correlation analysis between feature changes and sensitive feature flips, but this is presented as preliminary evidence rather than a complete methodology.
- Why unresolved: The paper provides a method for detecting proxy features through counterfactual analysis but doesn't establish a comprehensive framework for identifying all potential proxy features in a dataset.
- What evidence would resolve it: A systematic framework that can identify all proxy features in a dataset without requiring counterfactual generation for every sample, possibly through statistical analysis or machine learning techniques.

### Open Question 2
- Question: What is the relationship between the number of counterfactual samples generated and the reliability of bias detection metrics?
- Basis in paper: [explicit] The paper notes that results become stable after generating at least 20 counterfactuals per sample, but doesn't explore the optimal number or the relationship between sample size and detection accuracy.
- Why unresolved: While the paper provides empirical observations about sample size, it doesn't establish theoretical bounds or systematic guidelines for determining the optimal number of counterfactuals needed.
- What evidence would resolve it: Mathematical proofs or extensive empirical studies showing how detection accuracy scales with the number of counterfactual samples, including confidence intervals and error bounds.

### Open Question 3
- Question: How do different counterfactual generation strategies affect the detection of bias in machine learning models?
- Basis in paper: [explicit] The paper compares Genetic and KDtree strategies, noting that KDtree searches among dataset samples while Genetic explores unexplored space, but doesn't comprehensively evaluate their relative effectiveness.
- Why unresolved: The paper provides initial comparisons but doesn't systematically evaluate how different generation strategies impact bias detection across various model types and datasets.
- What evidence would resolve it: Comparative studies showing how different counterfactual generation strategies perform across multiple datasets, model architectures, and bias types, including quantitative measures of their effectiveness.

### Open Question 4
- Question: Can we develop a debiasing model that works effectively in the fairness under unawareness setting?
- Basis in paper: [explicit] The paper concludes by mentioning plans to define a strategy for generating fair and actionable counterfactual samples to develop a debiasing model for the fairness under unawareness setting.
- Why unresolved: While the paper demonstrates that fairness under unawareness is insufficient to prevent bias, it doesn't provide a concrete solution for developing effective debiasing models in this setting.
- What evidence would resolve it: A working debiasing algorithm that can be trained without sensitive features and demonstrates improved fairness metrics across multiple datasets and model types.

## Limitations

- Method effectiveness depends critically on sensitive-feature classifier accuracy, which is not extensively validated across diverse datasets
- Counterfactual generation strategies may struggle with high-dimensional feature spaces, limiting practical applicability
- Assumes proxy features can be reliably identified through correlation analysis, which may be non-linear or context-dependent

## Confidence

- Counterfactual flips mechanism: Medium confidence - novel approach but limited empirical validation
- Proxy feature identification: Low confidence - correlation analysis is a weak proxy for causal relationships
- Method effectiveness: Medium confidence - shown to work on three datasets but lacks external validation

## Next Checks

1. Conduct sensitivity analysis on sensitive-feature classifier accuracy - test with classifiers ranging from 60% to 95% accuracy to establish minimum threshold for reliable bias detection
2. Evaluate method on additional datasets with known proxy feature structures to validate the correlation-based proxy identification approach
3. Compare CFlips metric performance against established fairness metrics (DEO, DSP) on debiased models to establish relative effectiveness and false positive rates