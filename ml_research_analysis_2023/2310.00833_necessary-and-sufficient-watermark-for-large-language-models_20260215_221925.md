---
ver: rpa2
title: Necessary and Sufficient Watermark for Large Language Models
arxiv_id: '2310.00833'
source_url: https://arxiv.org/abs/2310.00833
tags:
- texts
- text
- words
- generated
- green
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Necessary and Sufficient Watermark (NS-Watermark)
  to detect texts generated by large language models (LLMs) without degrading text
  quality. Unlike prior methods that insert many watermark tokens, NS-Watermark derives
  the minimum constraint on token distributions needed to reliably identify LLM-generated
  text, formulated as a constrained optimization problem.
---

# Necessary and Sufficient Watermark for Large Language Models

## Quick Facts
- arXiv ID: 2310.00833
- Source URL: https://arxiv.org/abs/2310.00833
- Authors: 
- Reference count: 40
- Key outcome: NS-Watermark detects LLM-generated text with zero false negatives while improving BLEU scores by up to 30 points compared to existing methods

## Executive Summary
This paper introduces the Necessary and Sufficient Watermark (NS-Watermark), a novel approach for detecting texts generated by large language models without degrading text quality. Unlike prior watermarking methods that insert many watermark tokens, NS-Watermark derives the minimum constraint on token distributions needed for reliable detection, formulated as a constrained optimization problem. The method dynamically adjusts watermark frequency based on text length, ensuring detection accuracy with minimal impact on natural language generation. Experiments demonstrate that NS-Watermark outperforms existing watermarking methods by up to 30 BLEU points and achieves zero false negatives with negligible impact on text quality.

## Method Summary
NS-Watermark is formulated as a constrained optimization problem that maximizes the probability of generated text while imposing minimum constraints necessary for detection. The key innovation is relaxing the constraint from requiring all generated words to be "green" (as in Hard-Watermark) to requiring only a proportion that exceeds a statistical threshold. The method uses a z-score threshold that scales with text length, allowing detection thresholds to work across different text lengths without requiring constant high proportions of green words. An efficient algorithm with linear time complexity O(αkTmax) is proposed, using beam search with dynamic programming to find optimal text generation paths while maintaining the required detection guarantees.

## Key Results
- Achieves zero false negatives in detection experiments
- Outperforms Soft-Watermark by up to 30 BLEU points on machine translation tasks
- Maintains high detection accuracy with negligible impact on text quality (low perplexity)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NS-Watermark imposes the minimum necessary constraint to ensure reliable LLM detection while preserving text quality.
- Mechanism: By relaxing the constraint from requiring all generated words to be green to requiring only a proportion that exceeds a statistical threshold, NS-Watermark reduces unnecessary constraints on text generation.
- Core assumption: The proportion of green words needed for detection can be significantly lower than 100% without sacrificing detection accuracy, especially for longer texts.
- Evidence anchors: [abstract]: "derive minimum constraints required to be imposed on the generated texts to distinguish whether LLMs or humans write the texts"
- Break condition: If the statistical relationship between text length and detection threshold doesn't hold, the minimum constraint calculation would need adjustment.

### Mechanism 2
- Claim: The z-score threshold provides a mathematically sound basis for detection that scales with text length.
- Mechanism: The z-score formula accounts for text length, allowing detection thresholds to be set that work across different text lengths without requiring constant high proportions of green words.
- Core assumption: The binomial distribution of green word counts in human-written text holds approximately true for the types of text being evaluated.
- Evidence anchors: [abstract]: "minimum margin for identifying whether the texts are written by LLMs"
- Break condition: If the green word distribution in human writing significantly deviates from binomial, the z-score may become unreliable.

### Mechanism 3
- Claim: Dynamic adjustment of watermark constraints based on estimated text length prevents over-constraining longer texts.
- Mechanism: By estimating the typical length of generated text without watermarks and adjusting the constraint accordingly, NS-Watermark avoids requiring too many green words in longer texts.
- Core assumption: Text length without watermarks is a good predictor of text length with watermarks, allowing for appropriate constraint adjustment.
- Evidence anchors: [abstract]: "we propose an efficient algorithm to solve it"
- Break condition: If watermark insertion significantly affects text length, the length estimation would become inaccurate.

## Foundational Learning

Concept: Statistical hypothesis testing and z-scores
- Why needed here: The detection mechanism relies on z-score thresholds to distinguish LLM-generated from human-written text
- Quick check question: What does a z-score represent in the context of this watermarking system, and how does it relate to the detection threshold Z?

Concept: Constrained optimization
- Why needed here: The watermarking problem is formulated as maximizing text probability subject to constraints on green word proportions
- Quick check question: How does the NS-Watermark formulation differ from traditional beam search optimization?

Concept: Beam search and dynamic programming
- Why needed here: The efficient algorithm uses beam search with dynamic programming to find optimal text generation paths
- Quick check question: Why can't conventional beam search be directly applied to the NS-Watermark problem, and how does the proposed algorithm address this limitation?

## Architecture Onboarding

Component map: Prompt → Length estimation → Constraint calculation → Dynamic programming search → Text generation → Z-score verification

Critical path: Prompt → Length estimation → Constraint calculation → Dynamic programming search → Text generation → Z-score verification

Design tradeoffs: The algorithm trades some computational complexity for better text quality compared to simpler watermarking methods

Failure signatures: Low BLEU scores, high false negative rates, or excessive green word insertion indicate implementation issues

First 3 experiments:
1. Generate text with NS-Watermark and verify z-score exceeds threshold Z
2. Compare BLEU scores of NS-Watermark vs Soft-Watermark on same prompts
3. Test robustness by replacing green words and verifying detection still works with appropriate β setting

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the NS-Watermark's performance scale with longer text sequences beyond the tested maximum length of 200 words?
- Basis in paper: [inferred] The paper tests up to 200 words but notes the z-score behavior changes with text length
- Why unresolved: The paper only evaluates up to Tmax=200 words, leaving uncertainty about effectiveness for very long documents
- What evidence would resolve it: Experiments testing NS-Watermark on texts of 500+ words or varying Tmax parameters beyond 200

Open Question 2
- Question: Can the NS-Watermark be adapted to work effectively with non-autoregressive generation models or models using different decoding strategies?
- Basis in paper: [inferred] The current method relies on beam search and sequential generation assumptions
- Why unresolved: The paper only demonstrates results with standard autoregressive LLMs using beam search
- What evidence would resolve it: Testing NS-Watermark with non-autoregressive models, sampling-based decoding, or models using different architectures

Open Question 3
- Question: What is the impact of NS-Watermark on multilingual text generation quality across different language pairs?
- Basis in paper: [explicit] The paper tests English↔German, English↔French, and C4 dataset
- Why unresolved: The experiments are limited to specific language pairs
- What evidence would resolve it: Testing NS-Watermark across diverse language families, including languages with different scripts and morphological complexity

## Limitations

- Heavy reliance on statistical assumptions about word distributions in human-generated text that may not hold for specialized domains
- Limited experimental validation scope, focusing mainly on machine translation and general natural language generation tasks
- Insufficient exploration of robustness against intentional attacks or sophisticated evasion techniques

## Confidence

High confidence: The core mathematical formulation of NS-Watermark as a constrained optimization problem is well-defined and the algorithmic approach for efficient implementation appears sound.

Medium confidence: The practical effectiveness of NS-Watermark across diverse real-world scenarios is supported by experimental results but limited in scope.

Low confidence: The robustness of NS-Watermark against intentional attacks or sophisticated evasion techniques is not thoroughly explored.

## Next Checks

1. **Domain Transfer Testing**: Evaluate NS-Watermark performance on specialized corpora including medical literature, legal documents, and technical manuals where vocabulary and writing patterns differ substantially from general text.

2. **Adversarial Robustness Analysis**: Design experiments where watermarked text is intentionally modified through synonym replacement, paraphrasing, or grammatical restructuring. Quantify the false negative rate under these conditions.

3. **Cross-Lingual Validation**: Test NS-Watermark across multiple language pairs beyond the French-English and German-English pairs used in experiments. Include languages with different morphological structures to assess whether the statistical assumptions hold across linguistic families.