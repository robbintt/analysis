---
ver: rpa2
title: Automatic Integration for Spatiotemporal Neural Point Processes
arxiv_id: '2310.06179'
source_url: https://arxiv.org/abs/2310.06179
tags:
- intensity
- function
- integration
- point
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AutoSTPP, a method for automatic integration
  of spatiotemporal neural point processes (STPPs). STPPs model discrete events in
  space and time, but integrating their likelihood functions is challenging due to
  triple integrals.
---

# Automatic Integration for Spatiotemporal Neural Point Processes

## Quick Facts
- arXiv ID: 2310.06179
- Source URL: https://arxiv.org/abs/2310.06179
- Reference count: 15
- Primary result: AutoSTPP enables exact likelihood computation for complex spatiotemporal point processes without numerical integration errors

## Executive Summary
This paper addresses the challenge of integrating likelihood functions for spatiotemporal neural point processes (STPPs), which require computing triple integrals over space and time. The proposed AutoSTPP method extends the dual network approach of Omi et al. [2019] to 3D STPPs by using a decomposable parametrization called ProdNet. This approach sidesteps computational complexities while maintaining consistency guarantees under certain assumptions. The method demonstrates superior performance compared to baselines, particularly when the intensity function is sharply localized, and is validated on both synthetic and real-world datasets.

## Method Summary
AutoSTPP uses a dual network architecture where an integral network F_θ approximates the cumulative intensity, and its partial derivative f_θ represents the intensity function. The method employs a decomposable parametrization with ProdNet, expressing the 3D influence function as a product of three univariate AutoInt derivative networks. This formulation allows exact likelihood computation via the Fundamental Theorem of Calculus, eliminating numerical integration errors. The model is trained by maximizing the log-likelihood function, and consistency is proven under assumptions of stationarity, ergodicity, absolute continuity, and truncated influence functions.

## Key Results
- AutoSTPP outperforms baseline methods in test log-likelihood and Hellinger distance metrics
- The method is particularly effective when intensity functions are sharply localized
- ProdNet parametrization with 3 ProdNets achieves optimal balance between expressivity and computational efficiency
- Consistency is proven under assumptions of stationary, ergodic, and truncated influence functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AutoSTPP enables exact likelihood computation for complex spatiotemporal intensity functions without numerical integration errors.
- Mechanism: AutoSTPP uses a dual network approach where the integral network F_θ is constructed such that its partial derivative with respect to time and space yields the intensity function f_θ. This allows the integral to be computed exactly using the Fundamental Theorem of Calculus, eliminating the need for numerical integration methods like Monte Carlo sampling.
- Core assumption: The intensity function can be represented as the derivative of a neural network, and this neural network can be trained to maximize the data likelihood without explicit integration.
- Evidence anchors:
  - [abstract] "AutoSTPP extends the dual network approach of Omi et al. [2019] to 3D STPPs by using a decomposable parametrization with ProdNet, which sidesteps computational complexities."
  - [section 3.3] "Specifically, let x := s ⊕ t ⊕ h, we approximate the integral of the intensity function as a DNN... The influence network f_θ is the partial derivative of the integral network F_θ."
- Break condition: If the intensity function cannot be represented as the derivative of a neural network, or if the neural network cannot be trained to approximate the integral accurately, the method will fail.

### Mechanism 2
- Claim: The decomposable parametrization with ProdNet effectively handles the computational complexities of triple integration in spatiotemporal point processes.
- Mechanism: AutoSTPP formulates the influence function f_θ(s1, s2, t) as a product of three univariate AutoInt derivative networks: f1_θ(s1)f2_θ(s2)f3_θ(t). This allows the application of 1D non-negativity constraints to each derivative network, satisfying the 3D non-negativity constraint for the intensity function while avoiding overly stringent constraints that would hinder expressivity.
- Core assumption: The intensity function can be approximated as a sum of products of univariate functions, and this decomposition maintains the ability to represent complex intensity functions.
- Evidence anchors:
  - [section 3.6] "We propose a different solution to enforce the 3D non-negative constraint called ProdNet... The triple antiderivative of the influence function is then F^1_θ(s1)F^2_θ(s2)F^3_θ(t), the product of their respective integral networks."
- Break condition: If the intensity function cannot be well-approximated by a sum of products of univariate functions, the method will lose expressivity and fail to capture complex intensity patterns.

### Mechanism 3
- Claim: AutoSTPP is a consistent estimator for point processes with truncated influence functions.
- Mechanism: AutoSTPP leverages the universal approximation theorem for derivative networks, showing that the set of derivative networks corresponding to two-layer feedforward integral networks is dense in C(R) with respect to the uniform norm. This, combined with the maximum likelihood estimator properties of linear self-excitation processes, ensures that the learned intensity function converges to the true intensity function in probability as the number of events increases.
- Core assumption: The ground truth point process is stationary, ergodic, absolutely continuous, and predictable, and the influence function is truncated (i.e., it becomes zero after a finite time).
- Evidence anchors:
  - [section 3.4] "Proposition 3.2. (Consistency of AutoInt Point Process) Under the assumption that the ground truth point process is stationary, ergodic, absolutely continuous and predictable, if the ground truth influence function is truncated... the maximum likelihood estimators f_θ converge to the true influence function f in probability as T → ∞."
- Break condition: If the ground truth point process does not satisfy the assumptions (e.g., non-stationary, non-ergodic, or with non-truncated influence function), the consistency of the estimator may be compromised.

## Foundational Learning

- Concept: Point processes and intensity functions
  - Why needed here: Understanding the basic concepts of point processes and intensity functions is crucial for grasping the problem AutoSTPP addresses and the solution it provides.
  - Quick check question: What is the intensity function in the context of spatiotemporal point processes, and how does it relate to the probability of event occurrence?

- Concept: Neural networks and automatic differentiation
  - Why needed here: AutoSTPP relies on neural networks to approximate the integral of the intensity function and uses automatic differentiation to compute the derivative network, which represents the intensity function.
  - Quick check question: How does automatic differentiation enable the computation of the derivative network from the integral network in AutoSTPP?

- Concept: Universal approximation theorem and consistency of estimators
  - Why needed here: The universal approximation theorem for derivative networks and the consistency of maximum likelihood estimators are key theoretical foundations that support the effectiveness and reliability of AutoSTPP.
  - Quick check question: What is the universal approximation theorem, and how does it apply to the derivative networks used in AutoSTPP?

## Architecture Onboarding

- Component map:
  Integral network (F_θ) -> Derivative network (f_θ) -> ProdNet components -> Intensity function

- Critical path:
  1. Construct the integral network F_θ
  2. Compute the derivative network f_θ as the partial derivative of F_θ
  3. Train the integral network to maximize the data likelihood using the derivative network
  4. Reassemble the parameters of the integral network to obtain the intensity function
  5. Compute the exact likelihood using the Fundamental Theorem of Calculus

- Design tradeoffs:
  - Expressivity vs. computational complexity: The decomposable parametrization with ProdNet simplifies the computational graph but may limit the expressivity of the intensity function compared to a fully connected multivariate network
  - Consistency vs. bias: Truncating the influence function ensures consistency but may introduce bias if the true influence function decays slowly

- Failure signatures:
  - Numerical instability: If the integral network grows too quickly or the gradients explode during training, the method may fail to learn a stable intensity function
  - Overfitting: If the number of ProdNets is too large, the model may overfit to the training data, leading to poor generalization
  - Underfitting: If the number of ProdNets is too small, the model may not have enough expressivity to capture complex intensity functions

- First 3 experiments:
  1. Implement the integral and derivative networks and verify that the derivative network correctly represents the intensity function
  2. Train the integral network on a simple synthetic dataset with a known intensity function and compare the learned intensity to the ground truth
  3. Evaluate the performance of AutoSTPP on a real-world dataset and compare it to baseline methods in terms of test log-likelihood and Hellinger distance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of ProdNets affect the model's performance in fitting non-decomposable functions?
- Basis in paper: [explicit] The paper mentions that increasing the number of ProdNets improves the model's flexibility at the cost of time and memory, and performs an ablation study on the number of ProdNets' effect on model performance.
- Why unresolved: The paper does not provide a clear conclusion on the optimal number of ProdNets, as it mentions that employing more ProdNets does not always lead to better performance.
- What evidence would resolve it: Conducting further experiments with varying numbers of ProdNets and comparing their performance on a wide range of functions would help determine the optimal number of ProdNets.

### Open Question 2
- Question: How does the proposed AutoSTPP method perform on higher-dimensional spatiotemporal point processes?
- Basis in paper: [inferred] The paper focuses on 3D STPPs and does not explicitly discuss the method's performance on higher-dimensional cases.
- Why unresolved: The paper does not provide any results or discussion on the method's performance for higher-dimensional STPPs.
- What evidence would resolve it: Implementing and testing the AutoSTPP method on higher-dimensional STPPs and comparing its performance with existing methods would provide insights into its effectiveness in higher dimensions.

### Open Question 3
- Question: How does the choice of activation function in the integral network affect the model's performance?
- Basis in paper: [explicit] The paper mentions that the activation function must be differentiable everywhere for the intensity to be computed recursively, but does not discuss the impact of different activation functions on performance.
- Why unresolved: The paper does not provide any experimental results or discussion on the effect of different activation functions on the model's performance.
- What evidence would resolve it: Conducting experiments with different activation functions in the integral network and comparing their performance on various datasets would help determine the optimal activation function for the AutoSTPP method.

## Limitations
- The decomposable parametrization with ProdNet may limit expressivity for highly complex spatiotemporal interactions that cannot be well-approximated by products of univariate functions
- Consistency proof relies on specific assumptions about the ground truth process being stationary, ergodic, and having a truncated influence function, which may not hold in all real-world applications
- The method requires careful tuning of the number of ProdNets to balance expressivity and computational efficiency

## Confidence

- **High confidence**: The fundamental mechanism of using dual networks to avoid numerical integration is sound and well-established in the 1D case (Omi et al. [2019])
- **Medium confidence**: The extension to 3D with ProdNet decomposition is theoretically valid but requires empirical validation across broader function classes to confirm expressivity limitations
- **Medium confidence**: The consistency proof under the stated assumptions is rigorous, but the practical implications of violating these assumptions in real-world data are not fully explored

## Next Checks
1. **Expressivity test**: Generate synthetic data with known intensity functions that cannot be decomposed into products of univariate functions and evaluate AutoSTPP's ability to recover these intensities compared to fully connected alternatives

2. **Assumption sensitivity analysis**: Systematically relax the stationarity, ergodicity, and truncation assumptions in synthetic experiments to quantify how violations affect AutoSTPP's consistency and performance relative to baselines

3. **Computational complexity benchmark**: Compare the training time and memory usage of AutoSTPP against Monte Carlo baselines across varying spatial and temporal resolutions to validate the claimed computational advantages