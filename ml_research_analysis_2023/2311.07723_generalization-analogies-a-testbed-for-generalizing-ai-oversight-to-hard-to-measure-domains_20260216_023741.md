---
ver: rpa2
title: 'Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure
  Domains'
arxiv_id: '2311.07723'
source_url: https://arxiv.org/abs/2311.07723
tags:
- generalization
- arxiv
- alpaca
- distribution
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GENIES, a benchmark for evaluating how well
  reward models generalize from training data to hard-to-measure domains. The authors
  construct 69 distribution shifts spanning 8 categories, including extreme shifts
  and shifts that probe for specific misgeneralization failures.
---

# Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains

## Quick Facts
- arXiv ID: 2311.07723
- Source URL: https://arxiv.org/abs/2311.07723
- Authors: 
- Reference count: 38
- Primary result: Reward models generalize remarkably well across extreme shifts but often misgeneralize in unexpected ways, favoring personas resembling internet text.

## Executive Summary
This paper introduces GENIES, a benchmark for evaluating how well reward models generalize from training data to hard-to-measure domains. The authors construct 69 distribution shifts spanning 8 categories, including extreme shifts and shifts that probe for specific misgeneralization failures. They find that LLaMA reward models generalize remarkably well across extreme shifts but often misgeneralize in unexpected ways, favoring personas resembling internet text. To address this, they consolidate 15 challenging distribution shifts into the GENIES benchmark and evaluate several tuning interventions, finding that techniques eliciting internal representations outperform standard fine-tuning but still struggle to distinguish instruction-following from conflated behaviors.

## Method Summary
The paper constructs 69 distribution shifts spanning 8 categories to evaluate reward model generalization. LLaMA models (30B, 13B, 7B, 3B) are used as base reward models with randomly initialized linear layers replacing the final unembedding layer. Various fine-tuning interventions (LoRA, prompt-tuning, MMS, LAT, CCS, CRA, few-shot classification) are applied to train on source data and evaluate generalization on target data. The elicitation (El) and differential elicitation (DE) metrics measure generalization accuracy, while RMS calibration error evaluates confidence calibration. The GENIES benchmark consolidates 15 challenging distribution shifts for standardized evaluation.

## Key Results
- LLaMA reward models generalize remarkably well across extreme shifts but often misgeneralize in unexpected ways, favoring personas resembling internet text
- Techniques eliciting internal representations (MMS, LAT) outperform standard fine-tuning but still frequently fail to distinguish instruction-following from conflated behaviors
- The best intervention achieves worse than random or close to random generalization accuracy on 6 out of 15 GENIES shifts, highlighting the difficulty of the task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reward models trained on simple tasks generalize to harder versions of those tasks, but this generalization can be misleading because the models are actually optimizing for low perplexity rather than true instruction-following.
- **Mechanism**: When fine-tuned on source tasks, reward models learn to prefer low-perplexity responses, which correlates with correct answers on simple tasks. This creates strong generalization across extreme distribution shifts but fails on probing shifts where low perplexity does not indicate good instruction-following.
- **Core assumption**: Low perplexity is a strong proxy for correct instruction-following on source distributions.
- **Evidence anchors**:
  - [abstract]: "LLaMA reward models generalize remarkably well across extreme shifts but often misgeneralize in unexpected ways, favoring personas resembling internet text."
  - [section]: "One of the most noticeable patterns in Figure 5 is that generalization accuracy is strongly correlated with zero-shot accuracy (r = 0.7)."
  - [corpus]: "Generalizing Reward Modeling for Out-of-Distribution Preference Learning" (no strong evidence that perplexity drives generalization; corpus is weak here).
- **Break condition**: If source distributions contain instructions where low perplexity responses are not better, the correlation breaks down and generalization fails.

### Mechanism 2
- **Claim**: Fine-tuning interventions that elicit internal representations (like MMS, LAT) outperform standard fine-tuning because they can target specific concepts like truthfulness rather than general instruction-following.
- **Mechanism**: Standard fine-tuning may conflate multiple concepts (instruction-following, helpfulness, agreeableness) because these co-occur in training data. Interventions that directly elicit specific internal representations can separate these conflated concepts.
- **Core assumption**: The base model has distinct internal representations for truthfulness vs instruction-following that can be separately elicited.
- **Evidence anchors**:
  - [abstract]: "Techniques for interpreting reward models' internal representations achieve better generalization than standard fine-tuning, but still frequently fail to distinguish instruction-following from conflated behaviors."
  - [section]: "We test seven tuning interventions: few-shot classification... LoRA fine-tuning, prompt-tuning, Mass Mean Shift (MMS), Linear Artificial Tomography (LAT), Contrast Consistent Search (CCS), and Contrastive Representation Arithmetic (CRA)."
  - [corpus]: "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning" (no direct evidence that internal representations can be separately elicited; corpus is weak here).
- **Break condition**: If the base model does not have distinct representations for the target concepts, these interventions cannot separate them.

### Mechanism 3
- **Claim**: Generalization accuracy correlates strongly across different scales of models, suggesting that techniques that work at small scales are likely to work at larger scales.
- **Mechanism**: As models scale up, they develop more consistent representations for instruction-following, making generalization behavior more predictable across different model sizes.
- **Core assumption**: The scaling trends in generalization are consistent and predictable across different model sizes.
- **Evidence anchors**:
  - [abstract]: "We find that reward models do not learn to evaluate 'instruction-following' by default and instead favor personas that resemble internet text."
  - [section]: "Section 5.4. Generalization of small models is moderately predictive of how larger models will generalize... Generalization accuracy correlations are shown between models of various sizes and LLaMA-30B."
  - [corpus]: "Generalizing Analogical Inference from Boolean to Continuous Domains" (no strong evidence about scaling trends in generalization; corpus is weak here).
- **Break condition**: If scaling introduces new failure modes not present at smaller scales, the correlation breaks down.

## Foundational Learning

- **Concept**: Distribution shift types (extreme vs probing)
  - Why needed here: The paper distinguishes between extreme distribution shifts (same task, different difficulty/quality) and probing shifts (testing for specific misgeneralization failures). Understanding this distinction is crucial for interpreting the results.
  - Quick check question: What is the difference between an extreme distribution shift and a probing distribution shift?

- **Concept**: Reward model training and evaluation
  - Why needed here: The paper evaluates reward models that predict which of two responses better follows an instruction. Understanding how these models are trained and evaluated is essential for understanding the generalization results.
  - Quick check question: How does a reward model differ from a generative model in this context?

- **Concept**: Fine-tuning interventions (LoRA, MMS, LAT, etc.)
  - Why needed here: The paper tests multiple fine-tuning interventions to improve generalization. Understanding what each intervention does and how it differs from others is crucial for interpreting the results.
  - Quick check question: What is the key difference between standard fine-tuning and interventions like MMS or LAT?

## Architecture Onboarding

- **Component map**: Base LLaMA models (30B, 13B, 7B, 3B) -> Reward model head (randomly initialized linear layer) -> Fine-tuning interventions (LoRA, MMS, LAT, CCS, CRA, prompt-tuning, few-shot) -> Dataset pipeline (69 distribution shifts across 8 categories) -> Evaluation metrics (Elicitation, Differential Elicitation, RMS Calibration Error)

- **Critical path**:
  1. Load base model and initialize reward model head
  2. Prepare source and target datasets
  3. Apply fine-tuning intervention to train on source
  4. Evaluate generalization on target
  5. Compute Elicitation and Differential Elicitation metrics

- **Design tradeoffs**:
  - Using reward models vs generative models (easier to train/evaluate but may not generalize the same way)
  - Fine-tuning vs eliciting internal representations (fine-tuning may conflate concepts, eliciting may be more precise but harder to implement)
  - Source data augmentation vs restricting to original distribution (augmentation may improve generalization but weaken the analogy to hard-to-measure domains)

- **Failure signatures**:
  - Strong generalization across extreme shifts but poor performance on probing shifts indicates the model is optimizing for spurious correlations rather than true instruction-following
  - Zero-shot and fine-tuned policies making similar mistakes suggests the fine-tuning is not eliciting new capabilities
  - Poor calibration (high RMS calibration error) indicates the model's confidence does not match its accuracy

- **First 3 experiments**:
  1. Implement LoRA fine-tuning and verify it achieves strong generalization on extreme distribution shifts but poor performance on probing shifts
  2. Implement MMS and compare its performance to LoRA on the GENIES benchmark
  3. Test whether adding a few target examples to the source data changes generalization behavior (sensitivity analysis)

## Open Questions the Paper Calls Out

# Open Question 1
- Question: How do reward models' misgeneralizations correlate with their scale, and what are the underlying mechanisms?
- Basis in paper: [explicit] The paper investigates how reward models generalize across distribution shifts and finds that
generalization improves with scale for extreme shifts but not for probing shifts. It also observes that larger models tend to
make similar mistakes as smaller models.
- Why unresolved: The paper does not provide a clear explanation for why scaling improves generalization for extreme shifts
but not for probing shifts, or why larger models make similar mistakes. It only speculates that larger models might rely less
on a "perplexity heuristic" but this is not supported by strong evidence.
- What evidence would resolve it: Further analysis of the internal representations of reward models at different scales, and
experiments that manipulate the training data to isolate the effects of scale and data distribution on generalization.

# Open Question 2
- Question: Can techniques that improve reward model generalization at small scales also improve generalization at larger
scales, and how far do these correlations extrapolate?
- Basis in paper: [explicit] The paper finds that generalization accuracy correlates strongly across scales for LLaMA models
within an order of magnitude, suggesting that techniques that improve generalization at small scales may also improve
generalization at larger scales.
- Why unresolved: The paper only evaluates models within an order of magnitude, so it is unclear how far these correlations
extrapolate to much larger models. It also does not investigate whether techniques that improve generalization at small scales
would continue to be effective at larger scales.
- What evidence would resolve it: Experiments that evaluate reward model generalization at a wider range of scales, and
experiments that test the effectiveness of generalization-improving techniques at different scales.

# Open Question 3
- Question: What are the limitations of the capability measure used in the paper (target-tuned capability), and how can
alternative measures be developed?
- Basis in paper: [inferred] The paper acknowledges that the target-tuned capability measure has shortcomings, such as not
revealing whether models have an abstract representation of instruction-following, and potentially being influenced by spurious
cues in target datasets.
- Why unresolved: The paper does not propose or evaluate alternative capability measures that could address these
limitations.
- What evidence would resolve it: Development and evaluation of alternative capability measures that address the
limitations of target-tuned capability, and experiments that compare the effectiveness of different measures in predicting
generalization performance.

## Limitations

- The paper demonstrates that reward models can misgeneralize in unexpected ways but does not fully characterize the underlying mechanisms, particularly why scaling improves generalization for extreme shifts but not for probing shifts.
- While internal representation interventions show promise, the paper does not conclusively demonstrate that these interventions elicit separate concepts rather than new spurious correlations.
- The scaling trend analysis showing moderate predictability across model sizes has limited evidence, with only correlation coefficients reported but no systematic analysis of when and why these correlations might break down at larger scales.

## Confidence

- **High confidence**: The observation that reward models generalize well on extreme shifts but poorly on probing shifts is well-supported by the empirical results. The GENIES benchmark construction methodology is also clearly specified and reproducible.
- **Medium confidence**: The claim that fine-tuning interventions elicit internal representations differently from standard fine-tuning is supported by the results, but the paper does not conclusively demonstrate that these representations correspond to the intended concepts rather than new spurious correlations.
- **Low confidence**: The scaling trend analysis showing moderate predictability across model sizes has limited evidence, with only correlation coefficients reported but no systematic analysis of when and why these correlations might break down at larger scales.

## Next Checks

1. **Probe correlation robustness**: Systematically test the relationship between perplexity and instruction-following across a wider range of source distributions to identify when the correlation breaks down, particularly focusing on cases where low perplexity responses are not better.

2. **Validate internal representations**: Design targeted experiments to verify that MMS and LAT interventions are indeed eliciting separate representations for truthfulness vs instruction-following, rather than creating new spurious correlations that happen to work on the GENIES benchmark.

3. **Scale sensitivity analysis**: Test the generalization correlation across more model sizes (including smaller models than 3B) to determine the range over which the scaling trends are predictive, and identify specific failure modes that emerge at different scales.