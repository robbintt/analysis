---
ver: rpa2
title: Towards the Visualization of Aggregated Class Activation Maps to Analyse the
  Global Contribution of Class Features
arxiv_id: '2308.00710'
source_url: https://arxiv.org/abs/2308.00710
tags:
- visualization
- data
- values
- cams
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to aggregate Class Activation Maps
  (CAMs) from multiple samples to provide a global explanation of the classification
  for semantically structured data. The aggregation allows the analyst to make sophisticated
  assumptions and analyze them with further drill-down visualizations.
---

# Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features

## Quick Facts
- arXiv ID: 2308.00710
- Source URL: https://arxiv.org/abs/2308.00710
- Reference count: 40
- Key outcome: A method to aggregate CAMs from multiple samples to provide a global explanation of the classification for semantically structured data.

## Executive Summary
This paper presents a visualization approach for analyzing the global contribution of features in deep learning models by aggregating Class Activation Maps (CAMs) across semantically structured data samples. The method uses dual encoding (color for impact, size for variability) in a glyph-based visualization, combined with interactive histogram filtering to enable drill-down exploration of feature importance. The approach is demonstrated on network traffic classification, showing how analysts can identify significant features and refine their understanding through sample filtering.

## Method Summary
The method aggregates CAMs from multiple samples to create a global explanation of feature importance for semantically structured data. It extracts CAMs from a CNN with global average pooling, aggregates them using statistical measures (mean/median/mode for impact, entropy for variability), and visualizes results with square glyphs where color indicates impact direction and size indicates variability. Interactive histograms allow filtering samples by feature impact distribution to refine the global CAM. The approach is validated on the ISCX VPN-nonVPN dataset using a CNN with 1D convolutions.

## Key Results
- Aggregated CAMs provide a stable global explanation of feature importance for semantically structured data
- Dual encoding (color + size) effectively communicates both feature impact and variability at a glance
- Interactive histogram filtering enables refinement of global explanations by revealing subgroups within classes
- The visualization approach successfully identifies significant features and supports model adjustment decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating CAMs across semantically structured data produces a stable, global explanation of feature importance.
- Mechanism: CAMs from individual samples are collected into a 2D array (samples x features), then aggregated using statistical measures (mean, median, mode, etc.) to produce a single representative CAM per class. This aggregation leverages the fixed positional semantics of features to avoid alignment issues.
- Core assumption: The position of each feature is semantically consistent across all samples, so direct element-wise aggregation is valid.
- Evidence anchors:
  - [abstract] "We extend a recent method of Class Activation Maps (CAMs) which visualizes the importance of each feature of a data sample contributing to the classification. In this paper, we aggregate CAMs from multiple samples to show a global explanation of the classification for semantically structured data."
  - [section] "Semantically structured data is a special form of structured data where the order or position of each information unit is fixed. Consequently, it becomes feasible to aggregate local explanations in the form of CAMs for each information unit or feature across multiple samples."

### Mechanism 2
- Claim: Dual encoding (color + size) in the visualization allows analysts to perceive both feature impact and variability at a glance.
- Mechanism: Aggregated impact values are mapped to color using a diverging colormap (blue-white-red), while variability (e.g., entropy) is mapped to the area of a centered square patch. This leverages perceptual principles for faster pattern recognition than traditional heatmaps.
- Core assumption: Humans can simultaneously process color and size differences, and the diverging colormap will clearly distinguish high-impact from low-impact features.
- Evidence anchors:
  - [abstract] "Our visual representation for the global CAM illustrates the impact of each feature with a square glyph containing two indicators. The color of the square indicates the classification impact of this feature. The size of the filled square describes the variability of the impact between single samples."
  - [section] "We map the aggregated impact value for the feature importance to color... We also considered classical color maps... We discarded the former because of its issues regarding the brightness profile... Therefore, our choice was to implement a diverging color map..."

### Mechanism 3
- Claim: Interactive drill-down via histograms enables analysts to refine global CAMs by filtering samples with similar impact distributions.
- Mechanism: Clicking a glyph shows a histogram of impact values for that feature. Analysts select a range, which filters samples and recomputes a CAM based only on those samples, allowing iterative refinement.
- Core assumption: Multi-modal distributions within a class indicate distinct subgroups, and filtering by mode reveals cleaner patterns.
- Evidence anchors:
  - [abstract] "For interesting features that require further analysis, a detailed view is necessary that provides the distribution of these values. We propose an interactive histogram to filter samples and refine the CAM to show relevant samples only."
  - [section] "For example, if a feature of a class is significant in many samples, but, in others, this feature is not relevant for the classification... The expert can filter the histogram by selecting a unique mode."

## Foundational Learning

- Concept: Class Activation Maps (CAMs) and their role in local interpretability.
  - Why needed here: Understanding how CAMs are generated from CNNs with GAP layers is essential to grasp how local explanations are aggregated into global ones.
  - Quick check question: How is a CAM computed from the last convolutional layer and the output layer weights?

- Concept: Diverging colormaps and perceptual encoding principles.
  - Why needed here: The visualization relies on a blue-white-red diverging colormap; knowing why rainbow maps are problematic and why diverging maps are better helps justify the design choice.
  - Quick check question: Why is a diverging colormap preferable to a rainbow colormap for data with both positive and negative values?

- Concept: Statistical aggregation and variability measures (mean, median, entropy, etc.).
  - Why needed here: Aggregating CAMs requires choosing appropriate statistics for central tendency and variability; entropy is used here but others are discussed.
  - Quick check question: What is the difference between variance and entropy as measures of variability in this context?

## Architecture Onboarding

- Component map: Data preprocessing -> CNN model -> CAM extraction -> Aggregation engine -> Visualization renderer -> Histogram filter
- Critical path: 1. Input sample → CNN → CAM 2. Collect CAMs → Aggregate (impact + variability) 3. Render grid (color + size) 4. User interaction → Histogram → Filter → Recompute CAM
- Design tradeoffs:
  - Fixed feature positions simplify aggregation but limit applicability to non-semantically structured data
  - Diverging colormap vs. other schemes: chosen for clarity and perceptual ordering
  - Entropy vs. other variability measures: entropy provides broader spread but may be less interpretable than standard deviation
- Failure signatures:
  - Inconsistent CAM shapes → aggregation fails or misaligns
  - Non-semantic data → global patterns become meaningless
  - Poor colormap choice → misinterpretation of impact direction
  - Histogram filtering yields no change → likely unimodal distribution or too coarse a filter
- First 3 experiments:
  1. Verify CAM extraction: Feed a known sample through the CNN, manually check CAM against output weights and GAP values
  2. Test aggregation stability: Generate CAMs from multiple synthetic samples with known patterns, aggregate and compare to expected result
  3. Validate visualization encoding: Plot a small synthetic dataset with known variability, confirm color and size encodings match expected values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different aggregation methods (mean, median, mode, kernel density estimation) affect the global explainability and interpretation of class activation maps (CAMs) for semantically structured data?
- Basis in paper: [explicit] The paper mentions that different aggregation methods can represent different aspects of the impact distribution in the classification analysis and provides multiple methods for the aggregation.
- Why unresolved: The paper does not provide a comparative analysis of the different aggregation methods and their impact on the global explainability of CAMs.
- What evidence would resolve it: A study comparing the effectiveness of different aggregation methods in providing global explainability for CAMs on semantically structured data.

### Open Question 2
- Question: What is the impact of using different variability measures (variance, standard deviation, entropy, Gini coefficient) on the interpretation of aggregated CAMs and their visualization?
- Basis in paper: [explicit] The paper mentions that different variability measures describe different characteristics of variability and should all be included as a user parameter in a potential application.
- Why unresolved: The paper does not provide a comparative analysis of the different variability measures and their impact on the interpretation of aggregated CAMs.
- What evidence would resolve it: A study comparing the effectiveness of different variability measures in interpreting aggregated CAMs and their visualization.

### Open Question 3
- Question: How can the correlation between impact values of gathered local CAMs be visualized and interpreted to provide additional insights into the model's behavior?
- Basis in paper: [explicit] The paper mentions the possibility of using the collection of CAMs to construct a correlation matrix that represents the correlation between the impact values.
- Why unresolved: The paper does not provide a method for visualizing and interpreting the correlation matrix to gain insights into the model's behavior.
- What evidence would resolve it: A study on visualizing and interpreting the correlation matrix of impact values to provide additional insights into the model's behavior.

## Limitations
- Limited to semantically structured data with fixed feature positions, restricting applicability to domains like images or text
- Heavy reliance on a single dataset (ISCX VPN-nonVPN) and one CNN architecture limits generalizability
- Effectiveness of the diverging colormap and dual encoding scheme lacks empirical validation through user studies
- Claim that entropy is optimal for variability measure lacks comparative validation against alternatives

## Confidence

- **High Confidence**: The CAM aggregation mechanism for semantically structured data (Mechanism 1) is technically sound given the fixed positional semantics assumption
- **Medium Confidence**: The visualization encoding principles (color + size) are grounded in perceptual research, but their effectiveness in this specific application context requires empirical validation
- **Low Confidence**: The claim that histogram filtering meaningfully refines global explanations (Mechanism 3) depends heavily on the distribution characteristics of real data, which are not extensively characterized

## Next Checks

1. **Cross-Dataset Validation**: Apply the method to a second semantically structured dataset with different characteristics (e.g., time-series data) to test generalizability of the aggregation approach

2. **Visualization Comprehension Study**: Conduct a small-scale user study comparing the proposed visualization against traditional heatmaps for identifying important features and variability patterns

3. **Variability Measure Comparison**: Implement and compare alternative variability measures (standard deviation, IQR) against entropy to empirically determine which provides the most useful signal for the drill-down functionality