---
ver: rpa2
title: Gathering and Exploiting Higher-Order Information when Training Large Structured
  Models
arxiv_id: '2312.03885'
source_url: https://arxiv.org/abs/2312.03885
tags:
- hessian
- neural
- optimization
- which
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to efficiently compute higher-order
  derivative information (Hessian, third derivatives) for large structured models
  by partitioning parameters into subsets and computing "pseudo-Hessians" and "pseudo-gradients"
  that capture interactions between parameter groups. The key innovation is an optimization
  method that uses this partitioned information without requiring full Hessian computation.
---

# Gathering and Exploiting Higher-Order Information when Training Large Structured Models

## Quick Facts
- arXiv ID: 2312.03885
- Source URL: https://arxiv.org/abs/2312.03885
- Reference count: 12
- One-line primary result: Method computes pseudo-Hessians via parameter partitioning to capture cross-layer interactions efficiently

## Executive Summary
This paper introduces a method to efficiently compute higher-order derivative information (Hessian, third derivatives) for large structured models by partitioning parameters into subsets and computing "pseudo-Hessians" and "pseudo-gradients" that capture interactions between parameter groups. The key innovation is an optimization method that uses this partitioned information without requiring full Hessian computation. Experiments on LeNet-5 and VGG-11' show the proposed approach captures non-trivial cross-layer interactions in the Hessian inverse, contradicting assumptions in methods like K-FAC that rely on block-diagonal approximations.

## Method Summary
The method partitions model parameters into S subsets, computes directional derivatives between these subsets using automatic differentiation, and aggregates them into a dense S×S matrix that captures interactions between parameter groups without computing the full Hessian. This pseudo-Hessian is invariant to affine reparameterizations and includes cross-layer interactions, generalizing between Cauchy's steepest descent (coarsest partition) and Newton's method (finest partition).

## Key Results
- Captures non-trivial cross-layer interactions in Hessian inverse for LeNet-5 and VGG-11'
- Achieves affine invariance under linear reparameterization of parameter subsets
- Generalizes between Cauchy's steepest descent and Newton's method through partition granularity

## Why This Works (Mechanism)

### Mechanism 1
Computing pseudo-Hessians and pseudo-gradients via partitioned parameter groups reduces computational complexity from O(P^2) to O(S^2) while preserving cross-layer interactions. The method partitions parameters into S subsets, computes directional derivatives between these subsets using automatic differentiation, and aggregates them into a dense S×S matrix that captures interactions between parameter groups without computing the full Hessian.

### Mechanism 2
The pseudo-Hessian matrix is invariant under affine reparameterization of parameter subsets, ensuring optimization trajectory independence from scale factors. For any sequence of nonzero scalings (αs) applied to parameter subsets, the optimization method produces the same trajectory because the pseudo-Hessian computation and optimization step both account for these scalings in a coordinated way.

### Mechanism 3
The method generalizes between Cauchy's steepest descent (coarsest partition) and Newton's method (finest partition), providing a scalable middle ground. By varying the partition granularity S, the method interpolates between using minimal Hessian information (S=1, Cauchy) and full Hessian information (S=P, Newton), with intermediate partitions capturing useful interaction information at manageable computational cost.

## Foundational Learning

- Concept: Automatic differentiation and higher-order derivatives
  - Why needed here: The method relies on computing directional derivatives efficiently using automatic differentiation tricks to avoid full higher-order tensor computation
  - Quick check question: How does the Pearlmutter trick compute Taylor expansion terms with O(d×P) complexity instead of O(P^d) complexity?

- Concept: Linear algebra and matrix inversion
  - Why needed here: The optimization method requires solving linear systems of size S×S, and understanding the computational tradeoffs between different partition sizes
  - Quick check question: What is the computational complexity of inverting an S×S matrix, and how does this scale with partition size?

- Concept: Neural network architecture and parameter grouping
  - Why needed here: The canonical partition used in experiments groups parameters by layers, and understanding this structure is crucial for applying the method to neural networks
  - Quick check question: How many parameter subsets S would result from partitioning a neural network with L layers using the canonical partition?

## Architecture Onboarding

- Component map: Parameter partitioner -> Directional derivative calculator -> Pseudo-Hessian assembler -> Linear solver -> Parameter updater
- Critical path: Partition parameters → Compute directional derivatives → Assemble pseudo-Hessian → Solve linear system → Update parameters → Repeat
- Design tradeoffs:
  - Partition granularity vs. computational cost: Finer partitions capture more information but increase computational burden
  - Dense vs. sparse pseudo-Hessian: The method produces dense matrices capturing all interactions, but this could be approximated for efficiency
  - Layer-wise vs. other partitions: While canonical layer-wise partitioning works well for neural networks, other partitions might be better for different architectures
- Failure signatures:
  - Singular or near-singular pseudo-Hessian indicating poor conditioning or insufficient information
  - Slow convergence or divergence suggesting the partition doesn't capture the optimization landscape well
  - Numerical instability in directional derivative computation for very large models
- First 3 experiments:
  1. Implement on a simple 2-layer MLP with varying partition sizes (S=1, S=2, S=4) to verify the generalization between Cauchy and Newton methods
  2. Test affine invariance by training the same network with different layer-wise scalings and comparing optimization trajectories
  3. Apply to a larger CNN (e.g., LeNet-5) and visualize the pseudo-Hessian structure to verify cross-layer interaction capture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between higher-order derivatives (order 3 and beyond) and the effectiveness of neural network optimization methods?
- Basis in paper: [explicit] The paper mentions that higher-order derivatives (order 3 and beyond) have not been studied much and are typically too costly to compute and store, but suggests they could be valuable for improving optimization methods.
- Why unresolved: The paper does not provide experimental evidence or theoretical analysis demonstrating the benefits of using higher-order derivatives in neural network optimization.
- What evidence would resolve it: Empirical studies comparing optimization performance using different orders of derivatives, or theoretical bounds showing convergence improvements with higher-order information.

### Open Question 2
- Question: How does the proposed method's performance scale with the number of layers in neural networks?
- Basis in paper: [explicit] The paper mentions that the method can be adapted to very large models by regrouping tensors into coarser partitions, but does not provide experimental validation for this claim.
- Why unresolved: The paper only demonstrates the method on LeNet-5 and VGG-11', which have relatively few layers compared to modern deep networks.
- What evidence would resolve it: Experimental results showing performance on networks with 100+ layers, including transformers, with varying partition strategies.

### Open Question 3
- Question: What is the optimal strategy for partitioning parameters when the number of layers is very large?
- Basis in paper: [inferred] The paper suggests that for very large models, tensors "of the same kind" could be regrouped, but does not specify which criteria should be used for this regrouping.
- Why unresolved: The paper only provides the canonical partition (one tensor per layer) and does not explore alternative partitioning strategies for large networks.
- What evidence would resolve it: Comparative studies of different partitioning strategies (by layer type, by parameter magnitude, by connectivity patterns) showing their impact on optimization performance and computational efficiency.

## Limitations
- Computational complexity claims lack rigorous empirical validation
- Cross-layer interaction significance not demonstrated through practical performance gains
- Partition sensitivity not explored across alternative partitioning strategies

## Confidence

**High confidence**: The theoretical framework for partitioned derivative computation and the mathematical relationships between pseudo-Hessian, pseudo-gradient, and the full higher-order tensors are well-established and internally consistent.

**Medium confidence**: The affine invariance property and the generalization between Cauchy and Newton methods are theoretically sound, but practical verification through comprehensive experiments is limited.

**Low confidence**: Claims about computational efficiency gains and practical performance improvements over existing optimization methods lack rigorous empirical validation across diverse architectures and datasets.

## Next Checks

1. **Runtime profiling**: Measure wall-clock time for pseudo-Hessian computation and optimization steps across different partition sizes (S=1, S=2, S=4, S=8) on LeNet-5, comparing against full Hessian computation and standard optimizers.

2. **Ablation study on partitioning strategy**: Implement alternative partition schemes (channel-wise, block-wise, random) and compare optimization performance and convergence rates to the canonical layer-wise approach.

3. **Transferability test**: Train the method on CIFAR-10, then fine-tune on a different dataset (e.g., SVHN or Fashion-MNIST) to assess whether the learned pseudo-Hessian structure generalizes across datasets.