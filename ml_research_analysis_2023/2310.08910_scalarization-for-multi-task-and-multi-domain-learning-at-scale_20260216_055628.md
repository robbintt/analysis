---
ver: rpa2
title: Scalarization for Multi-Task and Multi-Domain Learning at Scale
arxiv_id: '2310.08910'
source_url: https://arxiv.org/abs/2310.08910
tags:
- e-04
- training
- hair
- weights
- scalarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work conducts a large-scale analysis of scalarization for
  multi-domain and multi-task learning. It demonstrates that scalarization with optimal
  weights can match or outperform state-of-the-art multi-task optimization methods,
  especially as the number of tasks grows.
---

# Scalarization for Multi-Task and Multi-Domain Learning at Scale

## Quick Facts
- arXiv ID: 2310.08910
- Source URL: https://arxiv.org/abs/2310.08910
- Reference count: 40
- Key outcome: Scalarization with optimal weights can match or outperform state-of-the-art multi-task optimization methods, especially as the number of tasks grows.

## Executive Summary
This work conducts a large-scale analysis of scalarization for multi-domain and multi-task learning, demonstrating that scalarization with optimal weights can match or outperform state-of-the-art multi-task optimization methods. The study reveals that larger models benefit more from joint learning, optimal scalarization weights are robust to model capacity changes, and gradient conflicts are not strongly linked to performance. The authors propose using population-based training to efficiently search for optimal scalarization weights in large-scale settings.

## Method Summary
The method employs scalarization, which combines multiple task/domain losses through a weighted linear combination, with weights optimized using population-based training (PBT). The approach is tested across multiple datasets including CIFAR-10+STL-10 (2 domains), DomainNet (6 domains), CelebA (8 tasks), and Taskonomy-tiny (7 tasks), using various model architectures from ViT-S/4 to ResNet with different widths and depths. The scalarization weights are optimized using PBT to maximize performance across all tasks/domains, and the results are compared against state-of-the-art multi-task optimization methods.

## Key Results
- Scalarization with PBT-tuned weights can match or outperform state-of-the-art MTO methods like PCGrad, GradDrop, and IMTL-L
- Larger models show greater benefit from joint learning across tasks/domains
- Optimal scalarization weights are robust to changes in model capacity
- Gradient conflicts are not strongly linked to performance and are scarcely impacted by model capacity or weight choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scalarization with optimal weights can match or outperform state-of-the-art multi-task optimization methods.
- Mechanism: By carefully selecting task weights in the linear combination of losses, scalarization can balance the learning dynamics across tasks without incurring the computational overhead of gradient-based methods.
- Core assumption: The optimal weights can be found efficiently and are robust to model capacity changes.
- Evidence anchors:
  - [abstract] "This work conducts a large-scale analysis of scalarization for multi-domain and multi-task learning. It demonstrates that scalarization with optimal weights can match or outperform state-of-the-art multi-task optimization methods"
  - [section 4.1] "tuning the scalarization weights is crucial in some settings: For instance in DomainNet, training with p1 âˆˆ [0.65, 0.75] is more advantageous than uniform scalarization"
  - [corpus] Weak: No direct mention of scalarization outperforming MTO in related papers, but "AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics" suggests ongoing research interest in this area.
- Break condition: If the search space for optimal weights becomes too large or if task conflicts are too severe to be balanced by scalarization alone.

### Mechanism 2
- Claim: Larger models benefit more from joint learning across tasks/domains.
- Mechanism: As model capacity increases, the ability to capture shared representations across tasks improves, leading to better generalization and reduced interference.
- Core assumption: The tasks/domains share enough common structure that can be leveraged by a larger model.
- Evidence anchors:
  - [section 4] "When compared to a model trained on each task/domain individually, MDL/MTL performance tends to improve for larger model sizes"
  - [section 4.1] "the relative ranking of the weight p1, with respect to model performance, does not change significantly across model capacities"
  - [corpus] Weak: No direct mention of model capacity benefiting multi-task learning in related papers, but "Parameter-efficient Multi-Task and Multi-Domain Learning using Factorized Tensor Networks" suggests ongoing research in efficient multi-task learning.
- Break condition: If the tasks/domains are too dissimilar or if the model capacity increase does not lead to better shared representation learning.

### Mechanism 3
- Claim: Gradient conflicts are not strongly linked to performance and can be managed through proper weight tuning.
- Mechanism: While gradient conflicts naturally occur during training, their impact on final performance is minimal if the scalarization weights are properly tuned to balance the learning dynamics.
- Core assumption: The optimal weights can effectively manage gradient conflicts without needing explicit conflict resolution methods.
- Evidence anchors:
  - [section 4.2] "Gradients conflicts between tasks/domains naturally occur during MTL training: They behave differently across layers and learning rates, but are scarcely impacted by model capacity and scalarization weights choice"
  - [section 4.2] "we observe intriguing properties of gradient conflicts in practice, in particular suggesting that the extra cost of measuring, storing and correcting conflicting gradients at every training iteration can be superfluous"
  - [corpus] Weak: No direct mention of gradient conflicts in related papers, but "Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective" suggests ongoing research in understanding the role of scalarization in managing conflicts.
- Break condition: If the gradient conflicts become too severe to be managed by weight tuning alone, or if the tasks/domains are too dissimilar to benefit from joint learning.

## Foundational Learning

- Concept: Multi-task and multi-domain learning
  - Why needed here: Understanding the difference between multi-task and multi-domain learning is crucial for setting up the experiments and interpreting the results.
  - Quick check question: What is the key difference between multi-task learning and multi-domain learning in terms of input and output spaces?

- Concept: Scalarization and loss weighting
  - Why needed here: Scalarization is the core technique being investigated, and understanding how to properly weight the losses is essential for achieving good performance.
  - Quick check question: How does the choice of scalarization weights affect the learning dynamics and final performance in multi-task and multi-domain settings?

- Concept: Model capacity and its impact on learning
  - Why needed here: The paper investigates how model capacity affects the benefits of multi-task and multi-domain learning, so understanding this relationship is crucial.
  - Quick check question: How does increasing model capacity impact the ability to learn shared representations across tasks/domains, and what are the potential trade-offs?

## Architecture Onboarding

- Component map: Shared backbone (ResNet or ViT) -> Task/domain-specific heads -> Weighted loss combination -> Scalarization weights optimization via PBT
- Critical path: Select optimal scalarization weights -> Train model with weighted losses -> Evaluate performance across all tasks/domains
- Design tradeoffs: Model capacity vs. computational cost; simplicity of scalarization vs. potential benefits of more complex MTO methods
- Failure signatures: Poor performance due to suboptimal weight selection; interference between tasks; underutilization of shared representations
- First 3 experiments:
  1. Train a small model on CIFAR-10+STL-10 with uniform scalarization weights and evaluate the performance
  2. Train the same model with PBT-tuned scalarization weights and compare the performance to the uniform case
  3. Increase the model capacity and repeat the previous experiments to observe the impact on performance and optimal weight selection

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but the results suggest several important areas for future research, particularly regarding the scalability of the PBT approach to hundreds of tasks and the generalizability of findings to non-vision domains.

## Limitations
- Computational cost of PBT for weight optimization may not scale efficiently to hundreds of tasks
- Primary focus on classification and dense prediction tasks, with limited exploration of other domains like NLP or RL
- Limited mechanistic understanding of when and why scalarization succeeds or fails

## Confidence
- High Confidence: Larger models benefit more from joint learning across tasks/domains
- Medium Confidence: Scalarization with optimal weights can match or outperform state-of-the-art MTO methods
- Low Confidence: Gradient conflicts are not strongly linked to performance

## Next Checks
1. Conduct experiments with PBT weight optimization on problems with 50+ tasks to quantify how the computational overhead scales and whether the method remains practical for large-scale multi-task learning scenarios.

2. Apply the scalarization approach to a non-vision domain (such as natural language processing or reinforcement learning) to assess whether the observed benefits generalize beyond image-based tasks and whether gradient conflict patterns differ across domains.

3. Perform controlled experiments that systematically vary the severity of gradient conflicts (e.g., by introducing synthetic conflicts or using more dissimilar task pairs) while measuring both performance and conflict metrics to better establish the relationship between conflicts and scalarization effectiveness.