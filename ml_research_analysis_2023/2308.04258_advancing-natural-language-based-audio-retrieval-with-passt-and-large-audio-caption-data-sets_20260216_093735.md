---
ver: rpa2
title: Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption
  Data Sets
arxiv_id: '2308.04258'
source_url: https://arxiv.org/abs/2308.04258
tags:
- audio
- clothov2
- captions
- embedding
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a text-to-audio retrieval system based on pre-trained
  text and spectrogram transformers. The method projects audio recordings and textual
  descriptions into a shared embedding space where related examples from different
  modalities are close.
---

# Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets

## Quick Facts
- arXiv ID: 2308.04258
- Source URL: https://arxiv.org/abs/2308.04258
- Reference count: 0
- Ranked first in 2023 DCASE Challenge; outperforms current state-of-the-art on ClothoV2 by 5.6 pp mAP@10

## Executive Summary
This paper presents a text-to-audio retrieval system that achieves state-of-the-art performance by combining a self-attention-based audio encoder (PaSST) with large audio-caption datasets during pre-training. The system projects audio and text into a shared embedding space using contrastive learning, achieving a 5.6 percentage point improvement over previous state-of-the-art on the ClothoV2 benchmark. The approach ranked first in the 2023 DCASE Challenge and demonstrates the effectiveness of pre-trained transformers for audio retrieval tasks.

## Method Summary
The system uses a dual-encoder architecture with PaSST for audio and RoBERTa for text, trained with NT-Xent contrastive loss to align embeddings in a shared 1024-dimensional space. Training occurs in two stages: pre-training on AudioCaps, WavCaps, and ClothoV2 datasets, followed by fine-tuning on GPT-augmented ClothoV2 captions. Audio is converted to log-MEL spectrograms (128 bins, 1024-point FFT, 320 hop size) and truncated to 10 seconds for PaSST input. The final model achieves 35.22 mAP@10 on ClothoV2 test set.

## Key Results
- State-of-the-art performance on ClothoV2 benchmark: 35.22 mAP@10 (5.6 pp improvement over previous best)
- First place in 2023 DCASE Challenge for text-to-audio retrieval
- Pre-training on multiple datasets (AudioCaps, WavCaps, ClothoV2) improves performance from 30.18 to 35.22 mAP@10
- GPT-augmented captions reduce overfitting with 1.9 pp mAP@10 improvement during fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-attention-based audio encoder (PaSST) outperforms CNN-based encoders on audio retrieval tasks
- Mechanism: PaSST uses Patchout regularization and ImageNet-pretrained vision transformer weights, enabling more efficient and effective learning of audio representations compared to CNNs
- Core assumption: Audio classification benchmarks (AudioSet) performance correlates with audio retrieval performance
- Evidence anchors:
  - [abstract] "the self-attention-based audio encoder for audio embedding"
  - [section 3.3] "Switching from CNN14 to PaSST-N further improved the mAP@10 by 7.1 pp"
  - [corpus] No direct evidence found
- Break condition: If retrieval performance doesn't correlate with audio classification performance on relevant benchmarks

### Mechanism 2
- Claim: Pre-training on large audio-caption datasets improves retrieval performance on smaller target datasets
- Mechanism: Large datasets provide diverse training examples that help models learn better cross-modal alignment, reducing overfitting when fine-tuning on limited target data
- Core assumption: Additional data from AudioCaps and WavCaps provides useful semantic information not present in ClothoV2 alone
- Evidence anchors:
  - [abstract] "the utilization of additional human-generated and synthetic data sets during pre-training"
  - [section 3.1] "The best result overall was achieved by combining all three data sets" with mAP@10 of 35.22
  - [corpus] No direct evidence found
- Break condition: If additional data introduces significant noise or distribution shift that harms target task performance

### Mechanism 3
- Claim: GPT-augmented captions reduce overfitting during fine-tuning
- Mechanism: Augmentation increases caption diversity, forcing the model to learn more robust audio-caption associations rather than memorizing specific phrase patterns
- Core assumption: Increased caption variety leads to better generalization even if individual performance improvements are marginal
- Evidence anchors:
  - [abstract] "augmenting ClothoV2 captions with available keywords... successfully reduces overfitting during fine-tuning but only results in a minor performance improvement"
  - [section 3.5] "When finetuned with GPT-Augmentation, overfitting was slightly reduced, and the model improved by 1.9 pp. mAP@10"
  - [corpus] No direct evidence found
- Break condition: If augmentation introduces inconsistent semantic mappings that confuse the model

## Foundational Learning

- Concept: Contrastive learning and NT-Xent loss
  - Why needed here: The system learns to align audio and text embeddings by pushing matching pairs together and non-matching pairs apart in the shared space
  - Quick check question: What happens to the loss if all audio embeddings are identical but text embeddings differ?

- Concept: Transformer architectures and self-attention
  - Why needed here: PaSST and text encoders (BERT/RoBERTa) both rely on self-attention to capture long-range dependencies in audio spectrograms and text
  - Quick check question: How does the computational complexity of self-attention scale with sequence length?

- Concept: Audio spectrogram representation
  - Why needed here: Understanding how audio is converted to log-MEL spectrograms and why this representation is suitable for transformer models
  - Quick check question: Why use overlapping patches in spectrogram transformers versus CNNs?

## Architecture Onboarding

- Component map: Audio → Spectrogram → PaSST → Embedding → Projection → Shared space ← Text → Tokenizer → Embedding → Projection
- Critical path: Audio → Spectrogram → PaSST → Embedding → Projection → Shared space ← Text → Tokenizer → Embedding → Projection
- Design tradeoffs:
  - PaSST vs CNN: Better performance but fixed 10-second input length vs flexible but less effective
  - GPT augmentation: Reduces overfitting but adds computational cost and potential semantic inconsistencies
  - Pre-training datasets: Larger datasets help but may introduce distribution shift
- Failure signatures:
  - Poor retrieval: Check if audio/text embeddings are properly normalized
  - Overfitting: Monitor validation performance gap during fine-tuning
  - Slow training: Verify Patchout configuration and batch size appropriateness
- First 3 experiments:
  1. Replace PaSST with CNN14 to establish baseline performance difference
  2. Train on ClothoV2 only vs all three datasets to measure pre-training benefit
  3. Fine-tune with vs without GPT augmentation to quantify overfitting reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Patchout regularization technique compare to other regularization methods in terms of improving retrieval performance and preventing overfitting in audio transformers?
- Basis in paper: [explicit] The paper mentions that Patchout increases training speed and memory efficiency while acting as a regularizer, but does not compare it to other regularization techniques.
- Why unresolved: The paper focuses on the benefits of Patchout but does not provide a direct comparison with other regularization methods like dropout, weight decay, or data augmentation.
- What evidence would resolve it: Comparative experiments showing the performance of PaSST with and without Patchout versus other regularization techniques on the same audio retrieval tasks.

### Open Question 2
- Question: What is the impact of using different audio context lengths on the retrieval performance when using PaSST models with varying positional encoding capabilities?
- Basis in paper: [explicit] The paper investigates the impact of audio context length on PaSST-S20's performance, but does not explore other PaSST variants with different positional encoding lengths.
- Why unresolved: The study only examines one PaSST variant (PaSST-S20) with a 20-second positional encoding, leaving the effects of other PaSST models with different context lengths unexplored.
- What evidence would resolve it: Systematic experiments comparing the retrieval performance of different PaSST variants with varying positional encoding lengths on the same audio retrieval tasks.

### Open Question 3
- Question: How does the performance of the proposed dual-encoder system compare to other state-of-the