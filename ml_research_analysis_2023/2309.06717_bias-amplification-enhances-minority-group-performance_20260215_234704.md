---
ver: rpa2
title: Bias Amplification Enhances Minority Group Performance
arxiv_id: '2309.06717'
source_url: https://arxiv.org/abs/2309.06717
tags:
- group
- class
- stage
- training
- worst-group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a two-stage algorithm BAM to improve worst-group
  accuracy on datasets with spurious correlations, without requiring group annotations
  during training. BAM introduces a bias amplification scheme in Stage 1 using learnable
  auxiliary variables for each training example, then upweights misclassified examples
  in Stage 2.
---

# Bias Amplification Enhances Minority Group Performance

## Quick Facts
- arXiv ID: 2309.06717
- Source URL: https://arxiv.org/abs/2309.06717
- Reference count: 40
- One-line primary result: BAM achieves competitive worst-group accuracy without group annotations on standard benchmarks

## Executive Summary
This paper introduces BAM (Bias Amplification Method), a two-stage training algorithm that improves worst-group accuracy on datasets with spurious correlations without requiring group annotations during training. The method first amplifies bias through learnable auxiliary variables for each training example, then upweights misclassified examples in a second stage. BAM demonstrates competitive performance against state-of-the-art methods like JTT and GEORGE on standard benchmarks including Waterbirds and CelebA, while offering the advantage of not requiring group annotations. Additionally, the authors identify a simple stopping criterion based on minimum class accuracy difference that can replace the need for group annotations with minimal performance loss.

## Method Summary
BAM is a two-stage training algorithm designed to improve worst-group accuracy without requiring group annotations. In Stage 1, the model is trained using a bias amplification scheme where learnable auxiliary variables are introduced for each training example, making the model rely more on easy-to-learn examples and less on hard-to-learn minority group samples. In Stage 2, the training examples that were misclassified in Stage 1 are upweighted, effectively identifying and focusing on the minority group samples. The method also introduces a stopping criterion based on minimum class accuracy difference that can be used when group annotations are unavailable, removing the need for group annotations entirely while maintaining competitive worst-group accuracy.

## Key Results
- BAM achieves competitive worst-group accuracy compared to state-of-the-art methods (JTT, GEORGE) on Waterbirds and CelebA datasets
- The minimum class accuracy difference stopping criterion removes the need for group annotations with little or no loss in worst-group accuracy
- BAM demonstrates robustness across different class and group imbalance ratios in controlled experiments
- The method outperforms standard ERM and matches or exceeds the performance of methods requiring group annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bias amplification via auxiliary variables in Stage 1 makes the model rely more on easy-to-learn examples and less on hard-to-learn examples.
- Mechanism: Adding trainable auxiliary variables increases the difficulty of learning for the main network, so the network shifts focus toward easier samples and away from harder minority group samples.
- Core assumption: Hard-to-learn examples have lower initial accuracy and thus benefit more from auxiliary variables in Stage 1.
- Evidence anchors:
  - [abstract] "we propose Bam, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample"
  - [section 4.1] "We expect this effect to be more pronounced for hard-to-learn examples... adding auxiliary variables amplifies the bias toward easy-to-learn examples, making hard examples even harder to learn"
- Break condition: If the auxiliary coefficient λ is too small, the bias amplification effect disappears; if too large, the model may over-rely on auxiliary variables and underfit the main network.

### Mechanism 2
- Claim: The error set E from Stage 1 identifies minority group samples for upweighting in Stage 2.
- Mechanism: Misclassified samples by the bias-amplified model in Stage 1 correspond to minority groups that are harder to learn due to spurious correlations.
- Core assumption: Standard ERM training naturally fits majority group samples first, leaving minority group samples misclassified in Stage 1.
- Evidence anchors:
  - [section 4.1] "the samples that the model misclassified in the first stage can be treated as a proxy for hard-to-learn groups"
  - [section 4.2] "we upweight the training examples that are misclassified in Stage 1"
- Break condition: If spurious correlation is weak or absent, the error set E may not correspond to true minority groups, reducing the effectiveness of Stage 2.

### Mechanism 3
- Claim: Minimum class accuracy difference provides an effective stopping criterion without group annotations.
- Mechanism: When all classes have similar accuracy, the model's performance is more balanced across groups, indicating higher worst-group accuracy.
- Core assumption: Small class accuracy differences correlate with small group accuracy differences across classes.
- Evidence anchors:
  - [abstract] "we find a simple stopping criterion based on minimum class accuracy difference that can remove the need for group annotations"
  - [section 4.3] "ClassDiff being small is a necessary condition for having a small group accuracy difference"
- Break condition: If class imbalance is extreme or if class differences fluctuate without trend, ClassDiff may not reliably predict worst-group accuracy.

## Foundational Learning

- Concept: Group robustness and spurious correlations
  - Why needed here: The paper addresses the problem of poor performance on minority groups due to reliance on spurious features
  - Quick check question: What is the difference between average accuracy and worst-group accuracy in the presence of spurious correlations?

- Concept: Distributionally robust optimization
  - Why needed here: Understanding Group-DRO and other methods that require group annotations provides context for BAM's contribution
  - Quick check question: Why does Group-DRO require group annotations while BAM does not?

- Concept: Auxiliary variables in neural networks
  - Why needed here: BAM introduces learnable auxiliary variables per example, which is central to its bias amplification mechanism
  - Quick check question: How do auxiliary variables affect the learning dynamics of the main network?

## Architecture Onboarding

- Component map: Stage 1 (Bias Amplification) -> Error set E identification -> Stage 2 (Rebalanced Training) -> Stopping criterion evaluation
- Critical path: Stage 1 → Error set E identification → Stage 2 with upweighting → Stopping criterion evaluation
- Design tradeoffs:
  - λ vs. T: Larger λ reduces need for careful T tuning but may require more regularization
  - Upweight factor µ: Too high causes overfitting to error set, too low insufficient correction
  - One-M vs. Two-M: Continued training (One-M) preserves Stage 1 knowledge vs. fresh start (Two-M)
- Failure signatures:
  - No improvement in worst-group accuracy: Check λ value and T duration
  - Large gap between validation and test worst-group accuracy: Potential overfitting to validation set
  - ClassDiff not correlating with worst-group accuracy: Check class imbalance or spurious correlation strength
- First 3 experiments:
  1. Run BAM on Waterbirds with λ=50, T=20, µ=140 and compare to JTT
  2. Visualize auxiliary variable distributions across groups to verify bias amplification
  3. Test ClassDiff as stopping criterion on CelebA without group annotations and compare to GEORGE

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the discussion and limitations, several important questions arise:

1. How does the bias amplification mechanism generalize to other types of spurious correlations beyond the binary classification tasks studied here?
2. What is the theoretical explanation for why the minimum class accuracy difference criterion works well as a stopping criterion?
3. How does BAM perform on datasets with more severe class and group imbalance ratios?

## Limitations
- The method's effectiveness may be limited to datasets with clear spurious correlations between class labels and attributes
- Performance on datasets with extreme class imbalance (e.g., 100:1 ratio) is not thoroughly investigated
- The stopping criterion based on class accuracy difference may not generalize well to datasets with multiple spurious features

## Confidence
- High confidence: The core two-stage framework and its theoretical motivation (Medium/Low)
- Medium confidence: The effectiveness of the stopping criterion across diverse datasets (Medium)
- Low confidence: Generalization to real-world datasets with unknown spurious correlations (Low)

## Next Checks
1. Test BAM on datasets with extreme class imbalance (1:99 ratio) to evaluate robustness limits
2. Validate the stopping criterion on datasets with multiple spurious features to check generalization
3. Analyze the relationship between auxiliary variable magnitudes and group membership across training epochs to verify bias amplification mechanism