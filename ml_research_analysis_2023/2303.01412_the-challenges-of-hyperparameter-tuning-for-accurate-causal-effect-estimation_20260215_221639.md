---
ver: rpa2
title: The Challenges of Hyperparameter Tuning for Accurate Causal Effect Estimation
arxiv_id: '2303.01412'
source_url: https://arxiv.org/abs/2303.01412
tags:
- metrics
- cate
- selection
- causal
- estimators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the challenges of hyperparameter tuning
  and model selection in causal effect estimation. The authors conduct an extensive
  empirical study on various combinations of causal estimators, base learners, hyperparameters,
  and model evaluation metrics across four benchmark datasets.
---

# The Challenges of Hyperparameter Tuning for Accurate Causal Effect Estimation

## Quick Facts
- arXiv ID: 2303.01412
- Source URL: https://arxiv.org/abs/2303.01412
- Reference count: 13
- Key outcome: Hyperparameter tuning is crucial for state-of-the-art causal effect estimation, with evaluation metrics being the most important choice among estimators, learners, hyperparameters, and metrics

## Executive Summary
This paper investigates the challenges of hyperparameter tuning and model selection in causal effect estimation through an extensive empirical study. The authors examine various combinations of causal estimators, base learners, hyperparameters, and evaluation metrics across four benchmark datasets. Their results demonstrate that hyperparameter tuning is essential for achieving state-of-the-art performance in causal effect estimation, regardless of the specific causal estimator or base learner chosen. However, they also find significant variability in the performance of commonly used evaluation metrics, highlighting the need for more reliable model selection methods.

## Method Summary
The study evaluates 7 causal estimators (S-Learner, T-Learner, X-Learner, DR, DML, IPSW, CF) combined with 9 base learners (L1, L2, DT, RF, ET, KR, CB, LGBM, NN) across 4 benchmark datasets with 10 iterations each. For each combination, hyperparameter tuning is performed using cross-validation on validation sets, followed by evaluation on test sets using metrics including ϵATE, PEHE, ϵATT, and Rpol. The authors systematically compare performance across different estimator-learner combinations, hyperparameter configurations, and evaluation metrics to identify which factors most significantly impact causal effect estimation accuracy.

## Key Results
- Hyperparameter tuning alone can achieve state-of-the-art causal effect estimation performance regardless of causal estimator or base learner choice
- The choice of model evaluation metric has a more significant impact on final performance than the choice of causal estimator or base learner
- Learning-based model evaluation metrics show significant variability in model selection compared to non-learning metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperparameter tuning alone can achieve state-of-the-art causal effect estimation performance regardless of the choice of causal estimator or base learner.
- Mechanism: Hyperparameters directly control the flexibility and bias-variance tradeoff of base learners, which are the primary drivers of predictive accuracy needed for estimating treatment effects. Optimal tuning enables flexible machine learning models with proper regularization to approximate necessary nuisance functions well enough for accurate causal effect estimation.
- Core assumption: Flexible ML models with proper regularization can approximate the necessary nuisance functions (propensity scores and outcome models) well enough to enable accurate causal effect estimation.
- Evidence anchors: [abstract]: "Our results show that hyperparameter tuning increased the probability of reaching state-of-the-art performance in average (65% → 81%) and individualised (50% → 57%) effect estimation with only commonly used estimators."

### Mechanism 2
- Claim: Model evaluation metrics are the most important choice among causal estimators, base learners, hyperparameters, and evaluation metrics for final causal effect estimation performance.
- Mechanism: The evaluation metric drives hyperparameter tuning decisions, which determine the final model quality. Different metrics lead to selecting vastly different hyperparameter configurations, resulting in dramatically different causal effect estimates.
- Core assumption: The evaluation metric provides reliable feedback for model selection that correlates well with true causal effect estimation performance.
- Evidence anchors: [section]: "We conclude that most causal estimators are roughly equivalent in performance if tuned thoroughly enough. We also find hyperparameter tuning and model evaluation are much more important than causal estimators and ML methods."

### Mechanism 3
- Claim: Learning-based model evaluation metrics show significant variability in selecting the best model and ranking models from best to worst compared to non-learning metrics.
- Mechanism: Learning-based metrics inherit the same challenges of model selection and hyperparameter tuning that they are meant to solve, creating a recursive problem. This creates additional sources of variability and potential bias in the model evaluation process.
- Core assumption: Learning-based metrics are subject to their own hyperparameter tuning and model selection issues, making them less reliable for selecting optimal models.
- Evidence anchors: [section]: "The lack of clear pattern suggests there is no single best model selection method. It is also perhaps a good example demonstrating how much the final CATE performance can vary by changing only model selection metrics."

## Foundational Learning

- Concept: Causal inference with observational data
  - Why needed here: The paper deals with estimating causal effects from observational data where counterfactuals are missing, requiring specific assumptions and techniques to identify causal relationships.
  - Quick check question: What are the three key assumptions (SUTVA, ignorability, positivity) needed to identify causal effects from observational data, and what does each assumption mean?

- Concept: Nuisance functions and double/debiased machine learning
  - Why needed here: The paper discusses estimating nuisance functions (propensity scores and outcome models) using machine learning, and how these relate to different causal estimators like doubly robust and double machine learning approaches.
  - Quick check question: What are the two main nuisance functions needed for causal effect estimation, and how does double machine learning help reduce bias in their estimation?

- Concept: Model selection and hyperparameter tuning in supervised learning
  - Why needed here: The paper applies concepts from supervised learning (cross-validation, hyperparameter tuning, model evaluation metrics) to the causal inference setting, adapting these techniques for the unique challenges of causal effect estimation.
  - Quick check question: How does cross-validation help prevent overfitting in model selection, and why is this particularly important when the evaluation metric cannot directly access the causal effect?

## Architecture Onboarding

- Component map: Data → Estimator + Base Learner → Hyperparameters → Evaluation Metric → Performance on Test Set
- Critical path: The evaluation metric is the bottleneck because it determines which hyperparameter configurations are selected, which in turn determines the final causal effect estimates.
- Design tradeoffs: The main tradeoff is between computational cost and search space coverage. Exhaustive exploration provides comprehensive results but requires significant computational resources.
- Failure signatures: Poor performance due to suboptimal hyperparameter choices, inconsistent results across different evaluation metrics, high variance in performance across dataset iterations, and gap between Oracle performance and practical model selection performance.
- First 3 experiments:
  1. Implement a simple S-learner with OLS as the base learner and test it on one dataset using MSE as the evaluation metric to establish a baseline.
  2. Add hyperparameter tuning for the base learner (e.g., regularization strength) and compare performance to the baseline to demonstrate the impact of hyperparameter tuning.
  3. Switch to a different evaluation metric (e.g., R-score) while keeping the same estimator and base learner to observe how evaluation metrics affect performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we reliably select model evaluation metrics for causal inference tasks that consistently achieve state-of-the-art performance across different datasets and estimators?
- Basis in paper: [explicit] The authors state "From the significant gap we find in estimation performance of popular evaluation metrics compared with optimal model selection choices, we call for more research into causal model evaluation to unlock the optimum performance not currently being delivered even by state-of-the-art procedures."
- Why unresolved: Despite numerous proposed metrics, none consistently outperforms others across all scenarios, and learning-based metrics introduce their own hyperparameter tuning challenges.
- What evidence would resolve it: A comprehensive study demonstrating a single evaluation metric that consistently achieves near-oracle performance across diverse causal inference datasets, estimators, and base learners.

### Open Question 2
- Question: Is it possible to develop a causal estimator that simultaneously performs well for both individual-level treatment effects (ITE) and average treatment effects (ATE) without requiring separate models for each?
- Basis in paper: [explicit] The authors note "Using an evaluation metric that matches estimation target may improve ranking, but not necessarily selecting the winning model" and "it is uncommon for a single causal estimator to provide accurate estimates for individual and average effects simultaneously."
- Why unresolved: Current estimators and evaluation metrics show poor correlation between ITE and ATE performance, suggesting fundamental limitations in single-model approaches.
- What evidence would resolve it: A causal estimator architecture that demonstrates consistently high performance for both ITE and ATE metrics across multiple benchmark datasets, with statistical significance.

### Open Question 3
- Question: What are the fundamental limitations of using goodness-of-fit metrics (like MSE) as proxies for causal effect estimation accuracy, and can we design better alternatives?
- Basis in paper: [explicit] The authors demonstrate through simulation that "MSE is a rather poor proxy for PEHE, which is quite counterintuitive as better fit is believed to generally lead to improved CATE estimates."
- Why unresolved: The disconnect between prediction accuracy on observed outcomes and causal effect estimation accuracy remains poorly understood, and no alternative metrics have proven consistently superior.
- What evidence would resolve it: A theoretical framework explaining the conditions under which goodness-of-fit metrics fail for causal inference, coupled with empirical validation showing improved alternative metrics.

## Limitations

- The study's findings may not generalize to other domains and datasets not included in the four benchmark datasets examined
- The computational cost of exploring the full space of estimator, learner, and hyperparameter combinations required limiting the exploration to commonly used methods and a subset of hyperparameter values
- The paper does not address potential issues with the ground truth causal effects used for evaluation, which may introduce bias or measurement error into the results

## Confidence

- Claims about hyperparameter tuning being crucial: Medium
- Claims about evaluation metrics being most important: Medium
- Claims about learning-based metrics showing variability: High
- Claims about the disconnect between goodness-of-fit and causal accuracy: High

## Next Checks

1. **Replicate the study on additional datasets**: Test whether the findings about hyperparameter tuning and evaluation metrics hold on datasets from different domains (e.g., healthcare, economics, social sciences) and with different characteristics (e.g., high-dimensional, non-linear relationships).

2. **Investigate the impact of ground truth quality**: Examine how sensitive the results are to the quality and accuracy of the ground truth causal effects used for evaluation. This could involve using multiple ground truth sources or introducing controlled amounts of noise into the ground truth.

3. **Explore the effect of extreme hyperparameter settings**: Conduct experiments with extreme hyperparameter values (e.g., very high regularization, very deep trees) to see if the observed patterns break down or if there are unexpected behaviors in these regimes.