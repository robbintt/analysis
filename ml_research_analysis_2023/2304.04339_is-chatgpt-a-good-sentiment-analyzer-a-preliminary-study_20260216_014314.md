---
ver: rpa2
title: Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study
arxiv_id: '2304.04339'
source_url: https://arxiv.org/abs/2304.04339
tags:
- chatgpt
- sentiment
- evaluation
- clause
- polarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates ChatGPT's sentiment analysis capabilities
  across five tasks (sentiment classification, aspect-based sentiment analysis, end-to-end
  ABSA, emotion cause extraction, and emotion-cause pair extraction) using 18 benchmark
  datasets. ChatGPT achieves competitive zero-shot performance, matching or surpassing
  fine-tuned BERT on many tasks, particularly in sentiment classification.
---

# Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study

## Quick Facts
- arXiv ID: 2304.04339
- Source URL: https://arxiv.org/abs/2304.04339
- Reference count: 22
- ChatGPT achieves competitive zero-shot performance, matching or surpassing fine-tuned BERT on many tasks, particularly in sentiment classification

## Executive Summary
This paper evaluates ChatGPT's sentiment analysis capabilities across five tasks using 18 benchmark datasets. The study finds that ChatGPT achieves competitive zero-shot performance, matching or surpassing fine-tuned BERT on sentiment classification tasks. While its exact-match performance on information extraction tasks is lower, human evaluation shows ChatGPT generates semantically reasonable results. Few-shot prompting significantly improves performance across tasks and domains, and ChatGPT demonstrates strong robustness to polarity shift phenomena. The model shows good open-domain generalization but performance drops on some long-tail domains like social media and medicine.

## Method Summary
The study evaluates ChatGPT (gpt-3.5-turbo) on five sentiment analysis tasks: sentiment classification, aspect-based sentiment analysis, end-to-end ABSA, emotion cause extraction, and emotion-cause pair extraction. The evaluation uses 18 benchmark datasets including SST-2, SemEval 2014-ABSA, and a Chinese news dataset. Zero-shot and few-shot prompting strategies are tested with 1, 3, 9, and 27 demonstration examples. Performance is compared against fine-tuned BERT baselines and state-of-the-art models using accuracy, F1 scores, and human evaluation for qualitative assessment.

## Key Results
- ChatGPT achieves competitive zero-shot performance on sentiment classification, matching or surpassing fine-tuned BERT
- Few-shot prompting significantly improves performance across tasks and domains, sometimes surpassing fine-tuned BERT
- ChatGPT demonstrates strong robustness to polarity shift phenomena (negation and speculation) compared to BERT

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT achieves competitive zero-shot performance on sentiment classification tasks through in-context learning. When provided with a task instruction and test example, ChatGPT leverages its pre-trained language understanding to classify sentiment polarity without additional training. The model's ability to generate semantically reasonable outputs even when exact matches aren't achieved suggests it captures underlying sentiment semantics rather than memorizing patterns. Core assumption: The pre-training corpus contains sufficient sentiment-labeled examples for the model to learn general sentiment understanding patterns.

### Mechanism 2
Few-shot prompting significantly improves ChatGPT's performance across tasks and domains through demonstration-based in-context learning. Providing demonstration examples allows ChatGPT to understand the expected output format and task-specific patterns, effectively transferring knowledge from the demonstrations to the test example. The improvement is particularly notable for long-tail domains where pre-training coverage may be sparse. Core assumption: The demonstration examples effectively convey the task requirements and expected output format to the model.

### Mechanism 3
ChatGPT demonstrates greater robustness to polarity shift phenomena (negation and speculation) compared to fine-tuned BERT models. ChatGPT's pre-training likely exposed it to diverse linguistic patterns including negation and speculation contexts, allowing it to learn to correctly interpret sentiment polarity in these challenging scenarios. The model appears to better understand contextual sentiment cues rather than relying solely on keyword matching. Core assumption: The pre-training corpus included sufficient examples of negation and speculation to teach the model these linguistic phenomena.

## Foundational Learning

- **Concept**: In-context learning
  - Why needed here: Understanding how ChatGPT can perform tasks without parameter updates is crucial for explaining its zero-shot and few-shot capabilities.
  - Quick check question: How does providing demonstration examples improve ChatGPT's performance without updating its parameters?

- **Concept**: Polarity shift phenomena
  - Why needed here: Recognizing negation and speculation as sentiment analysis challenges explains why ChatGPT's robustness in these areas is significant.
  - Quick check question: Why do negation and speculation present challenges for sentiment analysis models?

- **Concept**: Exact-match vs. semantic evaluation
  - Why needed here: Understanding the difference explains why human evaluation showed better results than automatic evaluation for ChatGPT on information extraction tasks.
  - Quick check question: What's the difference between exact-match evaluation and semantic evaluation, and why might semantic evaluation be more appropriate for LLM outputs?

## Architecture Onboarding

- **Component map**: Task instruction → Model input processing → ChatGPT inference → Output parsing → Evaluation → Results aggregation
- **Critical path**: Task instruction → Model input processing → ChatGPT inference → Output parsing → Evaluation → Results aggregation
- **Design tradeoffs**: The system trades computational efficiency (high inference costs for ChatGPT) for flexibility and generalization capability. It also trades exact-match precision for semantic understanding in information extraction tasks.
- **Failure signatures**: Poor performance on long-tail domains, failure to handle domain-specific terminology, incorrect handling of complex polarity shift combinations, and generation of semantically reasonable but dataset-nonconforming outputs.
- **First 3 experiments**:
  1. Test zero-shot performance on a held-out sentiment classification dataset to verify baseline capability.
  2. Conduct few-shot experiments with varying numbers of demonstrations to identify the optimal number of examples for performance improvement.
  3. Evaluate performance on a polarity shift test set to measure robustness to negation and speculation compared to a fine-tuned BERT baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's performance on sentiment analysis tasks vary across different language models (e.g., GPT-3.5 vs. GPT-4)?
- Basis in paper: The paper uses GPT-3.5-turbo for evaluation and mentions that results were obtained based on GPT-3.5 due to limited time and accessibility.
- Why unresolved: The paper does not compare ChatGPT's performance across different language model versions or architectures.
- What evidence would resolve it: Conducting the same evaluation using different language models (e.g., GPT-4) and comparing their performance on the same sentiment analysis tasks and datasets.

### Open Question 2
- Question: What is the impact of prompt engineering techniques (e.g., chain-of-thought prompting, few-shot prompting with different numbers of examples) on ChatGPT's performance across various sentiment analysis tasks?
- Basis in paper: The paper mentions that few-shot prompting can significantly improve performance and explores this technique with 1, 3, 9, and 27 examples.
- Why unresolved: The paper only tests a limited range of few-shot prompting variations and does not explore other advanced prompting techniques.
- What evidence would resolve it: Systematically testing various prompt engineering techniques (e.g., chain-of-thought, different shot numbers, different demonstration example selection strategies) and comparing their impact on ChatGPT's performance across all evaluated tasks.

### Open Question 3
- Question: How does ChatGPT's performance on sentiment analysis tasks compare to specialized domain-specific models when fine-tuned on the same data?
- Basis in paper: The paper compares ChatGPT to fine-tuned BERT models and state-of-the-art models but notes that SOTA models may have task-specific designs.
- Why unresolved: The paper does not fine-tune ChatGPT on the same data as the comparison models or compare it to specialized domain-specific models.
- What evidence would resolve it: Fine-tuning ChatGPT on the same datasets used to train comparison models and directly comparing performance across all tasks, including domain-specific datasets like social media and medical domains.

## Limitations

- The evaluation relies on ChatGPT's paid API (gpt-3.5-turbo), which may behave differently from the free ChatGPT interface users experience.
- The Chinese emotion cause extraction task uses a manually designed prompt and a non-public dataset from NEWS SINA, limiting reproducibility.
- While human evaluation shows ChatGPT generates semantically reasonable outputs, this introduces subjectivity that may not fully align with dataset annotation standards.

## Confidence

**High Confidence**: ChatGPT's competitive zero-shot performance on sentiment classification tasks is well-supported by multiple benchmark datasets showing matching or surpassing fine-tuned BERT baselines in accuracy and F1 scores.

**Medium Confidence**: The claim about ChatGPT's robustness to polarity shift phenomena is supported by comparative experiments showing 10% accuracy and 8% F1 improvements over BERT, but the underlying mechanism remains partially speculative.

**Medium Confidence**: The assertion that ChatGPT demonstrates strong open-domain generalization is supported by experiments across multiple domains, but the performance drop on long-tail domains suggests limitations not fully explored in the current scope.

## Next Checks

1. **Reproduce polarity shift experiments**: Conduct a focused experiment using a standardized polarity shift test set (negation and speculation scenarios) to independently verify ChatGPT's claimed 10% accuracy improvement over BERT on these challenging cases.

2. **Domain adaptation evaluation**: Test ChatGPT's performance on held-out examples from long-tail domains (medical forums, social media) to quantify the exact performance degradation and compare against multi-dataset fine-tuned BERT models to assess whether this represents a fundamental limitation or training data gap.

3. **Human evaluation reliability check**: Conduct a reliability study where multiple human annotators evaluate ChatGPT's outputs on information extraction tasks to establish inter-annotator agreement rates and determine whether the observed semantic quality matches dataset annotation standards.