---
ver: rpa2
title: 'PACOL: Poisoning Attacks Against Continual Learners'
arxiv_id: '2311.10919'
source_url: https://arxiv.org/abs/2311.10919
tags:
- data
- learning
- poisoning
- attacks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a new category of data poisoning attacks specific
  for continual learners, referred to as Poisoning Attacks Against Continual Learners
  (PACOL). The effectiveness of label flipping attacks inspires PACOL, but PACOL produces
  attack samples that do not change the sample's label and cause catastrophic forgetting.
---

# PACOL: Poisoning Attacks Against Continual Learners

## Quick Facts
- arXiv ID: 2311.10919
- Source URL: https://arxiv.org/abs/2311.10919
- Reference count: 40
- Key outcome: Presents PACOL, a novel clean-label poisoning attack for continual learners that causes catastrophic forgetting without changing sample labels.

## Executive Summary
This paper introduces PACOL (Poisoning Attacks Against Continual Learners), a new class of data poisoning attacks specifically designed for continual learning systems. Unlike traditional poisoning attacks that often rely on label flipping, PACOL generates poisoning samples by matching gradients of perturbed current-task data to gradients of label-flipped historical-task data, enabling catastrophic forgetting while keeping labels unchanged. The attack exploits the unique vulnerability of continual learners where injected poisoned samples from past tasks into future tasks can cause more severe forgetting than in offline settings.

## Method Summary
PACOL generates poisoning samples through an iterative optimization process that minimizes the distance between gradients computed from adversarial samples (with clean labels) on the current task and gradients from label-flipped samples of the targeted task. The method operates under an ℓ∞-norm constraint to maintain stealthiness. The paper evaluates PACOL against two main attack vectors: label-flipping attacks and the proposed PACOL attack, measuring their effectiveness at forcing continual learning systems to forget previously learned tasks. Experiments are conducted on multiple benchmark datasets (Rotation-MNIST, Split-MNIST, Split-SVHN, Split-CIFAR10) using both regularization-based methods (EWC, online EWC, SI) and generative replay (DGR).

## Key Results
- PACOL successfully causes catastrophic forgetting in continual learning systems while maintaining clean labels, making detection more difficult than label-flipping attacks.
- Label-flipping attacks are particularly effective in continual learning settings because poisoned samples from targeted tasks can be injected into future tasks, amplifying forgetting effects.
- The paper demonstrates that PACOL is more difficult to detect than label-flipping attacks when evaluated against data sanitization defenses.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PACOL generates poisoning samples by matching gradients of perturbed current-task data to gradients of label-flipped historical-task data.
- Mechanism: The algorithm iteratively updates poisoned samples to minimize the distance between two gradient vectors: one computed from adversarial samples with clean labels on the current task, and the other from label-flipped samples of the targeted task. This alignment ensures the malicious gradient mimics the effect of a label flip without changing labels.
- Core assumption: Gradient similarity translates into equivalent forgetting behavior on the targeted task during continual learning updates.
- Evidence anchors:
  - [abstract] "PACOL produces attack samples that do not change the sample's label and produce an attack that causes catastrophic forgetting."
  - [section 3.3] "If the continual learning model has gradients in adversarial subset ˜Dadv τ+1 that are close to the label-flipped gradients subset ˜Dadv τ then ˜Dadv τ+1 will have the same poisoning effect as ˜Dadv τ for making the model forget the knowledge on the targeted task Dτ."
  - [corpus] Weak evidence for "gradient matching" in other poisoning work; PACOL's gradient distance objective is novel in continual learning.
- Break condition: If the continual learner uses optimization strategies that damp or normalize gradients differently across tasks, gradient similarity may not map to identical forgetting effects.

### Mechanism 2
- Claim: Label-flipping attacks on continual learners cause more catastrophic forgetting than in offline settings because poisoned samples from the targeted task can be injected into future tasks.
- Mechanism: In continual learning, a small number of mislabeled samples from a learned task (task τ) can be injected into later tasks (τ+n). When the model updates on these mislabeled samples, it reduces performance on the original task τ because the model minimizes loss on both the new task and the poisoned historical data.
- Core assumption: The continual learner's loss minimization over mixed tasks causes weight updates that erase task τ knowledge.
- Evidence anchors:
  - [abstract] "Injecting samples from the target task into upcoming tasks could produce serious catastrophic forgetting even with a small amount."
  - [section 3.2] "Label-flipping attacks can be considered the most intuitive way to erase the knowledge learned from the model."
  - [corpus] Standard poisoning theory supports this, but PACOL extends it to the continual setting.
- Break condition: If the learner uses replay buffers or importance-based regularization strong enough to isolate task-specific parameters, injected label flips may not propagate forgetting.

### Mechanism 3
- Claim: PACOL's stealthiness comes from producing clean-label poisoning samples that evade outlier-based defenses better than label-flipping attacks.
- Mechanism: By using ℓ∞-norm bounded perturbations on current-task data while keeping labels unchanged, PACOL avoids the obvious distributional shift that label flips create. This makes detection harder for defenses that rely on label consistency or feature space outliers.
- Core assumption: Clean-label poisoning samples with small perturbations are harder to distinguish from benign data than samples with flipped labels.
- Evidence anchors:
  - [abstract] "we evaluate the data sanitization defense to filter out adversarial samples, and we demonstrate that the proposed attack is more difficult to detect than other attacks."
  - [section 4.4] "The detectability of the label-flipped samples under human censorship is higher than those with imperceptible perturbations."
  - [corpus] Weak evidence for detection difficulty; most poisoning work focuses on offline detection.
- Break condition: If defenses use semantic or content-based anomaly detection rather than purely statistical methods, clean-label perturbations may still be flagged.

## Foundational Learning

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: PACOL explicitly exploits catastrophic forgetting; understanding how replay, regularization, and architecture-based methods mitigate forgetting is key to appreciating the attack's impact.
  - Quick check question: In EWC, what role does the importance matrix play in preventing forgetting?

- Concept: Gradient-based optimization and adversarial perturbations
  - Why needed here: PACOL crafts poisoning samples by minimizing gradient distance; familiarity with PGD, gradient alignment, and loss landscapes is essential.
  - Quick check question: Why does PACOL clip perturbed samples to the [0,1] range?

- Concept: Data poisoning and backdoor attacks
  - Why needed here: PACOL is a clean-label poisoning attack; knowing how label flips and backdoor triggers affect model training helps contextualize the novelty.
  - Quick check question: How does a clean-label attack differ from a backdoor attack in terms of attacker capabilities?

## Architecture Onboarding

- Component map: Poisoning sample generation -> gradient computation -> perturbation optimization -> injection into continual learner -> evaluation pipeline (datasets, backbones, replay/reg methods)
- Critical path: Poisoning sample generation -> continual learning training loop -> targeted task performance evaluation -> defense evaluation
- Design tradeoffs: Larger perturbation bounds (ϵ) increase attack success but may reduce stealth; more optimization loops (K, S) improve gradient matching but increase compute cost
- Failure signatures: PACOL fails if the continual learner ignores incoming data from new tasks, if replay buffers perfectly restore old task knowledge, or if defenses successfully filter poisoned samples before training
- First 3 experiments:
  1. Implement PACOL on a simple 2-layer MLP with Rotation-MNIST and measure forgetting on task 1 after injecting 1% poisoned samples into task 5.
  2. Compare label-flipping vs. PACOL in terms of attack success and detectability using ℓ2-norm defense.
  3. Evaluate PACOL on DGR with CIFAR-10 splits and assess impact with and without replay buffers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are PACOL attacks on continual learning systems with different architectures, such as recurrent neural networks or transformers?
- Basis in paper: [inferred] The paper only evaluates PACOL on feed-forward neural networks and does not explore its effectiveness on other architectures.
- Why unresolved: The paper focuses on evaluating PACOL on a specific set of architectures, leaving open the question of how it performs on others.
- What evidence would resolve it: Experiments testing PACOL on a variety of continual learning architectures, including recurrent neural networks and transformers, would provide insights into its generalizability.

### Open Question 2
- Question: Can PACOL attacks be adapted to target specific tasks within a continual learning system, rather than causing catastrophic forgetting across all tasks?
- Basis in paper: [inferred] The paper demonstrates PACOL's ability to cause catastrophic forgetting, but does not explore its potential for targeted attacks on specific tasks.
- Why unresolved: The paper focuses on the overall impact of PACOL on continual learning performance, leaving open the question of its potential for targeted attacks.
- What evidence would resolve it: Experiments designed to evaluate PACOL's effectiveness in targeting specific tasks within a continual learning system would provide insights into its potential for more nuanced attacks.

### Open Question 3
- Question: How do PACOL attacks perform against continual learning systems with different regularization strategies, such as elastic weight consolidation or synaptic intelligence?
- Basis in paper: [explicit] The paper evaluates PACOL against three regularization-based methods (EWC, online EWC, and SI) but does not explore its effectiveness against other strategies.
- Why unresolved: The paper focuses on a limited set of regularization strategies, leaving open the question of PACOL's effectiveness against others.
- What evidence would resolve it: Experiments testing PACOL against a wider range of regularization strategies used in continual learning would provide insights into its robustness.

## Limitations

- The gradient matching mechanism of PACOL is theoretically sound but untested for robustness under different optimization settings and optimization strategies.
- Claims about PACOL's detectability advantage over label-flipping are weakly supported, as defense evaluations focus only on statistical outlier methods.
- The paper lacks ablation studies showing minimal poisoning budgets needed for successful attacks and does not evaluate against semantic-based detection methods.

## Confidence

- **High confidence**: The overall vulnerability of continual learners to data poisoning attacks is well-supported by experiments across multiple datasets and architectures.
- **Medium confidence**: The gradient-matching mechanism of PACOL is novel and theoretically motivated, but its effectiveness depends on specific optimization behaviors that are not fully characterized.
- **Low confidence**: Claims about PACOL's detectability advantage over label-flipping are weakly supported, as the defense evaluations focus only on statistical outlier methods.

## Next Checks

1. **Gradient sensitivity test**: Vary optimizer settings (learning rate, momentum) and batch sizes to see if PACOL's gradient alignment still induces forgetting under realistic continual learning conditions.
2. **Minimal poisoning budget analysis**: Measure how many poisoned samples are required for successful forgetting under PACOL vs. label-flipping, and assess if the attack remains effective with sub-1% poisoning rates.
3. **Semantic detection robustness**: Implement a defense based on feature-space or semantic outlier detection to evaluate whether clean-label PACOL samples remain stealthy under more advanced filtering techniques.