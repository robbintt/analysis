---
ver: rpa2
title: Evaluating Neighbor Explainability for Graph Neural Networks
arxiv_id: '2311.08118'
source_url: https://arxiv.org/abs/2311.08118
tags:
- neighbors
- loyalty
- explainability
- node
- important
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to measure the importance of each
  neighbor for node classification in Graph Neural Networks (GNNs). The method adapts
  existing explainability techniques from computer vision to the GNN domain and proposes
  four new metrics to evaluate their performance.
---

# Evaluating Neighbor Explainability for Graph Neural Networks

## Quick Facts
- arXiv ID: 2311.08118
- Source URL: https://arxiv.org/abs/2311.08118
- Reference count: 40
- This paper introduces a method to measure the importance of each neighbor for node classification in Graph Neural Networks (GNNs).

## Executive Summary
This paper addresses the challenge of explaining Graph Neural Network (GNN) predictions by focusing on neighbor importance. The authors adapt explainability techniques from computer vision to the GNN domain and propose four new metrics to evaluate their performance. These metrics assess how well techniques identify important neighbors when they have high or low impact on classification, as well as on average. Experiments on three citation network datasets with GCN and GAT models reveal that self-loops are critical for accurate neighbor importance assessment, and that different explainability techniques perform better in different scenarios.

## Method Summary
The paper introduces a framework to measure neighbor importance in GNNs by adapting explainability techniques from computer vision. Four new metrics are proposed: loyalty, inverse loyalty, loyalty probabilities, and inverse loyalty probabilities. These metrics evaluate how well explainability techniques identify important neighbors by progressively deleting neighbors and measuring classification changes. The framework is tested on GCN and GAT models using citation network datasets (Cora, CiteSeer, PubMed). Gradient-based techniques (saliency maps, smoothgrad, deconvnet, guided backprop) and GNN-specific methods (GNNExplainer, PGExplainer) are compared using these metrics to determine their effectiveness in identifying neighbor importance.

## Key Results
- GNNExplainer outperforms gradient-based methods in identifying high-impact neighbors (those that change classification when removed).
- Gradient-based explainability techniques (saliency maps, smoothgrad, deconvnet, guided backprop) perform similarly in GNNs, unlike in deep computer vision models.
- Many explainability techniques fail to identify all important neighbors when GNNs lack self-loops, highlighting the importance of using self-loops for accurate neighbor importance assessment.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-loops are critical for gradient-based explainability techniques to correctly identify all important neighbors in GNNs.
- **Mechanism:** Without self-loops, gradients are not backpropagated to the node itself, preventing gradient-based methods from capturing the contribution of certain neighbors that only influence the node through multi-hop paths.
- **Core assumption:** The contribution of a neighbor to a node's classification is reflected in the gradient of the node's score with respect to the neighbor's features.
- **Evidence anchors:**
  - [abstract] "many explainability techniques failed to identify important neighbors when GNNs without self-loops are used"
  - [section 4] "many explainability techniques are not able to identify all the important neighbors when there are no self-loops"
  - [appendix B] "the gradient of node i with respect to j will be zero (only gradients with respect to second-hop neighbors will be non-zero)"
- **Break condition:** If the GNN architecture inherently propagates information in a way that gradients do capture all relevant neighbor contributions (e.g., different aggregation or update rules).

### Mechanism 2
- **Claim:** GNNExplainer is superior to gradient-based methods for identifying high-impact neighbors (those that change the classification when removed).
- **Mechanism:** GNNExplainer uses a learned mask trained via gradient descent, allowing it to capture non-gradient-based importance signals and local minima that reflect high-impact neighbor contributions.
- **Core assumption:** High-impact neighbors can be identified by changes in classification, not just gradient signals.
- **Evidence anchors:**
  - [abstract] "GNNExplainer is the best for identifying high-impact neighbors"
  - [section 4] "the GNNExplainer is better for capturing the importance of the neighbors when the impact of them is higher (classification of the node changes)"
  - [section 3] "GNNExplainer is better for capturing the importance of the neighbors when the impact of them is higher (classification of the node changes) but gradient-based methods are better in general when the classification may not change, but the confidence of the model does"
- **Break condition:** If the classification changes are not the most relevant signal for importance in the target application domain.

### Mechanism 3
- **Claim:** Gradient-based explainability techniques perform similarly in GNNs due to their shallow architecture, unlike in deep computer vision models.
- **Mechanism:** Differences in gradient-based methods accumulate through layers; GNNs typically have fewer layers, so variations between techniques like saliency maps, smoothgrad, deconvnet, and guided backprop are minimized.
- **Core assumption:** The number of layers directly influences the difference between gradient-based explainability techniques.
- **Evidence anchors:**
  - [abstract] "gradient-based techniques (saliency maps, smoothgrad, deconvnet, guided backprop) perform similarly in GNNs, unlike in computer vision"
  - [section 4] "a plausible explanation for this is that the differences in gradient-based methods accumulate throughout the layers of the models. Since computer vision models tend to be really deep, the differences are greater, while GNNs tend to be shallow and produce much smaller variations"
  - [corpus] Weak evidence - no direct corpus support for this claim
- **Break condition:** If GNNs become deeper or if the aggregation and update rules amplify gradient differences.

## Foundational Learning

- **Concept:** Graph Neural Networks (GNNs) and their message-passing mechanism
  - Why needed here: Understanding how GNNs aggregate information from neighbors is essential to grasp why neighbor importance matters and how explainability techniques work.
  - Quick check question: In a two-layer GCN, how is the hidden representation of a node computed from its neighbors?

- **Concept:** Explainability techniques (e.g., saliency maps, GNNExplainer)
  - Why needed here: The paper evaluates multiple explainability methods, so understanding their principles and differences is critical.
  - Quick check question: What is the key difference between saliency maps and GNNExplainer in terms of how they compute neighbor importance?

- **Concept:** Evaluation metrics for explainability (loyalty, inverse loyalty, etc.)
  - Why needed here: The paper introduces novel metrics to assess neighbor importance; understanding them is necessary to interpret the results.
  - Quick check question: How does the "loyalty" metric measure the performance of an explainability technique?

## Architecture Onboarding

- **Component map:** Graph datasets (Cora, CiteSeer, PubMed) -> GCN and GAT models with and without self-loops -> Saliency maps, smoothgrad, deconvnet, guided backprop, GNNExplainer, PGExplainer -> Loyalty, inverse loyalty, loyalty probabilities, inverse loyalty probabilities metrics

- **Critical path:** Load dataset -> Train GNN model -> Apply explainability technique -> Compute neighbor importance -> Evaluate using loyalty metrics -> Compare methods

- **Design tradeoffs:**
  - Self-loops vs. no self-loops: Affects gradient propagation and explainability accuracy
  - Gradient-based vs. mask-based methods: Trade-off between simplicity and capturing non-gradient importance signals
  - Deletion percentage intervals: Balance between computational cost and granularity of evaluation

- **Failure signatures:**
  - Gradient-based methods failing to identify all important neighbors when self-loops are absent
  - Explainability techniques not correlating with actual neighbor impact on classification
  - Metrics not differentiating between methods due to dataset characteristics

- **First 3 experiments:**
  1. Train a GCN with self-loops on Cora and apply saliency maps; compute loyalty and loyalty probabilities.
  2. Repeat experiment 1 without self-loops; observe changes in explainability accuracy.
  3. Apply GNNExplainer to the same model; compare loyalty results with saliency maps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different graph reduction functions (average, norm, max) affect neighbor importance ranking in GNNs?
- Basis in paper: [explicit] The paper mentions that "since in GNNs there could be many features, and in images there are only three channels, it is reasonable to use another reduction function. In this paper, the average is used because the norm can cause features with a high absolute value in the gradient to be over-represented."
- Why unresolved: The authors acknowledge that choosing an optimal reduction function requires exhaustive analysis, which is out of scope for this paper.
- What evidence would resolve it: Systematic experiments comparing different reduction functions (average, norm, max) on multiple datasets and their impact on neighbor importance ranking accuracy.

### Open Question 2
- Question: Why do gradient-based explainability methods show similar performance in GNNs but not in computer vision?
- Basis in paper: [explicit] The authors note that "all gradient-based methods (saliency map, smoothgrad, deconvnet and guided backprop) surprisingly have almost the same result, in contrast to computer vision domain, where the differences between the gradient-based methods are notorious."
- Why unresolved: The authors suggest that "a plausible explanation for this is that the differences in gradient-based methods accumulate throughout the layers of the models. Since computer vision models tend to be really deep, the differences are greater, while GNNs tend to be shallow and produce much smaller variations."
- What evidence would resolve it: Experiments comparing gradient-based methods across GNNs with varying depths, and computer vision models with varying depths, to confirm if depth is the primary factor.

### Open Question 3
- Question: How can explainability techniques be improved for GNNs without self-loops?
- Basis in paper: [explicit] The paper shows that "the methods that use the gradients, either because they are gradient-based techniques that use it for computing the importance or because they need it for training a mask as in the GNNExplainer, are not able to identify many important neighbors when there are no self-loops."
- Why unresolved: The authors identify this as a significant limitation but do not propose specific solutions.
- What evidence would resolve it: Development and evaluation of new explainability techniques that do not rely on gradients, or modifications to existing gradient-based techniques to handle GNNs without self-loops.

## Limitations
- The evaluation relies on three citation network datasets, which may not generalize to other graph types or domains.
- The study focuses primarily on GCN and GAT architectures, potentially limiting applicability to other GNN variants.
- The proposed metrics assume binary classification changes as the primary signal for importance, which may not capture nuanced cases where confidence shifts matter more than class changes.

## Confidence

- **High confidence:** The finding that self-loops are critical for gradient-based explainability techniques to correctly identify all important neighbors.
- **Medium confidence:** The superiority of GNNExplainer for identifying high-impact neighbors.
- **Medium confidence:** The assertion that gradient-based methods perform similarly in GNNs due to their shallow architecture.

## Next Checks

1. **Architecture generalization test:** Evaluate the proposed metrics and findings on deeper GNN architectures (e.g., GraphSAGE with multiple layers) to verify if the similarity between gradient-based methods persists.

2. **Dataset diversity validation:** Apply the evaluation framework to non-citation graph datasets (e.g., social networks, molecular graphs) to assess the robustness of the conclusions across different graph types.

3. **Alternative importance signals:** Design experiments to test whether confidence-based importance metrics (rather than classification-change-based) would yield different rankings of explainability technique performance, particularly for cases where the model's confidence shifts without changing predictions.