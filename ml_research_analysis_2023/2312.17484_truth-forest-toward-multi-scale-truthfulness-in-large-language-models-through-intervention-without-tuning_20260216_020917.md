---
ver: rpa2
title: 'Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through
  Intervention without Tuning'
arxiv_id: '2312.17484'
source_url: https://arxiv.org/abs/2312.17484
tags:
- trfr
- llama
- question
- chat
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Truth Forest is a method that improves the truthfulness of large
  language models (LLMs) by uncovering hidden truth representations using multi-dimensional
  orthogonal probes. It creates multiple orthogonal bases for modeling truth by incorporating
  orthogonal constraints into the probes.
---

# Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning

## Quick Facts
- arXiv ID: 2312.17484
- Source URL: https://arxiv.org/abs/2312.17484
- Reference count: 4
- Improved truthfulness of Llama-2-7B from 40.8% to 74.5% on TruthfulQA

## Executive Summary
Truth Forest is a method that improves the truthfulness of large language models (LLMs) by uncovering hidden truth representations using multi-dimensional orthogonal probes. The approach creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes, addressing the multi-dimensional nature of truthfulness. It introduces Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs. By employing this approach, the authors improved the truthfulness of Llama-2-7B from 40.8% to 74.5% on TruthfulQA.

## Method Summary
Truth Forest uses multiple orthogonal probes to capture different aspects of truth representations in LLMs, with orthogonality constraints forcing each probe to capture a distinct dimension of truthfulness. These probes are weighted and combined to form a composite truth direction. Random Peek addresses the Generating-Discerning Gap by sampling truth features from various positions within the answer sequence, bridging the gap between discerning and generating truth. The method employs weighted intervention using multiple orthogonal directions to improve truthfulness more effectively than single-direction methods, with the weighted directions used to intervene in the MHA layer by shifting activations toward the composite truth direction.

## Key Results
- Improved truthfulness of Llama-2-7B from 40.8% to 74.5% on TruthfulQA
- Demonstrated effectiveness of multi-dimensional orthogonal probes in capturing truth features
- Showed Random Peek technique reduces the Generating-Discerning Gap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truth Forest uses multiple orthogonal probes to capture different aspects of truth representations in LLMs.
- Mechanism: Multiple probes are trained with orthogonality constraints, forcing each to capture a distinct dimension of truthfulness. These probes are then weighted and combined to form a composite truth direction.
- Core assumption: Truthfulness in LLMs is multi-dimensional and can be decomposed into orthogonal components.
- Evidence anchors:
  - [abstract] "it creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes"
  - [section 3.3] "To avert model collapse, we enforce soft orthogonality constraints, denoted as Lorth"
  - [corpus] Weak - no direct corpus evidence provided
- Break condition: If truth features are not truly orthogonal or if orthogonality constraints cause probe collapse, the method would fail to capture complementary truth dimensions.

### Mechanism 2
- Claim: Random Peek addresses the Generating-Discerning Gap by sampling truth features from various positions within the sequence.
- Mechanism: Instead of only using the last token for feature extraction, Random Peek randomly samples positions throughout the answer sequence to capture truth features, bridging the gap between discerning and generating truth.
- Core assumption: The G-D Gap exists because feature extraction at a single fixed position (last token) doesn't align with how the model generates truthful responses.
- Evidence anchors:
  - [abstract] "Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features"
  - [section 3.2] "This approach is grounded in the assumption that features sampled from different points in the answer sequence can be more informative"
  - [corpus] Weak - no direct corpus evidence provided
- Break condition: If Random Peek sampling doesn't capture relevant features or introduces noise that outweighs the benefits.

### Mechanism 3
- Claim: Weighted intervention using multiple orthogonal directions improves truthfulness more effectively than single-direction methods.
- Mechanism: After training orthogonal probes, they are weighted (with exponential decay) to balance their contributions, then used to intervene in the MHA layer by shifting activations toward the composite truth direction.
- Core assumption: Different orthogonal directions capture different aspects of truth, and their weighted combination is more effective than any single direction.
- Evidence anchors:
  - [section 3.4] "We compute the final axis Θl,h using exponential decay weighting W"
  - [section 3.3] "To intervene in the MHA layer, we modify it as a constant"
  - [corpus] Weak - no direct corpus evidence provided
- Break condition: If the weighting scheme doesn't properly balance the orthogonal directions or if the intervention causes harmful side effects.

## Foundational Learning

- Concept: Orthogonal probe training with soft constraints
  - Why needed here: To capture multi-dimensional truth features without probe collapse
  - Quick check question: How does the orthogonal loss Lorth prevent probe collapse while maintaining effectiveness?

- Concept: Random sampling for feature extraction
  - Why needed here: To address the Generating-Discerning Gap by capturing features at multiple sequence positions
  - Quick check question: What is the key assumption behind using Random Peek instead of just the last token?

- Concept: Inference-time intervention in MHA layer
  - Why needed here: To shift model activations toward truthfulness without retraining the entire model
  - Quick check question: How does the intervention modify the MHA layer output?

## Architecture Onboarding

- Component map: Random Peek → Orthogonal Probe Training → Exponential Weighting → MHA Layer Intervention

- Critical path: Random Peek → Orthogonal Probe Training → Weighting → Intervention

- Design tradeoffs:
  - More orthogonal directions vs. computational cost
  - Stronger orthogonality constraints vs. probe effectiveness
  - Intervention strength vs. maintaining informativeness

- Failure signatures:
  - Probes converging to same direction despite orthogonality constraints
  - Intervention causing model to produce uninformative answers
  - Random Peek sampling not capturing meaningful features

- First 3 experiments:
  1. Train orthogonal probes with varying λ values to find optimal orthogonality constraint
  2. Compare Random Peek vs. last token feature extraction on probe accuracy
  3. Test different numbers of orthogonal directions on truthfulness improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of orthogonal directions for capturing multi-scale truthfulness in different LLM architectures?
- Basis in paper: [explicit] The paper states "Our experiments reveal that the optimal number of directions depends on the specific intervention setting" but does not provide a definitive answer.
- Why unresolved: The relationship between the number of orthogonal directions and intervention effectiveness appears to be context-dependent, varying based on data volume and intervention strength.
- What evidence would resolve it: Systematic experiments varying the number of orthogonal directions across different LLM sizes, training datasets, and intervention strengths, with quantitative analysis of intervention effectiveness at each scale.

### Open Question 2
- Question: How does the Random Peek technique affect the generalization of truthfulness interventions to out-of-distribution data?
- Basis in paper: [explicit] The paper mentions that "TrFr slightly improves over the baseline" on the Natural Questions dataset, but the analysis is limited.
- Why unresolved: The paper only briefly explores generalization to one out-of-distribution dataset, and the impact of Random Peek specifically on this generalization is unclear.
- What evidence would resolve it: Comprehensive evaluation of intervention generalization across multiple out-of-distribution datasets, comparing Random Peek with fixed-position interventions, and analyzing the relationship between intervention diversity and generalization performance.

### Open Question 3
- Question: What is the theoretical relationship between the Generating and Discerning Gap (G-D Gap) and the effectiveness of multi-dimensional orthogonal probes?
- Basis in paper: [explicit] The paper states "We provide a theory perspective of TrFr in Appendix A" but this theoretical analysis is not included in the main paper text.
- Why unresolved: The paper mentions the G-D Gap as a motivation for the work but does not provide a clear theoretical framework connecting this gap to the probe methodology.
- What evidence would resolve it: A formal theoretical analysis deriving how orthogonal probes in different positions can bridge the G-D Gap, potentially through analysis of the model's internal representations and their relationship to generation versus evaluation modes.

## Limitations

- Probe orthogonality effectiveness: Limited empirical validation of whether orthogonal probes truly capture complementary truth dimensions versus simply forcing probe diversity
- Random Peek mechanism validation: Insufficient evidence that Random Peek effectively addresses the Generating-Discerning Gap, with the key assumptions not rigorously tested
- Generalization concerns: Method tested primarily on Llama-2-7B and TruthfulQA benchmark, with uncertainty about effectiveness on different model architectures and datasets

## Confidence

**High confidence:** The basic experimental setup and methodology are clearly described, including the use of orthogonal probes with constraints, Random Peek for feature extraction, and MHA layer intervention. The overall framework is coherent and technically sound.

**Medium confidence:** The effectiveness of orthogonal probes in capturing multi-dimensional truth features and the impact of Random Peek on addressing the Generating-Discerning Gap. These claims are supported by some evidence but lack comprehensive validation and detailed analysis.

**Low confidence:** The specific mechanisms by which the orthogonality constraints prevent probe collapse while maintaining effectiveness, and how the intervention balances truthfulness improvement with preserving model informativeness. These critical aspects are not thoroughly explored or empirically validated.

## Next Checks

1. **Probe orthogonality validation:** Conduct experiments to empirically verify that orthogonal probes capture truly distinct and complementary truth dimensions. This could involve analyzing probe activation patterns, measuring feature similarity between orthogonal directions, and testing whether removing one probe direction degrades performance in ways that correlate with the claimed orthogonality benefits.

2. **Random Peek ablation study:** Perform controlled experiments comparing Random Peek against baseline feature extraction methods (last token only) across different sequence lengths, question types, and sampling strategies. Measure not just truthfulness improvements but also any degradation in other model capabilities to assess the full impact of the approach.

3. **Intervention stability analysis:** Test the intervention across multiple model layers and head configurations with varying intervention strengths (α). Measure truthfulness improvements while monitoring for side effects such as reduced informativeness, changed response styles, or degradation on other benchmarks to understand the intervention's stability and generalizability.