---
ver: rpa2
title: Reranking Passages with Coarse-to-Fine Neural Retriever Enhanced by List-Context
  Information
arxiv_id: '2308.12022'
source_url: https://arxiv.org/abs/2308.12022
tags:
- passage
- information
- context
- passages
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a list-context attention mechanism for passage
  reranking. The main idea is to incorporate context information from other candidates
  to augment passage representation, addressing the limitation of existing neural
  architectures that match questions to each passage individually without considering
  contextual information from other passages.
---

# Reranking Passages with Coarse-to-Fine Neural Retriever Enhanced by List-Context Information

## Quick Facts
- **arXiv ID:** 2308.12022
- **Source URL:** https://arxiv.org/abs/2308.12022
- **Reference count:** 9
- **Primary result:** C2FRetriever improves 6.17% MAP and 6.82% MRR on BERT base for WIKIQA; achieves 0.377 MRR@10 on MARCO development set (3% improvement from TPU BERT base).

## Executive Summary
This paper presents a list-context attention mechanism for passage reranking that incorporates context information from other candidate passages to augment passage representation. The proposed coarse-to-fine (C2F) neural retriever divides the list-context modeling process into two sub-processes with a cache policy learning algorithm, addressing the out-of-memory limitation of traditional passage attention mechanisms. Experiments on WIKIQA and MARCO datasets demonstrate significant improvements over baseline BERT models, with the approach achieving state-of-the-art performance while maintaining computational efficiency through bucket-based filtering.

## Method Summary
The C2FRetriever architecture consists of two stages: a coarse ranker that filters irrelevant passages into a top-k bucket using BERT-based scoring, and a fine ranker that performs list-context modeling on this reduced set. The list-context attention mechanism includes both static attention (aggregating all passage representations into a shared context vector) and adaptive attention (computing pairwise correlations between passages weighted by semantic similarity). The model is trained end-to-end with a combined loss function, allowing joint optimization of both rankers. A bucket policy learning algorithm dynamically maintains the top-k scoring question-answer pairs for efficient processing of large candidate sets.

## Key Results
- C2FRetriever improves 6.17% MAP and 6.82% MRR on the BERT base model for WIKIQA dataset
- Achieves 0.377 MRR@10 on MARCO development set, improving 3% from TPU BERT base
- Demonstrates effective handling of large candidate sets through bucket-based filtering while maintaining ranking quality
- Shows that list-context information significantly enhances passage representations compared to individual question-passage matching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: List-context attention augments passage representation by incorporating comparative and reference information from other candidate passages.
- Mechanism: Uses static attention to aggregate all passage representations into a shared context vector, and adaptive attention to compute pairwise correlations weighted by semantic similarity to the question and context.
- Core assumption: Passage relevance depends on both question match and relationships to other candidates.
- Break condition: If passages are too diverse or irrelevant to each other, list-context signal may introduce noise.

### Mechanism 2
- Claim: Coarse-to-fine architecture efficiently handles large candidate sets through early filtering.
- Mechanism: Coarse ranker filters irrelevant passages into top-k bucket using BERT scoring, then fine ranker performs list-context modeling only on reduced set.
- Core assumption: Most candidates are irrelevant, so early filtering reduces computational cost without losing relevant passages.
- Break condition: If bucket size is too small, relevant passages may be filtered out in coarse stage.

### Mechanism 3
- Claim: Joint training of coarse and fine rankers enables better collaboration and parameter optimization.
- Mechanism: Both rankers trained end-to-end using combined loss function, allowing gradients to influence each other's behavior.
- Core assumption: Joint optimization leads to better overall performance than pipeline training.
- Break condition: If rankers have conflicting objectives or one dominates loss, joint training may not improve performance.

## Foundational Learning

- **Concept: BERT-based sentence pair representation**
  - Why needed here: Foundation for all subsequent attention and scoring operations
  - Quick check question: What is the input format for BERT in this model? (Answer: "[CLS]; Q; [SEP]; Oi; [SEP]")

- **Concept: Attention mechanisms for context aggregation**
  - Why needed here: Static and adaptive attention incorporate list-context information central to model's improvement capability
  - Quick check question: How does adaptive attention differ from static attention? (Answer: Adaptive computes pairwise correlations weighted by semantic similarity, while static aggregates all into shared context vector)

- **Concept: Bucket-based candidate filtering**
  - Why needed here: Coarse ranker maintains top-k candidates to enable efficient processing of large sets without memory issues
  - Quick check question: What is the purpose of the bucket in the coarse ranker? (Answer: To dynamically maintain top-k scoring question-answer pairs for fine ranker processing)

## Architecture Onboarding

- **Component map:** Input layer → BERT encoding → Coarse scoring → Bucket filtering → Fine ranker (list-context attention) → Final ranking
- **Critical path:** Question → BERT encoding → Coarse scoring → Bucket filtering → Fine ranker (list-context attention) → Final ranking
- **Design tradeoffs:**
  - Bucket size vs. computational efficiency: Larger buckets capture more context but increase computation
  - Joint training vs. separate training: Joint enables better collaboration but may be harder to optimize
  - Static vs. adaptive attention: Static is simpler but adaptive better captures pairwise relationships
- **Failure signatures:**
  - Poor performance on diverse passages: List-context signal may introduce noise
  - Memory issues with large candidate sets: Bucket size may need adjustment
  - Training instability: Joint training may require careful hyperparameter tuning
- **First 3 experiments:**
  1. Test varying bucket sizes (5, 10, 15) on WIKIQA subset to find optimal context-efficiency balance
  2. Compare joint vs. separate training of coarse and fine rankers on validation set
  3. Evaluate static vs. adaptive attention impact by training models with only one mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does C2FRetriever performance scale with increasing numbers of passages, and what is the impact on memory usage and computational efficiency?
- Basis in paper: [explicit] Paper discusses challenge of handling large candidate passages and mentions bucket policy learning algorithm
- Why unresolved: Paper lacks detailed empirical results on performance and efficiency when scaling to very large numbers of passages
- What evidence would resolve it: Experiments with varying passage numbers measuring performance, memory usage, and computational time

### Open Question 2
- Question: How does C2FRetriever compare to other multi-stage retrieval architectures in terms of effectiveness and efficiency?
- Basis in paper: [explicit] Paper mentions C2FRetriever integrates coarse and fine rankers into joint optimization, different from most multi-stage architectures
- Why unresolved: Paper doesn't provide direct comparison between C2FRetriever and other multi-stage retrieval architectures
- What evidence would resolve it: Experiments comparing C2FRetriever with other multi-stage architectures on benchmark datasets

### Open Question 3
- Question: How does choice of bucket size affect C2FRetriever performance, and what is optimal bucket size for different datasets?
- Basis in paper: [explicit] Paper mentions bucket size set to 15 and provides some analysis on bucket size influence
- Why unresolved: Paper doesn't provide comprehensive analysis of bucket size effects across different datasets
- What evidence would resolve it: Experiments with different bucket sizes on various datasets analyzing performance impact

## Limitations
- Implementation details of bucket policy learning algorithm (ranking function K(·) and selection function C(·)) are not fully specified
- Computational efficiency claims lack thorough validation with runtime comparisons against baseline models
- Corpus evidence supporting list-context mechanism is weak, as neighboring papers focus on reranking but don't directly discuss list-context mechanisms

## Confidence

- **High confidence**: Basic architecture using BERT for QA pair representations and coarse-to-fine filtering approach is well-established and clearly described
- **Medium confidence**: Effectiveness of list-context attention in improving ranking metrics is demonstrated on benchmark datasets, but mechanism's contribution relative to other factors is not fully isolated
- **Low confidence**: Computational efficiency claims and specific implementation details of bucket policy learning algorithm lack sufficient validation and specification

## Next Checks

1. Implement coarse ranker with varying bucket sizes (5, 10, 15, 20) on small WIKIQA subset to empirically determine optimal balance between context preservation and computational efficiency
2. Conduct ablation studies comparing joint training vs. separate training of coarse and fine rankers, and static vs. adaptive attention mechanisms, to isolate their individual contributions to performance gains
3. Perform runtime measurements comparing C2FRetriever with standard BERT reranker implementations on MARCO to validate claimed computational efficiency improvements