---
ver: rpa2
title: 'Turn Fake into Real: Adversarial Head Turn Attacks Against Deepfake Detection'
arxiv_id: '2309.01104'
source_url: https://arxiv.org/abs/2309.01104
tags: []
core_contribution: This paper proposes the first 3D adversarial face view attack (AdvHeat)
  against deepfake detectors. The attack synthesizes adversarial views of a single-view
  fake face image using neural radiance fields and a style-based GAN.
---

# Turn Fake into Real: Adversarial Head Turn Attacks Against Deepfake Detection

## Quick Facts
- arXiv ID: 2309.01104
- Source URL: https://arxiv.org/abs/2309.01104
- Reference count: 40
- Primary result: 3D adversarial head turn attacks achieve up to 96.8% success rate against deepfake detectors

## Executive Summary
This paper introduces AdvHeat, the first 3D adversarial face view attack that bypasses deepfake detectors by synthesizing novel head turn views of fake face images. Unlike conventional 2D perturbations, AdvHeat leverages neural radiance fields and style-based GANs to generate realistic multi-view images that exploit detectors' vulnerability to viewpoint changes. Experiments demonstrate high attack success rates across 10 different deepfake detectors and three quality levels, with strong cross-model transferability and robustness to common defenses.

## Method Summary
AdvHeat attacks deepfake detectors by synthesizing adversarial face views using neural radiance fields (NeRF) and StyleGAN. The process involves three key steps: (1) preprocessing the input fake image with GFP-GAN-v3 super-resolution, (2) projecting the image into StyleGAN latent space using a pre-trained PTI model, and (3) generating 360 multi-view images using Eg3D with varying yaw and pitch angles. The attack then searches for adversarial views using either random search or gradient estimation methods, exploiting detectors' sensitivity to viewpoint changes.

## Key Results
- AdvHeat achieves attack success rates up to 96.8% against 10 different deepfake detectors
- Random search-based AdvHeat-rand is surprisingly effective, achieving 92.5% success rate
- AdvHeat shows strong cross-model transferability, outperforming conventional attacks
- Generated adversarial images maintain natural appearance and fool high-quality detectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthesizing novel face views from a single-view fake image can bypass deepfake detectors by exploiting detectors' reliance on frontal-view biases
- Mechanism: The attack uses a neural radiance field (NeRF) and StyleGAN to project the input image into a latent space, then renders it from multiple viewpoints by varying yaw and pitch angles. Detectors trained on frontal views fail to recognize the same identity when the view changes, treating it as real
- Core assumption: Deepfake detectors are not trained on multi-view data and therefore rely on viewpoint-specific cues for detection
- Evidence anchors:
  - [abstract] "all existing attacks are limited to 2D image perturbations, which are hard to translate into real-world facial changes"
  - [section 2] "Current research on 3D view synthesis mainly focuses on directly generating multiple views from the latent space"
  - [corpus] weak - no direct citation; inferred from the focus on viewpoint modeling
- Break condition: If detectors are trained on multi-view datasets or explicitly model viewpoint invariance, the attack loses effectiveness

### Mechanism 2
- Claim: Random search over a low-dimensional attack space (view indices) is sufficient to find adversarial views due to detector sensitivity to viewpoint changes
- Mechanism: The attack space is reduced to a single index over 360 synthesized views. Even naive random search achieves high success rates because even small viewpoint shifts can cause misclassification
- Core assumption: The detection boundary is easily crossed by minimal semantic transformations like head turning
- Evidence anchors:
  - [section 3.3] "Since our attack space is just one dimensional, we do not need a sophisticated optimization algorithm"
  - [section 4.2] "even a naive random search-based attack can be effective"
  - [corpus] weak - no direct citation; inferred from the success of random search
- Break condition: If detectors are trained with viewpoint augmentation or are inherently viewpoint-robust, the attack success rate drops

### Mechanism 3
- Claim: Super-resolution preprocessing enhances the quality of synthesized views, making them harder for detectors to flag as fake
- Mechanism: GFP-GAN is applied to the original fake image to produce a 2× resolution version before 3D synthesis, improving the visual quality and reducing artifacts that detectors might exploit
- Core assumption: Detectors trained on low-quality data are sensitive to synthetic artifacts introduced during view synthesis
- Evidence anchors:
  - [section 3.1] "to enhance the quality of the original (fake) images, we adopt GFP-GAN-v3"
  - [section 4.2] "when the image quality is higher, there is more detailed information for the detector to make a more accurate prediction"
  - [corpus] weak - no direct citation; inferred from the role of super-resolution
- Break condition: If detectors are trained on high-quality data or explicitly handle super-resolution artifacts, the benefit of preprocessing diminishes

## Foundational Learning

- Concept: 3D face modeling and neural radiance fields
  - Why needed here: The attack depends on synthesizing realistic novel views from a single image, which requires understanding NeRFs and their integration with GANs
  - Quick check question: What is the key difference between NeRF-based view synthesis and traditional 2D image perturbation?

- Concept: Adversarial attack in low-dimensional semantic space
  - Why needed here: The attack operates not in pixel space but in a semantic space defined by viewpoint parameters, which is a departure from conventional Lp-bounded attacks
  - Quick check question: How does optimizing over yaw/pitch angles differ from optimizing over pixel values in terms of gradient availability and search strategy?

- Concept: Black-box attack strategies (random search vs. gradient estimation)
  - Why needed here: The attack is evaluated under black-box conditions, requiring understanding of zeroth-order optimization and query-efficient methods
  - Quick check question: Why might random search be surprisingly effective in a one-dimensional attack space?

## Architecture Onboarding

- Component map: Original image → GFP-GAN super-resolution → PTI projection → Eg3D synthesis (360 views) → adversarial view search → adversarial image
- Critical path: Original image → GFP-GAN super-resolution → PTI projection → Eg3D synthesis (360 views) → adversarial view search → adversarial image
- Design tradeoffs:
  - 360 views provide good coverage but increase computation; fewer views reduce runtime but may miss adversarial views
  - Random search is simple but query-inefficient; gradient estimation improves efficiency but requires score access
  - Super-resolution improves realism but may introduce its own artifacts
- Failure signatures:
  - Attack fails if all synthesized views are still classified as fake (detector is viewpoint-robust)
  - Attack degrades if the latent space projection fails to preserve identity
  - Attack becomes ineffective if detectors are trained on multi-view data
- First 3 experiments:
  1. Verify that the Eg3D model can synthesize consistent views for a known fake image and that these views vary smoothly with yaw/pitch
  2. Test random search on a simple detector (e.g., ResNet50) to confirm that some views flip the label from fake to real
  3. Compare attack success with and without super-resolution preprocessing to quantify its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adversarial robustness of deepfake detectors relate to their accuracy across different levels of image quality?
- Basis in paper: [explicit] The paper observes that detectors with higher accuracy (e.g., F3Net, M2TR, MAT) are more vulnerable to AdvHeat attacks, especially on high-quality images. This suggests a potential trade-off between accuracy and adversarial robustness
- Why unresolved: The paper does not provide a definitive explanation for this observation or conduct a systematic study to quantify the relationship between accuracy and robustness across various detector architectures and image qualities
- What evidence would resolve it: A comprehensive study analyzing the accuracy and robustness of multiple deepfake detectors across a range of image qualities, controlling for factors like model architecture and training data

### Open Question 2
- Question: What are the optimal turning angles (ϕ and θ) for synthesizing adversarial views that maximize attack success while maintaining natural-looking images?
- Basis in paper: [explicit] The paper explores the impact of different turning angles on attack success but finds that very large angles can lead to low synthesis quality and reduced effectiveness, especially for high-quality detectors
- Why unresolved: The paper does not provide a definitive answer on the optimal angle ranges for different detector architectures or image qualities. It also does not explore the trade-off between attack success and visual fidelity in detail
- What evidence would resolve it: A systematic study analyzing the relationship between turning angles, attack success rates, and visual quality across various detector architectures and image qualities

### Open Question 3
- Question: How effective are current defenses against AdvHeat attacks, and what new defense strategies could be developed to mitigate this threat?
- Basis in paper: [explicit] The paper evaluates the robustness of AdvHeat against common defenses like JPEG compression, R&P, BDR, and NRP but finds that AdvHeat still achieves high success rates
- Why unresolved: The paper does not explore more advanced or specialized defenses against 3D adversarial view attacks. It also does not investigate the potential for adaptive attacks that could bypass existing defenses
- What evidence would resolve it: A comprehensive study evaluating the effectiveness of various defense strategies against AdvHeat, including both traditional and novel approaches specifically designed for 3D adversarial view attacks

## Limitations
- Attack effectiveness relies on untested assumptions about detector viewpoint generalization
- Evaluation limited to specific detector architectures without exploring viewpoint-augmented training
- Quality improvements from preprocessing not independently quantified
- Computational cost of 360-view synthesis may limit practical deployment

## Confidence

- **High confidence**: The core attack mechanism (view synthesis + adversarial search) is technically sound and reproducible given the specified components. The experimental setup and evaluation metrics are clearly defined
- **Medium confidence**: Claims about cross-model transferability and robustness to defenses are plausible but may be dataset-specific. The superiority over conventional attacks is demonstrated but not comprehensively compared across all relevant baselines
- **Low confidence**: The claim that AdvHeat generates "natural-looking" adversarial images is qualitative and lacks rigorous perceptual evaluation. The assumption that detectors universally fail on multi-view data is not empirically validated across diverse detector architectures

## Next Checks

1. **Viewpoint robustness test**: Train a ResNet50 detector on multi-view face data (including yaw/pitch variations) and evaluate AdvHeat's success rate. This directly tests the core assumption about detector viewpoint bias
2. **Quality contribution ablation**: Compare attack success rates with and without GFP-GAN preprocessing across different input qualities (LQ, HQ, Raw). Quantify the exact contribution of super-resolution to attack effectiveness
3. **Real-world transferability**: Apply AdvHeat to dynamic face sequences from Celeb-DF or DFDC datasets and measure detection failure rates. This validates the attack's practical relevance beyond static images