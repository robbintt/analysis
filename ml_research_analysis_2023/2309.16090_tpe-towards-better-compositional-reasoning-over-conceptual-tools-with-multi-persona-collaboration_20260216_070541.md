---
ver: rpa2
title: 'TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona
  Collaboration'
arxiv_id: '2309.16090'
source_url: https://arxiv.org/abs/2309.16090
tags:
- student
- dialogue
- teacher
- knowledge
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TPE (Think-Plan-Execute), a multi-persona
  collaboration framework designed to enhance large language models'' (LLMs) compositional
  reasoning and planning capabilities when using conceptual tools in dialogue systems.
  TPE decomposes the response generation process into three distinct roles: Thinker,
  Planner, and Executor.'
---

# TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration

## Quick Facts
- arXiv ID: 2309.16090
- Source URL: https://arxiv.org/abs/2309.16090
- Reference count: 31
- Key outcome: TPE framework achieves superior performance on multi-source and multi-strategy dialogue tasks by decomposing reasoning into Thinker, Planner, and Executor roles

## Executive Summary
TPE introduces a multi-persona collaboration framework for enhancing large language models' compositional reasoning over conceptual tools in dialogue systems. By decomposing the response generation process into three specialized roles - Thinker, Planner, and Executor - TPE addresses limitations of observation-dependent planning approaches that suffer from token redundancy and poor explainability. The framework demonstrates significant improvements across three dialogue response generation tasks: multi-source (FoCus), multi-strategy interactions (CIMA), and psychology question answering (PsyQA), showing its potential for handling complex real-world dialogue interactions requiring conceptual tool learning.

## Method Summary
TPE implements a three-role architecture where the Thinker analyzes dialogue context to infer user status and formulate global guidelines, the Planner generates executable plans to call conceptual tools (sources or strategies), and the Executor compiles intermediate results into coherent responses. The framework uses in-context learning with demonstrations to teach LLMs the desired thinking patterns and planning sequences. For multi-source tasks, the Planner generates tool calls for retrievers, while for multi-strategy tasks, it generates sequential strategy executions. The internal status generated by Thinker enriches retrieval queries, improving multi-source dialogue performance.

## Key Results
- TPE achieves significant performance gains on FoCus multi-source dialogue task with BLEU scores improving by approximately 3% when internal status is used
- On CIMA dataset, TPE demonstrates superior strategy planning with the correction strategy being most frequently used (61.1%), reflecting its ability to handle student misconceptions
- TPE outperforms observation-dependent approaches like ReAct by reducing token redundancy through structured top-down planning rather than interleaved reasoning

## Why This Works (Mechanism)

### Mechanism 1
Decoupling response generation into Thinker, Planner, and Executor roles reduces token redundancy and improves explainability. By separating internal status reasoning, tool planning, and response execution, each component can focus on a single responsibility, preventing interleaved reasoning and execution steps that cause redundancy in observation-dependent approaches.

### Mechanism 2
Using internal status inferred by Thinker to enrich retrieval queries improves multi-source dialogue performance. Thinker analyzes dialogue context to generate user status (e.g., "user does not clearly remember location") which is incorporated into retrieval queries, helping retrievers better match relevant documents/personas through semantic enrichment.

### Mechanism 3
TPE's top-down planning approach handles complex strategy transitions better than observation-dependent methods. Planner generates complete strategy sequences upfront based on global context rather than reacting to each observation, enabling coherent multi-step reasoning across strategies.

## Foundational Learning

- **Multi-persona collaboration**: Why needed - Enables decomposition of complex reasoning tasks into specialized roles while maintaining coherent overall behavior. Quick check - Can you explain how three different personas with the same LLM can work together without interfering?
- **Conceptual vs functional tools**: Why needed - Distinguishes between API/calculator tools (functional) and cognitive concepts like strategies or knowledge sources (conceptual), which require different planning approaches. Quick check - What's the key difference between planning for a calculator tool versus planning for a psychological strategy?
- **In-context learning with demonstrations**: Why needed - TPE relies on few-shot examples to teach LLMs the desired thinking patterns, planning sequences, and execution styles. Quick check - How would you structure demonstrations to teach an LLM to use Hint → Question → Hint strategy sequence?

## Architecture Onboarding

- **Component map**: Thinker (context → status + blueprint) → Planner (status + context → tool calls/strategies) → Executor (plan + context → response)
- **Critical path**: Thinker → Planner → Executor (with potential loops if Executor needs to refine based on tool outputs)
- **Design tradeoffs**: Decoupling vs. efficiency (more components = better explainability but potentially higher token cost), global planning vs. flexibility (complete upfront planning enables coherence but may miss local adjustments), persona consistency (same LLM persona across roles requires careful prompt engineering)
- **Failure signatures**: Poor user status generation → downstream planning failures, inconsistent persona switching → context loss between roles, strategy repetition in Planner → inability to handle complex transitions, retrieval failures → poor query formulation from Thinker
- **First 3 experiments**: 1) Implement Thinker alone to test user status generation quality, 2) Implement Planner with fixed demonstrations to test strategy sequence generation, 3) Implement full TPE pipeline with single-source scenario to validate end-to-end flow

## Open Questions the Paper Calls Out

- **Question 1**: How does the performance of TPE scale when using larger language models (e.g., GPT-4) for the multi-source dialogue tasks compared to smaller models like ChatGPT?
- **Question 2**: What is the impact of using different types of retrievers (BM25 vs. DPR with and without fine-tuning) on the overall performance of TPE in multi-source dialogue generation?
- **Question 3**: How does the number of demonstrations used during in-context learning affect the strategy planning and execution in TPE for multi-strategy dialogue tasks?

## Limitations
- Performance depends heavily on underlying LLM's ability to perform consistent role-based reasoning, which may not generalize across different model families
- The framework's effectiveness for handling very large tool sets or highly diverse tool types remains unexplored
- The evaluation scope is limited to specific types of dialogue systems, and generalizability to other multi-tool reasoning scenarios is uncertain

## Confidence
- **High confidence**: The architectural decomposition into Thinker-Planner-Executor roles provides clear benefits for explainability and reduces token redundancy in multi-step reasoning tasks
- **Medium confidence**: The superiority of TPE over baseline methods on the three evaluation datasets is demonstrated, but generalizability to other scenarios remains uncertain
- **Low confidence**: The paper's claims about handling "complex tool learning beyond functional tools" lack exploration of what threshold of complexity can actually be managed

## Next Checks
1. Test TPE framework with smaller LLMs (7B-13B parameters) to assess whether multi-persona collaboration works independently of model scale
2. Conduct controlled experiments removing Thinker-generated internal status from retrieval queries to quantify exact contribution of semantic enrichment
3. Systematically evaluate Planner's ability to generate longer, more complex strategy sequences to identify breaking points in global planning approach