---
ver: rpa2
title: 'From Beginner to Expert: Modeling Medical Knowledge into General LLMs'
arxiv_id: '2312.01040'
source_url: https://arxiv.org/abs/2312.01040
tags:
- medical
- data
- llms
- knowledge
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates how to adapt a general large language model
  (LLM) into a medical expert. A 3-stage optimization framework is proposed: (1) general
  medical knowledge injection via continual pre-training, (2) medical domain instruction
  tuning, and (3) specific medical task adaptation.'
---

# From Beginner to Expert: Modeling Medical Knowledge into General LLMs

## Quick Facts
- arXiv ID: 2312.01040
- Source URL: https://arxiv.org/abs/2312.01040
- Authors: 
- Reference count: 10
- Key outcome: AntGLM-Med-10B achieves 80.6% accuracy on PubMedQA, outperforming many larger LLMs (>40B parameters) through 3-stage optimization

## Executive Summary
This paper presents a three-stage optimization framework to adapt a general large language model into a medical expert. The approach involves medical knowledge injection through continual pre-training, medical domain instruction tuning, and specific medical task adaptation. The authors introduce a novel Verification-of-Choice prompting approach that significantly enhances reasoning for multi-choice questions. The resulting AntGLM-Med-10B model demonstrates strong performance on medical benchmarks while maintaining a relatively compact 10B parameter size.

## Method Summary
The authors propose a 3-stage optimization framework for adapting a general LLM to the medical domain. First, they perform continual pre-training on large-scale medical datasets to inject domain-specific knowledge. Second, they apply medical domain instruction tuning to teach the model to understand and execute medical-specific tasks. Third, they adapt the model to specific medical tasks like multi-choice questions. The approach uses large-scale datasets covering question-answering, medical reasoning, and conversations, with a novel Verification-of-Choice method to enhance reasoning capabilities.

## Key Results
- AntGLM-Med-10B achieves 80.6% accuracy on PubMedQA benchmark
- The model outperforms many larger LLMs (>40B parameters) on medical tasks
- The 3-stage optimization framework effectively adapts a general LLM to the medical domain
- Verification-of-Choice approach significantly improves reasoning for multi-choice questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medical knowledge injection through continual pre-training improves LLM performance on medical tasks by providing domain-specific background knowledge.
- Mechanism: Pre-training on medical domain data allows the model to learn specialized vocabulary, concepts, and relationships not present in general corpora, creating a foundation of medical knowledge for fine-tuning.
- Core assumption: Medical domain data contains sufficient coverage of concepts and relationships needed for medical reasoning tasks.
- Evidence anchors: [abstract] "general medical knowledge injection via continual pre-training", [section] "continual pre-training over the large-scale medical data"
- Break condition: If medical domain data is too narrow or biased, the model may develop incorrect or incomplete medical knowledge.

### Mechanism 2
- Claim: Medical domain instruction tuning enables the model to understand and execute medical-specific tasks by learning task-related patterns.
- Mechanism: Instruction tuning exposes the model to diverse medical task types with corresponding instructions, allowing it to learn patterns and formats specific to each task type.
- Core assumption: Instruction dataset covers a representative sample of medical tasks the model will encounter.
- Evidence anchors: [abstract] "medical domain instruction tuning", [section] "instruction fine-tuning for the base LLM"
- Break condition: If instruction dataset is too limited in scope, the model may struggle with novel task formats.

### Mechanism 3
- Claim: Verification-of-Choice (VoC) prompting enhances reasoning for multi-choice questions by forcing the model to generate and compare explanations for each choice.
- Mechanism: VoC prompts the model to generate explanations for each answer choice, then compare these explanations to identify inconsistencies and select the most plausible answer.
- Core assumption: The model's self-generated explanations are logically sound and can be meaningfully compared to identify the correct answer.
- Evidence anchors: [abstract] "Verification-of-Choice approach for prompting engineering, which significantly enhances the reasoning ability of LLMs"
- Break condition: If the model's explanations are consistently illogical or biased, comparison may lead to incorrect answers.

## Foundational Learning

- Concept: Continual pre-training
  - Why needed here: General LLMs lack domain-specific knowledge needed for medical tasks, requiring additional pre-training on medical data to build foundational medical knowledge.
  - Quick check question: What type of data is typically used for continual pre-training of a general LLM in a specific domain?

- Concept: Instruction tuning
  - Why needed here: Medical tasks require understanding specific instructions and formats (e.g., multi-choice questions), which general LLMs may not be familiar with without task-specific fine-tuning.
  - Quick check question: How does instruction tuning differ from standard fine-tuning in terms of the data and objectives used?

- Concept: Prompt engineering
  - Why needed here: Medical multi-choice questions require complex reasoning, and carefully designed prompts (like VoC) can guide the model to produce more accurate and explainable answers.
  - Quick check question: What is the key difference between traditional chain-of-thought prompting and the Verification-of-Choice approach?

## Architecture Onboarding

- Component map: Base LLM (AntGLM-10B) -> Medical knowledge datasets -> Medical instruction datasets -> Multi-choice question datasets -> Fine-tuning strategies (LoRA, C-Poly) -> Prompting strategies (CoT, COVE, VoC)
- Critical path: Pre-training → Instruction tuning → Task-specific adaptation
- Design tradeoffs: Model size vs. performance (AntGLM-Med-10B achieves comparable performance to larger models), computational cost of fine-tuning vs. benefit of specialized knowledge
- Failure signatures: Poor performance on held-out medical tasks, inability to generalize to new medical concepts, inconsistent or illogical explanations for multi-choice answers
- First 3 experiments:
  1. Evaluate base LLM performance on PubMedQA to establish baseline
  2. Apply continual pre-training on medical knowledge datasets and re-evaluate
  3. Apply instruction tuning and re-evaluate to measure incremental improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dataset size for each stage of the 3-stage optimization process to achieve the best performance?
- Basis in paper: [inferred] The paper discusses the use of large-scale medical datasets but does not provide specific details on the optimal dataset size.
- Why unresolved: The paper mentions large-scale datasets but doesn't specify exact sizes or provide a comparison of different dataset sizes.
- What evidence would resolve it: Conducting experiments with varying dataset sizes for each stage and comparing resulting model performance.

### Open Question 2
- Question: How does the proposed Verification-of-Choice (VoC) approach compare to other prompting strategies like Chain-of-Thought (CoT) and Chain-of-Verification (COVE) in terms of enhancing reasoning ability for medical tasks?
- Basis in paper: [explicit] The paper introduces VoC as a novel prompting strategy but doesn't provide a direct comparison with other prompting strategies.
- Why unresolved: While the paper demonstrates VoC's effectiveness, it doesn't compare it to other prompting strategies on medical tasks.
- What evidence would resolve it: Conducting experiments that directly compare VoC with other prompting strategies on medical tasks.

### Open Question 3
- Question: What are the potential limitations or challenges of adapting a pre-trained general LLM to the medical domain, and how can they be addressed?
- Basis in paper: [inferred] The paper discusses the adaptation process but doesn't explicitly address potential limitations or challenges.
- Why unresolved: The paper provides a framework for adaptation but doesn't discuss potential limitations or challenges that may arise.
- What evidence would resolve it: Conducting further research to identify potential limitations and proposing solutions or strategies to address them.

## Limitations

- The paper lacks comparative ablation studies to determine which components of the 3-stage framework contribute most to performance gains.
- Dataset composition details are sparse, making it difficult to assess the quality and coverage of medical knowledge injection.
- The Verification-of-Choice approach is introduced without rigorous comparison to established prompting methods like chain-of-thought or chain-of-verification.

## Confidence

**High Confidence**: The overall methodology of using a staged approach (pre-training → instruction tuning → task adaptation) for domain adaptation is well-established and the implementation appears sound. PubMedQA results are clearly reported and reproducible.

**Medium Confidence**: The claim that AntGLM-Med-10B outperforms larger models (>40B parameters) on PubMedQA is supported by reported numbers, though the comparison set appears limited. The 3-stage framework's effectiveness is plausible but lacks comprehensive ablation analysis.

**Low Confidence**: The specific claims about VoC's superiority for multi-choice reasoning are weakly supported. The paper provides the mechanism and shows it works in context but doesn't rigorously compare against alternatives.

## Next Checks

1. **Ablation Study**: Run controlled experiments removing each stage of the optimization framework (pre-training only, pre-training + instruction tuning, full 3-stage) to quantify the contribution of each component to final performance.

2. **Prompting Method Comparison**: Systematically compare VoC against established prompting approaches (chain-of-thought, chain-of-verification, direct prompting) on the same multi-choice medical datasets with statistical significance testing.

3. **Dataset Quality Analysis**: Conduct an analysis of the medical datasets used for pre-training and instruction tuning, including size measurements, topic coverage analysis, and bias detection to validate that knowledge injection is comprehensive and balanced across medical subdomains.