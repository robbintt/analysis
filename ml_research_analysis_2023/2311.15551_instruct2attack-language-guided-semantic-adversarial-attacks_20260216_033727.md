---
ver: rpa2
title: 'Instruct2Attack: Language-Guided Semantic Adversarial Attacks'
arxiv_id: '2311.15551'
source_url: https://arxiv.org/abs/2311.15551
tags:
- image
- adversarial
- make
- semantic
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Instruct2Attack (I2A), a language-guided
  adversarial attack method that generates semantically meaningful image perturbations
  based on free-form text instructions. The key idea is to adversarially guide a conditional
  latent diffusion model to find an adversarial latent code that generates adversarial
  images while following the language instruction and preserving the perceptual similarity
  to the input.
---

# Instruct2Attack: Language-Guided Semantic Adversarial Attacks

## Quick Facts
- arXiv ID: 2311.15551
- Source URL: https://arxiv.org/abs/2311.15551
- Reference count: 40
- Primary result: I2A outperforms existing attack methods, achieving much lower classification accuracy especially on adversarially trained models and under strong defenses like DiffPure.

## Executive Summary
Instruct2Attack (I2A) is a novel language-guided adversarial attack method that generates semantically meaningful image perturbations based on free-form text instructions. The key innovation is adversarially guiding a conditional latent diffusion model to find an adversarial latent code that generates adversarial images while following the language instruction and preserving perceptual similarity to the input. The method also automates attack generation using GPT-4 to create image-specific text instructions. Experiments demonstrate I2A's superiority over existing methods under white-box and black-box settings, with particularly strong performance against adversarially trained models and robust defenses.

## Method Summary
I2A uses a conditional latent diffusion model where adversarial guidance factors α and β modulate the image and text guidance during the reverse diffusion process. Starting from a random latent code, the method iteratively optimizes these guidance factors using projected gradient descent to find an adversarial latent code that minimizes classification confidence while maintaining perceptual similarity (measured by LPIPS) and following the text instruction. The attack process can be automated using GPT-4 to generate diverse, image-specific instructions from captions. A perceptual projection step ensures the generated adversarial images stay within the LPIPS constraint.

## Key Results
- I2A achieves significantly lower classification accuracy than existing methods, especially on adversarially trained models
- Strong performance under defenses like DiffPure, with adaptive attacks reducing accuracy to near zero
- Demonstrates excellent transferability across different model architectures (ResNet, ConvNeXt, Swin)
- Generates natural, diverse adversarial examples that follow text instructions while maintaining perceptual similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial guidance factors α and β modulate the image and text guidance during the diffusion process to steer the latent space search toward adversarial codes.
- Mechanism: The adversarial guidance factors α and β are learned parameters that scale the image and text guidance terms in the denoising U-Net. By multiplying the guidance vectors element-wise, they selectively emphasize or de-emphasize certain directions in the latent space that are most likely to fool the classifier while still following the language instruction.
- Core assumption: The latent space of the diffusion model contains directions that correspond to semantically meaningful changes that can fool the classifier.
- Evidence anchors:
  - [abstract]: "Controlled by two learnable adversarial guidance factors corresponding to the image and text guidance respectively, I2A navigates in the latent space to find an adversarial latent code..."
  - [section 3.2]: "To adversarially guide the diffusion process to find an adversarial latent code zadv, we introduce two adversarial guidance factors α, β ∈ [0, 1]h×w×c that adversarially modulate the image guidance and text guidance at each diffusion step..."
  - [corpus]: Weak - no direct mention of adversarial guidance factors in related works, but the concept of adversarial guidance in diffusion models is novel.
- Break condition: If the classifier is trained to be robust against adversarial perturbations in the latent space of the diffusion model, or if the guidance factors become stuck in local minima.

### Mechanism 2
- Claim: The perceptual constraint ensures that the adversarial images generated by I2A are semantically meaningful and perceptually similar to the original images.
- Mechanism: I2A uses the LPIPS distance as a perceptual similarity metric to bound the adversarial perturbation. This encourages the generated adversarial images to preserve the overall appearance of the original images while allowing for controlled semantic edits based on the language instructions.
- Core assumption: LPIPS is a good proxy for human perceptual similarity and can effectively distinguish between natural and unnatural image edits.
- Evidence anchors:
  - [section 3.2]: "To preserve the similarity between x and xadv while allowing for semantic editing, we propose to use the perceptual similarity metric LPIPS [38, 88] in-stead of the typical pixel-wise Lp distance to bound the adversarial perturbation."
  - [section 4.4]: "As γ increases, we have stronger attacks but lower FID [26], which indicates lower image quality."
  - [corpus]: Weak - while LPIPS is mentioned in related works, its effectiveness as a perceptual similarity metric for adversarial attacks is not extensively studied.
- Break condition: If the LPIPS distance fails to capture the perceptual similarity between the original and adversarial images, or if the bound γ is set too high, allowing for unnatural edits.

### Mechanism 3
- Claim: Automatic instruction generation using GPT-4 enables I2A to generate diverse and image-specific adversarial examples without manual intervention.
- Mechanism: I2A uses GPT-4 to generate image captions and then creates editing instructions based on the captions and object categories. This allows I2A to automatically generate a large number of diverse and image-specific adversarial examples without the need for manual prompt engineering.
- Core assumption: GPT-4 can generate natural and plausible image captions and editing instructions that result in semantically meaningful adversarial examples.
- Evidence anchors:
  - [abstract]: "We further automate the attack process with GPT-4 [59] to generate diverse image-specific text instructions."
  - [section 3.3]: "We first convert the input image to an image caption using BLIP-2 [42]. Then we apply the language-only GPT-4 [59] to generate editing instructions based on the input captions and object categories."
  - [corpus]: Weak - while GPT-4 is mentioned in related works, its effectiveness in generating image-specific adversarial instructions is not extensively studied.
- Break condition: If GPT-4 generates implausible or nonsensical instructions, or if the generated instructions fail to produce semantically meaningful adversarial examples.

## Foundational Learning

- Concept: Latent Diffusion Models (LDMs)
  - Why needed here: LDMs are the core generative model used by I2A to generate adversarial images in the latent space.
  - Quick check question: What is the main advantage of using LDMs over traditional GANs for adversarial attacks?

- Concept: Perceptual Similarity Metrics
  - Why needed here: Perceptual similarity metrics like LPIPS are used by I2A to ensure that the generated adversarial images are semantically meaningful and perceptually similar to the original images.
  - Quick check question: Why is LPIPS preferred over pixel-wise Lp distances for measuring perceptual similarity in I2A?

- Concept: Adversarial Training
  - Why needed here: Adversarial training is a defense mechanism against adversarial attacks, and understanding its limitations is crucial for evaluating the effectiveness of I2A.
  - Quick check question: How does adversarial training impact the performance of I2A on defended models?

## Architecture Onboarding

- Component map: Input image and text instruction -> Image and text encoders (E and C) -> Conditional denoising U-Net (ϵθ) -> Adversarial guidance factors (α and β) -> Perceptual constraint (LPIPS distance) -> Output adversarial image
- Critical path: Input image and text instruction → Encoding → Adversarial guidance → Diffusion sampling → Perceptual projection → Output adversarial image
- Design tradeoffs:
  - Tradeoff between attack strength and perceptual similarity (controlled by γ)
  - Tradeoff between diversity of adversarial examples (controlled by prompts) and attack success rate
  - Tradeoff between computational cost (number of diffusion steps) and attack quality
- Failure signatures:
  - Adversarial examples that are easily detected by the classifier
  - Adversarial examples that are perceptually dissimilar to the original images
  - Adversarial examples that fail to follow the language instructions
- First 3 experiments:
  1. Generate adversarial examples using I2A on a simple dataset (e.g., MNIST) with manually crafted prompts to verify the basic functionality.
  2. Compare the attack success rate and perceptual similarity of I2A with and without the perceptual constraint on a larger dataset (e.g., CIFAR-10).
  3. Evaluate the transferability of I2A attacks across different model architectures (e.g., ResNet, VGG) on a benchmark dataset (e.g., ImageNet).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between perceptual similarity (LPIPS) and attack success rate when varying the constraint parameter γ in Instruct2Attack?
- Basis in paper: [explicit] The paper states "we set γ = 0.3 since it achieves a nice tradeoff between image quality and attack performance" and provides results for different γ values (0.1, 0.2, 0.3, 0.5, 0.7, 1.0).
- Why unresolved: While the paper provides results for different γ values, it doesn't systematically analyze the relationship between γ, attack success rate, and perceptual similarity across different model architectures and defense methods.
- What evidence would resolve it: A comprehensive study showing how attack success rate and LPIPS distance vary with γ for different victim models (ResNet, ConvNeXt, Swin) and under different defenses (DiffPure, adversarial training).

### Open Question 2
- Question: How does Instruct2Attack perform against more sophisticated adaptive defenses that specifically target semantic adversarial attacks?
- Basis in paper: [explicit] The paper evaluates I2A against DiffPure defense both in non-adaptive and adaptive settings, but only uses BPDA+EOT techniques.
- Why unresolved: The paper only considers DiffPure as the preprocessing defense and uses standard BPDA+EOT techniques. More sophisticated defenses specifically designed to counter semantic adversarial attacks or more advanced adaptive attack strategies are not explored.
- What evidence would resolve it: Experiments showing I2A performance against semantic-specific defenses (e.g., semantic consistency checks, multimodal verification) and under more advanced adaptive attacks (e.g., those targeting the diffusion model components).

### Open Question 3
- Question: What is the impact of different text instruction generation methods on the effectiveness and transferability of Instruct2Attack?
- Basis in paper: [explicit] The paper uses GPT-4 to generate image-specific instructions but only evaluates with five prompts (four manually written, one GPT-4 generated) per image.
- Why unresolved: The paper doesn't systematically analyze how different instruction generation strategies affect attack performance, nor does it explore the diversity and quality of instructions generated by different models or prompting strategies.
- What evidence would resolve it: A comprehensive study comparing attack success rates, transferability, and perceptual quality when using different instruction generation methods (e.g., GPT-3.5, Claude, different prompting strategies) and varying the number of instructions per image.

## Limitations
- Heavy reliance on the quality of the underlying diffusion model and perceptual similarity metric
- Computational cost significantly higher than traditional adversarial attack methods
- Automatic instruction generation using GPT-4 introduces uncertainty in instruction quality
- LPIPS constraint may limit attack effectiveness against highly robust models

## Confidence

- Core claims about I2A's effectiveness: Medium-High
- Theoretical understanding of adversarial guidance factors: Medium
- Claims about transferability across architectures: High
- Claims about performance against defenses: Medium-High

## Next Checks

1. **Ablation Study on Guidance Factors**: Systematically vary the initialization and learning rates of α and β to determine their sensitivity and optimal ranges for different datasets and model architectures.

2. **Transferability Under Defense Conditions**: Test I2A's transferability when attacking models that have been trained with adversarial training specifically designed to counter diffusion-based attacks, such as models trained on DiffPure-augmented data.

3. **Perceptual Quality Assessment**: Conduct a human study to validate whether the LPIPS-constrained adversarial examples are indeed perceived as natural and semantically similar to the original images, beyond the automated FID and LPIPS metrics.