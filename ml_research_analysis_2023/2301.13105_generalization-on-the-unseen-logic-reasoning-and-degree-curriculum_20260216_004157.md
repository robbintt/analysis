---
ver: rpa2
title: Generalization on the Unseen, Logic Reasoning and Degree Curriculum
arxiv_id: '2301.13105'
source_url: https://arxiv.org/abs/2301.13105
tags:
- learning
- have
- function
- training
- note
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates generalization on the unseen (GOTU), a
  strong out-of-distribution setting for learning Boolean functions. The authors provide
  both theoretical and experimental evidence that for certain network architectures
  including random features models, diagonal linear networks, and Transformers, a
  min-degree interpolator (MDI) is learned on the unseen.
---

# Generalization on the Unseen, Logic Reasoning and Degree Curriculum

## Quick Facts
- **arXiv ID**: 2301.13105
- **Source URL**: https://arxiv.org/abs/2301.13105
- **Reference count**: 40
- **Primary result**: Random features models, diagonal linear networks, and Transformers learn min-degree interpolators on unseen domains, explaining generalization in logic reasoning tasks.

## Executive Summary
This paper investigates generalization on the unseen (GOTU), a strong out-of-distribution setting for learning Boolean functions. The authors provide both theoretical and experimental evidence that for certain network architectures including random features models, diagonal linear networks, and Transformers, a min-degree interpolator (MDI) is learned on the unseen. An MDI is the interpolator with the lowest degree profile, concentrating Fourier-Walsh transform energy on the lowest degree basis elements. The findings explain the length generalization problem and motivate a curriculum learning algorithm called Degree-Curriculum that incrementally increases input complexity to learn monomials more efficiently. The work provides insights into implicit biases of neural networks in reasoning tasks with discrete/combinatorial data.

## Method Summary
The paper proposes a framework for analyzing generalization on the unseen by connecting it to vanishing ideals and Fourier-Walsh transforms. The method involves training neural networks (MLP, Transformer, RF, mean-field) on seen domains while evaluating performance on unseen subsets. Theoretical analysis establishes conditions under which different architectures learn MDIs, while experiments validate these findings across various Boolean functions and unseen domains. The Degree-Curriculum algorithm is introduced to improve learning efficiency by incrementally increasing input complexity.

## Key Results
- Random features models with strongly expressive activation functions learn MDIs on unseen domains
- Diagonal linear networks with small initialization exhibit MDI bias by effectively ignoring frozen variables
- Transformers show strong MDI bias, preferring lower-degree monomials in learned solutions
- The Degree-Curriculum algorithm improves learning efficiency for sparse Boolean functions

## Why This Works (Mechanism)

### Mechanism 1
Random features models trained via gradient descent converge to min-degree interpolators (MDIs) on unseen domains. Strongly expressive activation functions ensure that random features have decaying degree profiles in their Fourier-Walsh expansion. When minimizing the norm of the learned coefficients, the model preferentially captures lower-degree monomials, leading to MDIs.

### Mechanism 2
Diagonal linear neural networks with small initialization learn MDIs on unseen domains by effectively ignoring frozen variables. Gradient flow dynamics cause the bias parameter to learn the target function's bias while the contribution from frozen variables remains negligible. The balancedness property ensures parameters stay bounded, preventing high-degree monomial learning.

### Mechanism 3
Transformers exhibit a strong MDI bias on unseen domains, preferring lower-degree monomials in learned solutions. The attention and feedforward mechanisms implicitly regularize toward solutions with lower-degree profiles in the Fourier-Walsh basis, similar to RF models and diagonal networks.

## Foundational Learning

- **Concept**: Fourier-Walsh transform and Boolean function representation
  - Why needed here: The paper's analysis relies on representing Boolean functions in the Fourier-Walsh basis to analyze degree profiles and MDI properties.
  - Quick check question: Can you express a simple Boolean function (e.g., AND or XOR) in terms of its Fourier-Walsh coefficients?

- **Concept**: Algebraic geometry and vanishing ideals
  - Why needed here: The paper connects unseen domains to vanishing ideals to characterize the space of valid interpolators and MDIs.
  - Quick check question: Given an unseen domain U, can you construct the vanishing ideal I(U^c) and explain how it constrains valid interpolators?

- **Concept**: Curriculum learning and support incrementing
  - Why needed here: The paper proposes a Degree-Curriculum algorithm that incrementally increases input complexity to learn monomials more efficiently.
  - Quick check question: How does training on increasing Hamming weight balls (B_r1 ⊆ B_r2 ⊆ ...) help learn sparse Boolean functions?

## Architecture Onboarding

- **Component map**:
  - Random Features Model: Input → Random feature mapping (φ_w,b(x) = σ(⟨w,x⟩+b)) → Linear combination (∑a_i φ_i(x)) → Output
  - Diagonal Linear Network: Input bits → Per-bit weight products (∏w_i^(l)) → Bias term → Output
  - Transformer: Input encoding → Multi-head attention layers → Feedforward layers → Output projection

- **Critical path**: For MDI bias demonstration:
  1. Generate training data on seen domain U^c
  2. Train model (RF, DLN, or Transformer) with moderate learning rate
  3. Analyze learned function's Fourier-Walsh coefficients to check for MDI bias

- **Design tradeoffs**:
  - Activation function choice: Strongly expressive (e.g., polynomials) vs. ReLU (leaky MDI bias)
  - Learning rate: Moderate (MDI bias) vs. large (leaky MDI bias)
  - Model depth: Deeper networks may require smaller initialization for MDI bias

- **Failure signatures**:
  - Random Features: Leaky MDI bias with ReLU activation for odd-degree monomials >1
  - Diagonal Linear Network: MDI bias lost with large initialization scale
  - Transformer: MDI bias weakened with large learning rates

- **First 3 experiments**:
  1. RF model with polynomial activation on f1(x) = x0x1 - 1.25x1x2 + 1.5x2x0 under U1 = {x0x1x2 = -1} - expect MDI bias
  2. Diagonal linear network on parity function under canonical holdout - expect MDI bias with small initialization
  3. Transformer on f3(x) = x0x1x2 + ... + x13x14x0 + x14x0x1 under U3 = {(x0,x1,x2) = (-1,-1,-1)} - expect strong MDI bias

## Open Questions the Paper Calls Out

### Open Question 1
Does the min-degree interpolator (MDI) bias extend beyond Boolean functions to other discrete/combinatorial domains? The paper mentions that the MDI bias may be observed for non-sparse functions in some cases, and suggests that the concept could be extended to other settings using minimum description length (MDL) principles. This remains unresolved as the paper primarily focuses on Boolean functions and provides limited evidence for other discrete/combinatorial domains.

### Open Question 2
Can the Degree-Curriculum algorithm be effectively applied to continuous or non-Boolean functions? The paper introduces the Degree-Curriculum algorithm for learning monomials more efficiently in Boolean functions, but suggests that it could be extended to other settings. This question remains unresolved as the paper does not provide experimental evidence or theoretical justification for applying the Degree-Curriculum algorithm to continuous or non-Boolean functions.

### Open Question 3
What are the implications of the MDI bias on the interpretability and explainability of neural network models? The paper discusses the MDI bias as a form of Occam's razor, but does not explore its implications on model interpretability and explainability. This question remains unresolved as the paper does not address the relationship between the MDI bias and model interpretability, leaving open the question of how this bias affects the understanding of neural network decision-making.

## Limitations

- Theoretical analysis is limited to specific network architectures and activation functions
- Empirical validation is primarily focused on Boolean functions with limited exploration of other discrete/combinatorial domains
- The relationship between MDI bias and generalization performance lacks rigorous empirical validation across diverse function families

## Confidence

- **High**: Random features models with polynomial activations exhibit MDI bias under stated conditions
- **Medium**: Diagonal linear networks learn MDIs with small initialization when frozen variables remain negligible
- **Low**: Transformers consistently exhibit strong MDI bias across all function families and unseen domains

## Next Checks

1. **Activation function sweep**: Systematically test RF models with various activation functions (polynomials of different degrees, ReLU, tanh) on parity and sparse Boolean functions to map the boundary between MDI and leaky MDI behavior.

2. **Initialization sensitivity**: For diagonal linear networks, conduct a grid search over initialization scales to quantify the threshold where MDI bias breaks down, particularly for functions with frozen variables.

3. **Learning rate ablation**: Train Transformers on the same Boolean functions with varying learning rates (0.001 to 0.1) to empirically verify the relationship between learning rate magnitude and MDI bias strength.