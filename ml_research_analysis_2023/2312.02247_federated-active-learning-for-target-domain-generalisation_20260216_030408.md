---
ver: rpa2
title: Federated Active Learning for Target Domain Generalisation
arxiv_id: '2312.02247'
source_url: https://arxiv.org/abs/2312.02247
tags:
- domain
- target
- learning
- feda
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FEDALV, a federated active learning framework
  for target domain generalisation. The core idea is to combine federated learning
  with energy-based domain alignment (FEDA) and active learning to improve model performance
  on unseen target domains.
---

# Federated Active Learning for Target Domain Generalisation

## Quick Facts
- arXiv ID: 2312.02247
- Source URL: https://arxiv.org/abs/2312.02247
- Authors: 
- Reference count: 40
- One-line primary result: FEDALV achieves state-of-the-art results on domain generalization benchmarks while sampling only 5% of source client data.

## Executive Summary
This paper introduces FEDALV, a federated active learning framework that combines federated learning with energy-based domain alignment (FEDA) and active learning to improve model performance on unseen target domains. FEDA employs local and global optimization stages to reduce feature complexity and align free energies between source and target domains. FEDALV then uses an active learning strategy based on free energy to dynamically sample the most informative source data for the target client. Extensive experiments on PACS, OfficeHome, and OfficeCaltech benchmarks demonstrate that FEDALV outperforms conventional federated domain generalization and federated active learning baselines while requiring minimal labeled source data.

## Method Summary
FEDALV combines FEDA (Federated Learning with Energy-based Domain Alignment) and active learning for federated domain generalization. FEDA has two stages: local optimization using L2 norm and conditional mutual information losses to reduce feature complexity and enforce alignment with conditional distributions, and global optimization to align free energies between source and target domains. FEDALV uses free energy-based active learning to rank target samples and select the closest source samples in feature space, dynamically sampling the most informative data for the target client. The method is evaluated on three benchmark datasets (PACS, OfficeHome, OfficeCaltech) using leave-one-out cross-validation with classification accuracy as the primary metric.

## Key Results
- FEDALV achieves state-of-the-art results on PACS, OfficeHome, and OfficeCaltech benchmarks
- Outperforms conventional federated domain generalization and federated active learning baselines
- Reaches full training target accuracy while sampling only 5% of source client's data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FEDA reduces domain shift by minimizing feature complexity and aligning free energies between source and target domains.
- Mechanism: FEDA uses two local regularization losses (L2 norm and conditional mutual information) to simplify representations and enforce alignment with a conditional distribution. A global optimization stage then aligns free energies between source and target domains.
- Core assumption: Minimizing feature complexity and aligning free energies will lead to domain-invariant representations that generalize well to unseen target domains.
- Evidence anchors:
  - [abstract]: "FEDA employs local and global optimisation stages to reduce feature complexity and align free energies between source and target domains."
  - [section]: "For the client, the introduced losses aim to reduce feature complexity and condition alignment, while in the server, the regularisation limits free energy biases between source and target obtained by the global model."
  - [corpus]: Weak. No direct evidence in corpus papers about free energy alignment in federated learning.
- Break condition: If the local regularization losses are too strong, they may over-simplify the representations and harm performance. If the global free energy alignment is too weak, it may not effectively reduce domain shift.

### Mechanism 2
- Claim: FEDALV dynamically selects the most informative source data for the target client using an active learning strategy based on free energy.
- Mechanism: FEDALV ranks target samples by their free energy values and selects the closest source samples in the feature space. This prioritizes source samples that are most relevant for reducing the free energy gap between source and target.
- Core assumption: Samples with high free energy on the target domain are the most informative for improving generalization, and selecting similar samples from the source domain will help reduce the free energy gap.
- Evidence anchors:
  - [abstract]: "FEDALV then uses an active learning strategy to dynamically sample the most informative source data for the target client."
  - [section]: "FEDALV exploits similar principles of free energy alignment present in EADA. Consequently, the client models are approximated as EBMs and the selection criteria tracks the free energy biases against the target samples."
  - [corpus]: Weak. No direct evidence in corpus papers about active learning based on free energy in federated learning.
- Break condition: If the free energy estimation is inaccurate, the selection may prioritize irrelevant samples. If the feature space is not well-aligned, the closest samples may not be truly informative.

### Mechanism 3
- Claim: FEDA and FEDALV together achieve state-of-the-art performance on domain generalization benchmarks while requiring minimal labeled source data.
- Mechanism: FEDA's domain-invariant representations and free energy alignment provide a strong foundation for generalization. FEDALV then intelligently selects the most informative source samples to further improve target performance with minimal labeled data.
- Core assumption: The combination of FEDA's domain generalization and FEDALV's active learning will outperform both conventional federated domain generalization and federated active learning baselines.
- Evidence anchors:
  - [abstract]: "Extensive experiments on three benchmarks (PACS, OfficeHome, and OfficeCaltech) demonstrate that FEDALV achieves state-of-the-art results, outperforming both conventional federated domain generalisation and federated active learning baselines."
  - [section]: "Our extensive quantitative experiments demonstrate the superiority of our method in accuracy and efficiency compared to the multiple contemporary methods."
  - [corpus]: Weak. No direct evidence in corpus papers about the combined approach of FEDA and FEDALV.
- Break condition: If the benchmarks are not representative of real-world scenarios, the performance may not generalize. If the active learning budget is too small, the benefits may be limited.

## Foundational Learning

- Concept: Energy-based models (EBMs) and free energy
  - Why needed here: FEDA and FEDALV rely on free energy estimation and alignment to measure and reduce domain shift.
  - Quick check question: Can you explain the relationship between EBMs, free energy, and domain generalization?

- Concept: Federated learning and domain generalization
  - Why needed here: The paper combines federated learning with domain generalization techniques to address the challenge of training models on distributed data with domain shift.
  - Quick check question: What are the key challenges in federated domain generalization, and how do they differ from standard federated learning?

- Concept: Active learning and sample selection
  - Why needed here: FEDALV uses active learning to dynamically select the most informative source samples for the target client, reducing the need for labeled data.
  - Quick check question: What are the different criteria for active learning sample selection, and how does FEDALV's approach based on free energy differ from others?

## Architecture Onboarding

- Component map:
  Local client models -> Global model -> Free energy estimator -> Active learning selector

- Critical path:
  1. Local client models train with FEDA regularization
  2. Global model aggregates and aligns free energies
  3. Active learning selector identifies informative source samples
  4. Selected samples are labeled and added to the training set
  5. Repeat until convergence or budget exhausted

- Design tradeoffs:
  - Local regularization strength: Balancing feature simplicity and expressiveness
  - Free energy alignment strength: Balancing domain alignment and overfitting
  - Active learning budget: Balancing performance gains and labeling costs

- Failure signatures:
  - Poor target performance: Insufficient domain alignment or active learning selection
  - Slow convergence: Overly conservative regularization or insufficient active learning
  - High labeling costs: Inefficient active learning selection or insufficient budget

- First 3 experiments:
  1. Evaluate FEDA's performance on a single benchmark (e.g., PACS) with varying local regularization strengths.
  2. Evaluate FEDALV's performance on a single benchmark (e.g., PACS) with varying active learning budgets.
  3. Evaluate the combined FEDA+FEDALV approach on all benchmarks with default hyperparameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of FEDALV change if the global scaler λf ea was dynamically adjusted during training rather than kept fixed?
- Basis in paper: [explicit] The paper mentions that increasing the global scaler λf ea causes the global model to pull the sources towards the target, impacting local training and yielding larger source losses after communications.
- Why unresolved: The paper only provides a qualitative observation about the effect of increasing λf ea, but does not explore dynamic adjustment of this parameter during training.
- What evidence would resolve it: An experiment comparing the performance of FEDALV with a fixed λf ea versus a dynamically adjusted λf ea would provide evidence for the impact of this parameter on the model's performance.

### Open Question 2
- Question: How would the performance of FEDALV change if the local scalers λL2 and λCM I were learned locally instead of being fixed?
- Basis in paper: [explicit] The paper mentions that the scalers can be checked with random search, but proposes for future work to learn them locally, as each domain may overlap more or less with the others.
- Why unresolved: The paper only proposes learning the local scalers as future work, but does not provide any experimental results or analysis of the impact of this approach.
- What evidence would resolve it: An experiment comparing the performance of FEDALV with fixed local scalers versus learned local scalers would provide evidence for the impact of this approach on the model's performance.

### Open Question 3
- Question: How would the performance of FEDALV change if the selection function was modified to consider class uncertainty in addition to free energy and Euclidean distance?
- Basis in paper: [explicit] The paper mentions that class uncertainty is measured from the global model, but FEDALV is not entirely dependent on this measurement.
- Why unresolved: The paper does not explore the impact of incorporating class uncertainty into the selection function, only mentioning it as a potential supporting factor.
- What evidence would resolve it: An experiment comparing the performance of FEDALV with the current selection function versus a modified selection function that incorporates class uncertainty would provide evidence for the impact of this change on the model's performance.

## Limitations
- Theoretical foundations of free energy alignment in federated settings are not well-established
- Active learning based on free energy lacks direct evidence in the corpus
- Limited experimental validation across diverse scenarios and benchmarks
- Potential overfitting to specific dataset characteristics

## Confidence
- FEDA's effectiveness: Medium
- FEDALV's active learning strategy: Medium
- Combined approach's superiority: Low

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of FEDA's local regularization and global free energy alignment components.
2. Test the robustness of FEDALV's active learning strategy across different feature alignment methods and free energy estimation techniques.
3. Evaluate the combined approach on additional federated domain generalization benchmarks and real-world datasets to assess generalizability.