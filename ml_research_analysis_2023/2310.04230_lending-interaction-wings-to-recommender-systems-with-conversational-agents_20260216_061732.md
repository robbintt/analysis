---
ver: rpa2
title: Lending Interaction Wings to Recommender Systems with Conversational Agents
arxiv_id: '2310.04230'
source_url: https://arxiv.org/abs/2310.04230
tags:
- attribute
- querying
- user
- items
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORE introduces an offline-training and online-checking paradigm
  to bridge conversational agents and recommender systems via a unified uncertainty
  minimization framework. It treats the recommender system as an offline relevance
  score estimator and the conversational agent as an online checker that minimizes
  uncertainty by querying items or attributes.
---

# Lending Interaction Wings to Recommender Systems with Conversational Agents

## Quick Facts
- **arXiv ID**: 2310.04230
- **Source URL**: https://arxiv.org/abs/2310.04230
- **Reference count**: 40
- **Key outcome**: CORE consistently outperforms 9 popular recommendation approaches and recently proposed RL-based methods in both hot-start and cold-start settings on 8 industrial datasets.

## Executive Summary
CORE introduces an offline-training and online-checking paradigm to bridge conversational agents and recommender systems via a unified uncertainty minimization framework. It treats the recommender system as an offline relevance score estimator and the conversational agent as an online checker that minimizes uncertainty by querying items or attributes. The method derives expected certainty gain for each query and builds an online decision tree to decide what to query at each turn. Experiments on 8 industrial datasets show CORE consistently outperforms 9 popular recommendation approaches and recently proposed reinforcement learning-based methods in both hot-start and cold-start settings. It can also be empowered by pre-trained language models to communicate as a human.

## Method Summary
CORE operates by first training a recommender system offline to produce relevance scores for all items. During online interaction, a conversational agent uses these scores to compute uncertainty (sum of unchecked item relevance scores) and expected certainty gain for each possible query. The agent selects queries (either items or attributes) that maximize expected certainty gain, building an online decision tree to guide the conversation. The approach is plug-and-play, requiring only relevance scores from any recommender system without modifying the underlying recommendation algorithm. CORE can be enhanced with pre-trained language models to generate human-like queries and extract attribute values from free-text responses.

## Key Results
- CORE outperforms 9 popular recommendation approaches (FM, DeepFM, PNN, DIN, GRU, LSTM, MMOE, GCN, GAT) on 8 industrial datasets
- Consistently superior performance in both hot-start and cold-start settings compared to RL-based methods (AG, ME, CRM, EAR)
- Successfully bridges the gap between conversational agents and recommender systems with a unified uncertainty minimization framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The offline-training and online-checking paradigm decouples the recommendation computation from the interaction computation, allowing efficient real-time querying.
- Mechanism: The recommender system computes relevance scores offline using historical data, producing a static score for each item. The conversational agent then operates online, querying either items or attributes to minimize the uncertainty defined as the sum of unchecked relevance scores.
- Core assumption: Relevance scores computed offline remain valid during online sessions and can be used to guide uncertainty reduction.
- Evidence anchors:
  - [abstract] "CORE treats a recommender system as an offline relevance score estimator to produce an estimated relevance score for each item; while a conversational agent is regarded as an online relevance score checker to check these estimated scores in each session."
  - [section 2.1] "CORE treats a recommender system as an offline relevance score estimator that offline assigns a relevance score to each item, while a conversational agent is regarded as an online relevance score checker that online checks whether these estimated scores could reflect the relevance between items and the user’s current needs."
- Break condition: If the offline relevance scores become stale or irrelevant due to changing user preferences, the uncertainty minimization will guide queries poorly.

### Mechanism 2
- Claim: Expected certainty gain derivation enables optimal selection of the next query (item or attribute) by quantifying uncertainty reduction potential.
- Mechanism: For each possible query, the algorithm computes the expected reduction in the sum of unchecked relevance scores. For items, it uses the probability that the item is a target item based on estimated relevance. For attributes, it sums over all possible attribute values weighted by their probabilities.
- Core assumption: The estimated relevance scores can be used to approximate the probability that an item or attribute value matches the user's current preference.
- Evidence anchors:
  - [section 3.1] "We derive expected certainty gain to measure the expectation of how much uncertainty could be eliminated by querying each item and each attribute."
  - [section 3.1] "Then, during each interaction, our conversational agent selects an item or an attribute with the maximum certainty gain, resulting in an online decision tree algorithm."
- Break condition: If the estimated relevance scores are poorly calibrated, the expected certainty gain calculations will be inaccurate, leading to suboptimal query selection.

### Mechanism 3
- Claim: The plug-and-play design allows CORE to work with any recommendation approach without retraining or modifying the underlying recommender system.
- Mechanism: CORE only requires the estimated relevance scores from the recommender system, which can be obtained from any supervised learning-based recommendation method. The conversational agent operates independently, using these scores to guide its queries.
- Core assumption: Any recommender system can provide estimated relevance scores that reflect item-user relevance.
- Evidence anchors:
  - [abstract] "Therefore, CORE can be seamlessly applied to any recommendation platform."
  - [section 3.3] "From the algorithm, we can clearly see that our ΨCO(·) puts no constraints on ΨRE(·) and only requires the estimated relevance scores from ΨRE(·), therefore, CORE can be seamlessly integrated into any recommendation platform."
- Break condition: If a recommender system cannot produce relevance scores or produces scores that are not comparable across items, CORE cannot function properly.

## Foundational Learning

- Concept: Uncertainty minimization framework
  - Why needed here: Provides the theoretical foundation for why querying items or attributes reduces uncertainty about the user's true preferences.
  - Quick check question: If the uncertainty is defined as the sum of unchecked relevance scores, what happens to the uncertainty when we query an item that is not a target item?

- Concept: Expected value calculations for decision making
  - Why needed here: Enables the conversational agent to quantify the expected benefit of each possible query and select the optimal one.
  - Quick check question: How is the expected certainty gain for querying an attribute calculated when considering all possible attribute values?

- Concept: Online decision tree construction
  - Why needed here: Provides the algorithmic structure for recursively selecting queries based on previous answers and updating the set of unchecked items and attributes.
  - Quick check question: What information determines which node (item or attribute) is selected at each level of the decision tree?

## Architecture Onboarding

- Component map:
  - Recommender System (ΨRE) -> Conversational Agent (ΨCO) -> Uncertainty Calculator -> Decision Tree Builder -> Human-AI Simulator

- Critical path:
  1. Offline: Train recommender system to produce relevance scores
  2. Online: Initialize with all items and attributes unchecked
  3. Online: Compute expected certainty gains for all possible queries
  4. Online: Select query with maximum certainty gain
  5. Online: Receive user response and update unchecked sets
  6. Online: Repeat until target item found or maximum turns reached

- Design tradeoffs:
  - Offline computation vs. online latency: More complex offline scoring improves online efficiency
  - Query type selection: Balancing between querying items (direct recommendation) and attributes (information gathering)
  - Attribute value granularity: Choosing between querying attribute IDs vs. specific attribute values
  - Dependency modeling: Adding complexity to account for attribute correlations vs. simpler independent assumption

- Failure signatures:
  - High uncertainty persists despite many queries: Indicates poor relevance score estimation or suboptimal query selection
  - Conversational agent repeatedly queries same attribute types: May indicate missing attribute value generation or improper attribute selection criteria
  - Performance degrades on cold-start scenarios: Suggests relevance scores are not informative without user history
  - System gets stuck in long query chains: Could indicate need for better attribute selection or item recommendation integration

- First 3 experiments:
  1. Implement basic CORE with a simple FM-based recommender and test on Amazon dataset with KMAX=3, measuring T@3 and S@3
  2. Compare CORE performance with and without considering attribute dependencies on a tabular dataset
  3. Evaluate CORE in cold-start setting by using uniform relevance scores and measuring performance degradation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The paper claims CORE can be empowered by pre-trained language models to communicate as a human, but provides limited empirical validation of this capability.
- The experimental evaluation focuses on relatively clean, structured datasets, with performance in real-world conversational scenarios with noisy, ambiguous user inputs untested.
- The cold-start evaluation uses a specific initialization strategy, but the robustness of this approach across different cold-start scenarios and the impact of initialization strategies on performance are not thoroughly explored.

## Confidence
- **High confidence**: The core uncertainty minimization framework and offline-online paradigm are well-defined and theoretically sound. The derivation of expected certainty gain for query selection follows logically from the uncertainty definition.
- **Medium confidence**: The performance claims against 9 popular recommendation approaches and RL-based methods are supported by experimental results, though the paper doesn't provide detailed statistical significance testing across all comparisons.
- **Low confidence**: The claims about seamless integration with any recommender system and the effectiveness of LLM-powered human-like communication lack sufficient empirical validation and implementation details.

## Next Checks
1. Conduct ablation studies to quantify the contribution of each component (offline relevance estimation, online uncertainty minimization, decision tree construction) to overall performance.
2. Implement and evaluate the port function with a pre-trained LLM on at least one dataset, measuring the quality of generated queries and accuracy of extracted attribute values from free-text responses.
3. Test CORE's performance on conversational datasets with naturally occurring user feedback (e.g., dialogue datasets or real user interaction logs) rather than simulated responses based on structured attributes.