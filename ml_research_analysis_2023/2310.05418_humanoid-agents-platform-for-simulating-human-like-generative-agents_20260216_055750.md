---
ver: rpa2
title: 'Humanoid Agents: Platform for Simulating Human-like Generative Agents'
arxiv_id: '2310.05418'
source_url: https://arxiv.org/abs/2310.05418
tags:
- agents
- basic
- agent
- activities
- needs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Humanoid Agents, a platform that enhances
  generative agents with System 1 processing (basic needs, emotions, relationship
  closeness) to produce more human-like behavior. The method extends existing generative
  agents by adding mechanisms for agents to dynamically adjust their activities and
  conversations based on internal states.
---

# Humanoid Agents: Platform for Simulating Human-like Generative Agents

## Quick Facts
- arXiv ID: 2310.05418
- Source URL: https://arxiv.org/abs/2310.05418
- Authors: 
- Reference count: 5
- Primary result: Achieves F1≥0.84 alignment with human annotations for predicting activity satisfaction, emotions, and relationship changes in generative agent simulations.

## Executive Summary
Humanoid Agents is a platform that enhances generative agents with System 1 processing elements (basic needs, emotions, relationship closeness) to produce more human-like behavior. The system extends existing generative agents by adding mechanisms for agents to dynamically adjust their activities and conversations based on internal states. The platform includes a Unity WebGL interface and analytics dashboard for visualization, and is available as open-source software. Experiments demonstrate good alignment with human annotations for predicting activity satisfaction, emotions, and relationship changes, with basic needs and emotions significantly influencing agent activities.

## Method Summary
The Humanoid Agents platform extends generative agents by incorporating System 1 elements including basic needs (energy, fullness, health, social, fun), emotions (happiness, sadness, anger, fear, disgust), and relationship closeness between agents. The system evaluates agent internal states at each 15-minute time step, using language models to generate plan modifications when needs are unmet or emotions are non-neutral. Relationship closeness influences dialogue generation and is adjusted based on interaction enjoyment. The platform provides visualization through Unity WebGL and analytics through Plotly Dash dashboards.

## Key Results
- System achieves F1≥0.84 alignment with human annotations for predicting activity satisfaction, emotions, and relationship changes
- Basic needs and emotions significantly influence agent activities, with health, energy, and fullness having strongest effects
- Relationship closeness affects dialogue patterns in ways consistent with human behavior
- Platform successfully simulates human-like behavioral adaptation through System 1 processing elements

## Why This Works (Mechanism)

### Mechanism 1
Basic needs satisfaction and emotion feedback dynamically adjust agent activities to mimic human-like adaptation. The system evaluates agent internal states at each time step, and if needs are unmet (≤3/10) or emotion is non-neutral, generates a natural language plan modification prompt for the language model. The language model then generates modified plans that better address unmet needs or emotions.

### Mechanism 2
Relationship closeness influences dialogue patterns and sentiment in ways consistent with human social behavior. The system tracks relationship closeness as an integer value (0-30), converts it to natural language, and includes it in dialogue generation prompts. After conversations, the system evaluates if agents enjoyed the interaction and adjusts closeness values accordingly (+1 for enjoyment, -1 otherwise).

### Mechanism 3
The platform's visualization and analytics dashboard enable researchers to observe and validate System 1 attribute dynamics. The Unity WebGL interface visualizes agent locations and statuses in real-time, while the Plotly Dash dashboard provides time-series graphs of basic needs satisfaction and relationship closeness development.

## Foundational Learning

- Concept: System 1 vs System 2 thinking
  - Why needed here: The paper builds on this psychological framework to justify adding System 1 elements to generative agents that primarily use System 2 planning
  - Quick check question: What are the key differences between System 1 and System 2 thinking, and why does this paper argue that generative agents need System 1 elements?

- Concept: Basic needs hierarchy (Maslow's theory)
  - Why needed here: The paper uses Maslow's hierarchy to explain why certain basic needs have stronger effects on agent behavior than others
  - Quick check question: According to Maslow's hierarchy, which basic needs should have the strongest influence on behavior, and how does the paper's experimental data support this?

- Concept: Natural language processing and language model prompting
  - Why needed here: The entire system relies on language models to generate plan modifications, dialogue, and evaluate internal states based on natural language prompts
  - Quick check question: How does the paper use natural language descriptions of internal states to prompt the language model for plan modifications?

## Architecture Onboarding

- Component map: Agent Initialization -> Activity Planning -> Activity Execution -> Internal State Evaluation -> Plan Modification -> Dialogue Generation -> Visualization Update
- Critical path: The system runs in a loop at each 15-minute time step, evaluating internal states, modifying plans when needed, generating dialogue, and updating visualizations
- Design tradeoffs: The system trades computational efficiency for more human-like behavior by adding multiple layers of evaluation and modification; natural language prompts add flexibility but may be less efficient than rule-based systems
- Failure signatures: Agents may starve or become exhausted if they fail to adapt activities to basic needs; conversations may lack social nuance if relationship closeness doesn't influence dialogue; researchers cannot validate behavior if visualizations don't update correctly
- First 3 experiments:
  1. Test basic needs satisfaction prediction by comparing system predictions with human annotations on a sample day
  2. Test emotion influence by initializing agents with different emotions and observing activity changes compared to neutral baseline
  3. Test relationship closeness effects by varying initial closeness values and measuring conversation length and sentiment

## Open Questions the Paper Calls Out

### Open Question 1
How does the variability in natural decline rates of basic needs affect agent behavior and simulation outcomes? The paper mentions plans to allow customized decline rates for each character to account for individual differences, but provides no data or analysis on how different rates might influence behavior.

### Open Question 2
What is the impact of multiparty dialogues on the dynamics of agent interactions and relationship development? The system currently only supports two-agent dialogues even when multiple agents are present, and plans to support multi-party conversation in the future.

### Open Question 3
How does the synchronization of activities between agents influence the realism and coherence of the simulation? The paper mentions that activity planning is done independently by each agent and not forcibly synchronized, with plans to implement synchronization in the future.

## Limitations

- Evaluation methodology relies heavily on human annotations rather than objective behavioral metrics
- Platform dependency on language model performance for plan modification introduces significant variability
- Open-source release provides framework but lacks complete implementation details for critical components

## Confidence

*High confidence:* The architectural design connecting basic needs, emotions, and relationship closeness to agent behaviors is well-specified and internally consistent.

*Medium confidence:* The claim that basic needs and emotions significantly influence agent activities is supported by experimental results, but evaluation relies on human annotations rather than objective outcomes.

*Low confidence:* Insufficient detail on language model prompting strategy for plan modification makes it difficult to assess reliability and consistency of this critical mechanism.

## Next Checks

1. Conduct systematic evaluation of language model's plan modification success rate by testing with increasingly complex internal state descriptions and measuring coherence of generated plans

2. Implement automated sentiment analysis validation by comparing system's relationship closeness adjustments with independent sentiment scoring algorithms to quantify accuracy

3. Run cross-scenario validation by initializing agents in different social contexts and measuring consistency of basic needs influence patterns across scenarios