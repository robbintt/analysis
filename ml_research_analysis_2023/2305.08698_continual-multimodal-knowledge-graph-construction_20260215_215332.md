---
ver: rpa2
title: Continual Multimodal Knowledge Graph Construction
arxiv_id: '2305.08698'
source_url: https://arxiv.org/abs/2305.08698
tags:
- multimodal
- learning
- continual
- knowledge
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of continual multimodal knowledge
  graph construction (MKGC), where models must handle continuously emerging entities
  and relations while avoiding catastrophic forgetting. The authors identify that
  existing MKGC models struggle with imbalanced learning rhythms across modalities
  and inconsistent forgetting rates during multimodal interaction.
---

# Continual Multimodal Knowledge Graph Construction

## Quick Facts
- **arXiv ID**: 2305.08698
- **Source URL**: https://arxiv.org/abs/2305.08698
- **Reference count**: 40
- **Key outcome**: LMC framework achieves superior performance in continual multimodal knowledge graph construction by addressing imbalanced learning rhythms and inconsistent forgetting rates across modalities

## Executive Summary
This paper addresses the challenge of continual multimodal knowledge graph construction (MKGC), where models must handle continuously emerging entities and relations while avoiding catastrophic forgetting. The authors identify that existing MKGC models struggle with imbalanced learning rhythms across modalities and inconsistent forgetting rates during multimodal interaction. To address these challenges, they propose the Lifelong Multimodal Consistent Transformer (LMC) framework, which incorporates gradient modulation for balanced multimodal learning rhythm and hand-in-hand multimodal interaction with attention distillation to maintain consistent forgetting rates across modalities. Experiments on two sub-tasks (MRE and MNER) across various incremental settings demonstrate that LMC significantly outperforms both conventional MKGC models and continual unimodal KGC models, achieving superior performance in dynamic knowledge environments.

## Method Summary
The Lifelong Multimodal Consistent Transformer (LMC) framework consists of three key modules: gradient modulation for balanced multimodal learning rhythm, hand-in-hand multimodal interaction with attention distillation, and memory replay strategy. The framework uses dual-stream transformers (BERT for text, ViT for images) with gradient modulation to dynamically adjust gradients based on modality contribution ratios. Attention distillation preserves previous model attention patterns through shared learnable keys and asymmetric distance functions. Memory replay with random sampling combats catastrophic forgetting by storing and rehearsing samples from previous tasks. The framework is evaluated on Twitter2017 (MNER) and MNRE (MRE) datasets, with incremental tasks containing one entity/relation type each.

## Key Results
- LMC achieves significantly higher F1 scores than conventional MKGC models and continual unimodal KGC models across both MNER and MRE tasks
- Gradient modulation strategy effectively balances multimodal learning rhythms, addressing the issue of different convergence rates between text and vision modalities
- Memory replay with random sampling provides approximately 28.5% performance boost on average forgetting metric compared to models without memory
- Attention distillation successfully maintains consistent forgetting rates across modalities by preserving previous attention patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient modulation strategy balances multimodal learning rhythm by dynamically adjusting gradients based on modality contribution ratios
- Mechanism: Calculates contribution ratios (ùõæùë°) between text and vision modalities using softmax outputs, then applies modulation coefficients (ùëîùë°) to scale gradients during optimization
- Core assumption: Different modalities have inherently different convergence rates in continual learning scenarios
- Evidence anchors:
  - [abstract]: "incorporates gradient modulation for balanced multimodal learning rhythm"
  - [section 3.2]: "we propose a modulation coefficient ùëîùë° to adaptively modulate the gradient"
  - [corpus]: Missing - no direct evidence found in related papers about gradient modulation for multimodal continual learning

### Mechanism 2
- Claim: Hand-in-hand multimodal interaction with attention distillation maintains consistent forgetting rates across modalities
- Mechanism: Uses shared learnable keys (ùêæùë†) for both visual and textual encoders to create aligned attention maps, then applies asymmetric distance function (ReLU) to distill attention differences
- Core assumption: Previous model attention patterns should be preserved when learning new tasks to prevent forgetting
- Evidence anchors:
  - [abstract]: "hand-in-hand multimodal interaction with attention distillation to maintain consistent forgetting rates across modalities"
  - [section 3.3.2]: "we propose to distill the attention-level matrices... to reduce forgetting and maintain the stability"
  - [corpus]: Weak - related papers mention attention distillation but not in multimodal continual learning context

### Mechanism 3
- Claim: Memory replay with random sampling effectively combats catastrophic forgetting while maintaining plasticity
- Mechanism: Stores samples from previous tasks in memory buffer and replays them during current task training to maintain knowledge of old classes
- Core assumption: Rehearsal-based methods are most effective for continual knowledge graph construction
- Evidence anchors:
  - [section 3.1]: "our framework includes three modules... multimodal rehearsal"
  - [section 4.5.1]: "the 'Memory' mechanism yields about28.5% performance boost on average forgetting metric"
  - [corpus]: Strong - related papers confirm rehearsal-based methods are most promising for continual KGC

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper addresses the core challenge of retaining previously learned knowledge while acquiring new information in streaming scenarios
  - Quick check question: What happens to neural network performance when trained sequentially on multiple tasks without any mitigation strategy?

- Concept: Multimodal learning convergence rates
  - Why needed here: The paper identifies that different modalities (text vs. vision) converge at different rates, causing imbalanced learning rhythm
  - Quick check question: Why might text and image modalities converge at different speeds during training?

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper uses self-attention and attention distillation to maintain consistent multimodal interactions across tasks
  - Quick check question: How does self-attention compute relationships between different elements in a sequence?

## Architecture Onboarding

- Component map: Input ‚Üí Dual encoders (ViT for vision, BERT for text) ‚Üí Hand-in-hand interaction ‚Üí Gradient modulation ‚Üí Memory replay ‚Üí Output classification
- Critical path: Input ‚Üí Dual encoders ‚Üí Hand-in-hand interaction ‚Üí Gradient modulation ‚Üí Memory replay ‚Üí Output classification
- Design tradeoffs: Balancing between stability (retaining old knowledge) and plasticity (learning new knowledge) through gradient modulation and attention distillation
- Failure signatures: Performance drops on previous tasks indicate catastrophic forgetting; poor current task performance indicates loss of plasticity
- First 3 experiments:
  1. Test gradient modulation effectiveness by comparing modality contribution ratios with and without modulation
  2. Validate attention distillation by measuring attention map similarity between current and previous model versions
  3. Evaluate memory replay impact by varying memory buffer sizes and measuring forgetting metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LMC scale with increasing numbers of tasks beyond 10, particularly in terms of catastrophic forgetting and plasticity trade-offs?
- Basis in paper: [explicit] The paper shows experiments up to 10 tasks in Figure 7, with a note that they evaluate "the impact of the number of tasks" but only demonstrate results for 5, 7, and 10 tasks.
- Why unresolved: The paper doesn't explore scenarios with significantly more tasks (e.g., 20+, 50+) where forgetting and plasticity dynamics might change dramatically.
- What evidence would resolve it: Extended experiments showing performance metrics across 20+ tasks, particularly tracking forgetting curves and plasticity degradation over long task sequences.

### Open Question 2
- Question: How does LMC perform on multimodal datasets with more than two modalities (e.g., text, image, audio, video), and what modifications would be needed to handle such scenarios?
- Basis in paper: [inferred] The current framework only handles text and image modalities, but the introduction mentions "videos, etc." as potential modalities, and the authors mention future plans to extend to "more continual multimodal learning tasks."
- What evidence would resolve it: Experiments on datasets with three or more modalities, architectural modifications showing how the gradient modulation and attention distillation mechanisms would adapt, and performance comparisons with specialized multi-modal CL methods.

### Open Question 3
- Question: What is the impact of different memory sampling strategies (beyond random sampling) on LMC's performance, particularly in highly imbalanced or non-IID task distributions?
- Basis in paper: [explicit] The paper mentions "random sampling strategy" and compares LMC with RP-CRE which uses "relation prototypes," but doesn't explore alternative sampling strategies like class-balanced sampling, importance-based sampling, or clustering-based approaches.
- Why unresolved: The authors only evaluate random sampling despite acknowledging that "most memory-based continual KGC models are greatly impacted by the memory size."
- What evidence would resolve it: Systematic comparison of LMC with different sampling strategies (e.g., reservoir sampling, class-balanced sampling, prototype-based sampling) across various data distributions, showing when each strategy is most effective.

## Limitations
- The paper lacks direct empirical ablation studies showing the specific contribution of gradient modulation versus the combined effect of all three components
- Attention distillation effectiveness has limited validation through quantitative analysis of attention map preservation across tasks
- Memory replay mechanism doesn't explore the impact of different sampling strategies beyond random selection

## Confidence
- **High Confidence**: Claims about catastrophic forgetting being a significant problem in multimodal KGC (supported by strong experimental evidence showing baseline performance degradation)
- **Medium Confidence**: Claims about gradient modulation balancing learning rhythms (supported by theoretical framework but limited ablation studies)
- **Medium Confidence**: Claims about attention distillation maintaining consistent forgetting rates (supported by theoretical framework but limited quantitative validation)

## Next Checks
1. Run experiments with LMC framework components isolated (gradient modulation only, attention distillation only, memory replay only) to quantify individual contributions to performance
2. Measure cosine similarity between attention maps across tasks to quantify the effectiveness of attention distillation in preserving previous task knowledge
3. Compare random sampling with more sophisticated strategies (e.g., reservoir sampling, gradient-based selection) to evaluate if memory replay performance can be improved beyond the reported results