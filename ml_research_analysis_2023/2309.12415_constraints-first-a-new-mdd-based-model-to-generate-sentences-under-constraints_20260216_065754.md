---
ver: rpa2
title: 'Constraints First: A New MDD-based Model to Generate Sentences Under Constraints'
arxiv_id: '2309.12415'
source_url: https://arxiv.org/abs/2309.12415
tags:
- sentences
- constraints
- sentence
- n-grams
- mnread
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach for generating constrained
  text using Multi-Valued Decision Diagrams (MDD). The approach is motivated by the
  need to generate standardized sentences for vision screening tests like MNREAD,
  which require sentences to follow strict rules on grammar, vocabulary, length, and
  display.
---

# Constraints First: A New MDD-based Model to Generate Sentences Under Constraints

## Quick Facts
- arXiv ID: 2309.12415
- Source URL: https://arxiv.org/abs/2309.12415
- Reference count: 40
- Key outcome: MDD-based approach generates hundreds of valid MNREAD sentences efficiently, with GPT-2 perplexity predicting reading speed.

## Executive Summary
This paper introduces a novel approach to constrained text generation using Multi-Valued Decision Diagrams (MDDs). The method first generates sentences that satisfy strict constraints using MDDs, then applies GPT-2 to select the best sentences based on perplexity. Demonstrated on MNREAD vision screening tests, the approach generates hundreds of valid French and English sentences efficiently. The work shows that MDDs can be a promising alternative for constrained text generation when constraints are difficult to satisfy.

## Method Summary
The method extracts n-grams from a corpus of books, builds an MDDtrie to efficiently store and retrieve n-grams and their successors, then unfolds the MDDtrie to generate sentences satisfying MNREAD constraints. Finally, GPT-2 scores and sorts the generated sentences based on perplexity. The approach requires extracting n-grams, building the MDDtrie structure, unfolding it with constraint checking, and using GPT-2 for quality assessment.

## Key Results
- Generated hundreds of valid MNREAD sentences in French and English
- GPT-2 perplexity found to be a good predictor of reading speed
- MDD-based approach shown to be efficient and scalable for constrained generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MDDs enable exhaustive constraint satisfaction without search.
- Mechanism: MDDs store all valid n-gram sequences in a compressed DAG, allowing direct retrieval of constraint-satisfying sentences.
- Core assumption: The constraint space can be represented as a finite set of n-gram sequences that fit within MDD structure.
- Evidence anchors:
  - [abstract] "MDDs are a well-known data structure to deal with constraints" and "compute an exhaustive set of solutions without performing any search."
  - [section] "MDDs are structured in layers where each layer represents an ordered variable... Each path between the node root and tt forms a label tuple corresponding to an assignment of the variables."
  - [corpus] Weak: no explicit corpus test of MDD completeness.
- Break condition: If constraints involve dependencies longer than n-gram size, MDDs cannot encode them directly.

### Mechanism 2
- Claim: GPT-2 scoring effectively filters generated sentences for meaning and fluency.
- Mechanism: GPT-2 assigns perplexity scores to candidate sentences; low perplexity correlates with grammaticality and semantic coherence.
- Core assumption: GPT-2's training on large text corpora captures general language quality.
- Evidence anchors:
  - [abstract] "apply a language model (GPT-2) to keep the best ones" and "GPT-2 perplexity...is found to be a good predictor of reading speed."
  - [section] "we compute the perplexity of the sentences as a score...if GPT-2 assigns a low PPL to a sentence, we conclude that it is very likely to be a "good" sentence."
  - [corpus] Weak: corpus not evaluated for GPT-2 scoring consistency.
- Break condition: If GPT-2 model is trained on domain very different from target sentences, perplexity scores may be unreliable.

### Mechanism 3
- Claim: Combining n-grams with MDDTrie enforces succession rules while preserving sentence structure.
- Mechanism: MDDTrie stores all n-grams; edges connect n-grams whose last n-1 words match the next n-1 words, enabling grammatically valid sentence construction.
- Core assumption: Valid sentences can be decomposed into overlapping n-grams satisfying the MDDTrie linking rules.
- Evidence anchors:
  - [section] "we can link two n-grams if the n-1 last words of the first n-gram are the n-1 first words of the second n-gram" and "MDDtrie is a specific MDD."
  - [abstract] "we rely on combining existing sequences of words (n-grams)" and "MDD is a straightforward extension and an efficient way of compactly representing a corpus."
  - [corpus] Weak: corpus composition details not given for n-gram construction.
- Break condition: If input corpus lacks sufficient n-gram coverage, MDDTrie cannot generate diverse valid sentences.

## Foundational Learning

- Concept: Discrete combinatorial optimization.
  - Why needed here: The problem is formulated as assigning words to positions under hard constraints.
  - Quick check question: What is the difference between a constraint satisfaction problem and a pure optimization problem?
- Concept: Multivalued Decision Diagrams (MDDs).
  - Why needed here: MDDs compactly encode large sets of solutions and enable efficient constraint propagation.
  - Quick check question: How does an MDD differ from a BDD in terms of variable domains?
- Concept: Perplexity as a language quality metric.
  - Why needed here: Perplexity from GPT-2 is used to rank candidate sentences after generation.
  - Quick check question: What does a lower perplexity score indicate about a sentence's fluency?

## Architecture Onboarding

- Component map:
  - Corpus preprocessing → n-gram extraction → MDDTrie construction → MDDMNREAD expansion → GPT-2 scoring → selection
- Critical path:
  1. Build MDDTrie from n-grams (memory intensive).
  2. Unfold MDDMNREAD with constraint checks (CPU bound).
  3. Score sentences with GPT-2 (network/I/O bound).
- Design tradeoffs:
  - Larger n-gram size improves grammaticality but explodes combinatorics.
  - Memory vs. speed: MDDTrie can be rebuilt from corpus or cached.
  - GPT-2 scoring accuracy vs. latency.
- Failure signatures:
  - No solutions generated: constraints too strict or corpus too small.
  - All low PPL scores: GPT-2 mismatch or sentences too simple.
  - Memory errors: n-gram set too large for MDDTrie.
- First 3 experiments:
  1. Generate sentences with 3-grams, check MDDTrie size and solution count.
  2. Vary n-gram size (3,4,5) and measure GPT-2 PPL distribution.
  3. Replace GPT-2 with a smaller language model to test PPL sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the PPL constraint be integrated into the model as an additional constraint to avoid the need for post-generation selection?
- Basis in paper: [explicit] The paper discusses the potential of using PPL as a constraint but notes that it is not straightforward to define due to the non-monotonic nature of perplexity and its dependence on all variables.
- Why unresolved: Designing a perplexity constraint is complex because the relationship between the perplexity of a subset of variables and the full set of variables is not clear.
- What evidence would resolve it: A successful integration of the PPL constraint into the model, demonstrating that it can effectively filter out sentences with low quality during the generation process.

### Open Question 2
- Question: What is the impact of corpus selection on the quality and style of generated sentences?
- Basis in paper: [explicit] The paper discusses the implications of corpus construction and notes that the choice of corpus strongly impacts the kind of generated sentences.
- Why unresolved: The paper does not provide a comprehensive study on the impact of different corpus choices on the generated sentences.
- What evidence would resolve it: A comparative analysis of generated sentences using different corpora, evaluating their quality, style, and suitability for specific applications.

### Open Question 3
- Question: How can the PPL range and threshold values be determined to effectively distinguish between good and bad sentences?
- Basis in paper: [explicit] The paper mentions that identifying multiple threshold values to delineate ranges within the set of solutions is not straightforward.
- Why unresolved: The paper does not provide a clear method for determining the PPL range and threshold values.
- What evidence would resolve it: A study that investigates the relationship between PPL values and sentence quality, providing guidelines for setting appropriate threshold values.

## Limitations
- Scalability to more complex constraint sets and larger vocabularies remains unproven
- GPT-2 perplexity correlation with human judgments needs further validation
- Limited direct comparisons with existing methods restrict generalizability claims

## Confidence
- High confidence in MDD's ability to represent and efficiently traverse constraint-satisfying solutions
- Medium confidence in GPT-2 perplexity as a quality metric for generated sentences
- Medium confidence in the overall approach's superiority over existing methods

## Next Checks
1. Apply the MDD+GPT-2 approach to a different constrained generation task (e.g., technical writing guidelines, poetry generation) to verify generalizability beyond MNREAD
2. Systematically vary constraint complexity and vocabulary size to identify breaking points where MDD construction becomes infeasible
3. Conduct human evaluations of generated sentences alongside perplexity scores to verify that low perplexity correlates with human judgments of grammaticality and semantic coherence