---
ver: rpa2
title: Explaining Generalization Power of a DNN Using Interactive Concepts
arxiv_id: '2302.13091'
source_url: https://arxiv.org/abs/2302.13091
tags:
- concepts
- interactive
- generalization
- order
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization power of deep neural
  networks (DNNs) by analyzing interactive concepts encoded by the DNN. The core idea
  is to measure the complexity of each interactive concept and investigate how the
  complexity of interactive concepts is related to the generalization power of the
  entire DNN.
---

# Explaining Generalization Power of a DNN Using Interactive Concepts

## Quick Facts
- arXiv ID: 2302.13091
- Source URL: https://arxiv.org/abs/2302.13091
- Reference count: 38
- Key outcome: Low-order interactive concepts generalize better than high-order ones; overfitting correlates with encoding stronger high-order concepts

## Executive Summary
This paper investigates why deep neural networks (DNNs) generalize differently by analyzing the interactive concepts they encode. The authors propose that the generalization power of a DNN can be explained by the complexity distribution of interactive concepts - with low-order concepts (involving fewer input variables) generalizing better than high-order concepts. Through experiments across multiple architectures and datasets, they demonstrate that DNNs with stronger generalization power learn low-order interactive concepts more quickly and encode fewer complex interactive concepts.

## Method Summary
The method trains DNNs on datasets like MNIST and CIFAR-10, then computes interactive concepts using Harsanyi dividends to measure how subsets of input variables contribute to predictions. Salient concepts are identified based on thresholds, and their generalization power is analyzed by comparing distributions between training and testing data. The approach examines how concept complexity (measured by order - number of input variables) relates to generalization, tracks learning dynamics through training, and correlates overfitting levels with concept complexity patterns.

## Key Results
- Low-order interactive concepts generalize significantly better to testing data than high-order concepts
- DNNs with stronger generalization power encode fewer complex interactive concepts and learn low-order concepts more quickly
- Overfitted DNNs encode significantly stronger high-order interactive concepts compared to normally trained models
- High-order concept learning involves detouring dynamics where DNNs first learn incorrect low-order concepts before shifting to higher orders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNN generalization power is explained by the complexity distribution of interactive concepts it encodes
- Mechanism: Low-order concepts have more similar distributions across training/testing data, indicating better generalization; high-order concepts are sensitive to input perturbations
- Core assumption: Interactive concepts extracted via Harsanyi dividends faithfully represent symbolic patterns
- Evidence anchors: Abstract finding on low-order concept generalization; distribution computation methodology
- Break condition: If sparsity assumption fails, concept extraction becomes computationally intractable

### Mechanism 2
- Claim: Overfitted DNNs encode stronger high-order interactive concepts
- Mechanism: Label noise creates controlled overfitting; more noise leads to stronger high-order concepts
- Core assumption: Label noise varies generalization power without affecting other learning aspects
- Evidence anchors: Section 2.3 finding on overfitted DNNs and high-order concepts; noise ratio methodology
- Break condition: If label noise affects feature learning beyond generalization

### Mechanism 3
- Claim: High-order concepts are difficult to learn due to detouring dynamics
- Mechanism: DNNs first learn low-order concepts, then shift to higher orders while removing incorrect low-order concepts
- Core assumption: Learning trajectory can be tracked through interaction strength changes
- Evidence anchors: Section 2.4 finding on low-order concept learning; Figure 9 showing learning dynamics
- Break condition: If learning dynamics are dominated by other factors like optimization algorithm

## Foundational Learning

- Concept: Harsanyi dividend interaction measurement
  - Why needed here: Provides mathematical framework to quantify interactive concepts
  - Quick check question: How does the Harsanyi dividend formula decompose network output into interaction effects?

- Concept: Sparsity of interactive concepts
  - Why needed here: Analysis relies on assumption that only few interactions significantly influence predictions
  - Quick check question: What percentage of interactions typically have negligible effects (|I(S|x)|≈0)?

- Concept: Order of interactive concepts
  - Why needed here: Complexity measured by number of input variables determines generalization potential
  - Quick check question: How is order of an interactive concept formally defined?

## Architecture Onboarding

- Component map: Sample → Mask → Compute interactions → Identify salient concepts → Analyze generalization → Track learning dynamics
- Critical path: Data → Interaction measurement → Concept extraction → Generalization analysis → Learning dynamics tracking
- Design tradeoffs: Computational cost vs. faithfulness, noise level vs. controlled overfitting, measurement granularity vs. interpretability
- Failure signatures: Non-sparse interactions, unstable measurements, poor correlation between order and generalization
- First 3 experiments:
  1. Verify sparsity by computing percentage of negligible interactions on simple dataset
  2. Test generalization correlation by training models with controlled noise and measuring high-order concept strength
  3. Track learning dynamics by monitoring interaction evolution during training of a high-order target concept

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantify the trade-off between learning speed of low-order concepts and learning difficulty of high-order concepts in DNNs?
- Basis in paper: Paper discusses that DNNs with strong generalization learn low-order concepts more quickly but doesn't offer quantitative measure
- Why unresolved: While insights provided, no quantitative metric exists to evaluate this trade-off
- What evidence would resolve it: Mathematical model or metric quantifying learning speed and difficulty, tested across architectures and datasets

### Open Question 2
- Question: How does initialization of DNN weights affect emergence and generalization of interactive concepts?
- Basis in paper: Paper doesn't discuss impact of weight initialization on interactive concept learning
- Why unresolved: Weight initialization is crucial but its effect on interactive concepts remains unexplored
- What evidence would resolve it: Experiments with different initialization methods analyzing impact on interactive concept emergence and generalization

### Open Question 3
- Question: Can findings about interactive concepts be extended to other neural networks like RNNs or GNNs?
- Basis in paper: Focuses on DNNs but doesn't discuss applicability to other architectures
- Why unresolved: Interactive concepts might differ in other network types
- What evidence would resolve it: Extending analysis to other architectures and comparing generalization power and learning dynamics

### Open Question 4
- Question: How does presence of noise in input data affect stability and generalization of interactive concepts?
- Basis in paper: Discusses sensitivity of high-order concepts to perturbations but not impact of input noise
- Why unresolved: Understanding noise impact is crucial for real-world applications
- What evidence would resolve it: Experiments with noisy input data analyzing impact on stability and generalization of interactive concepts

### Open Question 5
- Question: How do findings about interactive concepts relate to interpretability and explainability of DNNs?
- Basis in paper: Uses interactive concepts to understand generalization but doesn't discuss interpretability implications
- Why unresolved: Interactive concepts could improve interpretability but relationship remains unexplored
- What evidence would resolve it: Investigating relationship with other interpretability methods and developing new techniques leveraging interactive concepts

## Limitations

- Computational Complexity: Exponential growth in computing Harsanyi dividends for all input subsets
- Measurement Fidelity: Reliance on input perturbations may capture measurement artifacts rather than true interaction structure
- Causality vs. Correlation: Strong correlations established but causal mechanisms remain somewhat speculative

## Confidence

- High Confidence: Low-order concepts generalize better than high-order concepts
- Medium Confidence: Overfitting correlates with encoding of high-order concepts
- Low Confidence: Detouring dynamics mechanism explaining learning difficulty

## Next Checks

1. Validate sparsity assumption by measuring actual sparsity of interactive concepts on tested datasets
2. Test whether observed patterns hold for transformer-based architectures and other non-convolutional networks
3. Validate cross-dataset generalization by testing relationship between concept complexity and generalization across fundamentally different data distributions