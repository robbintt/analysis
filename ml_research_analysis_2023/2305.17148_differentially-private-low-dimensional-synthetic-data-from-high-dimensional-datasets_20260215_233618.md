---
ver: rpa2
title: Differentially Private Low-dimensional Synthetic Data from High-dimensional
  Datasets
arxiv_id: '2305.17148'
source_url: https://arxiv.org/abs/2305.17148
tags:
- data
- private
- algorithm
- matrix
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a differentially private algorithm to generate
  low-dimensional synthetic data from high-dimensional datasets, with a utility guarantee
  measured by the Wasserstein distance. The key idea is to first project the data
  onto a low-dimensional subspace using a private PCA procedure, and then generate
  synthetic data on this subspace using existing methods.
---

# Differentially Private Low-dimensional Synthetic Data from High-dimensional Datasets

## Quick Facts
- **arXiv ID**: 2305.17148
- **Source URL**: https://arxiv.org/abs/2305.17148
- **Reference count**: 40
- **Primary result**: Differentially private algorithm for generating low-dimensional synthetic data with near-optimal utility bounds measured by Wasserstein distance, improving upon prior work's curse of dimensionality

## Executive Summary
This paper introduces a differentially private algorithm to generate low-dimensional synthetic data from high-dimensional datasets. The key innovation is a private PCA procedure that projects data onto a low-dimensional subspace without requiring a spectral gap assumption, followed by synthetic data generation on this subspace. The method achieves near-optimal accuracy guarantees with respect to the Wasserstein distance, particularly when the original data lies in a low-dimensional affine subspace. This approach overcomes the curse of dimensionality that plagued previous methods while maintaining pure ε-differential privacy.

## Method Summary
The algorithm first computes a private covariance matrix by adding Laplacian noise to the sample covariance matrix, then performs private PCA to identify the top d' eigenvectors. The data is projected onto this d'-dimensional subspace, and synthetic data is generated using either PMM (for d'=2) or PSMM (for d'≥3), both of which add calibrated Laplacian noise to counts or lattice structures. Finally, a private mean vector is added back and metric projection ensures outputs remain in the original domain [0,1]^d.

## Key Results
- Achieves near-optimal accuracy rate of (εn)^(-1/d') for Wasserstein distance when data lies in a d'-dimensional subspace
- Circumvents the curse of dimensionality present in prior work
- Provides privacy and accuracy bounds without requiring spectral gap assumptions
- Demonstrates utility improvement over previous methods through rigorous theoretical analysis

## Why This Works (Mechanism)

### Mechanism 1
Adding Laplacian noise to the sample covariance matrix yields ε-differentially private PCA subspace estimation without needing a spectral gap assumption. The centered sample covariance matrix M is perturbed by a symmetric Laplacian random matrix A with variance chosen as σ = 3d²/(εn). This ensures differential privacy via the exponential mechanism argument and enables projection onto the top d' eigenvectors without spectral gap constraints.

### Mechanism 2
Low-dimensional synthetic data generated in the projected subspace preserves 1-Wasserstein distance to the original data with near-optimal accuracy. After projecting onto a d'-dimensional subspace, synthetic data are generated using private mechanisms (PMM for d'=2, PSMM for d'≥3) that add calibrated Laplacian noise to counts over a covering or lattice. This yields synthetic samples close to the projected data.

### Mechanism 3
Adding a private mean vector and metric projection preserves differential privacy while bounding utility loss. A Laplacian noise vector λ' is added to the mean before shifting synthetic data back; metric projection onto [0,1]^d ensures outputs remain in the original domain without affecting privacy.

## Foundational Learning

- **Concept**: Differential privacy and composition
  - **Why needed here**: The algorithm composes multiple private steps (covariance matrix, mean vector, synthetic data) that must sum to total ε-DP
  - **Quick check question**: What is the privacy budget split among the three main components in Algorithm 1?

- **Concept**: Principal component analysis and subspace projection
  - **Why needed here**: PCA identifies the low-dimensional subspace onto which data is projected to avoid the curse of dimensionality
  - **Quick check question**: Why is the centered sample covariance matrix preferred over 1/n XX^T in this context?

- **Concept**: Wasserstein distance and optimal transport
  - **Why needed here**: The utility metric measures how close the synthetic data distribution is to the original under transport cost
  - **Quick check question**: How does the 1-Wasserstein distance relate to preserving Lipschitz statistics of the data?

## Architecture Onboarding

- **Component map**: Input Data (X) → Private Covariance (Algorithm 2) → Private PCA → Projection → Synthetic Data Subroutine → Add Private Mean → Metric Projection → Output (Y)
- **Critical path**: The chain from private covariance computation through projection to synthetic data generation is the bottleneck; failure here propagates to final output quality
- **Design tradeoffs**: Using Laplacian noise (pure ε-DP) vs Gaussian noise (ε,δ-DP) trades off between stronger privacy guarantees and potentially tighter utility bounds
- **Failure signatures**: Large Wasserstein error indicates either insufficient privacy budget, wrong choice of d', or data not well-approximated by a low-dimensional subspace
- **First 3 experiments**:
  1. Run Algorithm 1 on synthetic data drawn from a known d'-dimensional subspace; verify that E[W1] matches the theoretical bound
  2. Vary the privacy parameter ε and measure the trade-off between utility (Wasserstein distance) and privacy
  3. Test Algorithm 1 on data that does not lie near a low-dimensional subspace; observe degradation in accuracy and validate that the error term Σi>d' σi(M) dominates

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal rate for the private PCA step without relying on the Davis-Kahan theorem or assuming a spectral gap? The paper mentions that the accuracy bound for the private PCA step has a term involving (εn)^(-1/d') which may not be optimal. It also states that the analysis of private PCA works without using the Davis-Kahan theorem that requires a spectral gap.

### Open Question 2
Can the algorithm be extended to datasets lying on general low-dimensional manifolds beyond linear subspaces? The paper focuses on datasets lying in low-dimensional affine subspaces and improves the accuracy rate from (εn)^(-1/(d'+1)) in prior work to (εn)^(-1/d'). However, it mentions that it would be interesting to provide algorithms with optimal accuracy rates for datasets from general low-dimensional manifolds beyond the linear setting.

### Open Question 3
What is the optimal accuracy rate for generating synthetic data when d' = 1 without additional assumptions on the data? The paper states that the main theorem is not applicable when d' = 1 because the projection error bound does not match the optimal synthetic data accuracy bound. It provides a result with an additional dependence on σ1(M) but notes that it would be interesting to achieve the optimal rate without this dependence.

## Limitations
- The algorithm critically depends on the assumption that data lies in or near a low-dimensional affine subspace
- The Laplacian noise variance choice of 3d²/(εn) for the covariance matrix is presented without rigorous derivation of optimality
- Implementation details of the consistency step in PMM and linear programming formulation in PSMM are only briefly outlined

## Confidence
- **High confidence**: Core privacy guarantees of Algorithm 2 and composition of privacy budgets across steps
- **Medium confidence**: Theoretical utility bounds for PMM and PSMM given abstract descriptions of subroutines
- **Low confidence**: Practical performance on datasets that don't perfectly fit low-dimensional subspace assumptions

## Next Checks
1. Implement and benchmark PMM and PSMM subroutines separately to quantify the impact of unspecified consistency and linear programming steps on overall utility
2. Conduct experiments with synthetic data that gradually deviates from a low-dimensional subspace to identify the breaking point where theoretical guarantees fail
3. Compare Laplacian noise-based private PCA against Gaussian noise alternatives to verify if the stated variance choice (3d²/(εn)) is indeed near-optimal for the stated utility metrics