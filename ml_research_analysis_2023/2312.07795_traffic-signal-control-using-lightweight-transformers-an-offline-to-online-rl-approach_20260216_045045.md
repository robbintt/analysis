---
ver: rpa2
title: 'Traffic Signal Control Using Lightweight Transformers: An Offline-to-Online
  RL Approach'
arxiv_id: '2312.07795'
source_url: https://arxiv.org/abs/2312.07795
tags:
- traffic
- dtlight
- learning
- control
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DTLight is a lightweight Decision Transformer-based method for
  traffic signal control that addresses the high computational requirements and low
  sample efficiency of existing RL-based controllers. It employs knowledge distillation
  to create a compact student model from a larger teacher model and uses adapter modules
  for efficient online fine-tuning with minimal computational cost.
---

# Traffic Signal Control Using Lightweight Transformers: An Offline-to-Online RL Approach

## Quick Facts
- **arXiv ID**: 2312.07795
- **Source URL**: https://arxiv.org/abs/2312.07795
- **Reference count**: 15
- **Primary result**: DTLight achieves up to 42.6% delay reduction in traffic signal control using lightweight transformers with knowledge distillation and adapter-based online fine-tuning

## Executive Summary
DTLight introduces a novel Decision Transformer-based approach for traffic signal control that addresses the computational efficiency challenges of existing RL methods. The method combines knowledge distillation to create compact student models from larger teacher models, and adapter modules for efficient online fine-tuning. By leveraging pre-trained knowledge and minimal online adaptation, DTLight achieves superior performance on both synthetic and real-world traffic scenarios while maintaining practical deployment requirements.

## Method Summary
DTLight employs a Decision Transformer architecture trained using knowledge distillation, where a larger teacher model is first trained on an offline dataset (DTRL) and then distilled into a smaller student model. For online deployment, COMPACTER++ adapter modules are inserted after feed-forward layers in each Transformer block, allowing efficient fine-tuning by updating only adapter parameters while keeping the main model frozen. The method uses return-to-go (RTG) scaling for trajectory augmentation during online learning, initialized based on maximum returns from the offline dataset.

## Key Results
- Achieves up to 42.6% reduction in average delay compared to state-of-the-art online RL methods
- Demonstrates 2.7x reduction in computational requirements compared to full fine-tuning approaches
- Shows consistent performance improvements across both synthetic scenarios (up to 4x4 grids) and real-world testbeds (Cologne, Ingolstadt)

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation
- Claim: Knowledge distillation enables a compact student model to match or exceed the performance of a larger teacher model while reducing computational overhead
- Mechanism: Teacher DTLight model learns high-quality traffic signal control policies, then student model mimics teacher's action distributions using distillation loss and hard loss
- Core assumption: Teacher model's learned representations are generalizable enough to be effectively distilled without losing critical decision-making capabilities
- Evidence anchors: Abstract mentions knowledge distillation for lightweight controllers; section describes gap between large models and affordable models
- Break condition: If teacher model overfits to training data or learns idiosyncratic patterns not representative of real traffic dynamics

### Mechanism 2: Adapter-Based Online Fine-Tuning
- Claim: Adapter modules enable efficient online fine-tuning with minimal computational cost by updating only small, task-specific components
- Mechanism: COMPACTER++ adapters inserted after feed-forward layers; only adapter parameters updated during online fine-tuning while main Transformer parameters remain frozen
- Core assumption: Pre-trained Transformer backbone captures general traffic signal control patterns adaptable through small modifications
- Evidence anchors: Abstract mentions adapters for minimal computation during real deployment; section describes only updating adapter modules during online adaptation
- Break condition: If pre-trained model's representations are too specific to offline data distribution and cannot be adapted with small modifications

### Mechanism 3: RTG Scaling for Offline-to-Online Learning
- Claim: Offline-to-online learning with RTG scaling effectively bridges gap between pre-training on historical data and adapting to real-time traffic conditions
- Mechanism: Trajectories augmented with RTG values initialized as product of maximum return values in offline dataset and scale factor
- Core assumption: Offline dataset contains sufficient diversity and quality to provide good initialization for online learning
- Evidence anchors: Section describes initializing RTG values as product of maximum return values and scale factor
- Break condition: If offline dataset is biased or lacks diversity, initial RTG values may lead model in suboptimal directions

## Foundational Learning

- **Concept**: Reinforcement Learning and Markov Decision Processes
  - Why needed here: Traffic signal control formulated as MDP where agent takes actions based on states to maximize cumulative rewards
  - Quick check question: What are the key components of an MDP and how do they map to traffic signal control problem?

- **Concept**: Transformer Architecture and Self-Attention
  - Why needed here: Decision Transformer uses self-attention to model sequential dependencies in traffic patterns over time
  - Quick check question: How does self-attention in Transformers help capture temporal relationships in traffic flow data?

- **Concept**: Knowledge Distillation
  - Why needed here: Transfers learned policy from large teacher model to smaller student model, reducing computational requirements
  - Quick check question: What is the difference between hard targets and soft targets in knowledge distillation, and why are both used in DTLight?

## Architecture Onboarding

- **Component map**: Recent K-step states → Transformer layers → Adapter modules → Action distribution → Environment → Reward → RTG update → New state
- **Critical path**: State → Transformer layers → Adapter modules → Action distribution → Environment → Reward → RTG update → New state
- **Design tradeoffs**:
  - Model size vs. performance: Larger models perform better but are computationally expensive
  - Pre-training vs. online adaptation: More pre-training provides better initialization but may reduce flexibility
  - RTG initialization: Higher RTG scale factors provide stronger initial signals but may slow learning
- **Failure signatures**:
  - No improvement in online fine-tuning: Check if adapter modules are properly connected and if RTG initialization is appropriate
  - Poor pre-training performance: Verify knowledge distillation loss weights and ensure teacher model is well-trained
  - Inconsistent results across seeds: Check data preprocessing and ensure proper randomization in training
- **First 3 experiments**:
  1. Verify basic functionality: Run DTLight with small, fixed dataset and check if it can learn simple policy without adapters or knowledge distillation
  2. Test knowledge distillation: Train teacher model, then student model with varying distillation loss weights, and compare performance on validation set
  3. Validate adapter effectiveness: Take pre-trained student model, apply online fine-tuning with adapters, and measure performance improvement over few episodes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DTLight's performance scale with larger road networks beyond tested 4x4 grid and avenues scenarios?
- Basis in paper: Experiments only test up to 4x4 grid networks, suggesting scalability could be concern for larger real-world networks
- Why unresolved: Paper does not provide experimental results for larger network sizes
- What evidence would resolve it: Empirical results demonstrating DTLight's performance on significantly larger network sizes, comparing delay metrics and computational efficiency

### Open Question 2
- Question: What are long-term stability and adaptability characteristics when deployed in dynamic traffic environments with frequent changes in traffic patterns?
- Basis in paper: Paper mentions online fine-tuning capabilities but does not discuss long-term stability or performance under frequently changing traffic conditions
- Why unresolved: Experiments focus on short-term performance improvements without addressing how system adapts to evolving traffic patterns over extended periods
- What evidence would resolve it: Longitudinal studies tracking DTLight's performance over months or years in real-world deployments

### Open Question 3
- Question: How does knowledge distillation process affect DTLight's ability to handle edge cases or rare traffic scenarios not well-represented in training data?
- Basis in paper: While paper discusses knowledge distillation for model compression, it does not specifically address how this affects handling of rare or unusual traffic situations
- Why unresolved: Focus on model efficiency through distillation may potentially compromise model's ability to generalize to edge cases not seen in training data
- What evidence would resolve it: Comparative analysis of DTLight's performance on synthetic edge case scenarios versus original teacher model

## Limitations

- Dataset generalization concerns due to reliance on specific behavior policies (EMP and IDQN) in DTRL dataset generation
- Scalability questions for larger networks beyond tested 4x4 grid scenarios
- Lack of discussion on real-world deployment constraints such as communication delays and sensor noise

## Confidence

**High Confidence**: Core mechanisms of knowledge distillation for model compression and adapter-based online fine-tuning are well-established techniques with robust empirical results across multiple random seeds.

**Medium Confidence**: Effectiveness of RTG initialization strategy and specific architectural choices are well-supported by ablation studies but may vary depending on specific traffic characteristics of deployment environments.

**Low Confidence**: Claims about computational efficiency in real-world deployment scenarios are based on simulation metrics and may not fully capture actual hardware and network constraints.

## Next Checks

1. **Out-of-distribution testing**: Evaluate DTLight on traffic scenarios with significantly different characteristics (rush hour patterns, special events, accidents) not represented in DTRL dataset to assess generalization capabilities.

2. **Scalability stress test**: Implement DTLight on larger network topologies (8x8 grids or real urban networks with 50+ intersections) to verify computational efficiency claims hold at scale and identify potential bottlenecks.

3. **Robustness evaluation**: Test DTLight under realistic deployment conditions including communication delays (100-500ms), partial state observability (missing sensors), and noisy observations to assess practical viability.