---
ver: rpa2
title: Unexpected Improvements to Expected Improvement for Bayesian Optimization
arxiv_id: '2310.20708'
source_url: https://arxiv.org/abs/2310.20708
tags:
- optimization
- function
- acquisition
- functions
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses numerical optimization challenges in Expected
  Improvement (EI) and related acquisition functions used in Bayesian optimization.
  The core issue is that EI and its variants suffer from vanishing gradients, making
  them difficult to optimize numerically as the number of observations grows.
---

# Unexpected Improvements to Expected Improvement for Bayesian Optimization

## Quick Facts
- arXiv ID: 2310.20708
- Source URL: https://arxiv.org/abs/2310.20708
- Reference count: 40
- Primary result: LogEI reformulates EI and related acquisition functions in log-space with fat-tailed smooth approximations, achieving superior numerical stability and performance across diverse benchmarks, including high-dimensional, constrained, and multi-objective problems.

## Executive Summary
This paper addresses numerical optimization challenges in Expected Improvement (EI) and related acquisition functions used in Bayesian optimization. The core issue is that EI and its variants suffer from vanishing gradients, making them difficult to optimize numerically as the number of observations grows. To solve this, the authors propose LogEI, a family of acquisition functions that reformulate EI and related functions (e.g., Expected Hypervolume Improvement) to avoid numerical pathologies. This is achieved by transforming computations into log-space and using smooth approximations with fat-tailed decay for gradients. Experiments show that LogEI and its variants significantly outperform their canonical counterparts and match or exceed the performance of recent state-of-the-art methods across diverse benchmarks, including high-dimensional, constrained, and multi-objective problems. Notably, LogEI enables effective joint batch optimization in parallel BO, improving sample efficiency and optimization robustness.

## Method Summary
The paper introduces LogEI, a family of acquisition functions that reformulate Expected Improvement (EI) and related functions (qEI, qEHVI) to avoid numerical pathologies by transforming computations into log-space. This approach addresses vanishing gradients and underflow issues that arise when optimizing EI for large numbers of observations. The method uses fat-tailed smooth approximations (fat softplus, fat maximum, fat sigmoid) that decay polynomially as x → -∞, moderating the dynamic range and maintaining gradient information. The framework is extended to Monte Carlo variants and enables effective joint batch optimization, which can outperform traditional sequential greedy approaches. The method is evaluated across diverse benchmarks including high-dimensional problems, constrained optimization, and multi-objective scenarios.

## Key Results
- LogEI significantly outperforms canonical EI variants across diverse benchmarks including high-dimensional, constrained, and multi-objective problems
- Joint batch optimization with LogEI improves sample efficiency and optimization robustness compared to sequential greedy methods
- LogEI matches or exceeds performance of recent state-of-the-art methods like GIBBON and JES across multiple test scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transforming EI computations into log-space avoids numerical underflow and preserves meaningful gradients.
- Mechanism: By reformulating EI and its variants (qEI, qEHVI) in log-space, the acquisition function values and gradients remain numerically stable even when the original formulation would underflow to zero. This allows gradient-based optimization to proceed effectively.
- Core assumption: The mathematical optima of the log-transformed acquisition function correspond to those of the original function.
- Evidence anchors:
  - [abstract] The paper states that LogEI reformulates EI and related functions to avoid numerical pathologies by transforming computations into log-space.
  - [section] The paper provides a stable implementation of log_h for analytic EI and extends this approach to Monte Carlo variants like qLogEI and qLogEHVI.
  - [corpus] Related work like "Expected Improvement via Gradient Norms" and "A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization" suggest interest in addressing EI's numerical issues, though they do not propose log-space reformulations.
- Break condition: If the log transformation does not preserve the mathematical optima of the original acquisition function, the reformulated method may not perform as expected.

### Mechanism 2
- Claim: Using smooth approximations with fat-tailed decay (e.g., φ+ and φmax) prevents vanishing gradients for large batch sizes.
- Mechanism: Regular softplus approximations can still have a high dynamic range for large batches, leading to vanishing gradients. The proposed fat-tailed functions (φ+ and φmax) decay as O(1/x²) instead of O(exp(x)), moderating the dynamic range and maintaining gradient information.
- Core assumption: The fat-tailed smooth approximations maintain the key properties of the original functions (monotonicity, convexity, positivity) while providing better numerical behavior.
- Evidence anchors:
  - [section] The paper introduces fat softplus (φ+) and fat maximum (φmax) functions that decay polynomially as x → -∞, addressing the issue of vanishing gradients for large batches.
  - [section] Lemma 4 proves the monotonicity and convexity of φ+, and Lemma 5 bounds the approximation error of φmax.
  - [corpus] Related work like "Simulated Annealing-based Candidate Optimization for Batch Acquisition Functions" suggests interest in improving batch acquisition function optimization, but does not propose fat-tailed smooth approximations.
- Break condition: If the fat-tailed approximations do not maintain the desired properties (e.g., monotonicity, convexity) or if the approximation error becomes too large, the optimization performance may suffer.

### Mechanism 3
- Claim: Joint batch optimization can outperform sequential greedy optimization, especially for larger batch sizes.
- Mechanism: The paper shows that by using the proposed log-space formulations and fat-tailed smooth approximations, joint optimization of batch acquisition functions (qLogEI, qLogEHVI) can achieve better performance than the traditional sequential greedy approach.
- Core assumption: The improved numerical stability and gradient information provided by the proposed methods make joint batch optimization feasible and effective.
- Evidence anchors:
  - [abstract] The paper states that LogEI enables effective joint batch optimization in parallel BO, improving sample efficiency and optimization robustness.
  - [section] Figure 5 shows that joint optimization with qLogEI outperforms sequential greedy optimization on the Ackley function for various batch sizes.
  - [corpus] Related work like "A Simple and Efficient Approach to Batch Bayesian Optimization" suggests interest in improving batch BO, but does not propose joint optimization as a viable alternative to sequential greedy methods.
- Break condition: If the problem structure or constraints make joint batch optimization infeasible (e.g., non-separable constraints across batch elements), the proposed method may not be applicable.

## Foundational Learning

- Concept: Bayesian Optimization (BO)
  - Why needed here: The paper proposes improvements to acquisition functions used in BO, so understanding the BO framework is essential.
  - Quick check question: What are the main components of a BO algorithm, and how do acquisition functions fit into the overall framework?

- Concept: Gaussian Processes (GPs)
  - Why needed here: The paper uses GPs as surrogate models for the objective function, and the proposed methods rely on the GP's mean and variance predictions.
  - Quick check question: How do GPs provide probabilistic predictions for the objective function, and what role do the mean and variance play in acquisition function computation?

- Concept: Numerical stability and floating-point arithmetic
  - Why needed here: The paper addresses numerical issues arising from underflow and vanishing gradients in acquisition function computation, which are related to floating-point precision limits.
  - Quick check question: What are the main challenges in numerical computation with floating-point numbers, and how can transformations like log-space help mitigate these issues?

## Architecture Onboarding

- Component map: GP surrogate model -> Acquisition function (LogEI variants) -> Optimization method (L-BFGS-B, SLSQP) -> Objective function evaluation

- Critical path:
  1. Fit a GP surrogate model to the observed data.
  2. Compute the acquisition function value and gradient for a given input.
  3. Optimize the acquisition function using a gradient-based method.
  4. Evaluate the objective function at the recommended input.
  5. Update the GP model with the new observation.
  6. Repeat steps 2-5 until convergence or budget exhaustion.

- Design tradeoffs:
  - Analytic vs. Monte Carlo acquisition functions: Analytic methods (e.g., EI) are faster but may have numerical issues; Monte Carlo methods (e.g., qEI) are more flexible but computationally expensive.
  - Sequential vs. joint batch optimization: Sequential methods are simpler but may be suboptimal; joint methods can be more efficient but require solving a higher-dimensional optimization problem.
  - Number of initial starting points: More points can help avoid local optima but increase computational cost.

- Failure signatures:
  - Vanishing gradients: Acquisition function values and gradients become numerically zero, preventing effective optimization.
  - Poor model fit: The GP surrogate model does not accurately capture the objective function's behavior, leading to suboptimal recommendations.
  - Constraint violation: The recommended inputs do not satisfy the problem's constraints, requiring rejection or modification.

- First 3 experiments:
  1. Implement the log-space reformulation for analytic EI and compare its performance to the original EI on a simple test function (e.g., Sphere or Rosenbrock).
  2. Extend the log-space reformulation to Monte Carlo acquisition functions (e.g., qEI, qEHVI) and evaluate its impact on optimization performance.
  3. Compare the performance of joint batch optimization with the proposed methods to sequential greedy optimization on a parallel BO benchmark (e.g., Ackley or Levy functions).

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The method relies on smooth approximations of max operations whose approximation quality may degrade in high-dimensional spaces or with extremely large batch sizes
- Current implementation focuses on standard GP models with Matern kernels, with performance on alternative surrogate models unexplored
- Computational overhead from log-space transformations and multiple restarts could impact scalability for expensive objective functions

## Confidence
- **High confidence**: The numerical stability improvements from log-space reformulation are mathematically sound and clearly demonstrated through both theory and experiments
- **Medium confidence**: The performance gains over recent state-of-the-art methods (GIBBON, JES) are compelling but may depend on specific implementation details and hyperparameter choices not fully disclosed
- **Medium confidence**: The claim that joint batch optimization consistently outperforms sequential methods, while supported by experiments, may be problem-dependent and less generalizable to constrained or noisy optimization scenarios

## Next Checks
1. **Ablation study**: Systematically vary temperature parameters for smooth approximations (φ+, φmax) to quantify their impact on optimization performance and identify potential overfitting to specific values.

2. **Alternative surrogate models**: Test LogEI with non-GP surrogates (e.g., random forests, neural networks) to assess robustness beyond the Gaussian process assumption.

3. **Scaling analysis**: Evaluate LogEI's performance and computational overhead on problems with batch sizes exceeding those tested (n > 10) to understand practical scalability limits.