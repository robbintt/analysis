---
ver: rpa2
title: '"I''d Like to Have an Argument, Please": Argumentative Reasoning in Large
  Language Models'
arxiv_id: '2309.16938'
source_url: https://arxiv.org/abs/2309.16938
tags:
- reasoning
- argument
- passage
- output
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates GPT-3 and GPT-4 on argument mining (AM) and
  argument pair extraction (APE) tasks. The authors experiment with progressively
  more abstract input/output representations (e.g., line numbers, BIO tags, AMR graphs)
  and varying numbers of exemplars.
---

# "I'd Like to Have an Argument, Please": Argumentative Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2309.16938
- Source URL: https://arxiv.org/abs/2309.16938
- Authors: 
- Reference count: 23
- Key outcome: GPT-4 matches or exceeds state-of-the-art on argument pair extraction and approaches it on argument mining, but performance is highly sensitive to input/output representations, suggesting pattern-matching rather than true reasoning.

## Executive Summary
This paper evaluates GPT-3 and GPT-4 on argument mining (AM) and argument pair extraction (APE) tasks using the RRv2 dataset. The authors systematically test how different input/output representations (concrete text, symbolic formats like BIO tags and line numbers, and AMR graphs) and varying numbers of exemplars affect model performance. They find that GPT-4 achieves competitive results, particularly when using chain-of-thought prompting. However, the models' performance is highly sensitive to minor changes in how tasks are represented, suggesting they rely on pattern-matching rather than genuine argumentative reasoning. An "exemplar effect" shows performance peaks at 4-5 exemplars and decreases with more, except under chain-of-thought prompting.

## Method Summary
The study uses in-context learning with GPT-3 (text-davinci-003) and GPT-4 APIs on the RRv2 dataset containing 4,764 review-rebuttal pairs. Five evaluation settings are tested: concrete representations (full text output), symbolic representations (BIO tags, line indices, matrices), AMR graphs, with and without chain-of-thought prompting. Exemplar counts vary from 0 to maximum context length (0, 4, 8, 16, τ). Performance is measured using binary F1 for APE and micro-F1 for AM, with five runs per configuration for statistical analysis.

## Key Results
- GPT-4 matches or exceeds state-of-the-art performance on argument pair extraction and approaches it on argument mining
- Performance is highly sensitive to input/output representations, with minor changes causing significant performance differences
- An "exemplar effect" shows performance peaks at 4-5 exemplars and decreases with more, except under chain-of-thought prompting
- Chain-of-thought prompting improves performance and mitigates some representation sensitivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs' argumentative reasoning performance is highly sensitive to input/output representations
- Mechanism: The models rely on pattern-matching rather than true reasoning, so minor changes in representation (e.g., line numbers vs BIO tags) significantly alter their ability to extract arguments correctly
- Core assumption: The models are not performing genuine argumentative reasoning but rather matching patterns in the input to expected outputs
- Evidence anchors:
  - [abstract] "However, statistical analysis on the LLMs outputs when subject to small, yet still human-readable, alterations in the I/O representations (e.g., asking for BIO tags as opposed to line numbers) showed that the models are not performing reasoning"
  - [section] "When altering the input and output representation with conceptually minor changes–such as adding line numbers or requesting a BIO set as opposed to integers–the models had noticeably different performances. We argue that this is indicative of a potential false positive: the models are unable to reason in an argumentative setting, but give excellent appearance of being able to do so"
- Break condition: If the model were truly reasoning, performance should be more consistent across equivalent representations

### Mechanism 2
- Claim: Chain-of-thought prompting mitigates representation sensitivity in argumentative reasoning tasks
- Mechanism: CoT provides a templatized reasoning framework that helps the model follow consistent logical steps regardless of how the input/output is formatted
- Core assumption: The templated nature of CoT overrides the model's reliance on surface-level pattern matching
- Evidence anchors:
  - [abstract] "However, statistical analysis on the LLMs outputs when subject to small, yet still human-readable, alterations in the I/O representations... showed that the models are not performing reasoning. This suggests that LLM applications to some tasks, such as data labelling and paper reviewing, must be done with care"
  - [section] "That said, we noticed that this performance difference did not extend to the inclusion of CoT. These prompts had on average better performance than their non-CoT counterparts, and we observed that it yielded better results in ill-posted (here, overly abstract) problems"
- Break condition: If CoT were only providing surface-level guidance rather than enabling deeper reasoning, representation sensitivity would persist

### Mechanism 3
- Claim: There is an "exemplar effect" where performance peaks at 4-5 exemplars and decreases with more
- Mechanism: Too many exemplars introduce noise or conflicting patterns that overwhelm the model's ability to extract the relevant task structure
- Core assumption: The model's in-context learning has an optimal range where exemplars provide helpful guidance without causing confusion
- Evidence anchors:
  - [abstract] "We also find an 'exemplar effect', where too many exemplars increasingly become detrimental for task performance, and about 4 − 5 being the optimal amount"
  - [section] "On average, the LLMs performance peaked at 4 exemplars, and steadily decreased as the exemplars became more numerous. The exception to this observation was CoT prompting"
- Break condition: If the model were truly learning the task structure, more exemplars should monotonically improve performance

## Foundational Learning

- Concept: Argumentative reasoning
  - Why needed here: The paper evaluates LLMs on argument mining and argument pair extraction tasks, which require understanding argumentative structure
  - Quick check question: What distinguishes argumentative reasoning from other forms of reasoning in natural language processing?

- Concept: Input/output representation abstraction
  - Why needed here: The paper systematically varies how tasks are represented (concrete vs symbolic, with different label schemes) to test model reasoning capabilities
  - Quick check question: How does increasing abstraction in input/output representations test for genuine reasoning versus pattern matching?

- Concept: Chain-of-thought prompting
  - Why needed here: CoT is used as a technique to potentially improve reasoning performance and mitigate representation sensitivity
  - Quick check question: What is the hypothesized mechanism by which chain-of-thought prompting improves reasoning in LLMs?

## Architecture Onboarding

- Component map: Data preparation (RRv2 corpus) → Prompt engineering (varying representations and exemplar counts) → Model inference (GPT-3/GPT-4 APIs) → Result parsing (F1 scoring) → Statistical analysis
- Critical path: Prompt engineering → Model inference → Result parsing → Statistical analysis
- Design tradeoffs: Using few-shot prompting with exemplars vs. fine-tuning; testing representation sensitivity vs. focusing on absolute performance; including CoT vs. baseline prompting
- Failure signatures: Performance sensitivity to minor representation changes; non-monotonic relationship between exemplar count and performance; significant performance gap between tasks (AM vs APE)
- First 3 experiments:
  1. Test AM and APE performance with concrete representations (full text output) vs symbolic representations (line numbers or BIO tags)
  2. Vary the number of exemplars from 0 to maximum context length to identify the "sweet spot" for in-context learning
  3. Test AMR graph representations as the most abstract input format to evaluate model performance on highly abstract representations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed "exemplar effect" generalize to other large language models beyond GPT-3 and GPT-4?
- Basis in paper: Inferred - The paper notes that the exemplar effect was observed in both GPT-3 and GPT-4, but acknowledges that their results may not extend to other LLMs.
- Why unresolved: The study only evaluated two models, so the effect's generalizability to other models is unknown.
- What evidence would resolve it: Testing the exemplar effect on a broader range of LLMs with varying architectures and training regimes.

### Open Question 2
- Question: How does fine-tuning or specialized pre-training affect the input/output representation sensitivity observed in argumentative reasoning tasks?
- Basis in paper: Explicit - The paper mentions that fine-tuning or specialized pre-training could provide insights and potentially lead to more uniform performance across representations.
- Why unresolved: The study treated the models as generalists performing in-context learning, without fine-tuning or specialized pre-training.
- What evidence would resolve it: Comparing the performance of fine-tuned or specialized models on the same tasks and representations used in the study.

### Open Question 3
- Question: What is the underlying mechanism by which chain-of-thought prompting mitigates the input/output representation sensitivity in argumentative reasoning tasks?
- Basis in paper: Inferred - The paper notes that CoT mitigates the effect of input/output representations on task execution, but does not provide a detailed explanation of the underlying mechanism.
- Why unresolved: The study does not explore the specific reasons why CoT improves performance under ill-conditioned problems.
- What evidence would resolve it: Analyzing the intermediate steps and reasoning patterns in CoT responses to identify the key factors contributing to improved performance.

## Limitations
- Performance sensitivity to input/output representations suggests pattern-matching rather than true reasoning
- The "exemplar effect" mechanism remains unclear, with non-monotonic performance relationships
- Limited to two LLM models (GPT-3 and GPT-4), limiting generalizability

## Confidence
- Representation sensitivity findings: High
- Exemplar effect claims: Medium
- "Not performing reasoning" conclusion: Medium

## Next Checks
1. **Direct reasoning test**: Implement a targeted experiment comparing model performance on logically equivalent representations that differ only in superficial formatting (e.g., word order, punctuation) to isolate whether representation sensitivity stems from genuine reasoning limitations or surface-level pattern matching.

2. **Cross-task transfer**: Evaluate whether models that show strong performance on one argumentative reasoning task (e.g., AM) can transfer that capability to structurally similar but superficially different tasks, which would indicate genuine reasoning ability rather than memorization.

3. **Ablation of contextual cues**: Design prompts that systematically remove potentially helpful contextual cues (citation markers, formatting conventions, discourse markers) while maintaining semantic equivalence to determine which aspects of representation most strongly influence performance.