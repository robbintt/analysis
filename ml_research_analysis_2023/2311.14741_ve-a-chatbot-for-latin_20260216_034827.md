---
ver: rpa2
title: '@ve: A Chatbot for Latin'
arxiv_id: '2311.14741'
source_url: https://arxiv.org/abs/2311.14741
tags:
- latin
- chatbot
- language
- knowledge
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the development and evaluation of @ve, a chatbot
  for Latin based on GPT-3.0, aimed at preserving and promoting the dead language
  through conversational interaction. The chatbot was equipped with a manually created
  knowledge base and integrated into a website and Telegram.
---

# @ve: A Chatbot for Latin

## Quick Facts
- arXiv ID: 2311.14741
- Source URL: https://arxiv.org/abs/2311.14741
- Reference count: 21
- Primary result: Expert evaluation finds @ve has educational potential but requires teacher supervision due to grammatical and spelling errors

## Executive Summary
This paper presents the development and evaluation of @ve, a chatbot for Latin based on GPT-3.0, aimed at preserving and promoting the dead language through conversational interaction. The chatbot was equipped with a manually created knowledge base and integrated into a website and Telegram. An expert evaluation revealed that while @ve has potential for use in language education, it currently lacks error correction and is prone to grammatical mistakes, making it unsuitable for standalone use without teacher supervision. The project demonstrates the feasibility of using conversational AI for language preservation, but highlights the need for improved accuracy and feedback mechanisms before deployment in educational settings.

## Method Summary
The @ve chatbot was developed using GPT-3.0 via the Quickchat platform, with a manually created knowledge base sourced from Wikipedia. The chatbot was integrated into a website and Telegram for accessibility. An expert evaluation by a Latin specialist assessed the chatbot's conversational ability and educational potential, focusing on its use in language education and preservation contexts.

## Key Results
- Expert evaluation found @ve has educational potential but requires teacher supervision due to grammatical and spelling errors.
- The chatbot was successfully integrated into both a website and Telegram, increasing accessibility.
- Current implementation is too prone to glitches for stand-alone use without teacher accompaniment.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A manually created knowledge base improves a GPT-3-based chatbot's ability to answer domain-specific questions about Latin grammar and history.
- Mechanism: The knowledge base supplies structured, curated facts that supplement GPT-3's general training, allowing the chatbot to retrieve and reproduce domain-specific information more reliably.
- Core assumption: GPT-3's general training includes Latin but lacks the targeted accuracy and specificity that curated educational content can provide.
- Evidence anchors:
  - [abstract] "The chatbot was equipped with a manually created knowledge base."
  - [section 5.4] "The knowledge base... allows targeted extensions. The manually created knowledge base gives the chatbot a broader understanding of the history, grammar, and culture of the Latin language."
- Break condition: If the knowledge base is poorly curated, outdated, or not aligned with the chatbot's retrieval logic, it will not enhance accuracy and may even degrade responses.

### Mechanism 2
- Claim: Integrating a chatbot with a website and a messenger service increases accessibility and potential user engagement.
- Mechanism: By providing multiple access points (web interface and Telegram), users can interact with the chatbot in their preferred environment, lowering barriers to entry.
- Core assumption: Users are more likely to engage with educational tools when access is convenient and familiar (e.g., via a browser or a commonly used messaging app).
- Evidence anchors:
  - [section 4.1] "The chatbot was published on the website... made accessible to an interested audience."
  - [section 4.4] "The connection to Telegram was done as an optional step... messengers are a common and indispensable application for many people."
- Break condition: If the interface is not user-friendly or if the messaging integration introduces technical issues, users may disengage regardless of accessibility.

### Mechanism 3
- Claim: Supervised use in a classroom setting mitigates the chatbot's grammatical and spelling error issues.
- Mechanism: A teacher can monitor chat logs, provide corrections, and reinforce learning, compensating for the chatbot's lack of error detection.
- Core assumption: Active teacher involvement can identify and correct errors that the chatbot fails to catch, thus maintaining educational quality.
- Evidence anchors:
  - [abstract] "The present implementation is still too prone to glitches for stand-alone use – i.e., without the accompaniment of a teacher."
  - [section 6] "the chatbot does not correct users, which can lead to their own mistakes being deepened without this being noticed."
- Break condition: If teacher oversight is inconsistent or absent, learners may internalize incorrect grammar or spelling.

## Foundational Learning

- Concept: Language model pre-training and fine-tuning
  - Why needed here: Understanding how GPT-3's general training differs from a specialized knowledge base helps explain the chatbot's strengths and limitations.
  - Quick check question: What is the difference between pre-training a language model on large corpora and fine-tuning it with a specific knowledge base?

- Concept: Chatbot deployment channels (web vs. messenger)
  - Why needed here: Knowing the trade-offs between hosting a chatbot on a website versus integrating it with a messenger service informs design and accessibility decisions.
  - Quick check question: What are the main benefits and challenges of deploying a chatbot via a web interface compared to a messaging app?

- Concept: Error detection and correction in conversational AI
  - Why needed here: Recognizing that GPT-3 lacks built-in grammar correction explains why human supervision is necessary for language learning applications.
  - Quick check question: Why is it problematic for a language learning chatbot to lack error correction, and how can this be addressed?

## Architecture Onboarding

- Component map:
  - Frontend: Website (HTML/JavaScript) with chat widget
  - Backend: Quickchat platform hosting the GPT-3 model
  - Integration: JavaScript snippet embedded in the website; Telegram bot token for messenger connection
  - Knowledge Base: Curated text corpus (max 140 characters per entry) uploaded to Quickchat
  - Data Flow: User input → Chat widget → Quickchat API → GPT-3 + Knowledge Base → Response → Display

- Critical path:
  1. User opens website and clicks chat widget
  2. Chat widget initializes via JavaScript
  3. User sends message in Latin/German/English
  4. Message routed to Quickchat API
  5. Quickchat combines GPT-3 output with relevant knowledge base entries
  6. Response returned to chat widget and displayed

- Design tradeoffs:
  - Rule-based vs. AI-based chatbot: Rule-based offers higher accuracy but is less flexible; AI-based is more adaptable but prone to errors
  - Single vs. multiple access channels: Multiple channels increase reach but add integration complexity
  - Creativity setting in Quickchat: Higher creativity yields more varied responses but risks inaccuracies; lower creativity reduces errors but may lead to repetitive or unhelpful replies

- Failure signatures:
  - Chat widget fails to load: JavaScript embedding or Quickchat service down
  - Responses switch unexpectedly to English: Input too short or ambiguous for Latin/German detection
  - Grammatical errors in Latin: GPT-3 generation errors not caught by knowledge base or teacher oversight
  - No response or timeout: API connectivity or rate limiting issues

- First 3 experiments:
  1. Test chatbot response quality with minimal knowledge base (baseline) vs. enriched knowledge base (targeted Latin grammar/history entries).
  2. Evaluate error rate in Latin sentences with and without teacher supervision in a simulated classroom scenario.
  3. Compare user engagement metrics (messages sent, session duration) between web-only access and web + Telegram access.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the use of GPT-4 instead of GPT-3.0 significantly improve the grammatical accuracy and error correction capabilities of the @ve chatbot for Latin?
- Basis in paper: [explicit] The paper suggests that "The use of GPT-4 could be a solution as well as the extension of the knowledge base" to address current limitations.
- Why unresolved: The current implementation uses GPT-3.0, and the authors have not yet tested GPT-4 for this specific application.
- What evidence would resolve it: Comparative testing of @ve with both GPT-3.0 and GPT-4, measuring grammatical accuracy and error correction performance.

### Open Question 2
- Question: Would extending the manually created knowledge base with primary literature by Cicero and Caesar, or literature on Latin grammar, significantly improve the chatbot's ability to provide authoritative answers?
- Basis in paper: [explicit] The authors mention that "in cooperation with publishers, further texts could be included in the knowledge database, such as primary literature by Cicero and Caesar or literature on the grammar of Latin."
- Why unresolved: The current knowledge base is limited, and the authors have not yet explored the impact of expanding it with these additional sources.
- What evidence would resolve it: Implementation and testing of an extended knowledge base, comparing its performance with the current version in terms of accuracy and depth of responses.

### Open Question 3
- Question: How effective would @ve be in a classroom setting when used in conjunction with a teacher who can monitor chat history and provide error correction?
- Basis in paper: [explicit] The expert evaluation suggested that "the chatbot definitely provides good entertainment! It works well" but "is still too glitch prone for stand-alone use" and would benefit from teacher supervision.
- Why unresolved: The paper did not conduct a formal classroom study with teacher supervision.
- What evidence would resolve it: A controlled study in a school or university classroom, comparing learning outcomes with and without teacher supervision of @ve interactions.

## Limitations

- The expert evaluation was conducted by a single Latin specialist, limiting the robustness of the qualitative assessment.
- The knowledge base construction process is not fully specified, making it difficult to replicate or evaluate its impact systematically.
- The study's scope is limited to a single chatbot platform (Quickchat) and a single large language model (GPT-3.0), so results may not generalize to other models or deployment approaches.

## Confidence

- **High confidence**: The chatbot's integration with web and Telegram platforms is technically sound and well-documented. The basic feasibility of using GPT-3.0 with a knowledge base for Latin conversation is demonstrated.
- **Medium confidence**: The expert evaluation indicates the chatbot has educational potential but requires teacher supervision. However, this is based on a single expert's opinion, and error rates are not quantified.
- **Low confidence**: Claims about the specific impact of the knowledge base on accuracy are not supported by direct evidence or controlled experiments comparing chatbot performance with and without the knowledge base.

## Next Checks

1. Conduct a larger-scale expert evaluation with multiple Latin educators and a standardized rubric to quantify error rates in grammar and spelling.
2. Run a controlled experiment comparing chatbot performance (accuracy, error rate, user satisfaction) with and without the knowledge base, using a fixed set of Latin queries.
3. Deploy the chatbot in a real classroom setting for a semester, collecting both teacher feedback and student learning outcomes to validate the supervised use hypothesis.