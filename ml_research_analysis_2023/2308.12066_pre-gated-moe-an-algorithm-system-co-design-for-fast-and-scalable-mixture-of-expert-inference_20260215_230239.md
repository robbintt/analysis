---
ver: rpa2
title: 'Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert
  Inference'
arxiv_id: '2308.12066'
source_url: https://arxiv.org/abs/2308.12066
tags:
- expert
- experts
- memory
- block
- pre-gated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently deploying large-scale
  Mixture-of-Experts (MoE) models, which have high memory requirements and dynamic
  sparse expert activation. The proposed Pre-gated MoE system introduces a novel pre-gate
  function that preemptively selects experts for the next MoE block while the current
  block is being executed.
---

# Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference

## Quick Facts
- arXiv ID: 2308.12066
- Source URL: https://arxiv.org/abs/2308.12066
- Authors: 
- Reference count: 40
- Key outcome: Pre-gated MoE achieves 1.5x-1.6x higher inference throughput than baseline MoE systems while reducing peak GPU memory usage to 23% of GPU-only solutions

## Executive Summary
This paper addresses the challenge of efficiently deploying large-scale Mixture-of-Experts (MoE) models by introducing Pre-gated MoE, a system that decouples expert selection from execution. The key innovation is a pre-gate function that predicts which experts will be activated in the next MoE block while the current block is executing, enabling overlapping CPU-to-GPU expert migration with computation. This approach significantly improves inference throughput and reduces memory requirements, enabling deployment of large-scale LLMs on a single GPU.

## Method Summary
Pre-gated MoE introduces a novel pre-gate function that preemptively selects experts for the next MoE block while the current block executes. The system trains gate functions to predict expert activation one block ahead, allowing selective migration of only activated experts from CPU to GPU. By keeping all MoE parameters on CPU and only loading activated experts to GPU, the system minimizes GPU memory footprint while overlapping communication with computation. The architecture is implemented using NVIDIA's FasterTransformer library and evaluated on SwitchTransformer models across various NLP tasks.

## Key Results
- Achieves 1.5x-1.6x higher inference throughput compared to baseline MoE systems
- Reduces peak GPU memory usage to only 23% of GPU-only solutions
- Maintains comparable model accuracy across Xsum summarization and CB Web QA/SQuAD question answering tasks

## Why This Works (Mechanism)

### Mechanism 1
The pre-gate function allows expert selection for the next MoE block to happen concurrently with the current block's expert execution, enabling overlapping CPU→GPU expert migration with computation. This decouples the two stages across blocks, allowing the system to start transferring experts to GPU while the current block executes. Core assumption: The pre-gate function can accurately predict expert activation patterns one block ahead with sufficient accuracy to avoid major performance penalties.

### Mechanism 2
Only activated experts are migrated from CPU to GPU, reducing migration overhead by 75-99% compared to prefetching all experts. Since MoE models activate only top-1 or top-2 experts out of 128-256 total, the pre-gate function enables selective migration of just the activated experts rather than all possible experts. Core assumption: The fraction of activated experts remains small (typically 0.8-1.6% in SwitchTransformer) across different input patterns.

### Mechanism 3
Pre-gated MoE achieves peak GPU memory usage of only 23% of GPU-only while maintaining 81% of GPU-only throughput. By offloading all MoE parameters to CPU and only keeping activated experts for current and next blocks in GPU memory, the system minimizes GPU memory footprint while overlapping communication with computation. Core assumption: GPU memory is the primary constraint for large model deployment, and CPU memory is sufficiently large to hold all MoE parameters.

## Foundational Learning

- Concept: Sparse expert activation in MoE models
  - Why needed here: Understanding why only a small subset of experts are activated is crucial for grasping the memory and performance benefits of Pre-gated MoE
  - Quick check question: In a SwitchTransformer with 128 experts that activates the top-1 expert, what percentage of experts are actually executed per inference?

- Concept: CPU-GPU data transfer latency and bandwidth
  - Why needed here: The performance gains depend on hiding CPU→GPU migration latency through overlapping with computation
  - Quick check question: If CPU→GPU migration takes 100ms and expert execution takes 200ms, what speedup can be achieved by overlapping these operations?

- Concept: Transformer block architecture and MoE integration
  - Why needed here: Pre-gated MoE modifies the standard MoE block structure, so understanding the baseline is essential
  - Quick check question: What are the two key components of an MoE block, and how does the pre-gate function modify their relationship?

## Architecture Onboarding

- Component map:
  Pre-gate function -> Expert execution unit -> CPU memory (stores all MoE parameters) -> GPU memory (stores non-MoE parameters plus activated experts) -> PCIe interconnect

- Critical path:
  1. Input tokens processed through non-MoE layers
  2. Current block's pre-gate function predicts experts for next block
  3. Predicted experts migrated from CPU to GPU
  4. Current block's experts executed
  5. Process repeats for next block

- Design tradeoffs:
  - Memory vs. performance: More aggressive CPU offloading saves memory but risks CPU→GPU bottleneck
  - Prediction accuracy vs. complexity: More complex pre-gate functions may improve accuracy but increase computation
  - Cache size vs. hit rate: Caching recently used experts can reduce migration but consumes GPU memory

- Failure signatures:
  - High CPU→GPU migration latency relative to expert execution time
  - Pre-gate function prediction errors causing unnecessary expert migrations
  - GPU memory fragmentation preventing efficient expert storage
  - CPU memory bandwidth saturation with large models

- First 3 experiments:
  1. Measure baseline MoE block latency with different activation levels (top-1, top-2, top-4) to establish performance sensitivity
  2. Profile CPU→GPU migration time for different expert sizes to quantify communication overhead
  3. Evaluate pre-gate prediction accuracy across different input patterns to determine reliability thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper limit of performance improvement when increasing the number of activated experts in Pre-gated MoE systems?
- Basis in paper: [inferred] The paper discusses that Pre-gated MoE maintains high performance with sparse expert activation but notes that performance degrades as the number of activated experts increases.
- Why unresolved: The paper provides empirical results for a limited range of activated experts (1-64) but does not explore the full spectrum up to 100% activation.
- What evidence would resolve it: A comprehensive study measuring performance across all possible activation levels (from 1 expert to all experts) would establish the theoretical performance ceiling.

### Open Question 2
- Question: How does Pre-gated MoE's pre-gate function compare to alternative expert selection methods like hash-based routing or load-balanced routing?
- Basis in paper: [explicit] The paper mentions that prior work has explored various gate functions for expert selection but does not directly compare these methods to Pre-gated MoE's approach.
- Why unresolved: The paper focuses on demonstrating Pre-gated MoE's effectiveness against baseline CPU offloading methods rather than alternative expert selection strategies.
- What evidence would resolve it: A head-to-head comparison between Pre-gated MoE and other expert selection methods (hash routing, load-balanced routing) would reveal relative strengths and weaknesses.

### Open Question 3
- Question: What is the optimal number of MoE blocks ahead that the pre-gate function should predict experts for (beyond N=1 and N=2/3 tested)?
- Basis in paper: [explicit] The paper tests pre-gate functions predicting experts for 1, 2, and 3 MoE blocks ahead but notes that accuracy decreases as prediction distance increases.
- Why unresolved: The paper only tests a limited range of prediction distances and does not determine the optimal balance between prediction accuracy and performance benefits.
- What evidence would resolve it: An extensive study testing pre-gate functions predicting experts for varying numbers of MoE blocks ahead (e.g., 1-10) would identify the optimal prediction distance.

## Limitations
- Performance gains are fundamentally tied to pre-gate function prediction accuracy, which is not quantitatively analyzed
- Results are hardware-specific and may not generalize across different CPU memory bandwidths or PCIe generations
- System relies on sparse expert activation assumption, with performance degrading significantly under dense activation patterns

## Confidence
- High confidence: Memory reduction claims (23% of GPU-only peak memory usage) are well-supported by theoretical analysis
- Medium confidence: Throughput improvements (1.5x-1.6x higher) are supported by experimental results but may not generalize across hardware
- Low confidence: Model accuracy preservation claims lack comprehensive validation and detailed ablation studies

## Next Checks
1. Conduct systematic experiments measuring pre-gate function prediction accuracy across different input patterns, sequence lengths, and model scales to determine reliability thresholds
2. Evaluate Pre-gated MoE performance across diverse hardware configurations including different CPU memory bandwidths, PCIe generations, and GPU memory sizes
3. Design experiments that intentionally trigger dense expert activation patterns to quantify performance degradation and identify crossover points with alternative offloading strategies