---
ver: rpa2
title: Complex Claim Verification with Evidence Retrieved in the Wild
arxiv_id: '2305.11859'
source_url: https://arxiv.org/abs/2305.11859
tags:
- claim
- document
- evidence
- retrieval
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a fully automated pipeline for checking real-world
  political claims by retrieving raw evidence from the web. It addresses the challenge
  of fact-checking complex claims using only evidence available at the time the claim
  was made, rather than post-hoc fact-checks.
---

# Complex Claim Verification with Evidence Retrieved in the Wild

## Quick Facts
- arXiv ID: 2305.11859
- Source URL: https://arxiv.org/abs/2305.11859
- Authors: [List not provided]
- Reference count: 33
- Key outcome: Web evidence improves complex claim fact-checking over claim-only baselines

## Executive Summary
This work presents a fully automated pipeline for checking real-world political claims by retrieving raw evidence from the web. It addresses the challenge of fact-checking complex claims using only evidence available at the time the claim was made, rather than post-hoc fact-checks. The pipeline includes five stages: claim decomposition into subquestions, first-stage web retrieval, fine-grained document retrieval, claim-focused summarization, and veracity classification. Experiments on the ClaimDecomp dataset show that using web evidence improves veracity classification over claim-only baselines, and that the retrieved evidence summaries are mostly faithful and helpful for fact-checking, even though they often do not cover all aspects of a claim.

## Method Summary
The pipeline processes complex political claims through five stages: (1) decomposing claims into 10 yes/no subquestions using text-davinci-003, (2) first-stage web retrieval using Bing Search API with temporal and site constraints, (3) second-stage fine-grained retrieval using BM25 to isolate relevant text spans, (4) claim-focused summarization of retrieved documents using text-davinci-003 with few-shot prompting, and (5) veracity classification using DeBERTa-large. The system is evaluated on the ClaimDecomp dataset with human evaluation of summary faithfulness and comprehensiveness.

## Key Results
- Web evidence improves veracity classification accuracy over claim-only baselines
- Retrieved evidence summaries are mostly faithful and helpful for fact-checking
- Temporal and site constraints significantly reduce retrieval performance compared to unconstrained retrieval
- Summaries often do not cover all aspects of complex claims but still aid in veracity judgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex claims into subquestions improves retrieval quality and downstream veracity judgment.
- Mechanism: By breaking a claim into 10 yes/no subquestions, the search engine can more effectively locate relevant documents that address individual facets of the claim rather than a broad, ambiguous query.
- Core assumption: The subquestions are sufficiently specific to yield distinct, relevant documents and are not redundant.
- Evidence anchors:
  - [abstract] "Following the approach of Chen et al. (2022a), we first decompose a claim into a series of subquestions, targeting both explicit and implicit aspects of the claim."
  - [section 5.2] "Using the original claim instead of the generated subquestions as an input to web search ( B⃝ vs. 1⃝) results in a notable decrease in performance."
- Break condition: If subquestions are poorly generated (e.g., not sufficiently decontextualized or overly redundant), retrieval quality will degrade, and veracity classification will suffer.

### Mechanism 2
- Claim: Two-stage retrieval (first-stage raw document retrieval, second-stage fine-grained retrieval) improves precision by isolating the most relevant text spans.
- Mechanism: The first stage gathers broad document sets via web search; the second stage uses BM25 to extract the highest-scoring text spans within those documents, expanding with context to form final evidence.
- Core assumption: Relevant information exists in the documents retrieved in the first stage and can be isolated by BM25 scoring.
- Evidence anchors:
  - [abstract] "Then, we conduct a second stage of fine-grained retrieval to isolate the most relevant portions of the documents."
  - [section 3.3] "Most of the documents collected from the previous step contain only small snippets relevant to the claim, if relevant at all."
- Break condition: If first-stage retrieval fails to return relevant documents (e.g., claim is too niche or fact-checking only possible via fact-checking websites), second-stage retrieval cannot recover missing content.

### Mechanism 3
- Claim: Claim-focused summarization produces faithful and helpful summaries that aid both machines and humans in veracity judgment.
- Mechanism: Large language models (e.g., text-davinci-003) are prompted to summarize each retrieved document in the context of the claim, producing concise evidence snippets for classification.
- Core assumption: The summarization model does not hallucinate information and stays faithful to the source document.
- Evidence anchors:
  - [abstract] "Finally, we use state-of-the-art language models (Brown et al., 2020; Ouyang et al., 2022) to generate claim-focused summaries from the retrieved content."
  - [section 6.1] "Human evaluation finds the evidence summary produced by our system is reliable (it does not hallucinate information) and relevant to answering key questions about a claim."
- Break condition: If the summarization model generates major factual errors or hallucinates content, it will mislead both the classifier and human fact-checkers.

## Foundational Learning

- Concept: Temporal and site constraints in web retrieval
  - Why needed here: To simulate the realistic scenario where a fact-checker must verify a claim using only information available at the time the claim was made, avoiding post-hoc fact-checking evidence.
  - Quick check question: Does your retrieval pipeline filter out documents from fact-checking websites and only include those published before the claim date?

- Concept: Question decomposition for multi-aspect reasoning
  - Why needed here: Complex political claims often have multiple implicit or explicit facets; decomposing them into subquestions enables targeted retrieval of evidence for each facet.
  - Quick check question: Are your generated subquestions sufficiently decontextualized and specific to enable distinct, relevant retrieval results?

- Concept: Faithfulness evaluation of LLM-generated summaries
  - Why needed here: To ensure the summaries used as evidence are accurate and do not introduce hallucinated or misrepresented content that could bias veracity judgment.
  - Quick check question: Does your evaluation protocol distinguish between minor factual errors, major factual errors, and completely wrong hallucinations in the summaries?

## Architecture Onboarding

- Component map: Claim → Subquestions → First-stage Retrieval → Second-stage Retrieval → Summarization → Classification

- Critical path: Claim → Subquestions → First-stage Retrieval → Second-stage Retrieval → Summarization → Classification

- Design tradeoffs:
  - Temporal/site constraints vs. retrieval coverage: Stricter constraints reduce available evidence but increase realism.
  - Number of subquestions vs. redundancy: More subquestions increase coverage but risk redundancy and noise.
  - Summary length vs. classifier capacity: Longer summaries risk truncation in classifier input; shorter summaries may omit critical context.

- Failure signatures:
  - High proportion of unanswerable subquestions → First-stage retrieval insufficient.
  - Major factual errors in summaries → Summarization model hallucinations.
  - No improvement over claim-only baseline → Evidence retrieval ineffective or summaries not helpful.

- First 3 experiments:
  1. Compare veracity classification accuracy using claim-only vs. claim + summaries to verify retrieval improves judgment.
  2. Evaluate faithfulness of zero-shot vs. few-shot summarization prompts via human annotation.
  3. Test impact of removing temporal/site constraints on retrieval coverage and veracity accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the temporal constraint on document retrieval affect the system's ability to fact-check emerging claims in real-time?
- Basis in paper: Explicit
- Why unresolved: The paper notes that the temporal constraint dramatically reduces performance compared to unconstrained retrieval, but doesn't explore whether this is a fundamental limitation or if better retrieval methods could overcome it.
- What evidence would resolve it: Experiments comparing different temporal constraints (e.g., allowing some post-claim documents, varying time windows) and/or developing more sophisticated temporal retrieval methods that could maintain performance while respecting the constraint.

### Open Question 2
- Question: What is the optimal number and type of subquestions to decompose complex claims into for effective fact-checking?
- Basis in paper: Inferred
- Why unresolved: The paper uses 10 subquestions by default but shows that using gold subquestions actually yields worse performance than predicted ones. This suggests the current approach may not be optimal.
- What evidence would resolve it: Systematic experiments varying the number of subquestions (e.g., 5, 10, 15) and their generation method (e.g., different prompts, models, or manual annotation) to find the optimal balance between comprehensiveness and efficiency.

### Open Question 3
- Question: How can the system be improved to better handle claims that require evidence not available on the web?
- Basis in paper: Explicit
- Why unresolved: The paper identifies that some claims cannot be fact-checked because necessary evidence is not available on the web, but doesn't propose solutions beyond acknowledging this limitation.
- What evidence would resolve it: Developing methods to identify claims requiring non-web evidence early in the pipeline, integrating alternative evidence sources (e.g., databases, expert knowledge), or creating a triage system to route such claims to human fact-checkers.

### Open Question 4
- Question: How does the performance of the fact-checking pipeline vary across different domains of political claims?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on complex political claims but doesn't analyze whether performance differs based on claim type, complexity, or domain (e.g., economic vs. health vs. political claims).
- What evidence would resolve it: Annotating claims with domain labels and analyzing system performance across these categories to identify strengths, weaknesses, and potential domain-specific improvements.

### Open Question 5
- Question: What is the impact of different summarization approaches on the faithfulness and comprehensiveness of the generated evidence summaries?
- Basis in paper: Explicit
- Why unresolved: While the paper compares few-shot vs. zero-shot prompting, it doesn't explore other summarization techniques, model variants, or evaluation metrics that could further improve summary quality.
- What evidence would resolve it: Experiments comparing different summarization models (e.g., BART, T5, human-written summaries), techniques (e.g., extractive vs. abstractive), and evaluation metrics (e.g., ROUGE, BERTScore) to identify the most effective approach.

## Limitations

- Search engine results vary over time, affecting reproducibility and consistency of retrieved evidence
- Temporal and site constraints limit evidence availability, especially for recent or obscure claims
- LLM-generated summaries may contain subtle factual errors that could bias veracity classification
- Evaluation on synthetic claims may not fully capture real-world political claim complexity

## Confidence

- High Confidence: Overall pipeline design and core claim that web evidence improves veracity classification
- Medium Confidence: Faithfulness and helpfulness of claim-focused summaries based on human evaluation
- Low Confidence: Robustness to search engine variability and impact of temporal/site constraints in real-world scenarios

## Next Checks

1. **Reproducibility Check**: Re-run the pipeline on a fixed set of claims at different times to assess variability in retrieved documents and veracity classification results due to search engine updates or indexing changes.

2. **Faithfulness Audit**: Expand human evaluation of summary faithfulness to a larger sample (e.g., 100 claims) with inter-annotator agreement metrics to quantify evaluation reliability.

3. **Constraint Sensitivity Analysis**: Systematically vary temporal and site constraints (e.g., allow fact-checking sites, relax date filters) and measure impact on retrieval coverage and veracity accuracy to determine optimal balance between realism and evidence availability.