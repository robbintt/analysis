---
ver: rpa2
title: Human-in-the-loop Machine Translation with Large Language Model
arxiv_id: '2310.08908'
source_url: https://arxiv.org/abs/2310.08908
tags:
- translation
- feedback
- retrieval
- machine
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-in-the-loop pipeline to guide LLMs
  to produce domain-specific translations by leveraging in-context learning with revision
  feedback. The method first prompts the LLM to generate a draft translation, then
  retrieves relevant in-context demonstrations from a database to refine the draft.
---

# Human-in-the-loop Machine Translation with Large Language Model

## Quick Facts
- arXiv ID: 2310.08908
- Source URL: https://arxiv.org/abs/2310.08908
- Reference count: 3
- Primary result: Human-in-the-loop pipeline improves domain-specific LLM translation quality using revision feedback from edit distance

## Executive Summary
This paper introduces a human-in-the-loop pipeline that leverages Large Language Models (LLMs) for domain-specific translation improvement. The method uses in-context learning with revision feedback, where the LLM first generates a draft translation, then retrieves relevant demonstrations from a database to refine it. Human-like feedback, simulated via edit distance, creates revision instructions stored alongside translation pairs. The approach demonstrates significant improvements in translation quality across German-English translation tasks in five domains (IT, Koran, Law, Medical, Subtitles) using BLEU, TER, BERTScore, and COMET metrics.

## Method Summary
The method employs a two-stage pipeline where an LLM generates initial draft translations, then refines them using in-context learning with retrieved demonstrations. Revision feedback is created by computing edit distance between the draft and reference translations, converting the minimal-cost transformation path into natural language instructions. These demonstrations, along with source-target pairs, populate a retrieval database. For new translations, relevant examples are retrieved using BM25 or BM25-Rerank, presented to the LLM with revision instructions, and the model compares polished output against the draft to select the better version.

## Key Results
- Significant improvement in translation quality over direct LLM translation across all domains
- Best performance in IT and Medical domains using the human-in-the-loop pipeline
- BM25-Rerank retrieval method outperforms standard BM25 in most domains
- Demonstrated effectiveness of edit-distance-based feedback generation for in-context learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM learns domain-specific translation preferences by observing revision instructions derived from edit distance alignment.
- Mechanism: Edit distance between LLM draft and reference translation generates a minimal-cost path of insertions, deletions, and substitutions. This path is converted into natural language revision instructions, which are stored alongside the source-target pair in the retrieval database.
- Core assumption: The minimal edit path provides a faithful representation of domain-specific lexical and syntactic preferences.
- Evidence anchors:
  - [abstract]: "The feedback takes the form of a sequence of revision instructions, indicating the necessary edits to transform the LLM’s translation into the reference translation."
  - [section]: "Computing edit distance is a dynamic programming problem, where the cost matrix reflects what editing operations are needed to transform from the machine translation to the reference translation."
- Break condition: If edit distance misrepresents true domain preferences, the LLM may learn incorrect patterns.

### Mechanism 2
- Claim: In-context learning with retrieved domain-specific examples refines draft translations by simulating domain-aware human intervention.
- Mechanism: The pipeline retrieves top-N examples from the database using BM25 or BM25-Rerank, then presents these with revision instructions to the LLM. The model compares polished output to draft and selects the better version.
- Core assumption: The LLM's internal comparison mechanism can effectively choose between draft and polished versions without seeing the reference.
- Evidence anchors:
  - [abstract]: "The pipeline initiates by prompting the LLM to produce a draft translation, followed by the utilization of automatic retrieval or human feedback as supervision signals to enhance the LLM’s translation through in-context learning."
  - [section]: "We empirically found that the LLM may incorrectly modify the draft translation after observing the ICL demonstrations... we asked the LLM further compare the polished translations with the draft translations at the second stage."
- Break condition: If the LLM fails to self-select correctly, translation quality may degrade.

### Mechanism 3
- Claim: Iterative feedback storage expands the retrieval database, enabling continuous improvement in low-resource scenarios.
- Mechanism: Each new human-machine interaction (draft, polished, feedback) is appended to the database, increasing the density of domain-relevant examples for future retrieval.
- Core assumption: Database growth improves retrieval recall and translation quality over time.
- Evidence anchors:
  - [abstract]: "The human-machine interactions generated in this pipeline are also stored in an external database to expand the in-context retrieval database, enabling us to leverage human supervision in an offline setting."
  - [section]: "Any new human-machine interactions generated within this pipeline are incorporated to expand the in-context retrieval database."
- Break condition: If stored examples are noisy or unrepresentative, retrieval performance degrades.

## Foundational Learning

- Concept: Edit distance dynamic programming
  - Why needed here: The method computes the minimal-cost transformation path between LLM output and reference to generate revision instructions.
  - Quick check question: What are the three edit operations considered, and how is their cost matrix computed?

- Concept: BM25 relevance scoring
  - Why needed here: BM25 ranks candidate demonstrations by keyword overlap with the source sentence, ensuring relevant context is retrieved.
  - Quick check question: How does BM25 differ from simple keyword matching in selecting in-context examples?

- Concept: In-context learning mechanics
  - Why needed here: The LLM must generalize from a few demonstrations to refine new translations without parameter updates.
  - Quick check question: Why does the pipeline use 3-shot demonstrations instead of 1 or 5?

## Architecture Onboarding

- Component map: Input parser -> Draft generator (GPT-3.5) -> BM25/Rerank retriever -> Feedback synthesizer (edit distance) -> Demonstration assembler -> Polisher (GPT-3.5) -> Self-comparison selector -> Database updater
- Critical path: Draft -> Retrieve -> Revise -> Compare -> Output. Each step must complete within API rate limits.
- Design tradeoffs:
  - Retrieval depth (K=200) vs. latency: deeper retrieval improves recall but increases cost.
  - Edit distance granularity: finer-grained edits improve instruction fidelity but increase instruction length.
  - Self-comparison vs. reference comparison: avoids exposing reference but relies on LLM judgment quality.
- Failure signatures:
  - Low BLEU/TER after polishing: indicates poor retrieval or incorrect feedback generation.
  - Increased refusal rate: LLM may reject overly long or conflicting instructions.
  - Database drift: irrelevant examples dominate retrieval, reducing effectiveness.
- First 3 experiments:
  1. Validate edit distance feedback by comparing synthetic vs. human-generated instructions on a small sample.
  2. Test retrieval quality: retrieve 3 examples for 10 random source sentences and verify semantic relevance.
  3. A/B test 1-shot vs. 3-shot ICL with fixed retrieval method on IT domain to confirm performance gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of the ICL retrieval database affect the performance of the proposed HIL method, and what strategies can be employed to optimize its construction under low-resource scenarios?
- Basis in paper: [inferred] The paper mentions that the ICL retrieval database is constructed using randomly sampled training data and that the quality of data within the demonstration pool may not be very high.
- Why unresolved: The paper does not provide a detailed analysis of the impact of database quality on HIL performance or explore strategies for optimizing database construction.
- What evidence would resolve it: Experiments comparing HIL performance with databases of varying quality and diversity, along with analysis of strategies for optimizing database construction in low-resource scenarios.

### Open Question 2
- Question: How does the proposed HIL method perform when using real human feedback instead of simulated feedback based on edit distance, and what are the key differences in translation quality and efficiency?
- Basis in paper: [explicit] The paper mentions that the current study uses an automated feedback method based on edit distance to simulate human feedback due to resource limitations, and future work plans to collect real human feedback.
- Why unresolved: The paper does not provide experimental results or comparisons using real human feedback.
- What evidence would resolve it: Experiments comparing HIL performance using real human feedback versus simulated feedback, with analysis of differences in translation quality and efficiency.

### Open Question 3
- Question: How do different demonstration retrieval methods (e.g., BM25, BM25 Re-Rank) impact the performance of the proposed HIL method across various domains, and what are the underlying reasons for these differences?
- Basis in paper: [explicit] The paper discusses the use of BM25 and BM25 Re-Rank for demonstration retrieval and presents experimental results comparing their performance across different domains.
- Why unresolved: The paper does not provide a detailed analysis of the reasons behind the performance differences of different retrieval methods across domains.
- What evidence would resolve it: In-depth analysis of the characteristics of different retrieval methods and their impact on HIL performance across domains, along with explanations for the observed differences.

## Limitations
- Simulated human feedback via edit distance may not capture nuanced domain-specific translation preferences
- Effectiveness depends on edit distance accurately representing true translation quality improvements
- Limited validation of LLM's self-comparison mechanism without reference exposure

## Confidence
- High confidence: The two-stage pipeline architecture and basic retrieval mechanism are well-established in the literature
- Medium confidence: The edit-distance-based feedback generation method, while novel, lacks validation against human-generated feedback
- Low confidence: The assumption that the LLM's self-comparison mechanism reliably selects between draft and polished outputs without reference exposure

## Next Checks
1. Compare edit-distance-generated revision instructions against a small sample of human-generated feedback to quantify fidelity differences
2. Implement ablation study testing different retrieval methods (BM25 vs BM25-Rerank) across all five domains to isolate retrieval impact on translation quality
3. Conduct human evaluation of polished translations in the IT and Medical domains to validate automated metric improvements against human judgment