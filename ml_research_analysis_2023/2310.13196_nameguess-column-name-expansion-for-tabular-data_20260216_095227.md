---
ver: rpa2
title: 'NameGuess: Column Name Expansion for Tabular Data'
arxiv_id: '2310.13196'
source_url: https://arxiv.org/abs/2310.13196
tags:
- column
- table
- names
- name
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the NAME GUESS task to expand abbreviated
  column names in tabular data using natural language generation. The authors create
  a large training dataset of 384K examples by fabricating abbreviated-expanded column
  name pairs from well-curated web tables, and a human-annotated evaluation benchmark
  with 9.2K examples across four difficulty levels.
---

# NameGuess: Column Name Expansion for Tabular Data

## Quick Facts
- arXiv ID: 2310.13196
- Source URL: https://arxiv.org/abs/2310.13196
- Reference count: 24
- Key outcome: Fine-tuned language models with table context achieve close to human performance on expanding abbreviated column names

## Executive Summary
This paper introduces NAME GUESS, a task to expand abbreviated column names in tabular data using natural language generation. The authors create a large training dataset of 384K examples by fabricating abbreviated-expanded column name pairs from web tables, and evaluate on a human-annotated benchmark with 9.2K examples across four difficulty levels. By fine-tuning auto-regressive language models up to 2.7B parameters conditioned on table content and column headers, they achieve close to human performance. The key finding is that incorporating table content consistently improves performance, though larger models like GPT-4 still outperform fine-tuned smaller models on complex cases.

## Method Summary
The authors create a synthetic training dataset by fabricating abbreviated-expanded column name pairs from well-curated web tables. They fine-tune auto-regressive language models (up to 2.7B parameters) conditioned on table content and column headers using supervised fine-tuning with task-specific prompts. The model is trained to generate expanded column names from abbreviated forms, leveraging both the column header and sampled cell values from the table as context. Evaluation is performed on a human-annotated benchmark with 9.2K examples across four difficulty levels using Exact Match, F1, and BertScore metrics.

## Key Results
- Fine-tuned models with table context achieve close to human performance on column name expansion
- Incorporating table content consistently improves performance across all difficulty levels
- Larger pre-trained models like GPT-4 outperform fine-tuned smaller models on extra-hard examples
- Human performance on the test set is 74.5% F1, indicating the task is challenging even for humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning smaller language models with table content improves performance on column name expansion tasks.
- Mechanism: By conditioning the model on both column headers and sampled table content, the model learns to disambiguate abbreviations using context-specific information.
- Core assumption: Table content provides sufficient context to resolve polysemy in abbreviated column names.
- Evidence anchors:
  - [abstract]: "incorporating table content consistently improves performance"
  - [section 4]: "We fine-tuned pre-trained LMs by contextualizing column query names with table content, incorporating sampled cell values and table schema data"
  - [corpus]: Weak - no direct corpus evidence provided for this mechanism
- Break condition: If table content lacks diversity or is too sparse, the model cannot disambiguate effectively.

### Mechanism 2
- Claim: Larger pre-trained language models (like GPT-4) perform better on ambiguous abbreviations without fine-tuning.
- Mechanism: Larger models have broader knowledge and better reasoning capabilities to interpret complex abbreviations from various domains.
- Core assumption: The knowledge captured during pre-training is sufficient for handling ambiguous abbreviations across different domains.
- Evidence anchors:
  - [abstract]: "While GPT-4 exhibited promising performance on NAME GUESS"
  - [section 5.3]: "GPT-4 achieves 29.6% higher EM than the best fine-tuned model"
  - [corpus]: Weak - no direct corpus evidence provided for this mechanism
- Break condition: If abbreviations are highly domain-specific or novel, even large models may struggle without fine-tuning.

### Mechanism 3
- Claim: The difficulty of expanding abbreviations is asymmetric - expanding to full forms is harder than abbreviating.
- Mechanism: Human annotators find it challenging to reconstruct original phrases from abbreviations due to lost semantic information and context dependency.
- Core assumption: The information loss in abbreviation is irreversible without sufficient context.
- Evidence anchors:
  - [section 5.3]: "human performance on NAME GUESS test set...is far from perfect" and "expanding from query name x into logical name y is much more challenging than reverse"
  - [section 3.3.2]: The difficulty breakdown shows even humans struggle with extra-hard examples
  - [corpus]: Weak - no direct corpus evidence provided for this mechanism
- Break condition: If abbreviations follow highly predictable patterns, the asymmetry may be reduced.

## Foundational Learning

- Concept: Natural Language Generation
  - Why needed here: The task requires generating expanded column names from abbreviated forms, which is fundamentally a generation problem
  - Quick check question: What distinguishes generation from classification in NLP tasks?

- Concept: Context Conditioning in Language Models
  - Why needed here: The model needs to understand table context to disambiguate abbreviations effectively
  - Quick check question: How does conditioning on table content change the input representation for the model?

- Concept: Sequence-to-Sequence Learning
  - Why needed here: The task involves mapping abbreviated column names to expanded forms, requiring understanding of sequence relationships
  - Quick check question: What are the key differences between autoregressive and encoder-decoder architectures for sequence generation?

## Architecture Onboarding

- Component map: Table content → Model input preparation → Fine-tuning or inference → Prediction → Evaluation
- Critical path: Table content → Model input preparation → Fine-tuning or inference → Prediction → Evaluation
- Design tradeoffs: Larger models perform better but are more expensive; fine-tuning smaller models is cheaper but may not match LLM performance on complex cases
- Failure signatures: Poor performance on extra-hard examples, failure to extract answers from LLM predictions, low exact match scores
- First 3 experiments:
  1. Test model performance with and without table content on a small validation set
  2. Compare different K (number of columns) and N (number of rows) values for table linearization
  3. Evaluate human performance on the task to establish baseline difficulty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the NAME GUESS model perform on real relational database tables compared to web tables?
- Basis in paper: [explicit] The authors note that the training and evaluation sets used were all public web-related tables, which generally have fewer rows and lack the metadata of primary and secondary keys.
- Why unresolved: The authors did not have access to real relational database tables for evaluation, so the model's performance on such data remains unknown.
- What evidence would resolve it: Evaluating the NAME GUESS model on a dataset of real relational database tables and comparing its performance to that on web tables.

### Open Question 2
- Question: What is the impact of different table metadata information, such as primary and secondary keys, on the NAME GUESS task performance?
- Basis in paper: [inferred] The authors mention that one possible solution to improve performance is to collect and utilize more table metadata information.
- Why unresolved: The authors did not conduct experiments to evaluate the impact of table metadata on the model's performance.
- What evidence would resolve it: Conducting experiments to evaluate the NAME GUESS model's performance with and without additional table metadata information.

### Open Question 3
- Question: How does the NAME GUESS model handle cases where the original column names contain omitted information, such as "glucose" standing for "fasting glucose"?
- Basis in paper: [explicit] The authors acknowledge that the scope of handling omitted information in the original column names is beyond NAME GUESS.
- Why unresolved: The authors did not address this issue in their work, and it remains an open question.
- What evidence would resolve it: Developing a method to handle omitted information in column names and evaluating its impact on the NAME GUESS model's performance.

### Open Question 4
- Question: How does the performance of the NAME GUESS model vary with different table sizes, such as the number of columns and rows?
- Basis in paper: [explicit] The authors provide statistics on the distribution of the number of columns and rows in the training and evaluation sets.
- Why unresolved: The authors did not conduct experiments to evaluate the model's performance on tables of varying sizes.
- What evidence would resolve it: Conducting experiments to evaluate the NAME GUESS model's performance on tables with different numbers of columns and rows.

## Limitations

- The synthetic training data may not fully capture the complexity of real-world abbreviation patterns, particularly for domain-specific or rare abbreviations
- Performance degrades significantly on harder examples that require deeper domain knowledge or contextual reasoning
- The focus on web tables may limit generalizability to other tabular data sources with different abbreviation conventions

## Confidence

- **High confidence**: The core finding that table content improves performance is well-supported by ablation studies and consistent across metrics
- **Medium confidence**: The comparison with LLMs (GPT-4, LLaMA-65B) is valid but limited by the lack of fine-tuning on these models for the specific task
- **Medium confidence**: The human performance benchmarks are internally consistent but may not generalize to broader populations or domains

## Next Checks

1. **Cross-domain validation**: Test the fine-tuned models on tabular data from different sources (scientific papers, financial reports, medical records) to assess generalization beyond web tables
2. **Robustness analysis**: Systematically evaluate performance on abbreviations with varying levels of ambiguity, polysemy, and domain specificity to identify failure patterns
3. **Longitudinal stability test**: Measure performance decay over time as language evolves and new abbreviation patterns emerge, particularly for time-sensitive domains like finance or technology