---
ver: rpa2
title: Multiple Relations Classification using Imbalanced Predictions Adaptation
arxiv_id: '2309.13718'
source_url: https://arxiv.org/abs/2309.13718
tags:
- relation
- relations
- classification
- task
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multiple relations classification (RC) model
  called MRCA that adapts to imbalanced predictions in the RC task. The key idea is
  to adjust the output activation function and loss function to better handle the
  imbalanced distribution of positive and negative labels.
---

# Multiple Relations Classification using Imbalanced Predictions Adaptation

## Quick Facts
- arXiv ID: 2309.13718
- Source URL: https://arxiv.org/abs/2309.13718
- Reference count: 40
- Key outcome: MRCA achieves 96.65% and 93.35% F1 scores on Nyt and Webnlg datasets respectively

## Executive Summary
This paper addresses the challenge of imbalanced predictions in multiple relations classification (RC) tasks. The authors propose MRCA, a model that adapts both the output activation function and loss function to better handle the predominance of negative labels. By using a linear activation with [-1, 1] output range and a customized Dice loss function, MRCA achieves significant performance improvements over baseline models on standard RC datasets.

## Method Summary
MRCA employs a linear activation function in the output layer with a prediction range of [-1, 1] to allocate more range for negative labels. The model uses a customized Dice loss function designed to handle imbalanced predictions more effectively. Additionally, MRCA incorporates Glove word embeddings enhanced with entity features to improve relation directionality and entity distinction. The architecture includes a bidirectional LSTM encoder and average pooling for dimensionality reduction.

## Key Results
- MRCA achieves 96.65% F1 score on Nyt dataset
- MRCA achieves 93.35% F1 score on Webnlg dataset
- Ablation study confirms effectiveness of imbalanced predictions adaptation and entity features utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear activation with [-1,1] output range improves negative label discrimination.
- Mechanism: By restricting output to [-1,1], the model dedicates 75% of the range to negative labels (values below 0.5), allowing finer-grained separation of negative predictions compared to standard sigmoid.
- Core assumption: The imbalance in label distribution means negative examples dominate and benefit from finer discrimination.
- Evidence anchors:
  - [abstract]: "devote more range for negative labels"
  - [section 3.2]: "devoting 75% of the prediction range for the negative labels"
- Break condition: If the dataset becomes balanced or if negative examples are not the majority, this adaptation may hurt performance.

### Mechanism 2
- Claim: RC DiceLoss extension handles negative predictions better than standard Dice loss.
- Mechanism: When ground truth is 0 and prediction < 0.5, the extension uses a squared smoothing term to keep loss small instead of large loss from standard Dice loss.
- Core assumption: Negative ground truth with negative predictions should yield low loss, not high loss.
- Evidence anchors:
  - [section 3.2]: Table 2 shows invalid loss values and corrected ones.
  - [section 3.2]: Formal definition of RC DiceLoss in equation (3.5).
- Break condition: If negative predictions are rare or if the loss shape no longer aligns with f1 metric goals.

### Mechanism 3
- Claim: Entity-aware embedding vectors improve relation directionality and entity distinction.
- Mechanism: Appending +v for subject entities, -v for object entities, and 0 for non-entities creates a discriminative signal for relation direction and entity type.
- Core assumption: Knowing whether a word is subject vs. object is crucial for relation classification.
- Evidence anchors:
  - [section 3.1]: "We use the negative value in the object entity to emphasize the difference between entity types"
  - [section 4.4]: Ablation study shows MRCA-Bert-noLSTM performs much worse without entity vectors.
- Break condition: If relation direction is irrelevant for a task or if entity type is not explicitly marked in input.

## Foundational Learning

- Concept: Understanding label imbalance and its effect on classification.
  - Why needed here: The paper's main contribution is handling imbalanced predictions, so recognizing why imbalance hurts performance is essential.
  - Quick check question: In a dataset with 90% negative labels, what happens if you use a standard sigmoid output and BCE loss?

- Concept: Dice loss and its f1 metric alignment.
  - Why needed here: The RC DiceLoss is a modification of Dice loss designed to optimize f1 score rather than accuracy.
  - Quick check question: How does Dice loss differ from BCE loss in treating false positives vs. false negatives?

- Concept: Word embedding customization for entity recognition.
  - Why needed here: The paper modifies Glove embeddings with entity type and case vectors to improve relation extraction.
  - Quick check question: Why might a case vector be useful for identifying entity words in relation classification?

## Architecture Onboarding

- Component map: Input words → Glove embeddings + case vector + entity vector → Bi-LSTM → Average pooling → Linear activation [-1,1] → RC DiceLoss
- Critical path: Input → Embeddings (with vectors) → Bi-LSTM → Pooling → Output (linear) → RC DiceLoss → Backprop
- Design tradeoffs:
  - Glove + custom vectors vs. BERT: Simpler, better OOV support, but no contextual embeddings unless Bi-LSTM added.
  - Linear activation with [-1,1] vs. sigmoid: Better for imbalanced data but requires custom loss function.
  - RC DiceLoss vs. BCE: Optimizes f1 but needs careful handling of negative predictions.
- Failure signatures:
  - Poor precision/recall trade-off: Check if RC DiceLoss implementation correctly handles negative predictions.
  - High variance across runs: Check if dropout rate and random seed handling are consistent.
  - Degraded performance on balanced data: Linear activation range assumption may no longer hold.
- First 3 experiments:
  1. Run MRCA-Sigmoid-BCE variant on Webnlg to confirm ~3% drop in F1 vs. full MRCA.
  2. Remove entity vectors and retrain to observe impact on relation directionality.
  3. Replace Glove with BERT without LSTM to test contextual vs. non-contextual embeddings trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the imbalanced predictions adaptation approach be further optimized for relation classification tasks with even larger predefined relation sets?
- Basis in paper: [explicit] The paper discusses the imbalanced predictions pattern in relation classification and proposes an adaptation approach. However, it does not explore the limits of this approach when applied to datasets with significantly larger predefined relation sets.
- Why unresolved: The paper's experiments are limited to datasets with 24 and 216 predefined relations. The effectiveness of the approach on datasets with hundreds or thousands of relations remains untested.
- What evidence would resolve it: Experiments on relation classification tasks with significantly larger predefined relation sets, comparing the performance of the MRCA model with and without the imbalanced predictions adaptation approach.

### Open Question 2
- Question: Can the entity features utilization technique be extended to incorporate more complex entity information, such as entity types and relationships between entities?
- Basis in paper: [explicit] The paper introduces a technique to enrich the input with entity features by appending additional vectors to the word embeddings. However, it only considers the entity type (subject or object) and does not explore more complex entity information.
- Why unresolved: The paper's experiments only demonstrate the effectiveness of the technique with basic entity type information. The potential benefits of incorporating more complex entity information remain unexplored.
- What evidence would resolve it: Experiments comparing the performance of the MRCA model with and without the incorporation of more complex entity information, such as entity types and relationships between entities.

### Open Question 3
- Question: How can the MRCA model be adapted to handle multi-lingual relation classification tasks?
- Basis in paper: [explicit] The paper focuses on relation classification in English text. However, it does not address the challenges and potential solutions for adapting the model to handle multi-lingual relation classification tasks.
- Why unresolved: The paper's experiments are limited to English text, and the effectiveness of the model on multi-lingual datasets remains untested.
- What evidence would resolve it: Experiments on multi-lingual relation classification tasks, comparing the performance of the MRCA model with and without adaptations for handling multiple languages.

## Limitations
- The paper assumes negative predictions dominate in RC tasks, which may not hold for all datasets.
- Dice loss extensions for negative predictions rely on specific mathematical properties that may not generalize to other RC scenarios.
- The entity vector approach assumes entity type information is explicitly marked in input, which may not be available in all RC datasets.

## Confidence
- High confidence: The mathematical formulation of RC DiceLoss and its f1 metric alignment is sound.
- Medium confidence: The ablation study results are promising but could benefit from additional controls.
- Medium confidence: The entity vector approach is well-motivated but requires verification that entity type information is consistently available.

## Next Checks
1. Test MRCA on a balanced RC dataset to verify whether linear activation with [-1,1] range remains beneficial when negative predictions are not dominant.
2. Implement a variant of MRCA using BERT embeddings with entity vectors (without LSTM) to isolate the contribution of contextual vs. non-contextual embeddings.
3. Run a systematic ablation study removing each adaptation component (linear activation, Dice loss extension, entity vectors) to quantify their individual contributions to overall performance.