---
ver: rpa2
title: Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Shape Reconstruction
arxiv_id: '2309.13587'
source_url: https://arxiv.org/abs/2309.13587
tags:
- clinical
- figure
- reconstruction
- femur
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks encoder-decoder architectures for 3D bone
  shape reconstruction from biplanar X-rays. It provides a framework for collecting
  and preprocessing 6 public datasets and implements 8 models, many without prior
  public code.
---

# Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Shape Reconstruction

## Quick Facts
- arXiv ID: 2309.13587
- Source URL: https://arxiv.org/abs/2309.13587
- Reference count: 40
- 3D bone shape reconstruction from biplanar X-rays with 8 encoder-decoder architectures across 4 anatomies

## Executive Summary
This paper presents a comprehensive benchmark of encoder-decoder architectures for 3D bone shape reconstruction from biplanar X-rays. The study evaluates 8 different models across 4 anatomies (femur, hip, rib, spine) using 6 public datasets, with a particular focus on attention-based architectures that capture global spatial relationships. The research introduces a standardized framework for data preprocessing and evaluation, including clinical parameter extraction methods. Key findings show attention-based methods like SwinUNETR and AttentionUNet outperform traditional architectures, while highlighting the importance of disaggregated reporting for clinically relevant subgroups.

## Method Summary
The study benchmarks 8 encoder-decoder architectures for 3D bone reconstruction from biplanar X-rays. It uses 6 public datasets (TotalSegmentator-Rib-Dataset481, TotalSegmentator-Femur-Dataset465, TotalSegmentator-Pelvic-Dataset446, CTPelvic1k-Pelvic-Dataset, VerSe2019-Vertebra-Dataset, RSNACervicalFracture-Vertebra-Dataset) with Digitally Reconstructed Radiographs (DRRs) generated from CT scans. Models are trained with Adam optimizer, learning rates 2e-3 or 2e-4, batch sizes 1-8, and 4000-15000 iterations depending on dataset. Evaluation uses both image-based metrics (Dice Score, Hausdorff Distance, Average Surface Distance, Normalized Surface Distance) and clinical metrics for relevant anatomies.

## Key Results
- Attention-based architectures (SwinUNETR, AttentionUNet) outperform non-attention methods across all anatomies
- Rib reconstruction is significantly more challenging than femur, hip, or spine
- Clinical subgroup performance varies substantially, sometimes underestimated by aggregated metrics
- Dice score improvements don't always translate to better clinical parameter estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-based encoder-decoder architectures outperform non-attention architectures across all anatomies.
- Mechanism: Attention mechanisms capture global spatial relationships in the input X-ray images, allowing the model to learn more informative feature representations for 3D shape reconstruction.
- Core assumption: Global spatial relationships are important for accurate 3D reconstruction from 2D images.
- Evidence anchors:
  - [abstract] "attention-based methods that capture global spatial relationships tend to perform better across all anatomies and datasets"
  - [section] "across all tasks, we find that global spatial relationship learnt via self-attention and gating-based encoder-decoders such as SwinUNETR[20] and AttentionUNet[38] perform better"
- Break condition: If local spatial relationships are more important than global ones for a specific anatomy or task.

### Mechanism 2
- Claim: Reporting disaggregated metrics by clinically relevant subgroups provides more insight than aggregated metrics.
- Mechanism: Disaggregated reporting reveals performance variations across different subgroups (e.g., vertebra types, severities), which may be hidden when reporting average scores.
- Core assumption: Performance can vary significantly across different subgroups within a dataset.
- Evidence anchors:
  - [abstract] "performance on clinically relevant subgroups may be overestimated without disaggregated reporting"
  - [section] "Disaggregated reporting of metrics for the VerSe2019 dataset based on level, shape and severity in Figure 3 shows reduced performance on severely deformed vertebra"
- Break condition: If performance is consistent across all subgroups, making disaggregated reporting unnecessary.

### Mechanism 3
- Claim: Improvement in image-based metrics (e.g., Dice score) does not always translate to improvement in clinical parameter estimation.
- Mechanism: Image-based metrics measure overall reconstruction accuracy, while clinical parameters focus on specific anatomical features. Improving one does not guarantee improvement in the other.
- Core assumption: Image-based metrics and clinical parameters capture different aspects of reconstruction quality.
- Evidence anchors:
  - [abstract] "the dice score improvement does not always bring a corresponding improvement in the automatic estimation of clinically relevant parameters"
  - [section] "we find that improvement in metrics like DSC do not always lead to improved performance on clinical metrics"
- Break condition: If image-based metrics and clinical parameters are highly correlated for a specific anatomy or task.

## Foundational Learning

- Concept: 3D reconstruction from 2D images
  - Why needed here: The core task is to reconstruct 3D bone shapes from 2D X-ray images.
  - Quick check question: What are the main challenges in reconstructing 3D shapes from 2D images?

- Concept: Encoder-decoder architectures
  - Why needed here: The paper benchmarks different encoder-decoder architectures for the 3D reconstruction task.
  - Quick check question: How do encoder-decoder architectures work, and what are their key components?

- Concept: Attention mechanisms
  - Why needed here: Attention-based architectures perform better in this task, so understanding attention is crucial.
  - Quick check question: What is the role of attention mechanisms in deep learning models?

## Architecture Onboarding

- Component map: Data ingestion and preprocessing -> Encoder-decoder architectures (with and without attention) -> Clinical parameter extraction methods -> Evaluation metrics (image-based and clinical) -> Benchmarking tasks and experiments

- Critical path:
  1. Collect and preprocess the required datasets
  2. Implement the encoder-decoder architectures
  3. Train and evaluate the models on the datasets
  4. Extract clinical parameters from the reconstructed shapes
  5. Analyze the results and compare the architectures

- Design tradeoffs:
  - Model complexity vs. performance: More complex models (e.g., with attention) may perform better but require more computational resources.
  - Image-based metrics vs. clinical parameters: Focusing on one may come at the expense of the other.
  - Disaggregated vs. aggregated reporting: Disaggregated reporting provides more insights but may be more complex to analyze.

- Failure signatures:
  - Poor performance on clinically relevant subgroups despite good overall performance
  - Discrepancy between image-based metrics and clinical parameter estimation
  - Sensitivity to domain shifts (e.g., fractured bones, implants, population shifts)

- First 3 experiments:
  1. Implement and train a basic encoder-decoder architecture (e.g., UNet) on one anatomy (e.g., femur) and evaluate its performance.
  2. Implement and train an attention-based encoder-decoder architecture (e.g., AttentionUNet) on the same anatomy and compare its performance to the basic architecture.
  3. Extract clinical parameters (e.g., femoral head radius and neck shaft angle) from the reconstructed femur shapes and analyze the relationship between image-based metrics and clinical parameter estimation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental differences in model performance between different anatomies (femur, hip, rib, spine) that cannot be explained by image resolution or field-of-view differences alone?
- Basis in paper: [explicit] The paper shows ribs are substantially more difficult to reconstruct compared to femur, hip and spine, but doesn't explain the underlying reasons beyond technical factors.
- Why unresolved: The paper attributes difficulty differences to technical factors like image resolution and field-of-view but doesn't explore anatomical or biomechanical factors that might contribute to the difficulty.
- What evidence would resolve it: Comparative analysis of anatomical complexity, surface geometry variation, and biomechanical factors across different bone types, controlling for technical factors like image resolution.

### Open Question 2
- Question: How do improvements in Dice score translate to improvements in specific clinical decision-making tasks across different anatomies?
- Basis in paper: [explicit] The paper shows dice score improvements don't always bring corresponding improvements in automatic estimation of clinically relevant parameters.
- Why unresolved: The paper identifies this discrepancy but doesn't investigate which specific clinical tasks are most affected or develop metrics to better capture clinically meaningful improvements.
- What evidence would resolve it: Correlation analysis between Dice score improvements and improvements in specific clinical decision outcomes, with identification of which clinical parameters are most sensitive to reconstruction accuracy.

### Open Question 3
- Question: What are the optimal methods for automatically extracting clinically relevant parameters from 3D reconstructions that remain robust across diverse patient populations and pathologies?
- Basis in paper: [inferred] The paper mentions challenges in automatically extracting clinical metrics from diverse cohorts and notes some methods fail on implausible rib shapes.
- Why unresolved: The paper implements some extraction methods but identifies their limitations and suggests more robust methods are needed, particularly for ribs.
- What evidence would resolve it: Development and validation of robust parameter extraction algorithms that can handle diverse anatomical variations and pathologies, with demonstrated performance across multiple patient populations.

## Limitations
- Synthetic DRR data rather than real biplanar X-rays may not capture real-world imaging challenges
- Clinical parameter extraction depends on accurate 3D reconstruction, potentially compounding errors
- Performance differences may be smaller than domain shift effects across different datasets and populations

## Confidence
- Attention-based architecture superiority: High
- Disaggregated reporting importance: High
- Clinical subgroup performance variation: High
- Rib reconstruction difficulty: High
- Real-world clinical utility: Medium

## Next Checks
1. Test top-performing attention-based models on real biplanar X-ray datasets rather than synthetic DRRs to verify performance claims hold under actual imaging conditions.

2. Evaluate the complete pipeline from raw X-ray acquisition through 3D reconstruction to clinical parameter extraction in a prospective clinical study to assess practical utility.

3. Apply the best-performing architectures to additional skeletal structures (e.g., scapula, tibia) to test the generalizability of the attention mechanism advantage beyond the 4 anatomies studied.