---
ver: rpa2
title: Convolutional autoencoder-based multimodal one-class classification
arxiv_id: '2309.14090'
source_url: https://arxiv.org/abs/2309.14090
tags:
- data
- multimodal
- classification
- one-class
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep learning method for multimodal one-class
  classification using two jointly trained convolutional autoencoders. The method
  maps multimodal positive-class inputs into a compact latent space, with the L2 distance
  from the origin serving as the anomaly score.
---

# Convolutional autoencoder-based multimodal one-class classification

## Quick Facts
- arXiv ID: 2309.14090
- Source URL: https://arxiv.org/abs/2309.14090
- Reference count: 29
- Key outcome: Multimodal OCC achieves ROC of 0.578 vs 0.511 and 0.49 for unimodal variants on macroinvertebrate dataset

## Executive Summary
This paper proposes a multimodal one-class classification method using jointly trained convolutional autoencoders. The approach maps positive-class inputs from two image modalities into a shared latent space, with L2 distance from origin serving as the anomaly score. Tested on a macroinvertebrate image dataset with four one-class classification tasks, the multimodal approach consistently outperforms unimodal variants across all metrics, achieving average ROC of 0.578 versus 0.511 and 0.49. Ablation studies confirm optimal performance at 32x32 input resolution and show that feature diversity regularizers further improve results.

## Method Summary
The method uses two shared-weight convolutional autoencoders trained jointly on paired RGB images from different viewpoints. Inputs are encoded separately, flattened, and concatenated into a single latent representation that is regularized to have small L2 norm. The decoder reconstructs both input modalities from this shared representation. During inference, the L2 distance from origin in the latent space serves as the anomaly score, with threshold set at the 95th percentile of training data norms. The approach includes ablation studies varying input size (32x32, 64x64, 128x128) and applying feature diversity regularizers (WLD-Reg direct, det, logdet).

## Key Results
- Multimodal approach achieves average ROC of 0.578 versus 0.511 and 0.49 for unimodal variants
- 32x32 input size yields best performance, balancing resolution with background noise
- Feature diversity regularizers improve performance by preventing feature collapse
- Consistent performance improvements across all three metrics (Recall, Precision@rank N, ROC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal learning improves one-class classification performance over unimodal variants.
- Mechanism: Two autoencoders jointly trained on different data modalities (image viewpoints) learn complementary features that create a more discriminative latent representation. The L2 distance from origin in the shared latent space serves as an anomaly score, with multimodal fusion capturing more information than single modalities.
- Core assumption: Different modalities provide non-redundant information about the positive class distribution, and joint compact representation improves class separability.
- Evidence anchors:
  - [abstract] "Experimental results using a multimodal macroinvertebrate image classification dataset show that the proposed multimodal method yields better results as compared to the unimodal approach."
  - [section] "As shown in Table I, multimodal learning, indeed, yields better performance compared to both unimodal cases in all three metrics."
  - [corpus] No direct corpus evidence for this specific multimodal OCC mechanism, but related work on multimodal learning shows performance benefits in other domains.
- Break condition: If modalities are highly correlated or redundant, joint training may not provide benefit and could even degrade performance.

### Mechanism 2
- Claim: Feature diversity regularizers improve one-class classification performance by preventing feature collapse.
- Mechanism: Regularizers (WLD-Reg direct, det, logdet) encourage distinct feature representations in the latent space, preventing all inputs from mapping to similar regions while maintaining compactness.
- Core assumption: Feature diversity is beneficial for one-class classification because it preserves discriminative information while still maintaining class compactness.
- Evidence anchors:
  - [abstract] "We investigate how recently proposed feature diversity regularizers affect the performance of our approach. We show that such regularizers improve performance."
  - [section] "By comparing the results with the ones obtained without the regularizer, we note that applying diversity regularizers can indeed improve performance."
  - [corpus] No corpus evidence specifically addressing diversity regularizers in OCC context, though general regularization benefits are documented.
- Break condition: If regularizers are too strong, they may force the model to spread representations too far from origin, violating the compactness assumption.

### Mechanism 3
- Claim: 32x32 input size provides optimal balance between resolution and background noise for this dataset.
- Mechanism: Smaller input sizes reduce computational complexity and background noise while preserving essential class features, whereas larger sizes add detail that may confuse the model with limited training data.
- Core assumption: The critical discriminative features for macroinvertebrate identification are preserved at 32x32 resolution, and additional detail in larger images introduces noise rather than useful information.
- Evidence anchors:
  - [section] "The best performance is achieved using 32 × 32-pixel input size. We note that using a higher size improves the resolution. However, it adds more details and background noise to the images, which can render the learning task harder, especially with limited training data."
  - [corpus] No corpus evidence for this specific dataset, but general findings in computer vision suggest smaller inputs can reduce noise for small datasets.
- Break condition: If dataset had more samples or classes required finer detail, larger input sizes might become beneficial.

## Foundational Learning

- Concept: Autoencoder architecture and training objectives
  - Why needed here: The method relies on convolutional autoencoders to learn compact representations, with reconstruction loss preventing degenerate solutions.
  - Quick check question: What happens if we remove the reconstruction loss term from the objective function?

- Concept: One-class classification fundamentals
  - Why needed here: Understanding how positive class-only training differs from traditional binary classification and how anomaly scores are derived from learned representations.
  - Quick check question: How is the threshold τ determined from training data in this approach?

- Concept: Multimodal learning principles
  - Why needed here: The method fuses information from two image modalities (viewpoints) through shared encoders and concatenated latent representations.
  - Quick check question: What would happen if we used separate encoders without shared weights?

## Architecture Onboarding

- Component map: Input images → Encoder E1/E2 (shared weights) → Flatten → Concatenate → Latent representation → Decoder D1/D2 (shared weights) → Reconstructed outputs
- Critical path: Training: Input → Encoders → Latent space (L2 compactness + reconstruction) → Decoders → Loss computation. Inference: Input → Encoders → Latent space → L2 distance from origin → Anomaly score.
- Design tradeoffs: Shared weights reduce parameters but may limit modality-specific feature learning; joint compact representation balances class separation vs. information preservation.
- Failure signatures: Degenerate solutions (all inputs map to origin), poor anomaly detection (high false positives/negatives), overfitting with small datasets, sensitivity to threshold selection.
- First 3 experiments:
  1. Implement unimodal variant using only left modality and compare performance to multimodal baseline
  2. Test different input sizes (32x32, 64x64, 128x128) to verify optimal resolution findings
  3. Apply feature diversity regularizers (WLD-Reg variants) to measure performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multimodal approach generalize to datasets with more than two modalities or to tasks involving non-image modalities?
- Basis in paper: [inferred] The paper states that "an extension to more modalities could be easily obtained" but does not provide experimental validation or theoretical analysis of such extensions.
- Why unresolved: The experimental evaluation is limited to two-image modalities, leaving the effectiveness of the approach for higher-dimensional multimodal inputs or different data types unexplored.
- What evidence would resolve it: Experimental results comparing the approach's performance on datasets with three or more modalities, or incorporating non-image data types like text or audio, would provide insights into its scalability and generalizability.

### Open Question 2
- Question: What is the optimal way to determine the threshold τ for the L2 distance in the test phase, and how sensitive is the model's performance to this choice?
- Basis in paper: [explicit] The paper mentions using the 95th percentile of training data norms but does not explore alternative methods or analyze the impact of threshold selection on classification performance.
- Why unresolved: The choice of threshold τ is crucial for distinguishing between positive and negative classes, yet the paper does not provide a comprehensive analysis of its sensitivity or alternative determination strategies.
- What evidence would resolve it: A detailed study evaluating the performance of the model under different threshold determination methods (e.g., cross-validation, statistical approaches) and analyzing the impact of threshold variations on ROC scores would clarify this issue.

### Open Question 3
- Question: How do the proposed feature diversity regularizers compare to other regularization techniques, such as dropout or weight decay, in terms of improving one-class classification performance?
- Basis in paper: [inferred] The paper investigates the effect of three specific feature diversity regularizers but does not compare their performance against other common regularization methods.
- Why unresolved: While the paper demonstrates the effectiveness of feature diversity regularizers, it does not provide a comprehensive comparison with other regularization strategies that could offer similar or better improvements.
- What evidence would resolve it: Experimental results comparing the performance of the model with feature diversity regularizers against models using dropout, weight decay, or other regularization techniques would provide a clearer understanding of their relative effectiveness.

## Limitations

- Critical architectural details remain unspecified, including exact CNN block configurations and WLD-Reg regularizer hyperparameters
- Small dataset size (4 classes, ~230 samples each) limits generalizability to other OCC tasks or domains
- Focus exclusively on image-based modalities restricts applicability to other data types

## Confidence

- Multimodal performance improvement: **High confidence** - Results show consistent ROC improvement (0.578 vs 0.511/0.49) across all three metrics with clear statistical separation.
- Feature diversity regularizers benefit: **Medium confidence** - Performance gains reported but mechanism and optimal regularizer selection not fully explored.
- 32x32 input size optimality: **Medium confidence** - Empirical finding but limited input size exploration (only 3 sizes tested) and no ablation on dataset size effects.

## Next Checks

1. Implement controlled ablation testing with varying dataset sizes to determine if 32x32 remains optimal as training data increases.
2. Test alternative anomaly scoring methods (e.g., reconstruction error, combined scores) against the L2 distance baseline.
3. Validate on a larger, more diverse OCC dataset with multiple positive classes to assess generalizability beyond the macroinvertebrate domain.