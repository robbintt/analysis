---
ver: rpa2
title: 'Monte-Carlo Tree Search for Multi-Agent Pathfinding: Preliminary Results'
arxiv_id: '2307.13453'
source_url: https://arxiv.org/abs/2307.13453
tags:
- agents
- mcts
- tree
- reward
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the application of Monte-Carlo Tree Search (MCTS)
  to the challenging problem of Multi-Agent Pathfinding (MAPF). In MAPF, a set of
  agents must find collision-free paths to reach their respective goals on a graph
  or grid.
---

# Monte-Carlo Tree Search for Multi-Agent Pathfinding: Preliminary Results

## Quick Facts
- arXiv ID: 2307.13453
- Source URL: https://arxiv.org/abs/2307.13453
- Reference count: 26
- Primary result: MAMCTS with subgoal rewards outperforms modified A* on MAPF benchmarks

## Executive Summary
This paper introduces a novel Monte-Carlo Tree Search (MCTS) approach for Multi-Agent Pathfinding (MAPF) that addresses key scalability challenges. The authors propose decomposing the joint action space and introducing subgoal-based rewards to guide agents toward their goals while maintaining flexibility for collision avoidance. Experiments on cooperative random and maze maps demonstrate significant performance improvements over baseline algorithms, particularly in scenarios with many agents. The method achieves high individual and cooperative success rates while maintaining computational feasibility.

## Method Summary
The method adapts MCTS to MAPF by decomposing joint actions into sequential agent-level selections, reducing the branching factor from |A|^n to n·|A|. A subgoal-based reward function provides intermediate guidance by rewarding agents when they reach cells two steps along their shortest path to the goal. The algorithm uses UCT exploration at each agent's decision point during tree expansion. Four variants are compared: Joint MCTS (baseline), MAMCTS (decomposed actions), Subgoal MAMCTS (decomposed actions + subgoal rewards), and a modified A* algorithm.

## Key Results
- Subgoal MAMCTS achieves highest success rates on both cooperative random and maze maps
- Performance scales well with increasing agent numbers, particularly on maze maps
- Modified A* struggles in complex scenarios while MAMCTS variants maintain high success rates
- Computational overhead remains manageable with 1000 MCTS iterations per step

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing joint action space reduces branching factor from |A|^n to n·|A|
- Mechanism: Sequential agent-level action selection instead of simultaneous joint action selection
- Core assumption: Sequential decomposition preserves solution space without losing critical agent interaction information
- Evidence: Explicit statement about impractical |A|^n branching factor and decomposition approach

### Mechanism 2
- Claim: Subgoal rewards provide denser intermediate rewards for better guidance
- Mechanism: Rewards assigned when agents reach subgoals 2 steps along shortest path to goal
- Core assumption: 2-step subgoals provide meaningful guidance without being too restrictive
- Evidence: Reward function equation and specification of rs≪rg

### Mechanism 3
- Claim: UCT exploration bonus maintains proper exploration-exploitation balance
- Mechanism: Standard UCT formula applied to each decomposed agent action selection node
- Core assumption: Standard UCT transfers effectively from single-agent to decomposed multi-agent settings
- Evidence: Explicit UCT formula with exploration coefficient Cp

## Foundational Learning

- Concept: Monte Carlo Tree Search fundamentals (selection, expansion, simulation, backpropagation)
  - Why needed: Entire algorithm built on MCTS framework
  - Quick check: What are the four phases of MCTS and what happens in each phase?

- Concept: Multi-Agent Markov Decision Process (MAMDP) formulation
  - Why needed: Problem formally defined as MAMDP with state, action, transition, and reward structures
  - Quick check: How does MAMDP differ from single-agent MDP in terms of state and action representation?

- Concept: Reward shaping and sparse reward problems
  - Why needed: Paper addresses sparse reward problem in MAPF by introducing subgoal rewards
  - Quick check: Why are sparse rewards problematic for reinforcement learning algorithms like MCTS?

## Architecture Onboarding

- Component map: POGEMA environment wrapper -> MCTS tree structure with decomposed nodes -> Reward computation module
- Critical path: Receive state -> Run MCTS (1000 iterations) -> Execute first joint action -> Receive reward and new state -> Repeat
- Design tradeoffs: Decomposition trades potential optimality for computational feasibility; subgoal distance trades guidance strength for flexibility; simulation limit trades planning depth for speed
- Failure signatures: Poor performance in narrow passages suggests decomposition issues; slow planning indicates need for neural acceleration; low success rates despite iterations suggests reward problems
- First 3 experiments:
  1. Joint MCTS vs MAMCTS on 4-agent random map to verify branching factor reduction
  2. Test subgoal distances (1, 2, 3 steps) on cooperative random maps to find optimal guidance
  3. Compare computation time vs performance for varying MCTS iterations (100, 500, 1000) on maze maps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance scale with increasing numbers of agents beyond 16?
- Basis: Authors mention good performance on 16 agents but don't explore more crowded scenarios
- Why unresolved: Only results provided for up to 16 agents
- What evidence would resolve: Experiments with 32, 64+ agents measuring ISR, CSR, and computational time

### Open Question 2
- Question: What is the impact of different subgoal distances on algorithm performance?
- Basis: Authors mention experimental determination of 2-step subgoal distance
- Why unresolved: No systematic analysis of subgoal distance sensitivity
- What evidence would resolve: Systematic testing with distances 1, 3, 4+ steps comparing performance metrics

### Open Question 3
- Question: How does algorithm perform in dynamic environments with changing obstacles or goals?
- Basis: Authors mention future plans for changing map topology, implying current static environment design
- Why unresolved: Paper focuses on static MAPF scenarios
- What evidence would resolve: Experiments in environments with dynamic obstacles and changing goals measuring replanning ability

## Limitations

- Theoretical analysis missing for optimality guarantees of decomposition approach
- Subgoal distance of 2 steps appears arbitrary without sensitivity analysis
- Computational overhead of 1000 MCTS iterations may limit scalability to larger problems

## Confidence

- High Confidence: Branching factor reduction claim is mathematically sound
- Medium Confidence: Empirical performance improvements demonstrated but limited to specific configurations
- Medium Confidence: Subgoal rewards improve guidance but lack systematic parameter analysis

## Next Checks

1. Conduct ablation studies varying subgoal distance (1, 2, 3, 4 steps) on same map sets
2. Implement and test proposed neural network integration to assess computational overhead reduction
3. Evaluate performance on MAPF instances with higher agent densities and complex obstacle configurations