---
ver: rpa2
title: Leveraging Large Language Models for Node Generation in Few-Shot Learning on
  Text-Attributed Graphs
arxiv_id: '2310.09872'
source_url: https://arxiv.org/abs/2310.09872
tags:
- samples
- graph
- node
- learning
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ENG, a lightweight paradigm that leverages
  Large Language Models (LLMs) to generate node samples in few-shot learning scenarios
  on text-attributed graphs. The method uses LLMs to extract semantic information
  from labels and generate diverse exemplars, then employs an edge predictor to integrate
  these samples into the original graph structure.
---

# Leveraging Large Language Models for Node Generation in Few-Shot Learning on Text-Attributed Graphs

## Quick Facts
- arXiv ID: 2310.09872
- Source URL: https://arxiv.org/abs/2310.09872
- Reference count: 40
- Primary result: ENG achieves 76% improvement over baseline in 1-shot setting on ogbn-arxiv dataset

## Executive Summary
This paper introduces ENG, a novel paradigm that leverages Large Language Models (LLMs) to generate node samples for few-shot learning on text-attributed graphs. The approach addresses the challenge of scarce labeled data by using LLMs to extract semantic information from class labels and generate diverse exemplars, which are then integrated into the graph structure through an edge predictor. ENG is a lightweight, plug-and-play framework compatible with any GNN architecture. Experimental results demonstrate significant performance improvements, particularly in low-shot scenarios, with the ogbn-arxiv dataset showing a 76% improvement over the baseline model in the 1-shot setting.

## Method Summary
ENG works by first using LLMs to generate samples for each class based on label semantics. These generated samples are then embedded using a language model (Sentence-BERT). An edge predictor is trained on the original graph structure to predict connections between generated and existing nodes. The generated nodes are integrated into the graph with these predicted edges, creating an augmented graph. Finally, a standard GNN is trained on this augmented graph to perform node classification. The method is designed to be lightweight and compatible with any GNN architecture, providing a plug-and-play solution for few-shot learning on text-attributed graphs.

## Key Results
- ENG achieves a 76% improvement over the baseline model in the 1-shot setting on the ogbn-arxiv dataset
- The approach shows consistent performance gains across Cora, Pubmed, and ogbn-arxiv datasets in few-shot scenarios
- Generated samples provide effective supervision signals, with performance improving as the number of generated samples increases up to a point

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated samples provide supervision signals that compensate for scarce labeled data in few-shot learning.
- Mechanism: The paradigm mines semantic information from class labels using LLMs, generating diverse samples that serve as additional training examples. These samples capture domain-specific knowledge relevant to each class, enriching the training set with labeled data.
- Core assumption: Generated samples, even if not existing in reality, contain reliable semantic information about their respective classes that can help models learn better representations.
- Evidence anchors:
  - [abstract]: "we utilize LLMs to extract semantic information from the labels and generate samples that belong to these categories as exemplars"
  - [section 4.1]: "we can leverage the textual information within the set of label texts C to explore the semantics embedded in LLMs and generate ð‘€ relevant sample instances for each label"
  - [corpus]: Weak - no direct corpus evidence provided for this specific mechanism
- Break condition: If generated samples consistently contain irrelevant or contradictory information to their class labels, they could introduce noise and degrade model performance.

### Mechanism 2
- Claim: Edge predictor integration creates structural connections between generated and original nodes, enabling information propagation.
- Mechanism: An edge predictor is trained on raw graph structure to predict edges between generated nodes and existing nodes. This creates a unified graph where generated samples can influence unlabeled nodes through message passing.
- Core assumption: The similarity in node representations and structural patterns from the original graph can effectively guide edge prediction between generated and original nodes.
- Evidence anchors:
  - [abstract]: "we employ an edge predictor to capture the structural information inherent in the raw dataset and integrate the newly generated samples into the original graph"
  - [section 4.3]: "we use the edges in the raw graph as supervision signals and construct an edge predictor for the link prediction task"
  - [corpus]: Weak - limited evidence on how effective this edge prediction is in practice
- Break condition: If the edge predictor fails to establish meaningful connections, generated nodes remain isolated and cannot contribute to the learning process.

### Mechanism 3
- Claim: Class-level augmentation through sample generation provides more diverse supervision signals than node-level augmentation.
- Mechanism: Instead of augmenting individual nodes, the paradigm generates multiple samples per class based on label semantics. This approach captures broader class characteristics and creates a more diverse set of supervision signals.
- Core assumption: Generating samples at the class level rather than node level provides more effective augmentation by capturing class-level semantic patterns.
- Evidence anchors:
  - [section 4.1]: "we can leverage the textual information within the set of label texts C to explore the semantics embedded in LLMs and generate ð‘€ relevant sample instances for each label"
  - [section 5.5]: "as the number of generated samples increases, the accuracy shows an initial increase followed by a decrease, and eventually stabilizes"
  - [corpus]: Weak - limited comparative analysis between class-level and node-level augmentation
- Break condition: If generated samples per class become too similar to each other, they may not provide sufficient diversity to improve model performance.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: ENG relies on GNNs to process the augmented graph structure, so understanding how GNNs aggregate neighbor information is essential for implementing the paradigm
  - Quick check question: How does a standard GCN aggregate information from a node's neighbors?

- Concept: Large Language Models (LLMs) and prompt engineering
  - Why needed here: ENG uses LLMs to generate samples based on label semantics, requiring knowledge of how to craft effective prompts and interpret generated outputs
  - Quick check question: What are the key components of an effective prompt for generating text samples from an LLM?

- Concept: Edge prediction and link prediction in graphs
  - Why needed here: The edge predictor component requires understanding how to predict edges based on node representations, which is crucial for integrating generated nodes into the graph structure
  - Quick check question: How does a typical link prediction model determine whether an edge exists between two nodes?

## Architecture Onboarding

- Component map: LLM -> Language Model (LM) -> Edge Predictor -> GNN
- Critical path:
  1. Generate samples using LLM based on label semantics
  2. Create embeddings using LM
  3. Train edge predictor on original graph
  4. Predict edges and integrate generated nodes
  5. Train GNN on augmented graph

- Design tradeoffs:
  - Number of generated samples per class vs. computational cost
  - Edge prediction threshold vs. graph connectivity
  - LLM choice vs. generation quality and cost
  - Embedding model choice vs. representation quality

- Failure signatures:
  - Low accuracy improvement despite augmentation
  - Generated samples that don't match class semantics
  - Edge predictor that fails to create meaningful connections
  - GNN that doesn't effectively utilize augmented structure

- First 3 experiments:
  1. Test sample generation with different prompt variations on a small dataset
  2. Evaluate edge prediction accuracy on the original graph structure
  3. Measure accuracy improvement on a simple dataset with limited augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of generated samples impact the performance of ENG in different few-shot scenarios?
- Basis in paper: [explicit] The paper discusses the balance between sample diversity and relevance to the raw dataset, noting that more diverse samples can make the label space more distinct but may lack connection with the raw dataset.
- Why unresolved: The paper mentions the importance of balancing diversity and relevance but does not provide a detailed analysis of how different levels of diversity affect model performance across various few-shot scenarios.
- What evidence would resolve it: Conducting experiments with varying levels of sample diversity and analyzing the impact on model performance in different few-shot settings (e.g., 1-shot, 3-shot, 5-shot) would provide insights into the optimal balance for different scenarios.

### Open Question 2
- Question: What is the optimal number of generated samples per class for maximizing ENG's performance?
- Basis in paper: [explicit] The paper discusses the impact of the quantity of generated samples on model performance, noting that accuracy initially increases with more samples but then decreases and stabilizes.
- Why unresolved: While the paper observes this trend, it does not determine the optimal number of samples that maximizes performance across different datasets and few-shot scenarios.
- What evidence would resolve it: Performing a detailed sensitivity analysis to find the optimal number of generated samples per class for various datasets and few-shot settings would help determine the best approach.

### Open Question 3
- Question: How does the choice of LLM affect the quality of generated samples and the overall performance of ENG?
- Basis in paper: [inferred] The paper uses ChatGPT as the LLM for generating samples but does not explore the impact of using different LLMs on sample quality and model performance.
- Why unresolved: The paper does not compare the performance of ENG when using different LLMs, leaving the question of how LLM choice influences the results open.
- What evidence would resolve it: Conducting experiments with different LLMs (e.g., GPT-3, GPT-4, Llama) and comparing the quality of generated samples and ENG's performance would provide insights into the optimal LLM choice.

## Limitations
- The approach relies heavily on the quality and consistency of LLM-generated samples
- Computational overhead from sample generation and edge predictor training
- Effectiveness strongly dependent on LLM choice and prompt engineering

## Confidence

**High Confidence Claims:**
- The fundamental concept of using LLM-generated samples to augment few-shot learning scenarios is sound and supported by experimental results
- The integration mechanism through edge prediction is technically feasible and implemented as described
- Performance improvements in low-shot scenarios are well-documented across multiple datasets

**Medium Confidence Claims:**
- The optimal number of generated samples per class may be dataset-dependent
- The generalizability of prompt engineering strategies across different domains remains uncertain
- The edge predictor's effectiveness in capturing meaningful structural relationships requires further validation

**Low Confidence Claims:**
- Long-term scalability of the approach for very large graphs or numerous classes
- Robustness of the method when applied to domains with different linguistic characteristics
- Performance in zero-shot or many-shot scenarios outside the tested range

## Next Checks
1. Cross-domain validation: Test the ENG framework on text-attributed graphs from different domains (e.g., biomedical literature, social networks, product reviews) to assess generalizability of both sample generation and edge prediction components.

2. Ablation study on edge prediction: Systematically vary the edge prediction threshold Î´ and number of connected edges k to quantify their impact on performance and identify optimal configurations for different graph characteristics.

3. Generated sample quality analysis: Conduct a human evaluation of a sample of generated nodes to assess semantic alignment with their target classes, and analyze whether mis-aligned samples correlate with degraded model performance.