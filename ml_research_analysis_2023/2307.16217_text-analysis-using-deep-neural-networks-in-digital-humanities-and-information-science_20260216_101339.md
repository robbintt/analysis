---
ver: rpa2
title: Text Analysis Using Deep Neural Networks in Digital Humanities and Information
  Science
arxiv_id: '2307.16217'
source_url: https://arxiv.org/abs/2307.16217
tags:
- learning
- data
- digital
- deep
- humanities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenges of applying deep neural networks
  (DNNs) for text analysis in Digital Humanities (DH) research, focusing on training
  data availability and domain adaptation. It proposes a practical decision model
  to guide DH researchers in choosing appropriate deep learning approaches.
---

# Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science

## Quick Facts
- arXiv ID: 2307.16217
- Source URL: https://arxiv.org/abs/2307.16217
- Reference count: 36
- Key outcome: The study addresses challenges of applying DNNs for text analysis in DH, focusing on training data availability and domain adaptation, proposing a practical decision model for DH researchers.

## Executive Summary
This study examines the application of deep neural networks (DNNs) for text analysis in Digital Humanities research, identifying two primary challenges: training data availability and domain adaptation. The authors propose a practical decision model to guide DH researchers in selecting appropriate deep learning approaches. The framework considers strategies for handling limited or unavailable data through crowdsourcing, synthetic data generation, and transfer learning, while also addressing domain adaptation techniques including model optimization and pipeline customization. The research emphasizes the need for DH researchers to develop computational skills and highlights the importance of generating and releasing public DH corpora for training deep neural networks.

## Method Summary
The paper presents a decision model for applying DNNs in DH research, synthesizing strategies for addressing limited training data through crowdsourcing, synthetic data generation, and transfer learning. The method involves assessing data availability, selecting appropriate data generation approaches, optimizing DNN architectures for specific DH domains, and adapting preprocessing pipelines to suit both the model and domain requirements. The framework guides researchers through data acquisition, model selection, training, and evaluation processes while considering computational resource constraints.

## Key Results
- DNNs can achieve high accuracy on DH tasks when trained with domain-specific data and optimized architectures
- Transfer learning combined with task-specific fine-tuning enables leveraging pre-trained models on generic corpora for niche DH domains
- Crowdsourcing and synthetic data generation effectively address the lack of labeled training data in DH research

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Deep neural networks can achieve high accuracy on DH tasks when trained with domain-specific data and optimized architectures.
- **Mechanism**: Transfer learning combined with task-specific fine-tuning allows leveraging pre-trained models on large generic corpora and adapting them to niche DH domains.
- **Core assumption**: Domain-specific data, even if limited, provides enough signal to fine-tune pre-trained models effectively.
- **Evidence anchors**:
  - [abstract]: "DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks."
  - [section]: "Cilia et al. (2020) utilized transfer learning to identify medieval writers from scanned images. Instead of generating a large dataset, they used a model that was already trained on an open generic dataset MS-COCO (Lin et al., 2014) and trained it again using a small set of domain-specific examples from the Avila Bible (images of a giant Latin copy of the Bible)."
  - [corpus]: Weak evidence; corpus analysis found related papers but no direct citation of transfer learning for DH tasks.
- **Break condition**: If the domain-specific data is too limited or too dissimilar from the pre-training corpus, transfer learning may fail to improve accuracy.

### Mechanism 2
- **Claim**: Crowdsourcing and synthetic data generation can effectively address the lack of labeled training data in DH.
- **Mechanism**: Crowdsourcing provides labeled data for common knowledge tasks, while synthetic data generation uses existing patterns to expand limited labeled datasets.
- **Core assumption**: Human-labeled data, even if imperfect, can be used to train models that generate additional synthetic data.
- **Evidence anchors**:
  - [abstract]: "The study addresses the challenges of applying deep neural networks (DNNs) for text analysis in Digital Humanities (DH) research, focusing on training data availability and domain adaptation."
  - [section]: "Suissa, Elmalech, & Zhitomirsky-Geffet (2020) used crowd workers to fix a relatively small set of OCRed documents. Then, the Needleman â€“Wunsch alignment algorithm (Needleman, & Wunsch , 1970) was used to find common confusions between characters committed by the crowd workers."
  - [corpus]: No direct evidence of synthetic data generation for DH in corpus analysis.
- **Break condition**: If the task requires domain expertise beyond common knowledge, crowdsourcing may not be feasible, and synthetic data may not capture the nuances of the domain.

### Mechanism 3
- **Claim**: Domain adaptation of DNN architectures is crucial for achieving high accuracy on DH tasks.
- **Mechanism**: Customizing DNN architectures, preprocessing pipelines, and fine-tuning pre-trained models to the specific characteristics of DH data.
- **Core assumption**: DH data has unique characteristics (e.g., historical language, mixed formats) that require specialized processing.
- **Evidence anchors**:
  - [abstract]: "These supervised algorithms learn patterns from a large number of 'right' and 'wrong' examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation."
  - [section]: "Won et al. (2018) needed to adapt their domain data by 'translating' the XML markup into text sequences that a DNN model can receive as input. In this preprocessing phase, the researchers took into account the metadata that exists in the domain that was embedded in the XML file."
  - [corpus]: Weak evidence; corpus analysis did not find specific examples of DNN architecture adaptation for DH.
- **Break condition**: If the domain adaptation requires extensive preprocessing or architectural changes that are not feasible, DNNs may not be suitable for the task.

## Foundational Learning

- **Concept: Deep learning fundamentals**
  - Why needed here: Understanding the core principles of DNNs, including backpropagation, activation functions, and optimization algorithms, is essential for customizing and fine-tuning models for DH tasks.
  - Quick check question: What is the purpose of the activation function in a neural network?

- **Concept: Natural language processing (NLP)**
  - Why needed here: DH research often involves text analysis, requiring knowledge of NLP techniques such as tokenization, stemming, and named entity recognition.
  - Quick check question: What is the difference between stemming and lemmatization in NLP?

- **Concept: Transfer learning**
  - Why needed here: Transfer learning allows leveraging pre-trained models on large generic corpora and adapting them to niche DH domains, reducing the need for extensive labeled data.
  - Quick check question: What are the key steps involved in fine-tuning a pre-trained model for a specific task?

## Architecture Onboarding

- **Component map**: Input layer (text data encoded as vectors) -> Hidden layers (RNNs, CNNs, or transformers) -> Output layer (task-specific output)
- **Critical path**: Data preprocessing -> Model architecture selection -> Hyperparameter tuning -> Training -> Evaluation -> Fine-tuning
- **Design tradeoffs**:
  - Model complexity vs. training time and resources
  - Pre-trained model size vs. fine-tuning accuracy
  - Task-specific architecture vs. generic architecture
- **Failure signatures**:
  - Overfitting: High accuracy on training data but low accuracy on validation data
  - Underfitting: Low accuracy on both training and validation data
  - Vanishing gradients: Slow convergence or failure to learn long-term dependencies
- **First 3 experiments**:
  1. Fine-tune a pre-trained BERT model for a specific DH task (e.g., named entity recognition on historical texts).
  2. Compare the performance of different RNN architectures (e.g., LSTM, GRU) on a DH text classification task.
  3. Implement a transformer-based architecture for a DH sequence-to-sequence task (e.g., OCR post-correction).

## Open Questions the Paper Calls Out

- **Question**: How does the environmental impact of deep learning in digital humanities compare to its benefits for research outcomes?
- **Basis in paper**: [explicit] The paper discusses the environmental impact of training deep learning models, citing that training a transformer model like BERT produces similar CO2 emissions to air travel.
- **Why unresolved**: The paper acknowledges the environmental impact but does not provide a detailed analysis of how this trade-off affects the decision-making process for DH researchers.
- **What evidence would resolve it**: A comprehensive study comparing the environmental costs of deep learning models with the research benefits they provide in DH projects.

- **Question**: What are the most effective methods for generating synthetic datasets for niche languages in digital humanities research?
- **Basis in paper**: [inferred] The paper discusses the challenge of limited training data in DH and mentions synthetic data generation as a potential solution, but does not provide specific methods for niche languages.
- **Why unresolved**: The paper highlights the need for synthetic data but does not detail how to effectively generate such datasets for less common languages.
- **What evidence would resolve it**: Research demonstrating successful synthetic dataset generation techniques tailored for niche languages used in DH research.

- **Question**: How can digital humanities researchers balance the need for high accuracy with the constraints of limited computational resources?
- **Basis in paper**: [explicit] The paper mentions that training deep learning models requires significant computational resources and budget, which may be a constraint for DH researchers.
- **Why unresolved**: The paper suggests using classical machine learning algorithms for low accuracy tasks but does not provide a detailed framework for balancing accuracy needs with resource limitations.
- **What evidence would resolve it**: A decision-making framework that helps DH researchers determine the optimal balance between accuracy requirements and available computational resources.

## Limitations

- The proposed decision model remains largely theoretical with limited empirical validation across diverse DH tasks
- Effectiveness of suggested strategies is demonstrated through selective case studies rather than systematic evaluation
- Paper does not address computational resource requirements and accessibility challenges faced by DH researchers

## Confidence

- **High Confidence**: The identification of data availability and domain adaptation as key challenges in applying DNNs to DH research
- **Medium Confidence**: The proposed decision model framework and its general applicability to DH scenarios
- **Low Confidence**: Specific performance claims for the suggested strategies across different DH tasks and domains

## Next Checks

1. Implement the decision model on a diverse set of DH tasks (e.g., OCR post-correction, named entity recognition, author attribution) to evaluate its practical utility and identify potential gaps
2. Conduct a systematic comparison of the proposed data augmentation strategies (crowdsourcing vs. synthetic data generation) on representative DH datasets with varying levels of domain expertise requirements
3. Assess the computational resource requirements for implementing the suggested DNN approaches across different DH research contexts and evaluate their accessibility to typical DH research environments