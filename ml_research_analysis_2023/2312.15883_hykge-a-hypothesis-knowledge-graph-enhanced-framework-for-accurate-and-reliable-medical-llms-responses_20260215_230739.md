---
ver: rpa2
title: 'HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable
  Medical LLMs Responses'
arxiv_id: '2312.15883'
source_url: https://arxiv.org/abs/2312.15883
tags:
- knowledge
- graph
- medical
- hykge
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses improving accuracy and reliability of medical\
  \ LLMs by integrating knowledge graphs into the retrieval-augmented generation framework.\
  \ The proposed Hypothesis Knowledge Graph Enhanced (HyKGE) method leverages LLMs\u2019\
  \ reasoning ability to generate hypothesis outputs that guide knowledge graph exploration,\
  \ then uses a fragment reranking module to filter retrieved knowledge."
---

# HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses

## Quick Facts
- arXiv ID: 2312.15883
- Source URL: https://arxiv.org/abs/2312.15883
- Reference count: 40
- Key outcome: HyKGE achieves 47.94% exact match and 54.63% partial correct rate on Chinese medical datasets

## Executive Summary
HyKGE is a framework that improves medical LLM accuracy by integrating knowledge graphs into retrieval-augmented generation. The method leverages LLM reasoning to generate hypothesis outputs that guide knowledge graph exploration, then uses a fragment reranking module to filter retrieved knowledge. Experiments on Chinese medical datasets show HyKGE outperforms baselines with 47.94% exact match and 54.63% partial correct rate, demonstrating improved precision and reliability in medical question answering.

## Method Summary
HyKGE integrates knowledge graphs into medical LLM pipelines through a four-module framework. The system generates hypothesis outputs from user queries using LLMs, extracts medical entities via NER, retrieves knowledge graph paths within k hops, and applies fragment reranking to filter noise. The framework processes two Chinese medical datasets (MMCU-Medical with 2,819 questions and CMB with 4,000 sampled questions) using the CMeKG knowledge graph with entity descriptions from external sources.

## Key Results
- HyKGE achieves 47.94% exact match and 54.63% partial correct rate on Chinese medical datasets
- Outperforms baseline methods in medical question answering accuracy
- Case studies demonstrate HyKGE's ability to correct false answers and provide more precise medical advice through KG integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs' zero-shot reasoning capability compensates for incomplete user queries by generating hypothesis outputs that guide KG exploration
- Mechanism: LLM processes user query alone, generating hypothesis output that expands on query with relevant medical knowledge, then used to extract entities and guide KG retrieval
- Core assumption: LLMs have sufficient domain knowledge from pre-training to generate relevant hypotheses for specialized medical queries
- Evidence anchors:
  - [abstract]: "leverages LLMs' powerful reasoning capacity to compensate for the incompleteness of user queries"
  - [section 4.2]: "Through LLM's understanding, more medical knowledge relevant to the user query is obtained"
  - [corpus]: Weak evidence - no direct citations about zero-shot medical reasoning capability

### Mechanism 2
- Claim: Fragment reranking module improves alignment between user queries and retrieved KG knowledge by using segmented hypothesis output chunks
- Mechanism: Hypothesis output segmented into chunks, used to rerank retrieved KG paths ensuring fine-grained matching between user intent and retrieved knowledge
- Core assumption: Short text segments from hypothesis outputs can effectively match and rerank longer KG paths
- Evidence anchors:
  - [abstract]: "fragment reranking module to filter out noise while ensuring the balance between diversity and relevance in retrieved knowledge"
  - [section 4.5]: "we combine the segmented H O and Q list as: Chunk([Q ||H O])"
  - [corpus]: Weak evidence - no direct citations about fragment-based reranking in medical KGs

### Mechanism 3
- Claim: KG path retrieval (rather than entity-only or subgraph retrieval) provides optimal balance of knowledge richness and noise filtering
- Mechanism: System retrieves paths between entities within k hops, organizing knowledge as "entity name - relationship name - entity name" chains with entity descriptions
- Core assumption: Paths between entities capture meaningful reasoning chains while filtering out irrelevant information compared to full subgraphs
- Evidence anchors:
  - [section 4.4]: "compared to simply collecting graph entities and descriptive knowledge, collecting paths can provide more knowledge on K G"
  - [section 4.4]: "compared to using subgraphs, paths can filter out more irrelevant noise, akin to a pruning operation"
  - [corpus]: Weak evidence - no direct citations about path-based vs subgraph retrieval in medical KGs

## Foundational Learning

- Concept: Named Entity Recognition (NER) for medical entities
  - Why needed here: Essential for extracting medical entities from both user queries and hypothesis outputs to link with KG entities
  - Quick check question: What medical entities would you extract from "Patient with headache and eye pain after eating spicy food"?

- Concept: Knowledge Graph Entity Linking
  - Why needed here: Required to match extracted entities with KG entities using embedding similarity for knowledge retrieval
  - Quick check question: How would you compute similarity between extracted entity "migraine" and KG entities?

- Concept: Dense Passage Retrieval
  - Why needed here: Used for encoding entities and computing similarity scores between extracted entities and KG entities
  - Quick check question: What embedding model would you use for Chinese medical entity retrieval?

## Architecture Onboarding

- Component map: User query -> LLM (hypothesis output) -> NER (entity extraction) -> Embedding (entity linking) -> KG retrieval (paths) -> Reranker (filtering) -> LLM (final answer)

- Critical path: User query → LLM (hypothesis output) → NER (entity extraction) → Embedding (entity linking) → KG retrieval (paths) → Reranker (filtering) → LLM (final answer)

- Design tradeoffs:
  - Path-based vs subgraph retrieval: Paths reduce noise but may miss some context
  - Chunk size in reranking: Larger chunks preserve context but reduce granularity
  - k-hop limit: Higher k retrieves more knowledge but increases noise

- Failure signatures:
  - Poor entity linking: Low similarity scores between extracted and KG entities
  - Noisy paths: Irrelevant entities appear in retrieved paths
  - Ineffective reranking: Reranker doesn't improve relevance of retrieved knowledge

- First 3 experiments:
  1. Test entity linking accuracy with different embedding models on medical entities
  2. Compare path-based vs subgraph retrieval for different k values
  3. Evaluate reranking effectiveness with different chunk sizes on hypothesis outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HyKGE vary when applied to languages other than Chinese, particularly for medical question answering?
- Basis in paper: [inferred] The paper only evaluates HyKGE on Chinese medical datasets and does not explore its generalizability to other languages or medical domains.
- Why unresolved: The current study's scope is limited to Chinese language and medical datasets, leaving the model's effectiveness in multilingual or different medical contexts untested.
- What evidence would resolve it: Experiments applying HyKGE to medical QA datasets in multiple languages, comparing performance across languages and domains.

### Open Question 2
- Question: What is the impact of different knowledge graph structures and completeness on HyKGE's performance?
- Basis in paper: [inferred] While the paper uses CMeKG, it does not systematically analyze how variations in knowledge graph quality, size, or structure affect HyKGE's accuracy and explainability.
- Why unresolved: The evaluation is based on a single knowledge graph, and the paper does not explore the relationship between knowledge graph characteristics and model performance.
- What evidence would resolve it: Comparative experiments using different knowledge graphs with varying structures, completeness, and domain coverage, measuring HyKGE's performance across these variations.

### Open Question 3
- Question: How does HyKGE perform on long-form medical consultations compared to short-answer medical questions?
- Basis in paper: [inferred] The evaluation focuses on multiple-choice questions and short-answer formats, but does not test HyKGE's capabilities in handling complex, multi-turn medical consultations.
- Why unresolved: The paper does not include evaluation scenarios that require sustained reasoning over multiple exchanges or detailed medical advice generation.
- What evidence would resolve it: Testing HyKGE on datasets containing multi-turn medical conversations or complex case studies requiring detailed explanations and follow-up questions.

## Limitations
- Implementation details for key components like hypothesis generation prompts and reranker architecture are not fully specified
- Knowledge graph construction details including scope, entity count, and quality metrics are incomplete
- Dataset details such as CMB original size and sampling methodology are not specified

## Confidence

**High Confidence**: The core claim that integrating knowledge graphs with LLMs can improve medical question answering accuracy is well-supported by experimental results showing 47.94% exact match and 54.63% partial correct rate outperforming baselines.

**Medium Confidence**: The specific mechanism of using hypothesis outputs to guide KG exploration is theoretically sound but relies heavily on LLM's domain knowledge, with performance gains potentially coming from other factors.

**Low Confidence**: The claim about fragment reranking effectiveness is supported by relative performance improvements but lacks ablation studies showing what happens without reranking or with different chunk sizes.

## Next Checks

1. **Ablation Study**: Run experiments with HyKGE but removing the fragment reranking module to quantify its specific contribution to performance improvements, testing whether the 54.63% partial correct rate holds without reranking.

2. **Prompt Engineering Analysis**: Test different hypothesis generation prompt templates on the same LLM to determine how sensitive the HyKGE performance is to prompt engineering versus the underlying architecture.

3. **Generalization Test**: Evaluate HyKGE on a held-out test set from a different medical domain (e.g., traditional Chinese medicine vs. Western medicine) to assess whether the framework generalizes beyond the training distribution or is overfitting to specific medical subdomains.