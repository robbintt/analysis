---
ver: rpa2
title: Does GPT-4 surpass human performance in linguistic pragmatics?
arxiv_id: '2312.09545'
source_url: https://arxiv.org/abs/2312.09545
tags:
- human
- llms
- language
- pragmatics
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  interpret linguistic pragmatics better than humans. Using Grice's communication
  principles, the research tested LLMs (GPT-2, GPT-3, GPT-3.5, GPT-4, and Bard) and
  human subjects on dialogue-based tasks involving implied meanings and context.
---

# Does GPT-4 surpass human performance in linguistic pragmatics?

## Quick Facts
- arXiv ID: 2312.09545
- Source URL: https://arxiv.org/abs/2312.09545
- Reference count: 11
- Key outcome: GPT-4 achieved 4.80/5 score, surpassing best human score of 4.55 in pragmatic interpretation tasks

## Executive Summary
This study investigates whether large language models can interpret linguistic pragmatics better than humans. Using Grice's communication principles, the research tested GPT-2, GPT-3, GPT-3.5, GPT-4, and Bard against human subjects on dialogue-based tasks involving implied meanings and context. GPT-4 achieved the highest score of 4.80 out of 5, surpassing the best human score of 4.55. The average LLM score was 3.39, exceeding human averages of 2.80 (Serbian students) and 2.34 (U.S. participants). LLMs also completed tasks faster than humans on average (11 minutes vs. 41 minutes).

## Method Summary
The study used 20 dialogue-based tasks illustrating violations of Gricean maxims and conventional implicature. Human participants (N=147) including 71 Serbian students and 76 U.S. participants completed the tasks. LLM responses were collected on OpenAI Playground and Bard platform. Models were prompted with dialogues and asked "How would you interpret this dialogue?" Responses were graded using a 1-5 scale based on comprehension of literal meaning and pragmatic implications, with validation by linguists.

## Key Results
- GPT-4 achieved highest score of 4.80/5, surpassing best human score of 4.55
- Average LLM score (3.39) exceeded human averages (2.80 Serbian students, 2.34 U.S. participants)
- LLMs completed tasks faster than humans on average (11 minutes vs. 41 minutes)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs with higher parameter counts and more recent training exhibit superior pragmatic interpretation.
- Mechanism: GPT-4's architecture includes more parameters and longer context windows, allowing it to capture subtle implied meanings in dialogue more accurately than earlier models like GPT-2.
- Core assumption: The performance differences stem from architectural improvements rather than task-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "GPT-4 achieved the highest score of 4.80, surpassing the best human score of 4.55"
  - [section]: "Results revealed the superior performance and speed of LLMs, particularly GPT4, over human subjects in interpreting pragmatics"
  - [corpus]: Weak - no direct comparison of architectural differences in cited papers
- Break condition: If performance gains disappear when comparing models on tasks requiring no pragmatic reasoning, the advantage is specific to pragmatic tasks rather than general capability.

### Mechanism 2
- Claim: Gricean maxim violations serve as effective diagnostic tools for pragmatic comprehension.
- Mechanism: By presenting dialogues where speakers intentionally violate maxims (quantity, quality, relation, manner), the task forces models to detect non-literal meanings and infer implied intentions.
- Core assumption: Both humans and LLMs use similar pragmatic inference mechanisms when encountering maxim violations.
- Evidence anchors:
  - [section]: "Grice's communication principles" and "dialogue-based tasks involving implied meanings and context"
  - [section]: "Each illustrating a violation of one of the Gricean maxims"
  - [corpus]: Weak - cited papers don't address Gricean maxim-based evaluation methods
- Break condition: If models perform equally well on literal comprehension tasks but poorly on pragmatic tasks, the evaluation specifically measures pragmatic ability rather than general language understanding.

### Mechanism 3
- Claim: Speed advantage in LLMs reflects architectural efficiency in processing contextual information.
- Mechanism: LLMs can process all dialogue context simultaneously and apply learned pragmatic patterns faster than humans who must read sequentially and consciously reason through implications.
- Core assumption: Faster completion time indicates more efficient pragmatic processing rather than superficial pattern matching.
- Evidence anchors:
  - [abstract]: "LLMs also completed tasks faster than humans on average (11 minutes vs. 41 minutes)"
  - [section]: "LLMs demonstrated a higher level of efficiency when compared with human subjects"
  - [corpus]: Weak - no corpus evidence on processing speed differences for pragmatic tasks
- Break condition: If speed advantage disappears when controlling for task complexity or if rapid responses show lower accuracy, efficiency gains may be superficial.

## Foundational Learning

- Concept: Gricean Maxims and conversational implicature
  - Why needed here: The entire evaluation framework relies on detecting when speakers violate these maxims to convey implied meanings
  - Quick check question: Can you explain how the Maxim of Quality violation in dialogue 5 ("City started the game without a goalkeeper") creates ironic meaning?

- Concept: Pragmatic vs. semantic interpretation
  - Why needed here: Distinguishing between literal meaning and intended meaning is central to the task
  - Quick check question: How would you differentiate between a semantic misunderstanding and a pragmatic failure in interpreting dialogue 3?

- Concept: Context-dependent meaning extraction
  - Why needed here: Pragmatic interpretation requires integrating dialogue context with world knowledge
  - Quick check question: What contextual elements in dialogue 11 (train seat example) are necessary to resolve the apparent contradiction?

## Architecture Onboarding

- Component map: Model selection (GPT-2, GPT-3, GPT-3.5, GPT-4, Bard) -> Task input (dialogue) -> Scoring rubric (1-5 scale) -> Human vs. LLM comparison -> Validation by linguists
- Critical path: Task design -> Model evaluation -> Human participant recruitment -> Response grading -> Comparative analysis
- Design tradeoffs: Using Gricean maxims provides theoretical grounding but may not capture all pragmatic phenomena; human participants limited to English speakers may reduce generalizability
- Failure signatures: Inconsistent grading across human evaluators, LLM performance plateauing at certain complexity levels, human participants failing to recognize maxim violations
- First 3 experiments:
  1. Replicate the evaluation with a smaller subset of dialogues to verify scoring consistency
  2. Test model performance on literal vs. pragmatic comprehension tasks to isolate pragmatic advantage
  3. Conduct cross-linguistic evaluation to test cultural and language dependencies of pragmatic interpretation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT-4's superior performance in linguistic pragmatics extend to languages other than English?
- Basis in paper: [inferred] The study was restricted to English, and the authors note this as a limitation
- Why unresolved: The research only tested English-language dialogues, leaving open whether GPT-4's pragmatic understanding generalizes across languages
- What evidence would resolve it: Testing GPT-4's performance on parallel dialogue tasks in multiple languages with native speakers

### Open Question 2
- Question: What specific training mechanisms enabled GPT-4 to acquire pragmatic competence?
- Basis in paper: [explicit] The authors note "these models were not specifically trained for this task" and "these models have somehow managed to learn pragmatic rules in their training phase"
- Why unresolved: The study demonstrates GPT-4's capability but doesn't investigate the underlying learning process
- What evidence would resolve it: Analysis of training data for pragmatic examples or ablation studies testing model components

### Open Question 3
- Question: Does GPT-4's pragmatic understanding match or exceed that of diverse human populations?
- Basis in paper: [explicit] The study used predominantly young, educated Serbian participants and noted this as a limitation
- Why unresolved: The human sample lacked demographic diversity, making it unclear if GPT-4 truly surpasses general human performance
- What evidence would resolve it: Testing GPT-4 against diverse human populations across age, education, culture, and language backgrounds

## Limitations

- Study assumes Gricean maxims capture full scope of pragmatic competence, representing only one theoretical framework
- Human participant pool consisted primarily of students from specific geographic locations, limiting cultural generalizability
- Claims about "superior pragmatic interpretation capabilities" may not extend beyond the specific evaluation framework used

## Confidence

- **High Confidence**: GPT-4's superior performance on the specific Gricean maxim violation tasks (4.80 vs. 4.55 human maximum)
- **Medium Confidence**: Claims about LLM efficiency advantages (11 vs. 41 minutes) due to potential variations in participant testing conditions
- **Low Confidence**: Broad claims about "superior pragmatic interpretation capabilities" beyond the specific evaluation framework

## Next Checks

1. **Cross-cultural validation**: Replicate the evaluation with diverse participant pools across different linguistic and cultural backgrounds to test the robustness of human baselines.

2. **Alternative theoretical frameworks**: Test models using alternative pragmatic theories (relevance theory, neo-Gricean approaches) to assess whether GPT-4's advantage extends beyond Gricean frameworks.

3. **Transfer task evaluation**: Design tasks where models must apply pragmatic understanding to novel domains (e.g., legal contracts, technical documentation) to test practical utility beyond dialogue comprehension.