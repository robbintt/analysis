---
ver: rpa2
title: 'CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization
  of Clinical Trial Descriptions'
arxiv_id: '2307.14522'
source_url: https://arxiv.org/abs/2307.14522
tags:
- trials
- clinical
- clinidigest
- summaries
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CliniDigest is a tool that summarizes clinical trial descriptions
  using GPT-3.5, addressing the challenge of keeping up with the large volume of trials
  submitted daily. The tool can condense up to 85 clinical trial descriptions (approximately
  10,500 words) into a concise 200-word summary with references.
---

# CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions

## Quick Facts
- arXiv ID: 2307.14522
- Source URL: https://arxiv.org/abs/2307.14522
- Reference count: 35
- CliniDigest can reduce up to 85 clinical trial descriptions into a concise 200-word summary with references

## Executive Summary
CliniDigest addresses the challenge of information overload in clinical research by using GPT-3.5 to summarize large numbers of clinical trial descriptions. The system processes trials from ClinicalTrials.gov, batching them and recursively summarizing to create concise, reference-backed summaries of approximately 150-250 words. Tested on 457 trials across 27 medical subdomains, the tool achieves 54% source utilization and aims to provide real-time, readable summaries for clinical research coordinators while minimizing hallucinations through careful prompt engineering.

## Method Summary
CliniDigest extracts clinical trial data from ClinicalTrials.gov, filters for trials with at least 50 participants, and annotates them into 27 medical subdomains. The system batches trials (15 per batch) and uses GPT-3.5 Turbo with carefully crafted prompts emphasizing "thesis" or "argument" directives to generate intermediate summaries. These are recursively condensed into final 150-250 word summaries while maintaining reference citations. The pipeline employs temperature set to 0 to minimize hallucinations and evaluates readability using the SMOG formula.

## Key Results
- Generated summaries averaging 153 words using 54% of source trials on average
- Successfully reduced up to 85 clinical trial descriptions (approximately 10,500 words) into 200-word summaries
- SMOG readability scores comparable to or better than raw trial descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5 can be prompted to produce concise, reference-backed summaries of clinical trials despite the 4096-token context limit.
- Mechanism: The system splits trials into batches of 15, generates intermediate summaries, then recursively condenses those into a final 200-word summary. This avoids token overflow while preserving source fidelity.
- Core assumption: GPT-3.5 can maintain coherence and factual accuracy when summarizing within the per-batch context limit and when instructed to cite sources explicitly.
- Evidence anchors: [abstract] "CliniDigest can reduce up to 85 clinical trial descriptions (approximately 10,500 words) into a concise 200-word summary with references"; [section 3] "we split the studies into batches of 15 clinical trials, which were recursively summarized until it was condensed into one succinct summary"

### Mechanism 2
- Claim: Prompt engineering with "thesis" or "argument" directives yields more cohesive multi-trial summaries than generic "summary" prompts.
- Mechanism: GPT-3.5 is instructed to extract relevant information across trials to construct a unified thesis, which discourages isolated trial-by-trial summaries.
- Core assumption: The choice of prompt wording significantly influences GPT-3.5's ability to generalize across documents rather than listing them.
- Evidence anchors: [section 3] "we experimented with a variety of prompts that prioritized generalization through keywords such as 'summary', 'essay', and 'argument.' We found that 'thesis' and 'argument' work the best"; [section 3] "lackadaisical prompts often caused GPT-3.5 to simply summarize every study individually"

### Mechanism 3
- Claim: Instructing GPT-3.5 to cite references reduces hallucinations in the generated summary.
- Mechanism: By requiring references whenever possible, GPT-3.5 is constrained to draw directly from the provided trial texts, limiting fabrication.
- Core assumption: GPT-3.5's hallucination propensity can be partially mitigated through explicit citation requirements.
- Evidence anchors: [abstract] "limited hallucinations" and "utilizes ðœ‡ = 54%, ðœŽ = 30% of the sources"; [section 3] "we prompted GPT-3.5 to use references whenever possible" and "tuning the temperature, which controls creativity, to 0 and prompt engineering for explainability reduced hallucinations"

## Foundational Learning

- Concept: Clinical trial registration and structure
  - Why needed here: Summaries must be generated from raw ClinicalTrials.gov data; understanding trial components (title, description, status) is essential for correct parsing and summarization.
  - Quick check question: What are the mandatory fields in a ClinicalTrials.gov entry that would be used as input for CliniDigest?

- Concept: Prompt engineering and few-shot learning in LLMs
  - Why needed here: The system relies on carefully crafted prompts to guide GPT-3.5's summarization style, tone, and citation behavior.
  - Quick check question: How does changing the prompt's directive ("thesis" vs "summary") affect the coherence of GPT-3.5 outputs?

- Concept: Token limits and context windows in transformer models
  - Why needed here: The 4096-token constraint shapes the batch-splitting logic; understanding how token counting works is critical for tuning batch size.
  - Quick check question: How many tokens would 15 average clinical trials (1500 words each) likely consume, and how does this relate to the batch size choice?

## Architecture Onboarding

- Component map: Data extraction -> Preprocessor -> GPT-3.5 summarizer -> Recursive condenser -> Output formatter
- Critical path: 1. Fetch trials â†’ 2. Batch â†’ 3. Intermediate summarization â†’ 4. Recursive condensation â†’ 5. Final output
- Design tradeoffs:
  - Batch size vs. fidelity: Smaller batches reduce hallucination risk but increase processing steps.
  - Prompt verbosity vs. token budget: Longer prompts consume tokens but improve instruction clarity.
  - Reference density vs. readability: More citations improve traceability but can reduce narrative flow.
- Failure signatures:
  - Output significantly shorter/longer than target range.
  - Low reference usage (<20%) indicating hallucination or omission.
  - SMOG score much higher than source trials, suggesting overly technical language.
- First 3 experiments:
  1. Run a single batch of 15 trials; verify intermediate summary length and reference count.
  2. Execute full pipeline on a small medical field (e.g., 5 trials) to confirm recursive condensation works.
  3. Compare SMOG scores between raw trial text and generated summary to validate readability preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the quality of CliniDigest summaries be empirically evaluated for clinical research coordinators?
- Basis in paper: [explicit] Section 4.2 outlines a systematic evaluation plan involving participants writing summaries after reading CliniDigest outputs versus raw trial links.
- Why unresolved: The evaluation plan is described but has not been conducted yet.
- What evidence would resolve it: Conducting the evaluation as planned and comparing ROUGE-L F1 scores between CliniDigest and human-written summaries, as well as qualitative feedback from participants.

### Open Question 2
- Question: What are the limitations of using GPT-3.5 for clinical trial summarization and how can they be mitigated?
- Basis in paper: [explicit] Section 2 discusses limitations of large language models like hallucinations and lack of factuality, which are relevant concerns for CliniDigest.
- Why unresolved: While the paper mentions these limitations and attempts to mitigate them through prompt engineering, it does not provide a comprehensive analysis of the limitations or optimal mitigation strategies.
- What evidence would resolve it: A thorough analysis of the types and frequencies of hallucinations or factual errors in CliniDigest outputs, along with experiments testing different mitigation strategies like adjusting temperature, using few-shot prompting, or incorporating fact-checking modules.

### Open Question 3
- Question: How does the readability of CliniDigest summaries compare to other medical literature summarization methods?
- Basis in paper: [inferred] The paper uses SMOG readability tests to evaluate CliniDigest summaries, but does not compare the results to other summarization methods.
- Why unresolved: The SMOG test results only provide a baseline for CliniDigest's readability, not a comparison to other approaches.
- What evidence would resolve it: Conducting SMOG readability tests on summaries generated by other medical literature summarization methods (e.g., extractive summarization, rule-based summarization) and comparing the results to CliniDigest's scores.

## Limitations
- Evaluation relies on automated readability metrics rather than human assessment of clinical utility
- Claims of "limited hallucinations" based on reference usage rates rather than systematic hallucination detection
- 457-trial corpus may not represent full complexity of clinical trial literature
- Recursive summarization may introduce fidelity loss across multiple generation passes

## Confidence

- **High Confidence**: Technical feasibility of batch-splitting and recursive summarization; basic pipeline architecture; SMOG readability comparison methodology
- **Medium Confidence**: Effectiveness of "thesis" and "argument" prompt keywords; claim that citation requirements reduce hallucinations; generalizability across 27 medical subdomains
- **Low Confidence**: Clinical utility for actual research coordinators; absolute rate of hallucinations without systematic detection; scalability to thousands of daily trials

## Next Checks

1. Implement systematic hallucination detection by comparing summary claims against source trial texts using semantic similarity metrics and fact-checking specific statements across 50 randomly sampled summaries.

2. Recruit 10 clinical research coordinators to assess summary quality, completeness, and utility compared to reading raw trial descriptions, measuring time savings and decision-making confidence.

3. Stratify summary quality metrics (reference usage, readability, coherence) by medical field to identify which subdomains perform best/worst and investigate whether certain trial structures or medical jargon affect summarization quality.