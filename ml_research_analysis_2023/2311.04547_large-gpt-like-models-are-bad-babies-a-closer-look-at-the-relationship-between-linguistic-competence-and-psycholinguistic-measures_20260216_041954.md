---
ver: rpa2
title: 'Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between
  Linguistic Competence and Psycholinguistic Measures'
arxiv_id: '2311.04547'
source_url: https://arxiv.org/abs/2311.04547
tags:
- language
- reading
- size
- babylm
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large GPT-like models trained on the BabyLM corpus perform well
  on formal and functional linguistic competence tasks (BLiMP, GLUE, MSGS) but poorly
  on predicting human reading times. While model performance improves with size for
  the challenge tasks, reading time fit is best with small, shallow models.
---

# Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures

## Quick Facts
- arXiv ID: 2311.04547
- Source URL: https://arxiv.org/abs/2311.04547
- Reference count: 15
- Primary result: Larger GPT-like models perform better on linguistic competence tasks but worse at predicting human reading times.

## Executive Summary
This paper investigates the relationship between model size and performance on linguistic competence tasks (BLiMP, GLUE, MSGS) versus reading time prediction using GPT-like language models. The study finds a negative correlation between model size and reading time fit, with smaller models performing better at predicting human processing effort. While larger models excel at formal and functional linguistic competence, their ability to model incremental human language processing deteriorates with size. The results suggest that modeling processing effort requires fundamentally different approaches than training large GPT-like models for linguistic competence.

## Method Summary
The researchers trained OPT architecture transformer models with varying hidden sizes (192-1536) and decoder layers (1-24) on the BabyLM corpus (100M tokens). Models were trained for one epoch with short sequences followed by four epochs with full sequences. Performance was evaluated on linguistic competence tasks (BLiMP, GLUE, MSGS) and reading time prediction using the Natural Stories corpus. Linear mixed-effects models were used to quantify how well LM surprisal predicted human reading times while controlling for word-level and subject-level variability. The study examined how model size affected performance across these different evaluation paradigms.

## Key Results
- Larger models show significant improvements on BLiMP, GLUE, and MSGS linguistic competence tasks.
- Reading time fit shows a negative correlation with model size (ρ = -51.39, p < 0.05).
- The second-smallest model (1*192) achieved the largest log-likelihood reduction over baseline for reading time prediction.
- Multi-epoch training on small datasets does not meaningfully improve reading time fit compared to single epoch.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger GPT-like models perform better on formal and functional linguistic competence tasks but worse at predicting human reading times.
- Mechanism: Model capacity improves formal and functional competence due to more parameters capturing syntactic and semantic generalizations, but also introduces over-reliance on full sequence context which reduces fit to human processing effort.
- Core assumption: Human language processing is incremental and local, while large transformers use global context for predictions.
- Evidence anchors:
  - [abstract]: "negative correlation between LM size and reading time fit... with the second-smallest LM achieving the largest log-likelihood reduction over a baseline model"
  - [section 6]: "reading time fit was negatively correlated with model size (ρ = -51.39, p < 0.05)"
- Break condition: If reading time fit improved with model size, this mechanism would be invalidated.

### Mechanism 2
- Claim: Multi-epoch training on small datasets can maintain or improve reading time fit without degradation.
- Mechanism: Repeating training data before reaching the optimal token budget (2B tokens) does not necessarily harm reading time fit; the issue is more related to the mismatch between corpus domain and reading time stimuli.
- Core assumption: The deterioration in reading time fit is primarily due to token budget and domain mismatch, not repetition per se.
- Evidence anchors:
  - [section 7]: "exposing a transformer model to multiple repetitions... does not lead to a decrease in reading time fit, but also does not improve over the single epoch setting in a meaningful way"
  - [section 6]: "domain of the training data differs considerably from the data in the Natural Stories corpus"
- Break condition: If single-epoch training always outperformed multi-epoch for reading time tasks, this mechanism would be invalid.

### Mechanism 3
- Claim: Model width (hidden size) and depth (number of layers) interact differently across tasks; for reading times, shallow models with small hidden sizes perform best.
- Mechanism: Reading time prediction benefits from models that approximate incremental, local processing, which is more closely approximated by shallow architectures with fewer parameters.
- Core assumption: Human reading is sequential and local, so models that mimic this with fewer layers and smaller hidden sizes are more predictive.
- Evidence anchors:
  - [section 6]: "best fit on self-paced reading times... was obtained with the second-smallest model, with models with lhidden > 192 only slightly improving over the baseline"
  - [section 3]: "We therefore employ decoder-only, GPT-like LMs... we want to answer the following research questions: Are GPT-like models cognitively plausible... while being also predictive of human processing effort?"
- Break condition: If deeper models with larger hidden sizes consistently outperformed shallow ones on reading time prediction, this mechanism would fail.

## Foundational Learning

- Concept: Linear mixed-effects models (LMEs) for psycholinguistic data analysis
  - Why needed here: LMEs are used to quantify how well LM surprisal predicts human reading times by controlling for word-level and subject-level variability.
  - Quick check question: What is the role of random intercepts for word, subject, and item in the LME formula?

- Concept: Spearman correlation as a non-parametric measure of monotonic relationships
  - Why needed here: Spearman correlation is used to assess the relationship between model size and performance across different tasks without assuming linearity.
  - Quick check question: What does a negative Spearman correlation between model size and reading time fit imply?

- Concept: Autoregressive (causal) language modeling vs. masked language modeling
  - Why needed here: The study uses decoder-only GPT-like models because they better mirror human incremental processing compared to masked models like BERT.
  - Quick check question: Why might masked LMs be less cognitively plausible for reading time prediction than causal LMs?

## Architecture Onboarding

- Component map:
  - OPT architecture (decoder-only transformer)
  - Variable hidden sizes (192, 384, 768, 1536)
  - Variable number of decoder layers (1, 2, 4, 8, 16, 24)
  - Shortformer training regime (initial short sequence, then full length)
  - ALiBi positional embeddings (no learned positional embeddings)

- Critical path:
  1. Pretrain models on BabyLM corpus with 1 epoch initial short sequence, then 4 epochs full sequence
  2. Evaluate on BLiMP, GLUE, MSGS for linguistic competence
  3. Compute surprisal on Natural Stories corpus
  4. Fit LME models to reading times using surprisal as predictor
  5. Analyze correlation between model size and task performance

- Design tradeoffs:
  - Larger models: better on linguistic competence, worse on reading time fit
  - Smaller models: worse on linguistic competence, better on reading time fit
  - Multi-epoch training: necessary for small datasets but may not improve reading time fit
  - Domain mismatch: BabyLM corpus vs. Natural Stories may confound reading time results

- Failure signatures:
  - Overfitting on BabyLM corpus (high training perplexity, low validation perplexity)
  - Unstable GLUE fine-tuning results (high variance across runs)
  - Poor reading time fit despite low LM perplexity
  - No improvement in linguistic competence with increasing model size

- First 3 experiments:
  1. Train a small model (1*192) and a large model (24*1536) on BabyLM; compare performance on BLiMP, GLUE, MSGS, and reading time fit.
  2. Vary the number of pretraining epochs (1 vs. 5) for a fixed model size; measure impact on reading time fit.
  3. Retrain models on Wikitext-103 (closer domain to Natural Stories); compare reading time fit to BabyLM-trained models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model architectures (e.g., transformer-based vs. recurrent models) compare in their ability to model both linguistic competence and processing effort?
- Basis in paper: [inferred] The paper focuses on GPT-like transformer models but mentions that similar relationships between model size and reading time fit have been found for LSTM LMs, suggesting this could be a broader phenomenon across architectures.
- Why unresolved: The study only investigates transformer-based models, so direct comparisons with other architectures are not available.
- What evidence would resolve it: A systematic comparison of different model architectures (e.g., transformers, LSTMs, CNNs) trained on the same data and evaluated on the same tasks.

### Open Question 2
- Question: What is the optimal training data size for modeling both linguistic competence and processing effort simultaneously?
- Basis in paper: [explicit] The paper notes that reading time fit improves with larger training data sizes (up to 2B tokens), while linguistic competence tasks may require less data. It also discusses the impact of multi-epoch training on the BabyLM corpus.
- Why unresolved: The study uses a fixed dataset size (100M tokens) and does not systematically explore the effects of varying the training data size.
- What evidence would resolve it: Training models on datasets of varying sizes and evaluating their performance on both linguistic competence tasks and reading time prediction.

### Open Question 3
- Question: How do different fine-tuning strategies affect the trade-off between linguistic competence and processing effort modeling?
- Basis in paper: [explicit] The paper mentions that fine-tuning on GLUE was unstable and suggests that optimizing hyperparameters for each task individually could change the results.
- Why unresolved: The study uses default fine-tuning hyperparameters, which may not be optimal for all tasks.
- What evidence would resolve it: Systematic exploration of different fine-tuning strategies (e.g., different learning rates, optimizers, regularization techniques) and their impact on task performance.

## Limitations
- Results are based on a single training corpus (BabyLM) and reading time dataset (Natural Stories), limiting generalizability.
- The negative correlation between model size and reading time fit could be influenced by domain mismatch rather than inherent architectural limitations.
- The study does not explore alternative model architectures beyond GPT-like transformers.

## Confidence

- **High Confidence**: The claim that larger models perform better on formal and functional linguistic competence tasks (BLiMP, GLUE, MSGS) is well-supported by the data, with clear improvements in accuracy across all three benchmarks as model size increases.

- **Medium Confidence**: The assertion that shallow, smaller models best predict human reading times is supported by the data but requires caution due to potential confounding factors, particularly the domain mismatch between training and evaluation corpora.

- **Medium Confidence**: The claim that multi-epoch training does not improve reading time fit is based on limited experimentation and may not generalize to other training regimes or datasets.

## Next Checks
1. **Domain Transfer Test**: Train the same model architectures on Wikitext-103 (closer domain to Natural Stories) and compare reading time fit results to BabyLM-trained models to isolate the effect of domain mismatch from architectural factors.

2. **Cross-linguistic Validation**: Evaluate the trained models on reading time data from multiple languages to determine whether the observed relationship between model size and reading time fit is language-specific or a general phenomenon.

3. **Alternative Architecture Comparison**: Train a small masked language model (e.g., BERT) and a small causal model (e.g., GPT) with comparable parameter counts to determine whether the causal architecture itself is responsible for better reading time prediction or if other factors (like smaller size) are more important.