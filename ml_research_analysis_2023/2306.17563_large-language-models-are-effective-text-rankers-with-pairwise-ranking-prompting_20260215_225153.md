---
ver: rpa2
title: Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting
arxiv_id: '2306.17563'
source_url: https://arxiv.org/abs/2306.17563
tags:
- ranking
- llms
- passage
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of document ranking using Large
  Language Models (LLMs) without requiring fine-tuning or relying on commercial APIs.
  The key insight is that existing pointwise and listwise ranking formulations are
  too complex for LLMs, often leading to poor or inconsistent outputs.
---

# Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting

## Quick Facts
- arXiv ID: 2306.17563
- Source URL: https://arxiv.org/abs/2306.17563
- Reference count: 6
- Primary result: Achieves state-of-the-art ranking performance on TREC-DL2020 using moderate-sized LLMs without fine-tuning or commercial APIs.

## Executive Summary
This paper addresses document ranking using Large Language Models (LLMs) without requiring fine-tuning or commercial APIs. The authors propose Pairwise Ranking Prompting (PRP), a simple prompting paradigm that compares document pairs using a query, significantly reducing task complexity for LLMs. PRP supports both generation and scoring APIs and is robust to input ordering. On standard benchmarks, PRP achieves state-of-the-art ranking performance using moderate-sized open-sourced LLMs, outperforming previous approaches based on much larger models.

## Method Summary
The method tackles document ranking by breaking down the complex ranking task into pairwise document comparisons using LLMs. Instead of asking LLMs to rank entire lists or assign calibrated relevance scores, PRP asks them to compare only two documents at a time for relevance to a query. The pairwise preference judgments are then aggregated to produce the final ranking. PRP supports both generation and scoring LLM APIs by restricting outputs to two predefined choices ("Passage A" or "Passage B"), and is designed to be robust to input ordering through double prompting with swapped document order. The approach achieves competitive results on TREC-DL2019/2020 benchmarks using moderate-sized open-sourced LLMs like FLAN-UL2-20B.

## Key Results
- PRP based on the 20B-parameter Flan-UL2 model outperforms GPT-4-based approaches by over 5% at NDCG@1 on TREC-DL2020.
- PRP-Sliding-K variant achieves competitive results with linear complexity (O(N)) and fewer API calls.
- PRP demonstrates robustness to scoring vs generation API modes and input ordering effects.
- State-of-the-art ranking performance is achieved on standard benchmarks using moderate-sized open-sourced LLMs without fine-tuning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise Ranking Prompting reduces the complexity burden on LLMs by breaking the ranking task into pairwise document comparisons.
- Mechanism: Instead of asking LLMs to rank a list of documents or assign calibrated relevance scores, PRP asks LLMs to compare only two documents at a time for relevance to a query. The pairwise preference judgments are then aggregated to produce the final ranking.
- Core assumption: LLMs understand and can consistently judge pairwise relative relevance without requiring fine-tuning.
- Evidence anchors:
  - [abstract] "Pairwise Ranking Prompting (PRP), a simple prompting paradigm that compares document pairs using a query, significantly reducing the task complexity for LLMs."
  - [section 3.1] "Our pairwise ranking prompt is simple and intuitive... This pairwise prompting will serve the basic computation unit in all PRP variants."
  - [corpus] Weak - corpus does not directly confirm LLM understanding of pairwise comparisons.
- Break condition: If LLMs produce inconsistent or irrelevant outputs for pairwise comparisons, the aggregation step would fail.

### Mechanism 2
- Claim: PRP supports both generation and scoring APIs by only requiring binary outputs ("Passage A" or "Passage B").
- Mechanism: The pairwise prompt design restricts LLM outputs to two predefined choices, enabling use of scoring APIs that return log-likelihoods of each choice. This also mitigates generation failures.
- Core assumption: The LLM's log-likelihood for each choice correlates with relative relevance.
- Evidence anchors:
  - [abstract] "PRP naturally supports both generation and scoring LLM APIs."
  - [section 3.1] "Furthermore, as we focus on open-sourced LLMs, getting probabilities from LLMs is simple."
  - [section 4.6] "Our results above are all based on the scoring mode... PRP is extremely robust to scoring vs generation API."
- Break condition: If the LLM's log-likelihoods are poorly calibrated or uncorrelated with relevance, the pairwise scores will be meaningless.

### Mechanism 3
- Claim: PRP is robust to input ordering because each document pair is evaluated twice with swapped order, and aggregation uses majority voting.
- Mechanism: For each document pair, two promptings are issued (d1 vs d2 and d2 vs d1). Consistent judgments are taken as evidence; inconsistent ones are treated as ties. This design neutralizes ordering effects.
- Core assumption: Swapping document order in the prompt does not drastically change the LLM's pairwise judgment.
- Evidence anchors:
  - [abstract] "PRP... is robust to input ordering."
  - [section 3.1] "Since it is known that LLMs can be sensitive to text orders in the prompt... for each pair of documents, we will inquire the LLM twice by swapping their order."
  - [section 4.5] "PRP-Allpair is quite robust to initial ordering... PRP-Sliding-10 is quite robust since it focuses on Top-K ranking metrics."
- Break condition: If LLM outputs are highly sensitive to input order even with swapped prompts, aggregation will produce unreliable results.

## Foundational Learning

- Concept: Text ranking and the distinction between pointwise, listwise, and pairwise approaches.
  - Why needed here: Understanding these paradigms is essential to grasp why PRP's pairwise formulation is advantageous.
  - Quick check question: What are the main differences between pointwise, listwise, and pairwise ranking methods?

- Concept: Prompt engineering and zero-shot learning with LLMs.
  - Why needed here: PRP is a prompting technique that does not require fine-tuning, so understanding how to craft effective prompts is critical.
  - Quick check question: How does the design of a prompt affect the outputs of a zero-shot LLM?

- Concept: Computational complexity analysis (O(N²), O(N log N), O(N)).
  - Why needed here: PRP variants have different efficiency profiles, and understanding their complexity helps choose the right variant for a use case.
  - Quick check question: What is the computational complexity of comparing all pairs of N documents?

## Architecture Onboarding

- Component map: Query processor -> Document retriever -> Pairwise ranker (PRP) -> Aggregator -> Ranked results

- Critical path:
  1. Receive query.
  2. Retrieve candidate documents.
  3. For each document pair, send prompt to LLM (twice, with swapped order).
  4. Aggregate pairwise preferences into document scores.
  5. Sort documents by score and return ranked list.

- Design tradeoffs:
  - PRP-Allpair: High accuracy, O(N²) API calls, insensitive to input order.
  - PRP-Sorting: Lower API calls, O(N log N), still robust to order.
  - PRP-Sliding-K: Fewest API calls, O(N), but sensitive to input order for small K.

- Failure signatures:
  - Inconsistent pairwise judgments across swapped prompts.
  - LLM returns non-binary or irrelevant outputs.
  - Aggregation produces ties or falls back to initial ranking.

- First 3 experiments:
  1. Run PRP-Allpair on a small dataset (e.g., 10 docs) and verify output ranking stability under input order changes.
  2. Compare PRP-Sorting vs PRP-Sliding-10 on the same dataset to measure trade-offs in API calls and ranking quality.
  3. Test PRP with both scoring and generation APIs to confirm robustness to API mode.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ranking-aware LLMs be developed in a data-efficient manner while maintaining their generality for other tasks?
- Basis in paper: [explicit] The paper mentions this as a challenging research direction in the Limitations and Discussions section.
- Why unresolved: Developing LLMs that are both ranking-aware and maintain their generality for other tasks is a complex problem that requires balancing specialized training with preserving the model's ability to perform well on diverse tasks.
- What evidence would resolve it: Successful development of a ranking-aware LLM that demonstrates improved performance on ranking tasks without significant degradation on other tasks would provide evidence for this direction.

### Open Question 2
- Question: How can LLMs be adapted to non-standard ranking datasets, such as those requiring counter-argument ranking?
- Basis in paper: [explicit] The paper mentions this as a limitation in the Limitations and Discussions section, noting that the datasets used are for standard relevance-based text ranking.
- Why unresolved: Adapting LLMs to non-standard ranking datasets requires understanding the unique characteristics of these datasets and developing appropriate prompting or fine-tuning strategies.
- What evidence would resolve it: Successful application of PRP or other LLM-based ranking methods to non-standard ranking datasets, such as ArguAna, with competitive performance would demonstrate progress in this area.

### Open Question 3
- Question: How can the number of calls to LLMs be further reduced while maintaining or improving ranking performance?
- Basis in paper: [explicit] The paper discusses efficiency concerns and proposes variants of PRP to address them, but notes that further reducing LLM calls is an interesting research direction.
- Why unresolved: Balancing the trade-off between the number of LLM calls and ranking performance is challenging, as reducing calls may lead to decreased accuracy.
- What evidence would resolve it: Development of a more efficient variant of PRP or a new method that significantly reduces the number of LLM calls while maintaining or improving ranking performance would provide evidence for this direction.

## Limitations
- The method's generalizability to out-of-domain queries or languages other than English remains untested.
- PRP-Allpair may not scale efficiently for large document sets due to O(N²) API calls.
- The robustness of LLM-based pairwise judgments may vary across models or domains, especially for noisy or adversarial queries.

## Confidence
- High: Effectiveness of pairwise prompting in reducing task complexity and supporting both generation and scoring APIs.
- Medium: Claim that PRP is robust to input ordering, as this depends on the consistency of LLM outputs.
- Low: Assertion that PRP outperforms GPT-4-based methods on NDCG@1, as this may be specific to the chosen datasets and evaluation settings.

## Next Checks
1. Test PRP-Allpair on a dataset with 1000+ documents to assess scalability and API call limits.
2. Evaluate PRP on a multilingual query set to verify cross-lingual robustness.
3. Compare PRP against a supervised pairwise ranking model (e.g., DeFT) on the same benchmarks to quantify the trade-off between zero-shot simplicity and fine-tuned accuracy.