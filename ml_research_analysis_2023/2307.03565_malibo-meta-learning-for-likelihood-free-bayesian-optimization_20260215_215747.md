---
ver: rpa2
title: 'MALIBO: Meta-learning for Likelihood-free Bayesian Optimization'
arxiv_id: '2307.03565'
source_url: https://arxiv.org/abs/2307.03565
tags:
- function
- malibo
- task
- optimization
- meta-learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MALIBO, a meta-learning approach for Bayesian
  optimization that directly learns acquisition functions without relying on surrogate
  models. The method addresses limitations of existing meta-learning BO methods, such
  as scalability issues and sensitivity to different scales and noise types across
  tasks.
---

# MALIBO: Meta-learning for Likelihood-free Bayesian Optimization

## Quick Facts
- arXiv ID: 2307.03565
- Source URL: https://arxiv.org/abs/2307.03565
- Reference count: 40
- Primary result: MALIBO outperforms state-of-the-art meta-learning BO methods in benchmarks, showing strong anytime performance and robustness to heterogeneous scale and noise

## Executive Summary
MALIBO introduces a novel meta-learning approach for Bayesian optimization that directly learns acquisition functions without relying on surrogate models. By reframing BO as a density ratio estimation problem, MALIBO bypasses the scalability limitations of Gaussian processes and is robust to different scales and noise types across tasks. The method employs a probabilistic meta-learning model to capture task uncertainty and uses gradient boosting for robust adaptation to new tasks, demonstrating superior performance compared to existing meta-learning BO methods.

## Method Summary
MALIBO learns acquisition functions directly by estimating the density ratio between promising and non-promising configurations across tasks. The method uses a meta-learning classifier with task-specific embeddings to capture task uncertainty, adapts to new tasks using Laplace approximation and Thompson sampling, and employs gradient boosting as a residual model for robust task-specific refinement. This approach avoids the computational burden of GP-based surrogate modeling while maintaining strong anytime performance and adaptability across heterogeneous optimization landscapes.

## Key Results
- Outperforms state-of-the-art meta-learning BO methods on various benchmarks
- Demonstrates strong anytime performance with rapid convergence
- Shows robustness to heterogeneous scale and noise across different optimization tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method directly learns acquisition functions without surrogate models, bypassing the limitations of GP scaling and noise assumptions.
- Mechanism: By learning two distributions ℓ(x) = p(x|y≤τ,D) and g(x) = p(x|y>τ,D), the acquisition function becomes the ratio ℓ(x)/g(x), which has the same maximum as expected improvement but without requiring explicit surrogate modeling.
- Core assumption: The density ratio between promising and non-promising configurations can be estimated from meta-data across tasks, and this ratio is invariant to scale and noise distribution changes across tasks.

### Mechanism 2
- Claim: The probabilistic meta-learning model captures task uncertainty, enabling robust adaptation to new tasks with limited observations.
- Mechanism: The method learns a task-agnostic feature mapping and a task-specific embedding that follows a known prior distribution. When adapting to new tasks, it uses Laplace approximation to estimate the posterior distribution of the task embedding, allowing Thompson sampling to account for epistemic uncertainty in the task characterization.
- Core assumption: Task embeddings can be regularized to follow a known distribution (multivariate normal) during meta-training, and this prior can be used to approximate the posterior for new tasks.

### Mechanism 3
- Claim: Gradient boosting serves as a residual prediction model that enables robust adaptation when meta-learning fails.
- Mechanism: The meta-learned classifier serves as the initial estimator in a gradient boosting ensemble. The subsequent weak learners are trained only on observations collected during optimization, correcting errors from the meta-learned model. This creates a model that combines meta-learned knowledge with task-specific adaptation.
- Core assumption: The meta-learned model provides a reasonable starting point that can be improved by residual corrections learned from target task data.

## Foundational Learning

- Concept: Density ratio estimation and its equivalence to expected improvement
  - Why needed here: The entire approach builds on reframing BO as density ratio estimation rather than surrogate modeling. Understanding this equivalence is crucial for grasping why the method bypasses GPs.
  - Quick check question: Can you explain why the ratio ℓ(x)/g(x) has the same maximum as expected improvement for the threshold τ?

- Concept: Bayesian logistic regression and Laplace approximation
  - Why needed here: The method uses Bayesian logistic regression with Laplace approximation for task adaptation. Understanding how the posterior over task embeddings is approximated is essential for understanding the uncertainty quantification.
  - Quick check question: What are the mean and variance parameters of the Laplace approximation for the task embedding posterior?

- Concept: Thompson sampling for exploration
  - Why needed here: Thompson sampling is used to account for task uncertainty during early optimization. Understanding how Thompson sampling balances exploration and exploitation is important for understanding the algorithm's behavior.
  - Quick check question: How does Thompson sampling differ from purely greedy optimization in terms of exploration-exploitation tradeoff?

## Architecture Onboarding

- Component map: Meta-learning classifier (ResFFN + mean layer + task embedding) -> Laplace approximation for posterior inference -> Thompson sampling for exploration -> Gradient boosting as residual model -> Random search for acquisition function maximization

- Critical path: Meta-training -> Task adaptation (Laplace) -> Thompson sampling -> Gradient boosting refinement -> Acquisition function maximization

- Design tradeoffs:
  - Using density ratios vs. surrogate models: better scalability but requires careful threshold selection
  - Laplace approximation vs. more expensive inference: faster but potentially less accurate uncertainty estimates
  - Thompson sampling vs. deterministic acquisition: better exploration but introduces sampling noise
  - Gradient boosting vs. fine-tuning entire network: more robust but adds complexity

- Failure signatures:
  - Poor meta-training performance indicates issues with feature learning or regularization
  - Unstable task adaptation suggests Laplace approximation is failing (try HMC or SVI)
  - Lack of exploration indicates Thompson sampling parameters need adjustment
  - Gradient boosting overfitting indicates need for early stopping or regularization

- First 3 experiments:
  1. Implement and test the meta-learning classifier on synthetic data with known density ratio structure
  2. Add Laplace approximation and test task adaptation on a simple transfer learning problem
  3. Integrate Thompson sampling and test exploration on a function with multiple local optima

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MALIBO perform on extremely high-dimensional search spaces (e.g., >50 dimensions)?
- Basis in paper: The paper demonstrates MALIBO's performance on problems with up to 9 dimensions (HPOBench) and 6 dimensions (NASBench201), but does not explore higher-dimensional spaces.
- Why unresolved: The paper's experiments focus on moderate-dimensional problems, leaving the scalability of MALIBO to very high-dimensional spaces unexplored.
- What evidence would resolve it: Empirical results comparing MALIBO's performance on synthetic and real-world problems with varying dimensionalities, especially >50 dimensions.

### Open Question 2
- Question: How does MALIBO handle non-stationary or time-varying black-box functions?
- Basis in paper: The paper assumes static black-box functions but does not address scenarios where the function changes over time or with the number of evaluations.
- Why unresolved: The paper's theoretical framework and experiments are based on stationary functions, leaving the robustness of MALIBO to non-stationary environments untested.
- What evidence would resolve it: Experiments evaluating MALIBO's performance on time-varying or non-stationary benchmark functions, comparing it to methods specifically designed for such scenarios.

### Open Question 3
- Question: Can MALIBO be effectively extended to handle multi-objective optimization problems?
- Basis in paper: The paper focuses on single-objective optimization and does not explore extensions to multi-objective settings.
- Why unresolved: The likelihood-free acquisition function and meta-learning approach would need adaptation to handle multiple conflicting objectives, which is not addressed in the paper.
- What evidence would resolve it: A modified version of MALIBO that incorporates multi-objective acquisition functions (e.g., expected hypervolume improvement) and empirical results on multi-objective benchmark problems.

## Limitations
- Performance critically depends on the quality and representativeness of meta-data from related tasks
- Laplace approximation for task embedding posterior may fail when tasks are highly dissimilar
- Choice of threshold τ for density ratio estimation significantly impacts performance but lacks principled selection criteria
- Gradient boosting residual model adds computational overhead that scales with the number of observations

## Confidence
- High confidence: The density ratio estimation framework for BO (Mechanism 1) - well-established in TPE and BORE literature
- Medium confidence: The probabilistic meta-learning approach with Laplace approximation (Mechanism 2) - theoretically sound but sensitive to approximation quality
- Medium confidence: The gradient boosting residual model (Mechanism 3) - empirically validated but adds complexity

## Next Checks
1. Test robustness to task dissimilarity by systematically varying the similarity between meta-training and target tasks
2. Compare Laplace approximation against more expensive inference methods (HMC, SVI) on synthetic benchmarks
3. Evaluate sensitivity to threshold selection τ across different noise levels and scale conditions