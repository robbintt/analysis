---
ver: rpa2
title: Automatic Model Selection with Large Language Models for Reasoning
arxiv_id: '2305.14333'
source_url: https://arxiv.org/abs/2305.14333
tags:
- answer
- selection
- problem
- reasoning
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to combine Chain-of-Thought (CoT)
  and Program-Aided Language Models (PAL) for reasoning tasks. It employs a large
  language model (LLM) to dynamically select between the two methods based on their
  reasoning chains.
---

# Automatic Model Selection with Large Language Models for Reasoning

## Quick Facts
- arXiv ID: 2305.14333
- Source URL: https://arxiv.org/abs/2305.14333
- Authors: 
- Reference count: 19
- Key outcome: Combines CoT and PAL using LLM-based model selection, achieving 96.5% on GSM8K and 93.7% on SVAMP

## Executive Summary
This paper introduces a novel approach for automatic model selection in reasoning tasks by combining Chain-of-Thought (CoT) and Program-Aided Language Models (PAL). The method uses a large language model to dynamically choose between CoT and PAL based on their reasoning chains, with theoretical analysis showing performance improvement depends on the difference between methods and selection accuracy. The authors evaluate their approach on eight reasoning datasets using Codex, ChatGPT, and GPT-4, achieving significant improvements and new state-of-the-art results on GSM8K and SVAMP. They also integrate the method with self-consistency for additional performance gains.

## Method Summary
The proposed method dynamically selects between CoT and PAL reasoning approaches using an LLM as a selector. For each problem, the LLM receives reasoning chains from both methods as few-shot examples and outputs which approach is more likely correct. The method includes explanations to improve selection accuracy, particularly for models with weaker in-context learning capabilities. The approach is further enhanced by integrating with self-consistency, which samples multiple solutions and aggregates them to improve overall performance while reducing computational costs.

## Key Results
- Achieves 96.5% accuracy on GSM8K and 93.7% on SVAMP, setting new state-of-the-art results
- Demonstrates significant performance improvements across eight reasoning datasets
- Shows that explanations in prompts improve selection accuracy, especially for ChatGPT
- Proves that performance can improve even when selection accuracy is below random chance, given substantial accuracy differences between methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model selection improves performance when CoT and PAL make different kinds of errors
- Mechanism: LLM selects the method whose reasoning chain is more likely correct based on different error patterns from natural language vs code reasoning
- Core assumption: LLM can compare reasoning chains from different modalities to determine which is more likely correct
- Evidence anchors: Theoretical analysis showing improvement depends on accuracy differences and selection probability

### Mechanism 2
- Claim: Performance can improve even with below-random selection accuracy if methods have substantially different accuracy distributions
- Mechanism: Improvement comes from correctly selecting better method on problems with large accuracy gaps, outweighing wrong selections elsewhere
- Core assumption: Large accuracy differences between methods can compensate for poor selection rates
- Evidence anchors: Theorem 1 supports improvement despite not achieving Ïx > 0.5 in some instances

### Mechanism 3
- Claim: Explanations in selection prompts improve LLM's choice accuracy, especially for models with weaker in-context learning
- Mechanism: Generating explanations forces deeper analysis of reasoning chains, leading to better selection decisions
- Core assumption: Explaining choices requires explicit comparison of reasoning chains
- Evidence anchors: Larger improvement in success selection rate for ChatGPT with explanations

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: One of the two base methods being combined; understanding CoT is essential for grasping the approach
  - Quick check question: What distinguishes CoT from standard prompting in language models?

- Concept: Program-Aided Language Models (PAL)
  - Why needed here: Second base method being combined with CoT; understanding PAL's code-based reasoning is crucial
  - Quick check question: How does PAL differ from CoT in terms of reasoning style and computational guarantees?

- Concept: In-context learning and few-shot prompting
  - Why needed here: Selection mechanism relies on LLM's ability to learn from few examples in prompt
  - Quick check question: What is in-context learning and why is it particularly relevant for this model selection approach?

## Architecture Onboarding

- Component map: CoT method -> PAL method -> Model selection LLM -> Selected solution -> (Optional) Self-consistency module

- Critical path:
  1. Generate reasoning chains using CoT and PAL
  2. Present both chains to selection LLM with few-shot examples
  3. LLM outputs selection (A/B) and explanation
  4. Execute selected method's solution
  5. (Optional) Apply self-consistency to sampled solutions

- Design tradeoffs:
  - CoT vs. PAL: CoT is more interpretable but less computationally rigorous; PAL is more accurate but less explainable
  - Including explanations: Improves selection for weaker models but adds complexity
  - Self-consistency integration: Further improves performance but increases computation cost

- Failure signatures:
  - Low improvement despite combining methods: Methods may have similar error patterns
  - Inconsistent selection results: Insufficient few-shot examples or ambiguous reasoning chains
  - Performance worse than strongest base method: Poor selection model or similar method accuracies

- First 3 experiments:
  1. Run CoT and PAL separately on GSM8K to establish baseline accuracies
  2. Implement model selection with few-shot examples and test on GSM8K
  3. Add self-consistency to selected solutions and measure performance gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model selection performance vary when combining methods other than CoT and PAL?
- Basis in paper: Paper mentions possibility of exploring model selection with diverse system instructions and other domains
- Why unresolved: Paper focuses specifically on CoT and PAL combination
- What evidence would resolve it: Experiments comparing performance with different reasoning method pairs

### Open Question 2
- Question: What is the impact of different prompt designs or few-shot examples on model selection performance?
- Basis in paper: Paper mentions use of few-shot examples but doesn't extensively explore impact of different designs
- Why unresolved: Paper provides basic prompt design without investigating sensitivity to example quality
- What evidence would resolve it: Experiments comparing different prompt designs and example qualities

### Open Question 3
- Question: How does the approach scale with larger and more diverse datasets and real-world reasoning tasks?
- Basis in paper: Paper evaluates on eight datasets but doesn't discuss scalability to larger or real-world tasks
- Why unresolved: Paper focuses on limited set of datasets without exploring practical limitations
- What evidence would resolve it: Scaling experiments and case studies in real-world applications

## Limitations

- Theoretical assumptions about LLM selection accuracy may not hold in practice, particularly for models with weaker in-context learning capabilities
- Performance improvements highly dependent on specific CoT and PAL combination, may not generalize to other method pairs
- Selection mechanism effectiveness depends heavily on LLM's ability to meaningfully compare reasoning chains from different modalities

## Confidence

- **High confidence**: Theoretical framework showing improvement conditions (when accuracy differences are substantial and selection is accurate enough)
- **Medium confidence**: Empirical results showing SOTA performance on GSM8K and SVAMP, though limited to three specific LLMs
- **Low confidence**: Generalizability of selection mechanism to other method pairs beyond CoT and PAL, robustness of explanations improving selection accuracy

## Next Checks

1. Test model selection mechanism with different pairs of reasoning methods (e.g., CoT with another approach beyond PAL) to assess generalizability
2. Conduct ablation studies removing explanations from prompts across all three LLMs to quantify their actual contribution to selection accuracy
3. Measure correlation between reasoning chain similarity scores and selection accuracy to determine when mechanism breaks down