---
ver: rpa2
title: Goodhart's Law Applies to NLP's Explanation Benchmarks
arxiv_id: '2308.14272'
source_url: https://arxiv.org/abs/2308.14272
tags:
- metrics
- explanations
- explanation
- features
- comprehensiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that popular faithfulness metrics for evaluating
  NLP explanations can be easily gamed without improving actual explanation quality.
  The authors show that both ERASER's comprehensiveness/sufficiency metrics and EVAL-X's
  encoding detection metrics can be optimized arbitrarily using simple wrapper methods
  that manipulate model behavior on masked inputs.
---

# Goodhart's Law Applies to NLP's Explanation Benchmarks

## Quick Facts
- arXiv ID: 2308.14272
- Source URL: https://arxiv.org/abs/2308.14272
- Authors: 
- Reference count: 40
- Popular faithfulness metrics for NLP explanations can be easily gamed without improving actual explanation quality

## Executive Summary
This paper demonstrates that popular faithfulness metrics for evaluating NLP explanations can be easily gamed without improving actual explanation quality. The authors show that both ERASER's comprehensiveness/sufficiency metrics and EVAL-X's encoding detection metrics can be optimized arbitrarily using simple wrapper methods that manipulate model behavior on masked inputs. Specifically, they introduce a meta-algorithm that assigns different model confidences to explanation-only, non-explanation, and original inputs, achieving near-optimal faithfulness scores while preserving predictions on the original data. For EVAL-X, they show that two encoding schemes can achieve perfect scores even though they encode predictions in the explanations. These results suggest that current explanation benchmarks fail to measure what they claim to measure and may not be suitable for guiding explainability research.

## Method Summary
The authors introduce a meta-algorithm that wraps prediction models to strategically manipulate confidence scores based on whether inputs are original, explanation-only, or non-explanation features. This exploits the tendency for explanation features and their complements to be "out-of-support" relative to in-distribution inputs. For EVAL-X metrics, they propose two encoding schemes: token-label likelihood ratio and evaluator model queries, both of which encode the predicted label directly into explanation tokens. The methods are evaluated on ERASER benchmark datasets (Movies, BoolQ) and Evidence Inference, FEVER, and MultiRC datasets using BERT-based models with various saliency methods (LIME, IG, Attention, Random).

## Key Results
- The meta-algorithm achieves near-optimal faithfulness scores (average sum score of 0.78 on Movies dataset) while preserving predictions on original inputs
- Two simple encoding schemes achieve perfect EVAL-X scores (eACC = 1.0, eAUROC = 1.0) by encoding predictions into explanations
- Comprehensiveness and sufficiency scores are mathematically bounded such that their sum cannot exceed 1 due to a fundamental tradeoff
- Case detection accuracy exceeds 99% on Movies and BoolQ datasets, enabling the meta-algorithm to work effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ERASER benchmark metrics can be inflated without changing model predictions or explanations on in-distribution test inputs
- Mechanism: The wrapper model assigns different confidence scores to explanation-only features, non-explanation features, and original inputs, exploiting the fact that explanation-only and non-explanation features are easily distinguishable from original inputs
- Core assumption: The case detector can reliably distinguish between explanation-only features, non-explanation features, and original inputs
- Evidence anchors:
  - [abstract] "Our strategy exploits the tendency for extracted explanations and their complements to be 'out-of-support' relative to each other and in-distribution inputs"
  - [section 3.1] "Our meta-algorithm wraps the original predictor as follows... if the detected case is original, we run the input through the original model, thereby preserving the same prediction ŷ and explanation ê"
  - [corpus] No direct corpus evidence found; this is a novel attack mechanism not previously documented
- Break condition: The case detector becomes unable to reliably distinguish between the different input types, or the explanation extraction process produces features that are indistinguishable from original inputs

### Mechanism 2
- Claim: EVAL-X metrics can be inflated arbitrarily by encoding predictions into explanations
- Mechanism: The encoding methods either use token-label likelihood ratios or evaluator model queries to embed the predicted label directly into the explanation tokens
- Core assumption: The evaluator model EVAL-X can be accessed or approximated, allowing direct optimization of its metrics
- Evidence anchors:
  - [abstract] "we demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple method that encodes the label"
  - [section 4.1] "Method 2: Evaluator Model Queries... we can simply query the evaluator model and output the encoded explanation as any one input token for which the evaluator model outputs ŷ"
  - [corpus] No direct corpus evidence found; this represents a novel attack on EVAL-X metrics
- Break condition: The evaluator model becomes unavailable or the encoding method becomes detectable by EVAL-X's evaluation process

### Mechanism 3
- Claim: The sum of sufficiency and comprehensiveness scores cannot exceed 1 due to a mathematical tradeoff
- Mechanism: As model confidence in explanation features increases (improving sufficiency), model confidence in non-explanation features must decrease (improving comprehensiveness), creating a bounded sum
- Core assumption: The mathematical relationship between these metrics creates an inherent tradeoff that limits optimization
- Evidence anchors:
  - [section 3] "The upshot of this tradeoff is that the sum of sufficiency and comprehensiveness scores lies in the range [-1, 1] and thus cannot exceed 1"
  - [section 3.3] "On average, on the Movies dataset, our meta-algorithm's sum faithfulness score is 0.78, whereas the underlying method's faithfulness sum score is 0.14"
  - [corpus] No direct corpus evidence found; this is a novel mathematical insight about ERASER metrics
- Break condition: The mathematical relationship changes or additional metrics are introduced that break the bounded sum constraint

## Foundational Learning

- Concept: Goodhart's Law
  - Why needed here: The paper's central thesis is that optimizing for these metrics causes them to cease being useful measures
  - Quick check question: If a metric becomes a target, what happens to its usefulness as a measure?

- Concept: Out-of-distribution (OOD) evaluation
  - Why needed here: The metrics evaluate models on masked inputs that may be OOD relative to natural documents
  - Quick check question: Why might evaluating explanations on masked inputs be problematic compared to evaluating on natural documents?

- Concept: Faithfulness vs. plausibility in explanations
  - Why needed here: The paper focuses on faithfulness metrics that measure how well explanations reflect the true reasoning process
  - Quick check question: What's the difference between an explanation that's faithful to the model versus one that's plausible to humans?

## Architecture Onboarding

- Component map:
  - Prediction model (BERT-based)
  - Saliency method (LIME, IG, Attention, Random)
  - Case detector (distinguishes original vs. explanation-only vs. non-explanation inputs)
  - Meta-algorithm wrapper (manipulates confidence scores based on detected case)
  - EVAL-X evaluator model (trained on randomly masked inputs)

- Critical path: Input → Case detection → Confidence score manipulation → Metric calculation
- Design tradeoffs: High case detection accuracy vs. computational overhead; encoding method complexity vs. metric optimization potential
- Failure signatures: Case detector accuracy drops below threshold; explanation features become indistinguishable from original inputs; EVAL-X model becomes inaccessible
- First 3 experiments:
  1. Implement case detector on Movies dataset and measure accuracy distinguishing original, explanation-only, and non-explanation inputs
  2. Apply meta-algorithm wrapper to a simple BERT classifier with attention-based explanations and measure faithfulness score improvements
  3. Implement token-label likelihood ratio encoding method and test against EVAL-X metrics on the Movies dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental properties that a faithfulness metric for explanations should satisfy to be considered reliable?
- Basis in paper: [explicit] The paper demonstrates that current metrics fail to guide explainability research and highlights the need for a broader reassessment of what these metrics are intended to capture.
- Why unresolved: The paper shows that existing metrics can be easily gamed without improving actual explanation quality, suggesting that the fundamental properties of a reliable metric are not yet understood.
- What evidence would resolve it: Development and validation of new metrics that cannot be easily gamed while still capturing meaningful aspects of explanation faithfulness, along with theoretical proofs of their properties.

### Open Question 2
- Question: How can we design evaluation metrics that are robust to distribution shift while still measuring explanation faithfulness?
- Basis in paper: [explicit] The paper highlights the "out-of-support" issue where metrics evaluate model behavior on distributions different from the original data, raising concerns about the validity of such evaluations.
- Why unresolved: While the paper identifies this issue, it does not provide a concrete solution for designing metrics that can handle distribution shift while maintaining their effectiveness.
- What evidence would resolve it: Creation of evaluation protocols that either account for or avoid distribution shift while maintaining the ability to measure explanation faithfulness accurately.

### Open Question 3
- Question: What are the theoretical limits of using proxy metrics to evaluate explanation quality in NLP models?
- Basis in paper: [inferred] The paper's results suggest that current proxy metrics (like comprehensiveness and sufficiency) fail to capture the true quality of explanations, implying there may be fundamental limitations to using such metrics.
- Why unresolved: The paper demonstrates the limitations of current metrics but does not explore whether there are inherent theoretical constraints on using proxy metrics for this purpose.
- What evidence would resolve it: Mathematical proofs or empirical studies showing the theoretical bounds of what proxy metrics can and cannot measure in the context of explanation quality.

## Limitations

- The attack mechanisms rely heavily on the assumption that case detectors can reliably distinguish between different input types, which may not hold for more sophisticated explanation methods
- The evaluation focuses primarily on BERT-based models and relatively simple saliency methods, raising questions about generalizability to other architectures or more complex explanation techniques
- The benchmarks evaluated are relatively narrow in scope - it's unclear whether similar vulnerabilities exist in other popular explanation evaluation frameworks

## Confidence

- **High**: The fundamental claim that faithfulness metrics can be manipulated without improving explanation quality - the mathematical proofs and experimental results strongly support this.
- **Medium**: The generalizability of the attack mechanisms to other benchmark datasets and model architectures - while the framework is general, empirical validation is limited to specific datasets and BERT models.
- **Low**: The practical impact on real-world XAI applications - the paper doesn't address whether these attacks would succeed against deployed systems or in production settings.

## Next Checks

1. **Case Detection Robustness**: Test the case detector's performance when explanation features are intentionally designed to be indistinguishable from original inputs, such as using natural language explanations or continuous attribution methods.

2. **Cross-Architecture Generalization**: Apply the attack mechanisms to transformer architectures beyond BERT (e.g., RoBERTa, DeBERTa) and non-transformer models to assess whether the vulnerabilities are architecture-specific or more general.

3. **Real-World Deployment Scenarios**: Design a simulation of a production XAI system where the evaluator model is only partially accessible or where timing constraints prevent exhaustive evaluator model queries, then test whether the attack methods still succeed.