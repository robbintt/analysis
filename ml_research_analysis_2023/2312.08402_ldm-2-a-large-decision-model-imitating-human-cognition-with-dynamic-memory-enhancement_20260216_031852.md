---
ver: rpa2
title: 'LDM$^2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory
  Enhancement'
arxiv_id: '2312.08402'
source_url: https://arxiv.org/abs/2312.08402
tags:
- apple
- memory
- action
- subgoal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LDM2, a framework that leverages a dynamic
  memory mechanism to enhance large language models for decision-making tasks. The
  core idea is to store and retrieve state-action tuples from a memory to guide LLMs
  in making proper decisions according to the current state.
---

# LDM$^2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement

## Quick Facts
- arXiv ID: 2312.08402
- Source URL: https://arxiv.org/abs/2312.08402
- Reference count: 40
- Key outcome: LDM2 outperforms baselines in WebShop and ALFWorld tasks by leveraging dynamic memory to store and retrieve state-action tuples from human trajectories.

## Executive Summary
LDM2 introduces a dynamic memory mechanism that enhances large language models for decision-making tasks by storing and retrieving state-action tuples from human experiences. The framework consists of two stages: memory formation, where human behaviors are decomposed into state-action tuples and stored in memory, and memory refinement, where tree exploration discovers more suitable decision processes to enrich the memory. Extensive experiments demonstrate LDM2's superior performance compared to baselines in terms of both score and success rate.

## Method Summary
LDM2 employs a two-stage approach to decision-making. In the memory formation stage, human decision trajectories are decomposed into state-action tuples containing goal, history summary, observation, and action, which are stored in a clustered memory index. During memory refinement, tree exploration generates potential decision processes, evaluates them based on environmental rewards, and adds high-value tuples to the memory. At inference time, the model retrieves relevant tuples based on current state and uses them to construct dynamic prompts for the LLM, enabling decision-making guided by past human experiences.

## Key Results
- LDM2 achieves higher success rates than baselines on both WebShop and ALFWorld tasks
- The model demonstrates improved score performance through effective memory utilization
- Tree exploration successfully discovers valuable decision processes for memory refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The memory mechanism enables LLMs to make decisions based on past human experiences rather than fixed examples.
- Mechanism: LDM2 decomposes human trajectories into state-action tuples and stores them in memory. When making decisions, the model retrieves relevant tuples based on the current state and uses them to construct dynamic prompts.
- Core assumption: Human trajectories can be effectively decomposed into standard state-action tuples that capture the decision-making process.
- Evidence anchors:
  - [abstract] "LDM2 consists of two stages: memory formation and memory refinement. In the former stage, human behaviors are decomposed into state-action tuples utilizing the powerful summarizing ability of LLMs."
  - [section] "Given a human trajectory ti = {oi1, ai1, ..., oiTi, aiTi}, it can be divided into Ti standard state-action tuples."

### Mechanism 2
- Claim: The dynamic memory refinement stage allows the model to learn from environmental feedback and improve its decision-making over time.
- Mechanism: LDM2 uses tree exploration to generate potential decision processes, evaluates them based on environment rewards, and adds the most valuable state-action tuples to the memory.
- Core assumption: The model can identify the most valuable decision processes through tree exploration and reward evaluation.
- Evidence anchors:
  - [abstract] "In the latter stage, our LDM2 employs tree exploration to discover more suitable decision processes and enrich the memory by adding valuable state-action tuples."

### Mechanism 3
- Claim: The two-stage memory formation and refinement process allows the model to achieve better performance than standard prompting methods.
- Mechanism: The initial memory formation stage provides the model with a foundation of human experiences, while the refinement stage allows it to learn from its own interactions with the environment.
- Core assumption: The combination of human experience and environmental feedback leads to better decision-making than either alone.
- Evidence anchors:
  - [abstract] "Extensive experiments conducted in two interactive environments have shown that our LDM2 outperforms the baselines in terms of both score and success rate."

## Foundational Learning

- Concept: State-action tuples
  - Why needed here: The memory mechanism relies on decomposing human trajectories into state-action tuples that capture the decision-making process.
  - Quick check question: What information should be included in a state-action tuple to effectively represent a human decision?

- Concept: Tree exploration
  - Why needed here: The memory refinement stage uses tree exploration to generate potential decision processes and evaluate their quality.
  - Quick check question: How does tree exploration help the model discover more suitable decision processes?

- Concept: Reward evaluation
  - Why needed here: The model needs to evaluate the quality of decision processes based on environment rewards to determine which state-action tuples to add to the memory.
  - Quick check question: What factors should be considered when evaluating the quality of a decision process?

## Architecture Onboarding

- Component map:
  Memory formation -> State-action tuple decomposition -> Memory storage
  Memory refinement -> Tree exploration -> Reward evaluation -> Memory update
  Decision-making -> State-action tuple retrieval -> Dynamic prompt construction

- Critical path:
  1. Memory formation: Decompose human trajectories into state-action tuples.
  2. Memory refinement: Use tree exploration to generate and evaluate potential decision processes.
  3. Decision-making: Retrieve relevant state-action tuples and construct dynamic prompts for the LLM.

- Design tradeoffs:
  - Memory size vs. retrieval efficiency: Larger memory can store more state-action tuples but may slow down retrieval.
  - Tree exploration depth vs. computational cost: Deeper exploration can discover more potential decision processes but increases computational cost.

- Failure signatures:
  - Poor performance: If the model fails to effectively integrate human experience with environmental feedback, it may not outperform standard prompting methods.
  - Slow decision-making: If the memory retrieval process is inefficient, it may slow down the decision-making process.

- First 3 experiments:
  1. Evaluate the performance of LDM2 on a simple decision-making task with a small number of human trajectories.
  2. Test the memory refinement stage by comparing the performance of the model with and without tree exploration.
  3. Assess the impact of memory size on retrieval efficiency and decision-making performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LDM2's memory-based prompting scale with increasing task complexity and memory size?
- Basis in paper: [inferred] The paper shows improved performance with larger batch sizes but does not analyze the scaling behavior for extremely complex tasks or massive memories.
- Why unresolved: The experiments used a fixed number of tasks and batch sizes; no systematic study of scaling limits or computational bottlenecks was provided.
- What evidence would resolve it: Empirical results showing performance and efficiency metrics across a wide range of task complexities and memory sizes, including saturation points.

### Open Question 2
- Question: Can LDM2 effectively learn from limited or noisy human trajectory data?
- Basis in paper: [explicit] The paper notes the need for "a certain amount of high-quality human trajectories" and acknowledges potential data scarcity in some environments.
- Why unresolved: No experiments were conducted to test LDM2's robustness to limited, incomplete, or noisy training data.
- What evidence would resolve it: Comparative experiments with varying levels of trajectory data quality and quantity, demonstrating LDM2's performance degradation or adaptation strategies.

### Open Question 3
- Question: How does the choice of LLM (e.g., GPT-3.5 vs. GPT-4) impact LDM2's performance and efficiency?
- Basis in paper: [inferred] The paper uses GPT-3.5 but does not explore the effects of different LLM capabilities on LDM2's results.
- Why unresolved: The experiments only used one LLM model, leaving the impact of model size and capability unexplored.
- What evidence would resolve it: Systematic comparison of LDM2's performance and inference speed using different LLM models with varying parameter counts and capabilities.

## Limitations

- Memory refinement mechanism lacks detailed specification for tree exploration and reward evaluation criteria
- Performance improvements over baselines are modest, suggesting incremental rather than transformative gains
- Approach requires substantial human trajectory data upfront, limiting scalability to data-scarce domains

## Confidence

**High Confidence** (well-supported by evidence):
- The two-stage memory formation and refinement framework is clearly defined and experimentally validated
- State-action tuple decomposition from human trajectories is feasible using LLM summarization capabilities
- Memory retrieval based on goal and observation clustering is technically sound

**Medium Confidence** (partially supported):
- Dynamic memory enhancement provides meaningful performance improvements over baselines
- Tree exploration effectively discovers valuable decision processes for memory refinement
- The approach generalizes well to different interactive environments (WebShop and ALFWorld)

**Low Confidence** (weak or absent evidence):
- The specific mechanism by which memory refinement outperforms static memory approaches
- Long-term memory scalability and retrieval efficiency as memory grows
- Robustness of the approach to noisy or suboptimal human trajectories

## Next Checks

1. **Ablation study of memory refinement**: Run experiments comparing LDM2 with only memory formation (no tree exploration refinement) to quantify the exact contribution of the refinement stage to performance gains.

2. **Memory scalability test**: Evaluate retrieval latency and performance as memory size increases from 100 to 10,000+ state-action tuples to assess computational efficiency at scale.

3. **Robustness to trajectory quality**: Test LDM2 performance when trained on progressively noisier or suboptimal human trajectories to determine sensitivity to training data quality.