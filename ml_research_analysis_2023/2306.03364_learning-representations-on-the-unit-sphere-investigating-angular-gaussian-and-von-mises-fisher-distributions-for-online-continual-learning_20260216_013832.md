---
ver: rpa2
title: 'Learning Representations on the Unit Sphere: Investigating Angular Gaussian
  and von Mises-Fisher Distributions for Online Continual Learning'
arxiv_id: '2306.03364'
source_url: https://arxiv.org/abs/2306.03364
tags:
- learning
- data
- task
- distribution
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for online continual learning
  by learning representations on the unit sphere using maximum a posteriori estimation.
  The authors derive loss functions based on the angular Gaussian and von Mises-Fisher
  distributions, both designed for modeling symmetric directional data.
---

# Learning Representations on the Unit Sphere: Investigating Angular Gaussian and von Mises-Fisher Distributions for Online Continual Learning

## Quick Facts
- arXiv ID: 2306.03364
- Source URL: https://arxiv.org/abs/2306.03364
- Reference count: 40
- Key outcome: Proposed method achieves up to 8.05% higher accuracy on CIFAR-100 with 5k memory size compared to state-of-the-art methods in online continual learning

## Executive Summary
This paper introduces a novel approach for online continual learning by learning representations on the unit sphere using maximum a posteriori estimation. The authors propose two loss functions based on angular Gaussian and von Mises-Fisher distributions, both designed for modeling symmetric directional data. A key innovation is that representations are pushed toward fixed directions (prior means), making the approach resilient to data drift without requiring negative samples or task boundary knowledge. The method performs well with smaller batch sizes while being computationally efficient, outperforming state-of-the-art methods on standard and blurry task boundary scenarios.

## Method Summary
The method uses maximum a posteriori estimation with latent representations constrained to the unit sphere. Two loss functions are proposed: vMF-FD (von Mises-Fisher) and AGD-FD (angular Gaussian), both using fixed mean directions where each class is assigned to a standard basis vector. Training employs reservoir sampling for memory management, a multi-augmentation trick (5 views per batch), and a projection trick. After training, a nearest class mean classifier is trained on frozen representations. The approach does not require negative samples or knowledge of task boundaries and works effectively with smaller batch sizes.

## Key Results
- Achieves up to 8.05% higher accuracy on CIFAR-100 with 5k memory size compared to state-of-the-art methods
- Performs well on both clear and blurry task boundary scenarios
- Shows resilience to data drift through fixed direction strategy
- Maintains computational efficiency while requiring smaller batch sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The MAP-based loss pushes representations toward fixed directions on the unit sphere, providing resilience to data drift.
- **Mechanism**: By fixing class mean directions to standard basis vectors and modeling class conditionals as angular Gaussian or von Mises-Fisher distributions, learned representations are attracted to these fixed directions regardless of task changes.
- **Core assumption**: Representations lie on the unit sphere and follow a Saw distribution parameterized by mean direction and concentration.
- **Evidence anchors**:
  - [abstract]: "The learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift."
  - [section 3.3]: "Fixing directions also implies that they are independent of batch size or training step, which brings training stability."
  - [corpus]: Weak - no direct evidence found for Saw distributions in continuous learning contexts.
- **Break condition**: If data distribution changes such that optimal representations no longer align with fixed basis directions, the method's resilience breaks down.

### Mechanism 2
- **Claim**: The proposed loss function does not require negative samples or knowledge of task boundaries, enabling effective learning with smaller batch sizes.
- **Mechanism**: The MAP-based loss formulation inherently incorporates class priors and avoids explicit negative sampling by modeling full class conditional distributions.
- **Core assumption**: The posterior can be accurately estimated using only positive samples within each class and the fixed class mean directions.
- **Evidence anchors**:
  - [abstract]: "Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch sizes while being computationally efficient."
  - [section 3.1]: The derivation shows the posterior can be expressed using only class conditional densities without explicit negative sampling.
  - [corpus]: No direct evidence - contrastive methods typically require negative samples, but this method's MAP formulation is novel.
- **Break condition**: If class conditional distributions become too complex or multimodal, the fixed direction assumption may fail, requiring negative samples for discrimination.

### Mechanism 3
- **Claim**: Multi-augmentation and projection tricks improve performance by increasing effective batch size and regularization.
- **Mechanism**: Multiple augmentations per sample create diverse views within the same batch, while the projection trick (Guillotine Regularization) separates representation learning from classification, improving generalization.
- **Core assumption**: Data augmentation creates useful variations that help the model learn more robust representations, and separating the projection layer from the representation layer prevents overfitting.
- **Evidence anchors**:
  - [section 3.5]: "each image is augmented several times to artificially increase the current batch size and show many 'views' of current data simultaneously."
  - [section 3.5]: "we apply the projection trick, also called Guillotine Regularization [31], to our model."
  - [corpus]: Weak - while augmentation is common, the specific multi-view strategy for online CL is not well-established in the corpus.
- **Break condition**: If augmentations introduce too much noise or the projection layer becomes too shallow to capture task-relevant features, performance degrades.

## Foundational Learning

- **Concept: Maximum a Posteriori (MAP) Estimation**
  - Why needed here: The entire loss function is derived from MAP estimation under specific distributional assumptions about the representations.
  - Quick check question: Can you explain how MAP estimation differs from Maximum Likelihood Estimation and why the prior term matters in this context?

- **Concept: Directional Statistics on the Unit Sphere**
  - Why needed here: The method assumes representations lie on the unit sphere and uses distributions like von Mises-Fisher and angular Gaussian that are specifically designed for directional data.
  - Quick check question: What are the key properties of distributions on the unit sphere that make them suitable for normalized neural network representations?

- **Concept: Continual Learning and Catastrophic Forgetting**
  - Why needed here: The method is designed specifically to address the challenge of learning new tasks without forgetting previous ones in an online setting.
  - Quick check question: How does the fixed direction approach help mitigate catastrophic forgetting compared to methods that require task boundary knowledge?

## Architecture Onboarding

- **Component map**: Input → Encoder → Projection → Loss computation → Parameter update
- **Critical path**: Input → Encoder → Projection → Loss computation → Parameter update. Memory retrieval and augmentation happen in parallel with the main stream processing.
- **Design tradeoffs**:
  - Fixed vs. learned mean directions: Fixed directions provide stability but may be suboptimal for complex data distributions.
  - Angular Gaussian vs. von Mises-Fisher: Different concentration parameters and computational properties.
  - Number of augmentations: More views improve performance but increase computational cost.
- **Failure signatures**:
  - Performance degradation over tasks: May indicate insufficient concentration parameter or poor augmentation strategy.
  - High variance across runs: Could suggest instability in the MAP estimation or sensitivity to initialization.
  - Memory inefficiency: If the reservoir sampling isn't properly balanced, older tasks may be underrepresented.
- **First 3 experiments**:
  1. Verify the fixed direction property: Train on a simple dataset and visualize whether representations cluster around the fixed basis vectors.
  2. Ablation study on augmentation: Compare performance with 1, 3, 5, and 10 augmentations per sample to find the optimal tradeoff.
  3. Distribution fitting: Check if the learned representations actually follow the assumed angular Gaussian or von Mises-Fisher distributions using statistical tests.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would using a non-diagonal covariance matrix in the angular Gaussian distribution affect the performance of the proposed approach in continual learning?
- Basis in paper: [inferred] The paper mentions that using a non-diagonal covariance matrix could potentially improve results, but does not explore this possibility.
- Why unresolved: The authors chose to focus on the isotropic case for simplicity and computational efficiency, leaving the non-diagonal case unexplored.
- What evidence would resolve it: Experiments comparing the performance of the proposed approach using both diagonal and non-diagonal covariance matrices on various continual learning benchmarks.

### Open Question 2
- Question: How would the proposed approach perform in a non-online, batch learning scenario?
- Basis in paper: [inferred] The authors suggest that applying their new losses to non-online batch learning should be considered, but do not provide any results.
- Why unresolved: The paper focuses specifically on the online continual learning setting, leaving the batch learning scenario unexplored.
- What evidence would resolve it: Experiments comparing the performance of the proposed approach in both online and batch learning settings on standard continual learning benchmarks.

### Open Question 3
- Question: How would the proposed approach perform with other probability distributions on the unit sphere, such as a multivariate Student-t distribution?
- Basis in paper: [inferred] The authors mention that other probability distributions could be considered, but do not explore this possibility.
- Why unresolved: The paper focuses on the von Mises-Fisher and angular Gaussian distributions, leaving other potential distributions unexplored.
- What evidence would resolve it: Experiments comparing the performance of the proposed approach using different probability distributions on the unit sphere on various continual learning benchmarks.

### Open Question 4
- Question: How would the proposed approach perform in scenarios with more than 100 tasks or extremely large numbers of classes?
- Basis in paper: [inferred] The authors mention that their method performs well on CIFAR-100 with 10 tasks and Tiny ImageNet with 100 tasks, but do not explore scenarios with more tasks or classes.
- Why unresolved: The paper focuses on moderate-scale continual learning scenarios, leaving extreme-scale scenarios unexplored.
- What evidence would resolve it: Experiments testing the scalability of the proposed approach to scenarios with significantly more tasks and classes than those explored in the paper.

## Limitations
- The assumption that fixed basis directions provide optimal representation learning for complex, real-world data distributions remains largely untested
- Computational efficiency claims rely on specific hardware and implementation details that may not generalize
- Performance on datasets with highly overlapping classes or complex intra-class variations remains untested

## Confidence
- **High confidence**: The MAP-based loss formulation and its implementation are mathematically sound and the empirical results on standard benchmarks are robust.
- **Medium confidence**: The resilience to data drift mechanism through fixed directions is plausible but requires more extensive validation across diverse data distributions.
- **Low confidence**: The theoretical justification for the multi-augmentation strategy and its specific hyperparameters (5 views) in the online CL setting.

## Next Checks
1. **Distribution validation**: Perform statistical tests (e.g., Kolmogorov-Smirnov) to verify whether learned representations actually follow the assumed von Mises-Fisher or angular Gaussian distributions across different tasks and datasets.

2. **Ablation on augmentation**: Systematically vary the number of augmentations (1, 3, 5, 10) and measure the performance tradeoff, particularly focusing on the computational cost vs. accuracy gains.

3. **Robustness to distribution shift**: Test the method on datasets with gradual concept drift (e.g., CORe50 or iWildCam) where the fixed direction assumption may be more severely challenged.