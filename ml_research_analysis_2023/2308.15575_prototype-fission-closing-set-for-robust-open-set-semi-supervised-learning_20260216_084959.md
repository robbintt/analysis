---
ver: rpa2
title: 'Prototype Fission: Closing Set for Robust Open-set Semi-supervised Learning'
arxiv_id: '2308.15575'
source_url: https://arxiv.org/abs/2308.15575
tags:
- prototype
- prototypes
- learning
- class
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of open-set semi-supervised learning,
  where unlabeled data contains out-of-distribution (OOD) samples that cause over-confident
  pseudo-labeling of OODs as in-distribution (ID). The core method, Prototype Fission
  (PF), divides class-wise latent spaces into compact sub-spaces by forming multiple
  unique learnable sub-class prototypes for each class.
---

# Prototype Fission: Closing Set for Robust Open-set Semi-supervised Learning

## Quick Facts
- arXiv ID: 2308.15575
- Source URL: https://arxiv.org/abs/2308.15575
- Reference count: 5
- Primary result: Prototype Fission (PF) improves open-set semi-supervised learning by dividing class-wise latent spaces into compact sub-spaces, achieving 88.74% overall accuracy on CIFAR-10 and 47.92% on CIFAR-100 when combined with ORCA.

## Executive Summary
This paper addresses the problem of over-confident pseudo-labeling of out-of-distribution (OOD) samples as in-distribution samples in open-set semi-supervised learning. The core method, Prototype Fission (PF), divides class-wise latent spaces into multiple compact sub-spaces by forming multiple unique learnable sub-class prototypes for each class. PF achieves this through a combination of diversity modeling (encouraging samples to cluster by one of multiple sub-class prototypes) and consistency modeling (clustering all samples of the same class to a global prototype). The method focuses on "closing set" rather than "opening set," making it compatible with existing methods for further performance gains. Extensive experiments on CIFAR-10 and CIFAR-100 validate PF's effectiveness in forming sub-classes, discriminating OODs from IDs, and improving overall accuracy.

## Method Summary
Prototype Fission introduces a novel approach to open-set semi-supervised learning by dividing class-wise latent spaces into multiple compact sub-spaces. The method forms multiple learnable sub-class prototypes per class (V=5), each optimized towards both diversity and consistency objectives. The diversity modeling term encourages samples to be clustered by one of the multiple sub-class prototypes, while the consistency modeling term clusters all samples of the same class to a global prototype. This creates an adversarial training dynamic where prototypes maintain both diversity and consistency. PF is designed to work with existing methods like ORCA and FixMatch, focusing on compacting ID spaces rather than explicitly modeling OOD distributions. The overall loss function combines max-based loss (Lmax), diversity loss (Ldiv), and consistency loss (Lcst) with weighting factors λdiv=0.1 and λcst=1.0.

## Key Results
- PF combined with ORCA achieved 88.74% overall accuracy on CIFAR-10 and 47.92% on CIFAR-100, outperforming baseline methods.
- Significant gains in ID-OOD discrimination robustness on CIFAR-100, with improved AUC scores.
- PF effectively forms sub-classes and improves compactness of class-wise latent spaces, leading to better OOD rejection.
- The method shows consistent improvements across different experimental settings and is compatible with existing SSL frameworks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototype Fission divides class-wise latent spaces into multiple compact sub-spaces to make ID spaces more compact and reject OOD samples.
- Mechanism: Forms multiple learnable sub-class prototypes per class, optimized towards both diversity and consistency. The Diversity Modeling term encourages samples to be clustered by one of the multiple sub-class prototypes, while the Consistency Modeling term clusters all samples of the same class to a global prototype.
- Core assumption: Class-wise semantic can be formed by sub-optimally mixing different concepts, and clustering all samples to one prototype makes the class-wise latent space rather diffused.
- Evidence anchors:
  - [abstract] "A key underlying problem is class-wise latent space spreading from closed seen space to open unseen space, and the bias is further magnified in SSL's self-training loops."
  - [section] "As is illustrated in Fig.3, the existence of multiple sub-semantics can be easily observed from the sample distribution that shares an identical class label..."
- Break condition: If the class semantics are truly homogeneous and do not contain sub-semantics, then dividing into sub-spaces would not improve compactness.

### Mechanism 2
- Claim: The Adversarial Prototype Fission training strategy maintains both diversity and consistency of the prototypes.
- Mechanism: The overall loss function forms adversarial objectives: Lcst encourages that all of V prototypes are sufficiently activated, while Ldiv and Lmax encourage that one of V prototypes is slightly more activated.
- Core assumption: Adversarial objectives between consistency and diversity terms will lead to better prototype formation than simple max-based or average-based losses alone.
- Evidence anchors:
  - [section] "Optimizing this L helps maintain both consistency and diversity."
  - [section] "By weighted summing the aforementioned three loss functions, we form the final overall loss function as follows: L = (Lmax + λdiv · Ldiv) + λcst · Lcst"
- Break condition: If the weighting between consistency and diversity terms is not properly tuned, the adversarial objectives may lead to unstable training.

### Mechanism 3
- Claim: Prototype Fission focuses on "closing set" rather than "opening set", making it compatible with existing methods for further performance gains.
- Mechanism: Instead of modeling OOD distribution, Prototype Fission makes it hard for OOD samples to fit in sub-class latent space by compacting ID spaces.
- Core assumption: Improving the compactness of ID spaces will inherently make it harder for OOD samples to fit in, without needing to explicitly model OOD distribution.
- Evidence anchors:
  - [abstract] "Instead of 'opening set', i.e., modeling OOD distribution, Prototype Fission 'closes set' and makes it hard for OOD samples to fit in sub-class latent space."
  - [section] "This nature makes it highly compatible with 'opening set' methods for further accuracy and safety gains."
- Break condition: If OOD samples are very similar to ID samples in the original space, simply compacting ID spaces may not be sufficient to reject them.

## Foundational Learning

- Concept: Semi-supervised Learning (SSL) and its vulnerability to out-of-distribution (OOD) samples
  - Why needed here: Understanding the problem that Prototype Fission addresses - the over-confident pseudo-labeling of OOD samples as ID samples in SSL.
  - Quick check question: Why is SSL particularly vulnerable to OOD samples compared to fully supervised learning?

- Concept: Prototype Learning (PL) methods and their use of multiple prototypes
- Why needed here: Prototype Fission is a type of PL method that uses multiple prototypes per class. Understanding how PL methods work is crucial to understanding Prototype Fission.
  - Quick check question: How do traditional PL methods typically form prototypes, and how does Prototype Fission differ from them?

- Concept: Metric Learning for OOD detection
- Why needed here: While Prototype Fission focuses on compacting ID spaces, it can still benefit from metric learning methods for OOD detection. Understanding metric learning helps in understanding the full scope of OOD handling in SSL.
  - Quick check question: What are the different types of metrics used for OOD detection in the open-set vision literature?

## Architecture Onboarding

- Component map:
  - Shared feature extractor (f) -> Global prototype (gc) per class and Multiple local prototypes (l(c,i)) per class -> Trainable weights (λc,i) to balance global and local components -> Diversity loss (Ldiv) and Consistency loss (Lcst) -> Max-based loss (Lmax)

- Critical path:
  1. Extract features from input using shared feature extractor
  2. Compute similarities between features and prototypes
  3. Calculate diversity and consistency losses
  4. Update prototypes and weights using overall loss

- Design tradeoffs:
  - Number of prototypes per class (V): More prototypes may capture more sub-semantics but increase computational cost and risk of overfitting
  - Balance between diversity and consistency terms: Too much diversity may lead to overfitting to local appearances, while too much consistency may not capture enough sub-semantics

- Failure signatures:
  - If prototypes collapse to a single effective prototype, diversity loss will not be effective
  - If prototypes focus too much on low-level appearances, consistency loss may dominate and lead to poor generalization

- First 3 experiments:
  1. Ablation study removing the diversity loss to see its impact on OOD rejection
  2. Ablation study removing the consistency loss to see its impact on overall accuracy
  3. Experiment varying the number of prototypes per class to find the optimal balance between capturing sub-semantics and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Prototype Fission vary with different numbers of sub-class prototypes (V)?
- Basis in paper: [explicit] The paper mentions that V is set to 5, but does not explore how varying V affects performance.
- Why unresolved: The paper does not provide an ablation study on the number of prototypes.
- What evidence would resolve it: An ablation study varying V (e.g., 3, 5, 7, 10) and reporting the impact on accuracy and OOD detection performance.

### Open Question 2
- Question: How does Prototype Fission perform on datasets other than CIFAR-10 and CIFAR-100, such as large-scale datasets like ImageNet or real-world open-set datasets?
- Basis in paper: [inferred] The paper focuses on CIFAR datasets and does not evaluate on larger or more diverse datasets.
- Why unresolved: The paper does not provide experiments on larger-scale or real-world open-set datasets.
- What evidence would resolve it: Experiments on datasets like ImageNet, or real-world open-set datasets with varying class numbers and distribution mismatches.

### Open Question 3
- Question: How does the temperature term T in the assignment vector A(x) affect the performance of Prototype Fission?
- Basis in paper: [explicit] The paper mentions using T = 10 but does not explore its impact on performance.
- Why unresolved: The paper does not provide an ablation study on the temperature parameter.
- What evidence would resolve it: An ablation study varying T and reporting the impact on accuracy and OOD detection performance.

### Open Question 4
- Question: How does Prototype Fission compare to methods that explicitly model OOD distributions in terms of robustness to OOD samples?
- Basis in paper: [explicit] The paper claims that PF "closes set" and is orthogonal to OOD modeling methods, but does not directly compare to methods that explicitly model OOD distributions.
- Why unresolved: The paper does not provide a direct comparison to methods that explicitly model OOD distributions.
- What evidence would resolve it: Experiments comparing PF to methods like Reciprocal Points Learning or Open-Set Recognition methods on OOD detection performance.

## Limitations

- The paper's evaluation is limited to CIFAR-10 and CIFAR-100 datasets, and its effectiveness on larger-scale datasets like ImageNet remains untested.
- The choice of V=5 prototypes per class appears arbitrary, and the sensitivity analysis for this hyperparameter is limited.
- The computational overhead introduced by multiple prototypes is not thoroughly explored, which could be significant in real-world applications.

## Confidence

- **High Confidence**: The mechanism of using multiple prototypes to compact class-wise latent spaces and improve OOD rejection is well-supported by experimental results.
- **Medium Confidence**: The effectiveness of the adversarial training strategy balancing diversity and consistency terms is demonstrated but relies heavily on specific hyperparameter settings.
- **Medium Confidence**: The claim that Prototype Fission is highly compatible with existing "opening set" methods is supported by ORCA+PF results, but the nature of this compatibility needs more exploration.

## Next Checks

1. **Scalability Test**: Implement Prototype Fission on a larger dataset (e.g., TinyImageNet or STL-10) to evaluate whether the performance gains and computational overhead scale proportionally with dataset complexity.

2. **Hyperparameter Sensitivity Analysis**: Conduct a comprehensive ablation study varying V (number of prototypes per class) from 2 to 10, and systematically tune λdiv and λcst weights to identify optimal configurations and potential failure modes.

3. **Cross-method Compatibility Validation**: Test Prototype Fission's integration with alternative SSL methods beyond ORCA (such as FixMatch, UDA, or MixMatch) to verify the claimed compatibility and identify any method-specific limitations or synergistic effects.