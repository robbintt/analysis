---
ver: rpa2
title: Compositional Generalization in Spoken Language Understanding
arxiv_id: '2312.15815'
source_url: https://arxiv.org/abs/2312.15815
tags:
- slot
- compositional
- generalization
- training
- utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Spoken language understanding (SLU) models struggle with compositional
  generalization, failing to recognize novel slot combinations or longer utterances
  with more slots than seen during training. To address this, researchers created
  the first compositional splits of benchmark SLU datasets (ATIS and SNIPS) and proposed
  a compositional SLU model with two key techniques: a compositional loss that reduces
  spurious slot correlations and encourages focus on informative context words, and
  paired training that concatenates utterances to improve length generalization.'
---

# Compositional Generalization in Spoken Language Understanding

## Quick Facts
- **arXiv ID**: 2312.15815
- **Source URL**: https://arxiv.org/abs/2312.15815
- **Reference count**: 40
- **Primary result**: Compositional SLU model achieves up to 5% improvement in slot tagging F1 score compared to BERT-based models on benchmark datasets with novel compositional splits.

## Executive Summary
Spoken language understanding (SLU) models struggle with compositional generalization, particularly with recognizing novel slot combinations and longer utterances containing more slots than seen during training. To address this, researchers created the first compositional splits of benchmark SLU datasets (ATIS and SNIPS) and proposed a compositional SLU model with two key techniques: a compositional loss that reduces spurious slot correlations and encourages focus on informative context words, and paired training that concatenates utterances to improve length generalization. Their model significantly outperformed state-of-the-art BERT-based SLU models, achieving up to 5% improvement in slot tagging F1 score on both compositional and standard splits.

## Method Summary
The researchers created compositional splits of ATIS and SNIPS datasets focusing on novel slot combinations and length generalization. They proposed a compositional SLU model that builds upon BERT-base-uncased with two key innovations: a compositional loss function that encourages attention distributions to focus on informative context words rather than correlated slot patterns, and a paired training data augmentation technique that concatenates utterances with disjoint slot combinations. The model is fine-tuned with hyperparameters λ1=1, λ2=0.01, λ3=0.1, batch size 32, learning rate ∈ {10, 5}×10^-5, and training steps N ∈ {4K, 5K, 6K}.

## Key Results
- Compositional SLU model achieved up to 5% improvement in slot tagging F1 score compared to BERT-based models
- Novel slot combination split showed significant performance gains over baseline models
- Length generalization split demonstrated improved handling of longer utterances with more slots
- Both compositional and standard splits showed performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Compositional loss reduces spurious slot correlations by forcing the model to focus on informative context words for each slot.
- **Mechanism**: The compositional loss explicitly minimizes the KL divergence between attention distributions of different slot words, encouraging them to focus on disjoint sets of context words. This breaks reliance on correlated slot patterns and promotes learning true semantic meaning based on relevant context.
- **Core assumption**: The attention distribution in the final transformer layer can be effectively shaped by the compositional loss to focus on informative words rather than correlated slot patterns.
- **Evidence anchors**: [abstract] mentions compositional loss improving compositionality; [section] describes Lslot-pair encouraging disjoint attention distributions; related papers don't directly mention compositional loss.

### Mechanism 2
- **Claim**: Paired training improves length generalization by exposing the model to longer utterances and novel slot combinations through data augmentation.
- **Mechanism**: Paired training concatenates two distinct training utterances of the same intent but with disjoint slot combinations. This creates new training samples that are longer than any individual utterance and contain slot combinations not present in the original training set.
- **Core assumption**: Concatenating utterances with disjoint slot combinations creates meaningful longer utterances that the model can learn from, and this exposure is sufficient to improve length generalization.
- **Evidence anchors**: [abstract] mentions paired training improving length generalization; [section] describes concatenating utterances with disjoint slots; related papers don't directly mention paired training.

### Mechanism 3
- **Claim**: The combination of compositional loss and paired training leads to significant improvements in both novel slot combination and length generalization.
- **Mechanism**: The compositional loss addresses spurious slot correlations while paired training tackles length generalization. Together they provide comprehensive solution improving model's ability to handle both compositional scenarios.
- **Core assumption**: The compositional loss and paired training are complementary techniques addressing different aspects of compositional generalization, and their combined effect is greater than sum of individual effects.
- **Evidence anchors**: [abstract] shows overall performance improvement; [section] describes ablation study results showing similar effects on novel slot combination and length generalization; related papers don't mention combined effect.

## Foundational Learning

- **Concept**: Attention mechanisms in transformer models
  - **Why needed**: Understanding attention distributions is crucial for grasping how compositional loss shapes model's focus on informative context words.
  - **Quick check**: How does attention distribution in transformer model determine which input tokens contribute to output for given token?

- **Concept**: Data augmentation techniques
  - **Why needed**: Paired training is form of data augmentation, understanding principles behind data augmentation is essential for comprehending how it improves length generalization.
  - **Quick check**: What are key considerations when designing data augmentation strategy for specific task?

- **Concept**: Compositional generalization
  - **Why needed**: Paper focuses on improving compositional generalization in SLU models, understanding concept and challenges is fundamental to grasping motivation and significance.
  - **Quick check**: What are key differences between compositional generalization and other forms of generalization in machine learning?

## Architecture Onboarding

- **Component map**: BERT-base-uncased -> Compositional loss -> Paired training -> Intent classification and slot tagging tasks
- **Critical path**: 1) Fine-tune BERT model on SLU tasks (intent classification and slot tagging), 2) Apply compositional loss to shape attention distributions, 3) Use paired training to augment data with longer utterances and novel slot combinations, 4) Evaluate performance on compositional splits
- **Design tradeoffs**: Computational cost increases with compositional loss and paired training; complexity increases with new components and techniques; while improving compositional generalization, techniques may have unintended effects on other performance aspects
- **Failure signatures**: Spurious correlations persist if compositional loss fails to shape attention distributions; poor length generalization if paired training doesn't provide meaningful longer utterances; decreased overall performance if new components harm non-compositional aspects
- **First 3 experiments**: 1) Ablation study: remove compositional loss and evaluate on novel slot combination split, 2) Ablation study: remove paired training and evaluate on length generalization split, 3) Baseline comparison: train and evaluate baseline BERT SLU model on standard splits

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does compositional SLU model perform on even longer utterances than those tested in length generalization split, such as utterances with 8+ slots?
- **Basis in paper**: [inferred] Paper tests length generalization up to 7 slots in ATIS and 6 slots in SNIPS, but doesn't explore performance on even longer utterances that may occur in real-world scenarios.
- **Why unresolved**: Authors chose to focus on testing up to 7 slots in ATIS and 6 slots in SNIPS, which may not be representative of full range of utterance lengths model could encounter in practice.
- **What evidence would resolve it**: Evaluating compositional SLU model on utterances with 8+ slots in both ATIS and SNIPS datasets to determine if performance degrades significantly with increasing utterance length.

### Open Question 2
- **Question**: How does compositional SLU model's performance compare to other state-of-the-art models that use techniques like data augmentation or meta-learning to improve compositional generalization?
- **Basis in paper**: [inferred] Paper compares to baseline BERT models and few other variants, but doesn't compare to other state-of-the-art models that specifically target compositional generalization through techniques like data augmentation or meta-learning.
- **Why unresolved**: Authors focused on comparing to BERT-based models and few simple variants, but didn't include comprehensive comparison to other state-of-the-art models using more advanced techniques.
- **What evidence would resolve it**: Conducting experiments comparing compositional SLU model's performance to other state-of-the-art models using data augmentation, meta-learning, or other advanced techniques to improve compositional generalization on same compositional splits.

### Open Question 3
- **Question**: How well does compositional SLU model generalize to other domains and tasks beyond ATIS and SNIPS datasets, such as dialogue act classification or emotion recognition in conversations?
- **Basis in paper**: [inferred] Paper only evaluates on ATIS and SNIPS datasets specific to airline reservation and personal assistant domains, doesn't explore model's performance on other domains or tasks.
- **Why unresolved**: Authors focused on evaluating model on two specific benchmark datasets and didn't investigate generalization to other domains or tasks with different characteristics and requirements.
- **What evidence would resolve it**: Testing compositional SLU model on other benchmark datasets or real-world datasets from different domains (e.g., dialogue act classification, emotion recognition) and tasks to assess ability to generalize beyond airline reservation and personal assistant domains.

## Limitations

- Effectiveness of compositional loss relies heavily on attention mechanism responding as expected to loss function, if attention doesn't shape properly, model may still struggle with spurious slot correlations
- Paired training assumes concatenating utterances with disjoint slot combinations creates meaningful longer sequences that model can learn from, if concatenated utterances don't form coherent longer utterances, length generalization improvements may be limited
- Combined effect of compositional loss and paired training assumes these techniques are complementary addressing different aspects of compositional generalization, if assumption doesn't hold, overall improvement may be less than expected

## Confidence

- **High Confidence**: Core findings that proposed compositional SLU model outperforms state-of-the-art BERT SLU models on compositional splits (up to 5% F1 score improvement) supported by experimental results and ablation studies
- **Medium Confidence**: Mechanisms proposed for why compositional loss and paired training work are well-motivated and align with experimental results, but exact nature of how techniques interact and combined effect could benefit from further investigation
- **Low Confidence**: Generalizability of proposed techniques to other SLU datasets and tasks beyond ATIS and SNIPS not explicitly discussed, effectiveness may vary depending on specific characteristics of different datasets

## Next Checks

1. Apply proposed compositional SLU model to other SLU datasets with different characteristics (e.g., different domains, languages, or slot label distributions) to assess generalizability of techniques
2. Conduct more extensive ablation studies to isolate individual contributions of compositional loss and paired training, and understand their interaction effects by varying hyperparameters and evaluating impact on performance
3. Perform detailed analysis of attention distributions in final transformer layer with and without compositional loss to validate claim that loss effectively shapes attention to focus on informative context words and reduce spurious correlations