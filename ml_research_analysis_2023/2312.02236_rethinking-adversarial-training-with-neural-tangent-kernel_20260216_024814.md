---
ver: rpa2
title: Rethinking Adversarial Training with Neural Tangent Kernel
arxiv_id: '2312.02236'
source_url: https://arxiv.org/abs/2312.02236
tags:
- training
- kernel
- pgd-at
- clean
- normal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies adversarial training (AT) using Neural Tangent\
  \ Kernel (NTK) theory to uncover new insights and improve AT methods. It makes three\
  \ main contributions: (1) theoretically analyzes how data normalization impacts\
  \ AT, showing that unbiased estimators in batch normalization layers affect clean\
  \ and robust accuracy; (2) experimentally reveals that NTK undergoes a threefold\
  \ evolution during AT (\"kernel learning\" \u2192 \"lazy training\" \u2192 \"kernel\
  \ learning\"), enabling a new training paradigm that reduces AT time cost by up\
  \ to 50% without sacrificing robustness; and (3) explains catastrophic overfitting\
  \ in single-step AT as arising from isotropic features, proposing a simple solution\
  \ by adding anisotropic noise to mini-batches."
---

# Rethinking Adversarial Training with Neural Tangent Kernel

## Quick Facts
- **arXiv ID**: 2312.02236
- **Source URL**: https://arxiv.org/abs/2312.02236
- **Reference count**: 40
- **Primary result**: Shows that adversarial training changes the Neural Tangent Kernel (NTK) of clean data, explaining reduced clean accuracy, and reveals a threefold NTK evolution pattern that enables 50% reduction in AT training time without sacrificing robustness.

## Executive Summary
This paper uses Neural Tangent Kernel (NTK) theory to analyze adversarial training (AT) dynamics and improve AT methods. The authors make three main contributions: (1) they theoretically analyze how data normalization impacts AT, showing that unbiased estimators in batch normalization layers affect clean and robust accuracy differently; (2) they experimentally reveal that NTK undergoes a threefold evolution during AT ("kernel learning" → "lazy training" → "kernel learning"), enabling a new training paradigm that reduces AT time cost by up to 50% without sacrificing robustness; and (3) they explain catastrophic overfitting in single-step AT as arising from isotropic features, proposing a simple solution by adding anisotropic noise to mini-batches.

## Method Summary
The authors analyze adversarial training using Neural Tangent Kernel theory, computing NTK matrices for clean data and adversarial examples throughout training. They investigate how batch normalization layers with running statistics affect robust accuracy, observe NTK evolution patterns across different AT methods, and propose new training paradigms based on these observations. The method involves calculating kernel distances between different training strategies, kernel evolution rates, and kernel specialization metrics to understand model behavior during AT.

## Key Results
- Adversarial training changes the NTK of clean data, explaining why it harms clean accuracy
- NTK undergoes a threefold evolution during AT ("kernel learning" → "lazy training" → "kernel learning")
- New training paradigm reduces AT time cost by up to 50% without sacrificing robustness
- Adding anisotropic noise to mini-batches prevents catastrophic overfitting in single-step AT

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adversarial training changes the Neural Tangent Kernel (NTK) of clean data, explaining why it harms clean accuracy.
- **Mechanism**: When adversarial examples (AEs) are used in training, the perturbation added to clean samples shifts the NTK. This shift is approximated as Θ(X + Ωt-1, X)t-1 ≈ Θ(X, X)t-1 + o(ϵ), where Ωt-1 is the perturbation set independent of the data, parameters, and loss.
- **Core assumption**: The perturbation added to each pixel is either -ϵ or ϵ, determined by the sign of the gradient with respect to that pixel.
- **Evidence anchors**: [abstract] "theoretically analyzes how data normalization impacts AT, showing that unbiased estimators in batch normalization layers affect clean and robust accuracy"; [section] "Theorem 1: Given a C2 function f : Rnin → Rnout... NTK of X satisfies Θ(X + Ωt-1, X)t-1 ≈ Θ(X, X)t-1 + o(ϵ)"
- **Break condition**: If the perturbation distribution significantly deviates from {−ϵ, ϵ} or if the perturbation is not independent of x, θ, and ℓ.

### Mechanism 2
- **Claim**: Global statistics in batch normalization layers (running mean and variance) are crucial for robust accuracy but less important for clean accuracy.
- **Mechanism**: Batch normalization layers use running mean (Σ) and variance (Π) as global statistics. When evaluating on clean data, these estimators are close to the batch-specific statistics (Σ̃i, Π̃i), so replacing them has minimal impact. However, for AEs, the batch statistics differ significantly from the global ones, so removing the unbiased estimators reduces robust accuracy.
- **Core assumption**: The running mean and variance in batch normalization are unbiased estimators of the dataset's mean and variance.
- **Evidence anchors**: [abstract] "theoretically analyzes how data normalization impacts AT, showing that unbiased estimators in batch normalization layers affect clean and robust accuracy"; [section] "Theorem 2: Given a C-Lipschitz function f : R → R... the computation error of a kernel f between mini-batch training and full dataset training mainly depends on the gap between the mean and variance of the batch and dataset"
- **Break condition**: If the batch normalization layers do not use running statistics or if the data distribution is highly non-stationary.

### Mechanism 3
- **Claim**: The threefold evolution of NTK during adversarial training ("kernel learning" → "lazy training" → "kernel learning") enables reduced training cost without sacrificing robustness.
- **Mechanism**: At early epochs, NTKs from different training strategies converge to the same kernel. During "lazy training," the kernel remains static, allowing normal training to be used initially. After learning rate decay, the kernel evolves differently for each strategy. By starting with normal training and switching to adversarial training later, the same robust kernel is achieved with less computation.
- **Core assumption**: The "lazy training" phase exists where the kernel remains static, and the later "kernel learning" phase is where strategies diverge.
- **Evidence anchors**: [abstract] "experimentally reveals that NTK undergoes a threefold evolution during AT... enabling a new training paradigm that reduces AT time cost by up to 50%"; [section] "In Figure 1... the kernel distance is close to zero for all training strategies after a very quick 'kernel learning' process... The 'lazy learning' appears between the 10-th epoch and 100-th epoch, where the kernel stays almost unchanged"
- **Break condition**: If the kernel does not enter a static phase or if the later phase does not differentiate strategies enough to maintain robustness.

## Foundational Learning

- **Concept**: Neural Tangent Kernel (NTK)
  - Why needed here: NTK provides a theoretical framework to analyze how neural network training dynamics relate to kernel methods, which is central to understanding adversarial training effects.
  - Quick check question: What is the shape of the NTK matrix for a dataset with |X| samples and nout output classes?

- **Concept**: Batch Normalization
  - Why needed here: Batch normalization layers use running statistics that affect model robustness differently for clean data versus adversarial examples.
  - Quick check question: How do the running mean and variance in batch normalization layers differ from batch-specific statistics?

- **Concept**: Kernel Specialization
  - Why needed here: Kernel specialization measures how the NTK evolves to align with target functions for each class, indicating model robustness.
  - Quick check question: What does a high kernel specialization value indicate about a model's behavior on adversarial examples?

## Architecture Onboarding

- **Component map**: Data → Adversarial example generation → Model training (with/without batch norm buffers) → NTK computation → Evaluation
- **Critical path**: Data → Adversarial example generation → Model training (with/without batch norm buffers) → NTK computation → Evaluation
- **Design tradeoffs**: Using batch normalization with buffers increases robust accuracy but may slightly reduce clean accuracy; removing buffers simplifies training but hurts robustness.
- **Failure signatures**: If KD between training strategies remains high throughout training, the "lazy training" assumption fails; if robust accuracy drops sharply after a certain epoch, catastrophic overfitting may occur.
- **First 3 experiments**:
  1. Train a model with PGD-AT and compute NTK at epochs 10, 50, 100, 150, 200 to observe the threefold evolution.
  2. Train two models with PGD-AT: one with batch norm buffers enabled, one disabled; compare clean and robust accuracy.
  3. Train a model with normal training for 100 epochs, then switch to PGD-AT for 100 epochs; compare performance and training time to full PGD-AT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we theoretically explain why different initialization methods lead to different learned kernels while maintaining similar training dynamics and model performance?
- **Basis in paper**: [explicit] The paper shows empirically that different initialization methods (Gaussian, Truncated Normal, Uniform) lead to different kernel specializations (Figure 18) but similar training dynamics and performance (Figures 12-16).
- **Why unresolved**: The paper states "it seems like that the kernel dynamics is the main reason that influences the model's performance, and the two models having different kernels can still have similar performance on its task" but doesn't provide theoretical explanation for this observation.
- **What evidence would resolve it**: A theoretical framework explaining how kernel dynamics, rather than final kernel structure, determines model performance across different initialization methods.

### Open Question 2
- **Question**: What is the underlying mechanism that causes the threefold evolution of NTK during adversarial training ("kernel learning" → "lazy training" → "kernel learning")?
- **Basis in paper**: [explicit] The paper observes this threefold evolution pattern across multiple AT methods and datasets (Figures 1, 13-16) but states "we do not deeply study the reason why the kernel have three stages in the training process."
- **Why unresolved**: While the pattern is empirically observed, the paper doesn't provide theoretical justification for why this specific three-phase pattern emerges during AT but not normal training.
- **What evidence would resolve it**: Mathematical analysis showing how the interaction between adversarial perturbations and gradient descent creates these distinct phases in the NTK evolution.

### Open Question 3
- **Question**: Why does kernel specialization on clean data NTKs not correlate with model robustness while specialization on adversarial data NTKs does?
- **Basis in paper**: [explicit] The paper observes that "KS on C-NTKs does not have this property [correlation with robustness]" while "KS on AE-NTKs can reflect the model's robustness" (Section 3.2), but doesn't explain this discrepancy.
- **Why unresolved**: The paper hypothesizes that "robust models consider features from other classes to classify AEs, while non-robust models mainly depend on class-related features" but doesn't explain why this difference doesn't manifest in clean data NTKs.
- **What evidence would resolve it**: Analysis of how feature selection mechanisms differ between clean and adversarial examples, and why these differences are captured in AE-NTKs but not C-NTKs.

## Limitations

- The theoretical claims about NTK shift during adversarial training rely on specific assumptions about perturbation distributions that may not hold for all AT variants
- Empirical validation of the threefold NTK evolution pattern is based on ResNet-18 on CIFAR datasets, with unclear generalization to other architectures
- The proposed solution for catastrophic overfitting (anisotropic noise) requires more extensive validation across diverse scenarios

## Confidence

- **High**: The experimental methodology for computing NTK metrics and observing training dynamics is well-specified
- **Medium**: The theoretical analysis of batch normalization's impact on clean vs. robust accuracy has solid mathematical grounding but limited empirical breadth
- **Low**: The proposed solution for catastrophic overfitting (anisotropic noise) requires more extensive validation across diverse scenarios

## Next Checks

1. **Cross-architecture validation**: Test the threefold NTK evolution pattern on architectures beyond ResNet-18 (e.g., WideResNet, EfficientNet) to assess generalizability
2. **Alternative perturbation distributions**: Evaluate whether the NTK shift mechanism holds when perturbations deviate from {−ϵ, ϵ} distributions, particularly for adaptive AT methods
3. **Robustness of overfitting solution**: Test the anisotropic noise approach on datasets with different characteristics (e.g., SVHN, Tiny ImageNet) and with varying ε values to validate its effectiveness across diverse settings