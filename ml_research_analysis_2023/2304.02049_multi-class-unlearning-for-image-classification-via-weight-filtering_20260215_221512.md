---
ver: rpa2
title: Multi-Class Unlearning for Image Classification via Weight Filtering
arxiv_id: '2304.02049'
source_url: https://arxiv.org/abs/2304.02049
tags:
- classes
- unlearning
- class
- training
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-class unlearning framework for image
  classification that can unlearn all classes in a single round. The key idea is to
  use memory matrices to modulate network components, enabling selective unlearning
  behavior for any class.
---

# Multi-Class Unlearning for Image Classification via Weight Filtering

## Quick Facts
- arXiv ID: 2304.02049
- Source URL: https://arxiv.org/abs/2304.02049
- Authors: 
- Reference count: 40
- This paper introduces a multi-class unlearning framework for image classification that can unlearn all classes in a single round.

## Executive Summary
This paper proposes a novel multi-class unlearning framework for image classification that enables selective removal of any class's impact from a trained model in a single untraining round. The key innovation is the use of weight filtering layers that modulate network components through memory matrices, allowing the model to demonstrate selective unlearning behavior for any class after training. The approach is tested on small- and medium-scale datasets with CNN and Transformer backbones, showing significant unlearning performance while maintaining reasonable accuracy on retained classes.

## Method Summary
The method introduces Weight Filtering (WF-Net) layers that are inserted after each convolutional or attention layer in the backbone network. These layers contain learnable memory matrices α that selectively turn on/off inner network components (filters or attention projections) for each target class. The unlearning process uses a reciprocal forget loss formulation combined with retaining loss and label expansion to balance the conflicting objectives. The α matrices are trained using Adam optimizer while the original backbone weights remain frozen. The approach avoids the need for multiple untraining rounds or storing multiple models, making it parameter-efficient and scalable.

## Key Results
- Achieves near-zero accuracy on unlearned classes while maintaining reasonable accuracy on retained classes
- Provides interpretable insights into relationships between network components and output classes
- Successfully tested on MNIST, CIFAR-10, and ImageNet-1k with VGG-16, ResNet-18, and ViT backbones
- Unlearning performance measured by accuracy, activation distance, JS-divergence, and zero retrain forgetting score

## Why This Works (Mechanism)

### Mechanism 1
The weight filtering layer enables selective unlearning by modulating inner network components through memory matrices. Each weight filtering layer encapsulates network operators (e.g., convolutional filters or attention projections) and applies a learnable memory matrix α that selectively turns on/off these components. During forward pass, the layer multiplies the original weights by the corresponding row of α selected by the target class label, effectively filtering out information related to that class.

### Mechanism 2
The reciprocal forget loss formulation enables bounded optimization while maintaining class separation. Instead of directly maximizing the forget loss (which would be unbounded and numerically unstable), the method uses the reciprocal of the forget loss: L = λ₀∑Lr + λ₁∑(1/Lf). This formulation is bounded below by zero, ensuring numerical stability while still encouraging the model to minimize retention loss and maximize forgetting loss.

### Mechanism 3
Label expansion compensates for class imbalance in the unlearning objective by amplifying the impact of retaining samples. The method splits each mini-batch into two halves - one for unlearning (using ground truth labels) and one for retaining (using randomly sampled labels). The retaining portion is replicated χ times with different random labels, giving it χ times greater impact in the loss function.

## Foundational Learning

- Concept: Backpropagation and gradient-based optimization
  - Why needed here: The weight filtering approach relies on gradient-based training of the α matrices while keeping original network weights frozen. Understanding how gradients flow through the weight filtering layer is essential for implementing and debugging the training procedure.
  - Quick check question: If we have a convolutional layer with weights W and α matrix, what is the gradient of the loss with respect to α during backpropagation?

- Concept: Multi-task learning and objective balancing
  - Why needed here: The method simultaneously optimizes two conflicting objectives - unlearning specific classes while retaining others. Understanding techniques for balancing multiple objectives and preventing one from dominating is crucial for successful implementation.
  - Quick check question: What happens if λ₁ (the weight for the reciprocal forget loss) is set to zero? What if it's set too high?

- Concept: Convolutional neural networks and vision transformers architecture
  - Why needed here: The weight filtering layer must be adapted to different architectures (CNNs vs Transformers) by masking different components (convolutional filters vs attention projections). Understanding the internal structure of these architectures is essential for proper implementation.
  - Quick check question: In a ResNet block, which components would you mask with the weight filtering layer to achieve effective unlearning?

## Architecture Onboarding

- Component map:
  - Original backbone (VGG, ResNet, ViT) - frozen weights
  - Weight filtering layers inserted after each convolutional or attention layer
  - Memory matrices α for each layer (Nc × K shape, where Nc is number of classes and K is number of filters/projections)
  - Two loss functions: retaining loss and reciprocal forget loss
  - Label expansion mechanism for mini-batch processing

- Critical path:
  1. Initialize α matrices to 3 (σ(3) ≈ 1)
  2. During forward pass: select α row based on target class, multiply with layer weights
  3. Compute losses: retaining on half batch, unlearning on other half
  4. Apply label expansion to retaining portion
  5. Backpropagate only through α matrices
  6. Update α matrices with Adam optimizer
  7. Clip α values to [-3, 3] to prevent sigmoid saturation

- Design tradeoffs:
  - Weight filtering vs weight perturbation: Weight filtering is more parameter-efficient but may be less flexible than direct weight modification
  - Reciprocal vs direct forget loss: Reciprocal formulation is numerically stable but may converge slower
  - Label expansion factor: Higher values improve retention but increase memory usage and training time

- Failure signatures:
  - Poor unlearning: High accuracy on forget classes, high ZRF score
  - Catastrophic forgetting: Low accuracy on retain classes, accuracy close to random
  - Numerical instability: NaN or infinite loss values during training
  - Gradient vanishing: α matrices stop updating, weights remain at initialization

- First 3 experiments:
  1. Verify forward pass: Run a single forward pass with a known class label and check that the correct α row is being selected and applied to the weights
  2. Test loss computation: Create a small synthetic dataset and verify that the retaining and forget losses are computed correctly with label expansion
  3. Debug gradient flow: Check that gradients are flowing only to α matrices and not to the frozen backbone weights during backpropagation

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed approach change when applied to larger-scale datasets like ImageNet-22k or JFT-300M? The authors mention that they tested their approach on small-scale and medium-scale datasets, but do not provide results for larger-scale datasets.

### Open Question 2
How does the proposed approach compare to other unlearning methods in terms of computational efficiency and storage requirements? The authors mention that their approach avoids the need for storing multiple models and performing multiple untraining rounds, but do not provide a direct comparison with other methods.

### Open Question 3
How does the proposed approach perform when unlearning classes with overlapping features or similar visual characteristics? The authors mention that their approach can discover relationships between filters and classes, but do not specifically address the performance when unlearning classes with overlapping features.

## Limitations
- Requires access to training data for the unlearning process, which may not be available in practical privacy scenarios
- Computational overhead of adding weight filtering layers and maintaining memory matrices could be significant for large-scale models
- Tested primarily on relatively small datasets with limited evaluation on larger, more complex datasets

## Confidence

High confidence in the core mechanism of weight filtering and its ability to achieve selective unlearning on small datasets
Medium confidence in the numerical stability of the reciprocal forget loss formulation and its effectiveness on medium-scale datasets
Low confidence in the generalizability of the approach to very large-scale models and real-world privacy scenarios without access to training data

## Next Checks

1. Test the weight filtering approach on a larger dataset (e.g., ImageNet-22k) to evaluate scalability and performance on more complex classification tasks
2. Evaluate the method's performance when only a subset of training data is available for unlearning, simulating real-world data privacy constraints
3. Compare the computational overhead and memory requirements of weight filtering layers against baseline unlearning methods to assess practical feasibility