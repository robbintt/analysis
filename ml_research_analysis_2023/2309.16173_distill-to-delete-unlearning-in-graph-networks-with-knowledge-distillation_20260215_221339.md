---
ver: rpa2
title: 'Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation'
arxiv_id: '2309.16173'
source_url: https://arxiv.org/abs/2309.16173
tags:
- graph
- unlearning
- knowledge
- d2dgn
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes D2DGN, a novel knowledge distillation-based\
  \ method for graph unlearning that effectively removes information from pre-trained\
  \ graph neural networks (GNNs) while preserving performance on retained data. The\
  \ method introduces two separator models\u2014a preserver and a destroyer\u2014\
  that distill knowledge from the source model using response-based soft targets and\
  \ feature-based node embeddings."
---

# Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation

## Quick Facts
- arXiv ID: 2309.16173
- Source URL: https://arxiv.org/abs/2309.16173
- Reference count: 32
- Key outcome: D2DGN achieves up to 43.1% AUC improvement in graph unlearning with 10.2×10⁶ fewer FLOPs and 3.2× faster than state-of-the-art

## Executive Summary
This paper introduces D2DGN, a novel knowledge distillation-based approach for graph unlearning that removes information from pre-trained graph neural networks while preserving performance on retained data. The method uses two specialized separator models - a preserver and a destroyer - that distill knowledge from the source model using response-based soft targets and feature-based node embeddings. Evaluated across five real-world graph datasets and three GNN architectures, D2DGN demonstrates state-of-the-art performance in consistency, integrity, and membership privacy while being significantly more efficient than existing approaches.

## Method Summary
D2DGN is a knowledge distillation framework for graph unlearning that introduces two separator models to handle retained and deleted knowledge separately. The preserver model distills knowledge marked for retention, while the destroyer model handles knowledge marked for deletion. The method employs KL divergence for response-based soft targets and MSE for feature-based node embeddings across intermediate layers. This architecture enables model-agnostic unlearning without requiring access to training data or optimization details, achieving superior performance in both effectiveness and computational efficiency compared to the state-of-the-art GNNDelete approach.

## Key Results
- Achieves up to 43.1% improvement in AUC for edge and node unlearning tasks
- Demonstrates 2.4% higher AUC than GNNDelete while requiring 10.2×10⁶ fewer FLOPs
- Improves membership inference ratio by +1.3, indicating better privacy preservation
- Processes up to 3.2× faster than competing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-separator architecture enables targeted separation of retained vs. deleted knowledge
- Mechanism: Two separator models (preserver and destroyer) distill knowledge using different embeddings - preserver retains with positive embeddings, destroyer removes with neutral or negative embeddings
- Core assumption: Graph elements in retain and forget sets cannot be cleanly separated due to neighborhood dependencies
- Evidence anchors: [abstract] "D2DGN's distillation architecture introduces two separators for distilling knowledge from the source model." [section] "Since separating the graph entities... is challenging due to their intricate relationships... we introduce two models... that separate retained and deleted knowledge."
- Break condition: If graph structure is already partitioned, dual separators may add unnecessary complexity

### Mechanism 2
- Claim: KL divergence and MSE losses enable effective distillation of different knowledge types
- Mechanism: KL divergence measures similarity between probability distributions for response-based knowledge, while MSE measures similarity between node embeddings for feature-based knowledge
- Core assumption: Different knowledge representations require different distance metrics
- Evidence anchors: [abstract] "It performs distillation with response-based soft targets and feature-based node embedding while minimizing KL divergence." [section] "We explore two loss measures... Kullback-Leibler Divergence (KL-Divergence) and Mean Squared Error (MSE)."
- Break condition: If source model's knowledge is primarily in final layer outputs, MSE on intermediate embeddings may provide diminishing returns

### Mechanism 3
- Claim: Knowledge distillation enables model-agnostic unlearning without training data access
- Mechanism: Transfers knowledge from pre-trained source model to unlearned model through distillation, bypassing need for retraining
- Core assumption: Knowledge distillation can effectively transfer what to keep and what to forget
- Evidence anchors: [abstract] "D2DGN surpasses the state-of-the-art GNNDelete in AUC by 2.4%... requires 10.2 × 106 fewer FLOPs... and up to 3.2× faster."
- Break condition: If source model has significant architectural constraints that don't transfer well via distillation

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Enables transfer of knowledge from pre-trained model to unlearned model without retraining
  - Quick check question: What is the difference between response-based and feature-based knowledge distillation?

- Concept: Graph Neural Networks
  - Why needed here: Understanding how GNNs encode structural information and dependencies between nodes/edges
  - Quick check question: How do neighborhood dependencies in graphs complicate the unlearning process?

- Concept: KL Divergence and MSE Loss Functions
  - Why needed here: These are the core metrics for measuring similarity between different knowledge representations
  - Quick check question: When would you use KL divergence vs. MSE for comparing model outputs?

## Architecture Onboarding

- Component map: Source model -> Preserver separator -> Destroyer separator -> Unlearned model
- Critical path: Source model trained on complete dataset → Distillation process using both separators → Unlearned model operates on modified graph
- Design tradeoffs:
  - Preserver uses trained model (better preservation) vs. untrained (faster but less effective)
  - Destroyer uses untrained model (neutral knowledge) vs. trained model (negative knowledge)
  - Response-based vs. feature-based knowledge types
- Failure signatures:
  - High AUC on forget set but low MI ratio indicates overfitting to deleted data
  - Poor performance on retain set indicates loss of critical knowledge
  - Inference cost close to original model indicates distillation not effective
- First 3 experiments:
  1. Compare Strategy 1 (response-based) vs. Strategy 2 (feature-based) on small dataset to validate knowledge separation
  2. Test destroyer variants (untrained vs. trained) on edge unlearning to measure effectiveness
  3. Evaluate computational efficiency by measuring FLOPs on different GNN architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does D2DGN perform when unlearning edges that are not in the 2-hop neighborhood of retained edges?
- Basis in paper: [explicit] The paper mentions that unlearning Ef,OUT (edges outside the 2-hop enclosing subgraph) is "relatively easier" than Ef,IN, but doesn't provide comprehensive results
- Why unresolved: The paper focuses primarily on Ef,IN results and only briefly mentions Ef,OUT performance in ablation studies
- What evidence would resolve it: Comprehensive evaluation of D2DGN's performance on Ef,OUT across multiple datasets and GNN architectures

### Open Question 2
- Question: What is the impact of different temperature settings (T) in the softmax function on D2DGN's performance?
- Basis in paper: [inferred] The paper uses temperature parameter T in Eq. 8 for soft targets but doesn't explore how different values affect unlearning effectiveness
- Why unresolved: Temperature settings can significantly impact knowledge distillation, yet the paper uses a fixed value without sensitivity analysis
- What evidence would resolve it: Systematic evaluation of D2DGN with varying temperature parameters and their effect on AUC, membership inference ratio, and unlearning consistency

### Open Question 3
- Question: How does D2DGN scale to dynamic graphs with evolving structures over time?
- Basis in paper: [explicit] The paper mentions future work will evaluate D2DGN on "time-evolving dataset" but doesn't provide any results
- Why unresolved: The current evaluation is on static graphs, and real-world applications often involve dynamic graphs
- What evidence would resolve it: Experimental results showing D2DGN's performance on time-series graph data with continuous edge/node additions and deletions

## Limitations

- Computational efficiency claims (10.2×10⁶ fewer FLOPs) need independent verification and depend heavily on implementation details
- Membership privacy improvements (+1.3 MI ratio) are presented without baseline context, making practical significance unclear
- Performance improvements appear to be relative to baselines that may not represent current state-of-the-art

## Confidence

- **High Confidence:** Dual-separator architecture design and knowledge distillation mechanism are technically sound and well-motivated by graph structure dependencies
- **Medium Confidence:** Reported performance improvements (2.4% AUC gain over GNNDelete) appear reasonable but need more comprehensive ablation studies
- **Low Confidence:** Claim about being "up to 3.2× faster" requires clarification on implementation aspects and hardware configurations

## Next Checks

1. **Independent Implementation Verification:** Implement D2DGN on Cora dataset from scratch to verify the claimed 2.4% AUC improvement over GNNDelete, including careful FLOPs and inference time measurement

2. **Cross-Architecture Consistency:** Test D2DGN across GCN, GAT, and GIN architectures on multiple datasets to verify model-agnostic benefits and identify architecture-specific limitations

3. **Membership Privacy Validation:** Conduct independent membership inference attacks on unlearned models to verify the +1.3 improvement in membership privacy ratio and assess consistency across different attack strategies