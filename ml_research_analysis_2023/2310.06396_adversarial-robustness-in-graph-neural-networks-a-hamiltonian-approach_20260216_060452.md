---
ver: rpa2
title: 'Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach'
arxiv_id: '2310.06396'
source_url: https://arxiv.org/abs/2310.06396
tags:
- graph
- neural
- stability
- robustness
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hamiltonian-inspired graph neural flows (HANG)
  to enhance the adversarial robustness of graph neural networks. The authors argue
  that while Lyapunov stability is commonly used, it alone does not ensure adversarial
  robustness.
---

# Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach

## Quick Facts
- **arXiv ID**: 2310.06396
- **Source URL**: https://arxiv.org/abs/2310.06396
- **Reference count**: 40
- **Primary result**: HANG models significantly improve adversarial robustness of GNNs by leveraging conservative Hamiltonian flows with Lyapunov stability

## Executive Summary
This paper addresses the challenge of adversarial robustness in graph neural networks (GNNs) by introducing Hamiltonian-inspired graph neural flows (HANG). The authors argue that while Lyapunov stability is commonly used for robustness, it alone doesn't ensure protection against adversarial perturbations. By drawing inspiration from physics principles, particularly conservative Hamiltonian dynamics, they propose a framework where graph neural flows conserve a scalar energy function, creating a global constraint that limits how node features can evolve under attacks. Extensive experiments across multiple benchmark datasets demonstrate that HANG models, especially those incorporating Lyapunov stability, substantially improve robustness against various adversarial attacks while maintaining competitive clean accuracy.

## Method Summary
The paper proposes Hamiltonian-inspired graph neural flows (HANG) that model node features as positions q(t) and momenta p(t) evolving according to Hamiltonian dynamics. The core component is an energy function Hnet(q,p) that governs the system's evolution through differential equations. Two variants are introduced: HANG with pure energy conservation, and HANG-quad which combines energy conservation with Lyapunov stability. The models are trained end-to-end using ODE solvers to integrate the dynamics from initial node features to final classification outputs. The energy conservation property creates a global constraint that makes localized adversarial perturbations less effective, while Lyapunov stability provides additional local convergence guarantees.

## Key Results
- HANG-quad achieves 50.9% accuracy on Cora under PGD attack (vs 43.1% for baseline GCN)
- On Pubmed, HANG-quad maintains 69.9% accuracy under PGD attack while other methods drop below 40%
- HANG models show consistent improvements across multiple attack types (PGD, TDGIA, MetaGIA, Metattack) and datasets
- The energy-conservative property is maintained during training with minimal additional computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hamiltonian energy conservation provides a global constraint that mitigates adversarial perturbations.
- Mechanism: The model conserves a scalar energy function Hnet along the trajectory, creating a global constraint that limits how node features and their rates of change can evolve under perturbations.
- Core assumption: The energy function Hnet can be effectively learned by a neural network to represent meaningful graph dynamics.
- Evidence anchors:
  - [abstract] "Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows"
  - [section 4.2] "If the graph system evolves in accordance with (10), the total energy Hnet(q(t), p(t)) of the system remains constant"
  - [corpus] "Average neighbor FMR=0.482" - moderate relatedness to adversarial robustness in graph neural networks
- Break condition: If the learned Hnet fails to capture meaningful graph dynamics, the energy conservation becomes a weak constraint that doesn't prevent adversarial attacks.

### Mechanism 2
- Claim: Lyapunov stability combined with Hamiltonian conservation creates stronger robustness than either alone.
- Mechanism: Lyapunov stability ensures local convergence properties while Hamiltonian conservation provides global energy constraints, together creating multiple layers of protection against perturbations.
- Core assumption: The energy function can be designed to have appropriate stability properties (e.g., a single global minimum).
- Evidence anchors:
  - [abstract] "GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness"
  - [section 5.2] "Theorem 3 implies that we can design an energy function Hnet such that the induced graph neural flow is both Lyapunov stable and energy conservative"
  - [corpus] "Control the GNN: Utilizing Neural Controller with Lyapunov Stability for Test-Time Feature Reconstruction" - shows other work combining Lyapunov stability with GNNs
- Break condition: If the energy function has multiple local minima or the Lyapunov stability conditions are violated, the combined protection breaks down.

### Mechanism 3
- Claim: Hamiltonian modeling captures global graph structure rather than local node relationships, making attacks harder.
- Mechanism: The Hamiltonian framework considers the entire graph's energy, making localized adversarial perturbations less effective since they must maintain global consistency.
- Core assumption: Graph topology significantly influences the energy function Hnet in a way that captures global structure.
- Evidence anchors:
  - [section 4.1] "the energy function Hnet involves interactions between neighboring nodes, signifying the importance of the graph's topology"
  - [section 6.4] "HANG-quad model demonstrates superior performance compared to other methods" under modification attacks
  - [corpus] "Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness" - suggests global structure matters for robustness
- Break condition: If the energy function doesn't adequately capture global topology or if attacks can bypass the global constraint, this mechanism fails.

## Foundational Learning

- Concept: Hamiltonian mechanics and energy conservation
  - Why needed here: The paper's core contribution relies on understanding how energy conservation in Hamiltonian systems provides robustness properties
  - Quick check question: What is the mathematical expression for energy conservation in a Hamiltonian system, and how does it translate to the graph setting?

- Concept: Lyapunov stability in dynamical systems
  - Why needed here: The paper contrasts different stability notions and argues that Lyapunov stability alone is insufficient for adversarial robustness
  - Quick check question: What is the difference between Lyapunov stability and asymptotic stability, and why does the paper claim Lyapunov stability alone doesn't ensure robustness?

- Concept: Graph neural flows and their connection to ODEs
  - Why needed here: The paper builds on existing graph neural flow literature to introduce its Hamiltonian variant
  - Quick check question: How do graph neural flows generalize traditional GNNs, and what role do ODEs play in this generalization?

## Architecture Onboarding

- Component map:
  Raw features → FC compression layer → q(0), p(0) initialization → Hnet energy function → ODE solver integration → q(T) extraction → classification layer

- Critical path: Input features → Energy function Hnet → ODE integration → Output features → Classification
  The Hamiltonian energy function Hnet is the critical component that determines the system's evolution and robustness properties.

- Design tradeoffs:
  - Computational cost vs. robustness: HANG models require more computation due to ODE integration but provide better robustness
  - Model complexity vs. interpretability: More complex energy functions may provide better performance but are harder to interpret
  - Solver choice: Different ODE solvers offer tradeoffs between accuracy and computational efficiency

- Failure signatures:
  - Poor clean accuracy might indicate the energy function Hnet isn't capturing meaningful graph dynamics
  - Sensitivity to attack strength varying across datasets suggests the energy conservation may not be uniformly effective
  - Performance degradation with different ODE solvers might indicate numerical stability issues

- First 3 experiments:
  1. Implement the basic HANG model with a simple energy function and verify it maintains energy conservation on a small synthetic graph
  2. Compare HANG with and without Lyapunov stability (HANG vs HANG-quad) on a standard dataset to observe the robustness difference
  3. Test different ODE solvers (Euler, Symplectic-Euler) on the same model to understand their impact on both performance and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating KAM theory into HANG models affect their robustness against quasi-periodic adversarial attacks?
- Basis in paper: [explicit] The authors acknowledge that while the energy-conserving nature of their Hamiltonian-inspired model inherently offers some level of robustness to perturbations, an explicit incorporation of KAM theory could potentially further improve the robustness, particularly in the face of quasi-periodic adversarial attacks.
- Why unresolved: The authors state that this is a complex task due to the high dimensionality of typical graph datasets and the intricacies involved in approximating quasi-periodic dynamics.
- What evidence would resolve it: Experimental results demonstrating improved robustness against quasi-periodic attacks after incorporating KAM theory into HANG models.

### Open Question 2
- Question: How do different stability notions beyond BIBO and Lyapunov stability affect adversarial robustness in graph neural flows?
- Basis in paper: [inferred] The authors mention that their finding that energy-conservative Hamiltonian graph flows improve robustness is only one facet of the broader landscape of potential stability measures, and it is possible that other notions of stability, not covered in this work, could yield additional insights into adversarial robustness.
- Why unresolved: The authors did not explore other stability notions in their work.
- What evidence would resolve it: A comprehensive study comparing the adversarial robustness of graph neural flows under various stability notions.

### Open Question 3
- Question: How does the choice of ODE solver affect the adversarial robustness of HANG models?
- Basis in paper: [explicit] The authors state that different ODE solvers deliver comparable performance in terms of clean accuracy, but their performance under attack conditions remains fairly consistent. They chose the Euler solver for computational efficiency.
- Why unresolved: The authors did not conduct a detailed comparison of different ODE solvers' impact on adversarial robustness.
- What evidence would resolve it: A systematic evaluation of HANG models using various ODE solvers under different adversarial attacks.

## Limitations

- Theoretical framework connecting Hamiltonian mechanics to robustness needs more rigorous proof beyond empirical observations
- Claims about necessity of combining Hamiltonian conservation with Lyapunov stability for optimal robustness are not fully proven
- The specific design of Hnet energy functions and their relationship to graph topology remains somewhat heuristic

## Confidence

- **High confidence**: Empirical results showing HANG models outperform baselines in adversarial robustness (Section 6)
- **Medium confidence**: Theoretical framework connecting Hamiltonian mechanics to GNN robustness (Sections 4-5)
- **Low confidence**: Claims about the necessity of combining Hamiltonian conservation with Lyapunov stability for optimal robustness (Theorem 3 implications)

## Next Checks

1. Verify energy conservation numerically across multiple runs and datasets to ensure the Hamiltonian property is consistently maintained
2. Test HANG models against stronger adaptive attacks designed to specifically target the energy conservation mechanism
3. Compare performance with other graph regularization approaches (e.g., graph smoothing, spectral regularization) to isolate the contribution of Hamiltonian dynamics