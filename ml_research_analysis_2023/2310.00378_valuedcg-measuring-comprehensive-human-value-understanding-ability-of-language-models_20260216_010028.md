---
ver: rpa2
title: 'ValueDCG: Measuring Comprehensive Human Value Understanding Ability of Language
  Models'
arxiv_id: '2310.00378'
source_url: https://arxiv.org/abs/2310.00378
tags:
- value
- values
- llms
- know
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ValueDCG, a comprehensive metric for measuring
  the human value understanding ability of Large Language Models (LLMs). The authors
  argue that truly understanding values requires considering both "know what" and
  "know why" aspects.
---

# ValueDCG: Measuring Comprehensive Human Value Understanding Ability of Language Models

## Quick Facts
- arXiv ID: 2310.00378
- Source URL: https://arxiv.org/abs/2310.00378
- Reference count: 6
- Key outcome: Introduces ValueDCG metric showing larger LLMs don't necessarily understand values better, just recognize them more effectively

## Executive Summary
This paper introduces ValueDCG, a comprehensive metric for measuring the human value understanding ability of Large Language Models (LLMs). The authors argue that truly understanding values requires considering both "know what" and "know why" aspects. To evaluate this, they construct a dialogue dataset based on the Schwartz Value Survey and assess four representative LLMs. The results show that the growth rates of LLMs' "know what" and "know why" capabilities do not align with increases in parameter numbers, resulting in a decline in value understanding as models become larger. This suggests that LLMs might generate plausible explanations without truly comprehending the underlying values, indicating potential risks.

## Method Summary
The ValueDCG metric evaluates LLMs' value understanding by measuring the gap between their ability to recognize which values apply ("know what") and their ability to explain why those values apply ("know why"). The method constructs a thousand-level dialogue dataset based on the Schwartz Value Survey, containing 100 questions across 10 value categories. GPT-4 serves as both the discriminator (evaluating value similarity between LLM responses and baseline answers) and critique (evaluating reasoning similarity between LLM explanations and GPT-4's annotated reasons). The DCG score is calculated as the absolute difference between these two similarity measures, providing a quantitative assessment of value understanding capability.

## Key Results
- Larger LLMs show improved "know what" (value recognition) but not "know why" (value reasoning) capabilities
- LLMs understand neutral values better than potentially harmful values like Power and Hedonism
- Context significantly influences value expression more than inherent value understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ValueDCG metric effectively distinguishes between LLMs' surface-level response generation and deeper value understanding by measuring the gap between "know what" and "know why" components.
- Mechanism: The framework quantifies value understanding by comparing two similarity scores: discriminator score (semantic similarity between LLM response and baseline answers) and critique score (similarity between LLM's reasoning and GPT-4's annotated reasons). The absolute difference between these scores (DCG) indicates the gap between knowing which value applies versus understanding why it applies.
- Core assumption: GPT-4 can serve as a reliable reference for both value judgment (discriminator) and reasoning ability (critique) when prompted appropriately.
- Evidence anchors: [abstract] "We present a comprehensive evaluation metric, ValueDCG (Value Discriminator-Critique Gap), to quantitatively assess the two aspects" [section 4.3] "we calculate the estimation of the discrepancy between these two metrics to obtain the DCG value" [corpus] Weak evidence - no directly comparable frameworks found, but related work on "NormAd" and "TrustGPT" suggests growing interest in LLM value evaluation
- Break condition: If GPT-4's judgment is not reliable or consistent across different value contexts, the discriminator-critique gap measurement becomes invalid.

### Mechanism 2
- Claim: Context significantly influences LLMs' value expression more than their inherent value understanding.
- Mechanism: When LLMs are prompted with value-specific contexts versus no induction, their DCG scores show that parameter scaling primarily improves "know what" (discriminator) but not "know why" (critique), suggesting context-dependent behavior rather than genuine value comprehension.
- Core assumption: The scaling law affects discriminator scores more than critique scores, indicating that larger models are better at recognizing values in context but not necessarily understanding underlying reasons.
- Evidence anchors: [abstract] "the scaling law significantly impacts 'know what' but not much on 'know why', which has consistently maintained a high level" [section 5.3] "the scaling law (Kaplan et al., 2020) significantly impacts 'know what' but not much on 'know why'"
- Break condition: If parameter scaling begins to affect both discriminator and critique scores similarly, the mechanism fails to explain the observed pattern.

### Mechanism 3
- Claim: LLMs exhibit better understanding of neutral values compared to potentially harmful values due to alignment training constraints.
- Mechanism: Safety algorithms that constrain harmful outputs may inadvertently reduce LLMs' ability to comprehend and generalize about potentially harmful values like "Power" and "Hedonism," while neutral values like "Benevolence" are better understood.
- Core assumption: Alignment algorithms that suppress harmful outputs also reduce the model's exposure to and understanding of those value concepts.
- Evidence anchors: [abstract] "LLM's understanding of potentially harmful values like 'Power' is inadequate" [section 5.3] "LLMs tend to better understand neutral values, whereas their comprehension of potentially risky values is diminished"
- Break condition: If alignment algorithms can be designed to constrain harmful outputs while preserving understanding of those concepts, this mechanism fails.

## Foundational Learning

- Concept: Discriminator-Critique Gap (DCG) measurement
  - Why needed here: DCG provides a quantitative framework to assess the gap between surface-level value recognition and deeper value understanding in LLMs
  - Quick check question: How does the DCG metric differentiate between an LLM that correctly identifies a value versus one that understands why that value applies?

- Concept: Schwartz Value Survey framework
  - Why needed here: Provides a standardized set of 10 universal values (Self-Direction, Stimulation, Hedonism, Achievement, Power, Security, Conformity, Tradition, Spirituality, Benevolence) for consistent evaluation across different LLMs
  - Quick check question: Why is it important to use a standardized value framework like Schwartz's rather than ad-hoc value definitions?

- Concept: In-context learning vs. fine-tuning
  - Why needed here: The paper demonstrates that LLMs can be guided to express different values through context without requiring additional fine-tuning, which has implications for deployment strategies
  - Quick check question: What are the advantages and limitations of using in-context methods versus fine-tuning for value alignment?

## Architecture Onboarding

- Component map: ValueDCG metric -> Dataset construction -> GPT-4 annotation -> LLM evaluation -> DCG calculation -> Analysis
- Critical path: Dataset construction → GPT-4 annotation → LLM evaluation → DCG calculation → Analysis
- Design tradeoffs: Using GPT-4 as both discriminator and critique reference provides consistency but introduces potential bias; context-dependent evaluation captures real-world applicability but may not reflect inherent capabilities
- Failure signatures: Inconsistent GPT-4 annotations, high variance in DCG scores across similar contexts, or failure of scaling law patterns to emerge
- First 3 experiments:
  1. Validate GPT-4 annotation consistency by having multiple runs on the same dataset
  2. Test DCG sensitivity by varying prompt formulations for the same value questions
  3. Compare in-context vs. fine-tuned approaches for specific value alignment tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the decline in value understanding with increasing model size indicate a fundamental limitation in LLMs' ability to truly comprehend values, or is it an artifact of the current evaluation methodology?
- Basis in paper: [inferred] The paper shows that larger LLMs perform worse on value understanding tasks, but this could be due to the evaluation method rather than an inherent limitation.
- Why unresolved: The study uses a specific metric (ValueDCG) and dataset that may not fully capture the nuances of value understanding. Alternative evaluation methods or datasets could yield different results.
- What evidence would resolve it: Developing and testing alternative evaluation frameworks for value understanding in LLMs, and comparing results across different methodologies and datasets.

### Open Question 2
- Question: How do LLMs' value understanding capabilities evolve over time as they are exposed to more diverse and complex value-related contexts during training and fine-tuning?
- Basis in paper: [explicit] The paper mentions that LLMs' value comprehension is more context-dependent than inherent, but doesn't explore how this changes over time.
- Why unresolved: The study uses a static dataset and doesn't track changes in LLMs' value understanding as they are exposed to new information or fine-tuned on different datasets.
- What evidence would resolve it: Longitudinal studies tracking LLMs' value understanding performance on the same tasks as they are fine-tuned on increasingly diverse and complex value-related datasets.

### Open Question 3
- Question: Can we develop more effective safety mechanisms that not only prevent harmful outputs but also enhance LLMs' understanding and generalization of potentially risky values like power and hedonism?
- Basis in paper: [explicit] The paper suggests that current safety mechanisms may actually reduce LLMs' understanding of potentially harmful values, posing risks in practical applications.
- Why unresolved: The study identifies a problem with current safety approaches but doesn't propose or test alternative methods for improving value understanding while maintaining safety.
- What evidence would resolve it: Developing and testing new safety mechanisms that explicitly target value understanding, and measuring their impact on both safety and value comprehension in LLMs.

## Limitations

- The reliability of GPT-4 as both discriminator and critique reference creates potential circular dependencies in the evaluation framework
- The study doesn't provide direct causal evidence linking alignment algorithms to reduced understanding of potentially harmful values
- Dataset construction process and exact prompt formulations for GPT-4 annotations are underspecified, making it difficult to assess robustness

## Confidence

**High Confidence**: The core finding that larger LLMs show improved "know what" (value recognition) but not necessarily improved "know why" (value reasoning) capabilities is well-supported by the empirical results presented.

**Medium Confidence**: The claim that LLMs understand neutral values better than potentially harmful values due to alignment training effects has moderate support but lacks direct causal evidence linking specific alignment algorithms to reduced value comprehension.

**Low Confidence**: The broader implications about LLMs generating plausible explanations without true comprehension, and the associated risks this poses, are speculative extrapolations from the DCG metric results.

## Next Checks

1. **Inter-rater reliability test**: Conduct multiple independent GPT-4 annotation runs on the same dataset to quantify annotation consistency and establish confidence intervals for the DCG metric.

2. **Behavioral validation study**: Design human evaluation experiments to verify whether high DCG scores actually correspond to genuine value understanding versus pattern matching, using blinded comparisons between LLM responses and human responses.

3. **Causal analysis of alignment effects**: Systematically compare LLMs with different alignment training approaches (or the same model with and without alignment fine-tuning) to directly test whether alignment algorithms reduce understanding of potentially harmful values while preserving understanding of neutral values.