---
ver: rpa2
title: Label Supervised LLaMA Finetuning
arxiv_id: '2310.01208'
source_url: https://arxiv.org/abs/2310.01208
tags:
- classification
- llms
- ls-llama
- tasks
- ls-unllama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a label-supervised adaptation approach for
  LLMs, named LS-LLaMA, which aims to finetune the model with discriminant labels.
  The approach extracts latent representations from the final LLaMA layer and projects
  them into the label space to compute the cross-entropy loss.
---

# Label Supervised LLaMA Finetuning

## Quick Facts
- arXiv ID: 2310.01208
- Source URL: https://arxiv.org/abs/2310.01208
- Reference count: 11
- Key outcome: LS-LLaMA finetunes LLaMA using label supervision via latent representation projection and LoRA, outperforming much larger models on text classification and achieving SotA on NER by removing causal masks.

## Executive Summary
This paper introduces LS-LLaMA, a label-supervised adaptation approach for fine-tuning LLaMA models on classification tasks. The method extracts latent representations from the final LLaMA layer, projects them into the label space using feed-forward layers, and computes cross-entropy loss for training. By removing the causal mask for token-level tasks, LS-unLLaMA achieves bidirectional attention, enabling state-of-the-art performance on named entity recognition while maintaining strong results on text classification compared to much larger models.

## Method Summary
The LS-LLaMA approach finetunes LLaMA models for classification by extracting hidden state vectors from the final decoder layer and projecting them through feed-forward layers to the label space, computing cross-entropy loss. The method uses LoRA (rank=12, alpha=32, dropout=0.1) for parameter-efficient adaptation. For sequence classification, it applies last-token pooling, while for token classification (NER), it removes the causal mask and uses max-over-time pooling to enable bidirectional attention. The approach is evaluated on text classification datasets (SST2, SST5, AGNews, Twitter-Fin, multilingual Amazon reviews) and NER datasets (CoNLL2003, OntoNotes V5.0).

## Key Results
- LS-LLaMA substantially outperforms LLMs ten times its size in text classification accuracy
- LS-unLLaMA achieves state-of-the-art performance on named entity recognition by removing causal masks
- LS-LLaMA consistently improves over robust baselines like BERT-Large and RoBERTa-Large across multiple classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing the causal mask from the decoder allows bidirectional information flow, improving token-level classification performance.
- Mechanism: The causal mask in standard LLaMA decoders restricts each token to only attend to previous tokens, preventing the model from using future context. By removing this mask, each token can attend to all other tokens bidirectionally, similar to encoder architectures like BERT.
- Core assumption: Bidirectional context is necessary for accurate token-level classification tasks like NER, where each token's label depends on both preceding and following tokens.
- Evidence anchors:
  - [abstract]: "by removing the causal mask from decoders, LS-unLLaMA achieves the state-of-the-art performance in named entity recognition (NER)"
  - [section]: "The causal masks CM... prevent information leaking... leading to critical information loss at the token level"
  - [corpus]: Weak - only general paper title "Large Language Models Do Multi-Label Classification Differently" found

### Mechanism 2
- Claim: Extracting latent representations from the final LLaMA layer and projecting them into label space enables effective supervised classification.
- Mechanism: Instead of using LLM outputs for next-token prediction, the paper extracts the hidden state vectors from the final decoder layer and maps them through feed-forward layers to the label space, computing cross-entropy loss for classification.
- Core assumption: The latent representations in the final LLaMA layer contain sufficient semantic information about the input text for label prediction, even though they were originally trained for autoregressive generation.
- Evidence anchors:
  - [abstract]: "We extract latent representations from the final LLaMA layer and project them into the label space to compute the cross-entropy loss"
  - [section]: "The pooling operation is applied to the latent representation... After passing through fully connected layers and a softmax layer, vector representation h is mapped to the label space"
  - [corpus]: Weak - only general papers about multi-label classification and LLMs as annotators found

### Mechanism 3
- Claim: Using LoRA for parameter-efficient fine-tuning enables effective adaptation without full model retraining.
- Mechanism: Low-Rank Adaptation (LoRA) freezes the original model weights and learns small rank-decomposition matrices that modify the forward pass, allowing efficient fine-tuning on limited computational resources.
- Core assumption: The low-rank structure is sufficient to capture task-specific adaptations needed for classification while maintaining the general language understanding capabilities of the pretrained model.
- Evidence anchors:
  - [abstract]: "The model is finetuned by Low-Rank Adaptation (LoRA) to minimize this loss"
  - [section]: "We apply LoRA [Hu et al., 2021] to finetune the LLaMA model to maximize the probability of the correct label"
  - [corpus]: Weak - only general paper about label-efficient supervised fine-tuning found

## Foundational Learning

- Concept: Cross-entropy loss for classification
  - Why needed here: The approach computes cross-entropy loss between predicted label logits and ground truth labels to train the classification head
  - Quick check question: What does the cross-entropy loss measure between predicted and true label distributions?

- Concept: Autoregressive vs bidirectional attention
  - Why needed here: Understanding the difference between causal (unidirectional) attention in LLaMA and bidirectional attention in BERT is crucial for grasping why mask removal helps
  - Quick check question: How does the causal mask in LLaMA decoders affect the information flow compared to BERT's bidirectional attention?

- Concept: Pooling operations for sequence representation
  - Why needed here: Different pooling strategies (max, average, last) are used to aggregate token representations into sequence-level representations for classification
  - Quick check question: What are the differences between max pooling, average pooling, and using the last token for sequence representation?

## Architecture Onboarding

- Component map:
  - Tokenizer: Converts input text to token IDs using LLaMA's tokenizer
  - LlamaForSequenceClassification: Modified LLaMA model with sequence classification head
  - LlamaForTokenClassification: Modified LLaMA model with token classification head (causal mask removed for LS-unLLaMA)
  - Pooling layer: Aggregates token representations (last token for LS-LLaMA, max-over-time for LS-unLLaMA)
  - Classification head: Feed-forward layers + softmax projecting to label space
  - LoRA adapters: Low-rank matrices added to attention layers for efficient fine-tuning

- Critical path:
  1. Input text → Tokenizer → Token IDs
  2. Token IDs → LLaMA model → Latent representations
  3. Latent representations → Pooling → Sequence representation
  4. Sequence representation → Classification head → Logits
  5. Logits + Ground truth → Cross-entropy loss
  6. Loss → LoRA optimization → Updated adapter weights

- Design tradeoffs:
  - Using LLaMA vs BERT: LLaMA is smaller and more efficient but requires mask removal for token tasks; BERT natively supports bidirectional attention
  - LoRA vs full fine-tuning: LoRA is parameter-efficient but may be limited by low-rank constraint; full fine-tuning uses more resources but can capture more complex adaptations
  - Last token vs max pooling: Last token is faster but may lose information; max pooling captures more global information but is computationally heavier

- Failure signatures:
  - Poor performance on small datasets: LS-unLLaMA may overfit or struggle to reconstruct attention weights
  - Unstable training: Loss may bounce back during training, especially for LS-unLLaMA on small datasets
  - Degraded performance on NER: LS-LLaMA shows significantly worse performance than LS-unLLaMA on NER tasks

- First 3 experiments:
  1. Compare SST2 classification performance between LS-LLaMA (last pooling) and LS-unLLaMA (max pooling)
  2. Test NER performance on CoNLL2003 to observe the performance gap between masked vs unmasked versions
  3. Ablation study on pooling methods (max, average, last) to determine optimal strategy for each variant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LS-LLaMA scale with increasing model size when sufficient training samples are available?
- Basis in paper: [explicit] The paper mentions that larger LLaMA-2 models (e.g., 13B) did not yield substantial improvements over smaller models (e.g., 7B) in their experiments, and that the availability of sufficient training samples becomes a critical determinant in applying label-supervised adaptation for larger LLMs.
- Why unresolved: The experiments in the paper were limited to LLaMA-2-7B and LLaMA-2-13B models, and the authors did not explore larger model sizes or provide a clear explanation for the lack of scalability.
- What evidence would resolve it: Further experiments with larger LLaMA models (e.g., LLaMA-2-30B, LLaMA-2-65B) and sufficient training samples would help determine the relationship between model size and performance in label-supervised adaptation.

### Open Question 2
- Question: What are the limitations and potential applications of LS-LLaMA and LS-unLLaMA in domains beyond text classification and named entity recognition?
- Basis in paper: [explicit] The paper focuses on text classification and named entity recognition tasks, and the authors mention that their findings may extend beyond these specific tasks, but do not provide concrete examples or explore other domains.
- Why unresolved: The paper does not investigate the performance of LS-LLaMA and LS-unLLaMA on other NLP tasks or domains, leaving the potential applications and limitations unexplored.
- What evidence would resolve it: Experiments on a diverse set of NLP tasks (e.g., sentiment analysis, question answering, machine translation) and domains (e.g., biomedical, legal, financial) would help identify the strengths and limitations of LS-LLaMA and LS-unLLaMA in different contexts.

### Open Question 3
- Question: How does the choice of pooling method (e.g., max, average, last) affect the performance of LS-LLaMA and LS-unLLaMA on different tasks and datasets?
- Basis in paper: [explicit] The paper mentions that the choice of pooling method (max, average, or last) has an impact on the performance of LS-LLaMA and LS-unLLaMA, and that the optimal pooling method depends on the presence or absence of causal masks.
- Why unresolved: The paper only explores a limited set of pooling methods and tasks, and does not provide a comprehensive analysis of how different pooling methods affect performance across various tasks and datasets.
- What evidence would resolve it: A systematic study of different pooling methods (e.g., attention-based pooling, hierarchical pooling) on a wide range of tasks and datasets would help identify the optimal pooling strategy for different scenarios and provide insights into the underlying mechanisms.

## Limitations

- The causal mask removal for LS-unLLaMA may lead to unstable training and overfitting on smaller datasets
- The low-rank constraint of LoRA (rank=12) might be insufficient for capturing complex task-specific adaptations
- The paper lacks ablation studies on pooling strategies and detailed analysis of learned adapter weights

## Confidence

**High Confidence (8/10)**: The mechanism of using cross-entropy loss with projected latent representations for classification is well-established and the empirical results for text classification tasks are consistent and reproducible.

**Medium Confidence (6/10)**: The claim that LS-LLaMA substantially outperforms LLMs ten times its size relies heavily on specific comparisons that may not generalize across all tasks and domains.

**Low Confidence (4/10)**: The scalability of this approach to significantly larger datasets and more complex classification tasks remains unproven.

## Next Checks

1. **Ablation study on pooling strategies**: Systematically compare max pooling, average pooling, and last token pooling across all classification tasks to determine optimal pooling strategy for each task type and verify that the reported choices are indeed optimal.

2. **LoRA rank sensitivity analysis**: Evaluate performance across different LoRA ranks (e.g., 4, 8, 16, 32) to determine the minimum rank needed for effective adaptation and identify potential overfitting or underfitting patterns.

3. **Comparison with full fine-tuning baseline**: Implement and evaluate full fine-tuning of LLaMA-2-7B without LoRA to quantify the parameter efficiency gains and determine if the low-rank constraint imposes meaningful limitations on task performance.