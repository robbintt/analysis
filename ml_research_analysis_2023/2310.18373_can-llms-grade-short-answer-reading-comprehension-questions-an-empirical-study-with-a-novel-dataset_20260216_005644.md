---
ver: rpa2
title: 'Can LLMs Grade Short-Answer Reading Comprehension Questions : An Empirical
  Study with a Novel Dataset'
arxiv_id: '2310.18373'
source_url: https://arxiv.org/abs/2310.18373
tags:
- student
- reading
- dataset
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel dataset of short answer reading comprehension
  questions from over 150 students in Ghana, and evaluates whether generative Large
  Language Models (LLMs) like GPT-4 can grade such responses. It compares a fine-tuned
  BERT transfer-learning model against GPT-3.5 and GPT-4 in zero-shot and few-shot
  settings.
---

# Can LLMs Grade Short-Answer Reading Comprehension Questions : An Empirical Study with a Novel Dataset

## Quick Facts
- arXiv ID: 2310.18373
- Source URL: https://arxiv.org/abs/2310.18373
- Reference count: 0
- Primary result: GPT-4 achieved near-perfect agreement with expert raters (QWK 0.92, F1 0.89) on a novel Ghana dataset, exceeding both a fine-tuned BERT baseline and human-level performance.

## Executive Summary
This study introduces a novel dataset of short-answer reading comprehension questions from 162 students in Ghana and evaluates whether generative Large Language Models (LLMs) like GPT-4 can grade such responses. The paper compares a fine-tuned BERT transfer-learning model against GPT-3.5 and GPT-4 in zero-shot and few-shot settings. GPT-4 achieved near-perfect agreement with expert raters (QWK 0.92, F1 0.89), exceeding both the transfer-learning baseline and human-level performance. Results show generative LLMs can reliably grade formative literacy tasks with minimal technical setup, offering potential for scalable, low-cost assessment in low- and middle-income countries.

## Method Summary
The study collected responses from 162 students in Ghana to short-answer reading comprehension questions, creating a novel dataset of 1,068 items. A fine-tuned BERT model was trained on a modified version of SQuAD 2.0 as a baseline. GPT-3.5 and GPT-4 were evaluated using zero-shot and few-shot prompting approaches. Performance was measured using Quadratic Weighted Kappa (QWK) for 3-class classification and Linear Weighted Kappa for 2-class classification, along with F1 scores. All LLM evaluations used temperature=0 to minimize variability.

## Key Results
- GPT-4 achieved QWK of 0.923 and F1 of 0.88 on the Ghana dataset, exceeding expert human raters (QWK 0.906, F1 0.86).
- Both GPT-4 and GPT-3.5 dramatically outperformed the fine-tuned BERT baseline in both 3-class and 2-class conditions.
- Minimal prompt engineering was sufficient for GPT-4 to achieve high performance, demonstrating the feasibility of LLM-based grading with low technical barriers.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative LLMs generalize better than fine-tuned BERT for short-answer grading in LMIC contexts.
- Mechanism: GPT-4's instruction-following capability allows it to perform short-answer grading via prompting rather than fine-tuning, reducing domain shift.
- Core assumption: The instruction-tuned model's pre-training captures sufficient semantic understanding to grade diverse student responses without task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "GPT-4 achieved near-perfect agreement with expert raters (QWK 0.92, F1 0.89), exceeding both the transfer-learning baseline and human-level performance."
  - [section] "We find that GPT-4 performed extremely well (Quadratic Weighted Kappa 0.923, F1 0.88) on the novel dataset, even exceeding expert human raters (Quadratic Weighted Kappa 0.906, F1 0.86)."
- Break condition: Performance degrades sharply if prompts lack sufficient examples or task context, or if student responses use highly colloquial or code-mixed language not represented in pre-training.

### Mechanism 2
- Claim: Prompt engineering with few-shot examples improves LLM grading accuracy.
- Mechanism: Providing labeled examples in the prompt enables in-context learning, allowing the model to infer grading criteria without fine-tuning.
- Core assumption: The model can extrapolate grading logic from a small number of examples to unseen responses.
- Evidence anchors:
  - [abstract] "GPT-4, with minimal prompt engineering, performed extremely well on grading the novel dataset."
  - [section] "Both GPT-4 and GPT-3.5 models performed dramatically better than the fine-tuned model, in both the 3-class and 2-class condition."
- Break condition: If examples are ambiguous or the prompt fails to specify grading rubrics clearly, the model's output becomes inconsistent.

### Mechanism 3
- Claim: Minimal technical barriers make generative LLMs feasible for deployment in low-resource educational contexts.
- Mechanism: Zero-shot/few-shot prompting removes the need for labeled datasets and technical fine-tuning expertise.
- Core assumption: API access to GPT-4 is sufficient for deployment without local model hosting or data infrastructure.
- Evidence anchors:
  - [abstract] "Results show generative LLMs can reliably grade formative literacy tasks with minimal technical setup, offering potential for scalable, low-cost assessment in low- and middle-income countries."
  - [section] "Importantly, in contrast transfer learning based approaches, generative LLMs generalize well and the technical barriers to their use are low, making them more feasible to implement and scale in lower resource educational contexts."
- Break condition: High API costs or unreliable internet access in target regions negate the low-barrier advantage.

## Foundational Learning

- **Quadratic Weighted Kappa (QWK)**
  - Why needed here: QWK measures agreement between model predictions and expert ratings on ordinal scales (incorrect, partially correct, correct).
  - Quick check question: What does a QWK of 0.92 indicate about the model's agreement with human raters?

- **Cohen's Kappa**
  - Why needed here: Used for binary classification agreement (incorrect vs. correct) to account for chance agreement.
  - Quick check question: How does Cohen's Kappa differ from simple accuracy in imbalanced datasets?

- **Fine-tuning vs. In-context Learning**
  - Why needed here: Understanding the distinction explains why GPT-4 outperforms BERT without task-specific training.
  - Quick check question: Why might a model trained with in-context learning generalize better than one fine-tuned on a specific dataset?

## Architecture Onboarding

- **Component map**: Data ingestion -> Prompt construction -> OpenAI API call -> Response parsing -> Agreement metric calculation
- **Critical path**: Student response -> Prompt template -> LLM inference -> Label prediction -> Performance evaluation
- **Design tradeoffs**: Few-shot prompting offers better accuracy but increases prompt size; zero-shot is more scalable but less accurate.
- **Failure signatures**: Inconsistent grading across similar responses; high misclassification rates for certain demographics; prompt injection vulnerabilities.
- **First 3 experiments**:
  1. Test zero-shot GPT-4 on a small subset of the Ghana dataset and record QWK.
  2. Add 3â€“5 few-shot examples to the prompt and re-evaluate performance.
  3. Compare GPT-3.5 vs. GPT-4 using identical prompts on the same data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-4's performance on short-answer reading comprehension questions be replicated with open-source large language models?
- Basis in paper: [explicit] The paper mentions that GPT-4's cost is a barrier to widespread use in low-resource settings and suggests that further research could test the performance of open-source models.
- Why unresolved: The study only evaluated proprietary models (GPT-3.5 and GPT-4) and did not explore the capabilities of open-source alternatives.
- What evidence would resolve it: Comparative studies testing open-source LLMs on the same dataset used in this research, or similar datasets from low- and middle-income countries.

### Open Question 2
- Question: How does GPT-4's performance on short-answer grading change as task complexity increases?
- Basis in paper: [explicit] The paper notes that its research used relatively simple questions and answers, and further research is needed to explore how findings hold as task complexity increases.
- Why unresolved: The study was limited to a specific type of reading comprehension question, and did not test more complex tasks.
- What evidence would resolve it: Systematic evaluation of GPT-4 on progressively more complex short-answer tasks, measuring performance decline and identifying complexity thresholds.

### Open Question 3
- Question: What is the source of variability in GPT-4's classifications when using the same inputs and temperature settings?
- Basis in paper: [explicit] The paper observed high but not perfect intrarater agreement (Cohen's kappa of 0.98) when rerunning the same prompts, despite using temperature zero and identical inputs.
- Why unresolved: The paper speculates about OpenAI's "mixture of experts" approach as a potential cause but does not provide definitive evidence.
- What evidence would resolve it: Technical investigation into OpenAI's routing mechanisms or controlled experiments varying model settings to isolate sources of variability.

## Limitations
- The primary dataset is not publicly available, preventing independent validation of the core results.
- The study focuses exclusively on Ghanaian students, limiting generalizability to other educational contexts and cultural settings.
- Claims about economic feasibility lack quantitative support, as no cost analysis or deployment feasibility studies were conducted.

## Confidence
- High Confidence: GPT-4's superior performance on this specific dataset (QWK 0.92) is well-supported by the empirical results presented.
- Medium Confidence: The generalizability of these results to other LMIC educational contexts is plausible but unproven.
- Low Confidence: Claims about economic feasibility and implementation barriers in LMICs lack quantitative support.

## Next Checks
1. **Cross-context Validation**: Replicate the study with short-answer datasets from different countries, educational systems, and subject areas to test generalizability beyond Ghanaian reading comprehension.
2. **Cost-Benefit Analysis**: Calculate the total cost of API usage for grading at scale across different student populations and compare with traditional human grading or alternative automated approaches.
3. **Temporal Consistency Testing**: Evaluate whether GPT-4 maintains consistent grading performance over time by testing on multiple versions of the same dataset or using temporal splits to detect potential performance drift.