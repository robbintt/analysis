---
ver: rpa2
title: Predictive Authoring for Brazilian Portuguese Augmentative and Alternative
  Communication
arxiv_id: '2308.09497'
source_url: https://arxiv.org/abs/2308.09497
tags:
- pictogram
- corpus
- language
- sentences
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using a BERT-like model for pictogram prediction
  in augmentative and alternative communication systems for Brazilian Portuguese.
  The authors fine-tune BERTimbau, a Portuguese version of BERT, using a synthetic
  corpus constructed from AAC practitioners' sentences and augmented with GPT-3.
---

# Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication

## Quick Facts
- arXiv ID: 2308.09497
- Source URL: https://arxiv.org/abs/2308.09497
- Reference count: 9
- Primary result: BERT-like models with text-based pictogram embeddings (captions or synonyms) outperform image-based embeddings for AAC prediction in Brazilian Portuguese

## Executive Summary
This paper addresses the challenge of predicting pictograms in augmentative and alternative communication (AAC) systems for Brazilian Portuguese speakers with complex communication needs. The authors propose fine-tuning a BERT-like model (BERTimbau) using a synthetic AAC corpus constructed from practitioner sentences and GPT-3 augmentation. They experiment with four different methods of representing pictograms as embeddings: captions, synonyms, definitions, and images. Results show that text-based embeddings significantly outperform image-based approaches, with synonyms yielding lower perplexity and captions achieving higher prediction accuracies. The work provides a practical solution for AAC systems while highlighting important considerations for vocabulary representation and model training.

## Method Summary
The method involves constructing a synthetic Brazilian Portuguese AAC corpus by collecting sentences from AAC practitioners and augmenting them with GPT-3 to reach 13,796 sentences. The authors fine-tune BERTimbau, a Portuguese BERT variant, by modifying its vocabulary and embedding layer to handle pictogram identifiers instead of subword tokens. Four different pictogram embedding approaches are tested: using captions, synonyms, definitions, or images from the ARASAAC dataset. The model is trained using Adam optimizer with learning rate 1e-5 and batch size 768, with 15% of tokens masked during training. Performance is evaluated using perplexity and top-n accuracy metrics on held-out test data.

## Key Results
- Text-based embeddings (captions and synonyms) significantly outperform image-based embeddings for pictogram prediction
- Synonym embeddings yield lower perplexity while caption embeddings achieve higher prediction accuracy
- Using captions or synonyms as pictogram embeddings allows the BERT model to learn context-specific associations that improve prediction accuracy
- Fine-tuning BERTimbau with an AAC-specific corpus improves its ability to predict the next pictogram in AAC contexts
- Word-level tokenization is necessary for pictogram prediction because each pictogram is a unique concept

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using synonyms or captions as pictogram embeddings allows the BERT model to learn context-specific associations that improve prediction accuracy.
- Mechanism: The model maps words to their vector embeddings, and when those embeddings come from semantically related synonyms or direct captions, it captures contextual similarities better than random or image-based vectors.
- Core assumption: Semantic similarity encoded in embeddings (e.g., word2vec, BERT outputs) correlates with pictogram relevance in AAC contexts.
- Evidence anchors:
  - [abstract] "using embeddings computed from the pictograms' caption, synonyms, or definitions have a similar performance. Using synonyms leads to lower perplexity, but using captions leads to the highest accuracies."
  - [section] "Using synonyms requires the preexistence of a database of synonyms. Using only captions can cause problems when the vocabulary has many pictograms for the same word."
- Break condition: If the synonym database is incomplete or biased, synonym-based embeddings degrade and underperform captions.

### Mechanism 2
- Claim: Fine-tuning BERTimbau with an AAC-specific corpus improves its ability to predict the next pictogram in AAC contexts.
- Mechanism: The transformer learns language patterns from AAC sentence examples, so its next-token predictions align with AAC communication needs.
- Core assumption: AAC sentences have distinctive syntactic and lexical patterns that differ from general text.
- Evidence anchors:
  - [abstract] "we constructed an AAC corpus for Brazilian Portuguese to use as a training corpus."
  - [section] "Our experiments use the list of pictograms for Brazilian Portuguese from the ARASAAC dataset."
- Break condition: If the corpus lacks sufficient diversity or is too small, the model overfits to narrow patterns and fails on unseen vocabulary.

### Mechanism 3
- Claim: Word-level tokenization (instead of subword tokenization) is necessary for pictogram prediction because each pictogram is a unique concept.
- Mechanism: Subword tokenization splits terms into parts, which can fragment pictogram identifiers; word-level tokens preserve the identity of each pictogram.
- Core assumption: Pictograms are atomic symbols in the AAC system, not decomposable into smaller meaningful units.
- Evidence anchors:
  - [section] "BERT and BERTimbau use a vocabulary based on WordPiece... This vocabulary does not apply to pictogram prediction, for the tokens in pictogram-based sentences must be unique identifiers that cannot be divided into sub-items."
  - [section] "This way, we use a word-level tokenizer, which splits words in a sentence by white spaces."
- Break condition: If a language contains many rare words that require subword handling, word-level tokenization may fail to represent them.

## Foundational Learning

- Concept: Language modeling and perplexity
  - Why needed here: To evaluate and compare the quality of different pictogram prediction models.
  - Quick check question: If a model assigns higher probabilities to test sentences, what happens to its perplexity?

- Concept: Word embeddings and semantic similarity
  - Why needed here: To represent pictograms as vectors that capture meaning for prediction.
  - Quick check question: Why might synonyms yield lower perplexity than captions in some cases?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: To adapt a pre-trained model efficiently for a specialized AAC task.
  - Quick check question: What are the risks of fine-tuning too long on a small corpus?

## Architecture Onboarding

- Component map: AAC corpus construction -> Tokenizer setup (word-level) -> Embedding selection (captions/synonyms/definitions/images) -> Fine-tuning loop -> Evaluation
- Critical path: Corpus construction → Tokenizer setup → Embedding selection → Fine-tuning → Evaluation
- Design tradeoffs:
  - Caption vs. synonym embeddings: Captions are simpler but may duplicate vectors; synonyms require extra data but improve generalization.
  - Embedding source: Image embeddings need more training data and yield worse results; text-based embeddings are more effective.
  - Corpus size vs. diversity: Larger corpora improve robustness but increase training time.
- Failure signatures:
  - High perplexity but low accuracy: Model overfits or corpus is too narrow.
  - Very low perplexity but poor accuracy: Embeddings poorly aligned with pictogram semantics.
  - Slow convergence: Inappropriate embedding space mismatch.
- First 3 experiments:
  1. Train with caption embeddings only and evaluate perplexity/accuracy.
  2. Train with synonym embeddings only and compare to captions.
  3. Train with definition embeddings and measure performance impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to represent pictograms for prediction using transformer-based models?
- Basis in paper: [explicit] The paper compares four different approaches: using captions, synonyms, definitions, and images to represent pictograms, and finds that captions and synonyms perform best.
- Why unresolved: The results show similar performance between captions and synonyms, suggesting the choice depends on vocabulary characteristics. Further research is needed to determine optimal representation for different scenarios.
- What evidence would resolve it: Controlled experiments comparing different representation methods on various vocabulary sizes and characteristics, measuring prediction accuracy and perplexity.

### Open Question 2
- Question: Can transformer-based models effectively predict pictograms in AAC systems?
- Basis in paper: [explicit] The paper proposes using BERTimbau, a Portuguese version of BERT, for pictogram prediction and evaluates its performance.
- Why unresolved: The models were not evaluated in a real-world AAC system by actual users, limiting the assessment of effectiveness and efficiency.
- What evidence would resolve it: User studies testing the models in real AAC systems with people with CCN, measuring communication rate, accuracy, and user satisfaction.

### Open Question 3
- Question: How can synthetic AAC corpora be generated to improve pictogram prediction models?
- Basis in paper: [explicit] The paper describes a method for constructing a synthetic AAC corpus using human-composed sentences and GPT-3, but acknowledges potential biases and limitations.
- Why unresolved: The corpus construction method depends on the input sentences and vocabulary, and generating diverse and representative corpora is challenging.
- What evidence would resolve it: Comparative studies evaluating the quality and coverage of synthetic corpora generated using different methods and input data, assessing their impact on model performance.

## Limitations

- The synthetic AAC corpus quality depends heavily on GPT-3 augmentation and may not fully capture real-world AAC communication patterns
- Only tested on ARASAAC dataset for Brazilian Portuguese, limiting generalizability to other languages and pictogram vocabularies
- Does not address edge cases like homonyms, context-dependent meanings, or rare words outside the training corpus
- Computational cost of fine-tuning with different embedding approaches was not discussed for practical deployment considerations

## Confidence

**High Confidence**: The experimental methodology and evaluation metrics (perplexity and top-n accuracy) are clearly specified and appropriately applied. The finding that text-based embeddings (captions and synonyms) outperform image embeddings is robust across multiple experiments.

**Medium Confidence**: The comparative performance between caption and synonym embeddings. While the paper shows synonym embeddings yield lower perplexity and captions achieve higher accuracy, the practical significance of this difference and its generalizability to other contexts remains uncertain.

**Low Confidence**: The claim that the corpus construction method using GPT-3 augmentation produces high-quality, representative AAC data. Without access to the specific prompts used or independent evaluation of corpus quality beyond human judgment, we cannot verify the corpus adequately represents real AAC communication patterns.

## Next Checks

1. **Corpus Quality Validation**: Conduct an independent evaluation of the synthetic AAC corpus by comparing its linguistic characteristics (word distributions, sentence structures, semantic diversity) against a verified corpus of real AAC communications from AAC users and practitioners.

2. **Cross-Dataset Generalization Test**: Evaluate the model's performance on a different pictogram dataset (e.g., Mulberry Symbols or SymbolStix) to determine if the caption vs. synonym embedding advantage holds across different vocabulary structures and semantic relationships.

3. **Real-World Deployment Assessment**: Implement the model in an actual AAC application and measure user interaction metrics (prediction acceptance rate, communication speed, user satisfaction) rather than relying solely on perplexity and accuracy metrics from controlled experiments.