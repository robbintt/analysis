---
ver: rpa2
title: 'A Comprehensive Review and a Taxonomy of Edge Machine Learning: Requirements,
  Paradigms, and Techniques'
arxiv_id: '2302.08571'
source_url: https://arxiv.org/abs/2302.08571
tags:
- learning
- data
- edge
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey and taxonomy of edge
  machine learning (Edge ML) techniques. The authors identify the requirements for
  Edge ML solutions, driven by the constraints of both edge computing and machine
  learning.
---

# A Comprehensive Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques

## Quick Facts
- arXiv ID: 2302.08571
- Source URL: https://arxiv.org/abs/2302.08571
- Reference count: 40
- One-line primary result: Systematic survey of Edge ML techniques and their ability to meet joint edge computing and AI requirements

## Executive Summary
This paper presents a comprehensive survey and taxonomy of edge machine learning (Edge ML) techniques, identifying the requirements driven by the constraints of both edge computing and machine learning. The authors survey more than twenty paradigms and techniques covering edge inference and edge learning, analyzing how each technique fits into Edge ML by meeting a subset of the identified requirements. The work provides valuable insights into the challenges and opportunities in this field, offering a systematic review of Edge ML techniques and summarizing frameworks and open issues.

## Method Summary
The authors first identify Edge ML requirements driven by joint constraints from edge computing and AI, including low task latency, high performance, generalization and adaptation, labeled data independence, enhanced privacy and security, computational efficiency, optimized bandwidth, offline capability, energy efficiency, and cost optimization. They then survey and review more than twenty paradigms and techniques covering two main parts: edge inference (model compression, distributed inference, early exit) and edge learning (distributed learning, transfer learning, meta-learning, self-supervised learning). For each technique, they analyze how it meets the identified requirements using a notation system (+, -, *, /) to indicate positive impact, negative impact, conditional impact, or neutral impact.

## Key Results
- Identified eight key requirements for Edge ML solutions driven by edge computing and AI constraints
- Surveyed more than twenty Edge ML paradigms and techniques across edge inference and edge learning
- Analyzed how each technique addresses specific Edge ML requirements with systematic notation
- Identified open issues and future directions including theoretical foundations, hybrid approaches, and framework extensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization improves task latency by reducing precision and arithmetic complexity.
- Mechanism: Converting FP32/FP64 weights to lower precision (FP16, BFP16, fixed-point, binary) reduces the number of arithmetic operations and memory bandwidth needed for inference.
- Core assumption: The reduced precision still allows the model to maintain acceptable accuracy for the task.
- Evidence anchors:
  - [abstract] identifies low task latency as a key requirement for Edge ML.
  - [section] discusses post-training quantization and quantization-aware training methods, with empirical results showing 4x model size reduction and up to 50% inference latency reduction for MobileNet.
  - [corpus] includes a paper on in-memory computing hardware for quantized neural networks, indicating ongoing research in this area.
- Break condition: Accuracy degradation becomes unacceptable for the target application.

### Mechanism 2
- Claim: Federated Learning preserves privacy by keeping data local while enabling collaborative model training.
- Mechanism: Edge devices train local models on their private data and share only model updates (gradients or parameters) with a central server or peer devices, which aggregates these updates to construct a global model.
- Core assumption: The model updates do not leak sensitive information about the local data.
- Evidence anchors:
  - [abstract] highlights enhanced privacy and security as a key requirement for Edge ML.
  - [section] describes how FL enables collaborative learning without sharing raw data, citing works on privacy-preserving learning in healthcare and secure aggregation methods.
  - [corpus] includes a survey on topology-aware federated learning, indicating active research in this area.
- Break condition: Model updates can be reverse-engineered to infer sensitive information from the local data.

### Mechanism 3
- Claim: Transfer Learning accelerates training on edge devices by leveraging pre-trained models from related domains.
- Mechanism: A pre-trained model from a source domain is adapted to a target domain by either freezing early layers (layer freezing) or fine-tuning the entire model (model tuning), reducing the need for large labeled datasets and extensive training on the target domain.
- Core assumption: The source and target domains are sufficiently related for knowledge transfer to be effective.
- Evidence anchors:
  - [abstract] identifies labeled data independence as a key requirement for Edge ML.
  - [section] discusses transfer learning techniques for different domain relationships (data distribution, feature space, task space) and cites applications in healthcare and cross-lingual speech recognition.
  - [corpus] includes a survey on transfer learning, indicating its relevance to Edge ML.
- Break condition: The source and target domains are too dissimilar, leading to negative transfer and degraded performance.

## Foundational Learning

- Concept: Understanding the constraints of edge computing (limited computation, memory, energy, bandwidth, and unreliable network connections).
  - Why needed here: Edge ML solutions must be designed to operate within these constraints while delivering acceptable performance.
  - Quick check question: What are the key differences between edge computing and cloud computing that impact ML model design?

- Concept: Familiarity with machine learning paradigms (supervised, unsupervised, reinforcement learning) and their applicability to edge scenarios.
  - Why needed here: Edge ML leverages various learning paradigms to address different edge requirements, such as privacy preservation, offline capability, and adaptation to dynamic environments.
  - Quick check question: How do the different learning paradigms address the unique challenges of edge computing?

- Concept: Knowledge of model compression and approximation techniques (quantization, pruning, knowledge distillation) and their trade-offs between accuracy and efficiency.
  - Why needed here: These techniques are essential for deploying large ML models on resource-constrained edge devices while maintaining acceptable performance.
  - Quick check question: What are the key considerations when choosing a model compression technique for a specific edge application?

## Architecture Onboarding

- Component map: Edge ML solution architecture typically includes data preprocessing, model training/inference, and communication components. Data preprocessing may involve dimensionality reduction or feature extraction. Model training can be done locally, federated, or through transfer/meta-learning. Communication involves model updates or inference results exchange between edge devices and servers.
- Critical path: The critical path depends on the specific Edge ML application but often involves data collection, preprocessing, model inference, and result communication. Minimizing latency in this path is crucial for real-time edge applications.
- Design tradeoffs: Key tradeoffs include model accuracy vs. resource efficiency, privacy vs. performance, and communication cost vs. computation load. Balancing these tradeoffs is essential for effective Edge ML solutions.
- Failure signatures: Common failures include model accuracy degradation due to quantization or pruning, privacy breaches due to model inversion attacks, and system failures due to resource exhaustion or network disruptions.
- First 3 experiments:
  1. Implement a simple quantization technique (e.g., 8-bit quantization) on a pre-trained image classification model and measure the impact on accuracy and inference latency on an edge device.
  2. Set up a federated learning scenario with simulated edge devices and evaluate the impact of data heterogeneity and device availability on model convergence and performance.
  3. Apply transfer learning to adapt a pre-trained language model to a specific domain (e.g., medical text) and measure the reduction in training data and time compared to training from scratch.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Edge ML frameworks be extended to support full MLOps lifecycle management while maintaining resource efficiency?
- Basis in paper: [explicit] The paper identifies that existing Edge ML frameworks are lightweight for resource efficiency but limited in supporting ML features and functions, particularly for edge learning phases requiring model conversion and retraining.
- Why unresolved: There is a trade-off between resource efficiency and functionality that needs to be balanced, and current frameworks prioritize the former at the expense of the latter.
- What evidence would resolve it: Development of a framework architecture that demonstrates both comprehensive MLOps support and competitive resource efficiency metrics compared to existing lightweight frameworks.

### Open Question 2
- Question: What are the theoretical foundations for optimal compression ratios in model compression techniques for Edge ML?
- Basis in paper: [explicit] The paper notes that most model compression methods lack mathematical proofs for optimal compression ratios, and federated learning may not converge if data distribution varies significantly across clients.
- Why unresolved: The rapid emergence of Edge ML techniques has outpaced the development of theoretical foundations needed to validate empirical conclusions and provide design guidelines.
- What evidence would resolve it: Mathematical proofs or rigorous theoretical analysis establishing optimal compression ratios for different model types and federated learning convergence conditions under varying data distributions.

### Open Question 3
- Question: How can hybrid Edge ML approaches be designed to comprehensively cover all phases (data preprocessing, learning, inference) while maintaining performance?
- Basis in paper: [explicit] The paper observes that while hybrid strategies combining multiple techniques are commonly adopted, complete hybrid approaches covering all Edge ML phases are missing from the literature.
- Why unresolved: The complexity of integrating multiple techniques across different phases while optimizing for various requirements (latency, privacy, accuracy) presents significant design challenges.
- What evidence would resolve it: A complete hybrid Edge ML solution architecture demonstrating improved performance across multiple requirements compared to single-technique approaches, with empirical validation on relevant benchmarks.

## Limitations
- The survey's comprehensiveness is limited by the selection criteria for the more than twenty techniques reviewed, which are not fully specified.
- The analysis of how each technique addresses Edge ML requirements relies on the authors' interpretation of published works, which may vary in depth and rigor.
- The rapidly evolving nature of both edge computing and machine learning means some techniques may become outdated or new ones may emerge after the survey's completion.

## Confidence
- High Confidence: The identification of key Edge ML requirements (low latency, privacy, efficiency) is well-established and widely accepted in the field.
- Medium Confidence: The categorization of techniques into edge inference and edge learning is logical and reflects the current research landscape, but the specific techniques covered may not be exhaustive.
- Medium Confidence: The analysis of how each technique addresses Edge ML requirements provides valuable insights but may be subject to interpretation and limited by the available literature.

## Next Checks
1. Verify the accuracy and completeness of the technique descriptions and their corresponding requirement analyses by cross-referencing with the original publications.
2. Assess the survey's coverage by comparing the listed techniques against recent edge ML literature to identify any significant omissions or outdated techniques.
3. Evaluate the practical applicability of the identified techniques by implementing a subset of them on representative edge devices and measuring their performance against the stated requirements.