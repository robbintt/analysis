---
ver: rpa2
title: 'Simple is Better and Large is Not Enough: Towards Ensembling of Foundational
  Language Models'
arxiv_id: '2308.12272'
source_url: https://arxiv.org/abs/2308.12272
tags:
- flms
- bert
- ensemble
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the potential of ensemble methods to improve
  the performance of foundational language models (FLMs) like BERT on natural language
  processing tasks. The authors propose three ensemble strategies: Shallow-Ensemble,
  Semi-Ensemble, and Deep-Ensemble.'
---

# Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models

## Quick Facts
- arXiv ID: 2308.12272
- Source URL: https://arxiv.org/abs/2308.12272
- Reference count: 6
- This paper investigates ensemble methods to improve foundational language models' performance on NLP tasks, proposing Shallow-Ensemble, Semi-Ensemble, and Deep-Ensemble approaches.

## Executive Summary
This paper explores ensemble methods to enhance the performance of foundational language models (FLMs) like BERT on natural language processing tasks. The authors propose three ensemble strategies: Shallow-Ensemble (weighted averaging of probability outputs), Semi-Ensemble (concatenated embeddings with smaller model), and Deep-Ensemble (knowledge-guided reinforcement learning). Experiments on benchmark and real-world datasets demonstrate that ensembles significantly outperform individual FLMs, with Deep-Ensemble achieving the best overall results by incorporating knowledge graphs and reinforcement learning.

## Method Summary
The paper proposes three ensemble strategies to improve FLM performance. Shallow-Ensemble combines probability outputs from multiple FLMs using weighted averaging. Semi-Ensemble creates ensemble embeddings by concatenating individual FLM embeddings and training a smaller model on this combined representation. Deep-Ensemble incorporates knowledge graphs like Wikipedia and CommonSense into the ensemble process using a reinforcement learning approach that tunes the classifier loss based on KG-similarity rewards. The methods are evaluated on benchmark datasets (GoEmotions, SNLI) and real-world datasets (PRIMATE, Twitter COVID-19).

## Key Results
- Ensembles significantly outperform individual FLMs across all tested datasets
- Deep-Ensemble achieves the best overall performance by incorporating knowledge graphs
- All ensemble methods show robustness to issues like hallucination and predictive uncertainty
- The approaches are suitable for deployment on resource-constrained devices

## Why This Works (Mechanism)

### Mechanism 1
Ensemble methods reduce predictive uncertainty and improve robustness by aggregating diverse model predictions. Multiple FLMs with different attention patterns and embeddings are combined using weighted averaging (Shallow-Ensemble), embedding concatenation with a smaller model (Semi-Ensemble), or knowledge-guided reinforcement learning (Deep-Ensemble). The aggregation leverages complementary strengths and mitigates individual model weaknesses. Core assumption: Different FLMs capture distinct aspects of the input data, and their combination yields better overall performance than any single model.

### Mechanism 2
Knowledge graphs improve ensemble predictions by providing external semantic grounding. Deep-Ensemble uses Wikipedia and CommonSense knowledge graph embeddings to evaluate the similarity between the ensemble embedding and KG-based embeddings. A reward function based on this similarity guides a reinforcement learning policy to tune the classifier loss. Core assumption: Incorporating human-curated knowledge improves model predictions beyond learned representations alone.

### Mechanism 3
Attention interpretability helps diagnose and improve ensemble behavior. The paper uses an attention visualization tool to compare how different models attend to input tokens, revealing complementary focus areas that justify ensemble combination. Core assumption: Attention weights correlate with model reasoning and can expose complementary strengths.

## Foundational Learning

- Concept: Ensemble methods in machine learning
  - Why needed here: The paper builds on classical ensemble theory to improve FLMs, so understanding bagging, boosting, and stacking is essential to grasp the novelty.
  - Quick check question: What is the difference between averaging probabilities vs. averaging embeddings in ensemble learning?

- Concept: Attention mechanisms in Transformers
  - Why needed here: The ensemble performance is partly explained by complementary attention patterns, so knowing how self-attention works is critical.
  - Quick check question: How does multi-head attention allow a model to focus on different parts of the input simultaneously?

- Concept: Reinforcement learning with policy gradients
  - Why needed here: Deep-Ensemble uses RL to tune the classifier using knowledge graph rewards, so understanding policy gradient methods is necessary.
  - Quick check question: In policy gradient RL, what role does the reward signal play in updating the policy?

## Architecture Onboarding

- Component map: Text datasets -> BERT variants (tiny, mini, base, large) -> Ensemble strategies (Shallow, Semi, Deep) -> Knowledge graphs (for Deep-Ensemble) -> Predicted class labels
- Critical path: 1) Load and preprocess datasets (GoEmotions, PRIMATE, SNLI, Twitter) 2) Run each BERT variant to get predictions/embeddings 3) Combine via chosen ensemble strategy 4) Evaluate accuracy and compare to single models
- Design tradeoffs: Shallow vs. Semi: Simplicity and speed vs. richer representation but higher computational cost; Deep-Ensemble: Best performance but requires KG access and RL tuning; more complex deployment
- Failure signatures: All ensemble members agree on wrong predictions (low diversity); RL reward function misaligned with task goals; KG embeddings poorly matched to input domain
- First 3 experiments: 1) Run Shallow-Ensemble on GoEmotions with equal weights; verify improvement over single BERT 2) Implement Semi-Ensemble on PRIMATE; compare embedding dimensionality reduction impact 3) Train Deep-Ensemble on SNLI with KG rewards; check if RL tuning improves over baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the Deep-Ensemble's performance vary with different sizes and types of knowledge graphs beyond Wikipedia and CommonSense? The paper uses Wikipedia and CommonSense knowledge graphs in the Deep-Ensemble but does not explore the impact of using different sizes or types of knowledge graphs on the Deep-Ensemble's performance.

### Open Question 2
Can the ensemble methods proposed be extended to other foundational language models like XLNet and T5, and how would their performance compare? The paper mentions XLNet and T5 as larger FLMs but focuses on BERT and its variants for the ensemble methods.

### Open Question 3
How do the ensemble methods perform in zero-shot or few-shot learning scenarios? The paper focuses on benchmark and real-world datasets for classification tasks, implying that the models are trained on sufficient data, but does not explore performance in scenarios with limited training data.

## Limitations
- The Deep-Ensemble RL component lacks full algorithmic detail, particularly around knowledge graph embedding integration and reward function design
- The paper does not report statistical significance testing or confidence intervals for performance differences between ensemble methods
- Knowledge graph quality and domain relevance are critical but not systematically evaluated across different datasets

## Confidence
- High: Ensemble methods improve FLM performance over individual models (supported by multiple dataset experiments)
- Medium: Knowledge graphs enhance ensemble performance through semantic grounding (mechanism plausible but KG integration details unclear)
- Medium: Attention interpretability reveals complementary model behaviors (qualitative support only, no quantitative validation)

## Next Checks
1. Replicate Shallow-Ensemble on GoEmotions with equal and learned weights; measure performance variance across multiple runs to establish statistical significance
2. Implement Semi-Ensemble on PRIMATE; systematically vary embedding dimensionality and model size to identify optimal configuration
3. For Deep-Ensemble, create a synthetic knowledge graph task where ground truth similarity is known; verify that the RL reward function correctly guides model behavior toward improved predictions