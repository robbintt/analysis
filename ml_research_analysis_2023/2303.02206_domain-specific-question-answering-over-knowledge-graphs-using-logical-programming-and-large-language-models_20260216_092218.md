---
ver: rpa2
title: Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming
  and Large Language Models
arxiv_id: '2303.02206'
source_url: https://arxiv.org/abs/2303.02206
tags:
- question
- knowledge
- logical
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to domain-specific question
  answering over knowledge graphs by integrating logical programming languages with
  large language models (LLMs). The key idea is to represent natural language questions
  as Prolog queries, which are then executed over a Prolog-based knowledge base to
  derive exact answers.
---

# Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models

## Quick Facts
- arXiv ID: 2303.02206
- Source URL: https://arxiv.org/abs/2303.02206
- Reference count: 3
- This paper presents a novel approach to domain-specific question answering over knowledge graphs by integrating logical programming languages with large language models (LLMs).

## Executive Summary
This paper introduces a novel approach to domain-specific question answering over knowledge graphs by integrating logical programming languages with large language models (LLMs). The key idea is to represent natural language questions as Prolog queries, which are then executed over a Prolog-based knowledge base to derive exact answers. The method involves fine-tuning a T5-small sequence-to-sequence transformer to translate questions into Prolog queries, leveraging the MetaQA dataset for evaluation. The approach demonstrates high accuracy, achieving 100% correct answers on all test questions with only 1000 annotated training samples, outperforming prior methods that require larger datasets. The results highlight the effectiveness of combining logical reasoning with LLMs for explainable and robust question answering over domain-specific graphs.

## Method Summary
The method involves fine-tuning a T5-small sequence-to-sequence transformer to translate natural language questions into Prolog queries, which are then executed over a Prolog-based knowledge base to derive exact answers. The approach leverages the MetaQA dataset for evaluation, with a focus on multi-hop reasoning. The model is trained on a small fraction of annotated data, demonstrating high accuracy even with limited training samples. The method separates knowledge representation and inference from question translation, allowing each component to specialize and improving overall efficiency.

## Key Results
- Achieves 100% accuracy on all test questions in the MetaQA dataset
- Requires only 1000 annotated training samples, outperforming prior methods that need larger datasets
- Demonstrates high accuracy in identifying correct answer entities for multi-hop questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The approach works because it transforms natural language questions into logical Prolog queries, which can be executed precisely over a knowledge graph to retrieve exact answers.
- Mechanism: Natural language questions are mapped to structured Prolog predicates that represent traversal paths in the knowledge graph. The T5-small model learns to generate these queries from question text, and Prolog executes them to retrieve all matching entities.
- Core assumption: The logical structure of questions can be captured by a sequence-to-sequence model and faithfully represented as Prolog predicates that cover all valid inference paths.
- Evidence anchors:
  - [abstract] "By representing the questions as Prolog queries, which are readable and near close to natural language in representation, we facilitate the generation of programmatically derived answers."
  - [section] "Our goal is to extract the representation of the question in the form of a Prolog query, which can then be used to answer the query programmatically."
  - [corpus] Weak evidence; related papers focus on GraphRAG and multimodal retrieval, not logical programming integration.
- Break condition: If the question structure cannot be expressed as a finite set of Prolog predicates (e.g., highly contextual or ambiguous queries), the model will fail to generate correct queries.

### Mechanism 2
- Claim: The approach achieves high accuracy with minimal training data because the Prolog-based reasoning engine handles the knowledge storage and inference, while the LLM only learns to translate questions into logical form.
- Mechanism: By offloading knowledge representation and multi-hop reasoning to Prolog, the LLM training task is simplified to a translation problem. This reduces the amount of data needed to learn the mapping from questions to queries.
- Core assumption: Separating question representation from knowledge storage allows each component to specialize, improving overall efficiency and reducing data requirements.
- Evidence anchors:
  - [abstract] "Our experimental results demonstrate that our method achieves accurate identification of correct answer entities for all test questions, even when trained on a small fraction of annotated data."
  - [section] "We show that the model can answer all questions in the test dataset with 100% accuracy with less than 1% of the training data as compared to prior approaches which use full training dataset or a significant portion of it."
  - [corpus] No direct evidence; related work focuses on embedding-based or retrieval-based methods, not symbolic reasoning integration.
- Break condition: If the knowledge graph is too large or complex for Prolog to handle efficiently, or if the LLM fails to learn the translation accurately, the approach will degrade.

### Mechanism 3
- Claim: The use of reverse relations in Prolog enables the model to handle multi-hop questions by constructing bidirectional traversal paths in the knowledge graph.
- Mechanism: For each relation in the knowledge graph, a reverse predicate is created (e.g., written_by_reverse). This allows the Prolog engine to traverse edges in both directions, enabling multi-hop reasoning without additional graph preprocessing.
- Core assumption: All relevant multi-hop paths can be expressed using the original and reverse predicates without loss of information or ambiguity.
- Evidence anchors:
  - [section] "In order to capture reverse relations, for each of the 9 relations in MetaQA dataset we also create the reverse relation resulting in 18 total relations."
  - [abstract] "By representing the questions as Prolog queries... we facilitate the generation of programmatically derived answers."
  - [corpus] No evidence; related work does not discuss reverse relation construction in Prolog.
- Break condition: If the knowledge graph contains cycles or requires more complex inference than simple path traversal, reverse predicates may not suffice.

## Foundational Learning

- Concept: Prolog and first-order logic
  - Why needed here: The approach relies on Prolog as the reasoning engine to execute queries and retrieve answers from the knowledge graph.
  - Quick check question: Can you write a Prolog rule to represent the relation "movie directed by director"?
- Concept: Sequence-to-sequence modeling with transformers
  - Why needed here: The T5-small model is fine-tuned to translate natural language questions into Prolog queries.
  - Quick check question: What is the input-output format when fine-tuning T5 for a translation task?
- Concept: Multi-hop reasoning in knowledge graphs
  - Why needed here: The MetaQA dataset contains questions requiring traversal of multiple edges in the knowledge graph to reach the answer.
  - Quick check question: How would you represent a 3-hop path in Prolog using variables?

## Architecture Onboarding

- Component map:
  - T5-small sequence-to-sequence transformer: Translates natural language questions to Prolog queries.
  - Prolog knowledge base: Stores the knowledge graph as predicates and executes queries.
  - Annotation pipeline: Converts MetaQA inference paths to Prolog predicates for training data.
- Critical path: Question → T5 translation → Prolog query → Knowledge graph traversal → Answer retrieval.
- Design tradeoffs:
  - Using Prolog for reasoning provides exact answers but may not scale to very large graphs.
  - Fine-tuning a small T5 model reduces data requirements but may limit generalization to out-of-domain questions.
  - Reverse predicates simplify multi-hop reasoning but double the number of relations.
- Failure signatures:
  - Incorrect Prolog query generation (e.g., wrong predicate or variable assignment).
  - Missing or incorrect reverse predicates in the knowledge base.
  - Out-of-vocabulary entities or relations not covered by the training data.
- First 3 experiments:
  1. Test the T5 model on a held-out set of annotated questions to verify query generation accuracy.
  2. Execute generated queries on a small Prolog knowledge base to confirm answer retrieval.
  3. Measure hit@1 and exact match scores on the MetaQA test set to evaluate overall performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed approach compare to other methods when trained on the full MetaQA training dataset?
- Basis in paper: [inferred] The paper mentions that the proposed method achieves 100% accuracy on all test questions with only 1000 annotated training samples, outperforming prior methods that require larger datasets. However, it does not explicitly compare the performance of the proposed approach when trained on the full MetaQA training dataset.
- Why unresolved: The paper does not provide a direct comparison between the proposed approach and other methods when trained on the full MetaQA training dataset.
- What evidence would resolve it: A comparison of the proposed approach's performance when trained on the full MetaQA training dataset with other methods' performance under the same conditions would resolve this question.

### Open Question 2
- Question: How does the proposed approach perform on other domain-specific knowledge graphs beyond the MetaQA dataset?
- Basis in paper: [explicit] The paper focuses on the MetaQA dataset for evaluation, but it mentions that the approach is intended for domain-specific question answering over knowledge graphs. However, it does not provide any results or analysis on other domain-specific knowledge graphs.
- Why unresolved: The paper only evaluates the proposed approach on the MetaQA dataset, so its performance on other domain-specific knowledge graphs remains unknown.
- What evidence would resolve it: Evaluating the proposed approach on other domain-specific knowledge graphs and comparing its performance with other methods would resolve this question.

### Open Question 3
- Question: How does the proposed approach handle complex questions that require more than three hops in the knowledge graph?
- Basis in paper: [explicit] The paper mentions that the MetaQA dataset contains questions with answers that are 1, 2, or 3 hops away from the question entity in the knowledge graph. However, it does not discuss the approach's performance on questions that require more than three hops.
- Why unresolved: The paper does not provide any information or analysis on the proposed approach's ability to handle questions that require more than three hops in the knowledge graph.
- What evidence would resolve it: Testing the proposed approach on questions that require more than three hops in the knowledge graph and comparing its performance with other methods would resolve this question.

## Limitations

- Scalability concerns: The Prolog-based reasoning engine may not scale efficiently to larger, real-world knowledge graphs with more complex relational patterns.
- Limited evaluation scope: The approach is only evaluated on the MetaQA dataset, which has limited complexity and may not represent real-world domains.
- Potential overfitting: The 100% accuracy on MetaQA may indicate overfitting to the dataset's specific structure and may not generalize to more diverse or noisy question types.

## Confidence

- High Confidence: The core mechanism of translating questions to Prolog queries is technically sound and well-demonstrated in the results.
- Medium Confidence: The data efficiency claims are supported by the experimental results but lack comparison to diverse benchmarks.
- Low Confidence: The scalability and robustness claims to real-world domains remain largely theoretical without empirical validation.

## Next Checks

1. Test the approach on a more complex, real-world knowledge graph (e.g., biomedical or financial domain) with 50+ relations and evaluate performance degradation patterns.
2. Conduct an ablation study removing the reverse relation mechanism to quantify its contribution to multi-hop reasoning accuracy.
3. Evaluate model performance on questions requiring negation, disjunction, or aggregation (beyond simple path traversal) to assess the limits of the Prolog representation.