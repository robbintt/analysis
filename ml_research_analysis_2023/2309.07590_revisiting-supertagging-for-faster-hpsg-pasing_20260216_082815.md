---
ver: rpa2
title: Revisiting Supertagging for Faster HPSG Pasing
arxiv_id: '2309.07590'
source_url: https://arxiv.org/abs/2309.07590
tags:
- hpsg
- data
- training
- glove840
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce a new English supertagger for Head-Driven Phrase Structure
  Grammar (HPSG) parsing, trained on a gold-standard treebank. We use Support Vector
  Machines (SVM), neural Conditional Random Fields (CRF), and fine-tuned BERT models,
  and compare them to a Maximum Entropy baseline.
---

# Revisiting Supertagging for Faster HPSG Pasing

## Quick Facts
- arXiv ID: 2309.07590
- Source URL: https://arxiv.org/abs/2309.07590
- Reference count: 25
- Primary result: BERT-based supertagger achieves 97.26% accuracy on WSJ23, 93.88% on out-of-domain data

## Executive Summary
This paper introduces a new English supertagger for Head-Driven Phrase Structure Grammar (HPSG) parsing, trained on gold-standard treebanks. The authors compare Support Vector Machines (SVM), neural Conditional Random Fields (CRF), and fine-tuned BERT models against a Maximum Entropy baseline. Their best BERT-based model achieves 97.26% accuracy on the WSJ23 test set and 93.88% on out-of-domain data (The Cathedral and the Bazaar). The supertaggers significantly improve parsing speed by a factor of three while also increasing parser accuracy, demonstrating the potential of integrating high-accuracy supertaggers into HPSG parsers.

## Method Summary
The authors train multiple supertagging models on gold-standard HPSG treebanks from the English Resource Grammar (ERG) 2020 release. They implement SVM and NCRF++ models using word orthography, POS tags, and context words as features, following Dridan 2009's feature extraction approach. The BERT model is fine-tuned on the training data with standard hyperparameters. All models are evaluated on WSJ23 (950 sentences) and out-of-domain datasets including Wikipedia, technical essays, travel brochures, and transcribed conversations. The best model is then integrated into an HPSG parser to measure speed and accuracy improvements.

## Key Results
- BERT-based supertagger achieves 97.26% accuracy on WSJ23 test set
- Out-of-domain accuracy of 93.88% on The Cathedral and the Bazaar
- Three-fold improvement in parsing speed when integrated with HPSG parser
- All neural models outperform the Maximum Entropy baseline on all test datasets

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning BERT on HPSG lexical types yields the highest tagging accuracy due to its contextualized embeddings capturing nuanced syntactic behavior. BERT's pre-training on large corpora provides rich, contextualized representations that, when fine-tuned, adapt to the specific domain of HPSG lexical types, improving discrimination between fine-grained categories. Core assumption: Fine-grained lexical type distinctions require deep contextual understanding beyond traditional models. Evidence: 97.26% WSJ23 accuracy and 93.88% out-of-domain accuracy. Break condition: Large domain shift between BERT's pre-training data and HPSG lexical types may limit accuracy gains.

### Mechanism 2
Using diverse and challenging test datasets improves generalizability and robustness. Evaluating on datasets beyond WSJ23, such as technical essays and transcribed conversations, exposes models to a wider range of linguistic phenomena, forcing them to learn more robust representations. Core assumption: Models trained on narrow domains may overfit and fail to generalize to real-world text. Evidence: Test data includes news, Wikipedia, fiction, travel brochures, technical essays, customer service emails, and phone conversations. Break condition: Test dataset diversity may not represent target application domain.

### Mechanism 3
Integrating high-accuracy supertaggers into HPSG parsers significantly improves parsing speed and accuracy by reducing the search space. By assigning the most likely lexical type to each token before parsing, supertaggers eliminate unlikely interpretations, reducing the number of possible parse trees the parser needs to consider. Core assumption: Search space reduction from supertagging outweighs occasional incorrect lexical type assignments. Evidence: Three-fold parsing speed improvement while increasing parser accuracy. Break condition: If supertagger accuracy is too low, incorrect assignments may harm parsing more than help.

## Foundational Learning

- Concept: Head-Driven Phrase Structure Grammar (HPSG)
  - Why needed here: Understanding the underlying linguistic theory is crucial for appreciating the importance of fine-grained lexical types and HPSG parsing challenges.
  - Quick check question: What are the key components of an HPSG grammar, and how do they differ from traditional phrase structure grammars?

- Concept: Supertagging
  - Why needed here: Supertagging is the core technique used to improve HPSG parsing efficiency. Understanding how it works and its limitations is essential for interpreting results.
  - Quick check question: How does supertagging differ from traditional part-of-speech tagging, and what are the benefits and drawbacks of using it in HPSG parsing?

- Concept: Neural sequence labeling models
  - Why needed here: The experimental models use neural architectures for supertagging. Familiarity with these models and their training procedures is necessary for understanding methodology and results.
  - Quick check question: What are the key components of a neural sequence labeling model, and how do they contribute to its ability to perform supertagging?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature extraction -> Train baseline models -> Train experimental models -> Evaluate on test sets -> Integrate best model into parser
- Critical path: Preprocess data → Train baseline models → Train experimental models → Evaluate on test sets → Integrate best model into parser
- Design tradeoffs:
  - Accuracy vs. speed: Neural models are more accurate but slower than SVM
  - Complexity vs. interpretability: BERT is highly accurate but less interpretable than simpler models
  - Data requirements: Neural models require more training data than traditional models
- Failure signatures:
  - Low accuracy on test sets: Indicates overfitting or insufficient model capacity
  - Slow parsing speed: Suggests supertagger is not effectively reducing search space
  - Decreased parsing accuracy: Implies supertagger is assigning incorrect lexical types
- First 3 experiments:
  1. Train and evaluate MaxEnt baseline on WSJ23 test set
  2. Train and evaluate SVM model on WSJ23 test set
  3. Fine-tune BERT on training data and evaluate on WSJ23 test set

## Open Questions the Paper Calls Out

### Open Question 1
How would the BERT-based supertagger perform on a larger, more diverse set of out-of-domain test data beyond The Cathedral and the Bazaar? Basis: Paper only evaluates BERT model on one out-of-domain dataset, limiting generalizability. Evidence needed: Testing on wider range of out-of-domain datasets from different domains and genres.

### Open Question 2
What is the impact of integrating the new supertaggers into a modern HPSG parser on overall parsing accuracy and speed? Basis: Paper mentions three-fold speedup and accuracy improvements but lacks detailed results. Evidence needed: Detailed experimental results comparing parsing accuracy and speed with and without supertaggers.

### Open Question 3
How does the performance of SVM and neural CRF-based supertaggers compare to the BERT-based model on various test datasets? Basis: Paper reports SVM and neural models outperform baseline but lacks direct comparison between SVM, neural CRF, and BERT models. Evidence needed: Comprehensive comparison of accuracy and speed across different test datasets.

## Limitations

- Gold-standard POS tags and supertags during training inflate performance compared to realistic scenarios with automatic POS tagging
- Significant performance drop (93.88%) on out-of-domain data suggests limited generalization beyond news text
- Computational cost of fine-tuning BERT remains unspecified, making it unclear if three-fold improvement accounts for supertagger inference time

## Confidence

**High confidence**: WSJ23 results (97.26%) are well-supported with clear training procedures and evaluation protocols. Three-fold parsing speed improvement is directly supported by abstract statement.

**Medium confidence**: Out-of-domain generalization shows performance degradation but lacks comparison to parsers without supertagging on same data. Claim about diverse datasets improving robustness is supported by genre list but not empirically validated.

**Low confidence**: Mechanism by which BERT's contextualized embeddings specifically benefit HPSG lexical type discrimination is asserted but not experimentally isolated from other factors.

## Next Checks

1. **Error analysis on POS tagging cascades**: Evaluate supertagger's accuracy when fed automatic POS tags instead of gold-standard tags to measure real-world performance degradation.

2. **Cost-benefit analysis**: Measure total end-to-end parsing time including supertagger inference to verify three-fold improvement holds when accounting for additional computational overhead.

3. **Generalization benchmark**: Compare supertagger's out-of-domain performance against parser using no supertagging on same Cathedral and the Bazaar data to isolate actual contribution of supertagging to parsing accuracy in out-of-domain scenarios.