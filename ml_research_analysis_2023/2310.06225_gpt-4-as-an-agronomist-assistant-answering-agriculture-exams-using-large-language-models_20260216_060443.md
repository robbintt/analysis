---
ver: rpa2
title: GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language
  Models
arxiv_id: '2310.06225'
source_url: https://arxiv.org/abs/2310.06225
tags:
- questions
- gpt-4
- agriculture
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the capabilities of large language models
  (LLMs), including GPT-4, Llama 2, and GPT-3.5, in answering agriculture-related
  questions from exams in Brazil, India, and the USA. The study explores the impact
  of retrieval techniques (RAG) and ensemble refinement (ER) on LLM performance.
---

# GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models

## Quick Facts
- arXiv ID: 2310.06225
- Source URL: https://arxiv.org/abs/2310.06225
- Reference count: 8
- Primary result: GPT-4 achieved 93% accuracy on agronomist certification exams, outperforming GPT-3.5 (88%) and human subjects on Indian graduate program exams.

## Executive Summary
This paper evaluates large language models' capabilities in answering agriculture-related exam questions from Brazil, India, and the USA. The study tests GPT-4, Llama 2, and GPT-3.5 with and without retrieval-augmented generation (RAG) and ensemble refinement (ER) techniques. GPT-4 demonstrated exceptional performance, achieving a passing score of 93% on agronomist certification exams and the highest score among human subjects on an Indian graduate program exam. The results suggest that GPT-4, combined with ER and RAG techniques, can provide meaningful insights for agricultural education, assessment, and crop management.

## Method Summary
The evaluation employed a dual-agent system where one LLM answered questions and another evaluated responses. Three agricultural exam datasets were used: Certified Crop Adviser (CCA) questions, Embrapa questions from Brazil, and AgriExams questions from India. The study tested multiple models with various prompting strategies, both with and without RAG (retrieval-augmented generation) and ER (ensemble refinement) techniques. The ER technique used 11 first-stage and 33 second-stage samples with different temperatures to extract diverse domain-specific insights.

## Key Results
- GPT-4 achieved 93% accuracy on agronomist certification exams, outperforming all other models
- GPT-4 scored higher than human subjects on Indian graduate program exams
- RAG and ER techniques significantly improved model performance across all scenarios
- GPT-4 consistently outperformed GPT-3.5 and Llama 2 in every scenario tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's architecture allows it to generalize knowledge from pretraining to specialized domains without additional fine-tuning.
- Mechanism: GPT-4 uses a transformer-based architecture trained on diverse web-scale data, enabling pattern recognition and reasoning that transfers to domain-specific questions like agriculture.
- Core assumption: Pretraining corpus diversity captures enough cross-domain patterns for zero-shot performance.
- Evidence anchors: [abstract]: "Large Language Models (LLMs), such as GPT-4 and Llama 2, have made significant strides in showcasing remarkable capabilities across a wide range of domains and tasks." [section]: "GPT-4 consistently outperforms all other models in every scenario, with noticeable increases in performance when provided with different methods."
- Break condition: Domain requires knowledge outside pretraining distribution (e.g., hyperlocal or proprietary data).

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) improves accuracy by providing contextually relevant external knowledge.
- Mechanism: RAG retrieves domain-specific documents (e.g., Embrapa texts, CCA materials) and conditions generation on them, aligning model outputs with specialized knowledge.
- Core assumption: External knowledge sources are comprehensive and correctly indexed for retrieval.
- Evidence anchors: [abstract]: "RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the models' performance." [section]: "the most significant improvement for all models was observed when employing the RAG (Retrieval-Augmented Generation) technique in Scenarios 4 and 5. GPT-4 achieved an impressive 93% score."
- Break condition: Retrieved documents are irrelevant, outdated, or poorly indexed, leading to incorrect conditioning.

### Mechanism 3
- Claim: Ensemble Refinement (ER) improves accuracy by sampling multiple reasoning paths and aggregating results.
- Mechanism: ER generates multiple explanations via stochastic sampling, then conditions the model on these to refine the final answer, mimicking self-consistency.
- Core assumption: Diverse sampling captures complementary reasoning paths that converge on correct answers.
- Evidence anchors: [abstract]: "ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the models' performance." [section]: "ER also improves results by harnessing hidden knowledge within LLMs using different temperatures for each GPT call, enabling the extraction of diverse and domain-specific insights for more accurate and contextually relevant answers."
- Break condition: Sampling diversity is too low or too noisy, causing convergence on incorrect answers.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how GPT-4 processes input and generates answers is essential for debugging performance drops.
  - Quick check question: What role does self-attention play in allowing the model to weigh different parts of the input during generation?

- Concept: Prompt engineering and chain-of-thought
  - Why needed here: Effective prompts guide model reasoning; misprompting leads to irrelevant or incorrect answers.
  - Quick check question: How does adding a preamble or structured prompt improve GPT-4's accuracy on specialized questions?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG bridges knowledge gaps; understanding indexing and retrieval quality is key to diagnosing errors.
  - Quick check question: What happens if the retrieved context is irrelevant or contradictory to the question?

## Architecture Onboarding

- Component map: Data collection -> preprocessing -> prompt creation -> LLM inference (with optional RAG/ER) -> answer evaluation
- Critical path: Prompt creation -> LLM call -> answer validation. RAG/ER add optional steps before or after inference.
- Design tradeoffs: RAG improves accuracy but adds latency and cost. ER increases computational cost (multiple samplings) but can yield higher accuracy. Using open-ended vs. multiple-choice formats affects evaluation difficulty.
- Failure signatures: Low accuracy without RAG: missing domain knowledge in pretraining. Inconsistent answers across runs: sampling temperature or lack of self-consistency. High computational cost with marginal gains: over-engineering for simple questions.
- First 3 experiments:
  1. Run baseline GPT-4 on CCA text questions without RAG/ER to measure pretraining-only performance.
  2. Add RAG to the same set and compare accuracy gains.
  3. Apply ER (11 first-stage, 33 second-stage samples) to measure refinement impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 compare to human experts in agricultural knowledge assessment?
- Basis in paper: [explicit] The paper states "On one of our evaluation datasets that had published student scores, GPT-4 obtained the highest performance when compared to human subjects."
- Why unresolved: The paper only compares GPT-4 to one specific dataset with human scores. It's unclear how GPT-4 would perform across a broader range of agricultural assessments and against diverse groups of human experts.
- What evidence would resolve it: Conducting comprehensive evaluations of GPT-4 against multiple agricultural exams and diverse groups of human experts (agronomists, farmers, students) across different countries and specializations would provide clearer insights into its comparative performance.

### Open Question 2
- Question: What are the limitations of GPT-4 in providing context-specific agricultural advice for different regions?
- Basis in paper: [inferred] The paper mentions that "General models like GPT-3.5 and GPT-4 may struggle to provide accurate responses tailored to the Brazilian context due to potential limitations in their training data concerning Brazil's unique agricultural conditions and practices."
- Why unresolved: While the paper acknowledges potential limitations, it doesn't provide a detailed analysis of GPT-4's performance across different agricultural contexts and regions.
- What evidence would resolve it: Evaluating GPT-4's performance on region-specific agricultural questions and comparing it to local experts' knowledge would reveal its strengths and weaknesses in providing context-specific advice.

### Open Question 3
- Question: How can the computational costs of using RAG and ER techniques with GPT-4 be optimized for practical agricultural applications?
- Basis in paper: [explicit] The paper states "While these techniques can significantly improve the quality of answers generated by Large Language Models (LLMs), it is important to note that the combination of RAG and ER comes with increased computational costs for each question-answering task."
- Why unresolved: The paper acknowledges the increased computational costs but doesn't explore potential strategies for optimization or discuss the trade-offs between performance gains and resource requirements.
- What evidence would resolve it: Investigating various optimization techniques, such as model compression, efficient retrieval methods, or adaptive sampling strategies, and evaluating their impact on performance and computational costs would provide insights into practical implementation.

## Limitations

- The study lacks detailed implementation specifications for RAG and ER techniques, making exact reproducibility uncertain
- Limited error analysis prevents understanding of specific failure modes and model weaknesses
- Regional agricultural knowledge limitations are acknowledged but not thoroughly investigated

## Confidence

- Claim: GPT-4 achieves 93% accuracy on agronomist certification exams without fine-tuning (High confidence)
- Claim: Pretraining generalization mechanism enables zero-shot performance (Medium confidence)
- Claim: RAG and ER techniques improve accuracy (Medium confidence)

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of RAG and ER techniques versus prompt engineering alone
2. Test model performance across a broader range of agricultural scenarios including practical problem-solving and region-specific knowledge
3. Implement statistical significance testing and confidence interval calculations for all reported accuracy scores