---
ver: rpa2
title: Category Adaptation Meets Projected Distillation in Generalized Continual Category
  Discovery
arxiv_id: '2308.12112'
source_url: https://arxiv.org/abs/2308.12112
tags:
- learning
- classes
- novel
- task
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generalized Continual Category Discovery
  (GCCD), a novel continual learning framework that unifies semi-supervised learning
  and generalized category discovery. Unlike traditional settings, GCCD handles partially
  labeled datasets with both known and novel classes across sequential tasks, better
  reflecting real-world scenarios.
---

# Category Adaptation Meets Projected Distillation in Generalized Continual Category Discovery

## Quick Facts
- arXiv ID: 2308.12112
- Source URL: https://arxiv.org/abs/2308.12112
- Authors: 
- Reference count: 9
- Key outcome: CAMP achieves 57.0% task-agnostic accuracy on CIFAR100 (vs. 54.2% for best baseline) and 28.9% on Stanford Cars (vs. 27.0% for best baseline)

## Executive Summary
This paper introduces Generalized Continual Category Discovery (GCCD), a novel continual learning framework that unifies semi-supervised learning and generalized category discovery. Unlike traditional settings, GCCD handles partially labeled datasets with both known and novel classes across sequential tasks, better reflecting real-world scenarios. The proposed method, CAMP (Category Adaptation Meets Projected Distillation), employs a three-phase approach: feature extractor training with supervised and unsupervised learning, clustering new task data, and centroid adaptation to mitigate representation drift.

## Method Summary
The method implements a three-phase approach to GCCD: (1) feature extractor training using supervised (SupCon) and unsupervised (SimCLR) losses with projected distillation, (2) clustering new task data using semi-supervised k-means, and (3) centroid adaptation using prototype adaptation network. The key innovations include using learnable projectors for feature distillation to prevent forgetting and a centroid adaptation network to maintain knowledge of previously learned categories.

## Key Results
- CAMP achieves 57.0% task-agnostic accuracy on CIFAR100 (vs. 54.2% for best baseline)
- CAMP achieves 28.9% task-agnostic accuracy on Stanford Cars (vs. 27.0% for best baseline)
- Method excels at maintaining knowledge of known classes while discovering novel ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projected feature distillation with a learnable linear projector mitigates catastrophic forgetting by mapping current representations back to the old latent space.
- Mechanism: The learnable projector m maps Ft(xi) to approximate Ft-1(xi), creating a soft constraint that discourages large representational drift while allowing plasticity.
- Core assumption: The old feature extractor Ft-1 is frozen and serves as a reference point for regularization.

### Mechanism 2
- Claim: Centroid adaptation network ψ(t-1)→t compensates for representation drift by predicting how old class centroids shift in the new feature space.
- Mechanism: The network learns to predict Ft(xi) from Ft-1(xi) using only data from the current task, then applies this transformation to update old centroids.
- Core assumption: The mapping between consecutive feature spaces is smooth enough to be learned from a single task's data.

### Mechanism 3
- Claim: Three-phase training (feature extractor, clustering, centroid adaptation) enables effective balance between discovering novel classes and preserving known classes.
- Mechanism: Phase 1 trains with supervised and unsupervised losses plus distillation, Phase 2 clusters new data to find novel class centroids, Phase 3 updates old centroids using the adaptation network.
- Core assumption: Each phase addresses a distinct challenge that cannot be effectively handled in a single unified process.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The method must prevent loss of previously learned knowledge while adapting to new tasks
  - Quick check question: What happens to a neural network's performance on old tasks when trained on new tasks without any regularization?

- Concept: Contrastive learning (SimCLR and SupCon)
  - Why needed here: These methods provide effective self-supervised and supervised representations from both labeled and unlabeled data
  - Quick check question: How do SimCLR and SupCon differ in their treatment of positive and negative pairs?

- Concept: Semi-supervised clustering with k-means
  - Why needed here: The method needs to discover novel classes from unlabeled data while respecting known class boundaries
  - Quick check question: What constraint does semi-supervised k-means place on labeled data during clustering?

## Architecture Onboarding

- Component map:
  - Feature extractor F (ViT-small backbone)
  - Classifiers C and clustering centroids
  - Learnable projector m (linear layer for distillation)
  - Centroid adaptation network ψ(t-1)→t
  - Loss components: SimCLR, SupCon, distillation

- Critical path:
  1. Phase 1: Train feature extractor with combined losses
  2. Phase 2: Apply semi-supervised k-means to find centroids
  3. Phase 3: Train adaptation network and update centroids
  4. Inference: Nearest Mean Classifier with adapted centroids

- Design tradeoffs:
  - Linear vs MLP projector: Linear is more parameter-efficient but may be less expressive
  - Centroid adaptation vs exemplar replay: Adaptation is memory-efficient but may be less accurate
  - Three-phase vs unified training: Separation provides clarity but may miss synergies

- Failure signatures:
  - Known class accuracy drops sharply after few tasks → representation drift not properly handled
  - Novel class accuracy plateaus early → clustering or feature learning ineffective
  - Overall accuracy improves but representation strength drops → distillation too restrictive

- First 3 experiments:
  1. Run single task with and without learnable projector to measure forgetting reduction
  2. Test centroid adaptation network on synthetic drift to verify compensation capability
  3. Compare three-phase vs unified training on CIFAR100 to measure phase benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we estimate the number of clusters for the K-means algorithm in the proposed framework?
- Basis in paper: [explicit] The paper mentions that "for the sake of simplicity, we assume it is provided value" when discussing the number of clusters in the clustering phase.
- Why unresolved: Determining the number of clusters is crucial for the effectiveness of the clustering algorithm, and a fixed value may not generalize well across different datasets or tasks.
- What evidence would resolve it: A method or algorithm that can dynamically estimate the number of clusters based on the data distribution or other criteria would provide evidence to resolve this question.

### Open Question 2
- Question: How do different SSL solutions, such as Barlow Twins, compare to SimCLR in terms of performance on smaller datasets?
- Basis in paper: [explicit] The paper suggests implementing different SSL solutions, mentioning Barlow Twins as an example, to replace the SimCLR loss in the feature extractor training phase.
- Why unresolved: The paper only uses SimCLR for self-supervised learning, and it would be valuable to know if other SSL methods could improve the performance, especially on smaller datasets.
- What evidence would resolve it: Conducting experiments comparing the performance of SimCLR with other SSL methods like Barlow Twins on various datasets would provide evidence to resolve this question.

### Open Question 3
- Question: How would the inclusion of exemplars impact the performance of the proposed method in less privacy-restricted problems?
- Basis in paper: [explicit] The paper suggests allowing the method to include exemplars and mentions that they could be used in all phases presented in the training process.
- Why unresolved: The paper does not implement exemplars, so their impact on the method's performance is unknown.
- What evidence would resolve it: Implementing exemplars in the proposed method and evaluating its performance on various datasets would provide evidence to resolve this question.

## Limitations
- Evaluation scope limited to relatively small-scale image classification tasks (CIFAR100, Stanford Cars)
- Limited validation across diverse datasets and task sequences
- Uncertainty about scalability to larger-scale datasets and more complex domains

## Confidence
High: Effectiveness of projected distillation mechanism
Medium: Overall framework performance improvements
Low: Scalability and generalizability to larger datasets

## Next Checks
1. Test scalability on larger datasets (e.g., ImageNet-1K) with more tasks and classes per task
2. Conduct ablation studies to quantify individual contributions of each component
3. Evaluate on non-image domains (e.g., text or tabular data) to assess cross-modal applicability