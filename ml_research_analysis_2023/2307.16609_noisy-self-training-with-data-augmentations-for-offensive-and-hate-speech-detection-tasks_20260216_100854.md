---
ver: rpa2
title: Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection
  Tasks
arxiv_id: '2307.16609'
source_url: https://arxiv.org/abs/2307.16609
tags:
- data
- self-training
- offensive
- augmentation
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines self-training and noisy self-training for offensive
  and hate speech detection using five BERT model variants and two datasets. Self-training
  consistently improves F1-macro scores by up to 1.5% regardless of model size.
---

# Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks

## Quick Facts
- arXiv ID: 2307.16609
- Source URL: https://arxiv.org/abs/2307.16609
- Reference count: 21
- Key outcome: Self-training improves F1-macro by up to 1.5%; noisy self-training with augmentations decreases performance due to semantic shifts in label-sensitive examples.

## Executive Summary
This study examines self-training and noisy self-training for offensive and hate speech detection using five BERT model variants and two datasets. Self-training consistently improves F1-macro scores by up to 1.5% regardless of model size. However, noisy self-training with text data augmentations (backtranslation, random word swap, synonym substitution) does not improve performance and in some cases decreases it, likely due to semantic shifts in label-sensitive examples. Analysis shows that backtranslation introduces the most vocabulary changes and label shifts, but this does not correlate with better results. The authors conclude that data augmentations may harm performance in this domain and suggest future work on augmentations that preserve offensive keywords and context.

## Method Summary
The authors fine-tune five BERT model variants (DistilBERT, BERT-base, BERT-large, RoBERTa-base, RoBERTa-large) on OLID and ConvAbuse datasets, then apply self-training with or without data augmentation (backtranslation, random word swap, synonym substitution) using 365,456 unlabeled tweets. They use a teacher-student loop with a confidence threshold and compare F1-macro scores on test sets across four self-training iterations, repeating experiments with three random seeds.

## Key Results
- Self-training consistently improves F1-macro scores by up to 1.5% regardless of model size.
- Noisy self-training with textual data augmentations decreases performance on offensive and hate speech domains.
- Backtranslation introduces the most vocabulary changes and label shifts, but this does not correlate with better results.

## Why This Works (Mechanism)
- **Mechanism 1**: Self-training improves F1-macro by iteratively refining the model using weakly-labeled examples, expanding the effective training set and reducing overfitting. Works when teacher model's confidence threshold is sufficiently high to avoid noisy labels.
- **Mechanism 2**: Noisy self-training with textual augmentations decreases performance because augmentations like backtranslation, synonym substitution, and word swap introduce semantic shifts that alter the meaning of offensive/hate speech examples, leading to incorrect labels. Breaks when semantic invariance is not preserved.
- **Mechanism 3**: Backtranslation introduces the most vocabulary changes but does not correlate with performance improvement because in offensive/hate speech, these changes often affect key offensive terms and context, leading to label shifts. Breaks when augmentations change semantic meaning in label-sensitive domains.

## Foundational Learning
- **Concept: Self-training in semi-supervised learning**
  - Why needed here: Understanding how to iteratively use a teacher model to label unlabeled data and improve the student model is key to replicating and extending the study's approach.
  - Quick check question: What is the role of the confidence threshold in self-training, and what happens if it's set too low or too high?

- **Concept: Data augmentation for NLP**
  - Why needed here: Knowing how techniques like backtranslation, synonym substitution, and word swap work, and their typical effects on text semantics, is essential for analyzing why they fail in this domain.
  - Quick check question: How does backtranslation typically affect vocabulary and semantics, and why might this be problematic for hate speech detection?

- **Concept: BERT model architecture and fine-tuning**
  - Why needed here: Understanding the differences between BERT-base, BERT-large, DistilBERT, RoBERTa-base, and RoBERTa-large, as well as how fine-tuning works, is necessary for interpreting the results across different model sizes.
  - Quick check question: How does the number of parameters in BERT-base vs. BERT-large affect fine-tuning performance, and why might self-training help smaller models?

## Architecture Onboarding
- **Component map**: Teacher model (fine-tuned on human-labeled data) -> Confidence threshold filter -> Data augmentation module (backtranslation/synonym substitution/word swap) -> Student model (trained on combined human-labeled and weakly-labeled data) -> Next teacher model
- **Critical path**: 1. Train teacher model on human-labeled data; 2. Infer labels for unlabeled data; 3. Apply confidence threshold and downsampling; 4. Augment weakly-labeled data (for noisy self-training); 5. Train student model on combined data; 6. Repeat steps 2-5 for fixed iterations
- **Design tradeoffs**: Confidence threshold (higher reduces noise but limits data; lower increases data but risks noise); Augmentation method (different methods introduce varying levels of semantic shift; backtranslation most disruptive); Model size (larger models may benefit less from self-training but start with higher baseline performance); Number of self-training iterations (more iterations may overfit to noisy labels)
- **Failure signatures**: Performance degradation when using noisy self-training vs. default; High label shift percentages after augmentation (especially backtranslation); Vocabulary expansion without corresponding performance gain
- **First 3 experiments**: 1. Implement and run default self-training (no augmentation) on OLID with DistilBERT to confirm the +0.7% to +1.5% F1-macro improvement; 2. Run noisy self-training with backtranslation on OLID with DistilBERT and measure label shift and performance drop; 3. Compare performance of self-training on OLID vs. ConvAbuse with the same model to verify domain robustness

## Open Questions the Paper Calls Out
- **Open Question 1**: How can data augmentation methods be designed to preserve offensive keywords and context in hate speech detection tasks? Basis in paper: The authors suggest that data augmentations may harm performance in this domain and suggest future work on augmentations that preserve offensive keywords and context.
- **Open Question 2**: How effective are instruction-tuned large language models in generating task-specific data augmentations that preserve the semantics associated with hate speech detection? Basis in paper: The authors propose using recent instruction-tuned large language models as specialized data augmentation methods that are task-specific.
- **Open Question 3**: How do different model sizes impact the performance of self-training and noisy self-training in hate speech detection tasks? Basis in paper: The authors experiment with five different pre-trained BERT models of varying sizes and find that self-training improves classification performance for all model architectures.

## Limitations
- The study's findings are tightly bound to the characteristics of offensive and hate speech detection, and the exact boundary of what makes this domain unique is not rigorously defined.
- While experiments are repeated with three random seeds, the absolute performance differences are modest and may not be statistically significant across all model-dataset combinations.
- The paper does not fully specify the exact train/dev/test splits for the datasets, nor the precise preprocessing steps for the unlabeled Twitter data beyond basic cleaning.

## Confidence
- **High Confidence**: Self-training improves F1-macro scores by up to 1.5% regardless of model size; Noisy self-training with textual data augmentations decreases performance compared to default self-training in offensive/hate speech domains.
- **Medium Confidence**: Backtranslation introduces the most vocabulary changes but does not correlate with better results.
- **Low Confidence**: Data augmentations harm performance in this domain and suggest future work on augmentations that preserve offensive keywords and context.

## Next Checks
1. **Ablation Study on Augmentation Methods**: Run a controlled experiment where each augmentation method is applied to a subset of examples, and label shifts are measured and correlated with performance drops.
2. **Statistical Significance Testing**: Perform statistical tests (e.g., paired t-tests, bootstrap confidence intervals) on F1-macro scores across the three random seeds for each model-dataset-augmentation combination.
3. **Error Analysis on Label Shifts**: For each augmentation method, analyze which types of examples (e.g., mild vs. severe offensive content, specific keywords) are most likely to have their labels flipped.