---
ver: rpa2
title: Object Detector Differences when using Synthetic and Real Training Data
arxiv_id: '2312.00694'
source_url: https://arxiv.org/abs/2312.00694
tags:
- layers
- similarity
- data
- synthetic
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how object detection models trained on
  synthetic data differ from those trained on real data, using the YOLOv3 detector
  on city environment images. The authors use Centered Kernel Alignment (CKA) to analyze
  the similarity of layer outputs between models trained on real (Berkeley Deep Drive)
  and synthetic (Grand Theft Auto V) data.
---

# Object Detector Differences when using Synthetic and Real Training Data

## Quick Facts
- arXiv ID: 2312.00694
- Source URL: https://arxiv.org/abs/2312.00694
- Reference count: 40
- Primary result: Early layers of YOLOv3 show high similarity between models trained on real vs. synthetic data, while head layers show largest differences.

## Executive Summary
This paper investigates how object detection models trained on synthetic data differ from those trained on real data, using the YOLOv3 detector on city environment images. The authors use Centered Kernel Alignment (CKA) to analyze the similarity of layer outputs between models trained on real (Berkeley Deep Drive) and synthetic (Grand Theft Auto V) data. The key finding is that the largest similarity between models trained on real and synthetic data is in the early layers of the network, while the largest difference is in the head part. Additionally, the study finds no major difference in performance or similarity between models with frozen and unfrozen backbones when further trained on synthetic data. The results suggest that the early layers of object detection models are not significantly affected by the type of training data, while the head part is more sensitive to the data type.

## Method Summary
The study trains YOLOv3 object detectors on real (Berkeley Deep Drive) and synthetic (Grand Theft Auto V) datasets, comparing layer-wise similarity using Centered Kernel Alignment (CKA). Models are trained with both frozen and unfrozen backbones to assess the impact of backbone adaptation. Performance is measured using mAP@0.5, and CKA similarity is computed across layers to identify where domain differences manifest. Experiments include varying input resolutions for CKA analysis and evaluating similarity patterns between different training configurations.

## Key Results
- Early convolutional layers show the highest similarity between models trained on real and synthetic data.
- The head part exhibits the largest differences in layer similarity between real and synthetic training.
- No significant performance or similarity differences are observed between frozen and unfrozen backbone configurations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The early convolutional layers of YOLOv3 exhibit high similarity between real and synthetic training data because they learn generic visual features (edges, textures) that are preserved across domains.
- Mechanism: Feature extraction in early layers depends on low-level patterns that are common in both real and synthetic images; the backbone weights pre-trained on ImageNet are already tuned to detect these universal features.
- Core assumption: The synthetic dataset contains sufficient variation in lighting, textures, and scene structure to produce the same low-level patterns as real data.
- Evidence anchors:
  - [abstract] "The results show that the largest similarity between a detector trained on real data and a detector trained on synthetic data was in the early layers..."
  - [section] "These layers are likely targeting generic image features, such as edges in the first layer [41]."
  - [corpus] Weak: no corpus paper explicitly tests early-layer feature invariance between synthetic and real domains.
- Break condition: Synthetic data lacks sufficient visual diversity or has artifacts that obscure natural low-level patterns.

### Mechanism 2
- Claim: Freezing or unfreezing the backbone during synthetic data training does not significantly affect layer similarity or performance because the backbone is already well-tuned from ImageNet pretraining.
- Mechanism: Pre-trained backbone weights are robust; fine-tuning on synthetic data has minimal impact on already-optimized early layers, so both frozen and unfrozen settings yield similar representations.
- Core assumption: The domain gap between ImageNet and synthetic driving data is small enough that the backbone remains effective without further adaptation.
- Evidence anchors:
  - [abstract] "The results also show that no major difference in performance or similarity could be seen between frozen and unfrozen backbone."
  - [section] "The high similarity between all models in the backbone means that the pre-trained backbone is rather dominant in all models..."
  - [corpus] Weak: no corpus paper directly compares frozen vs. unfrozen backbones on synthetic vs. real domain pairs.
- Break condition: The synthetic domain is drastically different from ImageNet (e.g., extreme lighting, non-photorealistic rendering), requiring backbone adaptation.

### Mechanism 3
- Claim: The head layers show the largest difference between real and synthetic training because they must adapt to domain-specific object appearance and scene layout, which vary between datasets.
- Mechanism: Detection layers operate on high-level semantic features and rely on task-specific cues (scale, aspect ratio, object context) that differ between real and synthetic domains, leading to divergent learned representations.
- Core assumption: The semantic and spatial characteristics of objects in synthetic data do not perfectly match those in real data, forcing the head to specialize differently.
- Evidence anchors:
  - [abstract] "The largest difference was in the head part."
  - [section] "The head part had lower similarity than the backbone, which was also lower than the mean of all layers."
  - [corpus] Weak: no corpus paper directly links head-layer domain shift to object appearance differences.
- Break condition: Synthetic data is photorealistic enough that head layers can generalize directly to real data without adaptation.

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA)
  - Why needed here: CKA quantifies layer-wise similarity between models trained on different data types, allowing objective comparison of learned representations.
  - Quick check question: What property of CKA makes it invariant to invertible linear transforms but sensitive to orthogonal transformations?
- Concept: Residual blocks and shortcut connections
  - Why needed here: Residual layers sum outputs from earlier layers, potentially propagating high-similarity features and influencing overall similarity patterns.
  - Quick check question: How do shortcut connections in residual blocks affect gradient flow and feature reuse in deep networks?
- Concept: Object detection evaluation metrics (mAP@0.5)
  - Why needed here: mAP measures detection accuracy and is used to verify that models trained on different data perform as expected on their target domains.
  - Quick check question: What does an mAP of 0.43 on BDD versus 0.12 on GTAV imply about domain adaptation effectiveness?

## Architecture Onboarding

- Component map: Backbone (Darknet-53, layers 0-74) -> Residual blocks -> Head (layers 75-106 with 3 detection layers at 82, 94, 106) -> Output
- Critical path: Input -> Backbone -> Route layers (83, 86, 98) -> Detection layers -> NMS -> Predictions
- Design tradeoffs: Frozen backbone saves compute but may limit adaptation; unfrozen allows full domain adaptation but risks overfitting synthetic artifacts.
- Failure signatures: Low CKA similarity in early layers indicates synthetic data lacks real-world visual cues; high head-layer similarity despite domain shift suggests inadequate task specialization.
- First 3 experiments:
  1. Train U-Real and U-Synthetic on reduced datasets, measure CKA similarity curves, verify early-layer dominance.
  2. Swap backbone freezing between models, compare mAP and CKA to confirm no major difference.
  3. Visualize feature maps from early vs. head layers for both data types to confirm generic vs. domain-specific patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the realism level of synthetic data affect the similarity between early layers of neural networks trained on synthetic versus real data?
- Basis in paper: [inferred] The paper compares models trained on synthetic data from GTA V and real data from BDD, but does not vary the realism level of synthetic data.
- Why unresolved: The study only uses one synthetic dataset (GTA V) and one real dataset (BDD), limiting the ability to generalize findings about the impact of synthetic data realism on layer similarity.
- What evidence would resolve it: Conducting experiments with synthetic datasets of varying realism levels (e.g., different rendering qualities or domain randomization techniques) and comparing their layer similarities to a real dataset.

### Open Question 2
- Question: Does the size and diversity of the training dataset influence the similarity between models trained on synthetic and real data?
- Basis in paper: [inferred] The paper uses datasets of different sizes (80,000 images for BDD and 134,000 for GTAV) but does not explore the effect of varying dataset sizes or diversity.
- Why unresolved: The study does not investigate how changes in dataset size or diversity impact the similarity between models trained on synthetic versus real data.
- What evidence would resolve it: Training models on synthetic and real datasets of varying sizes and diversities, then comparing their layer similarities using CKA or other similarity metrics.

### Open Question 3
- Question: How does the choice of object detection architecture (beyond YOLOv3) affect the similarity between models trained on synthetic and real data?
- Basis in paper: [inferred] The study focuses on YOLOv3, but does not explore how other architectures might behave differently when trained on synthetic versus real data.
- Why unresolved: The paper's findings are limited to YOLOv3, leaving open the question of whether other architectures would show similar patterns in layer similarity.
- What evidence would resolve it: Conducting similar experiments with different object detection architectures (e.g., Faster R-CNN, RetinaNet) and comparing their layer similarities when trained on synthetic and real data.

### Open Question 4
- Question: How does the resolution of input images affect the similarity between models trained on synthetic and real data?
- Basis in paper: [explicit] The paper mentions using different input resolutions (32x32, 64x64, 128x128) for CKA analysis, but does not extensively explore the impact of resolution on layer similarity.
- Why unresolved: While the paper touches on input resolution, it does not provide a comprehensive analysis of how resolution affects the similarity between models trained on synthetic and real data.
- What evidence would resolve it: Systematically varying the input resolution for both training and CKA analysis, then comparing the resulting layer similarities between models trained on synthetic and real data at different resolutions.

## Limitations
- The study only uses one synthetic and one real dataset, limiting generalizability of findings.
- No ablation studies are conducted to isolate the impact of specific factors like synthetic rendering quality or dataset diversity.
- The assumption that ImageNet pretraining is universally beneficial may not hold for drastically different domains.

## Confidence
- Early-layer domain invariance: Medium
- Head-layer domain differences: Medium
- Backbone freezing impact: Low

## Next Checks
1. Test CKA similarity patterns on additional real-synthetic pairs (e.g., different games or simulators) to confirm early-layer invariance generalizes.
2. Perform controlled experiments varying synthetic rendering quality to isolate when backbone adaptation becomes necessary.
3. Conduct head-layer ablation studies comparing object appearance distributions between datasets to quantify their impact on domain shift.