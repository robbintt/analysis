---
ver: rpa2
title: Measuring reasoning capabilities of ChatGPT
arxiv_id: '2310.05993'
source_url: https://arxiv.org/abs/2310.05993
tags:
- room
- solution
- wrong
- puzzle
- both
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatGPT was evaluated on a benchmark of 100 logical puzzles from
  various domains, including arithmetic, logic equations, zebra-like puzzles, truth-telling
  puzzles, and self-reference puzzles. The correct solutions were validated using
  theorem prover Prover9 and model finder Mace4.
---

# Measuring reasoning capabilities of ChatGPT

## Quick Facts
- arXiv ID: 2310.05993
- Source URL: https://arxiv.org/abs/2310.05993
- Reference count: 40
- Primary result: ChatGPT solved only 7% of logical puzzles correctly with proper justification

## Executive Summary
This study evaluates ChatGPT's logical reasoning capabilities using a benchmark of 100 puzzles spanning arithmetic, logic equations, zebra-like puzzles, truth-telling puzzles, and self-reference puzzles. The evaluation reveals severe limitations in the model's reasoning abilities, with only 7% of puzzles solved correctly with valid justification. The study identifies 698 logical faults across all solutions, averaging 7 fallacies per task and with 26.03% of generated text containing logical errors. These findings demonstrate that current LLMs, despite proficiency in translation tasks, lack robust reasoning capabilities.

## Method Summary
The researchers compiled a benchmark of 100 logical puzzles from various domains and evaluated ChatGPT's solutions against correct answers validated using theorem prover Prover9 and model finder Mace4. Each solution was analyzed for logical consistency and justifications, with errors classified into a taxonomy of 67 distinct types. The analysis focused on identifying reasoning failures, inconsistencies, unsupported claims, and other logical faults in the generated responses.

## Key Results
- ChatGPT provided correct answers with proper justification for only 7% of the puzzles
- On average, 26.03% of generated text contained logical faults
- The 100 solutions contained 698 logical faults, averaging 7 fallacies per reasoning task
- A taxonomy of 67 distinct types of reasoning errors was identified

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's reasoning errors stem from a failure to distinguish logical form from surface pattern matching
- Mechanism: The model relies on recognizing familiar templates rather than building explicit logical derivations, incorrectly accepting superficially similar solutions that violate deeper constraints
- Core assumption: Training data contains many solved puzzle examples enabling pattern-based shortcuts
- Evidence anchors:
  - [abstract] "ChatGPT provided correct answers with proper justification for only 7% of the puzzles"
  - [section] "The 100 solutions generated by ChatGPT contain 698 logical faults. That is on average, 7 fallacies for each reasoning task"
  - [corpus] "average neighbor FMR=0.47" indicates moderate thematic relatedness but no direct citation overlap
- Break condition: When puzzles lack surface similarity to training examples or require multi-step deduction without intermediate reasoning

### Mechanism 2
- Claim: The model generates plausible-sounding but logically invalid reasoning chains
- Mechanism: It applies transformation rules to intermediate results without verifying their consistency, leading to contradictions within the same answer
- Core assumption: The model's inference process is heuristic-based, prioritizing fluency over logical validity
- Evidence anchors:
  - [abstract] "A total of 698 logical faults were identified, forming a taxonomy of 67 distinct types of reasoning errors"
  - [section] "Inconsistency means that within the same answer, chatGPT generates claims that contradict each other"
  - [corpus] Weak citation signals - no direct studies on LLM contradiction patterns in reasoning tasks
- Break condition: When explicit contradiction detection is applied or when output is cross-checked with formal verification tools

### Mechanism 3
- Claim: The model overgeneralizes from limited examples, leading to false universal claims
- Mechanism: After observing a few cases, it asserts general rules that don't hold, especially in combinatorial or probabilistic puzzles
- Core assumption: Training distribution favors short, pattern-matching responses over exhaustive case analysis
- Evidence anchors:
  - [abstract] "On average, 26.03% from the generated text was a logical fault"
  - [section] "There are 12 months (1 to 12) and 12 days (1 to 12) that are the same numeric representation. So, there are 12 ambiguous dates for these cases"
  - [corpus] Moderate similarity to puzzle-solving benchmarks but no citation of exhaustive enumeration methods
- Break condition: When tasks explicitly require enumeration of all cases or probabilistic analysis with many variables

## Foundational Learning

- Concept: Formal logic notation and inference rules
  - Why needed here: Understanding how the model's output deviates from valid logical inference requires familiarity with formal logic structures
  - Quick check question: Given premises "All A are B" and "All B are C", what can you conclude? Does ChatGPT's output match this inference?

- Concept: Constraint satisfaction and search algorithms
  - Why needed here: Many puzzles involve finding assignments that satisfy multiple constraints; understanding backtracking and pruning helps diagnose why the model fails
  - Quick check question: For a 4x4 Latin square, how many valid solutions exist? Can you verify a proposed solution manually?

- Concept: Common logical fallacies and their detection
  - Why needed here: The paper's taxonomy of 67 error types requires recognizing fallacies like circular reasoning, false dichotomy, and non sequitur
  - Quick check question: Identify the fallacy in "We know X is true because the source is reliable, and we know the source is reliable because it says X is true"

## Architecture Onboarding

- Component map: Input preprocessor -> Reasoning engine -> Output validator -> Error classifier
- Critical path:
  1. Parse puzzle constraints into formal representation
  2. Generate candidate solution using pattern matching + heuristic reasoning
  3. Validate each inference step against constraints
  4. Classify any detected errors using the taxonomy
- Design tradeoffs:
  - Speed vs. thoroughness: Quick pattern matching vs. exhaustive constraint satisfaction
  - Fluency vs. correctness: Generating natural-sounding text vs. formally valid derivations
  - Coverage vs. precision: Handling many puzzle types vs. deep verification for each
- Failure signatures:
  - Internal contradictions in generated text
  - Unsupported leaps between reasoning steps
  - Missing edge cases in combinatorial solutions
  - Violations of stated constraints
- First 3 experiments:
  1. Run the model on a set of puzzles with known solutions, measuring percentage of correct answers with valid justifications
  2. Instrument the reasoning engine to output intermediate logical steps, then manually verify each step
  3. Apply the error classifier to a sample of outputs, measuring distribution across the 67 error types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the 67 identified logical fault types be hierarchically organized into broader categories?
- Basis in paper: [explicit] The paper identifies 67 distinct types of logical faults but suggests they could be grouped into clusters like "unjustified claim, where claim can be the whole answer, or part of it like premise, conclusion, contradiction etc."
- Why unresolved: The paper only lists the 67 fault types without organizing them into a structured taxonomy. It merely suggests this could be done.
- What evidence would resolve it: A hierarchical taxonomy showing broader categories and subcategories of logical faults, demonstrating how the 67 types fit into this structure.

### Open Question 2
- Question: How does the proportion of faulty text (26.03%) compare across different types of logical puzzles?
- Basis in paper: [inferred] The paper analyzes 100 puzzles across 12 different types and reports an average of 26.03% faulty text, but doesn't break this down by puzzle type.
- Why unresolved: The analysis aggregates all puzzle types together, making it impossible to determine if certain puzzle types generate more faulty reasoning than others.
- What evidence would resolve it: A breakdown showing the percentage of faulty text for each of the 12 puzzle types, revealing which types are most challenging for the model.

### Open Question 3
- Question: Does the "traces of the past" phenomenon (using information from previous puzzles) occur consistently across all puzzle types or only specific ones?
- Basis in paper: [explicit] The paper identifies one instance where ChatGPT used information from a previous puzzle in Example 17, but doesn't investigate whether this is systematic.
- Why unresolved: Only one example is provided without analysis of whether this is an isolated incident or a consistent behavior pattern.
- What evidence would resolve it: A systematic analysis of whether ChatGPT's answers to puzzles show evidence of incorporating information from previously seen puzzles across the entire dataset.

### Open Question 4
- Question: How do the logical fault patterns differ between puzzles with textual clues versus those requiring graphical interpretation?
- Basis in paper: [inferred] The paper explicitly excludes 44 puzzles with graphical input from the dataset, suggesting these might behave differently.
- Why unresolved: The study only examines textual puzzles, leaving open whether the reasoning patterns would differ for graphical puzzles.
- What evidence would resolve it: Testing the same logical reasoning task on both textual and graphical puzzle representations and comparing the types and frequencies of logical faults.

### Open Question 5
- Question: Does the low success rate (7%) improve with fine-tuned prompts or is it inherent to the model's reasoning capabilities?
- Basis in paper: [explicit] The paper states "Since the dataset seems challenging, the researchers are invited to test the dataset on more advanced or tuned models than ChatGPT3.5 with more crafted prompts."
- Why unresolved: The study uses only ChatGPT3.5 with standard prompts, leaving open whether better prompting could significantly improve performance.
- What evidence would resolve it: Testing the same 100-puzzle dataset with various prompt engineering techniques, model variants, and fine-tuning approaches to measure improvement in correct solutions and justifications.

## Limitations

- The evaluation focused on a specific dataset of 100 logical puzzles, representing a narrow slice of possible reasoning tasks
- The error taxonomy was developed through manual inspection and may not capture all possible reasoning failures or their relative importance
- The study does not address potential improvements through fine-tuning, prompt engineering, or more advanced models

## Confidence

- Core finding (7% success rate): **High** - directly measurable and validated against formal theorem provers
- Mechanistic explanations (Mechanisms 1-3): **Medium** - correlational evidence without access to model internals
- Error taxonomy comprehensiveness: **Medium** - manual classification introduces potential subjectivity

## Next Checks

1. **Cross-model replication**: Test the same puzzle benchmark on multiple LLM architectures (GPT-4, Claude, LLaMA) to determine if reasoning failures are model-specific or represent a broader limitation of current transformer-based approaches.

2. **Formal verification pipeline**: Implement automated cross-checking of generated solutions against formal constraint satisfaction algorithms, measuring false positive/negative rates in the current manual error classification system.

3. **Incremental reasoning analysis**: Modify the evaluation to capture the model's intermediate reasoning steps, then measure whether errors compound progressively or occur randomly throughout the solution process.