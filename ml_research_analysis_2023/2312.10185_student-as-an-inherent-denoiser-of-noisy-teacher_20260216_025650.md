---
ver: rpa2
title: Student as an Inherent Denoiser of Noisy Teacher
arxiv_id: '2312.10185'
source_url: https://arxiv.org/abs/2312.10185
tags:
- teacher
- labels
- student
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge distillation from noisy teacher
  models, particularly large language models generating pseudo labels. The authors
  discover that students can inherently denoise teacher labels during training, producing
  better predictions than the noisy labels used for training.
---

# Student as an Inherent Denoiser of Noisy Teacher

## Quick Facts
- arXiv ID: 2312.10185
- Source URL: https://arxiv.org/abs/2312.10185
- Reference count: 40
- Key outcome: Student models can inherently denoise noisy teacher labels during knowledge distillation, leading to a new method called Peer-Advised KD that improves F1 by ~2% and matches supervised fine-tuning with only 50 labeled examples

## Executive Summary
This paper investigates knowledge distillation from noisy teacher models, particularly large language models generating pseudo labels for grammar induction tasks. The authors discover that students can inherently denoise teacher labels during training, producing better predictions than the noisy labels used for training. This insight leads to Peer-Advised KD, which partitions teacher labels by convergence quality, trains a peer student on high-quality labels, and uses it to re-label low-quality ones. Experiments on Penn Treebank and CRAFT datasets show that PA-KD improves over vanilla KD by around 2% F1 and can match supervised fine-tuning with only 50 labeled examples, offering significant label efficiency.

## Method Summary
The method involves partitioning teacher labels based on convergence quality between student and teacher predictions, then using a peer student trained on high-quality labels to re-annotate low-quality ones. The ultimate student is trained on the combined high-quality and re-labeled dataset. The approach leverages the observation that students converge faster to less noisy teacher labels, allowing them to learn reliable knowledge from clean portions of noisy data and use this to outperform noisier teacher labels.

## Key Results
- Students can generate more accurate predictions than the noisy teacher labels used to train them
- PA-KD improves vanilla KD by around 2% F1 on Penn Treebank and CRAFT datasets
- PA-KD matches supervised fine-tuning performance with only 50 labeled examples versus 500 for the supervised baseline
- The denoising effect is more noticeable when the distillation set is much larger than the real labels used to train the teacher model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Student models inherently denoise teacher labels during knowledge distillation by converging faster to less noisy labels and generating better predictions than noisy teacher labels.
- Mechanism: The student model learns from noisy teacher labels but shows a bias toward learning from less noisy instances first, allowing it to acquire reliable knowledge from high-quality teacher labels and outperform noisier ones.
- Core assumption: The student's learning process is imbalanced, with more frequent updates for instances with more frequent patterns in the training dataset.
- Evidence anchors:
  - [abstract]: "the student model can already generate more accurate predictions than the teacher labels used to train it during KD, indicating its inherent ability to denoise noisy teacher labels."
  - [section]: "Student converges faster to less noisy teacher labels than noisier ones... the student is observed to, after converging to more credible teacher labels (higher ground-truth F1), outperform the noisier teacher labels (lower ground-truth F1) by conditioning on its learnt knowledge."
- Break condition: If the distillation set is too small or the noise level is too high, the student may not have enough clean labels to learn from, preventing the denoising effect.

### Mechanism 2
- Claim: Peer-Advised KD improves vanilla KD by leveraging the student's denoising ability to generate better pseudo labels for under-converged teacher labels.
- Mechanism: The method partitions teacher labels by convergence quality, trains a peer student on high-quality labels, and uses it to re-label low-quality ones, magnifying the student's inherent denoising ability.
- Core assumption: The convergence of the student with teacher labels can be used as a proxy for label quality.
- Evidence anchors:
  - [abstract]: "Motivated by our findings, we propose Peer-Advised KD to improve vanilla KD from noisy teachers."
  - [section]: "To leverage and magnify this process in KD, we first extract teacher labels denoted by Thigh based on convergence to train a peer student model S1, which is then employed to re-annotate the left teacher labels other than Thigh to get peer labels S."
- Break condition: If the partition threshold is not set correctly, the method may not effectively separate high and low-quality labels, reducing its effectiveness.

### Mechanism 3
- Claim: The student's denoising ability is more noticeable when the distillation set is much larger than the real labels used to train the teacher model.
- Mechanism: A larger distillation set contains more diverse patterns, allowing the student to learn more reliable knowledge from high-quality teacher labels, which can then be used to outperform noisier teacher labels.
- Core assumption: The student's learned knowledge can be much richer than the teacher's encoded knowledge for specific patterns when the distillation set is large.
- Evidence anchors:
  - [section]: "Because knowledge of ξbase involving patterns of x may have not been learnt effectively by S after t0... the student's learnt knowledge ξbase at some time in the middle of KD can be possibly reliable and even much more sufficient than teacher's ξT for some patterns, especially when |Dunlabeled x | >> |Dlabeled (x,y) |."
- Break condition: If the distillation set is too small or the noise level is too high, the student may not have enough clean labels to learn from, preventing the denoising effect from being noticeable.

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: The paper investigates KD from noisy teacher models and proposes a new method to improve it.
  - Quick check question: What is the main goal of knowledge distillation, and how does it differ from standard supervised learning?

- Concept: Label Noise and Denoising
  - Why needed here: The paper focuses on learning from noisy teacher labels and proposes a method to denoise them during KD.
  - Quick check question: How does label noise affect the performance of machine learning models, and what are some common techniques for handling noisy labels?

- Concept: Semi-Supervised Learning
  - Why needed here: The paper considers a semi-supervised learning scenario where there is a scarcity of labeled in-domain data but an abundance of unlabeled data.
  - Quick check question: What are the main challenges in semi-supervised learning, and how do they differ from fully supervised learning?

## Architecture Onboarding

- Component map: Teacher model (LLM) -> Student model -> Partition function -> Peer student model -> Ultimate student model

- Critical path:
  1. Teacher model generates pseudo labels for unlabeled data
  2. Student model learns from teacher labels and converges faster to less noisy labels
  3. Partition function divides teacher labels based on student's convergence
  4. Peer student model is trained on high-quality labels
  5. Peer student re-labels low-quality teacher labels
  6. Ultimate student model is trained with high-quality and re-labeled labels

- Design tradeoffs:
  - Partition threshold: Setting the threshold too high may result in too few high-quality labels, while setting it too low may include too many noisy labels.
  - Number of peer students: Using more peer students may improve the quality of re-labeled labels but increases computational cost.
  - Size of distillation set: A larger distillation set may improve the student's denoising ability but also increases the risk of including more noisy labels.

- Failure signatures:
  - Poor performance: If the student model does not exhibit denoising ability or the peer-advised KD does not improve performance, it may indicate issues with the teacher model, the student model, or the partition function.
  - High computational cost: If the method is too computationally expensive, it may not be practical for large-scale applications.
  - Overfitting: If the student model overfits to the noisy teacher labels, it may not generalize well to new data.

- First 3 experiments:
  1. Compare the performance of the student model trained with vanilla KD to the teacher model to verify the denoising ability.
  2. Implement the peer-advised KD method and compare its performance to vanilla KD to verify the improvement.
  3. Vary the partition threshold and the size of the distillation set to study their effects on the performance of the peer-advised KD method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the student's denoising ability generalize to other NLP tasks beyond grammar induction, such as named entity recognition or sentiment analysis?
- Basis in paper: [inferred] The paper focuses on grammar induction tasks (Penn Treebank and CRAFT datasets) and demonstrates denoising ability in this context. The authors mention that "This work mainly focuses on learning linguistic structures as our test bed" but don't explicitly test other tasks.
- Why unresolved: The paper only provides empirical evidence for grammar induction tasks. While the theoretical framework might suggest broader applicability, there's no experimental validation on other NLP tasks where noisy teacher labels might behave differently.
- What evidence would resolve it: Experiments applying PA-KD to other sequence labeling tasks (NER, POS tagging) and classification tasks (sentiment analysis) would show whether the denoising phenomenon and effectiveness of PA-KD generalize across different NLP tasks and label distributions.

### Open Question 2
- Question: How does the threshold parameter r (percentage of high-convergence labels selected for Thigh) affect PA-KD performance across different dataset sizes and noise levels?
- Basis in paper: [explicit] The paper mentions that "r is set as 50%, i.e., the half of teacher labels with greater convergence with student's predictions are considered as Thigh" but doesn't explore sensitivity to this parameter.
- Why unresolved: The paper uses a fixed 50% threshold without exploring how different thresholds might perform better for different scenarios. The optimal threshold could vary based on noise level, dataset size, and task complexity.
- What evidence would resolve it: Systematic experiments varying r from 10% to 90% across multiple datasets with different noise levels and sizes would reveal the sensitivity of PA-KD to this hyperparameter and identify optimal thresholds for different conditions.

### Open Question 3
- Question: What is the theoretical limit of student denoising ability - can students theoretically surpass the best possible teacher model with sufficient data, or is there a fundamental ceiling?
- Basis in paper: [explicit] The paper demonstrates that "the student model can already generate more accurate predictions than the teacher labels used to train it during KD" and shows students outperforming teachers, but doesn't explore theoretical limits.
- Why unresolved: While empirical results show students outperforming teachers, the paper doesn't establish whether this is bounded by teacher quality or if there's a theoretical framework explaining when and how much students can surpass teachers.
- What evidence would resolve it: Mathematical analysis establishing the relationship between student performance, teacher quality, and dataset size would determine if there's a fundamental ceiling or if students can theoretically achieve arbitrary improvement over their teachers with enough data.

## Limitations
- The mechanism explaining why students inherently denoise teacher labels is largely speculative without strong theoretical justification
- The partitioning strategy uses a fixed 50% threshold without justification for why this is optimal across different noise levels or dataset characteristics
- Experiments focus on specific parsing tasks with T5 models, limiting generalizability to other domains or architectures

## Confidence
- High Confidence: The experimental results showing that students can outperform noisy teacher labels during KD (F1 improvements of ~2%) are well-supported with clear metrics and multiple datasets.
- Medium Confidence: The Peer-Advised KD method shows consistent improvements over vanilla KD, but the specific contribution of each component is not isolated and the comparison to supervised baselines needs more rigorous validation.
- Low Confidence: The theoretical mechanism explaining why students inherently denoise teacher labels is largely speculative and lacks strong evidence for the underlying learning dynamics.

## Next Checks
1. **Ablation Study on Partitioning Strategy**: Test PA-KD with different partition thresholds (30%, 70%, 90%) and random partitioning to isolate the effect of convergence-based partitioning versus simply using more peer-annotated labels.

2. **Noise Level Sensitivity Analysis**: Systematically vary the noise level in teacher labels (e.g., by adjusting few-shot prompting quality or using controlled label corruption) to determine at what noise thresholds the student denoising ability breaks down.

3. **Architecture and Task Generalization**: Apply PA-KD to different model architectures (BERT, RoBERTa) and task types (text classification, sequence labeling) to verify whether the denoising phenomenon and method effectiveness generalize beyond the specific parsing tasks studied.