---
ver: rpa2
title: 'Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept
  Rehearsal'
arxiv_id: '2302.01242'
source_url: https://arxiv.org/abs/2302.01242
tags:
- concepts
- knowledge
- learning
- task
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Neuro-Symbolic Continual Learning (NeSy-CL),
  where models must solve a sequence of neuro-symbolic tasks that involve mapping
  sub-symbolic inputs to high-level concepts and making predictions using prior knowledge.
  The authors identify a critical issue: existing continual learning strategies suffer
  from catastrophic forgetting, while neuro-symbolic architectures can fall prey to
  "reasoning shortcuts" - acquiring task-specific concepts that satisfy knowledge
  but have incorrect semantics, harming cross-task transfer.'
---

# Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept Rehearsal

## Quick Facts
- arXiv ID: 2302.01242
- Source URL: https://arxiv.org/abs/2302.01242
- Reference count: 35
- Primary result: COOL achieves 70.7% Class-IL(Y) and 80.2% Class-IL(C) on MNAdd-Shortcut with 10% concept supervision, with 57.1% OOD accuracy compared to DER's 77.5% and 49.5% respectively, and 6.8% OOD accuracy

## Executive Summary
This paper introduces Neuro-Symbolic Continual Learning (NeSy-CL), where models must solve a sequence of neuro-symbolic tasks involving mapping sub-symbolic inputs to high-level concepts and making predictions using prior knowledge. The authors identify a critical issue: existing continual learning strategies suffer from catastrophic forgetting, while neuro-symbolic architectures can fall prey to "reasoning shortcuts" - acquiring task-specific concepts that satisfy knowledge but have incorrect semantics, harming cross-task transfer. To address this, they propose COOL (COncept-level cOntinual Learning), a strategy that acquires high-quality concepts using minimal concept supervision and preserves them across tasks through concept rehearsal.

## Method Summary
COOL is a concept-level continual learning strategy that addresses both catastrophic forgetting and reasoning shortcuts in neuro-symbolic architectures. The method combines three key components: (1) concept rehearsal using a buffer of stored concept distributions from previous tasks, (2) minimal concept supervision on a small subset of examples to acquire high-quality concepts, and (3) knowledge integration that constrains the reasoning layer to maintain stable concept-to-label mappings. COOL integrates with existing neuro-symbolic architectures like DeepProbLog and Concept-bottleneck Models, adding a KL divergence loss between current and stored concept distributions plus supervised concept loss on annotated examples.

## Key Results
- COOL outperforms state-of-the-art continual learning strategies on three novel benchmarks (MNAdd-Seq, MNAdd-Shortcut, CLE4EVR)
- On MNAdd-Shortcut with 10% concept supervision, COOL achieves 70.7% Class-IL(Y) and 80.2% Class-IL(C) compared to DER's 77.5% and 49.5% respectively
- COOL shows substantially better out-of-distribution accuracy (57.1% vs 6.8% for DER) on the shortcut-prone MNAdd-Shortcut benchmark
- Concept rehearsal and minimal supervision enable COOL to maintain high concept quality while preventing reasoning shortcuts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge improves concept retention by constraining the reasoning layer, reducing catastrophic forgetting
- Mechanism: Prior knowledge encodes stable relationships between concepts and labels, constraining the reasoning layer to always map concepts to correct labels, making it immune to forgetting
- Core assumption: The symbolic knowledge is correct and remains stable across tasks
- Evidence anchors:
  - [abstract] "Traditional approaches fall short: existing continual strategies ignore knowledge altogether, while stock neuro-symbolic architectures suffer from catastrophic forgetting. We show that leveraging prior knowledge by combining neuro-symbolic architectures with continual strategies does help avoid catastrophic forgetting"
  - [section 3.1] "A natural first step toward solving NeSy-CL is to introduce knowledge into the continual learning loop... the knowledge encodes the valid, stable relationship between the concepts and the labels to be prediction. This implies that predicted concepts can be always correctly mapped to a corresponding label, and that this inference step is immune from forgetting"

### Mechanism 2
- Claim: Reasoning shortcuts emerge when knowledge and training data allow multiple concept distributions to satisfy the constraints
- Mechanism: Theorem 3.2 shows that maximizing likelihood only requires concept distributions to satisfy knowledge for training examples. When knowledge doesn't uniquely identify the correct concept distribution, models can learn unintended semantics that still achieve high accuracy on training data
- Core assumption: The knowledge alone cannot uniquely identify the ground-truth concept distribution from labels
- Evidence anchors:
  - [section 3.2] "Theorem 3.2 states that, as long as the concept distribution output by the learned neural network satisfies the knowledge for each training example, the log-likelihood is maximal"
  - [example 3.3] "Only the first distribution has the intended semantics, whereas the second one is a reasoning shortcut"

### Mechanism 3
- Claim: COOL prevents reasoning shortcuts by combining concept supervision with concept rehearsal
- Mechanism: COOL adds KL divergence between current and stored concept distributions to the loss, plus concept supervision on a small subset of examples. This ensures concepts remain semantically stable across tasks
- Core assumption: A small amount of concept supervision is sufficient to identify high-quality concepts
- Evidence anchors:
  - [section 4] "COOL makes use of a small amount of concept supervision to acquire high-quality concepts and explicitly preserves them with a concept rehearsal strategy"
  - [section 5.5] "COOL is explicitly designed to acquire high-quality concepts and retain them across tasks by combining knowledge, concept rehearsal, and a modicum of concept supervision"

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why neuro-symbolic models fail in continual settings
  - Quick check question: Why do standard continual learning strategies struggle with neuro-symbolic architectures?

- Concept: Reasoning shortcuts
  - Why needed here: Key failure mode that knowledge alone cannot prevent
  - Quick check question: Under what conditions can a model achieve high training accuracy while learning incorrect concept semantics?

- Concept: Concept-level supervision
  - Why needed here: Explains why COOL needs some concept annotations despite having knowledge
  - Quick check question: Why is label supervision alone insufficient to prevent reasoning shortcuts?

## Architecture Onboarding

- Component map: Encoder -> Concept layer -> Reasoning layer -> Output
- Critical path:
  1. Extract concepts from input using encoder
  2. Apply reasoning layer with current knowledge
  3. Add KL loss between current and stored concept distributions
  4. Add supervised loss on concept-annotated examples
  5. Update buffer with new concept distributions

- Design tradeoffs:
  - Buffer size vs. concept stability: Larger buffers provide better rehearsal but require more storage
  - Supervision rate vs. shortcut prevention: More supervision prevents shortcuts better but increases annotation cost
  - Knowledge expressiveness vs. shortcut susceptibility: More restrictive knowledge prevents shortcuts but may limit applicability

- Failure signatures:
  - High label accuracy but low concept accuracy: Model has learned reasoning shortcuts
  - Rapid concept drift across tasks: Concept rehearsal weight too low or buffer too small
  - Poor OOD performance: Concepts have learned task-specific shortcuts rather than general semantics

- First 3 experiments:
  1. Test COOL on MNAdd-Seq without concept supervision to verify knowledge helps concept retention
  2. Test COOL on MNAdd-Shortcut with 0% supervision to observe reasoning shortcut failure
  3. Test COOL with increasing supervision rates on CLE4EVR to find minimum supervision needed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does COOL's performance scale with increasing task complexity and concept diversity in NeSy-CL benchmarks?
- Basis in paper: [explicit] The paper evaluates COOL on three novel benchmarks (MNAdd-Seq, MNAdd-Shortcut, CLE4EVR) with varying levels of task complexity and concept diversity, but does not systematically explore performance scaling
- Why unresolved: The paper does not investigate how COOL's performance changes as the number of tasks, concepts, or the complexity of the knowledge base increases
- What evidence would resolve it: Experiments on larger and more complex NeSy-CL benchmarks with a wider range of task and concept configurations, measuring COOL's performance relative to other methods

### Open Question 2
- Question: Can COOL be extended to handle scenarios where knowledge is partially specified or learned rather than provided upfront?
- Basis in paper: [explicit] The paper mentions that COOL works even if knowledge K(t) changes across tasks and new concepts appear over time, but does not explore scenarios where knowledge is incomplete or learned
- Why unresolved: The paper focuses on scenarios where complete knowledge is provided upfront, and does not investigate how COOL performs when knowledge is partially specified or learned from data
- What evidence would resolve it: Experiments on NeSy-CL benchmarks where knowledge is incomplete, noisy, or learned from data, comparing COOL's performance to methods that handle such scenarios

### Open Question 3
- Question: How does COOL's concept supervision requirement compare to other methods in terms of annotation effort and efficiency?
- Basis in paper: [explicit] The paper shows that COOL requires minimal concept supervision (as low as 1% of training examples) to achieve high performance, but does not compare this requirement to other methods in terms of annotation effort
- Why unresolved: The paper does not quantify the annotation effort required by COOL relative to other methods, or explore ways to further reduce the supervision needed
- What evidence would resolve it: Comparative studies measuring the annotation effort required by COOL and other methods to achieve similar performance, as well as investigations into techniques for reducing COOL's supervision requirement

## Limitations
- Theoretical guarantees about knowledge preventing catastrophic forgetting may not extend to noisy or incomplete knowledge
- Assumption that minimal concept supervision can reliably identify high-quality concepts may not hold for more complex real-world tasks
- Performance scaling with task complexity and concept diversity remains unexplored

## Confidence
- High: Knowledge constraining reasoning layers prevents catastrophic forgetting
- Medium: Reasoning shortcuts emerge when knowledge doesn't uniquely identify correct concepts
- Medium: COOL's combination of rehearsal and supervision prevents both forgetting and shortcuts

## Next Checks
1. Test COOL with deliberately incorrect knowledge to verify it doesn't introduce new failure modes
2. Compare COOL's concept quality metrics against alternative neuro-symbolic architectures on the same benchmarks
3. Evaluate COOL's performance when concept supervision is reduced below the 10% threshold to find the minimum effective rate