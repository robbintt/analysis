---
ver: rpa2
title: 3D-Aware Visual Question Answering about Parts, Poses and Occlusions
arxiv_id: '2310.17914'
source_url: https://arxiv.org/abs/2310.17914
tags:
- object
- right
- left
- questions
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 3D-aware Visual Question Answering (VQA),
  a task requiring compositional reasoning over 3D scene structure including object
  parts, 3D poses, and occlusions. The authors address this challenge by creating
  a new dataset (Super-CLEVR-3D) and a novel model (PO3D-VQA).
---

# 3D-Aware Visual Question Answering about Parts, Poses and Occlusions

## Quick Facts
- arXiv ID: 2310.17914
- Source URL: https://arxiv.org/abs/2310.17914
- Reference count: 40
- Primary result: PO3D-VQA achieves >11% accuracy improvement over existing methods on 3D-aware VQA tasks involving parts, poses, and occlusions.

## Executive Summary
This paper introduces 3D-aware Visual Question Answering (VQA), a task requiring compositional reasoning over 3D scene structure including object parts, 3D poses, and occlusions. The authors address this challenge by creating a new dataset (Super-CLEVR-3D) and a novel model (PO3D-VQA) that combines probabilistic neural-symbolic program execution with 3D generative representations. Experimental results show significant improvements over existing methods, particularly on harder questions with heavy occlusions and small parts.

## Method Summary
The PO3D-VQA model combines probabilistic neural-symbolic program execution with deep neural networks using 3D generative representations. The approach first parses the image into a 3D-aware scene representation using a novel multi-class 6D pose estimation approach based on analysis-by-synthesis (render-and-compare), then executes a parsed program on this representation. The Super-CLEVR-3D dataset extends Super-CLEVR with 3D-aware questions focusing on parts, 3D poses, and occlusions, containing over 800k questions across these categories.

## Key Results
- PO3D-VQA outperforms existing methods (FiLM, mDETR, PNSVQA) by more than 11% accuracy on Super-CLEVR-3D
- Improvements are particularly pronounced on harder questions with heavy occlusions and small parts
- The model successfully handles part questions, 3D pose questions, and occlusion questions
- Performance remains strong even on questions requiring reasoning about occluded objects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generative 3D scene parser based on render-and-compare is more robust to occlusion and small part sizes than discriminative 2D detection methods.
- Mechanism: The render-and-compare approach optimizes a likelihood model that reconstructs the image from 3D object meshes, naturally integrating a reconstruction loss that is less sensitive to partial visibility than 2D bounding box predictions.
- Core assumption: The 3D mesh model and neural texture are sufficiently accurate to enable robust reconstruction even under partial occlusion.
- Evidence anchors: [abstract] "Experimental results on Super-CLEVR-3D show that PO3D-VQA significantly outperforms existing methods (FiLM, mDETR, PNSVQA) by more than 11% in accuracy. The improvements are particularly pronounced on harder questions with heavy occlusions and small parts..." [section 4.2] "we introduce several improvements over [38] that enable it to be integrated into a 3D-aware VQA model... The results show that our model outperforms existing methods significantly, leading to an improvement in accuracy of more than 11%..."
- Break condition: If the 3D mesh models are too coarse or the neural texture fails to capture fine details, reconstruction quality degrades and robustness advantage disappears.

### Mechanism 2
- Claim: The probabilistic neural-symbolic program execution enables better compositional reasoning by incorporating confidence scores from the scene parser.
- Mechanism: Each operation in the parsed program is executed on the 3D-aware scene representation in a probabilistic manner, using probability scores from the scene parser rather than hard detections.
- Core assumption: The confidence scores from the scene parser are well-calibrated and correlate with prediction accuracy.
- Evidence anchors: [abstract] "PO3D-VQA combines two key ideas: probabilistic neural-symbolic program execution for reasoning and deep neural networks with 3D generative representations for visual recognition." [section 4.3] "Like P-NSVQA [32], each operation in the program is executed on the scene representation in a probabilistic way."
- Break condition: If the confidence scores are poorly calibrated or the scene parser frequently produces high-confidence wrong predictions, the probabilistic reasoning becomes unreliable.

### Mechanism 3
- Claim: The object-part hierarchy knowledge enables more accurate part detection than direct part detection methods.
- Mechanism: Parts are located by projecting the 3D object mesh back onto the image using predicted object poses, leveraging the known geometric relationships between objects and their parts.
- Core assumption: The 6D pose estimation is accurate enough that part projection yields reliable part locations.
- Evidence anchors: [section 4.2] "Given the predicted location and pose of each object, we project the object mesh back onto the image to get the locations for each part." [section 5.3] "By detecting part using 3D model projection, PNSVQA+Projection improves the PNSVQA results by 4%..."
- Break condition: If the 6D pose estimation is inaccurate, the projected part locations will be wrong, degrading part detection performance.

## Foundational Learning

- Concept: 6D pose estimation (3D rotation, 3D translation, depth)
  - Why needed here: Accurate 6D poses are essential for projecting 3D meshes to locate parts and reason about occlusion relationships
  - Quick check question: What are the six dimensions that fully specify an object's pose in 3D space?

- Concept: Neural symbolic program execution
  - Why needed here: Enables interpretable, compositional reasoning over the 3D scene representation
  - Quick check question: How does probabilistic execution differ from deterministic execution in a neural symbolic system?

- Concept: Analysis-by-synthesis
  - Why needed here: Provides a principled framework for robust scene parsing through reconstruction
  - Quick check question: What is the key difference between analysis-by-synthesis and discriminative object detection approaches?

## Architecture Onboarding

- Component map:
  Image input → 3D scene parser (render-and-compare + CNN classifier) → 3D-aware scene representation (objects, parts, attributes, hierarchy, occlusion) → Question parser (LSTM) → Program execution (probabilistic) → Answer output

- Critical path: Scene parsing → Program execution. The system cannot answer questions without accurate scene parsing, and program execution quality depends on scene parsing confidence scores.

- Design tradeoffs:
  - Render-and-compare vs. discriminative detection: Better occlusion robustness vs. higher computational cost
  - Probabilistic vs. deterministic execution: Better handling of uncertainty vs. simpler implementation
  - Joint 6D pose and category estimation vs. separate stages: Better handling of unknown categories vs. simpler optimization

- Failure signatures:
  - Low accuracy on occlusion questions: Likely scene parser fails to handle occlusions well
  - Low accuracy on small part questions: Likely mesh projection is inaccurate or parts are too small to project reliably
  - Inconsistent performance across question types: Likely confidence calibration issues in scene parser

- First 3 experiments:
  1. Ablation: Compare render-and-compare scene parser vs. Mask R-CNN baseline on occlusion robustness
  2. Ablation: Compare probabilistic vs. deterministic program execution on compositional reasoning accuracy
  3. Error analysis: Visualize scene parser outputs (6D poses, part projections) to identify failure modes

## Open Questions the Paper Calls Out

- Question: Can the 3D-Aware VQA model be extended to real-world images beyond the synthetic Super-CLEVR-3D dataset?
  - Basis in paper: [inferred] The paper mentions that extending the model to real-world images is a meaningful research direction but notes challenges like lack of 3D annotations and highly articulated categories.
  - Why unresolved: The current model and dataset are synthetic, and real-world images have different characteristics and challenges.
  - What evidence would resolve it: Successful application and testing of the model on real-world images with 3D annotations, demonstrating comparable performance to synthetic data.

- Question: How can the model be improved to handle occlusion more effectively, especially for objects of the same category?
  - Basis in paper: [explicit] The paper discusses limitations of the 3D-NMS approach in dense scenes with objects of the same category and mentions the need for better handling of occlusions.
  - Why unresolved: Current methods struggle with accurately detecting and reasoning about occlusions in complex scenes.
  - What evidence would resolve it: Development and testing of new techniques or algorithms that improve occlusion handling, leading to higher accuracy in 3D-Aware VQA tasks.

- Question: What is the impact of incorporating z-direction variability in the dataset on the model's performance?
  - Basis in paper: [explicit] The paper discusses the potential extension of the dataset to include z-direction variability and tests the model on a subset with height and depth questions.
  - Why unresolved: The current dataset only considers objects on the same surface, and the impact of 3D spatial relationships is not fully explored.
  - What evidence would resolve it: Creation of a more comprehensive dataset with z-direction variability and evaluation of the model's performance on this dataset, showing improvements in handling 3D spatial relationships.

## Limitations
- The Super-CLEVR-3D dataset is synthetic and may not fully capture real-world scene complexity
- Current focus on vehicles limits generalizability to more diverse object categories
- Performance on highly occluded objects of the same category remains challenging
- The model requires 3D annotations that are not available in most real-world datasets

## Confidence
- Overall performance gains on Super-CLEVR-3D: High confidence
- Effectiveness of render-and-compare for occlusion handling: Medium confidence
- Benefits of probabilistic execution: Medium confidence
- Generalizability to real-world images: Low confidence

## Next Checks
1. **Ablation study**: Compare render-and-compare scene parser vs. a strong 2D baseline (e.g., Mask R-CNN) on occlusion handling and small part detection, using identical downstream program execution.

2. **Real-world transfer**: Evaluate PO3D-VQA on a real-world 3D VQA dataset (if available) or adapt Super-CLEVR-3D with more diverse object categories to assess generalization.

3. **Failure mode analysis**: Conduct systematic error analysis on the 11% accuracy gap between PO3D-VQA and existing methods, categorizing errors by question type, occlusion level, and part size to identify remaining limitations.