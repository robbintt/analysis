---
ver: rpa2
title: 'A Semi-Automated Solution Approach Recommender for a Given Use Case: a Case
  Study for AI/ML in Oncology via Scopus and OpenAI'
arxiv_id: '2307.04573'
source_url: https://arxiv.org/abs/2307.04573
tags:
- found
- learning
- methods
- 'true'
- artificial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents SARBOLD-LLM, a semi-automated tool for selecting
  AI/ML solution methods for any given use case. The tool uses keyword selection,
  Scopus API queries, and OpenAI (GPT 3.5) to extract and analyze methods from literature.
---

# A Semi-Automated Solution Approach Recommender for a Given Use Case: a Case Study for AI/ML in Oncology via Scopus and OpenAI

## Quick Facts
- arXiv ID: 2307.04573
- Source URL: https://arxiv.org/abs/2307.04573
- Reference count: 40
- Primary result: Tool achieves precision 0.68, recall 0.9, F1-score 0.77 for AI/ML method extraction in oncology case study

## Executive Summary
SARBOLD-LLM is a semi-automated tool that helps practitioners and engineers identify relevant AI/ML solution methods for any given use case by combining keyword-based literature retrieval, OpenAI API analysis, and performance metrics. The tool uses a three-level keyword selection scheme to query Scopus, extracts method names from abstracts using GPT-3.5, and calculates relevancy and popularity scores to guide method selection. Tested on AI applications in oncology, it demonstrates reasonable performance while highlighting the importance of systematic literature analysis for method discovery.

## Method Summary
The tool operates through three modules: (1) keyword selection using a two-domain, three-level setup to construct targeted Scopus queries, (2) method extraction from retrieved abstracts using OpenAI API with a custom prompt, and (3) performance evaluation and post-analysis including sensitivity testing. Papers are retrieved via Scopus API, filtered manually, and processed through GPT-3.5 to identify AI methods mentioned. The system calculates relevancy scores based on keyword matching and popularity scores using citation counts adjusted for publication age, providing users with ranked method recommendations.

## Key Results
- Achieved precision of 0.68, recall of 0.9, and F1-score of 0.77 in oncology case study
- Successfully identified 25 related papers from 67 filtered results, demonstrating moderate relevance overlap
- Demonstrated ability to extract method names like "deep learning" and "CNN" from abstracts using OpenAI API
- Showed sensitivity to keyword selection and prompt variations through systematic analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The keyword selection scheme with three specification levels enables targeted literature retrieval and method extraction.
- Mechanism: By structuring keywords into general (level 1), expanding (level 2), and detailed (level 3) categories, the tool constructs precise Scopus queries that filter papers relevant to the user's use case while avoiding overly broad or narrow results.
- Core assumption: The human-defined keywords accurately represent the problem and solution domains, and the combination of levels yields a sufficiently focused paper pool.
- Evidence anchors:
  - [abstract]: "Determining keywords systematically from the use case by a two-domain, three-level setup."
  - [section 3.1]: "three specification levels (a general, an expended, and a detailed one) are applied to the given problem and the searched solutions."
  - [corpus]: Weak evidence; only 25 related papers found, average neighbor FMR=0.413 suggests moderate relevance overlap.
- Break condition: If keywords are poorly chosen or levels are imbalanced (too many general or too many expanding), the resulting paper pool may be irrelevant or too sparse for method extraction.

### Mechanism 2
- Claim: OpenAI API with a tailored prompt extracts AI method names from abstracts with reasonable precision and recall.
- Mechanism: The prompt "Extract the names of the artificial intelligence approaches used from the following text" guides GPT-3.5 to identify method mentions, which are then compared against manually curated ground truth for performance evaluation.
- Core assumption: The GPT-3.5 model reliably recognizes AI method terminology within the context of titles and abstracts, and the prompt format is sufficiently specific.
- Evidence anchors:
  - [abstract]: "Extracting AI methods automatically from Scopus search results by using OpenAI API."
  - [section 3.2]: "the tool inputs the title and abstract information to OpenAI and outputs the AI approaches used in each article."
  - [section 4.4.1]: Provides precision 0.68, recall 0.9, F1-score 0.77 for oncology case study.
- Break condition: If the prompt is ambiguous or the model's training corpus lacks coverage of niche AI methods, false positives/negatives increase and performance degrades.

### Mechanism 3
- Claim: Relevancy and popularity metrics guide users toward methods most applicable and well-researched for their use case.
- Mechanism: Relevancy counts matching keywords in each paper; popularity uses citation count adjusted by publication age, highlighting methods with both domain fit and research traction.
- Core assumption: Higher relevancy implies better fit to the use case, and higher popularity reflects community validation and practical adoption.
- Evidence anchors:
  - [abstract]: "providing additional information about their uses in the literature to derive decision-making insights."
  - [section 3.2]: "The relevancy metrics count the number of unique level 2 and 3 keywords... The popularity metric is used to know the research interest of a paper and its methods."
  - [section 4.6]: Demonstrates method selection based on these metrics, e.g., prioritizing deep learning methods in oncology image processing.
- Break condition: If the use case is highly novel, literature may be sparse, making relevancy scores low and popularity misleading; user must manually validate trends.

## Foundational Learning

- Concept: Keyword selection and query construction
  - Why needed here: Accurate keywords determine the initial paper pool; poor selection yields irrelevant or incomplete method sets.
  - Quick check question: Can you construct a Scopus query for "oncology" and "deep learning" that returns papers published after 2018?

- Concept: Text completion models (GPT-3.5)
  - Why needed here: The tool relies on GPT-3.5 to identify AI method names within abstracts; understanding prompt engineering improves extraction accuracy.
  - Quick check question: What prompt format would you use to ask GPT-3.5 to list only AI methods from a given abstract, excluding general terms?

- Concept: Precision, recall, and F1-score metrics
  - Why needed here: These metrics quantify the accuracy of OpenAI's method extraction compared to manual ground truth, guiding tool refinement.
  - Quick check question: If OpenAI found 10 methods, 7 were correct, and 3 were missed by manual search, what are the precision, recall, and F1-score?

## Architecture Onboarding

- Component map: Keyword Selection -> Scopus Query Builder -> Scopus API -> Paper Pool -> OpenAI Prompt Engine -> Method Extraction -> Relevancy/Popularity Calculator -> Trend Analyzer -> Sensitivity Analyzer

- Critical path:
  1. Manual keyword selection (problem + solution domains)
  2. Automated Scopus query and result filtering
  3. Automated OpenAI method extraction
  4. Performance evaluation and post-analysis

- Design tradeoffs:
  - Manual keyword selection ensures domain fit but adds user effort; automating this could speed onboarding but risk lower precision.
  - Using only titles/abstracts limits method extraction to explicitly mentioned terms; full-text analysis could improve recall but increase cost.
  - Relying on OpenAI API simplifies implementation but introduces vendor lock-in and potential rate limits.

- Failure signatures:
  - Too few papers after query: keyword selection likely too narrow or Scopus API constraints hit.
  - Low precision/recall: OpenAI prompt may be too generic; consider adding method-specific keywords or examples.
  - Relevancy scores near zero: Keywords missing in abstracts, possibly due to reliance on Scopus INDEXTERMS; manual inspection needed.

- First 3 experiments:
  1. Run the initial oncology case study query, verify 67 relevant papers remain after manual filtering, and extract methods via OpenAI.
  2. Change level 2 keyword from "image processing" to "natural language processing" and compare method sets and trends.
  3. Modify the OpenAI prompt to include example method names; measure change in precision/recall.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SARBOLD-LLM compare to human experts when dealing with very specific or novel use cases?
- Basis in paper: [inferred] The paper mentions that the tool's applicability depends on the objective of the application and notes that it will not fare well when dealing with entirely new solution approach schemes.
- Why unresolved: The paper does not provide empirical comparisons between the tool's performance and human experts for highly specific or novel use cases.
- What evidence would resolve it: Conducting controlled experiments comparing the tool's recommendations to those of human experts for a range of specific and novel use cases would provide evidence of its performance relative to human judgment.

### Open Question 2
- Question: How sensitive is SARBOLD-LLM to variations in the prompt used for OpenAI API?
- Basis in paper: [explicit] The paper discusses sensitivity analyses for both Scopus and OpenAI, including testing different prompts for OpenAI API.
- Why unresolved: While the paper presents results for a few prompt variations, it does not exhaustively explore the full range of possible prompt variations or their impact on performance.
- What evidence would resolve it: Systematically testing a wide range of prompt variations and analyzing their effects on the tool's performance would provide a comprehensive understanding of its sensitivity to prompt changes.

### Open Question 3
- Question: Can SARBOLD-LLM be effectively extended to analyze characteristics other than solution approaches, such as problem formulations or data characteristics?
- Basis in paper: [explicit] The paper mentions that the tool can potentially investigate any arbitrary characteristic of the literature, not just solution approaches.
- Why unresolved: The paper does not provide any empirical evidence or detailed exploration of how the tool could be adapted to analyze other characteristics of the literature.
- What evidence would resolve it: Implementing and testing the tool for analyzing various characteristics of the literature, such as problem formulations or data characteristics, would demonstrate its potential for extension beyond solution approaches.

## Limitations
- Manual keyword selection remains a bottleneck requiring domain expertise
- Performance heavily dependent on Scopus API access and coverage
- OpenAI API integration introduces vendor lock-in and potential rate limits
- Ground truth creation for method extraction remains labor-intensive
- Limited validation beyond single oncology case study

## Confidence
- Mechanism 1 (Keyword-based literature retrieval): Medium - Relies on human expertise for keyword selection
- Mechanism 2 (GPT-3.5 method extraction): Medium - Performance varies with prompt engineering and domain specificity
- Mechanism 3 (Relevancy/popularity metrics): Medium - Assumes literature density and relevance align with use case needs

## Next Checks
1. Test the tool on a non-oncology use case (e.g., finance or manufacturing) to assess domain transferability
2. Compare GPT-3.5 method extraction performance against other text completion models or fine-tuned domain-specific models
3. Evaluate the impact of using full-text analysis versus abstracts-only on method extraction recall and precision