---
ver: rpa2
title: 'When mitigating bias is unfair: multiplicity and arbitrariness in algorithmic
  group fairness'
arxiv_id: '2302.07185'
source_url: https://arxiv.org/abs/2302.07185
tags:
- fairness
- methods
- bias
- fair
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the arbitrariness in algorithmic group
  fairness by comparing multiple bias mitigation approaches. The authors introduce
  the FRAME framework to evaluate debiasing through five dimensions: impact size,
  change direction, decision rates, affected subpopulations, and neglected subpopulations.'
---

# When mitigating bias is unfair: multiplicity and arbitrariness in algorithmic group fairness

## Quick Facts
- arXiv ID: 2302.07185
- Source URL: https://arxiv.org/abs/2302.07185
- Reference count: 19
- Key outcome: Different bias mitigation approaches can achieve similar fairness metrics while affecting different individuals and targeting different subpopulations, revealing arbitrariness in the fairness process itself.

## Executive Summary
This paper investigates the arbitrariness inherent in algorithmic group fairness by comparing multiple bias mitigation approaches. The authors introduce the FRAME framework to evaluate debiasing through five dimensions: impact size, change direction, decision rates, affected subpopulations, and neglected subpopulations. Using five representative datasets and multiple fairness approaches (pre-processing, in-processing, and post-processing methods), they find that different debiasing strategies can achieve similar fairness and accuracy metrics while affecting significantly different numbers of individuals and targeting different subpopulations. The study reveals that even when multiple runs of the same method are considered, there is considerable instability in which individuals are targeted. These findings highlight the limitations of current group fairness metrics and demonstrate the inherent arbitrariness in the debiasing process, raising concerns about the fairness of fairness itself.

## Method Summary
The study compares five bias mitigation approaches (LFR, Adversarial Debiasing, ROC, Threshold Optimizer, and Random Baseline) across five datasets using neural networks as biased models. The FRAME framework evaluates differences along five dimensions: impact size (number of affected individuals), change direction (positive/negative changes), decision rates, affected subpopulations, and neglected subpopulations. Methods are trained to achieve comparable fairness and accuracy metrics, then analyzed for differences in which individuals are affected. The analysis includes measuring stability across multiple runs and using IOU calculations to quantify overlap between methods.

## Key Results
- Different fairness approaches achieve similar accuracy and fairness metrics while affecting different numbers of individuals (up to 10% variation in impact size)
- There is low overlap (low IOU values) between methods in which individuals they affect, even when targeting similar populations
- Multiple runs of the same method produce different affected populations, with only 4.8% of instances consistently targeted across runs
- The arbitrariness is most pronounced in datasets with strong feature-class correlations and weak correlations with sensitive attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group fairness metrics create arbitrariness because they are underspecified - multiple models can achieve identical fairness scores while making different predictions on individual instances.
- Mechanism: Group fairness metrics like Demographic Parity and Equalized Odds only constrain aggregate statistics (e.g., equal acceptance rates across groups) without specifying which individuals should be affected. This allows multiple valid solutions where the same fairness metric can be achieved through different selections of individuals to change.
- Core assumption: The problem of enforcing group fairness is mathematically underspecified, meaning there are multiple valid solutions that satisfy the same fairness constraints.
- Evidence anchors: [abstract] "This paper investigates the arbitrariness in algorithmic group fairness by comparing multiple bias mitigation approaches"; [section] "Although numerous fairness metrics have been proposed, we describe in this section two of the most popular ones metrics, which are the ones considered in this study"

### Mechanism 2
- Claim: Different bias mitigation strategies employ fundamentally different approaches to achieve the same fairness goals, leading to different affected populations.
- Mechanism: Pre-processing methods modify the data distribution, in-processing methods use adversarial training to remove sensitive attribute information, and post-processing methods adjust decision thresholds. These different algorithmic approaches naturally lead to targeting different subsets of the population even when achieving similar fairness metrics.
- Core assumption: The choice of bias mitigation strategy fundamentally affects which individuals are targeted for prediction changes.
- Evidence anchors: [section] "We choose one approach from each category described in Section 2.1.2). Additionally, in order to ensure that the methods we choose are indeed comparable, we split the chosen methods by metric optimized"; [section] "There are clear differences in impact size within each dataset, with Demographic Parity methods differing by up to 10%"

### Mechanism 3
- Claim: Even when using the same bias mitigation algorithm, retraining produces different affected populations due to the stochastic nature of the optimization process.
- Mechanism: Neural network training involves random initialization, stochastic gradient descent, and other sources of randomness. Even with identical hyperparameters, different training runs can converge to different local optima, leading to different individuals being affected by the fairness adjustments.
- Core assumption: The optimization process for fairness is non-convex and has multiple local optima that satisfy the same fairness constraints.
- Evidence anchors: [section] "We additionally examine how stable the set of targeted instances for one method ∆g is across multiple runs"; [section] "Figure 2 shows the results obtained for the Dutch dataset. Here we see that, for the example of LFR, though the method targets on average 19.87% of the instances, those that are aﬀected in every run account for only 4.8% of the dataset"

## Foundational Learning

- Concept: Predictive multiplicity - the phenomenon where multiple models can achieve similar performance metrics while making different predictions on individual instances.
  - Why needed here: This paper's findings about different bias mitigation approaches affecting different individuals is a specific instance of predictive multiplicity in the fairness domain.
  - Quick check question: If two models achieve 90% accuracy and satisfy demographic parity, must they make the same predictions on all individuals?

- Concept: Underspecification in machine learning - when a problem admits multiple valid solutions that satisfy the same performance metrics.
  - Why needed here: The paper argues that group fairness metrics create an underspecified problem, allowing multiple different "fair" models.
  - Quick check question: What makes a machine learning problem underspecified, and how does this relate to the fairness problem?

- Concept: Bias mitigation strategies categorization - pre-processing (data modification), in-processing (training modification), and post-processing (prediction modification).
  - Why needed here: The paper compares methods from each category to understand how different approaches affect individuals differently.
  - Quick check question: How do pre-processing, in-processing, and post-processing methods fundamentally differ in their approach to fairness?

## Architecture Onboarding

- Component map: Data preprocessing (scaling, standardization) -> Biased model training (neural network) -> Fair model training (pre-processing/in-processing/post-processing) -> FRAME evaluation (impact size, change direction, decision rates, affected subpopulations, neglected subpopulations) -> Comparative analysis (IOU, PLS-DA)
- Critical path: Biased model → Fair model training → Performance evaluation → FRAME analysis → Comparative analysis
- Design tradeoffs: Using neural networks for fair models limits compatibility with some pre-processing methods like LFR but ensures fair adversarial learning can be applied; choosing specific fairness metrics (DP, EO) focuses the analysis but may miss other fairness definitions.
- Failure signatures: Models failing to achieve comparable fairness scores across methods; extreme instability in affected populations across runs; IOU values close to 1 (indicating all methods affect the same individuals).
- First 3 experiments:
  1. Verify that all fair models achieve similar accuracy and fairness scores on validation set before comparative analysis
  2. Calculate IOU between each pair of fair models to quantify overlap in affected individuals
  3. Run each bias mitigation method multiple times to measure stability of affected populations across runs

## Open Questions the Paper Calls Out

- Can we develop more specific group fairness metrics that reduce the arbitrariness in bias mitigation?
- How can we design bias mitigation algorithms that deliberately target specific populations rather than relying on arbitrary selection?
- What is the relationship between model instability in bias mitigation and the reliability of post-hoc interpretability methods?

## Limitations

- The study focuses only on two fairness metrics (Demographic Parity and Equalized Odds), potentially missing arbitrariness in other fairness definitions
- Neural network architecture choice may not be optimal for all pre-processing methods like LFR, limiting fair comparisons
- The analysis reveals instability in affected populations but doesn't investigate real-world consequences of different debiasing choices

## Confidence

- **High Confidence**: The core finding that different fairness approaches affect different individuals while achieving similar metrics is well-supported by quantitative evidence (IOU analysis, impact size comparisons).
- **Medium Confidence**: The interpretation that this arbitrariness stems from underspecification in group fairness metrics is theoretically sound but requires more empirical validation across additional metrics and datasets.
- **Low Confidence**: Claims about the fairness implications of this arbitrariness are largely theoretical, as the paper does not investigate real-world consequences of different debiasing choices.

## Next Checks

1. **Cross-metric validation**: Test whether arbitrariness persists when using additional fairness metrics like Equal Opportunity Difference and Disparate Impact.
2. **Stability threshold analysis**: Determine minimum number of training runs needed to achieve stable estimates of affected populations for each method.
3. **Real-world impact study**: Evaluate whether different fairness approaches that achieve similar metrics produce meaningfully different outcomes for affected individuals in practice.