---
ver: rpa2
title: 'XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering'
arxiv_id: '2312.03567'
source_url: https://arxiv.org/abs/2312.03567
tags:
- pairs
- xaiqa
- synthetic
- medical
- hardest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XAIQA, a novel method for generating synthetic
  question-answer (QA) pairs from medical documents to improve extractive QA models.
  XAIQA leverages a text classification model explainer to identify the most informative
  sentences in medical documents that can serve as answers to questions about medical
  concepts (e.g., diagnoses, procedures) encoded in the document labels.
---

# XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering

## Quick Facts
- **arXiv ID:** 2312.03567
- **Source URL:** https://arxiv.org/abs/2312.03567
- **Reference count:** 40
- **Primary result:** XAIQA generates synthetic QA pairs from medical documents using classification model explainers, identifying 2.2x more semantic matches and 3.8x more clinical abbreviations than similarity-based approaches, and improving GPT-4's extractive QA performance.

## Executive Summary
This paper introduces XAIQA, a novel method for generating synthetic question-answer (QA) pairs from medical documents to improve extractive QA models. XAIQA leverages a text classification model explainer to identify the most informative sentences in medical documents that can serve as answers to questions about medical concepts (e.g., diagnoses, procedures) encoded in the document labels. The generated QA pairs can be used to augment existing QA datasets or as few-shot examples for large language models (LLMs) like GPT-4. In an expert evaluation with two physicians, XAIQA identified 2.2x more semantic matches and 3.8x more clinical abbreviations compared to two baseline methods using sentence transformers. In an ML evaluation, adding XAIQA-generated QA pairs to GPT-4 prompts improved performance on extractive QA, particularly on difficult questions with low lexical overlap between questions and answers.

## Method Summary
XAIQA uses a classification model explainer (MSP) to identify the most informative sentences in medical documents for each label (e.g., ICD code). It then generates QA pairs where the question is the label description and the answer is the informative sentence. The method involves fine-tuning a Longformer model for multi-label ICD code classification on MIMIC-III discharge summaries, using MSP to identify important sentences per label, and optionally post-processing with ClinicalBERT sentence segmentation to shorten long answers. The generated QA pairs are used as few-shot examples to improve GPT-4's extractive QA performance.

## Key Results
- XAIQA identified 2.2x more semantic matches and 3.8x more clinical abbreviations than baseline methods in an expert evaluation with two physicians.
- Adding XAIQA-generated QA pairs to GPT-4 prompts improved performance on extractive QA, particularly on difficult questions with low lexical overlap.
- XAIQA outperformed zero-shot GPT-4 on the full emrQA relations test set and the hardest 25% and 50% of examples, with a performance improvement of about 5%.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classification model explainers identify text spans most predictive of medical codes, which correspond to answer spans for QA pairs about those codes.
- Mechanism: The explainer (MSP) perturbs sentences in a document and measures the change in classification probability for each label. Sentences causing the largest probability drops when masked are deemed most informative for that label.
- Core assumption: Text spans that are most predictive of a medical code can serve as correct answers to questions about that code.
- Evidence anchors:
  - [abstract] "Our method uses the idea of a classification model explainer to generate questions and answers about medical concepts corresponding to medical codes."
  - [section 4.1] "We present Algorithm 1 based on the intuition that whatever information in a document is useful for classifying document labels can also be used to form QA pairs, where the label descriptions form the Q's and the predictive text in the document form the A's."
  - [corpus] Weak - no direct evidence in cited papers that this intuition holds for medical QA.
- Break condition: If the most predictive text for a code is too indirect or requires reasoning beyond simple span extraction, the generated QA pairs will be incorrect or unhelpful.

### Mechanism 2
- Claim: Generating QA pairs from code descriptions and their most informative sentences creates pairs with higher semantic and abbreviation matches than similarity-based approaches.
- Mechanism: By using the explainer's importance scores rather than cosine similarity, the method can capture semantic relationships between questions (code descriptions) and answers (informative sentences) that don't share many keywords.
- Core assumption: Semantic and abbreviation matches are more valuable than lexical matches for training QA models, especially on difficult questions.
- Evidence anchors:
  - [abstract] "In an expert evaluation with two physicians, our method identifies 2 .2× more semantic matches and 3 .8× more clinical abbreviations than two popular approaches that use sentence transformers to create QA pairs."
  - [section 5.1] "XAIQA produced significantly more semantic matches than the baselines (2.2× more than ClinicalBERT with p < 0.001). Our method also identified significantly more abbreviations than both baselines (3.8× more than ClinicalBERT with p < 0.001)."
  - [corpus] Moderate - sentence transformer approaches are well-established, but the specific claim about semantic/abbreviation advantages is novel.
- Break condition: If the classifier explainer focuses on very general text that isn't specific to the medical code, the semantic matches may be too broad to be useful.

### Mechanism 3
- Claim: Adding synthetic QA pairs generated by XAIQA improves GPT-4's performance as an extractive QA model, particularly on difficult questions with low lexical overlap.
- Mechanism: The synthetic pairs provide few-shot examples that demonstrate the semantic relationships between questions and answers, helping GPT-4 learn to extract answers even when they don't share many keywords with the question.
- Core assumption: GPT-4 can learn from these synthetic examples and generalize to similar but unseen questions in the test set.
- Evidence anchors:
  - [abstract] "In an ML evaluation, adding our QA pairs improves performance of GPT-4 as an extractive QA model, including on difficult questions."
  - [section 5.2] "XAIQA provides statistically significant performance improvements (confidence intervals do not overlap) over zero-shot GPT-4 on the full emrQA relations test set as well as the hardest 25 and 50% of examples. Performance improvement is about 5%."
  - [corpus] Moderate - few-shot prompting is well-established, but the specific claim about synthetic pairs helping with low lexical overlap is novel.
- Break condition: If the synthetic examples are too different from the test questions, or if GPT-4 already has sufficient few-shot examples, adding more may not help or could even hurt performance.

## Foundational Learning

- **Concept:** Text classification explainers (like MSP)
  - **Why needed here:** To identify the most informative sentences for each medical code, which become the answer spans for QA pairs.
  - **Quick check question:** What does MSP do differently from simple feature importance methods like SHAP?

- **Concept:** Sentence transformers and embeddings
  - **Why needed here:** To compare the similarity between code descriptions and sentences when post-processing answer spans.
  - **Quick check question:** How do sentence transformers differ from regular word embeddings?

- **Concept:** Few-shot prompting with large language models
  - **Why needed here:** To evaluate if the synthetic QA pairs can improve GPT-4's extractive QA performance by providing relevant examples.
  - **Quick check question:** What are the key considerations when designing few-shot prompts for LLMs?

## Architecture Onboarding

- **Component map:** Medical documents → Longformer classifier → MSP explainer → ClinicalBERT post-processor → QA pairs
- **Critical path:** Document → Classifier → Explainer → Post-processor → QA pairs
- **Design tradeoffs:**
  - Using sentences as text blocks for MSP vs. fixed-length blocks: Sentences provide more natural answer spans but may miss multi-sentence information.
  - Post-processing with ClinicalBERT vs. using raw explainer output: Post-processing can shorten answers but may lose some context.
  - Number of synthetic pairs to generate: More pairs provide more diversity but may include lower quality pairs.
- **Failure signatures:**
  - Classifier performs poorly on medical codes → Explainer identifies wrong sentences → QA pairs are incorrect
  - Explainer focuses on too general text → QA pairs have correct codes but unhelpful answers
  - Post-processor segments answers incorrectly → QA pairs have incomplete or fragmented answers
- **First 3 experiments:**
  1. Evaluate classifier performance on a held-out set of medical documents to ensure it can accurately predict ICD codes.
  2. Generate a small set of synthetic QA pairs and manually check a sample for correctness and semantic match quality.
  3. Test the full pipeline by generating QA pairs for a subset of documents, using them as few-shot examples for GPT-4, and measuring performance on a held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal ratio of base to synthetic QA pairs for improving LLM in-context learning performance?
- **Basis in paper:** [explicit] The paper states "Future work is required to analyze optimal base to synthetic ratios for both LLM ICL and LM fine-tuning" and shows that at a 2:1 ratio, MPNET offers significant gains while at a 5:1 ratio, XAIQA with post-processing outperforms no synthetic data.
- **Why unresolved:** The paper only tests a limited set of ratios (1:1, 2:1, 5:1) and shows that performance varies depending on the method and evaluation metric. The optimal ratio likely depends on factors like the quality of the base data, the difficulty of the questions, and the specific LLM being used.
- **What evidence would resolve it:** Systematic experiments varying the ratio of base to synthetic QA pairs across a wider range of values (e.g., 1:1, 2:1, 3:1, 5:1, 10:1) for different LLMs and QA datasets, measuring performance on both easy and hard questions.

### Open Question 2
- **Question:** How does the performance of XAIQA-generated QA pairs compare to other methods for generating synthetic data, such as using generative models or data augmentation techniques?
- **Basis in paper:** [inferred] The paper only compares XAIQA to two sentence transformer baselines (ClinicalBERT and MPNET) and does not explore other methods for generating synthetic QA data. The authors mention that "beyond our experiments presented here, we believe XAIQA can turn any document-level classification dataset into an extractive QA dataset" but do not provide empirical evidence for this claim.
- **Why unresolved:** The paper focuses specifically on XAIQA and does not provide a comprehensive comparison to other state-of-the-art methods for generating synthetic QA data. It is unclear how XAIQA would perform compared to more advanced techniques like generative models or other data augmentation approaches.
- **What evidence would resolve it:** Experiments comparing XAIQA to other methods for generating synthetic QA pairs, such as using generative models (e.g., GPT-3, T5) fine-tuned on QA datasets or data augmentation techniques like back-translation or paraphrasing, on the same evaluation metrics and datasets used in the paper.

### Open Question 3
- **Question:** Can XAIQA be extended to generate QA pairs for other types of medical concepts beyond diagnoses, such as procedures, medications, or laboratory results?
- **Basis in paper:** [inferred] The paper focuses on generating QA pairs for diagnoses (ICD codes) but mentions that "XAIQA can turn any document-level classification dataset into an extractive QA dataset" and that the method could be applied to "any medical concept corresponding to medical codes." However, the paper does not provide evidence for this claim or explore the performance of XAIQA on other types of medical concepts.
- **Why unresolved:** The paper only evaluates XAIQA on a single type of medical concept (diagnoses) and does not demonstrate its generalizability to other types of concepts that are commonly found in medical records. It is unclear whether the method would perform equally well on concepts with different characteristics, such as procedures (which may have more specific and unambiguous descriptions) or laboratory results (which may require more complex reasoning to generate questions and answers).
- **What evidence would resolve it:** Experiments applying XAIQA to generate QA pairs for other types of medical concepts (e.g., procedures, medications, laboratory results) using appropriate classification datasets and evaluating the performance of the generated pairs on relevant QA datasets or through expert annotation.

## Limitations
- The paper's claims about XAIQA's superiority over similarity-based methods rely heavily on the expert evaluation, which involved only two physicians and focused on a small subset of generated QA pairs.
- The paper doesn't address potential biases in the MIMIC-III dataset or how they might affect the generated QA pairs.
- The computational cost of generating large numbers of synthetic pairs is not discussed.

## Confidence
- **High Confidence:** The mechanism of using a classification model explainer to identify informative text spans is well-established in the explainable AI literature.
- **Medium Confidence:** The expert evaluation methodology is sound, but the small sample size and single domain limit generalizability. The ML evaluation results are promising but may not hold for other QA models or datasets.
- **Low Confidence:** The paper doesn't provide examples of the generated QA pairs, making it difficult to assess their quality or potential limitations.

## Next Checks
1. Replicate the expert evaluation with a larger, more diverse group of medical professionals across multiple medical domains to assess generalizability of the semantic and abbreviation match improvements.
2. Generate a sample of 100+ QA pairs using XAIQA and manually evaluate them for correctness, usefulness, and potential biases or errors in the answer spans.
3. Test XAIQA-generated pairs on a different extractive QA dataset (e.g., BioASQ or SQuAD) to see if the improvements in GPT-4 performance hold for other models and domains.