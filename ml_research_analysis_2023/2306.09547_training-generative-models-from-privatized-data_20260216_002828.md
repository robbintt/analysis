---
ver: rpa2
title: Training generative models from privatized data
arxiv_id: '2306.09547'
source_url: https://arxiv.org/abs/2306.09547
tags:
- data
- distribution
- mechanism
- privacy
- privatized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for training generative adversarial
  networks (GANs) from locally differentially private data. The key idea is to use
  entropic regularization of the Wasserstein distance when training the GAN on privatized
  samples.
---

# Training generative models from privatized data

## Quick Facts
- arXiv ID: 2306.09547
- Source URL: https://arxiv.org/abs/2306.09547
- Authors: 
- Reference count: 40
- Key outcome: Proposes a method for training GANs from locally differentially private data using entropic regularization of Wasserstein distance to denoise and recover the original data distribution.

## Executive Summary
This paper addresses the challenge of training generative models on locally differentially private (LDP) data. The authors propose using entropic regularization of the Wasserstein distance when training GANs on privatized samples. This approach enables the generator to learn the original (unprivatized) data distribution despite only having access to noisy, privatized data. The method combines the strengths of optimal transport and differential privacy to achieve both privacy protection and high-quality generative modeling.

## Method Summary
The method involves training a GAN where the generator learns from locally privatized data samples. Entropic regularization is applied to the Wasserstein distance between generated and privatized data distributions. For Laplace and Gaussian LDP mechanisms, the authors prove that the optimal solution recovers the true data distribution. The training uses the Sinkhorn-Knopp algorithm to compute the regularized optimal transport plan. The approach theoretically and empirically demonstrates that the regularization helps denoise the privatized data, allowing the generator to learn the original distribution while maintaining privacy guarantees.

## Key Results
- Proves that the optimal solution under entropic regularization recovers the true data distribution for Laplace and Gaussian mechanisms
- Provides sample complexity bounds showing convergence at the parametric rate of O(1/√n)
- Demonstrates effectiveness on synthetic data (2D manifolds) and MNIST, showing meaningful generation from highly privatized data
- Shows that entropic regularization uniquely mitigates both regularization bias and privacy noise effects

## Why This Works (Mechanism)

### Mechanism 1
- Entropic regularization of Wasserstein distance denoises the privatized data distribution so the generator can recover the original distribution
- The additive noise from LDP mechanisms (Laplace/Gaussian) is itself a continuous distribution, and entropic regularization pushes the coupling toward independence between generated output and noise term
- Core assumption: privatization mechanism uses additive noise that is independent of the data
- Evidence: abstract states this enables learning the unprivatized distribution, section 3 proves PG*(Z) = PX for Laplace/Gaussian mechanisms
- Break condition: if noise is not additive, independent, or from assumed family

### Mechanism 2
- Regularization bias from entropic OT compensates for bias introduced by privacy noise
- Without regularization, generator matches noisy distribution PY; entropic term adds smoothness bias that counteracts noise bias
- Core assumption: generator class G is rich enough to represent PX and regularization strength λ is tuned to noise scale
- Evidence: abstract mentions mitigation of both biases, section 3 proves optimal solution recovers PX, corpus includes convergence rate bounds as λ→0
- Break condition: if λ too large (oversmoothing) or too small (noise dominates)

### Mechanism 3
- Sample complexity remains at parametric rate O(1/√n) despite privacy noise, breaking curse of dimensionality
- Entropic regularization makes optimization strongly convex, yielding faster statistical convergence than unregularized OT (Ω(n^−2/d))
- Privacy noise adds constant factor in σ² but doesn't change n^−1/2 rate
- Core assumption: data and noise are sub-Gaussian, generator set is Lipschitz and star-shaped
- Evidence: abstract mentions parametric rate, section 3 gives Theorem 2 with O(σ²n^−1/2) bound, corpus includes refined statistical bounds for EOT
- Break condition: if data is heavy-tailed or generator set not Lipschitz/star-shaped

## Foundational Learning

- **Concept:** Local Differential Privacy (LDP)
  - Why needed: Method assumes data is privatized locally before centralized processing
  - Quick check: What is privacy parameter ϵ in Laplace mechanism and how does it relate to noise scale Δ/ϵ?

- **Concept:** Wasserstein Distance and Optimal Transport
  - Why needed: Loss function is based on p-Wasserstein distance
  - Quick check: How does entropic regularization modify Wasserstein distance objective and what computational benefit does it provide?

- **Concept:** Mutual Information and Entropy in OT
  - Why needed: Entropic regularization expressed via mutual information between generated and target samples
  - Quick check: In entropic Wasserstein formulation, how is mutual information Iπ(X,Y) related to entropy terms of marginals?

## Architecture Onboarding

- **Component map:** Data privatization module -> GAN generator network -> Sinkhorn solver -> Loss layer -> Optimizer
- **Critical path:**
  1. Sample mini-batches from privatized dataset
  2. Sample latent vectors Z
  3. Forward pass through generator to get G(Z)
  4. Compute cost matrix C and run Sinkhorn-Knopp to get π
  5. Evaluate loss and backpropagate to update generator
- **Design tradeoffs:**
  - Sinkhorn iterations L vs. approximation accuracy: higher L → better π but more compute
  - Batch size b vs. privacy amplification: larger b → better gradients but may require more privacy budget
  - λ (entropy regularization strength) vs. denoising: must match noise scale σ; too large → oversmooth, too small → insufficient denoising
- **Failure signatures:**
  - Generated images too noisy → λ too small or privacy noise scale too high
  - Generated images too blurry → λ too large or overfitting to noise
  - Training instability → Sinkhorn not converging (increase L or adjust step size)
  - Poor privacy-utility tradeoff → privacy budget ϵ too low for given dataset size
- **First 3 experiments:**
  1. Train with no privacy noise, no regularization → sanity check baseline Wasserstein GAN
  2. Train with privacy noise but unregularized Wasserstein loss → confirm failure mode (learns noisy distribution)
  3. Train with privacy noise and entropic regularization → verify denoising and convergence to original distribution

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Assumes additive, independent noise for Laplace and Gaussian mechanisms, which may not generalize to other LDP mechanisms
- Sample complexity bounds depend on sub-Gaussian data and Lipschitz/star-shaped generator sets
- Optimal choice of regularization parameter λ depends on unknown noise scale, creating tuning challenges
- Limited experimental validation to synthetic and MNIST datasets with specific privacy budgets

## Confidence
- **High confidence**: Theoretical framework and proof that optimal solutions recover true distribution under stated conditions
- **Medium confidence**: Sample complexity bounds (depend on specific distributional assumptions)
- **Medium confidence**: Experimental results (limited to specific datasets and privacy budgets)
- **Low confidence**: Generalizability to other data types, GAN architectures, or privacy mechanisms

## Next Checks
1. Test method on real-world datasets with varying dimensions and distributions to assess curse of dimensionality claims
2. Implement with different LDP mechanisms (e.g., randomized response, RAPPOR) to verify robustness beyond additive noise models
3. Conduct ablation studies varying regularization parameter λ systematically to identify optimal tuning strategies and understand trade-offs between denoising and oversmoothing