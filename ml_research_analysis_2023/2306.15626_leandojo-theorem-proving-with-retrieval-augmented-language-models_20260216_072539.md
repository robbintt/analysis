---
ver: rpa2
title: 'LeanDojo: Theorem Proving with Retrieval-Augmented Language Models'
arxiv_id: '2306.15626'
source_url: https://arxiv.org/abs/2306.15626
tags:
- lean
- theorem
- data
- premises
- leandojo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LeanDojo provides open-source toolkits for data extraction and
  interaction with the Lean theorem prover, addressing barriers to research in machine
  learning for theorem proving. It extracts proof trees and premise annotations from
  Lean code, enabling retrieval-augmented language models for theorem proving.
---

# LeanDojo: Theorem Proving with Retrieval-Augmented Language Models

## Quick Facts
- arXiv ID: 2306.15626
- Source URL: https://arxiv.org/abs/2306.15626
- Reference count: 40
- Key outcome: ReProver achieves 51.4% success rate on LeanDojo Benchmark, outperforming non-retrieval baselines (47.5%) and GPT-4 (28.8%)

## Executive Summary
LeanDojo provides open-source toolkits for data extraction and interaction with the Lean theorem prover, addressing barriers to research in machine learning for theorem proving. It extracts proof trees and premise annotations from Lean code, enabling retrieval-augmented language models for theorem proving. Using this data, ReProver retrieves relevant premises from Lean's math library and generates tactics conditioned on the current proof state and retrieved premises. ReProver achieves 51.4% success rate on a challenging benchmark of 98,734 theorems, outperforming non-retrieval baselines and GPT-4 while requiring only one GPU week of training.

## Method Summary
LeanDojo extracts proof trees, premises, and file dependencies from Lean code to create training data for retrieval-augmented theorem proving. The ReProver model combines a dense passage retriever that identifies accessible premises from Lean's math library with a ByT5 encoder-decoder that generates tactics conditioned on both the current proof state and retrieved premises. The retriever uses program analysis to identify which premises are accessible to each theorem, reducing the search space from 128K to 33K premises on average. Training employs in-file negative sampling to provide hard negative examples. The complete system uses best-first search to assemble generated tactics into full proofs.

## Key Results
- ReProver achieves 51.4% Pass@1 success rate on LeanDojo Benchmark random split
- Retrieval augmentation improves performance over non-retrieval baseline (47.5% to 51.4%)
- Outperforms GPT-4 (28.8%) on the same benchmark
- Discovers 65 new proofs not present in the training data
- Requires only one GPU week of training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation improves theorem proving by explicitly incorporating relevant premises instead of relying solely on memorization.
- Mechanism: The model retrieves a small set of accessible premises from Lean's math library using a dense passage retriever, then conditions the tactic generation on both the current proof state and these retrieved premises.
- Core assumption: The retrieved premises contain the ground truth premise needed for the current proof step, and the model can effectively learn to use them.
- Evidence anchors:
  - [abstract]: "Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library."
  - [section]: "For example, Fig. 1 (Top left) illustrates the proof of '∀n ∈ N, gcd n n = n', where gcd stands for greatest common divisor. The proof starts from the original theorem as the initial state and repeatedly applies tactics to decompose states into simpler sub-states, until all states are solved. Tactics may rely on premises such as mod_self and gcd_zero_left defined in a large math library."

### Mechanism 2
- Claim: Restricting retrieval to accessible premises significantly improves retrieval effectiveness by reducing the search space.
- Mechanism: Program analysis using LeanDojo identifies which premises are accessible to the current theorem (defined in the same file before the theorem or imported from other files), limiting the retrieval corpus from 128K to 33K premises on average.
- Core assumption: Premises not accessible to the current theorem cannot be used in its proof, making their inclusion in the retrieval corpus unnecessary and potentially harmful.
- Evidence anchors:
  - [abstract]: "Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective."
  - [section]: "First, not all premises are accessible when proving a theorem (Sec. 3). LeanDojo can perform program analysis on Lean code to determine accessible premises. On our data, that reduces the average number of premises from 128K to 33K, significantly simplifying the retriever's task."

### Mechanism 3
- Claim: In-file negative sampling improves retrieval training by providing hard negative examples that are difficult to distinguish from positive premises.
- Mechanism: When training the dense passage retriever, k negative premises are sampled from the same file as the positive premise, and n-k are sampled randomly from the entire accessible premise set.
- Core assumption: Premises from the same file as the positive premise are more likely to be semantically similar and thus serve as harder negative examples, improving the retriever's discrimination ability.
- Evidence anchors:
  - [section]: "Second, DPR needs negative examples in training and benefits from hard negatives, i.e., irrelevant premises that are hard to distinguish from ground truth ones. We propose in-file negatives: a simple mechanism to find hard negatives in premise selection, which samples negative premises defined in the same Lean source file as the ground truth premise."

## Foundational Learning

- Concept: Theorem proving in proof assistants
  - Why needed here: Understanding how Lean represents theorems, proofs, and premises is fundamental to grasping the problem ReProver solves
  - Quick check question: What is the difference between a theorem and a premise in Lean?

- Concept: Dense passage retrieval
  - Why needed here: The retriever component uses DPR architecture, so understanding how it works is crucial for comprehending the retrieval mechanism
  - Quick check question: How does DPR compute the similarity between a query and candidate passages?

- Concept: Program analysis for dependency tracking
  - Why needed here: LeanDojo uses program analysis to determine accessible premises, which is key to the retrieval strategy
  - Quick check question: What information does LeanDojo extract from Lean files to determine which premises are accessible to a given theorem?

## Architecture Onboarding

- Component map:
  LeanDojo (data extraction) -> Dense Passage Retriever (premise retrieval) -> ByT5 encoder-decoder (tactic generation) -> Best-first search (proof assembly) -> Lean environment (tactic execution)

- Critical path:
  1. Extract accessible premises using LeanDojo's program analysis
  2. Train dense passage retriever on extracted data with in-file negatives
  3. Fine-tune ByT5 tactic generator on concatenated state + retrieved premises
  4. Combine tactic generator with best-first search for proof search
  5. Evaluate using Pass@1 metric on LeanDojo Benchmark

- Design tradeoffs:
  - Retrieval vs. memorization: Retrieval requires additional computation but generalizes better to novel premises
  - Number of retrieved premises: More premises provide more information but increase input length and computational cost
  - Training vs. inference efficiency: Dense retrieval is efficient at inference but requires careful negative sampling during training

- Failure signatures:
  - Low R@1 scores: Retriever failing to find relevant premises
  - High invalid tactic generation: Tactic generator not effectively using retrieved premises
  - Low Pass@1 scores despite high retrieval recall: Proof search or tactic execution issues
  - High proof-checking errors: Issues with Lean environment interaction

- First 3 experiments:
  1. Evaluate premise retrieval performance (R@1, MRR) on validation set to ensure retriever is working
  2. Test tactic generation with retrieved premises vs. without retrieval on validation set
  3. Run Pass@1 evaluation on full test set to measure overall proof success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do we determine when to use retrieved premises versus relying on the model's implicit knowledge?
- Basis in paper: [explicit] The paper notes that ReProver can decide whether to use an explicitly retrieved premise or an implicitly memorized one, but doesn't specify the decision mechanism.
- Why unresolved: The paper mentions this capability but doesn't detail the algorithmic approach or decision criteria for selecting between retrieved and memorized premises.
- What evidence would resolve it: Experimental comparisons showing the effectiveness of different strategies for selecting between retrieved and memorized premises, or a detailed description of the decision mechanism used in ReProver.

### Open Question 2
- Question: Can the retrieval-augmented approach scale to much larger premise libraries (e.g., 1 million+ premises) while maintaining effectiveness?
- Basis in paper: [inferred] The paper limits retrieval to 100 premises due to input length constraints, and focuses on accessible premises to reduce the search space from 128K to 33K.
- Why unresolved: The paper doesn't explore scaling to much larger premise libraries or discuss architectural modifications needed to handle such scale.
- What evidence would resolve it: Performance comparisons of ReProver with varying premise library sizes, or architectural innovations that enable effective retrieval from massive premise collections.

### Open Question 3
- Question: How does ReProver's performance compare to state-of-the-art methods that use reinforcement learning and online interaction?
- Basis in paper: [explicit] The paper notes it only compares with non-RL baselines and doesn't compare with existing LLM-based provers due to private code/data, but acknowledges this as a limitation.
- Why unresolved: The paper explicitly states it cannot compare with RL-based methods and existing provers due to practical barriers.
- What evidence would resolve it: Direct performance comparisons with state-of-the-art RL-based theorem provers on the same benchmarks under comparable conditions.

## Limitations

- Performance drops significantly (51.4% to 42.6%) on the novel_premises split, indicating struggles with genuinely novel mathematical concepts
- Reliance on 100 retrieved premises per theorem raises concerns about scalability and effective context processing
- Evaluation doesn't fully characterize whether the 65 discovered proofs represent genuinely novel insights or variations of existing proofs

## Confidence

**High Confidence**: The core retrieval mechanism and its improvement over non-retrieval baselines is well-supported by the experimental results (51.4% vs 47.5% Pass@1).

**Medium Confidence**: The claim of being "the first accessible LLM-based theorem prover" is supported by LeanDojo's open-source nature, though GPT-4 comparison (28.8%) doesn't account for resource differences.

**Medium Confidence**: The premise accessibility mechanism through program analysis is theoretically sound, but lacks ablation studies showing its specific contribution to performance gains.

## Next Checks

1. **Generalization Stress Test**: Evaluate ReProver on a dedicated test set of theorems requiring genuinely novel mathematical concepts not present in any training or validation premises. Measure the performance gap between this set and the existing novel_premises split to quantify true generalization capability.

2. **Proof Novelty Analysis**: Conduct a systematic analysis of the 65 discovered proofs to determine their mathematical novelty. This should include expert review to classify whether these proofs introduce genuinely new proof techniques or merely apply known techniques in slightly different contexts.

3. **Scalability Assessment**: Measure ReProver's performance when varying the number of retrieved premises (e.g., 10, 50, 200) to identify the optimal trade-off between retrieval recall and computational efficiency. This would help determine whether the current 100-premise approach is optimal or if simpler approaches could achieve similar results.