---
ver: rpa2
title: Artificial Intelligence in the automatic coding of interviews on Landscape
  Quality Objectives. Comparison and case study
arxiv_id: '2312.05597'
source_url: https://arxiv.org/abs/2312.05597
tags:
- codes
- coding
- interviews
- landscape
- atlas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated the performance of three AI tools (Atlas.ti,\
  \ ChatGPT, Google Bard) for automatically coding qualitative interviews on Landscape\
  \ Quality Objectives in Cayo Santa Mar\xEDa, Cuba. The AI-generated codes were compared\
  \ to manually coded interviews using criteria such as accuracy, comprehensiveness,\
  \ thematic coherence, redundancy, clarity, detail, and regularity."
---

# Artificial Intelligence in the automatic coding of interviews on Landscape Quality Objectives. Comparison and case study

## Quick Facts
- arXiv ID: 2312.05597
- Source URL: https://arxiv.org/abs/2312.05597
- Reference count: 0
- Primary result: AI tools can assist in initial coding of qualitative interviews but require expert refinement due to limitations in accuracy and thematic coherence

## Executive Summary
This study evaluated the performance of three AI tools (Atlas.ti, ChatGPT, and Google Bard) for automatically coding qualitative interviews on Landscape Quality Objectives in Cayo Santa María, Cuba. The AI-generated codes were compared to manually coded interviews using criteria such as accuracy, comprehensiveness, thematic coherence, redundancy, clarity, detail, and regularity. The results showed that while AI tools can serve as useful initial guides for thematic analysis, they exhibit notable limitations that necessitate expert refinement before final analysis.

## Method Summary
The study analyzed 12 semi-structured research interviews focused on Landscape Quality Objectives in Cayo Santa María, Cuba. Interview transcripts were processed using three AI tools (Atlas.ti's AI coding feature, ChatGPT, and Google Bard) to generate automatic codes and subcodes. These AI-generated codes were then compared against manual expert coding using seven evaluation criteria: Accuracy, Comprehensiveness, Thematic Coherence, Redundancy, Clarity, Detail, and Regularity. The analysis examined both the quantitative performance metrics and qualitative advantages and disadvantages of each tool.

## Key Results
- Atlas.ti showed low accuracy and high redundancy but medium clarity and regularity
- ChatGPT provided high thematic coherence, medium accuracy and comprehensiveness, and low redundancy with medium clarity and detail
- Google Bard exhibited medium accuracy and comprehensiveness, high thematic coherence, and very high detail but medium-low regularity
- All AI tools were useful as initial coding guides but required expert refinement due to notable limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI coding tools can serve as initial guides for thematic analysis but require expert refinement due to limitations in accuracy and thematic coherence.
- Mechanism: The AI tools provide a structured output of codes and subcodes, but due to variability in text complexity and phrasing, the generated codes may not fully align with the manual coding framework, necessitating expert intervention to correct inaccuracies and redundancies.
- Core assumption: Expert manual coding serves as a reliable reference point for evaluating AI-generated codes.
- Evidence anchors:
  - [abstract] "Overall, AI tools were useful as initial coding guides but required expert refinement due to notable limitations."
  - [section] "Based on the conducted analysis, the usefulness of AI has been confirmed as support for the coding of research interviews, an often long and tedious process. However, even taking into consideration that the three tools have been evaluated in their free version... the coding they provide exhibits numerous errors and limitations."
  - [corpus] Weak evidence for the specific accuracy and coherence values in other contexts, but the general claim aligns with the study's findings.
- Break condition: If the AI tools improve to the point where their accuracy and coherence match or exceed manual coding standards, the need for expert refinement would be reduced.

### Mechanism 2
- Claim: The effectiveness of AI coding tools is influenced by the clarity and organization of the input text.
- Mechanism: When interviewees provide clear, organized responses that directly address the questions, AI tools can generate more accurate and coherent codes. Conversely, when responses are complex or contain mixed themes, the AI's performance declines.
- Core assumption: The quality of input data directly impacts the quality of AI-generated outputs.
- Evidence anchors:
  - [abstract] "The study highlights the potential of AI-assisted coding while emphasizing the need for further improvements before full automation is feasible."
  - [section] "It has been observed that the quality of the input data greatly affects the responses provided by the AIs. For instance, in Interview 4 there were elevated levels of Accuracy, Completeness and Thematic Coherence... In contrast, when dealing with transcripts where topics and sub topics are mixed, or appear several times at different points in the response, AIs do not perform as well."
  - [corpus] Limited evidence from the corpus for this specific mechanism, but the study's findings support the claim.
- Break condition: If AI tools develop advanced natural language understanding capabilities that can handle complex and mixed-topic responses, the input text clarity would become less critical.

### Mechanism 3
- Claim: The choice of AI tool and its configuration (e.g., prompt design) significantly affects the coding results.
- Mechanism: Different AI tools have varying strengths and weaknesses in terms of accuracy, comprehensiveness, thematic coherence, redundancy, clarity, detail, and regularity. The way prompts are designed and the specific AI tool chosen can lead to different outcomes in the coding process.
- Core assumption: AI tools are not interchangeable, and their configuration can be optimized for specific tasks.
- Evidence anchors:
  - [abstract] "Atlas.ti showed low accuracy and high redundancy but medium clarity and regularity. ChatGPT provided high thematic coherence, medium accuracy and comprehensiveness, and low redundancy with medium clarity and detail. Google Bard exhibited medium accuracy and comprehensiveness, high thematic coherence, and very high detail but medium-low regularity."
  - [section] "In this study, we conducted a comparative analysis of the automated coding provided by three Artificial Intelligence functionalities (Atlas.ti, ChatGPT, and Google Bard)... Following the established comparison criteria, it can be observed that Atlas.ti yielded relatively low Accuracy results... The effectiveness of AI coding tools is influenced by the clarity and organization of the input text."
  - [corpus] The corpus provides comparative evidence for the different AI tools, supporting the claim that tool choice and configuration matter.
- Break condition: If AI tools converge to similar performance levels or if a universal configuration is developed, the impact of tool choice would diminish.

## Foundational Learning

- Concept: Qualitative content analysis
  - Why needed here: Understanding the process of coding qualitative data is essential for evaluating the performance of AI tools in this domain.
  - Quick check question: What is the primary goal of qualitative content analysis in research?

- Concept: Thematic analysis
  - Why needed here: The study uses thematic analysis as the methodology for coding interviews, so understanding this approach is crucial for interpreting the results.
  - Quick check question: How does thematic analysis differ from other qualitative analysis methods?

- Concept: AI language models
  - Why needed here: Familiarity with how AI language models work is necessary to understand their capabilities and limitations in coding tasks.
  - Quick check question: What are the key components of an AI language model that enable it to generate text?

## Architecture Onboarding

- Component map: Interview transcripts -> AI tools (Atlas.ti, ChatGPT, Google Bard) -> AI-generated codes -> Manual coding reference -> Comparison criteria
- Critical path: Input transcripts into AI tools -> Generate codes and subcodes -> Compare with manual coding using 7 criteria -> Evaluate performance and limitations
- Design tradeoffs: The tradeoff is between the convenience and speed of AI coding versus the accuracy and reliability of manual coding. Using AI tools can save time but may introduce errors that require expert refinement.
- Failure signatures: High redundancy, low accuracy, and poor thematic coherence are indicators that the AI tools are not performing well. Inconsistent results across different AI tools or different prompts also suggest issues.
- First 3 experiments:
  1. Test the AI tools on a small set of interview transcripts with clear, organized responses to assess their baseline performance.
  2. Vary the prompts used with the AI tools to see how different configurations affect the coding results.
  3. Compare the AI tools' performance on transcripts with mixed or complex themes to evaluate their limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of prompt specificity on AI coding accuracy and comprehensiveness for qualitative interview analysis?
- Basis in paper: [explicit] The authors noted that ChatGPT and Google Bard's performance heavily relies on prompt formulation, and they deliberately chose not to specify the number of codes to maintain fairness with Atlas.ti, which had limitations in this regard.
- Why unresolved: The study did not systematically test different prompt formulations to assess their impact on coding quality, leaving uncertainty about the optimal level of prompt specificity for AI-assisted qualitative analysis.
- What evidence would resolve it: Systematic testing of AI tools with varying levels of prompt specificity, comparing coding accuracy, comprehensiveness, and redundancy across different prompt formulations for the same interview data.

### Open Question 2
- Question: How do AI coding tools perform when analyzing interviews with more complex or less structured responses compared to the relatively clear and organized responses in this study?
- Basis in paper: [explicit] The authors observed that AI performance decreased when dealing with transcripts where topics and sub-topics were mixed or appeared multiple times at different points in the response, suggesting that input data quality significantly affects AI responses.
- Why unresolved: The study focused on interviews with relatively clear and organized responses, limiting the generalizability of findings to more complex or less structured interview data commonly encountered in qualitative research.
- What evidence would resolve it: Comparative analysis of AI coding performance across interviews with varying levels of response complexity and structure, measuring accuracy, comprehensiveness, and thematic coherence.

### Open Question 3
- Question: What is the long-term impact of AI coding tools on qualitative research methodology and researcher expertise development?
- Basis in paper: [inferred] The authors emphasized that AI tools should be used with caution and that expert judgment should prevail in the final analysis, suggesting potential implications for research methodology and skill development that were not explicitly explored.
- Why unresolved: The study focused on technical performance comparisons rather than examining the broader implications of AI adoption for research practices, skill development, and the evolution of qualitative research methodology.
- What evidence would resolve it: Longitudinal studies tracking changes in research practices, skill requirements, and methodological approaches as AI tools become more prevalent in qualitative analysis, including surveys of researcher experiences and skill development trajectories.

## Limitations

- Limited generalizability due to small sample size (12 interviews) from a single geographic context (Cayo Santa María, Cuba)
- AI tools were evaluated only in their free versions, potentially missing advanced features that could improve performance
- Manual coding baseline may contain subjective biases not explicitly addressed in the study
- Results may not generalize to different research domains, languages, or cultural contexts

## Confidence

**High Confidence**: The comparative framework and methodology are clearly defined and replicable. The observation that AI tools are useful as initial guides but require expert refinement is strongly supported by the empirical results across all three AI platforms tested.

**Medium Confidence**: The specific performance metrics for each AI tool (e.g., Atlas.ti's low accuracy and high redundancy) are reasonably reliable for this particular dataset and context, but may not generalize to other qualitative research domains or interview types.

**Low Confidence**: Claims about the broader applicability of these findings to different research fields or AI tools not tested in this study should be treated cautiously, given the limited scope and single-context nature of the evaluation.

## Next Checks

1. **Cross-context validation**: Test the same AI tools on interview transcripts from different research domains (e.g., healthcare, education, organizational studies) to assess whether the observed performance patterns hold across contexts.

2. **Prompt engineering experiment**: Systematically vary the prompts used with ChatGPT and Google Bard to determine how different instructions affect coding accuracy, redundancy, and thematic coherence, establishing optimal prompt formulations.

3. **Longitudinal comparison**: Repeat the study with updated versions of the AI tools (e.g., GPT-4 vs GPT-3.5) to measure improvements in coding capabilities over time and assess whether the identified limitations are being addressed in newer iterations.