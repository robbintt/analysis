---
ver: rpa2
title: Granger Causal Interaction Skill Chains
arxiv_id: '2306.09509'
source_url: https://arxiv.org/abs/2306.09509
tags:
- skills
- interaction
- hints
- learning
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Chain of Interaction Skills (COInS) algorithm
  for skill discovery in factored domains. COInS uses learned detectors to identify
  interactions between state factors and trains a chain of skills to control each
  factor successively.
---

# Granger Causal Interaction Skill Chains

## Quick Facts
- arXiv ID: 2306.09509
- Source URL: https://arxiv.org/abs/2306.09509
- Reference count: 23
- 2-3x improvement in sample efficiency and final performance on robotic pushing and Breakout variants

## Executive Summary
This paper introduces the Chain of Interaction Skills (COInS) algorithm for skill discovery in factored domains. The approach uses Granger-causal tests to detect interactions between state factors and trains a chain of skills to control each factor successively. By leveraging learned detectors to identify these interactions, COInS defines goal spaces for goal-conditioned reinforcement learning that significantly improve sample efficiency and final performance compared to standard RL baselines.

## Method Summary
COInS works by first learning passive and active forward dynamics models for each pair of state factors. The Granger-causal interaction detector compares these models to identify which factors predictively influence others. Once interactions are detected, the algorithm uses these interaction states as goals for goal-based RL to train skills that control specific factors. These skills are then chained hierarchically where the goal space of one skill becomes the action space for the next, creating a decomposition that enables efficient learning in complex tasks.

## Key Results
- 2-3x improvement in sample efficiency on robotic pushing task with obstacles
- 2-3x improvement in final performance on variants of Breakout
- Effective transfer learning demonstrated on novel tasks without additional training
- Successfully addresses low sample efficiency and limited transferability in standard RL approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Granger-causal interaction detection identifies which state factors are most useful for predicting the next state of a target factor.
- Mechanism: The method learns two models—a passive autoregressive model that predicts target state from its own history, and an active model that uses both the source and target factor states. When the active model significantly outperforms the passive model on states where the passive prediction is inaccurate, this indicates the source factor is Granger-causal to the target.
- Core assumption: Factorized state spaces are available, and interactions between factors are rare but impactful for task success.
- Evidence anchors:
  - [abstract] "COInS uses learned detectors to identify interactions between state factors"
  - [section] "We say a source factor a Markov Granger-causes (MG-causes) the transition dynamics of a target b if gϕ(xa,xb)[x′b] shows a statistically significant improvement in predictive power over fθ(xa)[x′b]"
  - [corpus] Weak evidence—related work focuses on skill discovery but does not explicitly use Granger-causal detection for interaction identification.
- Break condition: If interactions are not rare or the factorized state assumption fails, the interaction detector will not provide a useful signal for skill discovery.

### Mechanism 2
- Claim: Goal-based RL using interaction states as goals learns skills that control target factors effectively.
- Mechanism: The interaction detector defines a termination condition where a skill is considered complete when the target factor state is close to a goal state and an interaction is detected. The skill policy learns to reach these interaction-defined goals using hindsight experience replay.
- Core assumption: Goal-based RL can learn useful policies when provided with well-defined goal spaces derived from interaction states.
- Evidence anchors:
  - [abstract] "COInS uses learned detectors to identify interactions between state factors and trains a chain of skills to control each factor successively"
  - [section] "HIntS uses the interaction detector to define goals in goal-based RL, the skill to control the target factor"
  - [corpus] Weak evidence—related work on skill discovery does not use interaction-based goal definition.
- Break condition: If the interaction states do not provide meaningful controllability over the target factor, the learned skills will not be useful for downstream tasks.

### Mechanism 3
- Claim: Chaining skills where the goal space of one skill becomes the action space of the next creates a hierarchical policy that can solve complex tasks.
- Mechanism: Each learned skill controls a factor, and the controlled factor's state becomes the goal space for the next skill in the chain. This creates a hierarchical decomposition where higher-level skills can reason in the space of controlled factors rather than primitive actions.
- Core assumption: Factors in the environment can be controlled sequentially, and each factor's controllability is necessary for task success.
- Evidence anchors:
  - [abstract] "COInS uses learned detectors to identify interactions between state factors and trains a chain of skills to control each factor successively"
  - [section] "HIntS iteratively builds up from ω1, connecting a new factor i + 1 to i at iteration i of HIntS"
  - [corpus] Weak evidence—related hierarchical methods do not explicitly chain skills based on interaction detection.
- Break condition: If the factor chain does not capture the necessary dependencies for task completion, the hierarchical policy will fail to achieve high performance.

## Foundational Learning

- Factored Markov Decision Processes
  - Why needed here: The algorithm assumes the state can be decomposed into independent factors, which is essential for identifying Granger-causal relationships between factors.
  - Quick check question: In a factored MDP with state x = {x1, x2, x3}, what does the transition function P(x'|x,a) factor into?

- Granger Causality
  - Why needed here: The interaction detection mechanism relies on comparing autoregressive predictions with predictions that include a potential cause, which is the core of Granger causality.
  - Quick check question: What is the null hypothesis in a Granger causality test, and how does it differ from the alternative hypothesis?

- Goal-based Reinforcement Learning
  - Why needed here: Skills are learned by reaching specific goal states, and the interaction detector defines these goals based on detected interactions.
  - Quick check question: How does hindsight experience replay modify the reward function when using goal-based RL?

## Architecture Onboarding

- Component map:
  Interaction Detector -> Skill Learner -> Skill Chain Builder -> Model Evaluator

- Critical path:
  1. Random exploration to gather initial data
  2. Learn passive and active models for factor pairs
  3. Compute interaction scores and identify highest-scoring target factor
  4. Train goal-based skill to control target factor
  5. Add skill to hierarchy and repeat until no significant interactions remain

- Design tradeoffs:
  - Pairwise vs. higher-order interactions: Pairwise is computationally tractable but may miss complex interactions
  - Continuous vs. discrete goal spaces: Continuous allows finer control but requires more samples to cover the space
  - Model complexity vs. data efficiency: More complex models can capture interactions better but require more data

- Failure signatures:
  - No skills discovered: Interaction scores are all below threshold, possibly due to incorrect factorization or insufficient exploration
  - Skills don't transfer: Learned skills are too task-specific or don't capture the right factors
  - Poor performance on base task: Skills don't provide sufficient controllability or the chain order is incorrect

- First 3 experiments:
  1. Run on a simple factored environment (e.g., grid world with two objects) to verify interaction detection works
  2. Test skill chaining on a two-factor environment where controlling one factor enables control of the other
  3. Evaluate transfer by training on one task variant and testing on a related variant with similar factor structure

## Open Questions the Paper Calls Out
- Can the Granger-causal interaction detection method be extended to non-factored state spaces?
- How does the performance of Granger-causal interaction skills compare to diversity-based skill discovery methods in high-dimensional domains?
- Can the interaction detection mechanism be made more robust to spurious interactions caused by confounding factors?

## Limitations
- Method relies heavily on the assumption of factored state spaces, which may not hold in many real-world environments
- Pairwise Granger-causal interaction detection may miss higher-order interactions crucial for complex tasks
- Assumes interactions are rare, which could limit applicability in highly coupled systems

## Confidence
- High confidence: The mechanism of using Granger-causal tests to identify predictive interactions between state factors is well-established theoretically.
- Medium confidence: The effectiveness of chaining skills based on detected interactions for improving sample efficiency is supported by experimental results but needs validation on more diverse tasks.
- Low confidence: The transferability of learned skills to novel tasks without additional training is claimed but not thoroughly evaluated in the paper.

## Next Checks
1. Test the interaction detection mechanism on environments with known higher-order interactions to assess its ability to capture complex dependencies beyond pairwise relationships.
2. Evaluate skill transferability by training on one task variant and testing on structurally different but related variants to measure zero-shot transfer capability.
3. Compare against alternative skill discovery methods that do not rely on factored state assumptions to isolate the contribution of Granger-causal detection from the skill chaining approach.