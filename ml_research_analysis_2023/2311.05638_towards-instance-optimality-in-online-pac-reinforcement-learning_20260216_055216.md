---
ver: rpa2
title: Towards Instance-Optimality in Online PAC Reinforcement Learning
arxiv_id: '2311.05638'
source_url: https://arxiv.org/abs/2311.05638
tags:
- policy
- bound
- mdps
- optimal
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies instance-dependent lower bounds for probably
  approximately correct (PAC) reinforcement learning in tabular episodic Markov decision
  processes (MDPs). The authors derive the first instance-dependent lower bound for
  PAC RL that holds for any precision $\varepsilon \geq 0$ and any tabular MDP, using
  a change-of-measure argument.
---

# Towards Instance-Optimality in Online PAC Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.05638
- Source URL: https://arxiv.org/abs/2311.05638
- Reference count: 40
- Key outcome: Derives the first instance-dependent lower bound for PAC RL that holds for any precision ε ≥ 0 and any tabular MDP

## Executive Summary
This paper establishes the first instance-dependent lower bound for probably approximately correct (PAC) reinforcement learning in tabular episodic Markov decision processes (MDPs). The authors use a change-of-measure argument to derive a lower bound that depends on the specific instance characteristics rather than worst-case parameters. They show that the sample complexity of the PEDEL algorithm nearly matches this lower bound up to polynomial factors, but leave open whether a computationally-efficient algorithm can achieve the same bound.

## Method Summary
The paper derives instance-dependent lower bounds for PAC RL using a change-of-measure argument, computing the minimal KL divergence between observation distributions under different MDPs. It then compares this lower bound to the PEDEL algorithm's sample complexity, which uses an experimental design approach to minimize maximum variance across policies. The comparison reveals that PEDEL nearly matches the lower bound up to polynomial multiplicative factors of the horizon, but is computationally intractable due to explicit policy enumeration.

## Key Results
- Derives the first instance-dependent lower bound for PAC RL that holds for any ε ≥ 0 and any tabular MDP
- Shows PEDEL's sample complexity closely approaches the lower bound up to H^5 polynomial factors
- Establishes policy diversity conditions under which PEDEL achieves instance-optimality
- Formulates the open question of whether computationally-efficient algorithms can match the lower bound

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The change-of-measure argument enables deriving instance-dependent lower bounds for PAC RL
- Mechanism: By constructing alternative MDPs where a candidate policy is no longer near-optimal, the paper computes the minimal KL divergence between observation distributions. This divergence is expressed in terms of visitation probabilities and sub-optimality gaps, yielding the lower bound.
- Core assumption: MDPs share the same transition kernel but differ in reward means
- Evidence anchors:
  - [abstract] "derive the first instance-dependent lower bound for PAC RL that holds for any precision ε ≥ 0 and any tabular MDP, using a change-of-measure argument"
  - [section 3.1] "The idea of the proof is to explicitly compute the smallest KL divergence between the distribution of the observations under the MDP M and under any alternative ˜M that has the same transitions but a different mean reward function"
- Break condition: If the optimal state-action distribution is not unique, the KL divergence construction fails

### Mechanism 2
- Claim: The PEDEL algorithm nearly matches the lower bound up to polynomial multiplicative factors of the horizon
- Mechanism: PEDEL uses an experimental design approach that minimizes the maximum variance across policies. The complexity measure involves minimizing over state-action distributions and maximizing over policies, similar to the lower bound structure.
- Core assumption: The MDP has a unique minimum policy gap or ε is sufficiently large
- Evidence anchors:
  - [abstract] "demonstrate that the sample complexity of the PEDEL algorithm of Wagenmaker and Jamieson (2022) closely approaches this lower bound"
  - [section 4.2] "the complexity CPEDEL(M, ε ) is only a factor H 5 away from the instance-dependent lower bound"
- Break condition: When minimum policy gap approaches zero and ε is very small

### Mechanism 3
- Claim: Policy diversity condition ensures instance-optimality of PEDEL
- Mechanism: If for every ε-optimal policy there exists a sufficiently distant near-optimal policy, the complexity of PEDEL matches the lower bound up to horizon factors. This is formalized using total variation distance between policy distributions.
- Core assumption: There exist two near-optimal policies with disjoint state visitation sets or different actions in the same state
- Evidence anchors:
  - [section 4.2] "let us define the following divergence measure between any pair of policies π, π ′: d(π, π ′) := ∑h∈[H] TV(pπ h, pπ′ h )2"
  - [section 4.2] "Proposition 2 essentially states that, for MDPs where near-optimal policies are sufficiently 'diverse'..."
- Break condition: When all near-optimal policies have similar state visitation patterns

## Foundational Learning

- Concept: Change-of-measure argument in information theory
  - Why needed here: This is the core technique for deriving lower bounds by constructing hard-to-distinguish alternative problems
  - Quick check question: How does the KL divergence between two MDPs simplify when they share the same transition kernel?

- Concept: Experimental design and G-optimal design
  - Why needed here: PEDEL's approach to minimizing the maximum variance across policies is essentially a G-optimal design problem
  - Quick check question: What is the relationship between G-optimal design and the complexity measure in PEDEL?

- Concept: PAC RL sample complexity bounds
  - Why needed here: Understanding both instance-dependent and minimax bounds is crucial for assessing the significance of the lower bound
  - Quick check question: How do instance-dependent bounds improve upon minimax bounds in terms of dependence on problem parameters?

## Architecture Onboarding

- Component map:
  - Lower bound derivation module: Change-of-measure argument, KL divergence computation, optimization over state-action distributions
  - Upper bound analysis module: PEDEL algorithm review, complexity measure comparison
  - Instance-optimality assessment module: Policy diversity conditions, polynomial gap analysis

- Critical path:
  1. Define MDP class and PAC RL problem setup
  2. Derive lower bound using change-of-measure argument
  3. Review PEDEL algorithm and complexity measure
  4. Compare lower and upper bounds
  5. Analyze conditions for instance-optimality

- Design tradeoffs:
  - General lower bound vs. finite-δ bound: The general bound requires δ → 0 but applies to any ε, while the finite-δ bound requires unique optimal state-action distribution but applies for all δ
  - Polynomial factors vs. instance-dependence: The H^5 gap between PEDEL and the lower bound is acceptable for instance-optimality but not for tight bounds

- Failure signatures:
  - Lower bound derivation fails if alternative MDPs cannot be constructed (e.g., non-unique optimal state-action distribution)
  - PEDEL does not match lower bound if minimum policy gap approaches zero and ε is very small
  - Policy diversity condition not satisfied if all near-optimal policies visit similar states with similar probabilities

- First 3 experiments:
  1. Implement the change-of-measure argument for a simple 2-state MDP and verify the lower bound computation
  2. Compute PEDEL's complexity measure for a tree-structured MDP and compare with the derived lower bound
  3. Test the policy diversity condition on a deterministic MDP where two near-optimal policies take different actions in the same state

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a computationally-efficient algorithm achieve the instance-dependent lower bound for PAC RL?
- Basis in paper: [explicit] The paper states that PEDEL nearly matches the lower bound but is computationally intractable, and formulates an open question about whether a computationally-efficient algorithm can achieve the lower bound.
- Why unresolved: PEDEL is intractable because it explicitly enumerates all policies (size ASH). Existing polynomial-time algorithms like PRINCIPLE have strictly worse sample complexity than PEDEL.
- What evidence would resolve it: Either a proof that no polynomial-time algorithm can achieve the lower bound, or a construction of a computationally-efficient algorithm matching the lower bound.

### Open Question 2
- Question: Is there a polynomial-time implicit policy elimination scheme compatible with the optimal design used by PEDEL?
- Basis in paper: [inferred] The paper discusses that PRINCIPLE performs implicit policy elimination but only solves an upper bound on the optimal design, leading to worse sample complexity. It leaves open whether an implicit scheme can be made compatible with PEDEL's optimal design.
- Why unresolved: The optimal design in PEDEL involves computing an objective that requires enumerating all policies, making it computationally infeasible.
- What evidence would resolve it: Either a proof that implicit policy elimination cannot achieve the optimal design, or a construction of an implicit scheme achieving the same complexity as PEDEL.

### Open Question 3
- Question: What is the relationship between computational efficiency and instance-optimality in PAC RL compared to bandits?
- Basis in paper: [explicit] The paper suggests that if the answer to the main question is negative, it would indicate a clear separation between MDPs and bandits, where computationally-efficient instance-optimality is possible.
- Why unresolved: The computational complexity of achieving instance-optimality in MDPs versus bandits is not well understood.
- What evidence would resolve it: A formal proof of computational separation between MDPs and bandits for instance-optimal PAC RL, or an efficient algorithm for MDPs matching the lower bound.

## Limitations

- The instance-dependent lower bound may not extend to non-tabular settings or continuous state-action spaces
- The H^5 polynomial gap between PEDEL and the lower bound remains unexplained and may be inherent to the algorithm
- The policy diversity condition is characterized in general terms without specific structural conditions on the MDP

## Confidence

- High confidence in the correctness of the change-of-measure argument for deriving the general lower bound (Theorem 1)
- Medium confidence in the policy diversity condition for instance-optimality (Proposition 2) due to its general formulation
- Medium confidence in the characterization of PEDEL's complexity (Proposition 1) as it depends on details of the original algorithm not fully specified in the paper

## Next Checks

1. Verify the lower bound computation on a simple 2-state MDP where analytical solutions exist for both the lower and upper bounds
2. Test the policy diversity condition on a deterministic MDP with two near-optimal policies that take different actions in the same state
3. Implement a modified version of PEDEL that incorporates the optimal design principle more directly and check if the H^5 gap can be reduced