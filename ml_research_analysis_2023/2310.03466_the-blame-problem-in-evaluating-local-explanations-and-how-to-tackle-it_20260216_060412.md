---
ver: rpa2
title: The Blame Problem in Evaluating Local Explanations, and How to Tackle it
arxiv_id: '2310.03466'
source_url: https://arxiv.org/abs/2310.03466
tags:
- explanations
- local
- evaluation
- ground
- truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating local model-agnostic
  explanation techniques in machine learning, highlighting the lack of rigorous measures
  for comparing different approaches. The authors propose a taxonomy of evaluation
  methods, including robustness measures, ground truth extraction from synthetic data
  and interpretable models, model randomization, and human-grounded evaluation.
---

# The Blame Problem in Evaluating Local Explanations, and How to Tackle it

## Quick Facts
- arXiv ID: 2310.03466
- Source URL: https://arxiv.org/abs/2310.03466
- Reference count: 0
- Key outcome: The paper proposes a taxonomy of evaluation methods for local model-agnostic explanations and introduces the "blame problem" concept, highlighting that finding an optimal measure remains an open research problem.

## Executive Summary
This paper addresses the critical challenge of evaluating local model-agnostic explanation techniques in machine learning, where existing methods lack rigorous measures for comparison. The authors introduce the concept of the "blame problem" - the difficulty in determining whether poor explanation performance should be attributed to the explanation technique or the underlying model. They propose a comprehensive taxonomy of evaluation methods including robustness measures, ground truth extraction from synthetic data and interpretable models, model randomization, and human-grounded evaluation. The study argues that while ground truth extraction from interpretable models is the most reliable approach, it still has limitations, and finding an optimal evaluation measure remains an open research problem.

## Method Summary
The paper proposes a taxonomy of evaluation methods for local model-agnostic explanations, including robustness measures, ground truth extraction from synthetic data and interpretable models, model randomization, and human-grounded evaluation. The authors introduce the "blame problem" concept to highlight the challenge of attributing poor explanation performance to either the explanation technique or the underlying model. The method involves creating synthetic datasets with known ground truth importance scores, extracting ground truth from interpretable models, and comparing various explanation techniques using different evaluation metrics. The approach aims to provide a comprehensive framework for assessing local explanations while acknowledging the inherent limitations and trade-offs of existing evaluation methods.

## Key Results
- All evaluation categories except ground truth from interpretable models suffer from the "blame problem"
- Synthetic data evaluation methods may misattribute explanation quality to data generation rather than model learning
- Human-grounded evaluation measures can incorrectly blame explanations when human and model reasoning differ
- The choice of similarity metric significantly impacts which explanations are considered accurate
- Ground truth extraction from interpretable models is the most reliable evaluation method but has limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local explanations inherit blame for poor model performance when the model itself provides unreliable predictions.
- Mechanism: The "blame problem" occurs when evaluation measures rely on the black-box model's predictions as an oracle, but the model's predictions may be incorrect or unstable (e.g., adversarial examples, decision boundary proximity).
- Core assumption: Black-box model predictions are treated as ground truth in robustness and model randomization evaluations.
- Evidence anchors:
  - [abstract] "all categories of evaluation methods, except those based on the ground truth from interpretable models, suffer from a problem we call the "blame problem.""
  - [section 5.1] "However, they rely heavily on the role of the black-box model as an oracle to provide accurate and certain predictions... blaming the local explanation for their lack of robustness is not straightforward."
- Break condition: When evaluation uses ground truth extracted directly from interpretable models rather than black-box predictions.

### Mechanism 2
- Claim: Synthetic data evaluation methods can misattribute explanation quality to the data generation process rather than the model.
- Mechanism: Ground truth extracted from synthetic datasets may not align with what the model actually learned, leading to false negatives in explanation evaluation.
- Core assumption: The model learns representations that match the synthetic data's prior importance scores.
- Evidence anchors:
  - [section 5.2] "we are blaming local explanations even though proving that the explained model has learned a representation that follows our prior importance scores is difficult."
  - [section 5.5] "the ground truth of Orange Skin is equal for all instances irrespective of their position on the decision boundary" showing mismatch between data generation and model behavior.
- Break condition: When synthetic data generation does not reflect the model's actual learning process or when models learn different representations than assumed.

### Mechanism 3
- Claim: Human-grounded evaluation measures can incorrectly blame explanations when humans and models use different reasoning.
- Mechanism: Human subjects may fail to "replay" model predictions not because explanations are poor, but because humans and models solve tasks with fundamentally different logic.
- Core assumption: Human reasoning should align with model reasoning for explanations to be considered accurate.
- Evidence anchors:
  - [section 5.4] "we cannot subjectively measure how much of the mimicking is performed using human subjects' prior knowledge of data or the model" and "Studies have shown that humans and machine learning models rely on different knowledge in performing tasks."
- Break condition: When model and human reasoning processes are inherently different or when humans lack understanding of the model or data.

## Foundational Learning

- Concept: Ground truth extraction methods
  - Why needed here: Understanding how different evaluation methods obtain "ground truth" is crucial for recognizing the blame problem and evaluating explanation quality.
  - Quick check question: What are the three main approaches to obtaining ground truth for local explanation evaluation, and how does each handle the blame problem differently?

- Concept: Robustness measures and their limitations
  - Why needed here: Robustness measures are widely used but suffer from the blame problem, making it essential to understand their mechanics and limitations.
  - Quick check question: Why does nullifying features to test explanation robustness potentially blame explanations for the model's incorrect predictions?

- Concept: Similarity metrics in explanation evaluation
  - Why needed here: The choice of similarity metric significantly affects which explanations are considered accurate, yet this is often overlooked.
- Quick check question: How might using Euclidean distance versus Spearman's rank correlation lead to selecting different explanations as more accurate?

## Architecture Onboarding

- Component map:
  - Evaluation taxonomy: robustness measures, ground truth from synthetic data, ground truth from interpretable models, model randomization, human-grounded evaluation
  - Blame problem detector: identifies when evaluation methods incorrectly attribute poor performance to explanations vs models
  - Similarity metric handler: manages different metrics (Euclidean, cosine, rank correlation) and their implications
  - Synthetic data generator: creates controlled datasets with known importance scores
  - Interpretable model extractor: obtains ground truth directly from simpler models

- Critical path:
  1. Select evaluation method based on problem context and data type
  2. Implement similarity metric appropriate to data domain
  3. Extract ground truth (synthetic or interpretable model)
  4. Compare explanations to ground truth
  5. Analyze results for blame problem indicators

- Design tradeoffs:
  - Robustness measures are easy to implement but suffer from blame problem
  - Synthetic data methods provide controlled experiments but may not reflect model learning
  - Interpretable model methods avoid blame problem but require explanation techniques to work on simpler models
  - Human-grounded methods provide real-world validation but introduce human bias and reasoning differences

- Failure signatures:
  - High variance in evaluation results across similar models/datasets
  - Consistent disagreement between different evaluation methods
  - Explanations deemed poor by one metric but good by another
  - Poor human replay performance despite good quantitative scores

- First 3 experiments:
  1. Implement MIAS scores extraction from logistic regression and compare with LIME explanations on a synthetic dataset
  2. Test robustness measures on a model near decision boundaries to observe blame problem
  3. Compare evaluation results using Euclidean distance vs Spearman correlation on tabular data to observe metric impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal measure for evaluating local model-agnostic explanations, considering the limitations and trade-offs of existing methods?
- Basis in paper: [explicit] The paper explicitly states that "finding the optimal measure for evaluating local model-agnostic explanations remains an open research problem."
- Why unresolved: All existing evaluation methods have inherent weaknesses and trade-offs, including the "blame problem" where it's unclear whether poor explanation performance should be attributed to the explanation technique or the underlying model.
- What evidence would resolve it: