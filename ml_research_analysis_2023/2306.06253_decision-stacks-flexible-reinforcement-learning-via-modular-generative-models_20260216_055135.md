---
ver: rpa2
title: 'Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models'
arxiv_id: '2306.06253'
source_url: https://arxiv.org/abs/2306.06253
tags:
- decision
- learning
- offline
- reward
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Decision Stacks, a modular generative framework
  for goal-conditioned reinforcement learning. The method decomposes policy agents
  into three independent generative modules: observation prediction, reward estimation,
  and action generation.'
---

# Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models

## Quick Facts
- arXiv ID: 2306.06253
- Source URL: https://arxiv.org/abs/2306.06253
- Reference count: 13
- Key outcome: Decision Stacks achieves 15.7% improvement over closest baseline in POMDP environments through modular generative decomposition

## Executive Summary
Decision Stacks introduces a novel modular framework for goal-conditioned reinforcement learning that decomposes policy agents into three independent generative modules: observation prediction, reward estimation, and action generation. Each module can be independently parameterized using different generative architectures (e.g., transformers, diffusion models) optimized for its specific token type characteristics. The framework demonstrates significant performance gains on offline RL benchmarks, particularly in POMDP environments where reward modeling provides crucial context for hidden state inference.

The key innovation lies in combining architectural flexibility with autoregressive chaining, allowing each module to learn its own optimal representation while maintaining maximal expressivity through sequential generation. Decision Stacks shows particular strength in long-horizon planning tasks, generating robust trajectory plans that match action sequences without requiring hand-coded controllers. The method's modular design enables parallel training via teacher forcing while preserving the ability to represent complex joint distributions through autoregressive dependencies.

## Method Summary
Decision Stacks implements a three-module generative framework where observation, reward, and action modeling are separated into independent components trained in parallel using teacher forcing. The observation module uses diffusion models with inpainting conditioning for continuous observations, while reward and action modules employ transformers or MLPs for discrete and continuous data respectively. During inference, modules are chained autoregressively where outputs from one module become inputs to the next, enabling flexible open-loop or closed-loop planning strategies. The framework is evaluated on D4RL offline datasets including Maze2D planning tasks and locomotion environments in both MDP and POMDP settings.

## Key Results
- Decision Stacks achieves 15.7% improvement over closest baseline in POMDP environments
- Significant performance gains in long-horizon planning tasks, particularly in Maze2D environments
- Reward modeling is critical for POMDP performance, with ablation studies showing substantial degradation when removed
- Architectural flexibility allows mixing different generative models (diffusion for observations, transformers for rewards/actions) yields better performance than monolithic approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular decomposition reduces cross-modal interference and improves conditioning flexibility
- Mechanism: Independent modules allow each token type to learn its own optimal architecture and training objective without gradient interference
- Core assumption: Token types (observations, rewards, actions) are sufficiently distinct structurally and semantically
- Evidence anchors: Decision Stacks can mix diffusion models for observations with transformers for rewards/actions; structural differences in token types motivate token-level modularity
- Break condition: If token types become more homogeneous, modular design may not provide advantage

### Mechanism 2
- Claim: Reward modeling is essential for strong POMDP performance by providing hidden state context
- Mechanism: Reward module conditions future observations and actions on predicted reward trajectories, encoding task-relevant information missing from partial observations
- Core assumption: Rewards are informative about hidden state dynamics and can be accurately predicted from observation histories
- Evidence anchors: Ablating reward modeling leads to 15.7% lower performance in POMDPs; MLP action models without reward inputs underperform generative action models with rewards
- Break condition: If rewards are sparse or uninformative, benefit of reward modeling diminishes

### Mechanism 3
- Claim: Autoregressive chaining guarantees maximal expressivity while enabling independent training
- Mechanism: Output of one module becomes input to next, forming chain that can represent any joint distribution; training each module separately avoids gradient interference
- Core assumption: Autoregressive factorization is expressive enough to approximate true joint distribution given sufficient capacity
- Evidence anchors: Under idealized conditions, autoregressive structure guarantees maximal expressivity; Decision Stacks outperforms Trajectory Transformer which uses single autoregressive model
- Break condition: If dataset is too small relative to model capacity, overfitting in individual modules could degrade performance

## Foundational Learning

- Concept: Autoregressive generative models and teacher forcing
  - Why needed here: Decision Stacks relies on autoregressive transformers for reward and action modules; understanding teacher forcing is critical for independent module training
  - Quick check question: In teacher forcing, what data do we feed into the model during training versus what we sample during inference?

- Concept: Diffusion models and classifier-free guidance
  - Why needed here: Observation module uses diffusion with inpainting conditioning; understanding guidance and denoising steps is essential for implementation
  - Quick check question: In classifier-free guidance, what two models are interpolated, and what hyperparameter controls the interpolation strength?

- Concept: POMDPs and partial observability
  - Why needed here: Decision Stacks explicitly evaluated on POMDP environments where observations are incomplete; understanding difference from MDPs is key to interpreting results
  - Quick check question: In a POMDP, what is the relationship between the observation and the underlying state?

## Architecture Onboarding

- Component map: Observation module (diffusion/U-Net) -> Reward module (transformer/MLP) -> Action module (transformer/MLP)
- Critical path:
  1. Train observation module on observation sequences conditioned on goals
  2. Train reward module on observation+reward sequences
  3. Train action module on observation+reward+action sequences
  4. At inference, generate observation plan → reward plan → action plan (open-loop) or step-by-step (closed-loop)
- Design tradeoffs:
  - Modularity vs. parameter sharing: separate modules allow architectural flexibility but increase total parameters
  - Diffusion vs. transformer for observations: diffusion handles continuous image observations better, transformers are faster for discrete data
  - Reward modeling vs. no reward: including rewards improves POMDP performance but adds complexity
- Failure signatures:
  - Poor planning quality: likely observation module underfitting or inpainting conditioning broken
  - Actions not matching planned trajectory: action module not conditioning properly on predicted rewards/observations
  - Instability in POMDPs: missing or inaccurate reward modeling
- First 3 experiments:
  1. Train and evaluate observation module alone on Maze2D to verify plan quality
  2. Add reward module and compare closed-loop vs open-loop action generation
  3. Swap observation module architecture (diffusion ↔ transformer) to measure impact on downstream performance

## Open Questions the Paper Calls Out

- How would Decision Stacks perform in image-based environments where observations are high-dimensional visual inputs? The paper explicitly states this as a limitation, noting experiments are limited to state-based environments and extending to image-based environments is a promising direction, especially given POMDP gains.

- What is the relative importance of each module (observation, reward, action) in Decision Stacks' overall performance? While the paper shows different architectural choices for each module, it doesn't systematically ablate individual modules to determine their relative contributions.

- How well do Decision Stacks modules transfer across different environments with shared structure? The paper states interest in exploring modular design benefits for pretraining and transfer of modules across similar environments and testing generalization abilities.

## Limitations
- Modular decomposition may not generalize well to domains where observations, rewards, and actions share similar statistical properties
- Autoregressive chaining could propagate errors across modules, with error accumulation in long-horizon planning not thoroughly analyzed
- Performance advantages may partly stem from architectural choices (e.g., diffusion vs. transformer) rather than modular design itself

## Confidence

- High confidence: Modular architecture design and teacher forcing training procedure are clearly specified and reproducible; ablation showing reward modeling importance (15.7% performance drop in POMDPs) provides strong empirical support
- Medium confidence: Claim of significant baseline improvement (15.7%) is based on established methods but exact advantages beyond modularity not fully isolated
- Medium confidence: Assertion that autoregressive chaining guarantees maximal expressivity is theoretically sound but may not hold in practice given finite capacity and training data constraints

## Next Checks
1. Systematically compare Decision Stacks against non-modular variants using same architectures but joint training to isolate benefit of modularity
2. Measure and visualize error accumulation across three-module chain in long-horizon planning, comparing open-loop vs closed-loop strategies
3. Evaluate Decision Stacks on domains with homogeneous token types (e.g., low-dimensional continuous control) to test whether modular design still provides advantages or becomes a handicap