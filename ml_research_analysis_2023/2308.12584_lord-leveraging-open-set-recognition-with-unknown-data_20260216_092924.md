---
ver: rpa2
title: 'LORD: Leveraging Open-Set Recognition with Unknown Data'
arxiv_id: '2308.12584'
source_url: https://arxiv.org/abs/2308.12584
tags:
- kucs
- ratio
- ieee
- mixup-to-known
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LORD, a framework for improving open-set\
  \ recognition (OSR) by exploiting background data during classifier training. LORD\
  \ identifies three model-agnostic training strategies\u2014Single Pseudo Label (SPL),\
  \ Multi Pseudo Label (MPL), and Known vs."
---

# LORD: Leveraging Open-Set Recognition with Unknown Data

## Quick Facts
- arXiv ID: 2308.12584
- Source URL: https://arxiv.org/abs/2308.12584
- Reference count: 40
- This paper introduces LORD, a framework for improving open-set recognition (OSR) by exploiting background data during classifier training.

## Executive Summary
This paper introduces LORD, a framework for improving open-set recognition (OSR) by exploiting background data during classifier training. LORD identifies three model-agnostic training strategies—Single Pseudo Label (SPL), Multi Pseudo Label (MPL), and Known vs. Rest (KvR)—to leverage known unknown classes (KUCs) and improve recognition of unknown unknown classes (UUCs). Experiments across six classifiers and three datasets show that KUC exploitation improves OSR performance by up to 30% for biased evaluation and more modestly for unbiased evaluation. To address the lack of background data, LORD proposes constrained mixup as an effective synthetic data generation technique, with KvR emerging as the most stable strategy. The work also highlights the importance of selecting appropriate evaluation metrics and strategies based on application requirements.

## Method Summary
LORD introduces a framework that explicitly models open space during classifier training by incorporating known unknown classes (KUCs) into the training process. The framework proposes three model-agnostic training strategies: Single Pseudo Label (SPL), Multi Pseudo Label (MPL), and Known vs. Rest (KvR). These strategies leverage KUCs to improve the recognition of unknown unknown classes (UUCs). When genuine KUCs are unavailable, LORD employs constrained mixup as a synthetic data generation technique to create plausible unknown samples. The framework is evaluated across six classifiers (OSNN, DNN, EVM, C-EVM, W-SVM, PI-SVM) on three datasets (CIFAR-100, LFW, CASIA-WebFace) using both biased and unbiased evaluation protocols.

## Key Results
- KUC exploitation improves OSR performance by up to 30% for biased evaluation and more modestly for unbiased evaluation
- KvR emerges as the most stable training strategy across different classifiers and datasets
- Constrained mixup effectively substitutes for genuine background data when unavailable
- The choice of evaluation metric (AUC-ROC vs OSCR) significantly impacts the interpretation of results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly modeling open space during training improves open-set recognition by reducing overconfidence on unknown classes.
- Mechanism: Training strategies like Single Pseudo Label (SPL), Multi Pseudo Label (MPL), and Known vs. Rest (KvR) introduce background data or synthetic data into the training process. This forces the classifier to learn decision boundaries that explicitly account for the presence of unknown classes, reducing the tendency to assign high confidence to unknowns.
- Core assumption: Background data (genuine or synthetic) is available or can be effectively generated, and the classifier can learn from this data to improve open-set recognition.
- Evidence anchors:
  - [abstract] "LORD explicitly models open space during classifier training and provides a systematic evaluation for such approaches."
  - [section 4.1] "Incorporating KUCs into the training process of classifiers can be achieved using various strategies."
- Break condition: If background data is unavailable and synthetic data generation fails to adequately represent the unknown class distribution, the benefits of this mechanism will be limited.

### Mechanism 2
- Claim: Using constrained mixup as a data generation technique effectively substitutes for genuine background data in open-set recognition.
- Mechanism: Mixup generates synthetic data by convex combinations of samples from known classes, which are then assigned to an unknown label. Constrained mixup adds a filter to ensure these synthetic samples do not overlap with known class centroids, addressing the "occupation problem" where synthetic data inadvertently falls within known class boundaries.
- Core assumption: The feature space is well-behaved enough that linear combinations of known class samples can represent plausible unknown samples, and the centroid-based constraint effectively prevents overlap.
- Evidence anchors:
  - [abstract] "LORD offers a solution in the form of mixup as an off-the-shelf data generation technique."
  - [section 5.3] "We propose an effective constraint for an on-the-fly mixup generation."
- Break condition: If the feature space is highly non-linear or the known class distributions are complex, mixup may not generate representative unknown samples, and the centroid-based constraint may not be sufficient.

### Mechanism 3
- Claim: The choice of training strategy (SPL, MPL, KvR) significantly impacts open-set recognition performance depending on the underlying model and dataset characteristics.
- Mechanism: Different strategies have varying levels of complexity and assumptions about the background data. SPL treats all background data as a single class, MPL assigns individual labels to each background sample, and KvR uses background data only as negative examples for known classes. The effectiveness of each strategy depends on factors like the number of background samples, their diversity, and the classifier's ability to handle different label configurations.
- Core assumption: The underlying classifier is compatible with the chosen training strategy, and the strategy aligns with the characteristics of the background data.
- Evidence anchors:
  - [abstract] "LORD encompasses a comprehensive range of OSR metrics that facilitate in-depth analysis across various requirement levels."
  - [section 4.3] "For OSNN, cf. Figs. 3a – 3c, we observe consistent improvements by SPL, MPL, and KvR."
- Break condition: If the classifier is not compatible with the chosen strategy (e.g., MPL is not tractable for models with many background samples), or if the background data does not align with the strategy's assumptions, the performance may not improve.

## Foundational Learning

- Concept: Open-set recognition (OSR)
  - Why needed here: The paper's core contribution is improving OSR by exploiting background data. Understanding OSR is fundamental to grasping the problem and the proposed solutions.
  - Quick check question: What is the key difference between closed-set and open-set recognition?

- Concept: Semi-supervised learning
  - Why needed here: The paper leverages concepts from semi-supervised learning, such as using unlabeled data to improve model performance. Understanding these concepts is crucial for understanding how KUCs are utilized.
  - Quick check question: How does the use of unlabeled data in semi-supervised learning relate to the use of KUCs in OSR?

- Concept: Data augmentation techniques (e.g., mixup)
  - Why needed here: The paper proposes using mixup as a method for generating synthetic background data. Understanding mixup and its properties is essential for understanding this aspect of the solution.
  - Quick check question: How does mixup generate new data samples, and what are its potential benefits and limitations?

## Architecture Onboarding

- Component map:
  - LORD framework -> Training strategies (SPL, MPL, KvR) -> Background data (genuine or synthetic via mixup) -> Classifiers (OSNN, DNN, EVM, C-EVM, W-SVM, PI-SVM) -> Evaluation metrics (AUC-ROC, OSCR)

- Critical path:
  1. Collect or generate background data (KUCs)
  2. Choose an appropriate training strategy (SPL, MPL, KvR) based on the classifier and dataset
  3. Train the classifier with the chosen strategy and background data
  4. Evaluate the classifier's open-set recognition performance using relevant metrics
  5. If background data is unavailable, generate synthetic data using constrained mixup

- Design tradeoffs:
  - Using genuine KUCs vs. synthetic KUCs: Genuine KUCs may provide better performance but are often expensive or unavailable. Synthetic KUCs are more accessible but may not perfectly represent the unknown class distribution.
  - Choice of training strategy: Different strategies have varying levels of complexity and assumptions, impacting performance and scalability.
  - Evaluation metrics: AUC-ROC measures overall discrimination ability, while OSCR focuses on the relevant FPR range for OSR applications.

- Failure signatures:
  - Poor open-set recognition performance: May indicate issues with background data quality, training strategy choice, or classifier compatibility.
  - High computational cost: May result from using complex training strategies or large amounts of background data.
  - Overfitting to background data: May occur if the background data is not representative of the true unknown class distribution.

- First 3 experiments:
  1. Evaluate the impact of different training strategies (SPL, MPL, KvR) on a simple classifier (e.g., OSNN) using a standard dataset (e.g., CIFAR-100).
  2. Compare the performance of genuine KUCs vs. synthetic KUCs generated using constrained mixup on a DNN classifier.
  3. Investigate the effect of the occupation problem on OSNN and evaluate the effectiveness of the centroid-based constraint for mitigating this issue.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of mixup as a substitute for genuine known unknown classes (KUCs) generalize to other datasets beyond CIFAR-100, LFW, and CASIA-WebFace?
- Basis in paper: [inferred] The paper evaluates mixup primarily on CIFAR-100 and briefly mentions LFW and CASIA-WebFace, but does not explore its performance on a wider range of datasets.
- Why unresolved: The paper's experiments are limited to three datasets, which may not be representative of all possible data distributions and feature spaces.
- What evidence would resolve it: Experiments applying mixup across a diverse set of datasets with varying characteristics (e.g., medical imaging, satellite imagery, text data) and comparing its performance to genuine KUCs.

### Open Question 2
- Question: How do the three model-agnostic training strategies (SPL, MPL, KvR) perform on more complex models like transformer-based architectures?
- Basis in paper: [explicit] The paper evaluates the strategies on six classifiers spanning four categories (OSNN, DNN, EVM, and SVM), but does not include transformer-based models.
- Why unresolved: The paper's scope is limited to specific model types, leaving the generalizability of the strategies to other architectures unexplored.
- What evidence would resolve it: Applying the training strategies to transformer-based models and comparing their performance to the existing models in the paper.

### Open Question 3
- Question: Can the constraints for solving the occupation problem be further optimized to improve open-set recognition performance?
- Basis in paper: [explicit] The paper proposes a constraint based on the mean distance to class centroids, but does not explore other potential constraint formulations or optimization techniques.
- Why unresolved: The paper only evaluates one constraint formulation and does not investigate alternative approaches or methods for optimizing the constraint parameters.
- What evidence would resolve it: Exploring different constraint formulations (e.g., based on other distance metrics, clustering algorithms) and optimizing their parameters through hyperparameter tuning or automated search methods.

## Limitations
- Performance gains are dataset-dependent and may not transfer to domains with significantly different feature distributions
- The mixup constraint relies on centroid-based separation, which may not capture complex decision boundaries in high-dimensional spaces
- The study focuses on standard image datasets, limiting applicability to domains with different data characteristics

## Confidence
- High confidence in the overall framework design and evaluation methodology
- Medium confidence in the synthetic data generation effectiveness across diverse domains
- Medium confidence in the generalization of results across different classifier architectures

## Next Checks
1. Test mixup constraint effectiveness on non-image domains (e.g., text or audio) where feature distributions may be more complex
2. Evaluate KvR strategy scalability with varying numbers of background classes to identify computational bottlenecks
3. Compare constrained mixup against alternative synthetic data generation methods (e.g., GAN-based approaches) to validate effectiveness claims