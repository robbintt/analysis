---
ver: rpa2
title: 'Lion: Adversarial Distillation of Proprietary Large Language Models'
arxiv_id: '2305.12870'
source_url: https://arxiv.org/abs/2305.12870
tags:
- instructions
- have
- student
- your
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an adversarial distillation framework for transferring
  knowledge from a closed-source large language model (LLM) to an open-source LLM.
  The method leverages the versatility of LLMs to iteratively improve the student
  model's performance by identifying and generating challenging instructions.
---

# Lion: Adversarial Distillation of Proprietary Large Language Models

## Quick Facts
- arXiv ID: 2305.12870
- Source URL: https://arxiv.org/abs/2305.12870
- Authors: 
- Reference count: 40
- Key outcome: Achieves nearly 95% capability approximation of ChatGPT using only 70k training data

## Executive Summary
This paper introduces an adversarial distillation framework for transferring knowledge from closed-source large language models (LLMs) to open-source LLMs. The method leverages the versatility of LLMs to iteratively improve student model performance by identifying and generating challenging instructions. The framework is applied to distill ChatGPT into a compact 7B-parameter model, Lion, demonstrating superior performance across various task categories and evaluation paradigms. The approach outperforms previous state-of-the-art baselines while using significantly less training data.

## Method Summary
The framework employs a three-stage adversarial loop: imitation (student learns from teacher outputs), discrimination (referee identifies performance gaps), and generation (generator creates new challenging instructions). Starting with Alpaca's 52K instruction-following examples, the method iteratively runs three rounds of distillation using ChatGPT as teacher, referee, and generator. The student model is fine-tuned using the accumulated 70K instructions, achieving high capability approximation of the proprietary model while maintaining efficiency.

## Key Results
- Achieves nearly 95% capability approximation of ChatGPT
- Uses only 70k training data (significantly less than competitors)
- Outperforms Vicuna and WizardLM baselines across multiple task categories
- Demonstrates strong performance in both automatic GPT-4 evaluation and human alignment assessment

## Why This Works (Mechanism)

### Mechanism 1
The adversarial distillation loop improves student performance by iteratively focusing on "hard" instructions where student and teacher performance differ most. The framework creates a positive feedback loop where the student learns from the teacher's responses, the referee identifies instructions with significant performance gaps, and the generator creates new challenging instructions based on these gaps. The core assumption is that large language models can effectively serve as unbiased referees to quantify performance differences between two AI assistants.

### Mechanism 2
Generating new instructions that mirror the distribution of hard instructions accelerates student learning. The generator creates new instructions that belong to the same domain and task type as identified hard instructions, maintaining similar difficulty levels while increasing diversity. The core assumption is that LLMs can generate high-quality, diverse instructions that effectively represent challenging problem spaces.

### Mechanism 3
The three-stage adversarial loop creates a dynamic learning environment that adapts to student progress. Each iteration alternates between imitation (student learning), discrimination (identifying gaps), and generation (creating new challenges), forming a continuous improvement cycle. The core assumption is that the dynamic min-max game interpretation ensures the student progressively masters increasingly difficult tasks.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Extends standard KD principles to black-box settings where teacher weights are inaccessible
  - Quick check question: How does black-box KD differ from traditional KD where teacher logits are available?

- Concept: Adversarial Learning
  - Why needed here: Uses adversarial principles to iteratively generate harder training examples
  - Quick check question: What distinguishes this adversarial approach from standard GAN-based KD methods?

- Concept: Instruction Tuning
  - Why needed here: Student model is fine-tuned on instruction-following datasets rather than general language modeling
  - Quick check question: Why might instruction tuning be more effective than standard fine-tuning for this application?

## Architecture Onboarding

- Component map: Teacher (ChatGPT) -> Student (LLaMA-7B) -> Referee (ChatGPT) -> Generator (ChatGPT) -> Train Pool (instruction-response pairs) -> Cache Pool (evaluation set)
- Critical path: Train Pool → Imitation (student training) → Cache Pool → Discrimination (identify hard instructions) → Generation (create new instructions) → Update Train Pool
- Design tradeoffs: Using black-box teacher requires more API calls but enables knowledge transfer from proprietary models; iterative approach increases training time but improves sample efficiency
- Failure signatures: Student performance plateaus despite additional iterations; generated instructions become repetitive; referee consistently fails to identify meaningful performance gaps
- First 3 experiments:
  1. Baseline: Train student on Alpaca dataset only, compare to Lion performance
  2. Single iteration: Run one full adversarial cycle and measure improvement
  3. Ablation: Remove referee component and see if student still improves through other mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adversarial distillation framework perform when applied to larger models beyond 7B parameters?
- Basis in paper: [inferred] The paper applies the framework to a 7B parameter model and achieves 95% capability approximation, suggesting potential for scaling.
- Why unresolved: The paper does not explore the framework's effectiveness on larger models, leaving questions about its scalability and efficiency at different model sizes.
- What evidence would resolve it: Experiments applying the framework to models with 13B or more parameters, comparing performance and resource usage to the 7B model results.

### Open Question 2
- Question: What is the impact of the initial instruction dataset size on the final model performance?
- Basis in paper: [explicit] The framework starts with Alpaca's 52K instruction dataset and generates additional instructions through iterations.
- Why unresolved: The paper does not investigate how different initial dataset sizes might affect the final model's capabilities or the efficiency of the distillation process.
- What evidence would resolve it: Experiments with varying initial dataset sizes, measuring the number of iterations and final performance for each.

### Open Question 3
- Question: How does the framework handle tasks that require domain-specific knowledge not present in the teacher model?
- Basis in paper: [inferred] The framework's ability to generate and discriminate "hard" instructions suggests potential for domain adaptation.
- Why unresolved: The paper does not address the framework's performance on tasks requiring specialized knowledge beyond general instruction following.
- What evidence would resolve it: Testing the framework on domain-specific tasks (e.g., medical, legal) and evaluating the student model's performance compared to the teacher model.

### Open Question 4
- Question: What are the limitations of using GPT-4 as an automatic evaluator for the distilled models?
- Basis in paper: [explicit] The paper uses GPT-4 to automatically evaluate response quality against ChatGPT.
- Why unresolved: The paper acknowledges potential limitations of automated evaluation but does not explore the specific biases or inaccuracies of using GPT-4 as an evaluator.
- What evidence would resolve it: Comparative studies between GPT-4 evaluations and human assessments across various task categories, identifying areas where GPT-4 may be less reliable.

## Limitations

- Lack of complete prompt templates for the three ChatGPT roles (teacher, referee, generator)
- Unspecified exact thresholds and sampling ratios used during discrimination and generation stages
- Reliance on black-box teacher model prevents full validation of the approach
- Potential biases in evaluation metrics (GPT-4 automatic scoring and human alignment assessment)

## Confidence

**High confidence**: The core adversarial distillation framework and its three-stage iterative process are well-described and logically sound. The overall experimental design and baseline comparisons are clearly presented.

**Medium confidence**: The quantitative results showing 95% capability approximation are plausible given the methodology, but depend heavily on the quality and completeness of the evaluation setup. The claim of superior performance across task categories is supported by benchmark results but requires careful interpretation.

**Low confidence**: The specific claim about achieving such high performance with only 70k training data is difficult to fully verify without complete implementation details and independent replication. The effectiveness of the referee mechanism in providing unbiased performance gap measurements also has medium confidence given limited empirical validation.

## Next Checks

1. **Replication of the adversarial loop with simplified setup**: Implement the three-stage distillation process using publicly available open-source models as both teacher and student (rather than black-box systems), using the Alpaca dataset as initial training data. Run at least two complete adversarial iterations and measure performance improvements on standard benchmarks like Vicuna and AlpacaEval.

2. **Referee effectiveness validation**: Conduct controlled experiments to test whether the ChatGPT referee can consistently and accurately identify performance gaps between different model pairs. This could involve creating synthetic instruction-response pairs with known quality differences and measuring the referee's ability to correctly rank them.

3. **Instruction generation diversity analysis**: Track ROUGE-L overlap metrics during the adversarial distillation process to verify that generated instructions maintain diversity while targeting identified hard instruction domains. Set up a threshold-based monitoring system to detect when instruction generation begins to stagnate or produce repetitive content.