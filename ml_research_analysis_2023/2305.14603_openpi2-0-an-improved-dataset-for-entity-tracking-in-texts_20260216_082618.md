---
ver: rpa2
title: 'OpenPI2.0: An Improved Dataset for Entity Tracking in Texts'
arxiv_id: '2305.14603'
source_url: https://arxiv.org/abs/2305.14603
tags:
- entity
- entities
- salience
- attributes
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenPI2.0 is a dataset for entity tracking in procedural texts,
  improving upon OpenPI by canonicalizing entities/attributes and adding salience
  annotations. It uses GPT-3.5 to cluster entities/attributes and expand clusters
  with paraphrases.
---

# OpenPI2.0: An Improved Dataset for Entity Tracking in Texts

## Quick Facts
- **arXiv ID**: 2305.14603
- **Source URL**: https://arxiv.org/abs/2305.14603
- **Reference count**: 21
- **Primary result**: OpenPI2.0 improves entity tracking in procedural texts through canonicalization and salience annotations

## Executive Summary
OpenPI2.0 is an enhanced dataset for entity tracking in procedural texts that addresses key limitations of its predecessor, OpenPI. The dataset introduces entity and attribute canonicalization through GPT-3.5 clustering, adds human-labeled and model-predicted salience annotations, and provides fine-grained evaluation capabilities. The improvements enable more accurate evaluation by reducing false negatives from paraphrase variations and allow downstream tasks like question answering and planning to benefit from salience-aware prompting strategies.

## Method Summary
OpenPI2.0 builds upon OpenPI by implementing three key improvements: entity/attribute canonicalization through GPT-3.5 clustering of paraphrases, addition of salience annotations for entities, and fine-grained evaluation separating schemata prediction from state changes. The canonicalization process uses GPT-3.5 to cluster similar entity mentions and attributes, then expands clusters with additional paraphrases to improve coverage. Salience scores are collected through human annotation and model prediction, enabling the use of salient entities as chain-of-thought prompts for downstream tasks.

## Key Results
- Entity tracking models still struggle with state prediction despite improvements in OpenPI2.0
- Using salient entities as chain-of-thought prompts improves downstream task performance on question answering and planning
- GPT-3.5-based clustering effectively reduces false negatives in evaluation by mapping paraphrases to canonical entities/attributes

## Why This Works (Mechanism)

### Mechanism 1: Entity/Attribute Canonicalization
Canonicalization through GPT-3.5 clustering creates unified entity/attribute representations that map multiple surface forms to canonical clusters, enabling fairer evaluation by reducing false negatives from paraphrase variations. The mechanism clusters paraphrases using contextual prompts and expands clusters with additional paraphrases, addressing the challenge where different mentions of the same entity or attribute render evaluation difficult.

### Mechanism 2: Entity Salience Annotations
Entity salience annotations improve downstream task performance by focusing attention on critical elements through chain-of-thought prompting. Using salient entities as intermediate reasoning steps guides models to prioritize relevant state changes over peripheral ones, as demonstrated by improved performance on question answering and planning tasks when salience information is incorporated.

### Mechanism 3: Fine-Grained Evaluation
Fine-grained evaluation separating schemata (entities and attributes) prediction from state change prediction provides better insights into model capabilities than complete sentence evaluation. This modular approach allows for targeted model improvements by isolating different aspects of entity tracking and provides more diagnostic information about specific strengths and weaknesses.

## Foundational Learning

- **Entity canonicalization through clustering**: Needed to address evaluation challenges from different surface forms of the same entity/attribute. Quick check: If a model predicts "coffee maker" and the ground truth contains "espresso machine", should this be marked as correct or incorrect?
- **Chain-of-thought prompting**: Needed to improve downstream task performance by using salient entities as intermediate reasoning steps. Quick check: How does providing salient entities as chain-of-thought differ from directly prompting for the final answer?
- **Fine-grained evaluation metrics**: Needed to provide more diagnostic information than complete sentence evaluation. Quick check: Why might a model perform well on schemata prediction but poorly on states prediction?

## Architecture Onboarding

- **Component map**: OpenPI → GPT-3.5 clustering → canonicalization → salience annotation → fine-grained evaluation → downstream application
- **Critical path**: Data collection → canonicalization → evaluation → downstream application
- **Design tradeoffs**: Clustering accuracy vs computational cost (5 runs vs single pass), human annotation vs model prediction for salience, fine-grained vs holistic evaluation metrics
- **Failure signatures**: Low F1 scores with high BERTScore (paraphrase generation issues), high correlation between human annotators but low model performance (annotation subjectivity), downstream task failure despite good entity tracking (salience annotation problems)
- **First 3 experiments**: 1) Evaluate clustering accuracy on a small manually-labeled subset, 2) Compare downstream performance with/without salience annotations, 3) Test fine-grained vs holistic evaluation on model predictions

## Open Questions the Paper Calls Out

- **Attribute prediction precision**: How to improve precision of attribute predictions, particularly for models like gpt-3.5-turbo that tend to predict generic or static attributes despite clear instructions? The paper suggests re-prompting models to validate attribute changes but doesn't evaluate this strategy's effectiveness.

- **Entity clustering challenges**: What specific challenges exist in clustering entities compared to attributes, and how can these be addressed? While the paper identifies context-dependency as a key difference, it doesn't propose specific methods to improve entity clustering performance.

- **Salience annotation subjectivity**: How to minimize subjectivity in human annotation of entity salience to ensure more consistent annotations? The paper acknowledges the issue but doesn't provide detailed strategies for reducing inter-annotator variability.

- **Systematic evaluation of salience prompting**: How to systematically evaluate the effectiveness of using entity salience as chain-of-thought prompts across diverse downstream tasks? The paper provides qualitative examples but lacks comprehensive quantitative evaluation.

## Limitations

- The clustering mechanism may fail for context-dependent entities that share similar surface forms but refer to different things
- Salience annotations show moderate inter-annotator agreement (Pearson correlation 0.67-0.72), indicating some subjectivity
- The approach relies heavily on GPT-3.5 for canonicalization, which may not generalize well to all procedural domains

## Confidence

- **High confidence**: Canonicalization mechanism effectively reduces false negatives through paraphrase clustering
- **Medium confidence**: Downstream task improvements using salient entities as chain-of-thought prompts
- **Medium confidence**: Fine-grained evaluation approach provides better insights than holistic evaluation

## Next Checks

1. **Cluster accuracy validation**: Manually evaluate clustering accuracy on a small subset to verify correct grouping of entity/attribute paraphrases, particularly for context-dependent mentions.

2. **Salience annotation consistency**: Conduct additional human annotations on sample steps to measure inter-annotator agreement beyond reported Pearson correlations, focusing on borderline cases.

3. **Downstream task generalization**: Test chain-of-thought prompting approach across broader range of procedural domains and downstream tasks to assess generalizability beyond specific examples.