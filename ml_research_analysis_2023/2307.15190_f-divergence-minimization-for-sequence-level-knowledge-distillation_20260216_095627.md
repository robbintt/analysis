---
ver: rpa2
title: f-Divergence Minimization for Sequence-Level Knowledge Distillation
arxiv_id: '2307.15190'
source_url: https://arxiv.org/abs/2307.15190
tags:
- student
- teacher
- distillation
- distill
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces f-DISTILL, a framework for sequence-level
  knowledge distillation that formulates KD as minimizing f-divergence between teacher
  and student models. The authors propose four distilling variants (KL, RKL, JS, TVD)
  and show existing SeqKD and ENGINE methods are approximations of their approach.
---

# f-Divergence Minimization for Sequence-Level Knowledge Distillation

## Quick Facts
- arXiv ID: 2307.15190
- Source URL: https://arxiv.org/abs/2307.15190
- Reference count: 40
- Key outcome: Introduces f-DISTILL framework showing symmetric f-divergences (JS, TVD) better balance mode averaging/collapsing than asymmetric ones (KL, RKL)

## Executive Summary
This paper proposes f-DISTILL, a framework for sequence-level knowledge distillation that formulates KD as minimizing f-divergence between teacher and student models. The authors show that existing methods like SeqKD and ENGINE are approximations of their approach. By deriving step-wise decomposition, they reduce intractable sequence-level divergence to tractable word-level losses. Experiments on four datasets demonstrate that symmetric divergences (Jensen-Shannon, Total Variation Distance) outperform asymmetric ones by better balancing mode averaging and collapsing behaviors.

## Method Summary
The f-DISTILL framework treats knowledge distillation as minimizing f-divergence between teacher and student distributions. For sequence generation tasks, the authors derive step-wise decomposition that converts sequence-level divergences into word-level losses, making them computationally tractable. Four variants are implemented: KL divergence (asymmetric, causes mode averaging), Reverse KL (asymmetric, causes mode collapsing), Jensen-Shannon divergence (symmetric, balances both), and Total Variation Distance (symmetric, balances both). The method uses offline sampling from the teacher model to improve training efficiency.

## Key Results
- Symmetric divergences (JS, TVD) outperform asymmetric ones (KL, RKL) by balancing mode averaging and collapsing
- Offline sampling significantly improves training efficiency without sacrificing performance
- The framework generalizes existing methods like SeqKD and ENGINE as special cases
- JS and TVD variants achieve state-of-the-art results on four diverse datasets

## Why This Works (Mechanism)

### Mechanism 1
Sequence-level f-divergence can be decomposed into tractable word-level losses. The paper proves that KL, RKL, and JS divergences can be decomposed exactly, while TVD can be upper bounded by step-wise terms. This works because sequences can be expressed as products of conditional probabilities at each step. The decomposition fails if conditional independence assumptions break down or sequence length causes numerical instability.

### Mechanism 2
Symmetric f-divergences better balance mode averaging and collapsing than asymmetric ones. Asymmetric divergences require one distribution to cover the support of the other, leading to either mode averaging (KL) or mode collapsing (RKL). Symmetric divergences don't have this requirement, making them more flexible. This mechanism breaks down for inherently unimodal tasks like machine translation where mode averaging isn't problematic.

### Mechanism 3
Offline sampling from teacher model improves training efficiency without performance loss. Since the teacher model is fixed during training, its samples can be precomputed once and reused. This works because teacher distribution doesn't change during training. The mechanism breaks if teacher model parameters change during training, which they don't in standard KD.

## Foundational Learning

- **Concept**: f-divergence as generalization of KL divergence
  - Why needed: The paper builds a unified framework around f-divergences, showing existing methods are special cases
  - Quick check: What is the relationship between KL divergence and f-divergence?

- **Concept**: Mode averaging vs mode collapsing
  - Why needed: Understanding these phenomena is crucial to grasping why symmetric divergences work better
  - Quick check: In which scenarios would you prefer mode averaging over mode collapsing?

- **Concept**: Sequence-level vs word-level distillation
  - Why needed: The paper's key contribution is making sequence-level distillation tractable
  - Quick check: Why is direct sequence-level distillation intractable?

## Architecture Onboarding

- **Component map**: Teacher model → Sampling module → Student model → Loss computation → Optimizer
- **Critical path**: Sampling from teacher → Computing f-divergence loss → Backpropagating to student
- **Design tradeoffs**: Online vs offline sampling (efficiency vs flexibility), symmetric vs asymmetric losses (balance vs simplicity)
- **Failure signatures**: Poor student performance, mode averaging/collapsing, training instability
- **First 3 experiments**:
  1. Verify step-wise decomposition by comparing sequence-level and word-level loss values
  2. Compare training efficiency with and without offline sampling
  3. Test different f-divergence variants on a simple dataset to observe mode averaging/collapsing behavior

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise conditions under which mode-collapsing RKL distillation outperforms mode-averaging KL distillation? The paper observes RKL performs better on highly multimodal tasks like dialogue generation, while KL is preferred for less multimodal tasks like machine translation, but only uses TeacherDist score to characterize task multimodality without establishing rigorous thresholds.

### Open Question 2
How does the approximation error from offline sampling method for symmetric divergences affect final distilled model quality? The authors propose offline sampling to improve efficiency but acknowledge it's an approximation using fixed teacher samples. They show comparable performance to online sampling but don't quantify approximation error or explore different sampling strategies.

### Open Question 3
Can the f-DISTILL framework be extended to other divergence measures beyond KL, RKL, JS, and TVD, and what theoretical properties would guide optimal divergence selection? The paper mentions f-divergence is general but only implements four variants without systematic exploration of the divergence space or theoretical analysis of why these were chosen.

## Limitations

- The paper doesn't thoroughly explore hyperparameter sensitivity (teacher sampling temperature, beam size, number of samples)
- Evaluation relies on automatic metrics with known limitations in capturing semantic fidelity
- The claim that existing methods are special cases of f-DISTILL is presented but not rigorously proven

## Confidence

**High Confidence**: Mathematical derivation of step-wise decomposition and theoretical analysis of symmetric vs asymmetric divergences are well-established and rigorously proven.

**Medium Confidence**: Empirical superiority of symmetric divergences is supported by experiments but may be task-dependent. Observed trade-offs between mode averaging and collapsing are convincing but could vary across different datasets and architectures.

**Low Confidence**: Claim that SeqKD and ENGINE are special cases of f-DISTILL is presented but not rigorously proven. The analysis assumes these methods can be reformulated within f-divergence framework without demonstrating this transformation explicitly.

## Next Checks

1. **Ablation on Sample Size**: Systematically vary the number of teacher samples used in offline sampling to determine minimum effective sample size and assess sensitivity to this hyperparameter.

2. **Cross-Dataset Transfer**: Apply the same f-DISTILL variants to datasets not included in original experiments (e.g., CNN/DailyMail for summarization) to test generalizability of mode averaging/collapsing findings.

3. **Teacher Quality Analysis**: Train students using teachers of varying quality (different sizes or training epochs) to determine whether advantages of symmetric divergences persist when teacher model is weaker or stronger.