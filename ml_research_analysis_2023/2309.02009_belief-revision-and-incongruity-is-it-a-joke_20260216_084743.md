---
ver: rpa2
title: 'Belief revision and incongruity: is it a joke?'
arxiv_id: '2309.02009'
source_url: https://arxiv.org/abs/2309.02009
tags:
- listener
- which
- revision
- incongruity
- joke
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces incongruity as a new ingredient of humor,
  alongside surprise and revelation, using a belief revision framework based on nonmonotonic
  reasoning. The listener is modeled as an agent with a knowledge base composed of
  strict and default rules, inducing a pre-order on interpretations.
---

# Belief revision and incongruity: is it a joke?

## Quick Facts
- **arXiv ID**: 2309.02009
- **Source URL**: https://arxiv.org/abs/2309.02009
- **Reference count**: 9
- **Key outcome**: Introduces incongruity as a new humor ingredient using belief revision and nonmonotonic reasoning

## Executive Summary
This paper formalizes humor understanding through belief revision theory, modeling the listener as an agent with incomplete information who updates beliefs when hearing jokes. The framework captures three humor components: surprise (punchline contradicts revised beliefs), revelation (punchline explains context), and incongruity (violation of social norms). Using nonmonotonic reasoning, the listener's knowledge base is expressed as strict and default rules inducing a pre-order on interpretations. The approach is illustrated with examples connecting to key AI concepts of incomplete information and inconsistency.

## Method Summary
The method represents jokes as pairs of formulas (α, β) for context and punchline. A knowledge base Σ = (P, ∆) contains strict rules P and default rules ∆, converted to a possibility distribution inducing a pre-order ≼Σ. Surprise is detected when the revised beliefs after hearing context and full joke are inconsistent. Revelation occurs when the punchline logically explains the context. Incongruity is modeled as violating a social norm by reasoning under Σ with the norm temporarily removed.

## Key Results
- Formalizes humor as surprise + revelation + incongruity using belief revision
- Shows how nonmonotonic reasoning enables modeling of incomplete information
- Illustrates the framework with propositional logic examples
- Connects humor understanding to AI concepts of inconsistency and default reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A joke is potentially funny if the punchline is both surprising and revealing to the listener.
- **Mechanism**: The listener has a knowledge base expressed as a pair (K, ≼K), where K is a consistent set of propositional formulas and ≼K is a pre-order on interpretations induced by default rules. The punchline β is surprising when (K ◦ α) ∧ (K ◦ (α ∧ β)) is inconsistent, and revealing when K ◦ β ⊨ α.
- **Core assumption**: Surprise arises from inconsistency between the revised beliefs after hearing the context and after hearing the full joke, and revelation means the punchline logically explains the context.
- **Evidence anchors**:
  - [abstract]: "A joke is characterized as surprising if the punchline is inconsistent with the listener's revised beliefs after hearing the context, and revealing if the punchline logically explains the context."
  - [section]: Definition 4.2 formalizes the surprising condition; Definition 4.3 formalizes the revealing condition.
  - [corpus]: Average neighbor FMR 0.41, no direct citations, weak empirical support in corpus.
- **Break condition**: If either the surprising or revealing condition fails, the joke is not potentially funny.

### Mechanism 2
- **Claim**: Incongruity is modeled as the violation of a social norm that is disregarded during reasoning.
- **Mechanism**: A subset of formulas (PN, ∆N) is treated as norms. A statement (α, β) is incongruous if α ∧ β |∼m Σ\{ρ} ¬ρ (violation) and β |∼m Σ\{ρ} α (revelation in disregard of ρ), where Σ\{ρ} is Σ with ρ removed.
- **Core assumption**: Norm violation can be detected by reasoning under a knowledge base from which the norm is temporarily excluded.
- **Evidence anchors**:
  - [section]: Definition 5.6 gives the formal incongruity definition.
  - [abstract]: "Incongruity is modeled as the violation of a social norm, where the punchline disregards an expected law while still explaining the context."
  - [corpus]: Weak corpus evidence, no direct citations.
- **Break condition**: If the norm cannot be violated without causing inconsistency (e.g., it is entailed by the rest of P), incongruity cannot be modeled.

### Mechanism 3
- **Claim**: The listener's reasoning uses default rules to handle incomplete information, leading to non-monotonic inferences.
- **Mechanism**: A knowledge base Σ = (P, ∆) induces a pre-order ≼Σ via best-out or lexicographic ordering; entailment |∼m Σ is defined via minimal models of P ∪ {ϕ} under ≼Σ. This supports the revision and incongruity checks.
- **Core assumption**: Default rules can be systematically converted into constraints on possibility distributions, which yield a pre-order usable for belief revision.
- **Evidence anchors**:
  - [section]: Subsection 3.2 explains conversion of ∆ into possibility distribution π∆ and pre-order.
  - [section]: Subsection 3.3 shows how (K, ◦) can be built from Σ.
  - [corpus]: Weak corpus evidence.
- **Break condition**: If ∆ is empty (only strict rules), the listener is psycho-rigid and cannot be surprised, hence no humor.

## Foundational Learning

- **Concept: Non-monotonic reasoning**
  - Why needed here: Allows modeling of default assumptions that can be overridden by exceptions, essential for representing the listener's incomplete information.
  - Quick check question: If a default rule "Birds fly" is in the knowledge base and a penguin is mentioned, what inference does the listener make before learning it is a penguin?

- **Concept: Belief revision postulates (KM1–KM6)**
  - Why needed here: Provide the formal properties that any revision operator must satisfy to ensure consistency and minimal change.
  - Quick check question: What does postulate KM2 say happens when the new information is consistent with the old beliefs?

- **Concept: Possibility theory and pre-orders**
  - Why needed here: Translates default rules into a plausibility ordering on interpretations, enabling computation of minimal models for revision.
  - Quick check question: In a possibility distribution, what does a higher value of π(ω) signify about interpretation ω?

## Architecture Onboarding

- **Component map**:
  Input parser -> Natural language to propositional formulas (α, β)
  Knowledge base builder -> Constructs Σ = (P, ∆) from defaults and strict rules
  Pre-order generator -> Converts ∆ into ≼Σ via best-out or lexicographic ordering
  Revision engine -> Computes K ◦ ϕ using minimal models
  Surprise/revelation checker -> Evaluates inconsistency and entailment conditions
  Incongruity checker -> Tests violation of norms in disregard mode

- **Critical path**: Parse -> Build Σ -> Generate ≼Σ -> Compute K ◦ α and K ◦ β -> Check surprise -> Check revelation -> Check incongruity -> Output funniness

- **Design tradeoffs**:
  - Use best-out ordering for simpler implementation vs. lexicographic for finer discrimination
  - Strict rules (P) guarantee consistency but may block some incongruity modeling
  - Default rules (∆) enable reasoning under incomplete info but increase computational cost

- **Failure signatures**:
  - Empty ∆ -> Psycho-rigid listener, no surprise possible
  - Inconsistent P -> No rational revision possible
  - Norm ρ entailed by P\{ρ} -> Incongruity cannot be detected

- **First 3 experiments**:
  1. Test surprise detection on Example 4.7 with both orderings; verify K ◦ α and K ◦ (α ∧ β) models are disjoint.
  2. Test incongruity on Example 5.4; confirm that removing R3 allows violation detection and revelation in disregard.
  3. Compare psycho-rigid vs. default-reasoning listener on Example 3.2; show that only default reasoning yields surprise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the proposed incongruity model be extended to handle degrees of incongruity, reflecting how shocking or mandatory the violated norm is?
- **Basis in paper**: [explicit] The paper suggests this as a future research direction, noting that the comic effect may increase with the obviousness or mandatory nature of the violated social norm.
- **Why unresolved**: The current formalization treats norms as binary (either violated or not), lacking a mechanism to quantify the severity or degree of incongruity.
- **What evidence would resolve it**: Developing a quantitative measure of incongruity based on social norm violation, and testing it on joke corpora to show correlation with perceived funniness.

### Open Question 2
- **Question**: How does the proposed framework account for mixed emotions, such as the shame associated with violating social norms in humor?
- **Basis in paper**: [explicit] The paper acknowledges that modeling emotions like shame related to norm violation could enrich the current model, but this is left unexplored.
- **Why unresolved**: The current model focuses on logical violation of norms but does not capture the emotional complexity of how listeners react to such violations.
- **What evidence would resolve it**: Integrating emotional dimensions into the logical framework and empirically validating the emotional responses of listeners to different types of jokes.

### Open Question 3
- **Question**: Can the framework be adapted to handle jokes that rely on humor arising from violations of communication principles, such as Gricean maxims?
- **Basis in paper**: [inferred] The paper mentions that absurd jokes may disregard principles of good communication, but it does not formalize how these principles interact with the incongruity model.
- **Why unresolved**: The current model treats incongruity primarily as norm violation but does not address violations of implicit conversational rules or expectations.
- **What evidence would resolve it**: Extending the logical framework to include conversational maxims and analyzing joke corpora to identify patterns where humor arises from their violation.

## Limitations
- Propositional logic foundation restricts modeling of complex natural language phenomena
- Computational cost of pre-order construction and minimal model computation may limit scalability
- Assumes rational agents who apply KM postulates, which may not capture actual human humor perception

## Confidence
- High confidence: The formalization of surprise and revelation conditions (Mechanisms 1)
- Medium confidence: The incongruity detection mechanism (Mechanism 2)
- Low confidence: The claim that all humor can be reduced to these three components (surprise, revelation, incongruity)

## Next Checks
1. Test the framework on a corpus of classic jokes to measure precision/recall of humor detection
2. Implement both best-out and lexicographic orderings and compare their discrimination power on edge cases
3. Evaluate whether removing social norms from the knowledge base actually improves incongruity detection in practice