---
ver: rpa2
title: Prompt-based Pseudo-labeling Strategy for Sample-Efficient Semi-Supervised
  Extractive Summarization
arxiv_id: '2311.09559'
source_url: https://arxiv.org/abs/2311.09559
tags:
- text
- summarization
- summary
- confidence
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a prompt-based pseudo-labeling strategy for
  semi-supervised extractive summarization using LLMs. The method improves upon standard
  teacher-student learning by using GPT-4 to evaluate and generate high-quality pseudo-labels,
  leading to better performance than using just the teacher model's confidence.
---

# Prompt-based Pseudo-labeling Strategy for Sample-Efficient Semi-Supervised Extractive Summarization

## Quick Facts
- arXiv ID: 2311.09559
- Source URL: https://arxiv.org/abs/2311.09559
- Reference count: 4
- Primary result: Achieves ROUGE-1 scores of 58.9-58.9% on TweetSumm, 26.1-26.4% on WikiHow, and 29.1-29.7% on ArXiv/PubMed using only 300 labeled examples

## Executive Summary
This paper proposes a prompt-based pseudo-labeling strategy for semi-supervised extractive summarization using LLMs, specifically GPT-4. The method improves upon standard teacher-student learning by using GPT-4 to evaluate and generate high-quality pseudo-labels, leading to better performance than using just the teacher model's confidence. Experiments on three datasets show the proposed method achieves ROUGE-1 scores of 58.9-58.9% on TweetSumm, 26.1-26.4% on WikiHow, and 29.1-29.7% on ArXiv/PubMed, outperforming existing semi-supervised approaches and approaching fully supervised models with 300 labeled examples.

## Method Summary
The method builds on standard teacher-student learning by replacing the teacher's confidence-based pseudo-label selection with LLM-based evaluation. It uses GPT-4 to evaluate candidate pseudo-labels against explicit quality criteria (conciseness, key point coverage, extractive fidelity), producing scores that better reflect true summary quality than teacher confidence. The approach also includes a relabeling mechanism where GPT-4 regenerates summaries for top-scoring examples, correcting errors in the teacher model's pseudo-labels. The iterative process adds high-quality pseudo-labels to the training set, progressively improving the student model's performance until it approaches fully supervised baselines.

## Key Results
- Achieves ROUGE-1 scores of 58.9-58.9% on TweetSumm with only 300 labeled examples
- Outperforms standard semi-supervised approaches by 2-3 ROUGE points across all datasets
- Approaches fully supervised model performance with just 300 labeled examples versus thousands typically required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 provides more semantically aligned confidence than the teacher model's raw softmax probabilities for summarization quality
- Core assumption: The GPT-4 evaluation criteria are aligned with ground truth summary quality and are robust to summarization-specific nuances
- Evidence: [abstract] "we propose a prompt-based pseudo-labeling strategy with LLMs that picks unlabeled examples with more accurate pseudo-labels than using just the classifier's probability outputs"

### Mechanism 2
- Claim: Relabeling with GPT-4 after confidence-based selection substantially improves pseudo-label quality beyond selection alone
- Core assumption: GPT-4 can generate extractive summaries that are more accurate than the teacher model's pseudo-labels, even without task-specific fine-tuning
- Evidence: [abstract] "Our approach also includes a relabeling mechanism that improves the quality of pseudo-labels"

### Mechanism 3
- Claim: Iterative teacher-student learning with LLM-enhanced pseudo-labels achieves performance comparable to fully supervised models with far fewer labeled examples
- Core assumption: The quality of pseudo-labels remains high throughout the iterative process and does not degrade due to confirmation bias or drift
- Evidence: [abstract] "our method achieves competitive L-Eval scores (evaluation with LLaMa-3) as a fully supervised method in a data-scarce setting"

## Foundational Learning

- Concept: Semi-supervised learning (SSL) teacher-student framework
  - Why needed: The method builds on standard SSL by replacing the teacher's confidence-based pseudo-label selection with LLM-based evaluation
  - Quick check: What is the primary difference between standard SSL pseudo-label selection and the proposed LLM-based approach?

- Concept: ROUGE metric for summarization evaluation
  - Why needed: The method's effectiveness is measured using ROUGE-1, ROUGE-2, and ROUGE-L F1 scores, which quantify n-gram overlap between predicted and reference summaries
  - Quick check: How does ROUGE-L differ from ROUGE-1 and ROUGE-2 in measuring summary quality?

- Concept: Prompt engineering for LLM evaluation and generation
  - Why needed: The method relies on carefully crafted prompts to elicit consistent, extractive summaries and quality scores from GPT-4
  - Quick check: What are the four evaluation criteria used in the GPT-4 scoring prompt?

## Architecture Onboarding

- Component map: PreSumm teacher model (BERT-base) → confidence scoring → GPT-4 evaluation → pseudo-label selection → GPT-4 relabeling → student model training → iterate
- Critical path: Teacher training → pseudo-label generation → GPT-4 scoring → top-5 selection → GPT-4 relabeling → student training
- Design tradeoffs: Using GPT-4 for both scoring and relabeling increases computational cost but significantly improves pseudo-label quality; limiting to 5 pseudo-labels per cycle balances quality and training efficiency
- Failure signatures: Poor ROUGE scores despite high GPT-4 scores indicate prompt misalignment; student model overfitting to noisy pseudo-labels; GPT-4 generating non-extractive summaries
- First 3 experiments:
  1. Train PreSumm on 50 labeled examples and generate pseudo-labels for unlabeled set
  2. Implement GPT-4 scoring prompt and verify scores align with human judgment on a small validation set
  3. Test GPT-4 relabeling on top-scoring examples and measure ROUGE improvement compared to teacher pseudo-labels

## Open Questions the Paper Calls Out

- What is the minimum amount of labeled data needed for the proposed prompt-based pseudo-labeling strategy to outperform traditional semi-supervised approaches?
- How does the quality of pseudo-labels generated by GPT-4 compare to those generated by smaller, open-source LLMs?
- What is the impact of including examples in the GPT-4 relabeling prompt on the final summarization performance?
- How does the proposed method scale to extremely large unlabeled datasets?

## Limitations
- Limited evaluation diversity with only three datasets tested
- Prompt engineering opacity with unspecified prompt templates
- Computational cost considerations with substantial GPT-4 API usage
- Baseline limitations without comparison to advanced SSL methods

## Confidence
- **High confidence**: Core mechanism of using GPT-4 for pseudo-label evaluation and relabeling demonstrably improves ROUGE scores
- **Medium confidence**: Claims of competitive performance to fully supervised models require context about baseline strength
- **Low confidence**: Assertions about value in data-scarce settings lack systematic ablation studies

## Next Checks
1. Domain transfer validation: Test the method on additional summarization datasets from different domains to assess generalization
2. Prompt robustness analysis: Systematically vary GPT-4 prompts to determine if improvements are methodology-driven or prompt-specific
3. Cost-performance tradeoff evaluation: Conduct detailed analysis of computational costs versus performance gains across different budget constraints