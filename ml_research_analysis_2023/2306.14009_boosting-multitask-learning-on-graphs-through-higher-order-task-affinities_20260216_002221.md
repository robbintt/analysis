---
ver: rpa2
title: Boosting Multitask Learning on Graphs through Higher-Order Task Affinities
arxiv_id: '2306.14009'
source_url: https://arxiv.org/abs/2306.14009
tags:
- task
- tasks
- learning
- affinity
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for boosting multitask learning
  on graphs by leveraging higher-order task affinities. The core idea is to efficiently
  estimate the affinities between tasks using a sampling procedure, and then cluster
  the tasks into groups based on these affinities.
---

# Boosting Multitask Learning on Graphs through Higher-Order Task Affinities

## Quick Facts
- arXiv ID: 2306.14009
- Source URL: https://arxiv.org/abs/2306.14009
- Reference count: 40
- Primary result: Novel method for multitask learning on graphs that leverages higher-order task affinities to predict and mitigate negative transfers between tasks.

## Executive Summary
This paper introduces a method to improve multitask learning on graphs by estimating higher-order task affinities. The core idea is to efficiently sample random task subsets, train multitask models on these subsets, and use the average performance to estimate how useful one task is to another when combined with other tasks. These affinity scores are then used to cluster tasks into groups via spectral clustering, aiming to minimize negative transfer and maximize positive transfer. The method is theoretically grounded under a planted block model and shows strong empirical results on community detection and molecular graph prediction tasks.

## Method Summary
The method estimates task affinity scores by sampling random subsets of tasks, training a multitask model on each subset, and averaging the performance of a target task across subsets that include a source task. This provides a measure of how useful a source task is to a target task when combined with other tasks. The affinity score matrix is then used as input to a spectral clustering algorithm to group tasks. Separate multitask models are trained for each task group, aiming to minimize negative transfer and maximize positive transfer. The method is applied to community detection on social networks and molecular graph regression tasks.

## Key Results
- Outperforms previous methods in predicting negative transfers between tasks.
- Improves multitask learning performance on community detection and molecular graph prediction datasets.
- Theoretical analysis shows affinity scores can provably separate tasks into groups under a planted block model.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Higher-order task affinities can be estimated efficiently and used to predict negative transfers in multitask learning.
- **Mechanism:** The method estimates task affinity scores by sampling random subsets of tasks, training a multitask model on each subset, and averaging the performance of a target task across subsets that include a source task. This provides a measure of how useful a source task is to a target task when combined with other tasks.
- **Core assumption:** Task affinity scores can be estimated from a limited number of samples, and these scores capture higher-order transfer relationships beyond pairwise interactions.
- **Evidence anchors:**
  - [abstract]: "We estimate the higher-order task affinity measure between two tasks as the prediction loss of one task in the presence of another task and a random subset of other tasks."
  - [section]: "We design an efficient sampling procedure that only requires O(T) complexity... We estimate the task affinity scores θi,j by averaging the MTL performances fi over subsets containing both task i and j."
- **Break condition:** If task relationships are highly nonlinear and cannot be approximated by averaging over random subsets, the estimated affinity scores may not accurately predict negative transfers.

### Mechanism 2
- **Claim:** Spectral clustering on the task affinity score matrix can group tasks into clusters that minimize negative transfer and maximize positive transfer.
- **Mechanism:** The affinity score matrix is transformed into a symmetric matrix and used as input to a spectral clustering algorithm. This groups tasks based on their higher-order affinity scores, forming clusters where tasks within a cluster are likely to transfer positively to each other.
- **Core assumption:** The affinity score matrix exhibits a block-diagonal structure where tasks within a block have higher affinity scores to each other than to tasks outside the block.
- **Evidence anchors:**
  - [abstract]: "We use spectral clustering on the affinity score matrix to identify task grouping."
  - [section]: "We apply spectral clustering algorithms... on A and merge the clustered target and source tasks in one group of final task groupings."
- **Break condition:** If the affinity score matrix does not exhibit a clear block-diagonal structure, spectral clustering may not effectively separate tasks into groups that minimize negative transfer.

### Mechanism 3
- **Claim:** The task affinity score matrix can provably separate tasks into groups under a planted block model of tasks on graphs.
- **Mechanism:** Under a planted block model where tasks are drawn from linear models with distinct label vectors for each group, the affinity scores between tasks within the same group are higher than those between tasks from different groups. This separation allows spectral clustering to recover the underlying task structure.
- **Core assumption:** Tasks follow a planted block model with well-separated label vectors, and the graph neural network used is a one-layer linear model.
- **Evidence anchors:**
  - [abstract]: "We provide a theoretical analysis to show that under a planted block model of tasks on graphs, our affinity scores can provably separate tasks into groups."
  - [section]: "We prove that under this planted model, the affinity score matrix [θi,j]T×T exhibits a block-diagonal structure, each block corresponding to one cluster."
- **Break condition:** If tasks do not follow a planted block model or if the label vectors are not well-separated, the affinity scores may not exhibit a clear block structure, and the theoretical guarantees may not hold.

## Foundational Learning

- **Concept:** Multitask Learning (MTL)
  - Why needed here: The paper addresses the problem of predicting multiple node labeling functions on graphs simultaneously, which is formulated as a multitask learning problem.
  - Quick check question: What is the difference between multitask learning and single-task learning, and why is it important to consider task relationships in MTL?

- **Concept:** Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used as the encoder network in the multitask learning models to capture the graph structure and node features.
  - Quick check question: How do GNNs propagate information through a graph, and why are they suitable for learning node representations in multitask learning?

- **Concept:** Negative Transfer
  - Why needed here: The paper identifies negative transfer as a key challenge in multitask learning on graphs, where combining tasks can worsen performance compared to single-task learning.
  - Quick check question: What causes negative transfer in multitask learning, and how can it be mitigated?

## Architecture Onboarding

- **Component map:** Graph Neural Network (GNN) encoder -> Task-specific prediction layers -> Task affinity score estimator -> Spectral clustering algorithm -> Task grouping module

- **Critical path:**
  1. Sample random subsets of tasks.
  2. Train a multitask model on each subset and evaluate performance.
  3. Estimate task affinity scores by averaging performance across subsets.
  4. Apply spectral clustering on the affinity score matrix to group tasks.
  5. Train a multitask model on each task group.

- **Design tradeoffs:**
  - Sampling subset size (α) vs. capturing higher-order task relationships
  - Number of samples (n) vs. estimation accuracy of task affinity scores
  - Number of task groups (b) vs. model complexity and inference cost

- **Failure signatures:**
  - Poor affinity score estimation leading to incorrect task grouping
  - Spectral clustering failing to separate tasks into meaningful groups
  - Negative transfer persisting within task groups

- **First 3 experiments:**
  1. Vary the subset size (α) and number of samples (n) to study their impact on task affinity score estimation and multitask learning performance.
  2. Compare the proposed method with baselines that use first-order task affinities or naive multitask learning to validate the benefits of higher-order task affinities.
  3. Analyze the structural differences between tasks within and across groups using personalized PageRank vectors to understand the causes of negative transfer.

## Open Questions the Paper Calls Out
No explicit open questions were called out in the provided text.

## Limitations
- The method relies heavily on the assumption that task relationships can be effectively captured through higher-order sampling and spectral clustering, which may not hold in all scenarios.
- Computational cost of sampling and training multiple MTL models may be prohibitive for extremely large task sets.
- Performance is sensitive to hyperparameter choices (α, n, b) that are not extensively studied across diverse datasets.

## Confidence
- **High Confidence:** The method's effectiveness in community detection tasks (Amazon, YouTube, DBLP, LiveJournal) is well-supported by the empirical results.
- **Medium Confidence:** The theoretical analysis under the planted block model provides valuable insights but may not generalize to all real-world task distributions.
- **Low Confidence:** The method's performance on molecular graph datasets is less extensively validated, and the impact of different encoder architectures (SIGN vs. GIN) on the task affinity estimation is not thoroughly explored.

## Next Checks
1. **Ablation study:** Systematically vary α (subset size) and n (number of samples) to quantify their impact on task affinity estimation accuracy and multitask learning performance.
2. **Generalization test:** Apply the method to a broader range of graph datasets with varying characteristics (e.g., different community structures, node feature distributions) to assess robustness.
3. **Encoder architecture comparison:** Compare the proposed method's performance when using different GNN encoders (e.g., GAT, GCN, GraphSAGE) to isolate the contribution of the task affinity estimation from the choice of encoder.