---
ver: rpa2
title: 'DistillSpec: Improving Speculative Decoding via Knowledge Distillation'
arxiv_id: '2310.08461'
source_url: https://arxiv.org/abs/2310.08461
tags:
- draft
- distillation
- decoding
- block
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving the efficiency of
  large language model (LLM) inference, particularly in scenarios with stringent latency
  constraints. The core idea is to use knowledge distillation (KD) to better align
  a compact "draft" model with a larger "target" model, enhancing the effectiveness
  of speculative decoding (SD).
---

# DistillSpec: Improving Speculative Decoding via Knowledge Distillation

## Quick Facts
- arXiv ID: 2310.08461
- Source URL: https://arxiv.org/abs/2310.08461
- Reference count: 40
- Primary result: DistillSpec yields 10-45% speedups over standard speculative decoding on various benchmarks

## Executive Summary
This paper addresses the efficiency bottleneck of large language model inference by improving speculative decoding through knowledge distillation. The core insight is that using on-policy data generation from the draft model, combined with task-specific divergence function selection, better aligns the draft and target models for improved acceptance rates. The method, called DistillSpec, demonstrates substantial speedups (10-45%) across multiple benchmarks while maintaining task performance. Additionally, combining DistillSpec with lossy speculative decoding enables fine-grained control over the quality-latency tradeoff.

## Method Summary
DistillSpec improves speculative decoding by training the draft model using knowledge distillation with two key innovations: on-policy data generation from the draft model itself, and divergence function selection tailored to the task and decoding strategy. The draft model is trained to minimize a divergence (FKL, JSD, RKL, or TVD) between its output distribution and the target model's distribution using sequences generated by the draft. This distilled draft model is then used in speculative decoding, where it proposes tokens that are accepted or rejected based on their likelihood under the target model. The method can be combined with lossy speculative decoding using leniency functions to control the quality-latency tradeoff.

## Key Results
- 10-45% wall-clock time speedup over standard speculative decoding on multiple benchmarks
- On-policy data generation consistently outperforms ground-truth data for distillation alignment
- Task-specific divergence function selection significantly impacts performance
- Combining with lossy SD enables controllable quality-latency tradeoffs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: On-policy data generation improves draft-target alignment more effectively than fixed ground-truth datasets
- Mechanism: On-policy sampling creates distributions closer to the target model, reducing total variation distance and improving acceptance rates
- Core assumption: Draft model distribution is sufficiently close to target distribution to serve as effective proxy
- Evidence: Abstract states on-policy data is "crucial for ensuring strong student-teacher alignment"

### Mechanism 2
- Claim: Divergence function choice significantly impacts distillation effectiveness
- Mechanism: Different divergences measure alignment differently; optimal choice depends on task and decoding strategy
- Core assumption: Relationship between divergence choice and performance is task-dependent
- Evidence: Section 5.2 shows divergence choice is "highly task-dependent and sensitive to the decoding strategy"

### Mechanism 3
- Claim: Combining with lossy SD enables fine-grained quality-latency control
- Mechanism: Leniency functions adjust token acceptance probabilities, with DistillSpec providing better-aligned drafts for higher quality at faster speeds
- Core assumption: Quality-latency tradeoff is monotonic and controllable through leniency parameter
- Evidence: Abstract mentions achieving "fine-grained control over the latency vs. task performance trade-off"

## Foundational Learning

- Concept: Total variation distance as acceptance rate proxy
  - Why needed here: Acceptance rate is key efficiency metric, and TVD provides theoretical link between alignment and acceptance
  - Quick check question: How does minimizing total variation distance between draft and target distributions affect expected acceptance rate?

- Concept: Variational representation of divergences
  - Why needed here: Understanding divergence expectations helps explain why certain divergences work better for specific tasks
  - Quick check question: What is the variational form of total variation distance and how does it relate to acceptance rate calculation?

- Concept: On-policy vs off-policy training data
  - Why needed here: Choice between on-policy and off-policy data significantly impacts distillation effectiveness
  - Quick check question: Why might on-policy data generation be more effective than using fixed ground-truth datasets?

## Architecture Onboarding

- Component map: Target model -> Draft model -> Distillation training loop -> Speculative decoding inference pipeline -> Leniency function (lossy mode)

- Critical path: 1) Generate training data (on-policy or off-policy) 2) Perform knowledge distillation using chosen divergence 3) Integrate distilled draft into speculative decoding 4) Measure acceptance rate and block efficiency

- Design tradeoffs: Data generation cost vs alignment quality (on-policy cheaper but may be less aligned than teacher-generated); Divergence function choice vs task specificity (no universal best choice); Model size vs inference speed (larger draft models may accept more tokens but slower generation)

- Failure signatures: Low acceptance rates despite distillation (draft-target misalignment); No improvement over standard SD (divergence choice ineffective); Quality degradation with leniency (lossy SD pushed too far)

- First 3 experiments: 1) Compare on-policy vs off-policy data generation with fixed divergence (JSD) on XSum 2) Test different divergences (FKL, JSD, RKL, TVD) with on-policy data on GSM8K 3) Evaluate lossy SD with different leniency functions (flin, fsq, fexp) on CNNDM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does divergence function choice affect quality-latency tradeoff compared to other distillation methods?
- Basis: Paper explores impact of different divergences on block efficiency and task performance
- Why unresolved: Doesn't provide clear guidelines for selecting optimal divergence for given task and strategy
- What evidence would resolve it: Comprehensive study comparing quality-latency tradeoffs across tasks and strategies

### Open Question 2
- Question: Can DistillSpec extend to language models beyond GPT-like and T5 models?
- Basis: Paper focuses on specific model architectures, but principles could apply to others
- Why unresolved: Doesn't investigate applicability to other language model architectures or domains
- What evidence would resolve it: Empirical evaluation on diverse range of language models including non-transformer models

### Open Question 3
- Question: How does performance vary with different block sizes and lenience functions in lossy SD?
- Basis: Paper explores different block sizes and lenience functions but doesn't fully characterize impact
- Why unresolved: Provides insights into tradeoffs but doesn't systematically investigate interactions
- What evidence would resolve it: Comprehensive study analyzing impact of block sizes and lenience functions on quality-latency tradeoff

## Limitations

- Dataset and task coverage limited to English language tasks (summarization, arithmetic reasoning, translation)
- Scalability concerns for models beyond 11B parameters to modern 100B+ parameter LLMs
- Results may depend heavily on specific implementation details not fully specified

## Confidence

**High Confidence (4/5)**: Empirical observation that on-policy data generation outperforms ground-truth data, and that different divergences perform differently across tasks.

**Medium Confidence (3/5)**: Claim of 10-45% speedups over standard SD, though exact conditions affect reproducibility.

**Low Confidence (2/5)**: Assertion that DistillSpec enables fine-grained quality-latency control when combined with lossy SD, with inconsistent GSM8K results.

## Next Checks

**Check 1**: Replicate on-policy vs off-policy data generation comparison on held-out summarization dataset with fixed JSD divergence to isolate data generation impact.

**Check 2**: Systematically test all four divergence functions with on-policy data on reasoning task to verify task-dependency and identify effectiveness patterns.

**Check 3**: Evaluate lossy SD combination on multi-task benchmark with varying leniency functions to determine broad applicability of quality-latency control.