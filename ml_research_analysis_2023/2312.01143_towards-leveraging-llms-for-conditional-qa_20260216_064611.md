---
ver: rpa2
title: Towards leveraging LLMs for Conditional QA
arxiv_id: '2312.01143'
source_url: https://arxiv.org/abs/2312.01143
tags:
- evidence
- performance
- question
- input
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) on the Conditional
  Question Answering (CQA) dataset, focusing on generative models like T5 and UL2.
  The research assesses LLM performance across various question types, including extractive,
  generative, unanswerable, multi-hop, and incomplete questions.
---

# Towards leveraging LLMs for Conditional QA

## Quick Facts
- arXiv ID: 2312.01143
- Source URL: https://arxiv.org/abs/2312.01143
- Reference count: 14
- Key outcome: Fine-tuned LLMs can surpass SOTA performance on Yes/No questions in CQA tasks, with 7-8 point increases in EM and F1 scores, but face challenges in extractive QA and evidence generation.

## Executive Summary
This study evaluates Large Language Models (LLMs) like T5 and UL2 on the Conditional Question Answering (CQA) dataset, focusing on generative models. The research assesses LLM performance across various question types, including extractive, generative, unanswerable, multi-hop, and incomplete questions. Key findings show that fine-tuned LLMs can surpass state-of-the-art (SOTA) performance in some cases, with a 7-8 point increase in Exact Match (EM) and F1 scores for Yes/No questions. However, these models encounter challenges in extractive question answering, lagging behind SOTA by over 10 points, and in mitigating the risk of injecting false information. The study emphasizes the critical role of effective evidence retrieval and advocates for a more comprehensive evaluation framework due to the significant influence of evaluation metrics on performance assessments.

## Method Summary
The study fine-tunes T5 and UL2 models on the CQA dataset, which includes 2338 training examples and 285 dev examples. The models are trained and evaluated using different input formats: question only (S1), question with scenario (S2), and question with scenario and evidence generation (S3). Multi-answer models are also explored (M1-M3). Evaluation metrics include Exact Match (EM), F1 scores, and embedding-based methods (Cosine Similarity, Bert Score, Bart Score).

## Key Results
- Fine-tuned LLMs surpass SOTA performance on Yes/No questions, with 7-8 point increases in EM and F1 scores.
- Models struggle with extractive question answering, lagging behind SOTA by over 10 points.
- Evidence retrieval is identified as a major bottleneck in model performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning generative LLMs like T5 and UL2 can achieve SOTA performance on Yes/No questions in CQA tasks without encoding full input context.
- Mechanism: The model leverages its pre-trained parametric knowledge and fine-tuned parameters to generate contextually appropriate Yes/No answers, even when not all evidence lines are provided as input. This works because Yes/No questions require simpler reasoning compared to extractive or multi-hop questions.
- Core assumption: The model's pre-training and fine-tuning have sufficiently captured the necessary knowledge to answer Yes/No questions accurately based on partial context.
- Evidence anchors:
  - [abstract] "Our findings reveal that fine-tuned LLMs can surpass the state-of-the-art (SOTA) performance in some areas, with an increase of 7-8 points in Exact Match (EM) and F1 scores for Yes/No questions."
  - [section] "In these experiments (S1-S3) where we train using train-single and evaluate using dev-single, we find overall poor performance on extractive questions and a subset of conditional questions. Poor extractive and conditional performance is expected as the models do not see valid lines of evidence within their input context, meaning getting exact quotes or correct lines of evidence would be near-impossible to generate."
- Break condition: The mechanism breaks down when questions require detailed evidence extraction or multi-hop reasoning that depends on information not present in the partial context.

### Mechanism 2
- Claim: Including the scenario in the input context improves model performance across various question types in CQA tasks.
- Mechanism: The scenario provides additional context that helps the model better understand the conditions under which the answer should be generated, leading to more accurate and contextually appropriate responses.
- Core assumption: The scenario contains relevant information that, when included in the input, helps the model disambiguate between different possible answers or interpretations of the question.
- Evidence anchors:
  - [section] "We find including the scenerio in the input and evidence in the output improved model performance in nearly all metrics."
  - [section] "In Figure 1 we see the complete results of experiment S2. Here we expand on models created in experiment S1 by including the Scenario alongside the Question in the input context. We again see model size improves performance, with t5-xl and t5-xxl performing best."
- Break condition: The mechanism may not improve performance if the scenario is irrelevant or introduces noise that confuses the model's understanding of the question.

### Mechanism 3
- Claim: Embedding-based evaluation methods provide a more comprehensive assessment of model performance compared to token-wise matching methods like Exact Match (EM) and F1 scores.
- Mechanism: Embedding-based methods like Cosine Similarity, Bert Score, and Bart Score evaluate the semantic similarity between generated and target answers, capturing nuances that token-wise matching might miss, especially in generative settings.
- Core assumption: Semantic similarity is a more appropriate measure of answer quality for generative models than exact token matching, as it allows for paraphrases and variations in wording that still convey the same meaning.
- Evidence anchors:
  - [section] "The CQA dataset evaluates answers based on string matching, which may not be representative in a generative setting. An effective evaluation metric may look at the meaning of the generations rather than specific sequences in order to properly compare the large scope of generative answers."
  - [section] "These results indicate that a comprehensive evaluation is needed to show the whole story as to the best-performing model, rather than just token-matching or embedding-based methods alone."
- Break condition: The mechanism breaks down if the embedding models used for evaluation are not well-calibrated to the specific domain or task, leading to inaccurate assessments of semantic similarity.

## Foundational Learning

- Concept: Conditional Question Answering (CQA)
  - Why needed here: Understanding the CQA task is crucial for designing and evaluating models that can handle questions requiring conditional answers based on long-form context.
  - Quick check question: What distinguishes CQA from standard question answering tasks?

- Concept: Evidence Retrieval
  - Why needed here: Effective evidence retrieval is critical for conditioning the model's responses on relevant information from long documents, especially when the model cannot encode the entire context.
  - Quick check question: How does the quality of evidence retrieval impact the performance of CQA models?

- Concept: Embedding-based Evaluation
  - Why needed here: Embedding-based evaluation methods provide a more nuanced assessment of model performance in generative settings, capturing semantic similarity between answers.
  - Quick check question: What are the advantages of using embedding-based evaluation methods over traditional token-wise matching in CQA tasks?

## Architecture Onboarding

- Component map: Question + Scenario (optional) + Evidence (retrieved or gold) -> Fine-tuned T5 or UL2 -> Answer + Evidence (optional) -> EM, F1, Cosine Similarity, Bert Score, Bart Score
- Critical path: 1. Retrieve relevant evidence lines using search-based or oracle retrieval 2. Format input context with question, scenario, and evidence 3. Generate answer using fine-tuned LLM 4. Evaluate output using multiple metrics
- Design tradeoffs:
  - Model size vs. performance: Larger models generally perform better but require more computational resources
  - Evidence retrieval method: Oracle retrieval provides gold evidence but is not practical for real-world applications; search-based retrieval is more realistic but may introduce noise
  - Evaluation metrics: Token-wise matching is straightforward but may not capture semantic nuances; embedding-based methods are more comprehensive but may be less interpretable
- Failure signatures:
  - Poor extractive performance: Model struggles to generate exact quotes from the evidence
  - Low conditional score: Model fails to generate appropriate evidence lines to support answers
  - Discrepancy between evaluation metrics: Different metrics may rank models differently, indicating the need for a more comprehensive evaluation framework
- First 3 experiments:
  1. Train and evaluate models with only the question as input (S1)
  2. Include the scenario in the input context (S2)
  3. Require models to generate evidence lines to support their answers (S3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the performance of LLMs on extractive question answering tasks?
- Basis in paper: [explicit] The paper states that "these models encounter challenges in extractive question answering, where they lag behind the SOTA by over 10 points" and suggests that "evidence retrieval is a major bottleneck in the performance of our QA models."
- Why unresolved: Despite the paper's findings and suggestions, there is no definitive solution presented to address this challenge, and the gap in performance compared to the SOTA remains significant.
- What evidence would resolve it: A successful implementation and demonstration of an LLM model that significantly outperforms the current SOTA on extractive question answering tasks, with a focus on improved evidence retrieval mechanisms.

### Open Question 2
- Question: What is the most effective evaluation method for assessing LLM performance in conditional question answering tasks?
- Basis in paper: [explicit] The paper highlights the significant influence of evaluation metrics on performance assessments and advocates for a more comprehensive evaluation framework, noting that "token-wise and semantic evaluations disagreeing on what the top performing model is."
- Why unresolved: The paper presents different evaluation methods (e.g., Exact Match, F1 scores, embedding-based methods) but does not provide a definitive answer on which method is most effective or how to combine them for a comprehensive assessment.
- What evidence would resolve it: A study that systematically compares the effectiveness of different evaluation methods in assessing LLM performance on conditional question answering tasks, and provides guidelines on how to combine them for a more accurate and comprehensive assessment.

### Open Question 3
- Question: How can we minimize the risk of LLMs injecting false information in their responses?
- Basis in paper: [explicit] The paper mentions that "LLMs face several areas for improvement, especially in handling diverse types of questions and conversational contexts" and that "hallucinations can occur, especially without fine-tuning as the model defaults to pre-training knowledge which may be stale or biased."
- Why unresolved: While the paper acknowledges the issue of false information injection, it does not provide a concrete solution to address this challenge, and the risk remains a significant concern in LLM applications.
- What evidence would resolve it: A successful implementation and demonstration of an LLM model that consistently generates accurate and complete responses with justification via extracted evidence, minimizing the risk of injecting false information.

## Limitations

- Models struggle with extractive question answering, lagging behind SOTA by over 10 points.
- Evidence retrieval is identified as a major bottleneck in model performance.
- The study acknowledges the need for a more comprehensive evaluation framework due to the significant influence of evaluation metrics on performance assessments.

## Confidence

- Medium: Claims about overall model performance have Medium confidence due to challenges in extractive QA and evidence generation tasks, as well as the need for a more comprehensive evaluation framework.

## Next Checks

1. Conduct human evaluation studies to verify the quality of generated answers and evidence lines, particularly for unanswerable questions.
2. Test the models on out-of-distribution examples to assess generalization capabilities.
3. Implement and evaluate alternative evidence retrieval methods to improve the quality of retrieved context for conditioning answers.