---
ver: rpa2
title: Continual Adaptation of Vision Transformers for Federated Learning
arxiv_id: '2306.09970'
source_url: https://arxiv.org/abs/2306.09970
tags:
- learning
- server
- task
- data
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses continual federated learning (CFL) where a
  server trains on non-IID data distributed across multiple clients, each learning
  new classes incrementally over time. The main challenges are catastrophic forgetting
  due to heterogeneity and privacy concerns from sharing model parameters.
---

# Continual Adaptation of Vision Transformers for Federated Learning

## Quick Facts
- arXiv ID: 2306.09970
- Source URL: https://arxiv.org/abs/2306.09970
- Reference count: 40
- One-line primary result: Up to 7% improvement in average accuracy over baselines while drastically reducing communication and client-side computation costs

## Executive Summary
This paper addresses continual federated learning (CFL) where a server trains on non-IID data distributed across multiple clients, each learning new classes incrementally over time. The main challenges are catastrophic forgetting due to heterogeneity and privacy concerns from sharing model parameters. The authors propose HePCo, which leverages prompt tuning to minimize communication by only sharing prompts and classifier heads. A novel lightweight latent-space knowledge distillation scheme is introduced to consolidate client models without storing data or generating images, thereby enhancing privacy.

## Method Summary
HePCo addresses continual federated learning by freezing the ViT backbone and learning only prompts and classifier heads, minimizing communication and preventing catastrophic forgetting. The server performs latent-space knowledge distillation using pseudo-latents generated by lightweight generators trained on aggregated client prompts. The approach handles both inter-task and intra-task forgetting through separate generators for current and past tasks, mixing their outputs during server-side fine-tuning. Only prompts and classifier heads are communicated between clients and server, preserving privacy while achieving competitive accuracy.

## Key Results
- Up to 7% improvement in average accuracy over baselines
- Drastically reduced communication by only sharing prompts and classifier heads
- Significantly lower client-side computation costs due to frozen backbone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HePCo mitigates catastrophic forgetting by freezing the ViT backbone and learning only prompts and classifier heads, preserving previously learned representations.
- Mechanism: Prompts act as task-specific adapters that modify the input to the frozen transformer without altering its weights, thus preventing overwriting of old knowledge.
- Core assumption: The frozen ViT backbone retains sufficient representational capacity to generalize across tasks when combined with task-specific prompts.
- Evidence anchors:
  - [abstract]: "leveraging prompt tuning to minimize communication by only sharing prompts and classifier heads"
  - [section]: "Applying prompting schemes at the client level can combat inter-task forgetting because such prompts do not destroy information in the frozen model's weights."
- Break condition: If the frozen backbone loses representational power for new tasks, prompts alone cannot compensate.

### Mechanism 2
- Claim: Latent-space knowledge distillation enables efficient aggregation of heterogeneous client models without sharing raw data or full model weights.
- Mechanism: The server trains a lightweight generator to produce pseudo-latents conditioned on class labels, then distills knowledge from client models by matching predictions on these latents.
- Core assumption: The latent space of the frozen ViT encoder contains sufficient information to reconstruct task-relevant patterns for distillation.
- Evidence anchors:
  - [abstract]: "A novel lightweight latent-space knowledge distillation scheme is introduced to consolidate client models without storing data or generating images"
  - [section]: "We propose to fine-tune the server model using data-free distillation in the latent space to retain the performance of the client models."
- Break condition: If the generator cannot produce diverse enough latents to cover the true data distribution, distillation will be ineffective.

### Mechanism 3
- Claim: Combining current-task and previous-task pseudo-latents during server-side fine-tuning prevents both intra-task and inter-task forgetting.
- Mechanism: The generator is trained separately for current and past tasks; mixing their outputs during distillation provides replay-like effects without storing real data.
- Core assumption: The previous-task generator can produce latents that sufficiently represent past task distributions for effective replay.
- Evidence anchors:
  - [section]: "To prevent this, we train a separate copy of the generator (θgen) to generate latents corresponding to the previously seen tasks"
  - [section]: "We mix the current and previous task batches to form a single composite batch according to a hyperparameter named replay ratio"
- Break condition: If the previous-task generator degrades over time, replay quality and forgetting mitigation will decline.

## Foundational Learning

- Concept: Continual Learning (CL)
  - Why needed here: The paper addresses class-incremental learning where new classes are learned over time without forgetting old ones.
  - Quick check question: What is catastrophic forgetting and why does it occur in neural networks during sequential task learning?

- Concept: Federated Learning (FL)
  - Why needed here: The setting involves multiple clients collaboratively training a global model without sharing raw data, requiring privacy-preserving aggregation.
  - Quick check question: How does data heterogeneity across clients affect model convergence in federated learning?

- Concept: Prompt Tuning in Vision Transformers
  - Why needed here: The method leverages prompt tuning to adapt the frozen ViT backbone to new tasks with minimal parameter updates.
  - Quick check question: What is the difference between prompt tuning and full fine-tuning in terms of parameter efficiency and forgetting?

## Architecture Onboarding

- Component map:
  - Client side: Frozen ViT-B/16 encoder, learnable prompt pool, classifier head, communication module (sends prompts+classifier)
  - Server side: Parameter averaging, two generators (current and previous tasks), latent-space distillation module, global model updater
  - Shared: Adam optimizer, prompt length 20, prompt pool size 100, embedding dimension 768

- Critical path:
  1. Clients train prompts and classifier on local data for 10 epochs
  2. Clients send prompts and classifier to server
  3. Server averages received parameters
  4. Server trains generators to produce pseudo-latents for current and past tasks
  5. Server performs latent-space distillation using generated pseudo-latents
  6. Updated global model parameters sent back to clients for next round

- Design tradeoffs:
  - Communication efficiency vs. model expressiveness: Only sending prompts and classifier reduces communication but may limit adaptation capacity
  - Server computation vs. client privacy: Additional server-side generation and distillation preserves client privacy but increases server load
  - Generator complexity vs. distillation quality: Simpler generators reduce overhead but may produce less effective pseudo-latents

- Failure signatures:
  - High inter-task forgetting: Previous-task generator failing to capture past distributions
  - Poor current task performance: Current-task generator producing overly simple latents
  - Communication bottleneck: Prompt length or pool size too large for constrained networks
  - Client drift: Insufficient local training leading to poor prompt quality

- First 3 experiments:
  1. Ablation: Remove previous-task generator and measure increase in inter-task forgetting
  2. Sensitivity: Vary category ratio κ and observe performance degradation in high heterogeneity
  3. Communication: Reduce prompt length Lp and measure impact on average accuracy vs. communication cost

## Open Questions the Paper Calls Out

- Question: How can the framework be extended to accommodate other architectures beyond pretrained vision transformers?
  - Basis in paper: [inferred] The paper states "Additionally, our approach necessitates clients to use pretrained vision transformers, leaving open the question of how this framework can be extended to accommodate other architectures."
  - Why unresolved: The paper acknowledges this limitation but does not provide any solutions or experiments with other architectures.
  - What evidence would resolve it: Experiments demonstrating the effectiveness of the framework with other architectures like CNNs or MLPs would resolve this question.

- Question: How robust is the method to diverse setups and scenarios?
  - Basis in paper: [inferred] The paper mentions "Therefore, future work should focus on testing the robustness of these methods in diverse setups to ensure their effectiveness in different scenarios."
  - Why unresolved: The paper does not provide extensive experiments or analysis on the robustness of the method across diverse scenarios and datasets.
  - What evidence would resolve it: Experiments demonstrating the effectiveness of the method across a wide range of scenarios, datasets, and task distributions would resolve this question.

- Question: How can the computation overhead at the server be reduced?
  - Basis in paper: [inferred] The paper states "One potential limitation of this work is in the computation overhead introduced at the server, which may be an issue for some use-cases."
  - Why unresolved: The paper acknowledges the limitation but does not provide any solutions or optimizations to reduce the server-side computation overhead.
  - What evidence would resolve it: Experiments or analysis demonstrating techniques to reduce the server-side computation overhead, such as more efficient distillation algorithms or parallelization strategies, would resolve this question.

## Limitations

- The approach requires clients to use pretrained vision transformers, limiting its applicability to other architectures
- The paper does not provide extensive experiments or analysis on the robustness of the method across diverse scenarios and datasets
- The computation overhead introduced at the server may be an issue for some use-cases

## Confidence

**High confidence**: The core mechanism of using prompt tuning to minimize communication while freezing the backbone is well-established in literature and logically sound for preventing catastrophic forgetting. The empirical improvements of 7% average accuracy over baselines are supported by experimental results on multiple datasets.

**Medium confidence**: The effectiveness of latent-space knowledge distillation relies on the generator's ability to produce representative pseudo-latents. While the approach is theoretically sound, the quality of generated latents directly impacts distillation success, and this dependency is not thoroughly validated.

**Low confidence**: The paper's claims about privacy preservation through latent-space distillation lack rigorous privacy analysis. No formal privacy guarantees or threat model analysis is provided to demonstrate that the proposed approach actually prevents client data leakage.

## Next Checks

1. **Generator ablation study**: Systematically evaluate the impact of removing the previous-task generator on inter-task forgetting rates to quantify its contribution to the proposed solution.

2. **Latent space analysis**: Visualize and statistically compare the distribution of generated pseudo-latents against real data distributions to verify the generator's representational fidelity.

3. **Communication vs. performance tradeoff**: Conduct controlled experiments varying prompt length and pool size to identify the optimal balance between communication efficiency and model accuracy.