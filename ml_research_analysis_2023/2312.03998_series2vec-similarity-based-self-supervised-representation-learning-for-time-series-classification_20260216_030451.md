---
ver: rpa2
title: 'Series2Vec: Similarity-based Self-supervised Representation Learning for Time
  Series Classification'
arxiv_id: '2312.03998'
source_url: https://arxiv.org/abs/2312.03998
tags:
- series
- time
- learning
- series2vec
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new self-supervised learning approach for
  time series, addressing the challenge of defining meaningful transformations for
  time series data. Unlike previous methods that rely on augmentation techniques,
  the proposed Series2Vec method uses time series similarity measures to predict the
  similarity between pairs of time series in both temporal and spectral domains.
---

# Series2Vec: Similarity-based Self-supervised Representation Learning for Time Series Classification

## Quick Facts
- arXiv ID: 2312.03998
- Source URL: https://arxiv.org/abs/2312.03998
- Reference count: 40
- Key outcome: New self-supervised learning approach for time series using similarity measures instead of augmentations

## Executive Summary
Series2Vec introduces a novel self-supervised learning framework for time series classification that addresses the challenge of defining meaningful transformations for time series data. Unlike previous methods relying on data augmentation, Series2Vec leverages time series similarity measures in both temporal and spectral domains to predict pairwise similarities within batches. The method uses transformers with order-invariant attention to learn representations where similar time series are mapped to similar embeddings. Extensive experiments on nine real-world datasets demonstrate that Series2Vec outperforms state-of-the-art self-supervised techniques and achieves comparable performance to fully supervised training while being highly efficient with limited labeled data.

## Method Summary
Series2Vec uses a similarity-based self-supervised learning approach where time series representations are learned by predicting pairwise similarities within batches. The method employs two separate convolutional encoders to process temporal and frequency-domain representations of time series. Temporal similarity is measured using Soft-DTW while frequency similarity uses Euclidean distance. A transformer with order-invariant attention mechanism ensures that similar time series within each batch are mapped to similar representations. The model is trained using smooth L1 loss between predicted and true similarities, and can be applied through linear probing or fine-tuning for downstream classification tasks.

## Key Results
- Outperforms state-of-the-art self-supervised techniques on nine real-world datasets
- Achieves comparable performance to fully supervised training while requiring minimal labeled data
- Fusion with other representation learning models leads to enhanced classification performance
- Particularly effective on datasets with low sampling rates when frequency domain information is incorporated

## Why This Works (Mechanism)

### Mechanism 1
Series2Vec leverages time series similarity measures to provide implicit supervision signals, avoiding augmentation-based positive sample generation. Instead of creating transformed variants that may be less similar to the anchor than negative samples, the model uses similarity functions like Soft-DTW in the temporal domain and Euclidean distance in the frequency domain to define target outputs for the encoder loss. This ensures that the similarity between series in the original space is preserved in the learned representation space.

### Mechanism 2
The order-invariant attention mechanism ensures that similar time series within a batch are mapped to similar representations, enforcing batch-level consistency. Each time series in a batch acts as a query and attends to the keys of all other series in the batch. The resulting representations are aggregated such that similar series end up with similar attention-weighted outputs, preserving similarity structure within the batch.

### Mechanism 3
Incorporating frequency-domain information via FFT and frequency-specific similarity measures enhances the model's ability to capture diverse temporal patterns. The input time series is transformed into the frequency domain using FFT. Separate encoders process the temporal and frequency representations, and their outputs are combined. Frequency similarity is measured using Euclidean distance, which is appropriate for frequency spectra.

## Foundational Learning

- **Dynamic Time Warping (DTW) and Soft-DTW**: Robust similarity measure for time series that can handle temporal misalignments, making it suitable for defining meaningful supervision signals in self-supervised learning. Quick check: Why is Soft-DTW preferred over standard DTW? Answer: Because Soft-DTW is differentiable and can be used as a loss function to train neural networks.

- **Transformer self-attention and multi-head attention**: Allows each time series to aggregate information from all other series in the batch, enforcing that similar series have similar representations through order-invariant operations. Quick check: How does order-invariant property help preserve similarity? Answer: It ensures attention output doesn't depend on input order, so similar series get similar aggregated representations regardless of position.

- **Frequency domain analysis and Fourier Transform**: Captures periodic patterns and spectral content that may not be apparent in the time domain, providing additional discriminative features. Quick check: Why use Euclidean distance for frequency similarity instead of Soft-DTW? Answer: Because frequency spectra don't have temporal warping properties; Euclidean distance is sufficient for comparing frequency vectors.

## Architecture Onboarding

- **Component map**: Input time series -> FFT transformation -> Temporal and frequency encoders -> Pairwise similarity computation -> Transformer with multi-head attention -> Smooth L1 loss -> Concatenated representations

- **Critical path**: 1) Transform input series to frequency domain (FFT), 2) Encode temporal and frequency series separately, 3) Compute pairwise similarities in both domains, 4) Apply self-attention to enforce similarity preservation within batch, 5) Compute smooth L1 loss and backpropagate

- **Design tradeoffs**: Using Soft-DTW enables differentiable similarity learning but adds computational overhead compared to Euclidean distance; separate encoders for temporal and frequency domains increase model capacity but also complexity; batch-level attention enforces consistency but may be less effective if batch diversity is low.

- **Failure signatures**: If model fails to converge, check if similarity measures produce stable gradients; if performance is poor on frequency-sensitive datasets, verify FFT transformation and frequency encoder functionality; if attention doesn't improve results, batch may lack sufficient similarity diversity.

- **First 3 experiments**: 1) Train with only temporal encoder and Soft-DTW similarity on dataset with high temporal variation, 2) Add frequency encoder and Euclidean similarity on low-sampling-rate dataset, 3) Remove attention mechanism and assess impact on similarity preservation and downstream classification accuracy.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the limitations and discussion, several open questions emerge regarding scalability to very large datasets, effectiveness on anomaly detection tasks, and sensitivity to different similarity measures.

## Limitations

- **Similarity Measure Dependency**: Effectiveness critically depends on quality of chosen similarity measures (Soft-DTW and Euclidean distance) without extensive ablation studies on alternatives.
- **Batch-Level Supervision**: Relies on local batch constraints rather than global structure learning, which may not capture long-range dependencies effectively.
- **Computational Complexity**: Soft-DTW computation is expensive (O(n²T²)) for long time series, with limited analysis of computational efficiency compared to simpler measures.

## Confidence

- **High Confidence**: Core mechanism of using similarity measures as supervision signals is well-justified and empirically validated across multiple datasets.
- **Medium Confidence**: Claim that frequency-domain information consistently improves performance is supported but shows dataset-dependent effects.
- **Low Confidence**: Generalizability to very long time series or datasets with extreme class imbalance is not thoroughly evaluated.

## Next Checks

1. **Ablation Study on Similarity Measures**: Systematically replace Soft-DTW with alternative similarity measures (Euclidean distance, Edit distance) across all datasets to quantify specific contribution and identify scenarios where simpler measures suffice.

2. **Batch Size Sensitivity Analysis**: Conduct experiments varying batch sizes from 16 to 512 to determine minimum batch size required for effective similarity preservation and identify diminishing returns or performance degradation at extreme sizes.

3. **Long Series Performance Evaluation**: Test Series2Vec on datasets with very long time series (>10,000 time points) to evaluate computational scalability and whether similarity-based supervision remains effective when series length exceeds typical convolutional receptive fields.