---
ver: rpa2
title: Teaching Language Models to Self-Improve through Interactive Demonstrations
arxiv_id: '2310.13522'
source_url: https://arxiv.org/abs/2310.13522
tags:
- step
- feedback
- tripost
- answer
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces TriPosT, an iterative training algorithm\
  \ designed to enhance smaller language models\u2019 self-improvement capabilities,\
  \ addressing their performance gap compared to larger models on math and reasoning\
  \ tasks. TriPosT operates through three stages: interactive trajectory editing (where\
  \ a smaller model interacts with LLMs to generate and refine its own solutions),\
  \ data post-processing (filtering and rebalancing trajectories), and model training\
  \ (using weighted supervised learning on the curated data)."
---

# Teaching Language Models to Self-Improve through Interactive Demonstrations

## Quick Facts
- arXiv ID: 2310.13522
- Source URL: https://arxiv.org/abs/2310.13522
- Authors: Tianwei Zhang, Yikang Shen, Athul Paul Jacob, Zhou Yu, Yang Liu, Chuang Gan, Leslie Pack Kaelbling, Joshua B. Tenenbaum, Leslie Kaelbling, Joshua B. Tenenbaum, Jacob Andreas
- Reference count: 17
- Primary result: TriPosT improves LLaMA-7B accuracy on BIG-Bench Hard tasks by up to 7.13%

## Executive Summary
This paper introduces TriPosT, an iterative training algorithm that enables smaller language models to develop self-improvement capabilities by learning from their own mistakes with LLM assistance. The method addresses the performance gap between small and large models on math and reasoning tasks by creating a loop where the model generates attempts, receives corrections from an LLM, and learns from these edited trajectories. Experiments show that TriPosT significantly outperforms models trained with ground truth rationales or LLM demonstrations, with improvements of up to 7.13% on BIG-Bench Hard datasets.

## Method Summary
TriPosT is an iterative algorithm consisting of three stages: (1) Interactive Trajectory Editing, where a smaller model generates task attempts that are edited by LLM feedback and improvements, (2) Data Post-processing, which filters and rebalances trajectories to maintain a controlled proportion of self-improvement data, and (3) Model Training, which uses weighted supervised learning to train the model with higher loss weights on feedback and improvement tokens. The process repeats for multiple iterations, with each cycle improving both the model's initial task performance and its ability to self-improve.

## Key Results
- TriPosT improves LLaMA-7B accuracy by up to 7.13% on BIG-Bench Hard datasets compared to baseline approaches
- Models trained with TriPosT outperform those trained with ground truth rationales or LLM demonstrations on both in-domain and out-of-domain tasks
- Self-improvement frequency and contribution to performance are optimized with a data proportion of p=0.43 and weight of w=1.5
- Performance improvements compound through iterative training, with models after 3 iterations showing the best results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TriPosT enables small models to learn from their own mistakes by creating a loop where the model's errors are identified and corrected with LLM assistance, then used as training data.
- Mechanism: The algorithm generates trajectories where a small model attempts a task, receives feedback from an LLM, and updates its response. These edited trajectories are filtered and rebalanced, then used to train the model with weighted supervised learning. This process repeats iteratively.
- Core assumption: Small models can benefit from learning from their own mistakes if those mistakes are corrected with high-quality feedback, even if they initially can't self-improve on their own.
- Evidence anchors:
  - [abstract]: "We then replay this experience to train the small model."
  - [section]: "TRIPOST is an iterative algorithm consisting of three stages: Interactive Trajectory Editing, Data Post-processing, and Model Training."
  - [corpus]: Weak evidence. Corpus contains related work on vision-language models and RL but no direct evidence for this specific mechanism.
- Break condition: If the LLM feedback is consistently poor or the filtering process removes too many useful trajectories, the model may not learn effectively.

### Mechanism 2
- Claim: Weighted supervised learning with emphasis on feedback and improvement tokens helps the model focus on learning the self-improvement process rather than just correct answers.
- Mechanism: During training, tokens belonging to feedback or improvement steps in self-improving trajectories are given higher loss weights (w > 1.0), while tokens in directly correct trajectories have weight 1.0. This prioritizes learning from mistakes over memorizing correct answers.
- Core assumption: Emphasizing the learning of feedback and improvement steps will better enable the model to develop self-improvement capabilities than treating all tokens equally.
- Evidence anchors:
  - [abstract]: "Finally, we use weighted supervised learning to train a smaller model Mθ on the post-processed data."
  - [section]: "To promote the model to focus on learning the feedback and improvement steps in ximp, we use a weighted cross-entropy loss."
  - [corpus]: Weak evidence. No direct corpus evidence for this specific weighting mechanism, though related to general supervised learning principles.
- Break condition: If weights are too high, the model may overfit to feedback patterns and fail to generate correct initial attempts.

### Mechanism 3
- Claim: Iterative refinement through multiple TriPosT iterations progressively improves both the model's initial task performance and its self-improvement ability.
- Mechanism: Each iteration uses the current model to generate attempts, which are then edited by LLM feedback and improvements. The model is retrained on this expanded dataset, and the process repeats. This creates a virtuous cycle where better initial attempts lead to more useful feedback.
- Core assumption: The quality of feedback and improvements will remain high even as the model improves, and that iterative training will compound benefits rather than plateau quickly.
- Evidence anchors:
  - [abstract]: "TRIPOST repeats entire the process several times."
  - [section]: "We find that in all tasks, models trained after TRIPOST(t = 3) outperform both baselines" and "we also observe improvement in the performance of TRIPOST-trained models as the number of iterations t increases."
  - [corpus]: Weak evidence. No direct corpus evidence for this specific iterative mechanism, though related to general iterative training concepts.
- Break condition: If the model improves too quickly, it may generate fewer mistakes, making it harder to collect useful improvement demonstrations (as shown in Figure 3 where data collection becomes harder after 3 iterations).

## Foundational Learning

- Concept: Supervised learning with weighted loss functions
  - Why needed here: The weighted loss allows the model to focus learning on the more challenging aspects of self-improvement (generating feedback and improvements) rather than just memorizing correct answers.
  - Quick check question: If all tokens were weighted equally (w=1.0), what aspect of self-improvement would the model likely struggle with the most?

- Concept: Reinforcement learning concepts (exploration and feedback loops)
  - Why needed here: The interactive trajectory editing stage functions similarly to exploration in RL, where the model tries actions and receives feedback, creating a learning loop.
  - Quick check question: How does the feedback module FBK in TriPosT serve a similar function to a reward signal in traditional RL?

- Concept: Data rebalancing and filtering techniques
  - Why needed here: The post-processing stage ensures the training data isn't dominated by self-improvement examples when the model is already performing well, preventing the model from trying to "fix" correct answers.
  - Quick check question: What would happen to model performance if the proportion p of self-improvement data was set to 1.0 (all data being self-improvement)?

## Architecture Onboarding

- Component map: Small model Mθ -> Generate attempt -> FBK generates feedback -> IMP generates improvement -> Filter and rebalance -> Weighted SL training -> Repeat

- Critical path: Small model → Generate attempt → FBK generates feedback → IMP generates improvement → Filter and rebalance → Weighted SL training → Repeat

- Design tradeoffs:
  - LLM vs script for feedback: Scripts are more consistent but limited in error types; LLMs are more flexible but may introduce noise
  - Weight values: Higher weights emphasize feedback learning but may cause overfitting
  - Iteration count: More iterations improve performance but eventually hit diminishing returns when the model makes fewer mistakes

- Failure signatures:
  - Model rarely attempts self-improvement: Likely p is too low or weights are too low
  - Model tries to "fix" correct answers: Likely p is too high or filtering is too permissive
  - No performance improvement after iterations: Likely LLM feedback quality is poor or filtering is too aggressive

- First 3 experiments:
  1. Run with p=0.43 and w=1.5, measure SI.Freq and SI.Contrib to establish baseline self-improvement behavior
  2. Vary p from 0.05 to 0.70 while keeping w=1.5, observe trade-off between self-improvement attempts and task performance
  3. Vary w from 1.0 to 3.0 while keeping p=0.43, observe effect on model's tendency to self-improve vs. generating correct initial attempts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal proportion p of self-improvement data (ximp) versus directly correct data (xT) for training small models to achieve both good self-improvement ability and high task performance?
- Basis in paper: [explicit] The paper investigates varying p in Table 7, finding that too high p (e.g., 0.70) leads to many failed improvement attempts, while p = 0.42 works best for Multistep Arithmetic and Logical Deduction.
- Why unresolved: The paper only tests a limited range of p values (0.05 to 0.70) and focuses on two datasets. The optimal p may depend on task complexity, model size, and the difficulty of generating useful feedback.
- What evidence would resolve it: Systematic experiments varying p across a wider range of tasks and model sizes, combined with analysis of the trade-off between self-improvement frequency and task accuracy.

### Open Question 2
- Question: How can the feedback and improvement modules (FBK and IMP) be made more robust to handle the increasing difficulty of generating feedback and improvements as the model's performance improves over TRIPOST iterations?
- Basis in paper: [explicit] Section 4.3 discusses how the number of collectible ximp data decreases as TRIPOST iterations increase, as the model poses a greater challenge for FBK and IMP to generate feedback and improvements.
- Why unresolved: The paper identifies this as a limitation but does not propose solutions. The robustness of FBK and IMP is crucial for the scalability of TRIPOST to more advanced models and tasks.
- What evidence would resolve it: Development and evaluation of more sophisticated FBK and IMP modules, such as using fine-tuned LLMs or incorporating symbolic reasoning, to handle more complex error cases.

### Open Question 3
- Question: Can TRIPOST be effectively applied to tasks beyond math and reasoning, such as creative writing or knowledge-grounded dialogue generation?
- Basis in paper: [inferred] The paper mentions in the Limitations section that TRIPOST is task-agnostic and could theoretically be applied to other tasks like creative story writing or dialogue generation.
- Why unresolved: The paper only evaluates TRIPOST on math and reasoning tasks, which are well-defined and have clear performance metrics. Other tasks may require different feedback formats or success criteria.
- What evidence would resolve it: Experiments applying TRIPOST to diverse tasks, with appropriate modifications to the feedback and improvement generation process, and evaluation of both task performance and the model's ability to learn from its mistakes.

## Limitations
- Relies on external LLM assistance for feedback and improvements, creating a dependency that may not scale to real-world scenarios
- Shows diminishing returns after 3 iterations, suggesting potential plateaus in self-improvement capabilities
- Evaluation limited to BIG-Bench Hard tasks, raising questions about generalizability to other domains

## Confidence
**High Confidence:** The core claim that TriPosT improves LLaMA-7B accuracy on BIG-Bench Hard tasks by up to 7.13% compared to baseline approaches.

**Medium Confidence:** The claim that learning from self-corrected mistakes is crucial for small models to develop self-improvement abilities.

**Low Confidence:** The generalizability of TriPosT to tasks beyond BIG-Bench Hard or to models significantly smaller than LLaMA-7B.

## Next Checks
1. **Transferability Test:** Evaluate TriPosT-trained models on completely different reasoning tasks (e.g., commonsense reasoning, code generation) to assess whether the self-improvement capabilities generalize beyond the training domain.

2. **Model Size Scaling:** Apply TriPosT to progressively smaller models (e.g., LLaMA-7B → LLaMA-13B → LLaMA-33B → LLaMA-65B) to identify the minimum effective model size and determine if the approach has practical limitations for very small models.

3. **Autonomous Self-Improvement Test:** Remove LLM assistance entirely and measure whether models trained with TriPosT can still demonstrate self-improvement capabilities, or if they've merely learned to mimic the pattern of correction without genuine reasoning improvement.