---
ver: rpa2
title: Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis
  Generation
arxiv_id: '2312.15643'
source_url: https://arxiv.org/abs/2312.15643
tags:
- hypothesis
- graph
- knowledge
- logical
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of abductive logical reasoning over
  knowledge graphs (KGs), aiming to infer the most probable logical hypothesis to
  explain a set of observations. The authors propose a generative approach that first
  samples hypothesis-observation pairs and trains a transformer-based model via supervised
  learning.
---

# Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation

## Quick Facts
- arXiv ID: 2312.15643
- Source URL: https://arxiv.org/abs/2312.15643
- Reference count: 35
- One-line primary result: RLF-KG achieves state-of-the-art abductive reasoning in KGs by optimizing hypothesis generation with KG-based rewards, outperforming supervised learning and search-based methods.

## Executive Summary
This paper introduces abductive logical reasoning over knowledge graphs (KGs), where the goal is to infer the most probable logical hypothesis explaining a set of observations. The authors propose a generative approach that first samples hypothesis-observation pairs and trains a transformer-based model via supervised learning. However, supervised learning alone does not ensure better explanations due to structural similarity gaps. To address this, they introduce Reinforcement Learning from Knowledge Graph (RLF-KG), which uses PPO to minimize the discrepancy between observations and conclusions drawn from generated hypotheses according to the KG. Experiments on FB15k-237, WN18RR, and DBpedia50 show that RLF-KG significantly improves hypothesis quality, achieving state-of-the-art results. The method is also more efficient than search-based approaches.

## Method Summary
The method is a two-stage approach for abductive logical reasoning in KGs. First, hypothesis-observation pairs are sampled from the training KG using predefined logical query patterns. A transformer-based conditional generation model (encoder-decoder or decoder-only) is trained via supervised learning to generate hypotheses from observations. Second, RLF-KG fine-tunes the model using reinforcement learning with PPO. The reward is the Jaccard similarity between observations and conclusions drawn from generated hypotheses on the training KG, plus a KL divergence penalty. The final model is evaluated on test observations using Jaccard and SMATCH scores.

## Key Results
- RLF-KG significantly improves hypothesis quality, achieving state-of-the-art results on FB15k-237, WN18RR, and DBpedia50.
- The generative approach is more efficient than search-based methods, with faster inference times.
- Using both Jaccard and SMATCH as rewards improves structural similarity while maintaining explanatory quality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLF-KG improves hypothesis quality by using KG feedback to directly optimize the Jaccard similarity between observations and conclusions drawn from generated hypotheses.
- Mechanism: The method uses reinforcement learning (PPO) with a reward function based on the Jaccard index between the observation set and the conclusion of the generated hypothesis on the training KG. This aligns the model's generation objective with the actual abductive reasoning goal, rather than just structural similarity.
- Core assumption: The Jaccard similarity between observations and conclusions on the training KG is a valid proxy for the true abductive reasoning objective, since the test KG contains unseen edges but the training KG is large enough to approximate the target metric.
- Evidence anchors:
  - [abstract]: "we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusions drawn from generated hypotheses according to the KG"
  - [section 3.2]: "Since G is the observed training graph, the model cannot acquire any information from the test edges. Therefore, the Jaccard similarity between O and [[H]]G serves as an approximation, with no information leakage, to the objective of the abductive reasoning task defined in Equation 4."
- Break condition: If the training KG is too sparse or biased, the Jaccard reward may not reflect true explanatory quality, leading to overfitting to the training graph's structure rather than real abductive reasoning.

### Mechanism 2
- Claim: Supervised training alone cannot guarantee better explanations because it only minimizes structural differences between generated and reference hypotheses.
- Mechanism: The supervised model learns to mimic the reference hypothesis structure but does not optimize for explanatory power. RLF-KG addresses this by optimizing for the actual abductive reasoning objective using feedback from the KG.
- Core assumption: Higher structural similarity between generated and reference hypotheses does not imply better explanatory hypotheses for the given observations.
- Evidence anchors:
  - [abstract]: "However, supervised learning alone does not ensure better explanations due to structural similarity gaps."
  - [section 3]: "However, supervised training only minimizes structural differences between the generated and reference hypotheses, and higher structural similarity does not guarantee a better explanation."
- Break condition: If the reference hypotheses are poor explanations themselves, the supervised model might still generate reasonable hypotheses, but RLF-KG would optimize for a flawed target.

### Mechanism 3
- Claim: The generative approach is more efficient and robust than search-based methods for complex logical hypotheses in incomplete KGs.
- Mechanism: By directly generating hypotheses conditioned on observations, the model avoids the combinatorial explosion of the search space and is less sensitive to missing edges in the KG.
- Core assumption: The generative model can capture the distribution of plausible hypotheses without exhaustive search, and the incompleteness of KGs does not severely impact the generation process.
- Evidence anchors:
  - [abstract]: "Traditional approaches use symbolic methods, like searching, to tackle the knowledge graph problem. However, the symbolic methods are unsuitable for this task, because the KGs are naturally incomplete, and the logical hypotheses can be complex with multiple variables and relations."
  - [section 4.6]: "notably, generation-based models of both architectures consistently exhibit significantly faster performance compared to the search-based method."
- Break condition: If the generative model fails to capture the true hypothesis distribution or the KG is extremely incomplete, the generated hypotheses may be poor explanations, and search might still be necessary for certain cases.

## Foundational Learning

- Concept: First-order logic and logical reasoning over knowledge graphs
  - Why needed here: The task involves generating complex logical hypotheses with quantifiers, conjunctions, disjunctions, and negations to explain observations in a KG.
  - Quick check question: What is the difference between deductive and abductive reasoning in the context of knowledge graphs?

- Concept: Reinforcement learning and policy optimization (PPO)
  - Why needed here: RLF-KG uses PPO to optimize the hypothesis generation model based on feedback from the KG, aligning the generation objective with the abductive reasoning goal.
  - Quick check question: How does PPO differ from standard policy gradient methods, and why is it suitable for this task?

- Concept: Knowledge graph completion and reasoning tasks
  - Why needed here: Understanding related tasks like complex query answering and rule mining helps contextualize the abductive reasoning problem and the proposed solution.
  - Quick check question: What are the key differences between abductive reasoning and knowledge graph completion?

## Architecture Onboarding

- Component map:
  - Observation-hypothesis pair sampler -> Tokenizer -> Conditional generation model -> RLF-KG module -> Evaluation module

- Critical path:
  1. Sample observation-hypothesis pairs from the training KG.
  2. Tokenize observations and hypotheses.
  3. Train the conditional generation model using supervised learning.
  4. Fine-tune the model using RLF-KG with PPO and Jaccard rewards.
  5. Evaluate the model on test observations using Jaccard and SMATCH metrics.

- Design tradeoffs:
  - Encoder-decoder vs. decoder-only architecture: Encoder-decoder may better handle unordered observations, but decoder-only is simpler and faster.
  - Reward function: Jaccard only vs. Jaccard + SMATCH: Including SMATCH improves structural similarity but may reduce explanatory quality.
  - Sampling strategy: Restricting observation size and ensuring hypotheses have novel conclusions on validation/testing graphs prevents overfitting.

- Failure signatures:
  - Low Jaccard scores: The generated hypotheses do not explain the observations well.
  - High SMATCH but low Jaccard: The hypotheses are structurally similar to references but poor explanations.
  - Slow inference: The generation model is inefficient or the KG is too large for fast graph search.
  - Overfitting: The model performs well on training but poorly on validation/testing, indicating it memorized rather than generalized.

- First 3 experiments:
  1. Train the conditional generation model using only supervised learning and evaluate Jaccard and SMATCH scores.
  2. Apply RLF-KG to the supervised model and compare performance with and without the SMATCH reward term.
  3. Compare the generation-based method with a brute-force search approach in terms of Jaccard, SMATCH, and inference time.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- The evaluation relies on proxy metrics (Jaccard and SMATCH) computed on the training KG, which may not fully capture true explanatory quality for unseen test edges.
- The method's scalability and robustness to extremely incomplete KGs or complex hypotheses with many variables and relations are not fully explored.
- The exact tokenization and graph search algorithms for deriving conclusions from generated hypotheses are underspecified, which may hinder faithful reproduction.

## Confidence
- **High**: Task definition, experimental setup (datasets, metrics), core RLF-KG mechanism, and reported performance improvements are well-supported and clearly explained.
- **Medium**: The effectiveness of the Jaccard similarity reward as a proxy for true abductive reasoning quality, and the claim that supervised learning alone is insufficient, are supported but rely on assumptions about KG representativeness.
- **Low**: Detailed implementation specifics (tokenization, graph search, PPO hyperparameters) are not fully specified, making exact reproduction challenging.

## Next Checks
1. **Tokenization and Graph Search Verification**: Implement and verify the exact tokenization scheme for logical hypotheses and the graph search algorithm used to derive conclusions from generated hypotheses, ensuring consistency with the paper's approach.
2. **Reward Function Ablation**: Conduct experiments ablating the Jaccard reward (using only supervised learning or a different reward) to confirm that RLF-KG's improvements are due to the proposed KG-based feedback mechanism.
3. **KG Representativeness Analysis**: Analyze the training KG's coverage and bias to assess whether the Jaccard reward on the training graph is a valid proxy for test performance, potentially using synthetic or controlled KGs for validation.