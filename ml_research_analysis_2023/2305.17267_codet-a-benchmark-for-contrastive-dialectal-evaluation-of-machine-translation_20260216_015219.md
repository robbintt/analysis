---
ver: rpa2
title: 'CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation'
arxiv_id: '2305.17267'
source_url: https://arxiv.org/abs/2305.17267
tags:
- sentences
- comet
- dialects
- language
- varieties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CODET, a contrastive dialectal benchmark
  for machine translation. The authors compile contrastive data from various sources
  across 12 languages, including Arabic, Basque, Bengali, Central Kurdish, Occitan,
  Italian, Swiss German, Tigrinya, Griko, and others.
---

# CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation

## Quick Facts
- arXiv ID: 2305.17267
- Source URL: https://arxiv.org/abs/2305.17267
- Reference count: 20
- Primary result: MT systems excel at standard variants but struggle with dialectal variations, with performance correlating with geographic proximity to standard language regions.

## Executive Summary
This paper introduces CODET, a contrastive dialectal benchmark for machine translation that evaluates how well MT systems handle dialectal variations across 12 languages. The authors compile contrastive data from various sources and evaluate state-of-the-art NLLB models, revealing significant performance discrepancies between standard and dialectal variants. The study demonstrates that MT systems perform well on standard variants but show declining quality as dialectal variations increase, particularly for dialects geographically distant from standard language regions.

## Method Summary
The CODET benchmark compiles contrastive sentence pairs across 12 languages, comparing dialectal variants with their corresponding standard variants. The evaluation uses pseudo-reference scoring where translations of standard variants serve as references for evaluating translations of dialectal variants. Four different-sized NLLB-200 models are evaluated using COMET and BLEU metrics. For languages with multiple dialects, a subset of common sentences enables fair comparison across dialects. The methodology allows robust evaluation without requiring gold references for dialectal variants.

## Key Results
- MT systems show high performance on standard variants but declining quality as dialectal variations increase
- Geographic proximity to standard language regions correlates with better MT performance on dialects
- BLEU and COMET scores consistently drop for dialectal variants compared to standard variants across all evaluated languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NMT systems perform well on standard language variants but degrade on dialectal variations due to training data bias toward standard forms
- Mechanism: Standard language variants are overrepresented in MT training corpora, leading models to learn patterns that generalize poorly to dialectal forms
- Core assumption: Training data distribution reflects standard language usage more than dialectal usage
- Evidence anchors:
  - [abstract]: "MT systems excel at handling standard variants but struggle with dialectal variations"
  - [section 4]: "The findings demonstrate that MT systems excel at handling standard variants, but as the dialectal varieties start differing from the standard, the quality of the translations declines"
  - [corpus]: Weak - no explicit training data composition analysis provided

### Mechanism 2
- Claim: Contrastive examples between standard and dialectal variants allow robust evaluation of MT systems without requiring gold references
- Mechanism: By comparing translations of dialectal and standard variants with the same meaning, we can measure robustness through pseudo-reference scoring
- Core assumption: Standard variants are well-translated by MT systems and can serve as reliable pseudo-references
- Evidence anchors:
  - [section 3]: "we can treat Ëœy, the output of the MT system on the 'standard' input, as a pseudo-reference for the translation"
  - [abstract]: "we compile and release CODET, a contrastive dialectal benchmark encompassing 882 different variations from nine different languages"
  - [corpus]: Moderate - implementation details for pseudo-reference scoring are provided

### Mechanism 3
- Claim: Geographic proximity to the standard language region correlates with better MT performance on dialects
- Mechanism: Dialects closer to standard language regions share more linguistic features with the standard, making them easier for MT systems to translate
- Core assumption: Linguistic similarity between standard and dialectal variants decreases with geographic distance
- Evidence anchors:
  - [section 4]: "From the visualization, it becomes evident that regions close to Tuscany exhibit a darker green color, indicating higher scores" (Italian dialects)
  - [section 4]: "The map reveals a consistent pattern where the northern regions, being closer to Germany... obtain higher COMET scores" (Swiss German)
  - [corpus]: Moderate - geographic mapping of dialect performance is presented

## Foundational Learning

- Concept: Contrastive evaluation methodology
  - Why needed here: To evaluate MT performance on dialects without gold references
  - Quick check question: How can we measure MT robustness on dialects when we don't have gold standard translations?

- Concept: Pseudo-reference scoring
  - Why needed here: To create reference translations from model outputs on standard variants
  - Quick check question: Why is the standard variant translation used as a pseudo-reference for dialectal evaluation?

- Concept: Dialect geography and linguistic similarity
  - Why needed here: To understand performance patterns across different dialects
  - Quick check question: How might geographic distance from standard language regions affect dialect similarity?

## Architecture Onboarding

- Component map: Data collection -> Benchmark compilation -> Evaluation framework -> Analysis pipeline
- Critical path: 1. Compile contrastive data for target language varieties 2. Translate using baseline MT models 3. Score translations using pseudo-reference method 4. Analyze performance patterns and geographic correlations
- Design tradeoffs: Limited dataset size vs. comprehensive dialect coverage, Pseudo-reference evaluation vs. gold standard evaluation, Geographic visualization vs. detailed linguistic analysis
- Failure signatures: Poor COMET scores across all dialects suggest baseline model inadequacy, Inconsistent geographic patterns indicate non-linguistic factors, High variance within dialect groups suggests insufficient training data
- First 3 experiments: 1. Evaluate baseline NLLB models on CODET benchmark and establish performance baselines 2. Implement pseudo-reference scoring and compare with gold reference evaluation where available 3. Analyze geographic performance patterns and identify high-performing/low-performing regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we create effective training datasets for dialectal machine translation systems, given the scarcity of parallel data for many dialects?
- Basis in paper: [explicit] The authors note that the current datasets are small and mainly serve as test data, and that future work should prioritize developing training datasets for dialects
- Why unresolved: The paper does not propose specific methods for collecting or creating large-scale dialectal training data, which is a crucial step for improving dialectal MT
- What evidence would resolve it: Successful experiments demonstrating improved MT performance on dialectal variants after training on newly created dialectal datasets, along with methodologies for dataset creation and annotation

### Open Question 2
- Question: What are the specific linguistic features (lexical, morphosyntactic, phonological) that most significantly impact the performance of machine translation systems on dialectal variations?
- Basis in paper: [inferred] The authors mention that dialectal sentences can vary along several dimensions and that a qualitative analysis of these variations is lacking in their study
- Why unresolved: The paper focuses on benchmarking MT systems without deeply analyzing the linguistic differences between dialects and their impact on translation quality
- What evidence would resolve it: Detailed linguistic analyses identifying key features that cause translation errors in dialectal MT, supported by error analysis on translation outputs and correlation with specific linguistic variations

### Open Question 3
- Question: How does the performance of dialectal machine translation systems vary across different target languages, and what factors influence this variation?
- Basis in paper: [explicit] The authors note that they focus on English as a target language for practical reasons and acknowledge that future work should consider many other target languages
- Why unresolved: The paper only benchmarks X-to-English translation, leaving open questions about how dialectal MT performance might differ for other target languages and what factors (e.g., linguistic distance, availability of resources) influence this
- What evidence would resolve it: Comparative studies of dialectal MT performance across multiple target languages, including analysis of factors affecting performance differences and identification of target languages that pose particular challenges for dialectal translation

## Limitations
- The CODET benchmark remains relatively small with 891 total examples across 12 languages
- The pseudo-reference evaluation method depends on the assumption that standard variants are consistently well-translated by baseline models
- Limited citation count and neighboring papers suggest the benchmark is relatively novel and may not yet have been extensively validated

## Confidence
- Mechanism 1: Medium - Core assumption about training data bias is plausible but not directly validated
- Mechanism 2: Medium - Pseudo-reference method is well-justified but relies on standard variant translation quality
- Mechanism 3: Medium - Geographic patterns are observed but may reflect training data distribution rather than purely linguistic factors

## Next Checks
1. Conduct a comprehensive analysis of the training data composition for NLLB models to quantify the representation of standard versus dialectal variants, directly testing Mechanism 1's core assumption
2. Apply the CODET evaluation methodology to additional language pairs beyond the 12 covered, particularly focusing on languages with very different typological features to test the generalizability of geographic correlation patterns
3. For a subset of dialectal examples where gold standard translations are available, compare the pseudo-reference COMET/BLEU scores against direct reference-based evaluation to validate the reliability of the contrastive approach