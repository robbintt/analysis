---
ver: rpa2
title: 'PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization'
arxiv_id: '2305.06647'
source_url: https://arxiv.org/abs/2305.06647
tags:
- summarization
- copying
- association
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROM, a phrase-level copying mechanism designed
  to enhance n-gram attention in abstractive summarization, particularly for zero-shot
  settings. PROM incorporates an indicator layer that explicitly identifies tokens
  in n-grams eligible for copying from the source and computes an auxiliary loss for
  copying prediction.
---

# PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization

## Quick Facts
- arXiv ID: 2305.06647
- Source URL: https://arxiv.org/abs/2305.06647
- Authors: Multiple authors from various institutions
- Reference count: 40
- Primary result: PROM achieves ROUGE-2 of 21.59 on CNN/DM in supervised fine-tuning and 37.87 ROUGE-1 on CNN/DM in zero-shot settings

## Executive Summary
PROM introduces a phrase-level copying mechanism designed to enhance n-gram attention in abstractive summarization, particularly for zero-shot settings. The approach adds an indicator layer that explicitly identifies tokens in n-grams eligible for copying from the source and computes an auxiliary loss for copying prediction. PROM demonstrates significant improvements over baselines in both supervised fine-tuning and zero-shot scenarios across multiple summarization datasets. The method shows better factual consistency and more reasonable copying behavior compared to existing approaches.

## Method Summary
PROM extends the pointer-generator framework by adding a copying indicator layer that predicts copying probabilities for each token in n-grams. The model uses multi-task training with an auxiliary copying loss and applies self-supervised pre-training on raw corpora using gap sentence generation. The approach involves two-stage training: first pre-training the copying indicator layer, then training the full model. PROM is built on BART-large and evaluates on multiple summarization datasets including CNN/DM, XSum, WikiHow, and arXiv.

## Key Results
- ROUGE-2 score of 21.59 on CNN/DM in supervised fine-tuning
- ROUGE-1 score of 37.87 on CNN/DM in zero-shot setting
- ROUGE-1 score of 22.96 on XSum in zero-shot setting
- Improved factual consistency measured by BERTScore and FactCC metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PROM enhances copying attention specifically for n-grams by explicitly labeling overlapping phrases between source and target.
- Mechanism: PROM adds an indicator layer that predicts copying probabilities for each token, trained with an auxiliary loss. Tokens in overlapping n-grams are labeled as copyable, so the model learns to attend more to these spans rather than individual tokens.
- Core assumption: Copying performance improves when the model is guided to focus on phrase-level overlaps rather than individual token matches.
- Evidence anchors:
  - [abstract] "PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction."
  - [section 3.1.2] "Instead of considering entities only, the enhancement needs to be extended to n-grams as they contain expressions of language, adaptive or common cross domains."
- Break condition: If the auxiliary copying loss interferes with the main summarization objective, or if phrase-level copying introduces too much extraction bias.

### Mechanism 2
- Claim: PROM improves factual consistency by making the copying mechanism more selective and guided.
- Mechanism: By adding explicit copying labels and an indicator layer, PROM reduces random or excessive copying, leading to more faithful reproduction of source facts in summaries.
- Core assumption: Factuality improves when copying is guided rather than automatic, especially when the model has learned which spans are safe to copy.
- Evidence anchors:
  - [abstract] "Empirical studies show that PROM makes significant improvements in fine-tuning on benchmarks."
  - [section 6.2.1] "The higher numbers show that besides more overlapped bi-grams, the copying method tends to produce more faithful and stable summaries."
- Break condition: If the indicator layer overfits to the training data and fails to generalize to new domains.

### Mechanism 3
- Claim: PROM enables better zero-shot summarization through self-supervised pre-training on raw corpora.
- Mechanism: PROM's copying mechanism is applied during pre-training on large unlabeled text, creating pseudo document-summary pairs. This teaches the model to copy and summarize without relying on supervised target summaries.
- Core assumption: The copying patterns learned during pre-training transfer well to downstream tasks even without fine-tuning.
- Evidence anchors:
  - [abstract] "In zero-shot setting, PROM is utilized in the self-supervised pre-training on raw corpora and provides new general baselines on a wide range of summarization datasets."
  - [section 4] "Our self-supervised training dataset is constructed from corpora... The data is filtered by a minimum EFDminEF D."
- Break condition: If the pseudo summaries generated during pre-training are too extractive or lack diversity, limiting transfer to abstractive tasks.

## Foundational Learning

- Concept: Cross-entropy loss
  - Why needed here: PROM uses cross-entropy to compare predicted copying probabilities with ground-truth labels, and to train the main summarization model.
  - Quick check question: What does minimizing cross-entropy achieve in PROM's copying indicator training?

- Concept: Pointer-generator networks
  - Why needed here: PROM extends the pointer-generator framework by adding phrase-level copying instead of only token-level copying.
  - Quick check question: How does PROM's copying distribution differ from standard pointer-generator approaches?

- Concept: Self-supervised learning
  - Why needed here: PROM's zero-shot capability relies on pre-training on raw corpora without labeled summaries, using constructed pseudo pairs.
  - Quick check question: Why does PROM's pre-training strategy avoid using downstream task-specific information?

## Architecture Onboarding

- Component map: Encoder -> Decoder -> Copying module -> Indicator layer -> Total loss
- Critical path: Input text -> Encoder -> Cross-attention -> Indicator layer -> Copying distribution -> Output summary
- Design tradeoffs: PROM trades off between extraction and generation by enhancing phrase copying, which can improve faithfulness but may reduce abstractiveness.
- Failure signatures: If copying dominates generation too much, summaries become too extractive; if the indicator layer fails, copying becomes noisy.
- First 3 experiments:
  1. Implement PROM's copying indicator layer and test on a small summarization dataset to confirm it learns to copy labeled phrases.
  2. Compare ROUGE scores of PROM vs baseline pointer-generator on CNN/DM to validate phrase-level gains.
  3. Run zero-shot evaluation of pre-trained PROM on XSum and WikiHow to confirm transfer without fine-tuning.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the PROM mechanism's phrase-level copying approach affect factual consistency across different domains, and what domain-specific factors influence its effectiveness?
- **Open Question 2**: What are the trade-offs between generalization and specialization in zero-shot summarization when using PROM, and how can these be optimized for specific use cases?
- **Open Question 3**: How does the lead bias in datasets impact the effectiveness of PROM's phrase-level copying, and can the model be adjusted to handle non-lead-biased data more effectively?
- **Open Question 4**: What are the potential benefits and drawbacks of combining PROM with other advanced summarization techniques, such as reinforcement learning or adversarial training?

## Limitations

- Experimental validation beyond ROUGE metrics is limited, with factuality claims relying on indirect measures
- Generalizability across languages and technical domains remains untested
- Key architectural details and hyperparameters are underspecified, making exact reproduction challenging
- Substantial computational resources required for pre-training on 160GB of raw text

## Confidence

- **High Confidence**: PROM's ability to improve ROUGE-2 scores in supervised fine-tuning settings (21.59 on CNN/DM)
- **Medium Confidence**: Claims about zero-shot performance improvements supported by reported ROUGE scores
- **Low Confidence**: Factuality improvements based on proxy metrics rather than human evaluation

## Next Checks

1. Conduct an ablation study removing the copying indicator layer while keeping the same pre-training setup to quantify the specific contribution of the phrase-level copying mechanism.

2. Perform human evaluation of factual consistency on 100 randomly sampled summaries from CNN/DM and XSum, comparing PROM outputs with baselines.

3. Evaluate PROM's zero-shot performance on non-news domains such as scientific abstracts or legal documents to test cross-domain generalization.