---
ver: rpa2
title: The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS
arxiv_id: '2310.08496'
source_url: https://arxiv.org/abs/2310.08496
tags:
- chinese
- knowledge
- bert
- ancient
- uncertain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenging problem of word segmentation\
  \ and part-of-speech tagging for ancient Chinese, where ambiguity and lack of labeled\
  \ data pose significant hurdles. The proposed framework integrates local semantic\
  \ cues via bigram features with a BERT encoder, and then applies Monte Carlo dropout\
  \ to detect uncertain spans in the model\u2019s predictions."
---

# The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS

## Quick Facts
- arXiv ID: 2310.08496
- Source URL: https://arxiv.org/abs/2310.08496
- Reference count: 0
- Primary result: F1 scores up to 96.3% for segmentation and 92.4% for joint segmentation and POS tagging on Zuozhuan

## Executive Summary
This paper addresses the challenging problem of joint word segmentation and part-of-speech tagging for ancient Chinese texts, where ambiguity and limited labeled data hinder performance. The authors propose an uncertainty-based retrieval framework that combines local semantic cues via bigram features with a BERT encoder, then uses Monte Carlo dropout to detect uncertain spans. For these uncertain spans, the model retrieves analogous sentences from a pre-Qin text corpus and re-predicts using a knowledge-fusion BERT, achieving state-of-the-art results on the Zuozhuan and Shiji datasets.

## Method Summary
The framework employs a two-stage approach. First, a baseline BERT model with bigram feature concatenation predicts word boundaries and POS tags using CRF decoding. Monte Carlo dropout generates multiple label sequences to identify uncertain spans where the model's predictions vary. For these spans, the system retrieves structurally similar sentences from a pre-Qin corpus based on character overlap similarity. A knowledge-fusion BERT then re-predictions these uncertain spans by concatenating the original input with retrieved knowledge, averaging probabilities, and applying Viterbi decoding.

## Key Results
- Achieves 96.3% F1 for CWS and 92.4% for joint CWS+POS on Zuozhuan dataset
- Outperforms strong baselines including Siku-RoBERTa with CRF and Jiayan
- Demonstrates effectiveness on both Zuozhuan and Shiji ancient Chinese datasets

## Why This Works (Mechanism)

### Mechanism 1
Bigram feature concatenation captures local semantic cues better than BERT alone for ancient Chinese. The model concatenates hidden representations from previous and next characters, projects through separate linear layers, and fuses with BERT embeddings before classification. Core assumption: Ancient Chinese words are shorter and more semantically dense, making local context more informative than long-range dependencies.

### Mechanism 2
MC-dropout uncertainty sampling identifies hard spans where the model is less confident. After first-pass prediction, multiple dropout samples generate label sequences; differences between these and initial predictions flag uncertain spans for re-prediction with knowledge fusion. Core assumption: High variance in dropout predictions correlates with model uncertainty, and uncertain spans benefit most from retrieval.

### Mechanism 3
Retrieving structurally similar ancient sentences reduces ambiguity in uncertain spans. For each uncertain span, the framework searches pre-Qin texts for sentences containing the span (or bigrams for single characters), ranks by character overlap, and uses top matches as auxiliary knowledge in second BERT pass. Core assumption: Ancient Chinese grammar is consistent enough across texts that similar sentences share interpretable structures.

## Foundational Learning

- **Conditional Random Fields (CRF)**: Captures dependencies between adjacent tags, crucial for joint CWS and POS tagging where word boundaries and tags must align. Quick check: What is the Viterbi algorithm used for in this context?
- **MC-dropout as Bayesian approximation**: Provides practical way to estimate model uncertainty without retraining multiple models. Quick check: How many dropout samples (k) are used in the experiments?
- **Similarity metrics for short text retrieval**: Used to find relevant auxiliary sentences for ancient Chinese. Quick check: What is the similarity formula used in the paper?

## Architecture Onboarding

- **Component map**: Input → BERT encoder → Bigram feature layer → Linear fusion → MLP → CRF decoder (Stage 1) → Uncertainty sampling (MC-dropout) → Span identification → Corpus retrieval → Knowledge-fusion BERT → CRF decoder (Stage 2)
- **Critical path**: 1) Encode input with baseline BERT+bigram. 2) Generate k dropout samples, detect uncertain spans. 3) Retrieve similar sentences for each span. 4) Concatenate input + retrieval + mask padding labels. 5) Re-encode with KF-BERT, average probabilities, decode with Viterbi.
- **Design tradeoffs**: Retrieval corpus size vs. retrieval latency and noise; k vs. uncertainty estimation quality vs. compute cost; bigram concatenation vs. more complex local context encoders.
- **Failure signatures**: High variance across all tokens (bad uncertainty sampling); retrieval returns irrelevant sentences (bad re-prediction); overfitting to bigram features if ancient texts have unusual word lengths.
- **First 3 experiments**: 1) Ablation: Remove bigram features, compare F1 on Zuozhuan. 2) Vary k and measure uncertainty span quality and final F1. 3) Test retrieval without bigram handling for single-character spans; compare re-prediction accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed framework perform on ancient Chinese datasets from different historical periods beyond Zuozhuan and Shiji? The paper focuses on two specific datasets and lacks experiments on other historical periods of ancient Chinese.

### Open Question 2
What is the impact of varying the number of Monte Carlo dropout iterations (k) on uncertainty detection and overall model performance? The paper mentions using k candidate label sequences but does not explore how changing k affects results.

### Open Question 3
How does the framework handle out-of-vocabulary words or rare characters not present in the knowledge corpus? The paper describes retrieving similar sentences but does not address cases where exact or similar context is not found in the corpus.

## Limitations

- Limited empirical evidence for individual mechanisms (bigram efficacy, MC-dropout quality, retrieval relevance) in isolation
- Unspecified pre-Qin corpus details and preprocessing make retrieval quality assessment impossible
- Character overlap similarity metric and single-character span handling lack rigorous justification

## Confidence

- **High confidence**: Experimental results (F1 scores on Zuozhuan and Shiji) are directly reported
- **Medium confidence**: General two-stage architecture is clearly described and plausible
- **Low confidence**: Individual mechanisms (bigram features, MC-dropout uncertainty, retrieval relevance) lack direct ablation or diagnostic evidence

## Next Checks

1. **Ablation study on bigram features**: Remove bigram concatenation and Linear bigram layers from baseline BERT model, retrain, and measure F1 drop on Zuozhuan test set to isolate bigram feature contribution.

2. **Uncertainty detection diagnostic**: Compute oracle F1 score if all uncertain spans (as identified by MC-dropout) were predicted perfectly; compare to oracle F1 for certain spans to assess whether uncertainty sampling captures genuinely hard cases.

3. **Retrieval quality analysis**: Manually sample 20 uncertain spans and their top-1 retrieved sentences; evaluate whether retrieved sentences contain spans in structurally similar contexts to validate retrieval mechanism effectiveness.