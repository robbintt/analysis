---
ver: rpa2
title: 'Towards Reliable and Fluent Large Language Models: Incorporating Feedback
  Learning Loops in QA Systems'
arxiv_id: '2309.06384'
source_url: https://arxiv.org/abs/2309.06384
tags:
- feedback
- citation
- arxiv
- correctness
- aspects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses issues of citation, correctness, and fluency
  in LLM-generated text by training a critic model on a pseudo-labeled dataset and
  implementing a feedback learning loop. The approach provides iterative refinement
  signals to the LLM based on heterogeneous aspect scores, improving fluency (MAUVE:
  +8%), citation precision (+4%), and maintaining correctness.'
---

# Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems

## Quick Facts
- arXiv ID: 2309.06384
- Source URL: https://arxiv.org/abs/2309.06384
- Reference count: 0
- Primary result: Iterative feedback learning improves LLM-generated text fluency, citation precision, and correctness across multiple metrics

## Executive Summary
This study addresses the critical challenge of improving reliability and fluency in large language model (LLM) outputs for question-answering systems. The authors propose a novel approach that trains a critic model using pseudo-labeled data to evaluate and provide feedback on heterogeneous aspects of generated text. Through an iterative feedback learning loop, the LLM refines its outputs based on actionable feedback, demonstrating substantial improvements in citation precision (4%), fluency (8% MAUVE), and maintained correctness.

## Method Summary
The approach involves training a critic model on pseudo-labeled data generated by LLMs to evaluate generated text across three aspects: fluency, correctness, and citation. The critic provides actionable feedback for aspects scoring below threshold, which the LLM uses to refine its outputs iteratively. This feedback learning loop continues until performance plateaus or reaches a predefined iteration limit, with the goal of improving overall output quality while maintaining strengths in other areas.

## Key Results
- Citation precision improved by 4% through iterative feedback learning
- Fluency metrics (MAUVE) showed approximately 8% enhancement
- Correctness maintained at high levels while improving other aspects
- The approach outperformed baseline ChatGPT across EM Recall, citation recall, and other evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
The pseudo-labeled dataset enables training a critic model that can reliably assess heterogeneous aspects of LLM-generated text. LLMs generate both positive and negative examples for fluency, correctness, and citation aspects. The critic model learns to discriminate between these examples using contrastive learning objectives. Core assumption: The pseudo-labeled data generated by LLMs is sufficiently accurate to train a critic model that can generalize to real LLM outputs. Evidence: Critic model accuracy scores were remarkably high at 97.28% for fluency, 98.46% for correctness, and 97.96% for citation. Break condition: If pseudo-labeled data quality degrades significantly, critic model accuracy would drop below usable thresholds.

### Mechanism 2
Iterative feedback learning improves LLM performance by providing targeted corrections for specific aspects. The critic model evaluates generated answers and provides actionable feedback for aspects scoring below threshold. The LLM then refines its answer based on this feedback, creating an iterative improvement cycle. Core assumption: The LLM can effectively incorporate heterogeneous feedback to improve its output without degrading other aspects. Evidence: Experimental results show substantial improvements in citation and fluency metrics while maintaining high levels of correctness. Break condition: If feedback causes the LLM to overfit to critic preferences rather than genuine improvement, performance could degrade on unseen data.

### Mechanism 3
Heterogeneous aspect scoring allows for targeted improvement without sacrificing performance in other areas. The critic provides separate scores for fluency, correctness, and citation. The feedback learning loop only focuses on aspects below threshold, preserving strengths while improving weaknesses. Core assumption: The aspects (fluency, correctness, citation) are sufficiently independent that improving one doesn't necessarily harm others. Evidence: Substantial improvements in citation and fluency metrics while maintaining high levels of correctness. Break condition: If aspects are not truly independent, focusing feedback on one aspect could inadvertently harm others.

## Foundational Learning

- Concept: Contrastive learning for aspect discrimination
  - Why needed here: The critic model needs to distinguish between positive and negative examples across multiple heterogeneous aspects simultaneously
  - Quick check question: How does the loss function ensure the model assigns higher scores to positive examples than negative ones for each aspect?

- Concept: Reinforcement learning from critic feedback
  - Why needed here: The iterative feedback loop uses the critic's scores as reward signals to guide LLM improvement, similar to RL frameworks
  - Quick check question: What prevents the LLM from optimizing for critic preferences rather than genuine improvement in the target aspects?

- Concept: Pseudo-labeling with LLMs
  - Why needed here: The lack of labeled training data for heterogeneous aspects is addressed by using LLMs to generate synthetic training examples
  - Quick check question: How can we validate that pseudo-labeled data is sufficiently accurate for training a reliable critic model?

## Architecture Onboarding

- Component map: LLM generation → Critic evaluation → Feedback generation → LLM refinement → Evaluation
- Critical path: LLM generation → Critic evaluation → Feedback generation → LLM refinement → Evaluation
- Design tradeoffs:
  - Pseudo-labeling quality vs. quantity: Higher quality labels take longer to generate but improve critic performance
  - Feedback specificity vs. generality: More specific feedback may be more actionable but less generalizable
  - Iteration count vs. diminishing returns: More iterations improve performance but with decreasing marginal gains
- Failure signatures:
  - Critic accuracy drops below 90%: Indicates poor pseudo-labeled data quality
  - LLM performance degrades on correctness while improving fluency: Suggests aspect interference
  - Feedback becomes repetitive without improvement: Indicates feedback saturation
- First 3 experiments:
  1. Train critic model on small pseudo-labeled dataset and measure aspect accuracy on held-out examples
  2. Apply one iteration of feedback learning to ChatGPT and measure improvement in targeted aspects
  3. Compare performance across different iteration counts to identify optimal number of feedback cycles

## Open Questions the Paper Calls Out

### Open Question 1
How does the iterative feedback learning loop affect the long-term stability and generalization of the LLM across diverse domains? The paper introduces an iterative feedback learning loop using a critic model to refine LLM performance on heterogeneous aspects like fluency, correctness, and citation, but does not explore how repeated iterations of feedback might impact the model's adaptability or performance on tasks outside the tested domains. Conducting experiments that test the LLM's performance on diverse, unseen domains after multiple iterations of feedback, and analyzing any degradation or improvement in generalization capabilities would resolve this question.

### Open Question 2
Can the critic model's feedback mechanism be extended to other aspects of LLM outputs, such as ethical considerations or bias detection? The paper focuses on training the critic model to evaluate fluency, correctness, and citation, but does not explore other dimensions like ethical implications or bias. Expanding the critic model's training to include ethical and bias-related aspects, followed by empirical studies measuring its effectiveness in identifying and mitigating these issues would resolve this question.

### Open Question 3
How scalable is the proposed method for larger-scale applications with more complex queries or multilingual datasets? The study demonstrates improvements in fluency, citation precision, and correctness using a specific dataset and model architecture, but does not address the scalability of the method when applied to larger datasets, more complex queries, or multilingual contexts. Testing the method on larger, more diverse datasets, including multilingual queries, and analyzing the computational and performance impacts of scaling up the approach would resolve this question.

## Limitations
- Reliance on ChatGPT for pseudo-label generation may not generalize to other LLM architectures
- Potential for feedback overfitting to critic preferences rather than genuine capability improvement
- Evaluation focuses primarily on the ASQA dataset, limiting confidence in broader applicability

## Confidence
- Generalizability to other quality dimensions: **Medium**
- Cross-domain generalization: **Low**
- Dependency on specific LLM architecture: **Medium**

## Next Checks
1. Test the critic model's performance when trained on pseudo-labeled data from different LLM architectures (e.g., Claude, Gemini) to assess dependency on ChatGPT
2. Evaluate the feedback learning approach on datasets outside the ASQA domain to measure cross-domain effectiveness
3. Conduct ablation studies varying the number of feedback iterations to identify optimal stopping points and prevent overfitting