---
ver: rpa2
title: 'Allies: Prompting Large Language Model with Beam Search'
arxiv_id: '2305.14766'
source_url: https://arxiv.org/abs/2305.14766
tags:
- answer
- question
- arxiv
- language
- beam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BeamSearchQA, a novel approach to open-domain
  question answering that leverages large language models (LLMs) to iteratively generate
  new questions and expand the scope of the original query. The method addresses two
  key limitations of existing approaches: narrow information coverage and low fault
  tolerance.'
---

# Allies: Prompting Large Language Model with Beam Search

## Quick Facts
- arXiv ID: 2305.14766
- Source URL: https://arxiv.org/abs/2305.14766
- Authors: 
- Reference count: 13
- Primary result: BeamSearchQA significantly outperforms zero-shot baselines on NQ and WebQ datasets for open-domain question answering

## Executive Summary
This paper introduces BeamSearchQA, a novel approach to open-domain question answering that leverages large language models (LLMs) to iteratively generate new questions and expand the scope of the original query. The method addresses two key limitations of existing approaches: narrow information coverage and low fault tolerance. BeamSearchQA employs a beam search strategy, generating additional queries based on the original query-evidence pair and retrieving relevant evidence from external sources. The retrieved evidence is then used to augment the query-evidence pair, and the LLM answers the question based on the augmented information. The process continues iteratively until a sufficiently confident answer is obtained or the maximum depth is reached.

## Method Summary
BeamSearchQA is a zero-shot open-domain question answering method that uses large language models with iterative query generation and retrieval. The approach starts by answering the original query directly and with retrieved evidence, then iteratively generates new queries, retrieves additional evidence, answers the question, and scores the answers. This process continues until a sufficiently confident answer is obtained or the maximum depth is reached. The method employs a beam search strategy with dynamic pruning to retain the top B answers at each step, increasing robustness by allowing the LLM to make mistakes during the reasoning process.

## Key Results
- BeamSearchQA significantly outperforms other zero-shot baselines on NQ and WebQ datasets
- The iterative query generation with beam search captures implicit knowledge not directly retrievable with the original query
- The scoring function with a predefined threshold enables termination when a sufficiently confident answer is obtained

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative query generation with beam search allows the LLM to capture implicit knowledge that cannot be directly retrieved using the original query.
- Mechanism: BeamSearchQA generates additional queries by asking the LLM what other information it requires based on the current query-evidence pair. These queries are used to retrieve relevant evidence from external sources, expanding the scope of information available for answering the question.
- Core assumption: The LLM has sufficient reasoning ability to identify gaps in its knowledge and generate relevant follow-up queries.
- Evidence anchors:
  - [abstract] "By iteratively refining and expanding the scope of the original query, BeamSearchQA captures and utilizes hidden knowledge that may not be directly obtainable through retrieval."
  - [section 3.1] "Initially, we generate additional queries by asking the LLM what other information they require, based on the existing query-evidence pair."
  - [corpus] Weak evidence - no directly relevant papers found in the neighbor corpus.
- Break condition: The LLM fails to generate meaningful follow-up queries that lead to relevant evidence, or the retrieved evidence does not improve the answer quality.

### Mechanism 2
- Claim: The scoring function with a predefined threshold enables BeamSearchQA to terminate the search when a sufficiently confident answer is obtained.
- Mechanism: After generating an answer based on the augmented query-evidence pair, the LLM scores the answer based on the reasoning history. If the score exceeds the predefined threshold, the search terminates and the answer is outputted.
- Core assumption: The LLM's scoring ability is reliable in assessing the confidence of its own answers.
- Evidence anchors:
  - [abstract] "Subsequently, we solicit the LLM to score the answer, taking into account the question and the augmented query-evidence pair. This scoring process provides a measure of confidence in the generated answer."
  - [section 3.1] "Next, we employ the LLM to answer the question based on the augmented query-evidence pair. Subsequently, we solicit the LLM to score the answer, taking into account the question and the augmented query-evidence pair."
  - [corpus] Weak evidence - no directly relevant papers found in the neighbor corpus.
- Break condition: The LLM's scoring is inconsistent or unreliable, leading to premature termination or continued search when a satisfactory answer is already obtained.

### Mechanism 3
- Claim: The beam search strategy with dynamic pruning retains the top B answers at each step, increasing the robustness of the model.
- Mechanism: At the end of each search depth, the newly generated answers are ranked, and only the top B answers are retained. This allows the LLM to make mistakes during the reasoning process, as erroneous answers can be replaced by alternative answers.
- Core assumption: The LLM's ability to generate multiple potential answers and the ranking of these answers is reliable.
- Evidence anchors:
  - [abstract] "Secondly, during the iterative process, we employ a dynamic pruning technique that retains only the top B answers at each step. This increases the robustness of our model by allowing the LLM to make mistakes during the reasoning process."
  - [section 3.1] "At the end of each search depth, we rank the newly generated answers and keep only the top B answers."
  - [corpus] Weak evidence - no directly relevant papers found in the neighbor corpus.
- Break condition: The ranking of answers is unreliable, leading to the retention of incorrect answers or the pruning of potentially correct ones.

## Foundational Learning

- Concept: Iterative reasoning and query expansion
  - Why needed here: BeamSearchQA relies on iteratively generating new queries and expanding the scope of the original query to capture implicit knowledge.
  - Quick check question: What is the purpose of iteratively generating new queries in BeamSearchQA, and how does it help in answering complex questions?

- Concept: Beam search strategy
  - Why needed here: BeamSearchQA employs a beam search strategy to retain the top B answers at each step, increasing the robustness of the model.
  - Quick check question: How does the beam search strategy with dynamic pruning contribute to the robustness of BeamSearchQA?

- Concept: Confidence-based termination
  - Why needed here: BeamSearchQA uses a scoring function to assess the confidence of generated answers and terminates the search when a sufficiently confident answer is obtained.
  - Quick check question: What is the role of the scoring function in BeamSearchQA, and how does it determine when to terminate the search?

## Architecture Onboarding

- Component map:
  Query input -> Beam initialization (answering the query directly and with retrieved evidence) -> Beam expansion (generating new queries, retrieving evidence, answering, and scoring) -> Beam pruning and termination (ranking answers, retaining top B, checking score threshold) -> Final answer output

- Critical path:
  The critical path in BeamSearchQA is the iterative process of beam expansion, which involves generating new queries, retrieving evidence, answering the question, and scoring the answer. This process continues until a sufficiently confident answer is obtained or the maximum depth is reached.

- Design tradeoffs:
  - Number of generated queries (K): A higher value allows for more diverse queries but increases computational cost.
  - Maximum depth of extension (D): A higher value enables more iterations but also increases computational cost and the risk of accumulating errors.
  - Number of retrieved documents (N): A higher value provides more evidence but increases computational cost and the risk of including irrelevant information.
  - Score threshold (S): A higher value ensures more confident answers but may lead to longer search times or failure to find a satisfactory answer.
  - Beam size (B): A higher value increases the chances of finding a good answer but also increases computational cost.

- Failure signatures:
  - Inability to generate meaningful follow-up queries
  - Retrieved evidence not improving answer quality
  - LLM's scoring being inconsistent or unreliable
  - Ranking of answers being unreliable

- First 3 experiments:
  1. Vary the number of generated queries (K) and observe its impact on answer quality and computational cost.
  2. Adjust the score threshold (S) and analyze its effect on the confidence of generated answers and search termination.
  3. Experiment with different beam sizes (B) and evaluate their influence on the robustness and computational efficiency of the model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BeamSearchQA compare to other zero-shot QA methods when the retrieval component is removed entirely?
- Basis in paper: [inferred] The paper discusses various zero-shot baselines, including methods that use a retriever and those that do not. It would be interesting to see how BeamSearchQA performs without the retrieval component.
- Why unresolved: The paper focuses on the effectiveness of BeamSearchQA with the retrieval component, but does not provide a comparison without it.
- What evidence would resolve it: Conducting experiments to compare the performance of BeamSearchQA with and without the retrieval component on the NQ and WebQ datasets.

### Open Question 2
- Question: How does the choice of the number of generated queries (K) and the maximum depth of extension (D) affect the performance of BeamSearchQA?
- Basis in paper: [explicit] The paper mentions that the maximum number of generated queries (K) is set to 2 and the maximum depth of extension (D) is set to 2 in the implementation section. However, it does not explore the impact of different values for these hyperparameters.
- Why unresolved: The paper does not provide an analysis of how varying the values of K and D would affect the performance of BeamSearchQA.
- What evidence would resolve it: Conducting experiments with different values of K and D to evaluate their impact on the performance of BeamSearchQA.

### Open Question 3
- Question: How does the scoring threshold (S) affect the trade-off between answer accuracy and the number of iterations in BeamSearchQA?
- Basis in paper: [explicit] The paper mentions that the score threshold (S) is set to 0.8 in the implementation section. However, it does not discuss the impact of different threshold values on the trade-off between answer accuracy and the number of iterations.
- Why unresolved: The paper does not provide an analysis of how varying the value of S would affect the performance of BeamSearchQA.
- What evidence would resolve it: Conducting experiments with different values of S to evaluate the trade-off between answer accuracy and the number of iterations in BeamSearchQA.

### Open Question 4
- Question: How does the beam size (B) affect the robustness and accuracy of BeamSearchQA?
- Basis in paper: [explicit] The paper mentions that the beam size (B) is set to 2 in the implementation section. However, it does not explore the impact of different beam sizes on the robustness and accuracy of BeamSearchQA.
- Why unresolved: The paper does not provide an analysis of how varying the value of B would affect the performance of BeamSearchQA.
- What evidence would resolve it: Conducting experiments with different values of B to evaluate the impact on the robustness and accuracy of BeamSearchQA.

### Open Question 5
- Question: How does the choice of the retriever affect the overall performance of BeamSearchQA?
- Basis in paper: [explicit] The paper mentions that the retriever is fine-tuned separately for the NQ and WebQ datasets using their respective training sets. However, it does not discuss the impact of different retriever choices on the overall performance of BeamSearchQA.
- Why unresolved: The paper does not provide an analysis of how different retriever choices would affect the performance of BeamSearchQA.
- What evidence would resolve it: Conducting experiments with different retriever choices to evaluate their impact on the overall performance of BeamSearchQA.

## Limitations
- The method relies heavily on the LLM's ability to generate meaningful follow-up queries and accurately assess its own confidence, which may not always hold true in practice.
- The iterative nature of the approach could lead to compounding errors if early-stage mistakes are not properly handled by the beam search mechanism.
- The specific parameter choices (K=2, D=2, N=2, S=0.8, B=2) are not thoroughly justified or explored through ablation studies.

## Confidence
*High Confidence Claims:*
- The general framework of iterative query generation with beam search is technically sound
- The use of scoring-based termination provides a principled stopping criterion

*Medium Confidence Claims:*
- The effectiveness of the proposed scoring function in accurately assessing answer confidence
- The beam search strategy's ability to improve robustness through dynamic pruning

*Low Confidence Claims:*
- The specific parameter choices (K=2, D=2, N=2, S=0.8, B=2) without ablation studies
- The generalizability of results across different question types and domains

## Next Checks
1. Conduct ablation studies to systematically vary each parameter (K, D, N, S, B) and identify their individual impact on performance.
2. Perform detailed error analysis of failed cases to understand when and why the LLM generates poor follow-up queries or provides unreliable confidence scores.
3. Evaluate performance on adversarial queries designed to test the limits of the query generation and scoring mechanisms, including questions requiring multi-hop reasoning or dealing with ambiguous information.