---
ver: rpa2
title: On the Definition of Appropriate Trust and the Tools that Come with it
arxiv_id: '2309.11937'
source_url: https://arxiv.org/abs/2309.11937
tags:
- trust
- user
- appropriate
- predictions
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating explanation methods
  in explainable AI (XAI) by proposing a novel approach that treats human users as
  black box models and evaluates their performance using established metrics for model
  performance. The core method idea is to define appropriate trust as the user's F1
  score, which combines user precision and recall, enabling a comprehensive analysis
  of user performance.
---

# On the Definition of Appropriate Trust and the Tools that Come with it

## Quick Facts
- arXiv ID: 2309.11937
- Source URL: https://arxiv.org/abs/2309.11937
- Reference count: 39
- Key outcome: Treats users as black box models and evaluates their performance using established metrics for model performance

## Executive Summary
This paper addresses the challenge of evaluating explanation methods in explainable AI (XAI) by proposing a novel approach that treats human users as black box models and evaluates their performance using established metrics for model performance. The core method idea is to define appropriate trust as the user's F1 score, which combines user precision and recall, enabling a comprehensive analysis of user performance. The paper suggests that misuse (overtrust) is indicated by low user precision, and disuse (undertrust) is indicated by low user recall. By using these metrics, the paper provides a clear and objective framework for evaluating explanation methods, allowing for comparative evaluations and the identification of significant alterations in the user's appropriate trust based on the introduction of explanations.

## Method Summary
The paper proposes treating human users as black box classifiers when evaluating their trust decisions on AI predictions. Users make binary trust (trust/mistrust) decisions about predictions, which are then compared against ground truth outcomes to construct a confusion matrix. From this confusion matrix, user precision (Upr = Tt/(Tt+Ft)), user recall (Urc = Tt/(Tt+Fm)), and appropriate user trust (Uat = 2*Upr*Urc/(Upr+Urc)) are calculated. Misuse is defined as low user precision (high false trust), disuse as low user recall (high false mistrust), and appropriate trust as the F1 score of user performance. This framework enables objective, comparative evaluation of explanation methods by measuring how well users calibrate their trust.

## Key Results
- User precision directly measures overtrust (misuse) by quantifying how often trusted predictions are actually correct
- User recall directly measures undertrust (disuse) by quantifying how many correct predictions were identified as trusted
- The F1 score provides a single objective metric for appropriate trust that penalizes extreme values and captures the precision-recall trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating users as black-box classifiers enables objective measurement of trust calibration using precision and recall metrics.
- Mechanism: The paper maps trust evaluation onto classification performance by treating user trust decisions as classification outputs, allowing reuse of established ML metrics (precision, recall, F1).
- Core assumption: User trust decisions can be meaningfully treated as binary classification outputs (trust vs. mistrust) for evaluation purposes.
- Evidence anchors:
  - [abstract] "treats human users as black box models and evaluates their performance using established metrics for model performance"
  - [section] "Consequently, when the user is treated as a classifier in an evaluation, identifying predictions as either correct or incorrect, the result is possible to analyse with a confusion matrix"
- Break condition: If user trust decisions cannot be reasonably binarized or if trust operates on different cognitive dimensions than simple classification.

### Mechanism 2
- Claim: User precision and recall directly map to overtrust and undertrust behaviors.
- Mechanism: User precision (T_t / (T_t + F_t)) measures overtrust by quantifying how often trusted predictions are actually correct, while user recall (T_t / (T_t + F_m)) measures undertrust by quantifying how many correct predictions were identified.
- Core assumption: Low user precision indicates overtrust (misuse) and low user recall indicates undertrust (disuse) are the primary dimensions of inappropriate trust.
- Evidence anchors:
  - [section] "Misuse could be defined as when the user have a high number of false trust in the predictions" and "Disuse is another aspect that should be avoided... when the user performance has a high number of false mistrust"
  - [section] "Looking at the confusion matrix in Fig. 3, misuse could also be defined as a low proportion of true trust T_t in the total number of trusted predictions, T_t + F_t"
- Break condition: If user trust behavior exhibits dimensions beyond overtrust and undertrust, or if the binary trust decision framework doesn't capture the full complexity of user trust dynamics.

### Mechanism 3
- Claim: The F1 score for user performance provides a single objective metric for appropriate trust that enables comparative evaluations.
- Mechanism: By defining appropriate trust as the user's F1 score (harmonic mean of precision and recall), the framework creates a single scalar value that penalizes extreme values and captures the trade-off between overtrust and undertrust.
- Core assumption: A single scalar metric can adequately capture the multi-dimensional nature of appropriate trust and enable meaningful comparisons across different explanation methods.
- Evidence anchors:
  - [abstract] "By using these metrics, the paper provides a clear and objective framework for evaluating explanation methods, allowing for comparative evaluations"
  - [section] "Going back to the definition of the F1 score, the metric only gives a high value to a predictor when both recall and precision are high"
- Break condition: If the F1 score fails to capture important aspects of trust calibration or if comparative evaluations require more nuanced multi-dimensional analysis.

## Foundational Learning

- Concept: Confusion matrix and classification metrics (precision, recall, F1)
  - Why needed here: The entire evaluation framework builds on treating user trust as classification performance, requiring understanding of these metrics
  - Quick check question: How does the F1 score differ from simply averaging precision and recall, and why is this difference important for trust evaluation?

- Concept: Conformal prediction and prediction intervals
  - Why needed here: The paper extends the evaluation framework to regression problems using conformal prediction concepts
  - Quick check question: How do prediction intervals in conformal regression enable binary classification of trust decisions in regression contexts?

- Concept: Trust calibration theory
  - Why needed here: The framework assumes trust calibration can be measured through user performance metrics
  - Quick check question: What distinguishes appropriate trust from simple trust, and why is calibration important in human-AI interaction?

## Architecture Onboarding

- Component map:
  - User evaluation interface (collects trust decisions)
  - Prediction system (generates predictions with explanations)
  - Evaluation engine (calculates precision, recall, F1)
  - Data collection module (stores ground truth vs user decisions)
  - Analysis dashboard (visualizes trust metrics)

- Critical path:
  1. User receives prediction with explanation
  2. User indicates trust/mistrust decision
  3. Ground truth outcome becomes available
  4. Evaluation engine calculates performance metrics
  5. Metrics aggregated and compared across methods

- Design tradeoffs:
  - Binary vs. continuous trust scales (binary enables classification metrics but may lose nuance)
  - Real-time vs. batch evaluation (real-time enables adaptive explanations but increases complexity)
  - Individual vs. group metrics (individual enables personalized adaptation but reduces statistical power)

- Failure signatures:
  - High precision but low recall indicates users are overly cautious and missing opportunities to trust correct predictions
  - High recall but low precision indicates users are overly trusting and accepting incorrect predictions
  - Consistently low F1 across methods suggests fundamental issues with explanation quality or user understanding

- First 3 experiments:
  1. Compare user precision and recall for different explanation methods on the same prediction task
  2. Test how user uncertainty intervals affect the regression evaluation framework
  3. Measure changes in appropriate trust before and after explanation exposure to establish baseline calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can user uncertainty be effectively incorporated into the evaluation of appropriate trust in classification tasks, similar to how it is handled in regression using Conformal Regression?
- Basis in paper: [explicit] The paper mentions that while incorporating user uncertainty into regression evaluations is discussed, it suggests looking into how uncertainty is incorporated into probability estimates of Venn-Abers for classification as future work.
- Why unresolved: The paper acknowledges the potential for incorporating user uncertainty into classification evaluations but does not provide a concrete method for doing so, leaving this as an open area for further research.
- What evidence would resolve it: A method that allows users to estimate their certainty in predicted classes and a way to measure how well-calibrated this user uncertainty is, possibly through an adaptation of Venn-Abers or a similar approach.

### Open Question 2
- Question: What are the trade-offs between precision and recall in the context of user performance, and how do these trade-offs affect the overall level of appropriate trust?
- Basis in paper: [explicit] The paper discusses that there is a trade-off between precision and recall in user performance, where high precision tends to result in low recall and vice versa, affecting the F1 score and thus the level of appropriate trust.
- Why unresolved: While the paper identifies the trade-off, it does not explore in depth how these trade-offs impact user trust in practical scenarios or how to optimize for both metrics simultaneously.
- What evidence would resolve it: Empirical studies that demonstrate the impact of precision-recall trade-offs on user trust in various real-world applications, along with strategies to balance these metrics effectively.

### Open Question 3
- Question: How can the evaluation of explanation methods be standardized across different domains and applications to ensure consistent and comparable results?
- Basis in paper: [inferred] The paper highlights the challenge of subjective measurements in evaluating explanation methods and suggests that using objective metrics like appropriate trust can facilitate comparative evaluations.
- Why unresolved: The paper proposes a method for evaluating appropriate trust but does not address how to standardize these evaluations across diverse domains, which may have different requirements and contexts.
- What evidence would resolve it: Development of a universal framework or set of guidelines that can be adapted to various domains, ensuring that the evaluation of explanation methods remains consistent and comparable across different applications.

## Limitations
- The framework assumes trust can be meaningfully binarized into trust vs. mistrust decisions, potentially oversimplifying complex trust behaviors
- The approach may not capture all dimensions of trust that involve complex cognitive processes, social factors, and situational context
- The regression extension using conformal prediction is mentioned but lacks complete implementation details and practical validation

## Confidence
- High confidence: The mathematical framework for calculating precision, recall, and F1 scores for user trust decisions is well-defined and reproducible
- Medium confidence: The mapping of overtrust to low precision and undertrust to low recall is logically sound, though may oversimplify complex trust behaviors
- Low confidence: The effectiveness of treating users as black-box models for trust evaluation across diverse contexts and explanation types

## Next Checks
1. Test binary assumption: Conduct a controlled experiment comparing binary trust decisions vs. continuous trust scales to measure information loss and evaluate whether binary classification captures essential trust dynamics
2. Cross-method comparison validation: Apply the framework to compare at least three different explanation methods across multiple tasks, measuring whether the F1-based ranking aligns with qualitative user feedback about appropriate trust
3. Regression framework implementation: Develop and test the conformal prediction-based regression extension with real user data to validate the practical feasibility of extending the framework beyond classification tasks