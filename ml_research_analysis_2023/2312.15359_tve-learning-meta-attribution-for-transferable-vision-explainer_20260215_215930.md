---
ver: rpa2
title: 'TVE: Learning Meta-attribution for Transferable Vision Explainer'
arxiv_id: '2312.15359'
source_url: https://arxiv.org/abs/2312.15359
tags:
- fidelity
- leta
- explanation
- attribution
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LETA (Learning Transferable Attribution),
  a method for generating explanations for vision models that can be transferred across
  different tasks without task-specific training. LETA pre-trains a generic explainer
  on large-scale image datasets to learn transferable attribution, which captures
  essential elements for explaining various downstream tasks.
---

# TVE: Learning Meta-attribution for Transferable Vision Explainer

## Quick Facts
- arXiv ID: 2312.15359
- Source URL: https://arxiv.org/abs/2312.15359
- Reference count: 40
- The paper introduces LETA (Learning Transferable Attribution), a method for generating explanations for vision models that can be transferred across different tasks without task-specific training.

## Executive Summary
LETA (Learning Transferable Attribution) presents a novel approach for generating explanations of vision models that can be transferred across different tasks without requiring task-specific training. The method pre-trains a generic explainer on large-scale image datasets to learn transferable attribution patterns, which can then be deployed to explain different vision models and tasks through rule-based adaptation. Theoretical analysis demonstrates that LETA minimizes explanation error bounds aligned with conditional V-information, while experiments show competitive fidelity and efficiency compared to state-of-the-art baseline methods across three diverse datasets and three architecture types.

## Method Summary
LETA pre-trains a generic explainer on large-scale image datasets to learn transferable attribution that captures essential features for explaining various downstream tasks. The pre-trained explainer maps input patches to backbone encoder outputs, creating a representation that preserves task-agnostic attribution patterns. For downstream tasks, LETA adapts the transferable attribution using a rule-based method that combines the generic attributions with task-specific classifiers, eliminating the need for additional training on task-specific data. The approach leverages Mask-AutoEncoder backbone, Feed-Forward layers, and a pre-trained backbone encoder to generate attribution tensors that can be applied across different vision architectures.

## Key Results
- LETA achieves competitive fidelity scores compared to state-of-the-art baseline methods (LIME, IG, RISE, DeepLift, KernelSHAP, GradShap, ViT-Shapley) across three datasets and three architecture types.
- The method demonstrates efficient explanation generation without requiring task-specific training or fine-tuning on downstream data.
- Pre-training on ImageNet provides a strong initialization that enables effective explanation of fully fine-tuned target models without additional fine-tuning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transferable attribution captures essential features from large-scale pre-training that generalize across tasks.
- Mechanism: The explainer learns to map input patches to backbone encoder outputs, creating a representation that preserves task-agnostic attribution patterns.
- Core assumption: The backbone encoder's output space contains sufficient task-agnostic information to explain diverse downstream tasks.
- Evidence anchors:
  - [abstract] "The transferable attribution takes advantage of the versatile output of the target backbone encoders to comprehensively encode the attribution knowledge for the input instance"
  - [section 3.2] "Definition 1 is derived using the backbone encoder G, facilitating its application across different tasks"
  - [corpus] Weak - related papers focus on contrastive learning and adversarial attacks rather than attribution transfer
- Break condition: If downstream tasks require features not present in the pre-training distribution, transferability fails.

### Mechanism 2
- Claim: Rule-based adaptation enables task-specific explanations without fine-tuning on downstream data.
- Mechanism: The pre-trained explainer generates patch-to-embedding attributions, which are then combined with task-specific classifiers to produce explanations aligned with each task's objectives.
- Core assumption: Task-specific classifiers can effectively combine with generic attributions to produce meaningful explanations.
- Evidence anchors:
  - [abstract] "introduces a rule-based adaptation of the transferable attribution for explaining downstream tasks, without the need for additional training on task-specific data"
  - [section 3.3] "Definition 2...the explanation of ft(xk) is generated by ϕk,y,z = log Ht(gk,z; y) − log Ht(hk,z; y)"
  - [section 5.2] "LETA consistently exhibits promising performance in terms of both Fidelity+(↑) and Fidelity−(↓)"
- Break condition: If task-specific classifiers are poorly aligned with the explanation objectives, rule-based adaptation produces misleading results.

### Mechanism 3
- Claim: Pre-training on large-scale data provides better initialization than learning from scratch for downstream adaptation.
- Mechanism: The pre-training process minimizes explanation error bound aligned with conditional V-information, creating a strong starting point for downstream tasks.
- Core assumption: The ImageNet pre-training distribution overlaps sufficiently with downstream task distributions.
- Evidence anchors:
  - [abstract] "Theoretical analysis shows that LETA minimizes the explanation error bound aligned with the conditional V-information on downstream tasks"
  - [section 4.3] "Theorem 1...the pre-training of LETA contributes to minimizing the explanation error bound compared with the conditional V−information on downstream tasks"
  - [section 5.3] "LETA-PT can effectively explain the fully fine-tuned target model, even without fine-tuning"
- Break condition: If pre-training and downstream distributions have minimal overlap, the initialization advantage disappears.

## Foundational Learning

- Concept: V-information theory for explanation measurement
  - Why needed here: Provides theoretical foundation for attribution quality and enables error bound analysis
  - Quick check question: How does conditional V-information differ from mutual information in the context of feature attribution?

- Concept: Transfer learning principles for vision models
  - Why needed here: Underlies the assumption that backbone encoder features can generalize across tasks
  - Quick check question: What distinguishes feature transfer from task transfer in vision model architectures?

- Concept: Shapley value approximation methods
  - Why needed here: Many baseline methods use Shapley values, providing comparison context for LETA's approach
  - Quick check question: Why does LETA avoid explicit Shapley value computation while maintaining similar theoretical grounding?

## Architecture Onboarding

- Component map:
  - Mask-AutoEncoder backbone → Feature extractor
  - Feed-Forward layers (n=17) → Attribution mapper
  - Pre-trained backbone encoder → Generic attribution source
  - Task-specific classifier → Rule-based adapter

- Critical path:
  1. Pre-training: Input → Mask-AutoEncoder → FFN layers → Transferable attribution tensors
  2. Adaptation: Input → Pre-trained explainer → Task classifier → Final explanation heatmap

- Design tradeoffs:
  - Fixed vs. fine-tuned backbone: Classifier-tuning prevents overfitting but may limit adaptation
  - Patch size selection: 16×16 pixels balances local detail with computational efficiency
  - FFN layer count: 17 layers found empirically optimal for generalization

- Failure signatures:
  - Low fidelity scores indicate poor attribution-task alignment
  - High variance across tasks suggests insufficient pre-training coverage
  - Degradation on fine-tuned models reveals distribution shift issues

- First 3 experiments:
  1. Compare LETA vs. LETA-w/o-PT on single downstream task to verify pre-training benefit
  2. Test transferability across different backbone architectures using same pre-trained explainer
  3. Measure latency vs. fidelity tradeoff for different patch sizes and FFN depths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LETA perform on tasks outside the scope of its pre-training data distribution?
- Basis in paper: [inferred] The paper states LETA is pre-trained on ImageNet and deployed on downstream tasks "within the scope of pre-training data distribution". It does not test or discuss performance on out-of-distribution tasks.
- Why unresolved: The paper only evaluates LETA on datasets similar to ImageNet (Cats-vs-dogs, CIFAR-100, Imagenette). No experiments test generalization to significantly different data.
- What evidence would resolve it: Testing LETA on datasets with different data distributions, image styles, or tasks than ImageNet, and comparing performance to baseline methods.

### Open Question 2
- Question: What is the impact of the number of neighbors (hops) considered in the transferable attribution on explanation quality?
- Basis in paper: [explicit] The paper mentions considering 0-, 1-, and 2-hop neighbors for patches, but does not analyze the effect of varying this number on the quality of explanations.
- Why unresolved: The experiments use a fixed number of neighbor hops, and no ablation study is conducted to determine the optimal number of hops or its impact on fidelity.
- What evidence would resolve it: Conducting experiments with different numbers of neighbor hops (e.g., 0, 1, 2, 3) and comparing the resulting fidelity scores to determine the optimal setting.

### Open Question 3
- Question: How does the choice of backbone encoder (e.g., ViT, Swin, DeiT) affect the quality of transferable attribution and downstream explanations?
- Basis in paper: [inferred] The paper uses different backbone encoders (ViT, Swin, DeiT) for pre-training and evaluation, but does not analyze the impact of the backbone choice on the quality of transferable attribution or downstream explanations.
- Why unresolved: The experiments use a fixed backbone encoder for pre-training and evaluation, and no comparison is made between different backbones.
- What evidence would resolve it: Conducting experiments with different backbone encoders (e.g., ViT, Swin, DeiT) for pre-training and evaluating the resulting fidelity scores to determine the impact of the backbone choice.

## Limitations
- The method assumes sufficient overlap between pre-training and downstream task distributions, which may not hold for significantly different data distributions.
- The rule-based adaptation mechanism may not generalize well to tasks with objectives that diverge substantially from the pre-training distribution.
- The theoretical analysis relies on conditional V-information bounds that may not fully capture the complexity of real-world explanation scenarios.

## Confidence
- **High**: The mechanism of pre-training on large-scale data to learn transferable attribution patterns is well-established in transfer learning literature.
- **Medium**: The rule-based adaptation approach for task-specific explanations without fine-tuning is novel but requires more extensive validation.
- **Low**: The theoretical guarantees about explanation error bounds minimizing conditional V-information are sound but may not hold across diverse real-world scenarios.

## Next Checks
1. **Distribution Shift Analysis**: Test LETA on tasks with minimal overlap to pre-training distribution (e.g., medical imaging, satellite imagery) to measure the limits of transferability.
2. **Ablation on Backbone Architecture**: Evaluate whether the transferable attribution generalizes across different backbone architectures beyond the three tested (ViT, Swin, DeiT) to validate the architecture-agnostic claims.
3. **Long-tail Task Performance**: Assess LETA's fidelity on long-tail and imbalanced datasets to determine if the pre-training advantage persists when class distributions differ significantly from ImageNet.