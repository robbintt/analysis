---
ver: rpa2
title: Rethinking Privacy in Machine Learning Pipelines from an Information Flow Control
  Perspective
arxiv_id: '2311.15792'
source_url: https://arxiv.org/abs/2311.15792
tags:
- data
- privacy
- access
- information
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of preserving user privacy
  in machine learning pipelines by proposing an information flow control approach.
  The authors compare four methods for knowledge sharing under access control constraints:
  zero-shot baseline, differentially-private fine-tuning, personalized per-user models,
  and retrieval augmented models.'
---

# Rethinking Privacy in Machine Learning Pipelines from an Information Flow Control Perspective

## Quick Facts
- arXiv ID: 2311.15792
- Source URL: https://arxiv.org/abs/2311.15792
- Authors: 
- Reference count: 27
- Primary result: Retrieval augmented models achieve best utility while satisfying strict non-interference guarantees in privacy-preserving ML pipelines

## Executive Summary
This paper addresses the challenge of preserving user privacy in machine learning pipelines by proposing an information flow control approach. The authors compare four methods for knowledge sharing under access control constraints: zero-shot baseline, differentially-private fine-tuning, personalized per-user models, and retrieval augmented models. They evaluate these approaches on two datasets of scientific articles and demonstrate that retrieval augmented architectures deliver the best utility, scalability, and flexibility while satisfying strict non-interference guarantees.

## Method Summary
The paper evaluates four approaches to privacy-preserving ML: zero-shot baseline using GPT-3, differentially-private fine-tuning with LoRA, personalized per-user models, and retrieval augmented models. The evaluation uses two scientific article datasets (Elsevier and Arxiv) with access control based on authorship. Models are tested on perplexity scores for predicting abstracts from article bodies under different access conditions. Retrieval augmented models use sentence-bert for document retrieval with top-K selection and prompt augmentation strategies.

## Key Results
- Retrieval augmented models show clear improvements in perplexity scores compared to baselines
- Most significant gains observed when accessing the current article or articles accessible to all authors
- Retrieval augmented models provide the best balance of utility, scalability, and flexibility while maintaining privacy guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmented models enforce non-interference by design during inference
- Mechanism: At inference time, the model retrieves only documents accessible to the querying user group and conditions generation on those documents. Since the model never sees documents outside the user's access set, the output cannot leak information from unauthorized sources.
- Core assumption: The retrieval system correctly enforces the access control policy when selecting documents
- Evidence anchors:
  - [abstract] "retrieval augmented models that access user-specific datasets at inference time"
  - [section 2.2] "A function satisfying non-interference thus ensures that these inaccessible data items do not affect the output"
  - [corpus] Weak - no direct corpus evidence found
- Break condition: If the retrieval system fails to enforce access control or if the base model can memorize and reproduce unauthorized information despite retrieval

### Mechanism 2
- Claim: Personalized per-user models satisfy non-interference by restricting training data to user's accessible documents
- Mechanism: Each user group has a dedicated model trained only on documents they can access. Since the model never sees unauthorized data during training, it cannot leak that information in outputs.
- Core assumption: The user's model only sees their authorized documents during training and no other data
- Evidence anchors:
  - [section 3.2] "Concretely, each set of users U has their own fine-tuned model g_θ(D_U) that is trained on their data D_U"
  - [section 2.2] "The set of users U (collectively) should not have access to any data items in D outside D_U"
  - [corpus] Weak - no direct corpus evidence found
- Break condition: If training data leaks between user groups or if the model memorizes and reproduces unauthorized information

### Mechanism 3
- Claim: Source attribution in retrieval augmented models provides interpretability and security benefits
- Mechanism: The model can identify which specific documents contributed to the output, allowing users to verify that only authorized sources were used and making informed decisions about sharing results
- Core assumption: The retrieval system can accurately attribute sources to the output
- Evidence anchors:
  - [section 2.3] "source attribution...allows users to understand which data items were used by an ML model when processing a query"
  - [section 4.2] "Retrieval augmentation shows a pronounced improvement when the models are conditioned on a smaller fraction of the abstract"
  - [corpus] Weak - no direct corpus evidence found
- Break condition: If source attribution becomes inaccurate or if users cannot effectively use source information for decision-making

## Foundational Learning

- Concept: Information Flow Control (IFC)
  - Why needed here: Provides the theoretical foundation for defining and proving privacy guarantees in ML systems
  - Quick check question: What does non-interference mean in the context of IFC, and how does it relate to privacy?

- Concept: Differential Privacy (DP)
  - Why needed here: Serves as a comparison point and baseline for privacy guarantees in ML systems
  - Quick check question: How does user-level DP differ from document-level DP in terms of privacy guarantees?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: The core architecture used to implement privacy-preserving ML systems in this paper
  - Quick check question: What are the key differences between prompting, decoding, and cross-attention methods for RAG?

## Architecture Onboarding

- Component map: Access control policy enforcement -> Document retrieval system with policy filtering -> Base language model (GPT-3 in this case) -> Prompt augmentation system -> Output generation and source attribution

- Critical path: Query → Policy check → Document retrieval → Prompt augmentation → Model inference → Output with source attribution

- Design tradeoffs:
  - Utility vs. privacy: More restrictive policies may reduce model performance
  - Latency vs. comprehensiveness: Retrieving more documents increases latency but may improve results
  - Storage vs. flexibility: Maintaining per-user models provides flexibility but requires more storage

- Failure signatures:
  - Privacy violations: Unauthorized documents appearing in retrieved results
  - Performance degradation: Poor retrieval quality leading to worse model outputs
  - Scalability issues: Inability to handle growing numbers of users and documents

- First 3 experiments:
  1. Verify non-interference by testing that changing unauthorized documents doesn't affect outputs for a user group
  2. Measure retrieval accuracy by checking if the system retrieves relevant documents within the user's access set
  3. Test source attribution by verifying that the model can correctly identify which documents contributed to each output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise conditions under which differential privacy guarantees break down in collaborative datasets?
- Basis in paper: [explicit] The paper notes that Brown et al. [2022] identified that DP can severely degrade privacy guarantees when large unknown groups communicate and collaborate, but does not quantify this degradation.
- Why unresolved: The paper treats DP as a baseline comparison without deeply exploring the exact thresholds of group privacy loss or identifying specific collaboration patterns that are most problematic.
- What evidence would resolve it: Empirical studies measuring privacy budget exhaustion rates across different collaboration graph structures and user group sizes.

### Open Question 2
- Question: How does retrieval augmentation performance scale with increasing dataset size and complexity of access control policies?
- Basis in paper: [explicit] The paper demonstrates retrieval augmentation works well on two scientific datasets but notes scalability concerns are important and suggests "Developing a system that can take into account access control restrictions would improve inference efficiency."
- Why unresolved: The experiments use relatively small, structured datasets. The paper acknowledges scalability challenges but doesn't provide concrete performance data on how retrieval efficiency degrades with policy complexity.
- What evidence would resolve it: Systematic evaluation of retrieval performance across datasets of varying sizes (10x, 100x scale) and access control complexity (number of overlapping user groups).

### Open Question 3
- Question: What are the security implications when security labels contain errors or are intentionally poisoned?
- Basis in paper: [explicit] The discussion section notes that "the security labels are a reliable source of truth" is an assumption that may not hold in practice, and raises concerns about "unintentional mislabels or intentional data poisoning."
- Why unresolved: The paper identifies this as a potential threat but doesn't propose or evaluate specific defenses against label poisoning attacks or quantify the impact of label errors on model behavior.
- What evidence would resolve it: Experiments demonstrating model behavior under various poisoning scenarios and evaluation of different label validation or detection mechanisms.

## Limitations

- No direct empirical validation of non-interference properties under adversarial conditions
- Evaluation assumes fixed user group sizes and document collections without addressing scaling challenges
- Differentially-private fine-tuning baseline may not use optimized hyperparameters for fair comparison

## Confidence

**High Confidence**: The retrieval augmented approach provides superior perplexity scores compared to baselines when measured under the experimental conditions described.

**Medium Confidence**: The claim that retrieval augmented models satisfy non-interference by design during inference, though this requires empirical validation under adversarial conditions.

**Low Confidence**: The assertion that source attribution provides meaningful security benefits in practice, as the paper provides no user studies demonstrating practical value.

## Next Checks

1. **Non-interference Verification Test**: Implement an adversarial test where unauthorized documents are systematically modified and verify that retrieval augmented model outputs for user groups remain unaffected by these changes.

2. **Source Attribution Utility Assessment**: Conduct a user study where participants attempt to verify access control compliance using source attribution information, measuring both accuracy and time-to-decision.

3. **Large-scale Scaling Experiment**: Deploy the retrieval augmented system with 1,000+ simulated users and document collections of 100,000+ articles, measuring query latency, retrieval accuracy degradation, and system resource utilization.