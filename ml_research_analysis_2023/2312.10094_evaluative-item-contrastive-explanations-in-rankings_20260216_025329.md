---
ver: rpa2
title: Evaluative Item-Contrastive Explanations in Rankings
arxiv_id: '2312.10094'
source_url: https://arxiv.org/abs/2312.10094
tags:
- ranking
- explanations
- explanation
- contrastive
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes contrastive explanations for ranking problems
  within an evaluative AI framework. The proposed Evaluative Item-Contrastive Explanations
  method highlights both positive and negative attributes influencing a ranking to
  support human decision-making and reduce position bias.
---

# Evaluative Item-Contrastive Explanations in Rankings

## Quick Facts
- arXiv ID: 2312.10094
- Source URL: https://arxiv.org/abs/2312.10094
- Reference count: 21
- Primary result: Formalizes evaluative item-contrastive explanations for ranking problems to reduce position bias and support human decision-making

## Executive Summary
This paper introduces Evaluative Item-Contrastive Explanations (EICE) as a method for explaining rankings by highlighting both positive and negative attributes influencing the relative positions of items. The approach aims to counteract position bias by presenting comprehensive attribute comparisons rather than relying solely on ranking order. Demonstrated using logistic regression on a campus recruitment dataset, the method generates visual and textual explanations comparing pairs of candidates based on their distinguishing features.

## Method Summary
The method formalizes evaluative item-contrastive explanations using logistic regression coefficients to identify pros and cons for each item in a ranked pair. For linear models, positive coefficients indicate features favoring an item while negative coefficients indicate features against it. The approach generates explanations by computing feature differences between items, determining which attributes support each item, and presenting this information through visual and textual formats. The methodology can be generalized to black-box models through additional explanation techniques.

## Key Results
- Proposes a formal framework for evaluative item-contrastive explanations in ranking systems
- Demonstrates initial implementation using logistic regression on campus recruitment data
- Shows how to identify and present both positive and negative attributes for ranked items
- Positions the approach as a way to reduce position bias and support informed human decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluative item-contrastive explanations reduce position bias by presenting both positive and negative attributes for each compared item, not just those favoring the higher-ranked one.
- Mechanism: By explicitly showing advantages and disadvantages for both items in a pair, the explanation disrupts the cognitive shortcut where higher position is automatically perceived as better. The user must actively weigh the presented attributes rather than passively accept the ranking order.
- Core assumption: Users will engage with the full attribute comparison rather than defaulting to position-based judgments.
- Evidence anchors:
  - [abstract] "This approach is especially potent when combined with an Evaluative AI methodology, which conscientiously evaluates both positive and negative aspects influencing a potential ranking."
  - [section 2.5] "it has the potential to effectively reduce biases that affect decisions based on rankings. In particular, it could counteract the negative influence on the cognitive decision-making process of the position bias"

### Mechanism 2
- Claim: Contrastive explanations aligned with human cognitive processes improve decision quality by matching how people naturally reason about alternatives.
- Mechanism: People naturally compare alternatives by asking "why this rather than that?" The evaluative contrastive format mirrors this natural reasoning pattern, making the explanation more cognitively accessible and actionable.
- Core assumption: Human decision-making follows contrastive reasoning patterns as described in the literature on explanations.
- Evidence anchors:
  - [section 2.3] "contrastive explanations facilitate human comprehension by shedding light on the rationale behind choosing one outcome over another. This form of explanation is widely recognized as both effective and easily understandable."
  - [section 2.2] "Explanations are more effective when they are set in the landscape of the recipient's existing beliefs and values."

### Mechanism 3
- Claim: The logistic regression implementation provides interpretable feature contributions that can be directly used to construct contrastive explanations.
- Mechanism: Linear models produce weights that directly indicate how each feature contributes to the score difference between items. These weights can be used to identify which features favor each item and their relative importance.
- Core assumption: The logistic regression model adequately captures the relationship between features and ranking outcomes for the domain.
- Evidence anchors:
  - [section 3.3] "The linear nature of Logistic Regression streamlines the identification of pros and cons for each item. A positive coefficient Î±d in Logistic Regression, assigned to a specific feature, implies that higher values of that feature correspond to higher assigned scores."
  - [section 4] "As discussed in subsection 3.3, we trained a Logistic Regression to predict the placement of a student given the values of the other features."

## Foundational Learning

- Concept: Difference between ranking and recommendation systems
  - Why needed here: The paper explicitly distinguishes these, noting that ranking involves access to all items' positions while recommendation focuses on top-k items. Understanding this distinction is crucial for applying the contrastive explanation approach appropriately.
  - Quick check question: In a ranking problem, does the user have access to the positions of all items or only the top-k recommendations?

- Concept: Position bias in ranking systems
  - Why needed here: The paper identifies position bias as a key problem that evaluative contrastive explanations aim to address. Understanding what position bias is and why it occurs is essential for grasping the motivation behind the approach.
  - Quick check question: What cognitive tendency causes users to overvalue items that appear in higher positions of a ranking?

- Concept: Contrastive vs counterfactual explanations
  - Why needed here: The paper explicitly distinguishes these two types of explanations and positions its approach as contrastive rather than counterfactual. Understanding the difference is important for correctly categorizing the contribution.
  - Quick check question: Does a counterfactual explanation tell you why something happened, or what would need to change to get a different outcome?

## Architecture Onboarding

- Component map: Ranking model -> Feature importance extractor -> Contrastive explanation generator -> Presentation layer -> User interface
- Critical path:
  1. Get ranked items from model
  2. Select item pair for comparison
  3. Extract feature contributions for each item
  4. Identify positive and negative attributes for both items
  5. Order attributes by importance
  6. Generate visual and textual explanations
  7. Present to user

- Design tradeoffs:
  - Linear vs non-linear models: Linear models provide direct interpretability but may miss complex patterns; non-linear models require additional explanation methods
  - Amount of information: More features provide comprehensive explanation but risk overwhelming users; fewer features are more digestible but may miss important distinctions
  - Static vs dynamic selection: Predefined feature selection is simpler but may miss context-specific important features; dynamic selection adapts to the specific pair but adds complexity

- Failure signatures:
  - Users consistently ignoring evaluative content and relying on position
  - Explanation generation fails for certain item pairs (e.g., identical features)
  - Model weights are unstable or change significantly with small data changes
  - Visual representations are unclear or misleading

- First 3 experiments:
  1. Compare user decisions with and without evaluative contrastive explanations on a simple ranking task to measure impact on position bias
  2. Test different levels of information density (number of features shown) to find optimal balance between completeness and usability
  3. Evaluate explanation effectiveness across different user expertise levels to ensure accessibility to both domain experts and novices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the evaluative item-contrastive explanation approach affect user decision-making outcomes compared to traditional contrastive explanations in ranking systems?
- Basis in paper: [explicit] The paper states that the evaluative approach aims to reduce position bias and support human decision-making by presenting both pros and cons for each item in a pair.
- Why unresolved: The paper demonstrates the approach with a simple linear model on a campus recruitment dataset, but does not empirically compare its effectiveness against traditional contrastive explanations or measure its impact on user decision outcomes.
- What evidence would resolve it: Controlled experiments comparing user decisions and satisfaction between evaluative item-contrastive explanations and traditional contrastive explanations across different ranking domains and model types.

### Open Question 2
- Question: Can the evaluative item-contrastive explanation framework be effectively generalized to complex black-box models beyond linear regression?
- Basis in paper: [explicit] The authors state that while they demonstrate the approach using logistic regression, they believe the methodology can be generalized to black-box models and plan to explore this in future work.
- Why unresolved: The paper only implements the framework with a simple interpretable model (logistic regression) and does not address the challenges of applying it to complex models like neural networks or ensemble methods.
- What evidence would resolve it: Successful implementation and validation of the evaluative item-contrastive explanation approach with various black-box models across multiple ranking domains, demonstrating consistent interpretability and effectiveness.

### Open Question 3
- Question: What is the optimal balance between providing comprehensive explanations and avoiding information overload in evaluative item-contrastive explanations?
- Basis in paper: [explicit] The authors acknowledge the need to balance between offering concise explanations and providing sufficient information for informed decision-making, suggesting configurable methods for feature selection.
- Why unresolved: The paper does not empirically determine the optimal number of features to present or the most effective methods for selecting which information to include, leaving this as a context-dependent decision.
- What evidence would resolve it: User studies measuring comprehension, decision quality, and cognitive load with varying levels of explanation detail, identifying the optimal information presentation strategy for different user types and contexts.

## Limitations
- The approach is only demonstrated with logistic regression, limiting validation of effectiveness with complex models
- No empirical evidence shows the method actually reduces position bias or improves decision quality
- The framework doesn't address how to handle cases where items have identical or very similar features

## Confidence
- **High confidence**: The mathematical formulation of the evaluative item-contrastive explanation method and its implementation using logistic regression coefficients is well-specified and internally consistent.
- **Medium confidence**: The theoretical argument that contrastive explanations aligned with human cognitive processes should improve decision quality, based on established literature about contrastive reasoning.
- **Medium confidence**: The claim that explicitly showing both positive and negative attributes for ranked items can reduce position bias, though this requires empirical validation.

## Next Checks
1. **Controlled user study**: Design an experiment comparing user decisions with and without evaluative contrastive explanations on ranking tasks, measuring both decision accuracy and susceptibility to position bias.
2. **Cross-model validation**: Test the explanation method with non-linear ranking models (e.g., gradient boosting, neural networks) to assess whether the linear model assumption is critical to the approach's effectiveness.
3. **Information density optimization**: Systematically vary the number and type of features shown in explanations to identify the optimal balance between comprehensiveness and cognitive load, ensuring the method remains usable across different user expertise levels.