---
ver: rpa2
title: Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion
arxiv_id: '2311.09602'
source_url: https://arxiv.org/abs/2311.09602
tags:
- emotion
- triggers
- emotions
- words
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between emotion triggers
  (events that evoke emotions) and the predictions made by emotion detection models.
  The authors introduce a new dataset, EMOTRIGGER, consisting of 900 social media
  posts annotated with emotion triggers.
---

# Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion

## Quick Facts
- arXiv ID: 2311.09602
- Source URL: https://arxiv.org/abs/2311.09602
- Reference count: 14
- One-line primary result: Language models can predict emotions accurately but struggle to identify the specific triggers that cause those emotions.

## Executive Summary
This paper investigates whether emotion detection models consider emotion triggers (events that evoke emotions) when making predictions. The authors introduce EMOTRIGGER, a new dataset of 900 social media posts annotated with emotion triggers, and evaluate both large language models (LLMs) and fine-tuned transformer models on their ability to identify these triggers. The results show that while LLMs achieve high accuracy in emotion detection, their performance in identifying triggers is mixed, and emotion triggers are not considered salient features for emotion prediction models. Instead, models rely on emotion-related keywords and keyphrases rather than event-based triggers.

## Method Summary
The study uses the EMOTRIGGER dataset containing 900 social media posts from CancerEmo, HurricaneEmo, and GoEmotions datasets, annotated with emotion triggers by experts. Researchers evaluated LLMs (GPT-4, Llama2-Chat-13B, Alpaca-13B) and fine-tuned transformer models (EmoBERTa, BERT, DistilBERT, RoBERTa, DeBERTa) on two tasks: predicting emotions and identifying triggers. They compared salient features using SHAP values for fine-tuned models and prompts for LLMs, then analyzed correlations between extracted features and annotated triggers using performance metrics like F1, exact match, and partial match.

## Key Results
- LLMs achieve high emotion detection accuracy but show mixed performance in identifying triggers
- Emotion triggers are not considered salient features for emotion prediction models
- Correlation between SHAP-attributed feature importance and emotion triggers is low across models
- Models rely more on emotion-related keywords and keyphrases than event-based triggers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Emotion detection models primarily rely on emotion-related keywords and keyphrases rather than event-based triggers to predict emotions.
- **Mechanism:** The models learn to associate specific lexical items (emotion words, keyphrases) with emotion labels during training, bypassing the need to understand the causal event behind the emotion.
- **Core assumption:** Lexical features contain sufficient information to predict emotions without requiring comprehension of the situational context or triggers.
- **Evidence anchors:**
  - [abstract] "Our analysis reveals that emotion triggers are largely not considered salient features for emotion prediction models"
  - [section 4] "Surprisingly, the correlations with keyphrases are much higher than with EmoLex for all models and across datasets"
- **Break condition:** If the emotion is expressed through subtle linguistic cues, narrative context, or requires reasoning about events rather than keywords, the model's performance will degrade significantly.

### Mechanism 2
- **Claim:** Large language models (LLMs) can predict emotions with high accuracy but struggle to identify the specific triggers that cause those emotions.
- **Mechanism:** LLMs leverage their pretraining on vast text corpora to recognize patterns of emotion expression, but they haven't been specifically trained to map emotions back to their causal events.
- **Core assumption:** Emotion expression patterns are more prevalent in training data than explicit trigger-event-emotion relationships.
- **Evidence anchors:**
  - [abstract] "LLMs can identify emotions with high accuracy, but their performance in identifying triggers is mixed"
  - [section 4] "LLM performance is consistent across emotions and datasets while EmoBERTa's performance drops significantly"
- **Break condition:** When the trigger is implicit, requires multi-step reasoning, or depends on world knowledge not well-represented in the training corpus.

### Mechanism 3
- **Claim:** The correlation between SHAP-attributed feature importance and emotion triggers is low because models use different linguistic features than humans use to identify triggers.
- **Mechanism:** Humans identify triggers based on events and causal relationships, while models identify features based on statistical co-occurrence with emotion labels during training.
- **Core assumption:** The features that are statistically predictive of an emotion label are not necessarily the same features humans would identify as the cause of that emotion.
- **Evidence anchors:**
  - [abstract] "instead there is intricate interplay between various features and the task of emotion detection"
  - [section 4] "With the exception of GPT-4, word features deemed salient for emotion prediction are only marginally related to these triggers"
- **Break condition:** When the dataset contains explicit trigger-emotion pairs that are consistently labeled, the model might learn to map features to triggers more effectively.

## Foundational Learning

- **Concept:** Inter-annotator agreement and its interpretation
  - Why needed here: Understanding the high Fleiss Kappa score (0.91) is crucial for trusting the quality of the emotion trigger annotations
  - Quick check question: If three annotators achieve a Fleiss Kappa of 0.91 on a task, what does this indicate about their agreement level according to Artstein and Poesio's interpretation?

- **Concept:** Feature importance attribution methods (SHAP)
  - Why needed here: SHAP values are used to identify which words the emotion prediction models consider most important for their predictions
  - Quick check question: In SHAP values, what does a positive value indicate about a feature's relationship to the model's prediction?

- **Concept:** Zero-shot vs few-shot prompting performance
  - Why needed here: The paper notes that zero-shot performance yielded very low alignment with triggers, highlighting the importance of few-shot examples
  - Quick check question: Why might few-shot prompting significantly improve an LLM's ability to identify emotion triggers compared to zero-shot prompting?

## Architecture Onboarding

- **Component map:**
  - Data preparation: EMOTRIGGER dataset construction and annotation
  - Model evaluation: Emotion prediction and trigger identification across multiple model types (LLMs, fine-tuned transformers)
  - Analysis: SHAP-based feature importance analysis, keyphrase extraction, EmoLex comparison
  - Results aggregation: Performance metrics calculation and correlation analysis

- **Critical path:**
  1. Load and preprocess EMOTRIGGER dataset
  2. Run emotion prediction models on the dataset
  3. Extract salient features using SHAP for fine-tuned models or prompts for LLMs
  4. Compare extracted features against annotated triggers
  5. Calculate performance metrics (F1, exact match, partial match)
  6. Analyze correlations between different feature sets

- **Design tradeoffs:**
  - Using extractive triggers vs. abstract trigger descriptions: extractive is easier to annotate but may miss nuanced triggers
  - SHAP vs. other feature importance methods: SHAP is model-agnostic but computationally expensive
  - Including hallucinations in LLM evaluation: excluding them provides cleaner results but may underestimate real-world performance

- **Failure signatures:**
  - Low correlation between SHAP values and triggers across all models indicates the models aren't learning trigger-emotion relationships
  - High emotion detection accuracy with low trigger identification accuracy suggests the models rely on surface patterns rather than causal understanding
  - Inconsistent trigger identification across similar sentences suggests the model isn't capturing the underlying event structure

- **First 3 experiments:**
  1. Run the same analysis pipeline on a dataset with longer texts to see if trigger identification improves with more context
  2. Fine-tune a transformer model specifically to predict both emotions and their triggers simultaneously and compare performance
  3. Create an adversarial test set where emotion words are removed but the trigger remains, to test if models can still identify the emotion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models rely on emotion triggers or other features for emotion detection?
- Basis in paper: [explicit] The paper explicitly states that emotion triggers are not considered salient features for emotion prediction models, and instead, there is a complex interplay between various features and the task of emotion detection.
- Why unresolved: The paper provides evidence that language models do not primarily rely on emotion triggers for emotion detection, but it does not fully explore the specific features or mechanisms that these models use for emotion detection.
- What evidence would resolve it: Further research could investigate the specific features or mechanisms that language models use for emotion detection, and compare their performance with models that explicitly consider emotion triggers.

### Open Question 2
- Question: Can language models be improved to better identify and utilize emotion triggers for emotion detection?
- Basis in paper: [explicit] The paper concludes that current state-of-the-art language models cannot proficiently construe emotional reactions with events that trigger them, indicating the need for further research in this area.
- Why unresolved: The paper demonstrates that language models struggle with identifying emotion triggers, but it does not explore potential methods or techniques to improve their performance in this area.
- What evidence would resolve it: Future work could focus on developing and evaluating new methods or techniques to improve language models' ability to identify and utilize emotion triggers for emotion detection.

### Open Question 3
- Question: How do language models perform on emotion detection tasks in different domains or with longer text?
- Basis in paper: [inferred] The paper focuses on short social media texts, and it is unclear how language models would perform on longer texts or in different domains.
- Why unresolved: The paper does not explore the performance of language models on emotion detection tasks beyond short social media texts, leaving the question of their generalizability unanswered.
- What evidence would resolve it: Further research could investigate the performance of language models on emotion detection tasks in different domains or with longer text, to assess their generalizability and robustness.

## Limitations

- The analysis relies on extractive trigger identification, which may underestimate models' understanding of implicit or abstract triggers
- Evaluation metrics focus on lexical overlap rather than semantic equivalence, potentially missing true causal understanding
- The comparison conflates emotion prediction capability with trigger identification capability, which may require different mechanisms

## Confidence

**High confidence:** The finding that emotion detection models achieve high accuracy while showing low correlation between their salient features and annotated triggers is well-supported by the experimental results across multiple model types and datasets.

**Medium confidence:** The conclusion that current models cannot "proficiently construe emotional reactions with events that trigger them" is supported by the data but may overstate the case, as the evaluation method focuses on extractive trigger identification rather than broader causal understanding.

**Low confidence:** The claim that this limitation indicates a fundamental gap in current language models' emotional reasoning is speculative, as the paper doesn't explore alternative explanations or training approaches that might address this gap.

## Next Checks

1. **Adversarial testing:** Create test cases where emotion words are removed but the trigger remains explicit, then evaluate whether models can still predict the correct emotion based on the trigger alone.

2. **Cross-lingual validation:** Test whether models trained on one language can identify triggers in another language when provided with translation, to determine if trigger identification relies on language-specific patterns or deeper causal understanding.

3. **Multi-task training experiment:** Fine-tune a model simultaneously to predict both emotions and their triggers, then compare feature importance between the two tasks to see if joint training creates stronger trigger-emotion associations.