---
ver: rpa2
title: 'Lost In Translation: Generating Adversarial Examples Robust to Round-Trip
  Translation'
arxiv_id: '2307.12520'
source_url: https://arxiv.org/abs/2307.12520
tags:
- adversarial
- word
- translation
- examples
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that current text adversarial attacks lose
  significant effectiveness (66% on average) after round-trip translation, making
  them non-adversarial in multilingual settings. To address this, the authors propose
  an intervention-based method (NMT-Text-Attack) that integrates machine translation
  into adversarial example generation, guaranteeing 100% robustness to back-translation
  for user-specified languages.
---

# Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation

## Quick Facts
- arXiv ID: 2307.12520
- Source URL: https://arxiv.org/abs/2307.12520
- Reference count: 10
- Primary result: NMT-Text-Attack guarantees 100% robustness to round-trip translation while maintaining semantic quality

## Executive Summary
This paper addresses a critical vulnerability in text adversarial attacks: their effectiveness degrades significantly after round-trip translation. The authors demonstrate that six state-of-the-art adversarial attacks lose an average of 66% effectiveness when subjected to translation to another language and back. To solve this problem, they introduce NMT-Text-Attack, an intervention-based method that integrates machine translation directly into the adversarial example generation process, ensuring that generated examples remain adversarial even after translation. The approach maintains high semantic quality while improving attack success rates in multilingual settings, including for unseen languages.

## Method Summary
The NMT-Text-Attack method integrates machine translation into adversarial example generation by incorporating round-trip translation as a constraint during the word replacement phase. For each candidate adversarial example, the algorithm translates the example to user-specified languages and back, then only accepts examples that maintain their adversarial property after translation. This approach can be applied to any existing adversarial attack algorithm by modifying the constraint evaluation phase, making it attack-agnostic. The method trades computational efficiency for guaranteed robustness to specified languages.

## Key Results
- Baseline attacks lose 66% effectiveness on average after round-trip translation
- NMT-Text-Attack guarantees 100% robustness to user-specified languages
- Maintains high semantic quality (BLEU scores and similarity metrics above 0.9)
- Improves attack success rates in multilingual scenarios, including unseen languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Round-trip translation acts as a defense by altering word embeddings and context enough to break adversarial perturbations
- Mechanism: The translation process changes individual word choices and syntactic structures, disrupting the carefully crafted adversarial perturbations that depend on specific token embeddings and positions
- Core assumption: The original adversarial examples rely on precise token-level manipulations that are not preserved through translation
- Evidence anchors:
  - [abstract]: "We demonstrate that 6 state-of-the-art text-based adversarial attacks do not maintain their efficacy after round-trip translation"
  - [section 4.2]: "On average, over 66% of the examples generated originally by the attack are rendered non-adversarial on round-trip translation with at least one language"

### Mechanism 2
- Claim: NMT-Text-Attack integrates machine translation into the adversarial example generation process to ensure robustness
- Mechanism: By incorporating round-trip translation during the word replacement phase, the algorithm only accepts candidate adversarial examples that maintain their adversarial property after translation
- Core assumption: It's possible to find adversarial examples that are both effective against the target model and robust to translation
- Evidence anchors:
  - [abstract]: "we introduce an intervention-based solution to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation"
  - [section 3]: Algorithm 1, line 13-14: "Round-Trip-Translate Xadv with k language(s) using M... Evaluate classification scores for T... removing examples that do not maintain adversarial sentiment"

### Mechanism 3
- Claim: The attack-agnostic nature of NMT-Text-Attack allows it to improve robustness across different attack algorithms
- Mechanism: By modifying the constraint evaluation phase of any adversarial attack, the method can be applied universally to increase translation robustness
- Core assumption: Different adversarial attack algorithms share similar word replacement and constraint evaluation phases that can be enhanced with translation
- Evidence anchors:
  - [abstract]: "Furthermore, we introduce an intervention-based solution 1 to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation"
  - [section 3]: "we employ a generic template used by standard state-of-the-art adversarial attack examples in order to showcase the attack-agnostic capabilities"

## Foundational Learning

- Concept: Round-trip translation
  - Why needed here: Understanding how translating text to another language and back can alter meaning and break adversarial patterns
  - Quick check question: What happens to the word "color" when translated from English to British English and back to American English?

- Concept: Adversarial examples in NLP
  - Why needed here: Knowing how small perturbations to text can fool language models is crucial for understanding both the problem and solution
  - Quick check question: How does changing "not" to "no" in "I am not happy" affect sentiment classification?

- Concept: Word importance ranking in adversarial attacks
  - Why needed here: Understanding how attacks identify which words to modify is essential for seeing how NMT-Text-Attack integrates translation
  - Quick check question: In the sentence "The movie was absolutely terrible", which word would most attacks target for modification?

## Architecture Onboarding

- Component map: Original sentence -> Word Importance Ranking -> Word Replacement -> NMT Module (Round-trip translation) -> Constraint Evaluation -> Output (Robust adversarial example)

- Critical path:
  1. Word importance ranking
  2. Word replacement generation
  3. Round-trip translation of candidates
  4. Adversarial property validation post-translation
  5. Constraint application and best candidate selection

- Design tradeoffs:
  - More languages for translation → higher robustness guarantee but fewer viable candidates
  - Higher replacement limit → more candidates but increased computational cost
  - Stricter constraints → better quality but potentially lower success rate

- Failure signatures:
  - Low success rate: Indicates the space of robust adversarial examples is too constrained
  - High computational cost: Suggests need to optimize translation or reduce language combinations
  - Poor semantic similarity: Implies translation is overly aggressive in altering meaning

- First 3 experiments:
  1. Test NMT-Text-Attack with TextFooler on a small dataset (100 examples) with 1 language to verify basic functionality
  2. Compare success rates and semantic similarity with and without NMT integration on 500 examples
  3. Test robustness to an "unseen" language not used during training to validate generalization claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which NMT-Text-Attack guarantees 100% robustness to round-trip translation for user-specified languages?
- Basis in paper: [explicit] The paper states that NMT-Text-Attack "introduces a strict constraint to only allow examples that are robust to back-translation to be selected as candidates for the attack"
- Why unresolved: The paper does not provide details on the specific implementation of this constraint or how it ensures robustness across multiple languages.
- What evidence would resolve it: A detailed algorithmic explanation of the constraint implementation and experimental results showing robustness across various language combinations.

### Open Question 2
- Question: How does the performance of NMT-Text-Attack scale with the number of languages used for round-trip translation?
- Basis in paper: [explicit] The paper mentions that "there is considerable scope to increase the number of robust examples available simply by increasing the replacement limit" but does not explore the impact of using multiple languages.
- Why unresolved: The paper only tests the algorithm with up to 3 languages (Spanish, German, French) and does not investigate the effects of using more languages or different language combinations.
- What evidence would resolve it: Experiments testing the algorithm with a larger number of languages and various language combinations, along with analysis of the scaling effects on attack success rate and robustness.

### Open Question 3
- Question: Can NMT-Text-Attack be adapted to generate adversarial examples for languages other than the target language of the victim model?
- Basis in paper: [inferred] The paper focuses on generating adversarial examples for English text that remain robust after round-trip translation, but does not explore the possibility of targeting other languages.
- Why unresolved: The current implementation of NMT-Text-Attack is designed to work with English text and specific target languages for translation, but the potential for adapting it to other source languages is not discussed.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of NMT-Text-Attack in generating adversarial examples for languages other than English, along with any necessary modifications to the algorithm.

## Limitations

- Translation quality dependency: The method's effectiveness is fundamentally limited by the quality of the machine translation system used
- Computational cost: Integrating round-trip translation substantially increases processing time and computational requirements
- Semantic quality trade-offs: The iterative nature may force selection of suboptimal perturbations that better preserve meaning but are less effective at fooling the target model

## Confidence

**High Confidence Claims**:
- Round-trip translation reduces adversarial attack effectiveness (66% average degradation)
- NMT-Text-Attack guarantees 100% robustness to specified languages
- Improved attack success rates in multilingual settings

**Medium Confidence Claims**:
- Attack-agnostic nature of NMT-Text-Attack
- Semantic quality preservation

**Low Confidence Claims**:
- Generalization to unseen languages

## Next Checks

1. **Translation Quality Sensitivity Analysis**: Systematically evaluate how NMT-Text-Attack performance varies with translation quality by testing with different translation models (Google Translate API, DeepL, etc.) and across diverse language pairs including low-resource languages.

2. **Edge Case Semantic Preservation**: Analyze the 100 adversarial examples that failed round-trip translation in baseline attacks to identify patterns where NMT-Text-Attack succeeds vs. fails in maintaining both adversarial effectiveness and semantic integrity.

3. **Computational Efficiency Optimization**: Benchmark the algorithm's runtime performance and implement optimization strategies such as parallel translation processing, early termination criteria, or adaptive language selection based on translation difficulty.