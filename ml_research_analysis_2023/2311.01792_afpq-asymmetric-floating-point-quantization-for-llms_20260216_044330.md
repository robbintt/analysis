---
ver: rpa2
title: 'AFPQ: Asymmetric Floating Point Quantization for LLMs'
arxiv_id: '2311.01792'
source_url: https://arxiv.org/abs/2311.01792
tags:
- quantization
- weight
- arxiv
- asymmetric
- formats
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that existing floating-point (FP) quantization
  methods perform poorly for low-bit quantization of large language models (LLMs)
  due to the asymmetric distribution of weight tensors. To address this, the authors
  propose asymmetric FP quantization (AFPQ), which sets separate scales for positive
  and negative values within each quantization group.
---

# AFPQ: Asymmetric Floating Point Quantization for LLMs

## Quick Facts
- arXiv ID: 2311.01792
- Source URL: https://arxiv.org/abs/2311.01792
- Reference count: 16
- Primary result: Asymmetric FP quantization (AFPQ) achieves up to 3% MMLU accuracy improvement for 3-bit/4-bit LLM quantization by using separate scales for positive and negative values.

## Executive Summary
AFPQ addresses the fundamental limitation of symmetric floating-point quantization for LLMs by recognizing that weight tensors have asymmetric distributions. The method sets separate scales for positive and negative values within each quantization group, allowing rescaled FP values to better match the original asymmetric weight distribution. Experiments demonstrate significant accuracy improvements for both 3-bit and 4-bit quantization, with up to 3% gains on MMLU for LLaMA2-7B. The approach requires no additional storage compared to asymmetric integer quantization and can be easily integrated into existing quantization methods like GPTQ and AWQ.

## Method Summary
AFPQ improves LLM quantization accuracy by addressing asymmetric weight distributions through separate scaling for positive and negative values within each quantization group. Unlike symmetric FP quantization that uses a uniform scale, AFPQ computes distinct scales for positive and negative values, better matching the original weight distribution. The method operates on individual sub-tensors or groups and can be integrated as a plugin into other tensor-level quantization algorithms. During inference, weights are dequantized using the stored asymmetric scales. The approach maintains storage efficiency by requiring only two parameters per group, matching the storage requirements of asymmetric integer quantization.

## Key Results
- 3% improvement in MMLU accuracy for LLaMA2-7B with 3-bit and 4-bit FP quantization
- Up to 1.62x speedup compared to FP16 with low-bit FP inference system
- Successfully integrated into GPTQ and AWQ quantization methods
- No additional storage required compared to asymmetric integer quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric FP quantization improves quantization accuracy by separately scaling positive and negative values to match asymmetric weight distributions.
- Mechanism: Standard symmetric FP quantization uses a single scale for all values, which wastes precision when weights are asymmetric. AFPQ uses two scales—one for positive values and one for negative values—ensuring rescaled FP values better match the original weight distribution.
- Core assumption: Weight tensors in LLMs have asymmetric distributions where max and min values are not symmetric about zero, especially in small quantization groups.
- Evidence anchors:
  - [abstract] "the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors."
  - [section 3] "Unlike previous symmetric FP quantization, which uses a uniform scale for positive and negative values within a weight group, AFPQ sets separate scales for positive and negative values."
- Break condition: If weight tensors are approximately symmetric, using separate scales offers no benefit and may slightly increase computational overhead.

### Mechanism 2
- Claim: AFPQ maintains storage efficiency while improving accuracy compared to asymmetric integer quantization.
- Mechanism: Asymmetric integer quantization requires one scale and one zero-point per group, while AFPQ uses two scales per group. Both require two parameters per group, so storage overhead is identical.
- Core assumption: The number of parameters needed to represent asymmetric scaling is the limiting factor for storage, not the computational method.
- Evidence anchors:
  - [abstract] "no additional storage is needed compared with asymmetric integer (INT) quantization."
  - [section 3] "AFPQ requires no additional storage compared with asymmetric INT quantization (both need two parameters for one group)."
- Break condition: If additional metadata or more complex scaling becomes necessary, storage costs could increase.

### Mechanism 3
- Claim: Integrating AFPQ with advanced quantization methods like GPTQ and AWQ improves their accuracy.
- Mechanism: GPTQ and AWQ use integer quantization internally. By replacing their symmetric or asymmetric integer quantization with AFPQ's asymmetric FP quantization, the quantized weights better represent the original distribution, reducing quantization error.
- Core assumption: GPTQ and AWQ's error compensation and scaling strategies are compatible with the precision gains from asymmetric FP quantization.
- Evidence anchors:
  - [abstract] "can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance."
  - [section 4] "As AFPQ operates on each individual sub-tensor or group, it can work as a plugin to other tensor-level quantization algorithms, such as GPTQ and AWQ."
- Break condition: If GPTQ/AWQ's optimization strategies are incompatible with FP scaling, integration may not yield improvements.

## Foundational Learning

- Concept: Floating-point representation (sign, exponent, mantissa)
  - Why needed here: Understanding why FP formats have non-uniform distributions dense near zero is essential for grasping why asymmetric scaling helps.
  - Quick check question: Why does the mantissa design in FP formats make them dense near zero and sparse for large values?

- Concept: Asymmetric quantization (scale + zero-point)
  - Why needed here: AFPQ is inspired by asymmetric integer quantization; understanding how scale and zero-point handle asymmetric ranges is foundational.
  - Quick check question: How does asymmetric integer quantization use zero-point to shift the quantization range for asymmetric data?

- Concept: Quantization group granularity
  - Why needed here: The effectiveness of AFPQ depends on how weights are grouped; smaller groups show more asymmetry.
  - Quick check question: What happens to quantization accuracy as group size decreases, and why does asymmetry become more pronounced?

## Architecture Onboarding

- Component map:
  - Quantization engine -> Plugin interface -> Inference backend -> Kernel layer

- Critical path:
  1. Load weight tensor → split into quantization groups
  2. Compute separate scales for positive and negative values
  3. Quantize weights using asymmetric scaling
  4. Store two scales per group
  5. During inference, dequantize using stored scales

- Design tradeoffs:
  - Accuracy vs. speed: Two-scale dequantization adds minor overhead compared to single-scale symmetric dequantization
  - Bit-width selection: FP3 is less effective due to limited representable values; FP4/NF4 show better gains
  - Group size: Smaller groups capture asymmetry better but increase metadata overhead

- Failure signatures:
  - Degraded accuracy if weight distributions are nearly symmetric
  - Increased latency if kernel optimizations are missing
  - Incorrect dequantization if scales are swapped or misaligned

- First 3 experiments:
  1. Compare symmetric vs. asymmetric FP quantization on a small LLM with known asymmetric weight distribution (e.g., LLaMA2-7B) using group size 128
  2. Integrate AFPQ into GPTQ and measure perplexity improvement on WikiText-2
  3. Benchmark end-to-end inference latency on A6000 GPU for FP4-asym vs. INT4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the asymmetric FP quantization (AFPQ) perform when applied to other types of neural networks beyond large language models (LLMs)?
- Basis in paper: [inferred] The paper focuses on applying AFPQ to LLMs, but it mentions that the method can be easily integrated into other quantization methods, including GPTQ and AWQ, for better performance. However, it does not explicitly test the method on other types of neural networks.
- Why unresolved: The paper does not provide any evidence or experiments to support the effectiveness of AFPQ on other types of neural networks beyond LLMs.
- What evidence would resolve it: Experiments showing the performance of AFPQ on other types of neural networks, such as convolutional neural networks or recurrent neural networks, would provide evidence to support the generalizability of the method.

### Open Question 2
- Question: How does the asymmetric FP quantization (AFPQ) perform when applied to larger or smaller models compared to the LLaMA2 models used in the experiments?
- Basis in paper: [inferred] The paper evaluates AFPQ on LLaMA2 models with different sizes, but it does not explore the performance of the method on significantly larger or smaller models.
- Why unresolved: The paper does not provide any evidence or experiments to support the effectiveness of AFPQ on models with significantly different sizes compared to the LLaMA2 models used in the experiments.
- What evidence would resolve it: Experiments showing the performance of AFPQ on significantly larger or smaller models, such as models with billions or millions of parameters, would provide evidence to support the scalability of the method.

### Open Question 3
- Question: How does the asymmetric FP quantization (AFPQ) perform when applied to other quantization formats beyond the INT, FP, and NF formats used in the experiments?
- Basis in paper: [inferred] The paper evaluates AFPQ using INT, FP, and NF formats, but it does not explore the performance of the method on other quantization formats, such as binary or ternary formats.
- Why unresolved: The paper does not provide any evidence or experiments to support the effectiveness of AFPQ on other quantization formats beyond the INT, FP, and NF formats used in the experiments.
- What evidence would resolve it: Experiments showing the performance of AFPQ on other quantization formats, such as binary or ternary formats, would provide evidence to support the versatility of the method.

## Limitations

- Unknown FP format parameters: The paper references low-bit formats (FP4 E2M1, FP3 E2M0, NF formats) without fully specifying their bit allocations
- Kernel implementation details: The low-bit FP inference system implementation is described at a high level without specific kernel optimizations
- Dataset-specific performance: The asymmetric distribution benefit may vary significantly across different model architectures and domains

## Confidence

**High Confidence**:
- The asymmetric weight distribution problem in LLM quantization is well-documented and the proposed solution (separate scales for positive/negative values) is technically sound
- Storage efficiency claim is straightforward: two parameters per group for both asymmetric INT and FP quantization
- Plugin architecture for GPTQ/AWQ integration is logically consistent with the described approach

**Medium Confidence**:
- The specific performance improvements (3% MMLU accuracy gain, 1.62x speedup) are credible given the methodology, but would benefit from independent verification
- The effectiveness of FP3 quantization is acknowledged as limited, which aligns with theoretical expectations of the format's constraints

**Low Confidence**:
- The generalizability claim to "various LLMs" is supported by experiments on three models but hasn't been tested across the full spectrum of LLM architectures and use cases

## Next Checks

1. **Cross-Model Validation**: Apply AFPQ to a diverse set of LLM architectures (including decoder-only, encoder-decoder, and hybrid models) and measure accuracy retention across different weight distribution profiles to verify the claimed universal applicability.

2. **Kernel Performance Profiling**: Implement the asymmetric FP dequantization kernels and profile their computational overhead compared to symmetric dequantization, measuring both accuracy and latency on multiple GPU architectures to validate the claimed 1.62x speedup.

3. **Integration Robustness Test**: Systematically integrate AFPQ into multiple quantization frameworks beyond GPTQ and AWQ (such as QLoRA, SmoothQuant) and measure whether the asymmetric scaling consistently improves quantization error across different optimization strategies and model sizes.