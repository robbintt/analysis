---
ver: rpa2
title: '$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States'
arxiv_id: '2303.18242'
source_url: https://arxiv.org/abs/2303.18242
tags:
- diffusion
- neural
- data
- which
- nite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a new infinite-dimensional diffusion model\
  \ called \u221E-Diff that operates directly on raw sparse data coordinates rather\
  \ than latent vectors. The model learns a continuous function by denoising data\
  \ at randomly sampled coordinates during training, allowing arbitrary resolution\
  \ sampling."
---

# $\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States

## Quick Facts
- arXiv ID: 2303.18242
- Source URL: https://arxiv.org/abs/2303.18242
- Authors: [Not specified in input]
- Reference count: 40
- Key outcome: Proposed $\infty$-Diff achieves state-of-the-art FID scores on high-resolution image datasets and can scale beyond training resolution while retaining detail, even with 8× subsampling.

## Executive Summary
$\infty$-Diff introduces an infinite-dimensional diffusion model that operates directly on raw sparse data coordinates rather than latent vectors. By denoising data at randomly sampled coordinates during training, the model learns a continuous function allowing arbitrary resolution sampling. A mollified diffusion process smooths states to enable infinite resolution data modeling. Experiments on high-resolution image datasets show $\infty$-Diff achieves state-of-the-art FID scores and can scale beyond training resolution while retaining detail, even with an 8× subsampling rate.

## Method Summary
$\infty$-Diff operates in infinite-dimensional Hilbert spaces by smoothing diffusion states with mollifiers (e.g., truncated Gaussian kernels) to make them continuous. The model trains on randomly sampled coordinate subsets, learning to denoise only at those locations. A multiscale neural operator architecture combines sparse operators on raw data with dense convolutions on fixed grids to capture both local and global structure. This hybrid approach enables efficient training on sparse data while maintaining spatial context aggregation.

## Key Results
- Achieves state-of-the-art FIDCLIP scores on high-resolution image datasets (CelebAHQ-64, CelebAHQ-128, FFHQ-256, Church-256)
- Successfully scales beyond training resolution while retaining detail, even with 8× subsampling rate
- Outperforms prior neural field-based approaches with significant runtime and memory savings

## Why This Works (Mechanism)

### Mechanism 1: Mollified Diffusion in Hilbert Spaces
- Claim: Mollified diffusion allows the model to operate in infinite-dimensional Hilbert space by smoothing states to be continuous and well-behaved
- Mechanism: Convolution of diffusion states with mollifiers (e.g., truncated Gaussian kernel) transforms white noise into Gaussian random fields in Hilbert space, enabling stable training
- Core assumption: Mollifier bandwidth is sufficient to ensure states remain in Hilbert space while preserving data structure
- Evidence anchors: [abstract] "non-local integral operators to map between Hilbert spaces", [section] "approximates non-smooth input space with smooth function by convolving functions with a mollifier"
- Break condition: If mollifier is too narrow, states may still lie outside Hilbert space, breaking mathematical foundation

### Mechanism 2: Continuous Function Learning from Sparse Coordinates
- Claim: Training on randomly sampled coordinate subsets enables learning a continuous function that generalizes to arbitrary resolutions
- Mechanism: Denoising only at sampled coordinates forces model to learn full signal reconstruction from sparse observations, capturing global and local information
- Core assumption: Coordinate sampling is sufficient to approximate integral operators needed for denoising
- Evidence anchors: [abstract] "training on randomly sampled subsets of coordinates during training and denoising content only at those locations", [section] "hybrid approach with sparse operators applied on raw data"
- Break condition: If sampling rate is too low, integral operator approximation becomes poor, degrading sample quality

### Mechanism 3: Multiscale Neural Operator Architecture
- Claim: Hybrid sparse-dense architecture enables efficient training on sparse data while capturing both global and local structure
- Mechanism: Sparse neural operators at top level capture fine details from irregularly sampled coordinates; dense convolutions on fixed grids at lower levels aggregate global context
- Core assumption: Combination of sparse and dense operations provides sufficient capacity while maintaining computational efficiency
- Evidence anchors: [section] "U-Net inspired multiscale architecture that aggregates information locally and globally", [section] "hybrid approach with sparse operators applied on raw data"
- Break condition: If fixed grid resolution is too low, global information may be lost, leading to poor sample quality

## Foundational Learning

- Concept: Hilbert spaces and infinite-dimensional analysis
  - Why needed here: Model operates in infinite-dimensional function spaces to handle arbitrary resolution signals, requiring Hilbert space theory and operator theory
  - Quick check question: Why can't we directly use finite-dimensional diffusion models for infinite resolution data?

- Concept: Diffusion processes and stochastic differential equations
  - Why needed here: Model defines diffusion process in infinite dimensions, requiring understanding of discrete/continuous time diffusion and score-based generative models
  - Quick check question: What is the difference between forward and reverse diffusion processes in infinite-dimensional setting?

- Concept: Neural operators and integral kernels
  - Why needed here: Model uses neural operators to map between function spaces, requiring understanding of how integral kernels approximate differential operators
  - Quick check question: How do neural operators differ from standard CNNs in handling arbitrary coordinate locations?

## Architecture Onboarding

- Component map: Raw Data Coordinates -> Sparse Neural Operator Blocks -> Interpolation Layers -> Dense Convolutional Blocks -> Mollification Layer -> Encoder/Decoder

- Critical path:
  1. Sample random coordinates from input data
  2. Apply sparse neural operators to capture local details
  3. Interpolate to fixed grids at multiple resolutions
  4. Apply dense convolutions to aggregate global context
  5. Mollify diffusion states to ensure they lie in Hilbert space
  6. Predict noise or denoised signal using learned function

- Design tradeoffs:
  - Sparse vs dense operations: Sparse allows arbitrary resolution sampling but may miss global context; dense captures global structure but requires fixed grids
  - Kernel size in sparse convolutions: Larger kernels capture more spatial context but increase computational cost
  - Mollifier bandwidth: Wider mollifiers ensure Hilbert space membership but may oversmooth details

- Failure signatures:
  - Poor sample quality: Indicates issues with coordinate sampling, kernel design, or architecture capacity
  - Training instability: May indicate problems with mollification, learning rates, or gradient flow through sparse operations
  - Memory issues: Suggests need for more aggressive sparse sampling or model size reduction

- First 3 experiments:
  1. Train on simple dataset (e.g., MNIST) with full coordinate sampling to verify basic functionality
  2. Gradually reduce sampling rate to test impact on sample quality and identify minimum viable sampling rate
  3. Test discretization invariance by sampling at resolutions higher and lower than training resolution to verify continuous function generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does mollified diffusion process compare to other smoothing techniques for infinite-dimensional diffusion models?
- Basis in paper: [inferred] Paper introduces mollified diffusion process but doesn't compare to alternative smoothing methods
- Why unresolved: Paper focuses on demonstrating effectiveness of mollified diffusion without exploring alternatives
- What evidence would resolve it: Experiments comparing mollified diffusion to Gaussian blurring or kernel density estimation on various datasets and metrics

### Open Question 2
- Question: How does choice of neural operator architecture affect performance of infinite-dimensional diffusion model?
- Basis in paper: [explicit] Paper discusses neural operators but doesn't extensively explore impact of different architectures
- Why unresolved: Paper presents specific architecture without investigating how alternatives might influence performance
- What evidence would resolve it: Experiments comparing proposed architecture to Fourier Neural Operators or Graph Neural Networks on various datasets and metrics

### Open Question 3
- Question: How does proposed approach scale to even higher resolutions beyond training data?
- Basis in paper: [explicit] Paper demonstrates scaling beyond training resolution but doesn't explore limits
- Why unresolved: Paper showcases model's ability to generate high-resolution samples without investigating further scaling potential
- What evidence would resolve it: Experiments pushing model to generate samples at significantly higher resolutions than training data, analyzing limiting factors

## Limitations
- Limited empirical validation: Claims about infinite resolution generation lack quantitative evidence beyond 256×256 resolution
- Architectural complexity: Hybrid sparse-dense approach requires careful hyperparameter tuning with unstudied interactions
- Scaling claims: Asserted ability to generate beyond training resolution while maintaining detail lacks comprehensive quantitative validation

## Confidence
- **High Confidence**: Core mathematical framework for mollified diffusion in Hilbert spaces and use of neural operators for coordinate-based modeling
- **Medium Confidence**: Practical implementation details and architectural choices for multiscale neural operator
- **Low Confidence**: Claimed runtime and memory savings, and assertion that model can generate beyond training resolution while maintaining detail

## Next Checks
1. **Resolution Scaling Experiment**: Generate samples at 512×512 and 1024×1024 resolutions and quantitatively compare quality metrics against baseline models trained at those resolutions to validate infinite resolution claims.

2. **Sampling Rate Sensitivity Analysis**: Systematically vary subsampling rate from 2× to 32× and measure impact on sample quality, training time, and memory usage to identify optimal tradeoff and verify efficiency gains.

3. **Latent Space Interpolation Test**: Generate samples by interpolating between latent codes in learned function space and verify that interpolations produce smooth, realistic transitions maintaining spatial coherence at arbitrary resolutions.