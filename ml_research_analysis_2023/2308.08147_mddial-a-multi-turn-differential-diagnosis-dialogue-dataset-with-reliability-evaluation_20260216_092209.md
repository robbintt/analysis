---
ver: rpa2
title: 'MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability
  Evaluation'
arxiv_id: '2308.08147'
source_url: https://arxiv.org/abs/2308.08147
tags:
- dialogue
- symptoms
- doctor
- patient
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MDDial is a newly introduced English-language dialogue dataset
  for Automatic Differential Diagnosis (ADD) systems. It is constructed using a template-based
  method from the structured MDD dataset, containing 1,725 training dialogues across
  12 diseases and 118 symptoms.
---

# MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability Evaluation

## Quick Facts
- **arXiv ID**: 2308.08147
- **Source URL**: https://arxiv.org/abs/2308.08147
- **Reference count**: 9
- **Primary result**: Moderate-sized language models struggle with ADD tasks on MDDial, highlighting need for new training paradigms

## Executive Summary
MDDial is a newly introduced English-language dialogue dataset for Automatic Differential Diagnosis (ADD) systems, constructed using a template-based method from the structured MDD dataset. The dataset contains 1,725 training dialogues across 12 diseases and 118 symptoms, designed to simulate realistic multi-turn differential diagnosis conversations while preserving privacy. The paper proposes a new reliability metric that considers both symptom accuracy and diagnosis accuracy, addressing limitations of previous evaluation metrics. Experiments show that moderate-sized language models like GPT-2 and DialoGPT struggle with ADD tasks on MDDial, highlighting the need for new training paradigms or model architectures to handle this low-resource domain.

## Method Summary
The method involves creating a template-based dialogue generation system from the structured MDD dataset, using 1,725 dialogues across 12 diseases and 118 symptoms. The process includes pretraining GPT-2 and DialoGPT models on MedDialog, then fine-tuning them on the generated MDDial dataset. Evaluation uses disease-wise symptom score, dialogue-wise symptom score, diagnosis accuracy, and a proposed reliability score that combines symptom relevance with diagnostic correctness. The dataset and evaluation methodology are designed to facilitate future ADD research by providing a standardized benchmark for comparing different approaches.

## Key Results
- Moderate-sized language models (GPT-2, DialoGPT) struggle with ADD tasks on MDDial, showing low performance across all metrics
- The proposed reliability metric better captures the quality of diagnosis by penalizing irrelevant symptom queries
- Disease-wise symptom scoring provides more coherent evaluation compared to dialogue-level scoring when correlated with diagnosis accuracy

## Why This Works (Mechanism)

### Mechanism 1
Template-based dialogue generation can simulate real differential diagnosis conversations without requiring sensitive real patient data. By mapping structured MDD symptoms and diseases to natural language templates, the system produces realistic multi-turn dialogues while preserving privacy. The core assumption is that template variation is sufficient to mimic the diversity of real doctor-patient interactions. If templates fail to cover rare symptom combinations or nuanced symptom descriptions, generated dialogues become too repetitive and unrealistic.

### Mechanism 2
Disease-wise symptom scoring better reflects the relationship between symptoms and diagnosis than dialogue-level scoring. Credits are assigned based on whether a symptom belongs to the symptom set of the diagnosed disease, not just whether it matches the ground truth implicit symptoms. The core assumption is that the ground truth implicit symptoms in MDD are incomplete, so using disease-specific symptom sets captures more relevant symptoms. If the disease-symptom mappings in MDD are inaccurate or incomplete, the disease-wise score will over-credit irrelevant symptoms.

### Mechanism 3
Reliability score captures the quality of diagnosis beyond simple accuracy by penalizing irrelevant symptom queries. It combines disease-wise symptom score with diagnosis accuracy, only awarding full credit if both are high. The core assumption is that a correct diagnosis based on irrelevant symptoms is less trustworthy than one based on relevant symptoms. If symptom-disease relationships are too complex for the reliability threshold to capture meaningfully, the metric becomes arbitrary.

## Foundational Learning

- **Concept: Template-based data generation**
  - Why needed here: Real clinical dialogue data is hard to collect due to privacy concerns and requires domain expertise
  - Quick check question: How would you modify templates to include rare symptom combinations without breaking the generation pipeline?

- **Concept: Disease-symptom mapping**
  - Why needed here: To evaluate if a system is asking relevant questions, you need to know which symptoms indicate which diseases
  - Quick check question: Given a disease, how would you determine its most discriminative symptoms from MDD?

- **Concept: Multi-turn dialogue structure**
  - Why needed here: Differential diagnosis involves iterative questioning, so the dataset must capture this back-and-forth
  - Quick check question: What would be the impact on model performance if you removed the intermediate inquiry stages?

## Architecture Onboarding

- **Component map**: MDD structured dataset → Template selection → Template filling → Dialogue concatenation → GPT-2/DialoGPT models → Fine-tuning on MDDial → Automated evaluation pipeline → Reliability metric calculator → Performance analysis

- **Critical path**: 
  1. Load MDD data and symptom-disease mappings
  2. Select appropriate template for each dialogue stage
  3. Fill templates with symptoms and diagnosis
  4. Concatenate utterances into complete dialogues
  5. Fine-tune language model on generated dialogues
  6. Evaluate using disease-wise symptom score and reliability metric

- **Design tradeoffs**:
  - Template variety vs. generation consistency: More templates increase diversity but risk introducing noise
  - Symptom granularity vs. evaluation clarity: Finer symptom distinctions make evaluation more precise but increase complexity
  - Model size vs. domain adaptation: Larger models may capture nuances better but require more resources and careful fine-tuning

- **Failure signatures**:
  - Low S_dise score but high Diag_acc: Model is guessing diagnoses without asking relevant symptoms
  - High S_dise score but low Diag_acc: Model is asking many relevant questions but drawing wrong conclusions
  - Rscore near zero across all thresholds: Symptom-disease relationships are too complex for current models

- **First 3 experiments**:
  1. Generate dialogues with only the most common templates, then test on unseen templates to measure template dependency
  2. Train with varying threshold values for reliability score and observe the impact on symptom selection patterns
  3. Compare performance when using ground truth patient utterances vs. generated ones in the evaluation pipeline to quantify the effect of dialogue validity checking

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the proposed reliability metric perform on larger language models like GPT-3 or GPT-4 for the ADD task? The paper focuses on moderate-sized language models and does not explore the performance of larger models on the ADD task. Evaluating larger models would provide insights into the scalability of the metric and the models' capabilities in handling the ADD task.

- **Open Question 2**: Can the template-based generation method be improved to create more diverse and complex dialogues for the ADD task? The paper mentions that the template-based generation method might constrain the range and depth of the resulting data. Investigating alternative data generation methods, such as using large language models to increase both the diversity and complexity of the dialogues, would help determine if the template-based method can be enhanced.

- **Open Question 3**: How does the proposed reliability metric perform when evaluated using human assessment instead of automated evaluation? The paper acknowledges that human evaluation would be ideal but is expensive and challenging to replicate. Conducting human evaluation and comparing results with automated evaluation would provide insights into the effectiveness of the metric in capturing system reliability.

## Limitations

- Template-based generation may not fully capture the complexity and nuance of real clinical reasoning, potentially limiting the diversity of generated dialogues
- The effectiveness of the reliability metric depends heavily on the completeness and accuracy of symptom-disease mappings in the underlying MDD dataset
- Automated evaluation metrics may not fully capture clinically meaningful diagnostic accuracy, and human evaluation was not performed

## Confidence

- **High confidence**: Moderate-sized language models struggle with ADD tasks on MDDial - well-supported by experimental results showing consistently low performance
- **Medium confidence**: Reliability metric provides more nuanced evaluation than simple accuracy - depends heavily on quality of symptom-disease mappings
- **Medium confidence**: Template-based generation can produce realistic dialogue data - extent of capturing real clinical complexity remains uncertain

## Next Checks

1. **Template Coverage Validation**: Systematically test model performance on dialogues generated from common vs. rare templates to quantify the impact of template variety on model generalization and identify if certain symptom combinations are systematically underrepresented.

2. **Ground Truth Symptom Completeness**: Conduct a small-scale expert review of the implicit symptoms in MDD to assess the completeness of the ground truth data, as this directly affects the validity of the disease-wise scoring and reliability metrics.

3. **Human Evaluation Correlation**: Compare automated metric scores with human expert assessments of diagnostic reasoning quality to validate whether the proposed reliability score actually correlates with clinically meaningful performance.