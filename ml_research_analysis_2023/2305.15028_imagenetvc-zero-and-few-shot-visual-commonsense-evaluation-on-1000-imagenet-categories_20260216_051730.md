---
ver: rpa2
title: 'ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet
  Categories'
arxiv_id: '2305.15028'
source_url: https://arxiv.org/abs/2305.15028
tags:
- visual
- image
- commonsense
- valms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ImageNetVC, a human-annotated dataset for
  zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories.
  The dataset comprises 4,076 high-quality QA pairs covering diverse domains such
  as color, shape, material, component, etc.
---

# ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories

## Quick Facts
- arXiv ID: 2305.15028
- Source URL: https://arxiv.org/abs/2305.15028
- Authors: 
- Reference count: 28
- This paper introduces ImageNetVC, a human-annotated dataset for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories.

## Executive Summary
This paper introduces ImageNetVC, a human-annotated dataset for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories. The dataset comprises 4,076 high-quality QA pairs covering diverse domains such as color, shape, material, component, etc. The authors evaluate both unimodal PLMs and VaLMs on ImageNetVC, revealing that large-scale PLMs' visual commonsense knowledge adheres to the scaling law, with LLaMA consistently outperforming OPT and GPT families. VaLMs show significant improvements over their frozen PLM backbones. The study also analyzes factors affecting the visual commonsense knowledge of large-scale models, providing insights into the development of language models enriched with visual commonsense knowledge.

## Method Summary
The paper evaluates zero-shot visual commonsense knowledge using the ImageNetVC dataset containing 4,076 QA pairs across 1,000 ImageNet categories. Researchers test pre-trained language models (OPT, GPT, LLaMA, Pythia families) and visually-augmented language models (Z-LaVI, MAGMA, BLIP-2) using both textual prompts and visual inputs from web search and synthetic images. The evaluation framework measures top-1 accuracy across different visual commonsense domains including color, shape, material, and component attributes.

## Key Results
- LLaMA models consistently outperform other PLM families on visual commonsense tasks
- VaLMs show significant improvements over their frozen PLM backbones when incorporating visual information
- PLMs exhibit emergent capabilities in solving component subtasks when reaching model scale larger than 2.7B parameters
- Scaling law observations show performance improvements with increased model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale PLMs' visual commonsense knowledge adheres to the scaling law
- Mechanism: As model size increases, PLMs' performance on visual commonsense tasks correspondingly improves due to increased model capacity and exposure to diverse training data
- Core assumption: Larger models have more parameters and can capture more complex patterns in the training data
- Evidence anchors:
  - [abstract]: "Experimental results reveal that large-scale PLMs' visual commonsense knowledge adheres to the scaling law"
  - [section]: "As model size increases, PLMs' performance correspondingly improves"
  - [corpus]: Weak evidence - no direct citations in corpus about scaling laws in PLMs for visual commonsense
- Break condition: Diminishing returns on performance improvement as model size increases beyond a certain point

### Mechanism 2
- Claim: Incorporating visual information improves zero-shot visual commonsense capabilities of VaLMs
- Mechanism: VaLMs leverage additional visual inputs from web search or synthesis methods to augment frozen PLM backbones, enabling them to better understand visual concepts
- Core assumption: Visual information provides complementary knowledge that enhances textual understanding of visual concepts
- Evidence anchors:
  - [abstract]: "VaLMs show significant improvements over their frozen PLM backbones"
  - [section]: "VaLMs consistently outperform their 'frozen' PLM backbones, showing that incorporating visual information improves zero-shot visual commonsense capabilities"
  - [corpus]: Weak evidence - no direct citations in corpus about visual augmentation in VaLMs
- Break condition: Visual inputs are of poor quality or not relevant to the task at hand

### Mechanism 3
- Claim: Models exhibit emergent capabilities in solving component subtasks when reaching a certain scale
- Mechanism: Larger models develop new capabilities that smaller models lack, particularly in understanding fine-grained visual components
- Core assumption: Emergent capabilities arise from the increased model capacity and exposure to diverse training data
- Evidence anchors:
  - [abstract]: "Experimental results reveal that PLMs exhibit emergent capabilities with a model scale larger than 2.7B on the component subset"
  - [section]: "We observe that PLMs exhibit emergent capabilities in solving the component subtask"
  - [corpus]: Weak evidence - no direct citations in corpus about emergent capabilities in PLMs for visual commonsense
- Break condition: Model size is below the threshold required for emergent capabilities to manifest

## Foundational Learning

- Concept: Visual Commonsense
  - Why needed here: Understanding visual commonsense is crucial for evaluating how well models can grasp commonly shared human knowledge about generic visual concepts
  - Quick check question: What is the difference between visual information specific to a single image and visual commonsense shared across the world?

- Concept: Scaling Law
  - Why needed here: Recognizing the scaling law helps in understanding how model performance improves with increased model size and training data
  - Quick check question: How does the performance of PLMs on visual commonsense tasks change as model size increases?

- Concept: Emergent Capabilities
  - Why needed here: Identifying emergent capabilities is important for understanding how larger models can develop new abilities that smaller models lack
  - Quick check question: At what model scale do PLMs start to exhibit emergent capabilities in solving component subtasks?

## Architecture Onboarding

- Component map:
  Pretrained Language Models (PLMs) -> Visually-augmented Language Models (VaLMs) -> Image Sources -> Evaluation Framework

- Critical path:
  1. Train or obtain large-scale PLMs and VaLMs
  2. Collect and preprocess image sources for VaLMs
  3. Convert QA pairs in ImageNetVC to prompts
  4. Evaluate models using the prompts and image sources
  5. Analyze results and identify factors affecting performance

- Design tradeoffs:
  - Model size vs. computational resources: Larger models require more computational power
  - Quality of visual inputs vs. quantity: Higher quality images may improve performance but are harder to obtain
  - Generalizability vs. specificity: Balancing the need for models to perform well on a wide range of tasks while maintaining accuracy on specific visual concepts

- Failure signatures:
  - Poor performance on visual commonsense tasks despite large model size
  - Overfitting to specific image sources or QA pairs
  - Inconsistent results across different evaluation prompts

- First 3 experiments:
  1. Evaluate a small PLM (e.g., OPT-125m) on ImageNetVC to establish a baseline
  2. Evaluate the same PLM with visual inputs from web search images to assess the impact of visual augmentation
  3. Gradually increase the model size and repeat the evaluation to observe the scaling law in action

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do models acquire visual commonsense knowledge from textual pretraining data?
- Basis in paper: [explicit] The paper mentions that models exhibit emergent capabilities in solving visual commonsense tasks as their size increases, but it remains unclear how they acquire this knowledge from textual pretraining data alone.
- Why unresolved: The paper does not provide a detailed analysis of the mechanisms by which models extract and integrate visual information from text. It only observes that larger models perform better on visual commonsense tasks.
- What evidence would resolve it: A study that traces the development of visual concepts in models during pretraining, perhaps using probing techniques or attention analysis, could reveal how visual knowledge is encoded and accessed from text.

### Open Question 2
- Question: How does the quality of image sources affect the performance of visually-augmented language models (VaLMs)?
- Basis in paper: [explicit] The paper shows that VaLMs perform better with CLIP-ranked images compared to random images, but it does not explore the impact of image quality or diversity on model performance.
- Why unresolved: The study only compares CLIP-ranked images with random images, leaving open questions about how factors like image resolution, diversity, or relevance to the question impact VaLM performance.
- What evidence would resolve it: A systematic evaluation of VaLMs using image sources with varying quality, diversity, and relevance could quantify the impact of these factors on visual commonsense reasoning.

### Open Question 3
- Question: How do different types of visual commonsense knowledge (e.g., color, shape, material) relate to each other in model representations?
- Basis in paper: [inferred] The paper evaluates models on different visual commonsense subsets (color, shape, material, etc.) but does not analyze the relationships between these types of knowledge in model representations.
- Why unresolved: While the paper shows that models perform differently across subsets, it does not investigate whether these types of knowledge are learned independently or if they interact in model representations.
- What evidence would resolve it: An analysis of model representations using techniques like clustering or correlation analysis could reveal how different types of visual commonsense knowledge are organized and related in model embeddings.

### Open Question 4
- Question: How does fine-tuning on visual commonsense tasks affect the performance of pretrained language models (PLMs) and VaLMs?
- Basis in paper: [explicit] The paper only evaluates models in a zero-shot setting and acknowledges that it remains unclear whether fine-tuning would improve performance on visual commonsense tasks.
- Why unresolved: The study does not explore the potential benefits of fine-tuning on visual commonsense tasks, leaving open questions about whether additional training could enhance model capabilities.
- What evidence would resolve it: A comparison of zero-shot and fine-tuned model performance on visual commonsense tasks could quantify the impact of additional training and reveal the potential for improvement.

### Open Question 5
- Question: How do the visual commonsense capabilities of models generalize to categories and concepts outside of the ImageNetVC dataset?
- Basis in paper: [explicit] The paper acknowledges that the ImageNetVC dataset only covers 1,000 ImageNet categories and that models may perform differently on other types of images.
- Why unresolved: The study only evaluates models on a specific set of categories, leaving open questions about how well their visual commonsense knowledge generalizes to other concepts and domains.
- What evidence would resolve it: A study that evaluates models on a diverse set of visual commonsense tasks covering a wide range of categories and concepts could assess the generalization of their visual knowledge.

## Limitations
- The scaling law observations are based on relatively few model sizes (4-13B parameters)
- The emergent capabilities at 2.7B parameters may be an artifact of the specific subset chosen
- Visual augmentation results depend heavily on image quality from web search and synthesis
- The evaluation relies on single-answer correctness without considering partial credit

## Confidence
- High confidence: The ImageNetVC dataset construction and basic evaluation methodology
- Medium confidence: LLaMA consistently outperforming other PLM families
- Medium confidence: VaLMs show significant improvements over frozen PLM backbones
- Low confidence: The exact scaling law relationship and emergent capability thresholds

## Next Checks
1. Replicate the scaling analysis using additional model sizes spanning a wider range (both smaller and larger than tested) to verify the scaling law patterns hold across orders of magnitude
2. Conduct ablation studies varying image quality and relevance to quantify how much visual augmentation improvements depend on image selection quality versus the VaLM architecture itself
3. Test the emergent capability threshold claim by evaluating multiple different component subsets and cross-category tasks to determine if 2.7B is a robust threshold or task-specific phenomenon