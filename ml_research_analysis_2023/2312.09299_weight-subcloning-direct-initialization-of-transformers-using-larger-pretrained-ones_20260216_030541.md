---
ver: rpa2
title: 'Weight subcloning: direct initialization of transformers using larger pretrained
  ones'
arxiv_id: '2312.09299'
source_url: https://arxiv.org/abs/2312.09299
tags:
- weight
- training
- network
- layers
- subcloning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces weight subcloning, a method to initialize
  smaller transformer models using weights from larger pretrained ones. The core idea
  is to rank neurons by importance, reorder weights accordingly, and subsample them
  to match the smaller model's dimensions.
---

# Weight subcloning: direct initialization of transformers using larger pretrained ones

## Quick Facts
- arXiv ID: 2312.09299
- Source URL: https://arxiv.org/abs/2312.09299
- Authors: 
- Reference count: 12
- The paper introduces weight subcloning, a method to initialize smaller transformer models using weights from larger pretrained ones, achieving 4× faster training convergence.

## Executive Summary
This paper presents weight subcloning, a technique for initializing smaller transformer models using weights from larger pretrained ones. The method ranks neurons by importance, reorders weights accordingly, and subsamples them to match the smaller model's dimensions. By leveraging the residual structure of transformers and consistent neuron importance patterns, weight subcloning achieves significantly faster convergence compared to random initialization. Experiments on vision transformers and language models demonstrate 4× speedup in reaching target accuracy levels.

## Method Summary
Weight subcloning involves ranking neurons by importance based on their average absolute output magnitude, then reordering and subsampling weights from a larger pretrained model to initialize a smaller target model. The process includes scaling weights by sqrt(d/d') to maintain standard deviation when reducing embedding dimensions, and optionally removing or duplicating transformer blocks to match the target architecture's layer count. Training is performed with adjusted hyperparameters like lower learning rates to account for the proximity to a local optimum.

## Key Results
- Achieves 4× faster training convergence compared to random initialization
- Reaching 70% accuracy on ImageNet requires 40 epochs with random initialization versus 10 epochs with weight subcloning
- Demonstrates consistent improvements across vision transformers (VIT) and language models (GPT-2)
- Shows architectural similarity between parent and destination models matters more than parent model accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers exhibit additive residual property, where each layer's output is close to its input, enabling safe removal or duplication of layers.
- Mechanism: The residual connection ensures that the non-residual part of each block (f(x)) has small magnitude relative to the input (x), so removing or duplicating layers doesn't significantly alter network function.
- Core assumption: The relative magnitude of f(x) to x+f(x) remains consistently small across layers, especially in middle layers.
- Evidence anchors:
  - [abstract] "In a residual transformer block, the input x undergoes a transformation to produce the output y = x + f (x), where f (·) represents the non-residual part of the block."
  - [section] "Figure 2-Top, we present the average relative magnitude of f (x) in comparison to x + f (x) across image classification (VIT) and language modeling (GPT) tasks. Notably, the middle layers of the transformer networks exhibit a reduced relative magnitude in f (x)."
  - [corpus] Weak evidence - no direct corpus support for this specific residual property claim.
- Break condition: If f(x) magnitude becomes large relative to x in certain layers, removing/duplicating those layers would significantly impact network performance.

### Mechanism 2
- Claim: Neuron importance patterns are consistent across transformer layers, allowing reliable re-ordering for dimension reduction.
- Mechanism: Neurons with high absolute magnitude in early layers maintain their importance in subsequent layers, creating a stable ranking that can guide subsampling.
- Core assumption: The neuron importance pattern observed in layer 1 extends consistently to layer 2 and beyond across all layers.
- Evidence anchors:
  - [abstract] "we observe a correlation between the magnitudes of neurons in different layers. Specifically, neurons with large magnitudes in layer 1 tend to exhibit similar large magnitudes in layer 2."
  - [section] "Figure 3: The relationship between neuron magnitudes across layers in VIT (left) and GPT2-M (right) pretrained models. Each point shows the averaged absolute value for a single neuron index."
  - [corpus] Weak evidence - corpus doesn't provide supporting evidence for consistent neuron importance patterns.
- Break condition: If neuron importance patterns vary significantly between layers, re-ordering based on early layer scores would misallocate important neurons.

### Mechanism 3
- Claim: Weight scaling maintains consistent standard deviation of neuron outputs when reducing embedding dimensions.
- Mechanism: Multiplying subcloned weights by sqrt(d/d') compensates for reduced input dimensions, preserving signal magnitude through layers.
- Core assumption: Output neuron standard deviation is inversely proportional to sqrt(number of input neurons) under i.i.d. Gaussian assumptions.
- Evidence anchors:
  - [abstract] "Assuming independent and identically distributed (i.i.d.) Gaussian distributions for network weights and activations, the standard deviation of each output neuron in a linear layer is inversely proportional to the square root of the number of input neurons in that layer."
  - [section] "Hence, to maintain the standard deviation, we multiply the subcloned weights by sqrt(d/d'), where d and d' represent the embedding sizes for the parent and destination networks, respectively."
  - [corpus] No direct evidence in corpus for this specific scaling relationship.
- Break condition: If weight distributions deviate significantly from i.i.d. Gaussian assumptions, scaling factor would be incorrect.

## Foundational Learning

- Concept: Residual connections and their additive property
  - Why needed here: Understanding why layers can be safely removed/duplicated without breaking network function
  - Quick check question: If a transformer block has input x and produces output x + f(x), what condition must f(x) satisfy for the block to be removable?

- Concept: Neuron importance scoring and ranking
  - Why needed here: Core technique for determining which neurons to keep when reducing dimensions
  - Quick check question: How would you compute an importance score for neurons in a transformer layer?

- Concept: Weight initialization dynamics and signal propagation
  - Why needed here: Explains why scaling subcloned weights is necessary to maintain training stability
  - Quick check question: What happens to the variance of layer outputs when you reduce input dimensions without scaling?

## Architecture Onboarding

- Component map:
  - Parent model: Larger pretrained transformer
  - Destination model: Smaller transformer with fewer layers and/or reduced embedding dimension
  - Neuron importance ranker: Scores and ranks neurons based on average absolute output magnitude
  - Weight subcloner: Extracts, reorders, scales, and samples weights from parent to initialize destination
  - Training loop: Standard transformer training with potentially adjusted learning rate and weight decay

- Critical path:
  1. Load pretrained parent model
  2. Compute neuron importance scores on small data subset
  3. Reorder weights in parent model based on scores
  4. Remove middle layers to match destination layer count
  5. Subsample rows/columns to reduce embedding dimension
  6. Scale weights to maintain output variance
  7. Initialize destination model with subcloned weights
  8. Train destination model with adjusted hyperparameters

- Design tradeoffs:
  - Parent model selection: Higher accuracy parent doesn't always yield better convergence; architectural similarity matters more
  - Layer removal position: Middle layers preferred due to smaller f(x) magnitude, but this may not be optimal for all architectures
  - Learning rate for destination: Lower rates (0.0001) work better than typical random initialization rates (0.002) due to proximity to local optimum

- Failure signatures:
  - Poor convergence: May indicate incorrect weight scaling factor or suboptimal parent model selection
  - Catastrophic forgetting: Often caused by using too high learning rate with subcloned initialization
  - Inconsistent performance across runs: Could indicate insufficient neuron importance ranking data or unstable ranking

- First 3 experiments:
  1. Ablation study on VIT-B → VIT-B/2 (half embedding dimension) on ImageNet to verify 4x speedup claim
  2. Parent model architecture sensitivity test using different sized GPT parents for same destination architecture
  3. Weight scaling necessity test comparing convergence with and without scaling factor applied

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why neuron reordering consistently improves training convergence across different transformer architectures and tasks?
- Basis in paper: [inferred] The paper demonstrates that neuron reordering improves convergence but does not provide a theoretical foundation for why this occurs
- Why unresolved: The paper shows empirical evidence but lacks theoretical analysis of why certain neurons are more important and how their ordering affects optimization dynamics
- What evidence would resolve it: A theoretical framework explaining the relationship between neuron importance scores, residual connections, and optimization landscape, along with empirical validation across diverse architectures

### Open Question 2
- Question: How does weight subcloning performance scale with larger architectural differences between parent and destination models (e.g., different attention mechanisms, activation functions, or residual patterns)?
- Basis in paper: [explicit] "While the destination network can vary from the parent in terms of the number of blocks and/or the embedding dimension per layer, our study has not delved into the impact of more extensive architectural changes. These changes might include modifications to residual connections, nonlinear activations, block structures, and similar aspects."
- Why unresolved: The current method assumes architectural similarity between parent and destination models, but the limits of this assumption are unexplored
- What evidence would resolve it: Systematic experiments comparing subcloning performance across architectures with varying attention mechanisms, activation functions, and residual patterns, plus theoretical analysis of cross-architecture weight transfer

### Open Question 3
- Question: What is the optimal strategy for selecting parent models when multiple pretrained models of varying sizes and accuracies are available?
- Basis in paper: [explicit] "Our experiments reveal that opting for the highest accuracy parent model doesn't necessarily result in improved convergence for the destination model" - the paper shows that architectural similarity matters more than parent accuracy
- Why unresolved: While the paper demonstrates that architectural proximity is more important than parent accuracy, it doesn't provide a principled framework for parent selection
- What evidence would resolve it: A comprehensive study quantifying the trade-off between architectural similarity, parent model accuracy, and subcloning performance, potentially leading to a parent selection algorithm

## Limitations

- The theoretical justification for weight subcloning relies on simplifying assumptions about transformer behavior that may not generalize to all architectures
- Experimental validation focuses primarily on convergence speed rather than final accuracy or generalization
- The approach assumes architectural similarity between parent and destination models, with unexplored limits on this assumption

## Confidence

- **High confidence**: The core technical approach of neuron importance ranking followed by weight subsampling and scaling is well-defined and reproducible. The convergence speed improvements on tested benchmarks appear robust and measurable.
- **Medium confidence**: The theoretical justification for why weight subcloning works relies on several simplifying assumptions about transformer behavior. While the residual property claim is supported by the presented analysis, the neuron importance consistency claim needs stronger empirical validation across more diverse architectures.
- **Low confidence**: The generalizability of the approach to non-standard transformer variants, extremely small destination models, or different training regimes remains untested. The impact of architectural mismatches between parent and destination models is acknowledged but not thoroughly explored.

## Next Checks

1. **Layer position sensitivity**: Systematically test whether removing layers from different positions (early, middle, late) affects convergence differently, and whether this varies by task type (classification vs. language modeling).

2. **Scaling factor robustness**: Compare convergence with the theoretical sqrt(d/d') scaling against empirically optimized scaling factors for each architecture pair to quantify the impact of the i.i.d. Gaussian assumption.

3. **Final accuracy validation**: Run long-term training experiments (equal wall-clock time) comparing subcloned initialization against random initialization to verify that faster convergence translates to equal or better final accuracy, not just quicker early progress.