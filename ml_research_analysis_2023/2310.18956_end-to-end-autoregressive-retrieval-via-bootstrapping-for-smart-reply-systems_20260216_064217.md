---
ver: rpa2
title: End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems
arxiv_id: '2310.18956'
source_url: https://arxiv.org/abs/2310.18956
tags:
- reply
- replies
- star
- dataset
- message
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes STAR, an autoregressive text-to-text retrieval
  model for smart reply systems. STAR learns the smart reply task end-to-end from
  a bootstrapped dataset of (message, reply set) pairs.
---

# End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems

## Quick Facts
- arXiv ID: 2310.18956
- Source URL: https://arxiv.org/abs/2310.18956
- Reference count: 12
- Key outcome: STAR model achieves 5.1%-17.9% improvement in relevance and 0.5%-63.1% improvement in diversity over baselines

## Executive Summary
This paper introduces STAR, an end-to-end autoregressive text-to-text retrieval model for smart reply systems. STAR learns to generate diverse sets of replies by conditioning each reply on both the initial message and previous replies in the set. The model is trained on a bootstrapped dataset created using a modified SimSR planning algorithm, which generates reply sets for each message. Empirical results demonstrate consistent improvements over state-of-the-art baselines across three dialogue datasets, with STAR achieving higher relevance and diversity in suggested replies.

## Method Summary
STAR is a T5-based text-to-text model that treats each reply as a unique token in its vocabulary. The model is trained end-to-end on a bootstrapped dataset of (message, reply set) pairs, generated using a modified SimSR planning algorithm. During training, STAR learns to predict sequences of replies conditioned on both the message and previous replies. Inference is performed via greedy decoding, with the model outputting up to K replies. Reply embeddings are initialized using a bag-of-words approach, averaging the embeddings of individual words within each reply.

## Key Results
- STAR achieves 5.1%-17.9% improvement in relevance compared to best baseline
- STAR achieves 0.5%-63.1% improvement in diversity compared to best baseline
- Consistent performance improvements across Reddit, PersonaChat, and DailyDialog datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive retrieval allows each reply to be conditioned on both the initial message and the previous replies in the set, enabling the model to learn to predict sequences of replies in a differentiable way.
- Mechanism: The STAR model is trained to output sequences of replies, where each reply is conditioned on the message and the previous replies. This allows the model to account for interdependencies between replies through end-to-end backpropagation.
- Core assumption: The autoregressive architecture can effectively capture the complex relationships between replies in a set, leading to more diverse and relevant suggestions.
- Evidence anchors:
  - [abstract] "STAR is a text-to-text model trained to output sequences of replies, where each reply is conditioned both on the initial message and the previous replies in the sequence."
  - [section 3.2] "We train the model using cross-entropy loss to predict the next reply given the current sequence of replies and messages."
- Break condition: If the autoregressive dependencies are too complex to model effectively, the model may struggle to generate coherent and diverse reply sets.

### Mechanism 2
- Claim: Bootstrapping creates a high-quality dataset of (message, reply set) pairs, which is critical for training the autoregressive retrieval model.
- Mechanism: The offline dataset creation process uses a modified version of SimSR to generate reply sets for each message in the training data. This process leverages query augmentation and a redundancy penalty to improve the quality and diversity of the generated reply sets.
- Core assumption: The bootstrapped dataset captures the complex relationships between messages and reply sets, providing a suitable training signal for the autoregressive model.
- Evidence anchors:
  - [abstract] "we present both (i) a bootstrapping method for creating a high-quality dataset of (message, reply sets) and (ii) a novel autoregressive retrieval model which predicts sequences of replies."
  - [section 3.1] "While our method is general to any arbitrary planning algorithm, we choose to instantiate our approach with a modified version of SimSR (Towle and Zhou, 2023), a recently released publicly available state-of-the-art SR method, that employs model-based simulation to predict reply sets."
- Break condition: If the bootstrapping process fails to generate high-quality reply sets that capture the true diversity of possible responses, the model may learn to produce suboptimal or repetitive suggestions.

### Mechanism 3
- Claim: Treating each reply as a unique token in the vocabulary, combined with a bag-of-words initialization strategy, allows the model to leverage its existing semantic priors while learning to compose novel reply sets.
- Mechanism: The STAR model's vocabulary is expanded to include each reply as a token. The reply embeddings are initialized using a bag-of-words approach, averaging the embeddings of the individual words within the reply. This initialization allows the model to start with some semantic understanding of the replies while still learning to compose novel sets during training.
- Core assumption: The bag-of-words initialization provides a reasonable starting point for the reply embeddings, allowing the model to learn more effectively from the bootstrapped dataset.
- Evidence anchors:
  - [section 3.2] "We expand T5's vocabulary by treating each reply in the candidate pool as a novel token, and demonstrate a simple-yet-effective technique for initialising the new token embeddings, which leverages the model's existing semantic priors."
  - [section 3.2] "Intuitively, this ensures that the initial embeddings are close to the word embeddings of the original vocabulary, while also capturing some of the underlying semantics of the reply."
- Break condition: If the bag-of-words initialization fails to capture the essential semantics of the replies, or if the vocabulary becomes too large to manage effectively, the model's performance may degrade.

## Foundational Learning

- Concept: Autoregressive models
  - Why needed here: The STAR model uses an autoregressive architecture to predict sequences of replies, conditioning each reply on the previous ones in the set.
  - Quick check question: How does the autoregressive nature of the STAR model differ from traditional retrieval-based approaches for smart reply systems?

- Concept: Bootstrapping
  - Why needed here: The bootstrapping process creates a high-quality dataset of (message, reply set) pairs, which is critical for training the autoregressive retrieval model.
  - Quick check question: What are the key components of the bootstrapping process used to generate the training data for the STAR model?

- Concept: Semantic embeddings
  - Why needed here: The STAR model leverages semantic embeddings for both the input messages and the replies, allowing it to capture the meaning and context of the conversation.
  - Quick check question: How does the bag-of-words initialization strategy for the reply embeddings help the model leverage its existing semantic priors?

## Architecture Onboarding

- Component map:
  - Input: Message and previous replies in the set
  - Model: T5-based text-to-text model with expanded vocabulary (individual replies as tokens)
  - Output: Next reply in the sequence
  - Training: Cross-entropy loss on bootstrapped dataset of (message, reply set) pairs

- Critical path:
  1. Initialize STAR model with T5 backbone and expanded vocabulary
  2. Bootstrap dataset of (message, reply set) pairs using modified SimSR
  3. Train STAR model on bootstrapped dataset using cross-entropy loss
  4. During inference, greedily decode the next reply token conditioned on the message and previous replies

- Design tradeoffs:
  - Autoregressive vs. non-autoregressive: Autoregressive allows for better conditioning on previous replies but may be slower at inference time
  - Bag-of-words initialization vs. random initialization: Bag-of-words leverages semantic priors but may not capture all nuances of the replies
  - Expanded vocabulary vs. original vocabulary: Expanded vocabulary allows for more specific replies but increases model size and complexity

- Failure signatures:
  - Repetitive or irrelevant replies: Could indicate issues with the bootstrapping process or the autoregressive conditioning
  - Poor diversity in suggested replies: Could suggest the model is not effectively capturing the range of possible responses
  - Slow inference times: Could indicate the autoregressive decoding is too computationally expensive

- First 3 experiments:
  1. Train STAR on a small bootstrapped dataset and evaluate the quality and diversity of the generated reply sets
  2. Compare the performance of STAR with and without the bag-of-words initialization strategy
  3. Ablate the bootstrapping process by removing query augmentation or the redundancy penalty and observe the impact on model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed STAR model perform on proprietary smart reply datasets from real-world email and chat applications, as opposed to the publicly available dialogue datasets used in this study?
- Basis in paper: [inferred] The paper mentions that proprietary conversations in chat and email applications may have unique features not accounted for in the publicly available datasets used in the study, such as timestamps, cc and bcc information, and file attachments.
- Why unresolved: The study only evaluates the STAR model on publicly available dialogue datasets (Reddit, PersonaChat, and DailyDialog), which may not fully represent the characteristics of real-world smart reply data.
- What evidence would resolve it: Evaluating the STAR model on proprietary smart reply datasets from real-world email and chat applications, and comparing its performance to the results obtained on the publicly available datasets used in the study.

### Open Question 2
- Question: How would the performance of the STAR model be affected by using alternative strategies for initializing the reply embeddings, beyond the bag-of-words initialization demonstrated in this paper?
- Basis in paper: [explicit] The paper mentions that future work could consider alternative strategies for initializing the reply embeddings, beyond the bag-of-words initialization demonstrated in this study.
- Why unresolved: The study only demonstrates the use of bag-of-words initialization for the reply embeddings, and does not explore other potential initialization strategies.
- What evidence would resolve it: Experimenting with different initialization strategies for the reply embeddings, such as using pre-trained language models or domain-specific embeddings, and comparing the performance of the STAR model with these alternative initializations.

### Open Question 3
- Question: How would the performance of the STAR model be affected by increasing the number of replies in the candidate pool, beyond the 30k replies mentioned in the paper?
- Basis in paper: [inferred] The paper mentions that smart reply systems in deployment typically only need to retrieve from a pool of 30k or so replies to provide good coverage of possible user intents. However, it does not explore the potential impact of increasing the size of the candidate pool.
- Why unresolved: The study only considers a candidate pool of 30k replies, and does not investigate the potential benefits or drawbacks of using a larger candidate pool.
- What evidence would resolve it: Experimenting with larger candidate pools, such as 50k or 100k replies, and evaluating the performance of the STAR model on these larger pools to determine if there are any significant improvements in relevance or diversity of the suggested replies.

## Limitations

- The bootstrapping process relies on a modified SimSR algorithm whose exact implementation details are not fully specified in the paper
- The vocabulary expansion approach that treats each reply as a unique token could face scalability issues with larger datasets
- The evaluation methodology focuses on relevance and diversity metrics but does not address potential trade-offs between these objectives or practical utility in real-world applications

## Confidence

**High Confidence**: The core mechanism of autoregressive retrieval conditioned on previous replies is well-established in the literature and the paper provides clear technical specifications for the STAR model architecture, including the T5 backbone, vocabulary expansion, and bag-of-words initialization. The reported improvements over baselines (5.1%-17.9% for relevance, 0.5%-63.1% for diversity) are substantial and supported by empirical results across three datasets.

**Medium Confidence**: The bootstrapping methodology is described in sufficient detail to understand the general approach, but critical implementation details of the modified SimSR algorithm are missing. The effectiveness of the approach depends heavily on the quality of the bootstrapped training data, which cannot be fully assessed without reproducing the offline planning process.

**Low Confidence**: The practical implications of the reported improvements are unclear. The paper does not discuss whether the trade-off between relevance and diversity is optimal for real-world smart reply systems, nor does it address potential deployment challenges such as computational cost during inference or user acceptance of the generated reply sets.

## Next Checks

1. **Implement and validate the SimSR-based bootstrapping process**: Reconstruct the modified SimSR algorithm with the specified hyperparameters and verify that it produces high-quality reply sets. Test different configurations of query augmentation and redundancy penalty to understand their impact on the downstream model performance.

2. **Analyze vocabulary scaling behavior**: Systematically vary the size of the candidate pool and measure the impact on both model performance and computational requirements. Determine whether there is a point of diminishing returns where adding more replies to the vocabulary no longer improves performance but significantly increases model size.

3. **Evaluate trade-offs between relevance and diversity**: Design experiments that explicitly measure the relationship between relevance and diversity metrics. Use techniques like Pareto optimization to identify configurations that balance these competing objectives, and conduct user studies to validate whether the observed improvements translate to better user experience in practical settings.