---
ver: rpa2
title: 'RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior
  Predictability'
arxiv_id: '2309.00082'
source_url: https://arxiv.org/abs/2309.00082
tags:
- learning
- latent
- distribution
- dynamics
- repo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RePo, a method for visual model-based RL
  that learns representations resilient to spurious variations in dynamic environments.
  The core idea is to maximize mutual information between latent representations and
  future rewards while minimizing mutual information with observations, thereby discarding
  task-irrelevant information.
---

# RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability

## Quick Facts
- arXiv ID: 2309.00082
- Source URL: https://arxiv.org/abs/2309.00082
- Reference count: 40
- Primary result: RePo achieves higher returns faster than Dreamer in dynamic, cluttered environments with visual distractors

## Executive Summary
RePo introduces a novel approach to visual model-based reinforcement learning that learns representations resilient to spurious variations in dynamic environments. By maximizing mutual information between latent representations and future rewards while minimizing mutual information with observations, RePo discards task-irrelevant information. The method optimizes a variational lower bound on this objective using a recurrent state-space model without requiring pixel reconstruction, improving robustness to visual distractors. Experiments demonstrate RePo outperforms baselines like Dreamer in dynamic environments and enables rapid adaptation to new environments through a simple support-matching test-time adaptation scheme.

## Method Summary
RePo learns resilient visual representations by optimizing an information bottleneck objective that maximizes reward predictability while minimizing observation information. The method uses a recurrent state-space model with a visual encoder, latent dynamics, and reward predictor, training via dual gradient descent to balance KL divergence between posterior and prior distributions. Unlike reconstruction-based approaches, RePo directly optimizes reward prediction accuracy, avoiding the need to model complex visual elements. The method also includes a test-time adaptation mechanism that adapts only the visual encoder to new environments via a support constraint, enabling quick recovery of performance under distribution shift without retraining dynamics or policies.

## Key Results
- RePo outperforms Dreamer in dynamic, cluttered environments with visual distractors, achieving higher returns faster
- Test-time adaptation via support constraint enables quick adaptation to new environments without retraining dynamics or policies
- RePo's learned encoders can be adapted to new environments using a simple support-matching procedure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RePo discards task-irrelevant information by enforcing that latent representations must be predictable given past latents and actions, without conditioning on current observations.
- Mechanism: The method minimizes mutual information between latent representations and observations while maximizing mutual information with future rewards. By matching the posterior distribution (which conditions on current observations) to a prior distribution (which does not), RePo ensures that task-irrelevant variations that cannot be predicted from past information are excluded from the latent representation.
- Core assumption: Spurious variations are unpredictable given only past latents and actions.
- Evidence anchors:
  - [abstract]: "Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation."
  - [section 4]: "To match the latent posterior and the latent prior, the latent representation must omit these spurious variations."
  - [corpus]: No direct corpus evidence; related work on information bottleneck methods exists but not specific to RL.

### Mechanism 2
- Claim: RePo learns task-relevant dynamics without requiring pixel reconstruction, improving robustness to dynamic distractors.
- Mechanism: Instead of reconstructing observations to learn accurate dynamics, RePo directly optimizes reward prediction accuracy while maintaining predictive latent dynamics. This avoids the need to model complex, uncontrollable visual elements that would otherwise dominate reconstruction objectives.
- Core assumption: Accurate reward prediction and latent dynamics are sufficient for effective policy learning without explicit reconstruction.
- Evidence anchors:
  - [abstract]: "We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments."
  - [section 4]: "What distinguishes us from past work is a new desideratum of predictability: that, conditioned on past latents and actions, future latent dynamics should look as deterministic as possible."
  - [corpus]: No direct corpus evidence; related work on model-based RL exists but not specific to reconstruction-free approaches.

### Mechanism 3
- Claim: Test-time adaptation via support constraint enables quick adaptation to new environments without retraining dynamics or policies.
- Mechanism: The method adapts only the visual encoder to match the training-time latent distribution while keeping the learned dynamics and policy fixed. The support constraint ensures the adapted distribution is contained within the support of the training distribution, avoiding the need for exact distribution matching when exploration is limited.
- Core assumption: The learned latent dynamics and policy are invariant to changes in the visual encoder, and the support constraint is sufficient for adaptation.
- Evidence anchors:
  - [abstract]: "We propose a simple reward-free alignment procedure that enables test time adaptation of the encoder. This allows for quick adaptation to widely differing environments without having to relearn the dynamics and policy."
  - [section 4.1]: "We therefore propose to replace (4.5) with a support constraint, enforcing that the distribution of htest ◦ Dtest is contained in the support of htrain ◦ Dtrain."
  - [corpus]: No direct corpus evidence; related work on domain adaptation exists but not specific to this support constraint approach.

## Foundational Learning

- Concept: Information bottleneck
  - Why needed here: RePo uses an information bottleneck approach to constrain information flow from observations to latents, enabling the model to focus on task-relevant information.
  - Quick check question: What is the key difference between minimizing mutual information with observations versus minimizing reconstruction error?

- Concept: Variational inference
  - Why needed here: RePo uses variational methods to optimize intractable mutual information objectives by maximizing variational lower bounds.
  - Quick check question: How does the variational approximation in the information bottleneck objective differ from standard ELBO formulations?

- Concept: Model-based reinforcement learning
  - Why needed here: RePo is a model-based RL method that learns latent dynamics models for planning and policy learning, rather than directly learning policies from observations.
  - Quick check question: What are the advantages and disadvantages of model-based RL compared to model-free RL in visual domains?

## Architecture Onboarding

- Component map:
  Visual encoder (h) -> Latent encoder -> Latent dynamics model -> Reward predictor
  Visual encoder adapter (test-time) -> Reweighting function (test-time)

- Critical path:
  1. Encode observations using visual encoder
  2. Infer latent representations using recurrent state-space model
  3. Predict rewards and future latents
  4. Optimize mutual information objective with information bottleneck
  5. Learn policy using imagined rollouts from latent dynamics model

- Design tradeoffs:
  - Reconstruction vs. reward prediction: RePo avoids reconstruction to improve robustness but may sacrifice some dynamics accuracy
  - Information bottleneck strength: Too tight may lose task-relevant information; too loose may retain spurious variations
  - Adaptation scope: Adapting only the visual encoder preserves learned dynamics but may be insufficient for large distribution shifts

- Failure signatures:
  - Poor performance on dynamic distractors: Visual encoder captures too much task-irrelevant information
  - Slow adaptation: Support constraint insufficient or calibration samples inadequate
  - Degenerate representations: Information bottleneck too tight or KL balancing incorrect

- First 3 experiments:
  1. Verify mutual information objectives are optimized correctly by checking KL divergence between posterior and prior
  2. Test reconstruction-free dynamics accuracy by comparing predicted vs. actual rewards
  3. Validate test-time adaptation by measuring encoder adaptation speed on a simple domain shift task

## Open Questions the Paper Calls Out

- Can RePo be extended to multi-task or continual learning settings? The paper mentions this as a potential future direction, suggesting that a multi-task variant of RePo could allow for representations applicable to a distribution of tasks.

- How does the choice of the information bottleneck parameter (epsilon) affect RePo's performance and the quality of its learned representations? The paper mentions that performance is crucially dependent on the information bottleneck epsilon, but does not provide a detailed analysis of its effects.

## Limitations

- Test-time adaptation requires calibration data collection at test time, which may not be practical in all deployment scenarios
- The support constraint adaptation method is limited to specific types of domain shifts (background changes, camera angle changes)
- Performance sensitivity to information bottleneck hyperparameters (β, KL balancing, epsilon) is not extensively explored

## Confidence

**Confidence in claims about information bottleneck effectiveness: Medium**
- While the paper presents a theoretically sound information bottleneck formulation, the empirical validation focuses primarily on task performance rather than ablation studies isolating the bottleneck's contribution
- The choice of β learning rate, KL balancing ratio, and information bottleneck target (ϵ) are not extensively explored or justified across different tasks

**Confidence in claims about reconstruction-free dynamics: Medium**
- The paper claims avoiding pixel reconstruction improves robustness to dynamic distractors, but does not provide direct comparison with reconstruction-based baselines
- The theoretical justification assumes reconstruction objectives would capture task-irrelevant information, but empirical evidence is limited to performance comparisons

**Confidence in test-time adaptation claims: Low**
- The support constraint adaptation method is presented as a key contribution, but the evaluation is limited to specific types of domain shifts (background changes, camera angle changes)
- The method requires calibration data collection at test time, which may not be practical in all deployment scenarios

## Next Checks

1. **Ablation study on information bottleneck parameters**: Systematically vary β, KL balancing, and ϵ across multiple tasks to determine their sensitivity and optimal ranges, rather than presenting fixed values from grid search

2. **Direct comparison with reconstruction-based models**: Implement a reconstruction-based baseline using the same RSSM architecture but with observation reconstruction loss, then compare dynamics accuracy and robustness to distractors

3. **Stress test for test-time adaptation**: Evaluate adaptation performance under more challenging distribution shifts (object removal/addition, texture changes, lighting conditions) and measure the required amount of calibration data