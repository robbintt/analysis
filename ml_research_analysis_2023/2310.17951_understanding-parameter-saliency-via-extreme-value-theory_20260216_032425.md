---
ver: rpa2
title: Understanding Parameter Saliency via Extreme Value Theory
arxiv_id: '2310.17951'
source_url: https://arxiv.org/abs/2310.17951
tags:
- filters
- saliency
- parameter
- filter
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work reformulates parameter saliency ranking using extreme
  value theory (EVT), specifically the peaks-over-threshold (POT) method. The approach
  treats parameter saliency as a statistical anomaly detection problem, estimating
  the probability of observing extreme gradient values.
---

# Understanding Parameter Saliency via Extreme Value Theory

## Quick Facts
- arXiv ID: 2310.17951
- Source URL: https://arxiv.org/abs/2310.17951
- Reference count: 40
- Key outcome: Reformulates parameter saliency ranking using extreme value theory (POT method), showing improved performance on domain-shifted datasets and reduced layer depth bias

## Executive Summary
This paper introduces a novel approach to parameter saliency ranking using extreme value theory, specifically the peaks-over-threshold (POT) method with generalized Pareto distribution (GPD). The method treats parameter saliency as a statistical anomaly detection problem, estimating the probability of observing extreme gradient values for each filter. Experiments demonstrate that POT-saliency performs comparably to the baseline on standard ImageNet classification while outperforming it on domain-shifted data, successfully identifying salient filters across multiple layers rather than being biased toward deeper layers. The approach also enables effective one-step fine-tuning that corrects 12% of ImageNet misclassifications.

## Method Summary
The method computes filter-wise saliency profiles using gradient norms normalized per filter over a validation set. For each filter, GPD parameters are estimated via maximum likelihood estimation using Grimshaw's trick. When a misclassification occurs, POT-based saliency scores are calculated as tail probabilities for filters exceeding a threshold (typically 90th percentile). Filters are ranked by ascending probability to identify the most anomalous filters, which can then be fine-tuned or pruned. The approach is evaluated on ImageNet, MNIST→SVHN, and PACS datasets using ResNet and VGG architectures.

## Key Results
- POT-saliency performs comparably to baseline on ImageNet classification but shows improved performance on domain-shifted data
- Method successfully identifies salient filters across multiple layers, reducing layer depth bias present in the baseline approach
- One-step fine-tuning of top-25 filters detected by POT-saliency corrects 12% of ImageNet misclassifications
- POT method reduces incorrect class confidence by up to 6% on domain-shifted data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: POT-saliency reformulates parameter saliency as statistical anomaly detection using extreme value theory.
- Mechanism: The method treats gradient magnitudes of convolutional filters as extreme value data, estimating the probability of observing extreme gradients via the peaks-over-threshold (POT) method with generalized Pareto distribution (GPD).
- Core assumption: Gradient magnitudes of filters causing misclassification follow heavy-tailed distributions that can be modeled with GPD.
- Evidence anchors: [abstract] "we reformulate parameter saliency in terms of the POT method, where this reformulation is regarded as statistical anomaly detection"; [section 4.3] "we reformulate the rarity of each filter's saliency profile according to the probability P(X > x) by using the POT method"
- Break condition: If gradient magnitudes are not heavy-tailed or do not follow GPD well, POT estimates become unreliable.

### Mechanism 2
- Claim: POT-saliency eliminates layer depth bias present in the original z-score normalization method.
- Mechanism: By modeling each filter's gradient distribution separately with GPD and estimating tail probabilities, POT avoids the z-score normalization's dependence on layer-wise mean and variance patterns that create bias toward deeper layers.
- Core assumption: The original method's bias stems from z-score normalization's sensitivity to layer-wise gradient magnitude differences.
- Evidence anchors: [abstract] "parameter saliency based on POT shows less of this bias" and "the existing parameter saliency method exhibits a bias against the depth of layers"; [section 5.3] "we divide the saliency profile by std when calculating the score, and this operation is presumably what introduces the bias"
- Break condition: If GPD parameter estimation is poor for filters in certain layers, POT may still show bias.

### Mechanism 3
- Claim: POT-saliency maintains or improves performance on domain-shifted data compared to the baseline method.
- Mechanism: By focusing on extreme gradient events rather than normalized values, POT better identifies filters that are truly anomalous for a given misclassified input, regardless of the training domain.
- Core assumption: Domain shift causes different filters to become responsible for misclassification, and extreme value detection is more robust to domain changes.
- Evidence anchors: [abstract] "the POT method operates well even when domain shift occurs, while an intrinsic bias in the baseline method prevents consistent performance"; [section 5.2] "the POT method showed a better performance than the baseline method" on MNIST→SVHN task
- Break condition: If domain shift fundamentally changes the relationship between gradient magnitude and filter importance, POT may fail to identify relevant filters.

## Foundational Learning

- Concept: Extreme Value Theory (EVT) and Generalized Pareto Distribution (GPD)
  - Why needed here: EVT provides the mathematical framework for modeling extreme gradient values without assuming normality, which is the core innovation of POT-saliency
  - Quick check question: What are the two main theorems in EVT and how do they relate to modeling extreme values in data?

- Concept: Statistical anomaly detection using tail probabilities
  - Why needed here: POT-saliency treats parameter saliency as the probability of observing extreme events, which requires understanding how tail probabilities indicate anomalies
  - Quick check question: How does comparing tail probabilities differ from comparing z-scores in terms of interpretability and assumptions?

- Concept: Convolutional neural network architecture and filter operations
  - Why needed here: Understanding how filters extract features and how their gradients relate to classification decisions is essential for interpreting POT-saliency results
  - Quick check question: Why does zeroing out a convolutional filter affect the output while pruning a linear layer does not?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Calculate gradient magnitudes for all filters on validation set
  - Parameter estimation: Fit GPD parameters (ξ, σ̂) for each filter using Grimshaw's trick
  - Misclassification analysis: For each misclassified sample, compute saliency profiles and threshold exceedances
  - Probability calculation: Estimate P(X > x) for exceeding filters using GPD
  - Ranking: Sort filters by ascending probability to get most anomalous filters

- Critical path: Validation set gradient computation → GPD parameter estimation → Misclassification processing → Probability estimation → Filter ranking → Fine-tuning/pruning

- Design tradeoffs:
  - Threshold selection: 90th percentile vs adaptive threshold - higher threshold reduces false positives but may miss subtle anomalies
  - GPD assumptions: Requires sufficient exceedances (typically 10-20%) for reliable parameter estimation
  - Computational cost: Fitting GPD for each filter vs using global parameters

- Failure signatures:
  - Poor parameter estimation indicated by flat likelihood surfaces or convergence issues
  - Inconsistent rankings across similar misclassifications suggesting threshold problems
  - No improvement from fine-tuning top-ranked filters indicating misalignment between anomaly detection and actual importance

- First 3 experiments:
  1. Verify POT-saliency reproduces baseline performance on ImageNet without domain shift
  2. Test sensitivity to threshold selection by varying percentile from 80-95 and observing ranking stability
  3. Compare POT-saliency with random filter selection on domain-shifted data to confirm improved performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific assumptions about the distribution of gradient magnitudes underlie the baseline parameter saliency method, and how do these assumptions affect its performance in different domains?
- Basis in paper: [explicit] The paper states that the baseline method assumes gradient norms follow a normal distribution and shows this assumption may be unrealistic in practice.
- Why unresolved: The paper demonstrates that the POT method outperforms the baseline in domain-shifted settings, suggesting the normality assumption may be too restrictive, but doesn't explore alternative distributional assumptions for the baseline.
- What evidence would resolve it: Systematic comparison of baseline method performance under different distributional assumptions (e.g., log-normal, heavy-tailed distributions) across multiple domain shift scenarios.

### Open Question 2
- Question: How does the effectiveness of POT-based saliency ranking scale with model size and architecture complexity beyond the tested ResNet and Vision Transformer models?
- Basis in paper: [inferred] The paper tests POT saliency on ResNet-50, ResNet-18, VGG19, and Vision Transformer, showing consistent performance improvements, but doesn't explore very large-scale models.
- Why unresolved: The paper demonstrates POT saliency works across different architectures, but doesn't investigate whether the method maintains its advantages as model complexity increases or when applied to specialized architectures like transformers for vision tasks.
- What evidence would resolve it: Empirical evaluation of POT saliency on state-of-the-art large vision models (e.g., Swin Transformer, ConvNeXt) across diverse datasets and domain shifts.

### Open Question 3
- Question: Can the POT method be extended to incorporate temporal or sequential dependencies in filter saliency, particularly for video understanding tasks?
- Basis in paper: [inferred] The paper focuses on static image classification tasks and treats saliency as a spatial property of filters, without considering temporal dynamics.
- Why unresolved: While the paper demonstrates POT saliency's effectiveness for image classification, it doesn't explore how the method might adapt to tasks where temporal coherence is important, such as action recognition or video segmentation.
- What evidence would resolve it: Development and validation of a temporal extension of POT saliency that accounts for filter behavior across video frames, tested on video datasets with domain shift.

## Limitations
- The method assumes gradient magnitudes follow heavy-tailed distributions suitable for GPD modeling, which hasn't been validated across all architectures and datasets
- Fixed 90th percentile threshold for exceedance selection may not be optimal across different scenarios and architectures
- Computational cost of fitting individual GPDs for thousands of filters per layer raises scalability concerns for larger models

## Confidence
**High Confidence**: The experimental setup and methodology are clearly described. The POT method's theoretical foundation in EVT is well-established. Results showing reduced layer bias and improved domain shift performance are reproducible given the described procedures.

**Medium Confidence**: The claim that POT-saliency maintains performance while reducing layer bias is supported but could benefit from additional ablation studies. The 12% correction rate on ImageNet, while promising, represents a single-step result that may not generalize to multi-step optimization.

**Low Confidence**: The assumption that GPD provides superior modeling of filter saliency distributions compared to other heavy-tailed distributions isn't validated. The selection of 90th percentile threshold lacks justification beyond empirical performance.

## Next Checks
1. **Distribution Validation**: Systematically test whether filter gradient magnitudes actually follow GPD distributions across different layers, architectures, and datasets. Compare goodness-of-fit metrics with alternative heavy-tailed distributions.

2. **Threshold Sensitivity Analysis**: Vary the exceedance threshold from 80-95 percentile and measure impacts on parameter estimation quality, ranking stability, and downstream performance. Identify if there's an optimal threshold range or if adaptive thresholding would be superior.

3. **Computational Scalability Assessment**: Measure wall-clock time for GPD parameter estimation across increasing numbers of filters and layers. Compare against alternative methods like using global parameters or approximate fitting techniques to establish practical limitations.