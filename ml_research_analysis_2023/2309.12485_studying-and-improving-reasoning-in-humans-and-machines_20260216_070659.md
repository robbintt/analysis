---
ver: rpa2
title: Studying and improving reasoning in humans and machines
arxiv_id: '2309.12485'
source_url: https://arxiv.org/abs/2309.12485
tags:
- page
- palminteri
- anllo
- reasoninginhumansandmachines
- bill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared reasoning in humans and large language models
  (LLMs) using cognitive psychology tools. New variants of classical cognitive experiments
  were presented to human participants and LLMs, and their performances were cross-compared.
---

# Studying and improving reasoning in humans and machines

## Quick Facts
- arXiv ID: 2309.12485
- Source URL: https://arxiv.org/abs/2309.12485
- Reference count: 0
- Key outcome: Large language models exhibit reasoning biases similar to humans but show super-human performance in some tasks and respond differently to prompting strategies.

## Executive Summary
This study compares reasoning abilities between humans and large language models using cognitive psychology tools like the Cognitive Reflection Test and Linda/Bill problems. Researchers created new variants of classical cognitive experiments to avoid contamination issues and tested both human participants and multiple GPT model variants. While most models displayed reasoning errors similar to those in humans, recent LLM releases showed significantly improved performance with almost no reasoning limitations. The study reveals important differences in how humans and machines respond to prompting strategies, suggesting that while LLMs can exhibit human-like biases, their reasoning processes differ fundamentally from human cognition.

## Method Summary
The researchers created new variants of classical cognitive psychology experiments (Cognitive Reflection Test and Linda/Bill problems) to avoid contamination from training data. They collected responses from human participants recruited online via Prolific and from multiple GPT models (DV family, ChatGPT, GPT-4) using OpenAI's API. Responses were analyzed using generalized linear mixed models with random intercepts per participant, and log-likelihood analysis was employed to detect potential contamination. Prompt engineering conditions included Baseline, Reasoning, and Example approaches, with temperature=0.7 used to introduce variability in model responses.

## Key Results
- Recent LLM releases (ChatGPT, GPT-4) showed almost no reasoning limitations compared to earlier models
- Humans and LLMs responded differently to the same prompting strategies
- More refined models (e.g., DV2, DV3) were more prone to intuitive responses compared to simpler models
- Log-likelihood analysis detected contamination issues with certain models on specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models trained on human-generated text inherit human-like cognitive biases through statistical co-occurrence patterns
- Mechanism: During training, language models capture probabilistic relationships between words and concepts as they appear in human-authored text. When these relationships reflect systematic reasoning errors (like conjunction fallacy), the model learns to reproduce them when generating completions
- Core assumption: The training corpus contains sufficient instances of human reasoning errors for the model to learn their statistical patterns
- Evidence anchors: [abstract] "human performance could deviate from the normative prescriptions derived from statistics and decision-theory"; [section] "human reasoning, soaked with bounded rationality and biased reasoning, translates into written language, which is then captured by the model as it is"

### Mechanism 2
- Claim: Fine-tuning with human feedback can amplify or suppress cognitive biases present in the base model
- Mechanism: During fine-tuning, human raters provide preference labels that reflect human judgments about response quality. If these judgments are themselves biased, the fine-tuning process learns to reproduce those biases by adjusting model weights to maximize alignment with human preferences
- Core assumption: Human feedback reflects the same cognitive biases that appear in the pretraining corpus, or introduces new ones
- Evidence anchors: [section] "it is plausible to consider that certain cognitive biases may have been introduced through this interaction and tuning process, as these are inherently present in the human mind"; [corpus] "recent models (ChatGPT and GPT-4) displayed above-human performance when compared to our sample of human participants"

### Mechanism 3
- Claim: Model architecture and scale influence the ability to overcome learned biases through improved reasoning capacity
- Mechanism: Larger models with more parameters and training data can better represent complex reasoning patterns, allowing them to recognize and correct for statistical biases that simpler models cannot overcome. Architectural improvements may enable more deliberate, analytical processing
- Core assumption: Model size and architecture improvements directly translate to enhanced reasoning capabilities rather than just pattern matching
- Evidence anchors: [abstract] "recent LLM releases showed almost no reasoning limitations"; [section] "more refined models (e.g., DV2, DV3) were more prone to fall into intuitive responses compared to simpler ones"

## Foundational Learning

- Concept: Log-likelihood analysis for detecting model exposure to specific content
  - Why needed here: To determine whether a model's responses reflect genuine reasoning versus memorization of training data
  - Quick check question: Can you explain how a flat log-likelihood curve indicates the model has seen the content before?

- Concept: Chain-of-thought prompting and in-context learning
  - Why needed here: To understand how simple prompting strategies can improve model reasoning performance
  - Quick check question: What's the difference between chain-of-thought prompting and in-context learning in terms of how they affect model behavior?

- Concept: Jensen-Shannon divergence for model comparison
  - Why needed here: To quantify differences between models when direct access to internal representations is unavailable
  - Quick check question: How does JS divergence differ from KL divergence in terms of symmetry and interpretation?

## Architecture Onboarding

- Component map: OpenAI API -> GPT model variants (DV, DVB, DV1, CDV2, DV2, DV3, ChatGPT, GPT-4) -> response classification -> R-based statistical models (GLMMs) -> visualization
- Critical path: Prompt generation → API response collection → response classification → statistical analysis → visualization of results
- Design tradeoffs: Using temperature=0.7 for model responses balances exploration of response distribution against reproducibility; multiple model iterations per prompt account for stochasticity but increase computational cost
- Failure signatures: Contamination issues (model memorizing test items), temperature settings that don't adequately explore response space, statistical models that don't account for hierarchical structure of repeated measures
- First 3 experiments:
  1. Verify contamination detection by running log-likelihood analysis on known memorized vs novel content
  2. Test prompting strategies (chain-of-thought vs examples) on a simple reasoning task to establish baseline effects
  3. Compare model responses at different temperature settings to determine optimal exploration-exploitation balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do fine-tuning methods (e.g., supervised vs. reinforcement learning) influence the emergence of cognitive biases in LLMs?
- Basis in paper: [explicit] The paper suggests that more refined models (e.g., DV3) displayed increased intuitive responses compared to simpler ones, and posits that biases may be introduced through human feedback-based fine-tuning
- Why unresolved: The exact mechanisms by which different fine-tuning approaches contribute to bias development remain unclear, and the paper calls for further research to disentangle these effects
- What evidence would resolve it: Controlled experiments comparing the performance of LLMs fine-tuned using different methods on the same reasoning tasks, while controlling for model size and training data

### Open Question 2
- Question: To what extent do the reasoning processes of LLMs differ from those of humans, beyond surface-level performance metrics?
- Basis in paper: [explicit] The paper notes that humans and LLMs responded differently to prompting strategies, and that even when signs of heuristic reasoning were detected in LLMs, these were not necessarily elicited by the same items for all agents
- Why unresolved: The paper highlights the complexity of comparing human and machine reasoning and calls for more in-depth analyses to understand the underlying cognitive processes
- What evidence would resolve it: Detailed behavioral analyses, such as eye-tracking or response time studies, comparing human and LLM performance on the same reasoning tasks

### Open Question 3
- Question: Can LLMs be effectively used as models of human reasoning, or do their unique characteristics limit their applicability in this domain?
- Basis in paper: [inferred] The paper raises the question of whether LLMs can serve as useful tools to understand the computational foundations of decision-making, heuristics, and biases in humans, but also notes the significant variability observed between models and the potential for contamination issues
- Why unresolved: The paper suggests that while LLMs may exhibit some human-like biases, their overall performance and response patterns differ substantially from humans, making it challenging to draw direct parallels
- What evidence would resolve it: Comparative studies examining the performance of LLMs and humans on a wide range of reasoning tasks, while accounting for factors such as training data, model architecture, and task presentation

## Limitations

- Corpus contamination remains the primary limitation - despite efforts to create novel variants of classical cognitive tasks, we cannot definitively rule out that models may have encountered similar reasoning patterns during training
- Human sample representativeness is uncertain - the Prolific participant pool may not capture the full diversity of human reasoning patterns across cultures or educational backgrounds
- Hidden prompting mechanisms in ChatGPT/GPT-4 introduce unknown variables that may systematically affect reasoning performance independent of the model's inherent capabilities

## Confidence

- High confidence: Claims about differential responses to prompting strategies between humans and machines
- Medium confidence: Claims about models exhibiting human-like reasoning biases
- Low confidence: Claims about fundamental architectural differences driving performance gaps

## Next Checks

1. **Cross-cultural replication**: Test the same reasoning tasks with participants from diverse geographic regions to assess whether observed human-model differences generalize across cultural contexts of reasoning
2. **Controlled contamination study**: Systematically expose models to varying degrees of task-relevant content during controlled fine-tuning to establish causal links between training exposure and reasoning performance
3. **Ablation analysis**: Compare reasoning performance across model variants with systematically varied architectural features (size, attention mechanisms) while holding training data constant to isolate architectural contributions