---
ver: rpa2
title: Language Models for Business Optimisation with a Real World Case Study in Production
  Scheduling
arxiv_id: '2309.13218'
source_url: https://arxiv.org/abs/2309.13218
tags:
- problem
- formulation
- optimisation
- formulations
- code-generating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AI-Copilot, an LLM-based framework for automating
  problem formulation in business optimization. The approach uses fine-tuning of a
  pre-trained code-generating LLM with a specialized dataset for production scheduling,
  and applies modularization and prompt engineering techniques to handle large formulations
  within token limits.
---

# Language Models for Business Optimisation with a Real World Case Study in Production Scheduling

## Quick Facts
- arXiv ID: 2309.13218
- Source URL: https://arxiv.org/abs/2309.13218
- Reference count: 14
- Primary result: Introduces AI-Copilot, an LLM-based framework for automating problem formulation in business optimization, achieving accurate generation of complex optimization formulations.

## Executive Summary
This paper presents AI-Copilot, a novel framework that leverages fine-tuned large language models to automate problem formulation for business optimization tasks. The approach addresses the challenge of generating accurate optimization formulations from natural language problem descriptions by fine-tuning a code-generating LLM with a specialized dataset for production scheduling. The framework introduces modularization and prompt engineering techniques to handle complex formulations within token limits, enabling non-expert users to solve optimization problems efficiently.

## Method Summary
The method involves fine-tuning a pre-trained code-generating LLM (CodeRL) with a small dataset of 100 production scheduling problem-description/formulation pairs. Problem formulations are modularized into nine instruction-based components to fit within token limits. The framework uses performance evaluation metrics including training loss, success rate, failure rate, and execution-based metrics to assess the accuracy and quality of generated formulations.

## Key Results
- Success rates improved significantly with increased training epochs (1 to 8 epochs)
- Training and validation losses converged, indicating stable learning
- Outperformed existing prompt engineering approaches on general linear programming problems

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a pre-trained code-generating LLM with a small dataset of problem descriptions and formulations improves accuracy for business optimization problem formulation. The pre-trained model is adapted to generate correct optimization code from problem descriptions.

### Mechanism 2
Modularizing problem formulations and using prompt engineering techniques allows the LLM to handle complex formulations within token limits. Problem formulations are broken into nine modules that fit within token limits and can be combined into a complete formulation.

### Mechanism 3
The proposed performance evaluation metrics provide a more accurate assessment of problem formulation quality than existing metrics. Evaluation includes both training loss and execution-based metrics to capture syntactic and functional correctness.

## Foundational Learning

- Concept: Job Shop Scheduling (JSS) problem and its mathematical formulation
  - Why needed here: The case study uses JSS to demonstrate the framework's capabilities
  - Quick check question: What are the key components of a JSS problem formulation?

- Concept: Large Language Models (LLMs) and their architecture (e.g., transformers)
  - Why needed here: The framework relies on fine-tuning a pre-trained LLM
  - Quick check question: How do transformers work in the context of code generation?

- Concept: Prompt engineering and its role in guiding LLM output
  - Why needed here: Modularization and prompt engineering are key techniques used to manage token limits
  - Quick check question: How can prompts be designed to elicit specific outputs from an LLM?

## Architecture Onboarding

- Component map: Problem Description -> Code-Generating LLM (fine-tuned) -> Problem Formulation -> Solution -> Presentation
- Critical path: Problem Description -> Code-Generating LLM -> Problem Formulation -> Solution
- Design tradeoffs: Using a smaller pre-trained model (CodeRL) for efficiency vs. a larger model for potentially better performance; modularization vs. generating entire formulations at once
- Failure signatures: Incorrect or incomplete problem formulations, failure to solve the generated formulations, or poor convergence during training
- First 3 experiments:
  1. Fine-tune the LLM on a small subset of the dataset and evaluate its ability to generate correct formulations for those specific problems
  2. Test the modularization approach by generating each module separately and then combining them to form complete formulations
  3. Evaluate the performance metrics by comparing the generated formulations to the target formulations and checking if they solve correctly

## Open Questions the Paper Calls Out

### Open Question 1
How well does AI-Copilot generalize to optimization problems outside of production scheduling? The framework was only tested on production scheduling problems, limiting understanding of its broader applicability.

### Open Question 2
How does the modularization approach scale to extremely large problem formulations that exceed current token limits? The paper mentions modularization to handle large formulations but doesn't explore limits of this approach.

### Open Question 3
What is the optimal balance between fine-tuning data size and model performance for different problem types? The study used a fixed dataset size without exploring how varying the number of training examples impacts accuracy.

## Limitations

- The approach relies on a relatively small dataset of 100 manually created problem descriptions, which may limit generalizability
- The modularization approach's effectiveness depends on seamless combination of modules without introducing errors
- Evaluation metrics lack comparison to established benchmarks in the literature

## Confidence

- High Confidence: Fine-tuning a code-generating LLM can improve problem formulation accuracy for business optimization tasks
- Medium Confidence: Modularization and prompt engineering effectively manage token limitations
- Low Confidence: The proposed evaluation metrics are more suitable than existing ones

## Next Checks

1. Validate that the 9 modularized components generated by the LLM can be reliably combined into a coherent and correct problem formulation without introducing errors or inconsistencies

2. Test the framework on a larger and more diverse set of optimization problems beyond the 100 manually created job shop scheduling examples to assess generalizability

3. Compare the proposed evaluation metrics against established benchmarks in the literature to determine their relative effectiveness in assessing problem formulation quality