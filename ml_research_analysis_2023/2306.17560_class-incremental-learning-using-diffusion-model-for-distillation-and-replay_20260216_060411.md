---
ver: rpa2
title: Class-Incremental Learning using Diffusion Model for Distillation and Replay
arxiv_id: '2306.17560'
source_url: https://arxiv.org/abs/2306.17560
tags:
- learning
- incremental
- data
- synthetic
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the use of a pretrained Stable Diffusion model
  as a source of additional data for class-incremental learning. Compared to competitive
  methods that rely on external, often unlabeled, datasets of real images, their approach
  can generate synthetic samples belonging to the same classes as the previously encountered
  images.
---

# Class-Incremental Learning using Diffusion Model for Distillation and Replay

## Quick Facts
- **arXiv ID**: 2306.17560
- **Source URL**: https://arxiv.org/abs/2306.17560
- **Reference count**: 36
- **Primary result**: Combining LUCIR with Stable Diffusion synthetic data improves average incremental accuracy by up to 6.25 percentage points on ImageNet-Subset

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning by leveraging a pretrained Stable Diffusion model to generate synthetic images for previously seen classes. The synthetic images are used both for knowledge distillation and replay in classification loss, effectively augmenting the limited replay memory. Experiments show that combining this Stable Diffusion-based approach with existing methods like LUCIR significantly improves performance on CIFAR100, ImageNet-Subset, and ImageNet benchmarks, particularly under severe memory constraints.

## Method Summary
The approach generates synthetic images for previously learned classes using Stable Diffusion conditioned on class names and WordNet definitions. These synthetic images form an additional dataset S that supplements the limited replay memory. During training, batches are composed of half synthetic images from S and half real data from memory and current classes. The method is combined with existing CIL approaches (iCaRL and LUCIR) by incorporating synthetic data into both classification and distillation losses. The synthetic images are generated once and stored offline for efficient sampling during training.

## Key Results
- SDDR improves LUCIR by 6.25 percentage points on ImageNet-Subset with 20 exemplars/class
- Synthetic data for both distillation and replay is more effective than using it for either purpose alone
- Performance gains are most pronounced under severe memory constraints (5 exemplars/class)
- Combining SDDR with iCaRL improves performance by 3.29 percentage points on ImageNet-Subset

## Why This Works (Mechanism)

### Mechanism 1
Stable Diffusion generates synthetic samples from the same classes as previously encountered images, enabling both distillation and replay uses. The text-to-image diffusion model produces labeled synthetic images on demand for classes already seen by the incremental learner. Because the labels match past classes, these synthetic samples can replace real exemplars in both the knowledge distillation loss and the classification loss.

### Mechanism 2
Synthetic data can partially substitute for a limited replay memory, especially when memory per class is small. Each incremental step adds n synthetic samples per newly encountered class to dataset S. During training, half of each batch comes from S (synthetic) and half from the real replay memory plus new data. This increases the effective training signal for past classes without requiring more real exemplars.

### Mechanism 3
Combining synthetic data for both distillation and replay yields greater incremental accuracy gains than using it for either purpose alone. Distillation loss benefits from synthetic samples because they carry target labels and help retain knowledge of past classes. Replay in the classification loss benefits because synthetic samples provide additional gradient signals for fine-tuning the current model on past-class features. Together, they address both knowledge preservation and bias mitigation.

## Foundational Learning

- **Concept**: Class-incremental learning (CIL) and catastrophic forgetting
  - Why needed here: The paper's motivation is to mitigate forgetting while learning new classes sequentially without access to old data.
  - Quick check question: What is the primary challenge addressed by class-incremental learning, and why does it arise in sequential training?

- **Concept**: Knowledge distillation and replay memory
  - Why needed here: Standard CIL methods use distillation to retain old knowledge and replay memory to provide old-class examples during training.
  - Quick check question: In CIL, what roles do distillation loss and replay memory play in preserving performance on previously learned classes?

- **Concept**: Text-to-image generative models and conditioning
  - Why needed here: Stable Diffusion is used to generate synthetic labeled images by conditioning on class names and WordNet definitions.
  - Quick check question: How does Stable Diffusion generate images for a specific class, and why is adding the WordNet definition helpful?

## Architecture Onboarding

- **Component map**: Pretrained Stable Diffusion model -> Incremental learner (iCaRL/LUCIR) -> Replay memory (real exemplars) -> Synthetic dataset S (generated images) -> WordNet for prompt construction
- **Critical path**: 
  1. For each new class, generate n synthetic images using Stable Diffusion with class name + WordNet definition prompt.
  2. During each training batch, sample half from S (synthetic) and half from (memory âˆª new data).
  3. Update the incremental learner with both classification and distillation losses using the mixed batch.
  4. After training step, update memory and extend S with synthetic images for newly seen classes.
- **Design tradeoffs**:
  - Storage vs. online generation: Offline storage allows faster sampling but requires disk space; online generation avoids storage but adds latency and generation cost.
  - Number of synthetic samples per class (n): More samples can improve performance up to a point but increase storage and generation time.
  - Guidance scale: Lower values increase diversity but may reduce image quality; higher values improve fidelity but reduce variety.
- **Failure signatures**:
  - Degraded performance on past classes: May indicate synthetic-to-real domain gap too large or synthetic labels mismatched.
  - Overfitting to synthetic data: Could occur if synthetic samples outnumber real exemplars and model sees them too often.
  - Training instability: Could arise from batch mixing real and synthetic data with mismatched distributions.
- **First 3 experiments**:
  1. Baseline test: Run LUCIR with 20 exemplars/class, record average incremental accuracy.
  2. SDDR ablation: Run LUCIR + SDDR with same memory, compare accuracy and inspect forgetting on base classes.
  3. Memory size study: Repeat with 5, 10, and 20 exemplars/class to measure SDDR's impact under different memory constraints.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the quality of synthetic images generated by Stable Diffusion impact the performance of class-incremental learning models compared to using real images?
- **Open Question 2**: Can the generative models be fine-tuned during the training process to improve the performance of class-incremental learning models?
- **Open Question 3**: How can the different losses in class-incremental learning be modified to better utilize synthetic images and reduce the synthetic-to-real gap?

## Limitations
- The synthetic-to-real domain gap limits generalization, especially when synthetic images outnumber real exemplars in memory.
- Storage requirements for maintaining synthetic datasets S could become prohibitive at scale across multiple incremental steps.
- The approach assumes Stable Diffusion can generate semantically accurate images for all target classes, which may not hold for fine-grained or abstract concepts.

## Confidence
- **High Confidence**: Using synthetic data for replay in classification loss is well-established and clearly demonstrated through ablation studies.
- **Medium Confidence**: Combining synthetic data for both distillation and replay yields synergistic benefits, though exact contribution isolation is limited.
- **Low Confidence**: Claims about being "more data efficient" compared to external real datasets lack quantitative computational cost comparisons.

## Next Checks
1. Conduct systematic evaluation of synthetic data quality by measuring domain gap between generated and real images using FID or CLIP similarity scores across all datasets.
2. Perform ablation study isolating distillation vs. replay contributions with synthetic data under varying memory constraints (1, 5, 10 exemplars per class) to identify break points.
3. Test the approach on datasets with fine-grained classes (e.g., CUB-200) where Stable Diffusion might struggle with semantic distinctions to evaluate robustness across class types.