---
ver: rpa2
title: Automated Few-shot Classification with Instruction-Finetuned Language Models
arxiv_id: '2305.12576'
source_url: https://arxiv.org/abs/2305.12576
tags:
- aut-few
- answer
- choices
- prompts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of few-shot text classification
  by proposing AuT-Few, a method that automates prompt design for instruction-finetuned
  language models. The core idea is to eliminate the need for handcrafted prompts
  by using template retrieval from instruction-tuning knowledge bases and generating
  semantically meaningful class descriptions.
---

# Automated Few-shot Classification with Instruction-Finetuned Language Models

## Quick Facts
- arXiv ID: 2305.12576
- Source URL: https://arxiv.org/abs/2305.12576
- Authors: 
- Reference count: 40
- This paper automates prompt design for few-shot text classification using instruction-finetuned language models, achieving state-of-the-art performance without handcrafted prompts.

## Executive Summary
This paper addresses the challenge of few-shot text classification by proposing AuT-Few, a method that automates prompt design for instruction-finetuned language models. The core idea is to eliminate the need for handcrafted prompts by using template retrieval from instruction-tuning knowledge bases and generating semantically meaningful class descriptions. AuT-Few selects the most appropriate prompt configurations via cross-validation. Evaluated across 12 datasets and 8 classification tasks, AuT-Few achieves state-of-the-art performance without task-specific handcrafted prompts, outperforming current methods by 2.1 points on average. It also ranks best across datasets on the RAFT few-shot benchmark. The method demonstrates strong generalization capabilities on unseen tasks.

## Method Summary
AuT-Few automates few-shot text classification by retrieving relevant templates from instruction-tuning prompt collections, generating semantically meaningful answer choices, and selecting the best prompt configuration via cross-validation. The method uses template retrieval to find prompts matching task format, generates two types of answer choices (template-tailored and topic-specific), and evaluates configurations using 3-fold cross-validation based on F1 score and log-probability. The upstream model (T0-3B or BART0) is fine-tuned using LoRA with rescaling, and inference uses Monte Carlo approximation over templates. The approach eliminates handcrafted prompts while maintaining strong performance across diverse classification tasks.

## Key Results
- AuT-Few achieves state-of-the-art performance without task-specific handcrafted prompts
- Outperforms current methods by 2.1 points on average across 12 datasets and 8 classification tasks
- Ranks best across datasets on the RAFT few-shot benchmark
- Demonstrates strong generalization capabilities on unseen tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-finetuned language models exhibit low sensitivity to task-unspecific templates, meaning the template format is less important than previously assumed.
- Mechanism: The upstream model's training on diverse prompts allows it to generalize across varied template styles, reducing dependence on hand-crafted phrasing.
- Core assumption: Templates act primarily as scaffolding rather than as precise task descriptors.
- Evidence anchors:
  - [abstract] "instruction finetuned language models exhibit remarkable prompt robustness"
  - [section] "we observe that upstream models exhibit low variability towards task-unspecific templates"
- Break condition: If the upstream model lacks diverse prompt exposure during instruction tuning, template insensitivity may vanish.

### Mechanism 2
- Claim: Selecting semantically meaningful answer choices has a larger impact on downstream performance than the exact wording of the template.
- Mechanism: Answer choices define the output space and thus directly influence the model's ability to map inputs to correct classes; incorrect or vague choices degrade performance.
- Core assumption: The model's predictions are constrained by the provided answer choice vocabulary, regardless of template phrasing.
- Evidence anchors:
  - [abstract] "answer choices do not need to be tailored to the specific instruction"
  - [section] "the selection of answer choices makes a large difference in performance"
- Break condition: If answer choices are semantically empty or overly generic, performance collapses despite good templates.

### Mechanism 3
- Claim: Cross-validation over retrieved templates and automatically generated answer choices reliably selects the best prompt configuration without human intervention.
- Mechanism: By evaluating multiple prompt configurations on a subset of training data and scoring on held-out data, the system approximates optimal prompt design.
- Core assumption: Performance on small validation folds correlates with generalization to unseen test data.
- Evidence anchors:
  - [abstract] "AuT-Few selects the most appropriate prompt configurations via cross-validation"
  - [section] "We select the most appropriate template and answer choice configurations via cross-validation"
- Break condition: If training data is too small or noisy, cross-validation may not yield stable or meaningful rankings.

## Foundational Learning

- Concept: Prompt engineering for classification
  - Why needed here: Understanding how templates and answer choices affect model outputs is essential for automating prompt design.
  - Quick check question: Why does the wording of answer choices matter more than the wording of templates in this context?

- Concept: Retrieval-based template selection
  - Why needed here: AuT-Few relies on retrieving semantically relevant templates from the instruction-tuning corpus rather than generating new ones.
  - Quick check question: How does the retrieval step ensure the selected template matches the task format?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: AuT-Few uses LoRA and rescaling to adapt large upstream models without full fine-tuning, crucial for few-shot scenarios.
  - Quick check question: What is the trade-off between expressiveness and stability when combining LoRA with rescaling?

## Architecture Onboarding

- Component map:
  - Template Retrieval Module -> Answer Choice Generator -> Cross-Validation Selector -> Upstream Model Adapter -> Monte Carlo Inference Engine

- Critical path:
  1. Retrieve templates matching task format
  2. Generate answer choice candidates
  3. Evaluate configurations via cross-validation
  4. Fine-tune upstream model with selected prompt
  5. Perform inference with Monte Carlo approximation

- Design tradeoffs:
  - Template retrieval vs. generation: Retrieval is cheaper and leverages model familiarity but limits flexibility
  - Answer choice generation: Balancing semantic relevance with computational cost
  - Cross-validation folds: More folds increase stability but require more training data

- Failure signatures:
  - Poor template retrieval: Low diversity or irrelevance in retrieved prompts
  - Answer choice mismatch: Generated choices do not align with task semantics
  - Overfitting in cross-validation: Small folds yield unreliable configuration rankings

- First 3 experiments:
  1. Run template retrieval with a simple dataset and inspect top-5 retrieved prompts for relevance
  2. Generate answer choices for a single class and verify semantic coherence
  3. Evaluate a single prompt configuration on a held-out fold to test cross-validation scoring stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the size of the upstream model and the performance of AuT-Few, and how does this relationship change across different tasks?
- Basis in paper: [inferred] The paper shows that AuT-Few performs better with larger upstream models like Flan-T5 compared to smaller models like BART0, but the relationship is not quantified across all tasks.
- Why unresolved: The paper only provides a few examples of performance differences across upstream models, and does not systematically analyze the relationship between model size and performance across all tasks.
- What evidence would resolve it: A comprehensive analysis of AuT-Few's performance across a range of upstream model sizes for all tasks in the evaluation.

### Open Question 2
- Question: How does the performance of AuT-Few compare to other prompt-free methods on a wider range of tasks beyond the ones evaluated in the paper?
- Basis in paper: [explicit] The paper compares AuT-Few to SetFit, a prompt-free method, on a limited set of tasks, but does not evaluate it against other prompt-free methods or on a broader range of tasks.
- Why unresolved: The paper's evaluation is limited to a specific set of tasks and does not provide a comprehensive comparison to other prompt-free methods or across a wider range of tasks.
- What evidence would resolve it: A systematic comparison of AuT-Few to other prompt-free methods on a diverse set of tasks, including those not evaluated in the paper.

### Open Question 3
- Question: How does the choice of answer choices affect the performance of AuT-Few, and can this choice be further optimized?
- Basis in paper: [explicit] The paper discusses the importance of answer choices and presents two types of answer choices (template-tailored and topic-specific), but does not provide a detailed analysis of how the choice of answer choices affects performance or explore further optimization strategies.
- Why unresolved: The paper provides limited insights into the impact of answer choices on performance and does not explore potential optimization strategies beyond the two types presented.
- What evidence would resolve it: A detailed analysis of the impact of different answer choice strategies on AuT-Few's performance, including the exploration of additional optimization techniques.

## Limitations
- The method's reliance on PromptSource as the instruction-tuning corpus creates a dependency that may not generalize to other instruction datasets
- Cross-validation effectiveness is not rigorously tested across varying data regimes or edge cases
- The relationship between template insensitivity and upstream model properties remains underexplored

## Confidence

**High confidence** in the core empirical findings: The reported performance improvements of 2.1 points over baselines are supported by comprehensive evaluation across 12 datasets and 8 tasks, including strong results on the RAFT benchmark. The methodology for automated prompt design is clearly specified and reproducible.

**Medium confidence** in the mechanism claims: While the paper provides evidence that answer choice selection matters more than template phrasing, the exact relationship between these factors and their interaction with upstream model properties remains underexplored. The claim that templates act primarily as scaffolding requires more systematic ablation studies.

**Low confidence** in the generalizability of cross-validation effectiveness: The paper demonstrates cross-validation works for the tested datasets, but does not explore edge cases such as highly imbalanced classes, extremely few samples, or tasks that diverge significantly from the instruction-tuning distribution.

## Next Checks

1. **Template Sensitivity Boundary Test**: Systematically vary template complexity and domain alignment across a controlled set of tasks to identify when template insensitivity breaks down. Measure performance degradation as templates become increasingly task-irrelevant.

2. **Cross-Validation Stability Analysis**: Evaluate cross-validation reliability across different fold sizes and data regimes. Test whether the selected prompt configuration remains stable when training data is reduced below 16 samples per class.

3. **Answer Choice Robustness Study**: Generate answer choices with varying degrees of semantic coherence and specificity. Quantify the relationship between answer choice quality and downstream performance to establish minimum requirements for the generation process.