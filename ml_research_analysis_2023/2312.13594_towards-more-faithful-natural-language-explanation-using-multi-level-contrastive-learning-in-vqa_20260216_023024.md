---
ver: rpa2
title: Towards More Faithful Natural Language Explanation Using Multi-Level Contrastive
  Learning in VQA
arxiv_id: '2312.13594'
source_url: https://arxiv.org/abs/2312.13594
tags:
- explanations
- mcle
- answer
- explanation
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating faithful natural
  language explanations for visual question answering (VQA), where existing post-hoc
  methods often produce explanations that are logically inconsistent, factually incorrect,
  or insensitive to semantic perturbations. To tackle these issues, the authors propose
  a novel self-supervised Multi-level Contrastive Learning based natural language
  Explanation (MCLE) framework.
---

# Towards More Faithful Natural Language Explanation Using Multi-Level Contrastive Learning in VQA

## Quick Facts
- arXiv ID: 2312.13594
- Source URL: https://arxiv.org/abs/2312.13594
- Reference count: 12
- Key outcome: Novel MCLE framework achieves state-of-the-art performance on VQA-NLE benchmarks using multi-level contrastive learning

## Executive Summary
This paper addresses the challenge of generating faithful natural language explanations for visual question answering by proposing a multi-level contrastive learning framework. The authors identify three key problems with existing post-hoc explanation methods: deductive unsatisfiability, factual inconsistency, and semantic perturbation insensitivity. Their solution, MCLE, uses chain-of-thought generation combined with semantic-level, image-level, and instance-level contrastive learning to produce explanations that are more logically consistent, factually correct, and sensitive to semantic changes.

## Method Summary
The method introduces a vision-language model using GPT-2 with chain-of-thought generation, where explanations are generated before answers using special prefixes. A multi-level contrastive learning network operates at three granularities: semantic-level (explanation-answer alignment), image-level (explanation-image alignment), and instance-level (fine-grained object/word perturbations using Grad-CAM). The model is trained on VQA-NLE datasets with a combined loss of main VQA task and weighted contrastive losses, using factual and counterfactual samples to learn discriminative features.

## Key Results
- MCLE achieves state-of-the-art performance on VQA-X and A-OKVQA benchmarks
- Significant improvements in automatic metrics (SPICE, CIDEr) and human evaluation scores
- Chain-of-thought generation and multi-level contrastive learning reduce deductive unsatisfiability, factual inconsistency, and semantic perturbation insensitivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level contrastive learning aligns feature spaces of explanations with corresponding visual questions and answers.
- Mechanism: The model uses semantic-level, image-level, and instance-level contrastive losses to pull positive (factual) pairs closer and push negative (counterfactual) pairs apart in embedding space.
- Core assumption: The contrastive learning framework effectively discriminates between faithful and unfaithful explanations by modeling logical relationships across multiple granularities.
- Evidence anchors:
  - [abstract] "MCLE extracts discriminative features and aligns the feature spaces from explanations with visual question and answer to generate more consistent explanations."
  - [section] "In our multi-level CL network, three core modules are designed to learn high-quality representations to guide the model to generate faithful explanation, i.e., the semantic-level CL (SemanticCL) for deductive satisfiability, the image-level CL (ImageCL) for factual consistency, and the instance-level CL (InstanceCL) for semantic perturbation sensitivity."
  - [corpus] Weak - no direct evidence in neighbors about multi-level contrastive learning.
- Break condition: If the contrastive sampling strategy fails to produce meaningful negative pairs, the alignment process breaks down.

### Mechanism 2
- Claim: Chain-of-thought generation improves logical consistency between explanations and answers.
- Mechanism: The model generates explanations before answers using "because" and "so the answer is" prefixes, creating a rationale chain that logically connects explanation to answer.
- Core assumption: Structuring generation as a reasoning chain forces the model to produce explanations that can logically support the answer.
- Evidence anchors:
  - [abstract] "To reduce the inconsistency between explanations and visual question answers, we introduce the chain-of-thought generation for VQA-NLE, which can mimic a rationale leading to the answer and provide an interpretable window into the decision-making progress."
  - [section] "In our VL model, different from previous works, we consider the VQA-NLE task as a chain-of-thought (COT) generation task, where the answer is produced after the explanation."
  - [corpus] Missing - no evidence in neighbors about chain-of-thought strategies.
- Break condition: If the model learns to generate generic explanations that don't actually support the answer, the chain-of-thought structure becomes meaningless.

### Mechanism 3
- Claim: Instance-level contrastive learning improves sensitivity to semantic perturbations by focusing on fine-grained object and word contributions.
- Mechanism: Uses Grad-CAM-based counterfactual transformations to identify objects and words with highest contribution scores, creating factual samples from these and counterfactual samples by masking them.
- Core assumption: By contrasting explanations for perturbed vs. original samples, the model learns to recognize semantic changes caused by small perturbations.
- Evidence anchors:
  - [abstract] "the model can not recognize the semantic changes caused by small perturbations. These problems reduce the faithfulness of explanations generated by models."
  - [section] "In this module, a gradient-based counterfactual transformation strategy is adopted to synthesize factual and counterfactual samples. We apply the modified Grad-CAM to derive the contribution of the i-th object and the j-th word to answer."
  - [corpus] Weak - no direct evidence in neighbors about instance-level perturbation sensitivity.
- Break condition: If Grad-CAM gradients are noisy or uninformative, the perturbation-based contrastive samples won't effectively train semantic sensitivity.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: To learn discriminative features that distinguish faithful explanations from unfaithful ones by pulling positive pairs together and pushing negative pairs apart.
  - Quick check question: How does the contrastive loss function ensure that explanations stay close to their corresponding answers and images while staying far from counterfactual samples?

- Concept: Chain-of-Thought Reasoning
  - Why needed here: To structure the generation process so explanations logically precede and support answers, mimicking human reasoning.
  - Quick check question: What prefixes are used to structure the chain-of-thought generation, and how do they guide the model's reasoning process?

- Concept: Counterfactual Sampling
  - Why needed here: To create meaningful negative samples that represent unfaithful explanations for contrastive learning across all three levels.
  - Quick check question: What strategies are used to generate counterfactual samples at semantic, image, and instance levels?

## Architecture Onboarding

- Component map:
  Vision-Language Model (GPT-2 + CLIP) -> Multi-Level Contrastive Learning Network (SemanticCL, ImageCL, InstanceCL) -> Combined Loss (VQA loss + contrastive losses)

- Critical path:
  1. Input encoding: Image → CLIP, Question → GPT-2 embeddings
  2. Chain-of-thought generation: Generate explanation prefix + answer prefix
  3. Multi-level contrastive learning: Compute three contrastive losses
  4. Combined loss: Main VQA loss + weighted contrastive losses
  5. Output: Explanation and answer generation

- Design tradeoffs:
  - Single unified model vs. separate explanation and answer predictors
  - Trade-off parameters (α, β, γ) for balancing different contrastive losses
  - Number of negative samples per contrastive level (K values)

- Failure signatures:
  - High contrastive losses but low main VQA loss: contrastive learning not aligned with main task
  - Explanations unrelated to answers: SemanticCL not working
  - Explanations ignoring visual content: ImageCL not working
  - Insensitivity to perturbations: InstanceCL not working

- First 3 experiments:
  1. Ablation study removing chain-of-thought generation to verify its contribution
  2. Test with different negative sampling strategies in contrastive learning
  3. Evaluate model performance on semantic perturbation test set to measure InstanceCL effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further improve the logical consistency between explanations and answers in VQA-NLE, beyond the proposed multi-level contrastive learning approach?
- Basis in paper: [explicit] The paper identifies three main issues with existing post-hoc explanations: deductive unsatisfiability, factual inconsistency, and semantic perturbation insensitivity. It proposes a multi-level contrastive learning approach to address these issues.
- Why unresolved: While the proposed approach shows significant improvements, the paper does not explore other potential methods or combinations of techniques to further enhance logical consistency.
- What evidence would resolve it: Comparative studies of the proposed method with other state-of-the-art techniques, including different forms of contrastive learning, adversarial training, or other logical consistency-inducing methods.

### Open Question 2
- Question: Can the proposed multi-level contrastive learning approach be effectively applied to other vision-language tasks beyond VQA-NLE, such as image captioning or visual reasoning?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of multi-level contrastive learning in improving the logical consistency of explanations in VQA-NLE. This suggests that the approach might be applicable to other vision-language tasks that require logical reasoning.
- Why unresolved: The paper focuses specifically on VQA-NLE and does not investigate the potential application of the approach to other vision-language tasks.
- What evidence would resolve it: Experimental results showing the effectiveness of the proposed approach on other vision-language tasks, along with a comparison to existing methods for those tasks.

### Open Question 3
- Question: How can we develop more robust and reliable human evaluation methods for assessing the faithfulness and logicality of generated explanations in VQA-NLE?
- Basis in paper: [explicit] The paper mentions the use of human evaluation to measure the faithfulness and logicality of explanations, but it does not discuss the potential limitations or biases of this approach.
- Why unresolved: Human evaluation is subjective and can be influenced by various factors, such as the background and expertise of the evaluators. Developing more objective and reliable evaluation methods is an ongoing challenge in the field.
- What evidence would resolve it: Comparative studies of different human evaluation methods, including the use of crowdsourcing platforms, expert panels, and automated metrics, to determine the most reliable and consistent approach for assessing the quality of generated explanations.

## Limitations
- The implementation details for modified Grad-CAM in instance-level contrastive learning are not fully specified, making exact reproduction difficult.
- The paper lacks ablation studies to definitively quantify the contribution of each contrastive level and the chain-of-thought generation approach.
- The effectiveness of counterfactual sampling quality and its impact on contrastive learning performance is not thoroughly validated.

## Confidence
- **High Confidence**: The overall experimental setup and methodology are sound, with proper evaluation on established benchmarks (VQA-X, A-OKVQA) using standard metrics.
- **Medium Confidence**: The multi-level contrastive learning framework is theoretically sound, but implementation details are sparse, making full reproducibility challenging.
- **Medium Confidence**: The chain-of-thought generation approach shows promise, but the paper lacks ablation studies to definitively prove its contribution beyond correlation with improved metrics.

## Next Checks
1. **Contrastive Sample Quality Analysis**: Generate and analyze a sample of the counterfactual explanations produced at each level (semantic, image, instance) to verify they represent meaningful negative examples that challenge the model's faithfulness.

2. **Ablation Study on COT Generation**: Remove the chain-of-thought generation prefixes and retrain the model to quantify exactly how much the COT structure contributes to the reported improvements in logical consistency metrics.

3. **Semantic Perturbation Sensitivity Test**: Create a test set with controlled semantic perturbations (e.g., changing object attributes, spatial relationships) and measure whether the model's explanations actually show increased sensitivity as claimed by the InstanceCL mechanism.