---
ver: rpa2
title: 'SwissBERT: The Multilingual Language Model for Switzerland'
arxiv_id: '2303.13310'
source_url: https://arxiv.org/abs/2303.13310
tags:
- language
- swissbert
- https
- vocabulary
- romansh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SwissBERT is a masked language model adapted from a multilingual
  transformer for Switzerland-related text, covering German, French, Italian, and
  Romansh. It uses language adapters and a custom subword vocabulary to improve performance
  on Swiss news and Romansh tasks.
---

# SwissBERT: The Multilingual Language Model for Switzerland

## Quick Facts
- arXiv ID: 2303.13310
- Source URL: https://arxiv.org/abs/2303.13310
- Reference count: 40
- SwissBERT outperforms general multilingual models on named entity recognition in contemporary Swiss news and in zero-shot Romansh tasks, with F1 scores up to 83.7 and 95.6 in retrieval accuracy, respectively.

## Executive Summary
SwissBERT is a masked language model adapted from a multilingual transformer specifically for Switzerland-related text, covering German, French, Italian, and Romansh. The model uses language adapters and a custom subword vocabulary to improve performance on Swiss news and Romansh tasks. Through adaptive pre-training on contemporary Swiss news, SwissBERT achieves superior results on named entity recognition in modern Swiss contexts and demonstrates strong zero-shot capabilities on Romansh language tasks. The model shows moderate gains on user-generated political comment stance detection but has more modest performance on historical news data.

## Method Summary
SwissBERT extends the X-MOD multilingual transformer by adding language adapters for Swiss language variants (de_ch, fr_ch, it_ch, rm_ch) and creating a custom Switzerland-specific vocabulary. The model undergoes adaptive pre-training on 21M+ Swiss news articles (12B tokens) using masked language modeling, with language adapters frozen during downstream fine-tuning to enable cross-lingual transfer. Two vocabulary variants are created: one reusing XLM-R's 250k subwords and one with a new 50k SwissBERT vocabulary optimized for Swiss content. The model is evaluated on contemporary Swiss news NER, historical news NER, political stance detection, and Romansh alignment tasks.

## Key Results
- Achieves 83.7 F1 on named entity recognition in contemporary Swiss news, outperforming XLM-R's 81.3 F1
- Demonstrates 95.6% top-1 accuracy on Romansh sentence retrieval in zero-shot setting
- Shows moderate improvement on Swiss political comment stance detection (73.6 F1 vs 72.2 for XLM-R)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SwissBERT improves multilingual representations by leveraging language adapters.
- Mechanism: Language adapters allow SwissBERT to maintain monolingual performance while adding support for Romansh, enabling efficient transfer learning across the four national languages.
- Core assumption: Freezing language adapters during downstream fine-tuning enables cross-lingual transfer without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "Since SwissBERT uses language adapters, it may be extended to Swiss German dialects in future work."
  - [section 3.2] "We follow recommendations by Pfeiffer et al. [6] for ensuring the modularity of SwissBERT. When pre-training our language adapters, we freeze the shared parameters of the transformer layers."
  - [corpus] Weak evidence - no explicit ablation of freezing adapters.
- Break condition: If language adapters are not frozen during fine-tuning, cross-lingual transfer degrades and performance on individual languages drops.

### Mechanism 2
- Claim: A custom Switzerland-specific vocabulary improves segmentation of Romansh text.
- Mechanism: Creating a new vocabulary from Swiss news data allows Romansh words to be represented with fewer subwords, reducing fragmentation and improving model performance.
- Core assumption: Segmentation quality directly impacts downstream task performance.
- Evidence anchors:
  - [section 3.3] "Analyzing the new vocabulary, we find that 18k of the 50k subwords occur in the original XLM-R vocabulary, and the other 32k are new subwords. Most are Romansh words, orthographic variants, media titles, toponyms or political entities of Switzerland."
  - [section 3.3] "Co din ins quai per rumantsch? Co din in|s qua|i per rum|ants|ch ?"
  - [corpus] Weak evidence - no explicit ablation comparing vocabulary strategies.
- Break condition: If the vocabulary size is reduced too much, rare words are underrepresented and model performance drops.

### Mechanism 3
- Claim: Adaptive pre-training on contemporary Swiss news improves performance on current domain tasks.
- Mechanism: Continuing masked language modeling on a recent corpus of Swiss news allows SwissBERT to adapt to current language usage and entities.
- Core assumption: Temporal alignment between pre-training data and downstream tasks is crucial for performance.
- Evidence anchors:
  - [section 3.1] "Previous work shows that adaptation to more recent data can improve performance on present-time downstream tasks [25]."
  - [section 4.4] "The SwissBERT model and our evaluation experiments have a limited scope... we expect SwissBERT to perform best on input that is similar to our pre-training corpus of written news."
  - [corpus] Moderate evidence - corpus spans 2022, matching evaluation data timeframe.
- Break condition: If pre-training data becomes outdated relative to downstream tasks, performance degrades due to temporal misalignment.

## Foundational Learning

- **Concept**: Masked language modeling
  - Why needed here: SwissBERT uses masked language modeling to learn contextual representations from raw text.
  - Quick check question: What is the objective function minimized during SwissBERT pre-training?

- **Concept**: Language adapters
  - Why needed here: Language adapters allow SwissBERT to add support for Romansh while maintaining performance on other languages.
  - Quick check question: What happens if language adapters are not frozen during downstream fine-tuning?

- **Concept**: Subword tokenization
  - Why needed here: SwissBERT uses SentencePiece to create a vocabulary that balances coverage and segmentation quality for multiple languages.
  - Quick check question: How does vocabulary size affect the number of subwords a word is split into?

## Architecture Onboarding

- **Component map**: SwissBERT = X-MOD base + Swiss language adapters + custom Swiss vocabulary. Core components: Transformer layers, language adapters, embeddings, vocabulary.
- **Critical path**: Pre-training → Fine-tuning → Evaluation. Bottleneck is GPU memory during pre-training with large batch sizes.
- **Design tradeoffs**: Smaller vocabulary (50k vs 250k) improves Romansh segmentation but may hurt rare word representation. Language adapters add modularity but increase parameter count.
- **Failure signatures**: If training diverges, check learning rate and batch size. If Romansh performance is poor, check vocabulary segmentation. If cross-lingual transfer fails, verify adapter freezing.
- **First 3 experiments**:
  1. Train with default settings on WikiNEuRal to verify basic functionality.
  2. Compare performance with/without adapter freezing to verify cross-lingual transfer.
  3. Evaluate on a small Romansh dataset to check segmentation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal vocabulary size and creation strategy for multilingual SwissBERT variants to balance segmentation quality and model capacity?
- Basis in paper: [inferred] The paper compares a 50k SwissBERT vocabulary against XLM-R's 250k vocabulary, finding benefits but not exploring other sizes or strategies.
- Why unresolved: The paper only tests one custom vocabulary size and doesn't compare against other potential strategies like language-specific vocabularies or adaptive subword tokenization.
- What evidence would resolve it: Systematic experiments varying vocabulary size (e.g., 25k, 100k) and creation methods, measuring performance on downstream tasks across all Swiss languages.

### Open Question 2
- How does SwissBERT's performance generalize to non-news domains and Swiss German dialects that weren't in the pre-training corpus?
- Basis in paper: [explicit] "Since SwissBERT has been adapted to news articles only... we do not observe state-of-the-art accuracy when recognizing named entities in historical news" and "Swiss German dialects are absent in our training corpus."
- Why unresolved: The paper only evaluates on news and political comment data, leaving open questions about other Swiss text types and dialects.
- What evidence would resolve it: Evaluations on Swiss German text, historical documents beyond OCR, social media, and other non-news domains.

### Open Question 3
- What are the quantitative carbon emissions and environmental impacts of training SwissBERT compared to baselines?
- Basis in paper: [explicit] The paper provides a model card with training details and notes "We do not methodically compare different approaches" and mentions carbon efficiency.
- Why unresolved: The paper calculates carbon emissions for SwissBERT but doesn't compare them to XLM-R, X-MOD, or other baseline models.
- What evidence would resolve it: Comprehensive carbon footprint analysis including training time, energy source differences, and model size comparisons across all evaluated models.

## Limitations

- Performance on historical news is notably weaker (69.7 F1 vs 75.2 for XLM-R on HIPE-2022), suggesting limited cross-temporal transfer.
- The custom vocabulary size of 50k subwords may underrepresent rare words and limit coverage for other languages.
- Evaluation scope is restricted to Swiss-centric tasks, leaving open questions about performance on general multilingual tasks or non-Swiss domains.

## Confidence

**High confidence**: Claims about SwissBERT's superior performance on contemporary Swiss news NER (83.7 F1 vs 81.3 for XLM-R) and Romansh sentence retrieval (95.6% vs 94.1% accuracy) are well-supported by direct experimental comparisons.

**Medium confidence**: Claims about moderate improvements on x-stance (73.6 F1 vs 72.2 for XLM-R) and the benefits of custom vocabulary for Romansh segmentation are supported but would benefit from additional ablation studies.

**Low confidence**: Predictions about SwissBERT's performance on Swiss German dialects remain speculative, as the model was not evaluated on these varieties.

## Next Checks

1. **Temporal transfer experiment**: Evaluate SwissBERT on news articles from 2018-2020 to quantify performance degradation when pre-training and evaluation data are temporally misaligned, testing the mechanism that current domain adaptation drives improvements.

2. **Vocabulary ablation study**: Train SwissBERT variants with vocabulary sizes of 25k, 50k (current), and 100k subwords, then measure Romansh segmentation quality and downstream task performance to validate the tradeoff between segmentation and coverage.

3. **Adapter freezing validation**: Conduct a controlled experiment comparing SwissBERT fine-tuning with frozen vs. unfrozen language adapters on the Romansh alignment task to empirically verify that freezing is necessary for cross-lingual transfer.