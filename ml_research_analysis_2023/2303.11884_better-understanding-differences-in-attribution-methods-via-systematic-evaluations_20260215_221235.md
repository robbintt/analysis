---
ver: rpa2
title: Better Understanding Differences in Attribution Methods via Systematic Evaluations
arxiv_id: '2303.11884'
source_url: https://arxiv.org/abs/2303.11884
tags:
- methods
- layer
- attributions
- attribution
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes three novel evaluation schemes to better understand
  differences in attribution methods for deep neural networks. The key contributions
  are: (1) a novel evaluation setting (DiFull) that controls which parts of the input
  can influence the output, allowing to distinguish possible from impossible attributions
  and highlight definite failure modes of attribution methods; (2) a multi-layer evaluation
  scheme (ML-Att) that evaluates all methods on the same layers, addressing fairness
  concerns when comparing methods that explain networks to different depths; (3) a
  qualitative evaluation scheme (AggAtt) that aggregates attribution maps across many
  samples, providing a holistic view of a method''s performance.'
---

# Better Understanding Differences in Attribution Methods via Systematic Evaluations

## Quick Facts
- arXiv ID: 2303.11884
- Source URL: https://arxiv.org/abs/2303.11884
- Reference count: 40
- One-line primary result: A simple Gaussian smoothing step significantly improves localization performance for some attribution methods.

## Executive Summary
This paper proposes three novel evaluation schemes to better understand differences in attribution methods for deep neural networks. The key contributions are: (1) DiFull, which controls input influence to distinguish possible from impossible attributions; (2) ML-Att, which evaluates all methods on the same layers for fair comparison; and (3) AggAtt, which aggregates attribution maps across samples for holistic assessment. The authors use these schemes to study widely used attribution methods and propose a post-processing smoothing step that significantly improves localization performance for some methods.

## Method Summary
The paper introduces three evaluation schemes: DiFull creates isolated subimages with independent classification heads to control which parts of the input can influence the output; ML-Att evaluates all attribution methods at identical layers to enable fair comparison; and AggAtt aggregates attribution maps across many samples for qualitative assessment. The authors apply these schemes to study attribution methods like Saliency, GradCAM, IntGrad, IxG, and LRP across various CNN architectures. They also propose a Gaussian smoothing technique to improve localization performance by summarizing local pixel contributions.

## Key Results
- Apparent performance differences between attribution methods often vanish when compared fairly on the same layer
- A simple Gaussian smoothing step significantly improves localization for methods like IntGrad and IxG
- LRP methods perform poorly in all evaluation settings, likely due to lack of implementation invariance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DiFull provides ground truth for impossible attributions by controlling input influence.
- **Mechanism**: DiFull creates isolated subimages with independent classification heads, ensuring that features outside a subimage cannot influence its classification. This guarantees that attributions outside the target subimage are impossible.
- **Core assumption**: Disconnecting subimages at the feature map level effectively isolates their classification heads, preventing gradient flow from outside features.
- **Evidence anchors**:
  - [abstract] "DiFull, in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions."
  - [section] "we can guarantee that no feature outside the subimage of a given class can possibly have influenced the respective class logit—they are indeed fully disconnected."
  - [corpus] Weak - related papers don't discuss this specific construction.
- **Break Condition**: If batch normalization or other operations allow information to leak across subimage boundaries, the isolation guarantee fails.

### Mechanism 2
- **Claim**: Evaluating methods at identical layers enables fair comparison.
- **Mechanism**: Different attribution methods explain networks at different depths. By forcing all methods to explain at the same layer (e.g., input, middle, final), we eliminate the confounding effect of network depth on performance.
- **Core assumption**: The explanatory power of an attribution method is comparable across layers when properly adapted.
- **Evidence anchors**:
  - [abstract] "different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att)"
  - [section] "We find that the trends in performance at the chosen three layers in Fig. 4 generalize to all layers"
  - [corpus] Weak - related papers don't discuss layer-wise fairness comparisons.
- **Break Condition**: If an attribution method fundamentally cannot be adapted to explain intermediate layers, forcing comparison creates artifacts.

### Mechanism 3
- **Claim**: Gaussian smoothing improves localization by summarizing local pixel contributions.
- **Mechanism**: Piece-wise linear models have exact pixel contributions given by methods like IxG. Smoothing with a Gaussian kernel performs weighted average pooling, summarizing the local region's effect on the output and reducing noise.
- **Core assumption**: The effective receptive field of the model is small enough that local smoothing captures the model's decision-making process.
- **Evidence anchors**:
  - [section] "Since the sign of the input determines whether a contribution (weighted input) is positive or negative, a BatchNorm layer will randomize the sign of the contribution"
  - [section] "sum pooling IxG with a kernel of the same size accurately computes the model's local output"
  - [corpus] Weak - related papers don't discuss this specific smoothing mechanism.
- **Break Condition**: If the model has a large effective receptive field, smoothing may obscure important distant contributions.

## Foundational Learning

- **Concept: Attribution Methods**
  - Why needed here: Understanding the different mechanisms (backpropagation, activation-based, perturbation-based) is crucial for interpreting evaluation results and comparing methods fairly.
  - Quick check question: What are the three main categories of attribution methods, and how does each assign importance to input features?

- **Concept: Localization Metrics**
  - Why needed here: The localization score quantifies how well attributions identify relevant regions, which is the primary evaluation metric used in this paper.
  - Quick check question: How is the localization score calculated, and what does a score of 1.0 or 0.25 represent?

- **Concept: Implementation Invariance**
  - Why needed here: LRP methods lack implementation invariance, meaning functionally equivalent models can get different attributions, which affects evaluation fairness.
  - Quick check question: What is implementation invariance, and why is its absence in LRP methods a concern for evaluation?

## Architecture Onboarding

- **Component Map**: Input Layer -> CNN Backbone -> Classification Head -> Attribution Method
- **Critical Path**: 
  1. Input image passes through CNN backbone
  2. Features processed by classification head
  3. Attribution method applied to explain output
  4. Localization score calculated to evaluate attribution quality
- **Design Tradeoffs**:
  - Evaluating at input vs. final layer: Input provides complete explanation but noisier attributions; final layer provides cleaner but incomplete explanation.
  - Disconnected vs. partial disconnection: Full disconnection guarantees impossible attributions but is more artificial; partial disconnection is more natural but less strict.
- **Failure Signatures**:
  - Poor localization scores indicate attribution method isn't identifying relevant regions.
  - Perfect scores on DiFull but poor on GridPG suggest method is only avoiding impossible attributions but not identifying correct ones.
  - Large variance in AggAtt bins indicates inconsistent performance across inputs.
- **First 3 Experiments**:
  1. Run all attribution methods on a 2×2 grid using GridPG setting and calculate localization scores.
  2. Repeat experiment using DiFull setting to check for impossible attributions.
  3. Apply Gaussian smoothing to IntGrad and IxG attributions and compare localization scores before/after smoothing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed evaluation schemes (DiFull, ML-Att, AggAtt) compare in terms of computational cost and scalability to larger models and datasets?
- Basis in paper: [explicit] The paper discusses computational cost differences between DiFull and GridPG in Section F, but does not provide a comprehensive comparison across all three proposed schemes or their scalability.
- Why unresolved: The paper only briefly mentions computational considerations for DiFull versus GridPG, without analyzing the full cost implications of ML-Att (evaluating across all layers) or AggAtt (aggregating across datasets). The authors also do not discuss how these schemes would scale to larger models or datasets.
- What evidence would resolve it: Detailed runtime analysis comparing all three schemes across multiple model sizes and dataset scales, including memory requirements and processing time.

### Open Question 2
- Question: How does the proposed smoothing technique perform on attribution methods beyond IntGrad and IxG, particularly for activation-based and perturbation-based methods?
- Basis in paper: [explicit] The paper only tests the Gaussian smoothing technique on IntGrad and IxG in Section 5.3, noting that it significantly improves their localization performance.
- Why unresolved: The authors hypothesize that smoothing could benefit other attribution methods but do not empirically test this claim. They also note that the effectiveness depends on network architecture (presence of batch normalization), suggesting potential variability across methods.
- What evidence would resolve it: Systematic evaluation of the smoothing technique applied to all tested attribution methods across multiple architectures, measuring localization performance improvements.

### Open Question 3
- Question: How do the proposed evaluation schemes handle attribution methods that provide negative importance values or multi-class attributions?
- Basis in paper: [explicit] The paper focuses on positive attribution values in its quantitative metrics and visualization schemes, with specific mention that localization scores are undefined when all attributions are negative (Section I.3).
- Why unresolved: The authors do not address how their evaluation framework handles methods that assign negative importance values or provide separate attributions for multiple classes simultaneously, which is a limitation for comprehensive method comparison.
- What evidence would resolve it: Extension of the evaluation metrics to incorporate negative attributions and multi-class scenarios, demonstrating how the schemes would be adapted for these cases.

## Limitations
- The study focuses primarily on classification tasks and may not generalize to other domains like detection or segmentation
- LRP methods' lack of implementation invariance raises fairness concerns in cross-method comparisons
- The evaluation framework requires high-confidence predictions, potentially limiting applicability to uncertain or adversarial scenarios

## Confidence
- Medium confidence in core findings due to limitations in the evaluation framework and lack of theoretical grounding for the smoothing technique.

## Next Checks
1. Test DiFull's isolation guarantee by measuring cross-subimage gradient flow and verifying that batch normalization doesn't leak information
2. Conduct ablation studies on the Gaussian smoothing kernel size to determine optimal parameters and understand why it helps specific methods
3. Apply the evaluation framework to non-classification tasks (e.g., semantic segmentation) to test generalizability beyond the current scope