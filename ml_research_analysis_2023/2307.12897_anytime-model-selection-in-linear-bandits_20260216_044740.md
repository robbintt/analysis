---
ver: rpa2
title: Anytime Model Selection in Linear Bandits
arxiv_id: '2307.12897'
source_url: https://arxiv.org/abs/2307.12897
tags:
- regret
- which
- lemma
- where
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of model selection in linear bandit
  optimization, where the goal is to adaptively choose among multiple feature maps
  while simultaneously optimizing for reward. The key insight is that, for linear
  bandits, it's possible to emulate full-information feedback to an online learning
  algorithm with a favorable bias-variance trade-off.
---

# Anytime Model Selection in Linear Bandits

## Quick Facts
- arXiv ID: 2307.12897
- Source URL: https://arxiv.org/abs/2307.12897
- Reference count: 40
- Key outcome: ALEXP achieves logarithmic regret dependence on number of models M through Lasso-based hallucination, with anytime guarantees requiring no horizon knowledge

## Executive Summary
This work addresses model selection in linear bandits, where the goal is to adaptively choose among multiple feature maps while optimizing for reward. The key insight is that for linear bandits, we can emulate full-information feedback to an online learning algorithm with favorable bias-variance trade-off. This leads to ALEXP, an algorithm that achieves exponentially improved (log M) dependence on the number of models M for its regret compared to prior approaches. ALEXP has anytime guarantees, requiring neither knowledge of the horizon nor an initial purely exploratory stage, and uses a novel time-uniform analysis of the Lasso.

## Method Summary
ALE XP is an anytime model selection algorithm for linear bandits that uses Lasso regression to estimate rewards for non-selected agents, then updates agent probabilities using exponential weighting. The algorithm maintains a probability distribution over M agents, each using a different feature map. At each time step, it samples an agent, executes their policy, observes the reward, and uses a time-uniform Lasso estimator to construct low-variance reward estimates for all agents. These estimates are then used to update the agent probabilities via exponential weighting, achieving O(max{√n log³ M, n^(3/4)√log M}) regret with high probability for all n ≥ 1.

## Key Results
- Achieves logarithmic dependence on number of models M, improving upon prior poly M scaling
- Provides anytime guarantees without requiring horizon knowledge or initial exploration phase
- Shows consistent empirical improvements across various environments, including cases with many models or highly correlated features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ALEXP reduces model selection regret by emulating full-information feedback via Lasso-based hallucination
- Mechanism: Instead of receiving bandit feedback (only one agent's reward), ALEXP constructs low-variance reward estimates for all agents using a time-uniform Lasso estimator, then applies EXP4-style exponential weighting on these estimates
- Core assumption: The reward function is linearly parametrizable with at least one correct feature map in the class
- Evidence anchors:
  - [abstract]: "Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off."
  - [section 4]: "We propose the Anytime EXPonential weighting algorithm based on Lasso reward estimates (ALEXP), summarized in Algorithm 1... We then estimate the average return of each model by simply taking an expectation."
  - [corpus]: Weak - no direct mention of Lasso hallucination technique
- Break condition: If reward function is not linearly parametrizable or all feature maps are misspecified, the Lasso estimator cannot recover useful information

### Mechanism 2
- Claim: Time-uniform Lasso confidence bounds enable anytime guarantees without horizon knowledge
- Mechanism: Uses a self-normalized martingale analysis with curved Bernstein bounds to construct confidence sets that hold simultaneously for all time steps, allowing adaptive exploration without predetermined schedules
- Core assumption: Noise is i.i.d. sub-Gaussian and feature maps are bounded
- Evidence anchors:
  - [section 5.1]: "We develop novel confidence intervals for Lasso with history-dependent data, which are uniformly valid over an unbounded time horizon."
  - [section 4]: "To solve (1), we use CELER, a fast solver for the group Lasso... Every time UCB policy is used, we set the exploration coefficient βt = 2, and every time exploration is required, we sample according to π = Unif(X)."
  - [corpus]: Missing - no discussion of time-uniform analysis in related papers
- Break condition: If noise has heavy tails or feature maps are unbounded, confidence bounds may fail

### Mechanism 3
- Claim: Exponential weighting with full-information feedback achieves logarithmic regret dependence on number of models M
- Mechanism: By maintaining probability distribution qt over agents and updating via qt+1,j ∝ exp(ηt Σs=1 ˆrs,j), ALEXP mimics EXP4's optimal regret bound of O(√n log M) even with partial feedback through hallucination
- Core assumption: Reward estimates ˆrt,j are bounded and can be computed efficiently
- Evidence anchors:
  - [abstract]: "ALE XP, which has an exponentially improved (log M) dependence on M for its regret."
  - [section 4]: "We maintain a probability distribution qt ∈ ∆M over the agents, and update it sequentially as we accumulate evidence on the performance of each agent."
  - [section 5.2]: "A key technical observation in this work is that our online Lasso estimator leads EXP4 to achieve sublinear regret which depends logarithmically on M."
  - [corpus]: Weak - related papers focus on polyM regret without Lasso hallucination
- Break condition: If M grows faster than n or feature maps are highly correlated, the log M advantage may diminish

## Foundational Learning

- Online convex optimization and EXP4 algorithm
  - Why needed here: ALEXP is fundamentally an online learning algorithm that treats model selection as an expert advice problem, requiring understanding of how EXP4 achieves logarithmic regret with full-information feedback
  - Quick check question: What is the regret bound of EXP4 with full-information feedback when comparing to the best expert?

- High-dimensional statistics and Lasso regression
  - Why needed here: The Lasso estimator is used to construct low-variance reward estimates for all agents, requiring understanding of restricted eigenvalue conditions and confidence interval construction in high-dimensional settings
  - Quick check question: What is the restricted eigenvalue condition and why is it necessary for valid Lasso confidence intervals?

- Linear bandit algorithms and confidence bound construction
  - Why needed here: The oracle agent needs to provide sublinear regret guarantees, requiring understanding of how confidence bounds are constructed for linear bandits with history-dependent data
  - Quick check question: How does the confidence bound construction differ when the agent's actions are selected by a meta-algorithm rather than in isolation?

## Architecture Onboarding

- Component map:
  - Base agents -> Lasso estimator -> EXP4-style optimizer -> Exploration module -> Confidence bound calculator

- Critical path:
  1. Sample agent jt based on current distribution qt
  2. Execute agent's policy to select action xt
  3. Observe reward yt and update history
  4. Run Lasso estimator to get ˆθt and reward estimates ˆrt,j for all agents
  5. Update agent probabilities qt+1 using exponential weighting
  6. Compute confidence bounds for oracle agent's performance

- Design tradeoffs:
  - Exploration probability γt: Too high wastes samples on bad agents, too low may miss good agents
  - Lasso regularization λt: Balances bias-variance tradeoff in reward estimation
  - Learning rate ηt: Controls sensitivity of exponential weighting updates

- Failure signatures:
  - Linear regret growth: Exploration probability too low or Lasso estimates too noisy
  - Suboptimal agent selection: Reward estimates biased or confidence bounds too wide
  - Computational instability: Feature maps too correlated or number of models too large

- First 3 experiments:
  1. Test ALEXP on orthogonal feature maps with small M (e.g., Legendre polynomials with s=2, p=10) to verify O(√n log M) regret
  2. Test with highly correlated feature maps (e.g., s=8, p=10) to verify robustness to feature correlations
  3. Scale up M significantly (e.g., s=3, p=10 giving M=165) to verify logarithmic dependence on number of models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal exploration probability decay rate γt that balances exploration and exploitation in ALEXP?
- Basis in paper: [explicit] The paper states that "less exploration will be required as data accumulates" and chooses "a decreasing sequence of (γt)t≥1 the probabilities of exploration at step t ≥ 1," specifically using γt = O(t−1/4) in Theorem 1.
- Why unresolved: While the paper uses γt = O(t−1/4) and shows it works, the theoretical analysis doesn't prove this is the optimal rate. The regret bound's dependence on this choice isn't fully characterized.
- What evidence would resolve it: A rigorous analysis showing the optimal γt rate that minimizes regret, or empirical comparisons across different γt decay rates (e.g., t−1/3, t−1/2) showing their impact on regret.

### Open Question 2
- Question: Can ALEXP achieve the optimal √nd log n regret rate when the number of models M is comparable to or smaller than the horizon n?
- Basis in paper: [inferred] The paper states that "if M is comparable to n or smaller, our regret scales with O(n3/4√log M), and while it is still sublinear and scales logarithmically with M, the dependency on n is sub-optimal." This is worse than the optimal √nd log n rate for linear bandits.
- Why unresolved: The paper conjectures this might be avoidable with more assumptions about base agents but doesn't provide a definitive answer. The current analysis is "conservative" and doesn't leverage potential properties of sublinear algorithms.
- What evidence would resolve it: A modified ALEXP algorithm with stronger assumptions on base agents (e.g., that many achieve sublinear regret in isolation) that provably achieves √nd log n regret, or a lower bound showing this is impossible without such assumptions.

### Open Question 3
- Question: How does ALEXP perform in non-parametric reward classes beyond linearly parametrizable rewards?
- Basis in paper: [explicit] The conclusion states that "this problem remains open for more general, non-parametric reward classes."
- Why unresolved: ALEXP's analysis relies heavily on the linear parametrization assumption to use Lasso and construct confidence intervals. Extending this to non-parametric settings where no such feature map structure exists is unclear.
- What evidence would resolve it: Empirical evaluation of ALEXP variants using non-parametric estimators (e.g., kernel methods) in bandit problems with non-linear rewards, comparing regret to specialized non-parametric bandit algorithms.

## Limitations
- Performance degrades with highly correlated features or when the true model is not within the candidate class
- Computational complexity scales with M, making it impractical for extremely large model classes
- Requires bounded feature maps and i.i.d. sub-Gaussian noise; heavy-tailed distributions or unbounded features could break guarantees

## Confidence
- **High Confidence**: The fundamental insight about emulating full-information feedback via Lasso hallucination is sound and the logarithmic M regret bound is rigorously proven for the specified conditions
- **Medium Confidence**: The empirical results demonstrate consistent improvements over baselines, though experiments focus on synthetic settings with relatively small M
- **Low Confidence**: Practical performance in real-world scenarios with many models or highly correlated features is not thoroughly explored

## Next Checks
1. Test ALEXP on real-world datasets with many correlated feature maps to verify the logarithmic M advantage holds in practice
2. Implement a variant with sparse sampling of agents to reduce computational overhead while maintaining theoretical guarantees
3. Analyze sensitivity to noise distribution by testing with heavy-tailed noise to identify when confidence bounds fail