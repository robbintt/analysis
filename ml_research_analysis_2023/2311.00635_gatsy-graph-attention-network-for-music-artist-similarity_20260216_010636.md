---
ver: rpa2
title: 'GATSY: Graph Attention Network for Music Artist Similarity'
arxiv_id: '2311.00635'
source_url: https://arxiv.org/abs/2311.00635
tags:
- artist
- artists
- graph
- features
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GATSY, a graph attention network for music
  artist similarity. The authors address the challenge of defining similarity among
  artists, which is inherently subjective and can impact recommendation accuracy.
---

# GATSY: Graph Attention Network for Music Artist Similarity

## Quick Facts
- arXiv ID: 2311.00635
- Source URL: https://arxiv.org/abs/2311.00635
- Reference count: 0
- Primary result: GATSY achieves nDCG score of 0.5664 ± 0.0068 and F1-score of 0.5445 ± 0.0096 when trained on labeled data

## Executive Summary
This paper introduces GATSY, a graph attention network designed to address the challenge of music artist similarity. The framework leverages graph topology to achieve strong performance without relying heavily on hand-crafted features. GATSY can incorporate fictitious artists to bridge disconnected clusters in the artist similarity graph, enabling diverse recommendations from heterogeneous sources. The model demonstrates effectiveness through experimental results that outperform state-of-the-art solutions while maintaining flexibility in both unsupervised and supervised settings.

## Method Summary
GATSY is a graph attention network that takes artist connection graphs and audio features as input. The architecture consists of 3 fully connected layers followed by 2 GAT layers, producing 256-dimensional embeddings. The model can be trained in both unsupervised mode using triplet loss and supervised mode with additional genre classification. It uses distance-weighted sampling for anchor-positive-negative triplets, ADAM optimizer with cosine learning rate decay, and batch normalization after each layer. The framework demonstrates robustness to feature quality by maintaining performance even with random input features, and can augment datasets with fictitious artists created as mean feature vectors of existing artists.

## Key Results
- GATSY achieves nDCG score of 0.5664 ± 0.0068 on artist similarity task
- The model obtains F1-score of 0.5445 ± 0.0096 in genre classification
- GATSY outperforms GraphSAGE and FC baselines in both unsupervised and supervised settings
- The framework maintains reasonable performance even when trained with random features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GATSY achieves strong performance even with random node features because graph attention mechanisms extract topological structure independent of feature quality.
- Mechanism: The attention coefficients in GAT layers capture local neighborhood relationships, allowing the model to propagate and aggregate structural information without relying heavily on node-level feature quality.
- Core assumption: The graph topology contains sufficient information about artist similarity for the attention mechanism to learn meaningful embeddings.
- Evidence anchors:
  - [abstract]: "The proposed framework takes advantage of a graph topology of the input data to achieve outstanding performance results without relying heavily on hand-crafted features."
  - [section]: "In this work, we address the problem by relying on graph neural networks (GNNs)... Interestingly, we demonstrate that with a defined topology, hand-crafted input features lose their explanatory power, while using random features, our model retains its utility performance."

### Mechanism 2
- Claim: Adding fictitious artists improves recommendation diversity by bridging previously disconnected clusters in the artist similarity graph.
- Mechanism: By creating artificial nodes with features as the mean of selected artist features and connecting them to relevant artists, the model can generate recommendations that span multiple genres or styles.
- Core assumption: The mean feature vector of a set of artists is a valid representation for a fictitious artist connecting those artists.
- Evidence anchors:
  - [section]: "We exploit this insight to augment the dataset by inserting fake nodes... and, as a proof of concept, we use them as a starting point to generate recommendations."
  - [section]: "Interestingly in the latter scenario, the recommendations converge on 'Dance/Jazz' artists."

### Mechanism 3
- Claim: Supervised genre classification improves recommendation quality by providing semantic context to the embedding space.
- Mechanism: Adding genre labels and training with cross-entropy loss alongside triplet loss creates an embedding space where artists of similar genres are closer together.
- Core assumption: Genre labels are a reliable proxy for artist similarity and provide meaningful semantic structure to the graph.
- Evidence anchors:
  - [section]: "The new version of the dataset allows to extend the set of experiments, including not only an unsupervised regime but also supervised tasks. Our goal is to improve the quality of our recommendation system by leveraging the new characterization of the nodes in the graph."

## Foundational Learning

- Concept: Graph Neural Networks and Graph Attention Networks
  - Why needed here: GATSY uses GAT layers to leverage attention mechanisms for capturing complex relationships in the artist similarity graph.
  - Quick check question: How does the attention mechanism in GAT layers differ from simple message passing in GraphSAGE?

- Concept: Triplet Loss and Contrastive Learning
  - Why needed here: GATSY uses triplet loss to ensure similar artists are close in the embedding space while dissimilar ones are far apart.
  - Quick check question: What is the role of the margin parameter in triplet loss, and how does it affect the learned embeddings?

- Concept: Recommendation Systems and Evaluation Metrics
  - Why needed here: GATSY is evaluated as a recommendation system using nDCG to measure how well it ranks similar artists.
  - Quick check question: How does nDCG differ from precision@k, and why is it more suitable for ranking tasks?

## Architecture Onboarding

- Component map: Input features → FC layers (3) → Batch normalization → GAT layers (2) → Embedding → Optional genre classification head → Loss functions (triplet + cross-entropy)
- Critical path: Feature preprocessing → FC layers → GAT layers → Embedding → Recommendation retrieval
- Design tradeoffs: Using GAT over GraphSAGE adds attention mechanism but increases parameter count; batch normalization speeds training but adds complexity; supervised genre classification improves semantics but requires labeled data
- Failure signatures: Poor nDCG scores indicate graph structure issues; low F1-score indicates genre classification problems; training instability may indicate learning rate or batch normalization issues
- First 3 experiments:
  1. Train GATSY with random features to verify topology-based performance (as described in ablation study)
  2. Add a single fictitious artist and verify recommendations span expected genres
  3. Compare supervised vs unsupervised training with genre labels to measure semantic improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GATSY compare to other graph neural network architectures when using only fictitious nodes without any real artist data?
- Basis in paper: [explicit] The paper introduces the concept of fictitious nodes and discusses their effectiveness in connecting previously unrelated artists, but does not provide a detailed comparison of performance metrics when using only fictitious nodes.
- Why unresolved: The paper does not explore the scenario where the entire dataset is composed of fictitious nodes, which would provide insights into the robustness and generalizability of the GATSY framework in the absence of real data.
- What evidence would resolve it: Experiments comparing GATSY's performance metrics (e.g., nDCG, F1-score) when trained exclusively on fictitious nodes versus real artist data would provide a clear understanding of its capabilities in different data environments.

### Open Question 2
- Question: What is the impact of varying the number of attention heads in the GAT layers on the performance of GATSY?
- Basis in paper: [inferred] The paper mentions that the GATSY framework uses a single attention head and notes that using more than one head negatively affected performance during the design phase. However, it does not explore different configurations of attention heads in detail.
- Why unresolved: The paper does not provide a comprehensive analysis of how different numbers of attention heads affect the model's performance, leaving uncertainty about the optimal configuration for various tasks.
- What evidence would resolve it: Conducting experiments with varying numbers of attention heads and analyzing their impact on key performance metrics would help determine the optimal configuration for different use cases.

### Open Question 3
- Question: How does the inclusion of additional metadata, such as artist collaborations or album information, influence the recommendation quality of GATSY?
- Basis in paper: [explicit] The paper discusses the use of music genres as additional metadata and its positive impact on recommendations, but does not explore other types of metadata.
- Why unresolved: The paper focuses on genre information and does not investigate the potential benefits or drawbacks of incorporating other metadata types, such as collaborations or album details, which could further enhance the recommendation system.
- What evidence would resolve it: Experiments that incorporate various metadata types into the GATSY framework and evaluate their impact on recommendation accuracy and diversity would provide insights into the benefits of a richer data representation.

## Limitations

- The effectiveness of fictitious artists is demonstrated but lacks quantitative comparison with baselines
- The supervised genre classification results are limited by coarse genre taxonomy (25 genres from 2842 initially identified)
- Claims about random feature performance rely on the assumption that graph topology alone contains sufficient information about artist similarity

## Confidence

**High Confidence:** The overall framework design and experimental methodology are well-documented and reproducible. The comparison with GraphSAGE and FC baselines provides reasonable validation of GATSY's effectiveness.

**Medium Confidence:** Claims about random feature performance and fictitious artist augmentation are supported by ablation studies but would benefit from additional quantitative analysis and comparison with established methods.

**Low Confidence:** The supervised genre classification results are promising but the coarse genre taxonomy limits the assessment of semantic embedding quality. The claim that genre labels significantly improve recommendation quality needs more rigorous validation.

## Next Checks

1. **Random Feature Robustness Test:** Replicate the random feature experiment with multiple random seeds and compare performance degradation against GraphSAGE and FC baselines to quantify the advantage of the attention mechanism.

2. **Fictitious Artist Impact Analysis:** Systematically vary the number and selection criteria for fictitious artists, measuring their impact on recommendation diversity metrics (genre coverage, artist discovery rate) compared to baseline methods.

3. **Fine-grained Genre Classification:** Implement a hierarchical genre classification experiment using the full 2842 genre taxonomy or a medium-grained taxonomy (e.g., 100-200 genres) to better assess the semantic quality of embeddings and validate the claim about genre-supervised improvement.