---
ver: rpa2
title: On the Connection between Pre-training Data Diversity and Fine-tuning Robustness
arxiv_id: '2307.12532'
source_url: https://arxiv.org/abs/2307.12532
tags:
- data
- pre-training
- imagenet
- robustness
- scratch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates how properties of pre-training data distributions
  affect the robustness of fine-tuned models to natural distribution shifts. The authors
  examine five key properties: data quantity, label granularity, label semantics,
  image diversity, and data sources.'
---

# On the Connection between Pre-training Data Diversity and Fine-tuning Robustness

## Quick Facts
- arXiv ID: 2307.12532
- Source URL: https://arxiv.org/abs/2307.12532
- Reference count: 40
- Primary result: Data quantity is the dominant factor for fine-tuning robustness, while other pre-training data properties have limited significance.

## Executive Summary
This paper investigates how pre-training data properties affect the robustness of fine-tuned models to natural distribution shifts. Through extensive experiments across multiple datasets and architectures, the authors demonstrate that data quantity is the primary driver of downstream effective robustness. They show that reducing ImageNet pre-training classes by 4× while increasing images per class by 4× does not impact fine-tuned model robustness. The findings suggest that collecting as much data as possible from a few effective sources is more important than focusing on label semantics or image diversity when building pre-training datasets for robust generalization.

## Method Summary
The authors examine five key properties of pre-training data distributions: data quantity, label granularity, label semantics, image diversity, and data sources. They conduct controlled experiments using ImageNet, iNaturalist, FractalDB-1k, and Stable Diffusion synthetic data as pre-training sources, then fine-tune models on iWildCam-WILDS. The evaluation uses macro F1 scores on ID and OOD test sets to calculate effective robustness. Experiments vary data quantity (5K-150K samples), label granularity (5-232 superclasses), and data source types while keeping other factors constant.

## Key Results
- Data quantity is the dominant factor influencing downstream effective robustness, with larger pre-training datasets consistently yielding better robustness
- Reducing the number of pre-training classes by 4× while increasing images per class by 4× (keeping total data quantity fixed) does not impact fine-tuned model robustness
- Using 25K pre-training samples yields significant robustness improvements compared to training from scratch, demonstrating the importance of even modest data quantities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data quantity is the primary driver of fine-tuning robustness.
- Mechanism: Increasing pre-training data size provides more diverse examples that help models learn invariant features, improving generalization under distribution shifts.
- Core assumption: The relationship between data quantity and robustness is monotonic and consistent across different datasets.
- Evidence anchors:
  - [abstract] "the primary factor influencing downstream effective robustness [43] is data quantity"
  - [section 4.1] "Reducing the number of pre-training images... lowers the effective robustness of fine-tuned models. However, using only 25K pre-training samples... still yields significant robustness improvements"
  - [corpus] Weak evidence; no direct citations matching this claim
- Break condition: When data quality degrades significantly or when noise overwhelms signal.

### Mechanism 2
- Claim: Label granularity affects robustness more than label semantics.
- Mechanism: Fine-grained labels provide more detailed semantic information that helps models learn finer distinctions, improving robustness compared to coarse labels.
- Core assumption: The number of labels matters more than their semantic content for learning transferable features.
- Evidence anchors:
  - [abstract] "using 5 coarse classes instead of 1000 fine-grained classes... still preserves some of the robustness gains compared to training from scratch"
  - [section 4.2] "Using fine-grained labels during pre-training is better for learning representations that are robust to distribution shifts"
  - [corpus] Weak evidence; no direct citations matching this claim
- Break condition: When label semantics become too misaligned with downstream tasks.

### Mechanism 3
- Claim: Image diversity within classes doesn't significantly impact downstream robustness.
- Mechanism: Once a sufficient number of examples per class is reached, additional intra-class diversity provides diminishing returns for robustness.
- Core assumption: Inter-class diversity is more important than intra-class diversity for learning robust features.
- Evidence anchors:
  - [section 4.4.2] "For both ImageNet and iNaturalist, the resulting linear trends are highly similar regardless of the diversity ratio, or the number of subclasses per superclass"
  - [abstract] "increasing per-class diversity... has no effect on transfer robustness"
  - [corpus] Weak evidence; no direct citations matching this claim
- Break condition: When intra-class diversity is extremely limited (e.g., repeated identical images).

## Foundational Learning

- Concept: Distribution shift and robustness metrics
  - Why needed here: Understanding how models generalize to unseen data distributions is central to the paper's findings
  - Quick check question: What is "effective robustness" and how does it measure model performance under distribution shifts?

- Concept: Pre-training vs. fine-tuning paradigms
  - Why needed here: The paper examines how pre-training data properties affect fine-tuned model performance
  - Quick check question: What is the difference between pre-training and fine-tuning, and why is transfer learning beneficial?

- Concept: Data augmentation and synthetic data generation
  - Why needed here: The paper explores synthetic data (fractalDB, Stable Diffusion) as pre-training sources
  - Quick check question: How can synthetic data be used for pre-training, and what are its advantages and limitations compared to natural data?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Subsampling, class balancing, synthetic data generation
  - Model architecture: ResNet, ResNext, DenseNet, AlexNet, MobileNet-V3
  - Training pipeline: Pre-train on ImageNet/iNaturalist/FractalDB/Diffusion, fine-tune on iWildCam
  - Evaluation: Macro F1 scores on ID and OOD test sets, effective robustness calculation

- Critical path:
  1. Pre-train models on various datasets with controlled properties
  2. Fine-tune pre-trained models on iWildCam-WILDS
  3. Evaluate and compare effective robustness across different pre-training configurations

- Design tradeoffs:
  - Data quantity vs. label granularity: More data with coarse labels vs. less data with fine labels
  - Natural vs. synthetic data: Realistic data distribution vs. controlled diversity and scalability
  - Computational cost: Larger pre-training datasets require more resources

- Failure signatures:
  - Poor effective robustness despite large pre-training data: May indicate issues with label quality or domain mismatch
  - Inconsistent results across architectures: Could suggest hyperparameter or implementation issues
  - No improvement over training from scratch: May indicate pre-training dataset is not diverse enough

- First 3 experiments:
  1. Reproduce baseline results: Pre-train on full ImageNet, fine-tune on iWildCam, compare to training from scratch
  2. Test data quantity effect: Pre-train on subsampled ImageNet (5K, 25K, 50K, 100K, 150K), fine-tune on iWildCam
  3. Test label granularity effect: Pre-train on ImageNet with different label granularities (5, 17, 37, 85, 232, 1000 classes), fine-tune on iWildCam

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of pre-training data sources impact downstream robustness compared to data quantity and label granularity?
- Basis in paper: [explicit] The paper investigates the impact of data sources, including natural data (ImageNet, iNaturalist) and synthetic data (FractalDB-1k, Stable Diffusion), on downstream robustness.
- Why unresolved: The paper suggests that while data quantity and label granularity are primary factors, the impact of data source diversity on robustness is not fully explored, especially in comparison to the other factors.
- What evidence would resolve it: Controlled experiments varying data sources while keeping data quantity and label granularity constant would clarify the relative importance of source diversity.

### Open Question 2
- Question: What is the relationship between the granularity of pre-training labels and the robustness of fine-tuned models in different distribution shift settings?
- Basis in paper: [explicit] The paper finds that coarser labels reduce robustness but still offer gains over training from scratch. However, it does not explore how this relationship varies across different types of distribution shifts.
- Why unresolved: The impact of label granularity on robustness might differ depending on the nature of the distribution shift, which is not thoroughly examined in the paper.
- What evidence would resolve it: Testing the effect of label granularity across a wider range of distribution shifts would reveal if the relationship is consistent or context-dependent.

### Open Question 3
- Question: How does the semantic alignment between pre-training and fine-tuning tasks influence robustness, and is this effect consistent across different domains?
- Basis in paper: [explicit] The paper shows that pre-training on semantically similar classes (e.g., animal classes for iWildCam) does not significantly improve robustness, suggesting that diversity in classes is more important.
- Why unresolved: The paper does not explore whether this finding holds true for tasks in other domains, where semantic alignment might play a different role.
- What evidence would resolve it: Replicating the experiments in other domains would determine if semantic alignment consistently has a limited impact on robustness.

## Limitations

- The findings are primarily validated on vision tasks (iWildCam-WILDS) using specific architectures and datasets, with limited testing across different domains or modalities
- The synthetic data experiments show mixed results, with the nature of synthetic data and its alignment with real-world distributions remaining an open question
- The paper focuses on distribution shifts within natural image domains, potentially missing other robustness challenges like adversarial examples or temporal shifts

## Confidence

- **High confidence**: The data quantity findings are well-supported with consistent results across multiple architectures and datasets. The experimental design is rigorous with controlled variables and clear trends.
- **Medium confidence**: Label granularity effects show consistent directional trends but with some variability across different superclass hierarchies. The robustness gains from fine-grained labels are modest but reliable.
- **Medium confidence**: The lack of significant effects from image diversity and label semantics is well-documented, though these negative results could have more nuanced explanations in different contexts.

## Next Checks

1. **Cross-domain validation**: Test whether data quantity dominance holds for non-vision tasks (NLP, speech) and different types of distribution shifts (temporal, adversarial) using established robustness benchmarks like WILDS across multiple domains.

2. **Synthetic data alignment**: Conduct controlled experiments comparing synthetic data to natural data with matched statistics (e.g., using FractalDB with ImageNet-level statistics) to isolate whether synthetic data limitations are due to quantity, quality, or fundamental distributional mismatches.

3. **Long-tail distribution effects**: Investigate how data quantity interacts with class imbalance by testing pre-training on datasets with varying long-tail distributions to determine if quantity effects diminish when certain classes have severely limited samples.