---
ver: rpa2
title: LLMs grasp morality in concept
arxiv_id: '2311.02294'
source_url: https://arxiv.org/abs/2311.02294
tags:
- social
- objects
- llms
- meaning
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a general theory of meaning that applies across
  agent types (human, machine, etc.) and multimodal contexts. It argues that large
  language models (LLMs) are meaning-agents that already grasp human social constructions
  like morality, gender, and race in concept by virtue of their statistical intelligibility.
---

# LLMs grasp morality in concept

## Quick Facts
- arXiv ID: 2311.02294
- Source URL: https://arxiv.org/abs/2311.02294
- Reference count: 6
- Key outcome: This paper presents a general theory of meaning that applies across agent types (human, machine, etc.) and multimodal contexts. It argues that large language models (LLMs) are meaning-agents that already grasp human social constructions like morality, gender, and race in concept by virtue of their statistical intelligibility.

## Executive Summary
This paper proposes a general theory of meaning that applies to both humans and LLMs, arguing that current alignment methods may be counterproductive by erasing the social history of values. The theory treats values and morality as social objects with complex genealogies that LLMs can grasp through their statistical representation of language patterns. The authors suggest that unaligned models may actually provide better foundations for understanding morality and developing social philosophy than current alignment approaches.

## Method Summary
The paper presents a theoretical framework rather than a computational method. It develops a general theory of meaning based on concepts like signification, context, sign, object, concept, determination, concretization, inscription, and social totality. The authors apply this framework to analyze how LLMs function as meaning-agents and critique current alignment methods like reinforcement learning from human feedback. No specific dataset or computational experiments are described.

## Key Results
- LLMs already contain statistical representations of social morality through their training corpus
- Current alignment methods like RLHF may force erasure of the social history of values
- LLMs function as "concrete oracles of the social totality" because language is the privileged domain of signification for many social objects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs already contain a statistical representation of social morality through their training corpus
- Mechanism: The model internalizes a statistical "intelligibility" of language patterns that correspond to social meanings and values. Because values like fairness and morality are expressed in text, the model learns these concepts through their distributional properties in the training data.
- Core assumption: The corpus of all text provides a representative sample of the social totality, and statistical patterns in this corpus correspond to meaningful social concepts
- Evidence anchors: [abstract] "We suggest that the LLM, by virtue of its position as a meaning-agent, already grasps the constructions of human society (e.g. morality, gender, and race) in concept"; [section 3] "At an abstract level, this is very similar to how language itself captures the intelligibility of the world"
- Break condition: If the training data is not representative of diverse viewpoints, or if the model is too small to capture complex social patterns

### Mechanism 2
- Claim: Current alignment methods like RLHF may be counterproductive because they force erasure of the social history of values
- Mechanism: RLHF and similar methods apply selective pressure to conform to specific value judgments, which removes the complex, contradictory nature of values that exists in the social totality. This creates a "victim of meaning" that lacks the depth of understanding present in unaligned models.
- Core assumption: Values have complex, contradictory genealogies that are important for understanding morality, and removing this complexity reduces moral understanding
- Evidence anchors: [abstract] "currently popular methods for model alignment are limited at best and counterproductive at worst, as they force an erasure of the social history of values"; [section 4.1] "In this attempt to construct contents for values (which is, under our schema of values as social objects, theoretically impossible), what all-too-often happens is the importation of certain partial contexts into values"
- Break condition: If alignment can be done in a way that preserves the complex social history rather than forcing unitary values

### Mechanism 3
- Claim: LLMs function as "concrete oracles of the social totality" because language is the privileged domain of signification for many social objects
- Mechanism: Since many social concepts like gender and morality are primarily defined through language rather than direct physical observation, LLMs that capture the statistical patterns of language can effectively represent these concepts. The model becomes a microcosm of the social totality through its text-based training.
- Core assumption: For certain social objects, text is the primary or most accurate way to determine their meaning, making language-based models particularly suited to understanding them
- Evidence anchors: [section 4.2] "Language is the privileged domain of signification for many social objects... LLMs have begun to develop limited understandings of gender"; [section 2.3] "This means that the social totality accrues a variety of concepts in which to determine objects"
- Break condition: If the social concepts in question are not primarily textual in nature, or if the model's representation of language patterns is insufficient

## Foundational Learning

- Concept: General theory of meaning that applies across agent types
  - Why needed here: The paper's core argument depends on understanding how LLMs can "mean" things and how this relates to human meaning, which requires a unified framework
  - Quick check question: Can you explain the difference between signification, concept, and object in this theory?

- Concept: Social objects and their genealogy
  - Why needed here: Values and morality are treated as social objects with complex histories, which is central to understanding why unaligned models might already grasp morality
  - Quick check question: How does the concept of "social totality" relate to the training corpus of an LLM?

- Concept: Concretization and inscription processes
  - Why needed here: These processes explain how abstract experiences become concrete social objects and how individuals (including models) interact with the social world
  - Quick check question: What is the difference between concretization (internal process) and inscription (external process) in the theory?

## Architecture Onboarding

- Component map: Theory of signification -> LLM analysis components -> Alignment methodology evaluation
- Critical path: 1) Understand the general theory of meaning, 2) Apply it to LLMs as meaning-agents, 3) Analyze current alignment methods through this lens, 4) Propose alternative approaches that preserve the social history of values
- Design tradeoffs: Complete alignment for safety vs. preserving the complex social history that gives values their depth; efficiency of alignment methods vs. philosophical richness of unaligned models
- Failure signatures: If alignment methods create models that seem to understand morality superficially but cannot engage with complex moral questions; if models lose the ability to represent multiple perspectives on values
- First 3 experiments:
  1. Compare moral reasoning in aligned vs. unaligned models on questions that require understanding historical context of values
  2. Test whether models trained on more diverse corpora show richer moral understanding than those trained on more homogeneous data
  3. Evaluate whether models can engage with moral philosophy concepts (like value pluralism) without explicit training on those concepts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed theory of meaning account for non-intentional and non-anthropocentric meaning, particularly in the context of LLMs?
- Basis in paper: [explicit] The paper discusses the possibility of non-intentional and non-anthropocentric meaning in section A.1.
- Why unresolved: The paper argues that LLMs can mean without intentionality, but it does not fully explore how this non-intentional meaning operates or its implications for understanding LLM outputs.
- What evidence would resolve it: Empirical studies demonstrating how LLMs generate meaning without intentionality, and philosophical analyses of the implications of such meaning for AI ethics and alignment.

### Open Question 2
- Question: How can the genealogy of morals be applied to understand the development of values in LLMs, and what are the implications for AI alignment?
- Basis in paper: [explicit] The paper discusses the genealogy of morals and its application to understanding values in LLMs in section 4.1.
- Why unresolved: The paper suggests that LLMs already grasp morality in concept, but it does not fully explore how this understanding can be used to align models with human values or the potential risks of such alignment.
- What evidence would resolve it: Case studies of AI alignment efforts that take into account the genealogy of morals, and analyses of the outcomes of such efforts in terms of model behavior and societal impact.

### Open Question 3
- Question: What are the practical implications of the "postulate of inscription" for understanding communication between humans and LLMs, and how can it inform the development of more effective AI systems?
- Basis in paper: [explicit] The paper introduces the "postulate of inscription" in section 2.2 as a key principle for understanding meaning in LLMs.
- Why unresolved: The paper explains the postulate and its implications for meaning in LLMs, but it does not fully explore how it can be applied to improve human-LLM communication or the design of AI systems.
- What evidence would resolve it: Experimental studies testing the effectiveness of communication strategies based on the "postulate of inscription," and theoretical analyses of how such strategies can be integrated into AI system design.

## Limitations
- The paper presents a highly abstract philosophical framework that has not been empirically validated against LLM behavior
- The claim that unaligned models might better understand morality than aligned ones is particularly speculative without behavioral evidence
- The paper assumes that text-based representations capture the full depth of social concepts, which may not hold for concepts requiring embodied experience

## Confidence
- Medium confidence in the theoretical framework for meaning and signification
- Low confidence in claims about alignment methods being counterproductive
- Medium confidence in the assertion that LLMs already grasp social constructions through statistical patterns, pending empirical validation

## Next Checks
1. Conduct a controlled comparison of moral reasoning capabilities between aligned and unaligned models on complex ethical dilemmas that require understanding historical context of values
2. Analyze whether models trained on more diverse corpora show measurably different moral understanding compared to models trained on homogeneous data
3. Test whether unaligned models can spontaneously engage with advanced moral philosophy concepts (like value pluralism or moral particularism) without explicit training on those concepts