---
ver: rpa2
title: Learning to Rewrite Prompts for Personalized Text Generation
arxiv_id: '2310.00152'
source_url: https://arxiv.org/abs/2310.00152
tags:
- prompt
- original
- prompts
- learning
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to improve personalized text generation
  by rewriting prompts for frozen large language models. The approach uses a two-stage
  training paradigm combining supervised learning and reinforcement learning to revise
  key components of the original prompt.
---

# Learning to Rewrite Prompts for Personalized Text Generation

## Quick Facts
- arXiv ID: 2310.00152
- Source URL: https://arxiv.org/abs/2310.00152
- Reference count: 38
- Key outcome: Method rewrites prompts for frozen LLMs using SL-then-RL to improve personalized text generation

## Executive Summary
This paper introduces a method to improve personalized text generation by automatically rewriting prompts for frozen large language models. The approach uses a two-stage training paradigm combining supervised learning and reinforcement learning to revise key components of the original prompt. The rewritten prompts outperform both the original and those optimized by SL or RL alone across three domains, with significant improvements in Bleu, Rouge-1, Rouge-2, and Rouge-L metrics.

## Method Summary
The method employs a two-stage training paradigm that chains together supervised learning (SL) and reinforcement learning (RL) to train a prompt rewriter. The prompt rewriter uses a sequence-to-sequence model (T5) to modify summary and synthesis components of prompts. SL first adapts the model to the prompt rewriting task by generating training labels through randomizing elements in original prompts and selecting the best-performing variant. RL then fine-tunes the rewriter using PPO to maximize generation performance as reward, improving end-to-end training.

## Key Results
- Rewritten prompts outperform original prompts and those optimized by SL or RL alone on all three domains
- Significant improvements in Bleu, Rouge-1, Rouge-2, and Rouge-L metrics
- Removing the summary component improves generation quality for frozen LLMs
- Keyword repetition and reordering patterns emerge from the training process

## Why This Works (Mechanism)

### Mechanism 1
SL before RL reduces the effective search space for RL in prompt rewriting by first adapting a sequence-to-sequence model to the task, learning to copy, reorder, or delete elements from the original prompt. This narrows RL search space to more promising regions. Break condition: If optimal prompts require entirely new vocabulary or structures not present in the original prompt, SL may misdirect RL into suboptimal regions.

### Mechanism 2
Removing the summary component improves generation quality for frozen LLMs because summaries composed of snippets from retrieved historical entries may contain off-topic information. Frozen LLMs are more sensitive to noise than fine-tuned models, so removing the summary reduces irrelevant input. Break condition: If summaries become more accurate or if the frozen LLM gains capability to filter noise, removing the summary may no longer be beneficial.

### Mechanism 3
Repeating important keywords and reordering them based on expected appearance improves generation quality because the prompt rewriter learns that repeating keywords signals importance and reordering them to match likely output sequence aids the LLM in using them effectively. Break condition: If the LLM's internal processing of keywords does not align with these cues, or if the prompt becomes too repetitive, generation quality may not improve.

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: RL is used to optimize the prompt rewriter by maximizing the generation performance of the frozen LLM as the reward. PPO is chosen for stable policy updates.
  - Quick check question: What is the role of the reward function in training the prompt rewriter with RL?

- Concept: Sequence-to-Sequence Models (T5)
  - Why needed here: A sequence-to-sequence model is used to learn the mapping from input prompt variants to the best prompt, enabling automatic rewriting of prompt components.
  - Quick check question: How does the T5 model handle the task of transforming one prompt into another?

- Concept: Prompt Engineering for Personalization
  - Why needed here: The method focuses on optimizing context-dependent components (summary and synthesis) of prompts to improve personalized text generation by frozen LLMs.
  - Quick check question: Why is it important to personalize prompts for different users in text generation tasks?

## Architecture Onboarding

- Component map: Data Processing -> Document Generator (Frozen LLM) -> Prompt Rewriter -> Supervised Learning Stage -> Reinforcement Learning Stage
- Critical path: 1. Generate prompt variants by randomizing summary and synthesis components of the original prompt. 2. Evaluate each variant using the frozen LLM to find the best-performing prompt. 3. Train the prompt rewriter via supervised learning to map variants to the best prompt. 4. Fine-tune the prompt rewriter via reinforcement learning to further improve prompt quality. 5. Use the trained prompt rewriter to generate improved prompts for new personalized generation tasks.
- Design tradeoffs: Using a smaller model (e.g., T5-Large) for the prompt rewriter reduces computational cost but may slightly degrade performance compared to larger models. Removing the summary component simplifies the prompt but may lose some context if summaries become more accurate in the future. The randomization method for generating labels is simple but may not explore all possible prompt variations.
- Failure signatures: Poor performance of the prompt rewriter may indicate that the search space is too large for RL without adequate SL pre-training. If rewritten prompts are unreadable or contain meaningless tokens, the RL stage may not be properly constrained by the SL stage. Failure to improve over original prompts suggests that the prompt rewriter is not effectively learning useful patterns or that the frozen LLM is insensitive to prompt changes.
- First 3 experiments: 1. Train the prompt rewriter using only supervised learning (RewriterSl) and compare its performance to the original prompts on a validation set. 2. Train the prompt rewriter using only reinforcement learning without SL pre-training (RewriterRl) and evaluate its performance to confirm the necessity of the SL stage. 3. Train the prompt rewriter using the full SL-RL paradigm (RewriterSlRl) and compare its performance to both RewriterSl and RewriterRl to demonstrate the benefit of chaining the two stages.

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of the prompt rewriter scale with the size of the user's personal context? [explicit] The paper mentions that the user's personal context is used in the prompt generation process, but does not explore the impact of varying the size of this context on the rewriter's performance. Why unresolved: The study uses fixed datasets without systematically varying the amount of personal context available for each user. What evidence would resolve it: Controlled experiments testing the prompt rewriter with different amounts of personal context data for each user.

- Open Question 2: What is the impact of different types of noise in the original prompt on the effectiveness of the rewriter? [inferred] The paper mentions that frozen LLMs are more affected by noise in the input, but does not systematically study how different types of noise affect the rewriter's performance. Why unresolved: The experiments focus on overall performance improvement without isolating the impact of different noise types. What evidence would resolve it: Experiments introducing specific types of noise (e.g., irrelevant keywords, off-topic summaries) and measuring their impact on rewriter performance.

- Open Question 3: How does the prompt rewriter perform when the document generator is not a frozen LLM but a smaller, fine-tunable model? [explicit] The paper focuses on the scenario where the document generator is a frozen LLM, but does not explore performance when the generator can be fine-tuned. Why unresolved: The study's experimental setup only considers frozen LLMs as document generators. What evidence would resolve it: Comparative experiments using both frozen LLMs and smaller, fine-tunable models as document generators.

## Limitations
- Narrow focus on three specific datasets within the FtPersLlm framework may limit generalizability to other personalization scenarios
- Effectiveness of removing the summary component assumes frozen LLMs are inherently more sensitive to noise than fine-tuned models
- Randomization-based label generation approach may not explore the full space of possible prompt variations

## Confidence
**High Confidence:**
- The two-stage SL-RL training paradigm is necessary for effective prompt rewriting
- Prompt rewriting improves generation quality over both original prompts and single-stage optimization
- The learned rewriting patterns are interpretable and can guide manual prompt improvements

**Medium Confidence:**
- SL reduces the effective search space for RL in prompt rewriting
- Removing the summary component consistently improves frozen LLM performance
- Keyword repetition and reordering strategies are effective across domains

**Low Confidence:**
- The specific randomization method for label generation is optimal
- The discovered patterns (keyword repetition, reordering) will transfer to other personalization tasks
- The method's performance advantage would persist with different frozen LLM architectures

## Next Checks
1. **Cross-domain generalization test:** Apply the trained prompt rewriter to a new personalization domain (e.g., medical chatbot responses or educational content generation) to assess whether the learned rewriting patterns transfer beyond the three original datasets.

2. **Ablation study on summary component:** Systematically test different levels of summary retention (full summary, filtered summary, partial summary) across multiple frozen LLM variants to quantify the relationship between model sensitivity and summary utility.

3. **Alternative label generation evaluation:** Replace the randomization-based label generation with a more directed search approach (e.g., genetic algorithms or Bayesian optimization) to determine if better labels lead to improved prompt rewriting performance.