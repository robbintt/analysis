---
ver: rpa2
title: 'Interpretable and Explainable Machine Learning Methods for Predictive Process
  Monitoring: A Systematic Literature Review'
arxiv_id: '2312.17584'
source_url: https://arxiv.org/abs/2312.17584
tags:
- process
- predictive
- prediction
- event
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review provides a comprehensive analysis
  of explainable and interpretable machine learning methods in predictive process
  monitoring. The study examines 67 papers published between 2014 and 2023, categorizing
  them by application domain, ML models used, explanation methods, and evaluation
  approaches.
---

# Interpretable and Explainable Machine Learning Methods for Predictive Process Monitoring: A Systematic Literature Review

## Quick Facts
- arXiv ID: 2312.17584
- Source URL: https://arxiv.org/abs/2312.17584
- Reference count: 40
- Key outcome: Comprehensive analysis of 67 papers on explainable and interpretable ML methods in predictive process monitoring, identifying decision trees as most common interpretable model, deep learning as dominant black-box approach, and SHAP/LIME as prevalent explanation methods.

## Executive Summary
This systematic literature review provides a comprehensive analysis of explainable and interpretable machine learning methods in predictive process monitoring. The study examines 67 papers published between 2014 and 2023, categorizing them by application domain, ML models used, explanation methods, and evaluation approaches. Key findings include decision trees being the most common interpretable model, deep learning models dominating black-box approaches, SHAP and LIME as prevalent explanation methods, and evaluation remaining a significant challenge. The review identifies finance, healthcare, and customer support as primary application domains, with BPIC datasets being most frequently used. While the field shows growth in research activity, significant gaps remain in evaluation methodologies, transferability across domains, and real-world applications.

## Method Summary
The study employed a systematic literature review methodology using the PRISMA framework to identify relevant papers from five major databases: ACM Digital Library, AIS eLibrary, IEEE Xplore, Science Direct, and SpringerLink. Initial searches yielded 1,071 records, which were filtered through inclusion/exclusion criteria to arrive at 67 final papers. The template analysis methodology following King (2012) framework was applied to extract metadata including publication outlet, year, keywords, application domains, ML models used, explanation methods, and evaluation approaches. This structured approach enabled systematic categorization and synthesis of findings across the diverse body of research in explainable and interpretable ML for predictive process monitoring.

## Key Results
- Decision trees are the most common interpretable model used in predictive process monitoring applications
- Deep learning models dominate as black-box approaches, with LSTMs being particularly prevalent
- SHAP and LIME are the most frequently employed explanation methods across studies
- Evaluation remains a significant challenge with many studies lacking proper assessment methodologies
- Finance, healthcare, and customer support are the primary application domains studied
- BPIC datasets are the most frequently used benchmark datasets in the field

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decision trees provide interpretability by being decomposable and simulatable by humans, making them suitable for transparent predictive process monitoring.
- Mechanism: Decision trees partition the feature space into interpretable regions, where each path from root to leaf represents a sequence of interpretable decisions. This structure allows users to trace the exact reasoning behind predictions.
- Core assumption: Users can understand and verify the decision paths without requiring post-hoc explanation techniques.
- Evidence anchors: [abstract] mentions "decision trees are the most common interpretable model" in predictive process monitoring; [section] states "decision trees are often cited as interpretable models because their decision-making process is straightforward and can be visualized"
- Break condition: If decision trees become too deep or complex, they lose their interpretability advantage and may require additional explanation methods.

### Mechanism 2
- Claim: SHAP and LIME methods provide local and global explanations for black-box models by approximating model behavior in interpretable ways.
- Mechanism: SHAP uses game theory to calculate feature contributions based on all possible coalitions, while LIME trains interpretable surrogate models locally to approximate black-box predictions. Both methods transform complex model behavior into human-understandable explanations.
- Core assumption: The approximation quality is sufficient to capture meaningful model behavior for explanation purposes.
- Evidence anchors: [abstract] identifies "SHAP and LIME are prevalent explanation methods" in the literature; [section] explains that "SHAP values provide a model-agnostic approach" and "LIME rely on surrogate models that specialize on mimicking the behaviour of an underlying model"
- Break condition: If the model behavior is too non-linear or complex, approximations may fail to capture important decision patterns, leading to misleading explanations.

### Mechanism 3
- Claim: Feature importance methods quantify the contribution of input variables to model predictions, enabling global model understanding.
- Mechanism: Methods like permutation feature importance, SHAP feature importance, and connection weight analysis measure how changes in input features affect prediction outcomes across the dataset.
- Core assumption: Feature importance rankings reflect meaningful relationships between variables and predictions that are understandable to domain experts.
- Evidence anchors: [abstract] mentions "evaluation remains a challenge with many studies lacking proper assessment"; [section] states "Feature importance is an umbrella term for some of the most prevalent explanation methods" and provides multiple implementation approaches
- Break condition: If features are highly correlated or interact in complex ways, simple importance measures may not capture the true relationships, leading to incorrect interpretations.

## Foundational Learning

- Concept: Difference between interpretability and explainability
  - Why needed here: Understanding this distinction is crucial for selecting appropriate methods in predictive process monitoring
  - Quick check question: Can you explain why a decision tree is interpretable while a deep neural network requires explainability methods?

- Concept: Local vs global explanations
  - Why needed here: Different prediction tasks require different explanation scopes in process monitoring applications
  - Quick check question: When would you choose local explanations over global explanations in a predictive maintenance scenario?

- Concept: Model-agnostic vs model-specific explanation methods
  - Why needed here: This determines the flexibility and applicability of explanation techniques across different ML models
  - Quick check question: What are the advantages and disadvantages of using SHAP (model-agnostic) versus TreeSHAP (model-specific)?

## Architecture Onboarding

- Component map: Event log → Process mining → Feature extraction → Model training → Prediction → Explanation layer (interpretable models: direct; black-box models: post-hoc methods)
- Critical path: Event log → Process mining → Predictive model → Explanation method → User interface for decision support
- Design tradeoffs: Accuracy vs interpretability, computational cost vs explanation quality, local vs global explanation needs
- Failure signatures: Poor explanation quality, user distrust, computational bottlenecks, domain incompatibility
- First 3 experiments:
  1. Implement a decision tree for next event prediction on BPIC 2012 dataset and visualize the decision paths
  2. Apply SHAP to a random forest model for process outcome prediction and create summary plots
  3. Compare LIME explanations for LSTM and XGBoost models on the same process monitoring task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trade-off between interpretability and predictive accuracy be effectively balanced in different process mining domains?
- Basis in paper: [explicit] The paper discusses the trade-off between interpretability and performance, noting that as models become more complex and potentially more accurate, they often become less interpretable.
- Why unresolved: The paper identifies this as a fundamental tension in AI development but does not provide specific guidelines or methodologies for balancing this trade-off across different domains.
- What evidence would resolve it: Empirical studies comparing the effectiveness of various interpretable and black-box models across different process mining domains, with metrics for both accuracy and interpretability.

### Open Question 2
- Question: What are the most effective evaluation methodologies for explainability in predictive process monitoring?
- Basis in paper: [explicit] The paper highlights that many studies lack proper evaluation of explainability and interpretability, with evaluations often being one-dimensional and not capturing the full spectrum of user needs.
- Why unresolved: The paper identifies the need for more comprehensive, multifaceted evaluation approaches but does not provide a definitive framework or set of best practices for evaluating explainability.
- What evidence would resolve it: Development and validation of a comprehensive evaluation framework that incorporates quantitative, qualitative, and human-grounded methods, tested across various process mining scenarios.

### Open Question 3
- Question: How can XAI solutions be effectively transferred and adapted across different process mining domains?
- Basis in paper: [inferred] The paper notes that studies often focus on specific domains like healthcare or finance, raising questions about the transferability and generalization of XAI solutions.
- Why unresolved: The paper identifies this as a gap in the literature but does not explore mechanisms or frameworks for domain adaptation of XAI methods.
- What evidence would resolve it: Case studies demonstrating successful transfer of XAI methods across different process mining domains, along with a framework for adapting explanations to domain-specific contexts and user needs.

## Limitations
- Database coverage limitations may have missed relevant papers using alternative terminology or published in non-indexed venues
- Template analysis methodology introduces subjectivity in categorizing and interpreting findings across diverse research papers
- Focus on published papers may underrepresent ongoing work or gray literature in the field

## Confidence
- High confidence: Decision trees as most common interpretable model and deep learning as dominant black-box approaches
- Medium confidence: Prevalence of SHAP and LIME as explanation methods may reflect researcher preference rather than methodological superiority
- Medium confidence: Characterization of evaluation challenges is well-documented but specific gaps may not represent all critical shortcomings

## Next Checks
1. Conduct a focused validation study comparing the effectiveness of SHAP versus LIME explanations for a specific predictive process monitoring task using human evaluation metrics to assess explanation quality and user understanding.
2. Perform a domain transferability analysis by applying a successful explainable ML method from one application domain (e.g., finance) to another domain (e.g., healthcare) and measuring performance degradation and explanation quality changes.
3. Design and execute a benchmark study evaluating multiple interpretable and explainable ML methods on a standardized predictive process monitoring task using BPIC datasets, measuring both predictive accuracy and explanation quality across different user groups.