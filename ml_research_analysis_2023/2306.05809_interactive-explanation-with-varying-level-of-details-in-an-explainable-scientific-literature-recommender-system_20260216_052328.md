---
ver: rpa2
title: Interactive Explanation with Varying Level of Details in an Explainable Scientific
  Literature Recommender System
arxiv_id: '2306.05809'
source_url: https://arxiv.org/abs/2306.05809
tags:
- explanation
- users
- level
- explanations
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the design and impact of interactive explanations
  with varying levels of detail in a scientific literature recommender system. The
  authors developed three explanation levels (basic, intermediate, advanced) in the
  RIMA system, systematically varying the soundness and completeness based on intelligibility
  types (What, What if, Why, How).
---

# Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System

## Quick Facts
- arXiv ID: 2306.05809
- Source URL: https://arxiv.org/abs/2306.05809
- Reference count: 9
- Primary result: Interactive explanations with varying levels of detail improve user satisfaction and decision-making in scientific literature recommender systems

## Executive Summary
This paper presents a user-centered design approach for interactive explanations in the RIMA scientific literature recommender system. The authors developed three explanation levels (basic, intermediate, advanced) by systematically varying soundness and completeness based on intelligibility types (What, What if, Why, How). A qualitative user study with 14 participants demonstrated that interactive explanations with different detail levels meet diverse user needs and positively impact transparency, trust, satisfaction, and user experience. The intermediate explanation level was perceived as most effective for decision-making, while users appreciated the ability to control and personalize the explanation process.

## Method Summary
The study employed a user-centered design methodology involving prototyping, testing, and refinement phases. The RIMA system uses semantic scholar API to fetch publications, extracts user interests, generates keyword embeddings, and computes similarity scores for recommendations. Three explanation levels were implemented using intelligibility types from Lim & Dey (2009): basic (low soundness, medium completeness), intermediate (medium soundness, medium completeness), and advanced (high soundness, high completeness). A qualitative study with 14 participants used think-aloud sessions and semi-structured interviews to evaluate the effects on user control, personalization, transparency, trust, satisfaction, and user experience.

## Key Results
- Interactive explanations with varying detail levels meet diverse user needs across different backgrounds
- Intermediate explanation level was perceived as most effective for decision-making
- Users valued control and personalization in the explanation process, leading to positive effects on transparency, trust, and satisfaction

## Why This Works (Mechanism)

### Mechanism 1
Varying explanation levels (basic, intermediate, advanced) systematically improves user satisfaction by matching information density to individual needs. The system manipulates soundness (accuracy) and completeness (detail amount) to generate three levels, covering different user backgrounds and goals.

### Mechanism 2
Interactive explanations increase transparency and trust by allowing users to explore the system's reasoning process through What, What if, Why, and How intelligibility types.

### Mechanism 3
The intermediate explanation level is most effective for decision-making because it balances completeness and soundness optimally, providing sufficient detail without overwhelming users.

## Foundational Learning

- **Intelligibility types (What, What if, Why, How)**: Systematically organize explanation content and map to different user information needs. *Quick check*: What intelligibility type would you use to explain why a specific paper was recommended versus how the recommendation algorithm works?

- **Soundness vs. Completeness tradeoff**: Determines how much and how accurately to reveal system information at each explanation level. *Quick check*: If an explanation is completely accurate but omits major system components, is it sound, complete, both, or neither?

- **User-centered design methodology**: Ensures explanation interfaces meet actual user needs rather than theoretical assumptions. *Quick check*: Why might a qualitative approach be more appropriate than quantitative for determining optimal explanation detail levels?

## Architecture Onboarding

- **Component map**: Frontend (React with Cytoscape, Highcharts) -> Backend (RIMA recommender with semantic scholar API) -> Data pipeline (interest extraction → retrieval → keyword extraction → embeddings → similarity) -> Explanation engine (intelligibility selector + soundness/completeness config)

- **Critical path**: User creates account → system generates recommendations → user accesses explanation interface → user selects explanation level → system renders appropriate intelligibility types

- **Design tradeoffs**: Interactive vs static explanations (more development but better user coverage), technical depth vs accessibility (more detail increases transparency but risks overwhelming), real-time vs precomputed explanations (real-time allows personalization but increases latency)

- **Failure signatures**: Users abandoning explanation interface, high cognitive load despite intermediate selection, confusion about level transitions, technical terms causing drop-off

- **First 3 experiments**: A/B test static vs interactive multilevel explanation, user preference study forcing exposure to all levels, contextual analysis tracking intelligibility type usage patterns

## Open Questions the Paper Calls Out

### Open Question 1
What are the optimal combinations of intelligibility types and explanation levels for different user groups with varying levels of background knowledge? The paper mentions that users with different background knowledge have different needs for explanation detail, but does not determine the optimal combinations for each group.

### Open Question 2
How does the effectiveness of interactive explanations with varying levels of detail compare to static explanations with a single level of detail? The paper discusses benefits of interactive explanations but does not directly compare them to static explanations.

### Open Question 3
How do personal characteristics, such as cognitive abilities and learning styles, influence the perception and interaction with interactive explanations with varying levels of detail? The paper mentions plans to investigate this in future work but does not explore it.

## Limitations

- Small sample size (N=14) limits generalizability across diverse user populations
- The mechanism connecting explanation interactivity to trust is assumed rather than empirically validated
- The optimal soundness/completeness balance remains theoretically defined rather than empirically derived

## Confidence

- **High confidence**: Interactive explanations with multiple detail levels meet diverse user needs (directly supported by user feedback)
- **Medium confidence**: Intermediate level is most effective for decision-making (based on subjective user reports, needs objective metrics)
- **Low confidence**: The soundness/completeness framework generalizes to other XAI domains (only validated in scientific literature recommendation context)

## Next Checks

1. Conduct larger-scale quantitative study (N≥100) measuring decision accuracy and time across explanation levels
2. Implement A/B testing with forced exposure to different levels to identify mismatch costs
3. Test the framework in non-scientific domains (e-commerce, news recommendation) to assess generalizability of the three-level approach