---
ver: rpa2
title: Approximation Results for Gradient Descent trained Neural Networks
arxiv_id: '2309.04860'
source_url: https://arxiv.org/abs/2309.04860
tags:
- lemma
- neural
- have
- page
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides approximation guarantees for fully connected
  neural networks trained with gradient flow, measuring error in the continuous L2-norm
  on the d-dimensional unit sphere for Sobolev smooth target functions. The key innovation
  is that despite under-parametrization (network width smaller than number of samples),
  convergence is possible due to the natural smoothness assumption on targets.
---

# Approximation Results for Gradient Descent trained Neural Networks

## Quick Facts
- **arXiv ID:** 2309.04860
- **Source URL:** https://arxiv.org/abs/2309.04860
- **Reference count:** 40
- **Primary result:** Convergence guarantees for under-parametrized neural networks trained with gradient flow on Sobolev smooth functions, with error decaying exponentially in training time for sufficiently large networks

## Executive Summary
This paper provides theoretical guarantees for fully connected neural networks trained with gradient flow, focusing on approximation of Sobolev smooth functions on the d-dimensional unit sphere. The key innovation is proving convergence despite under-parametrization, where network width is smaller than the number of samples. The analysis shows that gradient flow dynamics are governed by the neural tangent kernel (NTK), which must be coercive in a negative Sobolev norm for convergence to occur. The convergence rate is lower than classical approximation methods under the same smoothness assumptions, suggesting that some redundancy in degrees of freedom is needed to aid optimization.

## Method Summary
The paper analyzes fully connected neural networks with untrained first and last layers, trained via gradient flow (continuous-time optimization) on the d-dimensional unit sphere. The key technical innovation is using a linearized analysis around initialization, where the NTK governs the dynamics. Convergence is established through showing that the NTK remains close to its initial value (concentration) and is coercive in a negative Sobolev norm. The analysis requires smooth activation functions and sufficiently large network width, with error bounds depending on the smoothness parameter α, network width m, and training time t.

## Key Results
- Convergence is possible despite under-parametrization due to NTK coercivity in negative Sobolev norms
- The approximation rate decays exponentially in training time for sufficiently large networks
- The convergence rate is lower than classical methods (finite elements, wavelets, splines) under the same smoothness assumptions
- Networks need some redundancy in degrees of freedom to aid optimization, which appears as a variant of over-parametrization in disguise

## Why This Works (Mechanism)

### Mechanism 1: NTK Coercivity in Negative Sobolev Norms
Convergence is possible despite under-parametrization because the neural tangent kernel (NTK) is coercive in a negative Sobolev norm. The NTK governs the gradient flow dynamics. When the NTK is coercive in a negative Sobolev norm (specifically H^{-β} where β is related to the activation smoothness), it provides a lower bound on the loss reduction that allows convergence even when the number of parameters is less than the number of samples. The core assumption is that the NTK must be coercive in a negative Sobolev norm for the specific activation functions used.

### Mechanism 2: Linearization Around Initialization
The gradient flow optimization is linearized around the initialization, allowing analysis through the NTK. During training, the weights stay close to their random initialization, which means the neural network's evolution can be approximated by its linearization around the initialization point. This linearization is captured by the NTK. The core assumption is that the weights must stay sufficiently close to their initial values during training.

### Mechanism 3: Interpolation Inequalities Bridge Sobolev Norms
Interpolation inequalities bridge the gap between negative and positive Sobolev norms, enabling L^2 error bounds. The analysis uses a coupled system of ODEs tracking the residual in both negative (H^{-α}) and positive (H^α) Sobolev norms. Interpolation inequalities connect these norms to the L^2 norm, allowing convergence in the continuous L^2 norm despite the under-parametrized regime. The core assumption is that the Sobolev spaces must satisfy interpolation inequalities.

## Foundational Learning

- **Concept:** Neural Tangent Kernel (NTK)
  - Why needed here: The NTK captures the linearized dynamics of neural network training and is central to the convergence analysis.
  - Quick check question: What is the NTK for a two-layer network with ReLU activation, and how does it relate to the NTK used in this paper?

- **Concept:** Sobolev Spaces on the Sphere
  - Why needed here: The analysis measures error in continuous L^2 norms on the d-dimensional unit sphere, requiring Sobolev smoothness assumptions on the target functions.
  - Quick check question: How do Sobolev spaces on the sphere differ from standard Sobolev spaces on Euclidean domains?

- **Concept:** Gradient Flow vs Gradient Descent
  - Why needed here: The paper uses gradient flow (continuous time) instead of discrete gradient descent, which simplifies the analysis while maintaining similar convergence properties.
  - Quick check question: What is the relationship between gradient flow and gradient descent, and why might gradient flow be preferred for theoretical analysis?

## Architecture Onboarding

- **Component map:** Untrained first layer -> Hidden layers (increasing width) -> Untrained last layer -> Gradient flow training -> NTK analysis -> Error bounds

- **Critical path:**
  1. Initialize weights (random Gaussian for hidden layers, fixed for first/last)
  2. Train via gradient flow while maintaining weights close to initialization
  3. Show NTK remains close to initial NTK (concentration)
  4. Prove NTK coercivity in negative Sobolev norm
  5. Derive error bounds using interpolation inequalities

- **Design tradeoffs:**
  - Untrained first/last layers vs fully trainable: Simplifies analysis but may limit expressivity
  - Gradient flow vs discrete GD: Cleaner theoretical analysis but less practical
  - Negative Sobolev coercivity vs positive: Enables convergence in under-parametrized regime but requires specific activation properties

- **Failure signatures:**
  - Weights move too far from initialization → linearization breaks
  - NTK eigenvalues approach zero → coercivity fails
  - Activation functions lack smoothness → Sobolev space requirements violated
  - Dimension too high relative to width → concentration bounds fail

- **First 3 experiments:**
  1. Verify NTK eigenvalues decay polynomially for smooth activations (like GELU) on the sphere
  2. Test weight distance during training to confirm it stays within the required bounds
  3. Implement gradient flow training and measure L^2 error convergence rates compared to theoretical predictions

## Open Questions the Paper Calls Out

### Open Question 1
Does the NTK remain coercive in Sobolev norms for smoother activation functions beyond ReLU? The authors require smoother activation functions than ReLU for their convergence theory, but existing literature only proves coercivity for ReLU. The numerical evidence is preliminary and subject to computational limitations. Rigorous mathematical proof that the NTK remains coercive for smooth activations like ELU or GELU, with explicit bounds on the decay rate of eigenvalues, would resolve this.

### Open Question 2
Can the approximation rates be improved beyond the current bounds that appear to be "over-parametrization in disguise"? The current bounds are upper bounds that may not reflect actual performance. The trade-off between approximation quality and optimization requirements is not fully understood. Either tighter theoretical bounds showing improved rates, or empirical evidence demonstrating that practical networks achieve better approximation rates than predicted by theory, would resolve this.

### Open Question 3
How does the dimension-dependence of β affect the practical applicability of these results in high-dimensional settings? The convergence rate analysis shows that β scales with dimension for ReLU networks, making the approach potentially impractical for high-dimensional problems where the curse of dimensionality would dominate. Either proof of dimension-independent bounds for certain activation functions or explicit characterization of how β scales with dimension for different activation functions would resolve this.

## Limitations

- The coercivity of the NTK for smooth activation functions is only numerically verified rather than rigorously proven
- The dimension dependence β in the error bounds remains unspecified, limiting practical application guidance
- The assumption that weights remain close to initialization is strong and may not hold in practice

## Confidence

- **High Confidence:** The mechanism connecting NTK coercivity to convergence is well-established in the literature and the mathematical framework is sound.
- **Medium Confidence:** The specific coercivity requirements for smooth activations and the dimension dependence β are plausible but require more rigorous analysis.
- **Low Confidence:** The practical relevance of the under-parametrization regime studied, given that real-world networks typically operate in over-parametrized settings.

## Next Checks

1. Rigorously prove the coercivity of the NTK for smooth activations on the sphere, extending beyond numerical verification to analytical guarantees.

2. Characterize the dimension dependence β in the error bounds as a function of activation smoothness and network architecture parameters.

3. Test the weight distance assumption (h ≤ 1) empirically across different network depths, widths, and training durations to validate the linearization approximation.