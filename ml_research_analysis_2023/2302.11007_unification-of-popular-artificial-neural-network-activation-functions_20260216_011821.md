---
ver: rpa2
title: Unification of popular artificial neural network activation functions
arxiv_id: '2302.11007'
source_url: https://arxiv.org/abs/2302.11007
tags:
- training
- functions
- activation
- neural
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified representation of popular activation
  functions using Mittag-Leffler functions of fractional calculus. The proposed gated
  representation interpolates between activation functions, mitigates vanishing/exploding
  gradients, and enables adaptive shapes learned from data.
---

# Unification of popular artificial neural network activation functions

## Quick Facts
- arXiv ID: 2302.11007
- Source URL: https://arxiv.org/abs/2302.11007
- Authors: 
- Reference count: 40
- Key outcome: Proposed a unified representation of popular activation functions using Mittag-Leffler functions, enabling smooth interpolation and adaptive shapes while maintaining computational efficiency.

## Executive Summary
This paper presents a unified representation of popular activation functions using Mittag-Leffler functions from fractional calculus. The proposed gated representation can interpolate between different activation functions while maintaining differentiability and computational efficiency. The framework reduces code complexity by requiring only one parameterized function instead of multiple separate implementations, and the derivatives can be expressed in terms of Mittag-Leffler functions for efficient backpropagation. Experiments on MNIST and CIFAR-10 with LeNet-5 architecture demonstrate competitive accuracy compared to individual activation function implementations.

## Method Summary
The paper proposes a unified activation function framework using Mittag-Leffler functions with five parameters (γ, α1, α2, β1, β2) that can represent various activation functions through appropriate parameter settings. The framework is tested by training LeNet-5 neural networks on MNIST and CIFAR-10 datasets, replacing ReLU layers with the unified function and comparing performance against individual implementations (sigmoid, swish-1, softsign, tanh, mish). Training uses ADAM optimizer with default parameters, batch size 64, and random initialization. Experiments are repeated 20 times and averaged, with computations performed on NVIDIA A100 80GB PCIe GPU.

## Key Results
- Unified representation achieves competitive accuracy on MNIST and CIFAR-10 compared to individual activation function implementations
- Smooth interpolation between activation functions is demonstrated by varying the β2 parameter
- Computational overhead of evaluating Mittag-Leffler functions is minimal compared to built-in activation functions
- The unified framework reduces code complexity and maintenance overhead by requiring only one parameterized function

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mittag-Leffler functions enable smooth interpolation between different activation functions.
- Mechanism: By varying the parameters γ, α1, α2, β1, and β2 in the gated representation, the function smoothly transitions between different activation shapes while maintaining differentiability.
- Core assumption: Mittag-Leffler functions have the mathematical properties needed to create smooth transitions between activation function behaviors.
- Evidence anchors:
  - [abstract]: "Adopting Mittag-Leffler functions of fractional calculus, we propose a flexible and compact functional form that is able to interpolate between various activation functions"
  - [section]: "Figure 2 illustrates an example where by fixing all parameters in the gated representation of hyperbolic tangent except β2, one can smoothly interpolate between linear (β2 = 2) and hyperbolic tangent (β2 = 1) activation functions"
- Break condition: If the interpolation range is too constrained or if the parameter space doesn't cover the desired activation function behaviors.

### Mechanism 2
- Claim: The unified representation reduces code complexity and maintenance overhead.
- Mechanism: Instead of implementing multiple separate activation functions, a single parameterized function handles all cases, reducing code duplication and potential bugs.
- Core assumption: The unified function can accurately represent all target activation functions with appropriate parameter settings.
- Evidence anchors:
  - [abstract]: "The uniﬁed form requires fewer lines of code to be implemented and leads to less confusion in dealing with a wide variety of empirical guidelines on activation functions"
  - [section]: "A uniﬁed representation of activation functions can also lead to large savings in programming efforts compared with what is otherwise required for individual implementations"
- Break condition: If the unified implementation becomes too complex to maintain or if specific activation functions require special handling that breaks the unified approach.

### Mechanism 3
- Claim: The unified form maintains computational efficiency despite additional complexity.
- Mechanism: The derivatives of the unified function can be expressed in terms of Mittag-Leffler functions themselves, enabling efficient backpropagation without significant computational overhead.
- Core assumption: The computational cost of evaluating Mittag-Leffler functions is comparable to existing activation functions.
- Evidence anchors:
  - [abstract]: "The derivatives of the proposed functional form can also be expressed in terms of Mittag-Leffler functions making it a suitable candidate for gradient-based backpropagation algorithms"
  - [section]: "The average timings reveal that the added cost of calculating one- or two-parameter Mittag-Leffler functions in the gated representation of activation functions is small compared with their built-in variants"
- Break condition: If Mittag-Leffler function evaluation becomes computationally prohibitive for large networks or if hardware acceleration is not available.

## Foundational Learning

- Concept: Mittag-Leffler functions and their properties
  - Why needed here: Understanding these functions is crucial for implementing and debugging the unified activation function representation
  - Quick check question: What are the key properties of Mittag-Leffler functions that make them suitable for activation function unification?

- Concept: Activation function behavior and characteristics
  - Why needed here: Knowing how different activation functions behave helps in parameter selection and understanding the interpolation capabilities
  - Quick check question: How do ReLU, sigmoid, and tanh differ in their behavior, and how can these differences be captured in the unified representation?

- Concept: Neural network training dynamics
  - Why needed here: Understanding how activation functions affect training (e.g., vanishing gradients) is important for evaluating the benefits of the unified approach
  - Quick check question: How do different activation functions impact gradient flow during backpropagation, and how does the unified approach address these issues?

## Architecture Onboarding

- Component map: Input layer -> Unified activation function -> Hidden layers -> Unified activation function -> Output layer -> Loss function
- Critical path:
  1. Forward pass: Apply unified activation function to each neuron's input
  2. Loss calculation: Compare predictions with ground truth
  3. Backward pass: Calculate gradients using the Mittag-Leffler function derivatives
  4. Parameter update: Adjust network weights and activation function parameters
- Design tradeoffs:
  - Flexibility vs. complexity: More parameters allow better function approximation but increase training difficulty
  - Computational cost: Mittag-Leffler functions may be more expensive to evaluate than simple functions
  - Convergence: The unified approach may converge differently than standard activation functions
- Failure signatures:
  - Training instability: Could indicate issues with the unified function's derivatives or parameter initialization
  - Poor accuracy: Might suggest the unified function cannot adequately represent the needed activation behaviors
  - Slow convergence: Could be due to the increased complexity of the unified function
- First 3 experiments:
  1. Replace ReLU with the unified function on MNIST using LeNet-5 architecture, compare accuracy and training time
  2. Test interpolation between linear and tanh functions by varying β2 parameter, verify smooth transition
  3. Implement the unified function with learnable parameters, train on CIFAR-10, compare with fixed-shape activations

## Open Questions the Paper Calls Out
The paper explicitly mentions that future work includes testing on "more complex datasets and deeper neural networks," suggesting that the current experiments are limited to relatively simple datasets and architectures.

## Limitations
- Limited empirical validation across diverse architectures and tasks beyond MNIST and CIFAR-10
- Computational overhead analysis focuses on timing comparisons without deeper architectural analysis
- Parameterization may introduce optimization challenges not fully explored in experiments

## Confidence
- **High confidence**: The mathematical framework for unifying activation functions using Mittag-Leffler functions is well-established and the derivative calculations are sound.
- **Medium confidence**: The empirical results showing competitive accuracy with existing activation functions, though limited to specific datasets and architectures.
- **Low confidence**: Claims about the unified approach being superior for training dynamics and gradient flow require more extensive experimental validation across diverse architectures and tasks.

## Next Checks
1. Cross-architecture validation: Test the unified activation function across different network architectures (CNNs, Transformers, MLPs) on multiple datasets to verify generalization of performance claims.
2. Gradient flow analysis: Conduct detailed experiments measuring gradient magnitudes throughout training to verify the claimed mitigation of vanishing/exploding gradients compared to individual activation functions.
3. Parameter sensitivity study: Systematically vary the Mittag-Leffler function parameters (γ, α1, α2, β1, β2) across a grid search to identify optimal configurations and potential failure modes in the interpolation space.