---
ver: rpa2
title: Using Offline Data to Speed Up Reinforcement Learning in Procedurally Generated
  Environments
arxiv_id: '2304.09825'
source_url: https://arxiv.org/abs/2304.09825
tags:
- learning
- agent
- arxiv
- levels
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how to leverage offline data to improve
  the sample-efficiency of reinforcement learning (RL) agents in procedurally generated
  (PCG) environments. We focus on Imitation Learning (IL) from offline trajectories,
  comparing pre-training and concurrent IL approaches during online RL training.
---

# Using Offline Data to Speed Up Reinforcement Learning in Procedurally Generated Environments

## Quick Facts
- arXiv ID: 2304.09825
- Source URL: https://arxiv.org/abs/2304.09825
- Authors: 
- Reference count: 40
- Primary result: Pre-training with imitation learning significantly improves sample efficiency in procedurally generated environments, with trajectory diversity more important than optimality.

## Executive Summary
This study investigates how to leverage offline data to improve the sample-efficiency of reinforcement learning (RL) agents in procedurally generated (PCG) environments. We focus on Imitation Learning (IL) from offline trajectories, comparing pre-training and concurrent IL approaches during online RL training. We collect diverse datasets of trajectories from trained agents across four MiniGrid tasks, varying in optimality and level coverage. Our results show that pre-training with IL significantly improves sample-efficiency and convergence, even with sub-optimal trajectories. Concurrent IL during online training further enhances robustness and performance. Notably, we find that using as few as 2-5 diverse trajectories for pre-training can make the difference between learning an optimal policy and failing to learn at all. Our findings highlight the importance of trajectory diversity over optimality for effective pre-training in PCG environments.

## Method Summary
The paper investigates two approaches for leveraging offline data: pre-training and concurrent imitation learning during online RL training. Offline trajectories are collected from trained agents across four MiniGrid tasks at different performance levels (10%, 60%, 90% of optimal returns). These trajectories are stored in replay buffers and used for behavior cloning (BC) using a log loss surrogate function. For pre-training, the actor network is trained with BC for 3,000-10,000 epochs before online RL training. For concurrent training, BC updates are performed after each PPO update during online training. The effectiveness is evaluated by measuring sample efficiency and final performance across held-out testing tasks.

## Key Results
- Pre-training with imitation learning significantly improves sample efficiency and convergence compared to pure RL approaches
- Using as few as 2-5 diverse trajectories for pre-training can make the difference between learning an optimal policy and failing to learn at all
- Concurrent imitation learning during online training further enhances robustness and performance across all tasks
- Trajectory diversity is more important than optimality for effective pre-training in procedurally generated environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training with imitation learning provides a strong policy initialization that significantly improves sample efficiency in procedurally generated environments.
- Mechanism: The pre-trained policy has already learned relevant state-action mappings from offline trajectories, allowing the online RL phase to focus on fine-tuning rather than learning from scratch. This reduces the number of interactions needed to reach optimal performance.
- Core assumption: The offline trajectories contain sufficient diversity to cover relevant state-action space of the target environment distribution.
- Evidence anchors:
  - [abstract] "pre-training with IL significantly improves sample-efficiency and convergence, even with sub-optimal trajectories"
  - [section] "Pre-training significantly improves sample efficiency and performance compared to pure RL (in yellow) and RAPID (in brown)"
  - [corpus] Weak - related work focuses on different aspects of offline RL
- Break condition: If the offline trajectories lack diversity or cover only a small subset of the procedural level distribution, the pre-trained policy may not generalize well.

### Mechanism 2
- Claim: Concurrent imitation learning during online training improves robustness and further reduces sample complexity beyond pre-training alone.
- Mechanism: By continuously providing additional learning signals from offline trajectories during online training, the agent can more quickly learn to generalize across the procedural level distribution while still benefiting from online experience.
- Core assumption: The offline trajectories remain relevant and useful throughout the online training process, even as the policy evolves.
- Evidence anchors:
  - [abstract] "Concurrent IL during online training further enhances robustness and performance"
  - [section] "Agents trained with concurrent IL manage to solve all the task and with all the analysed buffers"
  - [corpus] Weak - related work doesn't specifically address concurrent IL in PCG settings
- Break condition: If the offline trajectories become irrelevant as the policy changes significantly from its initial state, or if there's a distribution shift between offline and online data.

### Mechanism 3
- Claim: Trajectory diversity is more important than optimality for effective pre-training in procedurally generated environments.
- Mechanism: Diverse trajectories expose the agent to a wider range of states and situations across the procedural level distribution, which is more valuable for generalization than having fewer, more optimal trajectories.
- Core assumption: The procedural generation creates levels with varying difficulty and structure, making diversity crucial for covering the state action space.
- Evidence anchors:
  - [abstract] "we find that using as few as 2-5 diverse trajectories for pre-training can make the difference between learning an optimal policy and failing to learn at all"
  - [section] "we can state that the selection of levels (distribution shift of levels diversity) used for pre-training is perhaps surprisingly more important than the quality of these demonstrations"
  - [corpus] Weak - related work doesn't directly address diversity vs optimality tradeoff
- Break condition: If the procedural levels have very similar structure or if the agent can learn effective policies from limited state coverage.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: MiniGrid environments provide partial observations, making them POMDPs rather than full MDPs
  - Quick check question: How does the agent's observation function O(s,a) differ from full state information in an MDP?

- Concept: Behaviour Cloning (BC) as supervised learning
  - Why needed here: IL is implemented through BC, treating policy learning as a supervised learning problem
  - Quick check question: What is the loss function used for BC in this paper, and how is it optimized?

- Concept: Distribution shift in offline-to-online RL
  - Why needed here: The paper explicitly discusses how distribution shift between offline demonstrations and online experiences affects learning
  - Quick check question: What are the two main causes of distribution shift identified in the paper?

## Architecture Onboarding

- Component map: PPO agent with separate actor and critic networks, RAPID trajectory scoring, offline replay buffers for demonstrations, pre-training module, concurrent IL module
- Critical path: Data collection → Trajectory scoring (RAPID) → Buffer storage → Pre-training (BC) → Online RL with PPO → Concurrent IL updates
- Design tradeoffs: Separate optimization for BC and RL vs joint optimization, number of pre-training epochs vs sample efficiency, quality vs diversity of demonstrations
- Failure signatures: Pre-trained policy performs well on held-out data but fails during online training (distribution shift), online training collapses to zero return despite good pre-training
- First 3 experiments:
  1. Run pre-training with 10% buffer for 3k epochs and evaluate initial policy performance
  2. Compare online training with and without concurrent IL using same pre-trained policy
  3. Test pre-training with varying numbers of trajectories (2, 5, 10) from 90% buffer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between trajectory diversity and optimality for effective pre-training in procedurally generated environments?
- Basis in paper: [explicit] The paper states that "the diversity of the distribution of trajectories used for pre-training is more important than the quality of these demonstrations" and finds that using as few as 2-5 diverse trajectories can be sufficient for pre-training.
- Why unresolved: While the paper identifies diversity as more important than optimality, it doesn't specify the exact trade-off or optimal balance between these two factors.
- What evidence would resolve it: Systematic experiments varying both diversity and optimality metrics simultaneously across multiple tasks to identify the optimal combination for pre-training effectiveness.

### Open Question 2
- Question: How do different IL techniques (beyond BC) compare in effectiveness when used for pre-training in procedurally generated environments?
- Basis in paper: [explicit] The paper only uses Behavior Cloning for IL, stating "we adopt Behaviour Cloning (BC) using the log loss surrogate function" without comparing to other IL approaches.
- Why unresolved: The paper demonstrates BC's effectiveness but doesn't explore whether other IL methods like adversarial IL or inverse RL might yield better results.
- What evidence would resolve it: Comparative studies of multiple IL techniques (BC, adversarial IL, inverse RL) using the same pre-training framework and evaluation metrics.

### Open Question 3
- Question: What is the relationship between the distribution shift between offline demonstrations and online environments and the effectiveness of pre-training?
- Basis in paper: [explicit] The paper mentions "a strong shift between the level distribution of the demonstrations used in pre-training and the whole level distribution" as a potential cause for performance drops during online training.
- Why unresolved: The paper identifies distribution shift as a problem but doesn't quantify its impact or test methods to mitigate it.
- What evidence would resolve it: Empirical measurements of distribution shift between demonstration and online environments, along with experiments testing distribution alignment techniques to measure their impact on pre-training effectiveness.

## Limitations
- Limited theoretical grounding for the observed mechanisms, with results primarily empirical
- Specific implementation details of RAPID trajectory scoring algorithm not fully specified
- Hyperparameters for balancing BC and RL losses during concurrent training not fully specified
- Lack of comparison with other IL techniques beyond behavior cloning

## Confidence
- Pre-training improves sample efficiency: High
- Concurrent IL enhances robustness: Medium
- Diversity > Optimality for pre-training: Medium

## Next Checks
1. Implement a systematic ablation study varying the number of pre-training epochs (1k, 3k, 10k) and measure the trade-off between pre-training time and online sample efficiency across all four tasks.
2. Conduct experiments with artificially reduced trajectory diversity by subsampling from specific level types to quantify the exact impact of diversity on pre-training performance.
3. Test the robustness of pre-training by using trajectories from different policy types (e.g., expert vs. random policies) and measuring performance degradation to establish the limits of the approach.