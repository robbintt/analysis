---
ver: rpa2
title: Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR Decomposition
arxiv_id: '2305.02996'
source_url: https://arxiv.org/abs/2305.02996
tags:
- items
- anchor
- scores
- adacur
- topk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficient k-nearest neighbor
  (k-NN) search using cross-encoders, which are typically too slow for direct k-NN
  due to expensive per-pair scoring. Existing methods like ANN CUR use random anchors
  to approximate cross-encoder scores, but this hurts recall on the most relevant
  (top-k) items.
---

# Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR Decomposition

## Quick Facts
- arXiv ID: 2305.02996
- Source URL: https://arxiv.org/abs/2305.02996
- Reference count: 40
- Primary result: Adaptive anchor selection improves top-k recall by up to 70% for k=1 while maintaining computational efficiency

## Executive Summary
This paper addresses the challenge of efficient k-nearest neighbor (k-NN) search using cross-encoders, which are typically too slow for direct k-NN due to expensive per-pair scoring. The authors propose ADACUR, an adaptive multi-round CUR decomposition approach that iteratively refines anchor items based on approximate scores. This method significantly improves top-k recall—particularly for k=1 where it achieves up to 70% improvement—while maintaining computational cost comparable to or lower than existing methods.

## Method Summary
The method uses CUR matrix factorization to approximate cross-encoder scores using a small set of anchor items. Unlike previous approaches that use random anchors, ADACUR iteratively selects new anchors based on approximate scores from previous rounds. The algorithm performs multiple rounds of anchor sampling, updating the embedding space each time with exact scores of newly selected anchors. Two variants are explored: ADACUR (with budget split between anchor sampling and re-ranking) and ADACUR No-Split (without budget split). The approach uses either TopK or SoftMax sampling strategies to select new anchors.

## Key Results
- ADACUR consistently improves k-NN recall compared to both ANN CUR and dual-encoder-based retrieve-and-rerank approaches
- Top-k recall improves with number of rounds and saturates at around 10-20 rounds
- For k=1, ADACUR achieves up to 70% improvement in recall over baseline methods
- ADACUR outperforms baselines across multiple datasets and settings while maintaining similar computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive anchor selection reduces approximation error on top-k items compared to uniform random sampling
- Mechanism: In each round, the algorithm uses current approximate scores to sample new anchors likely to be near the query
- Core assumption: Items highly scored by current approximation are more likely to be true top-k nearest neighbors
- Evidence anchors:
  - [abstract] "Our proposed approach ADACUR consistently improves k-NN recall as compared to both ANN CUR and the widely-used dual-encoder-based retrieve-and-rerank approach."
  - [section] "Our proposed method incrementally selects a suitable set of anchor items for a given test query over several rounds, using anchors chosen in previous rounds to inform selection of more anchor items."
- Break condition: If early approximate scores are too inaccurate, the algorithm might sample irrelevant items as anchors

### Mechanism 2
- Claim: CUR matrix factorization provides efficient low-rank approximation using small anchor set
- Mechanism: Constructs latent embedding space by scoring all items against anchor queries and items
- Core assumption: Top-k rows and columns contain sufficient information to reconstruct top-k neighbor scores
- Evidence anchors:
  - [section] "CUR Decomposition (Mahoney and Drineas, 2009) Given a matrix M∈ Rn×m, CUR decomposition of M is computed using a subset of its rows R = M [Sr, :] ∈ Rk1×m, a subset of its columns C = M [:,Sc] ∈ Rn×k2 and a joining matrixU∈ Rk2×k1 as follows."
  - [section] "In ANN CUR , U is set to be the Moore-Penrose pseudo-inverse of M [Sr,Sc], the intersection of matricesC andR."
- Break condition: If anchor set is too small or poorly chosen, CUR approximation will be inaccurate

### Mechanism 3
- Claim: Multiple rounds of adaptive sampling improve recall more than single round
- Mechanism: Each round adds new anchors based on improved approximate scores
- Core assumption: Approximate scores become more accurate with each round as relevant anchors are added
- Evidence anchors:
  - [section] "As the number of rounds increase, the fraction of total time spent computing pseudo-inverse and updating the approximate scores increases as these operations are performed after each round while the total number of CE calls is fixed."
  - [section] "Figure 3 show Top-k-Recall for ADACUR TopK for varying number of anchor item sampling rounds. ADACUR TopK with one round is the same as ANN CUR method as ADACUR TopK samples the anchor items uniformly at random in the first round. As expected, Top-k-Recall for ADACUR TopK increases with number of rounds and saturates at around 10-20 rounds."
- Break condition: After 10-20 rounds, improvement plateaus while computational overhead continues

## Foundational Learning

- Concept: CUR matrix decomposition and low-rank approximation
  - Why needed here: Provides efficient way to approximate cross-encoder scores using small anchor set
  - Quick check question: What role does the pseudo-inverse matrix U play in CUR decomposition?

- Concept: Cross-encoder models and computational cost
  - Why needed here: Motivates the entire approach due to high computational cost of cross-encoders for k-NN
  - Quick check question: Why are cross-encoders more computationally expensive than dual-encoders for k-NN search?

- Concept: k-NN search and recall metrics
  - Why needed here: Paper evaluates approach on k-NN search task using recall as primary metric
  - Quick check question: How does top-k recall differ from standard recall metrics?

## Architecture Onboarding

- Component map:
  Cross-encoder model -> Anchor selection module -> CUR factorization module -> Approximate scoring module -> Budget allocation system

- Critical path:
  1. Initialize with small random anchor set
  2. Compute approximate scores for all items
  3. Select new anchors based on approximate scores
  4. Update embedding space with exact scores of new anchors
  5. Repeat steps 2-4 for N rounds
  6. Retrieve items using approximate scores
  7. Re-rank retrieved items with exact cross-encoder scores

- Design tradeoffs:
  - Number of anchor items vs. approximation accuracy
  - Number of rounds vs. computational overhead
  - Anchor sampling strategy (top-k vs. softmax) vs. diversity of anchor set
  - Budget allocation between anchor sampling and re-ranking

- Failure signatures:
  - Low recall improvement despite multiple rounds
  - Increasing latency without corresponding recall gains
  - Poor approximation quality for top-scoring items
  - Budget exhaustion before completing all rounds

- First 3 experiments:
  1. Compare recall of ADACUR vs ANN CUR with same number of anchors and rounds
  2. Vary the number of rounds to find point of diminishing returns
  3. Test different anchor sampling strategies (top-k vs softmax) on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ADACUR's performance change with different sampling strategies beyond TopK and SoftMax?
- Basis in paper: [explicit] Authors note it would be interesting to explore other efficient mechanisms to approximate cross-encoder output
- Why unresolved: Paper only tests two sampling strategies without exploring more sophisticated approaches
- What evidence would resolve it: Experiments comparing ADACUR with various sampling strategies on same datasets and metrics

### Open Question 2
- Question: What is the impact of using approximate scores for re-ranking instead of exact CE scores?
- Basis in paper: [inferred] Paper focuses on exact CE scores for re-ranking but notes CE calls are primary bottleneck
- Why unresolved: Paper doesn't explore using approximate scores for re-ranking which could reduce computational cost
- What evidence would resolve it: Ablation studies comparing ADACUR with and without exact re-ranking under varying budgets

### Open Question 3
- Question: How does ADACUR scale with extremely large item sets (millions of items)?
- Basis in paper: [inferred] Paper tests up to 100K items and mentions GPU speedup but doesn't address larger datasets
- Why unresolved: Scalability analysis limited to moderate-sized datasets without discussing bottlenecks in larger-scale settings
- What evidence would resolve it: Experiments on datasets with millions of items and analysis of computational/memory requirements

### Open Question 4
- Question: How sensitive is ADACUR to choice of anchor queries and could alternative methods improve performance?
- Basis in paper: [explicit] Authors use training queries as anchor queries but note it would be interesting to explore other mechanisms
- Why unresolved: Paper doesn't explore alternative methods for selecting anchor queries which could impact embedding space quality
- What evidence would resolve it: Experiments comparing ADACUR with different anchor query selection strategies on same datasets and metrics

## Limitations

- Computational overhead increases linearly with number of rounds despite recall gains plateauing after 10-20 rounds
- Method assumes small anchor set can sufficiently represent full score matrix, which may break down for highly diverse or sparse distributions
- Reliance on approximate score quality for anchor selection creates potential failure mode where early errors compound

## Confidence

- Medium: Results well-supported by experiments but specific to ZESHEL domain with limited evaluation on diverse retrieval tasks
- High: CUR decomposition approach is mathematically sound and well-established
- Medium: Adaptive sampling strategy is intuitive but its effectiveness depends on quality of approximate scores

## Next Checks

1. Test ADACUR on retrieval tasks beyond entity linking (web search, question answering) to validate generalization to different query types

2. Systematically vary anchor set size and measure impact on recall and computational cost to identify optimal trade-off points

3. Quantify relationship between early-round approximation errors and final recall to determine if adaptive approach could perform worse than random sampling in certain scenarios