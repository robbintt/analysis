---
ver: rpa2
title: 'How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and
  Cross-domain Settings'
arxiv_id: '2305.11853'
source_url: https://arxiv.org/abs/2305.11853
tags:
- table
- database
- prompt
- text-to-sql
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates how different prompt construction
  strategies impact large language model (LLM) performance on text-to-SQL tasks across
  zero-shot, single-domain, and cross-domain settings. Key findings include: table
  relationships and content are crucial for effective prompting but LLMs are sensitive
  to their representation in zero-shot and cross-domain settings; in-domain demonstrations
  can reduce this sensitivity but cannot fully replace explicit table content knowledge;
  and in cross-domain settings, there is an optimal prompt length (around 5500 tokens)
  beyond which performance declines.'
---

# How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings

## Quick Facts
- arXiv ID: 2305.11853
- Source URL: https://arxiv.org/abs/2305.11853
- Reference count: 40
- Primary result: Optimal prompt length for cross-domain text-to-SQL is ~5500 tokens; table relationships and content are crucial for LLM prompting.

## Executive Summary
This paper systematically evaluates how different prompt construction strategies impact large language model (LLM) performance on text-to-SQL tasks across zero-shot, single-domain, and cross-domain settings. The authors find that table relationships and content are crucial for effective prompting but LLMs are sensitive to their representation in zero-shot and cross-domain settings. They also discover that in-domain demonstrations can reduce this sensitivity but cannot fully replace explicit table content knowledge. For cross-domain tasks, they identify an optimal prompt length around 5500 tokens beyond which performance declines.

## Method Summary
The study uses the Spider dataset's development set, containing 20 databases with 1034 NLQ-SQL pairs. They evaluate GPT-3 Codex and ChatGPT models using in-context learning without fine-tuning. Different prompt constructions are tested: zero-shot (no demonstrations), single-domain (in-domain examples), and cross-domain (out-of-domain examples). Database prompts are constructed using various schema and content representations including Table(Columns), CreateTable, and CreateTable+SelectCol formats. Execution accuracy (EX) is measured by comparing denotation answers from predicted SQL against gold SQL.

## Key Results
- Table relationships and content are crucial for effective LLM prompting, but representation sensitivity varies by setting
- In-domain demonstrations reduce LLM sensitivity to database prompt representations but don't replace table content knowledge
- Cross-domain performance peaks at ~5500 tokens and declines with longer prompts
- Normalized CreateTable+SelectCol prompts work best for zero-shot tasks
- Multiple in-domain examples improve single-domain performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Table relationships and content are critical for LLM prompt effectiveness, but LLMs are sensitive to how they are represented in zero-shot and cross-domain settings.
- Mechanism: LLMs need explicit structural and content information about databases to generate correct SQL. When this information is missing or poorly represented, performance degrades. Normalization of schema and careful content representation (e.g., SelectCol vs InsertRow) mitigates this sensitivity.
- Core assumption: LLMs cannot infer database structure and content purely from NLQ context without explicit schema and data cues.
- Evidence anchors:
  - [abstract] "table relationships and content are crucial for effectively prompting LLMs. However, it is essential to carefully consider their representation in the prompt, as LLMs are sensitive to the specific presentation in the zero-shot and cross-domain settings."
  - [section 5.1] "CreateTable+SelectCol 3 958 | 827 75.2 | 75.1" shows improved performance when table content is column-wise presented rather than row-wise (InsertRow).

### Mechanism 2
- Claim: In-domain demonstrations can reduce LLM sensitivity to database prompt representations but cannot fully replace table content knowledge.
- Mechanism: Demonstrations provide contextual examples of how NLQ maps to SQL, which helps LLMs infer schema relationships. However, specific data values in tables (content) are not inferable from demonstrations alone, so content must still be explicitly provided.
- Core assumption: Demonstrations convey syntactic/semantic patterns but not the specific values or formats present in the target database.
- Evidence anchors:
  - [abstract] "In-domain demonstration examples can mitigate LLMs' sensitivity to different representations of database knowledge but they cannot replace table content knowledge."
  - [section 5.2] "With only 4 examples, the performance difference between SelectCol 3 and InsertRow 3 diminishes to 0 for Codex" demonstrates reduced sensitivity.

### Mechanism 3
- Claim: There is an optimal prompt length (around 5500 tokens) in cross-domain settings beyond which LLM performance declines.
- Mechanism: LLMs have a capacity limit for effective context integration; beyond a threshold, additional tokens introduce noise or overwhelm attention mechanisms, reducing accuracy.
- Core assumption: LLM attention and context processing degrade with excessive prompt length despite nominal token limits.
- Evidence anchors:
  - [abstract] "in cross-domain settings, there is an optimal prompt length (around 5500 tokens) beyond which performance declines."
  - [section 5.3.1] Figure 7 shows inverted-U shape performance vs prompt length, with decline after ~5500 tokens.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The study evaluates how LLMs perform text-to-SQL without fine-tuning, relying entirely on prompt examples.
  - Quick check question: What distinguishes zero-shot, single-domain, and cross-domain in-context learning settings?

- Concept: Database schema representation
  - Why needed here: Different prompt constructions (e.g., Table(Columns), CreateTable, Columns=[]+ForeignKey) encode schema relationships differently, affecting LLM performance.
  - Quick check question: How does including foreign keys in schema prompts influence LLM accuracy?

- Concept: Prompt normalization
  - Why needed here: Standardizing case and format reduces variability in how LLMs process schema and SQL keywords.
  - Quick check question: Why does converting SQL keywords to lowercase improve consistency in prompt evaluation?

## Architecture Onboarding

- Component map: Task instruction → Database prompt (schema + content) → Test NLQ → Optional demonstrations (NLQ-SQL pairs + database)
- Critical path: Normalize database schema → Select appropriate schema/content representation → Add demonstrations if needed → Evaluate prompt length constraints → Generate SQL
- Design tradeoffs: More demonstrations improve adaptation but increase prompt length; richer schema representation improves accuracy but adds tokens; content representation choice affects sensitivity
- Failure signatures: Performance plateaus or drops when prompt exceeds ~5500 tokens; sensitivity to representation persists without sufficient demonstrations; poor schema encoding leads to JOIN or WHERE clause errors
- First 3 experiments:
  1. Compare zero-shot accuracy using CreateTable+SelectCol vs CreateTable+InsertRow prompts.
  2. Measure single-domain performance improvement with 1, 4, and 8 in-domain demonstrations.
  3. Vary cross-domain prompt length by adjusting number of databases/examples to locate the ~5500 token sweet spot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt length for cross-domain text-to-SQL tasks beyond which performance declines?
- Basis in paper: Explicit - The paper identifies a "sweet spot" around 5500 tokens for cross-domain settings where performance peaks before declining.
- Why unresolved: The paper only tested up to 8K tokens due to model limitations, but the exact inflection point and optimal range remain unclear.
- What evidence would resolve it: Systematic testing across a broader range of prompt lengths (e.g., 3K-10K tokens) with consistent database and demonstration structures.

### Open Question 2
- Question: Why does SelectCol table content construction outperform InsertRow in zero-shot settings but this gap diminishes with in-domain demonstrations?
- Basis in paper: Explicit - The paper observes that SelectCol consistently outperforms InsertRow in zero-shot settings, but the performance difference becomes marginal with more in-domain examples.
- Why unresolved: The paper hypothesizes that LLMs can learn content patterns from demonstrations but doesn't empirically test this mechanism.
- What evidence would resolve it: Controlled experiments comparing different table content representations with varying numbers of demonstrations, possibly with ablation studies on specific content patterns.

### Open Question 3
- Question: How do different demonstration-retrieval strategies impact prompt effectiveness compared to random selection?
- Basis in paper: Inferred - The paper mentions similarity-based retrieval strategies exist but doesn't compare them to their random selection approach.
- Why unresolved: The paper focuses on prompt construction rather than retrieval methods, leaving the impact of demonstration quality unaddressed.
- What evidence would resolve it: Comparative experiments using both random and similarity-based demonstration retrieval with identical prompt constructions across all settings.

## Limitations

- The optimal prompt length findings (~5500 tokens) may be specific to current LLM architectures and could shift with model updates
- The study focuses on execution accuracy without exploring computational efficiency or runtime costs of different prompt constructions
- The sensitivity to table content representation assumes LLMs lack sufficient database knowledge in pretraining, which may not hold for future models

## Confidence

- High confidence: Mechanism 1 (table relationships and content importance) - supported by direct experimental comparisons across multiple prompt formats
- Medium confidence: Mechanism 3 (optimal prompt length) - based on observed performance patterns but potentially architecture-dependent
- Low confidence: Mechanism 2 (demonstration effects) - limited by small number of tested demonstration counts and potential selection bias in example choice

## Next Checks

1. Test prompt length optimization across multiple LLM architectures (Claude, Gemini, LLaMA) to verify the ~5500 token threshold is not model-specific
2. Evaluate whether including foreign key relationships in schema prompts consistently improves JOIN clause accuracy across all settings
3. Measure performance sensitivity to different normalization strategies (case sensitivity, whitespace handling) to quantify their impact on execution accuracy