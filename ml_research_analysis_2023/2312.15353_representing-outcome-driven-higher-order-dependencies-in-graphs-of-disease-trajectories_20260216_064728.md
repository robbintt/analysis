---
ver: rpa2
title: Representing Outcome-driven Higher-order Dependencies in Graphs of Disease
  Trajectories
arxiv_id: '2312.15353'
source_url: https://arxiv.org/abs/2312.15353
tags:
- each
- data
- outcome
- paths
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a novel method for constructing higher-order
  networks in a supervised learning context, designed to capture outcome-driven dependencies
  in disease trajectories. Their approach identifies combinations of risk factors
  predictive of specific health outcomes while skipping noisy or irrelevant diagnoses,
  enabling both improved prediction accuracy and enhanced interpretability.
---

# Representing Outcome-driven Higher-order Dependencies in Graphs of Disease Trajectories

## Quick Facts
- **arXiv ID**: 2312.15353
- **Source URL**: https://arxiv.org/abs/2312.15353
- **Reference count**: 40
- **Key outcome**: Novel method for constructing higher-order networks in supervised learning context to capture outcome-driven dependencies in disease trajectories, demonstrating improved prediction accuracy and interpretability for type 2 diabetes patients

## Executive Summary
This paper introduces a supervised approach to higher-order network construction for disease trajectory modeling that identifies outcome-driven dependencies in patient histories. The method constructs graphs where nodes represent sequences of diagnoses, and edges capture temporal relationships that are informative of specific health outcomes. Using type 2 diabetes patient data, the authors demonstrate that their networks encode significantly more information about disease progression toward outcomes like heart failure compared to existing approaches, and show improved predictive performance when integrated with transformer models, particularly in the presence of label noise.

## Method Summary
The method constructs higher-order networks by identifying conditional nodes that maximize information gain about outcomes through conditional entropy computation and Fisher-Yates shuffle significance testing. Nodes are added if their information gain exceeds a statistical threshold, and edges are constructed based on subsequence patterns rather than strict linear adjacency, allowing the network to skip noisy or irrelevant diagnoses. The resulting graph structure is integrated with transformer models through graph neural networks, where the GNN computes embeddings from the graph that augment the transformer's standard embeddings for improved prediction.

## Key Results
- Information gain (IG) values demonstrate that the proposed method's graphs encode significantly more information about disease progression toward outcomes compared to baseline networks
- Transformer models augmented with graph embeddings show improved AUPRC performance on heart failure, hypotension, and acute renal failure prediction tasks
- The method demonstrates robustness to label noise, with performance advantages increasing as noise levels rise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model identifies outcome-driven higher-order dependencies by computing conditional entropy with respect to the outcome variable, allowing it to detect combinations of diagnoses that are informative of specific health outcomes while ignoring noisy or irrelevant diagnoses.
- Mechanism: The model computes conditional entropy h(Y|u') for candidate conditional nodes u' and uses Fisher-Yates shuffle to test if the information gain IG(Y,u') is significantly higher than expected by chance. Nodes are added to the graph only if they meet this statistical threshold.
- Core assumption: The conditional entropy reduction is a reliable measure of how much a node informs the outcome, and the Fisher-Yates shuffle appropriately controls for random noise.
- Evidence anchors:
  - [abstract]: "identifies combinations of risk factors for a given outcome and accurately encodes these higher-order relationships in a graph"
  - [section 3.2]: "Using a t-statistic for IG(Y, u'), which we denote as tu', we construct the set of nodes as follows: Vk = A [ {u' ∈ A k : tu' ≥ α}"
- Break condition: If the Fisher-Yates shuffle incorrectly identifies noise as signal, or if the conditional entropy measure fails to capture true outcome-relevant dependencies.

### Mechanism 2
- Claim: The model's ability to skip noisy diagnoses through subsequence-based edge construction enables it to detect key risk factors while ignoring irrelevant diagnoses that fragment trajectory growth.
- Mechanism: Instead of requiring linear adjacency, the model connects nodes whenever they appear as subsequences in trajectories, allowing it to "jump over" irrelevant diagnoses and maintain informative paths.
- Core assumption: The subsequence approach correctly identifies relevant dependencies while avoiding overfitting to noise, and computational cost remains manageable.
- Evidence anchors:
  - [abstract]: "enables both improved prediction accuracy and enhanced interpretability" through skipping "noisy or irrelevant diagnoses"
  - [section 3.3]: "we must determine which nodes to connect; i.e., how to construct Ek. Although prior HON methods construct edges in a strictly linear fashion [28, 30], the supervised nature of our proposed approach allows us to focus the growth of the network to consider subsequences rather than substrings"
- Break condition: If the subsequence approach introduces too much noise or computational overhead, or if it fails to capture necessary linear dependencies.

### Mechanism 3
- Claim: Integrating the higher-order network structure into transformer models through graph neural networks improves prediction robustness, particularly in the presence of label noise.
- Mechanism: The model uses a GNN to compute embeddings from the graph structure, which are then concatenated with standard embeddings and fed into transformer encoder blocks, creating a more robust representation that leverages the graph's higher-order dependencies.
- Core assumption: The graph structure provides meaningful additional information that transformers can effectively utilize, and the GNN embedding approach captures the relevant higher-order relationships.
- Evidence anchors:
  - [abstract]: "demonstrate how structural information from the proposed graph can be used to augment the performance of transformer-based models on predictive tasks, especially when the data are noisy"
  - [section 4.2.2]: "Essentially, this means that we replace BEHRT's basic, fully-connected embedding layer (step 1 above) with a GNN, which learns embeddings mediated by the given graph structure"
- Break condition: If the graph embeddings do not provide meaningful additional information, or if the concatenation approach creates interference rather than augmentation.

## Foundational Learning

- Concept: Higher-order networks (HONs) and conditional nodes
  - Why needed here: The paper extends HONs from unsupervised to supervised learning, requiring understanding of how conditional nodes encode higher-order dependencies
  - Quick check question: How does a conditional node like 401|584 differ from a regular node 401 in a HON?

- Concept: Conditional entropy and information gain
  - Why needed here: The model uses these concepts to identify which higher-order dependencies are informative of outcomes
  - Quick check question: Why does the model use Fisher-Yates shuffle when computing information gain for candidate nodes?

- Concept: Graph neural networks (GNNs) and their integration with transformers
  - Why needed here: The model uses GNNs to extract embeddings from the higher-order network structure to augment transformer models
  - Quick check question: How does the GNN embedding approach differ from standard token embeddings in transformers?

## Architecture Onboarding

- Component map: Trajectory preprocessing → Conditional node identification → Graph construction → Embedding extraction → Transformer prediction → Classification
- Critical path: Trajectory → Conditional node identification → Graph construction → Embedding extraction → Transformer prediction → Classification
- Design tradeoffs:
  - Higher-order dependencies vs. computational complexity (k=4 chosen as practical limit)
  - Strict linear vs. subsequence-based edge construction (latter enables noise skipping but increases computation)
  - GNN complexity vs. embedding quality (simple 2-layer GCN used, but more sophisticated GNNs could improve performance)
  - Statistical significance threshold (α=1.0) vs. model sensitivity to outcome-relevant patterns
- Failure signatures:
  - Low information gain values across all graphs suggests poor outcome-outcome relationship capture
  - High computational cost with limited performance gain suggests inefficiency in higher-order dependencies
  - Transformer performance without graph augmentation matching or exceeding augmented version suggests GNN embeddings not adding value
  - Paths that cannot be reconstructed in baseline networks suggest overly complex dependencies
- First 3 experiments:
  1. Verify information gain computation: Compare IG(Y,G) values for a simple test graph with known outcome dependencies
  2. Test conditional node identification: Run the Fisher-Yates shuffle on synthetic data with known signal vs. noise patterns
  3. Validate GNN embedding integration: Compare transformer performance with and without graph embeddings on a small synthetic dataset with clear higher-order dependencies

## Open Questions the Paper Calls Out

- **Optimal order selection**: The authors acknowledge that k=4 was chosen based on prior work showing dependencies saturated at this point, but note this may vary across conditions and don't systematically explore how the optimal order varies with different outcomes, dataset sizes, or trajectory characteristics.

- **Multimodal data integration**: The paper recognizes that diagnosis history is unlikely to be the only factor driving patients to specific clinical outcomes, and mentions incorporating procedures, lab results, and demographics as ongoing work.

- **Scalability to rare outcomes**: While the authors evaluate on outcomes with reasonable prevalence (4-11% positive cases), they don't report performance on very rare disease outcomes that are common in healthcare applications.

## Limitations

- The method's dependence on Fisher-Yates shuffle for statistical significance testing may introduce instability for rare outcome events where permutation-based p-values may be unreliable.

- The computational complexity of higher-order dependencies (k=4) may limit scalability to larger patient populations or longer trajectories.

- The assumption that subsequence-based edge construction optimally captures outcome-relevant dependencies hasn't been thoroughly validated against alternative approaches like variable-order Markov models or attention-based methods.

## Confidence

- **High Confidence**: The core mechanism of using conditional entropy to identify outcome-driven dependencies (Mechanism 1) is well-grounded in information theory and the experimental results with information gain metrics are directly computed from the proposed method.
- **Medium Confidence**: The effectiveness of subsequence-based edge construction (Mechanism 2) is supported by improved prediction accuracy, but the ablation studies don't isolate the contribution of this specific innovation from other components.
- **Medium Confidence**: The claim that GNN-augmented transformers improve robustness to label noise (Mechanism 3) is supported by AUPRC improvements, but the experiments don't systematically vary noise levels to demonstrate the noise-robustness relationship.

## Next Checks

1. **Statistical robustness test**: Vary the Fisher-Yates shuffle parameters (number of permutations, significance threshold α) and measure stability of conditional node identification across multiple runs on the same dataset.

2. **Computational complexity analysis**: Measure wall-clock time and memory usage for graph construction at different orders (k=2,3,4,5) and trajectory lengths to establish practical scaling limits.

3. **Ablation study for edge construction**: Compare performance of the proposed subsequence-based approach against strictly linear edge construction and alternative methods like k-skip-n-grams to isolate the contribution of the noise-skipping mechanism.