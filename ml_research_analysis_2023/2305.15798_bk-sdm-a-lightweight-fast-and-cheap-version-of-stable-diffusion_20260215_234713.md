---
ver: rpa2
title: 'BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion'
arxiv_id: '2305.15798'
source_url: https://arxiv.org/abs/2305.15798
tags:
- diffusion
- u-net
- generation
- original
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for compressing large text-to-image
  diffusion models, specifically focusing on Stable Diffusion models (SDMs). The key
  idea is to remove architectural blocks from the U-Net component of SDMs and train
  the resulting compact model using knowledge distillation from the original SDM.
---

# BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion

## Quick Facts
- arXiv ID: 2305.15798
- Source URL: https://arxiv.org/abs/2305.15798
- Authors: 
- Reference count: 40
- Key outcome: Achieves up to 50% reduction in model size and latency while maintaining competitive performance on MS-COCO benchmark

## Executive Summary
BK-SDM introduces a method for compressing large text-to-image diffusion models by removing architectural blocks from the U-Net component and training the resulting compact model using knowledge distillation from the original Stable Diffusion model. The approach achieves significant efficiency gains - up to 50% reduction in model size and latency - while maintaining competitive performance on zero-shot image generation benchmarks. The authors demonstrate that their lightweight models can be effectively used for personalized image generation with reduced finetuning cost and memory requirements.

## Method Summary
The method focuses on compressing Stable Diffusion models by eliminating specific residual and attention blocks from the U-Net architecture. The first residual-attention pairs in each stage are preserved as they process critical spatial information, while subsequent pairs are removed to reduce computation. The compact models are trained using knowledge distillation from the original SDM, employing both feature-level and output-level distillation objectives. The training uses a small fraction of the original training data (0.22M pairs versus 600M) and demonstrates that effective compression can be achieved under limited resources.

## Key Results
- Achieves FID scores of 15.76 with 0.76B parameters (BK-SDM-Base)
- Achieves FID scores of 16.98 with 0.66B parameters (BK-SDM-Small)
- Achieves FID scores of 17.12 with 0.50B parameters (BK-SDM-Tiny) on zero-shot MS-COCO benchmark

## Why This Works (Mechanism)

### Mechanism 1: Residual and Attention Block Elimination
Removing specific pairs of residual and attention blocks from the U-Net reduces model size and latency without significant performance loss. The first R-A pairs in each stage are preserved because they process changed spatial information and are more critical, while subsequent pairs are removed to reduce computation. The core assumption is that the first R-A pairs contain the most important spatial information processing, and later pairs are redundant.

### Mechanism 2: Knowledge Distillation for Compressed Models
Training compact U-Nets with feature and output distillation from the original SDM enables competitive performance despite reduced parameters. The student model learns to mimic the teacher's outputs at both feature and output levels, with feature distillation providing abundant guidance across multiple stages. The core assumption is that feature-level distillation provides sufficient guidance for the student to learn the teacher's behavior effectively.

### Mechanism 3: Mid-Stage and Innermost Stage Removal
Removing entire mid-stages and innermost stages from the U-Net reduces computation while maintaining generation quality. The mid-stage and innermost stages are less critical for generation quality, as demonstrated by minimal performance degradation when removed. The core assumption is that outer stages with larger spatial dimensions and their skip connections play a crucial role in the U-Net for T2I synthesis, while inner stages are less important.

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD is the core mechanism that enables the compressed model to learn from the full model, maintaining performance despite reduced parameters.
  - Quick check question: What are the differences between output-level and feature-level knowledge distillation, and when would you use each?

- Concept: Diffusion Models and U-Net Architecture
  - Why needed here: Understanding the U-Net architecture and how diffusion models work is essential for grasping why certain blocks can be removed and how the model functions.
  - Quick check question: How does the U-Net in a diffusion model process latent representations through its down-sampling and up-sampling stages?

- Concept: Model Compression Techniques
  - Why needed here: This work uses architectural compression (removing blocks) rather than other compression methods like quantization, so understanding different compression approaches is important.
  - Quick check question: What are the trade-offs between different model compression techniques (pruning, quantization, distillation) in terms of performance and efficiency?

## Architecture Onboarding

- Component map:
  Original SDM: Text Encoder → U-Net (with full blocks) → Image Decoder
  BK-SDM: Text Encoder → Compressed U-Net (with reduced blocks) → Image Decoder
  Key changes: Reduced residual and attention blocks, knowledge distillation training

- Critical path:
  Text encoding → Latent space processing → Denoising steps → Image decoding
  Bottleneck: U-Net processing in each denoising step

- Design tradeoffs:
  More block removal → smaller model, faster inference, but potentially lower quality
  Less block removal → larger model, slower inference, but better quality
  More distillation → better student performance, but potentially longer training

- Failure signatures:
  Poor generation quality despite compression (blocks removed were too critical)
  Training instability or divergence (distillation not effective)
  Minimal performance gain from compression (removed blocks were essential)

- First 3 experiments:
  1. Implement block removal (first R-A pairs only) and verify parameter reduction and minimal quality loss
  2. Add knowledge distillation training and compare performance with and without distillation
  3. Test mid-stage removal and measure impact on performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of BK-SDMs change when using a larger dataset for pretraining compared to the 0.22M LAION pairs used in the study? The paper mentions using a significantly smaller dataset (0.22M pairs) compared to the original SDM training (600M pairs), but does not explore the impact of using a larger dataset.

### Open Question 2
How do the computational efficiency gains of BK-SDMs compare when integrated with other efficient diffusion techniques, such as reduced sampling steps or quantization? The paper mentions that their approach is orthogonal to other efficient diffusion techniques and can be readily integrated with them, but does not provide empirical results of such integration.

### Open Question 3
What is the impact of removing different combinations of residual and attention blocks on the performance and efficiency of BK-SDMs? The paper describes the block removal strategies used for creating BK-SDM-Base, BK-SDM-Small, and BK-SDM-Tiny, but does not explore alternative combinations or provide a systematic analysis of the trade-offs involved.

## Limitations
- Limited evaluation to zero-shot MS-COCO benchmark with no analysis of generation diversity or failure cases
- Training data is only 0.1% of the original LAION dataset, raising questions about knowledge transfer completeness
- Effectiveness of knowledge distillation not systematically compared with alternative distillation methods

## Confidence
**High Confidence:** The model compression approach (removing residual and attention blocks) successfully reduces parameters and latency by the claimed percentages (30% reduction in parameters, MACs, and latency).

**Medium Confidence:** The knowledge distillation training effectively enables compressed models to maintain competitive performance on MS-COCO. While results show promising FID scores, limited training data and lack of systematic comparison with alternative distillation methods reduce confidence.

**Low Confidence:** The claim that "removing the entire mid-stage from the original U-Net does not noticeably degrade the generation quality" is based on subjective assessment rather than quantitative analysis.

## Next Checks
1. Conduct an ablation study comparing the effectiveness of feature-level distillation versus output-level distillation to validate the core knowledge transfer mechanism.

2. Evaluate the compressed models on additional benchmarks beyond MS-COCO to verify performance gains are not dataset-specific and to assess generation diversity.

3. Test the block removal strategy on different U-Net configurations or generation tasks to determine whether the assumptions about critical versus redundant blocks generalize beyond the specific Stable Diffusion architecture.