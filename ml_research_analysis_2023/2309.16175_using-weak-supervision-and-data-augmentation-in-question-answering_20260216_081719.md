---
ver: rpa2
title: Using Weak Supervision and Data Augmentation in Question Answering
arxiv_id: '2309.16175'
source_url: https://arxiv.org/abs/2309.16175
tags:
- data
- question
- questions
- training
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of training QA models during the
  early COVID-19 pandemic when there was a scarcity of annotated biomedical literature.
  The authors explore weak supervision, data augmentation, and curriculum learning
  to improve model performance.
---

# Using Weak Supervision and Data Augmentation in Question Answering

## Quick Facts
- arXiv ID: 2309.16175
- Source URL: https://arxiv.org/abs/2309.16175
- Reference count: 40
- Key outcome: Best extractive QA model achieves 39.77 EM and 42.91 F1; best yes/no QA model achieves 72% accuracy

## Executive Summary
This paper addresses the challenge of training question answering models for biomedical literature during the early COVID-19 pandemic when annotated data was scarce. The authors develop a comprehensive approach combining weak supervision, data augmentation, and curriculum learning to improve model performance. Their method uses BM25-based weak supervision to automatically label structured abstracts, curates COVID-specific QA pairs through information retrieval, and augments training data with linguistic features. The staged fine-tuning approach shows significant improvements over single-stage training.

## Method Summary
The approach uses BM25-based weak supervision to generate labels from structured abstracts, where conclusions sections are labeled as context and the most relevant sentence is identified as the answer. COVID-specific QA pairs are curated using parameterized questions based on clinicaltrials.gov schema and retrieved PubMed abstracts. The BioBERT model is fine-tuned in stages, starting with broad biomedical questions and progressing to COVID-specific questions of increasing difficulty. Linguistic features including lemmas and neo-classical combining forms are injected into training data through various text transformation techniques.

## Key Results
- Best extractive QA model: 39.77 exact match score and 42.91 F1-score
- Best yes/no QA model: 72% accuracy
- Staged fine-tuning shows significant improvement over single-stage training
- BM25-based weak supervision achieves 80% inter-annotator agreement with human labels

## Why This Works (Mechanism)

### Mechanism 1
BM25-based weak supervision enables effective training of extractive QA models when labeled data is scarce. The BM25 algorithm extracts the most similar sentence from article conclusions as the answer label, creating automatically labeled QA pairs. This works under the assumption that conclusions sections contain answers to questions posed in article titles, and that BM25 can reliably identify the most relevant sentence.

### Mechanism 2
Curriculum learning through staged fine-tuning improves domain adaptation from general biomedical questions to COVID-specific questions. The progressive fine-tuning schedule moves from broad biomedical questions (PQA-L', PQA-A') to COVID-specific questions (parameterized, PubMed COVID QA pairs), with increasing difficulty. This gradually aligns the training data distribution with the target domain distribution.

### Mechanism 3
Linguistic feature augmentation (lemmas, neo-classical combining forms) improves model robustness by introducing morphological variations. Text transformation techniques inject these features into training data through replacement, concatenation, or augmentation strategies. This complements BERT's subword tokenization by providing explicit morphological information not captured during pre-training.

## Foundational Learning

- **Weak supervision and pseudo-labeling**: Enables training models without extensive human-labeled data by automatically generating labels from structured information. Quick check: How does BM25 labeling differ from traditional supervised learning, and what assumptions must hold for it to work effectively?

- **Curriculum learning and staged training**: Addresses domain shift challenges by progressively adapting the model from general to specific question distributions. Quick check: What characteristics of the training data subsets determine the optimal order for staged fine-tuning?

- **Text augmentation and morphological variation**: Introduces linguistic patterns not captured during pre-training to improve model generalization across different word forms. Quick check: How do lemmas and neo-classical combining forms complement BERT's WordPiece tokenization, and what trade-offs exist between different augmentation strategies?

## Architecture Onboarding

- **Component map**: User Interface -> QA Management Function -> Retrieval Function (Elasticsearch) -> Answer Generation Function -> Best QA Model -> Answer
- **Critical path**: Question → QA Management Function → Retrieval Function → Answer Generation Function → Best QA Model → Answer
- **Design tradeoffs**: BM25 for weak supervision trades labeling accuracy for scalability; staged fine-tuning trades training time for domain adaptation performance; linguistic augmentation trades model complexity for robustness to morphological variation
- **Failure signatures**: Poor exact match scores indicate unreliable weak supervision labels; degraded performance after fine-tuning stages indicates domain mismatch; no improvement from augmentation indicates model doesn't leverage morphological features
- **First 3 experiments**: 1) Test BM25 labeling accuracy on small manually labeled sample, 2) Compare single-stage vs multi-stage fine-tuning on held-out validation set, 3) Evaluate different text transformation strategies on small subset

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of BM25-based weak supervision compare to human ground-truth labeling for extractive QA tasks in biomedical literature? This remains unresolved because the paper does not provide a direct comparison of model performance using BM25-based weak supervision versus human-labeled data.

### Open Question 2
How does the diversity of parameterized COVID QA pairs affect the model's ability to handle variation in clinical questions? This is unresolved because the paper lacks empirical evidence on how parameterized pair diversity impacts performance on clinical questions.

### Open Question 3
What is the optimal combination of text transformation techniques for augmenting training data in yes/no QA tasks? While combining techniques improves performance, the paper does not identify the optimal combination or provide a systematic approach to determine it.

## Limitations

- The approach relies on the assumption that conclusions sections contain answers to questions posed in article titles, which may not generalize across all biomedical literature
- The BM25-based weak supervision quality depends critically on implementation details not fully specified in the paper
- The curriculum learning effectiveness is predicated on an assumed difficulty gradient that may not align with actual question complexity

## Confidence

- **High Confidence**: Staged fine-tuning approach is well-supported by curriculum learning literature with clear implementation details
- **Medium Confidence**: BM25 weak supervision is reasonable but relies on assumptions about article structure with adequate justification but limited validation
- **Low Confidence**: Linguistic feature augmentation has minimal supporting evidence in the paper with limited related work

## Next Checks

1. Test BM25-based weak supervision on a small manually labeled sample (50-100 examples) from PubMedQA dataset to measure inter-annotator agreement between human labels and BM25-generated labels

2. Compare staged fine-tuning approach against single-stage fine-tuning and random ordering of training subsets on held-out validation set to quantify curriculum learning benefit

3. Create controlled experiments isolating effects of lemma augmentation versus neo-classical form augmentation by testing each strategy individually on same QA task and comparing to baseline without augmentation