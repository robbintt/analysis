---
ver: rpa2
title: Robust Stochastic Graph Generator for Counterfactual Explanations
arxiv_id: '2312.11747'
source_url: https://arxiv.org/abs/2312.11747
tags:
- graph
- counterfactual
- rsgg-ce
- edges
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces RSGG-CE, a Robust Stochastic Graph Generator
  for Counterfactual Explanations that generates counterfactuals from a learned latent
  space using a partial-order generation sequence. The method employs a residual GAN
  architecture with a graph autoencoder-based generator and a GCN-based discriminator.
---

# Robust Stochastic Graph Generator for Counterfactual Explanations

## Quick Facts
- arXiv ID: 2312.11747
- Source URL: https://arxiv.org/abs/2312.11747
- Reference count: 31
- Primary result: 66.98% improvement in Correctness over state-of-the-art learning-based explainers

## Executive Summary
This paper introduces RSGG-CE, a Robust Stochastic Graph Generator for Counterfactual Explanations that generates counterfactuals from a learned latent space using a partial-order generation sequence. The method employs a residual GAN architecture with a graph autoencoder-based generator and a GCN-based discriminator. RSGG-CE learns to generate counterfactuals conditioned on the input graph's class, enabling stochastic sampling of counterfactual candidates without retraining. The proposed partial-order sampling strategy leverages learned edge probabilities to guide the generation process. Experiments show that RSGG-CE outperforms state-of-the-art learning-based explainers in terms of Correctness (66.98% improvement) and Graph Edit Distance on synthetic and real datasets.

## Method Summary
RSGG-CE uses a Residual GAN architecture combining a Graph Autoencoder (GAE) generator with a GCN-based discriminator. The generator takes input graph features and adjacency matrix, encodes them into a latent representation, decodes this into a reconstructed graph, and adds this reconstruction as a residual to the original adjacency matrix. This allows both edge addition and removal operations. The discriminator is trained to distinguish between real counterfactuals and generated counterfactuals. A partial-order sampling strategy uses learned edge probabilities to guide the generation process, reducing oracle calls by sampling edges in a prioritized order.

## Key Results
- Achieves 66.98% improvement in Correctness compared to state-of-the-art learning-based explainers
- Reduces Graph Edit Distance between original graphs and counterfactuals
- Outperforms existing methods on both synthetic (Tree-Cycles) and real (Autism Spectrum Disorder) datasets
- Demonstrates robustness to varying graph complexity and dataset sizes through ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generator learns to produce plausible counterfactuals by maximizing the probability that the discriminator classifies them as real while minimizing the probability that the generator's output is classified as fake.
- Mechanism: The Residual GAN architecture combines a Graph Autoencoder (GAE) generator with a GCN-based discriminator. The generator takes the input graph's features and adjacency matrix, encodes them into a latent representation, decodes this into a reconstructed graph, and then adds this reconstruction as a residual to the original adjacency matrix. This allows both edge addition and removal operations. The discriminator is trained to distinguish between real counterfactuals (from the opposite class) and generated counterfactuals.
- Core assumption: The generator can learn to produce valid counterfactuals by learning from the class distribution of real counterfactuals without direct access to the oracle during training.
- Evidence anchors:
  - [abstract] "The proposed partial-order sampling strategy leverages learned edge probabilities to guide the generation process."
  - [section] "Unlike vanilla GANs, the input to the Residual GAN's discriminator is Aj + ˆAj for a graph (Xj, Aj)∈G."
  - [corpus] Weak evidence - the corpus mentions "Generating Robust Counterfactual Witnesses for Graph Neural Networks" which is related but doesn't directly support this specific mechanism.
- Break condition: If the generator fails to learn meaningful latent representations, or if the discriminator overfits to the training distribution, the generator may produce invalid or unrealistic counterfactuals.

### Mechanism 2
- Claim: The partial-order sampling strategy enables efficient generation of counterfactuals by sampling edges in a prioritized order based on learned probabilities.
- Mechanism: After training, the generator's learned edge probabilities are used to create a partial order of edges. The algorithm first samples existing edges with high probability of being modified, then non-existing edges. The oracle is only called after all existing edges have been sampled, reducing oracle calls.
- Core assumption: The learned edge probabilities from the generator's latent space are meaningful indicators of which edges should be modified to create counterfactuals.
- Evidence anchors:
  - [abstract] "The proposed partial-order sampling strategy leverages learned edge probabilities to guide the generation process."
  - [section] "To the best of our knowledge, this is the first work that proposes a partial-order sampling approach on estimated edges."
  - [corpus] Weak evidence - the corpus doesn't mention partial-order sampling specifically.
- Break condition: If the learned edge probabilities are not meaningful or if the oracle verification fails too frequently, the sampling strategy may not produce valid counterfactuals efficiently.

### Mechanism 3
- Claim: The method achieves high correctness by generating counterfactuals that are both close to the original graph (low GED) and valid (classified as the opposite class by the oracle).
- Mechanism: The loss function (Eq. 5) combines discriminator optimization, generator optimization, and a term that encourages the generated counterfactuals to be close to real counterfactuals from the same class. This creates a balance between producing valid counterfactuals and keeping them similar to the original graph.
- Core assumption: The oracle's classification is reliable and the generator can learn to produce graphs that fool the discriminator while being valid counterfactuals.
- Evidence anchors:
  - [abstract] "Experiments show that RSGG-CE outperforms state-of-the-art learning-based explainers in terms of Correctness (66.98% improvement) and Graph Edit Distance"
  - [section] "Integrating the accuracy of correct predictions from Φ... steers the generator away from this behaviour, making it produce realistic counterfactuals"
  - [corpus] Weak evidence - the corpus doesn't mention the specific loss function or correctness metrics.
- Break condition: If the oracle is unreliable or if the generator cannot balance the competing objectives in the loss function, the method may produce counterfactuals with low correctness or high GED.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The discriminator uses GCNs to evaluate graph plausibility, and understanding GNNs is crucial for understanding how the discriminator works.
  - Quick check question: How does a GCN aggregate information from neighboring nodes, and why is this important for evaluating graph structure?

- Concept: Generative Adversarial Networks (GANs) and residual connections
  - Why needed here: The method uses a Residual GAN architecture, which combines a generator and discriminator in a specific way with residual connections.
  - Quick check question: What is the difference between a standard GAN and a Residual GAN, and how do residual connections help in this context?

- Concept: Graph edit distance and counterfactual explanations
  - Why needed here: The method is evaluated using GED and aims to generate counterfactual explanations for graph classification.
  - Quick check question: What is graph edit distance, and why is it an appropriate metric for evaluating counterfactual explanations in graphs?

## Architecture Onboarding

- Component map: Input graph -> GAE encoding -> GAE decoding -> Residual addition -> Discriminator evaluation -> Loss calculation -> Parameter updates
- Critical path: Input graph → GAE encoding → GAE decoding → Residual addition → Discriminator evaluation → Loss calculation → Parameter updates
- Design tradeoffs:
  - Using a GAE allows learning meaningful latent representations but adds complexity compared to simpler generators
  - The residual connection enables both edge addition and removal but requires careful training to avoid instability
  - Partial-order sampling reduces oracle calls but relies on the quality of learned edge probabilities
- Failure signatures:
  - High GED with high correctness: Generator is creating valid counterfactuals but making too many changes
  - Low correctness with low GED: Generator is making minimal changes but not creating valid counterfactuals
  - Oscillating loss: Discriminator and generator are in a strong adversarial relationship, possibly due to poor architecture choices
- First 3 experiments:
  1. Train on a simple synthetic dataset (like Tree-Cycles with few nodes) and visualize the learned edge probabilities to verify the generator is learning meaningful representations
  2. Test the partial-order sampling strategy on a pre-trained model to verify it reduces oracle calls while maintaining correctness
  3. Compare GED and correctness on a validation set during training to identify when the model starts overfitting or underfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RSGG-CE's performance be improved for datasets with multiple connected components?
- Basis in paper: [inferred] The paper mentions that ASD dataset contains graphs with multiple connected components, which complicates explanations. It states: "multiple connected components in the graphs further complicate explanations, as they introduce varying degrees of separation between subgraphs."
- Why unresolved: The paper discusses the challenge but does not propose a solution or evaluate RSGG-CE's performance on such datasets.
- What evidence would resolve it: Empirical results comparing RSGG-CE's performance on datasets with single vs. multiple connected components, or proposed modifications to handle multi-component graphs.

### Open Question 2
- Question: Can RSGG-CE be extended to handle node classification tasks effectively?
- Basis in paper: [explicit] The paper briefly mentions this in Section G, stating: "However, it can be adapted for node classification as follows..." and provides a general outline.
- Why unresolved: The paper only provides a high-level description of adaptation without implementing or evaluating it.
- What evidence would resolve it: Implementation of RSGG-CE for node classification, with experimental results comparing it to existing node classification explainers.

### Open Question 3
- Question: How does the choice of partial order sampling strategy affect RSGG-CE's performance in different domains?
- Basis in paper: [explicit] The paper states: "This kind of explainer is trained on samples and thus can be used to produce counterfactual instances at inference time. In this work, we propose RSGG-CE... considering a partially ordered generation sequence." It also mentions: "the function partial_order(·,·) can be specialised according to the application domain and prediction scenario."
- Why unresolved: While the paper uses a specific partial order strategy, it doesn't explore how different strategies might perform in various domains.
- What evidence would resolve it: Comparative studies of RSGG-CE with different partial order strategies across multiple domains (e.g., molecular graphs, social networks, biological networks).

## Limitations
- The partial-order sampling strategy's effectiveness relies heavily on the quality of learned edge probabilities, which isn't thoroughly validated
- Exact implementation details of the partial-order sampling function remain underspecified
- Hyperparameter values for the Bayesian optimization process used to select the best hyperparameters are unknown

## Confidence
- Residual GAN mechanism: Medium
- Partial-order sampling efficiency: Medium
- Correctness improvement claims: Medium
- Graph edit distance reduction: Medium

## Next Checks
1. Implement and test the partial-order sampling strategy on a pre-trained model to verify it actually reduces oracle calls while maintaining counterfactual validity
2. Conduct ablation studies comparing the residual connection approach against standard GAN architectures on synthetic datasets
3. Evaluate the robustness of learned edge probabilities across different graph sizes and densities to test the method's scalability claims