---
ver: rpa2
title: Ethical Considerations for Responsible Data Curation
arxiv_id: '2302.03629'
source_url: https://arxiv.org/abs/2302.03629
tags:
- data
- image
- conference
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the ethical issues in human-centric computer
  vision (HCCV) dataset curation, particularly concerning privacy and bias. The authors
  propose a set of key ethical considerations and practical recommendations for collecting
  more ethically-minded HCCV datasets.
---

# Ethical Considerations for Responsible Data Curation

## Quick Facts
- **arXiv ID**: 2302.03629
- **Source URL**: https://arxiv.org/abs/2302.03629
- **Reference count**: 40
- **One-line primary result**: The paper proposes ethical considerations and practical recommendations for collecting more ethically-minded human-centric computer vision datasets, focusing on purpose, privacy and consent, and diversity.

## Executive Summary
This paper addresses ethical issues in human-centric computer vision (HCCV) dataset curation, particularly concerning privacy and bias. The authors propose a set of key ethical considerations and practical recommendations for collecting more ethically-minded HCCV datasets. These recommendations cover purpose, privacy and consent, and diversity, and are motivated by lessons from current practices, dataset withdrawals and audits, and analytical ethical frameworks. The paper argues for the need to clearly define a purpose statement prior to data collection, obtain voluntary informed consent from all people depicted in or identifiable from a dataset, and collect self-reported annotations rather than relying on inference.

## Method Summary
The paper synthesizes existing ethical frameworks and dataset practices to propose proactive recommendations for ethical HCCV dataset curation. The method involves defining purpose statements before data collection, implementing informed consent workflows, collecting self-reported demographic annotations, redacting privacy-leaking content, and ensuring fair compensation for contributors. The approach emphasizes shifting from post-hoc ethical audits to ante-hoc ethical planning, with specific focus on mitigating privacy risks, reducing annotation bias, and improving dataset transparency through comprehensive documentation.

## Key Results
- Dataset creators should define clear purpose statements prior to data collection to prevent purpose creep and enable proper risk assessment
- Self-reported annotations are recommended over inferred annotations to reduce bias and improve accuracy, particularly for sensitive attributes like age, gender, and ancestry
- Privacy preservation methods including redaction of nonconsensual people, text, and metadata should be implemented, with validation that these methods actually protect privacy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper's recommendations reduce future dataset retractions by embedding ethical checkpoints early in the curation pipeline.
- Mechanism: By defining purpose statements before data collection, requiring informed consent, and collecting self-reported annotations, the framework shifts ethical considerations from post-hoc audits to ante-hoc planning, reducing risk of non-compliance and harm.
- Core assumption: Dataset creators will adopt these recommendations as standard practice rather than treating them as optional guidelines.
- Evidence anchors:
  - [abstract] "Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity..."
  - [section 2.2.3] "Purpose statements which are defined prior to data collection can mitigate against purpose creep..."
- Break Condition: If adoption remains voluntary and enforcement is weak, the framework will have limited real-world impact.

### Mechanism 2
- Claim: Collecting self-reported demographic and phenotypic attributes reduces annotation bias and representational harm.
- Mechanism: Self-reported data replaces inferred annotations, which are subject to own-anchor bias and cultural stereotypes, thereby increasing accuracy and reducing psychological harm from misclassification.
- Core assumption: Image subjects will provide accurate self-reported data and feel safe doing so.
- Evidence anchors:
  - [section 4.1.4] "The diﬃculty of the task, especially in unconstrained settings, can be observed from the Open-Images MIAP [212] dataset's crowdsourced age annotations..."
  - [section 4.2.1] "Rather than collecting stereotypical annotations from (culturally unaware) annotators, we recommend that dataset creators collect accurate labels directly from image subjects..."
- Break Condition: If self-reporting introduces social desirability bias or subjects fear misuse of their data, accuracy may decrease.

### Mechanism 3
- Claim: Recontextualizing annotators as contributing factors improves transparency and accountability in dataset development.
- Mechanism: Recording annotator identity and input allows tracking of annotation bias and enables bias mitigation in trained models.
- Core assumption: Annotator demographic data will be collected ethically and without introducing new privacy risks.
- Evidence anchors:
  - [section 4.2.8] "Dataset creators should reﬂect on, and record, whose behavior will eventually be modeled..."
  - [section 4.1.9] "Recent empirical studies conﬁrm this lack of regard for the impact the social identity of annotators has on the resulting data..."
- Break Condition: If annotator privacy is not protected, this could introduce new ethical concerns and reduce participation.

## Foundational Learning

- Concept: Institutional Review Board (IRB) oversight
  - Why needed here: The paper notes CV research often bypasses IRB review by claiming minimal risk, yet this can lead to ethical oversights.
  - Quick check question: What are the three key components of informed consent that IRBs typically require?
- Concept: Purpose creep and its mitigation
  - Why needed here: The paper emphasizes that undefined purposes lead to repurposing datasets for unintended uses, creating ethical risks.
  - Quick check question: How does a purpose statement differ from post-hoc dataset documentation?
- Concept: Sensitive attribute collection and aggregate reporting
  - Why needed here: The paper recommends collecting sensitive attributes like disability and pregnancy status in aggregate to monitor representation without compromising individual privacy.
  - Quick check question: Why might collecting exact disability status for individuals pose safety concerns?

## Architecture Onboarding

- Component map: Purpose definition -> Consent workflow -> Data collection pipeline -> Annotation schema -> Privacy redaction -> Metadata management -> Contributor compensation system
- Critical path: Purpose definition -> Consent workflow -> Data collection pipeline (blocks everything downstream)
- Design tradeoffs: Self-reported vs inferred annotations (accuracy vs scalability), exact vs aggregate sensitive data (granularity vs privacy), open vs closed response options (inclusivity vs analysis complexity)
- Failure signatures: High opt-out rates (consent issues), annotation inconsistency across demographics (bias), metadata leaks (privacy failures), contributor churn (compensation issues)
- First 3 experiments:
  1. Pilot consent workflow with 50 participants measuring completion rates and comprehension
  2. Compare self-reported vs inferred annotations on a subset of 100 images for accuracy
  3. Test privacy redaction pipeline on sample images with automated and human verification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically measure and compare the effectiveness of different privacy preservation methods (e.g., blurring, inpainting, metadata redaction) in human-centric computer vision datasets?
- Basis in paper: [explicit] The paper discusses various privacy preservation methods like blurring, inpainting, and metadata redaction but notes that "obfuscation methods do not necessarily provide privacy guarantees" and recommends empirical validation.
- Why unresolved: The paper highlights the need for validation but does not provide a systematic framework or methodology for comparing different privacy preservation approaches.
- What evidence would resolve it: A comprehensive evaluation framework that tests various privacy preservation methods across multiple datasets and measures both privacy protection and utility preservation.

### Open Question 2
- Question: What are the long-term impacts of dataset documentation practices (like Datasheets and Data Cards) on the actual ethical collection and curation of human-centric datasets?
- Basis in paper: [explicit] The paper mentions dataset documentation practices like Datasheets and Data Cards but notes they represent "an intrinsically post hoc reflective process" that can be manipulated.
- Why unresolved: While documentation is discussed as important, the paper does not address how documentation practices actually influence real-world dataset collection behaviors over time.
- What evidence would resolve it: Longitudinal studies tracking how the adoption of documentation practices changes dataset collection methodologies and ethical considerations in practice.

### Open Question 3
- Question: How can we design compensation structures that fairly account for both the complexity of annotation tasks and the economic contexts of annotators in different regions?
- Basis in paper: [explicit] The paper discusses exploitative annotation practices and recommends compensation "above the minimum wage of their country of residence" and "according to the complexity of tasks."
- Why unresolved: The paper identifies the problem and makes recommendations but does not provide concrete frameworks for balancing fair compensation with economic realities across different regions.
- What evidence would resolve it: Empirical studies comparing different compensation models and their effects on annotation quality, annotator well-being, and dataset quality across various economic contexts.

## Limitations

- The framework relies on voluntary adoption by dataset creators without enforcement mechanisms or regulatory requirements
- Self-reported annotations may introduce new biases through social desirability effects or fear of data misuse
- Practical implementation details for complex recommendations like aggregate sensitive attribute collection remain underdeveloped

## Confidence

- **High Confidence**: The core argument that purpose statements and informed consent reduce ethical risks has strong empirical support from existing dataset withdrawal cases and regulatory frameworks
- **Medium Confidence**: The recommendation for self-reported annotations is well-supported theoretically but lacks extensive empirical validation in computer vision contexts
- **Low Confidence**: The practical implementation details for complex recommendations (e.g., aggregate sensitive attribute collection, fair compensation systems) remain underdeveloped

## Next Checks

1. **Pilot Implementation Study**: Test the complete ethical framework with a small dataset (50-100 images) to measure consent completion rates, annotation accuracy differences, and participant comprehension of purpose statements
2. **Bias Comparison Analysis**: Compare model performance and fairness metrics between datasets using inferred versus self-reported demographic annotations across multiple HCCV benchmarks
3. **Stakeholder Impact Assessment**: Conduct structured interviews with dataset contributors, annotators, and intended users to evaluate the framework's practical burdens versus perceived ethical benefits