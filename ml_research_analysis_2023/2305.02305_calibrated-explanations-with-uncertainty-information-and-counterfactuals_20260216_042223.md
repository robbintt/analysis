---
ver: rpa2
title: 'Calibrated Explanations: with Uncertainty Information and Counterfactuals'
arxiv_id: '2305.02305'
source_url: https://arxiv.org/abs/2305.02305
tags:
- feature
- explanations
- probability
- uncertainty
- calibrated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Calibrated Explanations (CE), a feature importance
  explanation method that addresses instability, poor calibration, and lack of uncertainty
  quantification in local explanations for AI models. CE is based on Venn-Abers and
  calibrates the underlying model while generating reliable, stable, and robust explanations
  with uncertainty quantification.
---

# Calibrated Explanations: with Uncertainty Information and Counterfactuals

## Quick Facts
- arXiv ID: 2305.02305
- Source URL: https://arxiv.org/abs/2305.02305
- Reference count: 10
- Primary result: Calibrated Explanations (CE) provides stable, calibrated, and uncertainty-aware local explanations for AI models using Venn-Abers calibration and feature perturbation

## Executive Summary
This paper introduces Calibrated Explanations (CE), a feature importance explanation method that addresses instability, poor calibration, and lack of uncertainty quantification in local explanations for AI models. CE is based on Venn-Abers and calibrates the underlying model while generating reliable, stable, and robust explanations with uncertainty quantification. The method provides fast, easily interpretable rules and can generate counterfactual explanations with embedded uncertainty quantification. An evaluation with 25 benchmark datasets demonstrates CE's efficacy, showing improved calibration and log loss compared to uncalibrated models, while also being faster than existing methods like LIME and SHAP for calibrated models.

## Method Summary
Calibrated Explanations combines Venn-Abers calibration with feature importance extraction through conditional rules. The method first calibrates the underlying model using isotonic regression on a held-out calibration set, producing probability intervals that quantify uncertainty. For each instance to explain, CE perturbs features according to a discretizer, recalculates calibrated probabilities, and defines feature weights as the difference between original and perturbed probabilities. The approach is model-agnostic, working with any classifier that outputs probability scores, and can generate counterfactual explanations with uncertainty quantification through simple conditional rules.

## Key Results
- CE produces calibrated probability estimates with reduced log loss compared to uncalibrated models
- The method generates stable and reliable explanations with quantified uncertainty intervals
- CE is faster than LIME and SHAP for calibrated models while providing comparable or better calibration quality
- The approach works across 25 diverse binary classification datasets from UCI and PROMISE repositories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Venn-Abers calibration aligns predicted probabilities with observed frequencies, reducing systematic overconfidence in ML models.
- Mechanism: Venn-Abers uses isotonic regression to transform raw model scores into calibrated probability intervals, where the width of the interval quantifies uncertainty.
- Core assumption: The calibration set is representative of the data distribution, and isotonic regression can monotonically map raw scores to true probabilities.
- Evidence anchors:
  - [abstract]: "VA produces a probability interval for each prediction, which can be aggregated into a calibrated probability estimate through regularization"
  - [section]: "VA produces a probability interval for each prediction, which can be aggregated into a calibrated probability estimate through regularization to compare with other calibration methods or the underlying model's probability estimate"
  - [corpus]: Weak correlation; no direct citations to Venn-Abers calibration methods.
- Break condition: If the calibration set is not representative or isotonic regression assumptions are violated (e.g., non-monotonic relationships), calibration may not improve or could degrade predictions.

### Mechanism 2
- Claim: Calibrated Explanations (CE) creates feature weights that are directly interpretable as the change in calibrated probability estimate when a feature value is altered.
- Mechanism: CE perturbs each feature independently, recalculates calibrated probabilities, and defines the weight as the difference between the original calibrated probability and the average probability without that feature.
- Core assumption: Feature independence holds for the calculation, so that the effect of each feature can be isolated without considering interactions.
- Evidence anchors:
  - [abstract]: "CE goes beyond conventional solutions by addressing output uncertainty... providing uncertainty quantification for both feature weights and the model's probability estimates"
  - [section]: "the feature weights are defined to be the amount each feature contributes to the calibrated probability estimate for the positive class"
  - [corpus]: Weak correlation; limited literature connecting Venn-Abers to feature importance methods.
- Break condition: If feature interactions are significant and cannot be captured by simple marginal perturbations, the weights may misrepresent true contributions.

### Mechanism 3
- Claim: The method is model-agnostic because calibration and explanation generation operate on the output probabilities rather than the model internals.
- Mechanism: CE treats the underlying model as a black box, using only its probability outputs on perturbed instances, so any classifier producing scores can be explained.
- Core assumption: The model outputs scores that can be calibrated and perturbed without needing gradient information or access to internal representations.
- Evidence anchors:
  - [abstract]: "The method is model-agnostic, featuring easily comprehensible conditional rules"
  - [section]: "CE is model agnostic, since it is applied to the underlying model"
  - [corpus]: Weak correlation; no direct citations to model-agnostic explanation methods using Venn-Abers.
- Break condition: If the model only outputs hard class labels or does not provide a meaningful scoring function, the method cannot be applied.

## Foundational Learning

- Concept: Venn-Abers calibration and isotonic regression
  - Why needed here: Provides the theoretical basis for producing calibrated probability intervals and uncertainty quantification.
  - Quick check question: What is the role of isotonic regression in Venn-Abers, and how does it differ from Platt scaling?

- Concept: Feature importance via marginal perturbation
  - Why needed here: CE calculates feature weights by measuring the impact of removing or altering each feature, assuming feature independence.
  - Quick check question: How does CE define the feature weight, and what assumption about feature interactions does this rely on?

- Concept: Counterfactual explanations and uncertainty propagation
  - Why needed here: Enables generation of actionable "what-if" rules while preserving uncertainty quantification.
  - Quick check question: How does CE construct counterfactual rules, and how are uncertainty intervals propagated to these rules?

## Architecture Onboarding

- Component map:
  - Underlying ML model (black box, outputs scores) -> Venn-Abers calibrator (isotonic regression on calibration set) -> Discretizer (binary or entropy-based for numerical features) -> Perturbation engine (generates instances by altering feature values) -> Probability interval calculator (VA applied to each perturbed instance) -> Weight and uncertainty aggregator (computes differences and intervals) -> Rule formatter (translates feature groups and values into interpretable conditions) -> Plot generator (visualizes regular, uncertainty, and counterfactual explanations)

- Critical path:
  1. Train underlying model on training set.
  2. Apply Venn-Abers calibration using calibration set.
  3. For each instance to explain, perturb features as per discretizer.
  4. Compute probability intervals for each perturbed instance.
  5. Calculate feature weights and uncertainty intervals.
  6. Format and visualize explanations.

- Design tradeoffs:
  - Speed vs. granularity: More perturbation groups increase accuracy but slow computation.
  - Binary vs. multi-threshold discretizers: Binary rules are easier to interpret but may miss nuanced effects.
  - Model-agnosticism vs. exploitability: Black-box use avoids model-specific optimizations.

- Failure signatures:
  - Weights not summing to prediction change: Indicates feature interaction effects or discretizer misalignment.
  - Wide uncertainty intervals: Suggests calibration set is too small or model is overconfident.
  - Unchanged weights across perturbations: May indicate poor discretizer thresholds or lack of variation in calibration data.

- First 3 experiments:
  1. Run CE on a simple logistic regression with a small, synthetic dataset to verify weight interpretation matches intuition.
  2. Compare CE uncertainty intervals against ground truth calibration errors on a UCI dataset.
  3. Measure runtime and stability of CE across multiple random seeds for the same dataset and model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do conjunctive rules affect the computational cost and interpretability of Calibrated Explanations?
- Basis in paper: [explicit] The paper mentions that conjunctive rules could partly address the shortcoming of only considering one feature at a time, but implementing them would dramatically increase computational cost.
- Why unresolved: The authors did not explore conjunctive rules due to the expected computational burden and potential complexity in interpretability.
- What evidence would resolve it: Empirical results comparing explanation quality, computational efficiency, and interpretability with and without conjunctive rules across multiple datasets.

### Open Question 2
- Question: How does Calibrated Explanations perform on multi-class classification problems?
- Basis in paper: [explicit] The paper states that extending CE to multi-class problems is trivial but left for future work, as Venn-Abers is limited to binary classification.
- Why unresolved: The authors did not implement or test CE on multi-class problems, despite acknowledging the straightforward extension approach.
- What evidence would resolve it: Results showing explanation quality metrics (stability, reliability, uncertainty quantification) for CE applied to multi-class datasets compared to binary classification.

### Open Question 3
- Question: How does Calibrated Explanations perform on regression problems?
- Basis in paper: [explicit] The authors explicitly state that developing support for regression is an important direction for future work.
- Why unresolved: The method is currently only designed for classification problems, and no adaptation or testing has been done for regression tasks.
- What evidence would resolve it: Implementation and evaluation of CE on regression datasets, showing how feature importance and uncertainty quantification work in continuous output spaces.

## Limitations
- The method relies on the assumption of feature independence, which may not hold in datasets with significant feature interactions
- No direct validation against ground truth feature importance or comparison with state-of-the-art explanation methods beyond calibration metrics
- Unclear handling of numerical features with continuous distributions in the discretizer and perturbation process

## Confidence

- Mechanism 1 (Venn-Abers calibration): High - based on established methods
- Mechanism 2 (Feature importance via perturbation): Medium - novel application, limited validation
- Mechanism 3 (Model-agnosticism): High - follows standard black-box explanation patterns

## Next Checks

1. Compare CE feature weights against ground truth on synthetic datasets where true feature importance is known
2. Benchmark CE's explanation stability and fidelity against LIME and SHAP using established metrics (e.g., stability, faithfulness)
3. Test CE's performance on datasets with known feature interactions to assess the impact of the feature independence assumption