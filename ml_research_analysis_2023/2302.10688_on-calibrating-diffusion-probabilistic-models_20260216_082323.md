---
ver: rpa2
title: On Calibrating Diffusion Probabilistic Models
arxiv_id: '2302.10688'
source_url: https://arxiv.org/abs/2302.10688
tags:
- data
- calibration
- score
- logqt
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses miscalibration in diffusion probabilistic\
  \ models (DPMs) by leveraging the martingale property of scaled data scores. The\
  \ key insight is that \u03B1t\u2207xt log qt(xt) forms a martingale in the reverse-time\
  \ process, leading to a simple calibration method: subtracting the expected score\
  \ E{qt(xt)}[s\u03B8(xt)] from a pretrained model."
---

# On Calibrating Diffusion Probabilistic Models

## Quick Facts
- arXiv ID: 2302.10688
- Source URL: https://arxiv.org/abs/2302.10688
- Reference count: 40
- Key outcome: Simple post-training calibration of DPMs by subtracting expected scores improves FID scores and likelihood bounds

## Executive Summary
This paper addresses miscalibration in diffusion probabilistic models (DPMs) by leveraging the martingale property of scaled data scores. The key insight is that α_t∇_x_t log q_t(x_t) forms a martingale in the reverse-time process, enabling a simple calibration method: subtracting the expected score E_{q_t(x_t)}[s_θ(x_t)] from a pretrained model. This calibration provably reduces score matching objectives and improves lower bounds of model likelihood. Empirically, the method consistently improves sample quality (FID scores) on CIFAR-10 and CelebA datasets when used with DPM-Solver sampling, and enhances model likelihood bounds. The calibration can be performed post-training using training data or generated samples, and extends naturally to conditional models and various model parametrizations.

## Method Summary
The method involves computing the expected score E_{q_t(x_t)}[s_θ(x_t)] at each timestep using training data or generated samples via Monte Carlo estimation, then subtracting this calibration term from the pretrained model's score predictions. This can be done either as a post-processing step after training or dynamically during training using an auxiliary network to record the calibration terms. The calibrated model is then used with DPM-Solver or other sampling methods. The approach is model-agnostic and works with various score-based model parametrizations.

## Key Results
- FID scores improve by 2.63 on CIFAR-10 and 1.38 on CelebA when using calibrated DPMs with DPM-Solver sampling
- Calibration reduces score matching objectives by exactly ½||E_{q_t(x_t)}[s_θ(x_t)]||²₂
- Higher-order ODE solvers (DPM-Solver++) benefit more from calibration than lower-order solvers
- The method improves lower bounds of model likelihood across different model parametrizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The scaled data score α_t ∇_{x_t} log q_t(x_t) is a martingale in the reverse-time process, which means its expected value is constant over time.
- Mechanism: Because the scaled data score forms a martingale, we can subtract its expectation at each timestep from the pretrained model's score prediction. This calibration term is time-dependent but input-independent, so it doesn't affect the sampling dynamics while provably reducing the score matching objective.
- Core assumption: The forward diffusion process satisfies the Gaussian transition property with parameters α_t and σ_t that meet regularity conditions for martingale properties to hold.
- Evidence anchors:
  - [abstract] "we observe that the stochastic reverse process of data scores is a martingale"
  - [section] "Theorem 1 indicates that the stochastic process of the scaled data score α_t ∇_{x_t} log q_t(x_t) is a martingale w.r.t. the reverse-time process"
  - [corpus] Weak evidence - no direct citations found in neighbor papers about martingale properties in diffusion models
- Break condition: If the forward process doesn't follow the Gaussian transition structure, or if α_t becomes negative or zero at any timestep, the martingale property fails and calibration loses its theoretical guarantee.

### Mechanism 2
- Claim: Subtracting the expected score E_{q_t(x_t)}[s_θ(x_t)] reduces the score matching objective by exactly ½||E_{q_t(x_t)}[s_θ(x_t)]||²₂.
- Mechanism: The calibration creates an optimal unbiased estimator of the data score by removing the systematic bias in the pretrained model. This directly translates to tighter evidence lower bounds for model likelihood.
- Core assumption: The score matching objective is convex in the calibration term and the optimal solution is achieved when the expected score equals zero.
- Evidence anchors:
  - [section] "Eq. (16) indicates that for any pretrained score model s_θ(x_t), we can calibrate it into s_θ(x_t)-E_{q_t(x_t)}[s_θ(x_t)]"
  - [section] "reduces the SM/DSM objectives at timestep t by ½||E_{q_t(x_t)}[s_θ(x_t)]||²₂"
  - [corpus] No direct evidence found about objective reduction in neighbor papers
- Break condition: If the pretrained model already has zero expected score (perfect calibration), subtracting the expectation provides no benefit and may introduce unnecessary noise.

### Mechanism 3
- Claim: The calibration preserves the conservative property of score-based models, meaning there exists a probability distribution whose gradient equals the calibrated score.
- Mechanism: Subtracting a constant vector from a conservative score field maintains the conservative property because gradients are invariant to constant shifts. This ensures the calibrated model still corresponds to a valid probability distribution.
- Core assumption: The original pretrained model s_θ(x_t) is conservative, i.e., there exists a distribution p_θ(x_t) such that s_θ(x_t) = ∇_{x_t} log p_θ(x_t).
- Evidence anchors:
  - [section] "If s_θ(x_t) is conservative, its calibrated version s_θ(x_t)-η_t is also conservative"
  - [section] "there exists a probability distribution p_θ(x_t) such that ∀ x_t ∈ ℝ^k, we have s_θ(x_t) = ∇_{x_t} log p_θ(x_t)"
  - [corpus] No direct evidence found about conservative property in neighbor papers
- Break condition: If the original model is not conservative (common in practice), the calibrated version may not correspond to any valid probability distribution, though sampling can still work empirically.

## Foundational Learning

- Concept: Martingale theory in stochastic processes
  - Why needed here: The core insight relies on recognizing that the scaled data score forms a martingale, which enables the calibration technique
  - Quick check question: If X_t is a martingale, what is E[X_T | F_t] for t < T?

- Concept: Score matching and denoising score matching objectives
  - Why needed here: Understanding how calibration reduces these objectives is crucial for appreciating the theoretical guarantees
  - Quick check question: What is the relationship between score matching and denoising score matching objectives up to constants?

- Concept: Diffusion probabilistic models and their reverse-time processes
  - Why needed here: The calibration operates on the reverse-time score dynamics, so understanding forward/reverse process duality is essential
  - Quick check question: In a diffusion model, what is the role of the reverse-time SDE compared to the forward-time SDE?

## Architecture Onboarding

- Component map: Pretrained DPM model -> Calibration term estimator -> Score subtraction -> Sampling engine
- Critical path: Pretrained model → Calibration term computation → Score subtraction → Sampling
- Design tradeoffs:
  - Post-training calibration requires access to training data or high-quality generations, while dynamic recording adds training overhead
  - Higher-order ODE solvers benefit more from calibration, creating a synergy between solver order and calibration quality
  - The calibration term computation can be amortized across many samples but requires storage
- Failure signatures:
  - If FID scores don't improve after calibration, likely issues include incorrect expectation estimation or the original model being already well-calibrated
  - If sampling becomes unstable, possible causes include numerical issues in the calibration term application or the original model not being conservative
  - If calibration term computation is too expensive, consider reducing the number of samples used for Monte Carlo estimation
- First 3 experiments:
  1. Calibrate a pretrained DDPM on CIFAR-10 using training data for expectation estimation, compare FID scores with original model using DPM-Solver
  2. Test calibration effectiveness when using only a subset of training data (e.g., 10%, 50%, 100%) to understand data efficiency
  3. Apply calibration to a conditional DPM and evaluate whether conditional expectations improve generation quality for specific classes

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Theoretical guarantees rely on idealized assumptions about forward diffusion process structure
- Empirical validation limited to CIFAR-10 and CelebA datasets with DPM-Solver sampling
- Computational overhead for computing calibration terms may be prohibitive for very large models

## Confidence
- **High confidence**: The martingale property of scaled data scores and its implications for calibration (supported by rigorous mathematical proofs)
- **Medium confidence**: The empirical FID improvements on CIFAR-10 and CelebA (well-documented but limited to specific datasets and sampling methods)
- **Low confidence**: The generalization of calibration benefits to unconditional models, different sampling algorithms, and larger-scale datasets (not thoroughly explored)

## Next Checks
1. **Dataset Generalization Test**: Apply the calibration method to unconditional diffusion models on larger datasets (e.g., ImageNet-32x32) to verify if FID improvements scale with dataset complexity.
2. **Sampling Method Robustness**: Evaluate calibration effectiveness when used with non-DPM-Solver sampling methods (e.g., ancestral sampling, DDIM) to test method generality.
3. **Computational Overhead Analysis**: Measure the wall-clock time and memory requirements for both post-training and dynamic recording calibration approaches across different model sizes to quantify practical feasibility.