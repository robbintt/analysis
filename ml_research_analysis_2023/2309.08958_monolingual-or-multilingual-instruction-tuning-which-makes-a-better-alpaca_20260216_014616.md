---
ver: rpa2
title: 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca'
arxiv_id: '2309.08958'
source_url: https://arxiv.org/abs/2309.08958
tags:
- language
- multilingual
- tuning
- data
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates monolingual vs multilingual instruction
  tuning for large language models. It finds that multilingual tuning with downsampled
  data can achieve similar performance to monolingual tuning for each language while
  being more robust across languages.
---

# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca
## Quick Facts
- arXiv ID: 2309.08958
- Source URL: https://arxiv.org/abs/2309.08958
- Reference count: 5
- One-line primary result: Multilingual instruction tuning with downsampled data can achieve similar performance to monolingual tuning for each language while being more robust across languages

## Executive Summary
This paper investigates whether monolingual or multilingual instruction tuning produces better performance for large language models across multiple languages. The study finds that multilingual tuning with downsampled data can achieve comparable performance to monolingual tuning for each language while being more robust across languages. With a fixed budget, multilingual tuning is shown to be more effective than monolingual tuning for each language, providing guidance for expanding language support in large language models through instruction tuning with limited computational resources.

## Method Summary
The study compares three training approaches - monolingual instruction tuning (fine-tuning on each language separately), multilingual instruction tuning (fine-tuning on a mix of all languages), and using an English-tuned model to respond to other languages. Both low-rank adaptation (LoRA) and full-parameter fine-tuning are experimented with. The base LLMs used include Pythia, LLaMA, OpenLLaMA, and BLOOM at various scales. The Alpaca dataset is machine translated into eight languages (Bulgarian, Czech, Chinese, German, Finnish, French, Russian, and Spanish) for multilingual training data. Test data consists of 50 manually translated prompts from OpenAssistant into seen and unseen languages. Models are evaluated using GPT-3.5 as a judge, with scores summed across all test samples, and language consistency is also assessed.

## Key Results
- Multilingual instruction tuning with downsampled data can achieve similar performance to monolingual tuning for each language
- Multilingual tuning is more robust across all test languages compared to monolingual approaches
- With a fixed computational budget, multilingual tuning is more effective than monolingual tuning for each language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual instruction tuning with downsampled data can achieve comparable performance to monolingual tuning across languages.
- Mechanism: The multilingual model learns shared linguistic patterns across languages, enabling it to generalize well even with reduced data per language. Downsampling prevents overfitting to individual languages while maintaining cross-lingual transfer.
- Core assumption: The pre-trained model has sufficient multilingual transfer capability, and machine-translated instruction data preserves the semantic intent across languages.
- Evidence anchors:
  - [abstract] "multilingual tuning with downsampled data can be as powerful and more robust"
  - [section] "models trained on downsampled multilingual data are significantly more robust across all test languages"
  - [corpus] Weak - related papers discuss multilingual instruction tuning but don't specifically address downsampling strategy.
- Break condition: If the base model has poor multilingual transfer capability, or if machine translation introduces significant semantic drift.

### Mechanism 2
- Claim: LoRA fine-tuning is more effective than full-parameter fine-tuning for multilingual instruction tuning.
- Mechanism: LoRA adds low-rank adaptation layers that preserve the original model's multilingual knowledge while learning task-specific instructions, avoiding catastrophic forgetting of multilingual capabilities.
- Core assumption: The base model's multilingual knowledge is valuable and should be preserved during instruction tuning.
- Evidence anchors:
  - [section] "multilingualism works well with LoRA, but with full-parameter, separate monolingual tuning might be better"
  - [section] "superior results are obtained when using multilingual tuning with LoRA"
  - [corpus] Weak - corpus doesn't specifically compare LoRA vs full-parameter for multilingual tuning.
- Break condition: If the task requires significant parameter updates that LoRA cannot accommodate.

### Mechanism 3
- Claim: Language consistency in responses is better with multilingual instruction tuning than with English-only tuning.
- Mechanism: Exposure to multiple languages during training helps the model maintain language awareness and reduces the tendency to default to English regardless of the input language.
- Core assumption: Language consistency is learned behavior that requires explicit multilingual exposure during training.
- Evidence anchors:
  - [section] "English-only models to be the least robust" in terms of language consistency
  - [section] "models trained on downsampled multilingual data are significantly more robust across all test languages"
  - [corpus] Weak - corpus doesn't discuss language consistency evaluation.
- Break condition: If the model prioritizes task completion over language matching, or if evaluation criteria don't penalize language inconsistency.

## Foundational Learning

- Concept: Instruction tuning methodology
  - Why needed here: Understanding how instruction tuning differs from standard fine-tuning is crucial for implementing the experiments and interpreting results
  - Quick check question: What distinguishes instruction tuning from regular fine-tuning in terms of data format and training objectives?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: LoRA is the primary parameter-efficient fine-tuning method used in the experiments, and understanding its mechanics is essential for implementation
  - Quick check question: How does LoRA modify the attention matrices during fine-tuning, and what are the trade-offs compared to full-parameter tuning?

- Concept: Cross-lingual transfer
  - Why needed here: The paper relies heavily on the ability of models to transfer knowledge across languages, which is fundamental to understanding the multilingual results
  - Quick check question: What factors influence a model's ability to transfer knowledge from one language to another during instruction tuning?

## Architecture Onboarding

- Component map:
  - Alpaca dataset -> Machine translation -> Multilingual dataset creation
  - LoRA and full-parameter fine-tuning implementations
  - LLM-as-a-judge with language identification
  - Base models: Pythia, LLaMA, OpenLLaMA, BLOOM at various scales

- Critical path:
  1. Create multilingual instruction data via machine translation
  2. Implement LoRA and full-parameter fine-tuning procedures
  3. Set up evaluation pipeline with language identification
  4. Run experiments across different model sizes and data conditions
  5. Analyze results focusing on performance, robustness, and language consistency

- Design tradeoffs:
  - LoRA vs full-parameter: Parameter efficiency vs potential performance gains
  - Multilingual vs monolingual: Robustness across languages vs potential specialization
  - Downsampled vs full data: Computational efficiency vs potential performance loss

- Failure signatures:
  - Poor cross-lingual transfer: English-tuned models perform well but other languages fail
  - Language inconsistency: Model outputs don't match input language despite evaluation criteria
  - Overfitting to seen languages: Poor performance on unseen languages during testing

- First 3 experiments:
  1. Implement LoRA fine-tuning on a small Pythia model with monolingual English data, verify basic functionality
  2. Add machine translation to create multilingual dataset, implement multilingual LoRA fine-tuning, compare results
  3. Implement language identification module, modify evaluation pipeline to penalize language inconsistency, verify language consistency metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multilingual instruction tuning compare to monolingual tuning for low-resource languages when using models trained on the same computational budget?
- Basis in paper: [explicit] The paper compares multilingual and monolingual tuning under fixed computational budgets and finds that multilingual tuning with downsampled data can be as powerful and more robust than monolingual tuning for each language.
- Why unresolved: The paper focuses on high-resource languages and does not extensively analyze the performance differences for low-resource languages specifically.
- What evidence would resolve it: Conducting experiments with low-resource languages and comparing the performance of multilingual and monolingual tuning under the same computational budget would provide insights into this question.

### Open Question 2
- Question: What is the impact of the quality of machine translations on the performance of multilingual instruction tuning?
- Basis in paper: [inferred] The paper uses machine translations to create multilingual datasets for instruction tuning but does not explicitly discuss the impact of translation quality on the performance of the tuned models.
- Why unresolved: The quality of machine translations can vary significantly, and it is unclear how this variation affects the performance of multilingual instruction tuning.
- What evidence would resolve it: Conducting experiments with multilingual instruction tuning using machine translations of varying quality would help understand the impact of translation quality on model performance.

### Open Question 3
- Question: How does the choice of base LLM affect the effectiveness of multilingual instruction tuning?
- Basis in paper: [explicit] The paper experiments with base models from different families (BLOOM, LLaMA, OpenLLaMA, Pythia) and finds that the performance of multilingual instruction tuning varies depending on the base model used.
- Why unresolved: While the paper provides some insights into how different base models affect the performance of multilingual instruction tuning, it does not fully explore the reasons behind these differences or provide a comprehensive comparison.
- What evidence would resolve it: Conducting a more detailed analysis of the pre-training data and architecture of different base models and their impact on the effectiveness of multilingual instruction tuning would help answer this question.

## Limitations
- Machine translation quality for creating multilingual instruction data is not independently verified, raising concerns about potential semantic drift
- GPT-3.5 judge evaluation may introduce biases and variability that don't align with human judgment standards
- Limited test set of 50 manually translated prompts may not capture full diversity of instruction types and language-specific nuances

## Confidence
- High confidence: The comparative advantage of multilingual tuning over monolingual tuning within the same computational budget is well-supported by experimental results across multiple model families and sizes
- Medium confidence: The mechanism explanations for why multilingual tuning works are plausible but would benefit from more detailed ablation studies
- Low confidence: The specific effectiveness of the downsampling strategy is demonstrated empirically but lacks theoretical justification or sensitivity analysis

## Next Checks
1. **Independent human evaluation**: Conduct a small-scale human evaluation of a subset of model responses to verify that GPT-3.5 judge scores correlate with human preferences, particularly for language consistency and instruction-following quality.

2. **Translation quality audit**: Perform an independent quality assessment of the machine-translated instruction data using human evaluators to quantify semantic drift and identify any problematic translations that might affect downstream performance.

3. **Cross-lingual ablation study**: Design and execute a controlled experiment varying the amount of cross-lingual data while holding computational budget constant to isolate the specific contribution of cross-lingual transfer versus simple parameter sharing in the observed performance improvements.