---
ver: rpa2
title: A Single-Loop Deep Actor-Critic Algorithm for Constrained Reinforcement Learning
  with Provable Convergence
arxiv_id: '2306.06402'
source_url: https://arxiv.org/abs/2306.06402
tags:
- policy
- critic
- step
- convergence
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a single-loop deep actor-critic (SLDAC) algorithm
  for constrained reinforcement learning (CRL) problems with non-convex stochastic
  constraints and high interaction cost. The key idea is to combine the CSSCA method
  for handling non-convex constraints in the actor step with a single-loop design
  where the critic networks are updated only once per iteration using TD-learning,
  and old observations are reused to estimate policy gradients.
---

# A Single-Loop Deep Actor-Critic Algorithm for Constrained Reinforcement Learning with Provable Convergence

## Quick Facts
- arXiv ID: 2306.06402
- Source URL: https://arxiv.org/abs/2306.06402
- Reference count: 40
- Single-loop deep actor-critic algorithm for CRL with non-convex constraints achieves provable convergence to KKT points with reduced interaction cost

## Executive Summary
This paper introduces a single-loop deep actor-critic (SLDAC) algorithm for constrained reinforcement learning problems with non-convex stochastic constraints and high interaction cost. The key innovation is combining CSSCA for handling non-convex constraints with a single-loop design where critic networks are updated only once per iteration using TD-learning. This approach significantly reduces agent-environment interaction cost and computational complexity compared to existing two-loop methods. Despite using biased policy gradient estimation, the authors prove that SLDAC converges to a Karush-Kuhn-Tucker (KKT) point of the original problem almost surely with a feasible initial point. Simulation results demonstrate superior performance with much lower interaction cost compared to baselines like PPO-Lag, CPO, and SCAOPO on applications such as delay-constrained power control for downlink MU-MIMO systems and constrained linear-quadratic regulator problems.

## Method Summary
SLDAC solves constrained reinforcement learning problems by integrating a single-loop actor-critic framework with CSSCA for non-convex constraints. The actor step performs CSSCA optimization using convex surrogate functions constructed from current policy parameters, while the critic step updates value function estimates using TD-learning with only one update per iteration. The algorithm reuses old observations from a storage buffer to reduce variance in policy gradient estimation and improve data efficiency. The policy network outputs mean and covariance parameters for Gaussian policies, while two sets of critic networks estimate Q-values for each cost function. The method proves convergence to KKT points almost surely under specific step size conditions and bounded parameter spaces.

## Key Results
- SLDAC converges to KKT points of non-convex CRL problems almost surely with feasible initial points
- Achieves 10-50× reduction in agent-environment interactions compared to two-loop baselines
- Maintains constraint satisfaction while improving objective performance in MU-MIMO and LQR applications
- Proven convergence despite biased policy gradient estimation through observation reuse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Single-loop design reduces agent-environment interactions by updating critic once per iteration using TD-learning
- **Mechanism**: Replaces multiple inner-loop critic updates with single TD-learning update and observation reuse
- **Core assumption**: Critic network learns faster than actor, preventing error accumulation
- **Evidence anchors**: Abstract mentions "critic DNNs are only updated once", section describes "TD learning method at each iteration"
- **Break condition**: Critic error accumulation if learning rate is insufficient

### Mechanism 2
- **Claim**: Observation reuse reduces policy gradient variance and improves data efficiency
- **Mechanism**: Stores and reuses Tt previous observations to construct stable gradient estimates
- **Core assumption**: Stored observations remain representative of current policy distribution
- **Evidence anchors**: Abstract notes "variance of policy gradient estimation is reduced by reusing observations", section describes "off-policy estimation strategy"
- **Break condition**: Policy changes too rapidly, making old observations stale

### Mechanism 3
- **Claim**: CSSCA handles non-convex stochastic constraints through convex surrogate problems
- **Mechanism**: Constructs convex approximations of objective and constraints at each iteration
- **Core assumption**: Surrogate functions provide good local approximations of true functions
- **Evidence anchors**: Abstract mentions "CSSCA method applied to handle non-convex stochastic objective and constraints", section describes "solving a sequence of convex objective/feasibility optimization problems"
- **Break condition**: Surrogate functions deviate significantly from true functions

## Foundational Learning

- **Concept**: Constrained Markov Decision Processes (CMDP)
  - Why needed here: SLDAC solves CRL problems formulated as CMDPs with both objective and constraint functions
  - Quick check question: What distinguishes a CMDP from a standard MDP?

- **Concept**: Karush-Kuhn-Tucker (KKT) conditions
  - Why needed here: SLDAC's convergence analysis proves convergence to KKT points of the original problem
  - Quick check question: What are the necessary conditions for a point to be optimal in a constrained optimization problem?

- **Concept**: Temporal-Difference (TD) learning
  - Why needed here: SLDAC uses TD-learning to update critic network with single update per iteration
  - Quick check question: How does TD-learning differ from Monte Carlo methods in estimating value functions?

## Architecture Onboarding

- **Component map**: Observation buffer → TD-learning update → Surrogate construction → CSSCA optimization → Policy update
- **Critical path**: New observation → TD-learning update → Surrogate function construction → CSSCA optimization → Policy update
- **Design tradeoffs**:
  - Single-loop vs. two-loop: Reduced interaction cost vs. potentially slower critic learning
  - Observation reuse: Lower variance vs. potential bias from stale data
  - Fixed radius Rω: Tractability of convergence proof vs. representation capability
- **Failure signatures**:
  - Critic network divergence: Increasing TD error over iterations
  - Policy oscillation: Inconsistent behavior despite convergence in surrogate problems
  - Constraint violation: Average constraint costs exceeding limits
- **First 3 experiments**:
  1. Compare SLDAC with B=1, q=1 vs. B=10, q=5 on simple constrained LQR to verify interaction cost reduction
  2. Test observation reuse effectiveness by comparing SLDAC with and without storage on delay-constrained power control problem
  3. Validate convergence to KKT points by checking constraint satisfaction and objective improvement on CMDP with known optimal solution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different activation functions (beyond ReLU) on convergence rate of SLDAC?
- Basis in paper: [explicit] Paper assumes ReLU activation but notes result can be generalized to other activation functions in Section II-B
- Why unresolved: Theoretical analysis and simulations only consider ReLU activation
- What evidence would resolve it: Convergence rate analysis and simulations using different activation functions (sigmoid, tanh) in SLDAC

### Open Question 2
- Question: How does choice of radius Rω in critic parameter constraint set affect practical performance and convergence?
- Basis in paper: [explicit] Paper chooses specific radius Rω = a0m−1/2 Q L−4/9 for tractable analysis but notes it can be extended to other ranges in Section IV-B
- Why unresolved: Theoretical choice may not be optimal for practical performance
- What evidence would resolve it: Simulations and convergence analysis with varying Rω values to identify optimal choice for different problem settings

### Open Question 3
- Question: What is the effect of number of inner iterations q on convergence rate and computational complexity in practice?
- Basis in paper: [explicit] Paper discusses tradeoff between q and interaction cost/complexity but focuses on q=1 for theoretical analysis in Section III-B
- Why unresolved: Theoretical analysis assumes q=1, but practical performance may vary with different q values
- What evidence would resolve it: Simulations comparing convergence rates and computational complexity for various q values in different CRL problems

## Limitations
- Fixed radius Rω for policy parameter space may limit representation capability and prevent reaching optimal solutions outside initial region
- Convergence guarantee applies only to ergodic Markov chains with fixed initial state distribution, excluding non-ergodic or multi-modal MDPs
- Requires solving convex surrogate problems at each iteration, potentially expensive for high-dimensional constraint spaces

## Confidence
- Mechanism 1 (Single-loop critic updates): Medium - Theoretical analysis supports approach, but empirical validation against two-loop methods is limited
- Mechanism 2 (Observation reuse): Low - Theoretically justified but lacks ablation studies demonstrating variance reduction benefits
- Mechanism 3 (CSSCA for non-convex constraints): Medium - Convergence proof relies on strong assumptions about surrogate function quality

## Next Checks
1. **Interaction cost validation**: Implement controlled experiment comparing SLDAC against two-loop baseline on simple CMDP, measuring both convergence quality and environment interactions required
2. **Observation freshness analysis**: Design experiment where policy changes rapidly, then measure how observation reuse affects bias and variance in policy gradient estimates
3. **Constraint satisfaction verification**: Test SLDAC on problems where constraints have sharp boundaries to verify algorithm maintains feasibility while converging to KKT points