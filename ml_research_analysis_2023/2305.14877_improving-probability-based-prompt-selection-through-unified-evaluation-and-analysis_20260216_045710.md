---
ver: rpa2
title: Improving Probability-based Prompt Selection Through Unified Evaluation and
  Analysis
arxiv_id: '2305.14877'
source_url: https://arxiv.org/abs/2305.14877
tags:
- prompt
- selection
- performance
- prompts
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework for evaluating and improving
  probability-based prompt selection methods for large language models. The authors
  analyze existing methods and find they can all be interpreted as variants of maximizing
  mutual information (MI) between input and predicted output.
---

# Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis

## Quick Facts
- arXiv ID: 2305.14877
- Source URL: https://arxiv.org/abs/2305.14877
- Reference count: 40
- Key outcome: A unified framework shows existing probability-based prompt selection methods are variants of mutual information maximization, with combinational variants improving effectiveness from 87.79% to 94.98%, and calibration by marginalization (CBM) further increasing to 99.44%.

## Executive Summary
This paper introduces a unified framework for evaluating and improving probability-based prompt selection methods for large language models. The authors analyze existing methods and find they can all be interpreted as variants of maximizing mutual information (MI) between input and predicted output. Based on this unified view, they develop several combinational variants of MI that significantly improve prompt selection effectiveness. Additionally, they propose a novel calibration method called Calibration by Marginalization (CBM) that further increases effectiveness when applied to calibrate model output probabilities.

## Method Summary
The authors evaluate existing probability-based prompt selection methods (MI, GE, LE, MDL, ZLP, ZPM, ZMV, PPL) across 13 diverse NLP datasets using OPT 2.7B. They unify these methods under a mutual information framework, then create combinational variants by transferring design elements between methods. They also propose CBM calibration by dividing p(y|x,t) by p(y|t) to normalize model predictions. The evaluation measures scaled accuracy/F1 (ratio of selected prompt performance to optimal oracle prompt) to compare across datasets/models.

## Key Results
- All existing probability-based prompt selection methods can be interpreted as variants of mutual information maximization
- Combinational variants (MIAG, MIAL, MIAGL) improve prompt selection effectiveness from 87.79% to 94.98%
- Calibration by Marginalization (CBM) further increases effectiveness to 99.44% and proves most robust across datasets
- GE method shows consistent preference for balanced predictions on unbalanced datasets

## Why This Works (Mechanism)

### Mechanism 1: MI Unification Framework
All existing probability-based prompt selection methods can be interpreted as variants of maximizing mutual information (MI) between input and predicted output. Each method approximately corresponds to a sub-term of the MI equation: MI methods maximize entropy of mean prediction and minimize average entropy, while other methods use one of these terms or variants. The probabilistic assumption that p(x|t) = 1/|X| for all x ∈ X and t ∈ T is crucial. This unification enables understanding relationships between methods and developing combinational variants. Break condition: when the probabilistic assumption p(x|t) = 1/|X| does not hold, such as in dynamic datasets with extreme label imbalance or when prompts are selected conditionally on input.

### Mechanism 2: Calibration by Marginalization (CBM)
CBM improves prompt selection by normalizing model output probabilities with marginalized probabilities. It divides p(y|x,t) by p(y|t) = (1/|X|) Σx' p(y|x',t), where p(y|t) approximates the prior probability of y under prompt t. This reduces overconfidence and bias in model predictions. The marginalized probability p(y|t) provides a better estimate of the prior probability of y than using content-free inputs or domain-specific examples. Break condition: when the marginalized probability p(y|t) is not a good approximation of the prior probability, such as when prompts are highly specialized or when the dataset is very small.

### Mechanism 3: Design Element Transfer
Transferring design elements from one prompt selection method to another can improve performance. By identifying connections between methods through their shared MI sub-terms, combinational variants can be created that borrow design elements from different methods. For example, MIAG uses the first term of GE (one-hot p(y|x,t)) while keeping the second term of MI. The design elements that make one method effective can be successfully transferred to another method without losing the benefits of the original method. Break condition: when the design elements are not compatible or when the transfer introduces new biases or limitations.

## Foundational Learning

- Concept: Mutual Information (MI)
  - Why needed here: MI provides the theoretical foundation for understanding and comparing different prompt selection methods.
  - Quick check question: What is the difference between the first and second terms of the MI equation in the context of prompt selection?

- Concept: Entropy and Probability Distributions
  - Why needed here: Entropy is used to measure the uncertainty in model predictions and is a key component of many prompt selection methods.
  - Quick check question: How does the entropy of a probability distribution relate to the sharpness of the model's predictions?

- Concept: Calibration and Probability Normalization
  - Why needed here: Calibration is used to improve the reliability of model output probabilities, which is essential for accurate prompt selection.
  - Quick check question: What is the difference between calibration for answer selection and calibration for prompt selection?

## Architecture Onboarding

- Component map: Prompt generation -> Model inference -> PSS calculation -> Prompt selection -> Evaluation
- Critical path: Prompt generation → Model inference → PSS calculation → Prompt selection → Evaluation
- Design tradeoffs:
  - Using all tokens vs. only first token for p(y|x,t) calculation
  - Instance-wise vs. dataset-wise prompt selection
  - One-hot vs. raw p(y|x,t) in entropy calculations
- Failure signatures:
  - Low correlation between PSS and actual performance
  - Poor performance on dynamic datasets with label imbalance
  - Overconfidence in model predictions leading to poor prompt selection
- First 3 experiments:
  1. Compare MI vs. MIA (using all tokens) on datasets with multi-token verbalizers
  2. Test CBM calibration on MDLM to see if it mitigates overconfidence
  3. Evaluate combinational variants (MIAG, MIAL, MIAGL) on balanced datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do probability-based prompt selection methods perform on open-ended generation tasks where answer choices are not predefined?
- Basis in paper: The paper focuses on classification datasets and notes that some methods are only applicable to classification tasks. The authors acknowledge this as a limitation.
- Why unresolved: The study deliberately limited scope to classification tasks for methodological reasons, leaving open-ended generation performance unexplored.
- What evidence would resolve it: Direct experiments applying the same probability-based prompt selection methods to open-ended generation benchmarks, comparing their effectiveness to classification performance.

### Open Question 2
- Question: Can the unified framework connecting existing probability-based prompt selection methods be extended to non-gradient-based methods that use additional resources like retrievers or training sets?
- Basis in paper: The paper explicitly limits scope to gradient-free methods that use only output probabilities, but notes this is not comprehensive. The unified framework reveals structural connections that might apply more broadly.
- Why unresolved: The unified framework is derived specifically from probability-based methods; extending to resource-intensive methods would require incorporating additional computational components.
- What evidence would resolve it: Reformulating resource-intensive prompt selection methods (e.g., those using retrievers or fine-tuning) within the unified framework, showing how their components relate to the mutual information decomposition.

### Open Question 3
- Question: Under what conditions does Calibration by Marginalization (CBM) fail to improve prompt selection performance, and what are the theoretical bounds on its effectiveness?
- Basis in paper: The paper shows CBM works well across most datasets but fails in the "dynamic w/ label bias" setup. The authors note this limitation but don't provide theoretical analysis.
- Why unresolved: The empirical observation of CBM's limitations in biased label scenarios is presented without theoretical explanation of when or why calibration methods fail.
- What evidence would resolve it: Formal analysis of CBM's effectiveness bounds, identifying specific conditions (label distribution properties, model behavior) under which calibration helps or harms prompt selection.

## Limitations
- Theoretical framework may not generalize to all real-world scenarios, particularly datasets with severe label imbalance or conditional prompt selection
- Calibration method assumes marginalized probabilities provide accurate prior estimates, which may break down with specialized prompts or small datasets
- Results may not generalize to other model architectures beyond OPT 2.7B or different task types beyond tested classification datasets

## Confidence

**High Confidence:**
- The categorization of prompt selection methods based on entropy and prediction averaging is well-supported
- Combinational variants (MIAG, MIAL, MIAGL) effectiveness is clearly demonstrated through scaled accuracy metrics
- GE method's preference for balanced predictions on unbalanced datasets is consistently observed

**Medium Confidence:**
- The unified MI framework is theoretically sound but may have practical limitations
- CBM calibration superiority is demonstrated but may be influenced by implementation choices
- Design element transfer effectiveness is shown but underlying mechanisms are not fully explained

**Low Confidence:**
- Generalizability of MI unification framework to other prompt selection methods
- Robustness of CBM calibration across different model architectures and task types
- Long-term stability of combinational variants on dynamic real-world datasets

## Next Checks

1. **Empirical validation of MI unification across diverse models:** Test the unified MI framework on a broader range of model architectures (including larger models and different base architectures) to verify that the theoretical relationships between methods hold consistently.

2. **Stress-testing CBM calibration on extreme datasets:** Evaluate CBM calibration on datasets with extreme label imbalance, very small sample sizes, and highly specialized domains to determine the limits of its effectiveness.

3. **Long-term stability analysis of combinational variants:** Conduct longitudinal studies tracking the performance of combinational variants (MIAG, MIAL, MIAGL) over time as datasets evolve and models are updated.