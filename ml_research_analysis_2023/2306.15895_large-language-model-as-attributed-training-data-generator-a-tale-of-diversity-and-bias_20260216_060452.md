---
ver: rpa2
title: 'Large Language Model as Attributed Training Data Generator: A Tale of Diversity
  and Bias'
arxiv_id: '2306.15895'
source_url: https://arxiv.org/abs/2306.15895
tags:
- data
- attrprompt
- dataset
- attributes
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using LLM-generated data with attributed
  prompts to address biases and diversity issues in training data for topic classification.
  The core method involves generating training data with ChatGPT using diverse prompts
  that specify attributes like topic, length, style, and location.
---

# Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias

## Quick Facts
- arXiv ID: 2306.15895
- Source URL: https://arxiv.org/abs/2306.15895
- Reference count: 40
- Using LLM-generated data with attributed prompts outperforms simple class-conditional prompts while reducing biases and using only 5% of the querying cost.

## Executive Summary
This paper introduces AttrPrompt, a method for generating training data using large language models with attributed prompts to improve diversity and reduce bias in topic classification tasks. The approach uses diverse attribute configurations (topic, length, style, location) to guide LLM output, achieving comparable performance to traditional methods at significantly lower cost. Experiments on four diverse, high-cardinality datasets demonstrate AttrPrompt's superiority across accuracy, F1 score, and bias reduction metrics while using only 5% of the querying budget compared to simple class-conditional prompts.

## Method Summary
The method uses ChatGPT to generate training data through carefully constructed prompts that include multiple attributes beyond just the class label. Attribute dimensions (like location, style, length) and their values are first elicited from the LLM, then combined randomly with class labels to form attributed prompts. The generated data is filtered using a Class-Agnostic Filtering (CAF) process to remove attribute bias. BERT-base-uncased models are then trained on the generated data and evaluated against baseline methods using standard classification metrics and diversity measurements.

## Key Results
- AttrPrompt outperforms simple class-conditional prompts (SimPrompt) across all four tested datasets (NYT, Amazon, Reddit, StackExchange)
- Achieved comparable performance while using only 5% of the querying cost compared to SimPrompt
- Generated data effectively complements original training sets and improves performance on long-tail classes, particularly for underrepresented categories like "abortion"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attribute diversity in prompts reduces bias and improves model performance.
- Mechanism: By specifying diverse attributes (e.g., location, style, length) in prompts, the LLM generates training data that covers a broader range of semantic variations, reducing systematic biases like regional skew.
- Core assumption: Diverse attribute combinations lead to semantically distinct training samples that generalize better.
- Evidence anchors:
  - [abstract]: "attribute diversity plays a pivotal role in enhancing model performance"
  - [section]: "generated dataset is an effective complement to the original training set...since most of the generated datasets introduce performance gain when combined with the original training set"
  - [corpus]: Weak evidence; corpus shows related work on diversity in GANs and conditional generation, but not directly on prompt attribute diversity.
- Break condition: If attribute combinations are not truly diverse or if LLM fails to respect attribute constraints, bias reduction and performance gains will not materialize.

### Mechanism 2
- Claim: Diverse prompts achieve comparable performance with much lower cost.
- Mechanism: Using diverse attributes allows the LLM to generate fewer but higher-quality samples per class, reducing the number of queries needed to reach target performance.
- Core assumption: High attribute diversity offsets the need for large sample counts.
- Evidence anchors:
  - [abstract]: "attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5% of the querying cost"
  - [section]: "AttrPrompt only requires 5% of budget to be on par with or outperform SimPrompt"
  - [corpus]: Weak evidence; corpus contains related efficiency work in GANs but no direct cost-per-query comparison.
- Break condition: If LLM cost per query is fixed and attribute diversity does not improve sample informativeness, cost savings will not be realized.

### Mechanism 3
- Claim: Generated data mitigates long-tail class issues in real datasets.
- Mechanism: The generation process samples attributes uniformly across classes, producing balanced synthetic data that compensates for skewed real training distributions.
- Core assumption: Class-balanced synthetic data can effectively augment underrepresented classes.
- Evidence anchors:
  - [section]: "AttrPrompt renders the best per-class F1 score on 10 classes...especially for the class 'abortion' with the fewest examples"
  - [section]: "generated dataset are class-balanced...curious how the class balance...benefits the model performance on long-tail classes"
  - [corpus]: No direct corpus evidence; this is inferred from class-imbalance discussion in related literature.
- Break condition: If synthetic samples for rare classes are low-quality or semantically off-topic, performance on long-tail classes will not improve.

## Foundational Learning

- Concept: Prompt engineering for controlled text generation
  - Why needed here: The method relies on carefully structured prompts that embed multiple attributes to steer LLM output.
  - Quick check question: What happens if an attribute is ambiguous or missing from the prompt?

- Concept: Bias measurement and mitigation in ML datasets
  - Why needed here: The study explicitly measures regional and other biases in generated data and uses attribute control to reduce them.
  - Quick check question: How would you detect if a generated dataset still contains hidden bias after attribute balancing?

- Concept: Data augmentation and synthetic data quality evaluation
  - Why needed here: The work evaluates synthetic data quality via classifier performance, lexical diversity, and semantic similarity metrics.
  - Quick check question: What metric would you use to decide if synthetic data is diverse enough?

## Architecture Onboarding

- Component map: Attribute selection → Prompt construction → LLM query → Data filtering (CAF) → Model training → Evaluation

- Critical path: Attribute selection → Prompt construction → LLM query → Data filtering (CAF) → Model training → Evaluation

- Design tradeoffs:
  - High attribute diversity vs. query cost: More attributes improve diversity but may require more queries if constraints are hard to satisfy.
  - CAF filtering vs. attribute coverage: Filtering out ambiguous attributes reduces bias but may shrink the effective attribute space.
  - Model choice vs. performance: Larger models yield better performance but increase training time and cost.

- Failure signatures:
  - Low lexical diversity metrics (vocabulary size) indicate insufficient attribute variation.
  - High cosine similarity between samples signals mode collapse or lack of semantic diversity.
  - Persistent bias in attribute classifier predictions reveals incomplete bias mitigation.

- First 3 experiments:
  1. Generate a small dataset with random attributes, measure vocabulary size and attribute classifier accuracy.
  2. Compare model performance using fixed vs. random attribute configurations on one dataset.
  3. Run budget efficiency test: generate data with 5%, 10%, 20% of SimPrompt budget and compare F1 scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further optimize the process of generating diverse attributed prompts for training data generation?
- Basis in paper: [inferred] The paper discusses the use of attributed prompts to enhance data diversity but leaves the search for optimal data attributes for a specific task as future work.
- Why unresolved: The paper does not provide a concrete method for automating or semi-automating the identification of high-quality attribute dimensions and values.
- What evidence would resolve it: A systematic approach or algorithm that can efficiently and accurately identify the most relevant and diverse attribute dimensions and values for a given task would resolve this question.

### Open Question 2
- Question: Can the attributed prompts approach be extended to other data types beyond text classification, such as image or audio classification?
- Basis in paper: [explicit] The paper acknowledges the limitation of focusing primarily on text classification and suggests exploring the effectiveness of attributed prompts in other modalities as future work.
- Why unresolved: The paper does not provide any empirical evidence or theoretical framework for applying attributed prompts to non-text data types.
- What evidence would resolve it: Experiments demonstrating the effectiveness of attributed prompts in image or audio classification tasks would resolve this question.

### Open Question 3
- Question: How can we mitigate the issue of hallucination in LLM-based training data generation to ensure the accuracy and reliability of the generated data?
- Basis in paper: [explicit] The paper mentions the phenomenon of hallucination as a limitation of LLM-based training data generation and suggests incorporating fact-checking mechanisms and human review to mitigate this issue.
- Why unresolved: The paper does not provide a detailed methodology or empirical evaluation of these proposed mitigation strategies.
- What evidence would resolve it: A study comparing the effectiveness of different hallucination mitigation techniques, such as fact-checking and human review, in improving the accuracy and reliability of LLM-generated data would resolve this question.

## Limitations
- The exact relationship between attribute diversity and semantic diversity is not fully validated
- Experiments focus on four specific datasets, limiting generalization to other domains
- Cost analysis does not account for computational costs of attribute generation and filtering

## Confidence
**High Confidence Claims**:
- AttrPrompt outperforms SimPrompt on the tested datasets across multiple evaluation metrics
- Generated data can effectively complement original training sets
- Class-balanced synthetic data improves performance on long-tail classes

**Medium Confidence Claims**:
- Attribute diversity is the primary driver of performance improvements
- 5% querying cost reduction is consistently achievable across different scenarios
- Bias reduction through CAF filtering is complete and effective

**Low Confidence Claims**:
- The method generalizes to arbitrary domains and tasks beyond the tested classification problems
- The specific 5% cost threshold is optimal for all use cases
- Manual annotation verification would confirm automated classifier findings

## Next Checks
1. **Ablation Study on Attribute Types**: Systematically remove individual attribute categories (style, location, length) and measure the impact on model performance and bias metrics to isolate which attributes contribute most to the observed improvements.

2. **Cross-Domain Generalization Test**: Apply AttrPrompt to a completely different domain (e.g., medical text classification or legal document categorization) and compare performance against both SimPrompt and traditional data augmentation methods to validate broader applicability.

3. **Cost-Benefit Analysis with Full Pipeline**: Include all computational costs in the efficiency calculation - attribute generation, CAF filtering, LLM querying, and model training - to verify the true cost advantage of AttrPrompt over SimPrompt across different scale scenarios.