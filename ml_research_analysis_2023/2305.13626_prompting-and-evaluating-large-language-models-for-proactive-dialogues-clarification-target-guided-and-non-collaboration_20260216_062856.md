---
ver: rpa2
title: 'Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification,
  Target-guided, and Non-collaboration'
arxiv_id: '2305.13626'
source_url: https://arxiv.org/abs/2305.13626
tags:
- dialogue
- prompting
- proactive
- response
- dialogues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work conducts the first comprehensive evaluation of large
  language model (LLM)-based conversational systems for proactive dialogues, including
  clarification, target-guided, and non-collaborative dialogues. To enhance the proactivity
  of LLMs, the authors propose a proactive chain-of-thought (ProCoT) prompting scheme
  that augments LLMs with goal planning capability over descriptive reasoning chains.
---

# Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration

## Quick Facts
- arXiv ID: 2305.13626
- Source URL: https://arxiv.org/abs/2305.13626
- Authors: 
- Reference count: 23
- This work conducts the first comprehensive evaluation of large language model (LLM)-based conversational systems for proactive dialogues, including clarification, target-guided, and non-collaborative dialogues.

## Executive Summary
This paper presents the first comprehensive evaluation of LLM-based conversational systems for proactive dialogues across three dialogue types: clarification dialogues, target-guided dialogues, and non-collaborative dialogues. The authors introduce a proactive chain-of-thought (ProCoT) prompting scheme that augments LLMs with goal planning capability over descriptive reasoning chains. Experiments demonstrate that ProCoT significantly improves LLM performance in asking clarification questions and enables smoother topic transitions, but LLMs still struggle with strategic decision-making in non-collaborative dialogues even with enhanced prompting.

## Method Summary
The paper evaluates three prompting schemes - Standard, Proactive, and ProCoT - across three dialogue datasets using two LLM systems (ChatGPT and Vicuna). For each dialogue type, the authors construct prompts with task instructions, examples, and test inputs. ProCoT specifically augments LLMs with goal planning capability by generating descriptive thoughts about intermediate reasoning and planning steps. The evaluation covers zero-shot and few-shot settings, comparing LLM performance against fine-tuned SOTA baselines using automatic metrics (precision, recall, BLEU, ROUGE, F1) and human evaluation for specific tasks.

## Key Results
- LLMs struggle to ask clarification questions in ambiguous scenarios, but ProCoT significantly improves this capability (though performance remains limited in domain-specific applications)
- LLMs are proficient at topic shifting toward designated targets but tend to make aggressive transitions, while ProCoT enables smoother topic planning
- LLMs fail to make strategic decisions in non-collaborative dialogues even with ProCoT prompting, highlighting challenges in strategy learning and planning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProCoT prompting significantly improves the capability of LLM-based dialogue systems to ask clarification questions in ambiguous scenarios.
- Mechanism: ProCoT instructs the system to first perform ambiguity analysis over the user's question, then decide whether to ask a clarification question or directly answer. This explicit reasoning step surfaces the ambiguity before generating the response.
- Core assumption: LLMs can perform structured reasoning over text when prompted with explicit intermediate steps, and this reasoning can guide more appropriate dialogue actions.
- Evidence anchors:
  - [abstract] "ProCoT first instructs the system to generate descriptive thoughts about intermediate steps of reasoning and planning for reaching the conversational goal, and then make the decision of the next action to take."
  - [section 4.1.3] "ProCoT largely enhances the originally poor performance of asking clarification questions, but still limits in handling domain-specific applications."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.476" â€” weak signal, but some related work exists on LLM prompting for clarification.

### Mechanism 2
- Claim: ProCoT enables smoother topic transitions in target-guided dialogues by explicitly planning intermediate topics.
- Mechanism: ProCoT prompts the system to analyze the relationship between current topics and the target, then select intermediate topics that bridge the gap, resulting in more natural multi-turn topic progression.
- Core assumption: LLMs can model topic similarity and plan coherent topic sequences when given explicit planning instructions.
- Evidence anchors:
  - [abstract] "ProCoT further improves this capability by planning a more smooth transition."
  - [section 4.2.4] "ProCoT prompting enables the topic planning to be more smooth" with improved coherence scores.
  - [corpus] Weak signal; no direct citations but related work exists on topic planning in dialogues.

### Mechanism 3
- Claim: ProCoT enhances strategy planning in non-collaborative dialogues by explicitly analyzing negotiation progress and setting goals.
- Mechanism: ProCoT instructs the system to first analyze the current negotiation state, consider an appropriate negotiation goal, then select strategies and actions aligned with that goal.
- Core assumption: LLMs can perform strategic reasoning over dialogue states when prompted with explicit planning steps.
- Evidence anchors:
  - [abstract] "ProCoT first instructs the system to generate descriptive thoughts about intermediate steps of reasoning and planning for reaching the conversational goal."
  - [section 4.3.4] "ProCoT points out that the negotiation has not yet started" and improves decision-making in later turns.
  - [corpus] Weak signal; limited related work on LLM prompting for negotiation strategy planning.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: ProCoT and proactive prompting rely on few-shot demonstrations to guide LLM behavior.
  - Quick check question: What is the maximum context length used for Vicuna in the experiments, and why is this a constraint?

- Concept: Zero-shot vs few-shot prompting
  - Why needed here: The paper compares zero-shot and one-shot performance of ProCoT across tasks.
  - Quick check question: In which task does zero-shot ProCoT fail completely for Vicuna?

- Concept: Dialogue act and strategy prediction
  - Why needed here: Proactive prompting and ProCoT require selecting appropriate dialogue acts or negotiation strategies.
  - Quick check question: How are dialogue acts defined in the CraigslistBargain dataset?

## Architecture Onboarding

- Component map:
  - Prompt generation module -> LLM inference engine -> Response analysis module -> Dataset interface

- Critical path:
  1. Load dataset and construct conversation history.
  2. Generate prompt using selected prompting scheme (standard, proactive, or ProCoT).
  3. Send prompt to LLM and retrieve response.
  4. Evaluate response using appropriate metrics.
  5. Log results for analysis.

- Design tradeoffs:
  - Prompt complexity vs. LLM context limits: Longer prompts with more examples improve performance but risk exceeding context limits.
  - Automatic vs. human evaluation: Automatic metrics are scalable but may miss nuanced aspects of dialogue quality.

- Failure signatures:
  - Response does not follow required format (e.g., missing dialogue act or strategy).
  - LLM generates hallucinated content not grounded in the provided context.
  - Performance degrades significantly in domain-specific tasks compared to open-domain tasks.

- First 3 experiments:
  1. Run standard prompting on Abg-CoQA with zero-shot and one-shot settings; compare with ProCoT.
  2. Test target-guided dialogues on OTTers dataset; evaluate topic transition smoothness with and without ProCoT.
  3. Evaluate non-collaborative dialogues on CraigslistBargain; analyze strategy selection accuracy with ProCoT.

## Open Questions the Paper Calls Out
No specific open questions are called out in the paper.

## Limitations
- ProCoT's performance remains unsatisfactory in domain-specific applications, particularly for finance dialogues where clarification question generation achieves only 0.53 F1 compared to 0.80 on open-domain CoQA
- LLMs fail to learn strategic planning for non-collaborative dialogues even with explicit ProCoT prompting, indicating fundamental limitations in strategy learning
- The evaluation is constrained by using only two LLM variants (ChatGPT and Vicuna), with Vicuna's limited 4K context window potentially affecting performance in complex multi-turn dialogues

## Confidence
**High Confidence:**
- LLMs struggle with clarification questions in ambiguous scenarios without ProCoT prompting
- ProCoT significantly improves clarification question generation, though domain-specific performance remains limited
- LLMs are proficient at topic shifting toward targets but tend to make aggressive transitions

**Medium Confidence:**
- ProCoT enables smoother topic transitions through intermediate planning
- Standard prompting fails to provide adequate strategic planning for non-collaborative dialogues
- ProCoT improves strategy planning in non-collaborative dialogues but cannot fully overcome LLM limitations

**Low Confidence:**
- ProCoT represents the optimal approach for proactive dialogue systems
- The observed improvements generalize across all domain-specific applications
- LLMs cannot learn strategic planning regardless of prompting scheme

## Next Checks
1. **Domain Transfer Validation**: Evaluate ProCoT on additional domain-specific dialogue datasets (e.g., medical, technical support) to determine whether performance limitations observed in finance dialogues generalize across domains or are specific to financial terminology and concepts.

2. **Prompt Engineering Ablation**: Systematically vary the complexity and specificity of ProCoT prompts (number of reasoning steps, level of detail in instructions) to identify the minimum effective prompt structure and determine whether performance improvements scale with prompt complexity.

3. **Multi-LLM Comparison**: Test ProCoT prompting across a broader range of LLM architectures and sizes (including larger context models like GPT-4, Claude, or open-source alternatives) to distinguish between LLM-specific limitations and fundamental challenges in proactive dialogue planning.