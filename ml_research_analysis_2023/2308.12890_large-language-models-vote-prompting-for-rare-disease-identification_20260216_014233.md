---
ver: rpa2
title: 'Large Language Models Vote: Prompting for Rare Disease Identification'
arxiv_id: '2308.12890'
source_url: https://arxiv.org/abs/2308.12890
tags:
- rare
- disease
- prompting
- llama
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Models-Vote Prompting (MVP), a novel prompting
  approach for improving the performance of Large Language Models (LLMs) in Few-Shot
  Learning (FSL) settings. MVP works by prompting multiple LLMs to perform the same
  task and then conducting a majority vote on the resulting outputs.
---

# Large Language Models Vote: Prompting for Rare Disease Identification

## Quick Facts
- arXiv ID: 2308.12890
- Source URL: https://arxiv.org/abs/2308.12890
- Reference count: 18
- One-line primary result: MVP ensemble approach improves rare disease identification accuracy compared to individual LLMs

## Executive Summary
This paper introduces Models-Vote Prompting (MVP), a novel approach that improves few-shot learning performance for rare disease identification by aggregating predictions from multiple LLMs through majority voting. The authors demonstrate that this ensemble method outperforms individual models on rare disease identification and classification tasks using a newly released dataset from the MIMIC-IV database. Additionally, they explore JSON-formatted prompts to automate evaluation and reduce manual annotation burden.

## Method Summary
MVP prompts multiple LLMs (LLaMA 2, MedAlpaca, Stable Platypus 2, Vicuna) with the same task using Chain-of-Thought prompting and JSON-formatted output expectations. The method collects outputs from each model and conducts majority voting to determine the final prediction. The approach is evaluated on a rare disease dataset extracted from MIMIC-IV, containing 256 manually annotated clinical documents across four rare diseases. JSON compliance is assessed to enable automated evaluation, with fallback to manual annotation for non-compliant outputs.

## Key Results
- MVP achieves improved accuracy over any individual model in the ensemble for rare disease identification and classification
- JSON-formatted prompts enable automated evaluation but show compliance issues, particularly with MedAlpaca (80.8% compliance)
- Larger context windows (32→64→128→256 words) initially improve but then decrease model performance due to increased ambiguity
- Statistical significance testing confirms MVP's superiority over individual models with p-values indicating reliable improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Majority voting across multiple LLMs improves performance by aggregating diverse knowledge and reducing individual model bias/variance.
- Core assumption: The ensemble of diverse LLMs captures complementary strengths, and majority voting effectively combines them.
- Evidence anchors: Abstract states MVP achieves improved results to any one model in the ensemble; section notes MVP considers several models trained on different datasets.
- Break condition: If models' predictions are highly correlated (all make same errors), majority voting provides little benefit and may amplify systematic mistakes.

### Mechanism 2
- Claim: Using JSON-formatted prompts improves automated evaluation by producing structured, parsable outputs that reduce manual annotation burden.
- Core assumption: LLMs trained on diverse web data can reliably produce valid JSON when prompted with structured output expectations.
- Evidence anchors: Abstract mentions JSON assessment for automating generative LLM evaluation; section notes parsable JSON reduced need for human annotation.
- Break condition: If models frequently generate invalid or malformed JSON, automated evaluation becomes unreliable and requires fallback to manual annotation.

### Mechanism 3
- Claim: Chain-of-Thought prompting improves reasoning by forcing models to show intermediate steps before reaching conclusions.
- Core assumption: Showing intermediate reasoning steps improves quality of final predictions, especially for complex medical diagnosis tasks.
- Evidence anchors: Section describes CoT uses reasoning steps to get from input to output; same prompt format as pre-training improves performance.
- Break condition: If reasoning steps are superficial or incorrect, they may mislead rather than help the model reach better conclusions.

## Foundational Learning

- Concept: Few-Shot Learning (FSL)
  - Why needed here: Rare diseases have limited data, requiring models to learn from very few examples (1-4 shots) rather than extensive training
  - Quick check question: What's the maximum number of training examples typically used in few-shot learning scenarios?

- Concept: Prompt Engineering
  - Why needed here: Quality of task performance depends heavily on how instructions are phrased and structured for the LLM
  - Quick check question: How does chain-of-thought prompting differ from simple instruction prompting in terms of output structure?

- Concept: Ensemble Methods
  - Why needed here: Combining multiple models' predictions can reduce individual model weaknesses and improve overall accuracy
  - Quick check question: What's the mathematical basis for why majority voting can improve classification accuracy?

## Architecture Onboarding

- Component map: Input → Prompt Generator → Multiple LLM Workers → Majority Vote Aggregator → JSON Parser → Evaluation Metrics
- Critical path: Prompt generation → LLM inference → Vote aggregation → JSON parsing → Metric calculation
- Design tradeoffs: More models improve accuracy but increase cost; JSON parsing enables automation but requires careful prompt design
- Failure signatures: Inconsistent JSON outputs across models, tied votes requiring tie-breaking logic, model-specific failures affecting overall ensemble
- First 3 experiments:
  1. Test MVP with just two models (LLaMA 2 + Vicuna) to verify voting mechanism works
  2. Test JSON compliance by generating outputs and checking parse success rate
  3. Test ablation by removing one model at a time to measure performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number and combination of LLMs to include in the Models-Vote Prompting (MVP) ensemble for different tasks and domains?
- Basis in paper: Authors mention that "the number of models and models themselves are hyperparameters, and we hypothesize that determining optimal values may be domain and task-dependent."
- Why unresolved: Paper doesn't provide systematic study on impact of varying number and types of LLMs in MVP ensemble on performance across different tasks and domains.
- What evidence would resolve it: Experiments comparing performance of MVP ensembles with different numbers and combinations of LLMs on various tasks and domains, along with analysis of trade-offs between performance gains and computational costs.

### Open Question 2
- Question: How does the performance of MVP change when using smaller LLMs (e.g., 7 billion parameter models) instead of the 13 billion parameter models used in experiments?
- Basis in paper: Authors suggest that "performing the same tasks using smaller LLMs (e.g., 7 billion parameter models) may show promising results."
- Why unresolved: Paper doesn't provide empirical results on performance of MVP when using smaller LLMs, which could have implications for computational efficiency and accessibility.
- What evidence would resolve it: Experiments comparing performance of MVP using smaller LLMs with results obtained using 13 billion parameter models, along with analysis of trade-offs between performance and computational requirements.

### Open Question 3
- Question: How does the performance of MVP change when using different prompting approaches (e.g., Self-Consistency instead of Chain-of-Thought) in the ensemble?
- Basis in paper: Authors mention that "incorporating different prompting approaches (e.g., SC instead of CoT) may help improve MVP performance."
- Why unresolved: Paper doesn't explore impact of using different prompting approaches within MVP ensemble on performance.
- What evidence would resolve it: Experiments comparing performance of MVP ensembles using different prompting approaches (e.g., CoT vs. SC) on various tasks, along with analysis of strengths and weaknesses of each approach in ensemble setting.

## Limitations

- Limited dataset size (256 documents) restricts generalizability to broader rare disease populations
- Focus on only four rare diseases may introduce bias and limit applicability to other rare conditions
- Computational costs of running multiple LLMs versus single-model approaches not addressed, potentially limiting real-world deployment feasibility

## Confidence

- Majority voting mechanism: High - statistically significant improvements demonstrated with clear evidence
- JSON automation approach: Medium - promising but compliance issues observed (80.8% for MedAlpaca)
- Chain-of-Thought prompting: Low - limited evidence of specific contribution to rare disease reasoning tasks

## Next Checks

1. **Mechanism Isolation Test**: Conduct systematic ablation studies removing each component (JSON automation, CoT prompting, individual models) to quantify their specific contributions to MVP performance.

2. **Dataset Generalization Test**: Validate MVP performance on a larger, more diverse rare disease dataset spanning 10+ diseases to assess scalability and robustness beyond current four-disease scope.

3. **Cost-Benefit Analysis**: Measure computational costs and inference time for MVP versus single-model approaches across different ensemble sizes to establish practical deployment thresholds.