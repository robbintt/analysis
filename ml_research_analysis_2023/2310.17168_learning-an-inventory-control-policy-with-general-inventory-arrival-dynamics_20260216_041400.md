---
ver: rpa2
title: Learning an Inventory Control Policy with General Inventory Arrival Dynamics
arxiv_id: '2310.17168'
source_url: https://arxiv.org/abs/2310.17168
tags:
- order
- inventory
- arrival
- time
- quantity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of learning and backtesting inventory
  control policies under general inventory arrival dynamics, termed as a quantity-over-time
  arrivals model (QOT). The authors extend previous work by incorporating a deep generative
  model for the arrivals process, allowing for arbitrary downstream post-processing
  of order quantities to meet vendor constraints like minimum order quantities and
  batch sizes.
---

# Learning an Inventory Control Policy with General Inventory Arrival Dynamics

## Quick Facts
- arXiv ID: 2310.17168
- Source URL: https://arxiv.org/abs/2310.17168
- Authors: 
- Reference count: 33
- Key outcome: This paper tackles the challenge of learning and backtesting inventory control policies under general inventory arrival dynamics, termed as a quantity-over-time arrivals model (QOT). The authors extend previous work by incorporating a deep generative model for the arrivals process, allowing for arbitrary downstream post-processing of order quantities to meet vendor constraints like minimum order quantities and batch sizes. By formulating the problem as an exogenous decision process, they demonstrate a reduction to supervised learning. Through simulation studies and a real-world A/B test, the approach yields statistically significant improvements in profitability over production baselines. The learned policy generalizes well to off-policy data, outperforming traditional inventory management systems in real-world settings.

## Executive Summary
This paper presents a novel approach to learning inventory control policies under complex, real-world arrival dynamics, including multiple shipments, stochastic yields, and vendor constraints like minimum order quantities and batch sizes. The authors propose a quantity-over-time (QOT) arrivals model that generalizes existing frameworks and incorporate a deep generative model to simulate realistic arrival sequences. By framing the problem as an exogenous decision process, they reduce it to supervised learning, enabling efficient policy optimization. The approach is validated through simulation studies and a real-world A/B test, demonstrating statistically significant improvements in profitability over production baselines.

## Method Summary
The method involves training a generative model (Gen-QOT) to forecast the distribution of inventory arrivals based on historical purchase order data. This model is then integrated into a differentiable simulator, which is used to train a reinforcement learning policy via direct backpropagation through the inventory dynamics. The policy network, consisting of a Wavenet encoder and MLP decoder, maps the current state to order quantities, which are then post-processed to meet vendor constraints. The approach is evaluated using metrics such as CRPS, quantile loss, and calibration, and compared against baseline policies like Newsvendor.

## Key Results
- The Gen-QOT model accurately simulates inventory arrivals under vendor constraints, improving upon traditional arrival models.
- The learned policy outperforms production baselines in both simulation studies and a real-world A/B test, yielding statistically significant profitability gains.
- The policy generalizes well to off-policy data, demonstrating robustness beyond the training distribution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The inventory control policy can be reduced to supervised learning via exogenous decision process (IDP) formulation.
- Mechanism: By structuring the inventory system as an IDP, most state variables are exogenous (outside the agent's control), allowing the problem to be transformed into a supervised learning setup over historical data.
- Core assumption: The exogenous processes (demand, supply, lead times, constraints) are observable or accurately forecastable.
- Evidence anchors:
  - [abstract] "By formulating the problem as an exogenous decision process, we can apply results from Madeka et al., 2022 to obtain a reduction to supervised learning."
  - [section] "Madeka et al. (2022) demonstrate a reduction in complexity of the learning problem to that of supervised learning."
  - [corpus] "Neural Coordination and Capacity Control for Inventory Management" supports capacity control under constraints, consistent with exogenous processes.
- Break condition: If any key exogenous variable is unobservable and cannot be forecast with sufficient accuracy, the reduction to supervised learning fails.

### Mechanism 2
- Claim: General arrival dynamics (multiple shipments, stochastic yields, batch constraints) can be modeled accurately via generative modeling.
- Mechanism: Gen-QOT learns the joint distribution of arrival sequences conditioned on actions and historical context, allowing realistic simulation of inventory arrivals under vendor constraints.
- Core assumption: Historical data captures the true distribution of arrivals and constraints; the generative model can approximate it closely.
- Evidence anchors:
  - [abstract] "We incorporate a deep generative model for the arrivals process as part of the history replay."
  - [section] "We propose a novel quantity over time (QOT) arrivals model, which generalizes all the settings described above."
  - [corpus] "An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems" discusses non-centralized vendor dynamics, supporting the need for such generative modeling.
- Break condition: If vendor constraints or yield distributions change rapidly, historical data may not reflect future dynamics, degrading model accuracy.

### Mechanism 3
- Claim: A differentiable simulator enables efficient policy learning via direct backpropagation through the inventory dynamics.
- Mechanism: By sampling from the learned generative model and rescaling, the simulator remains pathwise differentiable, allowing gradient-based policy optimization.
- Core assumption: The forward model's sampling procedure can be inverted or approximated to allow gradients to flow.
- Evidence anchors:
  - [abstract] "We show via simulation studies that this approach yields statistically significant improvements in profitability over production baselines."
  - [section] "The approach we take in our empirical work is the following: first, given the action at and the exogenous H i t,O, X i t, sample the estimated forward model..."
  - [corpus] No direct corpus evidence for differentiable simulation; this is a novel contribution.
- Break condition: If the sampling procedure introduces discontinuities or high variance, gradient estimates may become unstable.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Inventory control is naturally framed as an MDP with states, actions, rewards, and transitions.
  - Quick check question: What are the four components required to define an MDP?

- Concept: Reinforcement Learning Policy Optimization
  - Why needed here: The RL agent learns a policy to maximize long-term profitability under inventory dynamics.
  - Quick check question: How does the policy gradient theorem relate to maximizing expected return in this context?

- Concept: Supervised Learning Reduction
  - Why needed here: The IDP formulation allows leveraging supervised learning techniques instead of full RL exploration.
  - Quick check question: Why does exogenous state simplify the learning problem compared to fully endogenous MDPs?

## Architecture Onboarding

- Component map:
  - Gen-QOT generative model: Encodes historical context and predicts arrival sequences.
  - Inventory simulator: Differentiable module that replays historical data and simulates future states.
  - RL policy network: Wavenet encoder + MLP decoder mapping state to order quantities.
  - Post-processor: Handles vendor constraints (min order qty, batch size) before sending to vendor.

- Critical path:
  1. Historical data → Gen-QOT training (sequence modeling).
  2. Gen-QOT + policy → Simulator forward pass (inventory transitions).
  3. Simulator output → Reward calculation → Policy gradient update.

- Design tradeoffs:
  - Accuracy vs. generalization: Finer arrival class bins improve fit but risk overfitting.
  - Differentiability vs. realism: Sampling introduces noise; reparameterization could improve gradients.
  - Model complexity vs. training time: Larger encoder/decoder increases fidelity but slows training.

- Failure signatures:
  - Policy suggests infeasible orders → post-processor overrides too aggressively.
  - Simulator predictions diverge from real arrivals → Gen-QOT calibration fails.
  - Gradients vanish or explode → check sampling variance and network initialization.

- First 3 experiments:
  1. Validate Gen-QOT calibration on historical arrivals (binning vs. true quantiles).
  2. Backtest policy in simulator vs. baseline (Newsvendor) to confirm reward improvements.
  3. Run small-scale A/B test in production to confirm simulator predictions match off-policy behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the learned policy compare to traditional inventory management systems in real-world settings?
- Basis in paper: [explicit] The paper states that the learned policy outperforms traditional inventory management systems in real-world settings.
- Why unresolved: The paper mentions the comparison but does not provide specific performance metrics or details on how the comparison was conducted.
- What evidence would resolve it: Specific performance metrics (e.g., profitability, cost reduction) and details on the comparison methodology would provide a clearer understanding of the performance difference.

### Open Question 2
- Question: What is the impact of vendor constraints such as minimum order quantities and batch sizes on the learned policy?
- Basis in paper: [explicit] The paper mentions that the learned policy can handle vendor constraints such as minimum order quantities and batch sizes.
- Why unresolved: The paper does not provide specific details on how these constraints affect the learned policy's performance or decision-making process.
- What evidence would resolve it: Analysis of the policy's performance with and without these constraints, and insights into how the policy adapts to these constraints, would provide a clearer understanding of their impact.

### Open Question 3
- Question: How does the learned policy generalize to off-policy data?
- Basis in paper: [explicit] The paper mentions that the learned policy generalizes well to off-policy data.
- Why unresolved: The paper does not provide specific details on the performance of the policy on off-policy data or the extent of its generalization capabilities.
- What evidence would resolve it: Detailed analysis of the policy's performance on off-policy data, including metrics such as accuracy, profitability, and adaptability, would provide insights into its generalization capabilities.

## Limitations
- **Data representativeness**: The Gen-QOT model relies heavily on historical purchase order data from 2017-2019 to simulate future arrivals. If vendor constraints or yield distributions change significantly after the training period, the model may fail to capture new dynamics.
- **Simulator fidelity**: While the differentiable simulator is claimed to enable efficient policy learning, the paper does not provide detailed validation of how well the simulator's predictions match real-world inventory arrivals, especially for out-of-distribution scenarios.
- **A/B test duration**: The real-world A/B test is described as lasting only a few weeks. Short testing periods may not capture seasonal effects or longer-term policy impacts, raising questions about the robustness of the reported improvements.

## Confidence

- **High confidence**: The reduction of the inventory control problem to supervised learning via exogenous decision process formulation is well-grounded in the literature and supported by strong theoretical results from Madeka et al. (2022).
- **Medium confidence**: The claim that Gen-QOT generalizes well to off-policy data is supported by simulation studies and a real-world A/B test, but the short duration of the A/B test and limited discussion of out-of-distribution performance introduce uncertainty.
- **Low confidence**: The claim that the differentiable simulator enables efficient policy learning is novel but lacks direct empirical validation against alternative non-differentiable approaches or detailed ablation studies.

## Next Checks

1. **Out-of-time validation**: Hold out a recent year of data not seen during training and evaluate Gen-QOT's ability to accurately simulate inventory arrivals under the same vendor constraints. Compare calibration and CRPS scores against in-time performance to quantify temporal generalization.
2. **Sim2Real gap analysis**: After the A/B test, collect and analyze the actual arrival sequences under the learned policy. Compare these to the simulator's predictions to quantify any systematic biases or divergences, especially for products with rare arrival patterns.
3. **Robustness to constraint changes**: Simulate scenarios where vendor constraints (e.g., minimum order quantities, batch sizes) are abruptly changed. Evaluate whether the policy and Gen-QOT model can adapt quickly or if performance degrades, highlighting potential brittleness in real-world deployment.