---
ver: rpa2
title: 'Towards Better Modeling with Missing Data: A Contrastive Learning-based Visual
  Analytics Perspective'
arxiv_id: '2309.09744'
source_url: https://arxiv.org/abs/2309.09744
tags:
- data
- negative
- missing
- sampling
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel contrastive learning framework for
  handling missing data in machine learning models. The approach learns similarity
  between incomplete and complete samples, as well as dissimilarity between other
  samples, without requiring data imputation.
---

# Towards Better Modeling with Missing Data: A Contrastive Learning-based Visual Analytics Perspective

## Quick Facts
- **arXiv ID**: 2309.09744
- **Source URL**: https://arxiv.org/abs/2309.09744
- **Reference count**: 40
- **Primary result**: Novel contrastive learning framework for handling missing data without imputation, integrated with visual analytics system CIVis

## Executive Summary
This paper introduces a novel contrastive learning framework for handling missing data in machine learning models without requiring data imputation. The approach learns similarity between incomplete and complete samples while pushing away other samples in the embedding space. The framework is integrated into CIVis, a visual analytics system that enhances interpretability and allows users to interactively adjust sampling strategies based on domain knowledge. The system includes several visualization techniques to monitor training, diagnose model collapse, and inspect prediction quality.

## Method Summary
The method employs a two-model architecture where a full model is trained on complete data and a semi-model is trained on incomplete data using contrastive loss against both the full model's embeddings and momentum encoders for negative samples. Users interactively guide positive and negative sampling through visual analytics, selecting hard negative samples via circle-based glyphs and defining positive pairs through Sankey-like visualizations. The framework minimizes both contrastive loss and task loss, achieving high predictive accuracy without imputation while providing interpretable results through interactive visualization.

## Key Results
- Achieves high predictive accuracy on house price prediction and credit card repayment classification tasks
- Eliminates need for data imputation while maintaining or improving model performance
- Provides interpretable results through interactive visual analytics system CIVis
- Demonstrates effectiveness through quantitative experiments, expert interviews, and user study

## Why This Works (Mechanism)

### Mechanism 1
The contrastive learning framework learns similarity between incomplete and complete samples without requiring imputation. For each incomplete sample, the model learns to align its embedding with the embedding of its complete counterpart (positive pair) while pushing away embeddings of other samples (negative pairs) in the latent space. The framework falls back to using only the task loss when complete forms cannot be identified.

### Mechanism 2
The two-model architecture (full model and semi-model) enables effective contrastive learning without imputation. The full model is trained on complete data and provides the "complete form" embedding for positive pairs, while the semi-model is trained on incomplete data using contrastive loss against both the full model's embeddings and a momentum encoder's embeddings for negative samples. The contrastive learning framework may not provide additional benefits if the full model cannot achieve higher performance than the semi-model.

### Mechanism 3
Interactive sampling strategies guided by visual analytics enable effective contrastive learning for missing data. Users interactively select negative samples through circle-based glyphs showing embedding distributions, and define positive pairs through Sankey-like visualizations of bin-to-bin mappings, allowing domain knowledge to guide the hardness of contrastive pairs. The visual representations may not accurately reflect underlying embedding distributions if users lack sufficient domain knowledge to make effective sampling decisions.

## Foundational Learning

- **Concept**: Contrastive Learning (CL) principles
  - Why needed here: The entire framework is built on CL concepts of alignment (similarity between positive pairs) and uniformity (dissimilarity between negative pairs)
  - Quick check question: Can you explain the difference between positive and negative sampling in CL and why both are necessary?

- **Concept**: Missing data mechanisms
  - Why needed here: Different missing data mechanisms (MCAR, MAR, MNAR) affect which imputation methods would work and how the contrastive framework should handle the data
  - Quick check question: What are the three main missing data mechanisms and how would they affect the identification of positive pairs?

- **Concept**: Embedding space analysis
  - Why needed here: Understanding how embeddings represent similarity/dissimilarity is crucial for interpreting the circle-based glyphs and Sankey visualizations
  - Quick check question: How does cosine similarity in embedding space relate to the visual representation in the circle-based glyph?

## Architecture Onboarding

- **Component map**: Raw data → feature selection → semi-data/full-data split → model training → inference
- **Critical path**: Feature selection → Pre-training → Negative sampling → Positive sampling → Training with contrastive loss → Inference
- **Design tradeoffs**:
  - Imputation vs. contrastive learning: Avoids imputation bias but requires finding complete counterparts
  - Two-model vs. single-model: Enables effective contrastive learning but increases computational complexity
  - Interactive vs. automated sampling: Allows domain knowledge integration but requires user expertise
- **Failure signatures**:
  - Model collapse: All embeddings converge to similar values (detected by mean distances approaching 1)
  - Poor alignment: Positive pairs remain distant despite training (detected by high mean positive distance)
  - Poor uniformity: Negative pairs remain close despite training (detected by low variance in negative distances)
- **First 3 experiments**:
  1. Test with synthetic data where complete counterparts are guaranteed to exist - verify that the framework can find them and improve semi-model performance
  2. Test with real data with small missing rates (<10%) - verify that the visual analytics effectively guide sampling strategy
  3. Test with varying missing rates (10%, 30%, 50%) - observe how framework performance degrades as complete counterparts become harder to find

## Open Questions the Paper Calls Out

- **Open Question 1**: What are the specific conditions under which contrastive learning-based approaches outperform traditional imputation methods in handling missing data?
  - Basis in paper: The paper mentions that the effectiveness of CIVis is demonstrated through quantitative experiments comparing it with imputation algorithms, but the specific conditions for superiority are not detailed.
  - Why unresolved: The paper does not provide a detailed analysis of the conditions under which CL-based approaches are more effective, such as the types of datasets or missing data patterns.
  - What evidence would resolve it: A comprehensive study comparing CL-based methods with imputation techniques across various datasets with different missing data patterns and distributions.

- **Open Question 2**: How does the choice of positive and negative sampling strategies impact the performance of the contrastive learning framework in different domains?
  - Basis in paper: The paper discusses the importance of sampling strategies in CL but does not provide a detailed analysis of how different strategies affect performance across various domains.
  - Why unresolved: The paper does not explore the impact of different sampling strategies on the performance of the CL framework in various application domains.
  - What evidence would resolve it: An empirical study evaluating the impact of different sampling strategies on the performance of the CL framework across multiple domains and datasets.

- **Open Question 3**: What are the limitations of the current visual analytics system, CIVis, in terms of scalability and usability for large-scale datasets?
  - Basis in paper: The paper mentions scalability concerns and learning curves as potential limitations of CIVis.
  - Why unresolved: The paper does not provide a detailed analysis of the scalability issues or user experience challenges when applying CIVis to large-scale datasets.
  - What evidence would resolve it: A user study or empirical analysis of CIVis's performance and usability with large-scale datasets, including latency measurements and user feedback.

## Limitations
- Framework effectiveness depends critically on availability of complete counterparts for incomplete samples
- Visual analytics components may require significant domain expertise to use effectively
- Two-model architecture increases computational complexity without clear evidence that benefits outweigh costs in all scenarios

## Confidence
- **High confidence** in theoretical framework and core mechanisms
- **Medium confidence** in practical effectiveness based on two usage scenarios and quantitative experiments
- **Low confidence** in generalizability across different missing data patterns and domain contexts

## Next Checks
1. **Missing Rate Sensitivity Analysis**: Systematically evaluate framework performance across varying missing rates (10%, 30%, 50%, 70%) using controlled synthetic datasets to establish performance boundaries and identify the point where complete counterparts become too scarce for effective contrastive learning.

2. **Expert Usability Study**: Conduct structured user studies with domain experts from different fields (healthcare, finance, social sciences) to assess the learning curve, effectiveness of visual analytics guidance, and whether the interactive sampling strategies actually improve model performance compared to automated approaches.

3. **Alternative Architecture Comparison**: Implement and benchmark a single-model variant using standard imputation methods against the proposed two-model contrastive approach to quantify the specific benefits of the contrastive learning framework beyond simply avoiding imputation.