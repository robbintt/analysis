---
ver: rpa2
title: De Novo Drug Design with Joint Transformers
arxiv_id: '2310.02066'
source_url: https://arxiv.org/abs/2310.02066
tags:
- joint
- transformer
- molecules
- training
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach for de novo drug design by
  framing it as a probabilistic black-box optimization problem. The core method is
  the Joint Transformer, which combines a Transformer decoder, encoder, and predictor
  in a single model with shared weights, trained using a penalized log-likelihood
  objective.
---

# De Novo Drug Design with Joint Transformers

## Quick Facts
- arXiv ID: 2310.02066
- Source URL: https://arxiv.org/abs/2310.02066
- Reference count: 26
- Primary result: Joint Transformer outperforms standard fine-tuning approaches and state-of-the-art SMILES-based optimization methods in generating optimized molecules while maintaining high generative and predictive performance.

## Executive Summary
This paper proposes a novel approach for de novo drug design by framing it as a probabilistic black-box optimization problem. The core method is the Joint Transformer, which combines a Transformer decoder, encoder, and predictor in a single model with shared weights, trained using a penalized log-likelihood objective. This joint training enables simultaneous molecule generation and property prediction. The Joint Transformer is then used in a probabilistic black-box optimization algorithm to generate optimized molecules. Results show that the Joint Transformer outperforms standard fine-tuning approaches and state-of-the-art SMILES-based optimization methods in generating optimized molecules, while maintaining high generative and predictive performance.

## Method Summary
The Joint Transformer is a single model combining a Transformer decoder, encoder, and predictor with shared weights (θ), trained using a penalized log-likelihood objective that alternates between generation and prediction tasks based on a task probability ptask. The model is pre-trained unsupervised on ChEMBL 24 dataset, then fine-tuned on property-labeled subsets. For optimization, it uses probabilistic black-box optimization with acceptance sampling to generate molecules meeting target property thresholds. The method is evaluated on molecule generation, targeted virtual screening, and de novo drug design tasks.

## Key Results
- Joint Transformer outperforms standard fine-tuning approaches with 42% reduction in prediction error on generated molecules
- Successfully generates molecules better than those in the dataset across all tasks, outperforming other methods by a large margin
- Maintains high generative performance (validity, uniqueness, novelty) while achieving state-of-the-art predictive performance

## Why This Works (Mechanism)

### Mechanism 1
The penalized log-likelihood objective enables simultaneous training of generation, prediction, and reconstruction without catastrophic forgetting. The training procedure randomly switches between input generation (maximizing pθ(x)) and property prediction/reconstruction (maximizing pθ,ϕ(y|x) and pθ(xd|m⊙x−d)) using a task probability ptask. This forces the shared weights θ to learn representations useful for both tasks. If ptask is set too close to 0 or 1, the model may overfit to prediction/generation respectively, leading to poor performance in the other task.

### Mechanism 2
The joint generative model provides reliable predictions for newly generated molecules, enabling probabilistic black-box optimization. By sharing weights between the decoder, encoder, and predictor, the model learns representations that capture both molecular structure and properties. This allows pθ,ϕ(y|x) to provide meaningful predictions for molecules sampled from pθ(x), which is essential for the probabilistic black-box optimization algorithm. If the model overfits to training distribution properties, predictions for novel molecules may become unreliable, breaking the optimization algorithm.

### Mechanism 3
The conditional generation procedure with acceptance sampling is equivalent to directly sampling from p(x|y) and has finite expected runtime. Proposition 1 shows that sampling (x,y) ~ pθ,ϕ(x,y) and accepting if y∈Y is equivalent to sampling from p(x|y). Proposition 2 provides conditions for finite expected runtime based on the probability of sampling y above threshold yc. If the target threshold yc is set too high (outside the support of p(y)), the acceptance probability approaches zero, leading to infinite expected runtime.

## Foundational Learning

- **Transformer architectures and attention mechanisms**: Why needed here - The JOINT TRANSFORMER relies on both decoder-only and encoder-only Transformers, requiring understanding of how causal vs bidirectional masking affects training and generation. Quick check question: What is the key architectural difference between a Transformer decoder and encoder, and how does this affect their respective use cases?

- **Probabilistic generative modeling and likelihood objectives**: Why needed here - The paper frames de novo drug design as a probabilistic optimization problem and uses penalized log-likelihood for training, requiring understanding of maximum likelihood estimation and its properties. Quick check question: How does the penalized log-likelihood objective in Eq. 6 combine the different training objectives, and what is the role of the task probability ptask?

- **Black-box optimization and conditional sampling**: Why needed here - The proposed method uses probabilistic black-box optimization with acceptance sampling, requiring understanding of how to optimize over distributions rather than point estimates. Quick check question: How does the acceptance sampling procedure in Algorithm 2 implement the conditional generation from p(x|y), and what guarantees does Proposition 2 provide?

## Architecture Onboarding

- **Component map**: Shared Transformer backbone (θ weights) with 6 layers, 8 heads, 256 dim embeddings -> Causal masking for decoder (input generation mode) -> Bidirectional masking for encoder (prediction/reconstruction mode) -> MLP predictor head (ϕ weights) stacked on encoder output -> Random task switching controlled by ptask hyperparameter

- **Critical path**: 1) Pre-training: Unsupervised training on ChEMBL dataset using penalized log-likelihood, 2) Fine-tuning: Supervised training on property-labeled subsets, 3) Generation: Unconditional sampling from pθ(x) followed by prediction from pθ,ϕ(y|x), 4) Optimization: Conditional sampling with acceptance based on property thresholds

- **Design tradeoffs**: Larger models (50M vs 6M parameters) provide marginal improvements in KL divergence and FCD but increase computational cost; Higher ptask values improve generative performance but may reduce predictive accuracy; The choice between supervised vs unsupervised fine-tuning depends on availability of property-labeled data

- **Failure signatures**: Zero validity in generated molecules indicates forgetting of generation task (ptask too low); Poor property prediction on generated molecules indicates misalignment between generation and prediction (imbalanced training); Infinite runtime in optimization indicates target threshold outside model support (improper threshold setting)

- **First 3 experiments**: 1) Train JOINT TRANSFORMER with ptask=0.5 and evaluate unconditional generation metrics (validity, uniqueness, novelty, KL divergence, FCD), 2) Compare predictive performance on test vs generated molecules using mean absolute error for a property prediction task, 3) Implement Algorithm 2 with different threshold values and measure optimization success rate and runtime

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of masking distribution q(m) in the penalized log-likelihood objective affect the generative and predictive performance of JOINT TRANSFORMER? The paper mentions using an arbitrary masking distribution q(m) but does not provide an analysis of how different masking distributions impact the model's performance. Experiments varying the masking distribution and analyzing the resulting generative and predictive performance would resolve this question.

### Open Question 2
What is the optimal value of the task probability ptask that balances generative and predictive performance in JOINT TRANSFORMER? While the paper provides results for specific values of ptask (0.1 and 0.5) in the ablation study, it does not explore a range of values to determine the optimal setting for different tasks. Experiments varying ptask across a range of values for different tasks would identify the optimal setting.

### Open Question 3
How does JOINT TRANSFORMER perform on molecule optimization tasks with continuous target properties compared to discrete ones? The paper focuses on optimizing molecules for discrete properties but does not explore continuous target properties. Experiments using JOINT TRANSFORMER to optimize molecules for continuous target properties (e.g., binding affinity) and comparing its performance to discrete property optimization tasks would resolve this question.

## Limitations
- Focuses on SMILES representation without exploring graph-based alternatives that may capture molecular structure more naturally
- Computational cost of training the joint model versus separate specialized models is not thoroughly analyzed
- Method requires fine-tuning on property-labeled subsets, which may not be available for all drug discovery applications

## Confidence
- **High**: The Joint Transformer architecture is technically sound; the penalized log-likelihood training objective provides a principled framework; the acceptance sampling procedure is mathematically correct
- **Medium**: Empirical improvements over standard fine-tuning are well-supported; method successfully generates molecules better than training data; comparison to other SMILES-based approaches is supported but limited
- **Low**: "State-of-the-art" claim based on limited baselines; exact contribution of joint architecture vs training procedure vs optimization algorithm is unclear; scalability and robustness to different molecular scaffolds remains unclear

## Next Checks
1. **Ablation Study**: Train separate decoder-only, encoder-only, and predictor-only models using the same penalized log-likelihood objective to quantify the contribution of joint architecture versus joint training
2. **Threshold Sensitivity Analysis**: Systematically vary the target threshold yc across multiple orders of magnitude to map the region of finite expected runtime and quantify optimization success rates
3. **Distribution Shift Analysis**: Compare the molecular properties of generated samples versus test set molecules using multiple statistical tests (KL divergence, FCD, property distributions) to better understand how generated molecules differ from training data