---
ver: rpa2
title: 'PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering
  via Pluggable Reward-Driven Contextual Adapter'
arxiv_id: '2310.18347'
source_url: https://arxiv.org/abs/2310.18347
tags:
- prca
- generator
- performance
- answer
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating large language
  models (LLMs) as generators in retrieval question answering (ReQA) tasks, where
  fine-tuning these models is computationally expensive or impossible due to their
  closed-source nature. The authors propose a Pluggable Reward-Driven Contextual Adapter
  (PRCA) that acts as an intermediary between the retriever and the generator, refining
  retrieved documents to improve answer quality without modifying the generator.
---

# PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter

## Quick Facts
- arXiv ID: 2310.18347
- Source URL: https://arxiv.org/abs/2310.18347
- Reference count: 8
- Primary result: Improves ReQA performance by up to 20% on average using a pluggable adapter that refines retrieved documents without modifying black-box generators

## Executive Summary
This paper addresses the challenge of integrating large language models (LLMs) as generators in retrieval question answering (ReQA) tasks without requiring fine-tuning of these expensive or closed-source models. The authors propose a Pluggable Reward-Driven Contextual Adapter (PRCA) that operates between the retriever and generator, refining retrieved documents into more concise and relevant contexts. PRCA is trained using a two-stage approach: first through supervised learning on summarization tasks, then through reinforcement learning guided by the generator's outputs. Experimental results demonstrate that PRCA achieves significant performance improvements across three benchmark datasets while maintaining computational efficiency with minimal parameter overhead.

## Method Summary
PRCA is a token-autoregressive adapter trained in two stages to refine retrieved documents for black-box LLM generators in ReQA tasks. The first stage uses supervised learning on summarization datasets to learn document extraction, while the second stage employs reinforcement learning with the generator's outputs as reward signals. The adapter operates as a pluggable component between any retriever-generator pair, distilling Top-K retrieved documents into more effective input contexts. The method achieves this without modifying the generator's parameters, enabling integration with closed-source models. Training uses BART-Large initialized from CNN Daily Mail summarization model, with evaluation conducted using GPT-4-based binary correctness judgments across 75 retriever-generator combinations on three benchmark datasets.

## Key Results
- Improves ReQA performance by up to 20% on average across SQuAD, HotpotQA, and TopiOCQA datasets
- Achieves 3% improvement on SQuAD, 6% on HotpotQA, and 9% on TopiOCQA compared to baseline retriever-generator pairs
- Adds only 0.4 billion parameters versus generator's 7 billion average, demonstrating computational efficiency
- Effectively simplifies input text, enabling generators to process complex queries more effectively

## Why This Works (Mechanism)

### Mechanism 1
PRCA improves generator performance by refining retrieved documents before passing them to the generator. It operates as a token-autoregressive adapter positioned between the retriever and generator, distilling retrieved documents into a more concise and relevant context that better guides the generator's answer generation. The core assumption is that generator performance improves when given refined, distilled information rather than raw retrieved documents.

### Mechanism 2
PRCA enables black-box LLM integration by avoiding direct fine-tuning of the generator. Instead of modifying the generator's parameters, PRCA learns to transform retrieved documents into optimal input format. This allows integration of closed-source or computationally expensive LLMs without requiring access to their weights. The core assumption is that the quality of generator outputs depends more on input formatting than on internal parameter tuning for specific datasets.

### Mechanism 3
Two-stage training enables effective learning despite black-box generator constraints. PRCA first learns document extraction through supervised learning on domain summarization tasks, then refines this knowledge through reinforcement learning guided by generator rewards. This circumvents the inability to backpropagate through the black-box generator. The core assumption is that the generator's output quality can serve as a reliable reward signal for training PRCA.

## Foundational Learning

- **Reinforcement Learning with Proximal Policy Optimization (PPO)**: PRCA requires optimization without direct access to generator parameters, necessitating policy gradient methods that can work with reward signals. Quick check: What are the key components of PPO's objective function and how do they prevent excessive policy updates?

- **Sequence-to-Sequence Modeling with Transformers**: PRCA operates as a token-autoregressive model that transforms input sequences (query + documents) into refined output contexts. Quick check: How does the attention mechanism in transformers enable PRCA to effectively process and transform variable-length document sequences?

- **Information Retrieval Metrics and Evaluation**: Understanding retrieval quality metrics (like ROUGE-L) is essential for evaluating PRCA's effectiveness in distilling relevant information. Quick check: What is the difference between ROUGE-N and ROUGE-L metrics, and why is ROUGE-L particularly suitable for evaluating PRCA's output?

## Architecture Onboarding

- **Component map**: Query → Retriever → Top-K documents → PRCA → Refined context → Generator → Answer

- **Critical path**: 
  1. Query → Retriever → Top-K documents
  2. Query + documents → PRCA → Refined context
  3. Refined context → Generator → Answer
  4. Answer vs ground truth → Reward signal → PRCA update

- **Design tradeoffs**:
  - Parameter efficiency vs performance: PRCA adds only 0.4B parameters vs generator's 7B average, trading some performance for significant efficiency gains
  - Training complexity vs flexibility: Two-stage training is more complex but enables black-box generator integration
  - Context length vs information preservation: PRCA must balance simplification with retaining critical information

- **Failure signatures**:
  - Performance degradation when PRCA is added (indicates distillation removing critical information)
  - Inconsistent training convergence across different retriever-generator pairs
  - Reward signal instability during reinforcement learning phase
  - GPU memory overflow during PRCA training with large document batches

- **First 3 experiments**:
  1. Baseline comparison: Run retriever + generator without PRCA on SQuAD to establish performance floor
  2. Single retriever ablation: Test PRCA with one retriever-generator pair (e.g., DPR + T5) to verify basic functionality
  3. Reward sensitivity: Vary reward calculation parameters (temperature, top-k) to find optimal configuration for stable training

## Open Questions the Paper Calls Out

### Open Question 1
How does PRCA perform when integrated with other retrieval methods not mentioned in the paper, such as ColBERT or ANCE? The paper only tested PRCA with five retrievers (BM25, SentenceBert, DPR, SimCSE, and Contriver), leaving the performance with other retrieval methods unexplored.

### Open Question 2
Can PRCA be extended to handle multi-modal data, such as combining text with images or other data types? The current PRCA model is designed for text-based data, and its performance with multi-modal data is not investigated.

### Open Question 3
How does the performance of PRCA change when the generator is fine-tuned on the specific dataset instead of keeping it frozen? The paper keeps the generator frozen throughout the experiments to demonstrate PRCA's effectiveness as a pluggable adapter.

## Limitations

- The evaluation relies entirely on GPT-4's binary judgment rather than human evaluation, which may not capture nuanced answer quality and could introduce systematic bias in assessment.
- While PRCA achieves significant improvements on average, the absolute performance gains vary considerably across datasets, with SQuAD showing only 3% improvement compared to 9% on TopiOCQA.
- The training procedure for PRCA in the reward-driven stage is not fully specified, particularly regarding the implementation of the PPO algorithm when working with a black-box generator API.

## Confidence

- **High Confidence**: The core mechanism of using a pluggable adapter to transform retrieved documents before generator input is well-established and the computational efficiency claims (0.4B vs 7B parameters) are verifiable through parameter counting.
- **Medium Confidence**: The two-stage training approach and its effectiveness in the black-box setting, as the paper provides methodology but lacks detailed hyperparameter specifications for the reinforcement learning phase.
- **Low Confidence**: The generalizability of PRCA across different retriever-generator combinations, given that the paper reports average improvements but doesn't deeply analyze failure cases or performance degradation scenarios.

## Next Checks

1. **Ablation Study**: Remove PRCA from the pipeline and compare raw retriever performance against generator-only performance to quantify the exact contribution of PRCA versus retriever improvements.

2. **Cross-Dataset Transfer**: Train PRCA on one dataset (e.g., SQuAD) and evaluate its performance on another (e.g., HotpotQA) to test whether the two-stage training creates generalizable document transformation capabilities or dataset-specific optimizations.

3. **Generator Sensitivity Analysis**: Systematically vary generator temperature and decoding parameters while keeping PRCA constant to determine whether performance improvements stem from PRCA's document transformation or from interactions with generator inference settings.