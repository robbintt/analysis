---
ver: rpa2
title: 'LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation'
arxiv_id: '2310.04535'
source_url: https://arxiv.org/abs/2310.04535
tags:
- bins
- coverage
- sampling
- responses
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether LLMs can generate test stimuli for
  hardware design verification. A benchmark framework LLM4DV is introduced, using
  LLM-generated prompts to iteratively create test stimuli and update coverage.
---

# LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation

## Quick Facts
- arXiv ID: 2310.04535
- Source URL: https://arxiv.org/abs/2310.04535
- Reference count: 37
- One-line primary result: LLM4DV achieves 98.94%, 86.19%, and 5.61% coverage on three hardware modules, outperforming constrained-random testing especially for simpler designs.

## Executive Summary
This paper introduces LLM4DV, a framework that uses large language models to generate test stimuli for hardware design verification. The approach employs iterative prompting with coverage feedback to target uncovered coverage bins. Four prompt improvements are proposed and evaluated. Experiments on three hardware modules show the LLM outperforms constrained-random testing on simpler designs but struggles with more complex modules, highlighting the need for further prompt engineering.

## Method Summary
The LLM4DV framework uses an LLM stimulus generation agent that iteratively creates test stimuli based on coverage feedback. The process involves a coverage-feedback prompt template with three parts: result summary, differences, and task description. Four improvements are implemented: missed-bin sampling to target uncovered bins, best-iterative-message sampling to leverage successful difficult responses, dialogue restarting to prevent LLM stagnation, and buffer resetting to maintain progress. The framework uses GPT-3.5-turbo-0613 with temperature=0.4, top_p=1.0, and max_tokens=600, running fixed-budget experiments with 10,000,000 tokens budget.

## Key Results
- Achieved 98.94% coverage on Primitive Data Prefetcher Core module
- Achieved 86.19% coverage on Ibex Instruction Decoder module
- Achieved only 5.61% coverage on Ibex CPU module
- Outperformed constrained-random testing baseline on simpler modules
- Performance degrades significantly on more complex hardware designs

## Why This Works (Mechanism)

### Mechanism 1
The LLM stimulus generation agent can leverage pre-trained knowledge to efficiently hit coverage bins in simpler hardware modules. The LLM uses its pre-trained understanding of RISC-V instructions and hardware design patterns to generate stimuli that satisfy complex coverage conditions without exhaustive enumeration. This assumes the LLM has been trained on sufficient examples of hardware design patterns and can generalize this knowledge to novel but similar designs. The break condition occurs when the LLM fails to generate valid stimuli that satisfy coverage conditions, leading to stagnation in coverage improvement.

### Mechanism 2
Iterative prompting with coverage feedback enables targeted test stimulus generation. The framework uses a coverage-feedback prompt template that provides the LLM with information about which bins have been covered and which remain uncovered, allowing it to generate targeted stimuli in subsequent iterations. This assumes the LLM can effectively process and utilize feedback about uncovered bins to generate more targeted stimuli in subsequent iterations. The break condition occurs when the LLM fails to generate novel stimuli even with feedback, leading to repeated generation of stimuli that don't cover new bins.

### Mechanism 3
Dialogue restarting mechanism prevents LLM stagnation by resetting context when progress stalls. When the LLM hits fewer than three new bins within a threshold number of responses, the dialogue record is cleared and restarted from the system message and initial query, effectively giving the LLM a fresh context to work from. This assumes the LLM's context window can become saturated or biased, leading to repeated mistakes, and resetting this context allows for fresh generation attempts. The break condition occurs when the dialogue restarting mechanism is triggered too frequently, indicating the LLM is fundamentally unable to progress on the task.

## Foundational Learning

- **Coverage plans and coverage bins in hardware design verification**: Needed because the LLM stimulus generation agent needs to understand the coverage plan to generate stimuli that target uncovered bins and achieve 100% coverage. Quick check: What are the three types of coverage bins used in the Primitive Data Prefetcher Core module?
- **Prompt engineering techniques (system messages, clear instructions, syntax)**: Needed because the framework relies on carefully crafted prompts to guide the LLM's stimulus generation behavior and output format. Quick check: What are the three parts of the initial query in the coverage-feedback prompt template?
- **Large language models and their capabilities**: Needed because the framework uses LLMs as the core component for automated test stimuli generation, so understanding their strengths and limitations is crucial. Quick check: What is the maximum number of responses allowed before a trial is considered "exhausted"?

## Architecture Onboarding

- **Component map**: Stimulus generation agent (prompted LLM) -> Design-under-test (DUT) -> Coverage monitor -> Prompt generator -> Stimulus extractor -> Stimulus buffer
- **Critical path**: 1. Prompt generator creates prompt based on coverage feedback 2. LLM generates response containing stimuli 3. Stimulus extractor extracts stimuli from response 4. Stimuli are input to DUT 5. Coverage monitor updates coverage based on DUT behavior 6. Process repeats until 100% coverage or exhaustion
- **Design tradeoffs**: Using LLM vs. traditional constrained-random testing offers the ability to leverage pre-trained knowledge but may struggle with very complex designs; iterative prompting allows for targeted generation but requires more LLM interactions; dialogue restarting frequency must balance between preventing stagnation and allowing meaningful progress
- **Failure signatures**: Coverage improvement plateaus despite multiple LLM interactions; LLM generates invalid or repetitive stimuli; trial "exhaustion" occurs (fewer than 3 bins hit in 25 responses or fewer than 3 bins hit in 40 responses)
- **First 3 experiments**: 1. Run the framework with the Primitive Data Prefetcher Core module using the best-performing configuration from Table 1 2. Compare coverage improvement over time between the LLM-based approach and constrained-random testing baseline 3. Experiment with different prompt templates (e.g., adding one-line task introduction) to observe impact on coverage rate

## Open Questions the Paper Calls Out

### Open Question 1
How can LLM4DV be optimized to achieve higher coverage rates on complex hardware modules like the Ibex CPU? The paper notes that the framework achieves only 5.61% coverage on the Ibex CPU, which is significantly lower than the coverage on simpler modules. This remains unresolved as the paper suggests that further prompt engineering and prompting improvements are needed but does not provide specific strategies or solutions. Demonstrating improved coverage rates on the Ibex CPU through specific prompt engineering techniques or new prompting improvements would resolve this question.

### Open Question 2
What are the specific challenges in using LLMs for hardware design verification compared to software code generation? The paper highlights that hardware design verification involves different settings and more sophisticated procedures compared to software code generation. This remains unresolved as the paper does not provide a detailed comparison of the challenges between hardware and software verification using LLMs. A comprehensive study comparing the effectiveness of LLMs in hardware versus software verification, highlighting specific challenges and potential solutions, would resolve this question.

### Open Question 3
How do different sampling methods in the missed-bin sampling improvement affect the efficiency and effectiveness of the LLM4DV framework? The paper introduces three sampling methods: Pure Random Sampling, Coverpoint Type-based Sampling, and Mixed Coverpoint Type-based and Pure Random Sampling, and notes their varying impacts on performance. This remains unresolved as the paper does not provide a detailed analysis of how each sampling method influences the framework's performance in different scenarios. Empirical results comparing the performance of each sampling method across various hardware modules and complexity levels would resolve this question.

## Limitations

- Framework performance degrades significantly on complex hardware modules, achieving only 5.61% coverage on the Ibex CPU
- Scalability to real-world hardware verification scenarios remains unproven given the limited scope of tested modules
- The specific contributions of each prompt improvement are not isolated in the experiments

## Confidence

- **High Confidence**: The framework's ability to outperform constrained-random testing on simpler modules (Primitive Data Prefetcher Core at 98.94% coverage) is well-supported by experimental evidence and clear mechanisms
- **Medium Confidence**: The effectiveness of the four proposed prompt improvements is demonstrated, though the specific contributions of each improvement are not isolated in the experiments
- **Low Confidence**: The scalability and generalization of LLM4DV to real-world hardware verification scenarios remains unproven, given the limited scope of tested modules

## Next Checks

1. **Module Complexity Gradient Test**: Evaluate LLM4DV on a series of increasingly complex hardware modules to determine the precise complexity threshold where the approach breaks down.

2. **Cross-Domain Generalization**: Test the framework with hardware modules from different application domains (e.g., networking, signal processing) to assess its ability to handle diverse design patterns and coverage requirements.

3. **Prompt Improvement Isolation**: Design ablation studies to measure the individual contribution of each prompt improvement (missed-bin sampling, best-iterative-message sampling, dialogue restarting, buffer resetting) to the overall performance.