---
ver: rpa2
title: Bridging Information-Theoretic and Geometric Compression in Language Models
arxiv_id: '2310.13620'
source_url: https://arxiv.org/abs/2310.13620
tags:
- wikitext
- size
- optcorpus
- tokens
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors analyze geometric and information-theoretic compression
  in large language models. They measure intrinsic dimension of linguistic data representations
  using multiple dimensionality estimators, and show that this dimension predicts
  information-theoretic compression (perplexity) and ease-of-adaptation.
---

# Bridging Information-Theoretic and Geometric Compression in Language Models

## Quick Facts
- arXiv ID: 2310.13620
- Source URL: https://arxiv.org/abs/2310.13620
- Authors: 
- Reference count: 40
- Key outcome: Intrinsic dimension of linguistic data representations predicts both information-theoretic compression (perplexity) and ease-of-adaptation in language models.

## Executive Summary
This paper establishes a novel empirical link between geometric and information-theoretic compression in large language models. The authors demonstrate that the intrinsic dimension (ID) of linguistic data representations - a measure of geometric compression - predicts information-theoretic compression as measured by perplexity, and further predicts how easily models can adapt to new datasets. Using multiple dimensionality estimation methods across OPT model variants, they show that learned linguistic structure enables compression, and that NN-based ID estimators best capture these relationships.

## Method Summary
The authors analyze pre-trained causal language models (OPT-350m, OPT-1.3b, OPT-6.7b) by extracting intermediate representations after embedding and attention/feed-forward blocks, aggregating them using the last token vector. They estimate intrinsic dimension using 12 different methods (9 NN-based, 1 PCA, 1 Fisher Separability, 1 Correlation Dimension) on representation matrices. Perplexity is calculated as the average negative log-likelihood loss. Models are fine-tuned using LoRA (except OPT-350m with full fine-tuning) on causal LM objectives with AdamW optimizer. The study correlates ID estimates with perplexity and adaptation metrics (final evaluation PPL, sample complexity, zero-shot PPL) using Spearman correlation.

## Key Results
- Information-theoretic and geometric description length are Spearman-correlated (ρ = 0.51, p = 0.01)
- Intrinsic dimension predicts sample complexity (ρ = 0.72, p < 0.01) and final PPL after convergence (ρ = 0.73, p < 0.01)
- Vocabulary size and entropy predict ID, while perplexity does not
- Ablating linguistic structure increases both ID and perplexity

## Why This Works (Mechanism)

### Mechanism 1
Geometric compression (intrinsic dimension) predicts information-theoretic compression (perplexity) because the intrinsic dimension captures the effective degrees of freedom in linguistic data representations. Lower ID implies more compact geometric structure, which translates to lower coding length. This relies on the low-dimensional manifold hypothesis holding for linguistic representations in neural networks. Evidence shows ID and perplexity are correlated (ρ = 0.51), though direct corpus evidence linking ID to perplexity is weak. Break condition: If linguistic representations don't lie on a low-dimensional manifold.

### Mechanism 2
Compression of linguistic data predicts ease-of-adaptation because models that compress data more effectively zero-shot have learned representations that are more generalizable. The bottleneck created by compression forces the model to capture essential linguistic structure. This assumes learned representations retain task-relevant information after compression. Evidence shows ID predicts sample complexity (ρ = 0.72) and final PPL (ρ = 0.73), though corpus evidence for compression predicting adaptation is weak. Break condition: If adaptation requires memorizing specific details rather than generalizing from compressed representations.

### Mechanism 3
Specific linguistic properties influence intrinsic dimension because more complex linguistic structures require more degrees of freedom to represent, increasing ID. Ablating syntax or lexical patterns increases ID, showing learned structure enables compression. This assumes linguistic structure has a measurable impact on representation geometry. Evidence shows ablating linguistic structure increases both ID and perplexity, and vocabulary size/entropy predict ID. Break condition: If linguistic representations are driven by factors other than surface linguistic properties.

## Foundational Learning

- **Manifold Learning Hypothesis**: High-dimensional data can lie on a lower-dimensional manifold. Needed to interpret intrinsic dimension results as capturing effective degrees of freedom. Quick check: What is the manifold learning hypothesis and why is it relevant to intrinsic dimension estimation?

- **Information Bottleneck Principle**: The relationship between compression and generalization in neural networks can be understood through this framework. Needed to understand why compressed representations generalize better. Quick check: How does the information bottleneck principle relate to the observed correlation between compression and ease-of-adaptation?

- **Intrinsic Dimension Estimation Methods**: Different ID estimation techniques have different assumptions and biases. Needed to interpret which estimators are most appropriate for linguistic data. Quick check: What are the key differences between linear (e.g., PCA) and nonlinear (e.g., NN-based) intrinsic dimension estimation methods?

## Architecture Onboarding

- **Component map**: Data Preprocessing (tokenization, sequence splitting) -> Model (OPT variants) -> Representation Extraction (intermediate activations) -> Aggregation (last token vector) -> ID Estimation (12 estimators) -> Perplexity Calculation -> Fine-tuning (LoRA/full) -> Adaptation Metrics

- **Critical path**: 1. Preprocess data and gather representations, 2. Estimate ID using NN-based methods and compare with PCA/FisherS, 3. Calculate zero-shot perplexity, 4. Establish correlation between ID and perplexity, 5. Fine-tune models and measure adaptation metrics, 6. Correlate ID with adaptation metrics

- **Design tradeoffs**: ID estimation vs. computational cost (subsampling 50,000 points), Linear vs. nonlinear ID estimators (NN-based more accurate but intensive), Full fine-tuning vs. parameter-efficient fine-tuning (adaptation quality vs. resources)

- **Failure signatures**: ID estimates that don't converge or are unstable across seeds, No correlation between ID and perplexity/adaptation metrics, Adaptation metrics that don't correlate with ID, Results highly sensitive to ID estimation hyperparameters

- **First 3 experiments**: 1. Run ID estimation on small dataset subsample with ESS, TwoNN, TLE methods, 2. Compute perplexity on same representations, 3. Establish initial correlation between ID and perplexity

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the "true" intrinsic dimension of natural language, and to what extent do language models recover it? The paper establishes LMs compress linguistic data into low-dimensional representations but doesn't determine if this matches inherent human language complexity. Resolution requires comparing LM representations to human cognitive/linguistic data.

- **Open Question 2**: How does finetuning dynamically affect data compression under the model? The paper analyzes pre-trained model compression and ease of finetuning but doesn't explore how compression changes during finetuning. Resolution requires tracking ID evolution throughout finetuning alongside performance metrics.

- **Open Question 3**: How do non-causal predictive objectives or instruction tuning affect the relationship between compression and ease of learning? The paper focuses on causal language models, leaving unclear if findings generalize to encoder-decoder or instruction-tuned models. Resolution requires replicating experiments with different model architectures/training objectives.

## Limitations
- Analysis focuses on causal language models and may not generalize to encoder-decoder or bidirectional architectures
- ID estimation methods have inherent limitations, particularly for high-dimensional data where manifold assumption may break down
- Study uses static pre-trained models without examining how compression properties evolve during training

## Confidence

- **High Confidence**: Empirical correlations between ID and perplexity (ρ = 0.51, p = 0.01), and between ID and adaptation metrics (ρ = 0.72-0.73, p < 0.01) are statistically robust
- **Medium Confidence**: Mechanism linking compression to generalization is theoretically sound but requires further validation
- **Low Confidence**: Specific contributions of individual linguistic properties to ID estimates need more controlled experiments

## Next Checks

1. **Intervention Study**: Systematically ablate specific linguistic structures (syntax, semantics, long-range dependencies) in controlled synthetic datasets to verify their individual contributions to ID and compression

2. **Cross-Architecture Validation**: Replicate core findings using encoder-decoder models (e.g., T5) and bidirectional models (e.g., BERT) to assess architecture-specific effects

3. **Training Dynamics Analysis**: Track ID evolution throughout training to determine whether compression emerges early or develops gradually with model capacity