---
ver: rpa2
title: 'CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language
  Models for Data Annotation'
arxiv_id: '2310.15638'
source_url: https://arxiv.org/abs/2310.15638
tags:
- llms
- arxiv
- annotation
- data
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoAnnotating, a novel framework for Human-LLM
  co-annotation of unstructured texts. It leverages uncertainty metrics to estimate
  LLM annotation capability and guide work allocation between humans and LLMs.
---

# CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation

## Quick Facts
- arXiv ID: 2310.15638
- Source URL: https://arxiv.org/abs/2310.15638
- Reference count: 12
- Primary result: Uncertainty-guided LLM-human collaboration improves annotation quality by up to 21% over random allocation

## Executive Summary
This paper introduces CoAnnotating, a framework for collaborative data annotation that leverages uncertainty metrics to optimally allocate annotation tasks between humans and large language models (LLMs). The framework estimates LLM annotation capability using entropy computed from multiple prompt variations, then guides work allocation to maximize annotation quality while minimizing cost. Experiments across six classification datasets demonstrate that entropy-guided allocation outperforms both random allocation and self-evaluation-guided allocation, with significant improvements in F1 scores.

## Method Summary
CoAnnotating employs a two-step approach: first, it generates multiple prompt variations for each instance and computes uncertainty metrics (entropy or self-evaluation scores) from LLM responses; second, it allocates instances to either human annotators or LLMs based on these uncertainty scores, with the allocation strategy optimized for quality-cost tradeoffs. The framework uses a Pareto efficiency framework to identify optimal allocation strategies and fine-tunes a RoBERTa classifier on the resulting annotated data. Experiments compare random allocation, entropy-guided allocation, and self-evaluation-guided allocation across six classification datasets with varying annotation proportions (0-100% LLM allocation).

## Key Results
- Entropy-guided allocation achieves up to 21% performance improvement over random allocation baselines
- Entropy-guided allocation consistently outperforms self-evaluation-guided allocation across all six datasets
- The framework enables effective cost-quality tradeoffs, with optimal allocation strategies identified via Pareto efficiency analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy of LLM responses across prompt variations serves as a reliable proxy for instance-level annotation capability
- Mechanism: The framework prompts the LLM multiple times with different prompt variations, then computes entropy over the resulting label distribution. Lower entropy indicates the LLM gives consistent predictions across prompts, suggesting high confidence and reliability for that instance.
- Core assumption: Consistent predictions across prompt perturbations correlate with true annotation quality
- Evidence anchors:
  - [section]: "We compute ui in two ways... entropy. Entropy is a measure of the impurity in a set of data and can be used to quantify the uncertainty associated with the class labels."
  - [abstract]: "Under this framework, we utilize uncertainty to estimate LLMs' annotation capability."
  - [corpus]: Weak correlation - corpus contains related uncertainty-guided approaches but no direct evidence for entropy effectiveness in LLM annotation
- Break condition: If LLM responses are systematically consistent even when incorrect (high bias), entropy would falsely indicate high capability

### Mechanism 2
- Claim: Self-reported confidence scores from LLMs can guide allocation but are unreliable compared to entropy
- Mechanism: The framework asks LLMs to report confidence scores, which are then inverted to estimate uncertainty. However, empirical results show these scores are often miscalibrated.
- Core assumption: LLMs can introspect their own confidence and report it accurately
- Evidence anchors:
  - [section]: "Self-Evaluation... ask the model to directly output its confidence score... uncertainty for ti is calculated by: ui = 1 − (average confidence)"
  - [section]: "Our results also show that confidence scores generated by LLMs are generally well-calibrated but not always reliable."
  - [corpus]: Weak evidence - corpus contains self-evaluation approaches but no direct evidence for LLM confidence calibration
- Break condition: When LLM systematically over/under-estimates its capabilities, self-evaluation becomes misleading

### Mechanism 3
- Claim: Pareto efficiency framework enables optimal tradeoff between annotation quality and cost
- Mechanism: By plotting F1 score against annotation cost for different allocation strategies, the framework identifies Pareto-optimal points where improving quality requires increased cost.
- Core assumption: The Pareto frontier represents the optimal balance point for practitioners
- Evidence anchors:
  - [section]: "We frame the co-annotation process as a multi-objective optimization problem... Inspired by Kang et al. (2023), we apply the Pareto efficiency concept in strategy selection."
  - [section]: "By adopting different allocation strategies and setting different proportions of data allocated to LLMs, we get various allocation patterns with different annotation qualities and costs."
  - [corpus]: Moderate evidence - corpus contains Pareto efficiency approaches in other domains but no direct evidence for NLP annotation
- Break condition: When cost-quality relationship is non-convex or has multiple local optima

## Foundational Learning

- Concept: Entropy as uncertainty measure
  - Why needed here: To quantify how consistently LLM responds across prompt variations
  - Quick check question: If an LLM gives 4 "positive" and 1 "negative" labels across 5 prompts, what is the entropy? (Answer: ~0.72)

- Concept: Multi-objective optimization
  - Why needed here: To balance competing goals of annotation quality vs. cost
  - Quick check question: What makes a solution Pareto efficient? (Answer: No other solution is better in one objective without being worse in another)

- Concept: Active learning principles
  - Why needed here: To select which instances to annotate with limited resources
  - Quick check question: What's the difference between instance-level and task-level expertise estimation? (Answer: Instance-level considers each data point's difficulty; task-level considers overall dataset performance)

## Architecture Onboarding

- Component map:
  Prompt generation module → LLM inference engine → Uncertainty computation → Allocation strategy → Fine-tuning pipeline → Evaluation module
  External dependencies: ChatGPT API, RoBERTa classifier, cost estimation functions

- Critical path: Prompt generation → LLM inference → Uncertainty computation → Allocation decision → Human/LLM annotation → Model training → Evaluation

- Design tradeoffs:
  - Multiple prompts vs. single prompt: Multiple prompts provide better uncertainty estimates but increase inference cost
  - Entropy vs. self-evaluation: Entropy is more reliable but requires multiple LLM calls; self-evaluation is cheaper but potentially miscalibrated
  - Fixed allocation vs. adaptive: Fixed ratios are simpler but adaptive approaches could optimize per-instance

- Failure signatures:
  - High entropy across all instances → LLM struggles with task generally
  - Low entropy but poor alignment with gold labels → LLM is consistently wrong (high bias)
  - Self-evaluation confidence high but actual performance poor → Miscalibration
  - Pareto frontier flat → Little benefit from additional cost

- First 3 experiments:
  1. Run baseline random allocation vs. entropy-guided allocation on AG News with 40% LLM allocation
  2. Compare entropy vs. self-evaluation on MRPC to demonstrate calibration issues
  3. Plot Pareto curves for TREC to identify optimal cost-quality tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CoAnnotating framework be extended to tasks beyond classification, such as generation tasks, and what impact would this have on the effectiveness of the proposed uncertainty-guided work allocation strategy?
- Basis in paper: [inferred] The paper mentions that the framework is generalizable to generation tasks and datasets in other languages, but does not provide experimental evidence or analysis of its effectiveness in these scenarios.
- Why unresolved: The current experiments focus solely on classification tasks and English datasets, leaving the framework's applicability and performance in generation tasks and other languages unexplored.
- What evidence would resolve it: Conducting experiments on generation tasks and datasets in multiple languages to compare the performance of CoAnnotating against other annotation strategies, and analyzing the framework's effectiveness in these new contexts.

### Open Question 2
- Question: How can the reliability of self-reported confidence scores from large language models (LLMs) be improved to enhance the accuracy of uncertainty-guided work allocation?
- Basis in paper: [explicit] The paper discusses the limitations of using self-reported confidence scores from LLMs, as they are not consistently reliable indicators of annotation quality, and suggests that entropy metrics may be more effective.
- Why unresolved: The current framework relies on entropy and self-evaluation metrics, but the reliability of self-reported confidence scores remains a concern, potentially impacting the effectiveness of the allocation strategy.
- What evidence would resolve it: Developing and testing methods to improve the calibration of self-reported confidence scores, such as fine-tuning or post-processing techniques, and evaluating their impact on the accuracy of uncertainty-guided work allocation.

### Open Question 3
- Question: How does the performance of the CoAnnotating framework vary with different prompt designs, and what is the optimal balance between prompt diversity and consistency in eliciting reliable annotations from LLMs?
- Basis in paper: [explicit] The paper explores the use of different prompt types to elicit annotations from LLMs and finds that entropy-guided allocation using diverse prompts is more effective than using the same prompt repeatedly.
- Why unresolved: While the paper demonstrates the benefits of prompt diversity, it does not investigate the optimal balance between prompt diversity and consistency, nor does it explore the impact of different prompt designs on the framework's performance.
- What evidence would resolve it: Conducting experiments with various prompt designs and analyzing their impact on the accuracy and consistency of LLM annotations, as well as the overall performance of the CoAnnotating framework.

## Limitations

- Entropy-based uncertainty estimation assumes consistent predictions indicate true capability, but may fail under systematic LLM bias
- Self-evaluation mechanism relies on potentially miscalibrated confidence scores from LLMs, as evidenced by the MRPC dataset
- Cost estimation uses fixed rates without accounting for variable human expertise levels or LLM API pricing fluctuations

## Confidence

- Entropy as reliable uncertainty proxy: Medium
- Self-evaluation inferiority to entropy: Medium
- Pareto efficiency framework utility: Medium

## Next Checks

1. Test entropy-based allocation on datasets where LLMs show systematic bias (e.g., sentiment analysis of specific demographic language) to verify the mechanism holds under bias conditions
2. Conduct ablation studies removing the multiple prompt variations to quantify the marginal benefit of entropy over single-prompt approaches
3. Evaluate the framework with dynamic cost parameters (varying human rates, LLM API costs) to assess robustness of Pareto-optimal recommendations