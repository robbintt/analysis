---
ver: rpa2
title: The Transient Nature of Emergent In-Context Learning in Transformers
arxiv_id: '2311.08360'
source_url: https://arxiv.org/abs/2311.08360
tags:
- learning
- training
- figure
- transience
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'In-context learning (ICL) in transformers is transient: it emerges
  early in training but then fades away as in-weights learning (IWL) takes over, even
  while training loss continues to decrease. This phenomenon was observed across different
  model sizes, dataset sizes, and data types, including both image-based and language
  model token embedding datasets.'
---

# The Transient Nature of Emergent In-Context Learning in Transformers

## Quick Facts
- arXiv ID: 2311.08360
- Source URL: https://arxiv.org/abs/2311.08360
- Reference count: 40
- In-context learning (ICL) in transformers is transient: it emerges early in training but then fades away as in-weights learning (IWL) takes over, even while training loss continues to decrease.

## Executive Summary
In-context learning (ICL) in transformers is a transient phenomenon where ICL emerges early during training but gradually fades as in-weights learning (IWL) takes over. This occurs across different model sizes, dataset sizes, and data types, including both image-based and language model token embedding datasets. The phenomenon is observed even while training loss continues to decrease, indicating that ICL and IWL represent different learning strategies that compete for resources within the transformer architecture. Increasing dataset complexity or applying L2 regularization can mitigate ICL transience by either delaying the onset of IWL or making ICL a lower norm solution that is favored by regularization.

## Method Summary
The paper studies ICL transience using synthetic datasets derived from Omniglot images and LLaMa token embeddings. Transformers with 12 layers, 64-dimensional embeddings, and 8 heads are trained on datasets with varying numbers of classes and exemplars per class. The model architecture includes an embedding layer, a ResNet encoder for images or a linear layer for token embeddings, and uses additive sinusoidal positional encoding. ICL and IWL evaluators are used to assess the model's reliance on context versus in-weights information throughout training. The experiments involve training for up to 5e7 steps while monitoring training loss and evaluator accuracies, with L2 regularization applied to test its effect on ICL persistence.

## Key Results
- ICL emerges early during training but gradually fades as IWL takes over, even as training loss continues to decrease
- Increasing the number of classes in the dataset can delay the onset of ICL transience and reduce its severity
- L2 regularization can eliminate ICL transience by making the ICL solution a lower norm solution than IWL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL transience occurs due to competition between ICL and IWL circuits for residual stream resources.
- Mechanism: When both ICL and IWL strategies can solve the training task, the transformer allocates resources to IWL circuits over ICL circuits, causing ICL to fade while IWL accuracy rises.
- Core assumption: ICL and IWL circuits use overlapping resources in the transformer's residual stream.
- Evidence anchors:
  - [abstract] "Initial evidence suggests that ICL transience is caused by competition between ICL and IWL circuits."
  - [section 6] "wider transformers with larger embedding size (and therefore greater residual stream capacity) to be less afflicted by ICL transience."
  - [corpus] No direct evidence found in corpus papers about residual stream resource competition.
- Break condition: If experiments show ICL persistence even in small models, or if ICL circuits are found to use disjoint resources from IWL circuits.

### Mechanism 2
- Claim: IWL is asymptotically preferred because it can achieve perfect accuracy with "imperfect matching" to prior context, while ICL relies on softer attention operations.
- Mechanism: Cross-entropy loss incentivizes solutions that minimize reliance on context (ICL) in favor of in-weights solutions (IWL) when both achieve the same accuracy.
- Core assumption: Cross-entropy loss penalizes the "softness" of attention-based matching in ICL more than in-weights solutions.
- Evidence anchors:
  - [section 7] "Possible explanations may concern optimization quirks...or perhaps lottery tickets contained in common initialization methods [28, 40] that incentivize ICL."
  - [corpus] No direct evidence found in corpus papers about cross-entropy loss preferences.
- Break condition: If experiments show ICL persistence when using alternative loss functions or if ICL circuits achieve the same accuracy without attention-based matching.

### Mechanism 3
- Claim: L2 regularization eliminates ICL transience by making the ICL solution a lower norm solution than the IWL solution.
- Mechanism: Weight decay penalizes the norm of weights, and if the ICL circuit has a lower norm than the IWL circuit, regularization favors ICL.
- Core assumption: The norm of weights in the ICL circuit is lower than in the IWL circuit.
- Evidence anchors:
  - [section 5] "we see that the decay slope of ICL performance goes to 0" with increasing L2 regularization.
  - [section 6] "We arrived at convergent evidence by building on the regularization experiments...When we applied weight decay to the MLP or ResNet layers...ICL transience was mitigated."
  - [corpus] No direct evidence found in corpus papers about norm differences between ICL and IWL circuits.
- Break condition: If experiments show ICL persistence with other regularization techniques or if the ICL circuit norm is found to be higher than the IWL circuit norm.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how transformers process sequences and how attention weights are computed is crucial for understanding ICL and IWL.
  - Quick check question: What is the difference between self-attention and cross-attention in a transformer?

- Concept: In-context learning (ICL) vs. in-weights learning (IWL)
  - Why needed here: The paper contrasts these two learning modes and their transience, so a clear understanding is essential.
  - Quick check question: How does a transformer perform ICL without weight updates?

- Concept: Regularization techniques (e.g., L2 regularization)
  - Why needed here: The paper explores how regularization can mitigate ICL transience by favoring lower norm solutions.
  - Quick check question: How does L2 regularization affect the optimization of neural network weights?

## Architecture Onboarding

- Component map: Embedding layer -> ResNet encoder (images) or Linear layer (token embeddings) -> 12 transformer layers (64-dim embedding, 8 heads) -> Output layer
- Critical path: Input exemplars and labels -> Embedding layer -> ResNet/Linear encoder -> 12 transformer layers with additive sinusoidal positional encoding -> Final prediction of query exemplar's label
- Design tradeoffs: Increasing the number of classes in the dataset can delay ICL transience but requires training for longer. L2 regularization can eliminate ICL transience but may lead to little-or-no IWL. Wider models (larger embedding sizes) have higher ICL peak heights and more gradual decay slopes but also enhance IWL abilities.
- Failure signatures: ICL transience is observed as a gradual decrease in ICL evaluator accuracy while training loss continues to decrease. This can occur across different model sizes, dataset sizes, and data types.
- First 3 experiments:
  1. Train a transformer on a synthetic dataset with 1,600 classes and 20 exemplars per class, then evaluate ICL and IWL performance over time to observe transience.
  2. Vary the number of classes in the dataset and observe the effect on ICL peak height and decay slope.
  3. Apply L2 regularization with different strengths and observe the effect on ICL transience and IWL performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific optimization quirks or initialization schemes could be responsible for the emergence of ICL in transformer networks?
- Basis in paper: [inferred] The paper suggests that optimization quirks or lottery tickets in common initialization methods might incentivize ICL emergence, but does not provide concrete evidence or identify specific mechanisms.
- Why unresolved: The paper acknowledges the possibility of optimization quirks or initialization schemes influencing ICL emergence but does not investigate this hypothesis experimentally or theoretically.
- What evidence would resolve it: Controlled experiments varying initialization schemes, optimizers, or learning rate schedules while monitoring ICL emergence and persistence could provide insights into the role of optimization in ICL dynamics.

### Open Question 2
- Question: Can ICL and IWL circuits co-exist asymptotically in the same transformer model, or is their interaction inherently competitive?
- Basis in paper: [explicit] The paper concludes that regularization may enable ICL persistence but leads to little or no IWL, suggesting that current methods do not allow for co-existence of both circuits.
- Why unresolved: The paper only explores regularization as a means to make ICL persistent and does not investigate other architectural or training modifications that might enable co-existence of ICL and IWL circuits.
- What evidence would resolve it: Experiments with alternative architectures, such as multi-task learning or specialized circuit allocation, could reveal whether ICL and IWL can co-exist in a single model without competition.

### Open Question 3
- Question: How does the competition between ICL and IWL circuits manifest at the mechanistic level within the transformer's residual stream?
- Basis in paper: [explicit] The paper presents initial evidence suggesting that ICL transience may be caused by competition between ICL and IWL circuits for resources in the transformer's residual stream, but does not provide a detailed mechanistic explanation.
- Why unresolved: The paper only provides preliminary evidence of competition between ICL and IWL circuits but does not explore the specific mechanisms or circuit-level interactions that lead to this competition.
- What evidence would resolve it: Detailed mechanistic interpretability analyses, such as circuit tracing or attention pattern analysis, could elucidate how ICL and IWL circuits interact and compete within the transformer's architecture.

## Limitations

- The mechanistic explanations for ICL transience (resource competition, loss preferences, weight norms) lack direct experimental evidence and remain speculative.
- The synthetic nature of the datasets (Omniglot-derived images and clustered LLaMa embeddings) may not capture the full complexity of real-world data, limiting generalizability.
- The evaluation methodology using ICL and IWL evaluators provides indirect measures that may not perfectly capture the underlying computational mechanisms of learning modes.

## Confidence

**High confidence**: The empirical observation that ICL transience occurs across different model sizes, dataset sizes, and data types. The effect of L2 regularization on eliminating transience is also well-established through controlled experiments.

**Medium confidence**: The claim that increasing dataset complexity (more classes) can delay or reduce ICL transience. While the paper shows this effect, the relationship appears to be more nuanced than a simple delay.

**Low confidence**: The mechanistic explanations for why ICL transience occurs. The resource competition hypothesis lacks direct evidence, and the cross-entropy loss preference hypothesis is the weakest, with only speculative connections to optimization dynamics.

## Next Checks

**Validation Check 1**: Direct measurement of residual stream resource usage by ICL and IWL circuits through ablation studies. This would involve selectively disabling attention heads or MLP layers suspected of implementing ICL versus IWL, then measuring the impact on both evaluators and training loss. Success would provide direct evidence for the resource competition hypothesis.

**Validation Check 2**: Testing alternative loss functions (e.g., mean squared error) to determine whether cross-entropy specifically drives the preference for IWL over ICL. If ICL persistence is observed with alternative losses while maintaining similar accuracy, this would support the hypothesis that cross-entropy loss penalizes the "softness" of attention-based matching.

**Validation Check 3**: Circuit tracing experiments to compare the actual weight norms and activation patterns of ICL versus IWL circuits during different training phases. This would involve analyzing the learned weights and attention patterns to quantify the norm differences and determine whether ICL circuits consistently have lower norm than IWL circuits, as hypothesized for the L2 regularization effect.