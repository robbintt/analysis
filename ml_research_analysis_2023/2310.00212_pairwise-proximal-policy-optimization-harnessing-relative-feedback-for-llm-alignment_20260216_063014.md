---
ver: rpa2
title: 'Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM
  Alignment'
arxiv_id: '2310.00212'
source_url: https://arxiv.org/abs/2310.00212
tags:
- reward
- policy
- arxiv
- gradient
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Pairwise Proximal Policy Optimization (P3O),
  a new policy gradient algorithm that operates directly on comparative rewards to
  address limitations of PPO when optimizing rewards trained from comparison-based
  losses. P3O avoids the need to learn a value function and uses Generalized Advantage
  Estimation (GAE) by working with trajectory-wise rewards instead of token-wise updates.
---

# Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment

## Quick Facts
- arXiv ID: 2310.00212
- Source URL: https://arxiv.org/abs/2310.00212
- Reference count: 38
- Key outcome: P3O achieves better KL-Reward trade-off than PPO and DPO, with 57% win rate against PPO and 69% win rate against SFT in GPT-4 evaluations

## Executive Summary
This paper introduces Pairwise Proximal Policy Optimization (P3O), a novel policy gradient algorithm designed to optimize large language models using comparative rewards. P3O operates directly on relative differences between paired trajectories, avoiding the instability issues of PPO when reward scales change. By using trajectory-wise rewards instead of token-wise updates, P3O eliminates the need for value function learning and Generalized Advantage Estimation. The algorithm demonstrates superior performance on KL-Reward trade-off metrics compared to PPO and Direct Preference Optimization (DPO) across summarization and question-answering tasks.

## Method Summary
P3O is a policy gradient algorithm that works with comparative rewards by generating two trajectories per prompt and computing their difference. The method uses trajectory-wise rewards instead of token-wise updates, which eliminates the need for value function learning and GAE. P3O employs clipping techniques (jointly in P3O-V2) to maintain stable updates while optimizing the policy. The algorithm is designed to be invariant to equivalent rewards, addressing a key limitation of PPO when dealing with reward models trained from preference data.

## Key Results
- On TL;DR summarization, P3O-V2 achieves nearly the same highest reward as DPO but with 25% lower KL-divergence
- On HH question-answering, P3O-V1 and P3O-V2 achieve strictly dominant KL-Reward frontiers compared to PPO and DPO across different model sizes
- GPT-4 evaluations show P3O has a 57% win rate against PPO and 69% win rate against SFT in terms of generating more helpful responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: P3O avoids instability from reward scaling by operating directly on relative differences between paired trajectories
- Mechanism: The algorithm uses (r(y1|x) - r(y2|x)) as the core gradient signal, so any constant shift in reward cancels out
- Core assumption: The reward model captures preferences via differences between pairs of responses
- Evidence anchors: [abstract] "P3O is invariant to equivalent rewards and avoids the complexity of PPO"; [section 4.4] "BTL is invariant under this equivalent relationship, while PPO is not"
- Break condition: If reward model is trained to output absolute values rather than relative preferences

### Mechanism 2
- Claim: P3O sidesteps the need for a value function and GAE by using trajectory-wise rewards instead of token-wise updates
- Mechanism: By generating two trajectories per prompt and computing rewards only at the trajectory level, P3O avoids sparse reward assignment to individual tokens
- Core assumption: Reward model is trained on full response pairs, not token-by-token comparisons
- Evidence anchors: [abstract] "P3O avoids the need to learn a value function and uses Generalized Advantage Estimation (GAE) by working with trajectory-wise rewards instead of token-wise updates"; [section 4.1] "Empirical evaluations show that P3O consistently outperforms PPO and DPO"
- Break condition: If reward model is extended to provide token-level feedback

### Mechanism 3
- Claim: Clipping applied jointly to both responses in P3O-V2 provides better KL-Reward trade-off than separate clipping in P3O-V1
- Mechanism: Joint clipping ensures the ratio πθ(y1|x)/πθ(y2|x) stays close to πθold(y1|x)/πθold(y2|x) within epsilon, maintaining stable updates for both responses simultaneously
- Core assumption: Stable updates require coordinated control of both responses' probability ratios
- Evidence anchors: [section 5.1] "P3O-V2 can reach a slightly higher reward than P3O-V1, while with a worse KL-Reward trade-off"; [section C.2] "The right figure compares P3O-V2 with and without the clipping technique"
- Break condition: If epsilon is set too large or too small

## Foundational Learning

- Concept: Reward equivalence and invariance
  - Why needed here: Understanding why PPO is unstable when reward scale changes is key to grasping P3O's design motivation
  - Quick check question: If r'(y|x) = r(y|x) + 5 for all x,y, would PPO and P3O produce the same policy updates?

- Concept: Contextual bandit vs MDP formulation for language generation
  - Why needed here: P3O operates in the contextual bandit setting, which affects how rewards are assigned and how the algorithm is structured
  - Quick check question: In the bandit formulation, what is the state transition function?

- Concept: Importance sampling in policy gradients
  - Why needed here: P3O uses importance sampling to incorporate old policy distributions when computing gradients from new trajectories
  - Quick check question: What happens to the importance weight if πθ(y|x) = πθold(y|x)?

## Architecture Onboarding

- Component map: Policy network → generates two responses per prompt → reward model scores both → P3O computes pairwise gradient → optimizer updates policy
- Critical path: Trajectory generation → pairwise reward computation → gradient estimation → policy update
- Design tradeoffs: Trajectory-wise rewards simplify computation but require two generations per prompt; joint clipping stabilizes updates but may reduce exploration
- Failure signatures: KL-divergence spikes indicate clipping is too restrictive; reward plateau suggests learning rate is too low
- First 3 experiments:
  1. Compare KL-Reward frontiers with and without clipping at different epsilon values
  2. Test invariance by adding constant offsets to rewards and checking policy updates
  3. Benchmark against PPO with token-wise updates on same reward model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the clipping technique in P3O impact performance across different learning rates and reward scales?
- Basis in paper: [explicit] The paper mentions that clipping is more effective with larger learning rates and that it improves the KL-Reward frontier, particularly in early training stages, but comes at a slight cost to asymptotic reward
- Why unresolved: The paper provides initial evidence but does not conduct a comprehensive ablation study across various reward scales or model sizes to quantify the trade-off
- What evidence would resolve it: A systematic ablation study varying learning rates, reward scales, and model sizes, with quantitative analysis of KL-Reward frontiers and reward maximization, would clarify the optimal clipping strategy

### Open Question 2
- Question: Can P3O be extended to handle more than two ranked responses in preference data?
- Basis in paper: [inferred] The paper focuses on pairwise comparisons but mentions interest in extending the policy gradient algorithm to accommodate more than two ranked responses, suggesting potential for generalization
- Why unresolved: The current formulation of P3O is designed for pairwise comparisons, and the paper does not explore extensions to k-wise preferences or their impact on alignment quality
- What evidence would resolve it: Experiments comparing P3O variants that handle k-wise preferences against pairwise P3O on the same tasks, measuring KL-Reward trade-offs and human preference alignment, would demonstrate feasibility and benefits

### Open Question 3
- Question: How does P3O compare to PPO and DPO on tasks beyond summarization and question-answering, such as code generation or creative writing?
- Basis in paper: [explicit] The paper evaluates P3O on summarization and question-answering tasks, showing superior KL-Reward trade-offs and human preference alignment, but does not test other domains
- Why unresolved: The paper's experiments are limited to two specific tasks, leaving uncertainty about P3O's generalizability to other open-ended generation tasks with different reward structures
- What evidence would resolve it: Applying P3O to diverse tasks like code generation, creative writing, or math problem-solving, and comparing its performance to PPO and DPO using domain-specific reward models and human evaluations, would clarify its broader applicability

## Limitations
- Reward Model Dependency: P3O's effectiveness is tightly coupled to the quality of the pairwise reward model
- Scalability Concerns: Generating two complete trajectories per prompt doubles computational cost
- Limited Scope of Evaluation: Experiments focus only on summarization and question-answering tasks

## Confidence

**High Confidence** (supported by theoretical analysis and experimental evidence):
- P3O's invariance to reward scaling
- The elimination of value function learning requirement
- Superior KL-Reward trade-off compared to PPO and DPO baselines

**Medium Confidence** (supported by experiments but with limitations):
- The effectiveness of joint clipping in P3O-V2
- GPT-4 evaluation results showing win rates against baselines
- Performance on TL;DR and HH datasets specifically

**Low Confidence** (primarily theoretical or with limited empirical support):
- Performance guarantees when reward models are imperfect
- Scalability to much longer sequences or larger models
- Generalization to tasks beyond summarization and question-answering

## Next Checks

1. **Reward Model Robustness Test**: Systematically degrade the quality of the reward model (add noise, introduce biases) and measure how P3O's performance degrades compared to PPO and DPO. This would validate the claim about reward model dependency and establish failure modes.

2. **Cross-Domain Generalization**: Apply P3O to at least three additional domains (e.g., code generation, mathematical reasoning, creative writing) with different reward models and measure KL-Reward trade-offs. This would test the generalizability claims beyond the two evaluated tasks.

3. **Scaling Analysis**: Conduct experiments across a wider range of model sizes (e.g., 1B, 7B, 70B parameters) and sequence lengths to establish how computational costs and performance scale. This would validate or challenge the scalability assumptions implicit in the current evaluation.