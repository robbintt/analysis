---
ver: rpa2
title: 'FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable
  AI Methods'
arxiv_id: '2308.06248'
source_url: https://arxiv.org/abs/2308.06248
tags:
- methods
- parts
- explanation
- part
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic dataset and evaluation framework
  for analyzing explainable AI methods. The dataset, named FunnyBirds, consists of
  50,500 images of 50 synthetic bird species with pixel-accurate part annotations.
---

# FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods

## Quick Facts
- **arXiv ID:** 2308.06248
- **Source URL:** https://arxiv.org/abs/2308.06248
- **Reference count:** 40
- **Primary result:** Synthetic dataset with 50,500 images of 50 bird species with pixel-accurate part annotations enables systematic evaluation of XAI methods across multiple explainability dimensions

## Executive Summary
This paper introduces FunnyBirds, a synthetic dataset and evaluation framework for analyzing explainable AI (XAI) methods in computer vision. The dataset consists of 50,500 images of 50 synthetic bird species with pixel-accurate part annotations, enabling part-level analysis closer to human comprehension than pixel-level evaluations. The framework uses semantically meaningful image interventions (removing individual object parts) to estimate ground-truth part importances and evaluate XAI methods across dimensions like completeness, correctness, and contrastivity. The authors evaluate 24 different combinations of neural models and XAI methods, revealing insights about method performance and limitations.

## Method Summary
The FunnyBirds dataset contains 50,500 synthetic bird images across 50 classes, with each class having 500 test images. Images are generated with pixel-accurate annotations for parts (beak, wings, feet, eyes, tail). The evaluation framework uses part interventions where individual parts are removed from training images, and these modified images are included in training to avoid domain shift. Interface functions (PI for part importance scores and P for important parts) abstract away from specific explanation formats, enabling unified evaluation of diverse methods including attribution maps, prototypes, and attention-based methods. The framework assesses 24 model-XAI combinations across multiple dimensions: accuracy, background independence, completeness, correctness, and contrastivity.

## Key Results
- The synthetic dataset enables ground-truth estimation of part importances through part interventions
- Part-level evaluation reveals insights about XAI method performance that pixel-level evaluation misses
- Negative gradient information proves crucial for certain explanation methods
- The framework provides systematic assessment revealing strengths and weaknesses of different XAI approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Part-based interventions allow estimation of ground-truth part importances that reflect the causal structure of the model.
- **Mechanism:** By removing individual parts from training images and including these modified images in the training set, the model learns to handle part-removed inputs as in-domain. Comparing the model's output with and without a part reveals the part's importance.
- **Core assumption:** The difference in model output when removing a part is a valid proxy for that part's importance, assuming the model processes parts individually or that this simplification approximates the true causal structure.
- **Evidence anchors:**
  - [abstract]: "Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations."
  - [section]: "Since the perfect causal structure (which is too complex to be comprehended) is already given by the model itself, we emphasize that a simplified approximation is needed for XAI, instead."
- **Break condition:** If the model does not process parts independently, or if removing a part causes complex, non-linear effects that cannot be captured by simple output differences, the estimated importances may not reflect true causal importance.

### Mechanism 2
- **Claim:** Interface functions enable comparison of diverse explanation types within a unified evaluation framework.
- **Mechanism:** Interface functions like PI(·) (part importance scores) and P(·) (important parts) abstract away from specific explanation formats (e.g., attribution maps, prototypes) and allow evaluation protocols to be applied consistently across methods.
- **Core assumption:** Different explanation types can be meaningfully mapped to common interface functions without losing essential information about their explanatory power.
- **Evidence anchors:**
  - [section]: "We thus propose to build our evaluation framework around so-called interface functions that implement more general properties, such as which object parts are important."
  - [section]: "This allows us to not only compare a spectrum of existing methods, but also to potentially include future, novel explanation types."
- **Break condition:** If an explanation type cannot be meaningfully mapped to the interface functions (e.g., if it communicates information not captured by part importance), the comparison becomes invalid.

### Mechanism 3
- **Claim:** Synthetic data generation with full control allows creation of semantically meaningful interventions, avoiding domain shift issues present in pixel-level evaluations.
- **Mechanism:** By designing a dataset where parts can be removed in a semantically valid way and including these modified images in training, the model is exposed to part-removed inputs during training, making evaluations on these inputs valid.
- **Core assumption:** Including images with removed parts in the training set ensures that the model treats these images as in-domain, and that the interventions are semantically meaningful.
- **Evidence anchors:**
  - [abstract]: "To ensure that images with removed parts are not out-of-distribution, we include images with missing parts in the training set."
  - [section]: "By performing unrealistic interventions in image space, e.g., masking out pixels, they introduce domain shifts compared to the training distribution."
- **Break condition:** If the model still behaves unexpectedly on part-removed images despite training inclusion, or if the part removals are not truly semantically valid, the evaluation becomes unreliable.

## Foundational Learning

- **Concept:** Synthetic data generation for controlled experiments
  - Why needed here: Allows full control over variables, eliminates irrelevant factors, and provides clearer evidence of observed behavior in XAI evaluation.
  - Quick check question: How does including images with removed parts in training help avoid domain shift in evaluations?

- **Concept:** Interface functions for unifying diverse explanation types
  - Why needed here: Different explanation methods (e.g., attribution maps, prototypes) have different formats; interface functions allow them to be compared within a single framework.
  - Quick check question: What is the role of the P(·) interface function in the FunnyBirds framework?

- **Concept:** Part-level vs. pixel-level evaluation
  - Why needed here: Humans perceive images in concepts rather than pixels; part-level evaluation is closer to human comprehension than pixel-level evaluation.
  - Quick check question: Why is evaluating explanations on a part level considered more aligned with downstream tasks of human comprehension?

## Architecture Onboarding

- **Component map:** Synthetic dataset generation → Model training with part interventions → Interface function extraction → Evaluation protocol application → Result aggregation and analysis
- **Critical path:** Dataset generation and model training must be completed before any interface function extraction or evaluation can occur.
- **Design tradeoffs:** Using synthetic data provides control but may not reflect all real-world challenges; part-level evaluation is more human-aligned but may miss pixel-level nuances.
- **Failure signatures:** Low accuracy on part-removed images suggests model doesn't handle interventions well; poor interface function mapping indicates explanation types may not be comparable.
- **First 3 experiments:**
  1. Train a simple CNN on FunnyBirds with part interventions included; verify accuracy on part-removed test images.
  2. Extract interface functions (PI, P) from a trained attribution map method; apply CSDC protocol to check completeness.
  3. Compare SD protocol scores across different XAI methods on the same backbone to assess correctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed framework adequately address all dimensions of explainability, or are there significant aspects missing?
- Basis in paper: [explicit] The paper discusses the dimensions of completeness, correctness, and contrastivity, but acknowledges that there are additional dimensions proposed by Nauta et al. [38] that are not considered, such as consistency, continuity, compactness, covariate complexity, composition, confidence, context, and controllability.
- Why unresolved: The paper does not provide a detailed analysis of why these omitted dimensions are less important or how they could be incorporated into the framework.
- What evidence would resolve it: A comprehensive evaluation of the proposed framework's performance on the omitted dimensions, or a clear justification for why they are not relevant to the proposed analysis.

### Open Question 2
- Question: How do the results of the proposed framework translate to real-world datasets and applications?
- Basis in paper: [inferred] The paper acknowledges that the synthetic dataset does not reflect all real-world challenges and is not fully representative of natural images. However, it argues that if XAI methods can truthfully explain models trained on the controlled dataset, the proposed method is a valuable tool for assessing the quality of XAI methods.
- Why unresolved: The paper does not provide empirical evidence to support the claim that the results translate well to real-world datasets.
- What evidence would resolve it: A study comparing the performance of XAI methods on the proposed framework and real-world datasets, or a demonstration of the framework's effectiveness in a real-world application.

### Open Question 3
- Question: How do the results of the proposed framework vary with different architectural choices and hyperparameters?
- Basis in paper: [inferred] The paper evaluates 24 different combinations of neural models and XAI methods, but it does not explore the impact of different architectural choices and hyperparameters within each model or method.
- Why unresolved: The paper does not provide a systematic analysis of how the results are affected by different architectural choices and hyperparameters.
- What evidence would resolve it: A study examining the sensitivity of the results to different architectural choices and hyperparameters, or a recommendation for optimal settings for each model and method.

## Limitations
- Synthetic data may not capture all real-world complexities and domain shifts
- Part independence assumption may not hold for objects with strongly interacting parts
- Interface function mapping requires method-specific thresholds that may introduce bias

## Confidence
- **Medium**: Synthetic nature provides precise ground truth but may not reflect real-world data distributions; part independence assumption may break down for complex objects; interface abstraction requires careful implementation

## Next Checks
1. Test the framework on a real-world dataset with part annotations to assess generalizability beyond synthetic data
2. Evaluate how sensitive results are to the threshold selection method for the P(·) interface function across different explanation types
3. Assess whether the part independence assumption affects evaluation reliability by testing on objects where parts have strong interactions