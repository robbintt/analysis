---
ver: rpa2
title: Practical Path-based Bayesian Optimization
arxiv_id: '2312.00622'
source_url: https://arxiv.org/abs/2312.00622
tags:
- snake
- optimization
- cost
- eipu
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper extends the SnAKe path-based Bayesian optimization
  algorithm to handle practical constraints in experimental design. The authors introduce
  three key modifications: a self-stopping mechanism using Expected Improvement per
  unit cost (EIpu) to terminate optimization when improvements are marginal; a truncated
  version to enforce maximum allowable input changes between experiments, preventing
  unsafe or infeasible parameter jumps; and a multi-objective extension using random
  scalarizations to find Pareto fronts.'
---

# Practical Path-based Bayesian Optimization

## Quick Facts
- arXiv ID: 2312.00622
- Source URL: https://arxiv.org/abs/2312.00622
- Reference count: 16
- Extends SnAKe algorithm with self-stopping, movement constraints, and multi-objective optimization for practical experimental design

## Executive Summary
This paper extends the SnAKe path-based Bayesian optimization algorithm to handle practical constraints encountered in experimental design. The authors introduce three key modifications: a self-stopping mechanism using Expected Improvement per unit cost (EIpu) to terminate optimization when improvements are marginal; a truncated version to enforce maximum allowable input changes between experiments, preventing unsafe or infeasible parameter jumps; and a multi-objective extension using random scalarizations to find Pareto fronts. Experiments on synthetic benchmarks (Branin2D, Hartmann3D/6D) and a real chemistry problem (SnAr) demonstrate that these extensions make path-based Bayesian optimization more applicable to real-world experimental scenarios where costs, constraints, and multiple objectives must be balanced.

## Method Summary
The paper modifies the SnAKe algorithm through three extensions. First, self-stopping uses EIpu to terminate optimization when marginal improvements are insufficient, comparing against a threshold δ with variance-based safety criteria. Second, truncated SnAKe enforces maximum input changes (δMAX) between experiments by truncating proposed points that exceed this distance. Third, multi-objective SnAKe uses random scalarizations (linear and Tchebyshev) to approximate Pareto fronts by sampling weight vectors λ and creating scalarized objectives. The implementations use Gaussian Process surrogate models with Thompson sampling for batch generation and TSP solvers for path ordering.

## Key Results
- Self-stopping SnAKe achieves similar performance to full-budget SnAKe with fewer experiments
- Truncated SnAKe handles movement constraints effectively while maintaining optimization quality
- Multi-objective SnAKe finds Pareto-optimal solutions at significantly lower cost than standard approaches
- All extensions successfully applied to synthetic benchmarks and real chemistry problem (SnAr)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-stopping mechanism uses Expected Improvement per unit cost (EIpu) to terminate optimization when marginal improvements are insufficient.
- Mechanism: By comparing EIpu against a threshold δ and adding a variance-based safety criterion, the algorithm terminates early when further experiments would be too costly relative to expected gains.
- Core assumption: The model uncertainty decreases predictably as more observations are added, and EIpu reliably reflects the cost-benefit tradeoff.
- Evidence anchors:
  - [abstract] "a self-stopping mechanism using Expected Improvement per unit cost (EIpu) to terminate optimization when improvements are marginal"
  - [section] "we modify the Point Deletion criteria... using EIpu... if the EIpu, EI(x)/C(0)(x), is less than a given threshold δ > 0, then we delete it"
  - [corpus] Weak evidence - no direct citations found in neighbor papers
- Break condition: Model misspecification could cause premature termination if uncertainty estimates are unreliable.

### Mechanism 2
- Claim: The truncated version enforces maximum allowable input changes between experiments to prevent unsafe or infeasible parameter jumps.
- Mechanism: When proposed experiment xt+1 exceeds δMAX distance from current point xt, it is truncated to lie exactly δMAX away in the direction of xt+1.
- Core assumption: The optimal path doesn't require large jumps that violate the constraint, and truncation doesn't significantly degrade optimization performance.
- Evidence anchors:
  - [abstract] "a truncated version to enforce maximum allowable input changes between experiments, preventing unsafe or infeasible parameter jumps"
  - [section] "truncate SnAKe's input change whenever it exceeds δMAX" and "if |xt+1 − xt| > δ MAX then xt+1 ← xt + [xt+1 − xt](u)δMAX"
  - [corpus] No direct evidence in neighbor papers
- Break condition: If the true optimum requires movement larger than δMAX, the algorithm may converge to suboptimal solutions.

### Mechanism 3
- Claim: Multi-objective extension uses random scalarizations to find Pareto fronts at lower cost than standard approaches.
- Mechanism: By sampling random weight vectors λ from p(λ) and creating scalarized objectives, the algorithm generates diverse candidate points that approximate the Pareto front.
- Core assumption: Random scalarizations adequately explore the Pareto front without requiring expensive multi-objective acquisition functions.
- Evidence anchors:
  - [abstract] "a multi-objective extension using random scalarizations to find Pareto fronts"
  - [section] "we consider a simple multi-objective extension of SnAKe by using random scalarizations of the objective functions" and "To create the batch, we simply sample both the independent GPs and λ ∼ p(λ)"
  - [corpus] Weak evidence - only one neighbor paper mentions multi-objective BO
- Break condition: Random sampling may miss parts of non-convex Pareto fronts if scalarization space is not well-sampled.

## Foundational Learning

- Concept: Bayesian Optimization fundamentals (GP surrogate models, acquisition functions)
  - Why needed here: The paper builds directly on BO principles, extending SnAKe which itself is a BO variant
  - Quick check question: What is the role of the acquisition function in Bayesian optimization?

- Concept: Path-based optimization and movement costs
  - Why needed here: The core innovation involves minimizing cumulative input changes between experiments
  - Quick check question: How does incorporating movement costs change the optimization objective?

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: The extension handles problems with multiple competing objectives
  - Quick check question: What distinguishes a Pareto optimal solution from a single-objective optimum?

## Architecture Onboarding

- Component map: Thompson Sampling -> TSP solver -> GP model -> EIpu acquisition -> Point deletion -> Experiment execution
- Critical path: Batch generation → Path optimization → Experiment execution → Model update → Termination check
- Design tradeoffs:
  - Batch size vs computational cost (larger batches improve path optimization but increase overhead)
  - EIpu threshold vs premature termination (lower thresholds save experiments but risk stopping too early)
  - Constraint tightness vs solution quality (tighter constraints improve safety but may degrade performance)
- Failure signatures:
  - Early termination despite good EIpu values indicates model uncertainty safety criteria are too strict
  - Poor performance with constraints suggests δMAX is too small relative to the optimization landscape
  - Missing Pareto front regions indicates insufficient random scalarization sampling
- First 3 experiments:
  1. Test self-stopping on Branin2D with varying EIpu thresholds to find optimal balance
  2. Validate truncation on a simple 1D problem with known constraint to ensure correctness
  3. Compare multi-objective performance against random search on a synthetic MOO benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal stopping criterion for self-stopping SnAKe that balances exploration and exploitation while avoiding early termination due to model misspecification?
- Basis in paper: [explicit] The paper discusses using Expected Improvement per unit cost (EIpu) with a variance threshold to prevent early termination, but notes that vanilla EIpu stops too early due to model misspecification.
- Why unresolved: The paper shows that adding a variance threshold improves performance, but doesn't determine the optimal threshold or explore alternative stopping criteria that might work better in different scenarios.
- What evidence would resolve it: Comparative experiments testing different stopping thresholds, alternative stopping criteria (e.g., entropy-based methods), and their performance across various benchmark functions and real-world applications.

### Open Question 2
- Question: How can path-based Bayesian optimization algorithms like SnAKe be extended to handle high-dimensional problems effectively?
- Basis in paper: [explicit] The paper mentions that SnAKe has poor performance in high-dimensional settings and references MONGOOSE as addressing this through meta-learning.
- Why unresolved: While the paper acknowledges this limitation and references related work, it doesn't explore specific modifications to SnAke itself for high-dimensional problems or compare different approaches to this challenge.
- What evidence would resolve it: Implementation and comparison of different dimensionality reduction techniques, feature selection methods, or alternative path-planning strategies specifically adapted for SnAke in high-dimensional spaces.

### Open Question 3
- Question: What are the most effective multi-objective scalarization methods for path-based Bayesian optimization, and how do they compare in terms of Pareto front quality and computational efficiency?
- Basis in paper: [explicit] The paper implements random scalarizations (linear and Tchebyshev) for multi-objective SnAKe but notes that "we choose this particular multi-objective method for its computational simplicity."
- Why unresolved: The paper doesn't compare different scalarization methods or explore non-scalarization approaches (e.g., Pareto-aware acquisition functions) for multi-objective path-based optimization.
- What evidence would resolve it: Systematic comparison of different scalarization methods, non-scalarization approaches, and their trade-offs in terms of Pareto front quality metrics (GD, IGD, MPFE) and computational cost across various benchmark problems.

## Limitations

- Self-stopping mechanism relies on accurate GP uncertainty estimates that may degrade in high-noise regimes
- Truncation approach assumes optimization landscape allows progress within δMAX constraint, potentially missing optimal regions
- Random scalarizations may struggle with non-convex Pareto fronts where linear scalarizations fail to adequately explore objective space

## Confidence

- Self-stopping mechanism: Medium - extends well-known concepts with limited theoretical guarantees in BO context
- Truncation approach: Medium - simple modification with clear intuition but no formal performance bounds
- Multi-objective extension: Low - relies on random sampling without convergence guarantees for complex Pareto geometries
- Core mechanisms: High - based on established BO theory and clear empirical demonstrations

## Next Checks

1. Test self-stopping sensitivity to EIpu threshold δ across problems with varying noise levels to establish robust parameter ranges.

2. Validate truncation performance on problems where optimal solutions require movements exceeding δMAX to quantify the performance degradation.

3. Compare random scalarization coverage against more sophisticated multi-objective BO methods (e.g., ParEGO, SMSego) on benchmark problems with known non-convex Pareto fronts.