---
ver: rpa2
title: Understanding Tool Discovery and Tool Innovation Using Active Inference
arxiv_id: '2311.03893'
source_url: https://arxiv.org/abs/2311.03893
tags:
- tool
- agent
- reward
- information
- room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors formalize the distinction between tool discovery and
  tool innovation within the active inference framework by introducing tool affordances
  into the hidden states of the generative model. This allows the agent to not only
  discover tools through exploration but also to invent them via offline induction
  of appropriate tool properties.
---

# Understanding Tool Discovery and Tool Innovation Using Active Inference

## Quick Facts
- arXiv ID: 2311.03893
- Source URL: https://arxiv.org/abs/2311.03893
- Reference count: 25
- Key outcome: Agent learns individual tool affordances (x-reach, y-reach) and composes them to invent new tools for novel tasks, demonstrating one-shot generalization

## Executive Summary
This paper formalizes the distinction between tool discovery and tool innovation within the active inference framework by introducing tool affordances into the hidden states of the generative model. The authors demonstrate that an agent equipped with an affordance model can not only discover tools through exploration but also invent them via offline induction of appropriate tool properties. Through simulations in discrete state space, the affordance model enables the agent to solve novel tool-invention tasks more efficiently than models lacking this factorization.

## Method Summary
The authors implement a discrete-state-space active inference agent using partially-observable Markov decision processes (POMDPs). The agent learns to solve tool-invention tasks in a 2x4 grid environment through three experiments: (1) perfect generative model with known tool use, (2) learned transition model for tool discovery, and (3) factorized generative model with tool affordances for tool innovation. The agent uses expected free energy minimization to balance utility (achieving preferred observations) and information gain (learning about state transitions), with parameter updates using Dirichlet distributions.

## Key Results
- Agent successfully learns individual tools (V and H) and their affordances through exploration
- Affordance factorization enables one-shot generalization to create compound tool (HV) for novel tasks
- Affordance model solves tool-invention tasks more efficiently than non-affordance models, requiring fewer discovery steps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The agent's ability to invent tools (rather than just discover them) is enabled by factorizing the hidden states of the generative model into tool affordances (x-reach and y-reach), which allows the agent to compose relevant affordances offline and solve novel tasks in one shot.
- **Mechanism**: By representing tools as combinations of independent affordance factors (horizontal reach and vertical reach), the agent can generalize from known tools (V and H) to create a new tool (HV) for a task requiring both affordances, without needing to explore the compound tool during learning.
- **Core assumption**: The affordance states are independent and disentangled, meaning the agent can learn about x-reach and y-reach separately and then combine them as needed.
- **Evidence anchors**:
  - [abstract]: "This particular state factorisation facilitates the ability to not just discover tools but invent them through the offline induction of an appropriate tool property."
  - [section]: "In order to achieve this, the agent must be able to analyse the problem and identify the kind of the tool required to solve the task. This entails developing a grounded understanding of the objects in the environment which can then be leveraged to construct a suitable tool through a process of generalisation."
- **Break condition**: If the affordance factors are not truly independent or disentangled, the agent may not be able to compose them effectively, and tool invention may fail or require additional exploration.

### Mechanism 2
- **Claim**: The agent learns only the tools it needs to solve the current task, leveraging the trade-off between utility (exploiting known tools) and information gain (exploring new tools) in the expected free energy.
- **Mechanism**: The expected free energy (G) is decomposed into a utility term (favoring policies that lead to preferred observations, like the reward) and an information gain term (favoring policies that reduce uncertainty about state transitions). This allows the agent to explore until it learns the necessary tools, then exploit that knowledge to solve the task efficiently.
- **Core assumption**: The agent maintains a probabilistic generative model that includes beliefs about state transitions, and updates these beliefs through Bayesian inference as it interacts with the environment.
- **Evidence anchors**:
  - [section]: "Given that the minimisation of G naturally incorporates two competing imperatives (utility and information gain), this means that the agent learns only the tools that it needs to learn in order to solve the task, and does not continue exploring its environment if it is able to leverage its current knowledge to effectively realise prior preferences."
- **Break condition**: If the prior preferences are not well-defined or if the information gain term is not properly balanced with the utility term, the agent may either over-explore (never settling on a solution) or under-explore (failing to learn necessary tools).

### Mechanism 3
- **Claim**: The agent's ability to solve novel tool-invention tasks more efficiently than non-affordance models is due to the factorization of hidden states into affordances, which enables one-shot generalization.
- **Mechanism**: When presented with a new task (e.g., a reward location requiring both horizontal and vertical reach), the affordance model agent can immediately compose the necessary tool (HV) from its existing knowledge of V and H, while the non-affordance model agent must explore to discover the compound tool.
- **Core assumption**: The agent has already learned the individual tools (V and H) and their respective affordances before being presented with the novel task.
- **Evidence anchors**:
  - [abstract]: "This is in contrast to a non-affordance model that requires additional discovery steps."
- **Break condition**: If the agent has not yet learned the individual tools and their affordances, or if the task requires a more complex composition of affordances than the model can handle, the one-shot generalization may fail.

## Foundational Learning

- **Concept**: Active Inference (AIF)
  - **Why needed here**: The paper uses AIF as the underlying framework for modeling tool discovery and innovation, where agents act to minimize expected free energy by balancing utility and information gain.
  - **Quick check question**: In AIF, what is the relationship between the expected free energy (G), utility, and information gain, and how does this trade-off drive agent behavior?

- **Concept**: Affordances (from ecological psychology)
  - **Why needed here**: The paper introduces the concept of tool affordances (x-reach and y-reach) as a way to factor the hidden states of the generative model, enabling the agent to generalize and invent new tools.
  - **Quick check question**: How does factorizing tool states into independent affordances (like x-reach and y-reach) enable the agent to compose new tools for novel tasks, and why is this different from simply discovering tools?

- **Concept**: Partially Observable Markov Decision Processes (POMDPs)
  - **Why needed here**: The paper implements the AIF framework using POMDPs in discrete state space, which provides the mathematical structure for modeling the agent's beliefs about the environment and its actions.
  - **Quick check question**: In the context of this paper, how are the observation likelihood, transition likelihood, and prior preference distributions (A, B, and C matrices) used to define the agent's generative model, and how do they relate to tool discovery and innovation?

## Architecture Onboarding

- **Component map**: Generative model (A, B, C matrices) -> Hidden states (room, tool, affordance factors) -> Policies (sequences of control states) -> Expected free energy (G) calculation -> Policy selection (sampling from G distribution) -> Parameter inference (Dirichlet updates) -> Tool affordance factorization

- **Critical path**:
  1. Initialize agent with generative model (A, B, C matrices)
  2. Agent observes environment and infers hidden states
  3. Agent evaluates expected free energy for all policies
  4. Agent selects policy (action sequence) based on G
  5. Agent executes actions and updates generative model parameters
  6. Agent learns state transitions and tool affordances
  7. Agent uses affordance factorization to invent new tools for novel tasks

- **Design tradeoffs**:
  - Affordance factorization vs. unfactorized tool states: Affordance factorization enables one-shot generalization but requires a more complex generative model.
  - Utility vs. information gain: Balancing exploration (learning new tools) and exploitation (using known tools) is crucial for efficient task-solving.
  - Model complexity: Adding affordance factors increases the state space and computational complexity but enables more flexible tool invention.

- **Failure signatures**:
  - Agent fails to invent new tools: May indicate insufficient learning of individual tool affordances or incorrect factorization.
  - Agent over-explores or under-explores: May indicate improper balance between utility and information gain in the expected free energy.
  - Agent does not generalize to novel tasks: May indicate incorrect or incomplete affordance representation.

- **First 3 experiments**:
  1. Implement a basic AIF agent with unfactorized tool states and test its ability to discover and use tools for simple tasks.
  2. Extend the agent with affordance factorization and test its ability to invent new tools for novel tasks, comparing performance to the unfactorized model.
  3. Vary the balance between utility and information gain in the expected free energy and observe the effects on tool learning and invention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the active inference framework account for the sensorimotor challenges associated with both tool manipulation and tool construction in tool innovation?
- Basis in paper: [inferred] The paper acknowledges that the current model is limited because it intentionally omits the sensorimotor challenges associated with tool manipulation and construction, and suggests that future work should construct models that can handle more physically realistic tasks.
- Why unresolved: The paper does not provide a detailed account of how the active inference framework would address these sensorimotor challenges, leaving this as a gap in the current model.
- What evidence would resolve it: Development and testing of an active inference model that incorporates realistic sensorimotor dynamics for tool manipulation and construction, demonstrating its ability to handle these challenges in a way that aligns with the framework's principles.

### Open Question 2
- Question: How can autonomous systems be designed to choose what to learn from the environment and factorise their model in a way that best explains the latent causes of sensory observations, as opposed to the intelligent designer approach used in the paper?
- Basis in paper: [explicit] The paper discusses the limitation of playing the role of an intelligent designer in factorizing the hidden states into affordances and suggests that ideally, autonomous systems should be able to choose what to learn and factorise their model accordingly.
- Why unresolved: The paper does not provide a solution or methodology for how autonomous systems can achieve this adaptive structure learning, leaving it as a future research direction.
- What evidence would resolve it: Creation of an autonomous system that can dynamically adapt its generative model by learning from the environment, effectively choosing and factorising its hidden states to explain sensory observations, and demonstrating this capability in tool innovation tasks.

### Open Question 3
- Question: Why does the utility component of the expected free energy not override the high information gain component in the northwest room task, leading to sub-optimal behavior in the affordance model?
- Basis in paper: [explicit] The paper notes that in the northwest room task, the affordance model agent seeks to investigate a specific state it has never seen before, driven by high information gain, even though it already has the knowledge of the correct tool and state transitions needed to solve the task optimally.
- Why unresolved: The paper does not provide a clear explanation for why the utility component is insufficient to override the information gain component in this scenario, suggesting a need for further investigation into the balance between these components in policy selection.
- What evidence would resolve it: Experimental results showing how adjusting the balance between utility and information gain components affects the agent's ability to select optimal policies, particularly in tasks where the agent has sufficient prior knowledge to solve the task without further exploration.

## Limitations
- The model assumes linear composability of affordances, which may not generalize to complex tool interactions
- Discrete state space simplification limits ecological validity and real-world applicability
- No validation on continuous or high-dimensional environments

## Confidence
- Tool discovery mechanism (Mechanism 2): **High** - Well-established active inference principles
- Affordance factorization enabling innovation (Mechanism 1): **Medium** - Theoretically sound but requires specific parameter tuning
- One-shot generalization advantage (Mechanism 3): **Medium** - Demonstrated in simulation but limited scope

## Next Checks
1. Implement sensitivity analysis for Dirichlet learning rates to identify robust parameter ranges
2. Test affordance factorization on a continuous state space environment to verify scalability
3. Evaluate model performance when affordances are not perfectly disentangled to test robustness