---
ver: rpa2
title: Using Experience Classification for Training Non-Markovian Tasks
arxiv_id: '2310.11678'
source_url: https://arxiv.org/abs/2310.11678
tags:
- state
- reward
- then
- tasks
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes an RL approach to train non-Markovian tasks
  specified by LTLf. The key contributions are: 1) An effective encoding method to
  transform non-Markovian tasks into Markovian tasks with linear increase in training
  space.'
---

# Using Experience Classification for Training Non-Markovian Tasks

## Quick Facts
- arXiv ID: 2310.11678
- Source URL: https://arxiv.org/abs/2310.11678
- Reference count: 7
- This paper proposes an RL approach to train non-Markovian tasks specified by LTLf with up to 499.53% improvement in policy quality and 206.34% improvement in training speed on the Cartpole problem.

## Executive Summary
This paper addresses the challenge of training reinforcement learning agents on non-Markovian tasks specified by Linear Temporal Logic over Finite Traces (LTLf). The authors propose an approach that encodes non-Markovian rewards into Markovian MDPs with only linear increase in state space complexity. They introduce an experience classification technique based on Deterministic Finite Automata (DFA) state rankings to improve training speed and policy quality. The method is evaluated on two benchmark problems (Waterworld and Cartpole) and demonstrates significant improvements over baseline methods.

## Method Summary
The method transforms non-Markovian LTLf tasks into Markovian MDPs by encoding the DFA state as an additional state variable, resulting in linear state space growth. Experience classification ranks DFA states by expected path length to accepting states, organizing experiences into separate replay buffers by rank. Prioritized experience replay samples from these buffers with probabilities adjusted by state rank, while reward shaping provides denser intermediate rewards using potential-based functions. The approach is implemented using TD3 and SAC algorithms from Stable-Baselines3.

## Key Results
- Achieves up to 499.53% improvement in policy quality on the Cartpole problem
- Demonstrates 206.34% improvement in training speed compared to baseline methods
- Shows consistent performance improvements across multiple benchmark tasks (Waterworld tasks 1-6 and Cartpole tasks 1-4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The encoding method transforms non-Markovian rewards into Markovian MDPs with only linear increase in state space.
- Mechanism: By introducing a single state variable fQ to track the current DFA state, the total state space becomes |Q| × |S| instead of the exponential blowup from using one variable per DFA state.
- Core assumption: The DFA is deterministic and only one state is active at each time step.
- Evidence anchors:
  - [abstract]: "an encoding of linear complexity from LTLf into MDPs"
  - [section 3.1]: "Reducing the amount of state space at the time of RL training is beneficial. Suppose the amount of original state space is |S|, and the state space in M is linear in the size of |S|, i.e. |S'| = |Q| × |S|."
- Break condition: If the DFA is non-deterministic or multiple states need to be tracked simultaneously, this encoding would fail.

### Mechanism 2
- Claim: Experience classification based on DFA state ranks prioritizes learning for states closer to task completion.
- Mechanism: States are ranked by expected path length to accepting states, and experiences are stored in separate replay buffers by rank. Sampling probabilities are adjusted to favor higher-ranked states.
- Core assumption: Closer proximity to accepting states correlates with higher task completion probability.
- Evidence anchors:
  - [section 3.2]: "Intuitively, the closer a DFA state to the accepting states, the higher the probability of accomplishing the task. Hence, we introduce criteria to prioritize DFA states, i.e., expected path length to accepting states."
  - [section 3.2]: "We classify experiences into N categories, having different priorities... The priorities of the 4 experience categories are p0 = 1/4, p1 = 1/3, p2 = 1/2 and p3 = 1"
- Break condition: If the DFA structure doesn't reflect task progress accurately, ranking may mislead the learning process.

### Mechanism 3
- Claim: Reward shaping using DFA state potentials provides denser intermediate rewards during training.
- Mechanism: The reward function is augmented with a potential-based shaping term: R'(st, at, st+1) = R(st, at, st+1) + γρ(q') - ρ(q), where ρ represents the priority of DFA states.
- Core assumption: Potential-based shaping preserves optimal policies while providing additional guidance.
- Evidence anchors:
  - [section 3.2]: "The idea of reward shaping is applied in line 10... Concretely, the reshaped reward function is shown as follows: R(st, at, st+1) = R(st, at, st+1) + γρ(q') - ρ(q)"
- Break condition: If the potential function is poorly designed, it could introduce bias or slow convergence.

## Foundational Learning

- Concept: Linear Temporal Logic over Finite Traces (LTLf)
  - Why needed here: LTLf is the formal language used to specify non-Markovian tasks that this approach addresses
  - Quick check question: What is the key difference between LTL and LTLf in terms of trace interpretation?

- Concept: Deterministic Finite Automata (DFA) and its relationship to LTLf
  - Why needed here: The DFA provides a finite-state representation of LTLf formulas that can be integrated into the MDP state space
  - Quick check question: How does a DFA accept a string that satisfies its corresponding LTLf formula?

- Concept: Prioritized Experience Replay (PER) and its extension to experience classification
  - Why needed here: The classification approach builds on PER principles but organizes experiences by DFA state rank rather than TD error
  - Quick check question: What is the main advantage of prioritizing experiences in reinforcement learning?

## Architecture Onboarding

- Component map:
  LTLf formula → DFA converter (external tool like LTLf2DFA) → MDP encoder (implements the linear state space expansion) → Experience classifier (implements Algorithm 2 with N categories) → RL algorithm (TD3 or SAC from Stable-Baselines3) → pyRDDLGym environment wrapper

- Critical path:
  1. Parse LTLf specification
  2. Generate DFA representation
  3. Encode MDP with DFA state tracking
  4. Initialize experience buffers by category
  5. Train RL agent with experience classification and reward shaping

- Design tradeoffs:
  - Linear vs. exponential state space growth (chosen: linear)
  - Fixed vs. adaptive number of experience categories (chosen: fixed N)
  - Uniform vs. prioritized sampling (chosen: prioritized by DFA rank)

- Failure signatures:
  - Training stalls or diverges: Check DFA encoding correctness and reward shaping parameters
  - Poor policy quality: Verify DFA state ranking algorithm and experience classification logic
  - Memory issues: Monitor replay buffer sizes, especially with many DFA states

- First 3 experiments:
  1. Run with N=1 (no classification) to establish baseline performance
  2. Test with N=3 categories on Waterworld task 1 to verify classification logic
  3. Compare training curves with different α values (0.25, 0.5, 0.75) on Cartpole task 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the exponent $\alpha$ in the prioritized experience replay (PER) impact the training speed and policy quality in different tasks?
- Basis in paper: [explicit] The paper mentions that the exponent $\alpha$ determines how much prioritization is balanced in the PER, with $\alpha = 0$ corresponding to the uniform case.
- Why unresolved: The paper provides experimental results showing the impact of different $\alpha$ values on training speed and policy quality, but does not offer a theoretical analysis or guidelines for choosing $\alpha$ based on task characteristics.
- What evidence would resolve it: Further theoretical analysis and experimental studies comparing different $\alpha$ values across a wider range of tasks would help establish guidelines for choosing $\alpha$ based on task characteristics.

### Open Question 2
- Question: Can the proposed approach be extended to handle more complex temporal logic specifications beyond LTL$_f$, such as those involving past-time operators or quantitative temporal operators?
- Basis in paper: [explicit] The paper focuses on LTL$_f$ specifications and mentions that other temporal logics may also be applicable.
- Why unresolved: The paper does not explore the applicability of the proposed approach to more complex temporal logic specifications, and it is unclear whether the encoding and experience classification methods can be generalized to handle such specifications.
- What evidence would resolve it: Developing extensions of the encoding and experience classification methods to handle more complex temporal logic specifications and evaluating their performance on benchmark problems would demonstrate the applicability of the approach to a broader class of tasks.

### Open Question 3
- Question: How does the proposed approach compare to other state-of-the-art methods for training non-Markovian tasks, such as those based on automata-based reward shaping or task decomposition?
- Basis in paper: [explicit] The paper compares the proposed approach to other methods, such as automatic reward shaping and prioritized experience replay, but does not provide a comprehensive comparison to all state-of-the-art methods.
- Why unresolved: The paper focuses on evaluating the proposed approach against a limited set of baselines, and it is unclear how it compares to other advanced methods in the literature.
- What evidence would resolve it: Conducting a thorough comparison of the proposed approach to other state-of-the-art methods, including those based on automata-based reward shaping or task decomposition, on a diverse set of benchmark problems would provide insights into its relative strengths and weaknesses.

## Limitations

- Several LTLf formulae for benchmark tasks are truncated in the paper, preventing exact replication of reported results
- The approach relies on external LTLf2DFA converter whose implementation details are not provided
- Performance improvements are reported for specific parameter settings without exploring sensitivity to different values

## Confidence

- High confidence: The core mechanism of DFA-based state encoding with linear complexity
- Medium confidence: The experience classification approach and reward shaping
- Low confidence: The generality of the approach to other LTLf specifications and RL algorithms

## Next Checks

1. Replicate with complete specifications: Obtain or reconstruct the full LTLf specifications for all benchmark tasks and verify that the reported improvements can be reproduced with the exact same formulae

2. Cross-domain generalization test: Apply the approach to at least one additional benchmark domain (e.g., from the pyRDDLGym repository) with a different type of LTLf specification to assess the method's generality

3. Algorithm comparison validation: Compare the proposed method against PER and other baselines using statistical significance testing (e.g., paired t-tests) across multiple random seeds to confirm that the reported improvements are not due to chance variation