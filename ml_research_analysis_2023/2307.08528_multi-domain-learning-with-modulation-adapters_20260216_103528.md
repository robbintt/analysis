---
ver: rpa2
title: Multi-Domain Learning with Modulation Adapters
arxiv_id: '2307.08528'
source_url: https://arxiv.org/abs/2307.08528
tags:
- adapters
- modulation
- network
- base
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Modulation Adapters (MAD) for multi-domain
  learning, addressing the problem of adapting a pre-trained convolutional neural
  network to multiple related tasks while minimizing the number of additional parameters.
  MAD scales the weights of existing convolutional filters in a multiplicative manner,
  parameterized per task, allowing for efficient adaptation with fewer parameters
  compared to methods that add new layers.
---

# Multi-Domain Learning with Modulation Adapters

## Quick Facts
- arXiv ID: 2307.08528
- Source URL: https://arxiv.org/abs/2307.08528
- Authors:
- Reference count: 40
- This paper proposes Modulation Adapters (MAD) for multi-domain learning, achieving state-of-the-art results on the Visual Decathlon Challenge and ImageNet-to-Sketch benchmark with high parameter efficiency.

## Executive Summary
This paper introduces Modulation Adapters (MAD), a parameter-efficient method for adapting pre-trained convolutional neural networks to multiple related tasks. MAD scales existing convolutional filter weights in a multiplicative manner, parameterized per task, enabling flexible adaptation with fewer parameters than methods that add new layers. The approach achieves state-of-the-art or comparable results across various parameter budgets on both the Visual Decathlon Challenge and ImageNet-to-Sketch benchmark.

## Method Summary
MAD adapts pre-trained CNNs to multiple domains by learning scalar coefficients that scale convolutional filter weights element-wise. Each domain has its own adapter parameters that perform non-binary multiplicative modulation of the base network's filters. To further improve parameter efficiency, MAD can be factorized as a product of two smaller matrices with reduced intermediate dimensionality. The method is evaluated on two challenging benchmarks: Visual Decathlon Challenge (10 datasets) and ImageNet-to-Sketch (6 datasets), using ResNet-26 and DenseNet-121 architectures respectively.

## Key Results
- Achieves mean accuracy of 78.7% and decathlon score of 3828 on Visual Decathlon Challenge
- Achieves average accuracy of 84.2% and total score of 1668 on ImageNet-to-Sketch benchmark
- Demonstrates state-of-the-art or comparable performance across a wide range of parameter budgets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modulation Adapters enable non-binary multiplicative adaptation of convolutional filters across multiple domains
- Mechanism: For each domain, MAD learns scalar coefficients that scale the pre-trained convolutional filter weights element-wise, allowing selective amplification or suppression of input-output channel combinations
- Core assumption: The multiplicative modulation of existing filter weights provides sufficient representational capacity to adapt to domain-specific characteristics
- Evidence anchors:
  - [abstract]: "MAD scales the weights of existing convolutional filters in a multiplicative manner, parameterized per task"
  - [section]: "To adapt the convolutional layers of the base network to a new domain d ∈ { 1, . . . , D}, we define a modulation adaptation unit ρ, with domain specific parameters αd ∈ I RM ×N that modulate the convolutional filter in a multiplicative manner"
  - [corpus]: Weak evidence - neighboring papers discuss modulation in sentiment analysis and image generation, but not specifically for multi-domain learning with multiplicative filter adaptation

### Mechanism 2
- Claim: Factorized parameterization of Modulation Adapters enables parameter-efficient adaptation across domains
- Mechanism: MAD is represented as a product of two matrices with smaller intermediate dimensionality (αd = βd × γd), reducing parameters from M×N to I×(M+N) where I < min(M,N)
- Core assumption: The rank restriction imposed by factorization does not significantly degrade adaptation quality while providing substantial parameter savings
- Evidence anchors:
  - [section]: "To further reduce the memory footprint, we propose a factorised representation of each adapter αd as a product of two matrices with a smaller intermediate dimensionality"
  - [section]: "The decomposed model with I = 36 , however, yields a saving of more than 60% in the parameter budget"
  - [corpus]: Weak evidence - only one neighbor paper mentions modulation, but not factorized parameterization for multi-domain learning

### Mechanism 3
- Claim: Modulation Adapters provide more flexible adaptation than binary masking or additive bypass methods
- Mechanism: Unlike binary masks that can only enable/disable weights, or additive bypasses that only modify central filter elements, MAD scales each filter element based on input-output channel combinations, providing continuous non-binary adaptation
- Core assumption: Non-binary scaling of individual filter elements provides richer adaptation space than binary operations or partial modifications
- Evidence anchors:
  - [section]: "In contrast to other approaches, our adapters allow to modulate all weights in the base network in a non-binary manner"
  - [section]: "While reduction in memory budget is an appealing property of the masking approach, the common downside is limited capability of model adaptation due to simplicity of the binary masks"
  - [corpus]: No direct evidence - neighboring papers discuss modulation but not comparative analysis with masking methods

## Foundational Learning

- Concept: Multi-domain learning and knowledge transfer
  - Why needed here: The paper addresses adapting a single model to multiple related tasks across different domains, requiring understanding of how knowledge transfers between domains
  - Quick check question: What is the fundamental difference between multi-domain learning and multi-task learning?

- Concept: Parameter-efficient model adaptation
  - Why needed here: MAD aims to minimize additional parameters while maximizing adaptation capability, requiring understanding of trade-offs between parameter count and model performance
  - Quick check question: How does parameter efficiency affect model generalization in low-data regimes?

- Concept: Convolutional neural network architecture
  - Why needed here: MAD specifically modulates convolutional filter weights, requiring understanding of how CNNs process input features through spatial filters
  - Quick check question: What is the computational difference between 3×3 and 1×1 convolutions in terms of parameter count?

## Architecture Onboarding

- Component map: Pre-trained base network (ResNet-26 or DenseNet-121) → Modulation Adapters (domain-specific) → Domain-specific BatchNorm layers → Domain-specific classification heads
- Critical path: Forward pass: input → base network convolutions → MAD scaling → BatchNorm → classification head; Backward pass: gradients flow through classification head → BatchNorm → MAD scaling → base network
- Design tradeoffs: Full MAD provides maximum adaptation (M×N parameters per layer) vs. decomposed MAD provides parameter efficiency (I×(M+N) parameters) at potential cost to adaptation quality
- Failure signatures: Poor performance despite MAD learning could indicate: (1) insufficient base network capacity, (2) rank restriction too severe in decomposed MAD, (3) learning rate too high/low for MAD parameters
- First 3 experiments:
  1. Implement MAD on a single convolutional layer with synthetic data to verify element-wise scaling behavior
  2. Compare full MAD vs. decomposed MAD (varying I) on a simple two-domain task to observe parameter-accuracy trade-off
  3. Test MAD vs. binary mask adaptation on a toy dataset to demonstrate superiority of non-binary scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to balance parameter efficiency and accuracy when using decomposed Modulation Adapters?
- Basis in paper: [explicit] The paper discusses decomposing Modulation Adapters to reduce the number of parameters, but notes that the optimal decomposition varies by dataset and that the decomposition is less effective on the ImageNet-to-Sketch benchmark.
- Why unresolved: The effectiveness of decomposition depends on the specific architecture and dataset, and there is no clear method to determine the optimal decomposition for a given scenario.
- What evidence would resolve it: Comparative studies of different decomposition strategies across various architectures and datasets, along with a theoretical framework to predict the optimal decomposition.

### Open Question 2
- Question: How does the sparsity of Modulation Adapters affect their performance, and what factors influence this sparsity?
- Basis in paper: [explicit] The paper mentions that learned adapters for the ImageNet-to-Sketch benchmark do not show the sparse structure observed in the adapters learned for the Visual Decathlon Challenge.
- Why unresolved: The relationship between sparsity, architecture, and dataset characteristics is not fully understood, and it's unclear why sparsity varies across benchmarks.
- What evidence would resolve it: Detailed analysis of the sparsity patterns in Modulation Adapters across different architectures and datasets, along with experiments to determine the impact of sparsity on performance.

### Open Question 3
- Question: Can Modulation Adapters be effectively combined with other parameter reduction techniques to further improve efficiency without sacrificing accuracy?
- Basis in paper: [explicit] The paper explores combining Modulation Adapters with other approaches like WTPB and finds that hybrid models can improve performance, but the effectiveness of such combinations is not fully explored.
- Why unresolved: The potential synergies and trade-offs between different parameter reduction techniques are not well understood, and there may be unexplored combinations that could offer significant benefits.
- What evidence would resolve it: Systematic experiments combining Modulation Adapters with various parameter reduction techniques, along with an analysis of the resulting trade-offs between efficiency and accuracy.

## Limitations
- The factorization approach's optimal intermediate dimension is empirically chosen without theoretical justification
- Effectiveness on truly dissimilar domains or extreme low-data regimes is not demonstrated
- Limited exploration of combining MAD with other parameter-efficient fine-tuning methods

## Confidence

- High confidence: MAD's core mechanism of multiplicative filter scaling provides efficient domain adaptation (supported by quantitative results across 16 datasets)
- Medium confidence: Factorized MAD provides optimal parameter-accuracy trade-off (I values appear empirically chosen without theoretical guidance)
- Medium confidence: MAD outperforms all compared methods across parameter budgets (results show MAD is state-of-the-art or comparable, but some methods are not directly compared under identical conditions)

## Next Checks

1. Test MAD on domains with fundamentally different data distributions (e.g., natural images vs. medical imaging) to evaluate cross-domain generalization limits
2. Implement ablation studies varying the intermediate dimension I systematically to identify optimal factorization strategies
3. Compare MAD against emerging parameter-efficient fine-tuning methods (e.g., LoRA, prefix tuning) under identical computational budgets to establish relative efficiency