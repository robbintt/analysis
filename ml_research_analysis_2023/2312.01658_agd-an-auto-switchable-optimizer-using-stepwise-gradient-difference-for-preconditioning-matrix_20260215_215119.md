---
ver: rpa2
title: 'AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning
  Matrix'
arxiv_id: '2312.01658'
source_url: https://arxiv.org/abs/2312.01658
tags:
- learning
- rate
- adam
- optimizers
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AGD, an auto-switchable optimizer that dynamically
  switches between SGD and adaptive optimization based on gradient differences between
  successive steps. The method uses diagonal preconditioning matrix elements computed
  as gradient differences, serving as an approximation of the inner product between
  Hessian row vectors and parameter differences.
---

# AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix

## Quick Facts
- **arXiv ID:** 2312.01658
- **Source URL:** https://arxiv.org/abs/2312.01658
- **Reference count:** 40
- **Primary result:** AGD dynamically switches between SGD and adaptive optimization using gradient differences, achieving better generalization on NLP, CV, and RecSys tasks

## Executive Summary
This paper proposes AGD, an auto-switchable optimizer that dynamically transitions between SGD and adaptive optimization based on gradient differences between successive steps. The method uses diagonal preconditioning matrix elements computed as gradient differences, serving as an approximation of the inner product between Hessian row vectors and parameter differences. The auto-switching mechanism toggles between modes based on a threshold hyperparameter δ. Experiments on NLP, CV, and RecSys tasks demonstrate AGD outperforms or matches state-of-the-art optimizers, achieving better generalization performance. The authors provide theoretical convergence guarantees for both convex and non-convex settings.

## Method Summary
AGD computes gradient differences between successive steps as diagonal preconditioning matrix elements, approximating Hessian information without explicitly computing the Hessian. The optimizer dynamically switches between SGD and adaptive optimization modes based on a threshold δ applied to these preconditioning elements. When gradient differences exceed δ, AGD uses adaptive steps; otherwise, it defaults to SGD-like updates with momentum. The method incorporates exponential moving average (EMA) for both gradient and preconditioning matrix estimation, with bias correction applied. Training involves standard forward/backward passes with the AGD optimizer replacing conventional optimizers like Adam or SGD with momentum.

## Key Results
- AGD achieves better generalization performance than Adam, SGD, and AdaBound across NLP, CV, and RecSys tasks
- On CIFAR-10 with ResNet20/32, AGD achieves 93.03%/94.66% accuracy compared to 92.59%/94.42% for Adam
- For language modeling on PTB, AGD reaches 63.41 PPL versus 64.54 for Adam
- AGD automatically transitions from adaptive to SGD-like behavior during training, improving generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** AGD uses gradient differences between successive steps as diagonal preconditioning matrix elements
- **Mechanism:** The difference between gradients at adjacent steps approximates the inner product of the Hessian row vectors and parameter differences, capturing curvature information without explicitly computing the Hessian
- **Core assumption:** When parameter changes are small, the gradient difference equals the directional derivative of the Hessian
- **Evidence anchors:**
  - [abstract]: "gradient difference between two successive steps as the diagonal elements... approximation of the inner product between the Hessian row vectors and difference of the adjacent parameter vectors"
  - [section 3.1]: "∇if(wt) − ∇if(wt−1) ≈ ∇∇ if(wt) · ∆w" showing the Taylor expansion relationship
  - [corpus]: Weak evidence - related work on second-order methods exists but none specifically use gradient differences in this way
- **Break condition:** If parameter changes become large between steps, the approximation error from Taylor expansion grows significantly

### Mechanism 2
- **Claim:** AGD dynamically switches between SGD and adaptive optimization modes based on gradient magnitude
- **Mechanism:** When the preconditioning matrix elements exceed threshold δ, AGD uses adaptive steps; otherwise it defaults to SGD-like updates with momentum
- **Core assumption:** Large gradient differences indicate regions where adaptive learning rates are beneficial, while small differences suggest SGD suffices
- **Evidence anchors:**
  - [abstract]: "auto-switching function that enables the preconditioning matrix to switch dynamically between Stochastic Gradient Descent (SGD) and the adaptive optimizer"
  - [section 3.1]: "max(Bt, δI)... If the element of ˆbt exceeds δ, AGD takes a confident adaptive step. Otherwise, the update is performed using EMA, i.e., mt"
  - [corpus]: Weak evidence - AdaBound and SWATS also use switching but rely on learning rate clipping rather than preconditioning matrix thresholding
- **Break condition:** If δ is set too high or too low, the switching mechanism may never activate or always activate, eliminating the benefit

### Mechanism 3
- **Claim:** AGD achieves better generalization by automatically transitioning from adaptive to SGD-like behavior during training
- **Mechanism:** Early in training, most parameters use adaptive updates for fast convergence; later, parameters switch to SGD-like updates for better generalization
- **Core assumption:** The transition from adaptive to SGD behavior naturally occurs as the model approaches a minimum where SGD's generalization properties are advantageous
- **Evidence anchors:**
  - [section 4.5]: Analysis showing "AGD behaves more like SGD during the initial stages... and switches to adaptive optimization for fine-tuning" on ResNet18
  - [section 4.5]: "The proportion of parameters taking adaptive updates grows from 3% to 5% afterward, resulting in a better PPL in the fine-tuning stage" for 2-layer LSTM
  - [corpus]: Weak evidence - no direct corpus support for this specific automatic transition pattern
- **Break condition:** If the dataset or architecture requires different optimization characteristics, the automatic transition may not align with optimal training dynamics

## Foundational Learning

- **Concept:** Taylor expansion and its use in approximating functions
  - **Why needed here:** The entire gradient difference mechanism relies on Taylor expansion to approximate Hessian information from gradient differences
  - **Quick check question:** If f(x) = x³ and we expand around x=2 with h=0.1, what is the first-order Taylor approximation of f(2.1)?

- **Concept:** Preconditioning in optimization and its effect on convergence
  - **Why needed here:** Understanding how preconditioning matrices adjust step sizes in different parameter directions is essential for grasping AGD's design
  - **Quick check question:** If the preconditioning matrix is diagonal with entries [2, 0.5] and gradient is [1, 4], what are the effective step sizes in each direction?

- **Concept:** Exponential Moving Average (EMA) and bias correction
  - **Why needed here:** AGD uses EMA to estimate gradients and applies bias correction, which is crucial for understanding the mt and st calculations
  - **Quick check question:** If β1=0.9 and the gradients are [1, 2, 3] for three steps, what is the EMA value at step 3 with bias correction?

## Architecture Onboarding

- **Component map:** Gradient computation -> Momentum estimation -> Gradient difference computation -> Preconditioning matrix update -> Auto-switching logic -> Parameter update
- **Critical path:**
  1. Compute gradient gt
  2. Update momentum mt with bias correction
  3. Calculate gradient difference st
  4. Update preconditioning matrix bt with EMA
  5. Apply auto-switch threshold to get effective preconditioning
  6. Update parameters using preconditioned gradient
- **Design tradeoffs:**
  - Computational cost vs. Hessian approximation quality: Using gradient differences is cheaper than computing Hessian but less accurate
  - Memory vs. adaptivity: AGD stores only diagonal preconditioning matrix, limiting adaptivity to per-parameter scaling
  - Hyperparameter sensitivity: δ controls switching behavior but requires tuning; too high prevents adaptive updates, too low prevents SGD-like generalization
- **Failure signatures:**
  - No improvement in training loss: Check if δ is set appropriately or if gradient difference computation has numerical issues
  - Oscillating training: Gradient differences may be too noisy; consider adjusting β2 or adding smoothing
  - Poor generalization: Switching threshold δ may be preventing proper transition to SGD-like behavior
- **First 3 experiments:**
  1. Test on a simple convex function (like Beale) with known Hessian to verify gradient difference approximation quality
  2. Run on ResNet20/CIFAR10 with different δ values to observe switching behavior and find optimal threshold
  3. Compare convergence speed and final accuracy against Adam and SGD on a small NLP task to validate practical benefits

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The approximation quality of gradient differences as Hessian proxies hasn't been thoroughly validated across diverse architectures
- The auto-switching mechanism's effectiveness depends heavily on the δ hyperparameter, but systematic sensitivity analysis is limited
- The generalization benefits observed may be task-specific rather than universal

## Confidence

- **Theoretical framework:** High - The convergence proofs for both convex and non-convex cases appear sound
- **Gradient difference mechanism:** Medium - The Taylor expansion foundation is solid, but approximation error under large parameter changes needs more analysis
- **Experimental results:** Medium - Strong results across tasks, but some ablations are missing (e.g., ablation on δ values, comparison with more recent optimizers)
- **Generalization claims:** Low - While results show better generalization, the mechanism for why switching to SGD-like behavior improves generalization needs deeper theoretical justification

## Next Checks

1. **Approximation Error Analysis:** Systematically measure the error between gradient differences and true Hessian directional derivatives across different training stages and architectures to quantify approximation quality

2. **δ Sensitivity Study:** Conduct a comprehensive ablation study varying δ from 1e-8 to 1e-2 on multiple tasks to understand its impact on switching behavior and final performance

3. **Generalization Mechanism:** Design experiments to isolate whether generalization improvements come from the switching mechanism itself or from other factors like different learning rate schedules or weight decay interactions