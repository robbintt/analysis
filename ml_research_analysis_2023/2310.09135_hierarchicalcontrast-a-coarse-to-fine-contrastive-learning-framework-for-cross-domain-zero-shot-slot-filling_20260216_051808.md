---
ver: rpa2
title: 'HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for
  Cross-Domain Zero-Shot Slot Filling'
arxiv_id: '2310.09135'
source_url: https://arxiv.org/abs/2310.09135
tags:
- none
- slot
- unseen
- slots
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Hierarchical Contrastive Learning (HiCL)
  framework to address the cross-domain zero-shot slot filling problem. HiCL leverages
  coarse-to-fine contrastive learning based on Gaussian-distributed embeddings to
  learn generalized semantic relations between utterance-tokens, enabling the model
  to generalize to unseen slot types in the target domain.
---

# HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling

## Quick Facts
- **arXiv ID:** 2310.09135
- **Source URL:** https://arxiv.org/abs/2310.09135
- **Reference count:** 40
- **Key outcome:** HiCL achieves comparable or better performance than state-of-the-art zero-shot slot filling approaches through hierarchical contrastive learning with Gaussian embeddings.

## Executive Summary
This paper addresses the challenge of cross-domain zero-shot slot filling, where models must identify and classify slot entities in utterances for unseen slot types in target domains without labeled training data. The authors propose Hierarchical Contrastive Learning (HiCL), a framework that leverages coarse-to-fine contrastive learning based on Gaussian-distributed embeddings to learn generalized semantic relations between utterance tokens. Additionally, they introduce an iterative label set semantics inference method to enable unbiased evaluation of unseen slot types by separating them from seen slot predictions. Extensive experiments on four datasets demonstrate that HiCL achieves competitive performance against current state-of-the-art approaches.

## Method Summary
HiCL employs a hierarchical contrastive learning approach with Gaussian-distributed embeddings to learn domain-invariant features for zero-shot slot filling. The method consists of two levels: coarse-grained entity-level contrastive learning that captures boundary and type knowledge, and fine-grained token-level contrastive learning that focuses on BIO labels and token-class features. Gaussian embeddings are used to model semantic uncertainty in token representations, with KL divergence optimizing distributional divergence across token classes. A conditional random field (CRF) layer is used for final slot filling predictions. The training procedure involves constructing samples with target slot types, other slot types, utterances, and BIO labels. For evaluation, an iterative label set semantics inference method separates seen and unseen slot predictions to provide unbiased performance measurement.

## Key Results
- HiCL achieves comparable or better performance than current state-of-the-art zero-shot slot filling approaches
- The framework demonstrates effective cross-domain generalization to unseen slot types
- Iterative label set semantics inference enables unbiased evaluation of unseen slot performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical contrastive learning with Gaussian embeddings improves cross-domain zero-shot generalization by modeling slot-agnostic features
- **Mechanism:** Coarse-to-fine contrastive learning optimizes distributional divergence across token classes, encouraging the model to extract domain-invariant features that generalize to unseen slot types
- **Core assumption:** Semantic uncertainty in token representations can be better captured with Gaussian embeddings than point embeddings, enabling robust clustering of unseen slot types
- **Evidence anchors:**
  - [abstract]: "we propose a coarse- to fine-grained contrastive learning based on Gaussian-distributed embedding to learn the generalized deep semantic relations between utterance-tokens"
  - [section]: "Different from existing slot filling contrastive learners... HiCL aims to optimize distributional divergence by leveraging effectively modeling Gaussian embeddings"
- **Break condition:** If the assumption that unseen slots share distributional properties with seen slots fails, the method loses generalization capability

### Mechanism 2
- **Claim:** Iterative label set semantics inference enables unbiased evaluation of unseen slots by separating seen and unseen slot predictions
- **Mechanism:** Instead of evaluating on utterance samples containing both seen and unseen slots, the method constructs separate test sets for each slot type, allowing isolated performance measurement
- **Core assumption:** Seen and unseen slot types do not overlap in semantic space when using Gaussian embeddings, enabling clean separation in evaluation
- **Evidence anchors:**
  - [abstract]: "we present a new iterative label set semantics inference method to unbiasedly and separately evaluate the performance of unseen slot types which entangled with their counterparts"
  - [section]: "We find unseen slots and seen slots overlapping problem in the previous methods for unseen slots performance evaluation, and rectify this bias by splitting test set from slot type granularity instead of sample granularity"
- **Break condition:** If the method cannot maintain separation between seen and unseen slots during training, evaluation bias reappears

### Mechanism 3
- **Claim:** Combining coarse-grained entity-level and fine-grained token-level contrastive learning yields superior generalization than either alone
- **Mechanism:** Entity-level contrastive learning provides boundary and type knowledge that complements token-level learning of BIO labels and token-class features
- **Core assumption:** Knowledge transfer from entity-level to token-level contrastive learning improves token-class generalization to unseen slots
- **Evidence anchors:**
  - [abstract]: "We argue that entity-level class knowledge contributes to token-level class learning, and their combination is beneficial for ZSSF task"
  - [section]: "Coarse-grained CL complements fine-grained CL with entity-level boundary information and entity type knowledge, while fine-grained CL complements coarse-grained CL with token-level boundary information (BIO) and token class type knowledge"
- **Break condition:** If entity-level and token-level contrastive signals conflict, the combined approach degrades rather than improves performance

## Foundational Learning

- **Concept:** Contrastive learning and distributional embeddings
  - **Why needed here:** The paper's core innovation relies on contrastive learning to optimize distributional divergence between token embeddings, which is central to understanding how the model generalizes to unseen slots
  - **Quick check question:** How does KL-divergence between Gaussian distributions differ from Euclidean distance between point embeddings in capturing semantic uncertainty?

- **Concept:** Zero-shot learning and domain adaptation
  - **Why needed here:** The paper addresses zero-shot slot filling across domains, requiring understanding of how models transfer knowledge without target domain training data
  - **Quick check question:** What distinguishes zero-shot learning from few-shot learning in terms of available training data and generalization requirements?

- **Concept:** Slot filling task formulation and evaluation metrics
  - **Why needed here:** Understanding the slot filling task structure (BIO labeling, slot type sets) and evaluation methodology (F1 scores for seen/unseen slots) is essential for interpreting experimental results
  - **Quick check question:** How does the iterative label set semantics inference method change the way seen and unseen slot performance is calculated compared to traditional evaluation approaches?

## Architecture Onboarding

- **Component map:** Input encoder (BERT/BiLSTM) → Gaussian transformation network → Coarse-level contrastive loss → Fine-level contrastive loss → CRF layer → Slot filling loss
- **Critical path:** Gaussian embedding generation → Hierarchical contrastive learning (coarse then fine) → CRF-based slot filling → Iterative inference for evaluation
- **Design tradeoffs:** Gaussian vs point embeddings: Gaussian captures uncertainty but increases computational complexity; Coarse vs fine contrastive levels: Entity-level provides boundary knowledge but may miss token-level nuances; Iterative vs batch inference: Iterative enables separate evaluation but increases inference time
- **Failure signatures:** Poor unseen slot performance despite good seen slot performance: Indicates failure in domain-invariant feature learning; Similar performance across seen/unseen slots: May indicate insufficient contrastive signal or improper Gaussian embedding configuration; Degraded performance when removing either contrastive level: Suggests the hierarchical approach is critical for the task
- **First 3 experiments:**
  1. Baseline comparison: Implement and evaluate HiCL against standard BERT on seen slots only to establish baseline performance
  2. Gaussian embedding ablation: Replace Gaussian embeddings with point embeddings and measure impact on unseen slot performance
  3. Hierarchical ablation: Remove coarse-level or fine-level contrastive learning separately to verify their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed HiCL framework perform when applied to large language models (LLMs) such as ChatGPT for zero-shot slot filling tasks?
- **Basis in paper:** [inferred] The paper mentions that LLMs like ChatGPT seem to struggle with sequence labeling tasks, including slot filling. It also states that the authors' work aims to remedy this with lighter models
- **Why unresolved:** The paper does not explore the application of HiCL to LLMs, focusing instead on smaller, lighter models. The potential benefits or drawbacks of applying HiCL to LLMs remain unexplored
- **What evidence would resolve it:** Experimental results comparing the performance of HiCL when applied to LLMs versus smaller models on zero-shot slot filling tasks would provide insights into its effectiveness and potential limitations with LLMs

### Open Question 2
- **Question:** What is the impact of dataset size and diversity on the performance of HiCL for cross-domain zero-shot slot filling?
- **Basis in paper:** [explicit] The paper discusses how the diversity of source domains in training is crucial for the model's capability of cross-domain migration of unseen slots. It also mentions that with increasing dataset volume, the performance gain of HiCL against baselines gradually diminishes
- **Why unresolved:** While the paper provides some insights into the relationship between dataset size/diversity and performance, it does not explore this relationship in depth or provide a clear threshold for optimal dataset size and diversity
- **What evidence would resolve it:** A systematic study varying dataset size and diversity, along with corresponding performance metrics, would help establish the optimal conditions for HiCL's performance in cross-domain zero-shot slot filling

### Open Question 3
- **Question:** How does the proposed iterative label set semantics inference method compare to other methods for handling the overlapping of unseen and seen slots in test sets?
- **Basis in paper:** [explicit] The paper introduces a new iterative label set semantics inference method to address the problem of unseen and seen slots overlapping in test sets, which was a limitation in previous methods
- **Why unresolved:** The paper does not compare the proposed method to other potential solutions for handling this issue, such as modifying the test set split or using different evaluation metrics
- **What evidence would resolve it:** A comparison of the proposed method with alternative approaches in terms of their effectiveness in addressing the overlapping problem and their impact on the evaluation of unseen slots performance would provide valuable insights

## Limitations

- The evaluation methodology creates artificial testing scenarios that may not reflect real-world deployment conditions where seen and unseen slots co-occur in the same utterances
- The theoretical justification for using Gaussian embeddings with KL divergence optimization is not fully validated with empirical analysis of the learned distributions
- The hierarchical contrastive learning design's effectiveness is primarily shown through combined performance rather than examining potential conflicts between coarse and fine-level signals

## Confidence

**High Confidence:** The core contribution of addressing evaluation bias in zero-shot slot filling is well-established through clear problem identification and solution methodology. The paper convincingly demonstrates that previous evaluation methods confounded seen and unseen slot performance.

**Medium Confidence:** The empirical results showing HiCL outperforming baseline methods on seen and unseen slot F1 scores are reproducible based on the provided implementation details. However, the practical significance of the performance gains relative to simpler approaches remains uncertain.

**Low Confidence:** The theoretical justification for using Gaussian embeddings with KL divergence optimization is not fully validated. The paper claims these embeddings better capture semantic uncertainty, but provides limited empirical evidence beyond task performance metrics.

## Next Checks

1. **Evaluate on mixed-slot utterances:** Test the model's performance on realistic utterances containing both seen and unseen slots to verify that the iterative inference method's benefits transfer to practical scenarios where slot type separation is not guaranteed.

2. **Analyze Gaussian embedding properties:** Conduct statistical analysis of the learned Gaussian parameters (μ, Σ) across different slot types to verify whether the distributional representations capture meaningful semantic uncertainty beyond what point embeddings would provide.

3. **Compare with simpler baselines:** Implement and evaluate standard contrastive learning approaches (without Gaussian embeddings or hierarchical levels) to determine whether the additional complexity of HiCL provides proportional performance benefits for the zero-shot slot filling task.