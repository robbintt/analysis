---
ver: rpa2
title: 'Character-LLM: A Trainable Agent for Role-Playing'
arxiv_id: '2310.10158'
source_url: https://arxiv.org/abs/2310.10158
tags:
- speaking
- your
- have
- about
- character
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Character-LLM, a method for training large
  language models to act as specific historical or fictional characters by reconstructing
  their experiences into narrative scenes. The approach involves collecting profiles,
  extracting scenes, completing experiences, and uploading them to fine-tune LLaMA
  models.
---

# Character-LLM: A Trainable Agent for Role-Playing

## Quick Facts
- arXiv ID: 2310.10158
- Source URL: https://arxiv.org/abs/2310.10158
- Reference count: 40
- Key outcome: Character-LLM trains LLaMA models to act as historical/fictional characters using reconstructed narrative scenes, outperforming Alpaca and Vicuna in personality consistency and reducing hallucinations.

## Executive Summary
This paper introduces Character-LLM, a method for training large language models to act as specific historical or fictional characters by reconstructing their experiences into narrative scenes. The approach involves collecting profiles, extracting scenes, completing experiences, and uploading them to fine-tune LLaMA models. Protective experiences are added to prevent character hallucination. Evaluation via interviews and LLM judges shows that trained agents outperform Alpaca and Vicuna in personality, memorization, and stability, achieving comparable performance to ChatGPT.

## Method Summary
Character-LLM trains LLaMA 7B models to simulate historical or fictional characters by fine-tuning on narrative scenes reconstructed from character profiles. The method collects character profiles (primarily from Wikipedia), uses LLMs to extract and complete experience scenes, and uploads these to fine-tune the base model. Protective experiences are added to prevent the model from generating out-of-character knowledge. The trained agents are evaluated through single-turn and multi-turn interviews, with ChatGPT serving as both interviewer and judge for personality consistency, memorization, hallucination resistance, and stability.

## Key Results
- Character-LLM agents outperform Alpaca and Vicuna in personality consistency, memorization, and stability
- Agents achieve comparable performance to ChatGPT in character simulation tasks
- Protective experiences effectively reduce character hallucination in trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Experience Reconstruction pipeline grounds character simulacra in fact-based scenes rather than relying on the LLM's inherent world knowledge.
- Mechanism: Profiles are extracted from reliable sources (e.g., Wikipedia), then LLM-generated scenes are derived directly from profile content. This anchors the generated text to documented events, limiting hallucinated knowledge outside the character's era.
- Core assumption: Profile-based scenes sufficiently capture the character's experience space and personality traits.
- Evidence anchors:
  - [abstract] "Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences."
  - [section] "We collect experiences of certain people, exemplified by Ludwig van Beethoven, Queen Cleopatra, and Julius Caesar, then we use LLMs to extract scenes based on the collected personal experiences as memories flashes."
- Break condition: If the profile is sparse or biased, the derived scenes may not reflect a complete or accurate persona, leading to shallow or inconsistent behavior.

### Mechanism 2
- Claim: Protective experiences effectively suppress Character Hallucination by teaching the model to "forget" knowledge outside the character's identity.
- Mechanism: A small set of adversarial scenes where the character is prompted about modern concepts they cannot know. The model learns to express ignorance rather than produce plausible-sounding but anachronistic responses.
- Core assumption: The protective scenes generalize to new, unseen out-of-scope prompts.
- Evidence anchors:
  - [abstract] "Further, as trained with wide worldwide knowledge, it is very likely that LLM-based agents will produce hallucinations that violate their characters."
  - [section] "we introduce protective experiences that help Character-LLMs to align to their characters rather than worldwide knowledge."
- Break condition: If the protective scenes are too few or too generic, the model may still default to its pre-trained knowledge on unseen prompts.

### Mechanism 3
- Claim: Supervised fine-tuning on reconstructed scenes yields agents with strong personality consistency and memory retention, even with limited data.
- Mechanism: Each character's fine-tuning dataset consists only of their own reconstructed scenes, avoiding cross-character knowledge collision and enabling focused personality alignment.
- Core assumption: Even a small dataset (~1-2K scenes) is sufficient to capture and generalize the character's behavioral style.
- Evidence anchors:
  - [abstract] "Experimental results show interesting observations that help build future simulacra of humankind."
  - [section] "Even though the data is limited, we are surprised to find that the specialized agents are capable of generalizing to new scenes and interactions with highly believable acting."
- Break condition: If the scenes are too homogeneous, the model may overfit to a narrow range of behaviors and fail to generalize to novel interview contexts.

## Foundational Learning

- Concept: Supervised fine-tuning of pre-trained LLMs
  - Why needed here: To specialize a general LLM into a character-specific persona by learning from curated experience scenes.
  - Quick check question: What changes in the model's weights after fine-tuning compared to the base LLM?

- Concept: Role-based data curation
  - Why needed here: To ensure training examples are tightly aligned with the target character's identity, avoiding cross-contamination from other personas.
  - Quick check question: How does limiting training data to a single character's scenes help reduce Character Hallucination?

- Concept: Adversarial prompting for hallucination mitigation
  - Why needed here: To teach the model to withhold knowledge that the character could not plausibly possess, preserving believability.
  - Quick check question: What is the effect of including protective scenes on the model's responses to out-of-scope questions?

## Architecture Onboarding

- Component map:
  - Profile Collection -> Scene Extraction -> Experience Completion -> Experience Upload -> Protective Experience Upload -> Base LLaMA 7B -> Character-specific fine-tuned models

- Critical path:
  1. Collect profile -> 2. Generate scenes -> 3. Fine-tune base model -> 4. Validate via interview

- Design tradeoffs:
  - Limited scene data vs. richer persona representation
  - Protective scenes vs. maintaining general knowledge utility
  - Fine-tuning scale vs. training cost and overfitting risk

- Failure signatures:
  - Model produces anachronistic knowledge -> missing or ineffective protective scenes
  - Character responses lack depth -> insufficient or unrepresentative experience scenes
  - Hallucinations persist across characters -> inadequate separation of training datasets

- First 3 experiments:
  1. Train a single character with only profile-based scenes; evaluate memorization vs. baseline.
  2. Add protective scenes; measure reduction in anachronistic responses.
  3. Train with cross-character data; observe performance degradation in personality consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are Character-LLM agents at maintaining character-specific knowledge across diverse and unexpected topics?
- Basis in paper: [explicit] The paper discusses the issue of character hallucination, where agents produce knowledge outside their character's era or identity, and introduces protective experiences to mitigate this.
- Why unresolved: The paper's evaluation focuses on specific scenarios and does not comprehensively test agents' responses to a wide range of unexpected topics.
- What evidence would resolve it: Testing agents with a broad set of unexpected questions and measuring the frequency and severity of character hallucinations.

### Open Question 2
- Question: How does the scale and diversity of training data affect the believability and performance of Character-LLM agents?
- Basis in paper: [inferred] The paper mentions using approximately 1K-2K scenes for training, but does not explore the impact of varying data scale or diversity on agent performance.
- Why unresolved: The paper does not provide a systematic study of how different amounts or types of training data influence the agents' ability to simulate characters.
- What evidence would resolve it: Conducting experiments with varying sizes and diversity of training datasets and comparing the resulting agents' performance.

### Open Question 3
- Question: How stable are Character-LLM agents during prolonged interactions, and how do they handle complex multi-turn dialogues?
- Basis in paper: [explicit] The paper introduces multi-turn interviews to test stability but acknowledges that models may deviate from character portrayal over time.
- Why unresolved: The evaluation does not provide a detailed analysis of agent stability in extended interactions or complex dialogue scenarios.
- What evidence would resolve it: Conducting long-duration, multi-turn interviews with agents and analyzing their consistency and stability over time.

### Open Question 4
- Question: How do Character-LLM agents compare to other methods of character simulation, such as prompt-based approaches or hybrid systems with external memory?
- Basis in paper: [explicit] The paper compares Character-LLM to Alpaca, Vicuna, and ChatGPT but does not explore hybrid approaches or more advanced simulation methods.
- Why unresolved: The evaluation is limited to comparing different LLM-based methods and does not consider alternative character simulation techniques.
- What evidence would resolve it: Implementing and testing hybrid systems that combine Character-LLM with external memory or other character simulation methods, and comparing their performance to Character-LLM alone.

## Limitations

- The profile-to-scene reconstruction pipeline's quality assurance is not fully specified, raising concerns about handling sparse or contradictory profile information
- Limited empirical evidence on how many protective scenes are needed or how well they generalize across different types of out-of-scope questions
- Evaluation methodology depends on LLM judges (ChatGPT) without cross-validation from human evaluators, raising concerns about potential judge bias

## Confidence

- **High confidence**: The core feasibility of fine-tuning LLaMA 7B on character-specific experience data to produce personality-consistent agents is supported by the experimental results and aligns with established supervised fine-tuning practices.
- **Medium confidence**: The protective experience mechanism is theoretically sound and addresses a recognized problem in character LLMs, but the paper lacks comprehensive ablation studies to quantify its effectiveness compared to models without protective training.
- **Low confidence**: The claim that 1-2K scenes are sufficient to capture a character's full behavioral range is not empirically validated, and there is no analysis of how training data size affects generalization to novel interview contexts.

## Next Checks

1. **Ablation study on protective experiences**: Train otherwise identical models with and without protective scenes, then systematically evaluate their responses to anachronistic questions (e.g., asking Cleopatra about smartphones) to measure the quantitative reduction in Character Hallucination.
2. **Human evaluation cross-validation**: Recruit human evaluators to rate the same interview transcripts used in the original LLM judge evaluation, comparing human vs. ChatGPT scores for personality consistency and hallucination resistance to assess potential judge bias.
3. **Training data size sensitivity analysis**: Train models on varying amounts of experience data (e.g., 500, 1000, 2000, 5000 scenes) for the same character and measure how memorization, personality consistency, and generalization performance scale with dataset size.