---
ver: rpa2
title: Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM
arxiv_id: '2309.14348'
source_url: https://arxiv.org/abs/2309.14348
tags:
- adversarial
- aligned
- attack
- harmful
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Robustly Aligned LLM (RA-LLM) to defend against
  alignment-breaking attacks in large language models. The method builds on an existing
  aligned LLM by adding a robust alignment checking function that randomly drops portions
  of the input and checks if the model still detects harmful content.
---

# Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM

## Quick Facts
- arXiv ID: 2309.14348
- Source URL: https://arxiv.org/abs/2309.14348
- Authors: 
- Reference count: 22
- Key outcome: RA-LLM reduces attack success rate from nearly 100% to around 10% or less while minimally affecting benign responses.

## Executive Summary
This paper proposes a Robustly Aligned LLM (RA-LLM) to defend against alignment-breaking attacks in large language models. The method builds on an existing aligned LLM by adding a robust alignment checking function that randomly drops portions of the input and checks if the model still detects harmful content. This makes the alignment checking more robust to adversarial prompts. Theoretical analysis and experiments show that RA-LLM can significantly reduce attack success rates while maintaining high benign answering rates.

## Method Summary
The method constructs a robustly aligned LLM by adding a robust alignment checking function to an existing aligned LLM. The key innovation is random token dropping: before checking if a request is harmful, the defense randomly drops a portion of tokens and verifies if the LLM still detects harmful content. This is repeated multiple times using Monte Carlo sampling to estimate the probability of harmful content detection. If the LLM consistently flags the content as harmful even after random dropping, the request is rejected. The method requires no retraining and works as a wrapper around existing aligned models.

## Key Results
- Reduces attack success rate from nearly 100% to around 10% or less
- Maintains benign answering rate with only 3% reduction
- Effective against both AdvBench adversarial prompts and handcrafted jailbreaking prompts
- Demonstrates robustness even when attackers increase prompt length to evade dropping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random token dropping breaks the structure of adversarial prompts.
- Mechanism: The defense randomly drops a portion of input tokens before checking alignment. Since adversarial prompts are carefully crafted sequences that rely on specific token positions, dropping tokens disrupts their effectiveness.
- Core assumption: Adversarial prompts are position-sensitive and fragile to token-level perturbations.
- Evidence anchors:
  - [abstract]: "Our key idea is that although an aligned LLM can, to some extent, identify if the input request is benign or not, we cannot directly rely on that as it may not be robust."
  - [section 3.2]: "We consider an input request to be benign, only if we randomly drop a certain portion of the request and the LLM still thinks it is benign in most cases."
- Break condition: If adversarial prompts are designed to be robust to token dropping or if they use repetitive patterns that survive random sampling.

### Mechanism 2
- Claim: Monte Carlo sampling approximates the robustness of alignment checking.
- Mechanism: By repeatedly sampling different token-dropping masks and checking if the LLM still flags harmful content, the defense estimates the probability that the input is genuinely benign.
- Core assumption: Multiple random samplings can statistically approximate the true probability of harmful content detection.
- Evidence anchors:
  - [section 3.3]: "Therefore in practice, we conduct Monte Carlo sampling to approximate the true probability: we randomly sample n indices masks to obtain n versions of the input request with random dropping."
  - [section 3.3]: "It is practically infeasible to obtain the exact value for the probability of Pr∼U(p)(AC(f([x]r)) = Fail), as it would require us to enumerate all possible random dropping cases and is computationally intractable."
- Break condition: If the number of Monte Carlo trials is too low to capture the variance in random dropping outcomes.

### Mechanism 3
- Claim: Threshold tuning balances false positives and false negatives.
- Mechanism: The defense uses a threshold t on the fraction of dropped samples that must fail alignment checking to reject the input. This allows some benign requests with dropped tokens to still pass while catching adversarial ones.
- Core assumption: Benign requests are more likely to survive random dropping than adversarial ones.
- Evidence anchors:
  - [section 3.3]: "We keep a relatively small threshold" and "if (1/n)Pn i=1 si > t then Reject the request."
  - [section 3.4]: "If N ≥ M (1−p) p and min j Pr∼U(p)(AC(f([xj pad]r)) = Fail) > t + c, where c = 1−( N (N +M )(1−p) )( N +M (N +M )(1−p) ) and t is the threshold used in Algorithm 1, then our robustly aligned LLM in Algorithm 1 with sufficiently large random drop trials n will reject the request on xadv = x ⊕ padv."
- Break condition: If threshold is set too high, it may let adversarial prompts through; if too low, it may reject benign requests.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding how LLMs are aligned with human values is critical to grasp why alignment can be bypassed.
  - Quick check question: How does RLHF differ from supervised fine-tuning in aligning LLMs?

- Concept: Adversarial examples and perturbations
  - Why needed here: The paper's defense is built on the idea that adversarial prompts are fragile to small changes, a core concept in adversarial machine learning.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks in the context of LLMs?

- Concept: Monte Carlo sampling
  - Why needed here: The defense uses repeated random sampling to estimate probabilities, a statistical technique.
  - Quick check question: How does increasing the number of Monte Carlo samples affect the variance of the estimate?

## Architecture Onboarding

- Component map:
  - Aligned LLM (f) -> Alignment Check Function (AC) -> Robust Alignment Check (RAC) -> Input Pipeline -> Output Filter

- Critical path:
  1. Input request arrives
  2. Original alignment check AC(f(x)) is run
  3. If AC passes, n random masks are generated and applied
  4. For each masked input, f is called and AC is applied to the output
  5. If the fraction of fails exceeds threshold t, reject; else, accept the original output

- Design tradeoffs:
  - More Monte Carlo trials (n) → higher robustness but higher latency
  - Higher dropping ratio (p) → stronger defense but risk of breaking benign inputs
  - Lower threshold (t) → stricter rejection but higher false positive rate

- Failure signatures:
  - High benign answering rate drop → threshold too low or dropping ratio too high
  - High attack success rate → threshold too high or insufficient Monte Carlo trials
  - Slow response times → excessive Monte Carlo sampling or large input length

- First 3 experiments:
  1. Test with varying p (0.1, 0.3, 0.5) on a fixed benign dataset to measure BAR degradation
  2. Test with varying n (5, 10, 20) on adversarial prompts to measure ASR reduction
  3. Sweep t from 0.0 to 0.5 and plot BAR vs ASR trade-off curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would this approach be against more sophisticated adaptive attacks that account for the random dropping mechanism?
- Basis in paper: [explicit] The paper discusses potential adaptive attacks where attackers increase the length of adversarial prompts to ensure they are not fully removed by random dropping.
- Why unresolved: The paper only provides preliminary results showing that even with repeated adversarial prompts, the defense method keeps the attack success rate low, but it does not explore the full range of possible adaptive attacks.
- What evidence would resolve it: Comprehensive experiments testing the defense against a wide variety of adaptive attacks, including those that dynamically adjust based on the defense's behavior.

### Open Question 2
- Question: What is the optimal balance between the random dropping ratio (p) and the threshold (t) to maximize both defense effectiveness and benign answering rate?
- Basis in paper: [inferred] The paper discusses the effects of different values of p and t in the ablation study, but does not provide a clear guideline for finding the optimal balance.
- Why unresolved: The optimal values of p and t likely depend on the specific LLM and the nature of the attacks, and the paper does not explore this in sufficient detail.
- What evidence would resolve it: Systematic experiments varying p and t across different LLMs and attack types to identify general trends and optimal configurations.

### Open Question 3
- Question: How does the computational overhead of the robustly aligned LLM scale with larger models and more complex tasks?
- Basis in paper: [explicit] The paper provides a theoretical analysis of the computational cost, but only for a specific example with limited parameters.
- Why unresolved: The computational cost analysis is based on a single example and does not consider the scaling effects with larger models or more complex tasks.
- What evidence would resolve it: Detailed experiments measuring the computational overhead of the robustly aligned LLM across a range of model sizes and task complexities.

## Limitations

- The defense assumes attackers cannot adapt their adversarial prompts to survive random token dropping
- Evaluation focuses on specific datasets and attack types, limiting generalizability
- Computational overhead increases with larger Monte Carlo sampling, potentially affecting real-time applications

## Confidence

**High confidence**: The core mechanism of using random token dropping to disrupt adversarial prompt structure is sound and well-grounded in adversarial machine learning literature. The statistical foundation of Monte Carlo sampling for probability estimation is well-established.

**Medium confidence**: The experimental results showing 10% ASR reduction appear robust for the tested scenarios, but the exact hyperparameter settings and their sensitivity are not fully disclosed, making it difficult to assess reproducibility.

**Low confidence**: The claim that the method "minimally affects benign responses" is supported by the 3% BAR reduction shown in experiments, but the long-term effects on user experience and potential for increased latency with larger Monte Carlo sampling are not fully characterized.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the dropping ratio (p ∈ {0.1, 0.3, 0.5}), threshold (t ∈ {0.1, 0.3, 0.5}), and Monte Carlo trials (n ∈ {5, 10, 20}) to create a comprehensive sensitivity heatmap showing the trade-off between ASR and BAR across all parameter combinations.

2. **Adaptive adversary test**: Design a second-round attack where the adversary is informed about the random dropping defense and attempts to craft prompts that survive the perturbation (e.g., using repetitive patterns, redundant information). Measure whether ASR increases significantly compared to the baseline attack.

3. **Real-world deployment simulation**: Test the defense on a curated dataset of actual user jailbreaking attempts collected from production LLM deployments, including both successful and unsuccessful attempts. Measure not just ASR and BAR, but also average response latency and false positive rates on edge cases.