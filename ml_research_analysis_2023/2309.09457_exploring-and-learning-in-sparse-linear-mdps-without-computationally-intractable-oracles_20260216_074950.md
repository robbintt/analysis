---
ver: rpa2
title: Exploring and Learning in Sparse Linear MDPs without Computationally Intractable
  Oracles
arxiv_id: '2309.09457'
source_url: https://arxiv.org/abs/2309.09457
tags:
- policy
- lemma
- algorithm
- nition
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of learning in sparse linear Markov\
  \ Decision Processes (MDPs) without computationally intractable oracles. The key\
  \ assumption is that the learner has access to a known feature map \u03C6(x,a) mapping\
  \ state-action pairs to d-dimensional vectors, with rewards and transitions being\
  \ linear functions in this representation."
---

# Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles

## Quick Facts
- **arXiv ID**: 2309.09457
- **Source URL**: https://arxiv.org/abs/2309.09457
- **Reference count**: 40
- **One-line primary result**: First polynomial-time algorithm for learning near-optimal policies in sparse linear MDPs without computationally intractable oracles

## Executive Summary
This paper introduces the first polynomial-time algorithm for learning near-optimal policies in sparse linear Markov Decision Processes (MDPs) without requiring computationally intractable optimization oracles. The key innovation is the introduction of emulators - succinct approximate representations of transitions that can be computed efficiently via convex programming. The algorithm, called POEM (Policy Optimization with Emulator-driven Exploration), constructs a small policy cover for the MDP using these emulators to guide exploration, achieving polynomial sample and time complexity.

## Method Summary
The POEM algorithm learns near-optimal policies in sparse linear MDPs by constructing emulators via convex programming and using them to build policy covers through greedy coverage. The method assumes known feature maps with linear reward and transition structures, and handles both reachable and general (non-reachable) MDPs through truncated MDP analysis. The algorithm consists of two main phases: constructing policy covers at each step using emulators, and then using these covers to find a near-optimal policy via PSDP. The approach significantly improves upon prior methods by avoiding computationally intractable optimization while maintaining polynomial complexity.

## Key Results
- First polynomial-time algorithm for sparse linear MDPs without computationally intractable oracles
- Sample complexity: poly(k, log d) where k is sparsity and d is feature dimension
- Time complexity: poly(d, log d)
- Extension to block MDPs with low-depth decision tree decoding functions in quasi-polynomial time

## Why This Works (Mechanism)

### Mechanism 1 - Emulator Construction via Convex Programming
The paper constructs emulators by solving a convex feasibility program that finds vectors satisfying bounded ℓ1 norm, approximate non-negativity under all policies, and approximate representation of true transition dynamics. The convex program is made feasible using importance-weighted resampling based on the current policy cover. The key insight is that polynomial-sized emulators exist and can be computed efficiently, providing a tractable alternative to computationally intractable optimization oracles.

### Mechanism 2 - Policy Cover Construction via Greedy Coverage
A small policy cover is constructed by iteratively selecting policies that maximize coverage of uncovered state-action pairs using emulator vectors as proxies for true transitions. The algorithm maintains uncovered emulator vectors and repeatedly computes policies optimizing in their directions. The emulator vectors must satisfy approximate non-negativity for all policies and have bounded total ℓ1 norm, ensuring the greedy coverage strategy produces a compact policy cover that covers the true MDP's states in proportion to their maximum visitation probability.

### Mechanism 3 - Truncated MDP Analysis for General Case
The algorithm handles general (non-reachable) linear MDPs by analyzing truncated MDPs that remove hard-to-reach states. Transitions to states with low visitation probability under any policy are redirected to a terminal state, creating a sequence of truncated MDPs M(Γ) used to define the emulator and policy cover. This ensures error terms remain bounded by the truncation threshold σtrunc, allowing the policy cover construction to remain tractable even when the original MDP is not reachable.

## Foundational Learning

- **Concept: Linear MDPs and Feature Maps**
  - Why needed here: The algorithm relies on rewards and transitions being linear functions of a known feature map φ(x,a). Understanding this structure is crucial for both emulator construction and policy optimization.
  - Quick check question: Given state x and action a, can you compute φ(x,a) and explain how it's used to compute expected rewards and transitions?

- **Concept: Sparse Linear Regression and Lasso**
  - Why needed here: The algorithm uses ℓ1-regularized regression (Lasso) to estimate emulator vectors and value functions. Understanding Lasso's statistical properties is essential for analyzing sample complexity.
  - Quick check question: Given n samples (xi, yi) where yi = ⟨wi, xi⟩ + noise, can you explain how Lasso estimates w and what prediction error bounds depend on?

- **Concept: Convex Programming and Ellipsoid Method**
  - Why needed here: Emulator construction relies on solving a convex feasibility program. Understanding the ellipsoid method's polynomial-time guarantees for convex programs with polynomially many constraints is crucial for analyzing computational complexity.
  - Quick check question: Given a convex set defined by polynomially many linear and second-order cone constraints, can you explain how the ellipsoid method finds a point in the set or certifies infeasibility in polynomial time?

## Architecture Onboarding

- **Component map**: EstEmulator/EstTruncEmulator -> GreedyCover -> PSDP -> FeatureEstimation
- **Critical path**: 1) Initialize policy covers Ψ1, Ψ2 with uniform policies, 2) For each step h, construct emulator using EstEmulator/EstTruncEmulator, 3) Use GreedyCover with emulator to extend policy covers to step h+2, 4) Repeat until covers for all H steps are constructed, 5) Use PSDPrew with all policy covers to find near-optimal policy
- **Design tradeoffs**: Sample complexity vs. computational complexity (increasing emulator vectors m improves approximation but increases convex program size), approximation parameters (εapx, εneg) vs. coverage quality, truncation threshold σtrunc vs. reachability
- **Failure signatures**: EstEmulator returns ⊥ (convex program infeasible), GreedyCover produces small covers (coverage parameter ξ too small), PSDP produces suboptimal policies (truncation threshold σtrunc too large)
- **First 3 experiments**: 1) Run ExploreRchLMDP on a simple reachable k-sparse linear MDP and verify α-approximate policy cover construction, 2) Test EstEmulator on fixed policy cover and verify output satisfies three properties, 3) Verify GreedyCover with valid emulator produces policy cover covering true MDP states in proportion to maximum visitation probability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the time complexity of learning near-optimal policies in decision tree block MDPs be improved from quasi-polynomial to polynomial?
- Basis in paper: [explicit] The paper states that improving sample complexity via representation learning is computationally feasible for decision tree block MDPs, but the current algorithm runs in quasi-polynomial time.
- Why unresolved: Achieving polynomial time would require improving the best-known time bound for learning depth-log(s) decision trees with stochastic noise, which is a challenging open problem in computational learning theory.
- What evidence would resolve it: A new algorithm for learning decision trees with stochastic noise achieving polynomial time complexity, or a proof that this is computationally hard.

### Open Question 2
- Question: Are there other well-motivated classes of block MDPs beyond decision trees for which computationally efficient learning algorithms exist?
- Basis in paper: [explicit] The paper poses this as a main question and discusses the challenge of finding computationally tractable models for reinforcement learning.
- Why unresolved: While the paper provides an efficient algorithm for decision tree block MDPs, it remains unclear whether other natural classes of block MDPs admit similar efficient algorithms.
- What evidence would resolve it: Identification of other function classes for decoding functions in block MDPs that admit efficient learning algorithms, or proofs of computational hardness for other classes.

### Open Question 3
- Question: Can the techniques developed for sparse linear MDPs be extended to learn in more general function approximation settings while maintaining computational efficiency?
- Basis in paper: [inferred] The paper's techniques involve introducing emulators and using convex programming, which could potentially be adapted to other function approximation settings.
- Why unresolved: The paper focuses on sparse linear MDPs and does not explore the applicability of its techniques to more general settings.
- What evidence would resolve it: Extension of the emulator-based approach to other function approximation settings, or a proof that such extension is not possible while maintaining computational efficiency.

## Limitations

- Strong assumption that rewards and transitions are linear functions of known feature maps may not hold in real-world environments
- Convex program for emulator construction may be computationally expensive in practice despite polynomial-time guarantees
- Greedy coverage using emulator vectors may accumulate approximation errors, especially with limited emulator vector counts
- Extension to block MDPs with decision tree decoding functions achieves only quasi-polynomial time complexity

## Confidence

**High Confidence Claims:**
- Existence of polynomial-sized emulators computable via convex programming
- Sample complexity of poly(k, log d)
- Time complexity of poly(d, log d)

**Medium Confidence Claims:**
- Policy cover construction via greedy coverage using emulators
- Truncated MDP analysis for general linear MDPs
- Extension to block MDPs with low-depth decision tree decoding functions

**Low Confidence Claims:**
- Practical performance on real-world problems given strong theoretical assumptions
- Computational efficiency of convex program in practice
- Robustness to violations of linear MDP assumption or poor feature map quality

## Next Checks

1. **Emulator Construction Feasibility**: Implement EstTruncEmulator on a simple linear MDP with known transitions and verify the convex program returns a feasible solution satisfying bounded norm, approximate non-negativity, and good approximation of true transitions. Measure runtime and compare against theoretical polynomial bound.

2. **Policy Cover Coverage Quality**: Run GreedyCover on a reachable linear MDP and measure coverage achieved by constructed policy cover. Compare against theoretical guarantee that policy cover covers states in proportion to maximum visitation probability under any policy. Test with different emulator vector counts m to understand tradeoff between coverage quality and computational complexity.

3. **Algorithm Performance on Block MDPs**: Implement extension to block MDPs with low-depth decision tree decoding functions and test on synthetic block MDP where true decoding function is known. Measure sample complexity and time complexity, compare learned policy's performance against optimal policy, and verify quasi-polynomial time complexity holds in practice.