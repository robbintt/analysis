---
ver: rpa2
title: Mitigating Open-Vocabulary Caption Hallucinations
arxiv_id: '2312.03631'
source_url: https://arxiv.org/abs/2312.03631
tags:
- image
- captioning
- captions
- hallucinations
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MOCHa, a reinforcement learning framework
  that mitigates open-vocabulary hallucinations in image captioning by jointly optimizing
  fidelity and adequacy objectives. The method uses a multi-objective reward function
  combining NLI-based fidelity, BERTScore-based adequacy, and KL regularization, optimized
  via PPO.
---

# Mitigating Open-Vocabulary Caption Hallucinations

## Quick Facts
- **arXiv ID:** 2312.03631
- **Source URL:** https://arxiv.org/abs/2312.03631
- **Reference count:** 40
- **One-line primary result:** MOCHa improves image captioning models on both standard metrics and hallucination metrics by jointly optimizing fidelity and adequacy via RL.

## Executive Summary
This paper addresses the problem of open-vocabulary hallucinations in image captioning, where models generate captions describing objects not present in the image. The authors propose MOCHa, a reinforcement learning framework that jointly optimizes fidelity (image consistency) and adequacy (descriptiveness) using a multi-objective reward function. The method uses NLI-based fidelity, BERTScore-based adequacy, and KL regularization, optimized via PPO. To evaluate hallucinations beyond COCO's fixed vocabulary, the authors introduce OpenCHAIR, a synthetic benchmark constructed using LLMs and text-to-image models.

## Method Summary
MOCHa applies reinforcement learning to image captioning by defining a multi-objective reward function combining NLI-based fidelity, BERTScore-based adequacy, and KL regularization. The method fine-tunes pre-trained captioning models (e.g., BLIP-Large, BLIP-2) using Proximal Policy Optimization (PPO) with nucleus sampling for caption generation. The reward function balances logical consistency with ground-truth captions (via NLI) and descriptive quality (via BERTScore), while KL regularization prevents mode collapse. The approach is evaluated on MS-COCO Karpathy splits and a new OpenCHAIR benchmark for open-vocabulary hallucination detection.

## Key Results
- MOCHa improves standard captioning metrics (BLEU-4, CIDEr, SPICE) while reducing hallucinations on both CHAIR and OpenCHAIR benchmarks.
- Ablation studies show that removing either fidelity or adequacy reward terms degrades performance, demonstrating the importance of joint optimization.
- The method outperforms prior approaches like TLC-A on open-vocabulary hallucination metrics while maintaining or improving general caption quality.
- MOCHa preserves caption diversity through KL regularization, avoiding the repetitive outputs seen in single-objective optimization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of fidelity and adequacy prevents overfitting to either metric alone, avoiding hallucinations or bland outputs.
- Mechanism: Multi-objective RL with weighted reward terms finds a Pareto-optimal trade-off between logical consistency (NLI-based fidelity) and descriptive richness (BERTScore-based adequacy).
- Core assumption: Fidelity and adequacy are complementary objectives that can be balanced via RL without ground-truth hallucination labels.
- Evidence anchors:
  - [abstract] "multi-objective reward function explicitly targets the trade-off between fidelity and adequacy"
  - [section 3.2] "jointly optimized at the sequence-level by applying RL with a multi-objective reward function"
- Break condition: If either reward term dominates due to poor weighting, the system reverts to problems seen in ablation studies (hallucinations or generic text).

### Mechanism 2
- Claim: KL regularization preserves diversity and prevents policy divergence from the pretrained model.
- Mechanism: KL divergence penalty constrains the learned policy to stay close to the initial captioning model, limiting generation of overly generic or repetitive captions.
- Core assumption: The pretrained model provides a reasonable prior for caption diversity that should not be discarded during RL fine-tuning.
- Evidence anchors:
  - [section 3.2] "constrains the agent to stay close to its initial policy π0"
  - [section 3.3] "prevent mode collapse (i.e. preserving diversity of outputs)"
- Break condition: If KL coefficient β is too low, the model may over-optimize fidelity/adequacy and produce repetitive or degenerate captions.

### Mechanism 3
- Claim: OpenCHAIR provides comprehensive evaluation of hallucinations by including open-vocabulary objects beyond COCO's fixed set.
- Mechanism: Synthetic captions with diverse objects are paired with generated images, and LLM-based evaluation identifies hallucinations by checking if predicted objects appear in ground-truth captions.
- Core assumption: LLM-based object detection in captions is sufficiently accurate to serve as reliable hallucination benchmark.
- Evidence anchors:
  - [section 4] "constructed automatically using LLMs and powerful text-to-image generation models"
  - [section 5.1] "OpenCHAIR extracts concrete objects from a predicted caption and identifies hallucinated objects from this list by querying an LLM"
- Break condition: If LLM evaluation is inconsistent or biased, OpenCHAIR scores may not reliably reflect actual hallucination rates.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: MOCHa applies RLHF-like fine-tuning to image captioning, replacing human feedback with automated reward signals.
  - Quick check question: How does the KL penalty in MOCHa differ from the KL penalty typically used in LLM alignment?

- Concept: Sequence-level optimization vs. token-level likelihood
  - Why needed here: Traditional image captioning optimizes token likelihoods, which does not directly minimize hallucinations; RL enables global sequence optimization.
  - Quick check question: Why does optimizing for NLI contradiction probability help reduce hallucinations more directly than token-level likelihood?

- Concept: Natural Language Inference (NLI) for image-grounded consistency
  - Why needed here: NLI models assess whether generated captions logically contradict ground-truth captions, serving as proxy for fidelity to the image.
  - Quick check question: What limitation does NLI-based fidelity have in detecting all types of hallucinations?

## Architecture Onboarding

- Component map:
  Image encoder (frozen) -> Text decoder (optimized via PPO) -> Multi-objective reward function (fidelity + adequacy + KL) -> LLM-based hallucination evaluator (OpenCHAIR) -> PPO optimizer with clipping

- Critical path:
  1. Generate captions for batch of images using current policy
  2. Compute reward scores using NLI, BERTScore, and KL terms
  3. Update policy parameters via PPO with clipped surrogate objective
  4. Repeat until convergence

- Design tradeoffs:
  - Fidelity vs. adequacy: Higher fidelity reduces hallucinations but may hurt detail; adequacy increases richness but risks hallucinations.
  - KL penalty strength: Higher β preserves diversity but may limit performance gains; lower β risks mode collapse.
  - OpenCHAIR vs. CHAIR: OpenCHAIR covers more objects but relies on LLM evaluation, which may be less reliable than fixed vocabularies.

- Failure signatures:
  - Over-optimization of fidelity → bland, generic captions
  - Over-optimization of adequacy → increased hallucinations
  - Insufficient KL penalty → repetitive or degenerate outputs
  - Poor reward weighting → imbalance between objectives

- First 3 experiments:
  1. Run MOCHa with only the fidelity reward (α=1) and observe if hallucinations decrease but caption quality degrades.
  2. Run MOCHa with only the adequacy reward (α=0) and observe if caption quality improves but hallucinations increase.
  3. Tune the KL penalty β and evaluate diversity and stability across different values.

## Open Questions the Paper Calls Out
- Question: How does the OpenCHAIR benchmark compare to human-annotated open-vocabulary hallucination datasets in terms of accuracy and diversity?
- Question: Can the MOCHa framework be effectively extended to other vision-language tasks beyond image captioning, such as visual question answering (VQA) or visual instruction following?
- Question: How does the choice of reward function components (fidelity, adequacy, KL penalty) in MOCHa impact the trade-off between hallucination reduction and caption quality?

## Limitations
- The evaluation relies heavily on automated LLM-based detection via OpenCHAIR, which is not validated against human judgments or provided with uncertainty estimates.
- OpenCHAIR is synthetically generated and may not fully capture the distribution of real-world open-vocabulary hallucinations.
- The paper lacks direct diversity metrics to quantify the effect of KL regularization on caption diversity.

## Confidence
- **High Confidence:** MOCHa improves both standard captioning metrics and hallucination metrics, with robust improvement over prior methods on open-vocabulary metrics.
- **Medium Confidence:** The assertion that MOCHa preserves caption diversity via KL regularization is plausible but lacks direct diversity measurements.
- **Low Confidence:** The reliability of OpenCHAIR as a comprehensive hallucination benchmark is uncertain due to its synthetic nature and dependence on LLM evaluation.

## Next Checks
1. Validate OpenCHAIR accuracy by conducting human evaluation study comparing LLM-based hallucination detection against human judgments.
2. Measure diversity preservation by computing direct diversity metrics (distinct-1, distinct-2, self-BLEU) for MOCHa outputs versus baselines.
3. Perform sensitivity analysis of reward weights by exploring different fidelity/adequacy reward weights (α) and KL penalty coefficient (β) values.