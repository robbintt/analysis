---
ver: rpa2
title: Learning Curricula in Open-Ended Worlds
arxiv_id: '2312.03126'
source_url: https://arxiv.org/abs/2312.03126
tags:
- learning
- training
- levels
- environment
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis develops unsupervised environment design (UED) methods
  for generating open-ended curricula in reinforcement learning. It introduces prioritized
  level replay (PLR), which selectively samples training environments based on estimated
  learning potential, improving sample efficiency and generalization.
---

# Learning Curricula in Open-Ended Worlds

## Quick Facts
- arXiv ID: 2312.03126
- Source URL: https://arxiv.org/abs/2312.03126
- Reference count: 0
- This thesis develops unsupervised environment design methods for generating open-ended curricula in reinforcement learning, introducing prioritized level replay (PLR) and extending it with evolutionary search (ACCEL).

## Executive Summary
This thesis addresses the challenge of curriculum learning in open-ended environments by developing unsupervised environment design (UED) methods that generate adaptive training curricula. The work introduces prioritized level replay (PLR), which selectively samples training environments based on estimated learning potential using temporal-difference errors. It then provides theoretical foundations through dual curriculum design (DCD), showing PLR is a special case with minimax regret guarantees. The approach extends to complex environment spaces with ACCEL, which uses evolutionary search to discover high-regret levels, and addresses curriculum-induced covariate shift with sample-matched PLR (SAMPLR).

## Method Summary
The thesis develops a progression of curriculum learning methods starting with PLR, which maintains a replay distribution over levels prioritized by estimated learning potential. DCD provides a game-theoretic framework modeling curriculum learning as interactions between a student agent and two teachers (generator and curator). ACCEL extends PLR by replacing random search with evolutionary search to discover high-regret levels through small mutations. SAMPLR addresses covariate shift by matching training and test level distributions. The methods are evaluated across navigation, car racing, and locomotion tasks, demonstrating improved zero-shot transfer and robustness.

## Key Results
- PLR improves sample efficiency and generalization by selectively sampling levels with high estimated learning potential
- DCD provides theoretical foundations with minimax regret guarantees, showing PLR is a special case
- ACCEL extends PLR with evolutionary search for complex environment spaces
- Empirical results demonstrate significant improvements in zero-shot transfer and robustness across multiple task domains

## Why This Works (Mechanism)

### Mechanism 1: Prioritized Level Replay (PLR) - Selective Sampling Based on Learning Potential
PLR improves sample efficiency by maintaining a replay distribution that prioritizes levels with higher estimated learning potential based on TD errors. After each episode, it updates scores for levels based on policy and TD errors, sampling from a mixture of score-prioritized and staleness-prioritized distributions.

- Core assumption: TD errors correlate with learning potential
- Evidence anchors: Abstract states PLR improves sample efficiency; section describes score updates based on TD errors
- Break condition: Fails if TD errors don't correlate with learning (e.g., deceptive rewards or inaccurate value functions)

### Mechanism 2: Dual Curriculum Design (DCD) - Game-Theoretic Framework for UED
DCD models curriculum learning as a three-player game between student agent and two teachers (generator and curator), providing theoretical guarantees for autocurriculum methods.

- Core assumption: Nash equilibria of dual curriculum game correspond to approximate Nash equilibria of base game
- Evidence anchors: Abstract states DCD shows PLR is special case with minimax regret guarantees; section describes theoretical results
- Break condition: Fails if teacher strategies aren't independent or mixing probability is inappropriate

### Mechanism 3: Evolving Curricula (ACCEL) - Evolutionary Search for High-Regret Levels
ACCEL extends PLR by replacing random search with evolutionary search, viewing the replay buffer as a population and regret estimates as fitness scores.

- Core assumption: Regret varies smoothly with environment parameters
- Evidence anchors: Abstract mentions ACCEL extension; section describes mutation-based editing of high-regret levels
- Break condition: Fails in high-dimensional or non-convex regret landscapes

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - Why needed here: Provides formal framework for modeling sequential decision-making in RL
  - Quick check question: What are the key components of an MDP, and how do they relate to the RL problem?

- **Concept: Policy Gradient Methods**
  - Why needed here: Class of RL algorithms that directly optimize policy parameters to maximize expected return
  - Quick check question: How do policy gradient methods estimate the gradient of expected return with respect to policy parameters?

- **Concept: Game Theory and Nash Equilibria**
  - Why needed here: Provides framework for modeling interactions between student agent and teacher agents in curriculum learning
  - Quick check question: What is a Nash equilibrium, and how does it relate to optimal behavior in multi-agent settings?

## Architecture Onboarding

- **Component map**: Environment -> Agent -> Curriculum Learning Method -> Regret Estimator -> Level Replay Buffer

- **Critical path**: 1) Environment generates level; 2) Agent interacts and collects trajectory; 3) Regret estimator estimates regret; 4) Curriculum method updates replay buffer and selects next level; 5) Agent's policy is updated

- **Design tradeoffs**:
  - Exploration vs. Exploitation: Balancing discovering new high-regret levels vs. exploiting known ones
  - Random Search vs. Evolutionary Search: ACCEL is more effective but computationally expensive
  - Simplicity vs. Theoretical Guarantees: PLR is simpler but lacks DCD's theoretical guarantees

- **Failure signatures**:
  - High regret levels not being discovered (inaccurate regret estimator or complex environment space)
  - Agent overfitting to training levels (curriculum not diverse enough or policy too complex)
  - Agent's performance not improving (ineffective curriculum or non-converging RL algorithm)

- **First 3 experiments**:
  1. Implement PLR on simple PCG environment (grid world with procedurally generated mazes) vs. uniform sampling
  2. Implement DCD on same environment vs. PLR and uniform sampling
  3. Implement ACCEL on complex PCG environment (continuous control with procedurally generated terrains) vs. PLR and uniform sampling

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the robustness induced by Unsupervised Environment Design (UED) persist in sim2real transfer?
- Basis in paper: [explicit

## Limitations
- Theoretical foundations rely on strong assumptions about teacher independence and mixing probabilities
- Core mechanism (TD errors indicating learning potential) lacks rigorous theoretical justification
- Evolutionary search assumes regret varies smoothly with environment parameters, which may not hold in high-dimensional spaces

## Confidence

- **High Confidence**: Empirical results demonstrating PLR's improvements over uniform sampling across multiple tasks
- **Medium Confidence**: Theoretical claims about DCD providing minimax regret guarantees
- **Low Confidence**: Claims about ACCEL's effectiveness in complex environment spaces beyond tested domains

## Next Checks

1. Test DCD's theoretical guarantees under varying teacher independence assumptions and mixing probabilities
2. Conduct controlled experiments to verify whether TD errors actually correlate with learning potential across different agent architectures
3. Evaluate ACCEL on environments with provably high-dimensional or non-convex regret landscapes