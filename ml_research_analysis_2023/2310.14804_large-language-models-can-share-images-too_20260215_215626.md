---
ver: rpa2
title: Large Language Models can Share Images, Too!
arxiv_id: '2310.14804'
source_url: https://arxiv.org/abs/2310.14804
tags:
- image
- dialogue
- arxiv
- image-sharing
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the image-sharing capabilities of large language
  models (LLMs) like GPT-4 in a zero-shot setting. The authors propose a two-stage
  framework, Decide, Describe, and Retrieve (DribeR), that predicts potential image-sharing
  turns and generates relevant image descriptions using a restriction-based prompt
  template.
---

# Large Language Models can Share Images, Too!

## Quick Facts
- arXiv ID: 2310.14804
- Source URL: https://arxiv.org/abs/2310.14804
- Reference count: 35
- LLMs can perform image-sharing in dialogue through a two-stage framework without visual foundation models

## Executive Summary
This paper investigates whether large language models (LLMs) can share images in dialogue without relying on visual foundation models. The authors propose a two-stage framework called Decide, Describe, and Retrieve (DribeR) that first predicts when to share images by understanding dialogue context and interlocutor mental states, then generates relevant image descriptions using a restriction-based prompt template. Experiments on the PhotoChat++ dataset demonstrate that LLMs, particularly GPT-4, can effectively share images in a zero-shot setting, achieving significant improvements over random performance.

## Method Summary
The study employs a two-stage pipeline: (1) a turn prediction stage that uses restriction-based prompting to identify appropriate moments for image-sharing in dialogue, and (2) a description generation stage that produces relevant image descriptions conditioned on dialogue context. The framework is evaluated on the PhotoChat++ dataset using metrics like PRECISION @K, RECALL @K, F1@K for turn prediction and CLIPScore, NLI Score, and diversity metrics for image descriptions. The approach is validated through human-bot interaction experiments and dataset augmentation tasks.

## Key Results
- GPT-4 achieves the best performance in image-sharing tasks, significantly outperforming smaller LLMs
- Zero-shot performance improves with model scale, with models below 175B parameters performing at or below random chance
- The restriction-based prompt template enhances performance by providing explicit formatting instructions and content requirements
- The framework successfully augments the PhotoChat dataset to create PhotoChat++ with generated image descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform image-sharing behavior through a two-stage process (decide + describe) without visual foundation models
- Mechanism: The LLM first predicts appropriate image-sharing turns by understanding dialogue context and interlocutor mental states, then generates relevant image descriptions conditioned on the dialogue history
- Core assumption: LLMs trained on large text corpora have learned enough about human social interaction patterns to simulate image-sharing behavior
- Evidence anchors:
  - [abstract] "This work explores whether LLMs contain the image-sharing capability without the help of visual foundation models, primarily focusing on a zero-shot performance."
  - [section 2.3] "To elicit the image-sharing ability of LLM in a zero-shot setting, we manually construct a prompt template for both stages."
  - [corpus] Weak - No direct corpus evidence found that LLMs contain emergent image-sharing ability specifically
- Break condition: If the LLM fails to understand conversational context or mental states, the two-stage framework breaks down

### Mechanism 2
- Claim: The restriction-based prompt template with specific formatting instructions improves LLM performance on image-sharing tasks
- Mechanism: Adding explicit Restrictions: tokens and formatting requirements (e.g., "your answer should be in the format of '<UTTERANCE> | <CONFIDENCE> | <SPEAKER> | <RATIONALE>'") guides the LLM to produce structured, relevant outputs
- Core assumption: LLMs respond predictably to constraint-based prompting that explicitly defines output format and content requirements
- Evidence anchors:
  - [section 2.3] "By leveraging our framework and restrictions-based prompt, we test the capability of LLM on PhotoChat dataset"
  - [section 4.3] "Prompting without Restrictions... eliminating Restrictions: leads to a decrease in performance"
  - [corpus] Weak - No direct corpus evidence that this specific restriction-based template is effective
- Break condition: If the restrictions are too constraining or poorly formulated, they may limit the LLM's ability to generate creative or contextually appropriate responses

### Mechanism 3
- Claim: Image-sharing ability in LLMs is an emergent ability that appears at larger model scales
- Mechanism: As LLM scale increases (175B vs smaller models), zero-shot performance on image-sharing tasks improves significantly, following a scaling law pattern
- Core assumption: Larger LLMs have learned more nuanced social interaction patterns through pretraining, enabling emergent capabilities
- Evidence anchors:
  - [section 4.1] "Overall, the zero-shot performance of LLMs improves as the scale of LLMs increases"
  - [section 4.1] "while the zero-shot performance of InstructGPTs (< 175B) is under the random performance, scaling the size of LLMs... significantly improves the zero-shot performance"
  - [corpus] Weak - No direct corpus evidence that image-sharing is specifically an emergent ability in LLMs
- Break condition: If the scaling law doesn't hold or smaller models perform unexpectedly well, the emergent ability claim breaks down

## Foundational Learning

- Concept: Zero-shot learning with LLMs
  - Why needed here: The paper evaluates image-sharing capabilities without any task-specific training, relying solely on prompt engineering
  - Quick check question: What distinguishes zero-shot from few-shot prompting in the context of LLM evaluation?

- Concept: Prompt engineering and template design
  - Why needed here: The restriction-based prompt template is central to eliciting image-sharing behavior from LLMs
  - Quick check question: How do specific formatting restrictions in prompts influence LLM output quality?

- Concept: Multi-modal dialogue understanding
  - Why needed here: Image-sharing in dialogue requires understanding both conversational context and when images would be appropriate
  - Quick check question: What conversational cues typically indicate an appropriate moment for image-sharing?

## Architecture Onboarding

- Component map: Turn prediction → Description generation → Dataset augmentation → Evaluation
- Critical path: Prompt generation → LLM inference → Output parsing → Image generation (optional) → Evaluation
- Design tradeoffs: Zero-shot approach vs. fine-tuning; open-source vs. proprietary LLMs; single-stage vs. two-stage framework
- Failure signatures: Poor turn prediction leads to irrelevant images; weak descriptions produce inappropriate images; restriction violations cause format errors
- First 3 experiments:
  1. Test restriction-based prompt vs. standard prompt on a small dialogue sample
  2. Compare performance across different LLM scales (small vs. 175B)
  3. Validate output parsing with regex pattern on generated results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of the image-sharing ability on the overall performance of large language models in multi-modal dialogue tasks?
- Basis in paper: [explicit] The paper states that the image-sharing ability is an emergent ability in zero-shot prompting, and it demonstrates the effectiveness of the restriction-based prompt template in enhancing zero-shot performance across both stages
- Why unresolved: While the paper shows that large language models can share images to some extent, it does not explicitly discuss the impact of this ability on the overall performance of the models in multi-modal dialogue tasks
- What evidence would resolve it: A comprehensive study comparing the performance of large language models with and without the image-sharing ability in multi-modal dialogue tasks would provide insights into the impact of this ability on the models' overall performance

### Open Question 2
- Question: How does the size of the language model affect the quality and diversity of the generated image descriptions?
- Basis in paper: [explicit] The paper mentions that as the scale of large language models increases, the zero-shot performance in image-sharing turn prediction improves, and models with RLHF tend to generate more specific and diverse descriptions
- Why unresolved: The paper does not provide a detailed analysis of how the size of the language model affects the quality and diversity of the generated image descriptions
- What evidence would resolve it: A systematic study comparing the quality and diversity of image descriptions generated by large language models of different sizes would help determine the relationship between model size and description quality

### Open Question 3
- Question: Can the restriction-based prompt template be effectively applied to other tasks beyond image-sharing behavior?
- Basis in paper: [explicit] The paper introduces the restriction-based prompt template as a key component of the proposed framework for image-sharing behavior, and it demonstrates its effectiveness in enhancing zero-shot performance
- Why unresolved: The paper focuses solely on the application of the restriction-based prompt template to image-sharing behavior and does not explore its potential use in other tasks
- What evidence would resolve it: Experiments applying the restriction-based prompt template to various tasks beyond image-sharing behavior would help determine its versatility and effectiveness in different contexts

### Open Question 4
- Question: What are the limitations of the proposed framework when dealing with more complex and nuanced dialogues?
- Basis in paper: [inferred] The paper presents a two-stage framework for image-sharing behavior and demonstrates its effectiveness in a zero-shot setting. However, it does not explicitly discuss the limitations of the framework when dealing with more complex and nuanced dialogues
- Why unresolved: The paper does not provide a detailed analysis of the framework's limitations in handling complex and nuanced dialogues
- What evidence would resolve it: A comprehensive evaluation of the framework's performance on a diverse set of dialogues with varying levels of complexity and nuance would help identify its limitations and potential areas for improvement

## Limitations
- Performance significantly degrades for models below 175B parameters, limiting practical deployment
- The framework appears heavily dependent on precise prompt formatting rather than genuine emergent capability
- Evaluation relies on proxy metrics that don't directly measure conversational enhancement from shared images

## Confidence

**High Confidence**: The two-stage framework architecture is technically sound and produces measurable outputs. The observation that larger models perform better than smaller ones is empirically robust.

**Medium Confidence**: The claim that LLMs can "effectively" share images is supported by metrics but requires stronger validation through human evaluation studies. The dataset augmentation approach appears viable but needs broader testing.

**Low Confidence**: The assertion that image-sharing is an "emergent ability" of LLMs lacks strong evidence beyond correlation with model size. The practical utility of automatically shared images in actual conversations remains unproven.

## Next Checks

1. **Human Preference Evaluation**: Conduct a blinded study where human participants judge whether LLM-generated images enhance conversational quality compared to baseline (no images) or random image selection. This would validate whether metric-based success translates to actual user benefit.

2. **Cross-Dataset Generalization Test**: Evaluate the framework on dialogue datasets from different domains (customer service, educational, social media) and languages to determine whether the image-sharing ability generalizes beyond the PhotoChat++ dataset used for training augmentation.

3. **Restriction Ablation with Controlled Variations**: Systematically test variations of the restriction template (different formats, constraint types, and instruction phrasings) to determine whether the current template represents a local optimum or if there are more effective prompting strategies that could work across model scales.