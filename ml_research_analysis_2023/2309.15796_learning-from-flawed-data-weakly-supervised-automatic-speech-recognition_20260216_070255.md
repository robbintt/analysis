---
ver: rpa2
title: 'Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition'
arxiv_id: '2309.15796'
source_url: https://arxiv.org/abs/2309.15796
tags:
- speech
- errors
- training
- data
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of training automatic speech recognition
  (ASR) systems with imperfect transcripts, which are common in real-world data. The
  authors propose Omni-temporal Classification (OTC), a novel training criterion that
  extends Connectionist Temporal Classification (CTC) to explicitly incorporate label
  uncertainties from transcript errors.
---

# Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition
## Quick Facts
- arXiv ID: 2309.15796
- Source URL: https://arxiv.org/abs/2309.15796
- Reference count: 0
- One-line primary result: OTC achieves 10-30% WER on LibriSpeech with 70% transcript errors, while CTC fails completely

## Executive Summary
This paper addresses the challenge of training automatic speech recognition (ASR) systems when transcripts contain errors, a common scenario in real-world data. The authors propose Omni-temporal Classification (OTC), a novel training criterion that extends Connectionist Temporal Classification (CTC) to explicitly incorporate label uncertainties from transcript errors. OTC achieves this by leveraging weighted finite-state transducers (WFST) to encode substitution, insertion, and deletion errors into the training graph. Experiments on LibriSpeech and LibriVox datasets demonstrate that OTC-based models maintain reasonable ASR performance even with transcripts containing up to 70% errors, whereas CTC models fail completely.

## Method Summary
The paper proposes OTC as an extension to CTC that handles imperfect transcripts by modifying the training graph G(y) to include self-loop arcs and bypass arcs, each associated with a special token ⋆ and penalty. The emission WFST E(x) is modified so that ⋆ is represented as the average probability of all non-blank tokens. Penalties λ1i = β1 * τ^i and λ2i = β2 * τ^i are applied to self-loop and bypass arcs respectively, with β decaying by factor τ each epoch. The model uses wav2vec 2.0 features and a 12-layer conformer network, trained with the OTC objective on LibriSpeech and LibriVox datasets with synthetic or real transcript errors.

## Key Results
- On LibriSpeech, OTC achieves 10-30% word error rates (WERs) with 70% transcript errors, compared to CTC's complete failure
- OTC significantly reduces human effort required for data preparation in training ASR systems on weakly supervised data
- OTC outperforms baseline methods on LibriVox LV-100 with uniform 60s segmentation

## Why This Works (Mechanism)

### Mechanism 1
OTC extends CTC by incorporating explicit label uncertainties from substitution, insertion, and deletion errors into the training graph via weighted finite-state transducers (WFST). The training graph G(y) is modified to include self-loop arcs and bypass arcs, each associated with a special token ⋆ and penalty. This allows the model to align acoustics with ⋆ instead of erroneous tokens during training.

### Mechanism 2
The ⋆ token is modeled as the average probability of all non-blank tokens, improving handling of insertion errors compared to previous methods. The emission WFST E(x) is modified so that the probability of ⋆ is the log average of all non-blank token probabilities at each frame, rather than a single garbage token.

### Mechanism 3
The penalty strategy with exponential decay encourages the model to rely on the given transcript early in training and gradually align with ⋆ when confused. Penalties λ1i = β1 * τ^i and λ2i = β2 * τ^i are applied to self-loop and bypass arcs respectively, with β decaying by factor τ each epoch.

## Foundational Learning

- Concept: Weighted Finite-State Transducers (WFST)
  - Why needed here: OTC uses WFST to encode transcript errors and alignment uncertainties into the training graph
  - Quick check question: How does composing WFSTs S(y) and E(x) yield the posterior probability P(y|x)?

- Concept: Connectionist Temporal Classification (CTC)
  - Why needed here: OTC extends CTC to handle imperfect transcripts by modifying the training graph and emission probabilities
  - Quick check question: What is the role of the blank symbol ⊘ in CTC, and how does OTC's ⋆ token differ?

- Concept: Subword Tokenization (BPE)
  - Why needed here: The model uses BPE units as output tokens, and the vocabulary size affects performance with transcript errors
  - Quick check question: How does the choice of vocabulary size (100, 200, 500) impact the model's ability to handle substitution and deletion errors?

## Architecture Onboarding

- Component map: wav2vec 2.0 features -> 12-layer conformer encoder -> Linear layer with softmax -> Modified training graph GOTC(y) and emission WFST EOTC(x)

- Critical path:
  1. Extract wav2vec 2.0 features from audio
  2. Construct modified training graph GOTC(y) with self-loop and bypass arcs
  3. Construct modified emission WFST EOTC(x) with ⋆ modeled as average non-blank probability
  4. Compose GOTC(y) and EOTC(x), compute loss, backpropagate
  5. Decode with greedy or language model decoding

- Design tradeoffs:
  - Vocabulary size: Larger vocabularies may better capture errors but increase computational cost and risk of averaging issues for ⋆
  - Penalty values (β1, β2): Higher penalties early in training may help but risk overfitting; lower penalties may allow faster alignment with ⋆ but risk ignoring useful transcript information
  - Segmentation length: Shorter segments may reduce memory usage but increase shift errors; longer segments may capture more context but increase memory usage and risk of overwhelming the conformer

- Failure signatures:
  - Model fails to converge: Penalties too low, or segmentation introduces too many shift errors
  - High WER even with few errors: Vocabulary size too small or too large, penalties not properly tuned
  - Slow training: Segmentation too long, or emission WFST computation too complex

- First 3 experiments:
  1. Train on LibriSpeech train-clean-100 with synthetic substitution errors at 0.1, 0.3, 0.5, 0.7 rates; compare CTC vs OTC WER
  2. Train on LibriVox LV-100 with uniform 60s segmentation; compare CTC vs OTC WER on test-clean and test-other
  3. Ablate vocabulary size (100, 200, 500) on LibriSpeech with synthetic deletion errors at 0.5 rate; measure impact on WER

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of OTC-based ASR systems scale with dataset size, particularly in the very-large-data regime where Whisper operates? The paper mentions that label errors are "relatively less hazardous in the very-large-data regime (such as Whisper)" but most academic and industrial systems operate at medium scale.

### Open Question 2
What is the optimal segmentation strategy for OTC when dealing with real-world data that contains both long-form speech and imperfect transcripts? The paper explores different segment durations (15, 30, 60 seconds) and finds 60 seconds optimal, but notes this "strikes the optimal balance" rather than being definitively best for all cases.

### Open Question 3
How do different types of transcript errors (substitution, insertion, deletion) interact with each other in real-world data, and how should OTC's penalties be dynamically adjusted to handle mixed error patterns? The paper shows OTC performs well across different synthetic error types individually, but real-world errors likely involve combinations of all three types.

## Limitations
- The method is primarily demonstrated on read speech datasets (LibriSpeech and LibriVox), leaving questions about performance on conversational or spontaneous speech
- The penalty tuning strategy (β1, β2, τ1, τ2) is empirically determined but lacks theoretical grounding for why these specific values work across different error types
- The computational overhead of WFST composition and emission WFST modification is not thoroughly analyzed

## Confidence
- High confidence: The core mechanism of extending CTC with self-loop and bypass arcs to handle transcript errors is well-supported by experimental results
- Medium confidence: The penalty strategy with exponential decay shows reasonable results, but the sensitivity analysis is limited
- Low confidence: The claim that the ⋆ token modeled as average non-blank probability is superior to a single garbage token lacks rigorous comparison

## Next Checks
1. Apply OTC to a conversational speech dataset (e.g., Switchboard or AMI) with naturally occurring transcript errors to verify whether synthetic error patterns generalize to more complex, real-world error distributions
2. Systematically evaluate OTC performance across a wider range of vocabulary sizes (50, 100, 200, 500, 1000, 2000) to determine the optimal range and identify when average probability calculation for ⋆ becomes ineffective
3. Measure and compare the training time and memory requirements of CTC, BTC, and OTC across different dataset sizes and segmentation strategies to quantify practical deployment costs