---
ver: rpa2
title: Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional
  Monotonicity
arxiv_id: '2306.02652'
source_url: https://arxiv.org/abs/2306.02652
tags:
- monotonicity
- anytime
- conditional
- test
- exit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Early-exit neural networks (EENNs) can provide intermediate predictions
  at various depths but often lack conditional monotonicity, meaning the quality of
  predictions for individual data points can degrade with longer computation. This
  violates a key requirement for anytime algorithms, which should improve with additional
  computation.
---

# Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity

## Quick Facts
- **arXiv ID**: 2306.02652
- **Source URL**: https://arxiv.org/abs/2306.02652
- **Reference count**: 40
- **Primary result**: Product-of-Experts ensembles reduce conditional monotonicity violations from ~30% to ~13% on CIFAR-100 while preserving accuracy.

## Executive Summary
Early-exit neural networks (EENNs) provide intermediate predictions at various depths but often violate conditional monotonicity, where prediction quality can degrade with longer computation. This paper addresses this critical limitation for anytime algorithms by introducing a lightweight post-hoc modification based on Product-of-Experts (PoE) ensembles. The method enforces conditional monotonicity by taking the product over all early-exits computed thus far, creating an aggregated predictive distribution that gradually refines over time with non-increasing support.

The approach achieves significant improvements in conditional monotonicity (e.g., reducing violations from ~30% to ~13% on CIFAR-100) while maintaining competitive test accuracy. The authors also propose caching the best prediction so far as a simpler baseline, and explore adaptive thresholding techniques for balancing monotonicity and calibration. Empirical results demonstrate that these behaviors can be achieved across standard image-classification tasks using various pre-trained EENN architectures.

## Method Summary
The paper proposes a post-hoc modification to pre-trained early-exit neural networks using Product-of-Experts ensembles to enforce conditional monotonicity. At each early exit, the method applies ReLU activation to logits and takes the product over all previous exits, creating an aggregated distribution that concentrates probability on a non-increasing subset of classes. A simpler caching baseline stores the most confident prediction encountered so far. The authors also explore adaptive thresholding, replacing ReLU's zero threshold with a data-dependent value learned via logistic regression to balance monotonicity and calibration.

## Key Results
- Conditional monotonicity violations reduced from ~30% to ~13% on CIFAR-100 using PA method
- Test accuracy preserved at competitive levels while improving monotonicity
- PA achieves high uncertainty early and progressive confidence at subsequent exits
- Caching baseline (CA) provides a simpler alternative that performs well in practice
- Adaptive thresholding can improve calibration at some cost to monotonicity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Product-of-Experts (PoE) ensembles enforce conditional monotonicity by gradually refining the predictive distribution over time.
- **Mechanism**: At each early exit, the PoE ensemble takes the product of all previous logits transformed by ReLU, which preserves high logits and nullifies low ones. This shrinks the support of the distribution, concentrating probability on a non-increasing set of classes and ensuring the ground-truth probability cannot decrease.
- **Core assumption**: The ReLU activation preserves the ranking of logits while filtering out low-scoring classes, and the product operation inherently aggregates evidence monotonically.
- **Evidence anchors**: [abstract] "by taking the product over all early-exits computed thus-far, the aggregated predictive distribution becomes gradually refined over time, with a non-increasing support"; [section] "At each exit, the set of candidate labels can only either remain the same or be reduced, thus concentrating probability in a non-increasing subset of classes"; [corpus] No direct evidence; this is a theoretical claim supported by the ablation study in Appendix B.2.
- **Break condition**: If logits across exits are non-overlapping, the product can collapse to a zero distribution, requiring fallback to the latest softmax output.

### Mechanism 2
- **Claim**: Caching the best prediction so far (CA) enforces conditional monotonicity by always returning the most confident prediction encountered.
- **Mechanism**: At each exit, compute confidence as max_y p(y|x). If this exceeds the cached confidence, update the cache. This guarantees the returned prediction never worsens in confidence over time.
- **Core assumption**: Confidence (max class probability) is a reliable proxy for prediction quality, and the model's internal confidence correlates with true accuracy.
- **Evidence anchors**: [abstract] "simply using the softmax confidence performs well in practice"; [section] "We propose using the EENN's internal measure of confidence... C(p_m, x) := max y p_m(y|x)"; [corpus] No direct evidence; this is a heuristic motivated by practical observation in Section 4.1.
- **Break condition**: If confidence is poorly calibrated or misleading, CA may cache a suboptimal prediction, though monotonicity in confidence is still preserved.

### Mechanism 3
- **Claim**: Adaptive thresholding in PA balances monotonicity and calibration by adjusting the support size per data point.
- **Mechanism**: Replace ReLU's zero threshold with a data-dependent threshold τ(f(x)) = C · p_ψ(f(x)), where p_ψ estimates correct-class probability. Higher thresholds shrink support (more monotonic, less calibrated), lower thresholds expand it (less monotonic, better calibrated).
- **Core assumption**: The model can learn to predict when it is confident enough to justify a smaller support, and this threshold can be tuned on a validation set.
- **Evidence anchors**: [section] "Rather than using the same threshold value for all data points, the model could dynamically adjust the threshold in the activation function based on the current example"; [section] "we fit a binary logistic regression model using labels y_τ_n := [ŷ_n = y*_n]"; [corpus] No direct evidence; this is a proposed extension supported by Figure 10 in Appendix B.3.
- **Break condition**: If the threshold model p_ψ is poorly trained or overfitting, adaptive thresholding may degrade both monotonicity and calibration.

## Foundational Learning

- **Concept: Product-of-Experts ensembles**
  - Why needed here: PoE is the core mechanism for enforcing conditional monotonicity by aggregating evidence across exits without averaging.
  - Quick check question: How does a PoE ensemble differ from a mixture-of-experts in terms of support size and probability concentration?

- **Concept: Conditional monotonicity**
  - Why needed here: It is the key property that ensures prediction quality never degrades with additional computation, making EENNs truly anytime.
  - Quick check question: Why is conditional monotonicity stronger than marginal monotonicity, and how can a model be marginally monotonic but not conditionally?

- **Concept: Uncertainty quantification in anytime models**
  - Why needed here: Anytime models should exhibit high uncertainty early and grow confident later; PA achieves this via support shrinking.
  - Quick check question: How does the shrinking support of PA's predictive distribution relate to the entropy of the output probabilities?

## Architecture Onboarding

- **Component map**: Input → Backbone (stacked layers) → Multiple classifier heads (early exits) → PoE transformation (optional) → Output distribution
- **Critical path**: Forward pass through backbone until exit m → Apply ReLU to logits → Take product over all m exits → Normalize → Output
- **Design tradeoffs**: PoE vs softmax (monotonicity vs calibration), ReLU vs Heaviside (practicality vs perfect monotonicity), post-hoc vs training-time application (ease vs performance)
- **Failure signatures**: Zero distribution collapse (fallback to softmax), poor calibration in early exits, increased computational overhead from product operations
- **First 3 experiments**:
  1. Apply post-hoc PA to a pretrained MSDNet on CIFAR-10 and measure monotonicity improvement.
  2. Compare CA vs PA on a small dataset to validate confidence caching works.
  3. Test adaptive thresholding on a validation set to tune C and p_ψ for best monotonicity-calibration balance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy-monotonicity trade-off vary across different datasets and model architectures?
- Basis in paper: [explicit] The paper mentions that PA's performance in terms of conditional monotonicity improves as the number of classes |Y| increases, and that using the CA approach is recommended for scenarios with |Y| < 5.
- Why unresolved: The paper provides limited empirical evidence on how the trade-off varies across different datasets and architectures beyond the ones tested.
- What evidence would resolve it: Empirical results showing the accuracy-monotonicity trade-off for PA and CA across a wider range of datasets (e.g., varying number of classes, image vs. text data) and model architectures (e.g., different backbone networks, ensemble methods).

### Open Question 2
- Question: What is the impact of the adaptive thresholding technique on the anytime uncertainty properties of PA?
- Basis in paper: [explicit] The paper introduces adaptive thresholding as a modification to PA to improve calibration, but notes that it comes at some cost to both test accuracy and conditional monotonicity.
- Why unresolved: The paper does not provide a detailed analysis of how adaptive thresholding affects the anytime uncertainty properties of PA, such as the evolution of entropy or conformal set size.
- What evidence would resolve it: Empirical results showing the entropy and conformal set size trajectories for PA with and without adaptive thresholding, demonstrating the impact on anytime uncertainty.

### Open Question 3
- Question: How does the choice of activation function in PA affect the conditional monotonicity guarantees?
- Basis in paper: [explicit] The paper discusses the use of ReLU and Heaviside activation functions in PA, noting that ReLU compromises the strong conditional monotonicity guarantees of Heaviside.
- Why unresolved: The paper does not provide a comprehensive analysis of how different activation functions affect the conditional monotonicity guarantees of PA.
- What evidence would resolve it: Theoretical analysis and empirical results showing the impact of different activation functions on the conditional monotonicity guarantees of PA, including a comparison of the accuracy-monotonicity trade-off for each activation function.

## Limitations
- Post-hoc application may not fully exploit model capacity compared to training-time integration
- Adaptive thresholding introduces additional hyperparameters requiring dataset-specific tuning
- Effectiveness of confidence caching assumes confidence correlates with accuracy
- Product operation can collapse to zero distribution if logits are non-overlapping

## Confidence
- **High confidence**: PA significantly reduces violations of conditional monotonicity (from ~30% to ~13% on CIFAR-100) while preserving accuracy
- **Medium confidence**: The inductive bias of PoE (shrinking support, increasing confidence) holds across diverse EENN architectures and datasets
- **Medium confidence**: CA baseline is a practical and effective solution for enforcing monotonicity in pre-trained models

## Next Checks
1. Compare PA against alternative aggregation methods (e.g., mixture-of-experts, weighted averaging) to isolate the specific benefits of PoE
2. Validate CA on a poorly calibrated EENN to test the robustness of confidence caching when confidence-accuracy correlation is weak
3. Conduct a hyperparameter sensitivity analysis for adaptive thresholding (C, p_ψ) across multiple datasets to understand the tradeoff between monotonicity and calibration