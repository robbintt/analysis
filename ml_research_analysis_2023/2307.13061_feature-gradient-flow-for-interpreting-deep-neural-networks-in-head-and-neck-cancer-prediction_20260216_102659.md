---
ver: rpa2
title: Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck
  Cancer Prediction
arxiv_id: '2307.13061'
source_url: https://arxiv.org/abs/2307.13061
tags:
- features
- classifier
- interpretable
- feature
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces feature gradient flow, a method to interpret
  deep learning models by measuring alignment between model gradients and human-interpretable
  features. The method decomposes the input space into dimensions where the model
  decision changes (sections) and where it remains constant (fibers).
---

# Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction

## Quick Facts
- arXiv ID: 2307.13061
- Source URL: https://arxiv.org/abs/2307.13061
- Authors: 
- Reference count: 0
- Primary result: Feature gradient flow method achieves 16-19% interpretable feature alignment while maintaining 0.68 balanced accuracy for head and neck cancer metastasis prediction

## Executive Summary
This paper introduces feature gradient flow, a method to interpret deep learning models by measuring alignment between model gradients and human-interpretable features. The approach decomposes the input space into dimensions where model decisions change (sections) and where they remain constant (fibers), then quantifies interpretability by measuring gradient alignment. Tested on a head and neck cancer CT dataset, the method shows that interpretable features (brightness, tumor extent, aspect ratio) account for 16% of gradient magnitude, increasing to 19% with an interpretability-promoting training regularization.

## Method Summary
The method measures how well interpretable features align with model gradients through two metrics: pointwise alignment (S) and gradient flow alignment (F). A regularization term is added to the training loss to encourage gradient alignment with interpretable features. The approach was applied to a head and neck cancer dataset using a 3-layer CNN with tumor segmentation masks, training on 209 samples and testing on 89 samples to predict distant metastasis.

## Key Results
- Baseline model shows interpretable features account for 16% of gradient magnitude
- With interpretability regularization, alignment increases to 19% while maintaining balanced accuracy around 0.68
- Features used: brightness, tumor extent, and log aspect ratio from CT images
- Dataset: 298 subjects, 40 positive cases (13% class imbalance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature gradient flow decomposes the input space into sections (where classifier decisions change) and fibers (where decisions remain constant), providing a geometric framework for interpretability
- Mechanism: The classifier's Jacobian matrix defines local coordinates that separate relevant dimensions (sections) from irrelevant ones (fibers). This fibered manifold structure allows measurement of how interpretable features align with classifier decision boundaries
- Core assumption: The classifier is a C¹ mapping with maximal rank K-1 at regular points, ensuring local fibered manifold structure exists
- Evidence anchors:
  - [abstract] "The gradient flow of a model locally defines nonlinear coordinates in the input data space representing the information the model is using to make its decisions"
  - [section] "A regular point of f is a point x ∈ R^d such that Df(x) has maximal rank, that is, rank(Df(x)) = K - 1. The set of regular points of f is open in R^d. This implies that there is a neighborhood about any regular point of f that is a fibered manifold"
  - [corpus] No direct corpus evidence found for fibered manifold geometry concept
- Break condition: The method fails when classifier decisions don't create well-defined regular points or when the manifold structure breaks down at singular points

### Mechanism 2
- Claim: Pointwise alignment measures how well interpretable features align with classifier gradients at specific locations
- Mechanism: The method computes the squared cosine similarity between classifier gradients and interpretable feature gradients, normalized by their magnitudes. This captures alignment while ignoring gradient magnitude and polarity
- Core assumption: The direction of gradients (not magnitude) indicates feature importance for classification decisions
- Evidence anchors:
  - [section] "At a single data point x ∈ R^d, this pointwise alignment is given by S(x) = (⟨∇f(x), ∇g(x)⟩ / (||∇f(x)|| · ||∇g(x)||))²"
  - [abstract] "Our idea is to measure the agreement of interpretable features with the gradient flow of a model"
  - [corpus] Weak corpus evidence - only general gradient-based interpretability papers found
- Break condition: When gradients are zero or near-zero, making alignment measurements unstable or undefined

### Mechanism 3
- Claim: Regularization encourages models to use interpretable features during training by aligning gradients
- Mechanism: An alignment term is added to the loss function that rewards gradient alignment between the classifier and interpretable features. This term uses the same squared cosine similarity measure as pointwise alignment
- Core assumption: Training with gradient alignment rewards will shift the model to rely more on interpretable features without sacrificing performance
- Evidence anchors:
  - [section] "We can further use it to encourage a model to be more interpretable. This is done by adding an alignment 'reward' term to the loss during training"
  - [section] "The training loss is: L(x,f,g) = Σx L(f(x),y) - Σx Σi λi (⟨∇f(x), ∇gi(x)⟩ / (||∇f(x)|| · ||∇gi(x)||))²"
  - [abstract] "We then develop a technique for training neural networks to be more interpretable by adding a regularization term to the loss function that encourages the model gradients to align with those of chosen interpretable features"
- Break condition: When the regularization weight λ is too large, causing performance degradation, or too small, making the effect negligible

## Foundational Learning

- Concept: Fibered manifold geometry and Jacobian rank analysis
  - Why needed here: Understanding how classifiers decompose input space into relevant and irrelevant dimensions is fundamental to the interpretability approach
  - Quick check question: What mathematical condition must hold for a classifier to have a well-defined fibered manifold structure at a point?

- Concept: Gradient flow and differential geometry
  - Why needed here: Following the gradient path from data points to decision boundaries is crucial for the global alignment measure
  - Quick check question: How does following the gradient flow differ from computing pointwise alignment in terms of capturing classifier behavior?

- Concept: Regularization techniques in deep learning
  - Why needed here: Adding the alignment term to the loss function requires understanding how regularization affects model training and generalization
  - Quick check question: What happens to model performance when the interpretability regularization weight becomes too large?

## Architecture Onboarding

- Component map:
  Input layer: 512×512 grayscale CT images with tumor segmentation masks → Convolutional backbone: 3 conv layers (5×5, 3×3, 3×3 kernels) with average pooling and ELU activations → Dense layers: 3 fully-connected layers (256, 128, 1 output units) → Gradient computation module: Computes gradients for both classifier outputs and interpretable features → Alignment scoring: Calculates S(x) and F(x) metrics during inference → Training loop: Standard Adam optimizer with optional alignment regularization term

- Critical path:
  1. Forward pass through CNN to get predictions
  2. Compute gradients of loss with respect to inputs
  3. Compute gradients of interpretable features with respect to inputs
  4. Calculate alignment scores using squared cosine similarity
  5. For training with alignment, add regularization term to loss

- Design tradeoffs:
  - Average pooling vs max pooling: Average pooling chosen for differentiability
  - ELU vs ReLU: ELU chosen for differentiability while maintaining performance
  - Regularization weight λ: Tradeoff between interpretability and accuracy
  - Number of interpretable features: More features provide better coverage but increase complexity

- Failure signatures:
  - Poor alignment scores despite good accuracy: Model using non-interpretable features
  - Performance degradation with alignment regularization: λ too large
  - Unstable alignment scores: Near-zero gradients causing numerical issues
  - No improvement in interpretability: Features poorly chosen or not relevant to classification

- First 3 experiments:
  1. Baseline: Train model without alignment regularization, measure S and F scores
  2. Alignment training: Add regularization with small λ, compare alignment scores and accuracy
  3. Feature ablation: Remove each interpretable feature individually, measure impact on alignment scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the interpretability method be extended to handle cases where the model relies heavily on unexpected features not provided by the user?
- Basis in paper: [explicit] "A limitation of our method is that it requires the user to input interpretable features of interest. Thus, our method may be less effective in cases where a model mostly makes use of unexpected features."
- Why unresolved: The paper acknowledges this limitation but does not propose solutions for automatically discovering or incorporating unexpected features that the model may be using.
- What evidence would resolve it: Experiments showing successful extension of the method to handle unexpected features, either through automated discovery or by incorporating additional user-provided features.

### Open Question 2
- Question: Can the interpretability scores be further improved while maintaining or improving model performance?
- Basis in paper: [explicit] "We can tune λi to be as large as is possible without hurting the performance of the model."
- Why unresolved: The paper shows improved interpretability with the regularization term but does not explore the full range of possible λi values or other techniques to further enhance interpretability without sacrificing performance.
- What evidence would resolve it: Systematic exploration of different λi values and comparison with alternative interpretability-enhancing techniques to find the optimal balance between interpretability and performance.

### Open Question 3
- Question: How does the method perform on datasets with different levels of class imbalance or different feature types?
- Basis in paper: [inferred] The paper uses a highly imbalanced dataset (13% positive cases) but does not explore how the method performs on balanced datasets or datasets with different feature types (e.g., non-image data).
- Why unresolved: The paper focuses on a single dataset with specific characteristics, limiting generalizability to other scenarios.
- What evidence would resolve it: Application of the method to datasets with varying levels of class imbalance and different feature types, comparing interpretability scores and model performance across these datasets.

## Limitations
- The method requires user-provided interpretable features and may be less effective when models use unexpected features
- Interpretability scores of 16-19% gradient alignment may be modest and need validation of practical significance
- The mathematical framework assumes well-defined fibered manifolds but doesn't address how frequently singular points occur in practice

## Confidence

- High confidence: The geometric framework for decomposing input space into sections and fibers is mathematically sound
- Medium confidence: The pointwise and gradient flow alignment measures are valid, though their practical utility depends on feature quality
- Low confidence: The claim that 16-19% gradient alignment represents meaningful interpretability needs more validation

## Next Checks

1. **Synthetic data test**: Apply the method to synthetic datasets where ground truth feature importance is known to verify alignment scores accurately reflect true feature contributions

2. **Feature ablation study**: Systematically remove each interpretable feature and measure the impact on both alignment scores and classification performance to validate feature relevance

3. **Regularization sweep**: Conduct a comprehensive analysis of different λ values to map the full performance-interpretability tradeoff curve and identify optimal regularization strength