---
ver: rpa2
title: 'SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis'
arxiv_id: '2310.18023'
source_url: https://arxiv.org/abs/2310.18023
tags:
- dataset
- data
- language
- languages
- sentmix-3l
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SentMix-3L, a novel dataset for sentiment
  analysis in Bangla-English-Hindi code-mixed text, containing 1,007 instances. The
  dataset was generated by fluent speakers and annotated in two stages to ensure high-quality
  labels.
---

# SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis

## Quick Facts
- arXiv ID: 2310.18023
- Source URL: https://arxiv.org/abs/2310.18023
- Reference count: 5
- Primary result: GPT-3.5 zero-shot prompting outperforms transformer models on sentiment analysis of Bangla-English-Hindi code-mixed text (F1 0.62).

## Executive Summary
This paper introduces SentMix-3L, a novel dataset for sentiment analysis in Bangla-English-Hindi code-mixed text, containing 1,007 instances. The dataset was generated by fluent speakers and annotated in two stages to ensure high-quality labels. Experiments show that zero-shot prompting with GPT-3.5 outperforms transformer-based models like XLM-R and mBERT on this task, achieving an F1 score of 0.62. While synthetic data allows strong performance from multilingual models (F1 0.77), the natural test set remains challenging due to transliteration and misspellings. SentMix-3L serves as a high-quality benchmark for multi-language code-mixing research.

## Method Summary
The authors created SentMix-3L by having fluent speakers generate code-mixed text and performing two-stage annotation. They generated synthetic training data using Amazon reviews and code-mixing algorithms (Random Code-mixing Algorithm and r-CM). They evaluated multiple models including monolingual (DistilBERT, BERT, BanglaBERT, roBERTa, HindiBERT), bilingual (BanglishBERT, HingBERT), and multilingual (mBERT, XLM-R, IndicBERT, MuRIL) models fine-tuned on synthetic data, plus GPT-3.5-turbo zero-shot prompting on the natural test set.

## Key Results
- GPT-3.5 zero-shot prompting achieved F1 0.62 on the natural test set, outperforming all transformer models
- Multilingual models trained on synthetic data achieved F1 0.77 on synthetic test set but only F1 0.59 on natural test set
- XLM-R showed the best performance among transformer models with F1 0.59 on natural test set
- Error analysis revealed transliteration and misspellings affect nearly 40% of instances and challenge most models

## Why This Works (Mechanism)

### Mechanism 1
- GPT-3.5 zero-shot prompting outperforms transformer-based models on SentMix-3L.
- GPT-3.5's strong multilingual and code-mixing capabilities allow direct classification without task-specific fine-tuning.
- Core assumption: GPT-3.5's pre-training includes sufficient exposure to Bangla-English-Hindi code-mixed text.
- Evidence: GPT-3.5 Turbo achieved F1 0.62 vs XLM-R 0.59, BanglishBERT 0.56 on SentMix-3L.
- Break condition: If GPT-3.5 lacks sufficient pre-training exposure to the specific code-mixing patterns, performance may degrade.

### Mechanism 2
- Synthetic training data enables strong performance from multilingual models.
- Synthetic code-mixed data provides sufficient training examples for multilingual models to learn patterns.
- Core assumption: Synthetic data generation methods produce realistic code-mixed text capturing natural patterns.
- Evidence: Multilingual models achieved F1 0.77 on synthetic test set using Amazon reviews + code-mixing algorithms.
- Break condition: If synthetic methods fail to capture natural code-mixing complexity, performance on natural test sets will suffer.

### Mechanism 3
- Multilingual models like XLM-R and mBERT perform well due to coverage of Indian languages.
- Pre-training on multiple Indian languages provides useful transfer learning for Bangla-English-Hindi code-mixing.
- Core assumption: Pre-training on Indian languages provides transfer benefits for code-mixing.
- Evidence: XLM-R achieved F1 0.59 on natural test set and 0.77 on synthetic test set, best among multilingual models.
- Break condition: If pre-training data lacks sufficient Bangla-English-Hindi code-mixing examples, transfer learning benefits will be limited.

## Foundational Learning

- Concept: Code-mixing vs code-switching
  - Why needed here: Understanding the difference is crucial for analyzing the dataset and model performance.
  - Quick check question: What distinguishes code-mixing from code-switching in linguistic terms?

- Concept: Multilingual model architectures (BERT, XLM-R, mBERT)
  - Why needed here: Different multilingual models have varying language coverage and performance on code-mixing tasks.
  - Quick check question: How does XLM-R's language coverage compare to mBERT and what implications does this have for code-mixing tasks?

- Concept: Synthetic data generation for code-mixing
  - Why needed here: The paper relies on synthetic data for training models, so understanding generation methods is important.
  - Quick check question: What are the key differences between the Random Code-mixing Algorithm and r-CM methods used in this paper?

## Architecture Onboarding

- Component map: Fluent speakers -> Code-mixed text generation -> Two-stage annotation -> Synthetic data generation -> Model training -> Evaluation framework -> GPT-3.5 zero-shot prompting

- Critical path: Generate code-mixed text from fluent speakers → Perform two-stage annotation → Create synthetic training data → Evaluate multiple models including GPT-3.5 → Analyze results and error patterns

- Design tradeoffs: Natural vs synthetic data (authenticity vs scalability), model selection (fine-tuning vs zero-shot prompting), annotation strategy (quality vs cost)

- Failure signatures: Poor performance on transliterated tokens, lower accuracy on code-mixed instances vs monolingual, models struggling with misspellings, GPT-3.5 underperforming on complex patterns

- First 3 experiments: 1) Train multilingual models on synthetic data, evaluate on natural test set; 2) Implement GPT-3.5 zero-shot prompting on same test set; 3) Analyze error patterns focusing on transliterated tokens, misspellings, and code-mixing complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would transformer-based models perform on a larger, naturally collected Bangla-English-Hindi code-mixed dataset?
- Basis: The current dataset is smaller and semi-natural, generated by speakers rather than scraped from social media.
- Why unresolved: Small size and semi-natural generation limit generalizability to real-world data.
- Evidence needed: Larger, naturally collected dataset with comprehensive transformer model evaluation.

### Open Question 2
- Question: What is the impact of transliteration and misspelling handling on sentiment analysis performance?
- Basis: Error analysis shows "Other tokens" (transliterations, misspellings) occur in 40% of dataset and pose challenges.
- Why unresolved: Paper doesn't explore specific techniques or quantify performance drop due to these factors.
- Evidence needed: Experiments comparing performance with/without transliteration/misspelling normalization.

### Open Question 3
- Question: How does GPT-3.5 zero-shot prompting compare to fine-tuned models on diverse sentiment analysis tasks?
- Basis: GPT-3.5 outperformed all transformer models on SentMix-3L sentiment analysis.
- Why unresolved: Evaluation limited to single sentiment analysis task; unclear if superiority extends to other tasks.
- Evidence needed: Comparative studies across multiple sentiment analysis tasks in code-mixed text.

## Limitations
- Dataset size of 1,007 instances may limit diversity of code-mixing patterns captured
- Reliance on synthetic data for training raises questions about fidelity to natural code-mixing patterns
- Error analysis identifies transliteration and misspellings as challenges but lacks detailed quantitative breakdown

## Confidence

**High Confidence (8-10/10)**: Dataset creation methodology and annotation process are well-documented. Experimental results showing GPT-3.5 outperforming transformer models are clearly presented.

**Medium Confidence (5-7/10)**: Synthetic data generation methods and their effectiveness are supported by results but lack extensive validation. Error analysis identifies challenges but doesn't provide detailed quantitative analysis.

**Low Confidence (1-4/10)**: Claims about GPT-3.5's superiority may not generalize to other code-mixed language pairs or different model sizes. Generalizability of synthetic data performance to other domains remains uncertain.

## Next Checks
1. Conduct ablation studies comparing Random Code-mixing Algorithm vs r-CM methods to identify which better captures natural code-mixing patterns.
2. Perform detailed error analysis with confusion matrices for specific error types (transliterations, misspellings) to quantify model robustness.
3. Test GPT-3.5 zero-shot prompting across multiple model sizes and different prompt templates to establish stability of performance advantage.