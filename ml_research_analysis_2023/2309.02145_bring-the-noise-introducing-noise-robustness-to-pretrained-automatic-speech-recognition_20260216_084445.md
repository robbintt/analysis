---
ver: rpa2
title: 'Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech
  Recognition'
arxiv_id: '2309.02145'
source_url: https://arxiv.org/abs/2309.02145
tags:
- speech
- conformer
- noise
- noisy
- cleancoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method to extract the denoising capabilities
  of a pretrained Conformer model into a preprocessor called Cleancoder. The method,
  called Parallel Weighted Sum, extracts hidden activations from the Conformer encoder
  and feeds them to a decoder to predict denoised spectrograms.
---

# Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2309.02145
- Source URL: https://arxiv.org/abs/2309.02145
- Reference count: 25
- Primary result: Proposes Cleancoder preprocessor that improves WER in noisy conditions by extracting denoising capabilities from pretrained Conformer models

## Executive Summary
This paper introduces Cleancoder, a novel preprocessor that extracts denoising capabilities from pretrained Conformer ASR models to improve noise robustness in downstream applications. The method, called Parallel Weighted Sum, extracts hidden activations from each Conformer block and uses Highway Networks to reconstruct denoised spectrograms. The Cleancoder is evaluated both as a frontend to pretrained Conformer models and as a training frontend for smaller models from scratch, showing consistent WER improvements in noisy conditions while maintaining performance in clean conditions.

## Method Summary
The Cleancoder extracts hidden activations from each Conformer block using parallel projection layers, combines them through weighted summation, and feeds them to a Highway Networks decoder to reconstruct denoised spectrograms. The model is trained on the Noisy Speech Database using L1 loss between predicted and clean spectrograms. After training, it serves as a frontend that preprocesses noisy inputs before they reach downstream ASR models, either pretrained Conformer CTC/Transducer models or smaller Conformer models trained from scratch on LibriSpeech using Cleancoder outputs.

## Key Results
- Cleancoder improves downstream ASR WER in noisy conditions for both pretrained and from-scratch models
- Denoised spectrograms show lower MAE than noisy baselines across all noise conditions
- Large Cleancoder achieves lowest mean WER on all NSD splits when used to train smaller models from scratch
- Performance improvement observed without degradation in clean conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel Weighted Sum can extract denoising capabilities from any encoder-decoder ASR architecture
- Mechanism: Hidden activations from each Conformer block are projected through separate layers and summed to create weighted representation
- Core assumption: Encoder hidden activations contain filtered noise representations that can be reconstructed
- Evidence: Authors propose method is architecture-agnostic but only test on Conformer
- Break condition: If encoder activations don't inherently filter noise or projections fail to weight contributions properly

### Mechanism 2
- Claim: Cleancoder improves downstream ASR performance in noisy conditions without clean condition degradation
- Mechanism: Reconstructed denoised spectrograms provide cleaner input features to downstream model
- Core assumption: Spectrograms matching clean speech lead to better ASR than noisy spectrograms
- Evidence: WER improvements observed in noisy conditions, lower MAE in reconstructions
- Break condition: If reconstruction introduces artifacts that confuse downstream model

### Mechanism 3
- Claim: Training smaller ASR models from scratch on Cleancoder outputs leads to faster convergence and better generalization
- Mechanism: Preprocessor provides denoised training data allowing cleaner speech representation learning
- Core assumption: Exposure to denoised spectrograms during training helps learn robust speech features
- Evidence: Smaller models trained on Cleancoder outputs show improved WER and convergence
- Break condition: If preprocessed data is too different from real noisy data, causing poor generalization

## Foundational Learning

- Concept: Conformer architecture and encoder-decoder structure
  - Why needed: Understanding how Conformer processes input and generates hidden activations is crucial for Parallel Weighted Sum implementation
  - Quick check: What are main components of a Conformer block and how do they process input?

- Concept: Spectrogram reconstruction and Highway Networks
  - Why needed: Decoder uses Highway Networks to reconstruct spectrograms from weighted hidden activations
  - Quick check: How do Highway Networks differ from regular feedforward networks and why are they suitable for spectrogram reconstruction?

- Concept: Noise robustness in ASR and evaluation metrics
  - Why needed: Understanding how noise affects ASR performance and how to measure improvements is key to evaluating Cleancoder effectiveness
  - Quick check: What is difference between WER and MAE and why might they not correlate in this context?

## Architecture Onboarding

- Component map: Input audio → Conformer encoder → Hidden activations → Parallel Weighted Sum → Combined representation → Highway Networks decoder → Reconstructed spectrogram → Downstream ASR model → Transcript

- Critical path: 1) Input audio → Conformer encoder → Hidden activations; 2) Hidden activations → Parallel Weighted Sum → Combined representation; 3) Combined representation → Highway Networks decoder → Reconstructed spectrogram; 4) Reconstructed spectrogram → Downstream ASR model → Transcript

- Design tradeoffs: Larger Conformer encoder yields better denoising but increases computational cost; number of Highway Network layers affects reconstruction quality and model size; training on diverse noise dataset improves generalization but requires more data

- Failure signatures: Downstream WER increases despite lower MAE in reconstructed spectrograms; Cleancoder overfits to specific noise types and performs poorly on unseen noises; reconstruction introduces artifacts that degrade ASR performance

- First 3 experiments: 1) Train Cleancoder on small NSD subset and evaluate MAE on held-out set; 2) Use trained Cleancoder as frontend to pretrained Conformer CTC model and measure WER on noisy test data; 3) Train small Conformer from scratch using Cleancoder outputs and compare convergence and WER to model trained on raw noisy data

## Open Questions the Paper Calls Out

- Question: What is the optimal loss function for training Cleancoder to maximize downstream ASR performance?
  - Basis: Authors note MAE improvements didn't correlate well with WER and suggest different loss function might be more appropriate
  - Why unresolved: Only tested L1 loss, didn't explore other loss functions like IRM or perceptual loss
  - What evidence would resolve: Experiments comparing Cleancoder performance with different loss functions on same downstream ASR tasks measuring resulting WER improvements

- Question: How does Cleancoder perform when used with other encoder-decoder ASR architectures beyond Conformer?
  - Basis: Authors state they'll evaluate preprocessor using additional downstream ASR architectures in future work, mentioning Whisper as candidate
  - Why unresolved: Current study only tested Cleancoder with Conformer-based models
  - What evidence would resolve: Testing Cleancoder as frontend to various ASR architectures including recurrent models, other transformer variants, and end-to-end models like Whisper, measuring WER improvements across different noise conditions

- Question: What is the relationship between MAE between noisy and clean speech and resulting WER in downstream ASR models?
  - Basis: Authors discuss MAE improvements didn't correlate well with WER and reference Möller et al. who suggested MAE and WER don't necessarily correlate
  - Why unresolved: Little research on impact of MAE between noisy and clean speech on resulting WER
  - What evidence would resolve: Systematic experiments varying preprocessing quality (measured by MAE) and measuring corresponding WER improvements, potentially revealing threshold or nonlinear relationship

## Limitations

- Evaluation limited to Noisy Speech Database, may not represent real-world acoustic conditions or noise distributions
- Weak correlation between reconstruction error (MAE) and downstream ASR performance suggests spectrogram quality alone may not predict practical improvements
- Method's generalizability to non-Conformer architectures remains theoretical as all experiments use Conformer-based models
- Performance on languages other than English and different types of speech distortions beyond additive noise not explored

## Confidence

- **High Confidence**: Cleancoder improves downstream ASR performance in noisy conditions when used as frontend for both pretrained and from-scratch models
- **Medium Confidence**: Parallel Weighted Sum can theoretically be applied to any encoder-decoder architecture, though only tested on Conformer models
- **Low Confidence**: Reconstruction error (MAE) reliably indicates downstream ASR performance improvements, as correlation is acknowledged to be weak

## Next Checks

1. Cross-dataset validation: Evaluate Cleancoder on multiple noise datasets (CHiME, DNS Challenge) to verify generalization beyond NSD and assess performance degradation with unseen noise types

2. Correlation analysis: Systematically measure relationship between reconstruction error metrics (MAE, MSE) and downstream WER across different noise conditions to determine if any reconstruction metric reliably predicts ASR performance

3. Architecture generalization: Implement and test Parallel Weighted Sum extraction method on non-Conformer architectures (Transformer, Wav2Vec) to validate claimed architecture-agnostic nature of approach