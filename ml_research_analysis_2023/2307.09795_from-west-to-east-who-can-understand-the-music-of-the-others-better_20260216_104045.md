---
ver: rpa2
title: 'From West to East: Who can understand the music of the others better?'
arxiv_id: '2307.09795'
source_url: https://arxiv.org/abs/2307.09795
tags:
- music
- audio
- learning
- transfer
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the transferability of music audio embedding
  models across different cultural domains, including Western, Eastern Mediterranean,
  and Indian art music. The research question is whether pre-trained models on Western
  music can effectively learn representations for non-Western music cultures.
---

# From West to East: Who can understand the music of the others better?

## Quick Facts
- arXiv ID: 2307.09795
- Source URL: https://arxiv.org/abs/2307.09795
- Reference count: 0
- Key outcome: Transfer learning across Western, Eastern Mediterranean, and Indian music cultures shows competitive performance, with Western datasets generally performing well and non-Western datasets contributing significantly to cross-cultural learning.

## Executive Summary
This study investigates the transferability of music audio embedding models across different cultural domains, including Western, Eastern Mediterranean, and Indian art music. Three deep learning architectures—VGG-ish, Musicnn, and AST—are employed for automatic music tagging across six datasets. The research question is whether pre-trained models on Western music can effectively learn representations for non-Western music cultures. Transfer learning experiments show that models trained on one cultural domain can be successfully adapted to others, with competitive performance achieved in all cases. The results indicate that while Western datasets (MagnaTagATune and FMA) consistently perform well, non-Western datasets also contribute significantly to cross-cultural learning.

## Method Summary
The study employs three deep learning architectures—VGG-ish, Musicnn, and AST—for automatic music tagging across six datasets spanning Western, Eastern Mediterranean, and Indian music cultures. Models are pre-trained on source datasets and then fine-tuned on target datasets using two approaches: fine-tuning only the output layer and fine-tuning the entire network. The datasets include MagnaTagATune and FMA-medium (Western), Lyra and Turkish-makam (Eastern Mediterranean), and Hindustani and Carnatic (Indian art music). Transfer learning experiments assess the effectiveness of cross-cultural adaptation, with performance measured using ROC-AUC and PR-AUC metrics.

## Key Results
- Models trained on Western datasets consistently achieve high performance when transferred to non-Western datasets.
- Non-Western datasets also contribute significantly to cross-cultural learning, demonstrating bidirectional transferability.
- Fine-tuning only the output layer is often sufficient for successful transfer, though full-network fine-tuning can provide additional gains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning across music cultures is effective because deep audio embeddings capture generalizable timbral and temporal patterns that are shared across diverse musical traditions.
- Mechanism: Pre-trained models trained on Western music datasets encode high-level features (e.g., instrument timbres, rhythmic structures) that are also present in non-Western music, enabling successful fine-tuning for cross-cultural tasks.
- Core assumption: Timbral and temporal features are largely invariant across cultures, so the learned representations transfer well even when musical scales or structures differ.
- Evidence anchors:
  - [abstract] "models trained on one cultural domain can be successfully adapted to others, with competitive performance achieved in all cases"
  - [section] "we expect that when applying transfer learning by parameter sharing, the more the similarity between the participating domains the better the performance"
  - [corpus] Weak—no direct corpus evidence cited, but implied by consistent performance across domains
- Break condition: If a target domain's audio features are fundamentally different from those in the source domain (e.g., highly non-Western pitch systems without overlapping timbral patterns), transfer performance will degrade.

### Mechanism 2
- Claim: Fine-tuning only the output layer is often sufficient for cross-cultural transfer because the earlier layers already capture generic music features, and only the classification layer needs adaptation to new labels.
- Mechanism: The convolutional and attention layers in VGG-ish, Musicnn, and AST act as feature extractors that remain valid across domains; adjusting only the final dense layers allows efficient adaptation without overfitting.
- Core assumption: The deep feature hierarchy learned on the source domain generalizes well enough that only the mapping to new labels requires retraining.
- Evidence anchors:
  - [abstract] "fine-tuning only the output layer as well as all the layers are reported"
  - [section] "we use the same hyper-parameters and evaluation procedure... in order to keep the duration of the training to less than 24 hours for each task"
  - [corpus] Weak—no explicit corpus evidence, but supported by reported competitive ROC-AUC scores
- Break condition: If the source and target label spaces are very different (e.g., Western genre labels vs. non-Western modal systems), partial fine-tuning may be insufficient and full-network fine-tuning becomes necessary.

### Mechanism 3
- Claim: The diversity of the source dataset improves cross-cultural transferability by exposing the model to a wider variety of timbral and rhythmic patterns.
- Mechanism: Training on multiple Western and non-Western datasets increases the diversity of learned embeddings, which in turn makes the model more adaptable to unseen musical traditions.
- Core assumption: Models trained on heterogeneous data develop richer, more generalizable feature representations than those trained on a single culture.
- Evidence anchors:
  - [abstract] "Western datasets (MagnaTagATune and FMA) consistently perform well, non-Western datasets also contribute significantly to cross-cultural learning"
  - [section] "we incorporate a mosaic of different cultures by including six datasets from Western to Mediterranean and Indian music"
  - [corpus] Weak—no direct corpus citation, but implied by the selection of diverse datasets and their comparative performance
- Break condition: If the diversity leads to conflicting feature representations (e.g., conflicting tuning systems), the model may struggle to generalize rather than benefit.

## Foundational Learning

- Concept: Transfer learning in deep neural networks
  - Why needed here: The paper relies on transferring knowledge from pre-trained models to new cultural domains, so understanding how parameters are reused and fine-tuned is essential.
  - Quick check question: What is the difference between fine-tuning the entire network versus only the output layer?

- Concept: Multi-label classification metrics (ROC-AUC, PR-AUC)
  - Why needed here: The study evaluates model performance using these metrics, which are standard for imbalanced multi-label problems.
  - Quick check question: Why is PR-AUC preferred over ROC-AUC when datasets are imbalanced?

- Concept: Audio feature extraction (mel-spectrograms)
  - Why needed here: All models in the study use mel-spectrograms as input, so understanding their construction and role is critical for interpreting results.
  - Quick check question: What parameters define a mel-spectrogram, and how do they affect the input to CNNs vs. Transformers?

## Architecture Onboarding

- Component map:
  - Audio input -> Mel-spectrogram (128 mel bands, 8 kHz max frequency, 16 kHz sample rate) -> VGG-ish/Musicnn/AST model -> Output layer (fine-tuned or replaced) -> ROC-AUC/PR-AUC evaluation

- Critical path:
  1. Convert audio to mel-spectrogram using Librosa
  2. Load pre-trained model for source dataset
  3. Replace or copy output layer to match target label set
  4. Fine-tune (partial or full)
  5. Evaluate on target validation set

- Design tradeoffs:
  - Training time vs. performance: Fine-tuning only the output layer is faster but may underperform if domains are very different.
  - Model choice: CNNs (VGG-ish, Musicnn) are faster to train; AST may capture longer-range dependencies but is slower.
  - Label space alignment: Requires careful mapping when source and target tags differ semantically.

- Failure signatures:
  - Very low ROC-AUC (< 0.5) suggests the model is not learning meaningful features from the target domain.
  - Overfitting signs: High training ROC-AUC but low validation ROC-AUC.
  - Slow convergence or divergence may indicate mismatched learning rates or incompatible label spaces.

- First 3 experiments:
  1. Single-domain baseline: Train each model from scratch on MagnaTagATune and evaluate ROC-AUC.
  2. Cross-domain output-layer fine-tuning: Transfer from MagnaTagATune to Lyra and evaluate.
  3. Full-network fine-tuning comparison: Repeat experiment 2 but fine-tune all layers, compare ROC-AUC to experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the semantic similarities between labels across different music cultures affect the transferability of music audio embedding models?
- Basis in paper: [explicit] The paper identifies this as a limitation and suggests examining semantic similarities between labels in future work.
- Why unresolved: The current study does not analyze the semantic relationships between tags across different cultural datasets, which could influence transfer learning performance.
- What evidence would resolve it: A detailed semantic analysis comparing labels across cultures, followed by transfer learning experiments controlling for label similarity.

### Open Question 2
- Question: How would incorporating raw audio processing models (like Wave-U-Net) compare to spectrogram-based models in cross-cultural transfer learning tasks?
- Basis in paper: [explicit] The paper mentions Wave-U-Net as a raw audio model used in MIR but does not include it in their experiments.
- Why unresolved: The study only used spectrogram-based models, leaving the question of whether raw audio models might perform differently in cross-cultural settings.
- What evidence would resolve it: Direct comparison experiments using raw audio models alongside spectrogram-based models across the same cultural transfer tasks.

### Open Question 3
- Question: Can semi-supervised or unsupervised learning techniques improve cross-cultural transfer learning performance compared to the fully supervised approach used in this study?
- Basis in paper: [explicit] The paper suggests that semi-supervised and unsupervised learning techniques could be explored in future work.
- Why unresolved: The current study only uses fully supervised transfer learning, without exploring how limited labeled data or unlabeled data might affect cross-cultural learning.
- What evidence would resolve it: Experiments comparing supervised transfer learning with semi-supervised and unsupervised approaches across the same cultural transfer tasks.

### Open Question 4
- Question: How do acoustic features correlate with cultural tags across different music traditions, and can these correlations improve cross-cultural understanding?
- Basis in paper: [explicit] The paper suggests using all datasets to learn music embeddings to "unveil cross-cultural links between acoustic features and tags."
- Why unresolved: While the paper suggests this direction, it does not perform an analysis of acoustic feature-tag relationships across cultures.
- What evidence would resolve it: Correlation analysis between acoustic features and tags across cultures, followed by experiments testing whether these correlations improve transfer learning performance.

## Limitations
- The study does not explore potential systematic biases in the embedding models themselves or investigate whether certain musical features are better captured than others during transfer.
- The experimental scope is constrained to six datasets, with Western datasets showing consistently strong performance across transfers, while non-Western datasets show more variable results.
- The study focuses on tagging accuracy without qualitative analysis of which features drive cross-cultural transfer success.

## Confidence

- Transfer learning effectiveness: **Medium** - supported by competitive ROC-AUC scores across all transfers, but limited by the small number of cultural domains tested
- Feature invariance assumption: **Low-Medium** - implied by consistent performance but not explicitly validated through feature analysis
- Diversity benefit claim: **Medium** - suggested by the selection of diverse datasets but not rigorously tested through controlled ablation studies

## Next Checks

1. Conduct feature importance analysis to identify which audio characteristics most strongly drive successful cross-cultural transfer
2. Test transfer performance with systematically modified datasets (e.g., pitch-shifted or tempo-altered) to isolate which musical dimensions transfer
3. Compare fine-tuning strategies across more extreme cultural pairs (e.g., Western to Indian classical) to determine failure thresholds