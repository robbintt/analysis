---
ver: rpa2
title: 'DataComp: In search of the next generation of multimodal datasets'
arxiv_id: '2304.14108'
source_url: https://arxiv.org/abs/2304.14108
tags:
- clip
- data
- score
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DataComp introduces a new benchmark for multimodal dataset design,
  flipping the traditional paradigm by fixing the training code while allowing participants
  to innovate on dataset curation. The benchmark provides a testbed of 12.8 billion
  image-text pairs from Common Crawl, with two tracks: filtering subsets of the provided
  pool or bringing external data sources.'
---

# DataComp: In search of the next generation of multimodal datasets

## Quick Facts
- arXiv ID: 2304.14108
- Source URL: https://arxiv.org/abs/2304.14108
- Reference count: 40
- Primary result: DataComp benchmark enables 9x compute reduction while improving CLIP ViT-L/14 ImageNet zero-shot accuracy to 79.2%

## Executive Summary
DataComp introduces a novel benchmark for multimodal dataset design that flips the traditional paradigm by fixing the training code while allowing participants to innovate on dataset curation. The benchmark provides a testbed of 12.8 billion image-text pairs from Common Crawl, with two tracks: filtering subsets of the provided pool or bringing external data sources. Participants evaluate their curated datasets by training CLIP models and testing on 38 downstream tasks across four compute scales. The baseline experiments demonstrate that smaller, more stringently filtered datasets can lead to better generalization than larger datasets from the same pool when total compute is constant.

## Method Summary
DataComp creates a controlled environment for dataset experimentation by holding model architecture, hyperparameters, and compute budget constant while allowing participants to propose new training sets. The benchmark consists of multiple scales ranging from 12.8M to 12.8B samples seen during training, facilitating the study of scaling trends and making the benchmark accessible to researchers with varying resources. Participants download the CommonPool dataset (12.8B image-text pairs from Common Crawl) or bring their own data, apply filtering strategies, train CLIP models using fixed procedures, and evaluate on 38 downstream tasks including ImageNet, retrieval tasks, and fairness datasets.

## Key Results
- DataComp-1B achieves 79.2% ImageNet zero-shot accuracy with CLIP ViT-L/14, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points
- The best baseline model trained on DataComp-1B outperforms a larger CLIP ViT-g/14 trained on LAION-2B by 0.7 points while using 9x less compute
- Smaller, more stringently filtered datasets can lead to better generalization than larger datasets from the same pool when total compute is constant
- The ranking between filtering strategies is typically consistent across different scales, with positive correlation between small and medium scale results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fixing the training code while innovating on dataset curation enables controlled dataset experimentation.
- Mechanism: By holding model architecture, hyperparameters, and compute budget constant, DataComp isolates the effect of dataset quality on downstream model performance.
- Core assumption: Dataset quality is the primary variable affecting model performance when other factors are fixed.
- Evidence anchors: [abstract] "DataComp flips the traditional benchmarking paradigm in machine learning where the dataset is fixed and the research community proposes new training algorithms."

### Mechanism 2
- Claim: Smaller, more stringently filtered datasets can lead to better generalization than larger datasets from the same pool when total compute is constant.
- Mechanism: By fixing the total number of samples seen during training, DataComp creates a trade-off between dataset size and quality.
- Core assumption: Quality of training data has a stronger impact on model generalization than quantity when compute is limited.
- Evidence anchors: [section 5.1] "A key result from our baselines experiments in DataComp is that smaller, more stringently filtered datasets can lead to models that generalize better than larger datasets coming from the same pool when the total amount of training compute is constant."

### Mechanism 3
- Claim: Multi-scale design facilitates the study of scaling trends and makes the benchmark accessible to researchers with varying resources.
- Mechanism: By providing four distinct compute and data scales (small to xlarge), DataComp allows researchers to participate regardless of their computational resources.
- Core assumption: Performance of dataset curation methods is consistent across different scales.
- Evidence anchors: [section 5.1] "We find that the ranking between filtering strategies is typically consistent across different scales."

## Foundational Learning

- Concept: Multimodal learning
  - Why needed here: Understanding how models learn from both image and text data is crucial for comprehending DataComp's focus on image-text dataset curation.
  - Quick check question: What are the key differences between unimodal and multimodal learning approaches?

- Concept: Zero-shot learning
  - Why needed here: DataComp evaluates models using zero-shot learning, making it important to understand this evaluation protocol.
  - Quick check question: How does zero-shot learning differ from traditional supervised learning in terms of model evaluation?

- Concept: Dataset filtering and curation
  - Why needed here: DataComp's core innovation is in dataset curation, making understanding filtering techniques essential.
  - Quick check question: What are the main challenges in filtering large-scale web-scraped datasets for multimodal learning?

## Architecture Onboarding

- Component map:
  - DataComp benchmark framework
    - Two tracks: Filtering and BYOD
    - Four compute scales: small, medium, large, xlarge
  - CommonPool dataset
    - 12.8B image-text pairs from Common Crawl
    - Safety preprocessing: NSFW filtering, face blurring, deduplication
  - Evaluation suite
    - 38 downstream tasks including ImageNet, retrieval tasks, fairness datasets
    - Zero-shot evaluation protocol
  - Baseline experiments
    - Various filtering strategies
    - Comparison with LAION datasets

- Critical path:
  1. Download CommonPool or BYOD data
  2. Apply filtering strategy
  3. Train CLIP model using fixed hyperparameters
  4. Evaluate on 38 downstream tasks
  5. Submit results to leaderboard

- Design tradeoffs:
  - Fixed training code vs. dataset innovation
  - Pool size vs. dataset quality
  - Scale accessibility vs. computational cost
  - Safety filtering vs. data diversity

- Failure signatures:
  - Poor performance on downstream tasks
  - High correlation between ImageNet accuracy and average performance
  - Inconsistent ranking of filtering strategies across scales
  - Overfitting to evaluation sets (detected through deduplication)

- First 3 experiments:
  1. Train on entire CommonPool without filtering to establish baseline
  2. Apply basic filtering (language, caption length, image size) to create subset
  3. Use CLIP score filtering with ViT-L/14 model to select top 30% of samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size for a multimodal dataset when considering the trade-off between data quality and quantity?
- Basis in paper: [explicit] The paper discusses how participants in DataComp face a trade-off between dataset size (more data points are better) and data quality (higher quality data points are better).
- Why unresolved: While the paper presents DataComp-1B as a successful dataset with 1.4 billion samples, it acknowledges that this is just the first step and expects future work to discover further dataset improvements.
- What evidence would resolve it: Comparative studies of models trained on datasets of varying sizes, all with the same quality filtering, would help determine the point of diminishing returns in dataset size.

### Open Question 2
- Question: How do different filtering techniques affect the diversity and fairness of multimodal datasets?
- Basis in paper: [explicit] The paper mentions that DataComp includes an evaluation of fairness and biases in the models trained on the datasets, using datasets like Dollar Street and GeoDE.
- Why unresolved: While the paper presents initial findings on fairness and biases, it acknowledges that these are areas for further exploration.
- What evidence would resolve it: Comprehensive studies comparing the diversity and fairness of datasets created using different filtering techniques, across various demographic groups and tasks, would help understand the trade-offs between data quality and fairness.

### Open Question 3
- Question: What is the impact of combining multiple data sources on the performance of multimodal models?
- Basis in paper: [explicit] The paper introduces the Bring Your Own Data (BYOD) track in DataComp, which allows participants to combine multiple data sources.
- Why unresolved: While the paper shows promising results from combining data sources, it acknowledges that there is still much to learn about the optimal way to combine data sources and the impact on model performance.
- What evidence would resolve it: Extensive experiments comparing the performance of models trained on datasets created by combining different data sources, in various proportions and with different filtering techniques, would help understand the optimal strategies for data source combination.

## Limitations

- The benchmark's reliance on CLIP-based filtering raises concerns about potential bias toward CLIP-like representations
- The evaluation suite's composition, with ImageNet dominating the overall score, may not fully capture downstream task diversity
- The CommonPool dataset's safety preprocessing pipeline details are not fully specified, making exact reproduction challenging

## Confidence

- **High confidence**: The core claim that fixing training code while innovating on dataset curation enables controlled experimentation is well-supported by the benchmark design and baseline results.
- **Medium confidence**: The claim that smaller, more stringently filtered datasets can outperform larger ones from the same pool is supported by baseline experiments but may not generalize to all dataset curation methods.
- **Medium confidence**: The multi-scale design's effectiveness in facilitating scaling studies is supported by initial results, but long-term consistency across scales remains to be seen.

## Next Checks

1. **Validation of safety filtering reproducibility**: Attempt to reproduce the exact NSFW filtering pipeline using the described components (CRISTA, MFN, CLIP) with specified thresholds.

2. **Testing dataset curation method consistency**: Verify if the ranking of filtering strategies remains consistent across all four compute scales using multiple independent dataset curation methods.

3. **Evaluating CLIP score filter bias**: Assess whether CLIP score-based filtering creates unintended bias by training models on datasets filtered by different methods and comparing their CLIP score distributions.