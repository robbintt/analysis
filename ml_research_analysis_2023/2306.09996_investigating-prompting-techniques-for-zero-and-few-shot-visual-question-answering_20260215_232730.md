---
ver: rpa2
title: Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering
arxiv_id: '2306.09996'
source_url: https://arxiv.org/abs/2306.09996
tags:
- question
- blip2
- answer
- task
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates various prompting strategies to improve
  zero- and few-shot Visual Question Answering (VQA) performance in Vision-Language
  Models (VLMs), specifically focusing on the BLIP2 model. The study examines the
  impact of different question templates, the use of image captions as additional
  visual cues, the application of chain-of-thought (CoT) reasoning, and the incorporation
  of text-only few-shot exemplars.
---

# Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering

## Quick Facts
- arXiv ID: 2306.09996
- Source URL: https://arxiv.org/abs/2306.09996
- Reference count: 15
- One-line primary result: BLIP2 VQA performance is highly sensitive to prompt design; image captions and few-shot exemplars improve results, but chain-of-thought reasoning hurts accuracy.

## Executive Summary
This paper investigates how different prompting strategies affect zero- and few-shot Visual Question Answering (VQA) performance in Vision-Language Models (VLMs), with a focus on the BLIP2 architecture. The study systematically evaluates the impact of question templates, image captions, chain-of-thought (CoT) reasoning, and text-only few-shot exemplars on VQA accuracy across multiple datasets. Key findings reveal that the choice of prompt template significantly influences model outputs, that captions can improve performance when paired with exemplars, and that CoT prompts tend to degrade accuracy due to hallucinated rationales. The results underscore the importance of carefully designed prompts and highlight limitations in current multimodal CoT reasoning.

## Method Summary
The study evaluates BLIP2 (with both OPT and FLAN-T5 language models) on VQA datasets (OKVQA, AOKVQA, GQA, Winoground-QA) using zero- and few-shot prompting. Researchers test different question templates, incorporate model-generated image captions as visual cues, apply CoT reasoning, and use text-only few-shot exemplars. Performance is measured by VQA accuracy, with prompts structured to guide the LM's output format and reasoning. Few-shot exemplars are selected via nearest-neighbor similarity, and beam search is used for answer generation.

## Key Results
- Question template choice significantly impacts VQA accuracy, with the OPT variant being especially sensitive to formatting.
- Incorporating image captions as prefixes improves performance when combined with few-shot exemplars.
- Chain-of-thought reasoning leads to longer rationales but does not improve—and often harms—VQA accuracy due to hallucinated reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question templates guide the model toward specific answer formats and reasoning styles.
- Mechanism: Templates like "Question: <q> Answer:" or "Your task is to answer..." set up a structured prompt that conditions the model's output to follow a particular format. The BLIP2 FLAN-T5 LM, being instruction-tuned, responds well to explicit task framing, while the OPT variant is more sensitive and may require stricter formatting to avoid empty outputs.
- Core assumption: The model's pre-training exposure to QA pairs or task-specific instructions determines how effectively it can adapt to different template structures.
- Evidence anchors:
  - [abstract] "Central to our investigation is the role of question templates in guiding VLMs to generate accurate answers."
  - [section 4.1] "We observed significant variations in the effectiveness of different question templates, with the instruction-tuned BLIP2 FLAN-T5 model showing moderate sensitivity to template variations, while OPT exhibiting considerable performance discrepancies depending on the chosen template."
- Break condition: If the template introduces ambiguity or overly complex language, or if the model lacks pre-training exposure to similar patterns, the guiding effect weakens or reverses.

### Mechanism 2
- Claim: Incorporating image captions as additional visual cues improves model performance when paired with few-shot exemplars.
- Mechanism: Captions provide supplementary context that complements the image features, but the model requires in-context exemplars to learn how to effectively integrate this extra information. Few-shot exemplars teach the model the mapping between caption, question, and answer, enabling better utilization of captions.
- Core assumption: VLMs do not inherently "see" captions as visual cues unless explicitly guided by exemplars; captions alone are insufficient without few-shot context.
- Evidence anchors:
  - [abstract] "We identify that specific templates significantly influence VQA outcomes, underscoring the need for strategic template selection."
  - [section 4.3] "We find that BLIP2's performance improved by incorporating model generated image-captions... BLIP2 does not have the bootstrapped CoT abilities of FLAN T5."
- Break condition: If captions are irrelevant or noisy, or if the exemplars are too similar to the query (causing copying), performance can degrade.

### Mechanism 3
- Claim: Chain-of-thought (CoT) reasoning does not improve VQA accuracy for BLIP2, and may hurt performance.
- Mechanism: Unlike large language models with strong CoT capabilities, BLIP2's FLAN-T5 LM lacks robust multi-step reasoning bootstrapped into its architecture. CoT prompts lead to longer, sometimes hallucinated rationales that distract from accurate final answers.
- Core assumption: The FLAN-T5 model in BLIP2 was not fine-tuned on CoT data specific to multimodal tasks, limiting its ability to generate correct intermediate reasoning steps.
- Evidence anchors:
  - [abstract] "we also identify a limitation in the use of chain-of-thought rationalization, which negatively affects VQA accuracy."
  - [section 4.4] "incorporating CoT prompting for rationalization resulted in a significant drop in performance... BLIP2 does not have the bootstrapped CoT abilities of FLAN T5."
- Break condition: If the model is explicitly fine-tuned on multimodal CoT data, or if the CoT examples in the prompt are high-quality and relevant, the performance drop may be mitigated.

## Foundational Learning

- Concept: Prompt engineering and in-context learning
  - Why needed here: BLIP2 relies on prompting rather than task-specific fine-tuning, so understanding how different prompt structures and exemplars affect performance is critical for zero- and few-shot VQA.
  - Quick check question: What happens to BLIP2's performance when you add a few-shot exemplar but no caption?
    - Expected answer: Performance typically decreases (as shown in Table 3, rows with "question(n=5)"), because the model lacks guidance on how to integrate the exemplar information.

- Concept: Vision-language model architecture (e.g., BLIP2)
  - Why needed here: Knowing that BLIP2 combines a frozen image encoder with a language model (OPT or FLAN-T5) helps explain why image features and text conditioning interact differently across variants.
  - Quick check question: Why does BLIP2 FLAN-T5 handle multiple-choice questions better than the OPT variant?
    - Expected answer: The FLAN-T5 LM was fine-tuned on more diverse QA tasks, including multiple-choice formats, giving it better task alignment.

- Concept: Chain-of-thought reasoning limitations in smaller models
  - Why needed here: Understanding why CoT doesn't help in BLIP2 requires recognizing that its LM lacks the scale or multimodal CoT fine-tuning present in larger models like GPT-3.
  - Quick check question: What is one reason BLIP2's CoT performance drops compared to standard prompting?
    - Expected answer: BLIP2 generates lengthy, hallucinated rationales that distract from the correct final answer.

## Architecture Onboarding

- Component map:
  - Frozen image encoder (e.g., ViT or similar) → extracts visual features
  - Language model (OPT or FLAN-T5) → generates answers conditioned on image features, text prompts, and optional captions
  - Prompt encoder → formats input (question, caption, exemplars) into model-compatible text
  - Beam search decoder → produces final answer (beam size 5, length penalty tuned per task)

- Critical path:
  Input (image, question, caption, exemplars) → image encoder → feature vector → concatenated with prompt text → LM generates answer

- Design tradeoffs:
  - Using FLAN-T5 vs OPT: FLAN-T5 offers better instruction-following but is smaller; OPT is larger but less aligned to task formats
  - Caption inclusion: Improves performance when combined with exemplars, but adds generation cost and dependency on caption quality
  - CoT vs standard: Standard prompting is more reliable for BLIP2; CoT adds reasoning steps that often mislead

- Failure signatures:
  - Empty or nonsensical outputs: Often due to improper template formatting for the OPT variant
  - Copying from exemplars: Indicates exemplar similarity threshold too high
  - Hallucinated answers: Common in CoT mode, especially when image context is insufficient

- First 3 experiments:
  1. Run Standard VQA with a null template vs a structured template (e.g., "Question: <q> Answer:") to observe sensitivity
  2. Add five few-shot exemplars without captions; record drop in accuracy
  3. Add captions (a-photo-of) with the same exemplars; check for accuracy recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal question template for different types of VQA tasks (e.g., knowledge-based, compositional reasoning)?
- Basis in paper: Inferred from the observation that the choice of question template significantly influences VQA outcomes and different templates perform differently across tasks.
- Why unresolved: The paper tested a limited set of templates and found performance variations, but did not systematically explore all possible template variations or determine which templates work best for specific task types.
- What evidence would resolve it: Systematic testing of a comprehensive set of question templates across different VQA task types, with statistical analysis of performance differences.

### Open Question 2
- Question: What is the ideal number and selection strategy for text-only few-shot exemplars in VQA prompting?
- Basis in paper: Explicit from the finding that few-shot exemplars hurt performance when used alone, but help when combined with captions or CoT rationales.
- Why unresolved: The paper only tested with 5 exemplars and a simple nearest-neighbor selection strategy. The negative results suggest the selection method and number of exemplars need optimization.
- What evidence would resolve it: Controlled experiments varying the number of exemplars (e.g., 1, 3, 5, 10) and testing different selection strategies (e.g., diversity-based, difficulty-based) while measuring performance impact.

### Open Question 3
- Question: What is the optimal way to incorporate chain-of-thought reasoning in multimodal VQA models?
- Basis in paper: Inferred from the observation that CoT reasoning leads to performance drops despite the model being well-suited for CoT prompting due to its instruction-tuned LLM.
- Why unresolved: The paper tested standard CoT templates and some variations but found consistent performance degradation, suggesting the current approach to CoT is not suitable for VQA.
- What evidence would resolve it: Development and testing of novel CoT prompting strategies specifically designed for VQA, potentially incorporating visual reasoning steps or different rationale formats.

## Limitations
- The study focuses on a single VLM architecture (BLIP2) with two language model variants, limiting generalizability to other VLMs.
- Image caption quality and relevance are not evaluated, which could impact the effectiveness of caption-based improvements.
- The selection criteria for few-shot exemplars and their similarity thresholds are not explicitly specified.
- The analysis does not explore the impact of caption quality or relevance on performance.

## Confidence
- High: The impact of question templates on VQA performance, particularly the sensitivity of the OPT variant to template variations
- High: The effectiveness of incorporating image captions as additional visual cues when combined with few-shot exemplars
- Medium: The negative impact of CoT reasoning on VQA accuracy for BLIP2, given that this may be model-specific and dependent on the LM's pre-training

## Next Checks
1. Test the proposed prompting techniques on a different VLM architecture (e.g., Flamingo or GIT) to assess generalizability.
2. Evaluate the impact of caption quality by using both high-quality and noisy captions in the prompting pipeline.
3. Investigate the effect of exemplar similarity thresholds by systematically varying the similarity cutoff and measuring performance changes.