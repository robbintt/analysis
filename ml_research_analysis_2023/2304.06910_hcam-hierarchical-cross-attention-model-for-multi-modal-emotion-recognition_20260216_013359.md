---
ver: rpa2
title: HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition
arxiv_id: '2304.06910'
source_url: https://arxiv.org/abs/2304.06910
tags:
- emotion
- audio
- text
- stage
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hierarchical cross-attention model (HCAM)
  for multi-modal emotion recognition in conversations. The model uses wav2vec for
  audio features and RoBERTa for text representations, processing them through bi-directional
  recurrent neural networks with self-attention to generate utterance-level embeddings.
---

# HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition

## Quick Facts
- arXiv ID: 2304.06910
- Source URL: https://arxiv.org/abs/2304.06910
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on IEMOCAP, MELD, and CMU-MOSI datasets using hierarchical cross-attention fusion of audio and text modalities

## Executive Summary
This paper introduces a hierarchical cross-attention model (HCAM) for multi-modal emotion recognition in conversations. The model combines wav2vec 2.0 for audio features and RoBERTa for text representations, processing them through bi-directional recurrent neural networks with self-attention to generate utterance-level embeddings. A co-attention layer then combines the audio and text embeddings to incorporate contextual knowledge and weigh relevant information for emotion recognition. The model is trained hierarchically in three stages: utterance-level modeling, inter-utterance contextual modeling, and multi-modal fusion with cross-attention. Experiments on three benchmark datasets show that HCAM significantly outperforms existing methods and achieves state-of-the-art results, particularly benefiting from the hierarchical training approach.

## Method Summary
HCAM uses wav2vec 2.0 and RoBERTa to extract audio and text features respectively, which are then processed through bi-directional GRUs with self-attention to create utterance-level embeddings. The model employs a three-stage hierarchical training approach: first training utterance-level classifiers for each modality using cross-entropy and supervised contrastive loss, then adding contextual modeling with inter-utterance GRU and self-attention, and finally fusing modalities using co-attention. During inference, predictions from different stages are combined through ensembling to improve performance. The supervised contrastive loss helps the model focus on hard examples by pulling embeddings of same-class utterances closer while pushing different-class embeddings apart.

## Key Results
- HCAM achieves state-of-the-art weighted F1-scores on IEMOCAP, MELD, and CMU-MOSI datasets
- The hierarchical training approach significantly improves performance compared to non-hierarchical alternatives
- Ensembling predictions from different training stages further boosts classification accuracy
- The model demonstrates robustness to text modality noise when tested with ASR-generated transcripts

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical training allows progressive complexity, improving generalization by learning simpler utterance-level patterns before contextual dependencies. The three-stage training where stage I learns utterance-level classification, stage II adds inter-utterance context via bi-GRU with self-attention, and stage III fuses modalities using co-attention, with each stage building on the previous without fine-tuning earlier parameters. This curriculum-style progression is based on the assumption that emotion recognition benefits from learning simpler, context-free classification before tackling more complex contextual modeling.

### Mechanism 2
Supervised contrastive loss improves discrimination of hard examples by pulling embeddings of same-class utterances closer while pushing different-class embeddings apart. The temperature-scaled loss encourages similarity between representations of utterances with the same emotion label within a mini-batch, sharpening decision boundaries for emotion recognition datasets that contain confusing pairs where class boundaries are fuzzy.

### Mechanism 3
Co-attention fusion captures complementary modality-specific cues and dynamically weighs utterance relevance across modalities. The cross-attention mechanism where query is from one modality and key/value from the other, followed by self-attention and concatenation, enables modality-specific contextual weighting. This approach is based on the assumption that emotional cues are distributed differently across modalities, with some emotions better captured in audio and others in text.

## Foundational Learning

- **Self-attention mechanism**: Enables the model to weigh the importance of different utterances in a conversation when modeling context, capturing long-range dependencies beyond bi-GRU alone. Quick check: In a conversation with utterances [U1, U2, U3], if U2 is highly self-attentive to U1 and U3, what does that imply about its role in emotion flow?

- **Contrastive loss and temperature scaling**: Encourages the model to cluster same-class utterance embeddings while separating different classes, improving emotion classification boundaries. Quick check: If temperature Ï„ is set very low in contrastive loss, what happens to the gradient magnitude for hard examples?

- **Hierarchical curriculum learning**: Allows the model to first learn basic utterance-level emotion cues before tackling the more complex inter-utterance contextual modeling, improving convergence and stability. Quick check: Why might freezing earlier stage parameters during later stage training help prevent catastrophic forgetting?

## Architecture Onboarding

- **Component map**: Wav2vec 2.0 (fine-tuned) -> 1D-CNN -> utterance embedding (audio path) -> Bi-GRU with self-attention -> co-attention fusion -> FC layers + softmax; RoBERTa -> Bi-GRU -> utterance embedding (text path) -> Bi-GRU with self-attention -> co-attention fusion -> FC layers + softmax

- **Critical path**: Wav2vec -> CNN -> self-attention -> co-attention -> classifier

- **Design tradeoffs**: Hierarchical stages reduce memory load but require careful checkpointing and inference ensembling; fine-tuning only transformer layers in wav2vec preserves lower-level features but may limit adaptation; co-attention symmetry adds parameters but captures richer cross-modal dependencies

- **Failure signatures**: Stage I accuracy plateaus early indicates insufficient capacity or noisy inputs; stage II performance drops suggests self-attention not learning meaningful weights and may need more data or regularization; co-attention underperforms indicates modality imbalance or noisy transcripts and may require weighting fusion

- **First 3 experiments**: 1) Train stage I audio and text separately; verify baseline utterance-level accuracy; 2) Add stage II Bi-GRU with and without self-attention; measure context gain; 3) Combine stages with co-attention; test multi-modal fusion gain over best single modality

## Open Questions the Paper Calls Out

- How does the hierarchical training approach specifically improve the model's ability to handle long conversations with many utterances? While the paper demonstrates improved performance, it doesn't provide a detailed analysis of how the hierarchical approach specifically addresses the challenges of long conversations.

- What is the impact of the supervised contrastive loss on the model's ability to generalize to unseen emotional states or nuances? The paper mentions that the supervised contrastive loss helps the model focus on hard-to-classify samples, but doesn't explore its impact on generalization to novel emotional states.

- How does the model's performance change when incorporating visual modality in addition to audio and text? The paper mentions plans to extend the approach to include visual modality in the future, suggesting that the current model is limited to audio and text.

## Limitations

- Architecture details such as specific implementation of co-attention network including number of attention heads and hidden dimensions are not provided
- Hyperparameter settings for supervised contrastive loss including temperature and weight values are not specified
- Dataset generalization beyond the three reported datasets is unclear, with no testing on additional emotion recognition datasets

## Confidence

**High Confidence**: The hierarchical training approach is well-justified and aligns with established curriculum learning principles; the use of pre-trained models for feature extraction is a sound choice given their proven effectiveness.

**Medium Confidence**: The combination of supervised contrastive loss with emotion recognition shows promise but its effectiveness may depend heavily on batch size and class distribution; the co-attention mechanism for multimodal fusion is theoretically sound but practical benefits may be limited if one modality is significantly noisier.

**Low Confidence**: The claim that hierarchical modeling is the primary driver of performance gains, as the paper doesn't provide ablation studies isolating the impact of each stage.

## Next Checks

1. Conduct ablation studies removing self-attention from stage II and co-attention from stage III to quantify their individual contributions to overall performance.

2. Evaluate model performance using automated speech recognition (ASR) transcripts with varying word error rates to assess robustness to text modality noise.

3. Test the trained model on an additional emotion recognition dataset not used in training to evaluate generalization capabilities beyond the three datasets reported.