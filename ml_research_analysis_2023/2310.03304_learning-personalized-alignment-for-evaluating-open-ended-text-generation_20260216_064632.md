---
ver: rpa2
title: Learning Personalized Alignment for Evaluating Open-ended Text Generation
arxiv_id: '2310.03304'
source_url: https://arxiv.org/abs/2310.03304
tags:
- plot
- evaluation
- story
- reviewer
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PerSE, a personalized evaluation framework
  for open-ended text generation that addresses the limitations of traditional metrics
  which rely on lexical similarity and fail to account for diverse human preferences.
  The framework is trained to infer specific preferences from in-context personal
  profiles and evaluate the alignment between generated content and these preferences.
---

# Learning Personalized Alignment for Evaluating Open-ended Text Generation

## Quick Facts
- arXiv ID: 2310.03304
- Source URL: https://arxiv.org/abs/2310.03304
- Reference count: 40
- Primary result: PERSE achieves 15.8% higher Kendall correlation and 13.7% higher accuracy than GPT-4 in personalized story evaluation

## Executive Summary
This paper introduces PERSE, a personalized evaluation framework for open-ended text generation that addresses the limitations of traditional metrics which rely on lexical similarity and fail to account for diverse human preferences. PERSE is trained to infer specific preferences from in-context personal profiles and evaluate the alignment between generated content and these preferences. The framework demonstrates significant improvements over GPT-4, achieving a 15.8% increase in Kendall correlation and a 13.7% rise in accuracy with zero-shot reviewers. PERSE also shows strong transferability, outperforming GPT-4 by 46.01% in Kendall correlation on new domains.

## Method Summary
PERSE is a 13B LLaMA-2-based model trained through instruction-tuning on two datasets: Per-MPST for individual story evaluation and Per-DOC for comparative evaluation. The model infers reviewer preferences from their historical reviews and uses chain-of-thought reasoning to generate evaluations. To address dataset contamination, the authors created Per-MPST and Per-DOC by anonymizing and summarizing existing datasets to reduce memorization bias. PERSE ind is trained for individual evaluation while PERSE comp handles comparative evaluation across five aspects: Interestingness, Adaptability, Surprise, Character Development, and Ending.

## Key Results
- 15.8% increase in Kendall correlation compared to GPT-4 for individual story evaluation
- 13.7% higher accuracy in comparative evaluation with zero-shot reviewers
- 46.01% better Kendall correlation than GPT-4 on new domains, indicating strong transferability

## Why This Works (Mechanism)

### Mechanism 1
Personalized evaluation outperforms general evaluation by aligning with reviewer-specific preferences. PERSE uses instruction-tuning on reviewer-specific reviews to infer individual preferences, then generates scores and explanations tailored to those preferences. Core assumption: Reviewer preferences are consistent across their reviews and can be inferred from limited examples. Evidence anchors: PERSE achieves a 15.8% higher Kendall correlation in individual story evaluation and a 13.7% higher accuracy in comparative evaluation. Break condition: Preferences change over time, or insufficient review examples fail to capture preference patterns.

### Mechanism 2
Anonymization and summarization reduce memorization bias in evaluation datasets. Replacing identifiable names and summarizing plots decreases the likelihood that LLMs recognize and are biased by memorized content. Core assumption: Reducing memorization exposure leads to more reliable evaluation on novel content. Evidence anchors: The neither known group exhibits much lower accuracy despite keeping main plot points, indicating that memorization can result in misleadingly high performance. By alleviating memorization problem with anonymization and summarization, we can also reduce position bias and improve consistency in evaluation. Break condition: Anonymization removes too much context, or summarization alters plot meaning.

### Mechanism 3
Chain-of-thought reasoning improves alignment with reviewer preferences. PERSE generates review content before predicting scores, allowing the model to reason through the preference before finalizing judgment. Core assumption: Explicit reasoning helps capture nuanced preference patterns better than direct scoring. Evidence anchors: Instead of directly predicting the numerical score y, PERSE generates the review content before giving the score, akin to chain-of-thought. PERSE cares more about the steady terribleness and only gives 3, which is more consistent with this reviewer's true score. Break condition: Reviewer preferences are too complex for simple chain-of-thought generation.

## Foundational Learning

- Concept: Personalization in evaluation
  - Why needed here: Traditional metrics fail to capture diverse human preferences in open-ended generation tasks
  - Quick check question: Why do general evaluation metrics show poor correlation with human judgments in story generation?

- Concept: Instruction-tuning for preference alignment
  - Why needed here: Pre-trained LLMs are aligned to general human values, not individual preferences
  - Quick check question: What training method enables LLMs to capture reviewer-specific preferences?

- Concept: Dataset contamination and memorization
  - Why needed here: Exposure to training data can bias LLM evaluation results, making them unreliable for novel content
  - Quick check question: How does memorization affect the reliability of LLM-based story evaluation?

## Architecture Onboarding

- Component map: Input → Preference inference → Reasoning generation → Score prediction → Output formatting
- Critical path: Input → Preference inference → Reasoning generation → Score prediction → Output formatting
- Design tradeoffs: Larger models (13B vs 7B) improve correlation but increase computational cost; joint training on multiple aspects increases data efficiency but may dilute aspect-specific signals
- Failure signatures: Poor correlation with human scores indicates failed preference inference; inconsistent predictions across different review orders suggest sensitivity to input order
- First 3 experiments:
  1. Compare PERSE performance with k=1 vs k=3 reviewer examples to assess preference capture robustness
  2. Evaluate PERSE on anonymized vs summarized plots to measure memorization bias reduction
  3. Test PERSE with and without review content in the preference history to quantify chain-of-thought impact

## Open Questions the Paper Calls Out

### Open Question 1
How many reviews are needed to accurately capture a reviewer's preferences for personalized story evaluation? The paper states "When we increase the number of reviews, it is easier for PERSE-13b to capture the reviewer's preference" but also notes that "if not limited by the context length, we suspect that the performance of PERSE-13b will also drop after achieving its maximum capability." The paper only tested up to 5 reviews (k=5) and found diminishing returns. The optimal number likely depends on reviewer consistency and the complexity of preferences.

### Open Question 2
How well does PERSE transfer to completely different story domains (e.g., science fiction, romance) beyond the movie plots and general stories tested? The paper states "it also outperforms GPT-4 by 46.01% in Kendall correlation on new domains, indicating its transferability" but only tested on movie plots and general stories. The tested domains may share structural similarities that make them easier to transfer between. More diverse domains could present greater challenges.

### Open Question 3
Does PERSE maintain its advantage over GPT-4 when evaluating longer stories (novel-length rather than plot summaries)? The paper only tested on plot summaries and stories that appear to be shorter than full-length works, with context length limitations noted. Longer texts may require different evaluation approaches.

## Limitations
- Reliance on reviewer preference inference from limited examples may not generalize to new reviewers with distinct taste patterns
- Anonymization and summarization procedures could remove critical contextual information needed for accurate evaluation
- 13B model size limits accessibility for researchers with computational constraints

## Confidence

**High Confidence**: The correlation improvements over GPT-4 (15.8% Kendall correlation increase, 13.7% accuracy improvement) are well-supported by experimental results.

**Medium Confidence**: The transferability claims (46.01% Kendall correlation improvement on new domains) are based on cross-domain evaluation but don't fully explore performance with reviewers from entirely different domains.

**Low Confidence**: The assumption that reviewer preferences can be reliably inferred from limited historical reviews may not hold for new reviewers or those with complex, evolving preferences.

## Next Checks
1. Conduct an ablation study testing PERSE performance with varying numbers of reviewer examples (k=1, k=3, k=5) to determine the minimum effective sample size for preference inference
2. Evaluate PERSE with reviewers from domains completely absent from training data (e.g., scientific abstracts, poetry) to test preference transferability limits
3. Conduct a controlled experiment comparing PERSE performance on original plots, anonymized plots, and summarized plots separately to quantify information loss from each transformation