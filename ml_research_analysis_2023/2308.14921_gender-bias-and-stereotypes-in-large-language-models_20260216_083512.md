---
ver: rpa2
title: Gender bias and stereotypes in Large Language Models
arxiv_id: '2308.14921'
source_url: https://arxiv.org/abs/2308.14921
tags:
- gender
- bias
- sentence
- pronoun
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper examines gender stereotypes in four recently published\
  \ LLMs, testing their tendency to align pronouns with stereotypically gendered occupations.\
  \ Using a novel paradigm inspired by WinoBias but designed to avoid inclusion in\
  \ training data, the study finds that LLMs are 3-6 times more likely to choose an\
  \ occupation that stereotypically matches a person\u2019s gender."
---

# Gender bias and stereotypes in Large Language Models

## Quick Facts
- arXiv ID: 2308.14921
- Source URL: https://arxiv.org/abs/2308.14921
- Reference count: 40
- Primary result: LLMs are 3-6 times more likely to choose occupations that stereotypically match a person's gender, amplifying societal biases beyond actual workforce statistics

## Executive Summary
This study examines gender stereotypes in four recently published Large Language Models (LLMs) by testing their tendency to align pronouns with stereotypically gendered occupations. Using a novel paradigm designed to avoid inclusion in training data, the research reveals that LLMs significantly amplify gender biases beyond what is reflected in societal statistics or human perceptions. The models frequently provide misleading explanations for their choices, obscuring their true reasoning. Despite correctly resolving pronouns when explicit gender information is present, the LLMs struggle with ambiguity in sentence structure 95% of the time, defaulting to biased interpretations. These findings highlight the need for careful evaluation and mitigation strategies before widespread LLM deployment to prevent the perpetuation and amplification of societal gender biases.

## Method Summary
The study employs a novel evaluation paradigm inspired by WinoBias but designed to avoid inclusion in training data. Researchers constructed 15 sentence schemas pairing stereotypically male and female occupations with masculine and feminine pronouns in ambiguous contexts. The LLMs were prompted three times per sentence in separate sessions, with follow-up questions about alternative referents. Responses were manually coded for occupation choice, ambiguity recognition, and explanation type. The evaluation compared model behavior to US Bureau of Labor Statistics employment figures and human perception ratings from Kennison & Trofe 2003 to assess bias amplification.

## Key Results
- LLMs are 3-6 times more likely to choose occupations that stereotypically match a person's gender
- Models amplify gender bias beyond what is reflected in societal statistics or human perceptions
- LLMs provide factually inaccurate explanations that obscure their true reasoning in 95% of ambiguous cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs amplify gender bias beyond what is reflected in societal statistics
- Mechanism: Training data reflects societal stereotypes, and reinforcement learning with human feedback (RLHF) does not fully correct these biases
- Core assumption: The LLM's training corpus contains imbalanced representations of occupations by gender
- Evidence anchors:
  - [abstract] "LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth"
  - [section 4.3] "we find that the models' behavior tracks people's beliefs about gender stereotypes concerning occupations more closely than it does the actual ground truth about this distribution as reflected in the BLS statistics"
  - [corpus] Weak evidence - no direct corpus analysis of gender bias amplification

### Mechanism 2
- Claim: LLMs provide misleading explanations that obscure their true reasoning
- Mechanism: The model generates plausible-sounding but factually incorrect explanations, often invoking grammar rules or context that don't apply
- Core assumption: The LLM prioritizes producing coherent text over factual accuracy in its explanations
- Evidence anchors:
  - [abstract] "LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions"
  - [section 4.5] Examples of contradictory or incorrect grammatical explanations provided by the models
  - [corpus] No direct evidence of explanation generation mechanisms

### Mechanism 3
- Claim: LLMs struggle with ambiguity in sentence structure, often ignoring it in favor of biased interpretations
- Mechanism: The model defaults to a bias-based strategy for pronoun resolution, ignoring the possibility of multiple valid interpretations
- Core assumption: The LLM's training data contains many unambiguous sentences, leading it to expect clear referents
- Evidence anchors:
  - [abstract] "LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items"
  - [section 4.4] The models mostly chose a single interpretation even when explicitly asked about ambiguity
  - [corpus] No evidence of the LLM's training data composition or its handling of ambiguous sentences

## Foundational Learning

- Concept: Gender stereotypes and their impact on society
  - Why needed here: Understanding the context of gender bias and its potential harms is crucial for evaluating the LLM's behavior and its implications
  - Quick check question: Can you explain how gender stereotypes can lead to discrimination and harm in various domains, such as education, employment, and healthcare?

- Concept: Sentence structure and pronoun resolution
  - Why needed here: The study relies on understanding how pronouns are typically resolved in sentences and how ambiguity can arise
  - Quick check question: In the sentence "The doctor phoned the nurse because she was late," who is more likely to be late according to traditional gender stereotypes, and why might this sentence be considered ambiguous?

- Concept: Evaluation of language models and bias detection
  - Why needed here: The study uses specific methods to test for gender bias in LLMs, and understanding these methods is key to interpreting the results
  - Quick check question: What are some common techniques for evaluating bias in language models, and how does the study's approach differ from previous methods like WinoBias?

## Architecture Onboarding

- Component map: LLM (transformer-based neural network) -> Prompt generator -> Response analyzer -> Bias classifier
- Critical path: Generate text from LLM -> Analyze output for gender-stereotypical associations -> Compare results to ground truth statistics and human perceptions
- Design tradeoffs: Simplified paradigm may not capture all nuances of real-world language use; human coders introduce subjectivity
- Failure signatures: Gender bias in pronoun resolution, misleading explanations, ignoring ambiguity in sentence structure
- First 3 experiments:
  1. Test the LLM on a diverse set of occupation-pronoun combinations to quantify the extent of gender bias
  2. Compare the LLM's behavior to ground truth statistics and human perceptions to assess the degree of bias amplification
  3. Analyze the LLM's explanations for its choices to identify patterns of misleading or inaccurate reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do gender biases in LLMs vary across different languages and cultural contexts?
- Basis in paper: [inferred] The paper acknowledges its focus on English data and Western/American biases, suggesting potential cultural effects from stereotypes and biases in other societies
- Why unresolved: The study's scope is limited to English language data and Western cultural context, leaving the impact of gender biases in other languages and cultures unexplored
- What evidence would resolve it: Comparative studies of gender biases in LLMs across multiple languages and cultural contexts, examining differences in stereotype representation and bias amplification

### Open Question 2
- Question: What are the long-term societal impacts of LLMs perpetuating and amplifying gender biases?
- Basis in paper: [explicit] The paper discusses potential harms, including reinforcement of stereotypes, microaggressions, and effects on children's perceptions of their abilities and career choices
- Why unresolved: The study focuses on immediate biases but does not explore the cumulative effects of widespread LLM use over time
- What evidence would resolve it: Longitudinal studies tracking societal attitudes and behaviors related to gender over time, correlating with the increasing use of LLMs in various domains

### Open Question 3
- Question: How can LLMs be designed to accurately reflect gender distribution in professions while avoiding the amplification of stereotypes?
- Basis in paper: [explicit] The paper finds that LLMs reflect and amplify societal biases more than actual statistics, suggesting a need for better balance in representation
- Why unresolved: The study identifies the problem but does not propose specific solutions for achieving accurate representation without bias amplification
- What evidence would resolve it: Development and testing of new LLM architectures or training methodologies that prioritize factual accuracy in gender representation while minimizing stereotype reinforcement

## Limitations
- The study's findings are limited by its use of a restricted set of occupation pairs and the inability to access the exact training data of the tested LLMs
- The reliance on human coders to interpret model outputs introduces potential subjectivity in the analysis
- The study focuses on English-language LLMs and occupations in the United States, which may limit the generalizability of the results to other languages and cultural contexts

## Confidence
- **High confidence**: The finding that LLMs exhibit gender bias in pronoun resolution, as evidenced by the consistent 3-6x skew toward stereotypical gender-occupation pairings across all tested models
- **Medium confidence**: The claim that LLMs amplify societal gender biases beyond what is reflected in ground truth statistics or human perceptions, as this requires comparison across multiple data sources and assumes the accuracy of the BLS statistics and Kennison & Trofe's perception ratings
- **Low confidence**: The assertion that LLMs provide misleading explanations that obscure their true reasoning, as this relies on the subjective interpretation of human coders and the assumption that the models' explanations do not reflect their actual decision-making process

## Next Checks
1. Replicate the study with a larger and more diverse set of occupation pairs, including occupations with less clear gender stereotypes, to assess the robustness of the findings
2. Analyze the training data of the tested LLMs to identify the sources of gender bias and quantify the degree of amplification compared to the raw data
3. Develop automated methods for detecting and classifying the explanations provided by LLMs to reduce the reliance on human coders and improve the objectivity of the analysis