---
ver: rpa2
title: Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling
arxiv_id: '2308.10705'
source_url: https://arxiv.org/abs/2308.10705
tags:
- pose
- human
- shape
- motion
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unsupervised 3D human pose
  estimation from 2D keypoint sequences. It introduces a novel modeling strategy inspired
  by non-rigid structure-from-motion (NRSfM), dividing the task into estimating a
  3D reference skeleton and frame-by-frame skeleton deformations.
---

# Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling

## Quick Facts
- arXiv ID: 2308.10705
- Source URL: https://arxiv.org/abs/2308.10705
- Authors: 
- Reference count: 40
- One-line primary result: Achieves state-of-the-art unsupervised 3D pose estimation with 41.6 mm N-MPJPE and 33.1 mm PA-MPJPE on Human3.6M

## Executive Summary
This paper addresses unsupervised 3D human pose estimation from 2D keypoint sequences by introducing a novel RMNRD (Rigid Motion and Non-Rigid Deformation) modeling strategy inspired by non-rigid structure-from-motion (NRSfM). The method divides the task into estimating a 3D reference skeleton and frame-by-frame skeleton deformations, using a mixed spatial-temporal NRSfMFormer transformer architecture. A diffusion-based motion prior is incorporated to resolve depth ambiguity without 3D supervision. The approach achieves state-of-the-art performance on multiple benchmarks, notably 41.6 mm N-MPJPE and 33.1 mm PA-MPJPE on Human3.6M.

## Method Summary
The method uses a mixed spatial-temporal NRSfMFormer transformer to jointly estimate a 3D reference skeleton and skeleton deformations from 2D keypoint sequences. It employs a rigid motion and non-rigid deformation disentanglement strategy to resolve depth ambiguity, and incorporates a pretrained diffusion model as a motion prior to guide plausible 3D pose generation. The network is trained on 2D measurements using reprojection loss, Procrustean alignment, diffusion prior loss, and temporal smoothness loss, without requiring 3D supervision.

## Key Results
- Achieves 41.6 mm N-MPJPE and 33.1 mm PA-MPJPE on Human3.6M
- Outperforms previous methods in both unsupervised 3D HPE and NRSfM
- Demonstrates state-of-the-art performance on MPI-INF-3DHP dataset
- Shows effectiveness of diffusion-based motion prior for depth ambiguity resolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RMNRD strategy resolves depth ambiguity by separating rigid motion from non-rigid deformation.
- Mechanism: By fixing the reference skeleton in a canonical pose and modeling each frame as a sum of the reference skeleton and residual deformation, the method eliminates the ambiguity caused by rigid motion. This is enforced through Procrustes alignment to ensure all shapes align to a common reference.
- Core assumption: Non-rigid deformations are independent of rigid motion and can be modeled separately.
- Evidence anchors:
  - [abstract] "Inspired by the field of non-rigid structure-from-motion, we divide the task of reconstructing 3D human skeletons in motion into the estimation of a 3D reference skeleton, and a frame-by-frame skeleton deformation."
  - [section] "We propose a way to alleviate this ambiguity by separating out the rigid motions. Specifically, we will keep the rigid pose fixed in a canonical form throughout the entire sequence."
  - [corpus] Weak corpus evidence; only tangentially related papers on NRSfM exist with no citations.
- Break condition: If rigid and non-rigid motions are highly correlated or if the canonical reference skeleton cannot be accurately estimated.

### Mechanism 2
- Claim: The diffusion-based motion prior guides the network to predict plausible 3D poses conditioned on 2D observations without 3D supervision.
- Mechanism: A pretrained diffusion model is used to define a plausible distribution of 3D and 2D joint locations. The NRSfMFormer is trained to minimize the difference between its predicted 3D pose and the output of the diffusion model, ensuring the predicted poses are consistent with learned motion priors.
- Core assumption: The diffusion model has learned a realistic distribution of human poses that can be used as a prior.
- Evidence anchors:
  - [abstract] "Subsequently, a loss term based on the diffusion model is used to ensure that the pipeline learns the correct prior motion knowledge."
  - [section] "We propose to utilize a diffusion-based pose generator... The whole framework consists of a forward diffusion process and a reverse sampling process."
  - [corpus] No direct corpus evidence; this is a novel application of diffusion models to NRSfM.
- Break condition: If the diffusion model's learned distribution does not align well with the actual pose distribution of the target dataset.

### Mechanism 3
- Claim: The mixed spatio-temporal NRSmFFormer architecture effectively captures both spatial and temporal dependencies in the pose sequence.
- Mechanism: The architecture uses alternating spatial and temporal transformer blocks to learn joint relationships within each frame and global temporal correlations across frames. Learnable embeddings are added to represent the sequence and pose information.
- Core assumption: Spatial and temporal information can be effectively modeled using transformer architectures with appropriate attention mechanisms.
- Evidence anchors:
  - [abstract] "A mixed spatial-temporal NRSmFFormer is used to simultaneously estimate the 3D reference skeleton and the skeleton deformation of each frame from 2D observations sequence."
  - [section] "It alternately uses Spatial Transformer Block (STB) and Temporal Transformer Block (TTB) to learn spatial and temporal correlation with multi-head self-attention."
  - [corpus] Weak corpus evidence; no directly related papers with high FMR scores and citations.
- Break condition: If the attention mechanisms fail to capture long-range dependencies or if the sequence length is too long for the transformer to handle effectively.

## Foundational Learning

- Concept: Non-rigid Structure-from-Motion (NRSfM)
  - Why needed here: Understanding NRSfM is crucial as the paper's method is inspired by and extends NRSfM techniques to 3D human pose estimation.
  - Quick check question: What is the main challenge in NRSfM that makes it more difficult than rigid SfM?

- Concept: Diffusion Models
  - Why needed here: The method uses a diffusion model as a prior to guide the generation of plausible 3D poses, which is key to resolving depth ambiguity.
  - Quick check question: How does a diffusion model differ from a standard generative model like a GAN?

- Concept: Transformer Architectures
  - Why needed here: The NRSmFFormer uses a transformer-based architecture to model spatial and temporal dependencies in the pose sequence.
  - Quick check question: What is the role of the positional encoding in a transformer model?

## Architecture Onboarding

- Component map: Input (2D keypoints) -> NRSmFFormer (mixed spatio-temporal transformer) -> 3D pose estimation -> Diffusion prior loss + reprojection loss + Procrustean alignment loss -> Updated parameters

- Critical path: Input → NRSmFFormer → 3D pose estimation → Diffusion prior loss + reprojection loss + Procrustean alignment loss → Updated parameters

- Design tradeoffs:
  - Using a diffusion model adds complexity but provides strong priors for depth ambiguity resolution.
  - The mixed spatio-temporal transformer allows effective modeling of both spatial and temporal information but may be computationally expensive.
  - Separating rigid and non-rigid motions simplifies the problem but relies on accurate canonical pose estimation.

- Failure signatures:
  - Poor performance on datasets with significant rigid motion or viewpoint changes.
  - Inability to generalize to poses not well-represented in the diffusion model's training data.
  - Computational inefficiency due to the complexity of the transformer architecture.

- First 3 experiments:
  1. Ablation study removing the diffusion prior to assess its impact on depth ambiguity resolution.
  2. Comparison with a single-frame baseline to evaluate the benefit of temporal modeling.
  3. Testing on a dataset with known rigid motion to verify the effectiveness of the RMNRD strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed RMNRD modeling strategy perform on non-human subjects such as animals or deformable objects beyond human bodies?
- Basis in paper: [explicit] The authors mention that "NRSfM is essentially a universal method that can be applied to diverse scenarios" but they only focused on human bodies in this work.
- Why unresolved: The paper only tested the method on human pose estimation datasets, leaving its generalization to other deformable objects unexplored.
- What evidence would resolve it: Experiments applying the same framework to animal pose estimation or other non-rigid object tracking datasets would demonstrate its broader applicability.

### Open Question 2
- Question: What is the computational overhead of using diffusion models compared to other priors or constraints for resolving depth ambiguity in NRSfM?
- Basis in paper: [explicit] The authors note that "without the pretrained prior, performance will drop dramatically and errors will even double," indicating the importance of diffusion models, but don't compare computational costs with alternatives.
- Why unresolved: While the diffusion prior is shown to be effective, the paper doesn't discuss its computational efficiency relative to other methods.
- What evidence would resolve it: Benchmarking the runtime and memory requirements of the diffusion-based approach against other priors (e.g., geometric constraints, learned shape spaces) on the same datasets.

### Open Question 3
- Question: How does the performance of NRSmFFormer vary with different sequence lengths, and is there an optimal sequence length for balancing accuracy and computational cost?
- Basis in paper: [inferred] The authors mention using "sequence length is 27" during training but don't explore the impact of varying sequence lengths on performance.
- Why unresolved: The choice of sequence length could significantly affect both the quality of the reference skeleton estimation and the computational requirements.
- What evidence would resolve it: Systematic experiments varying the input sequence length and measuring the trade-off between accuracy and computational cost.

### Open Question 4
- Question: Can the NRSmFFormer framework be extended to handle multi-view inputs or incorporate temporal consistency across multiple camera views?
- Basis in paper: [inferred] The current method processes monocular video sequences, but multi-view inputs could provide additional geometric constraints that might improve accuracy.
- Why unresolved: While the method achieves state-of-the-art results on monocular input, it doesn't explore whether incorporating multiple views could further enhance performance.
- What evidence would resolve it: Modifying the architecture to accept multi-view inputs and evaluating performance improvements on multi-view datasets.

## Limitations
- Limited validation on challenging in-the-wild scenarios with significant viewpoint variations
- Computational overhead of transformer architecture and diffusion model integration not fully characterized
- Scalability to longer sequences and more complex motions remains uncertain

## Confidence

- High confidence in the RMNRD disentanglement mechanism and its effectiveness in resolving depth ambiguity
- Medium confidence in the diffusion prior's contribution, as its impact is shown through ablation but not independently validated
- Medium confidence in the mixed spatio-temporal transformer's design, as architectural details are somewhat limited

## Next Checks

1. Test on a dataset with pronounced rigid motions (e.g., walking with significant viewpoint changes) to verify RMNRD strategy robustness
2. Conduct ablation study comparing with and without diffusion prior on datasets with different pose distributions
3. Evaluate computational efficiency and scalability by testing on sequences longer than 27 frames