---
ver: rpa2
title: Learning Transformer Programs
arxiv_id: '2306.01128'
source_url: https://arxiv.org/abs/2306.01128
tags:
- attention
- transformer
- position
- token
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for training Transformers that are
  mechanistically interpretable by design, called Transformer Programs. The key idea
  is to constrain the Transformer to have a disentangled residual stream where token
  embeddings encode a fixed set of discrete variables in orthogonal subspaces.
---

# Learning Transformer Programs

## Quick Facts
- arXiv ID: 2306.01128
- Source URL: https://arxiv.org/abs/2306.01128
- Reference count: 40
- Key outcome: Introduces Transformer Programs that constrain Transformers to have interpretable, rule-based modules with disentangled residual streams for mechanistically interpretable AI systems

## Executive Summary
This paper presents a novel approach to training Transformers that are inherently interpretable by design. The method, called Transformer Programs, constrains the network architecture to maintain a disentangled residual stream where token embeddings encode discrete variables in orthogonal subspaces. Each module implements interpretable, rule-based mappings between input and output variables. After training, the Transformer can be deterministically converted into a human-readable Python program, enabling direct analysis of the learned computation.

The authors validate their approach across various tasks including in-context learning, algorithmic problems (sorting, Dyck languages), and NLP tasks like named entity recognition and text classification. While achieving reasonable performance compared to standard Transformers, the key contribution is the ability to extract and debug the resulting programs using off-the-shelf code analysis tools, providing unprecedented insight into the "circuits" that solve different sub-problems.

## Method Summary
Transformer Programs work by constraining the Transformer architecture to maintain a disentangled residual stream where each token embedding encodes discrete variables in orthogonal subspaces. Each module (attention heads and MLPs) implements rule-based mappings between specific input and output variables. The discrete program parameters are optimized using Gumbel-Softmax relaxation during training, which enables gradient-based optimization while maintaining the discrete structure. After training, the network can be deterministically converted to a Python program where attention heads map to predicate functions and MLPs map to lookup tables.

## Key Results
- Transformer Programs achieve competitive performance on algorithmic tasks (sorting, Dyck languages) and NLP tasks (NER, text classification) compared to standard Transformers of similar size
- The extracted programs are human-readable and can be analyzed using standard code debugging tools
- Performance degrades on longer sequences and larger vocabularies, suggesting scalability limitations
- Transformer Programs use more layers and attention heads than human-written solutions for some tasks, indicating potential inefficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The disentangled residual stream enables direct variable-to-code mapping
- Mechanism: By encoding each variable in an orthogonal subspace with fixed cardinality, each attention head can be deterministically mapped to a predicate function that reads specific variables and writes to dedicated addresses
- Core assumption: Variables remain orthogonal throughout the network and module outputs are concatenated without interference
- Evidence anchors:
  - [abstract]: "constrain the Transformer to have a disentangled residual stream where token embeddings encode a fixed set of discrete variables in orthogonal subspaces"
  - [section]: "We constrain the Transformer to have a disentangled residual stream: the token embeddings encode a fixed set of discrete variables in orthogonal subspaces, and each module reads a fixed set of variables and writes a new variable to a dedicated address"
  - [corpus]: Weak - no direct corpus evidence of orthogonality preservation
- Break condition: Variable subspaces become entangled through non-orthogonal transformations or information leakage between modules

### Mechanism 2
- Claim: Gumbel-Softmax relaxation enables gradient-based optimization of discrete programs
- Mechanism: Discrete program parameters (gate indicators and predicate matrices) are sampled from Gumbel-Softmax distributions, allowing backpropagation through the discrete structure while annealing temperature drives convergence to discrete values
- Core assumption: The Gumbel-Softmax approximation remains stable and the temperature annealing schedule leads to clean discretization
- Evidence anchors:
  - [abstract]: "we develop a continuous relaxation scheme for learning these programs"
  - [section]: "we optimize a distribution over these discrete program weights... using the Gumbel reparameterization"
  - [corpus]: Weak - no specific corpus evidence about Gumbel-Softmax effectiveness in this context
- Break condition: Temperature annealing is too aggressive causing optimization to get stuck, or samples remain too soft at convergence

### Mechanism 3
- Claim: Hard attention with deterministic tie-breaking enables exact categorical output computation
- Mechanism: By using one-to-one predicate mappings and selecting closest matching keys with deterministic tie-breaking, each attention head produces categorical outputs that can be directly compiled to discrete code
- Core assumption: The one-to-one predicate constraint ensures categorical outputs and the tie-breaking strategy is consistent
- Evidence anchors:
  - [abstract]: "we implement a limited form of numerical attention, which guarantees that the output values are integers with a bounded range"
  - [section]: "we adopt a constraint from Tracr and require that each query token attends to a single key token; this constraint is necessary to ensure that the output variable will also be categorical"
  - [corpus]: Weak - no corpus evidence about effectiveness of this specific tie-breaking strategy
- Break condition: Predicate matrices become dense or the one-to-one constraint is violated, breaking the categorical output guarantee

## Foundational Learning

- Concept: Discretization through orthogonal variable encoding
  - Why needed here: Without orthogonal subspaces, attention heads cannot be deterministically mapped to separate predicate functions without interference
  - Quick check question: What happens if two attention heads write to overlapping subspaces of the residual stream?

- Concept: Gumbel-Softmax reparameterization for discrete optimization
  - Why needed here: Standard gradient descent cannot optimize discrete program parameters directly, requiring a continuous relaxation
  - Quick check question: How does the temperature parameter in Gumbel-Softmax affect the discreteness of samples?

- Concept: Hard attention with deterministic tie-breaking
  - Why needed here: Soft attention produces continuous outputs that cannot be directly compiled to discrete code, while hard attention with consistent tie-breaking preserves categorical structure
  - Quick check question: What is the difference between attending to "the closest match" versus "a random match" in the tie-breaking strategy?

## Architecture Onboarding

- Component map: Input embeddings → Attention layers (reading/writing variables) → MLP layers (feature computation) → Classifier (prediction)
- Critical path: Input embeddings → Attention layers (reading/writing variables) → MLP layers (feature computation) → Classifier (prediction)
- Design tradeoffs: Discrete vs continuous operations (accuracy vs interpretability), orthogonal vs entangled subspaces (program simplicity vs expressiveness), fixed vs learned variable cardinalities (training stability vs flexibility)
- Failure signatures: Degraded accuracy on longer sequences, programs that are too complex to interpret, optimization instability during temperature annealing
- First 3 experiments:
  1. Train on the in-context learning toy task with 2-layer attention-only model to verify basic program extraction works
  2. Test longer sequence lengths on RASP tasks to identify scalability limits
  3. Compare accuracy of categorical-only vs categorical+numerical attention on counting-heavy tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Transformer Programs scale to longer sequences and larger vocabularies?
- Basis in paper: [inferred] The paper shows that Transformer Programs struggle to learn effective solutions for longer sequences and larger vocabularies, with performance degrading as the sequence length and vocabulary size increase.
- Why unresolved: The paper only tests Transformer Programs on small-scale instances of tasks with maximum sequence lengths of 16 and vocabulary sizes of 8. Scaling to larger inputs introduces optimization challenges that are not fully explored.
- What evidence would resolve it: Empirical results showing Transformer Programs trained on longer sequences (e.g. 100+ tokens) and larger vocabularies (e.g. 10,000+ words) achieving competitive performance with standard Transformers on benchmarks like GLUE or SuperGLUE.

### Open Question 2
- Question: Can Transformer Programs learn parsimonious solutions using fewer layers and attention heads compared to human-written solutions?
- Basis in paper: [inferred] The paper finds that Transformer Programs often use more layers and attention heads than human-written solutions for tasks like sorting and reversing. For example, the best-performing Sort program uses 3 layers and 8 heads, while human-written RASP solutions require only 2 layers and 1 head.
- Why unresolved: The paper does not investigate whether Transformer Programs can be regularized or constrained to find more efficient solutions. Techniques like weight pruning or sparsity constraints are not explored.
- What evidence would resolve it: Empirical results showing Transformer Programs trained with sparsity or pruning constraints achieving similar performance to human-written solutions using fewer layers and heads. For example, a Sort program using only 2 layers and 1 head matching the accuracy of a 3-layer, 8-head program.

### Open Question 3
- Question: Can Transformer Programs be extended to support more complex operations beyond categorical attention and lookup-table MLPs?
- Basis in paper: [explicit] The paper mentions potential extensions like supporting numerical variables with bounded ranges, implementing general-purpose feed-forward layers, and incorporating continuous modules alongside discrete ones.
- Why unresolved: The paper only implements a limited set of operations in Transformer Programs. Extending the framework to support more complex modules like multi-head attention, layer normalization, or residual connections is left for future work.
- What evidence would resolve it: A proof-of-concept implementation of Transformer Programs with additional modules like multi-head attention or residual connections, showing they can still be discretized and interpreted as human-readable programs. Empirical results on benchmarks like ImageNet classification or machine translation would further demonstrate the framework's capabilities.

## Limitations
- The orthogonal subspace constraint may become increasingly difficult to maintain as network depth grows, potentially limiting scalability
- Performance gap on in-context learning tasks suggests fundamental limitations in handling context-dependent reasoning
- Generated programs often use more layers and attention heads than human-written solutions, indicating potential inefficiency

## Confidence
- **High Confidence**: The core mechanism of disentangled residual streams with orthogonal variable subspaces - well-grounded in the formalism and consistent with related work on interpretable neural networks
- **Medium Confidence**: The Gumbel-Softmax optimization approach - established technique but its effectiveness in this specific context requires empirical validation
- **Low Confidence**: The categorical output guarantee through hard attention - the one-to-one predicate constraint and tie-breaking strategy need more rigorous analysis

## Next Checks
1. **Orthogonality Preservation Analysis**: Instrument the training process to measure variable subspace entanglement throughout the network depth, identifying at which layers orthogonality breaks down and whether this correlates with performance degradation.

2. **Temperature Annealing Sensitivity**: Systematically vary the Gumbel-Softmax temperature annealing schedule to determine optimal convergence criteria and assess whether different schedules produce qualitatively different program structures.

3. **Program Complexity vs Performance Trade-off**: Quantify the relationship between program interpretability (measured by cyclomatic complexity, variable count, etc.) and task performance across all benchmark tasks to establish practical limits of the approach.