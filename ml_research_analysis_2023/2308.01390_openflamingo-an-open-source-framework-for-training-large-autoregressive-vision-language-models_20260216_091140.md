---
ver: rpa2
title: 'OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language
  Models'
arxiv_id: '2308.01390'
source_url: https://arxiv.org/abs/2308.01390
tags:
- image
- arxiv
- images
- openflamingo
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenFlamingo is an open-source framework for training autoregressive
  vision-language models (VLMs) by replicating DeepMind's Flamingo. The authors train
  3B, 4B, and 9B-parameter models using frozen CLIP ViT-L/14 vision encoders and language
  models (MPT, RedPajama, or OPT), adding cross-attention modules for multimodal fusion.
---

# OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models

## Quick Facts
- arXiv ID: 2308.01390
- Source URL: https://arxiv.org/abs/2308.01390
- Authors: Vincent Zamora, João F. Henriques, Arun Mallya, et al.
- Reference count: 40
- Key outcome: OpenFlamingo achieves 80-89% of Flamingo performance on 7 benchmarks with 3B, 4B, and 9B-parameter models

## Executive Summary
OpenFlamingo is an open-source framework that replicates DeepMind's Flamingo architecture for training autoregressive vision-language models (VLMs). The framework trains models ranging from 3B to 9B parameters using frozen CLIP ViT-L/14 vision encoders and language models (MPT, RedPajama, or OPT), with cross-attention modules added for multimodal fusion. The models are trained on web-scale data including LAION-2B image-text pairs and Multimodal C4 interleaved sequences, achieving 85-89% of Flamingo's performance across standard benchmarks while enabling research on web-scale VLMs.

## Method Summary
The framework follows Flamingo's architecture by augmenting frozen language models with cross-attention modules that attend to CLIP ViT-L/14 vision encoder outputs. Training uses a mixture of LAION-2B image-text pairs and Multimodal C4 interleaved sequences, with models predicting the next token in sequences. The authors train 3B, 4B, and 9B-parameter models using AdamW optimization with learning rate 1e-4 and weight decay 0.1 on cross-attention layers. Training occurs on 64 GPUs using DistributedDataParallel or FullyShardedDataParallel, with cross-attention modules placed at different intervals depending on model size (every 1 layer for 3B, 2 layers for 4B, 4 layers for 9B).

## Key Results
- OpenFlamingo-3B and -9B models achieve 85% and 89% of corresponding Flamingo models across 7 evaluation datasets
- Models show strong few-shot learning capabilities, performing well with 0-32 in-context examples
- Performance drops on knowledge-intensive tasks (OK-VQA, TextVQA) indicate limitations of frozen embeddings and web-scale data
- OpenFlamingo-4B models degrade after 4-8 in-context examples due to frozen <image> and <|endofchunk|> embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention modules enable interleaved image-text sequence processing
- Mechanism: Dense cross-attention modules are inserted into frozen language model layers to attend to CLIP ViT-L/14 vision encoder outputs
- Core assumption: Language model can effectively integrate visual features when provided cross-attention connections
- Evidence anchors:
  - [abstract]: "Following Flamingo, we augment the layers of pretrained, frozen language models so that they cross attend to the outputs of a frozen vision encoder while predicting the next token"
  - [section 3.1]: "Text tokens attend to their corresponding images via dense cross-attention modules, which we attach to the layers of a frozen, autoregressive language model"
  - [corpus]: Weak - no direct corpus evidence on cross-attention effectiveness
- Break condition: If cross-attention intervals are too sparse or if language model cannot handle cross-modal features

### Mechanism 2
- Claim: Frozen embeddings reduce training complexity while maintaining performance
- Mechanism: CLIP vision encoder, language model, and text embeddings remain frozen during training, only cross-attention modules and Perceiver resampler are trainable
- Core assumption: Frozen components provide sufficient feature representations for multimodal tasks
- Evidence anchors:
  - [section 3.1]: "As a preprocessing step, we first mark the locations of images in the text sequence with <image> tokens. We also insert <|endofchunk|> tokens after the text tokens following an image"
  - [section 3.2]: "All image-text pairs in LAION-2B have a cosine similarity of at least 0.28 according to CLIP ViT-B/32"
  - [corpus]: Weak - no direct corpus evidence on frozen embedding effectiveness
- Break condition: If frozen components are too rigid to capture necessary multimodal patterns

### Mechanism 3
- Claim: Large interleaved datasets enable few-shot learning capabilities
- Mechanism: Training on LAION-2B image-text pairs and Multimodal C4 interleaved sequences provides diverse multimodal context
- Core assumption: Web-scale multimodal data contains sufficient patterns for few-shot generalization
- Evidence anchors:
  - [section 3.2]: "We train our models on a mixture of image-text pairs and interleaved image-text sequences"
  - [section 4]: "When averaging performance across 7 evaluation datasets, OpenFlamingo-3B and -9B models attain 85% and 89% of their corresponding Flamingo models respectively"
  - [corpus]: Moderate - related works (Med-Flamingo, NVLM) cite OpenFlamingo as foundation for multimodal learning
- Break condition: If training data lacks sufficient diversity or quality for few-shot generalization

## Foundational Learning

- Concept: Autoregressive sequence modeling
  - Why needed here: OpenFlamingo predicts next token conditioned on previous tokens and images
  - Quick check question: How does next-token prediction enable multimodal generation?

- Concept: Cross-attention mechanisms
  - Why needed here: Allows language model layers to attend to visual features
  - Quick check question: What happens when cross-attention is placed between language model layers?

- Concept: Vision-language pretraining
  - Why needed here: Enables model to understand relationships between visual and textual information
  - Quick check question: Why freeze vision encoder and language model during training?

## Architecture Onboarding

- Component map:
  CLIP ViT-L/14 (frozen) → Perceiver resampler (trainable) → Cross-attention modules → Frozen language model → Text generation
  LAION-2B pairs + MMC4 sequences → Training pipeline → Evaluation datasets

- Critical path: Image → CLIP → Perceiver → Cross-attention → Language model → Text output
  - Any failure in vision encoding or cross-attention breaks multimodal understanding

- Design tradeoffs:
  - Frozen vs trainable embeddings: Frozen reduces parameters but may limit adaptation
  - Cross-attention density: More layers provide better integration but increase computation
  - Dataset composition: Balance between image-text pairs and interleaved sequences

- Failure signatures:
  - Poor VQA performance: May indicate cross-attention or frozen embeddings issues
  - Degraded performance with more in-context examples: Could signal data quality or sequence length problems
  - Unexpected performance drops: May indicate frozen embedding limitations

- First 3 experiments:
  1. Train with trainable <image> embeddings to verify frozen embedding hypothesis
  2. Vary cross-attention interval to find optimal integration point
  3. Test different dataset ratios to optimize few-shot performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different filtering thresholds for image-text similarity in MMC4 affect OpenFlamingo's performance and training efficiency?
- Basis in paper: [explicit] The paper discusses filtering MMC4 sequences based on CLIP cosine similarity thresholds (0.24, 0.32) and notes the trade-offs between sequence length, quality, and dataset size.
- Why unresolved: The paper only provides preliminary estimates of filtering effects (e.g., 88.7% sequence discard rate at 0.32 threshold) but doesn't empirically test how different thresholds impact model performance or training dynamics.
- What evidence would resolve it: Systematic experiments training OpenFlamingo models on MMC4 datasets filtered at different similarity thresholds (0.24, 0.28, 0.32) and comparing downstream performance, training loss curves, and sequence length distributions.

### Open Question 2
- Question: What is the optimal density of cross-attention layers for different model scales in autoregressive vision-language models?
- Basis in paper: [explicit] The paper uses different cross-attention intervals (1 for 3B models, 2 for 4B models, 4 for 9B models) but doesn't explore whether these choices are optimal or task-dependent.
- Why unresolved: The choice of cross-attention density appears to be inherited from Flamingo without systematic exploration of alternatives, and the paper observes that 4B models underperform compared to 3B models despite having more parameters.
- What evidence would resolve it: Experiments training OpenFlamingo variants with different cross-attention intervals for each model scale, measuring performance across all evaluation tasks and analyzing computational efficiency trade-offs.

### Open Question 3
- Question: Why do frozen <image> and <|endofchunk|> embeddings lead to degraded performance in larger models?
- Basis in paper: [explicit] The paper notes that OpenFlamingo-4B models underperform 3B models and uses frozen embeddings, while smaller models use trainable embeddings, but doesn't investigate the causal mechanism.
- Why unresolved: The paper speculates about gradient masking issues with FSDP but doesn't experimentally verify whether trainable embeddings would improve 4B performance or understand the underlying optimization dynamics.
- What evidence would resolve it: Training OpenFlamingo-4B models with trainable embeddings and comparing performance to frozen-embedding versions, along with ablation studies on embedding initialization and optimization strategies.

## Limitations
- Lower performance on knowledge-intensive tasks (OK-VQA, TextVQA) due to web-scale data limitations
- 4B-parameter model degrades with more than 4-8 in-context examples due to frozen embeddings
- Cross-attention interval optimization limited to two datasets without systematic exploration

## Confidence
- High Confidence: Core architecture replication and 85-89% performance achievement
- Medium Confidence: Few-shot learning capabilities and knowledge-intensive task performance
- Low Confidence: Scalability claims and optimal cross-attention interval selection

## Next Checks
1. Systematically test cross-attention placement at different layer intervals (every 1, 2, 3, and 4 layers) across multiple model sizes on all 7 evaluation datasets
2. Train OpenFlamingo-4B models with both frozen and trainable <image> and <|endofchunk|> embeddings, evaluating performance on knowledge-intensive tasks
3. Create controlled experiments varying the ratio of LAION-2B image-text pairs to MMC4 interleaved sequences to optimize few-shot learning and knowledge-intensive task performance