---
ver: rpa2
title: Improving Intrinsic Exploration by Creating Stationary Objectives
arxiv_id: '2310.18144'
source_url: https://arxiv.org/abs/2310.18144
tags:
- exploration
- state
- learning
- sofe
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that intrinsic rewards derived from count-based
  exploration methods are non-stationary, making them difficult to optimize and potentially
  leading to suboptimal policies. The authors propose Stationary Objectives For Exploration
  (SOFE), a framework that transforms non-stationary rewards into stationary ones
  by augmenting the state representation with sufficient statistics of the intrinsic
  reward distribution (e.g., state visitation frequencies).
---

# Improving Intrinsic Exploration by Creating Stationary Objectives

## Quick Facts
- **arXiv ID**: 2310.18144
- **Source URL**: https://arxiv.org/abs/2310.18144
- **Reference count**: 23
- **Key outcome**: SOFE improves exploration performance in challenging tasks by transforming non-stationary count-based rewards into stationary ones through augmented state representations.

## Executive Summary
This paper addresses the challenge of non-stationary rewards in count-based exploration methods, which make optimization difficult and can lead to suboptimal policies. The authors propose Stationary Objectives For Exploration (SOFE), a framework that augments the state representation with sufficient statistics of the intrinsic reward distribution, such as state visitation frequencies or E3B ellipsoid matrices. By making the rewards stationary, SOFE enables more effective optimization and improved exploration performance across various environments, including sparse-reward problems, pixel-based observations, 3D navigation, and procedurally generated environments.

## Method Summary
SOFE transforms non-stationary count-based intrinsic rewards into stationary rewards by augmenting the state space with visitation frequency statistics or E3B ellipsoid matrices. The method requires identifying sufficient statistics for different exploration bonuses and efficiently encoding these statistics as input to the policy network. This allows the agent to directly use the novelty estimate for decision-making, reducing the impact of non-stationarity in the auxiliary model. SOFE is evaluated using PPO, A2C, DQN, and SAC algorithms across various environments, showing significant improvements in map coverage, task rewards, and overall exploration efficiency compared to vanilla count-based methods.

## Key Results
- SOFE significantly improves map coverage and task rewards in hard-exploration mazes compared to vanilla count-based methods.
- The method demonstrates effectiveness in high-dimensional environments using the E3B algorithm, with improved performance over pseudo-count-based methods.
- SOFE shows consistent improvements across different RL algorithms (PPO, A2C, DQN, SAC) and environment types (2D mazes, 3D navigation, Procgen).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SOFE transforms non-stationary count-based intrinsic rewards into stationary rewards by augmenting the state space with visitation frequency statistics.
- **Mechanism**: The agent observes the current state visitation counts (or their embedding) as part of its state input, making the reward function deterministic and Markovian in the augmented space.
- **Core assumption**: The sufficient statistics for the intrinsic reward distribution can be efficiently encoded and provided to the agent without significantly increasing the effective state complexity.
- **Evidence anchors**:
  - [abstract]: "The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation."
  - [section 4]: "SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network."
  - [corpus]: "Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration" uses similar augmentation ideas, but not for stationarity.

### Mechanism 2
- **Claim**: By providing visitation frequency information, the agent can direct exploration more efficiently toward unvisited states.
- **Mechanism**: The agent learns to use the augmented visitation frequency signal to identify novel states and prioritize exploration toward them, effectively self-selecting goals.
- **Core assumption**: The agent's policy can generalize over the augmented state space and use the visitation frequency signal for decision-making.
- **Evidence anchors**:
  - [section 5.1]: "Augmented agents pre-trained on episodic exploration efficiently direct their exploration toward the unvisited states."
  - [section 5.1]: "Results show that augmented agents efficiently direct their exploration towards the unvisited states, self-identifying these as goals."
  - [corpus]: "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards" uses visitation counts for exploration, but not as state augmentations.

### Mechanism 3
- **Claim**: SOFE improves optimization of pseudo-count-based methods (e.g., E3B) in high-dimensional environments by providing a more accurate estimate of novelty.
- **Mechanism**: Including the E3B ellipsoid matrix (or its diagonal) in the state allows the agent to directly use the novelty estimate for decision-making, reducing the impact of non-stationarity in the auxiliary model.
- **Core assumption**: The ellipsoid matrix is a sufficient statistic for the E3B exploration bonus and can be efficiently encoded.
- **Evidence anchors**:
  - [section 4]: "For the elliptical bonus in Equation 4, we identify the matrix Ctâˆ’1 as the sufficient statistics."
  - [section 5.3]: "Figure 7 shows that SOFE also improves the performance of pseudo-count-based methods, providing empirical evidence that reducing the non-stationarity of a reward distribution enables better optimization even in high-dimensional environments."
  - [corpus]: "SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation" uses stationary distributions, but not for count-based exploration.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs)
  - **Why needed here**: Understanding the difference between MDPs and POMDPs is crucial for grasping why count-based exploration methods induce non-stationarity and why SOFE's augmentation is effective.
  - **Quick check question**: What is the key difference between an MDP and a POMDP, and how does non-stationarity of rewards relate to POMDPs?

- **Concept**: Count-based exploration methods and pseudo-counts
  - **Why needed here**: SOFE builds upon count-based exploration and extends it to pseudo-counts (E3B), so understanding these methods is essential.
  - **Quick check question**: How do count-based exploration methods derive intrinsic rewards from state visitation frequencies, and what are pseudo-counts?

- **Concept**: Sufficient statistics
  - **Why needed here**: SOFE relies on identifying and using sufficient statistics of the intrinsic reward distribution (e.g., visitation counts, ellipsoid matrix) to make the rewards stationary.
  - **Quick check question**: What are sufficient statistics, and why are they important for making the intrinsic rewards stationary in SOFE?

## Architecture Onboarding

- **Component map**:
  Environment -> Count tracker/E3B updater -> Policy network -> RL algorithm

- **Critical path**:
  1. Agent interacts with environment, receiving observation and reward.
  2. Sufficient statistics (visitation counts or ellipsoid matrix) are updated.
  3. Augmented state (observation + sufficient statistics) is fed to policy network.
  4. Policy network outputs action.
  5. RL algorithm updates policy based on stationary augmented reward.

- **Design tradeoffs**:
  - Tradeoff between the accuracy of sufficient statistics encoding and computational cost.
  - Tradeoff between the dimensionality of the augmented state and the policy's ability to generalize.
  - Choice of RL algorithm: SOFE is claimed to be agnostic to the RL algorithm, but some may benefit more than others.

- **Failure signatures**:
  - If the policy cannot effectively use the augmented information, exploration performance may not improve.
  - If the sufficient statistics are not updated correctly or efficiently, the stationary property may be lost.
  - If the augmented state becomes too high-dimensional, the policy may struggle to generalize.

- **First 3 experiments**:
  1. Implement SOFE for a simple count-based exploration method (e.g., using visitation counts) in a small gridworld environment and compare to vanilla count-based exploration.
  2. Implement SOFE for E3B in a Procgen environment and compare to vanilla E3B.
  3. Analyze how the policy uses the augmented information by visualizing visitation frequencies and agent trajectories.

## Open Questions the Paper Calls Out
No explicit open questions are called out in the provided content.

## Limitations
- The computational overhead of state augmentation is not fully characterized, particularly for high-dimensional E3B ellipsoids where only diagonal elements are used.
- The method's performance in continuous state spaces with partial observability is not directly tested, despite the theoretical motivation around POMDPs.
- The generality of sufficient statistics identification across different exploration bonuses needs further validation beyond the two cases studied (visitation counts and E3B).

## Confidence
- **High confidence**: The core mechanism of transforming non-stationary rewards into stationary ones through state augmentation is theoretically sound and empirically validated in discrete state spaces.
- **Medium confidence**: The extension to pseudo-count methods (E3B) is promising but relies on simplifying assumptions (using only diagonal elements) that may limit effectiveness in some scenarios.
- **Medium confidence**: The claim of RL algorithm agnosticism is supported by experiments with multiple algorithms, but optimal hyperparameter tuning for each combination is not explored.

## Next Checks
1. Test SOFE with diagonal-only ellipsoid matrices against full-matrix approaches in controlled environments to quantify the impact of this approximation.
2. Evaluate SOFE performance in partially observable continuous control tasks (e.g., using image observations) to test the POMDP motivation directly.
3. Benchmark the computational overhead of SOFE augmentation against its performance gains across different state space dimensions to establish scaling properties.