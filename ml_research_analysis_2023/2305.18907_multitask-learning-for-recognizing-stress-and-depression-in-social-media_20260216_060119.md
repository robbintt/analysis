---
ver: rpa2
title: Multitask learning for recognizing stress and depression in social media
arxiv_id: '2305.18907'
source_url: https://arxiv.org/abs/2305.18907
tags:
- learning
- task
- depression
- stress
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting stress and depression
  in social media by proposing multi-task learning frameworks that use depression
  and stress detection as primary and auxiliary tasks, respectively. The authors introduce
  two novel multi-task learning architectures that exploit shared BERT layers and
  task-specific layers, with one approach also incorporating an attention fusion network.
---

# Multitask learning for recognizing stress and depression in social media

## Quick Facts
- arXiv ID: 2305.18907
- Source URL: https://arxiv.org/abs/2305.18907
- Reference count: 32
- One-line primary result: Multi-task learning frameworks achieve F1-scores of 92.96% (depression) and 91.91% (stress), outperforming single-task and transfer learning baselines.

## Executive Summary
This paper introduces multi-task learning frameworks for detecting stress and depression in social media posts, using depression detection as the primary task and stress detection as an auxiliary task. The authors propose two architectures: Double Encoders (shared BERT + task-specific BERT layers) and Attention Fusion Network (shared + task-specific layers with attention weighting). Experiments on separate depression and stress datasets demonstrate that multi-task learning outperforms single-task learning, transfer learning, and M-BERT baselines, with the Double Encoders approach achieving an F1-score of 92.96% on depression detection.

## Method Summary
The paper presents two multi-task learning architectures that leverage shared BERT layers to capture common linguistic features between stress and depression detection. The Double Encoders approach uses a shared BERT layer for all posts, followed by two separate BERT layers for each task. The Attention Fusion Network approach adds an attention mechanism that dynamically weights shared and task-specific features before prediction. Both models are trained jointly using cross-entropy loss with a hyperparameter β controlling task importance, and evaluated on two separate datasets collected from Reddit and depression forums.

## Key Results
- Multi-task learning frameworks outperform single-task learning baselines, transfer learning, and M-BERT approaches.
- The Double Encoders approach achieves an F1-score of 92.96% on depression detection, outperforming baselines by 0.47-1.56%.
- The Attention Fusion Network approach achieves an F1-score of 91.91% on depression detection and outperforms M-BERT on stress detection in precision by 5.03% and F1-score by 0.51%.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning leverages shared BERT layers to capture common linguistic features between stress and depression, improving generalization.
- Mechanism: A shared BERT layer processes all posts, learning representations that benefit both tasks. Task-specific BERT layers then refine these shared representations for each task.
- Core assumption: Depression and stress detection share substantial underlying linguistic patterns in social media text.
- Evidence anchors:
  - [abstract]: "we present the first study, which exploits two different datasets collected under different conditions, and introduce two multitask learning frameworks, which use depression and stress as the main and auxiliary tasks respectively"
  - [section]: "According to [21], [22], it may be easier to learn several tasks at one time than to learn these same tasks separately. This can be justified by the fact that information and features can be shared between the tasks."
  - [corpus]: Weak. Related papers focus on transformer-based approaches but do not explicitly validate shared layer benefits.
- Break condition: If the tasks are too dissimilar, shared representations may introduce noise rather than benefit, reducing performance.

### Mechanism 2
- Claim: Attention fusion networks dynamically weight shared and task-specific features, allowing the model to adaptively focus on relevant information for each task.
- Mechanism: Concatenated shared and task-specific representations pass through dense layers to compute attention weights α_task and α_shared. These weights scale the respective feature vectors before final prediction.
- Core assumption: The relative importance of shared versus task-specific features varies between depression and stress detection.
- Evidence anchors:
  - [abstract]: "Regarding the second approach, it consists of shared and task-specific layers weighted by attention fusion networks"
  - [section]: "The attention fusion network aims at understanding the behavior of each of the task-specific and shared features in the decision process"
  - [corpus]: Weak. No corpus papers explicitly describe attention fusion for stress/depression tasks.
- Break condition: If attention weights converge to extremes (near 0 or 1), the fusion network provides minimal benefit over simple concatenation.

### Mechanism 3
- Claim: Multi-task learning prevents catastrophic forgetting by maintaining separate task-specific encoders while jointly optimizing both tasks.
- Mechanism: Separate BERT encoders for each task are updated by their respective losses, while a shared encoder receives gradients from both tasks, preserving task-specific knowledge.
- Core assumption: Task-specific knowledge remains valuable and should not be overwritten during joint training.
- Evidence anchors:
  - [section]: "In contrast, our proposed multi-task learning architecture includes both task-specific and shared layers. Therefore, each task learns some knowledge, which is proven to be beneficial to both related tasks"
  - [corpus]: Weak. Related papers discuss transformer architectures but do not specifically address catastrophic forgetting in multi-task settings.
- Break condition: If β (task weighting parameter) is set too high for one task, the other task's encoder may underfit.

## Foundational Learning

- Concept: BERT tokenization and padding
  - Why needed here: All input text must be converted to token IDs and padded to uniform length for batch processing.
  - Quick check question: What is the maximum sequence length used in this work?

- Concept: Cross-entropy loss function
  - Why needed here: Binary classification requires a loss function that penalizes incorrect predictions based on predicted probabilities.
  - Quick check question: How is the joint loss computed when training two tasks simultaneously?

- Concept: Attention mechanisms in neural networks
  - Why needed here: The attention fusion network requires understanding of how attention weights are computed and applied to feature vectors.
  - Quick check question: What activation function is used in the attention fusion network's dense layers?

## Architecture Onboarding

- Component map:
  Input: Raw text posts -> Tokenizer: BERT tokenizer (max length 512) -> Shared encoder: Single BERT layer (updated by both tasks) -> Task-specific encoders: Two separate BERT layers (updated by individual tasks) -> Attention fusion (Approach 2): Dense layers computing α_task and α_shared -> Output layers: Dense layers (2 units, softmax) for final predictions -> Loss: Weighted sum of cross-entropy losses (β parameter controls task importance)

- Critical path:
  1. Text → Tokenizer → Token IDs + attention masks
  2. Token IDs → Shared BERT → Task-specific BERT layers
  3. Task-specific outputs → Attention fusion (Approach 2) OR direct prediction (Approach 1)
  4. Predictions → Loss computation → Backpropagation

- Design tradeoffs:
  - Shared vs. separate encoders: Shared layers reduce parameters and may capture common features, but risk interference if tasks are dissimilar.
  - Attention fusion complexity: Adds parameters and computation but enables adaptive feature weighting.
  - β parameter tuning: Critical for balancing task performance; improper setting may cause one task to dominate.

- Failure signatures:
  - Both tasks perform worse than single-task baselines: Shared representations may be introducing noise.
  - One task performs well, the other poorly: β parameter may be imbalanced; attention weights may be skewed.
  - Training instability: Learning rates or β values may be improperly tuned.

- First 3 experiments:
  1. Train single-task models (stacked BERT) for depression and stress detection separately; establish baseline performance.
  2. Implement multi-task learning with shared encoder only; compare performance to single-task models.
  3. Add attention fusion network to multi-task architecture; evaluate improvement in both tasks.

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- The two datasets were collected under different conditions, potentially affecting generalizability of findings.
- No statistical significance testing is reported for performance improvements between approaches.
- The β parameter controlling task weighting is fixed at 0.01 without justification or sensitivity analysis.

## Confidence
- **High confidence**: The core claim that multi-task learning outperforms single-task learning and transfer learning approaches is well-supported by experimental results across multiple metrics (F1-scores of 92.96% and 91.91% for depression and stress tasks respectively).
- **Medium confidence**: The mechanism by which shared BERT layers capture common linguistic features between stress and depression is plausible but not explicitly validated through ablation studies or feature analysis.
- **Medium confidence**: The claim that attention fusion networks provide adaptive weighting of shared versus task-specific features is supported by performance improvements but lacks detailed analysis of the attention weight distributions.

## Next Checks
1. **Ablation study**: Remove the attention fusion network from Approach 2 and compare performance to the full model to quantify the specific contribution of attention weighting versus simple concatenation.

2. **Statistical significance testing**: Conduct paired t-tests or Wilcoxon signed-rank tests on the F1-scores across multiple runs to establish whether performance differences between approaches are statistically significant rather than due to random variation.

3. **Attention weight analysis**: Visualize and analyze the learned attention weights (α_task and α_shared) across different types of posts to understand whether the model truly learns task-specific importance patterns or if weights converge to fixed values.