---
ver: rpa2
title: Recovering high-quality FODs from a reduced number of diffusion-weighted images
  using a model-driven deep learning architecture
arxiv_id: '2307.15273'
source_url: https://arxiv.org/abs/2307.15273
tags:
- sdnet
- fods
- performance
- network
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FOD reconstruction using deep learning has the potential to produce
  accurate FODs from a reduced number of diffusion-weighted images (DWIs), decreasing
  total imaging time. Diffusion acquisition invariant representations of the DWI signals
  are typically used as input to these methods to ensure that they can be applied
  flexibly to data with different b-vectors and b-values; however, this means the
  network cannot condition its output directly on the DWI signal.
---

# Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture

## Quick Facts
- arXiv ID: 2307.15273
- Source URL: https://arxiv.org/abs/2307.15273
- Reference count: 34
- Fixel-based analysis of FODs can be improved by adding a fixel classification penalty to the loss function of a model-based deep learning architecture.

## Executive Summary
This paper presents a model-driven deep learning architecture for reconstructing high-quality fiber orientation distributions (FODs) from a reduced number of diffusion-weighted images (DWIs). The Spherical Deconvolution Network (SDNet) uses alternating DWI consistency blocks and deep regularisation blocks to ensure intermediate and output FODs are consistent with the input DWI signals. A fixel classification penalty is added to the loss function to encourage accurate segmentation of FODs into fixels, improving downstream fixel-based analysis.

## Method Summary
The SDNet architecture takes 9×9×9×30 voxel neighborhoods (9 voxels in each spatial dimension, 30 DWI signals) as input and produces 94-channel FOD predictions. The network alternates between DWI consistency blocks, which solve a linear system to ensure FOD coefficients satisfy the DWI signal model, and deep regularisation blocks implemented as residual neural networks. A separate fixel classification network predicts the number of fixels per voxel, and this prediction is used in the loss function to penalize reconstructed FODs that are segmented into an incorrect number of fixels. The network is trained on HCP data with a 20/3/7 split for training, validation, and testing.

## Key Results
- SDNet achieves competitive performance compared to state-of-the-art FOD super-resolution network FOD-Net across multiple metrics
- The fixel classification penalty can be tuned to improve fixel-based analysis metrics (fixel accuracy, PAE, AFDE) while maintaining reasonable FOD-based performance
- DWI consistency blocks are critical for performance, as ablation studies show significant degradation when they are removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-based deep learning ensures intermediate FODs are consistent with the DWI signal, reducing errors from acquisition-invariant representations.
- Mechanism: The SDNet architecture alternates between DWI consistency blocks and deep regularisation blocks. The consistency blocks solve a linear system to ensure the predicted FOD coefficients satisfy the DWI signal model, directly conditioning outputs on the true DWI signal rather than a pre-resampled version.
- Core assumption: The forward model linking FOD coefficients to DWI signals is accurate and stable enough for the matrix inversion to be robust.
- Evidence anchors:
  - [abstract]: "that ensures intermediate and output FODs produced by the network are consistent with the input DWI signals"
  - [section]: "Data consistency blocks use prior knowledge of an appropriate forward model to ensure a network produces solutions consistent with the input signal."
- Break condition: If the matrix inversion becomes ill-conditioned due to noise or undersampling, the consistency block may amplify errors rather than correct them.

### Mechanism 2
- Claim: The fixel classification penalty improves angular separation of FODs without distorting overall shape, enabling better fixel-based analysis.
- Mechanism: A separate fixel classification network predicts the number of fixels per voxel. This prediction is used in the loss function to penalize reconstructed FODs that are segmented into an incorrect number of fixels, guiding the main network toward FODs with better angular resolution.
- Core assumption: The fast marching level set algorithm used for ground truth fixel counts is accurate enough to train the classification network.
- Evidence anchors:
  - [abstract]: "we implement a fixel classification penalty within our loss function, encouraging the network to produce FODs that can subsequently be segmented into the correct number of fixels"
  - [section]: "To overcome the inherent non-differentiable nature of the fast marching level set FOD segmentation algorithm, a fixel classification network is applied to predict the number of fixels each voxel contains."
- Break condition: If the classification network overfits to the training data's FOD distribution, it may misguide the main network when applied to different FOD reconstructions.

### Mechanism 3
- Claim: Unrolling the alternating optimization scheme into a deep network allows data-driven learning of the regularisation term, optimized for FOD reconstruction rather than generic denoising.
- Mechanism: The SDNet architecture unrolls the variable splitting optimization for CSD with a learned denoising network replacing the hand-crafted regularisation. This enables the network to learn task-specific priors for FOD reconstruction.
- Core assumption: The unrolled optimization scheme converges to a good solution for the FOD reconstruction problem.
- Evidence anchors:
  - [section]: "In order to learn the regularisation to improve FOD reconstruction performance, the iterative process can be unrolled and the denoising step solved using a neural network"
- Break condition: If the unrolled iterations do not capture the full optimization dynamics, the learned regulariser may not generalize well beyond the training distribution.

## Foundational Learning

- Concept: Spherical harmonics representation of DWI signals and FODs
  - Why needed here: The entire method operates in the SH domain, representing both DWI signals and FODs as coefficients up to a maximum order (lmax=8 for white matter).
  - Quick check question: What SH order is used to model white matter FODs in this work, and why is this order chosen?

- Concept: Constrained spherical deconvolution (CSD) and its forward model
  - Why needed here: The DWI consistency blocks rely on the CSD forward model AQ to map FOD coefficients to DWI signals, and the entire reconstruction problem is framed as a CSD problem.
  - Quick check question: How does the matrix AQ relate FOD coefficients to DWI signals in the CSD framework?

- Concept: Fixel-based analysis and FOD segmentation
  - Why needed here: The fixel classification penalty is designed to improve FODs for subsequent fixel-based analysis, which requires accurate segmentation of FODs into fixels.
  - Quick check question: What metric is used to evaluate whether an FOD is segmented into the correct number of fixels?

## Architecture Onboarding

- Component map: Input layer (9×9×9×30 voxel neighborhood) -> initial consistency block (lmax=4) -> deep regularisation block -> consistency block (lmax=8) -> deep regularisation block -> consistency block (lmax=8) -> output FOD
- Critical path: DWI signals → initial consistency block (lmax=4) → deep regularisation block → consistency block (lmax=8) → deep regularisation block → consistency block (lmax=8) → output FOD
- Design tradeoffs: Spatial resolution is reduced by 2 voxels in each dimension after each deep regularisation block; fixel classification penalty trades off some FOD-based metrics for improved fixel-based performance
- Failure signatures: If DWI consistency blocks are removed, performance degrades significantly across all metrics (as shown in ablation study). If fixel classification penalty is too strong, fixel accuracy may improve but FOD-based metrics like SSE and ACC will worsen.
- First 3 experiments:
  1. Train SDNet without DWI consistency blocks to verify the ablation study results.
  2. Vary the fixel classification penalty weight κ to find the optimal tradeoff between FOD-based and fixel-based performance.
  3. Test SDNet on data with different b-values and b-vectors to verify acquisition invariance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed fixel classification penalty term in the loss function affect the overall quality and accuracy of FOD reconstruction compared to traditional methods?
- Basis in paper: [explicit] The paper introduces a fixel classification penalty term within the loss function to encourage the network to produce FODs that can be accurately segmented into the correct number of fixels.
- Why unresolved: The paper provides a qualitative comparison of the FODs reconstructed by SDNet and SDNetκ, but does not provide a quantitative analysis of the impact of the fixel classification penalty on the overall quality and accuracy of FOD reconstruction.
- What evidence would resolve it: A quantitative analysis comparing the FOD-based and fixel-based performance metrics of SDNet and SDNetκ, as well as a comparison with traditional FOD reconstruction methods.

### Open Question 2
- Question: How does the inclusion of DWI consistency blocks in the SDNet architecture impact the performance of the network compared to architectures without such blocks?
- Basis in paper: [explicit] The paper introduces DWI consistency blocks in the SDNet architecture to ensure intermediate and output FODs are consistent with the input DWI signals.
- Why unresolved: The paper provides a qualitative comparison of the FODs reconstructed by SDNet and FOD-Net, but does not provide a quantitative analysis of the impact of the DWI consistency blocks on the performance of the network.
- What evidence would resolve it: A quantitative analysis comparing the FOD-based and fixel-based performance metrics of SDNet and a variant of SDNet without DWI consistency blocks, as well as a comparison with other FOD reconstruction methods.

### Open Question 3
- Question: How does the proposed model-based deep learning approach for FOD reconstruction perform in the presence of abnormalities or pathologies in the data, which are not well-represented in the training data?
- Basis in paper: [inferred] The paper mentions that the ultimate goal of deep learning-based FOD reconstruction is to produce FODs that are useful for quantitative analysis, and that the performance of the network may be impacted by the presence of abnormalities or pathologies in the data.
- Why unresolved: The paper does not provide any analysis or discussion of the performance of the proposed method in the presence of abnormalities or pathologies in the data.
- What evidence would resolve it: A quantitative analysis comparing the FOD-based and fixel-based performance metrics of the proposed method and other FOD reconstruction methods in the presence of abnormalities or pathologies in the data.

## Limitations
- The method's performance depends critically on accurate matrix inversion in DWI consistency blocks, which may become ill-conditioned with noise or undersampling
- The fixel classification penalty shows sensitivity to hyperparameter tuning, suggesting potential overfitting to training data FOD distributions
- The proposed acquisition invariance has only been tested on HCP data with specific acquisition parameters

## Confidence
- High confidence: The overall architecture design and ablation study demonstrating the importance of DWI consistency blocks
- Medium confidence: The fixel classification penalty's effectiveness, given its sensitivity to hyperparameter tuning and potential overfitting to training data FOD distributions
- Medium confidence: The acquisition invariance claims, as the paper only tests on HCP data with specific acquisition parameters

## Next Checks
1. Test the SDNet architecture on data with varying b-values and b-vectors to verify true acquisition invariance beyond the HCP dataset
2. Conduct a thorough sensitivity analysis of the fixel classification penalty weight κ across a wider range of values to characterize the performance tradeoff more precisely
3. Evaluate the stability of the DWI consistency blocks under different noise levels and undersampling scenarios to assess robustness to real-world acquisition imperfections