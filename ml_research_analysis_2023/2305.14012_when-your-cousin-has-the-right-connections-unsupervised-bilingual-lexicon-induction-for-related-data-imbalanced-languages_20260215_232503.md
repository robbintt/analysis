---
ver: rpa2
title: 'When your Cousin has the Right Connections: Unsupervised Bilingual Lexicon
  Induction for Related Data-Imbalanced Languages'
arxiv_id: '2305.14012'
source_url: https://arxiv.org/abs/2305.14012
tags:
- language
- languages
- word
- embeddings
- bilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a simple method for unsupervised bilingual
  lexicon induction between closely related low-resource and high-resource languages,
  leveraging a pre-trained BERT model from the high-resource language. The approach
  only requires inference on the high-resource language BERT and shows superior performance
  for truly low-resource languages Bhojpuri and Magahi (with <5M tokens each), against
  near-zero performance of existing state-of-the-art methods.
---

# When your Cousin has the Right Connections: Unsupervised Bilingual Lexicon Induction for Related Data-Imbalanced Languages

## Quick Facts
- arXiv ID: 2305.14012
- Source URL: https://arxiv.org/abs/2305.14012
- Reference count: 16
- Key outcome: Introduces a method for unsupervised bilingual lexicon induction between closely related low-resource and high-resource languages, showing superior performance for Bhojpuri and Magahi with <5M tokens each.

## Executive Summary
This paper presents a novel approach for unsupervised bilingual lexicon induction between closely related low-resource and high-resource languages. The method leverages a pre-trained high-resource BERT model to predict translations for masked words in low-resource language contexts, iteratively building a bilingual lexicon. The approach is particularly effective for truly low-resource languages like Bhojpuri and Magahi, where existing methods fail completely. The resulting bilingual lexicons are released for five low-resource Indic languages.

## Method Summary
The method uses a pre-trained high-resource language BERT model to generate candidate translations for masked words in low-resource language sentences. It employs an iterative process where learned translation equivalents are reused to replace target dialect words in future sentences, improving BERT's ability to handle subsequent masked words. The approach uses orthographic similarity filtering (both basic and rule-based variants) to select likely cognate translations. Two main variants are presented: BASIC, which uses normalized orthographic similarity, and RULEBOOK, which incorporates learned character-level edit distances. The method processes sentences in priority order, focusing on those with words most likely to have clear translations.

## Key Results
- Superior performance for truly low-resource languages Bhojpuri and Magahi (<5M tokens each) against near-zero performance of existing state-of-the-art methods
- P@2 scores ranging from 0.1-0.4 for closely related language pairs
- Successfully induced bilingual lexicons for five low-resource Indic languages: Bhojpuri, Magahi, Awadhi, Braj, and Maithili

## Why This Works (Mechanism)

### Mechanism 1
High-resource BERT can predict target-language words masked in low-resource contexts when languages share vocabulary and syntax. Masking an unknown low-resource word and using high-resource BERT for prediction leverages shared lexical and syntactic patterns. Core assumption: closely related languages have sufficient shared vocabulary and syntax for the high-resource BERT to generate meaningful candidates. Break condition: if languages lack sufficient lexical/syntactic overlap, BERT predictions become random and unreliable.

### Mechanism 2
Iterative processing of low-resource sentences using learned bilingual equivalents improves BERT's ability to handle subsequent masked words. Growing the lexicon from sentence to sentence and reusing learned equivalents to make processed sentences more source-dialect-like. Core assumption: translation equivalents learned from one sentence can aid BERT in processing future sentences containing those equivalents. Break condition: if initial predictions are consistently incorrect, lexicon growth compounds errors rather than improving performance.

### Mechanism 3
Orthographic similarity filtering helps select likely cognate translations when mapping between closely related languages. Using normalized orthographic similarity and character-level edit distances to rerank BERT predictions. Core assumption: cognates between closely related languages share similar spellings, making orthographic similarity a useful heuristic. Break condition: for non-cognate words or languages with different orthographic conventions, orthographic filtering may incorrectly eliminate valid translations.

## Foundational Learning

- **Concept**: Contextual embeddings from pre-trained language models
  - Why needed here: The method relies on high-resource BERT's ability to generate contextually appropriate predictions
  - Quick check question: What makes contextual embeddings from pre-trained models particularly useful for low-resource language tasks?

- **Concept**: Cognate identification through orthographic similarity
  - Why needed here: The method uses orthographic similarity to filter and rank translation candidates
  - Quick check question: How can orthographic similarity help distinguish cognates from unrelated words in related languages?

- **Concept**: Iterative algorithm design for lexicon building
  - Why needed here: The method progressively improves by reusing learned translation equivalents
  - Quick check question: What advantages does an iterative approach offer when building bilingual lexicons from scratch?

## Architecture Onboarding

- **Component map**: Low-resource language corpus -> High-resource BERT model and tokenizer -> Priority queue for sentence-word pairs -> Orthographic similarity scoring module -> Lexicon storage and update system

- **Critical path**: 1. Load high-resource BERT and tokenizer 2. Initialize priority queue with (sentence, unknown_word) pairs 3. Iteratively process pairs: mask word, get BERT predictions, filter by orthographic similarity 4. Update lexicon and priority queue 5. Repeat until termination condition

- **Design tradeoffs**: Simple orthographic similarity vs. learned character-level edit distances; processing order (prioritizing "easy" sentences first); single-token vs. multi-token word handling

- **Failure signatures**: Low precision even with high-resource BERT (indicates insufficient language similarity); lexicon growth stalls early (indicates poor initial predictions); majority of predictions are identical words (indicates over-reliance on vocabulary overlap)

- **First 3 experiments**: 1. Run BASIC variant on a small test corpus and verify lexicon growth 2. Compare BASIC vs. RULEBOOK variants on the same data to measure orthographic similarity impact 3. Test performance degradation when masking words with no orthographic similarity to any high-resource word

## Open Questions the Paper Calls Out

### Open Question 1
How would the proposed method perform for language pairs where the low-resource language is not as closely related to the high-resource language, but still shares some cognates? The paper focuses on closely related language pairs with high lexical overlap, and discusses limitations for distant language pairs. Experiments testing the method on language pairs with moderate lexical overlap and shared cognates but less syntactic similarity would resolve this.

### Open Question 2
Could incorporating syntactic information improve the method's accuracy for syntactic words and postpositions, which currently show lower performance? The discussion section notes that the method fails on syntactic words and postpositions, and that syntactic words like auxiliary verbs are poorly handled. Experiments incorporating syntactic features or dependency parsing information into the method, with results showing improved accuracy for syntactic words and postpositions, would resolve this.

### Open Question 3
What is the impact of corpus contamination and code-mixing on the quality of bilingual lexicons generated by the proposed method? The discussion mentions that the Hindi BERT model has seen some Marathi/Nepali data due to corpus contamination and code-mixing, which may affect its performance. Controlled experiments varying the degree of corpus contamination and code-mixing in the training data, measuring its impact on lexicon quality and exploring potential mitigation strategies, would resolve this.

## Limitations
- Relies heavily on orthographic similarity between closely related languages, limiting generalization to language pairs with different scripts
- Assumes sufficient shared vocabulary between high-resource and low-resource languages, which may break down for distantly related pairs
- Evaluation limited to a small set of Indic languages with <5M tokens each, raising scalability questions

## Confidence

**High Confidence**: The core mechanism of using high-resource BERT to predict masked words in low-resource contexts (Mechanism 1) is well-established and experimental results are convincing.

**Medium Confidence**: The iterative lexicon building approach (Mechanism 2) shows promise, but the paper doesn't fully explore how initial prediction errors might compound over iterations.

**Low Confidence**: The orthographic similarity filtering mechanism (Mechanism 3) may have limited applicability beyond closely related languages with similar scripts.

## Next Checks
1. Test the method on language pairs with different scripts (e.g., Hindi-Marathi using Devanagari vs. Hindi-English using Latin) to evaluate robustness of orthographic similarity filtering.

2. Conduct ablation studies to quantify the contribution of each component: high-resource BERT predictions alone, orthographic filtering alone, and iterative lexicon building process.

3. Evaluate the method's performance on a low-resource language with significant vocabulary borrowing from unrelated languages to test the assumption that shared vocabulary is sufficient for successful predictions.