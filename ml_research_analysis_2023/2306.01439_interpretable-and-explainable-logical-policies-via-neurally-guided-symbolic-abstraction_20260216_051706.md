---
ver: rpa2
title: Interpretable and Explainable Logical Policies via Neurally Guided Symbolic
  Abstraction
arxiv_id: '2306.01439'
source_url: https://arxiv.org/abs/2306.01439
tags:
- rules
- action
- agent
- nudge
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NUDGE introduces neurally guided differentiable logic policies
  for interpretable and explainable reinforcement learning. It combines neural guidance
  with symbolic abstraction to efficiently learn interpretable policies as weighted
  logic rules, while maintaining strong performance.
---

# Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction

## Quick Facts
- **arXiv ID**: 2306.01439
- **Source URL**: https://arxiv.org/abs/2306.01439
- **Reference count**: 40
- **Primary result**: NUDGE learns interpretable, explainable policies that outperform neural baselines on logic-oriented environments while maintaining human-readable rule sets

## Executive Summary
NUDGE introduces neurally guided differentiable logic policies for interpretable and explainable reinforcement learning. The method combines neural guidance with symbolic abstraction to efficiently learn interpretable policies as weighted logic rules, while maintaining strong performance. It uses differentiable forward reasoning to compute action distributions from logical representations and employs gradient-based attribution to explain decisions. Experimental results show NUDGE outperforms purely neural baselines on logic-oriented environments, adapts well to environmental changes, and produces interpretable policies that can be read by humans.

## Method Summary
NUDGE learns interpretable policies by combining neurally guided symbolic abstraction with differentiable logic programming. It uses a pretrained neural policy to guide the discovery of candidate weighted logic rules, then optimizes these rules using an actor-critic framework with differentiable forward reasoning. The approach converts object-centric states into probabilistic logic facts, applies weighted rules through soft logical operations, and updates rule weights via policy gradient methods. Gradient-based attribution over logical representations provides explanations for action decisions.

## Key Results
- Outperforms neural baselines (DQN, PPO) on three custom logic-oriented environments while maintaining interpretability
- Successfully adapts to environmental changes (e.g., GetOut+ with more enemies) through rule weight updates
- Produces human-readable weighted logic rules that explain agent behavior through gradient-based attribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neurally-guided symbolic abstraction efficiently discovers interpretable action rules by leveraging pretrained neural policies as oracles
- Mechanism: The algorithm generates candidate rules through refinement operations, then uses a pretrained neural policy to score each rule's similarity to the neural policy's behavior. Top-k rules are selected based on this similarity score, ensuring only promising rules enter the symbolic policy.
- Core assumption: A well-performing neural policy encodes the optimal decision strategy that can be distilled into interpretable logic rules
- Evidence anchors:
  - [abstract]: "exploits trained neural network-based agents to guide the search of candidate-weighted logic rules"
  - [section]: "promising action rules for an RL task entail the same actions as the neural policy"
  - [corpus]: "Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach" shows similar neuro-symbolic integration, though without the top-k guided search
- Break condition: If the neural policy is poorly trained or doesn't capture the optimal strategy, the guidance will lead to suboptimal rule discovery

### Mechanism 2
- Claim: Differentiable forward reasoning enables end-to-end gradient-based optimization of weighted logic rules while maintaining interpretability
- Mechanism: Action rules are assigned learnable weights that are optimized through actor-critic methods. The differentiable forward reasoning module computes action probabilities by evaluating weighted rules against current state representations, allowing backpropagation through the logical inference process.
- Core assumption: Soft logical operations (like softor and softand) can approximate discrete logical reasoning while remaining differentiable
- Evidence anchors:
  - [abstract]: "uses differentiable forward reasoning to compute action distributions from logical representations"
  - [section]: "We compose the differentiable forward reasoning function following Shindo et al. [2023]. It computes soft logical entailment based on efficient tensor operations."
  - [corpus]: "Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference" uses similar differentiable logical structures
- Break condition: If the smoothing parameter γ is poorly tuned, the soft logic may either become too deterministic (losing differentiability) or too fuzzy (losing logical structure)

### Mechanism 3
- Claim: Gradient-based attribution over logical representations provides explanations for action decisions
- Mechanism: Since the policy is fully differentiable, gradients of the action distribution with respect to input atoms (∂vA/∂v(0)) reveal which input features were most influential in the decision, providing post-hoc explanations in terms of the original logical predicates.
- Core assumption: Gradient magnitudes in the logical representation space correspond to feature importance for the final action selection
- Evidence anchors:
  - [abstract]: "employs gradient-based attribution to explain decisions"
  - [section]: "we can compute attribution values over logical representations using their gradients"
  - [corpus]: "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents" explores similar attribution methods in hybrid systems
- Break condition: If the logical representation becomes too abstract or the attribution is computed too early in the pipeline, the explanations may lose semantic meaning

## Foundational Learning

- Concept: Markov Decision Processes and Reinforcement Learning
  - Why needed here: NUDGE operates within the RL framework, requiring understanding of states, actions, policies, and the goal of maximizing expected return
  - Quick check question: What distinguishes policy-based from value-based RL methods, and which approach does NUDGE use?

- Concept: First-Order Logic and Predicate Logic
  - Why needed here: NUDGE represents states and actions using FOL predicates and rules, requiring familiarity with atoms, literals, clauses, and definite clauses
  - Quick check question: How does NUDGE's action-state language differ from standard FOL, and why is this distinction important for RL?

- Concept: Differentiable Logic Programming
  - Why needed here: NUDGE uses soft logical operations that are differentiable, enabling gradient-based learning while maintaining logical interpretability
  - Quick check question: What role does the smoothing parameter γ play in the soft logical operations, and how does it affect the balance between differentiability and logical fidelity?

## Architecture Onboarding

- Component map:
  Relational Perception Module -> Differentiable Forward Reasoning Module -> Action Selection Module -> Environment -> Reward -> Critic Update -> Rule Weight Update

- Critical path: State → Relational Perception → Differentiable Forward Reasoning → Action Selection → Environment → Reward → Critic Update → Rule Weight Update

- Design tradeoffs:
  - Expressiveness vs. Efficiency: More complex predicates enable richer policies but increase computational cost exponentially
  - Interpretability vs. Performance: Simpler rules are more interpretable but may not capture optimal strategies
  - Neural Guidance vs. Autonomy: Using pretrained neural policies accelerates learning but may bias the symbolic policy

- Failure signatures:
  - Poor performance with high variance: Rule set may be insufficiently expressive or weights poorly optimized
  - Overly complex rules with low interpretability: May need constraint on rule complexity or better pruning strategy
  - Slow convergence: Could indicate poor neural policy guidance or need for hyperparameter tuning

- First 3 experiments:
  1. Run NUDGE on GetOut with expert supervision (using pretrained neural policy) to verify basic functionality and rule generation
  2. Test NUDGE without neural guidance (baseline) to quantify the benefit of the guided abstraction approach
  3. Evaluate NUDGE's adaptation to environment variations (e.g., GetOut+ with more enemies) to test robustness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section, several key questions emerge regarding the trade-offs between interpretability and performance, the impact of smoothing parameters on policy quality, and the scalability of the approach to more complex environments.

## Limitations
- Reliance on pretrained neural policies as oracles may introduce bias if the neural policy has systematic errors or suboptimal strategies
- Evaluation environments are relatively simple compared to real-world applications, raising questions about scalability
- Interpretability claims lack quantitative measures of human understandability through user studies

## Confidence

- **High Confidence**: The core technical approach (neurally-guided symbolic abstraction with differentiable forward reasoning) is well-defined and the experimental methodology is sound. The performance improvements over baselines in the tested environments are clearly demonstrated.
- **Medium Confidence**: The interpretability and explainability claims are supported by the methodology but would benefit from additional validation through user studies or quantitative interpretability metrics.
- **Medium Confidence**: The adaptation claims to environmental changes are demonstrated but limited to relatively simple modifications of the test environments.

## Next Checks

1. **Neural Policy Sensitivity Analysis**: Systematically evaluate NUDGE's performance when using neural policies of varying quality (trained with different hyperparameters or architectures) to quantify the sensitivity to the guidance oracle quality.

2. **Interpretability User Study**: Conduct a controlled user study where participants rate the understandability of NUDGE policies versus neural baselines, using standardized interpretability metrics like cognitive walk-through or time-to-understanding measurements.

3. **Scalability Benchmark**: Test NUDGE on more complex environments (e.g., multi-task RL problems or environments with longer time horizons) to evaluate whether the rule complexity and reasoning depth remain tractable while maintaining interpretability.