---
ver: rpa2
title: 'CLEVA: Chinese Language Models EVAluation Platform'
arxiv_id: '2308.04813'
source_url: https://arxiv.org/abs/2308.04813
tags:
- uni00000044
- uni00000051
- uni0000004c
- uni00000048
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLEVA is a comprehensive platform designed to holistically evaluate
  Chinese Large Language Models (LLMs). It addresses the lack of a standardized and
  thorough evaluation framework by introducing a robust benchmark that covers 84 datasets,
  9 metrics, and 31 tasks, including new Chinese-specific tasks.
---

# CLEVA: Chinese Language Models EVAluation Platform

## Quick Facts
- arXiv ID: 2308.04813
- Source URL: https://arxiv.org/abs/2308.04813
- Authors: 
- Reference count: 40
- Key outcome: Comprehensive platform for Chinese LLM evaluation with 84 datasets, 9 metrics, 31 tasks, and 23 evaluated models

## Executive Summary
CLEVA addresses the critical need for standardized Chinese Large Language Model evaluation by providing a comprehensive platform that goes beyond simple accuracy metrics. The platform covers 84 datasets across 31 tasks, incorporating 9 different evaluation metrics including robustness, fairness, and privacy. By introducing new Chinese-specific tasks and employing a sophisticated contamination mitigation strategy, CLEVA provides reliable and comparable evaluation results for the growing Chinese LLM ecosystem.

## Method Summary
CLEVA implements a standardized prompt-based evaluation methodology using multiple prompt templates per task to ensure comparability and analyze model sensitivity. The platform employs a sampling strategy to create unique test sets for each evaluation round, mitigating train-test contamination. It covers 84 datasets with 370K test instances from 9M augmented queries, evaluating models across 31 tasks using 9 metrics beyond accuracy. The platform provides an easy-to-use interface where users can submit models and receive comprehensive performance assessments across multiple dimensions.

## Key Results
- Validated 23 influential Chinese LLMs across 31 tasks and 9 metrics
- Achieved comprehensive coverage with 84 datasets and 33.98% newly collected data
- Demonstrated effectiveness of contamination mitigation through unique test set sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLEVA achieves comprehensive Chinese LLM evaluation by combining extensive task coverage, multi-metric assessment, and new Chinese-specific tasks.
- Mechanism: The platform integrates 84 datasets spanning 31 tasks across ability evaluation and application assessment, with 33.98% newly collected data. It employs 9 metrics beyond accuracy (calibration, robustness, fairness, bias, toxicity, efficiency, diversity, privacy) and introduces Chinese-specific tasks like Pinyin transliteration and classical Chinese understanding.
- Core assumption: A holistic evaluation requires diverse task types, multiple evaluation dimensions, and culturally relevant test cases.
- Evidence anchors:
  - [abstract] "It addresses the lack of a standardized and thorough evaluation framework by introducing a robust benchmark that covers 84 datasets, 9 metrics, and 31 tasks, including new Chinese-specific tasks."
  - [section 4.1] "Our platform employs a standardized workflow to assess LLMs' performance across various dimensions, regularly updating a competitive leaderboard."
  - [corpus] Weak evidence - only 8 related papers found, average FMR 0.51, no citations. Limited validation of comprehensiveness claim.

### Mechanism 2
- Claim: CLEVA standardizes evaluation through multiple prompt templates per task to ensure comparability and analyze model sensitivity.
- Mechanism: For each test set, CLEVA provides an average of 3.95 manually annotated prompt templates supporting all major prompting formats. It calculates performance statistics across templates to examine robustness and estimate performance bounds.
- Core assumption: Different prompt templates reveal model sensitivity and ensure comparable evaluation across studies.
- Evidence anchors:
  - [abstract] "CLEVA employs a standardized prompt-based methodology, using multiple prompt templates per task to ensure comparability and analyze model sensitivity."
  - [section 4.1] "CLEVA takes full control of key aspects of LLM evaluation, with data and prompts being the most important among them. All data are jointly prepared with unified preprocessing steps..."
  - [corpus] No direct evidence - related papers don't discuss prompt standardization methodology.

### Mechanism 3
- Claim: CLEVA proactively mitigates train-test contamination through new data collection and unique test set sampling.
- Mechanism: CLEVA curates 33.98% new data and develops a sampling strategy that guarantees unique test subsets for each leaderboard round, estimating acceptable approximation error thresholds.
- Core assumption: Continuous data refreshment and unique sampling prevent contamination while maintaining evaluation reliability.
- Evidence anchors:
  - [abstract] "To mitigate train-test contamination, it curates new data and employs a sampling strategy for unique test sets in each evaluation round."
  - [section 4.2] "CLEVA advocates a proactive method for securing trustworthy evaluation results. By collecting extensive new data, CLEVA suppresses the leakage of testing data prior to the evaluation."
  - [corpus] No direct evidence - related papers don't discuss contamination mitigation strategies.

## Foundational Learning

- Concept: Standardized evaluation methodology
  - Why needed here: Ensures comparability across different models and studies, preventing misleading conclusions from varying evaluation protocols
  - Quick check question: What are the key components that must be standardized in LLM evaluation to ensure fair comparison?

- Concept: Train-test contamination in LLM evaluation
  - Why needed here: LLMs are trained on massive corpora that may include test data, making contamination a critical threat to evaluation validity
  - Quick check question: Why is train-test contamination particularly problematic for large language models compared to traditional machine learning models?

- Concept: Multi-metric evaluation framework
  - Why needed here: Single metrics like accuracy fail to capture the full spectrum of LLM capabilities and risks, requiring comprehensive assessment
  - Quick check question: What are the limitations of using accuracy as the sole metric for evaluating large language models?

## Architecture Onboarding

- Component map: Data Collection Pipeline -> Prompt Template Engine -> Model Interface Layer -> Evaluation Engine -> Leaderboard System -> User Interface

- Critical path: User submits model -> API configuration -> Task selection -> Prompt template application -> Model inference -> Response collection -> Metric computation -> Result aggregation -> Leaderboard update

- Design tradeoffs:
  - Standardization vs. Flexibility: Strict prompt templates ensure comparability but may limit exploration of model capabilities
  - New Data vs. Existing Datasets: Fresh data reduces contamination but requires more resources to collect and validate
  - Comprehensive Metrics vs. Efficiency: Multiple metrics provide holistic view but increase computational cost and complexity

- Failure signatures:
  - Inconsistent results across prompt templates indicating model sensitivity issues
  - Significant performance drops suggesting potential contamination or data leakage
  - Metric computation errors from malformed responses or unexpected output formats
  - API connectivity problems affecting model evaluation throughput

- First 3 experiments:
  1. Test basic functionality with a simple model on a single task with one prompt template
  2. Evaluate model sensitivity by running same task with multiple prompt templates
  3. Validate contamination mitigation by comparing results with known contaminated datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inverse scaling phenomenon manifest in specific Chinese tasks, and what are the underlying mechanisms?
- Basis in paper: [explicit] The paper mentions inverse scaling appears in the instruction following task, where larger models like GPT-4, InternLM-104B, and LLaMA-65B perform worse than MOSS-16B.
- Why unresolved: The paper acknowledges this observation but does not rigorously verify the existence of inverse scaling or explore its underlying causes in Chinese tasks.
- What evidence would resolve it: Conducting controlled experiments with varying model sizes on a wider range of Chinese tasks, analyzing performance trends, and identifying specific task characteristics that trigger inverse scaling.

### Open Question 2
- Question: What are the emergent abilities of Chinese LLMs, and how do they differ from English LLMs?
- Basis in paper: [inferred] The paper notes that tasks with high standard deviation, such as mathematical reasoning, code synthesis, and Pinyin transliteration, are candidates for emergent abilities in the Chinese context.
- Why unresolved: The paper recognizes these tasks as potential emergent abilities but does not conduct a thorough investigation to confirm their emergent nature or compare them to English LLMs.
- What evidence would resolve it: Designing systematic evaluations to measure the performance of Chinese LLMs across a wide range of tasks and comparing the results with English LLMs, identifying tasks where Chinese LLMs exhibit unexpected capabilities.

### Open Question 3
- Question: How can privacy evaluation be improved beyond detecting personally identifiable information (PII)?
- Basis in paper: [explicit] The paper acknowledges the limitations of current privacy evaluation methods, which focus primarily on PII detection, and expresses the intention to incorporate more aspects of private content in the future.
- Why unresolved: The paper recognizes the need for more comprehensive privacy evaluation but does not provide specific details on how to address this limitation or what additional aspects of privacy should be considered.
- What evidence would resolve it: Developing and implementing new privacy evaluation techniques that go beyond PII detection, such as assessing the model's ability to generate sensitive information or its vulnerability to privacy attacks.

## Limitations

- The effectiveness of contamination mitigation relies heavily on the assumed uniqueness of test sets, but the sampling strategy details remain unspecified
- New data collection at 33.98% may not be sufficient given the scale of modern LLM training corpora
- The platform's generalization to non-Chinese languages and cross-lingual tasks has not been validated

## Confidence

- Mechanism 1 (Comprehensive Evaluation): High confidence - well-supported by specified task and metric coverage
- Mechanism 2 (Prompt Standardization): Medium confidence - methodology described but limited external validation
- Mechanism 3 (Contamination Mitigation): Low confidence - strategy outlined but lacks detailed implementation and effectiveness data

## Next Checks

1. Test contamination detection by evaluating a known contaminated model across multiple evaluation rounds
2. Validate prompt template effectiveness by measuring variance in model performance across different templates
3. Assess new data sufficiency by comparing evaluation results with and without the 33.98% newly collected data