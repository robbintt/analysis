---
ver: rpa2
title: Discovering Variable Binding Circuitry with Desiderata
arxiv_id: '2307.03637'
source_url: https://arxiv.org/abs/2307.03637
tags:
- desiderata
- variable
- components
- value
- circuitry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce an automated method to localize model components responsible
  for performing a specific subtask by specifying desiderata, or causal attributes
  of the target computation. We apply our method to automatically discover shared
  variable binding circuitry in LLaMA-13B, which retrieves variable values for multiple
  arithmetic tasks.
---

# Discovering Variable Binding Circuitry with Desiderata

## Quick Facts
- arXiv ID: 2307.03637
- Source URL: https://arxiv.org/abs/2307.03637
- Authors: 
- Reference count: 10
- We introduce an automated method to localize model components responsible for performing a specific subtask by specifying desiderata, or causal attributes of the target computation. We apply our method to automatically discover shared variable binding circuitry in LLaMA-13B, which retrieves variable values for multiple arithmetic tasks. Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream, achieving high accuracy on both Value Dependence and Operation Invariance desiderata. Interestingly, the learned mask transfers well to multiplication operations not seen during training, indicating the discovered circuitry is indeed shared across arithmetic operations.

## Executive Summary
This paper introduces a novel method for discovering specific neural circuit components by specifying causal desiderata that define desired behavior. The approach is demonstrated on LLaMA-13B to identify variable binding circuitry - components that retrieve variable values during arithmetic operations. By defining two desiderata (Value Dependence and Operation Invariance) and optimizing a sparse binary mask over model components, the method successfully localizes the target circuitry to just 10 components out of 1,640, while achieving strong generalization to unseen operations.

## Method Summary
The method learns a binary mask over model components (attention heads and MLPs) by optimizing for causal desiderata that define desired behavior. Components are patched by replacing their activations with values from alternate inputs, and the mask weights are learned via gradient descent with ℓ0.5 regularization to encourage sparsity. The approach alternates between optimizing for Value Dependence (output should change when variable values change) and Operation Invariance (output should remain unchanged when operations change) desiderata, ultimately identifying a sparse set of components that satisfy both criteria.

## Key Results
- Successfully localizes variable binding circuitry to 9 attention heads and 1 MLP (out of 1,640 total components)
- Achieves high accuracy on both Value Dependence and Operation Invariance desiderata
- Learned mask transfers well to multiplication operations not seen during training
- Sparsity regularization identifies only essential components while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The binary mask optimization finds sparse attention head and MLP combinations that specifically perform value-copying for variable binding.
- **Mechanism**: By defining desiderata that require specific causal effects (Value Dependence and Operation Invariance), the optimization learns a sparse mask where each component's activation can be replaced with an alternate input's activation to achieve the desired output change or invariance. The ℓ0.5 regularization encourages sparsity, leading to identification of only the essential components.
- **Core assumption**: The variable binding operation is performed by a small, localized set of components rather than being distributed across the entire model.
- **Evidence anchors**:
  - [abstract] "Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP"
  - [section] "We optimize the continuous mask according to LD, which measures how well the patching intervention defined by the mask meets our desiderata. We use ℓ0.5 regularization with tunable strength λ over the mask entries to encourage patching only a sparse set of model components"
  - [corpus] Weak evidence - no direct citation for this specific mechanism in related papers

### Mechanism 2
- **Claim**: The two desiderata (Value Dependence and Operation Invariance) work together to isolate the value-copying subtask from other arithmetic operations.
- **Mechanism**: Value Dependence ensures the mask identifies components that control output changes when variable values change, while Operation Invariance ensures these same components don't affect output when operations change. Together, they isolate components that specifically copy variable values regardless of the arithmetic operation.
- **Core assumption**: The value-copying circuitry is shared across different arithmetic operations and can be isolated by contrasting these two types of interventions.
- **Evidence anchors**:
  - [section] "We propose two desiderata to isolate this hypothesized value-copying circuitry: Value Dependence... Operation Invariance..."
  - [section] "We also find that including both desiderata is crucial for locating this circuitry; with only the VD desideratum, the identified heads successfully alter model behavior in the VD scenario, but also affect the model's output in the OI scenario"
  - [corpus] Weak evidence - related work mentions causal interventions but not this specific dual-desideratum approach

### Mechanism 3
- **Claim**: The continuous relaxation of the binary mask with convex combination (wi·v + (1-wi)·va) enables gradient-based optimization to find the optimal sparse mask.
- **Mechanism**: The continuous relaxation allows gradients to flow through the patching operation, enabling optimization via standard methods. The convex combination smoothly interpolates between original and patched activations, with wi controlling the degree of patching. After optimization, rounding to binary provides the final mask.
- **Core assumption**: The continuous relaxation is a good approximation of the discrete binary mask, and rounding preserves the desired causal properties.
- **Evidence anchors**:
  - [section] "We define a mask over the model components by assigning a learnable weight wi ∈ [0, 1], to each component ci... That is, wi = 0 corresponds to fully patching component ci with its value va from the sequence a, wi = 1 corresponds to not patching ci, and 0 < wi < 1 corresponds to taking a convex combination of the ci's activation value v and the value va"
  - [section] "Empirically, we find that rounding the mask to become binary typically has little effect on its ability to satisfy the desiderata, and attribute this to the regularization for sparsity during training"
  - [corpus] Weak evidence - convex combinations in optimization are standard but not specifically validated for this causal patching context

## Foundational Learning

- **Concept**: Causal mediation analysis
  - **Why needed here**: The method relies on understanding how interventions on specific model components affect downstream outputs, which is fundamentally a causal question about the computational graph.
  - **Quick check question**: If you patch an attention head's activation with a corrupted input, what does it mean if the output changes significantly versus not at all?

- **Concept**: Activation patching and causal intervention
  - **Why needed here**: The entire method is built on replacing activations from one input with activations from another to test component importance, which requires understanding how patching experiments work.
  - **Quick check question**: In activation patching, why do we run the model on both original and corrupted/alternate inputs before performing the patch?

- **Concept**: Sparse optimization and regularization
  - **Why needed here**: The method uses ℓ0.5 regularization to encourage sparse masks, which requires understanding how different regularization norms affect sparsity and why sparsity is desirable for interpretability.
  - **Quick check question**: How does ℓ0.5 regularization differ from ℓ1 regularization in terms of promoting sparsity, and why might ℓ0.5 be preferred for finding circuit components?

## Architecture Onboarding

- **Component map**: The model is decomposed into 1,640 individual components: 1,600 attention heads (across 40 layers × 40 heads per layer) and 40 MLPs (one per layer). Each component's contribution to the final token's residual stream is considered separately for patching.

- **Critical path**: The patching operation targets the final token's residual stream, where variable values are hypothesized to be copied before arithmetic operations are performed. This suggests the critical path for variable binding occurs in the middle-to-late layers of the model.

- **Design tradeoffs**: 
  - Granularity vs. computational cost: Using individual attention heads provides fine-grained localization but increases computational cost compared to layer-level patching.
  - Sparsity vs. completeness: Strong regularization finds sparse circuits but may miss distributed computation; weaker regularization captures more components but reduces interpretability.
  - Desiderata specificity vs. generalizability: Highly specific desiderata may isolate the target computation but may not transfer to related tasks.

- **Failure signatures**: 
  - High Value Dependence accuracy but low Operation Invariance accuracy suggests the mask captures arithmetic computation rather than just value copying.
  - Low accuracy on both desiderata suggests the method failed to identify the correct components or the components don't exist as hypothesized.
  - Good performance on training desiderata but poor transfer to multiplication suggests overfitting to specific operations.

- **First 3 experiments**:
  1. Run the full method with both desiderata on the training set to verify it identifies approximately 10 components and achieves high accuracy on both VD and OI.
  2. Test the learned mask on held-out addition/subtraction problems to confirm it generalizes within the training distribution.
  3. Test the learned mask on multiplication problems (even though not in training) to verify transfer to unseen operations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed method generalize to more complex tasks beyond simple arithmetic, such as logical reasoning or natural language understanding tasks?
- Basis in paper: [explicit] The paper focuses on variable binding for arithmetic tasks as a proof of concept, but does not explore more complex tasks.
- Why unresolved: The paper does not test the method on tasks beyond arithmetic, so its effectiveness on more complex tasks is unknown.
- What evidence would resolve it: Applying the method to a variety of complex tasks (e.g., logical reasoning, natural language understanding) and evaluating its performance compared to existing methods would provide evidence.

### Open Question 2
- Question: How does the choice of granularity (e.g., attention heads vs. layers) impact the effectiveness and computational efficiency of the method?
- Basis in paper: [explicit] The paper uses attention heads and MLPs as model components, but mentions that more or less granular representations are possible.
- Why unresolved: The paper does not explore the impact of different granularities on the method's performance or computational efficiency.
- What evidence would resolve it: Experiments comparing the method's performance and computational cost using different granularities would provide insights.

### Open Question 3
- Question: What is the relationship between the discovered variable binding circuitry and the model's overall performance on arithmetic tasks?
- Basis in paper: [explicit] The paper discovers circuitry responsible for variable binding but does not analyze its impact on overall task performance.
- Why unresolved: The paper focuses on localizing the circuitry but does not investigate how it affects the model's accuracy on arithmetic tasks.
- What evidence would resolve it: Ablation studies or performance comparisons with and without the discovered circuitry would provide insights into its importance.

## Limitations
- The method assumes variable binding is performed by a small, localized set of components, which may not hold for all models or tasks
- Success depends heavily on the choice and definition of desiderata - misspecification could lead to incorrect circuitry identification
- The continuous relaxation of binary masks may not perfectly capture discrete patching behavior, though reported minimal impact in practice
- Generalizability to more complex tasks beyond simple arithmetic remains unproven

## Confidence

- **High confidence**: The method successfully identifies sparse circuitry for variable binding in the specific experimental setup (LLaMA-13B on two-digit arithmetic). The quantitative results (9 attention heads + 1 MLP, high accuracy on both desiderata) are well-supported by the reported experiments.

- **Medium confidence**: The transfer of learned masks to multiplication operations not seen during training indicates some degree of generalization, but the extent and limitations of this transfer are not fully characterized. The claim that this proves the discovered circuitry is "indeed shared across arithmetic operations" is suggestive but requires more systematic investigation.

- **Low confidence**: The broader applicability of the method to other variable binding tasks or different model architectures is largely untested. The paper focuses on a narrow experimental domain, and the robustness of the approach to different desiderata formulations or model types is unknown.

## Next Checks

1. **Systematic ablation study**: Remove individual components from the identified mask one-by-one to determine which are essential versus redundant for variable binding performance, providing a more complete characterization of the circuit.

2. **Cross-task generalization**: Apply the same desiderata and method to a different variable binding task (e.g., text-based variable assignments or symbolic reasoning) to test whether the approach generalizes beyond arithmetic operations.

3. **Alternative desiderata comparison**: Test whether other formulations of Value Dependence and Operation Invariance (or entirely different desiderata) can identify the same circuitry, establishing the robustness of the approach to specification choices.