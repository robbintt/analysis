---
ver: rpa2
title: Proximity-Informed Calibration for Deep Neural Networks
arxiv_id: '2306.04590'
source_url: https://arxiv.org/abs/2306.04590
tags:
- proximity
- calibration
- bias
- confidence
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a novel problem called proximity bias in
  deep learning calibration, where models are more overconfident on samples from low-density
  regions (low proximity) of the data distribution. Through analysis of 504 pretrained
  ImageNet models, the authors show that proximity bias persists even after applying
  standard calibration methods like temperature scaling, and that Transformer-based
  models are more susceptible to this issue than CNNs.
---

# Proximity-Informed Calibration for Deep Neural Networks

## Quick Facts
- arXiv ID: 2306.04590
- Source URL: https://arxiv.org/abs/2306.04590
- Reference count: 40
- Key outcome: PROCAL method improves calibration by addressing proximity bias, showing consistent improvements across 504 ImageNet models and multiple dataset types

## Executive Summary
This paper identifies a novel problem called proximity bias in deep learning calibration, where models are more overconfident on samples from low-density regions (low proximity) of the data distribution. Through analysis of 504 pretrained ImageNet models, the authors show that proximity bias persists even after applying standard calibration methods like temperature scaling, and that Transformer-based models are more susceptible to this issue than CNNs. To address this, they propose PROCAL, a plug-and-play method that adjusts confidence estimates based on proximity, with two variants: Density-Ratio Calibration for continuous confidences and Bin-Mean-Shift for discrete confidences. They also introduce PIECE, a new calibration metric that accounts for proximity bias. Extensive experiments on balanced, long-tail, and distribution-shifted datasets show that PROCAL consistently improves calibration across four metrics (ECE, ACE, MCE, PIECE) over existing methods.

## Method Summary
The paper proposes PROCAL, a proximity-informed calibration method that addresses proximity bias by adjusting confidence estimates based on sample proximity to the data distribution. The method has two variants: Density-Ratio Calibration, which uses Kernel Density Estimation to estimate the ratio of densities for correct vs incorrect predictions in the proximity-confidence space, and Bin-Mean-Shift, which adjusts confidence scores based on the difference between accuracy and mean confidence within 2D bins of proximity and confidence space. The authors also introduce PIECE, a new calibration metric that extends ECE by conditioning on both confidence and proximity. The approach is evaluated on 504 ImageNet models across balanced, long-tail, and distribution-shifted datasets, showing consistent improvements over existing calibration methods.

## Key Results
- Proximity bias persists even after standard calibration methods like temperature scaling, with low proximity samples showing larger calibration errors
- Transformer-based models are more susceptible to proximity bias than CNN-based models
- PROCAL consistently improves calibration across four metrics (ECE, ACE, MCE, PIECE) on balanced, long-tail, and distribution-shifted datasets
- Bin-Mean-Shift provides a Brier score guarantee in theoretical analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Density-Ratio Calibration adjusts confidence estimates based on proximity by estimating the ratio of densities for correct vs incorrect predictions in the proximity-confidence space.
- **Mechanism:** The method splits samples into correct and incorrect groups, estimates their joint density distributions using Kernel Density Estimation (KDE) in the two-dimensional space of confidence and proximity, then applies Bayes' rule to compute calibrated probabilities.
- **Core assumption:** The joint distribution of confidence and proximity differs systematically between correct and incorrect predictions, and KDE can accurately estimate these densities.
- **Evidence anchors:**
  - [abstract] "To tackle proximity bias and further improve confidence calibration, we propose a plug-and-play method, PROCAL. Intuitively, PROCAL learns a joint distribution of proximity and confidence to adjust probability estimates."
  - [section 5.1] "P( ˆY = Y | ˆP , D) can be computed via Bayes' rule: P( ˆY = Y | ˆP , D) = P( ˆP , D | ˆY = Y )P( ˆY = Y ) / P( ˆP , D)"
  - [corpus] Weak evidence - the corpus contains calibration methods but none specifically use density ratio approaches for proximity-aware calibration.
- **Break condition:** KDE fails to accurately estimate densities in high-dimensional or sparse regions, or the assumption that proximity-confidence distributions differ systematically between correct and incorrect predictions does not hold.

### Mechanism 2
- **Claim:** Bin-Mean-Shift calibrates by adjusting confidence scores based on the difference between accuracy and mean confidence within 2D bins of proximity and confidence space.
- **Mechanism:** The method creates equal-sized bins in the 2D proximity-confidence space, calculates the accuracy and mean confidence for each bin, then shifts the confidence of all samples in that bin by a weighted difference between actual accuracy and mean confidence.
- **Core assumption:** Calibration errors vary systematically across different regions of the proximity-confidence space, and local adjustments can correct these errors without disrupting well-calibrated regions.
- **Evidence anchors:**
  - [abstract] "To fully leverage the distinct properties of the input information, we develop two separate algorithms tailored for continuous and discrete inputs."
  - [section 5.2] "For each bin Bmh, we calculate its accuracy A(Bmh) and mean confidence F (Bmh). Then, for all the samples in the bin, we adjust their confidence scores as follows: ˆPours = ˆP + λ · (A(Bmh) − F (Bmh))"
  - [corpus] Weak evidence - corpus contains calibration methods but none specifically use bin-mean-shift approaches for proximity-aware calibration.
- **Break condition:** The binning strategy fails to capture meaningful variation in the proximity-confidence space, or the regularization parameter λ is poorly chosen leading to overcorrection.

### Mechanism 3
- **Claim:** PIECE metric captures miscalibration errors that ECE misses by accounting for proximity-dependent calibration errors.
- **Mechanism:** PIECE extends ECE by conditioning on both confidence and proximity, thus measuring calibration error within proximity subgroups rather than just across confidence levels.
- **Core assumption:** Calibration errors are not uniform across proximity levels, and conditioning on proximity reveals miscalibration that would otherwise cancel out in aggregate metrics.
- **Evidence anchors:**
  - [abstract] "To further quantify the effectiveness of calibration algorithms in mitigating proximity bias, we introduce proximity-informed expected calibration error (PIECE) with theoretical analysis."
  - [section 4] "PIECE = E ˆP ,D [P( ˆY = Y | ˆP , D) − ˆP ]"
  - [corpus] Weak evidence - corpus contains various calibration metrics but none specifically designed to capture proximity-dependent miscalibration.
- **Break condition:** Calibration errors are actually uniform across proximity levels, or the additional complexity of PIECE provides no meaningful differentiation from ECE.

## Foundational Learning

- **Concept:** Bayes' theorem for probability estimation
  - **Why needed here:** The density-ratio calibration method fundamentally relies on Bayesian inference to compute calibrated probabilities from density ratios
  - **Quick check question:** If P(correct|confidence=0.8, proximity=0.3) = 0.7 and P(correct|confidence=0.8, proximity=0.7) = 0.9, what does this tell us about proximity bias?

- **Concept:** Kernel Density Estimation (KDE)
  - **Why needed here:** KDE is used to estimate the joint density distributions of confidence and proximity for correct vs incorrect predictions
  - **Quick check question:** What happens to KDE estimates when the number of samples in a region is very small?

- **Concept:** Brier score decomposition
  - **Why needed here:** The theoretical analysis of Bin-Mean-Shift relies on decomposing the Brier score to show improvement
  - **Quick check question:** How does the Brier score relate to both calibration and accuracy aspects of model performance?

## Architecture Onboarding

- **Component map:** Model predictions (confidence, predicted label) and validation set with features -> Proximity computation using K-nearest neighbors -> Calibration methods (Density-Ratio or Bin-Mean-Shift) -> Calibrated confidence scores -> Evaluation using ECE, ACE, MCE, PIECE

- **Critical path:**
  1. Compute proximity for all validation samples using K-nearest neighbors
  2. Choose calibration method (Density-Ratio or Bin-Mean-Shift)
  3. Train calibration model on validation set
  4. Apply calibration to test samples
  5. Evaluate using multiple calibration metrics

- **Design tradeoffs:**
  - Density-Ratio Calibration: More expressive but computationally heavier due to density estimation; better for continuous confidences
  - Bin-Mean-Shift: Simpler and more robust but potentially less expressive; better for discrete confidences
  - Proximity computation: Trade-off between accuracy (larger K) and computational cost

- **Failure signatures:**
  - Poor calibration performance despite method application: Check if proximity computation is correct and if validation set is representative
  - High variance in calibration results: Check binning strategy or KDE bandwidth parameters
  - Memory issues: Check if neighbor search is optimized and if validation set size is manageable

- **First 3 experiments:**
  1. Apply Density-Ratio Calibration to temperature-scaled outputs on a small dataset to verify basic functionality
  2. Compare Bin-Mean-Shift vs Density-Ratio on a dataset with known proximity bias to see which performs better
  3. Evaluate PIECE vs ECE on a calibrated model to verify that PIECE captures additional information about proximity bias

## Open Questions the Paper Calls Out

- **Open Question 1:** How does proximity bias vary across different model architectures beyond those tested, particularly in newer transformer variants or specialized architectures?
  - **Basis in paper:** [explicit] The paper shows transformer-based models are more susceptible to proximity bias than CNN-based models, but only examined 504 models from timm library.
  - **Why unresolved:** The study focused on a specific set of architectures and model sizes, leaving open the question of how proximity bias manifests in other architectural families or more recent model developments.
  - **What evidence would resolve it:** Testing proximity bias across a broader range of architectures including newer transformer variants, specialized vision architectures, and models from different development frameworks.

- **Open Question 2:** Can the theoretical guarantees of Bin-Mean-Shift be extended to settings beyond binary classification?
  - **Basis in paper:** [explicit] The theoretical analysis provides a Brier score guarantee specifically for binary classification.
  - **Why unresolved:** The paper focuses on multi-class classification in experiments but only proves theoretical bounds for binary cases, leaving open the question of whether similar guarantees exist for multi-class settings.
  - **What evidence would resolve it:** Extending the theoretical analysis to multi-class classification and proving analogous bounds for Brier score or other appropriate metrics.

## Limitations

- The paper does not provide detailed implementation specifications for critical components like KDE bandwidth selection and binning strategies, making exact reproduction challenging
- All experiments are conducted on vision datasets (ImageNet, iNaturalist, etc.), leaving open questions about generalizability to other domains like NLP or speech
- The proximity computation relies on K-nearest neighbors which becomes computationally expensive at scale, though the paper claims efficiency improvements

## Confidence

- **High Confidence:** The existence of proximity bias and its persistence after standard calibration (supported by extensive experiments on 504 models)
- **Medium Confidence:** The effectiveness of PROCAL across different dataset types (balanced, long-tail, distribution-shifted), though some variation in improvement magnitude is observed
- **Medium Confidence:** The theoretical guarantee for Bin-Mean-Shift's Brier score improvement, as the proof assumes idealized conditions

## Next Checks

1. **Cross-domain validation:** Test PROCAL on non-vision datasets (e.g., text classification) to verify the proximity bias phenomenon and calibration improvements extend beyond computer vision
2. **Ablation study on hyperparameters:** Systematically vary K (for proximity computation), number of bins, and KDE bandwidth to quantify their impact on calibration performance
3. **Real-world deployment test:** Evaluate PROCAL on a safety-critical application where calibrated uncertainty estimates are crucial, measuring both calibration accuracy and decision-making performance