---
ver: rpa2
title: Dataset Quantization
arxiv_id: '2308.10524'
source_url: https://arxiv.org/abs/2308.10524
tags:
- dataset
- data
- arxiv
- selection
- ratio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dataset quantization (DQ) framework to compress
  large-scale datasets into small subsets that can be used for training any neural
  network architectures. The key idea is to divide the dataset into non-overlapping
  bins recursively based on submodular gains, and then uniformly sample from each
  bin to form the final compact set.
---

# Dataset Quantization

## Quick Facts
- arXiv ID: 2308.10524
- Source URL: https://arxiv.org/abs/2308.10524
- Reference count: 40
- Compresses ImageNet-1K by 40% and Alpaca by 80% with negligible performance loss

## Executive Summary
This paper introduces Dataset Quantization (DQ), a framework that compresses large-scale datasets into compact subsets suitable for training any neural network architecture. The method recursively divides datasets into non-overlapping bins using submodular gains, then uniformly samples from each bin to ensure diversity and coverage. DQ achieves state-of-the-art compression ratios on both vision and language tasks while maintaining performance across multiple architectures.

## Method Summary
DQ recursively divides the dataset into non-overlapping bins based on submodular gains, then uniformly samples from each bin to form a compact subset. The framework employs patch dropping with GradCAM++ importance scoring and MAE-based reconstruction to further reduce storage. This approach ensures diversity preservation while maintaining the original data distribution, enabling cross-architecture generalization that outperforms existing dataset distillation methods.

## Key Results
- Compresses ImageNet-1K by 40% and Alpaca by 80% with negligible performance drop
- Outperforms previous dataset distillation methods in scalability and cross-architecture generalization
- Achieves 388x speedup compared to DM for ImageNet compression with 60% data retention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive dataset division via submodular gains preserves diversity while ensuring representativeness
- Mechanism: Recursive splitting constrains each subsequent bin to samples farther from already-selected data, mathematically enforcing diversity through submodular optimization
- Core assumption: Submodular gain function effectively captures both representativeness and diversity in feature space
- Evidence anchors:
  - Theoretical proof in Appendix shows recursive selection increases diversity compared to one-shot coreset selection
  - Claims DQ outperforms GraphCut by "maximizing the diversity gains"

### Mechanism 2
- Claim: Patch dropping with importance scoring reduces storage without harming training performance
- Mechanism: GradCAM++ scores pixel importance; low-scoring patches are dropped and reconstructed by pre-trained MAE decoder
- Core assumption: MAE reconstruction capability is sufficient to recover dropped patches without losing discriminative information
- Evidence anchors:
  - Mentions GradCAM++ attention scoring with ablation showing improvement over random dropping
  - Cites MAE decoder for reconstruction

### Mechanism 3
- Claim: Uniform sampling across bins ensures cross-architecture generalization
- Mechanism: Each bin captures different data distribution slices; uniform sampling prevents architecture-specific bias
- Core assumption: Bins collectively span full feature space distribution of dataset
- Evidence anchors:
  - Claims DQ can train "any neural network architectures"
  - Tab. 2 shows cross-architecture performance significantly better than DM

## Foundational Learning

- **Submodular optimization**
  - Why needed here: Core selection strategy relies on maximizing submodular gains to balance diversity and representativeness
  - Quick check question: What property of submodular functions makes them suitable for diversity maximization in dataset selection?

- **Masked Auto-Encoder (MAE)**
  - Why needed here: Used to reconstruct images after patch dropping, enabling storage reduction without losing information
  - Quick check question: How does MAE's masked reconstruction capability enable lossy compression of images without harming training?

- **t-SNE visualization**
  - Why needed here: Empirically verifies that bins and final dataset preserve original data distribution across architectures
  - Quick check question: What does t-SNE reveal about the diversity of samples within and across DQ bins?

## Architecture Onboarding

- **Component map**: Feature extractor (ResNet-18/ViT) -> Submodular gain calculator -> Recursive bin generator -> Uniform sampler -> Patch dropper + GradCAM++ scorer -> MAE-based reconstructor

- **Critical path**: 
  1. Extract features from full dataset
  2. Recursively select K samples per bin using submodular gains
  3. Aggregate bins via uniform sampling
  4. Drop low-importance patches and store compressed dataset

- **Design tradeoffs**:
  - More bins → better distribution coverage but higher pre-processing cost
  - Higher patch drop ratio → lower storage but risk of information loss
  - Feature extractor choice → affects submodular gain quality and bin composition

- **Failure signatures**:
  - Low diversity in bins → one or few bins dominate → poor generalization
  - Excessive patch dropping → reconstructed images lose critical detail → training collapse
  - Too few bins → sampling variance increases → unstable performance

- **First 3 experiments**:
  1. Verify submodular gain computation on small dataset; visualize bin diversity via t-SNE
  2. Test MAE reconstruction quality on dropped patches; measure PSNR vs drop ratio
  3. Compare cross-architecture performance of DQ-compressed vs original dataset on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DQ compare to other dataset distillation methods in terms of computational efficiency and scalability?
- Basis in paper: Explicit claim that DQ is 388 times faster than DM in GPU hours for ImageNet compression
- Why unresolved: Only compared with one method (DM); lacks comprehensive comparison with other methods
- What evidence would resolve it: Comprehensive comparison of DQ with multiple dataset distillation methods in computational efficiency and scalability

### Open Question 2
- Question: How does DQ performance vary with different network architectures and tasks?
- Basis in paper: Mentions DQ can train various architectures including ViT, ResNet, MobileNetV2, and language tasks
- Why unresolved: Results provided for limited architectures and tasks; doesn't explore wide range of architectures
- What evidence would resolve it: Comprehensive evaluation across wide range of network architectures and tasks

### Open Question 3
- Question: How does DQ handle imbalanced datasets and datasets with varying data distributions?
- Basis in paper: Inferred from recursive selection based on submodular gains aiming to maximize diversity and coverage
- Why unresolved: No detailed analysis of DQ's performance on imbalanced datasets or varying distributions
- What evidence would resolve it: Analysis of DQ's performance on imbalanced datasets and datasets with varying distributions

## Limitations
- Implementation details for patch dropping strategy and MAE reconstruction are underspecified
- Computational cost of recursive selection process could be prohibitive for very large datasets
- Cross-architecture generalization claims may not hold for architectures beyond those tested

## Confidence
- High confidence: Core theoretical framework for submodular gain-based dataset division is well-established
- Medium confidence: Empirical results showing performance retention are compelling but depend on hyperparameter choices
- Low confidence: Generalization claims to arbitrary architectures and patch dropping effectiveness require further validation

## Next Checks
1. Implement recursive bin generation on CIFAR-10; verify diversity preservation through t-SNE visualization
2. Test MAE reconstruction quality at different patch drop ratios (10%, 30%, 50%); measure PSNR and downstream performance
3. Evaluate cross-architecture performance by training ResNet and ViT on DQ-compressed ImageNet-1K; compare with DM and KDT