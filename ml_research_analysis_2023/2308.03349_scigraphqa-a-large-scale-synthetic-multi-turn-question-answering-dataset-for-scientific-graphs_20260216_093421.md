---
ver: rpa2
title: 'SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset
  for Scientific Graphs'
arxiv_id: '2308.03349'
source_url: https://arxiv.org/abs/2308.03349
tags:
- dataset
- scigraphqa
- graphs
- data
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SciGraphQA, a large-scale synthetic multi-turn
  question-answering dataset for scientific graphs. The dataset is constructed by
  using a commercial large-language model to generate question-answering dialogues
  based on academic graphs, paper titles, abstracts, and graph context.
---

# SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs

## Quick Facts
- arXiv ID: 2308.03349
- Source URL: https://arxiv.org/abs/2308.03349
- Reference count: 31
- Key outcome: 295K synthetic multi-turn question-answering dialogues for scientific graphs, 13× larger than previous largest dataset

## Executive Summary
SciGraphQA introduces a large-scale synthetic dataset for multi-turn question-answering on scientific graphs, constructed using a commercial LLM (Palm-2) to generate questions and answers based on academic papers. The dataset is 13 times larger than previous chart-visual question-answering datasets and includes open-vocabulary questions that better capture real-world scientific discourse. The authors evaluate various multimodal large language models on this dataset, with LLaVA-13B achieving the highest performance, and demonstrate that fine-tuning LLaVA using SciGraphQA significantly improves its CIDEr score from 0.08 to 0.26.

## Method Summary
The dataset is constructed by extracting figures, captions, paragraphs, and OCR text from 290K ArXiv papers (2010-2020) in computer science and machine learning domains. This textual context (paper title, abstract, paragraph mentioning the graph, and rich text contextual data) is provided to Palm-2, which generates multi-turn conversations about the graphs. The quality of these synthetic samples is assessed using GPT-4, which rated the dataset with an average of 8.7/10. The dataset is then used to evaluate various multimodal large language models (MLLMs) including BLIP-2, LLaVA variants, mPLUG-owl, and OpenFlamingo, with and without DePlot table augmentation. Finally, LLaVA-13B is fine-tuned using the dataset with LoRA adapters.

## Key Results
- SciGraphQA contains 295K samples, making it 13× larger than the previous largest chart-visual question-answering dataset
- LLaVA-13B achieves the highest zero-shot performance with a CIDEr score of 0.08 on the test set
- Fine-tuning LLaVA-13B on SciGraphQA improves CIDEr score to 0.26
- DePlot table augmentation significantly improves performance across multiple MLLM models
- GPT-4 quality assessment rates the dataset at 8.7/10 average, with only 1.2% of samples rated ≤ 1/10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The synthetic dataset achieves high quality through a carefully constructed context pipeline that combines OCR-extracted text, figure captions, and the first paragraph referencing the figure with paper title and abstract before prompting the LLM.
- Mechanism: By providing comprehensive textual context to Palm-2, the model can generate question-answer pairs that are deeply connected to the graph's content and the paper's narrative. This multi-source context enables the LLM to understand not just the visual elements but also the scientific discourse surrounding the graph.
- Core assumption: The combination of multiple textual sources (OCR, caption, paragraph, title, abstract) provides sufficient context for an LLM to generate meaningful and accurate questions and answers about scientific graphs.
- Evidence anchors: [abstract]: "As context, we provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich text contextual data from the graph itself" [section]: "We combined figures, captions, paragraphs, and text-from-OCR from the SciCap+ dataset with the corresponding paper titles and abstracts, and subsequently prompted the PALM2 Chat API to generate multi-turn conversations"

### Mechanism 2
- Claim: The open-vocabulary nature of the generated questions and answers, as opposed to template-based approaches, creates a more challenging and realistic evaluation benchmark for MLLMs.
- Mechanism: By generating questions conditioned on the specific text context rather than selecting from fixed templates, the dataset includes a wider variety of question types, including those requiring reasoning about data trends, interpreting axes, and understanding relationships between variables.
- Core assumption: Open-vocabulary questions better capture the complexity of real-world scientific discourse than template-based questions with fixed vocabularies.
- Evidence anchors: [abstract]: "SciGraphQA distinguishes itself from previous datasets by using real-world academic graphs rather than synthetic data or charts. The questions and answers inherently support open vocabulary." [section]: "We observed a greater diversity in our question set, owing to their generation process conditioned on text context."

### Mechanism 3
- Claim: The DePlot table extraction augmentation significantly improves MLLM performance by providing structured data that compensates for the vision encoder's limitations with text-heavy scientific graphs.
- Mechanism: DePlot converts charts into linearized tables with interpolated axis values and legends, which are then prepended to the question prompt. This structured data format allows even non-vision-specialized LLMs like GPT-3.5 to answer chart-related questions effectively.
- Core assumption: Providing structured tabular data extracted from charts is more effective for MLLMs than relying solely on image features, especially for text-heavy scientific graphs.
- Evidence anchors: [section]: "We further augmented the question prompts by prepending them with the data tables extracted from the graphs using the DePlot model... LLaVa-13B received a considerable performance boost compared to mPLug-owl-7B"

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs) and their adapter architectures
  - Why needed here: Understanding how LLaVA, mPLUG-owl, and other MLLMs work is crucial for interpreting the performance differences and designing effective fine-tuning strategies
  - Quick check question: What is the key architectural difference between LLaVA's linear projector and mPLUG-owl's combination of perceived resampler and cross-attention?

- Concept: Evaluation metrics for visual question answering (CIDEr, BLEU-4, ROUGE)
  - Why needed here: The paper reports multiple NLP metrics to evaluate generated answers against ground truth, and understanding these metrics is essential for interpreting the quantitative results
  - Quick check question: Why might CIDEr be a more appropriate metric than BLEU-4 for evaluating generated answers in a scientific graph QA context?

- Concept: Fine-tuning techniques for large language models (LoRA, learning rate scheduling)
  - Why needed here: The paper describes using LoRA for fine-tuning LLaVA-13B and employs specific learning rate schedules, which are critical for understanding the experimental results
  - Quick check question: Why did the authors choose a lower learning rate (5 × 10−6) for fine-tuning compared to the original LLaVA instruction tuning (2 × 10−5)?

## Architecture Onboarding

- Component map: ArXiv paper corpus -> PDF extraction (PDFFigures 2.0) -> Figure categorization -> Context assembly (title, abstract, caption, paragraph, OCR) -> Palm-2 generation -> Filtering -> GPT-4 quality assessment -> MLLM evaluation (BLIP-2, LLaVA variants, mPLUG-owl, OpenFlamingo) -> DePlot augmentation -> NLP metric calculation (CIDEr, BLEU-4, ROUGE) -> Fine-tuning pipeline (LLaVA-13B -> LoRA adapters -> Two-stage training)
- Critical path: Data generation -> Quality assessment -> MLLM evaluation -> Fine-tuning -> Performance analysis
- Design tradeoffs: Synthetic vs. human-annotated data (scaling vs. quality); Open-vocabulary vs. template-based questions (complexity vs. evaluation difficulty); LoRA vs. full fine-tuning (computational cost vs. performance)
- Failure signatures: Low CIDEr scores despite high-quality ground truth (vision encoder limitations); High variance in GPT-4 ratings (inconsistent synthetic generation); Performance degradation during fine-tuning (catastrophic forgetting)
- First 3 experiments: 1) Replicate zero-shot evaluation with 100 samples across all MLLM models to verify relative performance ordering; 2) Test DePlot augmentation with LLaVA-13B to confirm performance boost; 3) Run small-scale fine-tuning (1 epoch on 10K samples) to verify LoRA training works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of MLLMs on scientific graph question-answering tasks be further improved beyond the current state-of-the-art?
- Basis in paper: [inferred]
- Why unresolved: While the paper shows that fine-tuning LLaVA-13B on SciGraphQA achieves a CIDEr score of 0.26, the authors anticipate further improvements by including segmentation mask tokens and leveraging larger LLM backbones coupled with emergent prompting techniques.
- What evidence would resolve it: Experiments demonstrating improved performance on SciGraphQA or other scientific graph QA benchmarks using these proposed methods.

### Open Question 2
- Question: How effective would GPT-4 be as an evaluation tool for MLLMs on scientific graph QA tasks, and how does its evaluation compare to traditional NLP metrics like CIDEr, BLEU, and ROUGE?
- Basis in paper: [explicit]
- Why unresolved: The paper discusses the potential of using GPT-4 for evaluation but notes challenges such as inconsistent results and non-deterministic properties. The authors were unable to run extensive evaluations due to resource constraints.
- What evidence would resolve it: A comprehensive study comparing GPT-4 evaluations with traditional metrics on a large set of scientific graph QA examples, addressing the challenges mentioned.

### Open Question 3
- Question: How can MLLMs be adapted to handle out-of-distribution data in scientific graph QA tasks while mitigating catastrophic forgetting?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions the challenges of adapting MLLMs to out-of-distribution data, including the risk of catastrophic forgetting, and explores using LoRA to mitigate fine-tuning costs. However, further exploration of methods to alleviate these issues is needed.
- What evidence would resolve it: Experiments demonstrating successful adaptation of MLLMs to new scientific graph QA tasks with minimal catastrophic forgetting, using techniques such as prompt augmentation or other regularization methods.

## Limitations
- The dataset's quality depends entirely on the underlying LLM's generation capabilities, with only 0.4% of samples evaluated by GPT-4
- Lack of human-annotated ground truth makes it impossible to verify whether higher MLLM scores indicate better understanding or just better matching of LLM-generated patterns
- The first-paragraph context extraction may miss the most relevant information for some figures
- OCR quality issues can introduce errors that propagate through the generation pipeline

## Confidence

**High Confidence**: The claim that SciGraphQA is 13 times larger than the previous largest chart-visual question-answering dataset is supported by clear quantitative evidence in the paper and the corpus analysis shows related work in synthetic dataset generation.

**Medium Confidence**: The claim that open-vocabulary questions create a more challenging benchmark is plausible given the described generation process, but lacks comparative analysis with template-based approaches using the same dataset. The performance differences between MLLM models are reported with specific metrics, but the lack of human-annotated ground truth makes it difficult to assess whether higher scores indicate better understanding or just better matching of LLM-generated patterns.

**Low Confidence**: The claim that DePlot augmentation significantly improves performance is supported by specific CIDEr score improvements, but the mechanism explanation is somewhat speculative. The paper suggests that LLaVA-13B better utilizes the data-table strings than mPLUG-owl-7B, but doesn't provide a clear explanation for why the larger model backbone would be more effective at processing linearized table data.

## Next Checks

1. **Human Evaluation Study**: Conduct a human evaluation of 1,000 randomly sampled question-answer pairs from SciGraphQA, where human annotators assess both the relevance of questions to the graphs and the correctness of answers. This would provide ground truth data to validate the GPT-4 quality ratings and help identify systematic biases in the synthetic generation process.

2. **Cross-Domain Generalization Test**: Evaluate the fine-tuned LLaVA model on scientific graphs from domains not well-represented in the training data (e.g., biology, chemistry, or medicine) to assess whether the dataset enables genuine understanding or merely pattern matching on computer science and machine learning graphs.

3. **Ablation Study on Context Components**: Systematically remove each context component (title, abstract, caption, paragraph, OCR) and regenerate questions using the same Palm-2 model to quantify the contribution of each information source to question quality and answer accuracy. This would validate the claimed importance of the comprehensive context pipeline.