---
ver: rpa2
title: An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing
  and Development
arxiv_id: '2312.16211'
source_url: https://arxiv.org/abs/2312.16211
tags:
- causal
- relation
- chart
- these
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) like ChatGPT
  to audit and refine causal networks discovered from observational data. The key
  idea is to present ChatGPT with causal edges one at a time and ask it to assess
  directionality, identify confounders and mediators, and provide insights.
---

# An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development

## Quick Facts
- arXiv ID: 2312.16211
- Source URL: https://arxiv.org/abs/2312.16211
- Reference count: 38
- Key outcome: LLM-assisted causal model refinement increased BIC score by 70.69% and achieved 94% edge direction accuracy

## Executive Summary
This paper proposes using large language models like ChatGPT to audit and refine causal networks discovered from observational data. The approach involves querying ChatGPT with structured prompts about each causal edge to assess directionality, identify confounders and mediators, and provide insights. Visualization tools are developed to summarize the rich text responses for human analysts. Experiments on real-world data showed that using ChatGPT's insights increased model fit by 70.69% (BIC score improved from -3673.4 to -6270.3), with ChatGPT-4 achieving 94% accuracy in determining edge direction.

## Method Summary
The method combines automated causal discovery with LLM-based auditing and human-in-the-loop refinement. First, an initial causal DAG is generated using algorithms like PC or F-GES. For each edge, the system queries ChatGPT with multiple polarity variations to assess directionality and identify mediators/confounders. NLP is used to extract insights from LLM responses, which are then visualized through Causal Debate Charts, Causal Relation Environment Charts, and Confounder/Mediator Charts. Analysts use these insights to refine the model by adding/removing edges or variables, and the process can iterate with updated data and BIC evaluation.

## Key Results
- BIC score improved from -3673.4 to -6270.3 (70.69% increase in model fit)
- ChatGPT-4 achieved 94% accuracy in determining causal edge direction
- LLM-identified mediators and confounders provided actionable insights for model refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM prompts can reduce directional uncertainty in causal edges by providing human-interpretable confidence scores.
- Mechanism: The system queries ChatGPT with multiple polarity variations of the same causal relation and takes the highest consistent score as the likely direction. This "causal debate" aggregates multiple perspectives to increase confidence.
- Core assumption: The LLM's internal causal reasoning is internally consistent across prompt variations and correlates with actual data-generating processes.
- Evidence anchors: ChatGPT-4 achieved 94% accuracy in determining edge direction; GPT-4 produced correct direction 103/110 times in experiments.
- Break condition: If LLM responses show high variance across polarity variations or scores don't correlate with known causal directions.

### Mechanism 2
- Claim: Visualizing mediators and confounders helps analysts refine causal models beyond binary edge corrections.
- Mechanism: ChatGPT's responses include identified mediators and confounders with strength ratings. The system extracts these via NLP and displays them in dedicated charts, allowing analysts to see contextual factors influencing causal relations.
- Core assumption: The mediators/confounders identified by ChatGPT are relevant and actionable for model refinement.
- Evidence anchors: LLM-identified mediators and confounders were used to guide model refinement and improve BIC scores.
- Break condition: If identified mediators/confounders are generic, irrelevant, or don't improve model fit.

### Mechanism 3
- Claim: Iterative refinement using LLM insights and automated causal discovery converges to better models.
- Mechanism: Initial DAG from PC algorithm is refined by LLM-based edge auditing. Newly identified mediators/confounders are added as nodes, the model is recomputed, and BIC score is used to assess fit improvement.
- Core assumption: Adding mediators/confounders as explicit nodes in the DAG and recomputing improves model fit (BIC).
- Evidence anchors: BIC score improved from -3673.4 to -6270.3 after LLM-assisted refinement.
- Break condition: If BIC improvement plateaus or decreases after several iterations, or if adding nodes causes overfitting.

## Foundational Learning

- Concept: Causal DAG structure and assumptions (Markov property, faithfulness, sufficiency)
  - Why needed here: The system relies on these assumptions to interpret observational data and LLM outputs; understanding violations helps diagnose edge orientation errors.
  - Quick check question: What does it mean for a DAG to be "faithful" to the data, and why does violation cause orientation errors?

- Concept: Bayesian Information Criterion (BIC) for model selection
  - Why needed here: BIC is used to quantify model fit improvement after LLM-based refinement; understanding its calculation and interpretation is essential for evaluating results.
  - Quick check question: How does BIC penalize model complexity, and why is a lower (more negative) score worse?

- Concept: Large Language Model prompting strategies
  - Why needed here: The effectiveness of the system depends on crafting prompts that elicit consistent, relevant causal judgments from the LLM.
  - Quick check question: Why does querying the same causal relation in multiple polarities help reduce hallucination risk?

## Architecture Onboarding

- Component map: Data input -> Causal discovery (PC/F-GES) -> LLM interface (structured prompts) -> NLP processor (extract insights) -> Visualization engine (3 charts) -> Model refinement -> BIC evaluation
- Critical path: 1. User uploads data → 2. PC algorithm generates DAG → 3. User selects edge → 4. LLM queried with 10 polarity variations → 5. NLP extracts insights → 6. Visualizations generated → 7. User reviews and accepts/refines → 8. Model recomputed with new variables → 9. BIC score compared
- Design tradeoffs:
  - Prompt verbosity vs. cost: More detailed prompts may yield richer insights but increase API cost and latency.
  - Visualization complexity vs. usability: Rich charts aid understanding but may overwhelm non-expert users.
  - Automated vs. manual refinement: Full automation risks propagating LLM errors; human-in-the-loop ensures domain relevance.
- Failure signatures:
  - LLM responses are inconsistent across polarity variations → check prompt wording or switch model.
  - NLP extraction fails to identify variables → update entity recognition rules or simplify text.
  - BIC score decreases after refinement → verify added variables are truly relevant; check for overfitting.
- First 3 experiments:
  1. Run PC algorithm on a small benchmark dataset with known causal structure; verify DAG matches ground truth.
  2. Select one edge with ambiguous direction; query ChatGPT with all 10 polarity variations; check consistency of scores.
  3. Add mediators/confounders identified by ChatGPT as new nodes; recompute DAG; verify BIC improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively incorporate the large amount of textual information generated by LLMs into a causal model without overwhelming the user?
- Basis in paper: [explicit] The paper discusses the challenge of dealing with the large amount of textual information generated by LLMs and the need for dedicated visualizations to make it easier for users to grasp the essence of this information.
- Why unresolved: The paper presents some initial visualizations but acknowledges that they are still too cluttered and need further development.
- What evidence would resolve it: A study evaluating the effectiveness of different visualization techniques in helping users understand and utilize the LLM-generated information in causal model development.

### Open Question 2
- Question: How can we ensure the accuracy and reliability of the causal knowledge extracted from LLMs, especially in domains with limited training data?
- Basis in paper: [explicit] The paper mentions that the causal knowledge extracted from ChatGPT is restricted by the resources used to train its large language model, and there is a need to explore alternative approaches such as training open-source LLMs with specific corpora.
- Why unresolved: The paper does not provide a concrete solution to this issue and suggests that further research is needed to address this limitation.
- What evidence would resolve it: A study comparing the accuracy and reliability of causal knowledge extracted from different LLMs, including open-source models trained on domain-specific data.

### Open Question 3
- Question: How can we develop an interactive interface that allows users to effectively verify, contextualize, and refine causal models using LLM-generated insights?
- Basis in paper: [explicit] The paper discusses the need for an interactive interface that allows users to click on causal edges to verify their direction, contextualize them, and refine them using LLM-discovered mediators, confounders, and colliders.
- Why unresolved: The paper presents this as a future direction and does not provide a concrete implementation or evaluation of such an interface.
- What evidence would resolve it: A prototype of an interactive interface that allows users to perform the described tasks, along with a study evaluating its usability and effectiveness in causal model development.

## Limitations
- Lack of external validation on multiple datasets or comparison with alternative causal refinement methods
- Limited ground truth verification for the 94% edge direction accuracy claim
- No comparative baseline for the BIC improvement of 70.69%

## Confidence

| Claim | Confidence |
|-------|------------|
| LLM accuracy for edge direction | Medium (based on single dataset, no ground truth verification) |
| BIC score improvement | Medium (significant improvement but no comparative baseline) |
| Mediator/confounder identification utility | Low (qualitative observations without quantitative validation) |

## Next Checks
1. Validate edge direction accuracy on a benchmark dataset with known ground truth causal structure to confirm the 94% accuracy claim.
2. Compare BIC improvement from LLM-assisted refinement against alternative methods (e.g., expert review, automated sensitivity analysis) on multiple datasets.
3. Conduct ablation studies removing LLM-identified mediators/confounders to quantify their actual contribution to model fit improvement.