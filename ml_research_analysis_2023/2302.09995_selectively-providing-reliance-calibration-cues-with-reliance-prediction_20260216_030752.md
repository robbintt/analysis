---
ver: rpa2
title: Selectively Providing Reliance Calibration Cues With Reliance Prediction
arxiv_id: '2302.09995'
source_url: https://arxiv.org/abs/2302.09995
tags:
- reliance
- human
- task
- pred-rc
- rccs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Pred-RC, a method for selectively providing
  reliance calibration cues (RCCs) to calibrate human trust in AI systems. Pred-RC
  uses a cognitive reliance model to predict whether a human will assign a task to
  an AI agent, and compares predictions with and without an RCC to evaluate its impact
  on human reliance.
---

# Selectively Providing Reliance Calibration Cues With Reliance Prediction

## Quick Facts
- arXiv ID: 2302.09995
- Source URL: https://arxiv.org/abs/2302.09995
- Authors: 
- Reference count: 25
- Key outcome: Pred-RC selectively provides reliance calibration cues to maintain human decision accuracy while reducing communication overhead compared to random RCC provision.

## Executive Summary
This paper introduces Pred-RC, a method for selectively providing reliance calibration cues (RCCs) to help humans calibrate their trust in AI systems. Pred-RC uses a cognitive reliance model to predict whether a human will assign a task to an AI agent, and compares predictions with and without an RCC to evaluate its impact on human reliance. In a human-AI collaboration task, Pred-RC successfully calibrated human reliance while reducing the number of RCCs needed compared to random RCC provision. The F-score for human decision accuracy remained high with Pred-RC despite fewer RCCs, whereas accuracy declined with random RCC provision as fewer cues were given. This demonstrates Pred-RC's ability to select effective RCC timing by predicting and comparing human reliance with and without cues.

## Method Summary
Pred-RC employs a Transformer-based reliance model that predicts human task assignment decisions based on collaboration history and current task state. For each new task, the model generates two predictions: rw/ (reliance if RCC is provided) and rw/o (reliance if RCC is not provided). Pred-RC compares the absolute difference between each predicted reliance rate and the AI's actual success probability (pi). If the difference is smaller when the RCC is provided, indicating better alignment between human reliance and AI capability, the RCC is provided. The method uses confidence rates from the task AI as RCCs, which were found to be more effective than success probabilities in pilot experiments. A threshold parameter controls RCC provision frequency, allowing the system to reduce communication overhead while maintaining calibration effectiveness.

## Key Results
- Pred-RC successfully calibrated human reliance while reducing RCCs needed compared to random provision
- F-score for human decision accuracy remained stable with Pred-RC even as fewer RCCs were provided
- With random RCC provision, human accuracy declined as fewer RCCs were provided

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pred-RC selectively provides RCCs by predicting whether the human will assign the current task to the AI, and comparing predictions with and without an RCC to evaluate its impact on reliance calibration.
- Mechanism: The reliance model predicts two probabilities: rw/ (reliance if RCC is provided) and rw/o (reliance if RCC is not provided). Pred-RC compares the absolute difference between each predicted reliance rate and the AI's actual success probability (pi). If the difference is smaller when the RCC is provided, it indicates the RCC helps align human reliance with AI capability, so the RCC is provided.
- Core assumption: The reliance model can accurately predict human reliance rates in both scenarios (with and without RCC), and the difference in these predictions reflects the RCC's calibration effect.
- Evidence anchors:
  - [abstract] "Pred-RC uses a cognitive reliance model to predict whether a human will assign a task to an agent. By comparing the prediction results for both cases with and without an RCC, Pred-RC evaluates the influence of the RCC on human reliance."
  - [section] "Pred-RC compares ∆w/i and ∆w/o i and decides whether to provide ˆci. Equation 1 is rewritten as follows: ci = {ˆci (∆w/o i − ∆w/ i < threshold ) [MASK ] ( elsewise)."
- Break condition: If the reliance model predictions are inaccurate or if the threshold setting is inappropriate, Pred-RC may fail to provide RCCs when needed or provide them when unnecessary, leading to poor calibration.

### Mechanism 2
- Claim: By reducing the number of RCCs while maintaining human decision accuracy, Pred-RC achieves effective reliance calibration with lower communication cost.
- Mechanism: Pred-RC uses the threshold parameter to control RCC provision. A higher threshold allows more RCCs to be omitted. The evaluation shows that with Pred-RC, F-score (decision accuracy) remains stable even as fewer RCCs are provided, whereas random RCC provision leads to declining accuracy with fewer RCCs. This demonstrates Pred-RC's ability to select effective RCC timing.
- Core assumption: The reliance model's predictions are reliable enough that Pred-RC can safely omit RCCs without degrading calibration quality.
- Evidence anchors:
  - [abstract] "In a human-AI collaboration task, Pred-RC successfully calibrated human reliance while reducing the number of RCCs needed compared to random RCC provision."
  - [section] "The results show that the workers' accuracy did not decline with Pred-RC's selective RCCs, whereas that of workers whose RCCs were randomly provided got worse as fewer RCCs were provided."
- Break condition: If the task environment changes significantly or if human behavior becomes less predictable, the model may no longer be able to reliably omit RCCs without degrading performance.

### Mechanism 3
- Claim: The reliance model captures human beliefs about AI capability by considering collaboration history, including when RCCs were provided and what tasks were involved.
- Mechanism: The reliance model is a Transformer-based architecture that takes as input the collaboration history (x:i−1, c:i−1, d:i−1, f:i−1) and current state (xi, ci). By encoding this history, the model can learn patterns in how humans update their beliefs about AI capability based on past successes, failures, and RCC provision.
- Core assumption: Human reliance is influenced by the sequence of past interactions with the AI, and this sequential information can be captured by the Transformer architecture.
- Evidence anchors:
  - [section] "The reliance model is trained to predict human reliance, taking into account the collaboration history between a human and an AI, and it is expected to capture these aspects."
  - [section] "Each feature in the collaboration history is first embedded with perceptrons. The embedded vectors are summed up with position embeddings, which give index information."
- Break condition: If the reliance model overfits to specific patterns in the training data that don't generalize to new users or tasks, it may make poor predictions and Pred-RC decisions.

## Foundational Learning

- Concept: Cognitive reliance models and human trust calibration
  - Why needed here: Understanding how humans form beliefs about AI capability and how to calibrate trust through communication is fundamental to Pred-RC's approach.
  - Quick check question: What is the difference between trust and reliance, and why does Pred-RC focus on reliance rather than trust?

- Concept: Transformer architectures and sequential modeling
  - Why needed here: The reliance model uses a Transformer encoder to capture the sequential nature of human-AI collaboration history.
  - Quick check question: How does the Transformer's attention mechanism help capture the importance of different events in the collaboration history?

- Concept: Binary classification and confidence calibration
  - Why needed here: The reliance model predicts whether a human will assign a task to the AI (binary outcome), and the RCCs provide confidence information that needs to be calibrated.
  - Quick check question: Why might humans benefit more from confidence rates than from success probabilities as RCCs?

## Architecture Onboarding

- Component map:
  Task AI -> Reliance model -> Pred-RC controller -> Human interface -> Dataset collection

- Critical path:
  1. Human-AI collaboration history is collected and preprocessed
  2. Reliance model is trained on this history to predict human decisions
  3. During deployment, for each new task:
     - Reliance model predicts rw/ and rw/o
     - Pred-RC compares ∆w/ and ∆w/o to decide RCC provision
     - Task AI provides confidence rate if RCC is provided
     - Human makes task assignment decision
     - Result is recorded for future training

- Design tradeoffs:
  - RCC granularity: Confidence rates vs. success probabilities - confidence rates were found to calibrate better in pilot experiments
  - Model complexity: Transformer vs. simpler architectures - Transformer captures sequential dependencies better
  - Threshold tuning: Strict vs. lenient RCC provision - balances communication cost against calibration effectiveness

- Failure signatures:
  - Over-provision of RCCs: Threshold too low, reliance model predictions unreliable
  - Under-provision of RCCs: Threshold too high, reliance model fails to capture when humans need guidance
  - Model bias: Reliance model overfits to specific task types or visual features
  - Communication overload: Too many RCCs provided despite good calibration

- First 3 experiments:
  1. Ablation study: Compare Pred-RC performance with and without reliance model predictions (baseline: always provide RCC, never provide RCC)
  2. Threshold sensitivity: Test different threshold values to find optimal balance between RCC reduction and accuracy maintenance
  3. Model generalization: Test reliance model trained on one CAPTCHA dataset on tasks from different datasets to assess transfer capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Pred-RC be extended to account for human capability alongside AI capability when making reliance calibration decisions?
- Basis in paper: [explicit] The paper explicitly mentions this as a limitation: "An important limitation of Pred-RC is that it does not consider human capability for a task."
- Why unresolved: The current Pred-RC framework focuses solely on comparing human reliance rates with AI success probabilities without considering the human's own task performance capabilities.
- What evidence would resolve it: Implementation and testing of a modified Pred-RC system that incorporates human capability metrics (e.g., human accuracy rates on similar tasks) and demonstrates improved total collaboration performance compared to the original Pred-RC.

### Open Question 2
- Question: What are the most effective strategies for repairing trust when Pred-RC or RCCs are distrusted by users?
- Basis in paper: [explicit] The paper discusses this issue: "Depending on the situation, the problem of distrusted RCCs should be handled in another way such as apologies, excuses, or explanations and dialogues."
- Why unresolved: While the paper mentions potential approaches like meta-RCCs and multiple RCCs, it does not empirically test these strategies or compare their effectiveness.
- What evidence would resolve it: User studies comparing different trust repair strategies (e.g., apologies vs. explanations vs. meta-RCCs) in terms of their impact on restoring trust after Pred-RC failures.

### Open Question 3
- Question: How can reinforcement learning be integrated into Pred-RC to dynamically adjust the threshold for RCC provision based on collaboration performance and communication costs?
- Basis in paper: [explicit] The paper suggests this as a future direction: "A future direction for this work is to integrate machine-learning methods to adjust the threshold."
- Why unresolved: The paper only mentions this as a theoretical possibility without providing any implementation details or experimental results.
- What evidence would resolve it: Development and evaluation of an RL-based Pred-RC system that learns optimal threshold adjustment policies, demonstrating improved balance between collaboration performance and communication costs compared to the fixed-threshold approach.

## Limitations

- Pred-RC does not consider human capability for tasks, only AI capability
- The approach is evaluated on a specific CAPTCHA-based task, limiting generalizability
- The reliance model's ability to generalize to new users or substantially different tasks is not fully established

## Confidence

- Confidence in core claims: Medium
  - Results are promising but evaluation scope is relatively narrow
  - Effectiveness may vary across different task domains and user populations
  - Long-term user behavior and potential model drift are not addressed

## Next Checks

1. Test Pred-RC across multiple task domains beyond CAPTCHA recognition to assess generalizability
2. Evaluate performance with diverse user populations and varying baseline trust levels
3. Assess the reliance model's generalization capability to new users with no training history