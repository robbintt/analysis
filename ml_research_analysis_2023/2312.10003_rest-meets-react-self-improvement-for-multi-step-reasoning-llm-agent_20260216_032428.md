---
ver: rpa2
title: 'ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent'
arxiv_id: '2312.10003'
source_url: https://arxiv.org/abs/2312.10003
tags:
- answer
- agent
- link
- search
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a ReAct-style LLM agent
---

# ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent

## Quick Facts
- arXiv ID: 2312.10003
- Source URL: https://arxiv.org/abs/2312.10003
- Reference count: 19
- This work introduces a ReAct-style LLM agent with self-improvement capability through iterative fine-tuning on its own reasoning trajectories

## Executive Summary
This paper presents a multi-step reasoning LLM agent that can self-improve by learning from its own reasoning traces. The agent uses a ReAct-style architecture to reason and act upon external knowledge through web search, and employs a ReST-like iterative training method that refines the agent using its own trajectories as training data. The approach achieves state-of-the-art performance on challenging reasoning benchmarks while demonstrating successful knowledge distillation to smaller models.

## Method Summary
The method combines a ReAct-style reasoning agent with iterative self-improvement through synthetic data generation. The agent performs multi-step reasoning using Python code-formatted prompts, interacts with external search APIs, and generates reasoning trajectories. These trajectories are converted into training data through LLM-based ranking and fine-tuning, creating a self-reinforcing improvement cycle. The approach also enables knowledge distillation from larger models to smaller ones while maintaining performance.

## Key Results
- The self-improving agent achieves state-of-the-art performance on challenging multi-step reasoning benchmarks
- Auto-eval model shows 0.98 Pearson correlation with human evaluations for reliable performance measurement
- Small models fine-tuned on synthetic data perform comparably to larger prompted models
- Multiple trajectories per question (up to 2x) improve data quality and performance

## Why This Works (Mechanism)

### Mechanism 1
Self-improvement works because the agent can learn from its own reasoning traces without requiring external supervision. The ReST-like iterative training uses the agent's own trajectories as training data, with AI feedback (LLM-based ranking) to filter and select high-quality examples. Core assumption: The agent's self-generated data contains useful patterns that can be extracted through ranking-based selection.

### Mechanism 2
Code-as-prompt approach improves agent performance by providing structured, parseable communication. Using Python code as prompts gives the LLM clear structure while maintaining natural language flexibility through comments and descriptive naming. Core assumption: LLMs can effectively read and write code while maintaining reasoning capabilities.

### Mechanism 3
Auto-eval alignment with human ratings enables reliable performance measurement without expensive human evaluations. An instruction-tuned PaLM 2-L model is used to automatically evaluate agent outputs, with Pearson correlation of 0.98 with human ratings. Core assumption: The auto-eval model can accurately judge the quality of long-form answers and handle stochasticity in agent trajectories.

## Foundational Learning

- Concept: Multi-step reasoning with external tool integration
  - Why needed here: The agent must coordinate multiple reasoning steps (search decisions, result summarization, answer generation, self-checks) while interacting with external search APIs
  - Quick check question: How does the agent decide when to terminate the search loop and proceed to answer generation?

- Concept: Reinforcement learning with non-differentiable rewards
  - Why needed here: Since interactions with external knowledge are non-differentiable, traditional gradient-based training cannot be used directly
  - Quick check question: What alternative to gradient descent does the ReST-like method use to improve the agent?

- Concept: Self-distillation and knowledge transfer
  - Why needed here: The synthetic data generated by the large model is used to train smaller models, achieving comparable performance with fewer parameters
  - Quick check question: How does the performance of the fine-tuned small model compare to the prompted large model?

## Architecture Onboarding

- Component map: Agent flow → Search API → LLM reasoning steps → Auto-eval → Fine-tuning → Self-distillation
- Critical path: Question → Decision step → Search → Result summarization → Answer generation → Self-checks → Final answer
- Design tradeoffs: Structured code prompts vs natural language prompts, multiple trajectories per question vs single trajectory, human filtering vs AI ranking
- Failure signatures: Poor search queries, empty thoughts, missing information in summaries, failure to address the original question
- First 3 experiments:
  1. Test the agent with a simple answerable question to verify the basic flow works
  2. Run the agent with temperature=0.5 on Bamboogle questions to measure baseline performance
  3. Generate 500 pilot trajectories and fine-tune the XS model to verify the self-improvement pipeline works

## Open Questions the Paper Calls Out

### Open Question 1
What is the saturation point for self-improvement iterations beyond the second iteration? The paper only conducts two iterations of self-improvement and does not explore whether additional iterations would yield further improvements or diminishing returns.

### Open Question 2
How does the quality of the ranking "reward" model affect self-improvement performance? The paper uses an instruction-tuned PaLM 2-L model for ranking but does not systematically evaluate how different ranking models or approaches would impact results.

### Open Question 3
Does the self-improvement algorithm generalize to multiple tool settings or unseen tools? The paper only uses a single web search tool and does not test whether the self-improvement approach transfers to agents using multiple tools or novel tools.

## Limitations

- The self-improvement mechanism's effectiveness depends heavily on initial model quality and may stall with poor-quality trajectories
- The code-as-prompt approach may not generalize well across different model families or domains where code interpretation varies
- The evaluation framework lacks transparency about specific criteria and how auto-eval handles ambiguous cases

## Confidence

- High confidence: The core ReAct-style agent architecture and basic iterative fine-tuning pipeline are well-specified and reproducible
- Medium confidence: The self-improvement mechanism works as described for the specific use case, but generalizability to other domains remains uncertain
- Medium confidence: The auto-eval correlation with human ratings is strong, but the evaluation methodology lacks transparency about edge cases

## Next Checks

1. Test the self-improving agent on datasets outside the training distribution to assess generalizability of learned reasoning patterns
2. Run experiments starting with intentionally weaker initial models to determine minimum performance threshold for successful self-improvement
3. Compare Python code prompt format against natural language prompts across multiple model sizes to quantify the contribution of structured communication approach