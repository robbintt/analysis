---
ver: rpa2
title: Startup success prediction and VC portfolio simulation using CrunchBase data
arxiv_id: '2309.15552'
source_url: https://arxiv.org/abs/2309.15552
tags:
- series
- success
- companies
- longtime
- company
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a novel deep learning model to predict startup
  success, focusing on companies at Series B and C stages, and aiming to forecast
  milestones such as IPO, unicorn status, or M&A. The model integrates funding metrics,
  founder features, and industry categories, and is evaluated using a comprehensive
  backtesting algorithm simulating VC investment processes.
---

# Startup success prediction and VC portfolio simulation using CrunchBase data

## Quick Facts
- arXiv ID: 2309.15552
- Source URL: https://arxiv.org/abs/2309.15552
- Reference count: 22
- 14x capital growth achieved using deep learning model on CrunchBase data

## Executive Summary
This study introduces a novel deep learning model to predict startup success, focusing on companies at Series B and C stages, forecasting milestones such as IPO, unicorn status, or M&A. The model integrates funding metrics, founder features, and industry categories, and is evaluated using a comprehensive backtesting algorithm simulating VC investment processes. On Crunchbase data, the model achieved a 14x capital growth and successfully identified high-potential startups, with an 86% ROC_AUC score. The approach demonstrates the potential of deep learning and diverse feature sets in enhancing predictive accuracy for startup success.

## Method Summary
The study uses CrunchBase daily CSV export (2022-06-14) to train a deep learning model predicting startup success for Series B and C companies. The model incorporates heterogeneous features including founders, investors, funding rounds, and industry categories, with categorical variables encoded using OrdinalEncoder and numerical features normalized. Matrix factorization (NMF) is applied to text tag features. A rolling backtesting approach retrains the model every 3 months using only data available before each training window, simulating realistic VC investment timelines. Portfolio simulation adds the top 3 companies monthly based on predicted scores, with exits triggered by success events or two-year inactivity.

## Key Results
- 86% ROC_AUC score on predicting startup success events
- 14x capital growth achieved through portfolio simulation
- Model successfully identified high-potential startups at Series B stage

## Why This Works (Mechanism)

### Mechanism 1
Backtesting with time-aware training prevents data leakage and simulates real VC fund performance. The model is retrained every 3 months using only companies founded before the current training window and whose success events are known prior to the start of the test window. This ensures predictions are made without future information.

### Mechanism 2
Incorporating diverse feature sets (founders, investors, rounds, categories) improves predictive accuracy. Features are engineered from heterogeneous data types and encoded using OrdinalEncoder for categoricals and normalization for numericals. Matrix factorization (NMF) is used for text tag features.

### Mechanism 3
Using a rolling portfolio simulation with monthly entry and dynamic threshold selection balances capital allocation and risk. Each month the top 3 companies above a threshold score are added to the portfolio; companies are removed on success, expiration, or two-year inactivity.

## Foundational Learning

- Concept: Time-series cross-validation
  - Why needed here: Ensures model evaluation respects temporal order and prevents leakage from future events.
  - Quick check question: What happens if you include future data in training when evaluating a time-series model?

- Concept: Feature engineering from multiple data sources
  - Why needed here: Diverse features (founders, investors, rounds, categories) provide richer signal for predicting startup success.
  - Quick check question: How does encoding categorical variables with OrdinalEncoder affect model performance compared to one-hot encoding?

- Concept: Portfolio simulation and backtesting
  - Why needed here: Allows realistic evaluation of investment strategy performance over time, not just static model accuracy.
  - Quick check question: Why is it important to simulate entry and exit decisions in a backtest rather than just predicting outcomes?

## Architecture Onboarding

- Component map: Data preprocessing pipeline → Feature encoding (OrdinalEncoder + normalization + NMF) → Deep learning model (architecture in Figure 1) → Backtest loop (train/test windows every 3 months) → Portfolio simulation (monthly entries/exits) → Capital growth calculation.
- Critical path: Clean and encode features → Train model on past data → Predict on current window → Select companies for portfolio → Update portfolio and calculate returns.
- Design tradeoffs: Retraining every 3 months vs. every month (accuracy vs. computational cost); strict time cutoffs vs. flexibility (leakage prevention vs. data availability).
- Failure signatures: Overfitting to early data (poor generalization later); incorrect time cutoffs (data leakage); threshold mis-calibration (missed or false opportunities); poor feature encoding (signal loss).
- First 3 experiments:
  1. Run backtest with dummy model (random predictions) to confirm pipeline logic and identify time leakage bugs.
  2. Vary retraining frequency (1 vs. 3 months) and measure impact on portfolio returns and computational cost.
  3. Test different portfolio entry thresholds and sizes to optimize capital growth while controlling risk.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do macroeconomic factors and technological trends influence startup success and investment returns?
- Basis in paper: [inferred] The paper mentions analyzing macroeconomic elements and technological trajectories as a potential future research direction.
- Why unresolved: The current model does not incorporate macroeconomic or technological trend data, focusing instead on company-specific features.
- What evidence would resolve it: Incorporating macroeconomic indicators and trend data into the model and evaluating its impact on predictive accuracy and investment performance.

### Open Question 2
- Question: What is the optimal threshold for adding companies to the portfolio based on predicted scores?
- Basis in paper: [explicit] The paper mentions that the choice of the optimal threshold for adding companies to the portfolio is an empirical task and requires careful consideration.
- Why unresolved: The current threshold is set empirically, and the paper suggests conducting experiments to determine the optimal value.
- What evidence would resolve it: Running experiments with different threshold values and evaluating their impact on portfolio performance metrics such as capital growth and success rate.

### Open Question 3
- Question: How can the inclusion of unstructured data sources, such as social media activity and company websites, enhance the predictive model's performance?
- Basis in paper: [explicit] The paper suggests using different sources of text data about companies, founders, and investors as a promising direction for further research.
- Why unresolved: The current model relies on structured data from Crunchbase, and the potential benefits of incorporating unstructured data are not yet quantified.
- What evidence would resolve it: Integrating unstructured data sources into the model and comparing its performance against the baseline model using structured data only.

## Limitations

- Model architecture specifics are not disclosed, limiting exact replication
- Performance validated only on CrunchBase data, limiting generalizability
- Success heavily dependent on accurate timestamp data and strict temporal separation

## Confidence

- 14x capital growth: Medium confidence (depends on accurate time cutoffs and data quality)
- 86% ROC_AUC score: Medium confidence (architecture unspecified, potential overfitting concerns)
- Backtesting methodology: Medium confidence (conceptually sound but requires strict data integrity)

## Next Checks

1. Implement a dummy/random prediction baseline to verify the backtesting pipeline correctly simulates VC investment processes and doesn't introduce data leakage through improper time cutoffs.
2. Test model sensitivity to threshold parameter selection by varying entry criteria and measuring impact on both capital growth and portfolio turnover rates.
3. Conduct cross-dataset validation by testing the trained model on a different startup database (e.g., PitchBook) to assess generalizability beyond CrunchBase.