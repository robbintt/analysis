---
ver: rpa2
title: 'Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution
  and Decision-Making'
arxiv_id: '2310.13610'
source_url: https://arxiv.org/abs/2310.13610
tags:
- rationale
- arxiv
- sadm
- framework
- fid-ex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the issue of unreliable links between generated
  rationales and model decisions in existing NLP frameworks. The proposed Self-Attribution
  and Decision-Making (SADM) framework adopts a two-stage approach: first, the model
  generates a rationale, then it makes a decision based on the generated rationale.'
---

# Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making

## Quick Facts
- arXiv ID: 2310.13610
- Source URL: https://arxiv.org/abs/2310.13610
- Authors: 
- Reference count: 13
- Primary result: SADM framework achieves competitive results in task performance and rationale quality while establishing a more reliable link between generated rationales and model decisions

## Executive Summary
This paper addresses the issue of unreliable links between generated rationales and model decisions in existing NLP frameworks. The proposed Self-Attribution and Decision-Making (SADM) framework adopts a two-stage approach: first, the model generates a rationale, then it makes a decision based on the generated rationale. SADM incorporates the Fusion-In-Decoder (FID) architecture, Sentence Mark (SM) strategy, and Reasoning Augment Learning (RAL) strategy to enhance performance. Experimental results on five reasoning datasets from the ERASER benchmark demonstrate that SADM establishes a more reliable link between the generated rationale and model decision, achieving competitive results in task performance and rationale quality. The RSQ metric, introduced to measure this link, shows significant improvements with SADM.

## Method Summary
SADM is a two-stage framework built on T5-base that separates rationale generation from decision-making. In the first stage, the model generates a rationale using a self-attribution template, and in the second stage, it makes a decision based on the generated rationale using a decision-making template. The framework employs Fusion-In-Decoder architecture to handle long passages, Sentence Mark strategy to constrain rationales to sentence-level selections, and Reasoning Augment Learning strategy for multi-granularity training. SADM is trained jointly on both rationale generation and decision-making objectives using teacher-forcing, and evaluated on five ERASER benchmark datasets using metrics including accuracy, IOU F1, TF1, R-Acc, and the newly introduced RSQ metric.

## Key Results
- SADM achieves competitive performance on five ERASER benchmark datasets (FEVER, MultiRC, BoolQ, Evidence Inference, Movie Reviews)
- The framework significantly improves the RSQ metric, indicating a more reliable link between rationales and decisions
- SADM performs well in semi-supervised scenarios and demonstrates robustness across different threshold values in the RSQ metric
- Ablation studies show that both the SM and RAL strategies contribute to improved performance, with RCP metric decreasing by 1.3 points when RAL is removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage SADM framework creates a more reliable link between rationales and decisions by separating rationale generation from decision-making.
- Mechanism: The framework first generates a rationale using a self-attribution template, then uses that rationale to make a decision with a decision-making template. This mimics human reasoning where we first gather relevant information before making judgments.
- Core assumption: Separating rationale generation from decision-making reduces the chance of models making decisions based on spurious correlations rather than the actual rationale.
- Evidence anchors:
  - [abstract] "Although existing frameworks excel in generating high-quality rationales while achieving high task performance, they neglect to account for the unreliable link between the generated rationale and model decision."
  - [section] "Drawing inspiration from this cognitive thinking, we propose our SADM framework, as depicted in Fig. 2. Firstly, we employ the Trationale template to prompt the model to generate a task-related rationale, the process called self-attribution."
  - [corpus] Weak - no direct corpus evidence supporting this mechanism specifically.
- Break condition: If the rationale generation stage fails to capture relevant information, the subsequent decision-making stage will be compromised regardless of the separation.

### Mechanism 2
- Claim: The Sentence Mark (SM) strategy prevents the model from generating random and irrelevant rationales.
- Mechanism: By adding index numbers before each sentence in the passage, the model learns to select sentences as rationales rather than generating arbitrary natural language. During training, the model outputs sentence indexes instead of free text.
- Core assumption: Constraining the output format to sentence indexes during training forces the model to focus on selecting relevant sentences rather than inventing content.
- Evidence anchors:
  - [section] "we adopt the sentence mark strategy (Lakhotia et al., 2020), which involves adding an index number before each sentence in the passage p."
  - [section] "when optimizing the objective Orationale during the training process, we need to take the sentence indexes of human-annotated rationales as the learning objective instead of the rationale in natural language form."
  - [corpus] Moderate - the FID-Ex framework used in the paper also employs this strategy, suggesting some validation.
- Break condition: If the passage structure is complex or sentences are very long, sentence-level selection might be too coarse and miss important details.

### Mechanism 3
- Claim: The Reasoning Augment Learning (RAL) strategy enhances the model's reasoning ability by exposing it to information at different levels of granularity.
- Mechanism: During training, the model is prompted to make decisions using three different inputs: (1) just the question and passage, (2) question, passage, and annotated rationale, and (3) question and annotated rationale only. This multi-granularity exposure improves comprehension.
- Core assumption: Models benefit from learning to make decisions with varying levels of information, similar to how humans can reason with both detailed and summary information.
- Evidence anchors:
  - [section] "we add two new formats of training samples. We respectively provide the model with Tanswer, q, and p as input, as well as Tanswer, q, r, and p as input."
  - [section] "Our experimental results show that when the RAL strategy is removed, the RCP metric decreases by an average of 1.3 points across the five datasets."
  - [corpus] Moderate - the paper provides quantitative evidence of the RAL strategy's effectiveness through ablation studies.
- Break condition: If the model overfits to one level of granularity during training, it may not generalize well to unseen examples requiring different reasoning depths.

## Foundational Learning

- Concept: Sequence-to-sequence (seq2seq) models
  - Why needed here: SADM is built on T5-base, a seq2seq model, and uses its conditional generation capabilities for both rationale generation and decision-making.
  - Quick check question: How does a seq2seq model differ from a discriminative classifier in handling the dual task of generating rationales and making decisions?

- Concept: Encoder-decoder architecture with Fusion-in-Decoder (FID)
  - Why needed here: FID allows the model to handle long passages by encoding each passage segment separately with the question, then decoding with all encoded representations, addressing input length limitations.
  - Quick check question: What problem does the FID architecture solve when processing lengthy medical passages in the Evidence Inference dataset?

- Concept: Prompt engineering with discrete templates
  - Why needed here: SADM uses natural language prompts (Trationale and Tanswer) to guide the model's behavior without introducing additional parameters, making it more efficient than fine-tuning.
  - Quick check question: Why might discrete prompt templates be preferable to continuous prompt tuning in a two-stage reasoning framework?

## Architecture Onboarding

- Component map: T5-base model -> Trationale template -> Sentence Mark strategy -> Fusion-In-Decoder architecture -> Tanswer template -> Reasoning Augment Learning strategy

- Critical path:
  1. Input passage is segmented (if using FID)
  2. Trationale template is prepended to question and passage
  3. Model generates rationale (sentence indexes if using SM)
  4. Tanswer template is prepended to question and generated rationale
  5. Model makes final decision

- Design tradeoffs:
  - Two-stage vs. parallel: Two-stage ensures rationales are generated before decisions but may be slower; parallel is faster but can create unreliable links.
  - Sentence-level vs. token-level rationales: Sentence-level is more interpretable and easier to constrain with SM but may miss nuanced information.
  - With or without passage in decision stage: Including passage improves accuracy but may weaken the rationale-decision link.

- Failure signatures:
  - Low RSQ scores indicate rationales and decisions are not reliably linked
  - Poor IOU F1/TF1 scores suggest rationale quality issues
  - Low R-Acc scores indicate rationales don't support correct decisions
  - If SADM performs worse than FID-Ex on RSQ, the two-stage approach isn't working as intended

- First 3 experiments:
  1. Compare SADM with and without the SM strategy on FEVER to verify its impact on rationale quality and RSQ
  2. Test SADM with different thresholds (0.5, 0.7, 0.9) in the RSQ metric to understand robustness
  3. Run ablation study removing RAL strategy to quantify its contribution to reasoning performance (RCP metric)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the SADM framework be extended to unsupervised or self-supervised learning scenarios where annotated rationales are unavailable?
- Basis in paper: [explicit] The paper notes that SADM performs well in full-supervised and semi-supervised scenarios but states future work will explore unsupervised applications.
- Why unresolved: The framework currently relies on annotated rationales during training, and the paper does not propose methods for adapting SADM to scenarios without such annotations.
- What evidence would resolve it: Experiments demonstrating SADM's performance on unsupervised datasets, or a proposed method for self-supervised rationale generation within the SADM framework.

### Open Question 2
- Question: What is the impact of using discrete prompt templates versus continuous prompt tuning methods in SADM's performance?
- Basis in paper: [explicit] The paper uses discrete prompt templates for SADM but acknowledges they do not introduce additional parameters or training costs, suggesting potential exploration of alternatives.
- Why unresolved: The paper does not compare discrete prompts to continuous prompt tuning methods, leaving the optimal prompting strategy for SADM unclear.
- What evidence would resolve it: Comparative experiments between discrete prompt templates and continuous prompt tuning methods within the SADM framework, measuring performance across multiple datasets.

### Open Question 3
- Question: How does the RSQ metric behave when different thresholds are applied, and what is the optimal threshold for balancing rationale quality and decision reliability?
- Basis in paper: [explicit] The paper introduces the RSQ metric and tests different threshold values (0.6, 0.7, 0.8, 0.9, 1.0) to show SADM's advantage, but does not determine an optimal threshold.
- Why unresolved: While the paper demonstrates SADM's robustness across thresholds, it does not analyze which threshold best balances rationale quality and decision reliability.
- What evidence would resolve it: A detailed analysis of RSQ performance across datasets at different thresholds, identifying the threshold that maximizes both rationale quality and decision reliability.

## Limitations

- The exact prompt templates (Trationale and Tanswer) are not fully specified, making exact replication difficult
- The threshold of 0.5 for determining correct rationales in the RSQ metric is not validated across datasets
- Sentence-level rationale constraint through the SM strategy may be too coarse-grained for complex passages with lengthy sentences

## Confidence

**High Confidence:** The experimental results demonstrating SADM's competitive performance across five ERASER benchmark datasets are well-supported with quantitative metrics including accuracy, IOU F1, TF1, R-Acc, and RSQ scores. The ablation studies showing the contributions of SM and RAL strategies provide strong evidence for their effectiveness.

**Medium Confidence:** The claim that SADM establishes a "more reliable link" between rationales and decisions is supported by RSQ metric improvements, but the threshold-based nature of RSQ (0.5) introduces some uncertainty about whether this threshold is optimal or dataset-specific.

**Low Confidence:** The generalizability of the two-stage framework beyond the five tested datasets remains uncertain. The paper does not address how SADM would perform on datasets with different characteristics or discuss computational efficiency compared to single-stage approaches.

## Next Checks

1. **Threshold Sensitivity Analysis:** Conduct experiments varying the RSQ threshold (0.3, 0.5, 0.7, 0.9) across all five datasets to determine if 0.5 is universally optimal or if dataset-specific thresholds yield better reliability measurements.

2. **Cross-Dataset Generalization Test:** Apply SADM to at least two additional text classification datasets outside the ERASER benchmark (such as MultiNLI or SciTail) to evaluate whether the two-stage framework maintains its performance advantages in different reasoning contexts.

3. **Efficiency Benchmarking:** Measure and compare the computational time and resource requirements of SADM versus single-stage FID-Ex approach on identical hardware to quantify the practical cost of improved rationale-decision reliability.