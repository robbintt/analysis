---
ver: rpa2
title: Replay-enhanced Continual Reinforcement Learning
arxiv_id: '2311.11557'
source_url: https://arxiv.org/abs/2311.11557
tags:
- learning
- task
- tasks
- continual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REPLAY-ENHANCED CONTINUAL REINFORCEMENT LEARNING (RECALL) addresses
  plasticity limitation in replay-based continual RL caused by reward scale differences
  across tasks. The method introduces adaptive normalization on approximate targets
  to balance task contributions during updates and policy distillation to prevent
  forgetting from offline training.
---

# Replay-enhanced Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.11557
- Source URL: https://arxiv.org/abs/2311.11557
- Authors: 
- Reference count: 40
- Primary result: RECALL achieves 0.91 average success rate on second tasks vs 0.44 for naive replay

## Executive Summary
REPLAY-ENHANCED CONTINUAL REINFORCEMENT LEARNING (RECALL) addresses the plasticity limitation in replay-based continual reinforcement learning caused by reward scale differences across tasks. The method introduces adaptive normalization on approximate targets to balance task contributions during updates and policy distillation to prevent forgetting from offline training. On the Continual World benchmark, RECALL significantly outperforms naive perfect memory replay, achieving average success rates of 0.91 vs 0.44 on second tasks, while matching or exceeding state-of-the-art methods across multiple metrics.

## Method Summary
RECALL is a replay-based continual reinforcement learning algorithm that combines multi-head neural network training with adaptive normalization (PopArt) and policy distillation. The method uses a multi-head architecture where each task has its own output head for both policy and Q-value networks. During training, RECALL applies PopArt normalization to Q-value targets using task-specific scale and shift parameters, ensuring balanced learning across tasks with different reward scales. Policy distillation is implemented using KL divergence regularization between current and historical policies, preventing forgetting when learning from replayed experiences. The algorithm maintains a replay buffer containing experiences from all tasks and uses a 50-50 mixture of new and replayed experiences during training.

## Key Results
- RECALL achieves 0.91 average success rate on second tasks compared to 0.44 for naive perfect memory replay
- Matches or exceeds state-of-the-art methods on Continual World benchmark across average performance, forgetting, and forward transfer metrics
- Demonstrates superior scalability on longer task sequences (CW20) compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive normalization on approximate targets balances task contributions during updates
- Mechanism: PopArt normalization rescales Q-value targets using task-specific scale (σ) and shift (μ) parameters, making rewards from different tasks contribute equally to learning updates
- Core assumption: Reward scale differences across tasks create imbalance in learning dynamics that can be corrected through affine transformation
- Evidence anchors:
  - [abstract]: "adaptive normalization on approximate targets to balance task contributions during updates"
  - [section 4]: "we employ PopArt normalization...to facilitate learning on new tasks"
  - [corpus]: Weak evidence - no corpus papers specifically address PopArt in continual RL context
- Break condition: If reward scales are already normalized across tasks or if tasks have identical reward structures

### Mechanism 2
- Claim: Policy distillation on old tasks prevents forgetting from offline training
- Mechanism: KL divergence regularization between current and historical policies constrains updates to preserve performance on replayed tasks
- Core assumption: Offline learning on replayed experiences creates distributional shift that can be mitigated by constraining policy updates
- Evidence anchors:
  - [abstract]: "policy distillation to prevent forgetting from offline training"
  - [section 4]: "we employ the distillation technique to the policies for old tasks to prevent forgetting"
  - [corpus]: Weak evidence - no corpus papers specifically address policy distillation in continual RL context
- Break condition: If replay buffer is sufficiently large to capture all necessary state-action distributions or if online interaction with old tasks is available

### Mechanism 3
- Claim: Combining adaptive normalization with policy distillation enables simultaneous plasticity and stability
- Mechanism: The two mechanisms address orthogonal failure modes - normalization prevents overemphasis on high-reward tasks while distillation prevents forgetting from offline updates
- Core assumption: Plasticity-stability dilemma can be partially decoupled when failure modes have distinct root causes
- Evidence anchors:
  - [section 4]: "RECALL employs multi-head neural network training...We propose to utilize adaptive normalization...together with the distillation technique"
  - [section 5.2]: "the aforementioned perception is fundamentally based on the premise that the capacity of the neural system is fully and well exploited"
  - [corpus]: Moderate evidence - related papers discuss plasticity-stability trade-offs but not this specific decoupling
- Break condition: If network capacity becomes saturated or if task similarity makes the two failure modes inseparable

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (MDP framework, Q-learning, policy optimization)
  - Why needed here: RECALL builds on SAC algorithm and modifies its value function and policy updates
  - Quick check question: How does the soft Bellman residual differ from standard Q-learning updates?

- Concept: Continual learning concepts (catastrophic forgetting, plasticity-stability trade-off)
  - Why needed here: The paper explicitly addresses catastrophic forgetting and the plasticity-stability dilemma
  - Quick check question: What distinguishes catastrophic forgetting from general performance degradation?

- Concept: Experience replay and multi-task learning
  - Why needed here: RECALL uses experience replay for both current and past tasks with multi-head architecture
  - Quick check question: How does multi-head architecture enable task-specific updates while sharing representations?

## Architecture Onboarding

- Component map: Multi-head SAC with PopArt normalization and policy distillation
  - Actor network: Multi-head policy network with distillation loss on old tasks
  - Critic network: Multi-head Q-network with PopArt normalization and task-specific output heads
  - Replay buffer: Combined current and historical experiences with task identifiers
  - Target networks: Normalized Q-targets with adaptive scale and shift parameters

- Critical path: Experience collection → Batch sampling → Value function update with PopArt → Policy update with distillation → Parameter synchronization
- Design tradeoffs: Multi-head vs shared networks, normalization frequency vs stability, distillation strength vs plasticity
- Failure signatures: Poor plasticity indicates inadequate normalization; forgetting indicates insufficient distillation; instability indicates incorrect PopArt parameter updates
- First 3 experiments:
  1. Implement basic SAC with multi-head architecture on single task to verify baseline
  2. Add PopArt normalization and test on two tasks with different reward scales
  3. Add policy distillation and test on three tasks to verify forgetting prevention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RECALL's performance scale with increasingly diverse reward distributions across tasks?
- Basis in paper: [explicit] The paper identifies that reward scale differences across tasks cause plasticity limitations in replay-based continual RL
- Why unresolved: The paper only tested RECALL on the Continual World benchmark which has relatively similar reward scales across tasks
- What evidence would resolve it: Experiments on benchmark suites with tasks having highly varied reward scales (e.g., multiple orders of magnitude differences)

### Open Question 2
- Question: What is the theoretical guarantee that adaptive normalization prevents catastrophic forgetting?
- Basis in paper: [inferred] The paper proposes adaptive normalization on approximate targets but doesn't provide theoretical analysis of its effectiveness
- Why unresolved: The paper demonstrates empirical effectiveness but lacks theoretical foundation for why this approach prevents forgetting
- What evidence would resolve it: Mathematical proof showing that adaptive normalization preserves value function estimates across tasks with different reward scales

### Open Question 3
- Question: How sensitive is RECALL's performance to the policy distillation regularization coefficient λ?
- Basis in paper: [explicit] The paper mentions λ was searched in {0.01, 0.1, 1, 10, 100} and final value is 10
- Why unresolved: The paper only provides one optimal value and doesn't analyze sensitivity or optimal range
- What evidence would resolve it: Systematic analysis showing performance across the full range of λ values and identifying optimal ranges for different task distributions

## Limitations
- Implementation details for PopArt normalization schedule and optimal loss component weighting are not fully specified
- Best-return exploration strategy lacks implementation specifics that could impact performance
- Only single data points provided for scalability claims on CW20 without statistical significance testing

## Confidence
- High: Core mechanism claims regarding adaptive normalization improving plasticity on tasks with different reward scales
- Medium: Policy distillation claims showing strong performance on forgetting and forward transfer metrics
- Low: Scalability claims on CW20 due to limited data points and lack of statistical analysis

## Next Checks
1. Implement ablation studies removing either PopArt normalization or policy distillation to quantify individual contributions
2. Test RECALL on continuous task sequences beyond CW20 to validate scalability claims
3. Measure individual task performance rather than just average metrics to understand failure modes