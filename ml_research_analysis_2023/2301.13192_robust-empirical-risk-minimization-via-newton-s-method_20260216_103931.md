---
ver: rpa2
title: Robust empirical risk minimization via Newton's method
arxiv_id: '2301.13192'
source_url: https://arxiv.org/abs/2301.13192
tags:
- robust
- gradient
- algorithm
- newton
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a robust variant of Newton's method for empirical
  risk minimization, where robust estimators from existing literature on multivariate
  mean estimation replace the gradient and Hessian at each iteration. The algorithm
  employs Huber's $\epsilon$-contamination model or heavy-tailed distributions, and
  uses backtracking linesearch for stepsize selection.
---

# Robust empirical risk minimization via Newton's method

## Quick Facts
- arXiv ID: 2301.13192
- Source URL: https://arxiv.org/abs/2301.13192
- Reference count: 30
- The paper introduces a robust variant of Newton's method for empirical risk minimization, achieving quadratic convergence in contaminated data settings.

## Executive Summary
This paper presents a robust Newton's method for empirical risk minimization that replaces standard gradient and Hessian estimates with robust counterparts. The algorithm uses Huber's ε-contamination model or heavy-tailed distributions, employing backtracking linesearch for adaptive stepsize selection. Under appropriate conditions, the method achieves quadratic convergence to a small ball around the population minimizer, with convergence rates faster than robust gradient descent. The approach is demonstrated on generalized linear models and extended to high-dimensional settings using a conjugate gradient variant.

## Method Summary
The method implements a robust variant of Newton's method where gradient and Hessian estimates at each iteration are replaced with robust estimators from the multivariate mean estimation literature. For Huber contamination, it uses the AgnosticMean algorithm, while for heavy-tailed data, it employs median-of-means estimators. The algorithm includes backtracking linesearch for adaptive stepsize selection without prior tuning. A high-dimensional variant approximates Hessian-vector products using finite differences of robust gradients, solved via conjugate gradient method. The approach maintains the Newton update structure while providing resistance to outliers and achieving faster convergence than robust gradient descent.

## Key Results
- Achieves quadratic convergence to a small ball around the population minimizer under appropriate conditions
- Outperforms robust gradient descent in convergence rates
- Demonstrates improved performance compared to existing robust methods through simulations
- Extends to high-dimensional settings via conjugate gradient-based Newton direction estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing gradients and Hessians with robust estimators enables quadratic convergence in contaminated data settings.
- Mechanism: The algorithm maintains the Newton update structure but substitutes robust mean estimators (Huber or median-of-means) for gradient and Hessian, preserving local curvature information while resisting outliers.
- Core assumption: The robust estimators satisfy concentration bounds that keep estimation error small relative to true gradient/Hessian norms.
- Evidence anchors:
  - [abstract] "The algorithm employs Huber's $\epsilon$-contamination model or heavy-tailed distributions, and uses backtracking linesearch for stepsize selection."
  - [section 3.1] "We assume that f(θ) := R(θ) is twice-differentiable and satisfies the Lipschitz condition... we also assume that f satisfies the strong convexity and smoothness conditions"
  - [corpus] Weak - no direct evidence in neighbors about Newton-based robust methods
- Break condition: When contamination fraction exceeds the breakdown point of the robust estimator or when robust estimation error dominates the gradient norm.

### Mechanism 2
- Claim: Backtracking linesearch with robust estimates enables adaptive stepsize selection without prior tuning.
- Mechanism: The exit condition compares robust loss estimates at candidate and current iterates, automatically adjusting stepsize based on local curvature and contamination level.
- Core assumption: Robust loss estimates concentrate around true loss values with sufficient accuracy.
- Evidence anchors:
  - [abstract] "with a stepsize that may be chosen adaptively via backtracking linesearch"
  - [section 3.1] "we prove that the rate of convergence of this algorithm is faster than that of robust gradient descent, and successive iterates converge quadratically to a small ball around θ*"
  - [corpus] Weak - no direct evidence in neighbors about robust backtracking
- Break condition: When robust loss estimates become too noisy to distinguish between iterates, or when gradient/Hessian estimates are too inaccurate.

### Mechanism 3
- Claim: Conjugate gradient-based Newton direction estimation extends the method to high-dimensional settings.
- Mechanism: Instead of computing full Hessian matrix, approximate Hessian-vector products are estimated using finite differences of robust gradients, enabling iterative solution of Newton system.
- Core assumption: Finite difference approximation error is small enough relative to gradient estimation error.
- Evidence anchors:
  - [section 6] "the idea is to estimate ∇2f(θ)v, for any vector v, using the approximation hv(θ) = ∇f(θ+δv) − ∇f(θ)/δ"
  - [section 6] "we conjecture that the robust conjugate gradient method would allow us to incur an overall estimation error of O(ϵ1/4)"
  - [corpus] Weak - no direct evidence in neighbors about CG-based robust methods
- Break condition: When dimension is too high for finite difference approximation to maintain sufficient accuracy, or when conjugate gradient iterations accumulate too much error.

## Foundational Learning

- Concept: Huber contamination model
  - Why needed here: Provides the statistical framework for understanding how outliers affect estimation and motivates robust methods
  - Quick check question: What's the key difference between Huber contamination and adversarial contamination models?

- Concept: Strong convexity and smoothness conditions
  - Why needed here: Ensure that Newton's method has quadratic convergence properties and that the true parameter is unique
  - Quick check question: How do strong convexity and smoothness conditions relate to the step size behavior in Newton's method?

- Concept: Concentration of measure for robust estimators
  - Why needed here: Justifies why robust estimators can replace classical ones in the algorithm while maintaining theoretical guarantees
  - Quick check question: What role does the bounded fourth moment assumption play in the analysis of robust estimators?

## Architecture Onboarding

- Component map:
  - Robust gradient estimator (Huber/median-of-means) -> Robust Hessian estimator (vectorized robust mean) -> Newton direction -> Backtracking linesearch -> Update iterate

- Critical path: Robust gradient → Robust Hessian → Newton direction → Backtracking linesearch → Update iterate

- Design tradeoffs:
  - Vectorized Hessian vs. conjugate gradient: memory vs. computation
  - Huber vs. median-of-means: contamination model assumptions
  - Fixed vs. adaptive stepsize: simplicity vs. robustness
  - Exact vs. approximate Newton direction: convergence rate vs. scalability

- Failure signatures:
  - Divergence: robust estimation error too large relative to true gradient
  - Slow convergence: contamination fraction near breakdown point
  - Memory issues: Hessian computation in high dimensions
  - Numerical instability: poor conditioning of robust Hessian estimates

- First 3 experiments:
  1. Linear regression with synthetic Huber contamination at varying epsilon levels
  2. Logistic regression with label flipping contamination
  3. High-dimensional setting using conjugate gradient variant with varying dimension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the estimation error upper bound of O(p√ϵ log p) for robust Newton's method in the Huber contamination model tight, or can better bounds be achieved with different robust matrix estimators?
- Basis in paper: [explicit] The paper discusses that "obtaining optimal rates for robust matrix estimation under a variety of metrics and contamination settings is an active area of research" and suggests that "a different robust matrix estimator would lead to better overall error bounds for our robust Newton algorithm."
- Why unresolved: Different robust matrix estimation techniques exist in the literature, but their impact on the overall convergence of robust Newton's method has not been rigorously analyzed.
- What evidence would resolve it: A theoretical analysis comparing the convergence rates of robust Newton's method using different robust matrix estimators under Huber contamination, along with supporting simulation results.

### Open Question 2
- Question: Can the robust conjugate gradient method achieve the conjectured O(ϵ^(1/4)) estimation error bound in the Huber contamination model with rigorous proof?
- Basis in paper: [explicit] The paper states "we conjecture that the robust conjugate gradient method would allow us to incur an overall estimation error of O(ϵ^(1/4)) in the case of Huber's ϵ-contamination model" and acknowledges that "a rigorous analysis of the robust conjugate gradient method is beyond the current scope of this paper."
- Why unresolved: The propagation of errors through iterations of the conjugate gradient method when applied to solve linear systems with inexact matrix-vector pairs is an open question in optimization.
- What evidence would resolve it: A rigorous mathematical proof establishing the convergence rate of the robust conjugate gradient method with error propagation analysis, supported by numerical experiments.

### Open Question 3
- Question: How does the performance of robust Newton's method compare to direct robust plug-in estimators for linear regression in terms of estimation error and computational efficiency?
- Basis in paper: [explicit] The paper notes that "a much more direct way to obtain a robust estimator for linear regression would be to directly robustify the estimator" and mentions that such an approach "would also give an error of O(√ϵ) in the robust case, but a direct analysis would provide an error bound which depends on ∥θ∗∥2."
- Why unresolved: While direct robust plug-in estimators for linear regression are computationally simpler, their statistical properties and dependence on the true parameter norm have not been fully characterized in comparison to iterative robust Newton's method.
- What evidence would resolve it: A comparative theoretical and empirical analysis of the estimation errors and computational complexities of robust Newton's method versus direct robust plug-in estimators for linear regression under Huber contamination.

## Limitations
- The high-dimensional conjugate gradient variant lacks rigorous theoretical guarantees and remains conjectural
- The algorithm relies heavily on assumptions about contamination models and concentration properties that may not hold in practice
- Fixed assumptions of strong convexity and smoothness may not apply to complex loss landscapes
- Computational overhead of robust estimation at each iteration could be prohibitive for very large datasets

## Confidence

- Theoretical convergence guarantees in classical settings: High
- Empirical performance claims: Medium
- High-dimensional conjugate gradient variant: Low (conjectural)
- Robustness to various contamination models: Medium

## Next Checks

1. Test algorithm breakdown points by systematically increasing contamination fraction until convergence fails
2. Benchmark computational overhead of robust estimation versus standard Newton method across dataset sizes
3. Verify that strong convexity/smoothness assumptions are necessary by testing on non-convex robust learning problems