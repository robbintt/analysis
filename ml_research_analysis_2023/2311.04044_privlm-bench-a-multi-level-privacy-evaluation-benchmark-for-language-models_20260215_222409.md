---
ver: rpa2
title: 'PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models'
arxiv_id: '2311.04044'
source_url: https://arxiv.org/abs/2311.04044
tags:
- privacy
- data
- tuning
- attacks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PrivLM-Bench, a multi-perspective privacy
  evaluation benchmark for language models. It addresses the challenge of fairly comparing
  privacy-preserving language models (PPLMs) that use differential privacy (DP) by
  moving beyond just reporting DP parameters.
---

# PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models

## Quick Facts
- arXiv ID: 2311.04044
- Source URL: https://arxiv.org/abs/2311.04044
- Authors: Xinyu Hu, Junqi Dai, Hanting Chen, Jianyuan Zhong, Yulong Wang, Shuhui Qu, Yunhe Wang
- Reference count: 35
- Primary result: Introduces a benchmark showing DP fine-tuning protects against attacks on training data but not inference data, with existing attacks less effective than expected

## Executive Summary
PrivLM-Bench addresses the challenge of fairly comparing privacy-preserving language models (PPLMs) that use differential privacy (DP) by moving beyond just reporting DP parameters. The benchmark defines multi-faceted privacy objectives, constructs a unified pipeline for private fine-tuning, and uses empirical privacy attacks (data extraction, membership inference, and embedding-level attacks) to evaluate privacy leakage. The core finding is that while DP tuning effectively protects against attacks targeting fine-tuning data privacy, it does not protect inference data privacy, and existing privacy attacks are less effective than anticipated.

## Method Summary
The benchmark evaluates language models across multiple privacy dimensions using a standardized pipeline. Pre-trained models (BERT, RoBERTa, GPT-2, T5, FLAN-T5) are fine-tuned on GLUE tasks using four methods (full, prompt, prefix, infilling) with and without DP. DP training uses gradient clipping (norm ≤ 0.1), batch size 1024, 5 epochs, and fixed learning rates per method. Three attack types are implemented: data extraction using canary patterns and exposure metrics, membership inference using shadow models and likelihood ratios, and embedding inversion using GPT-2 attacker decoders with micro-F1 evaluation. The benchmark compares utility (accuracy) against privacy (attack success) across model architectures and tuning methods.

## Key Results
- DP fine-tuning effectively protects against membership inference attacks on fine-tuning data (AUC reduction)
- Naive DP fine-tuning does not protect inference data privacy (similar EIA success rates with/without DP)
- Existing privacy attacks are less effective than anticipated, indicating a gap between assumed and actual attacker capabilities
- Different model architectures and tuning methods yield varying utility-privacy tradeoffs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP fine-tuning protects against membership inference attacks on fine-tuning data
- Mechanism: DP bounds worst-case privacy loss by injecting noise calibrated to the gradient norm and privacy budget, preventing the adversary from distinguishing whether a sample was in the training set
- Core assumption: The adversary is bounded by the DP guarantee and cannot mount more powerful attacks than those considered in the theoretical framework
- Evidence anchors:
  - [abstract] "DP tuning is effective against existing privacy attacks targeted at fine-tuning data privacy"
  - [section] "DP-tuned LMs conform to DP's definition and offer strong fine-tuning data protection against MIAs"
  - [corpus] "Average neighbor FMR=0.371, average citations=0.0" - weak corpus support
- Break condition: If an attacker can circumvent the DP bound (e.g., via side-channel attacks or adaptive composition), the protection fails

### Mechanism 2
- Claim: Different model architectures and tuning algorithms yield different utility-privacy tradeoffs
- Mechanism: The model's capacity, pretraining task, and tuning method affect how much noise is needed for a given privacy guarantee and how much utility is lost
- Core assumption: The privacy-utility tradeoff is architecture- and method-dependent
- Evidence anchors:
  - [section] "LMs' utility is affected by multiple factors, including model architectures, pre-trained weights, model sizes, tuning algorithms and DP constraints"
  - [corpus] "Average neighbor FMR=0.371, average citations=0.0" - weak corpus support
- Break condition: If a new architecture or tuning method changes the relationship between privacy budget and utility loss

### Mechanism 3
- Claim: Naive DP fine-tuning does not protect inference data privacy
- Mechanism: DP noise is added only during training/fine-tuning and is independent of the inference data distribution, so the model can still leak information about the inference data
- Core assumption: The DP guarantee is specific to the training data and does not extend to arbitrary inference data
- Evidence anchors:
  - [abstract] "naive DP tuning cannot defend against inference-stage privacy attacks"
  - [section] "DP tuning results in similar inference data leakage on EIAs compared with non-DP settings"
  - [corpus] "Average neighbor FMR=0.371, average citations=0.0" - weak corpus support
- Break condition: If the inference data is similar to the training data or if additional privacy mechanisms are applied during inference

## Foundational Learning

- Concept: Differential privacy (DP)
  - Why needed here: DP is the theoretical foundation for privacy guarantees in the benchmark
  - Quick check question: What is the definition of (ε, δ)-differential privacy and what do ε and δ represent?

- Concept: Privacy attacks (MIA, EIA, DEA)
  - Why needed here: The benchmark evaluates privacy by measuring vulnerability to these attacks
  - Quick check question: How do membership inference attacks (MIAs) differ from embedding inversion attacks (EIAs) in terms of what they target and how they work?

- Concept: Language model pretraining and fine-tuning
  - Why needed here: The benchmark assumes a pretrain-then-fine-tune paradigm common in NLP
  - Quick check question: What is the difference between full fine-tuning, prompt tuning, and prefix tuning, and how might each affect privacy-utility tradeoffs?

## Architecture Onboarding

- Component map: Privacy objectives → Unified fine-tuning pipeline → Privacy attacks → Evaluation metrics
- Critical path: Define privacy objectives → Implement unified fine-tuning pipeline → Implement privacy attacks → Evaluate privacy leakage
- Design tradeoffs: Granularity of privacy objectives vs. complexity of evaluation; comprehensiveness of attacks vs. computational cost; generality of benchmark vs. specificity to certain model types
- Failure signatures: Attacks succeeding on DP models (DP guarantee broken); inconsistent results across similar models (implementation error); high variance in results (insufficient evaluation data)
- First 3 experiments:
  1. Compare MIA success on BERT models with and without DP fine-tuning to verify DP protection
  2. Compare EIA success on RoBERTa models with and without DP fine-tuning to verify lack of inference data protection
  3. Vary the privacy budget (ε) and measure impact on utility (accuracy) and privacy (attack success) to understand the tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of privacy attacks (DEAs, MIAs, EIAs) compare in terms of effectiveness when applied to language models with and without differential privacy?
- Basis in paper: [explicit] The paper evaluates the effectiveness of various privacy attacks on language models with and without differential privacy, showing varying levels of success across different attack types and model settings.
- Why unresolved: While the paper provides empirical results showing the effectiveness of these attacks, it does not deeply analyze the underlying reasons for the differences in attack success rates across different privacy-preserving methods or model architectures.
- What evidence would resolve it: Detailed comparative studies analyzing the mechanisms of each attack type and their interaction with specific model architectures and privacy-preserving techniques would provide insights into why certain attacks are more effective than others.

### Open Question 2
- Question: What are the specific factors that contribute to the utility degradation observed in language models when implementing differential privacy?
- Basis in paper: [explicit] The paper notes that differential privacy tuning leads to non-negligible utility degradation across different models and tuning methods, but it does not identify the specific factors contributing to this degradation.
- Why unresolved: The paper identifies the presence of utility degradation but does not delve into the mechanisms or factors causing it, such as the impact of noise injection levels, the choice of optimizer, or the architecture of the model itself.
- What evidence would resolve it: Empirical studies isolating and testing the impact of different factors (e.g., noise levels, optimizer choice, model architecture) on utility degradation in differentially private models would clarify the primary contributors to performance loss.

### Open Question 3
- Question: How does the choice of model architecture (e.g., BERT, RoBERTa, GPT-2, T5) influence the effectiveness of differential privacy in protecting against privacy attacks?
- Basis in paper: [explicit] The paper evaluates various model architectures under differential privacy and observes differences in their resilience to privacy attacks, suggesting that architecture plays a role in privacy protection.
- Why unresolved: While the paper demonstrates that different architectures respond differently to privacy attacks, it does not explore the underlying reasons for these differences or how architectural features contribute to privacy resilience.
- What evidence would resolve it: Comparative analyses of model architectures focusing on their structural features and how these features interact with privacy-preserving mechanisms would elucidate why some architectures are more effective at resisting privacy attacks.

## Limitations
- The benchmark focuses only on English GLUE tasks, limiting generalizability to other languages and domains
- Fixed privacy budget (ε=8, δ=1e-5) represents only one point in the privacy-utility tradeoff space
- May reflect limitations in attack implementations rather than fundamental privacy guarantees

## Confidence
- High confidence: DP fine-tuning effectively protects against membership inference attacks on fine-tuning data
- Medium confidence: Naive DP fine-tuning does not protect inference data privacy
- Low confidence: The gap between attackers' actual capabilities and defenses' assumed attacker abilities is significant

## Next Checks
1. Expand attack diversity by implementing and evaluating additional privacy attacks beyond the three included, particularly adaptive attacks that can exploit specific characteristics of DP-trained models
2. Vary privacy budgets systematically across a wider range (ε from 1 to 16) to characterize the full privacy-utility tradeoff curve and identify potential phase transitions
3. Apply the benchmark to non-GLUE tasks and non-English datasets to test whether observed privacy-utility relationships hold across different data distributions, vocabulary sizes, and task complexities