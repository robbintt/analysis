---
ver: rpa2
title: Exploring Semi-supervised Hierarchical Stacked Encoder for Legal Judgement
  Prediction
arxiv_id: '2311.08103'
source_url: https://arxiv.org/abs/2311.08103
tags:
- legal
- document
- transformer
- encoder
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of predicting legal case judgments
  from lengthy, unstructured documents with minimal annotations. The authors propose
  a two-level classification framework that combines supervised fine-tuning of domain-specific
  BERT models with unsupervised clustering of sentence embeddings.
---

# Exploring Semi-supervised Hierarchical Stacked Encoder for Legal Judgement Prediction

## Quick Facts
- arXiv ID: 2311.08103
- Source URL: https://arxiv.org/abs/2311.08103
- Reference count: 7
- Primary result: 85.01% accuracy using InLegalBERT with transformer encoders and clustering

## Executive Summary
This work addresses the challenge of predicting legal case judgments from lengthy, unstructured documents with minimal annotations. The authors propose a two-level classification framework combining supervised fine-tuning of domain-specific BERT models with unsupervised clustering of sentence embeddings. By processing documents through InLegalBERT to extract [CLS] embeddings and applying transformer encoder layers for document-level representation, the model achieves state-of-the-art performance on the ILDC dataset. The results demonstrate the value of domain-specific pre-training and semi-supervised learning for legal text classification.

## Method Summary
The proposed method involves chunking legal documents into sequential word sets, tokenizing with [CLS] and [SEP] tokens, and fine-tuning domain-specific BERT models (InLegalBERT) to extract [CLS] embeddings. Transformer encoder layers are then applied to these embeddings to capture document-level representations through inter-chunk attention. HDBSCAN clustering is performed on the chunk embeddings to provide additional features, with optional dimensionality reduction using pUMAP before clustering. The final classification combines both supervised BERT features and unsupervised clustering features through dense layers.

## Key Results
- Achieves 85.01% accuracy using InLegalBERT + pUMAP + HDBSCAN + BiLSTM on ILDC dataset
- Outperforms baseline BERT models and other domain-specific approaches
- Demonstrates importance of domain-specific pre-training for legal text classification
- Shows effectiveness of combining supervised and unsupervised learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training improves legal text understanding by capturing specialized vocabulary and syntax.
- Mechanism: InLegalBERT is pre-trained on Indian legal documents, allowing it to better represent legal terminology and sentence structures compared to general BERT.
- Core assumption: Legal text contains domain-specific language patterns that general pre-training cannot adequately capture.
- Evidence anchors: [abstract] "Our experimental results also show the importance of domain-specific pre-training of Transformer Encoders in legal information processing." [section] "We experiment with two domain-specific pre-trained BERT models (LEGAL-BERT[1] and InLegalBERT[5]) with the hypothesis that domain pre-training of a transformer model is necessary for the in-domain vocabulary and lexical understanding [6]."

### Mechanism 2
- Claim: Hierarchical processing with transformer encoders captures document-level dependencies better than flat approaches.
- Mechanism: The model processes document chunks through BERT to get [CLS] embeddings, then applies transformer encoder layers to enable inter-chunk attention, creating a document-level representation.
- Core assumption: Legal document understanding requires modeling relationships between different parts of the document, not just individual sentences.
- Evidence anchors: [abstract] "We explore and propose a two-level classification mechanism; both supervised and unsupervised; by using domain-specific pre-trained BERT to extract information from long documents in terms of sentence embeddings further processing with transformer encoder layer" [section] "In step II, We use transformer layers on the extracted [CLS] embeddings for the inter-chunk attention to learn the whole document representation."

### Mechanism 3
- Claim: Unsupervised clustering provides additional features that help the model identify topic-related chunks across documents.
- Mechanism: HDBSCAN clustering is applied to [CLS] embeddings to group similar chunks, and these cluster labels are used as extra features during supervised training.
- Core assumption: Legal documents contain recurring topics or themes that can be identified through clustering, and this information is useful for prediction.
- Evidence anchors: [abstract] "use unsupervised clustering to extract hidden labels from these embeddings to better predict a judgment of a legal case" [section] "The [CLS] embeddings are also used for the unsupervised learning mechanism i.e. clustering the individual chunks which are used as extra information while training."

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The model relies on transformer encoders for both the BERT fine-tuning and the hierarchical processing steps.
  - Quick check question: How does multi-head attention in transformers help capture different types of relationships between document chunks?

- Concept: Domain adaptation in NLP
  - Why needed here: Understanding why domain-specific pre-training (InLegalBERT) is necessary for legal text classification.
  - Quick check question: What specific linguistic features of legal text might be better captured by in-domain pre-training versus general pre-training?

- Concept: Unsupervised clustering techniques
  - Why needed here: HDBSCAN is used to create additional features from chunk embeddings, requiring understanding of how density-based clustering works.
  - Quick check question: How does HDBSCAN differ from K-means clustering, and why might it be more suitable for legal document chunks?

## Architecture Onboarding

- Component map: Document → Chunking → BERT tokenization ([CLS] + text + [SEP]) → BERT fine-tuning → Extract [CLS] embeddings → HDBSCAN clustering → Transformer encoder layers → Dense layers → Prediction

- Critical path: Chunking → BERT fine-tuning → [CLS] extraction → Transformer encoder → Classification
  The clustering step is additive but not on the critical path for basic functionality.

- Design tradeoffs:
  - Chunk size vs. context: Larger chunks preserve more context but may exceed model limits
  - Clustering parameters: Minimum cluster size affects feature quality
  - Encoder depth: More layers may capture better document representations but increase training time

- Failure signatures:
  - Poor performance with both validation and test sets suggests issues with BERT fine-tuning or chunking strategy
  - Large gap between validation and test performance suggests overfitting or clustering domain mismatch
  - Instability during training suggests learning rate or architecture issues

- First 3 experiments:
  1. Baseline: Fine-tune InLegalBERT on chunked documents without hierarchical processing or clustering
  2. Add transformer encoder layers without clustering to test document-level attention benefits
  3. Add HDBSCAN clustering to baseline to isolate clustering contribution

These experiments isolate each architectural component to understand its individual contribution to performance improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the clustering algorithm's performance vary when trained on the entire dataset including test data versus just training and validation data?
- Basis in paper: [explicit] The paper mentions that clustering was only trained on the train and validation set, not the test set, which affected the clusters on new data points (test set).
- Why unresolved: The authors note that this approach affects performance on the test set, but they don't explore how performance would change if clustering were trained on all available data.
- What evidence would resolve it: Comparing classification accuracy, precision, and recall when using clustering trained on different subsets of the data (train+validation vs. full dataset).

### Open Question 2
- Question: What is the optimal chunk size for document processing that balances computational efficiency with classification accuracy?
- Basis in paper: [inferred] The paper mentions dividing documents into chunks but doesn't explore how different chunk sizes affect performance or efficiency.
- Why unresolved: The current approach uses sequential sets of words without specifying an optimal chunk size, which could impact both performance and computational requirements.
- What evidence would resolve it: Systematic experiments varying chunk sizes while measuring both classification metrics and computational resources required.

### Open Question 3
- Question: How does the semi-supervised approach perform on other legal domains or languages beyond Indian legal documents?
- Basis in paper: [inferred] The experiments are limited to the Indian Legal Document Corpus, but the methodology could potentially be applied to other legal domains.
- Why unresolved: The paper demonstrates effectiveness on one dataset but doesn't explore generalization to other legal domains or languages.
- What evidence would resolve it: Testing the same methodology on legal datasets from different countries, legal systems, or languages while comparing performance.

## Limitations
- Limited dataset specificity: Evaluated only on Indian Supreme Court documents, may not generalize to other legal systems
- Chunking strategy ambiguity: Exact chunking parameters not specified, significantly impacts performance
- Clustering stability concerns: Doesn't validate whether clusters remain stable across train/validation/test splits

## Confidence

**High confidence**: Domain-specific pre-training (InLegalBERT) improves legal text classification performance over general BERT models, supported by ablation results showing consistent improvements.

**Medium confidence**: Hierarchical transformer encoder layers improve document-level representation compared to flat approaches, though the exact contribution relative to other components remains partially unclear.

**Medium confidence**: Unsupervised clustering provides useful additional features for legal judgment prediction, though the paper doesn't thoroughly validate cluster interpretability or stability.

## Next Checks
1. **Cross-dataset validation**: Evaluate the InLegalBERT + transformer encoder approach on legal datasets from different jurisdictions (e.g., European Court of Human Rights cases) to test domain generalization.

2. **Chunking sensitivity analysis**: Systematically vary chunk sizes (50, 100, 200 tokens) and measure impact on both individual chunk classification and document-level aggregation performance.

3. **Clustering stability test**: Apply HDBSCAN with identical parameters to both train and test sets, then measure cluster overlap and consistency to validate whether unsupervised features transfer meaningfully across datasets.