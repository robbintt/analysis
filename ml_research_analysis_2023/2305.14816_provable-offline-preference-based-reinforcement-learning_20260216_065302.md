---
ver: rpa2
title: Provable Offline Preference-Based Reinforcement Learning
arxiv_id: '2305.14816'
source_url: https://arxiv.org/abs/2305.14816
tags:
- where
- policy
- have
- learning
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes an offline RL algorithm for preference-based
  RL (PbRL) with human feedback. The algorithm consists of two steps: (1) estimate
  the implicit reward using Maximum Likelihood Estimation (MLE) with general function
  approximation from offline data, and (2) solve a distributionally robust planning
  problem over a confidence set around the MLE.'
---

# Provable Offline Preference-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2305.14816
- Source URL: https://arxiv.org/abs/2305.14816
- Reference count: 40
- This paper proposes an offline RL algorithm for preference-based RL (PbRL) with human feedback that can learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data.

## Executive Summary
This paper presents FREEHAND, a provably sample-efficient algorithm for offline preference-based reinforcement learning (PbRL) with human feedback. The algorithm addresses the challenge of learning from preference data over trajectory pairs without requiring full coverage of the state-action space. By combining Maximum Likelihood Estimation (MLE) with distributionally robust planning, FREEHAND can learn target policies under partial coverage conditions, extending previous work that required full coverage. The theoretical analysis establishes sample complexity bounds that scale with a novel concentrability coefficient measuring how well the offline data covers the target policy's trajectory distribution.

## Method Summary
FREEHAND is a two-step offline RL algorithm for preference-based RL. First, it estimates the implicit reward function from preference feedback over trajectory pairs using MLE with general function approximation, constructing a confidence set around the estimate. Second, it solves a distributionally robust planning problem that optimizes the policy under the worst-case reward within this confidence set. This pessimistic approach ensures the learned policy performs well even with reward estimation uncertainty. The algorithm extends to settings with unknown transition dynamics and action-based preferences, incorporating a soft margin assumption to enable learning without pessimism in the action-based case.

## Key Results
- Establishes first sample complexity bounds for offline PbRL under partial coverage using a novel concentrability coefficient
- Proves that FREEHAND can learn any target policy with polynomial samples as long as the target policy is covered by the offline data
- Shows that under a soft margin assumption, the algorithm can learn optimal policies without pessimism when preferences are over actions
- Demonstrates that the per-trajectory concentrability coefficient is necessary in the preference-based setting, unlike standard RL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves policy learning under partial coverage by combining MLE-based reward estimation with distributionally robust planning.
- Mechanism: The MLE step constructs a confidence set for the reward function using log-likelihood with a slackness parameter ζ, ensuring the true reward r⋆ is contained with high probability. The distributionally robust planning step then optimizes the policy under the worst-case reward in this confidence set, which penalizes policies not well-covered by the offline data. This allows learning any target policy as long as it is covered by the offline data.
- Core assumption: The offline dataset provides sufficient preference feedback to distinguish between reward functions within the general function class Gr, and the true reward r⋆ is in Gr (realizability).
- Break condition: If the function class Gr is too large or complex (e.g., very high bracket number), the confidence set becomes too wide, weakening the pessimistic planning effect and potentially allowing learning of poorly covered policies.

### Mechanism 2
- Claim: The newly defined concentrability coefficient Cr(Gr, πtar, µref) enables sample complexity analysis under partial coverage for preference-based RL.
- Mechanism: Cr(Gr, πtar, µref) measures the discrepancy between the target policy's trajectory distribution and the offline data distribution, scaled by the reward estimation error. By incorporating this coefficient into the sample complexity bound, the algorithm's performance guarantee scales with how well the offline data covers the target policy rather than requiring full coverage of the entire state-action space.
- Core assumption: The preference feedback model accurately reflects the underlying reward differences, and the function class Gr can approximate the true reward well enough to bound the estimation error.
- Break condition: If the target policy πtar has a trajectory distribution that is too different from the offline data distribution µref, Cr becomes large, leading to worse sample complexity or failure to learn the policy.

### Mechanism 3
- Claim: Extending to action-based comparison with a soft margin allows learning under partial coverage without requiring pessimism.
- Mechanism: When preferences are over actions rather than trajectories, the algorithm estimates the advantage function Ah using MLE. Under a soft margin assumption, which bounds the probability of small differences in Q-values, the algorithm can select actions greedily based on the estimated advantage function. The concentrability coefficient Cact measures coverage of the optimal policy's state distribution and action space, allowing sample complexity guarantees without pessimism.
- Core assumption: The optimal advantage function A⋆h is in the function class GAh, and the soft margin assumption holds, ensuring that Q-value gaps are not too small with high probability.
- Break condition: If the soft margin assumption fails (e.g., the optimal Q-function has very small gaps), the algorithm may fail to distinguish between optimal and suboptimal actions, leading to poor policy learning.

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE) with general function approximation
  - Why needed here: MLE is used to estimate the reward function from preference feedback over trajectory pairs, which is the core of the first step in the algorithm. General function approximation allows handling complex reward structures beyond linear models.
  - Quick check question: What is the role of the slackness parameter ζ in the MLE step, and how does it affect the confidence set construction?

- Concept: Distributionally robust optimization
  - Why needed here: This technique is used in the second step to find a policy that performs well under the worst-case reward within the confidence set, ensuring robustness to reward estimation uncertainty and allowing learning under partial coverage.
  - Quick check question: How does the choice of the reference distribution µref influence the learned policy, and why is it generally recommended to set it to µ1?

- Concept: Concentrability coefficients for measuring coverage
  - Why needed here: Concentrability coefficients quantify how well the offline data covers the target policy's trajectory distribution (Cr) or the optimal policy's state-action distribution (Cact), enabling sample complexity analysis under partial coverage.
  - Quick check question: What is the difference between the per-trajectory concentrability coefficient Ctr and the per-step concentrability coefficient Cst, and why does the former arise in the preference-based RL setting?

## Architecture Onboarding

- Component map: Preference data → MLE estimation → Confidence set construction → Distributionally robust planning → Learned policy
- Critical path: Preference data → MLE estimation → Confidence set construction → Distributionally robust planning → Learned policy
- Design tradeoffs:
  - Function class complexity vs. sample complexity: More complex function classes (higher bracket numbers) allow better reward approximation but require more samples and may lead to wider confidence sets.
  - Slackness parameter ζ vs. coverage guarantee: Larger ζ leads to wider confidence sets, potentially including more reward functions but weakening the pessimistic planning effect.
  - Reference distribution choice vs. performance guarantee: Choosing µref = µ1 ensures the learned policy is at least as good as the offline data, while choosing µref = dπtar can lead to better performance if πtar is well-covered.
- Failure signatures:
  - Poor reward estimation: High estimation error leads to wide confidence sets, reducing the effectiveness of pessimistic planning.
  - Insufficient coverage: High concentrability coefficients indicate poor coverage of the target or optimal policy, leading to high sample complexity or failure to learn.
  - Soft margin violation: Small Q-value gaps under the action-based setting can cause the algorithm to fail in distinguishing optimal actions.
- First 3 experiments:
  1. Validate MLE estimation: Generate synthetic preference data from a known reward function, run the MLE step, and check if the estimated reward is close to the true reward.
  2. Test confidence set coverage: Verify that the true reward r⋆ is contained in the confidence set R(D) with high probability for different dataset sizes.
  3. Evaluate policy learning under partial coverage: Compare the learned policy's performance against a target policy in a simulated environment with known dynamics, varying the coverage of the target policy in the offline data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the concentration coefficient C_r(G_r, π_tar, μ_ref) be further reduced for linear function approximation classes?
- Basis in paper: The paper discusses the concentration coefficient and its dependence on the function class G_r, particularly in the context of linear models.
- Why unresolved: The paper provides bounds and comparisons but does not explicitly explore whether tighter bounds can be achieved for linear models specifically.
- What evidence would resolve it: Empirical or theoretical results showing improved concentration coefficients for linear function classes under specific conditions.

### Open Question 2
- Question: How does the sample complexity scale with the horizon H in the action-based comparison setting under the soft margin assumption?
- Basis in paper: The paper mentions that under the soft margin assumption, the sample complexity can be faster than O(1/ε^2) when a hard margin is imposed, but it does not provide explicit scaling with H.
- Why unresolved: The paper provides a sample complexity bound but does not explicitly analyze how it scales with the horizon H in the action-based setting.
- What evidence would resolve it: A detailed analysis or experiments showing the scaling behavior of sample complexity with respect to H in the action-based comparison setting.

### Open Question 3
- Question: What are the practical implications of using the per-trajectory concentration coefficient versus the per-step concentration coefficient in offline RLHF?
- Basis in paper: The paper discusses the necessity of using the per-trajectory concentration coefficient in the trajectory-based comparison setting and provides lower bounds showing its importance.
- Why unresolved: The paper provides theoretical justification but does not explore the practical differences in performance or sample efficiency between using per-trajectory and per-step coefficients.
- What evidence would resolve it: Comparative studies or experiments demonstrating the impact of using per-trajectory versus per-step concentration coefficients in real-world RLHF tasks.

## Limitations
- Requires realizability assumption (r* ∈ Gr), which may not hold in practice and can significantly impact performance
- Concentrability coefficient can become large when the target policy is poorly covered by the offline data, leading to worse sample complexity or failure
- Soft margin assumption for action-based preference learning may not hold in many practical scenarios where optimal Q-values have small gaps

## Confidence
- High confidence: The overall framework combining MLE with distributionally robust planning is sound and well-supported by the theoretical analysis
- Medium confidence: The sample complexity bounds and their dependence on concentrability coefficients are mathematically rigorous but rely on strong assumptions
- Medium confidence: The extension to action-based preferences and unknown transitions follows logically from the main results but adds additional assumptions

## Next Checks
1. Implement the FREEHAND algorithm on benchmark RL environments with simulated preference feedback to verify the theoretical sample complexity bounds and policy learning performance under partial coverage
2. Evaluate the algorithm's performance when the realizability assumption is violated or when the preference feedback contains noise, to assess its practical applicability
3. Compare FREEHAND with other offline RL methods that handle preference feedback, such as those using pessimism or uncertainty quantification, to understand the relative strengths and weaknesses of the approach