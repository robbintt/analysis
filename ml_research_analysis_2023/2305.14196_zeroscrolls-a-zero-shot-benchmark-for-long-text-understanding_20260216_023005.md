---
ver: rpa2
title: 'ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding'
arxiv_id: '2305.14196'
source_url: https://arxiv.org/abs/2305.14196
tags:
- question
- tasks
- gpt-4
- zeroscrolls
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ZeroSCROLLS introduces a new benchmark for evaluating large language
  models' ability to reason over long texts without any task-specific training. The
  benchmark extends SCROLLS with four new datasets, including novel aggregation tasks
  like determining the percentage of positive hotel reviews and reordering book chapter
  summaries.
---

# ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding

## Quick Facts
- arXiv ID: 2305.14196
- Source URL: https://arxiv.org/abs/2305.14196
- Reference count: 10
- Key outcome: ZeroSCROLLS introduces a new benchmark for evaluating large language models' ability to reason over long texts without any task-specific training

## Executive Summary
ZeroSCROLLS is a new benchmark designed to evaluate large language models' ability to understand and reason over long texts using only zero-shot prompting. The benchmark extends the SCROLLS benchmark with four new datasets and introduces novel aggregation tasks like determining percentages of positive reviews and reordering book chapter summaries. The evaluation includes both open-source models and closed products like ChatGPT and GPT-4, using only test sets without training or development data.

## Method Summary
ZeroSCROLLS provides a comprehensive framework for evaluating long text understanding through zero-shot prompting. The benchmark includes 10 datasets spanning summarization, question answering, multiple choice, and two novel aggregation tasks. Each task uses specific automatic metrics (ROUGE, F1, accuracy, exponential similarity, concordance index) for evaluation. Models are evaluated using greedy decoding with canonical prompts, and performance is measured as an average score across all tasks. The evaluation pipeline processes model outputs and applies per-task automatic metrics to generate results.

## Key Results
- GPT-4 achieved the highest average score across all ZeroSCROLLS tasks
- All models struggled significantly with the new aggregation tasks (SpaceDigest and BookSumSort)
- Increasing model size and context length generally improved performance across tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZeroSCROLLS evaluates long text understanding without requiring task-specific training data
- Mechanism: The benchmark provides only test sets with canonical prompts, forcing models to rely on zero-shot reasoning capabilities rather than fine-tuning
- Core assumption: Large language models can generalize to novel long text tasks through prompt engineering alone
- Evidence anchors:
  - "which contains only test sets, without training or development data"
  - "ZeroSCROLLS is designed to test zero-shot capabilities, and contains test sets with simple natural prompts and private gold references, without train or validation sets"

### Mechanism 2
- Claim: Automatic evaluation metrics can accurately score diverse long text tasks without human intervention
- Mechanism: Each task uses task-specific automatic metrics (ROUGE, F1, exponential similarity, concordance index) that capture task-relevant performance
- Core assumption: These metrics reliably distinguish correct from incorrect outputs across varied long text tasks
- Evidence anchors:
  - "ZeroSCROLLS evaluation is fully automatic. Given a model's response to every test instance, we apply per-task automatic evaluation metrics"
  - "For our newly proposed tasks (SpaceDigest and BookSumSort), we use two new automatic metrics"

### Mechanism 3
- Claim: Long context models show consistent improvement with increased parameters and context length
- Mechanism: The benchmark systematically tests models across different sizes and context lengths to identify scaling relationships
- Core assumption: Performance scales predictably with model size and input length for long text tasks
- Evidence anchors:
  - "As expected, increasing model size drives performance upwards across almost all tasks"
  - "In general, increasing the number of tokens helps the models perform the tasks better"

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: Models must understand tasks from instructions alone without training examples
  - Quick check question: Can you design a prompt that makes a model perform a new task it hasn't seen before?

- Concept: Automatic text evaluation metrics
  - Why needed here: Human evaluation is too slow/expensive for large-scale benchmark testing
  - Quick check question: How would you evaluate a summary without human judgment?

- Concept: Long sequence processing
  - Why needed here: Tasks require reasoning over thousands of words, not just short passages
  - Quick check question: What architectural changes enable models to handle 8k+ token inputs?

## Architecture Onboarding

- Component map: Benchmark framework (datasets + prompts + metrics) → Evaluation pipeline (model inference → metric computation → leaderboard) → Analysis tools (format checking, human evaluation)
- Critical path: Data preparation → Prompt design → Model evaluation → Results aggregation → Analysis
- Design tradeoffs: Automatic metrics vs. human evaluation accuracy; simple prompts vs. task-specific instructions; comprehensive task coverage vs. benchmark size
- Failure signatures: Low scores across all tasks (model limitation); inconsistent format compliance (prompt issues); metric-score mismatch (evaluation problems)
- First 3 experiments:
  1. Run a small subset of tasks with multiple models to verify evaluation pipeline works
  2. Test different prompt variations on one task to optimize zero-shot performance
  3. Compare automatic metric scores against human judgments on sampled outputs to validate evaluation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different decoding strategies (e.g., beam search, sampling) impact the performance of LLMs on ZeroSCROLLS tasks?
- Basis in paper: The paper mentions using greedy decoding for all models but suggests that future work could explore other decoding strategies.
- Why unresolved: The paper does not provide any experimental results or analysis comparing different decoding strategies.
- What evidence would resolve it: Conducting experiments with various decoding strategies (e.g., beam search, sampling) and comparing their performance on ZeroSCROLLS tasks would provide insights into their impact.

### Open Question 2
- Question: How do model-specific prompts and chain-of-thought prompting affect LLM performance on ZeroSCROLLS?
- Basis in paper: The paper uses common prompt templates across models but acknowledges that model-specific prompts and chain-of-thought prompting may improve performance.
- Why unresolved: The paper does not explore the effects of model-specific prompts or chain-of-thought prompting on ZeroSCROLLS performance.
- What evidence would resolve it: Experimenting with model-specific prompts and chain-of-thought prompting for each task and comparing their performance to the baseline would reveal their impact on ZeroSCROLLS.

### Open Question 3
- Question: What is the impact of increasing the context length beyond the current limits of evaluated models on ZeroSCROLLS performance?
- Basis in paper: The paper discusses the effect of increasing input length for some models but does not explore the impact of extending context lengths beyond the evaluated limits.
- Why unresolved: The paper only evaluates models with specific context length limits and does not investigate the effects of longer contexts.
- What evidence would resolve it: Evaluating models with extended context lengths on ZeroSCROLLS tasks and comparing their performance to the current results would provide insights into the impact of longer contexts.

## Limitations
- The benchmark focuses exclusively on English-language datasets, limiting generalizability to multilingual contexts
- Automatic metrics may not accurately reflect human judgment quality, particularly for novel aggregation tasks
- Zero-shot evaluation approach may not fully isolate true zero-shot reasoning from implicit web training

## Confidence
- High confidence: GPT-4 achieved highest average score across tasks
- Medium confidence: All models struggled with aggregation tasks
- Low confidence: Scaling relationships between model size, context length, and performance

## Next Checks
1. Compare automatic metric scores against human judgments on a stratified sample of outputs across all tasks to validate evaluation quality
2. Systematically test different prompt variations on BookSumSort and SpaceDigest tasks to determine if performance limitations stem from task difficulty or suboptimal prompting
3. Evaluate the same models on translated versions of benchmark tasks to assess cross-lingual generalization and identify language-specific biases