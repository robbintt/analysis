---
ver: rpa2
title: Designing Optimal Behavioral Experiments Using Machine Learning
arxiv_id: '2305.07721'
source_url: https://arxiv.org/abs/2305.07721
tags:
- design
- designs
- optimal
- data
- experimental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a machine learning-based approach to Bayesian
  optimal experimental design for simulator models without tractable likelihoods.
  The method combines neural networks with mutual information estimation to optimize
  experimental designs for model discrimination and parameter estimation.
---

# Designing Optimal Behavioral Experiments Using Machine Learning

## Quick Facts
- arXiv ID: 2305.07721
- Source URL: https://arxiv.org/abs/2305.07721
- Reference count: 40
- Primary result: Machine learning approach discovers optimal behavioral experiment designs with extreme reward probabilities that outperform commonly-used designs

## Executive Summary
This work presents a machine learning-based approach to Bayesian optimal experimental design for simulator models without tractable likelihoods. The method combines neural networks with mutual information estimation to optimize experimental designs for model discrimination and parameter estimation. Applied to multi-armed bandit tasks, the approach discovers optimal designs with extreme reward probabilities that outperform commonly-used designs in both simulations and human experiments. The method yields improved model recovery, more informative posterior distributions, and better parameter disentanglement. By-products include automatically-learned sufficient summary statistics and amortized posterior inference.

## Method Summary
The method employs neural networks to estimate mutual information between experimental designs and model parameters, using this as a utility function in Bayesian optimization to search for optimal designs. The neural network architecture uses block-specific sub-networks that learn compressed representations of data from each experimental block, which are then concatenated and fed into a larger network. Bayesian optimization with a Gaussian Process surrogate model navigates the design space based on the estimated mutual information. The trained networks also enable amortized posterior inference for efficient computation of posterior distributions after collecting real data. The approach is validated through simulations and a human participant study, demonstrating superior performance compared to baseline designs.

## Key Results
- Optimal designs with extreme reward probabilities (0.8, 0.8, 0.2, 0.2) discovered for multi-armed bandit tasks
- Superior model recovery rates and posterior entropies compared to baseline designs
- Better parameter disentanglement achieved through the optimal designs
- Human experimental data confirms simulation predictions with improved model recovery

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neural network architecture automatically learns sufficient summary statistics from multi-block behavioral data.
- Mechanism: The architecture uses separate sub-networks for each block, which output compressed representations that are then concatenated and fed into a larger network. This allows the model to extract task-relevant features without manual summary statistic design.
- Core assumption: The learned summary statistics are approximately sufficient for the mutual information estimation task.
- Evidence anchors:
  - [abstract]: "by-products include automatically-learned sufficient summary statistics"
  - [section]: "we propose an architecture devised specifically for application to behavioral experiments... Each sub-network conveniently learns to approximate sufficient summary statistics of the data from each block"
  - [corpus]: Weak - no direct corpus evidence about sufficient statistics in this context
- Break condition: If the learned summary statistics fail to capture all relevant information, the mutual information estimate becomes biased and suboptimal designs may be selected.

### Mechanism 2
- Claim: Bayesian optimization efficiently searches the high-dimensional design space using the estimated mutual information as a utility function.
- Mechanism: The neural network estimates the mutual information for candidate designs, and Bayesian optimization uses these estimates as a noisy objective function to guide the search toward optimal designs.
- Core assumption: The estimated mutual information surface is smooth enough for Bayesian optimization to effectively navigate the design space.
- Evidence anchors:
  - [section]: "we optimize the design d by means of Bayesian optimization (BO)... using a Gaussian Process (GP) as our probabilistic surrogate model"
  - [abstract]: "the approach discovers optimal designs with extreme reward probabilities that outperform commonly-used designs"
  - [corpus]: Weak - no direct corpus evidence about Bayesian optimization performance in this specific application
- Break condition: If the mutual information surface is too noisy or has many local optima, Bayesian optimization may converge to suboptimal designs.

### Mechanism 3
- Claim: The learned neural network enables amortized posterior inference, allowing efficient computation of posterior distributions after collecting real data.
- Mechanism: The trained neural network approximates the posterior distribution through a simple forward pass, avoiding expensive likelihood-free inference methods.
- Core assumption: The neural network trained during BOED generalizes well to real experimental data.
- Evidence anchors:
  - [abstract]: "by-products of this procedure allow for quick and straightforward evaluation of models and their parameters against real experimental data"
  - [section]: "the trained neural networks can then conveniently be used to compute posterior distributions"
  - [corpus]: Weak - no direct corpus evidence about generalization of BOED-trained networks to real data
- Break condition: If the real data distribution differs significantly from the simulated data used for training, the posterior estimates may be inaccurate.

## Foundational Learning

- Concept: Bayesian optimal experimental design
  - Why needed here: Provides the theoretical framework for optimizing experimental designs based on expected information gain
  - Quick check question: What is the utility function used to measure the quality of experimental designs in this work?

- Concept: Mutual information estimation
  - Why needed here: Serves as the quantitative measure of how informative an experimental design is about model parameters or model choice
  - Quick check question: How does the neural network lower bound relate to the true mutual information?

- Concept: Simulator models and likelihood-free inference
  - Why needed here: The approach applies to models where likelihoods are intractable, requiring simulation-based methods
  - Quick check question: Why is it advantageous that the method doesn't require tractable likelihoods?

## Architecture Onboarding

- Component map: Data simulation -> Neural network training for mutual information estimation -> Bayesian optimization to find optimal designs -> Validation with simulations -> Application to real data with posterior estimation
- Critical path: Data simulation → Neural network training for mutual information estimation → Bayesian optimization to find optimal designs → Validation with simulations → Application to real data with posterior estimation
- Design tradeoffs: The neural network architecture trades computational cost for expressiveness, while Bayesian optimization trades sample efficiency for the ability to handle noisy objectives.
- Failure signatures: Poor model recovery indicates the learned summary statistics are insufficient; convergence to baseline designs suggests the BO optimization is not effective; slow posterior computation indicates the amortized inference is not working properly.
- First 3 experiments:
  1. Test the neural network on synthetic data with known optimal designs to verify it can learn the correct mutual information surface
  2. Run Bayesian optimization on a simple 2D design space to verify it can find known optima
  3. Apply the full pipeline to a small bandit task with a tractable likelihood to compare optimal designs with analytical solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ML-based BOED compare to human-designed experiments across different experimental paradigms beyond multi-armed bandit tasks?
- Basis in paper: [explicit] The authors state that their method yields optimal designs that outperform human-crafted designs found in the literature, offering more informative data for model discrimination and parameter estimation.
- Why unresolved: The study focuses on multi-armed bandit tasks. It's unclear if the same level of improvement would be observed in other experimental paradigms.
- What evidence would resolve it: Conducting similar ML-based BOED experiments across a variety of experimental paradigms and comparing the results to human-designed experiments.

### Open Question 2
- Question: How does the performance of ML-based BOED scale with the complexity of the computational models being tested?
- Basis in paper: [inferred] The authors mention that their approach is applicable to any model from which we can simulate data and scales well to realistic numbers of design variables, experimental trials, and blocks.
- Why unresolved: The study focuses on three computational models of human behavior in multi-armed bandit tasks. It's unclear how the method would perform with more complex models.
- What evidence would resolve it: Testing the ML-based BOED method on increasingly complex computational models and observing the performance.

### Open Question 3
- Question: How robust is the ML-based BOED method to changes in the prior distribution over model parameters?
- Basis in paper: [explicit] The authors mention that the applicability and usefulness of experimental design optimization has been demonstrated in many scientific disciplines, but this has often been restricted to settings with simple and tractable classes of models.
- Why unresolved: The study uses uninformative priors for the model parameters. It's unclear how the method would perform with different prior distributions.
- What evidence would resolve it: Testing the ML-based BOED method with different prior distributions and observing the performance.

## Limitations
- The neural network-based mutual information estimation introduces approximation error that may accumulate through the optimization pipeline
- The computational cost of training neural networks for each new experimental context may limit practical deployment
- The extent to which optimal designs generalize to more complex behavioral paradigms remains unclear

## Confidence
- **High**: The neural network architecture effectively learns sufficient summary statistics (validated through improved model recovery)
- **High**: Bayesian optimization successfully navigates the design space to find superior designs (confirmed by outperforming baseline designs)
- **Medium**: The learned networks generalize to real experimental data (supported by human study but with limited sample size)
- **Medium**: The amortized posterior inference provides accurate estimates (demonstrated in simulations but not extensively validated with real data)

## Next Checks
1. **Cross-paradigm validation**: Test the optimal designs discovered for bandit tasks on completely different behavioral paradigms (e.g., decision-making under risk or social cognition tasks) to assess generalizability.

2. **Sample efficiency analysis**: Systematically vary the number of training simulations used for neural network training to quantify the relationship between computational cost and design quality.

3. **Robustness to model misspecification**: Evaluate how the method performs when the true data-generating process deviates from the assumed simulator models, particularly focusing on model recovery rates and parameter estimation accuracy.