---
ver: rpa2
title: Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in
  Scientific Document Reasoning
arxiv_id: '2311.04348'
source_url: https://arxiv.org/abs/2311.04348
tags:
- scientific
- tasks
- atlas
- document
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates retrieval-augmented large language models
  (LLMs) on scientific document reasoning tasks to understand their effectiveness
  in retrieving relevant evidence. The authors fine-tune multiple variants of the
  ATLAS model with science-focused instructions and evaluate them on the SciRepEval
  benchmark.
---

# Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning

## Quick Facts
- arXiv ID: 2311.04348
- Source URL: https://arxiv.org/abs/2311.04348
- Reference count: 15
- Key outcome: Retrieval-augmented LLMs achieve reasonable accuracy on scientific tasks but justify predictions with fabricated, irrelevant evidence passages.

## Executive Summary
This paper investigates whether retrieval-augmented large language models can effectively retrieve relevant scientific evidence to support their reasoning. The authors evaluate ATLAS and ATLAS-Science models on scientific document classification tasks, finding that while the models achieve reasonable accuracy, they often cite irrelevant or fabricated evidence passages as justification. The study reveals a fundamental tension in RAG systems: high task performance can coexist with poor evidence quality, raising questions about whether these systems genuinely reason with retrieved information or merely use it as post-hoc justification.

## Method Summary
The authors evaluate retrieval-augmented LLMs on scientific document reasoning tasks using the SciRepEval benchmark. They fine-tune multiple variants of the ATLAS model (220M parameters) with science-focused instructions and compare them against a T5 baseline. The models use Contriever-based retrieval over a 354M passage corpus from S2ORC, with evidence selection and answer generation handled by a fusion-in-decoder architecture. Evaluation focuses on both task accuracy (FoS and MAG classification) and evidence quality (relevance and diversity metrics).

## Key Results
- ATLAS models achieve 84.42% accuracy on FOS and 59.10% on MAG tasks, but evidence relevance scores are only 0.06, indicating fabricated evidence
- Scientific corpus pretraining (ATLAS-Science) provides minimal improvement in either task accuracy or evidence quality
- Retrieved evidence passages show extremely low diversity, with top-20 passages often being identical across different queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ATLAS generates plausible answers by fabricating evidence when retrieval fails to provide relevant context.
- Mechanism: The model uses a parametric decoder to generate answers based on the prompt and any retrieved evidence. When retrieved passages are irrelevant, the decoder still produces an answer by relying on its pretrained knowledge, but then cites the irrelevant passages as "evidence" to make the response appear grounded.
- Core assumption: The decoder prioritizes generating a fluent, task-appropriate answer over strict fidelity to retrieved content, and the evidence selection module is decoupled from the answer generation logic.
- Evidence anchors:
  - [abstract] "models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication"
  - [section] "The retrieved passages are not at all related to the corresponding query topics, rendering them useless... The model only achieves 0.06 relevance score suggesting that the passages returned by the model as evidences do not align with the domain of the query."
- Break condition: When retrieval relevance exceeds a threshold (e.g., >0.5) and diversity is high, evidence is no longer fabricated.

### Mechanism 2
- Claim: Retrieval-augmented LLMs can achieve reasonable task accuracy even when evidence is fabricated, because the parametric model knowledge compensates.
- Mechanism: The autoregressive decoder leverages its parametric knowledge base to answer questions accurately, while the retrieval module's poor performance is masked by the model's ability to generate plausible-sounding justifications using any retrieved text.
- Core assumption: The decoder's parametric knowledge is sufficiently rich to answer scientific classification questions without needing accurate retrieved evidence.
- Evidence anchors:
  - [abstract] "models justify predictions in science tasks with fabricated evidence... does not alleviate the risk of evidence fabrication"
  - [section] "we observe that although the ATLAS model has achieved a acceptable accuracy of 84.42% in FOS and 59.10% in MAG, the retrieved evidences are extremely poor in terms of relevance to the query topic."
- Break condition: When task complexity exceeds the parametric model's knowledge or when answers require precise factual recall from specific documents.

### Mechanism 3
- Claim: Pretraining with scientific corpus does not improve evidence quality because the retrieval mechanism, not the parametric model, determines evidence relevance.
- Mechanism: The Contriever-based retriever used in ATLAS-Science is trained to retrieve relevant passages, but its performance depends on the training data and objective, not on the pretraining corpus of the language model. The scientific pretraining improves the decoder's knowledge but not the retriever's ability to find relevant evidence.
- Core assumption: Evidence quality is determined by the retriever's training, not by the language model's pretraining data distribution.
- Evidence anchors:
  - [abstract] "leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication"
  - [section] "In comparison to ATLAS, the accuracy of ATLAS-Science model has a slight improvement in FOS, whereas it depreciates in MAG. Thus we see that scientific corpus does not help much in improving the performance of the model. More importantly, the relevance and diversity of the retrieved passages only slightly improves over ATLAS."
- Break condition: When the retriever is retrained with scientific data or when retrieval quality is explicitly optimized for the scientific domain.

## Foundational Learning

- Concept: Retrieval-augmented generation architecture (RAG)
  - Why needed here: Understanding how retrieval modules integrate with parametric models is crucial for diagnosing why evidence fabrication occurs despite reasonable accuracy.
  - Quick check question: In a RAG system, does the retriever's output directly constrain the decoder's generation, or can the decoder override it?

- Concept: Evidence grounding and faithfulness metrics
  - Why needed here: The paper introduces custom metrics for evidence relevance and diversity, which are essential for evaluating whether retrieved passages actually support model predictions.
  - Quick check question: What's the difference between evidence relevance (matching query domain) and evidence diversity (uniqueness across queries)?

- Concept: Few-shot learning in retrieval-augmented models
  - Why needed here: The ATLAS model's few-shot capability means it can perform well without extensive task-specific training, which may explain why it generates plausible answers even with poor evidence.
  - Quick check question: How does few-shot learning in retrieval-augmented models differ from standard fine-tuning approaches?

## Architecture Onboarding

- Component map: Query → Retriever (Contriever) → Document Index → Retrieved Passages → Fusion-in-Decoder (T5-based) → Answer + Evidence
- Critical path: Query encoding → Document retrieval → Evidence selection → Answer generation
- Design tradeoffs: Retrieval quality vs. inference speed; parametric knowledge vs. non-parametric retrieval; few-shot capability vs. evidence faithfulness
- Failure signatures: High task accuracy but low evidence relevance; identical retrieved passages across different queries; diversity scores near zero
- First 3 experiments:
  1. Test retriever-only performance: measure relevance and diversity of top-5 passages for 100 random queries
  2. Test decoder-only performance: generate answers without retrieval to establish baseline accuracy
  3. Test retrieval quality impact: compare performance with and without retrieval to quantify parametric vs. non-parametric contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does pretraining with scientific corpus not alleviate evidence fabrication in retrieval-augmented LLMs?
- Basis in paper: [explicit] The authors observe that using scientific corpus as pretraining data does not improve the relevance or diversity of retrieved evidence, and the ATLAS-Science model shows only slight improvements over the original ATLAS model.
- Why unresolved: The paper does not provide a detailed analysis of why scientific pretraining fails to improve evidence quality, leaving the underlying mechanism unclear.
- What evidence would resolve it: Further experiments isolating pretraining data effects, or ablation studies on different types of scientific corpora, could clarify whether the issue lies in data quality, model architecture, or training methodology.

### Open Question 2
- Question: What architectural or training modifications could improve the relevance and diversity of retrieved evidence in retrieval-augmented LLMs?
- Basis in paper: [inferred] The authors note that while retrieval-augmentation improves accuracy, the retrieved evidence is largely irrelevant and lacks diversity, suggesting a gap in the current design.
- Why unresolved: The paper does not propose or test alternative architectures or training strategies to address evidence fabrication.
- What evidence would resolve it: Testing alternative retriever designs (e.g., cross-encoder models), fine-tuning strategies, or post-retrieval filtering mechanisms could provide insights into effective solutions.

### Open Question 3
- Question: How does the scale of the retrieval corpus affect the quality of retrieved evidence in retrieval-augmented LLMs?
- Basis in paper: [inferred] The authors use a corpus of 354M text passages but do not analyze how corpus size or composition impacts evidence relevance and diversity.
- Why unresolved: The study does not explore the relationship between corpus scale and retrieval quality, leaving this as an open empirical question.
- What evidence would resolve it: Experiments varying corpus size, domain coverage, or passage granularity could reveal the optimal scale for improving evidence quality.

### Open Question 4
- Question: Can post-retrieval filtering or re-ranking improve the relevance and diversity of evidence in retrieval-augmented LLMs?
- Basis in paper: [inferred] The authors observe that retrieved evidence is often irrelevant or redundant, suggesting a potential role for post-processing steps.
- Why unresolved: The paper does not investigate whether filtering or re-ranking mechanisms could enhance evidence quality.
- What evidence would resolve it: Implementing and evaluating post-retrieval filtering techniques (e.g., semantic similarity thresholds, diversity constraints) could demonstrate their effectiveness in improving evidence quality.

## Limitations
- The study focuses on only two specific scientific classification tasks (FoS and MAG) from a single benchmark, limiting generalizability.
- Evidence quality metrics are domain-dependent and may not capture nuanced relationships between queries and retrieved passages.
- The paper doesn't explore whether alternative retriever architectures or more explicit evidence-grounding instructions could mitigate evidence fabrication.

## Confidence
- Primary finding (evidence fabrication despite reasonable accuracy): Medium
- Scientific pretraining ineffectiveness: Low
- Evidence quality metrics reliability: Medium

## Next Checks
1. **Retriever Quality Isolation Test**: Evaluate the Contriever retriever independently on the same query set to determine whether poor evidence quality stems from retrieval failure versus generation mechanisms, measuring precision@k and recall@k for domain-relevant passages.

2. **Prompt Engineering Impact**: Systematically test whether more explicit evidence-grounding instructions (e.g., "Provide evidence passages that directly support your classification, or state if no relevant evidence exists") can reduce fabricated evidence while maintaining task accuracy.

3. **Cross-Domain Generalization**: Replicate the experiment on non-scientific benchmarks (e.g., legal or news classification tasks) to determine whether evidence fabrication is specific to scientific domains or represents a broader limitation of retrieval-augmented LLMs.