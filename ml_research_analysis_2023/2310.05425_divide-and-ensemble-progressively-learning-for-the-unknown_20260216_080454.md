---
ver: rpa2
title: 'Divide and Ensemble: Progressively Learning for the Unknown'
arxiv_id: '2310.05425'
source_url: https://arxiv.org/abs/2310.05425
tags:
- test
- samples
- dataset
- training
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel Divide and Ensemble (DEEM) method for
  classifying wheat nutrient deficiencies using RGB images. The key innovation lies
  in partitioning the dataset based on image collection dates, recognizing that samples
  from different dates exhibit notable discrepancies.
---

# Divide and Ensemble: Progressively Learning for the Unknown
## Quick Facts
- arXiv ID: 2310.05425
- Source URL: https://arxiv.org/abs/2310.05425
- Reference count: 8
- 1st place in the Deep Nutrient Deficiency Challenge with 93.6% average accuracy

## Executive Summary
This paper introduces a novel Divide and Ensemble (DEEM) method for classifying wheat nutrient deficiencies from RGB images. The key innovation is partitioning the dataset by collection dates to address domain shifts between samples. The method uses iterative pseudo-labeling with model ensemble voting to progressively incorporate test samples into training, achieving 93.6% average accuracy and winning first place in the Deep Nutrient Deficiency Challenge.

## Method Summary
DEEM partitions data by collection dates, recognizing domain gaps between samples. It employs an ensemble of four different model architectures (ResNet50, ResNet101, ResNext50, EfficientNet v2 s) to generate pseudo-labels through majority voting. High-confidence predictions are iteratively added to the training set, allowing models to adapt to test data patterns. The process continues until all test samples are labeled, with final inference routed by date to appropriate group models.

## Key Results
- Achieved 93.6% average Top-1 accuracy on test set
- Scored 94.0% on WW2020 dataset and 93.2% on WR2021 dataset
- Won 1st place in the Deep Nutrient Deficiency Challenge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain shifts between collection dates degrade model generalization.
- Mechanism: Partitioning data by date isolates domain-specific patterns, enabling models to learn date-consistent representations before being exposed to cross-date samples.
- Core assumption: Image appearance and lighting conditions differ substantially across dates, creating implicit domain gaps.
- Evidence anchors:
  - [abstract] "samples are equipped with their collection dates; (3) the samples of different dates show notable discrepancies"
  - [section 2.1] "Based on the image name ai, we extract the 'date' attribute from each sample and derive a date set defined as S = ( date1, ..., dateM ). Leveraging this set S, we group the training and testing samples with the same collection date"
- Break condition: If date metadata is unavailable or date clusters are too small to train meaningful models.

### Mechanism 2
- Claim: Progressive pseudo-labeling with ensemble voting reduces noise in self-training.
- Mechanism: High-confidence pseudo-labels are generated by majority vote among multiple architectures, and only added to training when either all models agree or exactly one model diverges.
- Core assumption: Correctly classified samples will be consistently predicted across architectures, while mislabeled ones will show disagreement.
- Evidence anchors:
  - [abstract] "we leverage models ensemble with different architectures to enhance the reliability of predictions"
  - [section 2.2] "Case 1. Consistent predictions are observed across different experts F0, · · · , FE, resulting in a uniform labels... Case 2. Discrepancies emerge in the predictions made by experts F0, · · · , FE"
- Break condition: If ensemble models are too similar in architecture or if data is too ambiguous to generate consensus predictions.

### Mechanism 3
- Claim: Iterative refinement with test samples improves model adaptability to unseen data.
- Mechanism: After each pseudo-labeling round, newly labeled samples are incorporated into training, allowing models to adapt to patterns present in test data without explicit labels.
- Core assumption: Test samples share latent structure with training samples, so pseudo-labels can serve as noisy but useful supervision.
- Evidence anchors:
  - [section 2.3] "Following this, we use the newly labeled samples from D1test and D2test to update the training dataset Dtrain, creating Dupdate train. We then retrain the ensembled models F0, · · · , FE on this updated dataset"
  - [section 3.3] "Our method achieves an average of 93.6% Top-1 accuracy on the test set"
- Break condition: If pseudo-labels are frequently incorrect, causing model drift or overfitting to noise.

## Foundational Learning

- Concept: Domain adaptation and dataset shift
  - Why needed here: Different collection dates introduce domain shifts; understanding how models cope with these shifts is key to why splitting by date helps.
  - Quick check question: What is covariate shift and how does it differ from label shift?

- Concept: Ensemble methods and majority voting
  - Why needed here: Multiple architectures reduce overfitting risk in pseudo-labeling; voting thresholds ensure only high-confidence predictions are used.
  - Quick check question: How does majority voting reduce variance in model predictions compared to a single model?

- Concept: Semi-supervised learning with pseudo-labels
  - Why needed here: The approach iteratively adds pseudo-labeled test samples to the training set, a classic semi-supervised paradigm.
  - Quick check question: What are the risks of self-training with noisy pseudo-labels, and how can ensemble voting mitigate them?

## Architecture Onboarding

- Component map:
  - Date extraction module -> Group splitter -> Base model trainer -> Ensemble predictor -> Pseudo-label validator -> Iterative updater -> Branch selector

- Critical path:
  1. Extract dates from filenames.
  2. Group data by date.
  3. Train base models per group.
  4. Predict pseudo-labels for test set.
  5. Validate and assign labels.
  6. Incorporate labeled samples and retrain.
  7. Repeat until all test samples labeled.
  8. Merge group models for final inference.

- Design tradeoffs:
  - Granularity of date splitting vs. training data size per group.
  - Number of ensemble models vs. computational cost and redundancy.
  - Strictness of pseudo-label criteria vs. speed of convergence.
  - Feature similarity validation overhead vs. label quality.

- Failure signatures:
  - Low ensemble agreement → pseudo-label assignment stalls.
  - Overfitting to test samples → reduced validation accuracy.
  - Imbalanced date groups → biased model performance.
  - Poor feature extractor → feature similarity check fails to filter noise.

- First 3 experiments:
  1. Train a single ResNet50 on the whole dataset vs. per-date groups; compare validation accuracy.
  2. Vary the number of ensemble models (1, 2, 3, 4) and measure pseudo-label quality and final test accuracy.
  3. Compare progressive pseudo-labeling vs. direct voting on a held-out validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of models to use in the ensemble for pseudo-labeling to achieve the best balance between performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions using four experts in their experiments and observes that the performance remains stable when the number is large enough, but does not explore beyond this point.
- Why unresolved: The paper does not provide a detailed analysis of how performance scales with the number of models beyond four, nor does it discuss the computational trade-offs involved.
- What evidence would resolve it: Experimental results showing the performance and computational cost for a range of model numbers, such as 1, 2, 3, 4, 5, and 10, would help determine the optimal number of models.

### Open Question 2
- Question: How does the DEEM method perform on datasets with different domain gaps, such as those collected under varying environmental conditions or from different geographical locations?
- Basis in paper: [inferred] The paper highlights the effectiveness of DEEM in handling domain gaps between samples collected on different dates, but does not explore its performance on more diverse or challenging datasets.
- Why unresolved: The paper focuses on a specific dataset and does not test the method's generalizability to other datasets with potentially larger or more complex domain gaps.
- What evidence would resolve it: Applying DEEM to multiple datasets with varying domain gaps and comparing its performance to other state-of-the-art methods would provide insights into its generalizability.

### Open Question 3
- Question: What is the impact of different data augmentation techniques on the performance of the DEEM method, and are there any specific augmentations that are particularly beneficial for this task?
- Basis in paper: [explicit] The paper mentions using a specific set of data augmentation techniques but does not explore the impact of different augmentations or combinations thereof.
- Why unresolved: The paper does not provide an analysis of how different data augmentation strategies affect the model's performance, which could lead to further improvements.
- What evidence would resolve it: Systematic experimentation with various data augmentation techniques, both individually and in combination, and their impact on the model's accuracy would help identify the most effective augmentations.

## Limitations
- The exact magnitude and nature of date-based domain shifts are not quantitatively characterized.
- The pseudo-labeling process relies heavily on ensemble agreement, but the paper does not provide detailed analysis of how often the ensemble fails to reach consensus.
- The feature similarity validation step mentioned in the method is not clearly explained, leaving uncertainty about how it contributes to label quality.

## Confidence
- **High confidence** in the overall method framework and its competitive performance (93.6% accuracy) on the test set.
- **Medium confidence** in the mechanism of date-based splitting reducing domain shift effects, as the paper provides qualitative evidence but lacks detailed domain gap analysis.
- **Medium confidence** in the progressive pseudo-labeling approach, though the exact criteria for label acceptance and rejection are not fully specified.

## Next Checks
1. Conduct ablation studies to quantify the contribution of each component: date-based splitting, ensemble voting, and iterative retraining. Compare performance when removing each element.
2. Analyze the distribution of pseudo-label confidence scores and ensemble agreement rates across all test samples to understand where the method succeeds or fails.
3. Perform cross-date validation by training models on one date and testing on another to measure domain shift magnitude and validate the need for date-based partitioning.