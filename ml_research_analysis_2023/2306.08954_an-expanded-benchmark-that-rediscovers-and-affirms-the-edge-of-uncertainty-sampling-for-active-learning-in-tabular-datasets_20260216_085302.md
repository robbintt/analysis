---
ver: rpa2
title: An Expanded Benchmark that Rediscovers and Affirms the Edge of Uncertainty
  Sampling for Active Learning in Tabular Datasets
arxiv_id: '2306.08954'
source_url: https://arxiv.org/abs/2306.08954
tags:
- learning
- query
- active
- strategies
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study revisits and expands active learning benchmarks to\
  \ address discrepancies in the performance of uncertainty sampling across existing\
  \ works. Through comprehensive experiments, the authors uncover a key issue\u2014\
  model compatibility\u2014where using mismatched query and task models can significantly\
  \ degrade uncertainty sampling\u2019s effectiveness."
---

# An Expanded Benchmark that Rediscovers and Affirms the Edge of Uncertainty Sampling for Active Learning in Tabular Datasets

## Quick Facts
- arXiv ID: 2306.08954
- Source URL: https://arxiv.org/abs/2306.08954
- Reference count: 40
- This study revisits and expands active learning benchmarks to address discrepancies in the performance of uncertainty sampling across existing works.

## Executive Summary
This study revisits and expands active learning benchmarks to address discrepancies in the performance of uncertainty sampling across existing works. Through comprehensive experiments, the authors uncover a key issue—model compatibility—where using mismatched query and task models can significantly degrade uncertainty sampling’s effectiveness. Their results reaffirm that uncertainty sampling remains competitive when paired with compatible models and outperforms many other strategies on most datasets. The work also highlights that over half of the examined active learning methods fail to show significant improvements over a simple random baseline. By providing a transparent, reproducible framework, the study offers actionable guidance for practitioners and sets a foundation for future research in active learning for tabular data.

## Method Summary
The authors expanded the pool-based active learning benchmark by conducting experiments on 26 binary classification datasets, comparing 13 query strategies including uncertainty sampling (US) with both compatible and non-compatible model settings. They used SVM(RBF) as the default task model and logistic regression for US-C, while US-NC paired logistic regression for querying with SVM(RBF) for training. The experiments involved 100 repeated runs for datasets with fewer than 2000 samples and 10 runs for larger datasets, using a fixed 60/40 train/test split with an initial labeled pool of 20 samples. Performance was measured using the area under the budget curve (AUBC), and statistical significance was assessed using Friedman and t-tests.

## Key Results
- Model compatibility significantly impacts uncertainty sampling effectiveness, with non-compatible models degrading performance.
- Uncertainty sampling remains competitive and often outperforms other strategies when paired with compatible models.
- Over half of the examined active learning methods fail to show significant improvements over random sampling.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using mismatched query and task models degrades uncertainty sampling effectiveness.
- Mechanism: The query model selects samples based on its own uncertainty measure, which may not align with the uncertainty that matters for the task model's training. When the two models differ (e.g., logistic regression with C=0.1 for querying and SVM(RBF) for training), the queried samples are less informative for the task model, reducing the benefit of active learning.
- Core assumption: Uncertainty sampling's value depends on the query model and task model sharing a compatible notion of uncertainty.
- Evidence anchors:
  - [abstract]: "adopting the different models for the querying unlabeled examples and learning tasks would degrade US's effectiveness."
  - [section]: "Figure 3 illustrates that this particular setting can lead the query-oriented model to query samples that are not the most uncertain for the task-oriented model."
- Break condition: If the task model can adapt to any distribution of queried samples, or if uncertainty sampling is used only to diversify the dataset rather than improve model accuracy.

### Mechanism 2
- Claim: Over half of examined active learning strategies do not show significant improvements over random sampling.
- Mechanism: Many active learning strategies are designed to balance exploration and exploitation or incorporate complex heuristics, but in practice these mechanisms may not yield better query decisions than uniform random sampling when model compatibility and dataset characteristics are not favorable. The strategies fail to add enough value beyond what random sampling achieves.
- Core assumption: The strategies' added complexity does not translate into better label efficiency compared to random sampling.
- Evidence anchors:
  - [abstract]: "over half of the examined active learning methods fail to show significant improvements over a simple random baseline."
  - [section]: "Figure 5 reveals the difficulty of this query strategy on Haberman, Tic, Banana and Ringnorm, which less than three (20%) query strategy are significantly better than Uniform."
- Break condition: If the dataset has a very simple structure or if the model is already very accurate with a small number of random labels.

### Mechanism 3
- Claim: Uncertainty sampling remains competitive when paired with compatible models.
- Mechanism: When the query model and task model are compatible (e.g., both using the same model family or hyperparameters), uncertainty sampling effectively targets the samples that are most informative for training, leading to faster convergence and higher accuracy. This compatibility ensures that the uncertainty measure is meaningful for the task model's learning process.
- Core assumption: Model compatibility directly translates to better query efficiency and learning performance.
- Evidence anchors:
  - [abstract]: "our findings affirm that US maintains a competitive edge over other strategies when paired with compatible models."
  - [section]: "Table 3 demonstrates that uncertainty sampling outperforms other query strategies on most datasets."
- Break condition: If the dataset is too noisy or too small for uncertainty to be a reliable signal, or if the model's uncertainty estimates are poorly calibrated.

## Foundational Learning

- Concept: Pool-based active learning framework
  - Why needed here: The paper builds its benchmark on this framework, and understanding it is essential to interpret the results and mechanisms.
  - Quick check question: What are the four main steps in one round of pool-based active learning?

- Concept: Model compatibility in active learning
  - Why needed here: The paper's main finding hinges on the impact of using different models for querying and training. Understanding what compatibility means is key to applying the results.
  - Quick check question: Why might using a logistic regression model for querying and an SVM model for training lead to degraded performance?

- Concept: Statistical significance testing in benchmarking
  - Why needed here: The paper uses t-tests and other methods to compare query strategies. Knowing how these tests work is important for interpreting the claims about improvements over random sampling.
  - Quick check question: What does it mean if a query strategy's improvement over random sampling is not statistically significant?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (query and task models) -> Query strategy selection -> Evaluation (AUBC calculation) -> Statistical analysis
- Critical path: Data → Preprocessing → Initial Labeled/Unlabeled split → Iterative Query → Model Update → Evaluation → AUBC Calculation → Statistical Comparison
- Design tradeoffs: Using a fixed train/test split ensures reproducibility but may not reflect all possible data distributions. Allowing different query and task models increases flexibility but introduces compatibility issues. Running many seeds improves robustness but increases computational cost.
- Failure signatures: If query strategies do not outperform random sampling, or if AUBC curves plateau quickly, this may indicate model incompatibility or that the dataset is not well-suited for active learning. If statistical tests yield many false positives/negatives, the number of seeds or the significance level may need adjustment.
- First 3 experiments:
  1. Run uncertainty sampling with a compatible model pair (e.g., both logistic regression) on a small, well-understood dataset and verify that AUBC increases over random sampling.
  2. Repeat with an incompatible pair (e.g., logistic regression for querying, SVM for training) and observe if performance degrades relative to the compatible case.
  3. Run a set of complex query strategies (e.g., BMDR, SPAL) on the same dataset and check if any outperform uncertainty sampling, to see if added complexity translates to gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of datasets cause uncertainty sampling to fail, as seen in Haberman, Tic, Banana, and Ringnorm?
- Basis in paper: [explicit] The authors note these datasets are particularly challenging for uncertainty sampling and mention related work that identifies possible reasons for its failure.
- Why unresolved: The paper does not provide a detailed analysis of the specific dataset characteristics that lead to uncertainty sampling's poor performance on these cases.
- What evidence would resolve it: A systematic study analyzing the feature distributions, class imbalance, and other properties of these datasets to identify common patterns that make them difficult for uncertainty sampling.

### Open Question 2
- Question: How do different kernel functions in SVMs affect the compatibility between query-oriented and task-oriented models in uncertainty sampling?
- Basis in paper: [explicit] The authors discuss the impact of model compatibility on uncertainty sampling's effectiveness and show results for different model combinations.
- Why unresolved: The paper only examines a limited set of model combinations and does not explore the impact of different kernel functions on compatibility.
- What evidence would resolve it: Experiments testing various kernel functions (linear, polynomial, sigmoid) for both query and task models to determine which combinations yield optimal compatibility and performance.

### Open Question 3
- Question: Can uncertainty sampling be made more robust for real-world applications by incorporating additional information beyond pure uncertainty?
- Basis in paper: [inferred] The authors note that uncertainty sampling remains competitive but fails on certain datasets, suggesting room for improvement.
- Why unresolved: The paper does not explore modifications to uncertainty sampling that could enhance its robustness across diverse datasets.
- What evidence would resolve it: Development and testing of hybrid uncertainty sampling strategies that combine uncertainty with other criteria (e.g., diversity, representativeness) and evaluation on the challenging datasets mentioned.

## Limitations

- The study focuses exclusively on binary classification, leaving multiclass and regression tasks unexplored.
- The benchmark does not address active learning in streaming or online settings, where data distributions may shift over time.
- The study does not investigate the impact of dataset size and class imbalance on the observed effects, which could be significant for real-world applications.

## Confidence

- High confidence in the core finding that model compatibility significantly impacts uncertainty sampling's effectiveness, supported by direct experimental comparisons and clear statistical evidence.
- Medium confidence in the generalizability of these results across all tabular datasets and model families, as the benchmark focuses primarily on binary classification with SVM(RBF) and logistic regression.

## Next Checks

1. Test uncertainty sampling with compatible and non-compatible models on multiclass classification datasets to verify if the compatibility effect extends beyond binary tasks.
2. Evaluate query strategies on imbalanced datasets with varying class distributions to assess robustness to class imbalance.
3. Conduct experiments in an online learning setting with concept drift to determine if model compatibility remains critical when data distributions change over time.