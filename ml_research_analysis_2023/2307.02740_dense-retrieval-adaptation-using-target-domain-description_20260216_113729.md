---
ver: rpa2
title: Dense Retrieval Adaptation using Target Domain Description
arxiv_id: '2307.02740'
source_url: https://arxiv.org/abs/2307.02740
tags:
- domain
- retrieval
- query
- target
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of domain adaptation with description
  in information retrieval (IR), where a retrieval model needs to be adapted to a
  new target domain based solely on a textual description of that domain, without
  access to the actual target data. To address this, the authors propose a novel pipeline
  that first uses a language model to understand the domain description and extract
  key attributes (such as query topics, document types, and relevance notions) based
  on a defined taxonomy.
---

# Dense Retrieval Adaptation using Target Domain Description

## Quick Facts
- arXiv ID: 2307.02740
- Source URL: https://arxiv.org/abs/2307.02740
- Reference count: 40
- Primary result: Introduces domain adaptation with description in IR, achieving significant NDCG@10 and Recall@100 improvements over dense retrieval baselines

## Executive Summary
This paper addresses the novel problem of domain adaptation with description in information retrieval, where a retrieval model must adapt to a new target domain using only a textual description rather than actual target data. The authors propose an automatic pipeline that extracts domain attributes from the description, constructs synthetic document collections and queries, generates pseudo relevance labels, and uses this synthetic data to adapt a dense retrieval model. Experiments across five diverse IR tasks demonstrate that this approach significantly outperforms existing dense retrieval baselines and even surpasses BM25 on some tasks, while being cost-effective compared to traditional fine-tuning methods.

## Method Summary
The method involves a multi-stage pipeline: first, a language model (ChatGPT) extracts domain attributes from the textual description using a defined taxonomy; second, it generates a seed document and iteratively retrieves relevant documents from a large heterogeneous corpus to build a synthetic collection; third, an instruction-tuned T5 model generates synthetic queries conditioned on the domain attributes; fourth, a cross-encoder reranker trained on the source domain assigns pseudo relevance labels to the synthetic query-document pairs; finally, a dense retrieval model is adapted to the target domain using the synthetic dataset through knowledge distillation. The approach is evaluated on five IR tasks, showing substantial improvements over baselines while requiring only domain descriptions as input.

## Key Results
- The proposed approach achieves significant improvements in NDCG@10 and Recall@100 compared to dense retrieval baselines across five IR tasks
- Performance surpasses BM25 on certain tasks, demonstrating the effectiveness of the adaptation approach
- The method provides a cost-effective alternative to traditional fine-tuning, which requires large amounts of target domain data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative document selection from a large heterogeneous corpus based on seed document retrieval can reconstruct a domain-relevant synthetic collection even without access to the actual target data.
- Mechanism: The approach uses a seed document generated from domain attributes to query a large corpus, retrieves similar documents using BM25 and a reranker, and iteratively expands the seed set to build a synthetic collection aligned with the target domain.
- Core assumption: The large heterogeneous corpus contains documents that are representative of or similar to the target domain; BM25 + reranker can effectively identify relevant documents for iterative expansion.
- Evidence anchors:
  - [abstract] "The extracted properties are used to generate a seed document using generative language models and then an iterative retrieval process is employed to construct a synthetic target collection, automatically."
  - [section 3.4.1] "We then run an iterative retrieval process using BM25 and a BERT-based cross-encoder reranking model trained on the source domain [22]."
  - [corpus] Evidence is indirect; the approach assumes the Web or similar large collection contains relevant documents but does not prove coverage or representativeness.
- Break condition: If the large corpus lacks sufficient domain-relevant documents, or BM25/reranker cannot accurately retrieve them, the synthetic collection will be misaligned and adaptation will fail.

### Mechanism 2
- Claim: Synthetic query generation guided by domain attributes and pseudo-labeling with a cross-encoder teacher model can create effective training data for dense retrieval adaptation.
- Mechanism: Queries are generated from the synthetic collection using an instruction-tuned T5 model conditioned on query and relevance attributes; documents are annotated with pseudo-labels via a cross-encoder reranker trained on the source domain.
- Core assumption: The cross-encoder teacher model can accurately score document relevance for the target domain despite being trained on the source domain; generated queries preserve the target domain's query properties.
- Evidence anchors:
  - [abstract] "The synthetic dataset is used to adapt a dense retrieval model to the target domain."
  - [section 3.4.2] "It is similar to the docT5query [21], but also takes query and relevance properties of the target domain as input."
  - [section 3.4.3] "We use a cross-encoder re-ranking model based on BERT [22] that is trained on MS MARCO (our source domain) as a teacher model and annotate documents through soft labeling."
  - [corpus] The source domain MS MARCO may not cover all relevance notions needed for target domains, potentially limiting pseudo-label accuracy.
- Break condition: If the teacher model's relevance judgments are poor for the target domain, the pseudo-labels will misguide adaptation, leading to degraded retrieval performance.

### Mechanism 3
- Claim: Domain description understanding via instruction-tuned language models can accurately extract attribute values needed for synthetic data construction.
- Mechanism: ChatGPT is prompted with domain descriptions and examples to extract values for query attributes, document attributes, and relevance notions from the defined taxonomy.
- Core assumption: The language model can interpret natural language descriptions and generalize attribute values without explicit examples; the taxonomy is comprehensive and mutually exclusive.
- Evidence anchors:
  - [abstract] "We introduce a novel automatic data construction pipeline that produces a synthetic document collection, query set, and pseudo relevance labels, given a textual domain description."
  - [section 3.3] "We instruct the model to get the description of the domain and extracts the value of attributes introduced in the taxonomy."
  - [section 4.3] "Table 6 presents the results of ChatGPT for domain description understanding."
  - [corpus] Performance varies by attribute; some like query and document modality are poorly predicted without examples, indicating limitations in understanding complex domain attributes.
- Break condition: If the language model fails to extract correct attribute values, the synthetic data will be misaligned with the target domain, breaking the adaptation pipeline.

## Foundational Learning

- Concept: Taxonomy of domain attributes in IR (query topics, linguistic features, document topics, relevance notion, etc.)
  - Why needed here: Provides a structured framework to identify what properties must be adapted between source and target domains; enables systematic synthetic data construction.
  - Quick check question: What are the three main categories of attributes in the taxonomy, and why must they all be considered for effective domain adaptation?

- Concept: Instruction-tuned language models and prompting strategies
  - Why needed here: Used for domain description understanding and synthetic query generation; require careful prompt engineering to elicit correct attribute extraction and query creation.
  - Quick check question: How does providing examples in prompts improve language model performance for attribute extraction?

- Concept: Dense retrieval adaptation via knowledge distillation
  - Why needed here: Adapts the dense retriever to the target domain using synthetic data and pseudo-labels, leveraging the teacher-student paradigm to transfer relevance knowledge.
  - Quick check question: What is the role of the listwise loss function in adapting the dense retriever with pseudo-labeled data?

## Architecture Onboarding

- Component map: Domain Description Understanding (ChatGPT) -> Synthetic Document Collection Construction -> Synthetic Query Generation (instruction-tuned T5) -> Pseudo Labeling (BERT cross-encoder) -> Dense Retrieval Adaptation (Contriever fine-tuning)
- Critical path: Domain description → attribute extraction → seed generation → iterative retrieval → synthetic collection → query generation → pseudo-labeling → dense retrieval fine-tuning
- Design tradeoffs:
  - Using ChatGPT for seed generation is effective but potentially costly; alternatives like GPT-3 or open models are less reliable.
  - Iterative retrieval with small k improves accuracy but increases computation; large k may include irrelevant documents.
  - Pseudo-labeling with a source-domain cross-encoder may be inaccurate for novel relevance notions; could use ensemble or multiple teachers.
- Failure signatures:
  - Poor synthetic collection quality: seed document generation fails, iterative retrieval retrieves off-topic documents, or large corpus lacks relevant content.
  - Ineffective query generation: instruction-tuned model fails to generate queries matching target domain properties.
  - Inaccurate pseudo-labels: cross-encoder teacher model poorly generalizes to target domain relevance.
  - Adaptation failure: synthetic data is misaligned with target domain, causing dense retriever to overfit to synthetic distribution.
- First 3 experiments:
  1. Validate domain description understanding: Provide a simple domain description and check if ChatGPT extracts correct attribute values for all taxonomy categories.
  2. Test synthetic collection construction: Generate a seed document from attributes, run iterative retrieval with k=10 and N=1000, inspect the top retrieved documents for domain relevance.
  3. Verify pseudo-labeling quality: Use the cross-encoder teacher to label a small set of query-document pairs from the synthetic data, compare scores to human judgments if available.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the domain description understanding component be made more robust to variations in how users describe target domains?
- Basis in paper: [explicit] The paper discusses the challenge of extracting domain attributes from natural language descriptions and notes that the current approach using ChatGPT with examples is not perfect, particularly for complex attributes like relevance notions.
- Why unresolved: The paper relies on a fixed instruction template with examples, which may not generalize well to all possible ways users describe their domains. Different users might use different terminology or levels of detail.
- What evidence would resolve it: Experiments comparing different instruction templates, example selection strategies, or alternative models for attribute extraction would help identify more robust approaches.

### Open Question 2
- Question: What is the optimal trade-off between synthetic corpus size and quality for effective domain adaptation?
- Basis in paper: [explicit] The paper mentions that larger synthetic collections may degrade accuracy (Figure 2) and discusses the cost of generating documents one-by-one using language models, but doesn't systematically explore the size-quality trade-off.
- Why unresolved: The paper uses a fixed corpus size (10,000 documents) without exploring how different sizes affect adaptation performance or cost-effectiveness.
- What evidence would resolve it: Systematic experiments varying synthetic corpus size across different target domains while measuring both adaptation performance and generation costs.

### Open Question 3
- Question: How does the iterative document selection process compare to alternative approaches for synthetic collection construction?
- Basis in paper: [explicit] The paper compares its iterative approach to naive document generation and single-retrieval alternatives, but doesn't explore other potential approaches like clustering-based selection or diversity-promoting methods.
- Why unresolved: The paper only evaluates its proposed iterative approach against two baselines, leaving open the question of whether other synthetic collection construction methods might be more effective.
- What evidence would resolve it: Comparative experiments with alternative collection construction approaches (e.g., clustering-based, diversity-focused, or topic-balanced methods) across multiple target domains.

## Limitations
- The approach assumes the large heterogeneous corpus contains representative documents for all target domains, which may not hold for specialized or rare domains
- The effectiveness depends heavily on the quality of domain descriptions and the language model's ability to extract accurate attributes
- The cross-encoder reranker's pseudo-labeling quality may degrade when target domains have relevance notions not well-represented in the source domain MS MARCO

## Confidence
- Confidence is **High** in the experimental results showing improved NDCG@10 and Recall@100 over baselines, as these are measured on standard IR benchmarks with established protocols
- Confidence is **Medium** in the general applicability of the approach, as performance may vary significantly depending on the quality of domain descriptions and the heterogeneity of the large corpus
- Confidence is **Low** in the scalability and cost-effectiveness of the method, particularly regarding the repeated use of ChatGPT for seed generation and attribute extraction

## Next Checks
1. **Robustness testing**: Apply the approach to additional target domains with varying levels of similarity to MS MARCO and evaluate how performance degrades as domain distance increases
2. **Ablation study on taxonomy completeness**: Remove specific attribute categories from the taxonomy and measure the impact on synthetic data quality and retrieval performance to identify which attributes are most critical
3. **Human evaluation of pseudo-labels**: Compare the cross-encoder reranker's pseudo-labels against human relevance judgments on a sample of synthetic query-document pairs to quantify annotation accuracy and identify systematic biases