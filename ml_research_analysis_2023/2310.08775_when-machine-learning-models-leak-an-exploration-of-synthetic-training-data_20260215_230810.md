---
ver: rpa2
title: 'When Machine Learning Models Leak: An Exploration of Synthetic Training Data'
arxiv_id: '2310.08775'
source_url: https://arxiv.org/abs/2310.08775
tags:
- data
- attack
- individuals
- learning
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the privacy risks of releasing a machine
  learning model trained to predict whether a person or household will move within
  the next two years. The authors consider an attacker who has access to the model
  and can query it to obtain predictions.
---

# When Machine Learning Models Leak: An Exploration of Synthetic Training Data

## Quick Facts
- arXiv ID: 2310.08775
- Source URL: https://arxiv.org/abs/2310.08775
- Reference count: 31
- Primary result: Models trained on synthetic data maintain predictive performance but are equally vulnerable to attribute inference attacks as models trained on original data.

## Executive Summary
This paper investigates the privacy risks associated with releasing machine learning models trained on sensitive data, specifically examining whether synthetic training data can mitigate attribute inference attacks. The study focuses on a propensity-to-move prediction task using real-world data, where attackers attempt to infer sensitive attributes (gender, age, income) about target individuals. The research demonstrates that synthetic data generation techniques, while preserving predictive performance, do not reduce vulnerability to privacy attacks because they maintain the same statistical dependencies between sensitive and non-sensitive attributes.

## Method Summary
The authors investigate attribute inference attacks on a machine learning model predicting whether a person or household will move within two years. They use real-world data (~150K individuals, 700 variables) and generate synthetic training data using CART-based synthesis with the Synthpop R toolkit. The methodology involves training propensity-to-move classifiers on both original and synthetic data, then performing three types of attribute inference attacks (random, baseline, black-box) to evaluate privacy leakage. The study compares utility metrics (F1-score, MCC, AUC) and attack success rates (precision, recall, accuracy) between models trained on original versus synthetic data.

## Key Results
- Models trained on synthetic data maintain similar predictive performance as those trained on original data
- Synthetic data models are just as susceptible to attribute inference attacks as original data models
- Attack success using only prior distributions reveals that synthetic data does not provide additional privacy beyond what is already exposed by data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data preserves predictive performance but does not reduce privacy leakage because sensitive attribute distributions are retained in the synthetic model.
- Mechanism: The synthetic training set is generated to mimic the joint distribution of the original data, including the correlation between non-sensitive and sensitive attributes. This means that even though the synthetic data does not contain exact training records, it still encodes the same statistical dependencies that enable attribute inference.
- Core assumption: The attacker can exploit the preserved joint distribution structure in synthetic data just as effectively as in real data.
- Evidence anchors:
  - [abstract] "The results show that the model trained on synthetic data maintains similar predictive performance but is just as susceptible to attribute inference attacks as the original model."
  - [section] "Synthetic data mimics properties of the original data including overall structure, correlation between features, and the joint distributions."
  - [corpus] Weak evidence: neighbor papers discuss leakage in synthetic data but do not confirm the specific joint-distribution preservation mechanism.
- Break condition: If synthetic data generation explicitly masks or breaks the correlation between sensitive and non-sensitive attributes, the mechanism fails.

### Mechanism 2
- Claim: Attack success using only prior distributions reveals that synthetic data does not provide additional privacy beyond what is already exposed by data distributions.
- Mechanism: When an attacker uses only the marginal distribution of sensitive attributes (priors) to infer values, success indicates that the sensitive attribute information is already embedded in the statistical patterns of the data. Synthetic data, which replicates these patterns, does not add or remove this exposure.
- Core assumption: The marginal distribution of sensitive attributes in synthetic data is statistically similar to that in the original data.
- Evidence anchors:
  - [abstract] "The success of attacks using only prior distributions highlights the magnitude of the challenge in protecting sensitive attributes when releasing ML models."
  - [section] "The synthetic training set can replace the original training set... trained on synthetic data maintained prediction performance, but was found to leak in the same way as the original classiï¬er."
  - [corpus] Weak evidence: neighbor papers discuss synthetic data leakage but do not confirm the specific role of prior distributions in attack success.
- Break condition: If synthetic data generation alters the marginal distribution of sensitive attributes, the mechanism fails.

### Mechanism 3
- Claim: The synthetic data generation process based on CART models preserves the conditional dependencies needed for attribute inference attacks.
- Mechanism: CART-based synthesis models each variable conditionally on previous ones, preserving the hierarchical dependencies that link non-sensitive to sensitive attributes. This conditional preservation allows attribute inference models to exploit the same pathways as with real data.
- Core assumption: The CART synthesis order and stopping rules maintain the critical dependencies between sensitive and non-sensitive attributes.
- Evidence anchors:
  - [section] "Data synthesis is based on sequential modeling by decomposing a multidimensional joint distribution into conditional and univariate distributions... models and generates one variable at a time, conditionally to previous variables."
  - [section] "Synthesis using CART model has two important parameters... the order in which variables are synthesized called visiting.sequence... the stopping rules that dictate the number of observations that are assigned to a node in the tree."
  - [corpus] Weak evidence: neighbor papers discuss synthetic data synthesis but do not confirm the specific role of CART-based conditional dependencies.
- Break condition: If the synthesis order is altered to break critical dependencies or if stopping rules prevent capturing sensitive attribute correlations, the mechanism fails.

## Foundational Learning

- Concept: Joint probability distribution and conditional dependencies
  - Why needed here: Understanding how synthetic data generation preserves the relationships between sensitive and non-sensitive attributes is crucial to explaining why privacy attacks remain effective.
  - Quick check question: If you change the visiting sequence in CART synthesis, how might that affect the ability of an attacker to infer sensitive attributes?

- Concept: Attribute inference attack mechanics
  - Why needed here: The paper's findings hinge on the attacker's ability to use partial information and model predictions to recover sensitive attributes, which requires understanding the attack model.
  - Quick check question: What is the difference between a black-box attack and a baseline attack in the context of this paper?

- Concept: Statistical disclosure control (SDC) and synthetic data
  - Why needed here: The paper's exploration of synthetic data as a privacy countermeasure is grounded in SDC principles, so understanding these concepts is essential.
  - Quick check question: What is the difference between fully synthetic data and partially synthetic data in SDC?

## Architecture Onboarding

- Component map: Original data -> Feature selection (SelectKBest) -> Synthetic data generation (CART-based) -> Model training (ML algorithms) -> Attack evaluation (random, baseline, black-box)
- Critical path: The critical path for reproducing the results is: load original data -> select top K features -> generate synthetic data -> train models on both original and synthetic data -> perform attribute inference attacks.
- Design tradeoffs: The tradeoff is between utility (predictive performance) and privacy (resistance to attribute inference). The paper shows that synthetic data maintains utility but does not improve privacy.
- Failure signatures: If synthetic data generation fails to preserve the joint distribution, the predictive performance will drop. If it fails to preserve conditional dependencies, the attribute inference attacks will be less successful.
- First 3 experiments:
  1. Train a decision tree on the original data and evaluate its predictive performance on the test set.
  2. Generate synthetic data using CART and train a decision tree on the synthetic data; evaluate its performance on the same test set.
  3. Perform attribute inference attacks (random, baseline, black-box) on both the original and synthetic models using the inclusive individuals (2013) subset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can synthetic data generation techniques be modified to reduce the risk of attribute inference attacks while maintaining utility for machine learning tasks?
- Basis in paper: [explicit] The paper discusses how synthetic data maintains similar predictive performance but is equally susceptible to attribute inference attacks as the original data. It suggests applying privacy-preserving techniques during synthesis or exploring different combinations of ML and conventional models.
- Why unresolved: The paper acknowledges this as a potential solution but does not investigate specific techniques or their effectiveness.
- What evidence would resolve it: Empirical studies comparing the success rate of attribute inference attacks on synthetic data generated using various privacy-preserving techniques (e.g., differential privacy, data masking) versus standard synthetic data, while measuring the impact on ML model performance.

### Open Question 2
- Question: How do different attack scenarios, where the attacker has limited resources or access to only a subset of attributes, affect the success rate of attribute inference attacks?
- Basis in paper: [explicit] The paper mentions expanding the threat model to include attack scenarios with more limited resources, such as the attacker not having access to all attributes in the data.
- Why unresolved: The paper focuses on a specific attack scenario and does not explore the impact of varying attacker resources on attack success.
- What evidence would resolve it: Experiments comparing the success rate of attribute inference attacks under different attacker resource constraints, such as limited access to target individual data or partial knowledge of the ML model.

### Open Question 3
- Question: What evaluation metrics from statistical disclosure control (SDC) and machine learning can be used to quantify the success of attribute inference attacks for a given target individual?
- Basis in paper: [explicit] The paper suggests that future research should explore other metrics from SDC and ML perspectives to evaluate and quantify the success of attribute inference attacks for specific target individuals.
- Why unresolved: The paper uses precision, recall, and accuracy to measure attack success but does not explore alternative metrics that might provide additional insights.
- What evidence would resolve it: Comparative studies using various SDC and ML metrics to evaluate attribute inference attacks, analyzing the strengths and limitations of each metric in quantifying attack success for individual targets.

## Limitations

- The paper does not provide specific implementation details for the Synthpop R toolkit and the CART model used for synthetic data generation, making reproduction difficult.
- The exact subset of data and marginal prior distribution used by the attacker in each attack model is not specified, limiting understanding of the attack scenarios.
- The findings are based on a specific application domain (propensity-to-move prediction) and dataset, raising questions about generalizability to other domains.

## Confidence

- **High Confidence**: The claim that synthetic data preserves predictive performance is supported by the empirical results presented in the paper.
- **Medium Confidence**: The claim that synthetic data is just as susceptible to attribute inference attacks as original data is supported by the results, but the underlying mechanisms and generalizability require further investigation.
- **Low Confidence**: The claim that synthetic data does not provide additional privacy beyond what is already exposed by data distributions is based on the success of attacks using only prior distributions. However, the paper does not provide a comprehensive analysis of the privacy guarantees offered by synthetic data in comparison to other privacy-preserving techniques.

## Next Checks

1. **Reproduce the synthetic data generation**: Obtain the Synthpop R toolkit and implement the CART model with the same parameters used in the paper. Generate synthetic data from the original dataset and compare the statistical properties of the synthetic and original data.

2. **Analyze the attack models**: Examine the specific attack models used in the paper and assess their effectiveness in different scenarios. Evaluate the impact of varying the subset of data and prior distribution used by the attacker.

3. **Explore alternative synthetic data generation methods**: Investigate other synthetic data generation techniques, such as differential privacy or generative adversarial networks, and compare their performance in terms of utility and privacy guarantees.