---
ver: rpa2
title: 'Deep Unlearning: Fast and Efficient Gradient-free Approach to Class Forgetting'
arxiv_id: '2312.00761'
source_url: https://arxiv.org/abs/2312.00761
tags:
- class
- unlearning
- algorithm
- samples
- retain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel SVD-based algorithm for class unlearning,
  eliminating the need for gradient-based steps. The approach strategically suppresses
  class-discriminatory activations by estimating Retain and Forget Spaces through
  SVD on layerwise activations.
---

# Deep Unlearning: Fast and Efficient Gradient-free Approach to Class Forgetting

## Quick Facts
- **arXiv ID**: 2312.00761
- **Source URL**: https://arxiv.org/abs/2312.00761
- **Reference count**: 40
- **Primary result**: Novel SVD-based algorithm eliminates gradient-based steps for class unlearning, achieving strong performance with only ~1.5% drop in retain accuracy on ImageNet.

## Executive Summary
This paper introduces a gradient-free algorithm for class unlearning that leverages Singular Value Decomposition (SVD) to strategically suppress class-discriminatory activations. The method estimates Retain and Forget Spaces from layerwise activations, isolates class-discriminatory features by removing shared information, and updates model weights to suppress these features. The approach achieves strong unlearning performance while maintaining computational efficiency, requiring no gradient-based optimization steps and using minimal samples from forget classes.

## Method Summary
The algorithm performs class unlearning through a series of linear algebra operations. It first collects layerwise activations for retain and forget samples, then applies SVD to estimate the Retain Space (Ur) and Forget Space (Uf). Shared information between these spaces is removed to isolate class-discriminatory features, which are then suppressed by projecting model weights onto the orthogonal space of these features. The method includes an importance scaling mechanism for SVD basis vectors and a grid search over scaling hyperparameters (αr and αf) to optimize the balance between retain accuracy and forget accuracy.

## Key Results
- Achieves under 1% accuracy on unlearned classes while maintaining ~1.5% drop in retain accuracy compared to original model on ImageNet
- Requires up to 10× fewer samples than baseline methods while showing 1.38% average accuracy improvement
- Demonstrates strong resilience against Membership Inference Attacks while maintaining computational efficiency
- Successfully scales to multi-class unlearning scenarios with competitive performance across CIFAR-10, CIFAR-100, and ImageNet

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The algorithm removes class-discriminatory information by projecting model weights onto the orthogonal space of the forget class representation.
- **Mechanism**: Singular Value Decomposition (SVD) is applied to layerwise activations to estimate Retain Space (Ur) and Forget Space (Uf). Shared information between these spaces is removed from the Forget Space to isolate class-discriminatory features. Model weights are then updated to suppress these features.
- **Core assumption**: The SVD of layerwise activations captures the essential class-discriminatory features, and projecting weights orthogonally to these features effectively removes the target class information without harming retain class performance.
- **Evidence anchors**:
  - [abstract] "Our algorithm first estimates the Retain and the Forget Spaces using Singular Value Decomposition on the layerwise activations... We then compute the shared information between these spaces and remove it from the forget space to isolate class-discriminatory feature space."
  - [section] "Our algorithm aims to suppress the class-discriminatory activations associated with forget class... Post multiplying the parameters with (I − Pdis)T, is mathematically the same as removing class-discriminatory information from xli."
- **Break condition**: If the forget and retain class distributions are too similar, SVD may fail to isolate class-discriminatory features, leading to ineffective unlearning.

### Mechanism 2
- **Claim**: Importance scaling of SVD basis vectors ensures that the projection matrices effectively capture class-discriminatory information.
- **Mechanism**: Basis vectors from SVD are scaled by an importance matrix Λ, which is proportional to the variance explained by each vector. This scaling adjusts the contribution of each basis vector to the projection matrices.
- **Core assumption**: The variance explained by each SVD basis vector is indicative of its importance in distinguishing class features, and scaling by this variance improves the isolation of class-discriminatory features.
- **Evidence anchors**:
  - [abstract] "We then compute the shared information between these spaces and remove it from the forget space to isolate class-discriminatory feature space."
  - [section] "To capture the importance the ith basis vector in the matrix U... we formulate an diagonal importance matrix Λ having the ith diagonal component λi given by Equation 2."
- **Break condition**: If the scaling coefficient α is set too high or too low, it may overemphasize or underemphasize certain features, leading to either loss of retain accuracy or incomplete unlearning.

### Mechanism 3
- **Claim**: Grid search over scaling hyperparameters (αr and αf) optimizes the balance between retain and forget accuracy.
- **Mechanism**: The algorithm performs a grid search over predefined lists of αr and αf values, evaluating each combination using a scoring function that penalizes both high forget accuracy and low retain accuracy.
- **Core assumption**: Different classes may have varying degrees of confusion with retain classes, requiring different scaling for the retain and forget spaces to achieve optimal unlearning.
- **Evidence anchors**:
  - [abstract] "Our approach only perform inference and does not rely on computationally intensive gradient based optimization steps... leading to a fast and efficient approach."
  - [section] "Our algorithm searches for the optimal hyperparameter values for αr and αf within the predefined lists... We observe this search is necessary for our algorithm."
- **Break condition**: If the hyperparameter search space is not sufficiently large or well-chosen, the algorithm may not find the optimal scaling parameters, resulting in suboptimal unlearning performance.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: SVD is used to decompose layerwise activation matrices into basis vectors that capture the essential features of retain and forget classes.
  - Quick check question: What is the primary purpose of applying SVD to the layerwise activations in this algorithm?

- **Concept: Projection Matrices**
  - Why needed here: Projection matrices are used to isolate and suppress class-discriminatory features by projecting model weights onto the orthogonal space of these features.
  - Quick check question: How does the algorithm use projection matrices to remove class-discriminatory information from the model?

- **Concept: Importance Scaling**
  - Why needed here: Importance scaling adjusts the contribution of each SVD basis vector to the projection matrices, ensuring that more significant features have a greater impact on the unlearning process.
  - Quick check question: Why is it important to scale the SVD basis vectors by their variance before constructing the projection matrices?

## Architecture Onboarding

- **Component map**: SVD computation module -> Importance scaling module -> Projection matrix computation module -> Weight update module -> Hyperparameter search module

- **Critical path**:
  1. Collect layerwise activations for retain and forget samples
  2. Apply SVD to estimate Retain and Forget Spaces
  3. Scale basis vectors by their variance
  4. Compute projection matrices and isolate class-discriminatory features
  5. Update model weights to suppress class-discriminatory features
  6. Evaluate performance and perform hyperparameter search if necessary

- **Design tradeoffs**:
  - Accuracy vs. efficiency: Using fewer samples for SVD estimation reduces computational cost but may impact unlearning effectiveness
  - Retain accuracy vs. forget accuracy: Balancing the scaling of retain and forget spaces is crucial for maintaining retain accuracy while achieving effective unlearning
  - Complexity vs. interpretability: The algorithm's reliance on SVD and projection matrices adds complexity but provides a clear mechanism for feature isolation

- **Failure signatures**:
  - High forget accuracy with low retain accuracy: Indicates overemphasis on forget space scaling
  - Low forget accuracy with high retain accuracy: Suggests insufficient scaling of forget space or ineffective feature isolation
  - Inconsistent performance across layers: May indicate issues with SVD estimation or importance scaling

- **First 3 experiments**:
  1. **Baseline unlearning**: Apply the algorithm to a simple linear model on a synthetic dataset to verify that it can effectively unlearn a target class while maintaining retain accuracy
  2. **Layerwise analysis**: Evaluate the algorithm's performance when applied to different subsets of layers in a convolutional network to understand the impact of layerwise feature isolation
  3. **Hyperparameter sensitivity**: Test the algorithm's performance across a range of αr and αf values to identify optimal scaling parameters for different datasets and models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed algorithm scale to very large models like GPT-3 or GPT-4?
- Basis in paper: [inferred] The paper demonstrates effectiveness on ViT models and ImageNet, but doesn't test on extremely large language models.
- Why unresolved: The computational complexity of SVD operations and representation collection may become prohibitive for extremely large models with billions of parameters.
- What evidence would resolve it: Results showing successful unlearning on models with 10B+ parameters, or theoretical analysis of computational complexity scaling.

### Open Question 2
- Question: What is the minimum number of samples required for effective unlearning across different datasets and model architectures?
- Basis in paper: [explicit] The paper mentions using very few samples (less than 4% of training data) but doesn't systematically study the relationship between sample size and unlearning effectiveness.
- Why unresolved: The paper shows good results with small sample sizes but doesn't establish a lower bound or analyze how sample size affects performance.
- What evidence would resolve it: A comprehensive study varying the number of samples from very small (1-10) to larger percentages of the training data, showing the trade-off between sample size and unlearning effectiveness.

### Open Question 3
- Question: How does the unlearning algorithm perform when the forget class is highly similar to retain classes?
- Basis in paper: [inferred] The paper shows good results on standard datasets but doesn't test scenarios where classes are semantically similar (e.g., different dog breeds).
- Why unresolved: The algorithm relies on SVD to find class-discriminatory spaces, which may be less effective when classes share many features.
- What evidence would resolve it: Experiments unlearning from datasets with highly similar classes (e.g., fine-grained classification datasets like CUB-200 or Stanford Dogs) showing how performance degrades with class similarity.

### Open Question 4
- Question: Does the unlearning algorithm maintain fairness properties when removing demographic classes?
- Basis in paper: [inferred] The paper focuses on image classification accuracy but doesn't analyze fairness implications of removing certain classes.
- Why unresolved: Unlearning demographic information could potentially affect model fairness on remaining classes, but this aspect is not explored.
- What evidence would resolve it: Analysis of fairness metrics (e.g., demographic parity, equal opportunity) before and after unlearning demographic classes, showing whether biases are introduced or reduced.

## Limitations

- The method's effectiveness depends heavily on the quality of SVD estimation from small sample subsets, which may not capture all class-discriminatory features in complex datasets
- The grid search for optimal hyperparameters adds computational overhead and may not guarantee global optimality
- The algorithm's performance on very large-scale models (e.g., Vision Transformer on ImageNet) suggests potential scalability issues

## Confidence

- **High confidence**: The algorithm's core mechanism of using SVD to estimate class-discriminatory features and projecting weights orthogonally to remove them is well-founded and mathematically sound
- **Medium confidence**: The claim that the method is "fast and efficient" compared to gradient-based approaches is supported by results but may not hold for all model architectures
- **Low confidence**: The assertion that the method is "resilient against Membership Inference Attacks" requires further validation with extensive attack analysis

## Next Checks

1. **Robustness analysis**: Test the algorithm's performance against adaptive membership inference attacks and adversarial samples designed to bypass unlearning, particularly focusing on scenarios where forget and retain class distributions overlap significantly.

2. **Scalability study**: Evaluate the method's computational efficiency and unlearning effectiveness on larger models (e.g., ViT-Large, ResNet50+) and datasets (e.g., ImageNet-1k full training set) to assess its practical applicability in real-world scenarios.

3. **Hyperparameter generalization**: Investigate whether the grid search over αr and αf can be replaced with a more efficient optimization strategy (e.g., Bayesian optimization) or if the optimal parameters transfer across similar datasets and model architectures, reducing the need for extensive search.