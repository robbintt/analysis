---
ver: rpa2
title: 'IW-GAE: Importance Weighted Group Accuracy Estimation for Improved Calibration
  and Model Selection in Unsupervised Domain Adaptation'
arxiv_id: '2310.10611'
source_url: https://arxiv.org/abs/2310.10611
tags:
- accuracy
- group
- estimation
- domain
- iw-gae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel importance weighted group accuracy estimator
  for addressing model calibration and selection tasks in unsupervised domain adaptation.
  The key idea is to estimate the group accuracy in the target domain, which can benefit
  both model calibration and selection tasks.
---

# IW-GAE: Importance Weighted Group Accuracy Estimation for Improved Calibration and Model Selection in Unsupervised Domain Adaptation

## Quick Facts
- arXiv ID: 2310.10611
- Source URL: https://arxiv.org/abs/2310.10611
- Reference count: 40
- Primary result: Improves state-of-the-art by 22% in model calibration and 14% in model selection tasks.

## Executive Summary
This paper introduces Importance Weighted Group Accuracy Estimation (IW-GAE), a novel approach for improving model calibration and selection in unsupervised domain adaptation. The method estimates group accuracy in the target domain by formulating an optimization problem to find importance weights that minimize estimation error. Theoretical analyses and extensive experiments demonstrate that IW-GAE significantly outperforms existing methods, achieving substantial improvements in both calibration error reduction and model selection accuracy across benchmark datasets.

## Method Summary
IW-GAE estimates target domain group accuracy by optimizing importance weights between source and target domains. The method groups samples by maximum softmax confidence to reduce variance, then solves a nonlinear optimization problem to find weights that minimize source domain group accuracy estimation error. A temperature scaling parameter is introduced to adapt group construction to domain-specific calibration differences. The approach uses labeled source data and unlabeled target data, computing binned importance weights with confidence intervals, constructing accuracy groups, solving an optimization to refine weights, and outputting group accuracy estimates for calibration and model selection tasks.

## Key Results
- Achieves 22% improvement over state-of-the-art in model calibration tasks
- Delivers 14% improvement over state-of-the-art in model selection tasks
- Demonstrates robust performance across Office-Home dataset with 15,000 images from 65 categories
- Shows effectiveness of grouping by softmax confidence and nested temperature optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The optimization problem reduces source group accuracy estimation error, which bounds target accuracy error
- **Mechanism**: Optimization finds importance weights making source domain group accuracy estimators agree, with target accuracy error bounded by source error scaled by domain mismatch terms
- **Core assumption**: Identical accuracy assumption (constant accuracy within groups) and importance weight within estimated confidence interval
- **Evidence anchors**: Abstract states optimization leads to accurate group accuracy estimation with theoretical analyses; theoretical analysis shows optimization results in accurate target domain estimator
- **Break condition**: Fails when identical accuracy assumption breaks down (high variance within groups)

### Mechanism 2
- **Claim**: Grouping by maximum softmax value reduces variance of prediction correctness within groups
- **Mechanism**: Proposition 4.2 shows bias decomposition into bias and variance terms; grouping samples with similar confidence scores reduces variance term
- **Core assumption**: Maximum softmax value highly correlated with true prediction accuracy
- **Evidence anchors**: Abstract mentions constructing M groups and estimating average accuracy of each group; section justifies grouping by maximum softmax output based on empirical correlation with accuracy
- **Break condition**: Loses benefit when correlation between softmax confidence and accuracy breaks down

### Mechanism 3
- **Claim**: Nested optimization with temperature scaling adapts group construction to domain-specific calibration differences
- **Mechanism**: Different domains have systematically different softmax output scales; optimizing over temperature parameters finds scaling that aligns group construction across domains
- **Core assumption**: Optimal temperature for group construction lies within explored finite set
- **Evidence anchors**: Abstract introduces temperature scale parameter adjusting softmax output sharpness; section explains temperature parameter for target samples
- **Break condition**: Loses benefit if true optimal temperature not in explored set or temperature scaling doesn't meaningfully align group boundaries

## Foundational Learning

- **Concept**: Importance weighting under covariate shift
  - **Why needed here**: Method relies on reweighting source domain samples to estimate target domain quantities; optimization and bounds depend on understanding importance weighting
  - **Quick check question**: What is the importance weight formula for estimating an expectation in target domain using source data under covariate shift?

- **Concept**: Bias-variance decomposition of estimators
  - **Why needed here**: Analysis of group accuracy estimation error depends on decomposing error into bias and variance components to justify grouping strategies
  - **Quick check question**: How does bias-variance decomposition explain why group accuracy estimator can outperform individual accuracy estimator?

- **Concept**: Conformal prediction and confidence intervals
  - **Why needed here**: Method uses confidence intervals for binned importance weights to guide optimization; understanding construction is critical
  - **Quick check question**: How is Clopper-Pearson interval constructed for binomial proportion, and why used for binned probability estimation?

## Architecture Onboarding

- **Component map**: Source data → feature extraction → domain classifier → IW CI estimation → group construction → optimization → target group accuracy estimation
- **Critical path**: Source data → feature extraction → domain classifier → IW CI estimation → group construction → optimization → target group accuracy estimation
- **Design tradeoffs**: Grouping by softmax confidence vs other features trades estimation bias for variance; nested temperature scaling adds computation but may improve alignment; nonlinear optimization more expensive than direct estimation but yields tighter bounds
- **Failure signatures**: High ECE on target data despite optimization suggests group construction or IW estimation failing; large variance within groups indicates poor grouping; optimizer failing to converge suggests ill-conditioned problem or poor initialization
- **First 3 experiments**:
  1. Verify grouping by softmax confidence reduces variance within groups compared to random grouping
  2. Check solving optimization problem reduces source group accuracy estimation error as predicted by Proposition 4.1
  3. Test sensitivity of calibration performance to number of groups M and bins B to confirm robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does number of samples in source and target domains affect IW-GAE performance, particularly quality of confidence interval estimation for importance weights?
- **Basis in paper**: [inferred] Paper mentions advancements in importance weight estimation or confidence interval estimation would benefit IW-GAE, but provides no experimental results on sample size impact
- **Why unresolved**: Paper focuses on effectiveness under fixed sample sizes without exploring performance scaling with different data amounts
- **What evidence would resolve it**: Experiments varying source and target samples, showing impact on IW-GAE's calibration and selection performance

### Open Question 2
- **Question**: What is optimal group construction function for minimizing bias of identical accuracy assumption in IW-GAE?
- **Basis in paper**: [explicit] Paper discusses importance of group construction for reducing variance of prediction correctness within group and preventing misspecification of accuracy within group
- **Why unresolved**: Paper uses maximum softmax output as grouping criterion but acknowledges finding optimal group function is combinatorial optimization problem
- **What evidence would resolve it**: Development of method to learn optimal group function or comprehensive comparison of different group construction methods on IW-GAE performance

### Open Question 3
- **Question**: How does IW-GAE perform when applied to pre-trained large language models, and what are reasons for effectiveness or lack thereof?
- **Basis in paper**: [explicit] Paper mentions applying IW-GAE and other baselines to large language models (XLM-R and GPT-2) didn't result in improvements, conjecturing pre-trained models less subject to distribution shifts
- **Why unresolved**: Paper doesn't provide detailed analysis of why large language models less affected by distribution shifts or how IW-GAE could be adapted for such models
- **What evidence would resolve it**: Experiments applying IW-GAE to various pre-trained models on domain adaptation tasks, along with analysis of distribution shift characteristics and model's internal representations

## Limitations
- Relies on strong identical accuracy assumption that may break down for complex datasets with heterogeneous subgroups
- Method requires estimating confidence intervals for binned importance weights, which becomes unstable when bin counts are small
- Theoretical bounds depend on domain mismatch terms that are difficult to quantify in practice

## Confidence
- **High confidence**: Core mechanism of importance weighting for domain adaptation is well-established, and general approach of grouping by confidence for calibration has empirical support
- **Medium confidence**: Theoretical analysis connecting source error bounds to target accuracy estimation is sound, but practical tightness of bounds depends heavily on validity of identical accuracy assumption
- **Low confidence**: Empirical claims of 22% improvement in calibration and 14% in model selection are difficult to verify without access to exact implementation details and codebase

## Next Checks
1. **Sensitivity analysis**: Systematically vary number of groups M and bins B to test robustness of calibration improvements across different granularities
2. **Assumption validation**: Quantify variance within groups for different grouping strategies to empirically validate identical accuracy assumption on held-out data
3. **Ablation study**: Compare performance with and without nested temperature optimization to isolate its contribution to reported improvements