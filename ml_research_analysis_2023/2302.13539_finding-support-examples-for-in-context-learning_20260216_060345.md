---
ver: rpa2
title: Finding Support Examples for In-Context Learning
arxiv_id: '2302.13539'
source_url: https://arxiv.org/abs/2302.13539
tags:
- examples
- learning
- in-context
- random
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting informative and
  representative examples for in-context learning (ICL) in language models. The key
  issue is that ICL performance is highly sensitive to the choice of examples, and
  randomly sampled examples often yield unstable results.
---

# Finding Support Examples for In-Context Learning

## Quick Facts
- arXiv ID: 2302.13539
- Source URL: https://arxiv.org/abs/2302.13539
- Authors: 
- Reference count: 37
- One-line primary result: LENS significantly outperforms baselines in finding informative and representative examples for in-context learning

## Executive Summary
This paper addresses the challenge of selecting informative and representative examples for in-context learning (ICL) in language models. The key issue is that ICL performance is highly sensitive to the choice of examples, and randomly sampled examples often yield unstable results. The proposed solution, LENS (FiLter-thEN-Search), is a two-stage method. First, it filters the dataset to obtain individually informative examples using a novel metric called InfoScore, which evaluates informativeness based on the language model's feedback. This is followed by a progressive filtering strategy to remove uninformative examples. In the second stage, a diversity-guided beam search method iteratively refines and evaluates the selected example permutations to find those that fully depict the task. Experimental results show that LENS significantly outperforms a wide range of baselines, including methods designed for gradient-based learning scenarios. The method is particularly effective in finding examples that are less sensitive to ordering and better transferable across different language models.

## Method Summary
LENS is a two-stage method for finding supporting examples in in-context learning. The first stage uses progressive filtering with a novel InfoScore metric to identify individually informative examples from the dataset. The second stage employs diversity-guided beam search to refine and evaluate permutations of the filtered examples, selecting those that are both informative and diverse. The method is evaluated on multiple text classification tasks and shows significant improvements over baselines in terms of accuracy, stability, and transferability.

## Key Results
- LENS outperforms a wide range of baselines, including coreset selection methods designed for gradient-based learning
- The method is effective in finding examples that are less sensitive to ordering and better transferable across different language models
- Ground truth labels are more important for supporting examples than for randomly sampled examples
- The superiority of one model's supporting examples can be well transferred to other models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LENS uses InfoScore to quantify how much each example helps the model classify other examples
- Mechanism: InfoScore measures the difference between P(y'|x,y,x') and P(y'|x'), where the first probability includes the conditioning example (x,y) and the second doesn't. This gap represents the contribution of (x,y) to correctly classifying x'
- Core assumption: Language models' probability outputs can serve as meaningful signals about example informativeness
- Evidence anchors:
  - [abstract]: "we propose a novel metric, InfoScore, to evaluate the example's in-context informativeness based on the language model's feedback"
  - [section 3.1]: "Eq (3) is the gap between the probabilities of the ground truth y' conditioned on (e,x') and (x'), respectively. It evaluates how helpful e is for LM to correctly classify x' and thus measures e's contribution for x' on ICL"
  - [corpus]: Weak - only 5 related papers found, none specifically about InfoScore or this specific metric formulation

### Mechanism 2
- Claim: Progressive filtering reduces computational cost while maintaining effectiveness
- Mechanism: Start with small score set S, evaluate all examples, filter top 1/ρ fraction, then expand S by ρ× and repeat. This way, examples get more refined evaluation only if they survive previous rounds
- Core assumption: Examples that are uninformative in small-scale evaluation will also be uninformative in larger evaluation
- Evidence anchors:
  - [section 3.1]: "we propose a progressive scheme where promising examples receive more computational cost while low-quality examples get less computation"
  - [algorithm 1]: Shows the iterative expansion of S and progressive filtering logic
  - [corpus]: Weak - no direct evidence about progressive filtering approaches in related work

### Mechanism 3
- Claim: Diversity-guided beam search finds permutations that are both informative and complementary
- Mechanism: Iteratively replace examples in current permutations with those that maximize InfoScore minus λ times similarity to existing examples, where similarity is measured by InfoScore vectors across the score set
- Core assumption: Examples with similar InfoScore vectors across the score set provide redundant information
- Evidence anchors:
  - [section 3.2]: "we propose a diversity-guided beam search method to refine and evaluate the selected examples, iteratively"
  - [equation 4]: Shows the diversity term λ∑sim(f(e),f(e')) that penalizes similar examples
  - [corpus]: Weak - related papers focus on retrieval or ordering but not this specific diversity-guided approach

## Foundational Learning

- Concept: In-context learning vs fine-tuning distinction
  - Why needed here: The paper explicitly states that methods designed for gradient-based learning (fine-tuning) don't work for ICL because ICL doesn't use parameter updates
  - Quick check question: Why can't we just use coreset selection methods designed for fine-tuning?

- Concept: Combinatorial explosion in example selection
  - Why needed here: With 50 candidates and 8 examples needed, there are C(50,8) ≈ 536 million combinations, and order matters, making exhaustive search impossible
  - Quick check question: If we had only 10 candidates and needed 3 examples, how many ordered permutations would we need to evaluate?

- Concept: Submodular functions for diversity
  - Why needed here: The paper mentions submodular functions in related work and uses a diversity term that has submodular properties (diminishing returns)
  - Quick check question: If adding a highly similar example to a set provides little additional value, what property of submodular functions does this illustrate?

## Architecture Onboarding

- Component map: Progressive filtering pipeline -> Diversity-guided beam search -> Evaluation
- Critical path:
  1. Compute initial InfoScore on small score set
  2. Filter top fraction
  3. Expand score set and repeat until m examples remain
  4. Initialize beam search with diverse permutations
  5. Iteratively refine via diversity-guided substitutions
  6. Evaluate on validation set and keep top B
- Design tradeoffs:
  - Progressive filtering vs one-shot evaluation: Saves computation but risks aggressive early filtering
  - Diversity penalty λ: Higher λ increases diversity but may reduce overall informativeness
  - Beam size B: Larger B explores more but increases computation quadratically
- Failure signatures:
  - Poor performance despite high InfoScore: Diversity term may be too low or similarity metric may be flawed
  - Extremely slow runtime: Progressive factor ρ may be too small, causing too many iterations
  - Sensitivity to ordering remains: Beam search may not be exploring enough permutations
- First 3 experiments:
  1. Run stage 1 only (progressive filtering) on SST-2 with m=500, compare to random baseline
  2. Run stage 2 only with fixed candidate set from random sampling, test ordering sensitivity
  3. Test cross-LLM transfer: Use GPT2-L supporting examples on GPT2-XL and GPT-Neo-2.7B, compare to their random examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LENS compare to other coreset selection methods when using larger language models (e.g., GPT-3 or GPT-4)?
- Basis in paper: [inferred] The paper primarily uses GPT-2-L for experiments and explores transferability to other LMs like GPT-2-M, GPT-2-XL, and GPT-Neo-2.7B. However, it does not evaluate LENS on significantly larger models.
- Why unresolved: The paper focuses on demonstrating the effectiveness of LENS on a range of tasks and models, but does not explore its performance on the latest, largest language models.
- What evidence would resolve it: Conducting experiments with LENS on GPT-3 or GPT-4 and comparing its performance to other coreset selection methods would provide insights into its scalability and effectiveness with larger models.

### Open Question 2
- Question: How does the diversity-guided beam search method in LENS perform compared to other search strategies, such as genetic algorithms or reinforcement learning?
- Basis in paper: [inferred] The paper introduces a diversity-guided beam search method as part of LENS, but does not compare it to other search strategies.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the diversity-guided beam search method, but does not explore how it compares to other search strategies in terms of efficiency and performance.
- What evidence would resolve it: Implementing and comparing LENS with other search strategies, such as genetic algorithms or reinforcement learning, would provide insights into the relative strengths and weaknesses of different search approaches for finding supporting examples.

### Open Question 3
- Question: How does the performance of LENS change when applied to different types of NLP tasks, such as machine translation, summarization, or question answering?
- Basis in paper: [explicit] The paper focuses on text classification tasks and demonstrates the effectiveness of LENS in this domain.
- Why unresolved: The paper does not explore the applicability and performance of LENS on other NLP tasks beyond text classification.
- What evidence would resolve it: Applying LENS to other NLP tasks, such as machine translation, summarization, or question answering, and evaluating its performance would provide insights into its generalizability and effectiveness across different task types.

## Limitations
- The method's computational overhead is significant due to the progressive filtering and beam search approach
- The generalizability of LENS across different task types and model architectures is not fully explored
- The reliability of the InfoScore metric is evaluated only against random baselines, with no comparison to alternative metrics

## Confidence
- High confidence: Claims about LENS outperforming baselines on tested datasets
- Medium confidence: Mechanism explanations for why InfoScore works
- Low confidence: Transferability claims beyond tested LLMs

## Next Checks
1. Apply LENS to at least 3 new task types not in the original study to assess generalization
2. Implement and compare InfoScore against alternative example quality metrics
3. Evaluate LENS on progressively larger datasets and with longer example permutations to understand scaling behavior