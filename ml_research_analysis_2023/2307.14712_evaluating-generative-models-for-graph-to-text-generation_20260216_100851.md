---
ver: rpa2
title: Evaluating Generative Models for Graph-to-Text Generation
arxiv_id: '2307.14712'
source_url: https://arxiv.org/abs/2307.14712
tags:
- text
- generative
- generation
- graph
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates generative models, specifically GPT-3 and
  ChatGPT, for graph-to-text generation in a zero-shot setting. The authors compare
  the performance of these models with finetuned models like T5 and BART on two datasets:
  AGENDA and WebNLG.'
---

# Evaluating Generative Models for Graph-to-Text Generation

## Quick Facts
- arXiv ID: 2307.14712
- Source URL: https://arxiv.org/abs/2307.14712
- Reference count: 13
- Generative models like GPT-3 and ChatGPT can produce fluent text from graphs but underperform fine-tuned models like T5 and BART in graph-to-text tasks.

## Executive Summary
This paper evaluates large language models (GPT-3 and ChatGPT) for graph-to-text generation in a zero-shot setting, comparing their performance to fine-tuned models on AGENDA and WebNLG datasets. While generative models can produce fluent and coherent text, they fall short of state-of-the-art results achieved by models like T5 and BART. Error analysis reveals significant challenges in understanding semantic relations between entities and a tendency to generate hallucinations or irrelevant information. The study also demonstrates that BERT-based classification can effectively detect machine-generated text with high macro-F1 scores.

## Method Summary
The study uses a zero-shot approach where graph structures are linearized using `<H>`, `<R>`, `<T>` tokens and fed into GPT-3 and ChatGPT with prompts generated by ChatGPT. Generated texts are evaluated using BLEU, METEOR, ROUGE, and BLEURT metrics against reference texts. Error analysis includes manual inspection and BERT-based binary classification (machine vs. human text) trained on an 80:20 split of the datasets.

## Key Results
- GPT-3 and ChatGPT achieve BLEU scores of 10.57 (AGENDA) and 11.08 (WebNLG) in zero-shot graph-to-text generation.
- Generative models produce fluent but semantically inaccurate text, struggling with entity relations and generating hallucinations.
- BERT-based detection of machine-generated text achieves high macro-F1 scores, indicating consistent patterns in LLM outputs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative models can produce fluent, coherent text from graph-structured data when provided with linearized graph inputs.
- Mechanism: The models use the transformer decoder architecture to attend over token sequences that encode graph structure (via `<H>`, `<R>`, `<T>` tags). The pretraining on large corpora provides linguistic fluency, enabling the models to generate readable text even without finetuning.
- Core assumption: The linearized graph representation preserves sufficient structural information for the model to infer entity relationships and produce semantically appropriate text.
- Evidence anchors:
  - [abstract]: "Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively."
  - [section 4]: "Since GPT-3 and ChatGPT require a sequence of text as input, we convert the graph structure into a linearized representation following Ribeiro et al. (2021a)."
  - [corpus]: FMR scores indicate moderate semantic overlap with related graph-to-text literature, suggesting the approach is contextually aligned.
- Break condition: If the linearization omits or misrepresents key graph edges, the model cannot infer correct semantic relations, leading to hallucinations or factual errors.

### Mechanism 2
- Claim: Zero-shot performance is limited because generative models lack explicit training on graph-to-text alignment.
- Mechanism: Without finetuning, the models must rely on implicit knowledge from pretraining. This leads to fluent but potentially inaccurate text, as the model cannot learn dataset-specific graph-to-text mappings.
- Core assumption: The zero-shot setting does not provide sufficient task-specific guidance for the model to accurately capture entity relationships in structured data.
- Evidence anchors:
  - [abstract]: "However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information."
  - [section 5]: "Generative models still face challenges in comprehending the semantic relations between entities, which can result in the generation of inaccurate text."
  - [corpus]: Neighbor papers focus on finetuning or specialized architectures, indicating that zero-shot is generally insufficient for graph-to-text tasks.
- Break condition: If the model is finetuned on aligned graph-text pairs, the performance gap narrows, as shown by the baseline T5/BART results.

### Mechanism 3
- Claim: BERT-based classification can reliably detect machine-generated text from graph inputs.
- Mechanism: Finetuning BERT on a binary classification task (machine vs. human text) leverages the model's ability to capture subtle distributional differences in fluency, coherence, and factual consistency.
- Core assumption: Machine-generated texts from the same model follow consistent patterns that are distinguishable from human-written references.
- Evidence anchors:
  - [abstract]: "As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores."
  - [section 5]: "We create several datasets for AGENDA, WebNLG, and a combined dataset... BERT achieves high F1 scores in distinguishing between machine-generated text and human-written text."
  - [corpus]: No direct evidence, but high FMR with similar detection tasks suggests alignment.
- Break condition: If the generative model's outputs become more human-like (e.g., through advanced finetuning), the classification accuracy may degrade.

## Foundational Learning

- Concept: Linearization of graph structures into token sequences
  - Why needed here: Generative models require sequential text input; linearization preserves graph topology for model consumption.
  - Quick check question: What tokens are used to denote head, relation, and tail entities in the linearized graph?

- Concept: Zero-shot prompting and instruction tuning
  - Why needed here: Enables evaluation of generative models without finetuning, isolating pretraining effects on graph-to-text tasks.
  - Quick check question: How does the prompt differ between AGENDA and WebNLG, and why?

- Concept: Evaluation metrics for text generation (BLEU, METEOR, ROUGE, BLEURT)
  - Why needed here: Quantifies fluency, coherence, and semantic fidelity of generated text against references.
  - Quick check question: Which metric best captures semantic meaning beyond surface n-gram overlap?

## Architecture Onboarding

- Component map: Data preprocessing -> Linearized graph construction (H/R/T tokens) -> Model inference (GPT-3/ChatGPT) -> Evaluation pipeline (metric computation) -> Error analysis (manual + BERT classification)

- Critical path: 1. Load and linearize graph data 2. Send prompt + linearized graph to generative model 3. Collect and store generated text 4. Compute evaluation metrics 5. Conduct error analysis (manual + BERT classification)

- Design tradeoffs:
  - Zero-shot vs. finetuning: Speed and resource efficiency vs. higher accuracy
  - Linearization vs. graph encoders: Simplicity vs. richer structural representation
  - Metric selection: Surface match (BLEU) vs. semantic match (BLEURT)

- Failure signatures:
  - Low BLEU/ROUGE but high BLEURT: Fluent but off-topic text
  - Consistent misclassification by BERT: Systematic pattern in hallucinations
  - High variance across prompts: Sensitivity to instruction phrasing

- First 3 experiments:
  1. Compare linearized vs. non-linearized input formats on GPT-3 output quality.
  2. Test multiple prompt phrasings to assess sensitivity and optimal instruction design.
  3. Evaluate BERT classifier on a held-out test set to measure detection reliability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of generative models in capturing semantic relations between entities in graph-to-text generation?
- Basis in paper: [explicit] The paper states that generative models struggle with understanding semantic relations between entities, leading to the generation of inaccurate text.
- Why unresolved: The paper does not provide detailed insights into the specific types of semantic relations that generative models fail to capture or the reasons behind their inability to comprehend these relations.
- What evidence would resolve it: Further analysis and experiments focusing on different types of semantic relations (e.g., cause-effect, part-whole) and investigating the underlying reasons for generative models' failures in capturing these relations would help resolve this question.

### Open Question 2
- Question: How can the performance of generative models in graph-to-text generation be improved without fine-tuning?
- Basis in paper: [explicit] The paper highlights the limitations of generative models in achieving state-of-the-art performance in graph-to-text generation tasks without fine-tuning.
- Why unresolved: The paper does not explore potential techniques or approaches to enhance the performance of generative models in graph-to-text generation without fine-tuning.
- What evidence would resolve it: Investigating and experimenting with different strategies, such as data augmentation, transfer learning, or architecture modifications, to improve the performance of generative models in graph-to-text generation without fine-tuning would help address this question.

### Open Question 3
- Question: What are the factors contributing to the hallucinations and generation of irrelevant information by generative models in graph-to-text generation?
- Basis in paper: [explicit] The paper mentions that generative models tend to generate text with hallucinations or irrelevant information.
- Why unresolved: The paper does not delve into the underlying causes or factors that lead to the generation of hallucinations and irrelevant information by generative models.
- What evidence would resolve it: Conducting in-depth analysis and experiments to identify the specific factors, such as model architecture, training data, or prompt design, that contribute to the generation of hallucinations and irrelevant information would help resolve this question.

## Limitations
- Generative models struggle with understanding semantic relations between entities, leading to inaccurate text generation.
- Zero-shot performance is significantly lower than fine-tuned models like T5 and BART.
- The study relies on manual inspection for error analysis without systematic hallucination quantification.

## Confidence
- BLEU/METEOR/ROUGE/BLEURT score comparisons: **High**
- BERT-based detection mechanism: **Medium**
- Error analysis conclusions: **Medium**

## Next Checks
1. Replicate the BERT classification with multiple random seeds to assess stability of macro-F1 scores.
2. Compare linearization variants (e.g., edge-first vs. node-first) to test sensitivity to input format.
3. Evaluate on an additional graph-to-text dataset (e.g., MReD) to check generalization of the observed limitations.