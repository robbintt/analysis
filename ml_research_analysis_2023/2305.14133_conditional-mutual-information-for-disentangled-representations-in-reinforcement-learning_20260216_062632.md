---
ver: rpa2
title: Conditional Mutual Information for Disentangled Representations in Reinforcement
  Learning
arxiv_id: '2305.14133'
source_url: https://arxiv.org/abs/2305.14133
tags:
- learning
- training
- features
- representation
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of learning robust, disentangled
  representations in RL when training data contains spurious correlations between
  features. Standard disentanglement techniques assume independent features and fail
  when correlations exist.
---

# Conditional Mutual Information for Disentangled Representations in Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2305.14133
- **Source URL**: https://arxiv.org/abs/2305.14133
- **Reference count**: 34
- **Primary result**: CMID achieves up to 77% improvement in generalization returns compared to baselines under correlation shifts in continuous control tasks.

## Executive Summary
This paper addresses the challenge of learning robust, disentangled representations in reinforcement learning when training data contains spurious correlations between features. Standard disentanglement techniques assume independent features and fail when correlations exist. The authors propose Conditional Mutual Information for Disanglement (CMID), an auxiliary task that minimizes conditional mutual information between representation features using an adversarial approach. By leveraging the temporal structure of MDPs and conditioning on recent representations and actions, CMID achieves conditional independence even when features are correlated in the data. Experiments on continuous control tasks from the DeepMind Control Suite demonstrate that CMID significantly improves both training performance and zero-shot generalization under correlation shifts compared to baselines like SVEA, DrQ, CURL, and TED.

## Method Summary
CMID minimizes conditional mutual information (CMI) between representation features to achieve conditional independence in RL, even when features are correlated in the data. The approach uses an adversarial discriminator to distinguish between true joint samples of representation features and their k-nearest neighbor permutations, effectively minimizing the KL divergence between the joint and product of marginals distributions conditioned on recent history and actions. The conditioning set leverages the MDP's temporal structure, using recent representations and actions to block backdoor paths between features. This allows CMID to prevent the encoding of spurious correlations in the representation, improving both training performance and generalization under correlation shifts.

## Key Results
- CMID achieves up to 77% improvement in generalization returns compared to baselines under correlation shifts
- Saliency map analysis reveals CMID learns features that focus on individual image aspects, unlike baselines that entangle multiple features
- CMID consistently improves generalization of the base RL algorithm across all tasks tested
- CMID outperforms other baselines including SVEA, DrQ, CURL, and TED in zero-shot generalization tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CMID minimizes conditional mutual information (CMI) between representation features to achieve conditional independence, even when the features are correlated in the data.
- **Mechanism**: The approach trains a discriminator to distinguish between true joint samples of representation features and their k-nearest neighbor permutations. This adversarial training pushes the encoder to make the two distributions indistinguishable, effectively minimizing the KL divergence between the joint and the product of marginals distributions conditioned on recent history and actions.
- **Core assumption**: The causal graph of an MDP allows the conditioning set (recent representation features and actions) to block backdoor paths between features, making them conditionally independent.
- **Evidence anchors**: [abstract], [section], [corpus]

### Mechanism 2
- **Claim**: Using k-nearest neighbor (kNN) permutations provides an efficient way to sample from the product of marginals distribution required for CMI estimation.
- **Mechanism**: For each representation sample, the approach finds kNN of the conditioning set in the batch by Euclidean distance, then permutes the sample with the kNN to create a sample from the product of marginals distribution while keeping the conditioning set and one feature fixed.
- **Core assumption**: The kNN permutation preserves the marginal distributions while breaking the joint dependence between features given the conditioning set.
- **Evidence anchors**: [section], [corpus]

### Mechanism 3
- **Claim**: CMID improves both training performance and generalization under correlation shifts by preventing the encoding of spurious correlations in the representation.
- **Mechanism**: By minimizing CMI between representation features, CMID ensures that each feature captures only information relevant to the task that is not redundant given the conditioning set. This prevents features from encoding spurious correlations between task-irrelevant factors (like color) and task-relevant factors (like dynamics).
- **Core assumption**: When spurious correlations are removed from the representation, the agent can learn policies that generalize across correlation shifts without relying on these correlations.
- **Evidence anchors**: [abstract], [section], [corpus]

## Foundational Learning

- **Concept**: Markov Decision Process (MDP) structure and causal graphs
  - **Why needed here**: Understanding the MDP structure is crucial for determining the appropriate conditioning set that can render features conditionally independent. The causal graph shows which variables are direct causes of representation features and which backdoor paths need to be blocked.
  - **Quick check question**: Given a causal graph of an MDP, can you identify the backdoor paths between two representation features and determine a sufficient conditioning set to block them?

- **Concept**: Conditional Mutual Information (CMI) and its relationship to conditional independence
  - **Why needed here**: CMI measures the amount of information one random variable contains about another given a third variable. Minimizing CMI between representation features given a conditioning set achieves conditional independence, which is the goal of CMID.
  - **Quick check question**: If I(zn_t ; z−n_t | cn_t) = 0, what does this tell us about the relationship between zn_t and z−n_t given cn_t?

- **Concept**: k-Nearest Neighbors (kNN) permutation sampling for product of marginals
  - **Why needed here**: This technique provides an efficient way to generate samples from the product of marginals distribution required for CMI estimation, without needing to know the true marginal distributions.
  - **Quick check question**: How does the kNN permutation approach ensure that the permuted sample {zn_t, z−n_t′, cn_t} is from the distribution p(zn_t, cn_t)p(z−n_t | cn_t)?

## Architecture Onboarding

- **Component map**: Encoder (fθ) -> Momentum encoder (fθ′) -> Discriminator (Dϕ) -> Base RL algorithm (SVEA)
- **Critical path**:
  1. Encode current observation and previous observation (via momentum encoder)
  2. For each feature dimension, create conditioning set from previous representation and action
  3. Find kNN permutations and compute discriminator loss
  4. Update discriminator parameters
  5. Compute adversarial loss for encoder and update encoder parameters
  6. Compute RL loss and update RL networks

- **Design tradeoffs**:
  - Conditioning set size: Larger conditioning sets provide stronger guarantees of conditional independence but are harder to learn and computationally expensive
  - kNN value: Higher k values provide more diverse permutations but may include less relevant samples
  - Loss coefficient α: Higher values prioritize disentanglement over RL performance, potentially slowing down policy learning

- **Failure signatures**:
  - Discriminator loss plateaus at a high value: The encoder may not be learning to minimize CMI effectively
  - RL performance degrades: The CMI minimization may be too aggressive, harming task-relevant feature learning
  - Generalization performance remains poor: The conditioning set may be insufficient to block all backdoor paths

- **First 3 experiments**:
  1. Verify that CMID reduces CMI between representation features on a simple synthetic dataset with known correlations
  2. Test CMID with different conditioning set sizes (1, 2, 3 timesteps) to find the optimal tradeoff between performance and computational cost
  3. Compare CMID's generalization performance against baselines when training on strongly correlated data and testing on uncorrelated data

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CMID performance scale with more complex environments and longer temporal correlations (e.g., background videos)?
  - **Basis in paper**: [inferred] "As such, the colour correlations are sufficient to demonstrate the effectiveness of our approach... However, future work could evaluate our approach on correlations with more complex distractors, such as background videos and camera angles. In particular, we use a conditioning set containing only the most recent timestep in our experiments, but more complex environments can have strong correlations over multiple timesteps (e.g. background videos) which may require more history in the conditioning set."
  - **Why unresolved**: The paper only evaluates on tasks with correlations between object properties and color, not on more complex temporal correlations that might require longer conditioning histories.
  - **What evidence would resolve it**: Experiments applying CMID to environments with background videos or other multi-timestep correlations, showing whether increasing conditioning history length improves performance.

- **Open Question 2**: Can CMID be adapted to work with frame stacking while maintaining disentanglement?
  - **Basis in paper**: [explicit] "Instead of the common practice of frame stacking, we stack representations when using CMID to avoid introducing causal relationship between variables in the stack of frames as discussed in Section 4.2. Future work could consider how to adapt CMID to allow for these more complex causal relationships."
  - **Why unresolved**: The paper explicitly avoids frame stacking due to causal relationships between stacked frames, but this is a common practice in RL that could improve performance.
  - **What evidence would resolve it**: A modified CMID approach that successfully incorporates frame stacking while maintaining conditional independence between features.

- **Open Question 3**: What is the most efficient method to sample from the product of marginals distribution for minimizing conditional mutual information?
  - **Basis in paper**: [explicit] "The kNN permutations approach to minimise CMI also adds computational complexity to update the encoder for each kNN, so future work could consider more efficient ways to sample from the product of marginals distribution."
  - **Why unresolved**: The paper uses k-nearest neighbors permutations which adds computational overhead, but doesn't explore alternative sampling methods.
  - **What evidence would resolve it**: Comparison of CMID performance using different sampling methods for the product of marginals (e.g., importance sampling, GAN-based approaches) showing reduced computational cost while maintaining or improving disentanglement quality.

## Limitations

- The paper's reliance on k-nearest neighbor permutations for CMI estimation may be sensitive to batch size and representation dimensionality, though specific thresholds are not provided
- The assumption that a 1-step conditioning set (previous representation and action) is sufficient to block all backdoor paths in complex MDPs is not empirically validated
- The trade-off between CMI minimization strength and RL performance preservation lacks systematic exploration

## Confidence

- **High confidence**: The mechanism of CMI minimization for achieving conditional independence given appropriate conditioning
- **Medium confidence**: The kNN permutation approach for efficient CMI estimation
- **Medium confidence**: The improvement in zero-shot generalization under correlation shifts, though absolute performance gains vary across tasks

## Next Checks

1. **Conditioning set ablation study**: Systematically test CMID with conditioning sets of varying temporal depth (1-3 timesteps) to identify the optimal balance between conditional independence and learning efficiency
2. **Batch size sensitivity analysis**: Evaluate how different batch sizes affect the quality of kNN permutations and subsequent CMI estimation accuracy
3. **Correlation strength scaling**: Test CMID's performance across a spectrum of correlation strengths (from weak to strong spurious correlations) to determine the method's breaking point