---
ver: rpa2
title: 'HyMNet: a Multimodal Deep Learning System for Hypertension Classification
  using Fundus Photographs and Cardiometabolic Risk Factors'
arxiv_id: '2310.01099'
source_url: https://arxiv.org/abs/2310.01099
tags:
- hypertension
- fundus
- learning
- deep
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of improving hypertension detection
  by leveraging multimodal deep learning, combining fundus images with cardiometabolic
  risk factors (age and gender). The proposed HyMNet system integrates a DenseNet-201
  architecture for retinal image analysis and a fully connected neural network for
  demographic data, jointly trained through feature fusion.
---

# HyMNet: a Multimodal Deep Learning System for Hypertension Classification using Fundus Photographs and Cardiometabolic Risk Factors

## Quick Facts
- arXiv ID: 2310.01099
- Source URL: https://arxiv.org/abs/2310.01099
- Reference count: 40
- Primary result: Multimodal model combining fundus images with age and gender achieves F1 0.771 and AUC 0.791 for hypertension classification

## Executive Summary
This study presents HyMNet, a multimodal deep learning system that integrates retinal fundus images with demographic risk factors (age and gender) to improve hypertension detection. The system combines a DenseNet-201 architecture for analyzing retinal vasculature with a fully connected neural network for processing demographic data, jointly trained through feature fusion. Evaluated on 1,143 retinal images from 626 individuals, HyMNet demonstrates superior performance compared to unimodal approaches, achieving an AUC of 0.791 versus 0.766 for fundus images alone.

## Method Summary
HyMNet integrates retinal fundus images with demographic data through a multimodal deep learning architecture. The system uses DenseNet-201 with ImageNet pretraining to extract vascular features from fundus images, while a fully connected neural network processes age and gender data. These feature streams are concatenated and fed through a fusion network before binary classification. The model was trained using binary cross-entropy loss with Adam optimization, employing early stopping and data augmentation including random rotations, horizontal flips, and Gaussian blur. The dataset comprised 1,143 retinal images from 626 individuals (309 hypertensive, 317 non-hypertensive) from the Saudi Ministry of National Guard Health Affairs.

## Key Results
- HyMNet achieved F1-score of 0.771 [0.747, 0.796] and AUC of 0.791 [0.735, 0.848]
- Outperformed unimodal fundus-only model (F1: 0.745 [0.719, 0.772], AUC: 0.766 [0.705, 0.828])
- Demonstrated added value of incorporating demographic factors for enhanced hypertension classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HyMNet improves hypertension classification by combining retinal vascular features with demographic predictors
- Mechanism: DenseNet-201 captures microvascular changes in fundus images while FCNN encodes sociodemographic risk signals; concatenation merges complementary evidence streams
- Core assumption: Microvascular retinal alterations and demographic patterns are conditionally independent risk indicators for hypertension
- Evidence anchors: Abstract states multimodal model achieved AUC of 0.791; paper notes hypertension develops later in life with different patterns between males and females
- Break condition: If age and gender distributions are uniform across hypertensive/non-hypertensive groups, demographic path adds no predictive signal

### Mechanism 2
- Claim: Pretrained DenseNet-201 weights reduce overfitting on small retinal datasets
- Mechanism: ImageNet pretraining supplies rich low-level visual filters that generalize to fundus vascular patterns, accelerating convergence
- Core assumption: Vascular textures in fundus images share enough low-level visual statistics with ImageNet objects to benefit from transfer learning
- Evidence anchors: Paper employed DenseNet-201 with ImageNet weights and froze convolutional layers during training
- Break condition: If fundus image characteristics differ drastically from natural images, frozen ImageNet features may be irrelevant

### Mechanism 3
- Claim: Concatenation of deep features outperforms late fusion (classifier-level fusion)
- Mechanism: Early feature fusion allows network to learn joint representations of vascular and demographic signals before final decision layers
- Core assumption: Vascular changes and demographic factors interact non-additively in predicting hypertension
- Evidence anchors: FeatureFusion method achieved AUC of 0.79, surpassing unimodal models; concatenates feature vectors from each path
- Break condition: If feature distributions are conditionally independent given the label, simple concatenation may add noise rather than signal

## Foundational Learning

- Concept: Microvascular changes in retina reflect systemic vascular health
  - Why needed here: Justifies using fundus images as non-invasive biomarker for hypertension
  - Quick check question: What retinal vascular changes are associated with chronic hypertension?

- Concept: Multimodal learning via feature concatenation vs. decision-level fusion
  - Why needed here: Determines how complementary data streams are integrated for maximal predictive gain
  - Quick check question: When does early fusion outperform late fusion in multimodal deep learning?

- Concept: Transfer learning with frozen convolutional layers
  - Why needed here: Enables effective learning from limited labeled fundus data by reusing pretrained low-level filters
  - Quick check question: Why freeze ImageNet weights when adapting DenseNet to fundus images?

## Architecture Onboarding

- Component map: DenseNet-201 (frozen conv layers) → 64 deep features → concatenate → FusionPath (3 FC layers, dropout) → binary output; FCNN (4 layers, ReLU, dropout) → 64 deep features → concatenate → FusionPath

- Critical path: FundusPath → FusionPath → output; DemographicPath → FusionPath → output

- Design tradeoffs:
  - Frozen vs. fine-tuned DenseNet: stability vs. task adaptation
  - 64-dim bottleneck: compact representation vs. expressiveness
  - Dropout in paths: regularization vs. underfitting risk

- Failure signatures:
  - AUC ≈ 0.5 → model not learning; check data split or label leakage
  - High variance in bootstrap CI → insufficient data or unstable features
  - Demographic-only model near chance → age/gender not predictive in this cohort

- First 3 experiments:
  1. Train unimodal DenseNet-201 on fundus only; compare AUC to baseline
  2. Train unimodal FCNN on age/gender only; evaluate baseline performance
  3. Train FeatureFusion with reduced fusion layers (2 vs. 3); observe impact on AUC and overfitting

## Open Questions the Paper Calls Out

- Question: How would integrating additional cardiometabolic risk factors (e.g., BMI, smoking status) affect HyMNet's performance for hypertension detection?
  - Basis: Paper explicitly states future studies should explore integration of additional features like BMI and smoking status
  - Why unresolved: Current study only uses age and gender; impact of incorporating more risk factors untested
  - What evidence would resolve it: Comparative studies showing model performance with and without additional risk factors using same dataset

- Question: Does diabetes mellitus act as a confounding variable in HyMNet's hypertension predictions, and how does it affect model's decision-making?
  - Basis: Paper studied effect of underlying diabetes mellitus on model's predictive ability, concluding it's used as confounding variable
  - Why unresolved: Study mentions confounding effect but lacks detailed analysis of how diabetes influences predictions or mitigation strategies
  - What evidence would resolve it: Detailed analysis of feature importance scores, SHAP values, or ablation studies showing inclusion/exclusion effects

- Question: What is optimal classification threshold for HyMNet in clinical setting, and how does threshold vary based on intended use?
  - Basis: Paper emphasizes optimizing decision threshold depends on system's goal; current fixed threshold of 0.5 may not be optimal
  - Why unresolved: Uses fixed threshold of 0.5 which may not optimize sensitivity/specificity for clinical deployment
  - What evidence would resolve it: ROC curve analysis to determine optimal Youden's index threshold, validated on independent clinical dataset

## Limitations
- Dataset size is modest (626 individuals), raising concerns about generalization beyond Saudi population
- Model architecture details remain underspecified, particularly fusion network configuration
- Study lacks cross-validation across diverse populations or external validation sets
- Does not explore whether simpler fusion strategies could achieve similar results

## Confidence
- **High Confidence**: Unimodal DenseNet-201 baseline performance and demographic-only model results are straightforward to reproduce
- **Medium Confidence**: Multimodal improvement claim is reasonable but depends on unspecified architectural details
- **Low Confidence**: Specific contribution of demographic factors versus vascular features is difficult to assess without ablation studies

## Next Checks
1. Implement and test exact fusion network configuration to confirm reported performance can be reproduced
2. Evaluate trained model on fundus datasets from different geographic regions to assess generalization beyond Saudi cohort
3. Conduct systematic ablation analysis removing demographic features, vascular features, or modifying fusion strategies to quantify marginal contribution of each modality