---
ver: rpa2
title: 'ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing'
arxiv_id: '2310.11166'
source_url: https://arxiv.org/abs/2310.11166
tags:
- social
- vietnamese
- media
- language
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ViSoBERT, the first monolingual pre-trained
  language model for Vietnamese social media text processing. ViSoBERT is built on
  the XLM-R architecture and pre-trained on a large-scale corpus of high-quality and
  diverse Vietnamese social media texts.
---

# ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing

## Quick Facts
- arXiv ID: 2310.11166
- Source URL: https://arxiv.org/abs/2310.11166
- Reference count: 40
- ViSoBERT achieves state-of-the-art performance on five Vietnamese social media downstream tasks, demonstrating effectiveness in capturing unique characteristics of Vietnamese social media texts.

## Executive Summary
This paper introduces ViSoBERT, the first monolingual pre-trained language model specifically designed for Vietnamese social media text processing. Built on the XLM-R architecture, ViSoBERT is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts. The model is evaluated on five downstream tasks: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. ViSoBERT achieves state-of-the-art performances across all these tasks, demonstrating its effectiveness in capturing the unique characteristics of Vietnamese social media texts.

## Method Summary
ViSoBERT is built on the XLM-R architecture with 12 transformer layers, 768 hidden units, and 12 attention heads. It uses a custom tokenizer trained on Vietnamese social media data to handle unique features like emojis, teencode, and diacritics. The model is pre-trained for 1.2M steps on a 1GB Vietnamese social media corpus from Facebook, TikTok, and YouTube comments. Fine-tuning is performed with batch size 40, learning rate 2e-5, max token length 128, and AdamW optimizer across five downstream tasks using the simpletransformers library.

## Key Results
- 75.65% accuracy on emotion recognition task
- 88.51% accuracy on hate speech detection task
- 77.83% accuracy on sentiment analysis task
- 90.99% accuracy on spam reviews detection task
- 91.62% accuracy on hate speech spans detection task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on social media text with emojis, teencode, and diacritics improves downstream performance.
- Mechanism: Domain-specific pre-training data captures linguistic patterns unique to Vietnamese social media.
- Core assumption: Social media text has distinct features not present in general Vietnamese text.
- Evidence anchors: The model is pre-trained on high-quality and diverse Vietnamese social media texts including emojis.
- Break condition: If social media text does not have unique linguistic features, or if pre-training data is not representative.

### Mechanism 2
- Claim: Custom tokenizer improves performance by handling Vietnamese social media text.
- Mechanism: Tokenizer is trained specifically on Vietnamese social media data, capturing unique vocabulary and patterns.
- Core assumption: General tokenizers are not effective for Vietnamese social media text.
- Evidence anchors: A custom tokenizer built on Vietnamese social media data using SentencePiece.
- Break condition: If custom tokenizer does not significantly improve tokenization compared to general tokenizers.

### Mechanism 3
- Claim: Masking rate of 30% is optimal for ViSoBERT's pre-training.
- Mechanism: Masking rate balances contextual information retention and training efficiency.
- Core assumption: There is an optimal masking rate for pre-training.
- Evidence anchors: The model achieved highest performance with 30% masking rate, though optimal rate depends on specific task.
- Break condition: If masking rate of 30% does not consistently improve performance across tasks.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: ViSoBERT is based on the transformer architecture.
  - Quick check question: What are the key components of a transformer model?

- Concept: Pre-training vs. fine-tuning
  - Why needed here: ViSoBERT is pre-trained on a large corpus and then fine-tuned on downstream tasks.
  - Quick check question: What is the difference between pre-training and fine-tuning in NLP?

- Concept: Masking in language models
  - Why needed here: ViSoBERT uses a masked language objective during pre-training.
  - Quick check question: How does masking work in masked language models like BERT?

## Architecture Onboarding

- Component map: Custom tokenizer -> XLM-R transformer (12 layers, 768 hidden units, 12 attention heads) -> Downstream task heads
- Critical path: Pre-training on Vietnamese social media data -> Custom tokenizer training -> Fine-tuning on downstream tasks
- Design tradeoffs: ViSoBERT has fewer parameters than some baselines but achieves better performance on Vietnamese social media tasks.
- Failure signatures: Poor performance on social media-specific tasks, inability to handle emojis/teencode/diacritics, tokenization errors.
- First 3 experiments:
  1. Compare performance of ViSoBERT vs. general Vietnamese PLM on social media task.
  2. Test impact of different masking rates during pre-training.
  3. Evaluate tokenization performance on social media text with and without custom tokenizer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal masking rate for different Vietnamese social media tasks, and how does it vary based on task characteristics?
- Basis in paper: The paper mentions experimenting with masking rates from 10% to 50% and finding 30% optimal overall, but notes that hate speech detection performed best at 50%.
- Why unresolved: The paper only tested a limited range of masking rates and did not deeply investigate why different tasks benefit from different masking rates.
- What evidence would resolve it: Comprehensive experiments testing a wider range of masking rates on each task, coupled with analysis of how task-specific features correlate with optimal masking rates.

### Open Question 2
- Question: How do specific Vietnamese social media characteristics (emojis, teencode, diacritics) impact the model's ability to capture context and semantics?
- Basis in paper: The paper discusses the importance of emojis, teencode, and diacritics in Vietnamese social media and presents experiments showing their impact on model performance.
- Why unresolved: While the paper provides some insights into the impact of these characteristics, it does not delve into the specific mechanisms by which they affect the model's understanding of context and semantics.
- What evidence would resolve it: Detailed analysis of model behavior on social media texts with and without these characteristics, including attention patterns, feature importance, and semantic similarity measures.

### Open Question 3
- Question: How does the performance of ViSoBERT compare to other pre-trained models on Vietnamese social media tasks in terms of robustness and generalizability?
- Basis in paper: The paper demonstrates ViSoBERT's state-of-the-art performance on various Vietnamese social media tasks, but does not directly compare its robustness and generalizability to other models.
- Why unresolved: The paper focuses on comparing ViSoBERT's performance to other models on the same tasks, but does not investigate how well it generalizes to new tasks or how robust it is to noise and variations in social media text.
- What evidence would resolve it: Experiments evaluating ViSoBERT's performance on out-of-domain social media tasks, its ability to handle noisy and adversarial inputs, and its sensitivity to different types of social media text variations.

## Limitations
- Data Scope Limitation: The pre-training corpus is described as "high-quality and diverse" but the exact size, diversity metrics, and representativeness are not specified, creating uncertainty about generalizability.
- Reproducibility Gap: Critical pre-training hyperparameters (optimizer settings, warmup steps, exact learning rate schedule) are not fully specified, making exact replication difficult.
- Baseline Comparison Constraints: Evaluation shows state-of-the-art performance but comparisons are primarily against other Vietnamese PLMs, with limited analysis relative to multilingual models.

## Confidence
- High Confidence: The architectural choices (XLM-R foundation, transformer-based design) and fine-tuning methodology are well-established with clearly reported downstream task results.
- Medium Confidence: The claimed improvements over baselines are supported by experimental results, but lack of detailed pre-training specifications and comprehensive ablation studies reduces confidence in exact contribution of each design choice.
- Low Confidence: Claims about the custom tokenizer's superiority and the optimal 30% masking rate are supported by limited evidence without systematic comparison with alternative tokenizers or masking strategies.

## Next Checks
1. **Ablation Study**: Conduct controlled experiments removing the custom tokenizer and testing with standard SentencePiece tokenizers on the same Vietnamese social media corpus to quantify the actual contribution of the tokenizer customization.

2. **Pre-training Hyperparameter Sensitivity**: Systematically vary the masking rate (15%, 30%, 50%) and other pre-training hyperparameters to verify the claimed optimal settings across all five downstream tasks.

3. **Cross-Domain Evaluation**: Test ViSoBERT's performance on formal Vietnamese text datasets (news articles, literature) to assess domain generalization capabilities and identify limitations outside the social media context.