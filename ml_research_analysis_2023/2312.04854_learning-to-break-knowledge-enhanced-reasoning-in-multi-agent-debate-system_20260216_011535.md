---
ver: rpa2
title: 'Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System'
arxiv_id: '2312.04854'
source_url: https://arxiv.org/abs/2312.04854
tags:
- answer
- evidence
- debate
- arxiv
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MADKE, a multi-agent debate framework that
  integrates retrieval-augmented knowledge to overcome cognitive limitations and improve
  reasoning performance. The approach involves introducing external prior knowledge
  through a shared retrieval knowledge pool and implementing an adaptive knowledge
  selection method that allows agents to choose whether to use external knowledge
  based on their needs in each conversation round.
---

# Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System

## Quick Facts
- arXiv ID: 2312.04854
- Source URL: https://arxiv.org/abs/2312.04854
- Authors: 
- Reference count: 15
- Primary result: MADKE achieves state-of-the-art performance across six datasets, with Qwen1.5-72B-Chat surpassing GPT-4 by +1.26% on average.

## Executive Summary
This paper introduces MADKE, a multi-agent debate framework that integrates retrieval-augmented knowledge to overcome cognitive limitations in large language models. The system employs debaters, a judge, and a summarizer who engage in structured debates while accessing a shared knowledge pool. By allowing agents to selectively incorporate external evidence and iteratively refine their arguments, MADKE achieves state-of-the-art performance on six benchmark datasets. Notably, the approach demonstrates that open-source models can match or exceed GPT-4's performance when enhanced with appropriate knowledge retrieval and debate mechanisms.

## Method Summary
MADKE implements a multi-agent debate system where two debaters generate initial responses and engage in up to three debate rounds, with a judge determining consensus and a summarizer compiling the final answer. The framework incorporates a shared retrieval knowledge pool containing evidence from Wikipedia (top 15 passages via DPR) and Google search (top 10 results). Agents use a self-selection module to filter relevant evidence from the pool, choosing no more than three pieces per round. The system employs chain-of-thought prompting to guide step-by-step reasoning and uses temperature 0.5 for generation. Experiments were conducted on six datasets with 500 samples each, comparing against baseline single-agent and multi-agent methods.

## Key Results
- MADKE achieves state-of-the-art performance across six datasets including TriviaQA, NQ, HotpotQA, 2WikiMultiHopQA, FEVER, and FEVEROUS
- Qwen1.5-72B-Chat using MADKE surpasses GPT-4 by +1.26% average across all datasets
- The framework improves model consistency and correctness by breaking cognitive constraints through external knowledge integration
- MADKE demonstrates superior performance on both discriminative (EM metric) and generative (GPT4 Eval metric) tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-selection module allows agents to filter out irrelevant or misleading evidence from the evidence pool.
- Mechanism: Agents autonomously identify and select evidence that is most helpful for answering the question, reducing the impact of noise in retrieval results.
- Core assumption: Agents can effectively distinguish between relevant and irrelevant evidence when given the self-selection prompt.
- Evidence anchors:
  - [abstract] "we propose an adaptive knowledge selection method to guarantee the accuracy and personalization of knowledge"
  - [section 3.3] "To ensure precision and relevance in their selection, each debater is limited to choosing no more than three pieces of evidence"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.354, average citations=0.0" (weak signal, but suggests the mechanism is novel)
- Break condition: If the self-selection prompt is ineffective, agents may still select irrelevant evidence, negating the benefits of the retrieval-augmented approach.

### Mechanism 2
- Claim: Introducing external retrieval knowledge breaks cognitive constraints by providing agents with additional information to overcome knowledge gaps.
- Mechanism: The shared retrieval knowledge pool supplies agents with relevant evidence that they might not have access to otherwise, enabling them to correct misconceptions or strengthen arguments.
- Core assumption: The retrieved evidence is sufficiently relevant and helpful to improve agent performance.
- Evidence anchors:
  - [abstract] "incorporates retrieval of prior knowledge into the debate process, effectively breaking cognitive constraints and enhancing the agents' reasoning capabilities"
  - [section 2.2] "Integrating external retrieval knowledge has emerged as a crucial strategy to mitigate hallucinations in LLMs"
  - [corpus] No direct evidence, but related papers suggest this is an active area of research
- Break condition: If the retrieved evidence is not helpful or introduces too much noise, it could worsen agent performance rather than improve it.

### Mechanism 3
- Claim: The multi-agent debate structure with a judge and summarizer improves consistency and correctness by allowing agents to learn from each other and reach consensus.
- Mechanism: Agents present their viewpoints, debate, and iteratively refine their answers based on the discussion and evidence, with the judge determining when consensus is reached.
- Core assumption: The debate process leads to convergence on the correct answer rather than divergence or deadlock.
- Evidence anchors:
  - [abstract] "aims to align the correct cognition of different agents for the optimal solution"
  - [section 3.1] "The judge is tasked with determining whether the debaters have achieved a consensus on their responses"
  - [corpus] No direct evidence, but related papers suggest this is a common approach in multi-agent systems
- Break condition: If the debate becomes circular or the judge prematurely ends the debate before consensus is reached, the final answer may be incorrect.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The method relies on retrieving relevant evidence from external sources to enhance the agents' knowledge and improve their reasoning.
  - Quick check question: What is the purpose of using RAG in this multi-agent debate framework?

- Concept: Chain-of-thought prompting
  - Why needed here: The agents are instructed to think step-by-step and reason through the problem, which is a key aspect of chain-of-thought prompting.
  - Quick check question: How does chain-of-thought prompting help the agents in this framework?

- Concept: Multi-agent collaboration
  - Why needed here: The method uses multiple agents with different roles (debater, judge, summarizer) to work together and reach a consensus on the answer.
  - Quick check question: What are the benefits of using a multi-agent approach in this debate framework?

## Architecture Onboarding

- Component map: Debaters -> Judge -> Summarizer (with Evidence Pool and Self-Selection Module)
- Critical path: Retrieve evidence → Agents select evidence → Debaters present viewpoints → Judge evaluates consensus → Summarizer compiles final answer
- Design tradeoffs:
  - Number of debate rounds vs. time and computational cost
  - Number of agents vs. diversity of viewpoints and potential for deadlock
  - Source of evidence (Wikipedia vs. Google) vs. relevance and noise
- Failure signatures:
  - Agents select irrelevant evidence
  - Debate becomes circular or fails to reach consensus
  - Judge prematurely ends debate
  - Summarizer introduces bias or errors in final answer
- First 3 experiments:
  1. Test the self-selection module with a small evidence pool and known relevant evidence
  2. Evaluate the impact of different debate round limits on consistency and correctness
  3. Compare the performance of different evidence sources (Wikipedia vs. Google) on a subset of datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MADKE scale with the number of agents beyond 3, and what is the optimal number of agents for different task complexities?
- Basis in paper: [explicit] The paper mentions that when the number of agents changes from 2 to 3, the number of inconsistencies increases and model performance decreases. However, as the number of agents further increases, the model performance gradually increases and inconsistencies gradually decrease.
- Why unresolved: The paper only briefly mentions the effect of agent number but does not provide detailed experimental results for different agent counts or analyze the optimal number of agents for various task complexities.
- What evidence would resolve it: Conducting experiments with varying numbers of agents (e.g., 2, 3, 4, 5, 6) on different task complexities and analyzing the relationship between agent count, performance, and task difficulty.

### Open Question 2
- Question: How does the quality of retrieved evidence impact the performance of MADKE, and what is the optimal retrieval strategy for different types of reasoning tasks?
- Basis in paper: [inferred] The paper mentions that evidence from Wikipedia includes many relevant but unhelpful evidence, leading to challenges in effective distinction by agents. It also discusses the effectiveness of combining Wikipedia and Google search results.
- Why unresolved: The paper does not provide a detailed analysis of how the quality of retrieved evidence affects the model's performance or discuss optimal retrieval strategies for different types of reasoning tasks.
- What evidence would resolve it: Analyzing the correlation between evidence quality metrics (e.g., relevance, helpfulness) and model performance, and experimenting with different retrieval strategies (e.g., retrieval methods, evidence filtering techniques) for various task types.

### Open Question 3
- Question: How does the self-selection module's performance vary with different types of evidence pools, and what are the key factors that influence its effectiveness?
- Basis in paper: [explicit] The paper mentions that the self-selection module shows superior performance when utilizing evidence from Wikipedia for QA datasets, and it effectively allows agents to filter out noise.
- Why unresolved: The paper does not provide a comprehensive analysis of how the self-selection module's performance varies with different types of evidence pools or discuss the key factors that influence its effectiveness.
- What evidence would resolve it: Conducting experiments with different types of evidence pools (e.g., varying sizes, sources, and qualities) and analyzing the relationship between evidence pool characteristics and the self-selection module's performance.

## Limitations

- The self-selection module's effectiveness is not fully validated, with unclear implementation details that may impact real-world performance
- Reliance on external retrieval sources introduces variability in evidence quality that could affect performance across different domains or languages
- Insufficient evidence that the debate process actually improves correctness rather than just adding computational steps without substantive benefit

## Confidence

- High Confidence: MADKE improves performance over single-agent baselines (supported by experimental results across six datasets)
- Medium Confidence: External retrieval knowledge breaks cognitive constraints (plausible but relies on consistent relevance of retrieved evidence)
- Low Confidence: Multi-agent debate structure specifically improves consistency and correctness (insufficient evidence that debate process leads to better convergence)

## Next Checks

1. Conduct ablation studies removing the self-selection module to quantify its actual contribution to performance improvements
2. Systematically evaluate how retrieval quality impacts final performance by testing with controlled evidence pools containing varying ratios of relevant to irrelevant information
3. Implement detailed analysis of debate trajectories to determine whether consensus reached by the judge actually correlates with correct answers