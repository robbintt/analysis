---
ver: rpa2
title: Distance Guided Generative Adversarial Network for Explainable Binary Classifications
arxiv_id: '2312.17538'
source_url: https://arxiv.org/abs/2312.17538
tags:
- samples
- augmentation
- disgan
- generated
- distances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a distance-guided generative adversarial network
  (DisGAN) to improve binary classification performance on limited datasets. The method
  addresses the problem of data insufficiency by controlling the variation degrees
  of generated samples through vertical and horizontal distances relative to a learned
  decision hyperplane.
---

# Distance Guided Generative Adversarial Network for Explainable Binary Classifications

## Quick Facts
- arXiv ID: 2312.17538
- Source URL: https://arxiv.org/abs/2312.17538
- Reference count: 40
- Method improves binary classification accuracy and AUC scores on limited datasets through distance-guided sample generation

## Executive Summary
This paper introduces a distance-guided generative adversarial network (DisGAN) that improves binary classification performance on limited datasets by generating diverse training samples conditioned on vertical and horizontal distances relative to a learned decision hyperplane. The method consists of two components: VerDisGAN generates inter-domain samples based on vertical distances from the hyperplane, while HorDisGAN generates intra-domain samples based on horizontal distances between samples. Experiments on four public datasets demonstrate consistent improvements over traditional augmentation and other GAN-based methods, with the added benefit of producing interpretable class-difference maps showing the difference between source images and their projections onto the decision hyperplane.

## Method Summary
The method trains a binary classifier with hinge loss to create an optimal decision hyperplane, then fixes its weights to serve as an auxiliary classifier that measures distances from samples to this hyperplane. Two GAN components are developed: VerDisGAN generates inter-domain samples using vertical distances, and HorDisGAN generates intra-domain samples using horizontal distances. Both components include cycle-consistency losses to preserve original information during bidirectional translation. The model is trained using LSGAN loss, distance loss, and cycle-consistency loss, with hyperparameters λverDIS (0.01-0.1) and λhorDIS (0.001-0.01) controlling the relative importance of distance constraints.

## Key Results
- DisGAN consistently outperforms traditional augmentation and other GAN-based methods across all four tested datasets
- Classification accuracy improvements range from 2-8% compared to baseline methods
- The method provides interpretable class-difference maps showing decision boundary refinement
- AUC score improvements demonstrate better classification confidence across datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlling vertical and horizontal distances improves decision boundary precision
- Mechanism: The method explicitly controls the variation degrees of generated samples by conditioning them on distances relative to a learned decision hyperplane. Vertical distances control inter-domain variation (between classes), while horizontal distances control intra-domain variation (within classes).
- Core assumption: Distance measurements in hyperplane space accurately capture the variation needed to reshape decision boundaries
- Evidence anchors:
  - [abstract]: "controls the variation degrees of generated samples in the hyperplane space"
  - [section]: "we develop a hyperplane distance GAN (DisGAN) which consists of vertical distance GAN (VerDisGAN) and horizontal distance GAN (HorDisGAN)"
  - [corpus]: Weak - corpus neighbors focus on GAN applications but don't directly address distance-guided approaches

### Mechanism 2
- Claim: Fixed classifier as auxiliary provides stable distance references
- Mechanism: A binary classifier trained with hinge loss creates an optimal hyperplane, then its weights are fixed to serve as an auxiliary classifier that measures vertical distances from samples to this hyperplane
- Core assumption: A fixed classifier provides consistent and meaningful distance measurements throughout the generation process
- Evidence anchors:
  - [section]: "we train a binary classifier via hinge loss to obtain an optimal hyperplane... Secondly, we measure the two types distances"
  - [abstract]: "Training a binary classifier via hinge loss and fixing its weights to produce an optimal hyperplane"
  - [corpus]: Missing - no direct corpus evidence supporting fixed classifier approaches

### Mechanism 3
- Claim: Cycle consistency ensures information preservation during bidirectional translation
- Mechanism: Both VerDisGAN and HorDisGAN include cycle-consistency losses that map generated samples back to their source images, ensuring the generation process doesn't lose original information
- Core assumption: Cycle consistency effectively prevents information loss and maintains semantic meaning during generation
- Evidence anchors:
  - [section]: "To make samples not lose the original information after translating twice, we develop the cycle-consistency losses"
  - [abstract]: "Furthermore, VerDisGAN can produce the class-specific regions by mapping the source images to the hyperplane"
  - [corpus]: Weak - corpus neighbors discuss GANs but don't specifically address cycle consistency for distance-guided approaches

## Foundational Learning

- Concept: Hyperplane distance calculations in binary classification
  - Why needed here: The entire method relies on measuring and controlling distances from samples to a decision boundary hyperplane
  - Quick check question: How do you calculate the signed distance from a point to a hyperplane defined by wTx + b = 0?

- Concept: Generative adversarial network architecture components
  - Why needed here: Understanding generators, discriminators, and loss functions is essential for implementing both VerDisGAN and HorDisGAN components
  - Quick check question: What are the key differences between LSGAN loss and traditional GAN loss formulations?

- Concept: Cycle-consistent image translation
  - Why needed here: The method uses cycle consistency to ensure bidirectional translation preserves information between source and generated samples
  - Quick check question: How does cycle consistency help prevent mode collapse in unpaired image translation tasks?

## Architecture Onboarding

- Component map:
  Binary classifier (trained with hinge loss) → creates optimal hyperplane
  Auxiliary classifier (fixed weights) → measures vertical distances
  VerDisGAN component → generates inter-domain samples using vertical distances
  HorDisGAN component → generates intra-domain samples using horizontal distances
  Four discriminators → distinguish real from generated samples for both components
  Distance reconstruction → auxiliary classifier reconstructs distances from generated samples

- Critical path:
  1. Train binary classifier with hinge loss
  2. Fix classifier weights to create auxiliary classifier
  3. Measure vertical and horizontal distances
  4. Generate samples conditioned on these distances
  5. Apply cycle consistency and distance reconstruction
  6. Train discriminators to distinguish real from generated samples

- Design tradeoffs:
  - Fixed vs. adaptive classifier: Fixed provides stability but may become suboptimal
  - Distance measurement accuracy vs. computational cost: More precise measurements require more complex calculations
  - Cycle consistency strength: Higher weights preserve information but may limit generation diversity

- Failure signatures:
  - Mode collapse in generators
  - Distance reconstruction errors (curves diverge from targets)
  - Discriminator overfitting to generated samples
  - Decision boundary distortion rather than refinement

- First 3 experiments:
  1. Implement binary classifier with hinge loss and verify hyperplane construction on simple 2D dataset
  2. Test distance measurement accuracy by comparing ground truth distances with auxiliary classifier outputs
  3. Validate cycle consistency by generating samples and measuring reconstruction quality against original inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the DisGAN framework be extended to multi-class classification problems?
- Basis in paper: [explicit] The authors state "The current method has potential to be extended to multi-class classification" and suggest adding distance constraints into StarGAN with multi-hinge loss classifiers.
- Why unresolved: The proposed method currently focuses on binary classification and lacks implementation details for multi-class scenarios.
- What evidence would resolve it: Successful experimental results demonstrating DisGAN's effectiveness on multi-class classification tasks with appropriate distance metrics and loss functions.

### Open Question 2
- Question: How does the iterative process of using the binary classifier for distance calculation and then retraining with generated samples affect overall performance?
- Basis in paper: [explicit] The authors mention "We hypothesize that this can be an interactive process: the binary classifier which uses generated samples can be used again as the fixed auxiliary classifier."
- Why unresolved: The paper does not investigate or demonstrate this iterative refinement process.
- What evidence would resolve it: Comparative studies showing performance improvements when iteratively updating the classifier and distance measurements versus a single training cycle.

### Open Question 3
- Question: What are the limitations of the distance-based generation approach when dealing with complex decision boundaries or highly imbalanced datasets?
- Basis in paper: [inferred] While the method shows success on various datasets, the paper does not explore its limitations or performance on highly imbalanced data or complex boundary scenarios.
- Why unresolved: The evaluation focuses on moderately sized datasets with reasonable class balance, leaving uncertainty about performance in more challenging scenarios.
- What evidence would resolve it: Systematic experiments on datasets with varying levels of class imbalance and complex decision boundaries to quantify performance degradation or limitations.

## Limitations
- Experimental validation primarily on image datasets with relatively small sample sizes
- Method relies heavily on quality of initial binary classifier and its hyperplane construction
- Performance depends on optimal hyperparameter tuning (λverDIS, λhorDIS)
- Cycle consistency assumes bidirectional translation preserves semantic meaning

## Confidence

### Mechanism 1 (Distance control improves boundaries): Medium
### Mechanism 2 (Fixed classifier provides stable distances): Low-Medium
### Mechanism 3 (Cycle consistency preserves information): Medium

## Next Checks

1. Test distance measurement accuracy by comparing ground truth distances with auxiliary classifier outputs on a controlled synthetic dataset where distances are known
2. Validate cycle consistency by generating samples and measuring reconstruction quality against original inputs using quantitative metrics (PSNR, SSIM)
3. Conduct ablation studies to isolate the contribution of each component (VerDisGAN, HorDisGAN, cycle consistency) to overall performance improvements