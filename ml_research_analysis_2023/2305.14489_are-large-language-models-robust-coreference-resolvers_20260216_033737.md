---
ver: rpa2
title: Are Large Language Models Robust Coreference Resolvers?
arxiv_id: '2305.14489'
source_url: https://arxiv.org/abs/2305.14489
tags:
- coreference
- instructgpt
- mentions
- resolution
- mention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the zero-shot coreference resolution abilities
  of instruction-tuned large language models (LLMs) like InstructGPT. The authors
  find that prompting LLMs with a document-level template that marks candidate mentions
  and asks the model to annotate cluster IDs can outperform unsupervised coreference
  systems.
---

# Are Large Language Models Robust Coreference Resolvers?

## Quick Facts
- arXiv ID: 2305.14489
- Source URL: https://arxiv.org/abs/2305.14489
- Authors: [Not specified in input]
- Reference count: 17
- Key outcome: InstructGPT achieves competitive zero-shot coreference resolution on CoNLL-2012, especially for pronoun resolution, but struggles with mention detection.

## Executive Summary
This paper investigates whether large language models (LLMs) like InstructGPT can perform coreference resolution in a zero-shot manner through prompt engineering. The authors demonstrate that a document-level template approach—marking candidate mentions and asking the model to annotate cluster IDs—can elicit coreference resolution capabilities without fine-tuning. InstructGPT shows strong performance on the CoNLL-2012 benchmark, particularly for pronoun resolution, and generalizes well across domains, languages, and time periods. However, the study reveals that mention detection quality is a critical bottleneck, with zero-shot mention detection being particularly challenging for the model.

## Method Summary
The authors evaluate zero-shot coreference resolution by prompting InstructGPT with a document-level template that marks candidate mentions using special tokens and asks the model to annotate cluster IDs. They use the CoNLL-2012 English OntoNotes dataset as the primary benchmark and compare results against unsupervised systems like dcoref and few-shot neural models. The approach relies on external mention detection systems to generate candidate mentions, and coreference clusters are extracted from the LLM's output using fuzzy string matching. Performance is measured using CoNLL F1 score (average of MUC, B3, and CEAFϕ4 metrics), mention detection F1, and resolution accuracy by mention type.

## Key Results
- InstructGPT achieves competitive zero-shot coreference resolution on CoNLL-2012, especially for pronoun resolution.
- The model generalizes surprisingly well across domains, languages, and time periods when provided with high-quality mentions.
- InstructGPT struggles with zero-shot mention detection, performing much worse than supervised mention detectors like dcoref and SpanBERT.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting LLMs with a document-level template that marks candidate mentions and asks for cluster IDs can elicit coreference resolution capabilities without fine-tuning.
- Mechanism: The template format provides explicit structural scaffolding (mention markers + cluster ID instructions) that guides the LLM to generate structured coreference output rather than free-form text.
- Core assumption: LLMs retain and can access implicit coreference knowledge learned during pre-training when prompted with appropriate templates.
- Evidence anchors:
  - [abstract] "prompting LLMs is a feasible strategy for zero-shot coreference resolution"
  - [section] "we found that prompting LLMs is a feasible strategy for zero-shot coreference resolution"
  - [corpus] Weak - no direct citations found in related papers about this specific template approach.
- Break condition: If mention detection quality drops below a threshold, coreference performance degrades significantly regardless of prompt quality.

### Mechanism 2
- Claim: InstructGPT shows strong zero-shot generalization across domains, languages, and time periods when provided with gold mentions.
- Mechanism: The instruction-tuned model has learned robust cross-domain linguistic patterns during pre-training that transfer effectively to new contexts without task-specific fine-tuning.
- Core assumption: Pre-training on diverse corpora captures generalizable linguistic knowledge that applies to coreference resolution across different domains and languages.
- Evidence anchors:
  - [abstract] "instruction-tuned LMs generalize surprisingly well across domains, languages, and time periods"
  - [section] "InstructGPT performs competitively with other models in the zero-shot setting"
  - [corpus] Weak - related papers focus on domain adaptation but not specifically on zero-shot generalization across temporal shifts.
- Break condition: Significant temporal shifts or domain-specific linguistic conventions not present in pre-training data may reduce performance.

### Mechanism 3
- Claim: Coreference performance is highly dependent on mention detection quality, with zero-shot mention detection being particularly challenging.
- Mechanism: Coreference resolution requires accurate candidate mentions as input; errors in mention detection cascade into coreference linking errors.
- Core assumption: The quality of input mentions is a prerequisite bottleneck for coreference resolution performance.
- Evidence anchors:
  - [abstract] "the models struggle with mention detection, relying heavily on high-quality mention detectors"
  - [section] "InstructGPT struggles with zero-shot mention detection, performing much worse than dcoref"
  - [corpus] Weak - related papers discuss mention detection importance but not specifically zero-shot detection challenges.
- Break condition: If mention detection F1 falls below approximately 80%, coreference performance drops sharply regardless of LLM capabilities.

## Foundational Learning

- Concept: Coreference resolution as a pipeline task
  - Why needed here: Understanding the mention detection + antecedent linking decomposition is crucial for interpreting LLM performance
  - Quick check question: What are the two main components of traditional coreference resolution systems?

- Concept: Zero-shot and few-shot learning paradigms
  - Why needed here: The paper's experimental setup relies on prompting without task-specific fine-tuning
  - Quick check question: How does zero-shot prompting differ from few-shot prompting in terms of in-context examples?

- Concept: Document-level vs. sentence-level coreference
  - Why needed here: The paper emphasizes document-level complexity versus simpler sentence-level benchmarks
  - Quick check question: Why is document-level coreference resolution more challenging than sentence-level resolution?

## Architecture Onboarding

- Component map: Raw text document -> Mention detector -> LLM prompt generator -> InstructGPT -> Output parser -> Evaluator
- Critical path: Mention detection → Prompt generation → LLM inference → Output parsing → Evaluation
- Design tradeoffs:
  - Zero-shot vs. few-shot: Tradeoff between flexibility and performance
  - Mention quality vs. mention quantity: High-precision detectors may miss mentions but improve coreference accuracy
  - Prompt template complexity: More structured prompts may yield better results but require more careful design
- Failure signatures:
  - Low mention detection F1 → Poor coreference performance regardless of LLM
  - LLM generating unstructured output → Output parser fails to extract clusters
  - Incorrect cluster ID format → Evaluation metrics cannot be computed
- First 3 experiments:
  1. Test zero-shot prompt with gold mentions on OntoNotes dev set to establish baseline
  2. Vary mention detection quality (dcoref vs. SpanBERT vs. zero-shot) and measure impact on coreference
  3. Test domain generalization by evaluating on LitBank and QuizBowlCoref datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of zero-shot prompting with large language models (LLMs) for coreference resolution compare to few-shot and supervised approaches across different domains and languages?
- Basis in paper: [inferred] The paper discusses the effectiveness of zero-shot prompting with LLMs for coreference resolution, comparing it to unsupervised systems and few-shot cross-lingual neural models.
- Why unresolved: The paper focuses on zero-shot learning and does not provide a comprehensive comparison across all settings (zero-shot, few-shot, and supervised) across multiple domains and languages.
- What evidence would resolve it: Conducting experiments that directly compare zero-shot, few-shot, and supervised approaches for coreference resolution across a variety of domains and languages would provide a clearer understanding of the relative performance of each method.

### Open Question 2
- Question: What are the specific linguistic challenges that LLMs face in zero-shot coreference resolution, and how can these challenges be addressed?
- Basis in paper: [explicit] The paper mentions that InstructGPT struggles with nominal resolution and mention detection, particularly for nominal noun phrases.
- Why unresolved: The paper identifies some challenges but does not provide a detailed analysis of the linguistic aspects that LLMs struggle with or propose solutions to address these issues.
- What evidence would resolve it: A comprehensive linguistic analysis of LLM outputs in zero-shot coreference resolution, identifying specific patterns of errors and proposing targeted strategies to improve performance, would help address these challenges.

### Open Question 3
- Question: How does the temporal robustness of zero-shot coreference resolution with LLMs compare to that of traditional coreference systems, and what factors contribute to this difference?
- Basis in paper: [explicit] The paper demonstrates that InstructGPT is more robust to temporal changes in dataset compared to dcoref, but does not explore the underlying factors contributing to this difference.
- Why unresolved: The paper shows a difference in temporal robustness but does not investigate the reasons behind it or identify the factors that make LLMs more adaptable to temporal changes.
- What evidence would resolve it: Conducting experiments that analyze the performance of LLMs and traditional systems on coreference resolution tasks with varying temporal distances and identifying the linguistic and model-specific factors that influence temporal robustness would provide insights into this question.

## Limitations
- The approach relies heavily on external mention detection systems, with LLM performance constrained by mention quality rather than model capabilities.
- The study does not explore the full potential of prompt engineering or few-shot learning strategies.
- Evaluation focuses primarily on English OntoNotes with limited analysis of truly low-resource languages or languages with different coreference conventions.

## Confidence

- **High confidence**: InstructGPT achieves competitive zero-shot coreference performance on CoNLL-2012 when provided with high-quality mentions.
- **Medium confidence**: InstructGPT generalizes well across domains and time periods.
- **Low confidence**: Zero-shot mention detection using InstructGPT is feasible for coreference resolution.

## Next Checks

1. **Prompt engineering ablation**: Systematically test different prompt templates (varying mention markers, cluster ID formats, and instruction specificity) to determine if performance can be improved beyond the current document-level template approach.

2. **Few-shot performance analysis**: Compare zero-shot performance against few-shot settings with varying numbers of in-context examples to establish the learning curve and identify whether minimal fine-tuning could yield substantial gains.

3. **Cross-linguistic robustness test**: Evaluate the approach on languages with typologically diverse coreference patterns (e.g., pro-drop languages, languages with gender-neutral pronouns) to assess whether pre-training coverage translates to cross-linguistic generalization.