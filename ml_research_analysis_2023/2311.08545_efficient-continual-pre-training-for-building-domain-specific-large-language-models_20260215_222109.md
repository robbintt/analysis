---
ver: rpa2
title: Efficient Continual Pre-training for Building Domain Specific Large Language
  Models
arxiv_id: '2311.08545'
source_url: https://arxiv.org/abs/2311.08545
tags:
- data
- pre-training
- continual
- domain
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores continual pre-training as a cost-effective
  alternative to building domain-specific large language models (LLMs) from scratch.
  The authors introduce FinPythia-6.9B, a financial domain LLM developed through continual
  pre-training of Pythia on a curated financial corpus.
---

# Efficient Continual Pre-training for Building Domain Specific Large Language Models

## Quick Facts
- arXiv ID: 2311.08545
- Source URL: https://arxiv.org/abs/2311.08545
- Authors: [Not specified in input]
- Reference count: 40
- Key outcome: Continual pre-training on domain-specific data improves domain performance while maintaining open-domain capabilities, with efficient data selection strategies achieving similar results using only 10% of the data and cost.

## Executive Summary
This paper presents a cost-effective approach to building domain-specific large language models through continual pre-training rather than training from scratch. The authors introduce FinPythia-6.9B, a financial domain LLM developed by continually pre-training Pythia on a curated financial corpus. They propose two efficient domain-adaptive continual pre-training methods (ETS-DACP and ETA-DACP) that outperform vanilla continual pre-training using only 10% of the corpus size and cost, while maintaining performance on standard open-domain tasks.

## Method Summary
The authors develop domain-adaptive continual pre-training methods for building financial domain LLMs. They curate a 23.9 billion token financial corpus from CommonCrawl and SEC filings, then apply three approaches: vanilla DACP (continual pre-training on full corpus), TACP (task-adaptive pre-training on unlabeled task data), and efficient methods (ETS-DACP and ETA-DACP) using data selection based on similarity, perplexity, and entropy. The efficient methods select high-value samples from the corpus, achieving comparable performance with 10% of the data and cost.

## Key Results
- FinPythia-6.9B and FinPythia-1B show superior performance on financial tasks (FPB, Headline, NER) compared to baseline Pythia models
- ETS-DACP and ETA-DACP outperform DACP with 100% data when trained on only 10% of the corpus
- Domain-adaptive pre-training maintains open-domain capabilities without significant degradation on ARC, MMLU, TruthfulQA, and HellaSwag tasks
- Task-agnostic data selection based on perplexity and entropy achieves near-equal performance to task-aware selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-agnostic data selection based on perplexity and entropy can achieve near-equal performance to task-aware selection
- Mechanism: Perplexity identifies novel tokens not well-represented in original training, while entropy captures distributional diversity; together they approximate task alignment without needing task data
- Core assumption: Novel and diverse samples provide equivalent signal to task-specific samples for domain adaptation
- Evidence anchors: [abstract] "Our data selection strategies outperforms vanilla continual pre-training's performance with just 10% of corpus size and cost, without any degradation on open-domain standard tasks"; [section 2.4.2] "Documents with higher perplexity are less represented in the original training corpus, thus being more likely to contain novel knowledge for the model"; [section 2.4.2] "Diversity captures the diversity of distributions of token types in the domain corpus"
- Break condition: If task-specific signal is critical and cannot be approximated by novelty/diversity, or if domain and task distributions are orthogonal

### Mechanism 2
- Claim: Domain-adaptive continual pre-training maintains open-domain capabilities while improving domain performance
- Mechanism: Pre-training on domain corpus enriches domain-specific knowledge without overwriting general capabilities, as evidenced by maintained performance on out-of-domain tasks
- Core assumption: Language model parameters have sufficient capacity to maintain both general and domain-specific knowledge simultaneously
- Evidence anchors: [abstract] "Our data selection strategies... without any degradation on open-domain standard tasks"; [section 4.2] "We do not observe any significant change in the performance on the four out-of-domain tasks except for DACP with 100% data"; [section 4.1] "FinPythia-6.9B and FinPythia-1B exhibit superior performance on tasks FPB, Headline, and NER while showing comparatively lower results on the FiQA SA task"
- Break condition: If continual pre-training causes catastrophic forgetting or if domain corpus contains conflicting knowledge that degrades general capabilities

### Mechanism 3
- Claim: Data selection strategies outperform vanilla continual pre-training with 10% of data and cost
- Mechanism: Selecting high-value samples (task-similar, novel, diverse) focuses learning on most informative examples, avoiding dilution from redundant or irrelevant data
- Core assumption: Not all data is equally valuable for domain adaptation; targeted selection improves learning efficiency
- Evidence anchors: [abstract] "Our data selection strategies outperforms vanilla continual pre-training's performance with just 10% of corpus size and cost"; [section 4.2] "While TACP shows significant improvement in model performance compared to the original Pythia-1B, ETS-DACP stands out as the top-performing approach"; [section 4.2] "ETS-DACP trained on 10% outperforms DACP with 100% of the data"
- Break condition: If data selection introduces bias that harms generalization, or if remaining 90% contains critical information missed by selection criteria

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why continual pre-training with small learning rates prevents forgetting of general knowledge
  - Quick check question: What would happen to open-domain performance if we used large learning rates for domain adaptation?

- Concept: Domain adaptation theory and distribution discrepancy
  - Why needed here: The paper uses theoretical bounds to justify why task-similar data selection improves performance
  - Quick check question: How does minimizing distribution discrepancy between task and domain data relate to continual pre-training effectiveness?

- Concept: Perplexity as a measure of token novelty
  - Why needed here: Perplexity is used to identify novel samples that were underrepresented in original training
  - Quick check question: Why would samples with high perplexity be more valuable for domain adaptation?

## Architecture Onboarding

- Component map: Data curation pipeline (financial corpus creation) -> Embedding similarity computation (Spacy for efficiency) -> Perplexity calculation (surrogate model Pythia-70m) -> Entropy calculation (POS tagging for diversity) -> Sampling strategy (hard vs soft selection) -> Continual pre-training (DeepSpeed ZeRO Stage 2, bf16 precision)

- Critical path:
  1. Data curation and preprocessing
  2. Compute embeddings/perplexity/entropy for all samples
  3. Rank samples by selection metric(s)
  4. Select top-k samples for pre-training
  5. Run continual pre-training with small learning rate
  6. Evaluate on financial and open-domain tasks

- Design tradeoffs:
  - Speed vs accuracy: Using Pythia-70m as surrogate for perplexity vs full model
  - Hard vs soft sampling: Binary selection vs weighted sampling
  - Task-aware vs task-agnostic: Requires task data vs works without it
  - Spacy embeddings vs LLM embeddings: Computational efficiency vs potential accuracy

- Failure signatures:
  - Degradation on open-domain tasks: Overfitting to domain, learning rate too high
  - No improvement on domain tasks: Selection metrics ineffective, insufficient data
  - High variance across runs: Sampling instability, need for more seeds
  - Loss curves don't correlate with task performance: Validation setup issues

- First 3 experiments:
  1. Run DACP with 100% financial corpus and compare to Pythia baseline
  2. Implement ETS-DACP with task data and compare to DACP performance
  3. Implement ETA-DACP with perplexity and entropy, compare to task-aware methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term effects of continual pre-training on domain-specific LLMs' ability to generalize across multiple domains?
- Basis in paper: [inferred] The paper focuses on domain-specific continual pre-training but does not extensively explore the trade-offs between specialization and generalization across multiple domains over extended periods.
- Why unresolved: The study evaluates the impact of continual pre-training on domain-specific tasks and general tasks but does not address how this affects the model's performance on tasks from other domains not seen during pre-training. It also does not explore the long-term effects as the model is continually updated with new domain-specific data.
- What evidence would resolve it: Longitudinal studies comparing the performance of continually pre-trained models on a diverse set of tasks from various domains over time, including assessments of their ability to transfer knowledge and skills to new, unseen domains.

### Open Question 2
- Question: How does the choice of data selection metrics (similarity, perplexity, entropy) influence the balance between domain-specific performance and the retention of general capabilities in LLMs?
- Basis in paper: [explicit] The paper introduces and evaluates three data selection metrics (similarity, perplexity, entropy) for efficient domain-adaptive continual pre-training, observing their effects on domain-specific and general task performance.
- Why unresolved: While the paper demonstrates the effectiveness of these metrics in improving domain-specific performance and maintaining general capabilities, it does not deeply analyze how the choice and combination of these metrics specifically influence this balance. The optimal configuration for different domains or tasks remains unclear.
- What evidence would resolve it: Systematic experiments varying the weights and combinations of the data selection metrics across different domains and tasks, measuring the impact on both domain-specific and general performance, and identifying the optimal configurations for various scenarios.

### Open Question 3
- Question: What are the computational and environmental costs of continually pre-training large language models on domain-specific data compared to training from scratch, and how do these costs scale with model size and domain specificity?
- Basis in paper: [explicit] The paper highlights the cost-effectiveness of continual pre-training over training from scratch but does not provide a detailed analysis of the computational and environmental costs involved.
- Why unresolved: The study mentions the cost savings of continual pre-training but lacks a comprehensive breakdown of the computational resources required, including energy consumption and carbon footprint. It also does not explore how these costs scale with different model sizes and the specificity of the domain data.
- What evidence would resolve it: Detailed cost analyses comparing continual pre-training and training from scratch across various model sizes and domain specificities, including metrics on computational resources, energy usage, and environmental impact, to provide a clearer understanding of the trade-offs involved.

## Limitations

- Data curation process is underspecified, making reproducibility challenging for different financial corpora
- Evaluation relies on a limited set of tasks, with FiQA SA showing notably different performance patterns
- Statistical significance testing is absent, with only single runs reported for most experiments

## Confidence

- High Confidence: The core finding that continual pre-training on domain-specific data improves domain task performance while maintaining open-domain capabilities is well-supported by consistent patterns across multiple experiments and model scales (Pythia-1B and Pythia-6.9B)
- Medium Confidence: The efficiency claims (10% data achieving similar performance) are supported by direct comparisons, but the generalization to other domains remains untested
- Low Confidence: The mechanism explanations, particularly around why perplexity and entropy capture "novel knowledge" effectively, remain somewhat hand-wavy without deeper analysis of what specific knowledge is being learned or transferred

## Next Checks

1. **Ablation on Data Selection Components**: Run experiments isolating perplexity selection, entropy selection, and similarity selection independently to quantify their individual contributions to the 10% efficiency claim

2. **Cross-Domain Generalization**: Apply the exact methodology to a non-financial domain (e.g., biomedical or legal) to test whether the 10% efficiency finding generalizes beyond the financial domain

3. **Learning Rate Sensitivity Analysis**: Systematically vary learning rates during continual pre-training to establish the precise threshold where open-domain capability degradation begins, providing clearer guidance on safe adaptation ranges