---
ver: rpa2
title: Bayesian Online Learning for Consensus Prediction
arxiv_id: '2312.07679'
source_url: https://arxiv.org/abs/2312.07679
tags:
- distribution
- consensus
- rate
- infexp
- querying
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a Bayesian framework for online consensus prediction
  from multiple human experts and a single pre-trained model. The problem setting
  assumes that an oracle ground truth is unavailable, and instead the consensus of
  N human experts defines the target label.
---

# Bayesian Online Learning for Consensus Prediction

## Quick Facts
- arXiv ID: 2312.07679
- Source URL: https://arxiv.org/abs/2312.07679
- Reference count: 40
- Primary result: Proposed Bayesian methods (InfExp and FinExp) significantly outperform baselines in online consensus prediction with human experts and pre-trained models

## Executive Summary
This work introduces a Bayesian framework for online consensus prediction from multiple human experts and a single pre-trained model. The framework assumes no oracle ground truth is available, instead defining the target label as the consensus of N human experts. A generative model based on the multivariate hypergeometric distribution is proposed, with a computationally efficient limiting case (InfExp) for large N. The methods dynamically learn prior parameters online to calibrate model performance and estimate unobserved expert votes.

## Method Summary
The method uses a Bayesian framework where the multivariate hypergeometric distribution models the likelihood of observing partial expert votes without replacement. A generative model with prior parameters (θ, ϕ, τ) is learned via MAP estimation using a sliding window approach. The limiting case InfExp provides a closed-form posterior approximation for large N, reducing computational cost while maintaining accuracy. Model predictions serve as priors to estimate unobserved votes, and the system dynamically adapts to changing conditions.

## Key Results
- InfExp and FinExp methods significantly outperform random, entropy-based, and model picker baselines
- Methods demonstrate robustness to distribution shift in experiments on CIFAR-10H and ImageNet-16H
- Performance improvements maintained across various budget and accuracy settings
- Improved performance under no-query scenarios compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
The multivariate hypergeometric distribution models the likelihood of observing partial expert votes without replacement. Votes are drawn from a finite pool of N experts; without replacement sampling follows a multivariate hypergeometric distribution. Core assumption: Expert votes are conditionally independent given the true class and the total number of experts N is fixed and known.

### Mechanism 2
MAP estimates of prior parameters (θ, ϕ, τ) adapt to model performance and calibrate predictions per class. Learning τ allows per-class scaling of model confidences; high τk means model is well-calibrated for class k. Core assumption: Model predictions are exchangeable with expert votes after scaling by τ; τ can be learned online from observed votes.

### Mechanism 3
InfExp (infinite expert limit) provides a closed-form posterior that approximates FinExp for large N, reducing computational cost. As N→∞, normalized vote counts Ht/N converge in distribution to πt, yielding DirMult posterior instead of exact hypergeometric sampling. Core assumption: N is large enough that finite-sample effects are negligible and the limiting distribution is a good approximation.

## Foundational Learning

- Concept: Bayesian updating with conjugate priors (Dirichlet-Multinomial)
  - Why needed here: Allows efficient online posterior updates for vote distributions without expensive sampling
  - Quick check question: If αt is updated to αt + H Nt t after observing Nt votes, does the posterior remain Dirichlet?

- Concept: Online learning via sliding window MAP estimation
  - Why needed here: Enables adaptation to changing model performance and distribution shift without retraining
  - Quick check question: If the sliding window size w is too small, what happens to parameter stability?

- Concept: Decision heuristics based on posterior predictive accuracy
  - Why needed here: Provides interpretable stopping rule for querying experts, trading off cost vs accuracy
  - Quick check question: If Acct(H Nt t) is computed from p(yt = k | H Nt t, α∗ t), what does a high value imply?

## Architecture Onboarding

- Component map: Pre-trained model predictions ft, partial expert votes H Nt t -> Bayesian posterior update (Dirichlet-Multinomial) -> MAP parameter learning (θ, ϕ, τ) -> Predicted class yt, query decision based on Acct threshold
- Critical path: 1. Receive ft and optionally new vote ht 2. Update posterior p(yt | history, α∗ t) 3. Compute Acct(H Nt t) = maxk p(yt = k | H Nt t, α∗ t) 4. If Acct > threshold ρ, predict; else query next expert 5. Periodically update MAP estimate of Θ using sliding window
- Design tradeoffs: Exact vs approximate posterior (FinExp is accurate but expensive; InfExp is fast but approximate), sliding window size w (larger w = stable parameters but slower adaptation; smaller w = fast adaptation but noisier parameters), threshold ρ (higher ρ = fewer queries but possibly lower accuracy; lower ρ = higher accuracy but higher cost)
- Failure signatures: Poor calibration (τ values do not correlate with true model performance; Acct predictions are overconfident/underconfident), slow adaptation (MAP parameters lag behind distribution shift; error rate increases sharply after shift), computational bottleneck (sampling in FinExp likelihood dominates runtime)
- First 3 experiments: 1. Run FinExp and InfExp on a small synthetic dataset with known ground truth to verify posterior updates 2. Evaluate calibration by plotting τ against true per-class model accuracy on CIFAR-10H 3. Test distribution shift robustness by running methods on ImageNet-16H with clean→noisy transition and measuring error-rate jump

## Open Questions the Paper Calls Out
- How do the proposed methods perform under open-set conditions where new classes may be progressively observed?
- How sensitive are the methods to the choice of prior hyperparameters (aθ, bθ, aϕ, bϕ, aτ, bτ)?
- Can the methods handle non-stationary data streams where the underlying data distribution changes over time?
- How do the methods perform with variable cost experts (e.g., different costs for querying different experts)?

## Limitations
- Framework assumes conditional independence of expert votes given true class, which may not hold when experts share biases
- Limiting case (InfExp) requires sufficiently large N for accurate approximation, with no minimum N specified
- MAP estimation relies on sliding window updates with unclear window size selection criteria

## Confidence

**High Confidence**: Theoretical derivation of multivariate hypergeometric distribution and conjugate Dirichlet-Multinomial posterior updates are mathematically sound; computational efficiency gains from InfExp vs FinExp are clearly demonstrated.

**Medium Confidence**: Calibration mechanism through per-class τ parameters shows promise, but exchangeability assumption between model predictions and expert votes needs further validation.

**Low Confidence**: Robustness claims for distribution shift based on limited experiments; real-world shifts may exhibit more complex patterns challenging the online adaptation mechanisms.

## Next Checks
1. Systematically evaluate FinExp vs InfExp performance across different N values (e.g., N=10, 50, 100, 200) on CIFAR-10H to quantify when limiting approximation becomes reliable.

2. Design experiments to measure empirical correlations between expert votes and assess how violations of conditional independence affect posterior estimates and final accuracy.

3. Conduct ablation studies varying sliding window size w on distribution-shifted data to identify optimal tradeoffs between parameter stability and adaptation speed.