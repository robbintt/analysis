---
ver: rpa2
title: No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models
  with Monte Carlo Tree Search Guided by Energy Function
arxiv_id: '2309.03224'
source_url: https://arxiv.org/abs/2309.03224
tags:
- reasoning
- arxiv
- function
- energy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method to improve the mathematical
  reasoning ability of fine-tuned large language models (LLMs) without additional
  fine-tuning. The core idea is to reformulate the fine-tuned LLM into a Residual
  Energy-Based Model (Residual-EBM) and employ Monte Carlo Tree Search (MCTS) guided
  by an energy function to search the output space and evaluate the reasoning path.
---

# No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function

## Quick Facts
- arXiv ID: 2309.03224
- Source URL: https://arxiv.org/abs/2309.03224
- Authors: 
- Reference count: 40
- Primary result: Improves mathematical reasoning of fine-tuned LLMs without additional fine-tuning using Residual-EBM + MCTS + NCE

## Executive Summary
This paper introduces a novel approach to enhance mathematical reasoning in fine-tuned large language models without requiring additional fine-tuning. The method reformulates the fine-tuned LLM into a Residual Energy-Based Model (Residual-EBM) and employs Monte Carlo Tree Search (MCTS) guided by an energy function to search for optimal reasoning paths. The energy function is trained using Noise Contrastive Estimation (NCE) to discriminate between real and generated data. Extensive experiments on GSM8k and MATH benchmarks demonstrate significant improvements in pass@1 accuracy, achieving comparable performance to state-of-the-art methods without additional fine-tuning or reinforcement learning.

## Method Summary
The method involves reformulating a fine-tuned LLM into a Residual-EBM, training an energy function using NCE, and applying MCTS guided by the energy function to search for optimal reasoning paths. The Residual-EBM allows the model to adjust its output distribution without retraining, while MCTS efficiently explores the reasoning path space by balancing exploration and exploitation using the energy function as a reward signal. NCE enables efficient training of the energy function without requiring a partition function, using the fine-tuned LLM as a noise distribution to generate samples for training.

## Key Results
- The proposed method significantly improves pass@1 accuracy on GSM8k and MATH benchmarks
- Achieves comparable performance to state-of-the-art methods without additional fine-tuning or RLHF
- MCTS guided by energy function outperforms greedy decoding and achieves comparable results to self-consistency majority voting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating the fine-tuned LLM into a Residual Energy-Based Model (Residual-EBM) allows the model to adjust its output distribution without retraining, enabling better mathematical reasoning.
- Mechanism: The fine-tuned LLM acts as a base distribution, and the energy function Eθ(x) modifies this distribution to better align with the target distribution of correct mathematical reasoning paths. This is achieved by optimizing the energy function using Noise Contrastive Estimation (NCE), which discriminates between real and generated data.
- Core assumption: The energy function can effectively guide the model towards correct reasoning paths without the need for additional fine-tuning.
- Evidence anchors:
  - [abstract]: "The core idea is to reformulate the fine-tuned LLM into a Residual Energy-Based Model (Residual-EBM) and employ Monte Carlo Tree Search (MCTS) guided by an energy function to search the output space and evaluate the reasoning path."
  - [section]: "We first re-formalize the finetuned-LLMs to a Residual-based Energy Model (Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function."
- Break condition: If the energy function fails to effectively discriminate between real and generated data, or if the Residual-EBM reformulation does not improve the model's reasoning capabilities.

### Mechanism 2
- Claim: Using Monte Carlo Tree Search (MCTS) guided by the energy function allows for a more efficient exploration of the reasoning path space, balancing exploration and exploitation.
- Mechanism: MCTS explores the reasoning path tree by selecting nodes that maximize the Upper Confidence Trees (UCT) criterion, which combines the average reward from simulations and the node's prior probability. The energy function serves as the reward, guiding the search towards more promising reasoning paths.
- Core assumption: The energy function can provide meaningful rewards that reflect the quality of reasoning paths, enabling MCTS to effectively balance exploration and exploitation.
- Evidence anchors:
  - [abstract]: "We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path."
  - [section]: "We propose to apply energy function derived from Residual-EBM to guide the searching process of MCTS to find the optimal solution."
- Break condition: If the energy function rewards do not correlate with the actual quality of reasoning paths, or if MCTS fails to find optimal solutions due to the complexity of the search space.

### Mechanism 3
- Claim: The use of Noise Contrastive Estimation (NCE) to train the energy function allows for efficient training without the need for a partition function, which is typically intractable.
- Mechanism: NCE trains the energy function by treating it as a binary classifier that distinguishes between real data and generated data. The model uses the fine-tuned LLM as a noise distribution to generate samples, which are then used to train the energy function.
- Core assumption: NCE can effectively train the energy function without the need for explicit computation of the partition function, and the noise distribution (fine-tuned LLM) can provide sufficient diversity in generated samples.
- Evidence anchors:
  - [abstract]: "We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path."
  - [section]: "Since the partition function is intractable, it’s hard to train globally normalized models [23, 28] via Maximum Likelihood Estimation (MLE). Instead, we apply Noise Contrastive Estimation (NCE) [24] to train energy function Eθ(x)."
- Break condition: If the noise distribution is insufficient or the NCE training fails to converge, the energy function may not be effectively trained.

## Foundational Learning

- Concept: Energy-Based Models (EBMs)
  - Why needed here: Understanding EBMs is crucial for grasping how the Residual-EBM reformulation works and how the energy function guides the model's reasoning.
  - Quick check question: What is the role of the partition function in an Energy-Based Model, and why is it challenging to compute?

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS is the core algorithm used to explore the reasoning path space, and understanding its mechanics is essential for implementing the method.
  - Quick check question: How does the Upper Confidence Trees (UCT) criterion balance exploration and exploitation in MCTS?

- Concept: Noise Contrastive Estimation (NCE)
  - Why needed here: NCE is used to train the energy function without requiring the partition function, and understanding its principles is key to the method's efficiency.
  - Quick check question: How does NCE use a noise distribution to train a binary classifier, and why is this approach effective for training energy functions?

## Architecture Onboarding

- Component map: Fine-tuned LLM -> Energy Function (Eθ) -> MCTS -> Mathematical Problem Solution
- Critical path: Fine-tune LLM on instruction-response pairs -> Reformulate LLM into Residual-EBM -> Train energy function using NCE with samples from fine-tuned LLM -> Apply MCTS guided by energy function to solve mathematical problems
- Design tradeoffs:
  - Using a sentence as a state in MCTS reduces computation but may lose granularity
  - Rejection sampling and suboutput sampling methods for generating noise samples offer different trade-offs in terms of diversity and overlap with ground-truth data
- Failure signatures:
  - If the energy function does not effectively discriminate between real and generated data, MCTS may not find optimal solutions
  - If the fine-tuned LLM does not provide sufficient diversity in generated samples, NCE training may fail
- First 3 experiments:
  1. Fine-tune Llama 2 on GSM8k and MATH datasets using the provided prompt format
  2. Train the energy function using NCE with rejection sampling and suboutput sampling methods
  3. Apply MCTS guided by the energy function to solve mathematical problems on GSM8k and MATH benchmarks, comparing results with baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of noise samples generated by rejection sampling and suboutput sampling affect the performance of the energy function in guiding MCTS?
- Basis in paper: [explicit] The paper mentions that "The MCTS guided by energy function outperforms the greedy-decoding and achieves comparable results to self-consistency-majority-voting demonstrating the effectiveness of our method" and discusses the importance of noise samples.
- Why unresolved: The paper does not provide a detailed analysis of how the quality of noise samples specifically impacts the performance of the energy function and MCTS.
- What evidence would resolve it: A comparative study of MCTS performance using noise samples of varying quality, along with an analysis of the energy function's ability to discriminate between real and generated data under different noise sample conditions.

### Open Question 2
- Question: Can the proposed method be extended to other complex reasoning tasks beyond mathematical problems, such as logical reasoning or code generation?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the method on mathematical reasoning tasks, but does not explore its applicability to other types of reasoning tasks.
- Why unresolved: The paper focuses on mathematical reasoning benchmarks and does not provide evidence or analysis of the method's performance on other reasoning tasks.
- What evidence would resolve it: Experiments applying the method to other reasoning tasks, such as logical reasoning or code generation, and a comparison of performance with state-of-the-art methods in those domains.

### Open Question 3
- Question: How does the proposed method compare to other approaches that use reinforcement learning or human feedback for fine-tuning language models on mathematical reasoning tasks?
- Basis in paper: [explicit] The paper mentions that "Compared to WizardMath [21] and RFT [6], our method achieve comparable pass@1 accuracy without using more supervised finetuning data [6] or complicated RLHF alignment methods [21]."
- Why unresolved: The paper does not provide a detailed comparison of the proposed method with other reinforcement learning or human feedback-based approaches, including their strengths and weaknesses.
- What evidence would resolve it: A comprehensive comparison of the proposed method with other reinforcement learning or human feedback-based approaches, including their performance, computational efficiency, and scalability.

## Limitations
- The computational cost of MCTS guided by the energy function may become prohibitive for larger models or problems requiring deeper reasoning chains
- The effectiveness of the Residual-EBM reformulation depends heavily on the quality of the fine-tuned base model, limiting performance gains when starting from weaker models
- The method's reliance on a specific sentence-level state representation in MCTS could potentially lose important intermediate reasoning details

## Confidence
- **High confidence** in the core methodology: The combination of Residual-EBM reformulation, NCE training for the energy function, and MCTS guided search is technically sound and well-grounded in existing literature
- **Medium confidence** in the empirical results: While the reported improvements are substantial, the comparison with limited baselines and absence of extensive ablation studies make it difficult to fully attribute the gains to specific components
- **Low confidence** in the practical deployment considerations: The paper does not adequately address computational overhead, inference latency, or the method's behavior on out-of-distribution mathematical problems

## Next Checks
1. **Ablation study on state representation granularity**: Systematically evaluate the impact of using different state granularities (words vs. sentences vs. entire solutions) in MCTS on both performance and computational efficiency
2. **Robustness testing on adversarial and out-of-distribution problems**: Generate or curate a dataset of mathematically valid but challenging problems that differ significantly from the GSM8k and MATH training distributions
3. **Scalability analysis with larger models and deeper search**: Implement the method with larger language models and systematically vary the MCTS parameters to measure the trade-off between performance gains and computational cost