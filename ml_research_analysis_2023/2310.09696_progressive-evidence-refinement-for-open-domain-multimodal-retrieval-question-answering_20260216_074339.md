---
ver: rpa2
title: Progressive Evidence Refinement for Open-domain Multimodal Retrieval Question
  Answering
arxiv_id: '2310.09696'
source_url: https://arxiv.org/abs/2310.09696
tags:
- evidence
- question
- retrieval
- sources
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a two-stage evidence retrieval and question-answering
  framework to address challenges in multimodal retrieval-based question answering,
  namely information loss due to compressed evidence features and a gap between evidence
  and question feature extraction. The framework introduces a progressive evidence
  refinement strategy with initial screening and iterative retrieval to select crucial
  evidence, a semi-supervised contrastive learning approach with negative samples
  to expand the question domain, and a multi-turn retrieval and question-answering
  strategy to handle multimodal inputs.
---

# Progressive Evidence Refinement for Open-domain Multimodal Retrieval Question Answering

## Quick Facts
- arXiv ID: 2310.09696
- Source URL: https://arxiv.org/abs/2310.09696
- Reference count: 0
- State-of-the-art performance: 89.6% retrieval F1 and 44.4% QA accuracy on WebQA

## Executive Summary
This paper introduces a two-stage evidence retrieval and question-answering framework for multimodal retrieval-based QA. The framework addresses challenges of information loss from compressed evidence features and gaps between evidence and question feature extraction through progressive evidence refinement, negative sample semi-supervised contrastive learning, and multi-turn retrieval with cross-modal attention. Experiments demonstrate state-of-the-art performance on WebQA and MultimodelQA benchmarks.

## Method Summary
The framework consists of an Evidence Initial Screening Module (EISM) that uses cosine similarity between question and evidence encoders to retrieve top candidates, followed by Iterative Evidence Retrieval (IER) that uses BERT-based matching to incrementally select evidence. A multi-turn QA model combines ViT, LLaMA, and LoRA to handle multimodal inputs through cross-attention mechanisms. Training employs negative sample semi-supervised contrastive learning to expand the question domain and enhance robustness.

## Key Results
- 89.6% retrieval F1 and 44.4% QA accuracy on WebQA benchmark
- 62.8% F1 and 67.8% F1 on MultimodelQA benchmark
- State-of-the-art performance across both evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
Progressive evidence refinement mitigates information loss by using multimodal input directly in the QA stage instead of pooled features. Initial screening narrows candidates, then iterative retrieval selects most relevant evidence based on logical ordering. The QA model receives raw multimodal content rather than compressed embeddings, preserving fine-grained details. If iterative retrieval fails to find correct sequence, QA receives noisy or incomplete evidence, degrading accuracy.

### Mechanism 2
Negative sample semi-supervised contrastive learning improves initial screening by leveraging distractor evidence. Each distractor is paired with itself as pseudo-question during training, forcing encoder to distinguish relevant from irrelevant sources more robustly. If distractor samples are too similar to real evidence, model may learn incorrect associations.

### Mechanism 3
Multi-turn dialogue with cross-modal attention enables simultaneous reasoning over multiple images. Each evidence piece is fed as dialogue history; cross-attention captures relationships between images and text in real time. If attention weights collapse to single modality, model loses multi-hop reasoning capability.

## Foundational Learning

- **Concept**: Contrastive learning with negative samples
  - Why needed here: Enables model to differentiate between relevant and irrelevant evidence in high-dimensional space
  - Quick check question: In batch of question-evidence pairs, how do you ensure distractors are not inadvertently treated as positives?

- **Concept**: Cross-modal attention in transformer architectures
  - Why needed here: Allows model to jointly reason over text and images without modality-specific preprocessing
  - Quick check question: What happens to attention distribution if one modality has significantly more tokens than other?

- **Concept**: Iterative evidence retrieval with logical ordering
  - Why needed here: Ensures retrieved evidence follows causal chain needed for multi-hop reasoning
  - Quick check question: How does termination condition prevent infinite retrieval loops?

## Architecture Onboarding

- **Component map**: Question → EISM → IER → QA Module → Answer
- **Critical path**: Question flows through EISM for initial screening, IER for iterative retrieval, then QA Module for final answer generation
- **Design tradeoffs**:
  - Raw multimodal input preserves details but increases compute vs pooled embeddings
  - Top-k filtering speeds retrieval but risks discarding marginally relevant evidence
  - Self-paired distractors improve robustness but may introduce noise if distractors are too similar to true evidence
- **Failure signatures**:
  - Low recall@k in EISM → insufficient candidates for IER
  - IER selects termination symbol too early → missing key evidence
  - QA model attends only to one modality → loss of multi-hop reasoning
- **First 3 experiments**:
  1. Verify EISM recall@k on held-out validation set
  2. Test IER retrieval accuracy with and without negative sample contrastive loss
  3. Measure QA accuracy when feeding raw vs pooled evidence features

## Open Questions the Paper Calls Out

### Open Question 1
How does the progressive evidence refinement strategy perform in terms of computational efficiency compared to other state-of-the-art models? The paper mentions temporal efficiency but does not provide detailed comparison of computational efficiency, including time and space complexity, with other models.

### Open Question 2
How does the negative sample semi-supervised contrastive learning training strategy impact the model's ability to filter out irrelevant evidence in real-world applications? The paper introduces this strategy but lacks empirical evidence on its effectiveness in real-world applications.

### Open Question 3
How does the multi-turn retrieval and question-answering strategy handle the challenge of multimodal inputs with varying levels of complexity? The paper proposes this strategy but does not provide detailed analysis of how it handles multimodal inputs with varying complexity levels.

## Limitations

- Implementation details for iterative retrieval module are unspecified, particularly how to determine optimal number of evidence sources and handle distractors
- Exact hyperparameters for negative sample semi-supervised contrastive learning strategy are missing
- Cross-modal attention mechanism's effectiveness in capturing inter-evidence dependencies across multiple images requires empirical validation

## Confidence

- **High Confidence**: Overall framework design and benchmark performance claims
- **Medium Confidence**: Progressive evidence refinement and negative sample contrastive learning mechanisms
- **Low Confidence**: Cross-modal attention mechanism's ability to model inter-evidence dependencies in multi-turn dialogue manner

## Next Checks

1. Evaluate EISM recall@k on held-out validation set to ensure sufficient candidates for iterative retrieval
2. Test IER retrieval accuracy with and without negative sample contrastive loss to quantify impact on filtering distractors
3. Measure QA accuracy when feeding raw vs pooled evidence features to assess information preservation benefits