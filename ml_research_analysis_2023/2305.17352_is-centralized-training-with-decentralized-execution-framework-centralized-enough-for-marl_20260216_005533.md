---
ver: rpa2
title: Is Centralized Training with Decentralized Execution Framework Centralized
  Enough for MARL?
arxiv_id: '2305.17352'
source_url: https://arxiv.org/abs/2305.17352
tags:
- uni00000013
- uni00000011
- uni00000003
- agent
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Centralized Advising and Decentralized
  Pruning (CADP) framework for multi-agent reinforcement learning (MARL) to address
  the inefficiency of the widely-used Centralized Training with Decentralized Execution
  (CTDE) framework in utilizing global information for training. CADP enables agents
  to exchange advice during centralized training, enhancing the agent policy network
  from individual to global cognition.
---

# Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?

## Quick Facts
- arXiv ID: 2305.17352
- Source URL: https://arxiv.org/abs/2305.17352
- Reference count: 40
- This paper proposes a novel Centralized Advising and Decentralized Pruning (CADP) framework for MARL that outperforms state-of-the-art CTDE methods on StarCraft II and Google Research Football benchmarks.

## Executive Summary
This paper addresses a fundamental limitation in the widely-used Centralized Training with Decentralized Execution (CTDE) framework for multi-agent reinforcement learning (MARL): its inability to effectively leverage global information during centralized training while maintaining decentralized execution. The proposed Centralized Advising and Decentralized Pruning (CADP) framework introduces an explicit communication channel during training where agents can exchange advice, enhancing their policies from individual to global cognition. Crucially, a smooth pruning mechanism progressively removes these communication dependencies, ensuring true decentralized execution without sacrificing performance. Extensive experiments on StarCraft II micromanagement and Google Research Football demonstrate that CADP achieves superior performance compared to both traditional CTDE and teacher-student CTDE methods.

## Method Summary
CADP extends the CTDE framework by introducing two key innovations: cross-attention-based advice exchange during centralized training and a smooth pruning mechanism for decentralized execution. During training, each agent uses cross-attention to aggregate advice from other agents' cognition (key) and intention (value) vectors, weighted by confidence scores, enabling collective policy optimization beyond individual local observations. After centralized training converges (controlled by timestep T), a KL-divergence-based pruning loss progressively encourages agent confidence weights to converge to one-hot vectors, removing cross-agent dependencies while preserving learned cooperative strategies. The framework is evaluated on StarCraft II micromanagement and Google Research Football benchmarks, demonstrating significant performance improvements over state-of-the-art CTDE methods.

## Key Results
- CADP outperforms state-of-the-art CTDE and teacher-student CTDE methods on StarCraft II micromanagement benchmarks
- The decentralized CADP model maintains performance close to its centralized counterpart after pruning
- CADP achieves superior results across different MARL backbones, validating its general applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention between agents during centralized training improves team-oriented policy learning.
- Mechanism: Each agent uses cross-attention to aggregate advice from other agents' cognition (key) and intention (value) vectors, weighted by confidence scores. This enables collective policy optimization beyond individual local observations.
- Core assumption: Agent policies can be improved by modeling dependencies among agents during training, and these dependencies can be safely pruned for decentralized execution.
- Evidence anchors:
  - [abstract] "CADP endows agents the explicit communication channel to seek and take advice from different agents for more centralized training."
  - [section 4.1] "we design a novel centralized training scheme to augment the agent policy network from the local cognition of an individual agent to the global cognition from all agents."
- Break condition: If pruning loss cannot effectively remove agent dependencies without degrading performance, or if cross-attention introduces instability during training.

### Mechanism 2
- Claim: Smooth model pruning via auxiliary loss enables safe transition from centralized to decentralized execution.
- Mechanism: A KL-divergence-based pruning loss progressively encourages agent confidence weights to converge to one-hot vectors, removing cross-agent dependencies. The process starts after centralized training converges (controlled by timestep T).
- Core assumption: Agent models can be incrementally pruned without catastrophic forgetting, preserving learned cooperative strategies.
- Evidence anchors:
  - [abstract] "a smooth model pruning mechanism is introduced, which progressively constrains agent communication without degrading cooperation capability."
  - [section 4.2] "we design an auxiliary loss function named pruning loss Lp to help the decentralized agent gradually alleviate the dependence of other agents."
- Break condition: If pruning starts too early (T too small) before centralized convergence, or if pruning rate (α) is too aggressive causing performance collapse.

### Mechanism 3
- Claim: Global state information can be implicitly leveraged via agent advice instead of direct input.
- Mechanism: Rather than feeding global state into agent policies (as in teacher-student methods), CADP enables agents to infer global context through exchanging local cognition and intention vectors. This avoids violating decentralized execution constraints while enriching centralized training.
- Core assumption: Local observations contain sufficient information to reconstruct global context when aggregated across agents via cross-attention.
- Evidence anchors:
  - [abstract] "agents can exchange their advice during centralized training and then prune the dependence relationship for decentralized execution."
  - [section 1] "Several works recently improve CTDE with the teacher-student framework... However, these works just simply take the additional state information as the input of agent policy..."
- Break condition: If local observations are too limited or redundant to reconstruct global context, leading to suboptimal policy learning.

## Foundational Learning

- Concept: Cross-attention mechanisms in transformer architectures
  - Why needed here: CADP uses cross-attention to enable agents to exchange cognition and intention vectors during centralized training.
  - Quick check question: What are the roles of query, key, and value in a cross-attention operation, and how does scaling by √d_x stabilize training?

- Concept: Kullback-Leibler divergence as a regularization tool
  - Why needed here: KL divergence measures the difference between current agent confidence weights and one-hot vectors during pruning, guiding the model toward independence.
  - Quick check question: How does minimizing KL divergence between a probability distribution and a one-hot vector affect the entropy of the distribution?

- Concept: Value decomposition in multi-agent reinforcement learning
  - Why needed here: CADP builds upon value decomposition methods like QMIX, requiring understanding of how individual Q-values are combined into a joint action value.
  - Quick check question: What is the Individual-Global-Max (IGM) principle, and why is it important for decentralized execution in value decomposition methods?

## Architecture Onboarding

- Component map: Agent policy network -> Mixing network -> Target network
- Critical path:
  1. Collect trajectories with advice exchange enabled during centralized training
  2. Compute TD loss and pruning loss (after T timesteps)
  3. Backpropagate through agent networks and mixing network
  4. Periodically update target network
  5. After training, deploy decentralized model (without Q, K modules) for execution

- Design tradeoffs:
  - Starting pruning early (small T) vs. late (large T): Early pruning may hinder centralized learning but speeds up transition; late pruning ensures better centralized convergence but delays decentralized readiness
  - Pruning rate (α): Higher α speeds up pruning but risks performance degradation; lower α is safer but slower
  - Cross-attention complexity vs. performance: More sophisticated attention mechanisms may improve advice quality but increase computational cost

- Failure signatures:
  - Decentralized model performance much lower than centralized model: Pruning loss too aggressive or started too early
  - Both models perform poorly: Cross-attention not improving centralized training, or hyperparameters mismatched
  - Centralized model overfits to advice exchange: Decentralized execution cannot recover essential coordination without communication

- First 3 experiments:
  1. Run CADP with T=0 (pruning from start) on a simple SMAC scenario to test robustness to aggressive pruning
  2. Compare CADP with different α values (e.g., 0.1, 1.0, 10.0) on 5m_vs_6m to find optimal pruning rate
  3. Validate that decentralized model (CADP(D)) maintains performance close to centralized model (CADP(C)) on corridor after pruning completes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of global information that can be effectively utilized in the CADP framework before the decentralized pruning mechanism starts to degrade performance?
- Basis in paper: [inferred] The paper mentions that the pruning loss Lp is introduced after the centralized model reaches a high level of performance, suggesting there is a limit to how much global information can be used before pruning becomes necessary. The paper does not provide a theoretical analysis of this limit.
- Why unresolved: The paper does not provide a theoretical analysis of the maximum amount of global information that can be effectively utilized in the CADP framework before the pruning mechanism starts to degrade performance. This would require a deeper theoretical understanding of the interaction between global information utilization and the pruning mechanism.
- What evidence would resolve it: A theoretical analysis of the CADP framework that derives the optimal amount of global information that can be utilized before pruning, along with experimental validation of this theoretical limit.

### Open Question 2
- Question: How does the CADP framework perform in scenarios with more than 10 agents, and what are the scaling limitations?
- Basis in paper: [inferred] The paper only tests the CADP framework on scenarios with up to 10 agents. It is unclear how the framework would perform in larger-scale multi-agent systems, which are common in real-world applications.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis of the CADP framework's performance in scenarios with more than 10 agents. This would require extensive experimentation and potentially new theoretical insights into the scalability of the framework.
- What evidence would resolve it: Experimental results showing the performance of the CADP framework in scenarios with more than 10 agents, along with a theoretical analysis of the scaling limitations of the framework.

### Open Question 3
- Question: Can the pruning loss Lp be dynamically adjusted during training to optimize the trade-off between centralized and decentralized performance?
- Basis in paper: [explicit] The paper mentions that the pruning loss Lp is introduced at a fixed timestep T during training, but does not explore the possibility of dynamically adjusting the pruning loss during training to optimize performance.
- Why unresolved: The paper does not explore the possibility of dynamically adjusting the pruning loss Lp during training. This would require new algorithmic developments and experimental validation to determine the optimal way to adjust the pruning loss during training.
- What evidence would resolve it: An experimental study comparing the performance of the CADP framework with fixed and dynamically adjusted pruning loss, along with a theoretical analysis of the optimal way to adjust the pruning loss during training.

## Limitations

- The paper lacks critical implementation details for the pruning loss function and message exchanging mechanism
- Key hyperparameters (α coefficient, pruning start timestep T) are not specified per scenario, making exact reproduction challenging
- The "efficient training" claim is not substantiated with wall-clock time comparisons or sample efficiency metrics

## Confidence

**High confidence**: The fundamental claim that CADP outperforms CTDE baselines on SMAC and GRF benchmarks, supported by win rate improvements across multiple scenarios. The pruning mechanism successfully transitions from centralized to decentralized execution without catastrophic performance loss.

**Medium confidence**: The mechanism by which cross-attention improves centralized training is theoretically sound but the paper doesn't provide quantitative evidence of how much individual performance improves during the centralized phase. The claim that local observations contain sufficient information for global context reconstruction needs more rigorous validation.

**Low confidence**: The assertion that CADP provides "more efficient training" is not substantiated with wall-clock time comparisons or sample efficiency metrics. The paper focuses solely on final performance without analyzing training dynamics.

## Next Checks

1. **Component isolation**: Run ablation studies comparing CADP with only the advising component (no pruning) and only the pruning component (no advising) to quantify their individual contributions to performance gains.

2. **Training efficiency analysis**: Measure and compare the sample efficiency and wall-clock training time of CADP versus baseline CTDE methods across all benchmark scenarios to validate the "efficient training" claim.

3. **Pruning sensitivity**: Conduct a systematic hyperparameter sweep over pruning rate α and start timestep T to identify optimal values and determine the robustness of CADP to these critical hyperparameters.