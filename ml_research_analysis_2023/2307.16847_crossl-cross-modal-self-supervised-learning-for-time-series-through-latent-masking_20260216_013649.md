---
ver: rpa2
title: 'CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent
  Masking'
arxiv_id: '2307.16847'
source_url: https://arxiv.org/abs/2307.16847
tags:
- data
- masking
- crossl
- learning
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning from limited labeled
  data in multimodal time-series, particularly in healthcare applications. The authors
  propose CroSSL (Cross-modal Self-Supervised Learning), a novel approach that introduces
  latent masking of intermediate embeddings from modality-specific encoders and their
  aggregation into a global embedding through a cross-modal aggregator.
---

# CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking

## Quick Facts
- arXiv ID: 2307.16847
- Source URL: https://arxiv.org/abs/2307.16847
- Authors: 
- Reference count: 12
- Primary result: Proposed method CroSSL achieves 7.2% higher F1-score than previous SSL techniques and 3.2% higher than supervised benchmarks for multimodal health time-series classification.

## Executive Summary
This paper addresses the challenge of learning from limited labeled data in multimodal time-series, particularly in healthcare applications. The authors propose CroSSL (Cross-modal Self-Supervised Learning), a novel approach that introduces latent masking of intermediate embeddings from modality-specific encoders and their aggregation into a global embedding through a cross-modal aggregator. This enables handling of missing modalities and end-to-end cross-modal learning without requiring prior data preprocessing or negative-pair sampling. The authors evaluate CroSSL on various multimodal health time-series benchmarks, including motion sensors and biosignals. Results demonstrate superior performance compared to previous SSL techniques and supervised benchmarks, with an average F1-score improvement of 7.2% and 3.2%, respectively. The study also investigates the impact of different masking ratios and strategies and assesses the robustness of the learned representations to missing modalities.

## Method Summary
CroSSL employs modality-specific encoders to process each sensor's data into intermediate embeddings, which are then masked (either randomly or spatially) before being aggregated into a global embedding. The model is trained using VICReg, a regularization-based objective that maximizes variance, enforces invariance, and penalizes covariance between views. This approach eliminates the need for contrastive negative pairs while ensuring the global embedding captures cross-modal information rather than modality-specific details. After pre-training, the model is fine-tuned on labeled data for downstream classification tasks.

## Key Results
- CroSSL achieves 7.2% higher average F1-score compared to previous self-supervised learning techniques
- CroSSL outperforms supervised benchmarks by 3.2% average F1-score
- Spatial masking strategy provides 2.06%, 2.04%, and 5.3% higher F1-score compared to random masking across different datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent masking in the embedding space forces the aggregator to learn cross-modal information rather than copying modality-specific features.
- Mechanism: By randomly or spatially masking intermediate embeddings Qm, the aggregator A cannot rely on all modalities to reconstruct the global embedding Z. This forces Z to capture only information shared across modalities, discarding modality-specific details.
- Core assumption: Different modalities contain redundant cross-modal information about the same underlying phenomena, but also carry modality-specific noise that should be filtered out.
- Evidence anchors: [abstract] "masking intermediate embeddings produced by modality-specific encoders, and their aggregation into a global embedding through a cross-modal aggregator"; [section] "To retain cross-modal information in Z and disregard modality-specific information, we employ randomization in generating Z during training. This randomization is introduced through masking strategies..."
- Break condition: If modalities are truly independent (no shared underlying phenomena), the masking strategy would fail because there's no cross-modal information to learn from.

### Mechanism 2
- Claim: The VICReg regularization objective ensures that masked embeddings remain informative while preventing trivial solutions.
- Mechanism: VICReg enforces variance maximization, invariance between views, and covariance penalization. This combination ensures that the global embedding Z captures meaningful cross-modal patterns while preventing collapse to a constant or copied representation.
- Core assumption: VICReg's three terms (variance, invariance, covariance) are sufficient to prevent trivial solutions while preserving meaningful cross-modal relationships.
- Evidence anchors: [abstract] "training the embedding model using a regularization-based objective function like VICReg"; [section] "By training the embedding model using a regularization-based objective function like VICReg (Bardes et al., 2022), CroSSL eliminates the need for costly contrastive-based negative pair mining"
- Break condition: If VICReg's hyperparameters are poorly tuned, the model could either collapse (variance too low) or become too noisy (variance too high).

### Mechanism 3
- Claim: Spatial masking (hiding entire modalities) is more effective than random masking because it better simulates real-world missing data scenarios.
- Mechanism: Spatial masking forces the model to learn representations that are robust to entire sensors being unavailable, which better matches deployment scenarios where sensors may be completely missing rather than just partially corrupted.
- Core assumption: Real-world deployment scenarios are more likely to involve complete sensor failure than partial corruption of sensor data.
- Evidence anchors: [section] "Spatial masking is clearly beneficial compared to random masking by providing 2.06%, 2.04%, and 5.3% higher f1-score in fine-tunedCroSSL"; [section] "The intuition behind spatial masking is that one or more sources of data are not available hence the whole data for those sensors will be masked"
- Break condition: If real-world scenarios involve mostly partial data corruption rather than complete sensor failure, random masking might be more effective.

## Foundational Learning

- Concept: Self-supervised learning and pretext tasks
  - Why needed here: CroSSL relies on learning representations without labels by defining an artificial task (latent masking and aggregation)
  - Quick check question: How does SSL differ from unsupervised learning, and why is the pretext task formulation critical for CroSSL?

- Concept: Multimodal learning and cross-modal alignment
  - Why needed here: The core innovation involves aggregating information from heterogeneous modalities into a unified representation
  - Quick check question: What challenges arise when aligning modalities with different sampling rates, scales, and feature spaces?

- Concept: Information bottleneck and mutual information
  - Why needed here: The theoretical motivation section uses information-theoretic concepts to justify why the learned embeddings should capture only cross-modal information
  - Quick check question: How does I(Z; Qi|Qj) = 0 relate to the goal of learning modality-invariant representations?

## Architecture Onboarding

- Component map: Xi → Em → Qm → Masking → A → Z → G → Prediction
- Critical path: Xi → Em → Qm → Masking → A → Z → G → Prediction
- Design tradeoffs:
  - Encoder complexity vs. computational efficiency: Deeper encoders capture more features but increase latency
  - Masking ratio vs. information preservation: Higher masking ratios improve robustness but may lose too much information
  - VICReg hyperparameters vs. stability: Requires careful tuning to prevent collapse
- Failure signatures:
  - Performance collapses to random when masking ratio is too high
  - Model overfits to training modalities when masking is too low
  - Global embeddings Z show no variation across different inputs
- First 3 experiments:
  1. Verify encoder outputs: Check that Em produces meaningful intermediate embeddings Qm for each modality before aggregation
  2. Test masking strategies: Compare random vs. spatial masking with different ratios on a small subset of data
  3. Validate VICReg stability: Ensure the regularization terms prevent trivial solutions by monitoring variance and covariance metrics during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of CroSSL vary across different types of missing data (e.g., random vs. systematic sensor failures) in real-world healthcare applications?
- Basis in paper: [inferred] The paper discusses the robustness of CroSSL to missing modalities at inference time and mentions spatial vs. random masking strategies, but does not extensively explore different types of missing data patterns in real-world scenarios.
- Why unresolved: The paper primarily focuses on the impact of masking strategies and ratios, but does not investigate how different patterns of missing data (e.g., random sensor failures vs. systematic issues) affect the model's performance in practical healthcare settings.
- What evidence would resolve it: Empirical studies comparing CroSSL's performance across various real-world datasets with different missing data patterns, including both random and systematic failures, would provide insights into its robustness in diverse healthcare applications.

### Open Question 2
- Question: What is the optimal masking strategy for CroSSL when dealing with multimodal data from sensors with significantly different sampling rates?
- Basis in paper: [explicit] The paper mentions that CroSSL employs sensor-specific encoders to handle variable sampling rates but does not explore the optimal masking strategy for sensors with vastly different sampling rates.
- Why unresolved: While the paper demonstrates the effectiveness of spatial and random masking strategies, it does not address the specific challenges posed by sensors with significantly different sampling rates, which could impact the choice of masking strategy.
- What evidence would resolve it: Comparative studies evaluating different masking strategies (e.g., spatial, random, temporal) for CroSSL when applied to multimodal data from sensors with varying sampling rates would help identify the optimal approach for such scenarios.

### Open Question 3
- Question: How does the transferrability of CroSSL's learned embeddings perform across different healthcare tasks and datasets?
- Basis in paper: [inferred] The paper discusses the potential for transfer learning with SSL methods but does not provide empirical evidence on the transferrability of CroSSL's learned embeddings across diverse healthcare tasks and datasets.
- Why unresolved: While the paper demonstrates CroSSL's effectiveness on specific datasets, it does not explore how well the learned embeddings generalize to other healthcare tasks or datasets, which is crucial for practical applications.
- What evidence would resolve it: Experimental studies evaluating the performance of CroSSL's learned embeddings when transferred to different healthcare tasks (e.g., disease detection, patient monitoring) and datasets (e.g., ICU data, wearable sensor data) would provide insights into its transferrability and generalizability.

## Limitations
- Limited ablation study on VICReg hyperparameters and their impact on different dataset characteristics
- No investigation of temporal dependencies in the masking strategy beyond spatial and random approaches
- Results primarily validated on human activity recognition and stress detection, limiting generalizability to other healthcare domains

## Confidence
- **High confidence**: Cross-modal learning effectiveness (consistent improvements across all datasets and scenarios)
- **Medium confidence**: VICReg objective superiority (compared only to SimCLR-style contrastive learning, not other regularization methods)
- **Low confidence**: Generalization to real-world deployment scenarios (missing data simulation may not capture actual sensor failure patterns)

## Next Checks
1. Conduct extensive hyperparameter sensitivity analysis for VICReg coefficients across different dataset characteristics and class imbalances
2. Test temporal masking strategies that preserve temporal continuity (e.g., block masking vs. random element masking) to better simulate realistic sensor failures
3. Validate performance on additional healthcare domains with different temporal characteristics, such as seizure detection or respiratory monitoring, to assess generalizability beyond activity recognition