---
ver: rpa2
title: 'CuTS: Customizable Tabular Synthetic Data Generation'
arxiv_id: '2307.03577'
source_url: https://arxiv.org/abs/2307.03577
tags:
- data
- progsyn
- constraints
- synthetic
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProgSyn is the first general programmable synthetic tabular data
  generation method that allows users to declaratively specify privacy, fairness,
  logical, statistical, and downstream constraints. It pre-trains a generative model
  on the original data and fine-tunes it to satisfy custom constraints by converting
  them into a differentiable loss.
---

# CuTS: Customizable Tabular Synthetic Data Generation

## Quick Facts
- arXiv ID: 2307.03577
- Source URL: https://arxiv.org/abs/2307.03577
- Reference count: 40
- Primary result: ProgSyn achieves 2.3% higher downstream accuracy than state-of-the-art at the same fairness level on the Adult dataset.

## Executive Summary
ProgSyn introduces a programmable approach to synthetic tabular data generation that allows users to declaratively specify privacy, fairness, logical, statistical, and downstream constraints. The method pre-trains a generative model on original data and fine-tunes it using differentiable losses derived from user constraints. Experiments demonstrate that ProgSyn matches or outperforms existing methods on supported constraints while being more general, particularly in stacking diverse constraints without degrading data quality.

## Method Summary
ProgSyn is a programmable synthetic tabular data generation method that first pre-trains a generative model on the original data using marginals (typically three-way), then fine-tunes the model to satisfy custom constraints by converting them into differentiable loss terms. The approach uses discrete data representation with 32-bin uniform discretization and one-hot encoding. Differentiable relaxations of logical and statistical constraints are incorporated into the training loss, allowing gradient-based optimization. The method supports differential privacy through noise injection during marginals measurement and training, and can handle a wide range of constraint types including logical expressions, statistical requirements, and downstream task objectives.

## Key Results
- Achieves 2.3% higher downstream accuracy than previous state-of-the-art at the same fairness level on Adult dataset
- Successfully stacks diverse constraints (up to four) without degrading data quality
- Provides general constraint satisfaction capability that previous methods cannot match

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training a generative model on the original data and then fine-tuning it with a differentiable loss derived from user-defined constraints preserves high data quality while enforcing custom requirements.
- Mechanism: The model first learns the data distribution via marginals. Fine-tuning then adjusts the distribution to satisfy constraints without losing fidelity.
- Core assumption: Differentiable relaxations of logical and statistical constraints can be backpropagated through the generative model.
- Evidence anchors:
  - [abstract]: "pre-trains a generative model on the original data and fine-tunes it to satisfy custom constraints by converting them into a differentiable loss."
  - [section 4.2]: Describes row constraints and statistical constraints implemented as differentiable terms.
  - [corpus]: The average neighbor FMR is 0.36, indicating moderate relevance, but no direct evidence for this mechanism.
- Break condition: If the differentiable relaxation is too coarse, the model may not converge to satisfying the true (non-differentiable) constraints.

### Mechanism 2
- Claim: Using differential privacy during pre-training and/or fine-tuning protects individual privacy without severely degrading data utility.
- Mechanism: DP noise is added during marginals measurement and training. This limits the influence of any single data point.
- Core assumption: The privacy budget (epsilon) can be allocated between pre-training and fine-tuning without collapsing data quality.
- Evidence anchors:
  - [abstract]: "differentially private (DP) synthetic data generation algorithms are of increasing interest."
  - [section 4.1]: Explains DP training using AIM's iterative and budget-adaptive approach.
  - [corpus]: No explicit DP-related evidence; FMR suggests weak alignment.
- Break condition: Excessive DP noise causes the model to lose distributional fidelity.

### Mechanism 3
- Claim: The ProgSyn language allows expressive specification of constraints using logical, statistical, and downstream terms, enabling applications beyond what prior methods support.
- Mechanism: Constraints are encoded as loss terms in a DSL; each constraint type maps to a differentiable objective that can be combined.
- Core assumption: The DSL can represent the full range of needed constraints in a differentiable form.
- Evidence anchors:
  - [abstract]: "declarative statistical and logical expressions, supporting a wide range of requirements."
  - [section 4.2]: Lists all supported constraint types with syntax examples.
  - [corpus]: Only 0.0 average citations among neighbors, indicating novelty and lack of corroboration.
- Break condition: If a required constraint cannot be expressed in the DSL, it cannot be enforced.

## Foundational Learning

- Concept: Differentiable relaxations of logical constraints
  - Why needed here: To enable gradient-based optimization for constraints like implications and row-level conditions.
  - Quick check question: Can a logical AND be implemented as element-wise multiplication in the relaxed space?

- Concept: Marginals and their role in tabular data synthesis
  - Why needed here: Marginals capture the statistical dependencies between features, which the generative model learns.
  - Quick check question: How does a 3-way marginal differ from a 2-way marginal in terms of captured dependencies?

- Concept: Differential privacy (epsilon-delta definition)
  - Why needed here: To quantify the privacy guarantee when training on sensitive data.
  - Quick check question: What happens to the privacy budget when you compose multiple DP mechanisms?

## Architecture Onboarding

- Component map: Data preprocessing -> Generative model -> Pre-training module -> Fine-tuning module -> Sampling interface
- Critical path: 1. Preprocess original data -> Train generative model -> Apply constraints -> Generate synthetic data
- Design tradeoffs:
  - Using 3-way marginals balances expressiveness and computational cost
  - Gumbel-softmax enables differentiable sampling but adds sampling noise
  - Separate pre-training and fine-tuning phases simplify constraint enforcement but may cause distributional drift
- Failure signatures:
  - High TV distance after fine-tuning -> constraint relaxations too strict
  - Low downstream accuracy -> generative model underfit during pre-training
  - Constraint violation after sampling -> constraint satisfaction not properly encoded in loss
- First 3 experiments:
  1. Run pre-training without constraints; measure TV distance to original marginals
  2. Add a single logical row constraint; verify CSR and accuracy trade-off
  3. Stack a fairness and a statistical constraint; check composability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different discretization strategies (beyond uniform discretization into 32 bins) affect the quality of synthetic data generated by ProgSyn?
- Basis in paper: The authors note that uniform discretization into 32 bins was used for simplicity and acknowledge that a more carefully chosen discretization scheme could improve the inherent performance of ProgSyn.
- Why unresolved: The paper did not explore discretization strategies beyond the uniform approach.
- What evidence would resolve it: Comparing the performance of ProgSyn using different discretization strategies (e.g., quantile-based, adaptive binning) on the same datasets and constraints.

### Open Question 2
- Question: How does the choice of marginals for training (beyond three-way marginals) impact the quality of synthetic data and constraint satisfaction in ProgSyn?
- Basis in paper: The authors used three-way marginals for training and suggested that an advanced scheme for choosing marginals could benefit ProgSyn.
- Why unresolved: The paper did not explore different sets of marginals for training.
- What evidence would resolve it: Comparing the performance of ProgSyn using different sets of marginals (e.g., including higher-order interactions, selecting based on importance) on the same datasets and constraints.

### Open Question 3
- Question: How does the scalability of ProgSyn change with increasing dataset size and dimensionality?
- Basis in paper: The paper evaluated ProgSyn on datasets with up to 261 dimensions and did not explicitly discuss scalability.
- Why unresolved: The paper did not test ProgSyn on larger or higher-dimensional datasets.
- What evidence would resolve it: Evaluating ProgSyn on datasets with varying sizes and dimensionalities to measure computational resources, training time, and data quality.

## Limitations
- Differentiable relaxations of logical constraints may not capture true semantics for all constraint types
- Performance improvements based on limited dataset comparisons (Adult dataset only)
- Constraint stacking experiments limited to four constraints maximum

## Confidence
- High confidence: The core methodology of pre-training followed by fine-tuning with constraint losses is technically sound and well-supported by the framework description
- Medium confidence: Claims about DP training effectiveness are plausible but lack direct evidence beyond citing AIM's approach
- Medium confidence: Downstream accuracy improvements are demonstrated but based on limited comparisons and datasets

## Next Checks
1. **Constraint Expressiveness Test**: Verify that the DSL can correctly encode a logical constraint that no prior method supports (e.g., "if education level > bachelor then income > 50K" combined with fairness constraints)
2. **DP Budget Allocation Experiment**: Systematically vary the privacy budget split between pre-training and fine-tuning to find optimal allocation for different epsilon values
3. **Scaling Experiment**: Test constraint stacking beyond four constraints to identify the breaking point where either constraint satisfaction or data utility degrades significantly