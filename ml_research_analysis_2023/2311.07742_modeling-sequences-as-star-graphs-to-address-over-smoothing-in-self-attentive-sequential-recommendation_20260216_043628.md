---
ver: rpa2
title: Modeling Sequences as Star Graphs to Address Over-smoothing in Self-attentive
  Sequential Recommendation
arxiv_id: '2311.07742'
source_url: https://arxiv.org/abs/2311.07742
tags:
- mssg
- sasrec
- methods
- performance
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses over-smoothing in self-attentive sequential
  recommendation (SR) methods, where item embeddings within a sequence become increasingly
  similar across attention blocks, leading to degraded performance and scalability.
  To tackle this, the authors propose MSSG, a method that models user interaction
  sequences using star graphs.
---

# Modeling Sequences as Star Graphs to Address Over-smoothing in Self-attentive Sequential Recommendation

## Quick Facts
- **arXiv ID**: 2311.07742
- **Source URL**: https://arxiv.org/abs/2311.07742
- **Reference count**: 40
- **Primary result**: MSSG significantly outperforms 10 SOTA baselines on 6 datasets, achieving up to 10.10% improvement in Recall@10.

## Executive Summary
This paper addresses over-smoothing in self-attentive sequential recommendation (SR) methods, where item embeddings within a sequence become increasingly similar across attention blocks, leading to degraded performance and scalability. The authors propose MSSG, which models user interaction sequences using star graphs with an internal node that aggregates global information from all item nodes. This design avoids information propagation among items, fundamentally addressing over-smoothing while achieving linear time complexity. Experiments on six benchmark datasets demonstrate that MSSG significantly outperforms ten state-of-the-art baseline methods, with improvements up to 10.10% at Recall@10.

## Method Summary
MSSG addresses over-smoothing in self-attentive sequential recommendation by modeling user interaction sequences as star graphs. The method introduces an internal node that aggregates information from all item nodes, while keeping item node embeddings fixed across attention blocks. This architecture enables linear time complexity and avoids the information propagation among items that causes over-smoothing in traditional self-attentive methods. MSSG achieves superior recommendation performance through this design, particularly excelling on datasets with diverse user interests.

## Key Results
- MSSG significantly outperforms 10 SOTA baselines on 6 benchmark datasets, achieving up to 10.10% improvement in Recall@10
- MSSG achieves linear time complexity with respect to sequence length, making it more efficient than self-attentive methods
- The model demonstrates superior scalability with respect to both the number of blocks and embedding dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MSSG's star graph structure fundamentally addresses over-smoothing by avoiding information propagation among item nodes.
- Mechanism: MSSG introduces an internal node that aggregates information from all item nodes, while keeping item node embeddings fixed across attention blocks.
- Core assumption: Information propagation among item nodes is the essential cause of over-smoothing in self-attentive sequential recommendation methods.
- Evidence anchors:
  - [abstract] "Different from existing self-attentive methods, MSSG introduces an additional internal node to specifically capture the global information within the sequence, and does not require information propagation among items. This design fundamentally addresses the over-smoothing issue"
  - [section 4.2.2] "Different from SA-based methods, MSSG updates only the embedding of the internal node across blocks, while fixing the item node embeddings. As a result, MSSG could preserve the information on individual items and will not be affected by the over-smoothing issue"
- Break condition: If empirical evidence shows that item node embeddings still become increasingly similar across blocks, or if the internal node cannot effectively capture sequence-level information.

### Mechanism 2
- Claim: MSSG achieves linear time complexity by modeling sequences as star graphs rather than fully connected graphs.
- Mechanism: Each SA block requires O(n²d) operations due to attention weight computation between all item pairs. MSSG's star graph requires only O(nd²) operations since the internal node attends to all items in O(nd) and the feed-forward network applies to O(d²) per item.
- Core assumption: The computational bottleneck is the quadratic attention computation in fully connected graphs.
- Evidence anchors:
  - [section 4.5] "each SA block requires 6nd² + 2n²d + 2nd operations to compute, whereas each block in MSSG requires only 2nd² + 4d² + 2nd + 2d operations. The difference amounts to 4d²(n - 1) + 2d(n² - 1), which is quadratic with respect to both d and n"
- Break condition: If the constant factors in the linear complexity become prohibitive, or if the internal node attention mechanism itself becomes a bottleneck for very long sequences.

### Mechanism 3
- Claim: MSSG's design enables superior scalability with respect to both the number of blocks and embedding dimensions.
- Mechanism: By avoiding over-smoothing, MSSG can stack more blocks without performance degradation. The fixed item node embeddings also make the model less sensitive to embedding dimension increases.
- Core assumption: Over-smoothing is the primary factor limiting the depth of self-attentive models, and fixed item embeddings reduce the impact of embedding dimension on model capacity.
- Evidence anchors:
  - [section 6.4] "MSSG maintains reasonable performance as n_b increases on all datasets, while SASRec fails on four out of the six datasets (Children, Comics, ML-1M, and ML-20M) when n_b=6"
  - [section 6.5] "On all the datasets, MSSG achieves stronger run-time performance compared to SASRec over different numbers of blocks, and the improvement increases as the number of blocks increases"
- Break condition: If empirical results show that MSSG also degrades with more blocks or larger embeddings, or if other factors (e.g., optimization instability) limit scalability.

## Foundational Learning

- Concept: Over-smoothing in graph neural networks and self-attention models
  - Why needed here: Understanding the problem MSSG is designed to solve, and why traditional self-attentive approaches suffer from it
  - Quick check question: In a 6-layer self-attentive model, if node embeddings converge to the same vector, what phenomenon is occurring and how does it affect downstream task performance?

- Concept: Star graph structure and its computational properties
  - Why needed here: MSSG's core architectural innovation relies on modeling sequences as star graphs, which enables both the solution to over-smoothing and the linear complexity
  - Quick check question: What is the time complexity of computing attention weights in a star graph with n nodes and d-dimensional embeddings, compared to a fully connected graph?

- Concept: Multi-head attention and transformer architectures
  - Why needed here: MSSG builds on transformer blocks, so understanding how attention mechanisms work and how they're typically applied in sequential recommendation is essential
  - Quick check question: In a standard transformer block, what is the total number of parameters in the attention layer for n items and d dimensions?

## Architecture Onboarding

- Component map:
  - Input layer: Fixed-length sequence with item and position embeddings
  - Attention layer: Multi-head attention from internal node to all item nodes
  - Feed-forward layer: Non-linear transformation of internal node embedding
  - Output layer: Combination of internal node and user embeddings for recommendation
  - MSSG-u variant: Same architecture without user embeddings

- Critical path: Input sequence → Fixed item/position embeddings → Internal node initialization (using last item) → Multi-block processing (attention + feed-forward) → Final recommendation scores

- Design tradeoffs:
  - Star graph vs fully connected: Linear vs quadratic complexity, no over-smoothing vs potential information loss
  - Internal node only vs item node updates: Simpler optimization vs richer representation
  - Fixed item embeddings vs dynamic updates: Preserves individual information vs may miss item-item interactions

- Failure signatures:
  - Over-smoothing: Internal node embedding becomes similar across blocks
  - Underfitting: Recommendation performance close to random guessing
  - Optimization instability: NaN or exploding gradients during training
  - Memory issues: Out-of-memory errors with large sequence lengths or embedding dimensions

- First 3 experiments:
  1. Ablation study: Compare MSSG with a variant that allows item node updates to verify over-smoothing is addressed
  2. Complexity analysis: Measure actual runtime vs theoretical O(n) complexity across varying sequence lengths
  3. Attention weight analysis: Compute average entropy of attention weights in MSSG vs SASRec to verify information gain differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the star graph representation compare to other non-fully-connected graph structures in addressing over-smoothing in sequential recommendation?
- Basis in paper: [explicit] The paper proposes using a star graph structure to address over-smoothing, contrasting it with fully connected graphs used in self-attentive methods.
- Why unresolved: The paper does not explore or compare the star graph structure with other potential graph structures that could also mitigate over-smoothing.
- What evidence would resolve it: Empirical comparisons of the star graph approach with other graph structures (e.g., chain, tree, or hybrid structures) on the same benchmark datasets would provide insights into the relative effectiveness of different approaches.

### Open Question 2
- Question: What is the impact of the star graph representation on capturing complex sequential patterns compared to self-attentive methods?
- Basis in paper: [inferred] The paper suggests that the star graph representation effectively captures long-range dependencies but does not explicitly compare its ability to capture complex sequential patterns with self-attentive methods.
- Why unresolved: The paper focuses on the efficiency and scalability of the star graph representation but does not provide a detailed analysis of its ability to capture complex sequential patterns, which is a key strength of self-attentive methods.
- What evidence would resolve it: A comprehensive analysis of the types of sequential patterns each method can capture, including experiments on datasets with known complex patterns, would clarify the trade-offs between the star graph and self-attentive approaches.

### Open Question 3
- Question: How does the performance of the star graph representation vary with different types of recommendation data (e.g., sparse vs. dense, short vs. long sequences)?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of the star graph representation on six benchmark datasets but does not systematically explore its performance across different data characteristics.
- Why unresolved: The paper does not investigate how the star graph representation performs on datasets with varying levels of sparsity, sequence length, or other characteristics that could affect its effectiveness.
- What evidence would resolve it: Experiments on a diverse set of datasets with controlled variations in sparsity, sequence length, and other relevant characteristics would provide insights into the robustness and generalizability of the star graph approach.

## Limitations

- **Indirect over-smoothing evidence**: The paper claims MSSG addresses over-smoothing but doesn't directly measure embedding similarity across attention blocks for both MSSG and baselines.
- **Scalability beyond blocks/embeddings**: While MSSG shows linear complexity advantages, the practical significance for extremely long sequences isn't thoroughly examined.
- **User activity methodology**: The analysis showing MSSG's effectiveness for users with diverse interests lacks detailed specification of how user activity levels are quantified and binned.

## Confidence

- **High confidence**: MSSG achieves significantly better recommendation performance than self-attentive baselines on standard benchmarks
- **Medium confidence**: MSSG's linear complexity provides practical runtime advantages over self-attentive methods
- **Low confidence**: MSSG specifically addresses over-smoothing in a way that enables superior performance for users with diverse interests

## Next Checks

1. **Direct over-smoothing measurement**: Implement quantitative metrics to measure embedding similarity across attention blocks for both MSSG and SASRec, comparing average cosine similarity or attention weight entropy to verify that MSSG maintains distinct item representations while baselines suffer from over-smoothing.

2. **Sequence length scalability**: Conduct experiments with artificially extended sequences (e.g., 1000+ items) to empirically validate the linear complexity advantage and identify at what sequence lengths the practical performance gap between MSSG and self-attentive methods becomes most pronounced.

3. **User interest diversity analysis**: Replicate the user activity level analysis with clearly specified methodology for quantifying user interest diversity, and test whether the performance gap between MSSG and baselines correlates with user diversity metrics across all six datasets.