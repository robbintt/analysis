---
ver: rpa2
title: 'Measuring Equality in Machine Learning Security Defenses: A Case Study in
  Speech Recognition'
arxiv_id: '2302.08973'
source_url: https://arxiv.org/abs/2302.08973
tags:
- adversarial
- training
- defenses
- rejection
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the lack of fairness evaluation in machine
  learning security defenses, specifically focusing on adversarial robustness and
  rejection-based methods. The authors propose a framework to measure defense equality
  across social subgroups (gender, age, accent) using two metrics: AUC for accuracy
  under attack (AUCacc) and AUC for false positive rejection rate (AUCFPR).'
---

# Measuring Equality in Machine Learning Security Defenses: A Case Study in Speech Recognition

## Quick Facts
- arXiv ID: 2302.08973
- Source URL: https://arxiv.org/abs/2302.08973
- Reference count: 40
- Primary result: First systematic examination of fairness disparities in adversarial robustness for speech data, finding randomized smoothing more equitable than neural rejection for minority groups

## Executive Summary
This paper addresses the critical gap in fairness evaluation of machine learning security defenses by proposing a framework to measure defense equality across social subgroups in speech recognition systems. The authors introduce two key metrics‚ÄîAUC for accuracy under attack (AUC_acc) and AUC for false positive rejection rate (AUC_FPR)‚Äîto systematically evaluate how adversarial robustness training and rejection-based defenses perform across gender, age, and accent subgroups. Through case studies on speech command recognition, they demonstrate that standard security interventions can have differential impacts on subgroups, with randomized smoothing showing better fairness properties than neural rejection due to its sampling mechanism for minority groups.

## Method Summary
The authors developed a framework to measure defense equality across social subgroups using two AUC-based metrics. They conducted experiments on the Common Voice English dataset, evaluating convolutional neural networks (M5 and variants) with adversarial training, noise augmentation, and two rejection methods (randomized smoothing and neural rejection). The evaluation measured accuracy under varying attack budgets and false positive rejection rates across gender, age, and accent subgroups, providing the first systematic analysis of fairness in adversarial robustness for speech data.

## Key Results
- Adversarial training and noise augmentation have complex, non-uniform effects on subgroup fairness
- Large-scale pretraining generally improves defense parity across most subgroups except those with Australian English accents and those in their seventies
- Randomized smoothing with sampling offers better fairness than neural rejection due to its sampling mechanism for minority groups
- Defense parity and false positive rejection parity can be measured and optimized independently

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AUC-based metrics capture security-accuracy trade-offs better than accuracy alone for subgroup fairness analysis
- Mechanism: The AUC for accuracy under attack (AUC_acc) integrates performance across attack budgets, preventing early drops for subgroups from being masked by average accuracy
- Core assumption: Attack budgets are varied enough to reveal differential robustness patterns across subgroups
- Evidence anchors:
  - [abstract]: "We propose a framework to measure defense equality across social subgroups... using two metrics: AUC for accuracy under attack (AUC_acc) and AUC for false positive rejection rate (AUC_FPR)"
  - [section]: "We measure the area under the curve (AUC) for samples of varying levels of attack budgets... This summary statistic of accuracy area under the curve accounts (ùê¥ùëà ùê∂ùëéùëêùëê ) for the early loss of performance for subgroups when introducing a defense"
- Break condition: If attack budgets do not span realistic threat levels, AUC may over/underestimate real-world subgroup protection

### Mechanism 2
- Claim: Randomized smoothing's sampling mechanism improves fairness compared to neural rejection for minority groups
- Mechanism: By averaging predictions over multiple noisy samples, RS smooths out subgroup-specific vulnerabilities that could cause systematic false rejections
- Core assumption: Noise injection does not disproportionately degrade performance for any subgroup beyond what is captured in the AUC_FPR metric
- Evidence anchors:
  - [abstract]: "We present a comparison of equality between two rejection-based defenses: randomized smoothing and neural rejection, finding randomized smoothing more equitable due to the sampling mechanism for minority groups"
  - [section]: "We achieved higher FPR parity across all groups by increasing the number of samples with randomized smoothing"
- Break condition: If noise variance is too high, RS may degrade accuracy for subgroups with less training data, reversing fairness gains

### Mechanism 3
- Claim: Large-scale pretraining increases defense parity across most subgroups by exposing models to broader acoustic diversity
- Mechanism: Pretrained models like wav2vec 2.0 capture more varied speech patterns, reducing the performance gap between majority and minority subgroups under attack
- Core assumption: Pretraining dataset diversity is representative of the target subgroup distribution
- Evidence anchors:
  - [section]: "We see that pretraining is correlated with higher levels of defense for all groups except those with Australian English accents and those in their seventies"
  - [section]: "For the age subgroups, in particular, we see a much higher correlation than in the binary case shedding light on the dynamics of adversarial training's budget needing to be optimized for security and fairness in applications"
- Break condition: If pretraining data underrepresents certain subgroups, pretraining may widen the defense gap rather than close it

## Foundational Learning

- Concept: AUC (Area Under the Curve) calculation and interpretation
  - Why needed here: AUC metrics are central to measuring subgroup defense parity across attack budgets and security thresholds
  - Quick check question: What does a lower AUC_acc value indicate about a subgroup's robustness under attack?

- Concept: False Positive Rate (FPR) in security context
  - Why needed here: FPR measures how often clean examples from a subgroup are incorrectly rejected as adversarial, directly capturing fairness in rejection-based defenses
  - Quick check question: How does measuring FPR parity across subgroups reveal systematic rejection bias?

- Concept: Adversarial training with PGD (Projected Gradient Descent)
  - Why needed here: PGD-based adversarial training is the primary method tested for robustness, and understanding its impact on subgroup fairness is key to interpreting the results
  - Quick check question: Why might increasing the PGD attack budget during training affect defense parity differently across subgroups?

## Architecture Onboarding

- Component map:
  Data preprocessing (audio down-sampling, noise augmentation) -> Model architectures (CNN variants, wav2vec 2.0) -> Defense interventions (adversarial training, rejection methods) -> Evaluation pipeline (AUC metrics per subgroup)

- Critical path:
  1. Load and preprocess audio data
  2. Train base model(s)
  3. Apply defense interventions (adversarial training, augmentation)
  4. Generate adversarial examples across attack budgets
  5. Compute AUC_acc per subgroup
  6. Train rejection models (NR or RS)
  7. Compute AUC_FPR per subgroup
  8. Compare parity metrics

- Design tradeoffs:
  - Computation vs. fairness granularity: More subgroups increase fairness insights but computational cost
  - Attack strength vs. fairness: Higher attack budgets reveal subgroup vulnerabilities but may reduce overall accuracy
  - Noise level vs. rejection parity: Higher noise in RS improves parity but increases computation

- Failure signatures:
  - Low AUC_acc variance across subgroups ‚Üí defenses are fair but potentially weak
  - High AUC_FPR for a subgroup ‚Üí systematic over-rejection of that subgroup
  - High AUC_FPR parity but high overall FPR ‚Üí equally bad treatment for all subgroups

- First 3 experiments:
  1. Train M5 model with and without noise augmentation (œÉ=0.1); compute AUC_acc per subgroup
  2. Train M5 with adversarial training (Œµ=0.1); compute AUC_acc per subgroup
  3. Compare NR vs RS (N=1000) rejection on M5; compute AUC_FPR per subgroup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adversarial training budgets specifically affect different social subgroups' robustness, and what is the optimal budget that balances security and fairness?
- Basis in paper: [explicit] The paper states "When breaking the results down to the noise augmentation level, we see a weak correlation between gender and accent for increasing the noise augmentation level and defense parity" and "For the age subgroups, in particular, we see a much higher correlation than in the binary case shedding light on the dynamics of adversarial training's budget needing to be optimized for security and fairness in applications."
- Why unresolved: The paper identifies correlations but doesn't determine optimal budget levels that maximize both security and fairness across subgroups
- What evidence would resolve it: Empirical studies testing multiple adversarial training budget levels across diverse social subgroups to find the sweet spot that maximizes robustness while maintaining fairness

### Open Question 2
- Question: What is the relationship between subgroup representation in training data and their corresponding false positive rejection rates in both neural rejection and randomized smoothing methods?
- Basis in paper: [explicit] The paper notes "we investigate if having a higher subgroup representation in training set results in higher ùê¥ùëà ùê∂ùêπ ùëÉùëÖ" and finds correlations for both methods
- Why unresolved: While correlations are found, the causal relationship and optimal representation levels for fairness are not established
- What evidence would resolve it: Controlled experiments varying subgroup representation in training data while measuring FPR across different rejection methods to determine optimal representation ratios

### Open Question 3
- Question: How does large-scale pretraining affect defense parity across social subgroups, and what factors in the pretraining data contribute to these effects?
- Basis in paper: [explicit] The paper states "We see that pretraining is correlated with higher levels of defense for all groups except those with Australian English accents and those in their seventies" and "Large-scale training presents a pivotal way to increase defense parity in most cases, but the underlying data and social group representation of this process may play a role."
- Why unresolved: The paper identifies that pretraining generally improves defense parity but doesn't explain why certain subgroups (Australian English speakers, people in their seventies) don't benefit equally
- What evidence would resolve it: Analysis of pretraining dataset composition and its correlation with subgroup performance in downstream tasks, potentially leading to guidelines for more equitable pretraining data collection

## Limitations
- The framework relies on AUC metrics that may not fully capture real-world attack scenarios, particularly for subgroups with fewer samples where variance in robustness measures could be higher
- The adversarial training configurations were not fully specified, making it difficult to assess whether observed subgroup effects are due to the defense mechanism itself or hyperparameter choices
- The wav2vec 2.0 pretraining benefits for defense parity are based on correlation rather than causal analysis - it's unclear whether improved parity is due to better feature representation or simply better overall accuracy

## Confidence
- Core claims about fairness disparities in adversarial robustness: Medium
- Methodological contribution of AUC-based fairness metrics: High
- Specific findings about randomized smoothing fairness advantages: Medium

## Next Checks
1. **Subgroup sample size analysis**: Quantify the relationship between subgroup representation in training data and defense parity metrics to identify whether fairness gains are primarily driven by sample size rather than defense mechanism effectiveness.

2. **Cross-dataset replication**: Apply the same framework to a different speech dataset (e.g., LibriSpeech) to verify that the observed patterns of pretraining benefits and randomized smoothing advantages generalize beyond Common Voice.

3. **Attack budget sensitivity analysis**: Systematically vary the maximum attack budget in AUC_acc calculations to determine at what point subgroup vulnerabilities become apparent, establishing guidelines for realistic threat modeling in fairness evaluations.