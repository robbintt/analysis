---
ver: rpa2
title: How to Solve Few-Shot Abusive Content Detection Using the Data We Actually
  Have
arxiv_id: '2305.14081'
source_url: https://arxiv.org/abs/2305.14081
tags:
- datasets
- target
- abusive
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of building abusive language detection
  models for new target label sets and/or languages using only a few training examples,
  leveraging existing annotated corpora. The proposed two-step approach involves training
  a foundation model in a multi-task fashion on multiple external datasets, then adapting
  it to the target task with a few-shot setup.
---

# How to Solve Few-Shot Abusive Content Detection Using the Data We Actually Have

## Quick Facts
- arXiv ID: 2305.14081
- Source URL: https://arxiv.org/abs/2305.14081
- Reference count: 40
- This work addresses few-shot abusive language detection by leveraging existing annotated corpora through a two-step approach

## Executive Summary
This paper tackles the challenge of building abusive language detection models for new target label sets and languages with limited training data. The proposed solution uses a two-step approach: first training a foundation model on multiple external abusive language datasets using prompt-learning with separate pattern-verbalizer-pairs (PVPs) for each dataset, then adapting it to the target task with only a few shots per label. Experiments demonstrate consistent improvements across various abusive language detection tasks (hate speech, misogyny, target identification) in both monolingual and cross-lingual settings (English to German, Italian, Brazilian Portuguese, and Hindi), showing that existing datasets can effectively bootstrap new abusive language detection models.

## Method Summary
The approach involves training a foundation model in a multi-task fashion on multiple external abusive language datasets, each with its own pattern-verbalizer-pair (PFP), using prompt-learning to transform classification into a masked language modeling task. This foundation model is then adapted to the target task using only a few training examples (typically 4 shots per label). The method leverages XLM-RoBERTa as the base model and employs cross-entropy loss for both training steps. The two-step approach prevents the suppression of minority classes that can occur in single-step multitask learning with limited target data.

## Key Results
- Using existing abusive language datasets and few-shot target examples improves model performance over baseline systems
- Models acquire general abusive language understanding, improving both seen and unseen labels in target datasets
- Cross-lingual transfer works effectively, with average improvements of 9.51% monolingually and 6.09% cross-lingually
- The approach outperforms MLM and MTL baselines, demonstrating the effectiveness of the two-step prompt-learning method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dataset training creates a foundation model with general abusive language understanding that benefits both seen and unseen labels
- Mechanism: Joint training on diverse abusive language datasets with separate PVPs for each dataset allows the model to learn general abusive language features while maintaining task-specific flexibility
- Core assumption: Different abusive language datasets share underlying semantic features that can be learned jointly without catastrophic interference
- Evidence anchors:
  - [abstract]: "Our analysis also shows that our models acquire a general understanding of abusive language, since they improve the prediction of labels which are present only in the target dataset"
  - [section 5]: "Our analysis also shows that not only seen but unseen labels as well were improved (even ToLD-Br with more than half of its labels unseen), suggesting that the general abusive language aware ME model helps learning the fine-grained label sets"
- Break condition: If datasets have fundamentally different abusive language phenomena that don't share semantic features

### Mechanism 2
- Claim: Prompt-learning with few-shot target adaptation is more effective than traditional fine-tuning for abusive language detection
- Mechanism: Using pattern-verbalizer-pairs transforms classification into a masked language modeling task, leveraging the pre-trained LM's existing capabilities
- Core assumption: The masked language modeling objective in prompt-learning is better suited for few-shot adaptation than classification heads
- Evidence anchors:
  - [section 3.1]: "Prompt-learning was shown to be effective when only a small training set is available"
  - [section 5]: "The MLM and MTL baselines also improve over the LM-base system, however, not as consistently and to a lesser extent than our approach"
- Break condition: If the target task requires complex decision boundaries that cannot be captured by the masked language modeling objective

### Mechanism 3
- Claim: Cross-lingual transfer works even with only English external datasets due to shared abusive language patterns
- Mechanism: The foundation model trained on English abusive language datasets captures general abusive language patterns that transfer to other languages
- Core assumption: Abusive language patterns are cross-lingual and can be transferred from English to other languages
- Evidence anchors:
  - [abstract]: "Our experiments show that using already existing datasets and only a few-shots of the target task the performance of models can be improved not only monolingually but across languages as well"
  - [section 5]: "Comparing the improvements of the monolingual and cross-lingual setups of MDL, i.e., English and non-English target datasets, we found that the external datasets are more beneficial monolingually"
- Break condition: If abusive language is highly language-specific with different cultural contexts

## Foundational Learning

- Concept: Prompt-learning with pattern-verbalizer-pairs
  - Why needed here: Enables few-shot learning by transforming classification into a masked language modeling task that leverages pre-trained LM capabilities
  - Quick check question: How does the PVP transform "This is offensive" into a format the model can predict?

- Concept: Multi-task vs. two-step learning
  - Why needed here: Multi-task learning with limited target data can suppress minority classes; two-step approach prevents this by training on external data first
  - Quick check question: Why does the paper use a two-step approach instead of training on both external and target data simultaneously?

- Concept: Cross-lingual transfer learning
  - Why needed here: Enables building models for languages with limited abusive language data by transferring knowledge from high-resource languages
  - Quick check question: What evidence suggests that abusive language patterns transfer across languages?

## Architecture Onboarding

- Component map:
  Base LM (XLM-RoBERTa) → Multi-dataset training with separate PVPs → Foundation model (ME) → Few-shot target adaptation → Final model (Mt)

- Critical path: External dataset training → Foundation model creation → Few-shot target adaptation → Evaluation
  - Failure at any step prevents successful model creation

- Design tradeoffs:
  - Simple uniform patterns across datasets vs. dataset-specific patterns (chosen: uniform for simplicity)
  - Separate PVPs per dataset vs. single PVP (chosen: separate for flexibility)
  - Two-step training vs. single-step multitask (chosen: two-step to prevent target data suppression)

- Failure signatures:
  - No improvement over baseline → Check PVP quality, dataset diversity, or few-shot sample quality
  - Performance drops on target task → Check for catastrophic interference during adaptation
  - Inconsistent results across seeds → Increase number of training samples or stabilize training process

- First 3 experiments:
  1. Run step 1 training on external datasets only (check foundation model quality)
  2. Test few-shot adaptation on a simple target task with known good performance (baseline validation)
  3. Compare prompt-learning vs. traditional fine-tuning on a target task (validate prompt-learning effectiveness)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do modular approaches like adapters or soft-prompts compare to the proposed two-step approach for few-shot abusive language detection?
- Basis in paper: The authors mention they plan to investigate modular approaches in future work to mitigate dataset and label interference issues.
- Why unresolved: The paper only speculates about the potential benefits of modular approaches without providing empirical evidence.
- What evidence would resolve it: Direct comparison experiments between the two-step approach and modular approaches on the same abusive language detection tasks.

### Open Question 2
- Question: What is the optimal balance between specializing the foundation model for a target task versus maintaining its general abusive language understanding capabilities?
- Basis in paper: The authors discuss the trade-off between specializing the foundation model (ME) for best performance on a target task and its negative effects on model adaptability.
- Why unresolved: The paper only explores one extreme (specializing ME by removing external-only labels) and does not investigate intermediate strategies.
- What evidence would resolve it: Experiments varying the degree of specialization and measuring both target task performance and adaptability to new tasks.

### Open Question 3
- Question: How do different few-shot training sizes affect the stability and performance of abusive language detection models across various label sets and languages?
- Basis in paper: The authors experiment with different n-shot values and observe instability at lower values and diminishing returns at higher values.
- Why unresolved: The paper only tests a limited range of n-shot values and does not explore the full spectrum of possibilities or provide a theoretical framework for predicting optimal n-shot sizes.
- What evidence would resolve it: Comprehensive experiments across a wider range of n-shot values and theoretical analysis of the relationship between training size, label distribution, and model performance.

## Limitations

- Dataset selection bias: The foundation model's effectiveness depends heavily on the diversity and representativeness of the external datasets
- P-value selection sensitivity: The paper claims separate PVPs per dataset improve flexibility but doesn't thoroughly explore how PVP design choices affect performance
- Cross-lingual transfer mechanism unclear: While the paper shows cross-lingual improvements, it doesn't provide evidence for why English-trained models transfer well to other languages

## Confidence

**High confidence**: The two-step training approach (multi-dataset training → few-shot adaptation) is effective for abusive language detection
**Medium confidence**: Prompt-learning with PVPs is superior to traditional fine-tuning for few-shot adaptation
**Low confidence**: Cross-lingual transfer works effectively with only English external datasets

## Next Checks

1. **P-value ablation study**: Systematically vary the PVPs used for each dataset in the multi-dataset training phase while keeping all other factors constant. Compare foundation model quality across different PVP designs to identify optimal patterns for abusive language detection.

2. **Dataset coverage analysis**: For each target task, analyze which external datasets most contribute to performance improvements. This would reveal whether the foundation model learns general abusive language features or simply memorizes specific dataset patterns.

3. **Cross-lingual transfer diagnostic**: Create controlled experiments where the foundation model is trained on non-English datasets and tested on English targets (reverse of current setup). Compare transfer effectiveness in both directions to validate the assumption of shared abusive language patterns across languages.