---
ver: rpa2
title: 'BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite
  for Large Language Models'
arxiv_id: '2309.06085'
source_url: https://arxiv.org/abs/2309.06085
tags:
- gpt-4
- tamil
- language
- indonesian
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BHASA, a holistic linguistic and cultural
  evaluation suite for large language models (LLMs) in Southeast Asian (SEA) languages.
  The suite comprises three components: an NLP benchmark covering eight tasks across
  NLU, NLG, and NLR; LINDSEA, a linguistic diagnostic toolkit; and a cultural diagnostics
  dataset.'
---

# BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models

## Quick Facts
- arXiv ID: 2309.06085
- Source URL: https://arxiv.org/abs/2309.06085
- Reference count: 40
- Key outcome: BHASA reveals significant SEA language capability gaps in GPT-4 despite improvements over GPT-3.5-Turbo, with performance decreasing in the order of Indonesian, Vietnamese, Thai, and Tamil

## Executive Summary
BHASA introduces a comprehensive evaluation suite for assessing large language models' capabilities in Southeast Asian languages and cultures. The benchmark comprises three components: an NLP benchmark with eight tasks, LINDSEA linguistic diagnostics, and cultural diagnostics. Initial experiments with GPT-3.5-Turbo and GPT-4 reveal that while GPT-4 shows marked improvement over its predecessor, both models still exhibit notable deficiencies in SEA language capabilities, cultural representation, and sensitivity. The evaluation particularly highlights challenges in named entity recognition, toxicity detection, machine translation, and natural language inference, with performance varying significantly across the four tested SEA languages.

## Method Summary
BHASA evaluates LLM performance through zero-shot prompting across eight NLP tasks, LINDSEA's handcrafted minimal pairs for linguistic diagnostics, and cultural diagnostics for representation and sensitivity. The evaluation uses manually translated prompts in four SEA languages (Indonesian, Vietnamese, Thai, Tamil) to avoid translationese bias. Models are tested with both English and native language prompts at temperature 0 for NLU tasks and 0.3 for summarization. Human evaluation is employed for machine translation and summarization tasks. The benchmark systematically compares GPT-3.5-Turbo and GPT-4 to identify capability gaps, using standard metrics (F1, accuracy, ROUGE-L) alongside human judgment for NLG tasks.

## Key Results
- GPT-4 shows significant improvement over GPT-3.5-Turbo but still underperforms in SEA languages, with performance decreasing in the order of Indonesian, Vietnamese, Thai, and Tamil
- Manual translation of prompts results in large drops in performance across most tasks, except for GPT-3.5-Turbo in Indonesian and Vietnamese
- LINDSEA reveals severe inadequacies in syntactic, semantic, and pragmatic understanding, particularly with complex linguistic phenomena like long-distance reflexives and resumptive pronouns
- Cultural evaluations show both models struggle with cultural representation and sensitivity, often providing inappropriate or culturally unaware responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BHASA reveals systematic gaps in SEA language capabilities through comparative evaluation
- Mechanism: By benchmarking GPT-3.5-Turbo and GPT-4 on identical SEA language tasks, performance differences highlight specific linguistic weaknesses that would be masked in English-only evaluations
- Core assumption: Language-specific performance differences are meaningful indicators of model capability gaps
- Evidence anchors: [abstract] "initial experiments on GPT-4 with BHASA find it lacking in various aspects of linguistic capabilities, cultural representation and sensitivity"; [section 3.6] "performance decreases in the order of Indonesian, Vietnamese, Thai, and Tamil"

### Mechanism 2
- Claim: Manual translation of prompts eliminates translationese bias present in machine-translated benchmarks
- Mechanism: Native speaker translations preserve linguistic nuances that machine translations distort, preventing unfair penalization of models
- Core assumption: Translation quality directly impacts model performance evaluation validity
- Evidence anchors: [section 3.4.2] "we worked with native speakers to translate the English prompts into their respective languages"; [section 3.6.4] "translated prompts result in large drops in performance across the board except for GPT-3.5-Turbo when prompted in Indonesian or Vietnamese"

### Mechanism 3
- Claim: LINDSEA's handcrafted minimal pairs provide more precise diagnostic capability than automatically generated benchmarks
- Mechanism: Linguist-crafted examples target specific linguistic phenomena with natural sentence pairs, avoiding the artificial constructions that plague template-generated tests
- Core assumption: Naturalness of test examples affects model's ability to demonstrate true linguistic understanding
- Evidence anchors: [section 4.1] "examples in LINDSEA are manually handcrafted by linguists in tandem with native speakers"; [section 4.5.1] "both models performed dismally on minimal pairs, although GPT-4 was significantly better than GPT-3.5-Turbo"

## Foundational Learning

- Concept: Zero-shot vs few-shot vs fine-tuned evaluation
  - Why needed here: BHASA specifically tests zero-shot capabilities to measure base model knowledge without in-context learning
  - Quick check question: What would happen to BHASA results if we added few-shot examples to the prompts?

- Concept: Linguistic diagnostics vs task-based evaluation
  - Why needed here: LINDSEA provides fine-grained analysis of linguistic understanding that complements the broader task-based NLP benchmark
  - Quick check question: How does testing syntax with minimal pairs differ from testing it through NLI tasks?

- Concept: Cultural representation vs cultural sensitivity
  - Why needed here: BHASA distinguishes between whether models have cultural knowledge versus whether they respond appropriately to cultural contexts
  - Quick check question: Why might a model know about Indonesian customs but still respond inappropriately to questions about them?

## Architecture Onboarding

- Component map:
  - NLP Benchmark (8 tasks: NER, QA, Sentiment, Toxicity, MT, Summarization, NLI, Causal Reasoning)
  - LINDSEA (Syntax, Semantics, Pragmatics diagnostics)
  - Cultural Diagnostics (Representation + Sensitivity)
  - Evaluation Framework (English + Native prompts, Human evaluation for NLG)

- Critical path:
  1. Dataset selection and cleaning
  2. Prompt translation and validation
  3. Model evaluation with both prompt languages
  4. Human evaluation for NLG tasks
  5. Analysis of performance patterns

- Design tradeoffs:
  - Manual translation vs. machine translation: Accuracy vs. scalability
  - Limited language coverage vs. resource constraints: Depth vs. breadth
  - Human evaluation vs. automated metrics: Quality vs. cost

- Failure signatures:
  - Inconsistent model responses across runs → Answer bias
  - Poor performance across all SEA languages → Baseline capability issue
  - High variance between English and native prompts → Language understanding gap

- First 3 experiments:
  1. Run same prompt in English and native language to check for consistency
  2. Test with different temperature settings to observe stability
  3. Compare performance on similar tasks across different SEA languages to identify language-specific gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BHASA account for the potential influence of translation errors on LLM performance in Southeast Asian languages?
- Basis in paper: [explicit] The paper explicitly discusses the challenges of using machine-translated prompts and mentions that GPT-4 still underperforms in Thai and Tamil even with translated prompts.
- Why unresolved: The paper doesn't provide a detailed analysis of how specific translation errors in prompts might impact model performance across different languages and tasks.
- What evidence would resolve it: A study that systematically manipulates translation quality in prompts and measures the corresponding impact on LLM performance across multiple SEA languages and tasks.

### Open Question 2
- Question: What are the limitations of using zero-shot evaluation for complex linguistic phenomena like long-distance reflexives and resumptive pronouns in LINDSEA?
- Basis in paper: [explicit] The paper notes that both GPT-3.5-Turbo and GPT-4 struggle with these phenomena in Indonesian and Tamil, and suggests that the models might be relying on their understanding of English syntax.
- Why unresolved: The paper doesn't explore alternative evaluation methods that might be more suitable for these complex linguistic phenomena.
- What evidence would resolve it: A study that compares the effectiveness of different evaluation methods for these specific phenomena and investigates the models' internal representations.

### Open Question 3
- Question: How can BHASA be expanded to include other Southeast Asian languages and cultural contexts beyond Indonesian, Vietnamese, Thai, and Tamil?
- Basis in paper: [explicit] The paper acknowledges the limitations of focusing on only four languages and two cultures, and mentions the intention to expand BHASA in the future.
- Why unresolved: The paper doesn't provide a concrete roadmap for how this expansion will be carried out, including the challenges of data collection, annotation, and evaluation for low-resource languages.
- What evidence would resolve it: A detailed plan outlining the steps for expanding BHASA to other SEA languages, including strategies for data collection, annotation, and evaluation.

## Limitations

- Limited language coverage: Only four SEA languages tested, which doesn't fully represent the linguistic diversity of Southeast Asia
- Resource-intensive methodology: Manual translation and human evaluation create scalability challenges for broader application
- Zero-shot constraint: Results may not reflect potential improvements achievable through fine-tuning or few-shot learning approaches

## Confidence

- **High Confidence**: The comparative analysis between GPT-3.5-Turbo and GPT-4 performance on identical tasks provides reliable evidence of systematic capability gaps in SEA languages
- **Medium Confidence**: The LINDSEA linguistic diagnostics effectively reveal specific gaps in syntactic, semantic, and pragmatic understanding, though handcrafted nature may introduce unintended confounds
- **Medium Confidence**: The cultural evaluation framework appropriately distinguishes between cultural representation and sensitivity, but severity of inadequacies may be influenced by specific evaluation prompts

## Next Checks

1. **Translation Quality Validation**: Conduct a blind review of the manually translated prompts by independent native speakers to verify translation accuracy and naturalness across all four languages
2. **Cross-Model Generalization**: Test the BHASA benchmark on additional LLMs (e.g., Claude, LLaMA variants) to determine if the observed SEA language gaps are model-specific or indicative of broader architectural limitations
3. **Few-Shot Impact Analysis**: Re-run key BHASA tasks with minimal few-shot examples to quantify the gap between zero-shot and few-shot performance, helping to distinguish between knowledge gaps and prompting challenges