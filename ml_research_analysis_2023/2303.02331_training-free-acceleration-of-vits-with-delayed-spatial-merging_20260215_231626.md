---
ver: rpa2
title: Training-Free Acceleration of ViTs with Delayed Spatial Merging
arxiv_id: '2303.02331'
source_url: https://arxiv.org/abs/2303.02331
tags:
- token
- tokens
- merging
- compression
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of accelerating Vision Transformers
  (ViTs) inference without requiring retraining or fine-tuning. The authors propose
  a training-free compression framework that leverages two key insights: 1) delaying
  token merging in the early layers to avoid error amplification, and 2) augmenting
  token merging with spatial information to better exploit redundancies between visual
  tokens.'
---

# Training-Free Acceleration of ViTs with Delayed Spatial Merging

## Quick Facts
- **arXiv ID**: 2303.02331
- **Source URL**: https://arxiv.org/abs/2303.02331
- **Reference count**: 29
- **Primary result**: Training-free compression framework achieving up to 1.8× FLOP reduction and 1.6× throughput speedup on ViTs with negligible accuracy loss

## Executive Summary
This work introduces a training-free compression framework for accelerating Vision Transformers (ViTs) inference without requiring retraining or fine-tuning. The method leverages two key insights: delaying token merging in early layers to avoid error amplification, and augmenting token merging with spatial information to better exploit redundancies between visual tokens. The authors propose Delayed Spatial Merging (DSM), a unified inference framework that combines these insights. Extensive evaluation on various ViT model scales (Tiny to Huge) and tasks (ImageNet-1k and transfer learning) demonstrates significant computational savings while maintaining accuracy.

## Method Summary
The method employs a two-stage pipeline: Dense Feature Extractor (DFE) that skips early token merging, followed by Local-Global Token Merging (LGTM) that merges tokens spatially-aware in local windows then globally. The approach uses cosine similarity on K embeddings for token similarity scoring and applies weighted averaging to track merged token locations. The framework is designed to be training-free, working directly on pre-trained ViT/DeiT models with sharpness-aware minimization (SAM) pretraining to improve compressibility.

## Key Results
- Achieves up to 1.8× GFLOPS reduction and 1.6× throughput speedup on ViT models
- Maintains negligible accuracy loss across model scales (ViT-T to ViT-Huge)
- Outperforms existing methods by two orders of magnitude in speed
- Effective for both ImageNet-1k classification and transfer learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Delaying token merging in early layers avoids error amplification
- **Mechanism**: Early ViT layers have divergent token embeddings that are still learning representations; merging them too early introduces noise that compounds through subsequent layers
- **Core assumption**: Token embeddings in bottom layers are still diversifying rather than converging to stable representations
- **Evidence anchors**: [abstract] "characterize a delayed onset of the convergent attention phenomenon, which makes token merging undesirable in the bottom blocks of ViTs"; [section] "merging tokens that are in the process of diversifying is challenging, as the tokens are still learning the correct representations"
- **Break condition**: If token embeddings converge earlier than expected or if the model architecture fundamentally changes token learning dynamics

### Mechanism 2
- **Claim**: Spatially-aware token merging improves similarity preservation during compression
- **Mechanism**: By restricting merging to local windows, tokens are merged based on both semantic similarity and spatial proximity, maintaining the spatial structure learned by positional embeddings
- **Core assumption**: Neighboring tokens in images are more likely to be semantically similar, and positional embeddings encode this spatial relationship
- **Evidence anchors**: [abstract] "augmenting token merging with a hierarchical processing scheme to capture multi-scale redundancy between visual tokens"; [section] "A unique component of ViT compared to CNN is its positional encoding... neighboring pixels in an image having stronger semantic relationships with each other"
- **Break condition**: If spatial relationships are not well-preserved in positional embeddings or if the local merging window size is inappropriate for the task

### Mechanism 3
- **Claim**: Sharpness-aware minimization makes models more compressible without accuracy loss
- **Mechanism**: SAM training finds flatter loss landscapes that are more robust to perturbations from token merging, reducing accuracy degradation during compression
- **Core assumption**: Models with flatter loss landscapes (lower Hessian eigenvalues) are more robust to post-training compression
- **Evidence anchors**: [abstract] "model pretrained with sharpness-aware minimization, which helps tolerate accuracy degradation"; [section] "we observe the compressibility of ViTs for our dynamic inference scheme" and "SAM can minimize the increase in sharpness from 9.9× to 1.6×"
- **Break condition**: If SAM training does not significantly flatten the loss landscape or if the model is already at the edge of the loss basin

## Foundational Learning

- **Concept**: Vision Transformer architecture and token processing
  - **Why needed here**: Understanding how ViTs process image tokens and the role of positional embeddings is crucial for grasping why spatial merging works
  - **Quick check question**: What is the relationship between patch size and initial token count in ViTs?

- **Concept**: Attention mechanisms and token importance
  - **Why needed here**: The paper leverages attention scores and token similarity metrics; understanding these concepts is essential for following the merging heuristics
  - **Quick check question**: How do K, Q, and V matrices contribute to attention computation in ViTs?

- **Concept**: Loss landscape geometry and optimization
  - **Why needed here**: SAM training's effectiveness relies on understanding sharpness-aware minimization and how loss curvature affects model robustness
  - **Quick check question**: What is the difference between sharp and flat minima in loss landscapes, and why does it matter for generalization?

## Architecture Onboarding

- **Component map**: Image → Patch embedding → Dense feature extraction → Local token merging → Global token merging → Classification head

- **Critical path**: Image → Patch embedding → Dense feature extraction → Local-Global Token Merging (LGTM) → Classification head

- **Design tradeoffs**:
  - Skipping early layers improves accuracy but increases final layer computational load
  - Local merging preserves spatial structure but requires additional computation for window partitioning
  - SAM training improves compressibility but requires additional training time upfront

- **Failure signatures**:
  - Accuracy degradation at high compression rates indicates merging too aggressively
  - Throughput slowdown despite GFLOPS reduction suggests overhead from local merging operations
  - Poor performance on spatially structured tasks may indicate loss of spatial information

- **First 3 experiments**:
  1. Compare vanilla ToMe vs DFE-only on a small model (ViT-S) to validate the skipping strategy
  2. Test different skip values (s=0, 2, 4) to find optimal early layer skipping point
  3. Implement local merging with different window sizes (w=5, 7, 9) to find sweet spot between accuracy and efficiency

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the analysis, several remain unresolved:
1. What is the theoretical limit of token merging efficiency for Vision Transformers, and at what point does accuracy degradation become unavoidable?
2. How do different positional encoding schemes affect the effectiveness of spatially-aware token merging?
3. Can the principles of delayed spatial merging be generalized to other transformer architectures beyond Vision Transformers?

## Limitations
- **Temporal Generalization Gap**: The delayed merging strategy assumes that token embeddings in early layers remain non-convergent throughout inference, but this assumption may not hold across different input distributions
- **Spatial Assumption Validity**: The local merging approach assumes that spatial proximity correlates with semantic similarity across all datasets and tasks, which may break down for datasets with strong spatial structure
- **SAM Pretraining Dependency**: The claimed compressibility benefits from SAM pretraining are demonstrated primarily on DeiT-SAM models, and transferability to other architectures remains unclear

## Confidence

**High Confidence**: The empirical results demonstrating 1.8× FLOP reduction and 1.6× throughput speedup with negligible accuracy loss are well-supported by extensive experiments across multiple model scales (ViT-T to ViT-Huge) and tasks (ImageNet-1k, transfer learning).

**Medium Confidence**: The mechanism explanations for delayed merging and spatial awareness are internally consistent and supported by ablation studies, but lack external validation from independent research or theoretical guarantees.

**Low Confidence**: The generalizability of the spatial merging assumption and the SAM pretraining benefits to new domains and architectures has not been thoroughly tested.

## Next Checks

1. **Distribution Shift Robustness Test**: Evaluate DSM on out-of-distribution datasets (e.g., ImageNet-C, natural adversarial examples) to verify that delayed merging maintains stability when token convergence patterns differ from training data.

2. **Spatial Dependency Analysis**: Systematically test DSM on datasets where spatial relationships do NOT correlate with semantic similarity (e.g., medical scans with anatomical structure, satellite imagery with geographic patterns) to quantify the limits of the local merging assumption.

3. **Architecture Transfer Validation**: Apply DSM to non-DeiT architectures (e.g., Swin Transformers, PVT, ConViT) to assess whether the delayed merging benefits generalize beyond models specifically designed for early convergence.