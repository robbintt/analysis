---
ver: rpa2
title: A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical
  Variables
arxiv_id: '2307.02071'
source_url: https://arxiv.org/abs/2307.02071
tags:
- effects
- data
- random
- categorical
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study empirically compares various machine learning methods,
  including tree-boosting, deep neural networks, and linear mixed effects models,
  on multiple tabular datasets with high-cardinality categorical variables. The authors
  find that machine learning models incorporating random effects consistently outperform
  their classical counterparts without random effects.
---

# A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables

## Quick Facts
- arXiv ID: 2307.02071
- Source URL: https://arxiv.org/abs/2307.02071
- Reference count: 3
- Primary result: Tree-boosting with random effects outperforms deep neural networks with random effects on tabular data with high-cardinality categorical variables

## Executive Summary
This study empirically compares various machine learning methods for handling high-cardinality categorical variables in tabular datasets. The authors find that incorporating random effects consistently improves model performance across multiple datasets. Tree-boosting with random effects achieves the highest prediction accuracy, outperforming deep neural networks with random effects. The results suggest that random effects serve as an effective regularizer by pooling information across levels and learning appropriate shrinkage from the data.

## Method Summary
The study compares eight different methods: Linear mixed effects models, Deep neural networks with embeddings, LMMNN (combining DNN and random effects), LightGBM approaches (LGBM Num, LGBM Cat), CatBoost, and GPBoost (combining tree-boosting and random effects). Models were evaluated using 5-fold cross-validation on multiple datasets including Airbnb, IMDb, Spotify, News, InstEval, Rossmann, AUimport, and Wages, with mean squared error as the metric. The key innovation was incorporating random effects into tree-boosting and deep learning models to handle high-cardinality categorical variables.

## Key Results
- Machine learning models with random effects consistently outperform classical models without random effects
- Tree-boosting with random effects achieves the highest prediction accuracy among all tested methods
- Deep neural networks with random effects perform better than their counterparts without random effects but worse than tree-boosting with random effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random effects act as an effective regularizer that balances the bias-variance tradeoff for high-cardinality categorical variables.
- Mechanism: Random effects shrink group-specific estimates toward a global mean, reducing overfitting when few samples per level exist. The shrinkage factor is data-driven, adapting to the number of samples per level and variance parameters.
- Core assumption: High-cardinality categorical variables have few observations per level, making traditional models prone to overfitting or underfitting.
- Evidence anchors:
  - [abstract]: "machine learning models incorporating random effects consistently outperform their classical counterparts without random effects"
  - [section]: "Broadly speaking, random effects act as a prior, or regularizer, which models the difficult part of a function... provide an effective way for finding a balance between over- and underfitting or bias and variance"
  - [corpus]: Weak evidence - corpus contains related papers but lacks direct empirical support for this specific mechanism.
- Break condition: When the number of samples per level is large, the shrinkage factor approaches zero and random effects provide negligible benefit.

### Mechanism 2
- Claim: Random effects improve estimation efficiency of fixed effects functions for high-cardinality categorical variables.
- Mechanism: By modeling the difficult part of the function (high-cardinality categorical effects) with random effects, the fixed effects function can be estimated with lower variance, leading to better overall model performance.
- Core assumption: The random effects model can separate the signal from high-cardinality categorical variables from the signal from other predictors.
- Evidence anchors:
  - [section]: "In line with the above argumentation, Sigrist [2023, Section 4.1] find in empirical experiments that tree-boosting combined with random effects outperforms traditional independent tree-boosting the more, the lower the number of samples per level of a categorical variable"
  - [abstract]: "tree-boosting with random effects achieves the highest prediction accuracy"
  - [corpus]: Weak evidence - corpus contains related papers but lacks direct empirical support for this specific mechanism.
- Break condition: When high-cardinality categorical variables have little predictive power, the efficiency gain from random effects is minimal.

### Mechanism 3
- Claim: Tree-boosting with random effects outperforms deep neural networks with random effects on tabular data with high-cardinality categorical variables.
- Mechanism: Tree-boosting algorithms handle categorical splits more naturally than neural networks, and when combined with random effects, they leverage both the strengths of tree structures and regularization.
- Core assumption: Tree-boosting algorithms are inherently better suited for tabular data than deep neural networks.
- Evidence anchors:
  - [abstract]: "tree-boosting with random effects outperforms deep neural networks with random effects"
  - [section]: "this is in line with the recent work of Grinsztajn et al. [2022] who find that tree-boosting outperforms deep neural networks (and also random forest) on tabular data without high-cardinality categorical variables"
  - [corpus]: Weak evidence - corpus contains related papers but lacks direct empirical support for this specific mechanism.
- Break condition: When the data has complex nonlinear relationships that neural networks can capture better than tree-boosting, or when the number of samples per level is large.

## Foundational Learning

- Concept: High-cardinality categorical variables
  - Why needed here: Understanding what high-cardinality categorical variables are and why they pose challenges for machine learning models is crucial for understanding the problem this paper addresses.
  - Quick check question: What is the definition of a high-cardinality categorical variable according to the paper?
- Concept: Random effects models
  - Why needed here: Random effects models are the key technique used in this paper to handle high-cardinality categorical variables. Understanding how they work and why they are effective is essential.
  - Quick check question: How do random effects models handle high-cardinality categorical variables differently from traditional models?
- Concept: Tree-boosting algorithms
  - Why needed here: Tree-boosting algorithms are one of the main machine learning methods compared in this paper. Understanding their strengths and weaknesses, especially in handling categorical variables, is important.
  - Quick check question: What are the main tree-boosting algorithms mentioned in the paper, and how do they handle categorical variables?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Model selection -> Evaluation
  - Handling high-cardinality categorical variables -> Training with/without random effects -> 5-fold cross-validation with MSE
- Critical path:
  1. Preprocess data to handle high-cardinality categorical variables
  2. Select and train models (with and without random effects)
  3. Evaluate models using 5-fold cross-validation
  4. Compare results and identify best-performing model
- Design tradeoffs:
  - Random effects vs no random effects: Random effects provide regularization but add complexity and computational cost
  - Tree-boosting vs deep neural networks: Tree-boosting may be more effective for tabular data, but deep neural networks may capture more complex relationships
  - Including vs excluding high-cardinality categorical variables in fixed effects: Including allows for interactions but may add unnecessary complexity
- Failure signatures:
  - Overfitting: High training accuracy but low validation accuracy
  - Underfitting: Low accuracy on both training and validation data
  - Poor handling of high-cardinality categorical variables: High error on levels with few samples
- First 3 experiments:
  1. Compare a tree-boosting model with random effects to the same model without random effects on a dataset with high-cardinality categorical variables
  2. Compare a deep neural network with random effects to a tree-boosting model with random effects on the same dataset
  3. Compare the effect of including vs excluding high-cardinality categorical variables in the fixed effects part of a mixed effects model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance of different machine learning methods with random effects vary across different types of high-cardinality categorical variables (e.g., variables with different distributions of sample sizes per level)?
- Basis in paper: [inferred] The paper mentions that the effectiveness of random effects is related to the number of samples per level, but does not explicitly explore variations across different types of high-cardinality variables.
- Why unresolved: The paper provides a general comparison of methods but does not delve into how the nature of the high-cardinality variables themselves affects the performance of random effects.
- What evidence would resolve it: A detailed analysis comparing the performance of methods with random effects across various types of high-cardinality categorical variables, such as those with uniform versus skewed distributions of sample sizes per level.

### Open Question 2
- Question: What are the computational trade-offs between using tree-boosting with random effects and deep neural networks with random effects for large-scale datasets?
- Basis in paper: [explicit] The paper mentions that the MERF algorithm is prohibitively slow for large sample sizes, but does not provide a detailed comparison of computational efficiency between tree-boosting and deep neural networks with random effects.
- Why unresolved: The paper focuses on prediction accuracy but does not explore the computational costs associated with different methods, which is crucial for practical applications.
- What evidence would resolve it: A comprehensive study measuring the computational time and resource usage of tree-boosting and deep neural networks with random effects on large-scale datasets.

### Open Question 3
- Question: How do interactions between high-cardinality categorical variables and other predictor variables affect the performance of machine learning models with random effects?
- Basis in paper: [explicit] The paper mentions that including high-cardinality categorical variables in the fixed effects function can allow for potential interactions, but the impact of these interactions on model performance is not explicitly studied.
- Why unresolved: The paper suggests that interactions might not be significant based on minor differences in results, but a thorough investigation is needed to confirm this.
- What evidence would resolve it: An experimental study examining the impact of including interactions between high-cardinality categorical variables and other predictors on the performance of machine learning models with random effects.

## Limitations

- Deep neural network architectures used for comparison are not fully specified, limiting reproducibility
- The study focuses on regression tasks, leaving uncertainty about generalization to classification problems
- Computational costs of different methods are not compared, which could affect practical adoption

## Confidence

High: Random effects improve model performance on high-cardinality categorical variables
Medium: Tree-boosting with random effects outperforms deep neural networks with random effects
Low: The specific architectures of deep neural networks used in the study

## Next Checks

1. Conduct ablation studies to isolate the contribution of random effects versus model architecture (tree-boosting vs neural networks)
2. Test the generalizability of results to classification tasks with high-cardinality categorical variables
3. Compare computational efficiency across methods, including training time and memory requirements