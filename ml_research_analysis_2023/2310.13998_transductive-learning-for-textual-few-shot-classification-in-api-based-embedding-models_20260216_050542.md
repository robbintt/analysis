---
ver: rpa2
title: Transductive Learning for Textual Few-Shot Classification in API-based Embedding
  Models
arxiv_id: '2310.13998'
source_url: https://arxiv.org/abs/2310.13998
tags:
- learning
- arxiv
- few-shot
- language
- transductive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses few-shot text classification in an API-based
  setting where only embeddings from a pre-trained encoder are accessible. It proposes
  a transductive learning approach that leverages unlabeled query data, overcoming
  the limitations of standard inductive methods.
---

# Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models

## Quick Facts
- arXiv ID: 2310.13998
- Source URL: https://arxiv.org/abs/2310.13998
- Reference count: 40
- Key outcome: Proposed transductive methods outperform inductive baselines by up to 3.7% F1 score on a benchmark of 8 datasets (up to 151 classes, 4 languages)

## Executive Summary
This paper addresses few-shot text classification using API-based pre-trained encoders where only black-box access to embeddings is available. The authors propose a transductive learning approach that leverages unlabeled query data to improve classification performance. A novel parameter-free transductive regularizer based on Fisher-Rao distances is introduced, which maximizes mutual information between predictions without requiring hyperparameter tuning. Experiments on 8 datasets across 4 languages show consistent improvements over inductive baselines.

## Method Summary
The method operates within API-based constraints where model gradients are inaccessible and data privacy is critical. It uses transductive inference to jointly classify all query samples, leveraging their statistics to refine decision boundaries. The core innovation is a Fisher-Rao distance-based regularizer that acts as a surrogate for mutual information, encouraging informative predictions without requiring parameter tuning. The approach trains a classification head on the support set using cross-entropy loss while applying the Fisher-Rao regularizer using query set predictions, all while only having access to embeddings via API.

## Key Results
- Transductive methods outperform inductive baselines by up to 3.7% F1 score
- Fisher-Rao regularizer achieves strong performance without hyperparameter tuning
- Method shows consistent gains across 8 datasets, 4 languages, and 8 backbone models
- Particularly effective in few-shot scenarios with limited labeled examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transductive inference improves few-shot classification by leveraging unlabeled query set statistics
- Mechanism: Unlike inductive learning which makes independent predictions per sample, transductive methods jointly classify all query samples, allowing mutual information maximization between predictions and labels to refine decision boundaries
- Core assumption: The query set shares the same class distribution as the support set and contains enough samples to provide meaningful statistical leverage
- Evidence anchors:
  - [abstract]: "Transductive inference, unlike traditional inductive learning, leverages the statistics of unlabeled data."
  - [section]: "Transductive inference succeeds in FSL because it jointly classifies all unlabeled query samples of a single task, leading to more efficient and accurate classification."

### Mechanism 2
- Claim: Fisher-Rao distance provides an effective parameter-free transductive regularizer
- Mechanism: The Fisher-Rao distance between soft predictions of query samples acts as a surrogate for mutual information, encouraging predictions to be informative about each other without requiring hyperparameter tuning
- Core assumption: The Fisher-Rao distance between probability distributions correlates with their mutual information in the classification context
- Evidence anchors:
  - [abstract]: "We also introduce a new parameter-free transductive regularizer based on the Fisher-Rao loss."
  - [section]: "Expression (4) yields a surrogate of the Mutual Information as shown by the following proposition."

### Mechanism 3
- Claim: API-based access constraints make transductive methods particularly valuable
- Mechanism: When model gradients are inaccessible and data privacy is critical, transductive methods can leverage query set statistics without requiring parameter updates or label sharing, unlike prompt-based or in-context learning approaches
- Core assumption: The API provides sufficient embedding quality to enable effective transductive inference without access to the underlying model parameters
- Evidence anchors:
  - [abstract]: "This method fully utilizes unlabeled data, does not share any label with the third-party API provider."
  - [section]: "Instead, we focus on methods that can operate within API-based constraints."

## Foundational Learning

- Concept: Transductive vs. Inductive Learning
  - Why needed here: The paper's core contribution relies on understanding how transductive inference differs from standard inductive approaches in few-shot learning
  - Quick check question: What is the key difference between transductive and inductive learning in the context of few-shot classification?

- Concept: Mutual Information and Information Maximization
  - Why needed here: The proposed Fisher-Rao regularizer is motivated by mutual information maximization principles, which are central to understanding why it works
  - Quick check question: How does mutual information maximization help improve few-shot classification performance?

- Concept: Fisher-Rao Distance
  - Why needed here: The paper introduces a novel parameter-free regularizer based on Fisher-Rao distance, which requires understanding its properties and relationship to other divergences
  - Quick check question: What makes Fisher-Rao distance particularly suitable as a transductive regularizer compared to other distance measures?

## Architecture Onboarding

- Component map: Text sequences -> API-based embedding layer -> Classification head g_Ï• -> Fisher-Rao regularizer -> Class predictions

- Critical path:
  1. Retrieve embeddings for support and query sets via API
  2. Train classification head on support set with cross-entropy loss
  3. Apply Fisher-Rao transductive regularizer using query set predictions
  4. Generate final predictions by combining both objectives

- Design tradeoffs:
  - Fisher-Rao vs. Entropic regularization: Fisher-Rao is parameter-free but computationally heavier
  - Query set size: Larger sets provide better statistics but increase computation
  - API embedding quality: Critical for performance but outside user control

- Failure signatures:
  - Performance degrades with very small query sets (<5 samples per class)
  - No improvement over inductive baselines when API embeddings lack discriminative power
  - High variance in results across different random seeds

- First 3 experiments:
  1. Compare Fisher-Rao regularizer vs. standard entropic regularization on a simple dataset (e.g., Emotion)
  2. Test sensitivity to query set size by varying N and K parameters
  3. Evaluate performance across different API-based models (e.g., RoBERTa vs. text-davinci) on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Fisher-Rao (FR) regularizer compare to other transductive methods on datasets with a very large number of classes (e.g., 100+ classes)?
- Basis in paper: [inferred] The paper mentions that the proposed Fisher-Rao based regularizer is parameter-free and outperforms traditional fine-tuning based on cross-entropy by a margin of 3.7%. However, it does not provide results on datasets with a very large number of classes.
- Why unresolved: The paper only evaluates the methods on datasets with up to 151 classes. It is unclear how the Fisher-Rao regularizer would perform on datasets with an even larger number of classes.
- What evidence would resolve it: Additional experiments on datasets with a very large number of classes would provide evidence to resolve this question.

### Open Question 2
- Question: How does the performance of the Fisher-Rao regularizer vary across different languages, especially for low-resource languages?
- Basis in paper: [inferred] The paper mentions that the proposed method is evaluated on datasets in four different languages. However, it does not provide a detailed analysis of the performance across different languages, especially for low-resource languages.
- Why unresolved: The paper does not provide a detailed analysis of the performance of the Fisher-Rao regularizer across different languages, especially for low-resource languages. This could be an important factor in real-world applications.
- What evidence would resolve it: Additional experiments on datasets in different languages, especially low-resource languages, would provide evidence to resolve this question.

### Open Question 3
- Question: How does the performance of the Fisher-Rao regularizer change with different embedding dimensions?
- Basis in paper: [inferred] The paper mentions that the proposed method is evaluated using different backbone models with different embedding dimensions. However, it does not provide a detailed analysis of the performance of the Fisher-Rao regularizer with different embedding dimensions.
- Why unresolved: The paper does not provide a detailed analysis of the performance of the Fisher-Rao regularizer with different embedding dimensions. This could be an important factor in real-world applications where the embedding dimensions may vary.
- What evidence would resolve it: Additional experiments on datasets with different embedding dimensions would provide evidence to resolve this question.

## Limitations

- The approach relies heavily on the assumption that query set statistics are representative of support set classes, which may not hold in imbalanced or domain-shifted scenarios
- The Fisher-Rao regularizer introduces computational overhead that scales quadratically with query set size, potentially limiting scalability
- The evaluation framework may not fully capture real-world deployment scenarios where class distributions can be highly imbalanced or open-set

## Confidence

**High Confidence** - The core claim that transductive inference improves few-shot classification performance is well-supported by experimental results across multiple datasets and backbone models.

**Medium Confidence** - The effectiveness of the Fisher-Rao distance as a parameter-free regularizer is supported by results, but the theoretical connection to mutual information could benefit from additional rigorous analysis.

**Medium Confidence** - The API-based constraint argument is logically sound, but the paper's focus on this specific scenario limits generalizability to other black-box optimization techniques.

## Next Checks

1. **Query Set Size Sensitivity Analysis**: Systematically evaluate the performance of the Fisher-Rao regularizer across a broader range of query set sizes to identify the optimal trade-off between statistical leverage and computational cost, particularly focusing on the break condition where query sets become too small to provide meaningful statistics.

2. **Class Distribution Robustness Test**: Design experiments with varying class imbalance ratios in the support set to test the Fisher-Rao regularizer's robustness to non-uniform class distributions, measuring how performance degrades as the assumption of balanced classes is violated.

3. **Domain Adaptation Evaluation**: Test the transductive approach on datasets where the query set contains samples from classes not present in the support set (open-set scenario), measuring the method's ability to detect and handle out-of-distribution samples while maintaining performance on known classes.