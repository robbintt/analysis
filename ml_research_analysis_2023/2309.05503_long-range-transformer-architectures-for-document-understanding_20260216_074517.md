---
ver: rpa2
title: Long-Range Transformer Architectures for Document Understanding
arxiv_id: '2309.05503'
source_url: https://arxiv.org/abs/2309.05503
tags:
- attention
- document
- documents
- layoutlm
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of applying Transformer-based
  models to long multi-page documents, which is limited by the computational complexity
  of self-attention operations. The authors propose two new multi-modal (text + layout)
  long-range models for Document Understanding (DU) based on efficient implementations
  of Transformers for long sequences: LayoutLinformer, using a low-rank approximation,
  and LayoutCosformer, using a kernel-based method.'
---

# Long-Range Transformer Architectures for Document Understanding

## Quick Facts
- **arXiv ID**: 2309.05503
- **Source URL**: https://arxiv.org/abs/2309.05503
- **Reference count**: 0
- **Primary result**: Improved Information Retrieval performance on multi-page business documents using LayoutLinformer and LayoutCosformer with 2D relative attention bias.

## Executive Summary
This paper addresses the challenge of applying Transformer-based models to long multi-page documents, which is limited by the computational complexity of self-attention operations. The authors propose two new multi-modal (text + layout) long-range models for Document Understanding (DU) based on efficient implementations of Transformers for long sequences: LayoutLinformer, using a low-rank approximation, and LayoutCosformer, using a kernel-based method. They also introduce 2D relative attention bias to guide self-attention towards relevant tokens without harming model efficiency. The proposed models are compared to LayoutLM, a classical Transformer adapted for DU and pre-trained on millions of documents. The results show improvements on multi-page business documents for Information Retrieval, with a small performance cost on smaller sequences. The 2D relative attention is effective on dense text for both normal and long-range models.

## Method Summary
The authors propose two efficient Transformer architectures for long document sequences: LayoutLinformer, which uses low-rank approximation of the attention matrix, and LayoutCosformer, which employs a kernel-based similarity method. Both models incorporate 2D relative attention bias to capture spatial relationships in document layouts. The models are pre-trained on business documents using Masked Visual-Language Modeling and fine-tuned on downstream tasks with BIESO tags for sequence tagging. Training uses Adam optimizer with learning rate 2e-5, linear warmup 5%, and linear decay, with batch sizes adjusted for sequence length (48 for 512 tokens, 16 for 2048 tokens).

## Key Results
- LayoutLinformer and LayoutCosformer outperform LayoutLM on multi-page business documents for Information Retrieval tasks.
- 2D relative attention bias improves performance on dense text layouts for both normal and long-range models.
- The proposed models show a small performance cost on smaller sequences compared to LayoutLM.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the full NxN self-attention with a low-rank approximation allows the model to handle longer sequences without exploding memory usage.
- Mechanism: The attention matrix QK^T is approximated by projecting keys and values into a lower-dimensional space of size k, yielding complexity O(Nk) instead of O(N²).
- Core assumption: The attention matrix is low-rank enough that the approximation preserves essential information for document understanding.
- Evidence anchors:
  - [abstract] "... low-rank approximation ... allows the model to handle longer sequences without exploding memory usage."
  - [section] "Empirical observations on multiple NLP tasks show that the attention matrix can be replaced with a lower rank approximation without harming the attention process too much [30]."
  - [corpus] Weak/no direct citations on low-rank effectiveness for document understanding.
- Break condition: If the attention matrix is not sufficiently low-rank for dense documents, the approximation will degrade performance significantly.

### Mechanism 2
- Claim: Using a kernel-based similarity instead of dot-product attention enables linear complexity O(N) while still capturing contextual relationships.
- Mechanism: The model replaces exp(QK^T) with Φ(Q)Φ(K^T), where Φ is a nonlinear function (e.g., ReLU), and exploits the separability of Φ(Q) to reduce complexity.
- Core assumption: The nonlinear function Φ can be chosen so that the transformed similarities preserve meaningful token relationships.
- Evidence anchors:
  - [abstract] "... kernel-based method ... replacing the non-linear similarity computation between Q and K with a linear operation."
  - [section] "Qin et al. [25] proposed to replace exp(QK^T) with Φ(Q)Φ(K^T) ... Computations can then be reordered to decrease the complexity to O(N)."
  - [corpus] No direct corpus evidence on kernel-based similarity for documents.
- Break condition: If Φ fails to preserve the semantics of token relationships, the model's performance will suffer.

### Mechanism 3
- Claim: 2D relative attention bias guides the model to attend more to tokens that are spatially close in the document layout, improving understanding of structured text like tables.
- Mechanism: An attention bias matrix B is computed from 2D token positions and multiplied element-wise with the attention matrix before softmax, effectively reshaping attention distributions.
- Core assumption: Proximity in layout correlates with semantic relevance for document understanding tasks.
- Evidence anchors:
  - [abstract] "... propose 2D relative attention bias to guide self-attention towards relevant tokens without harming model efficiency."
  - [section] "We first prove that the product of two separable functions is also itself separable ... We chose to compare 2 different attention biases ... squircle and cross."
  - [corpus] No corpus evidence specifically for 2D relative attention on documents.
- Break condition: If the 2D positional encoding does not align with semantic structure, the bias may mislead the model.

## Foundational Learning

- **Concept**: Self-attention mechanism in Transformers
  - Why needed here: Understanding how full self-attention leads to O(N²) complexity is essential to grasp why efficient approximations are necessary.
  - Quick check question: In a Transformer with sequence length N, what is the time and memory complexity of computing the full self-attention matrix?

- **Concept**: Low-rank matrix approximation
  - Why needed here: The Linformer approach relies on approximating the attention matrix with a low-rank projection; knowing how this works is critical to understand its efficiency gains.
  - Quick check question: If an NxN matrix is approximated by a rank-k matrix with k << N, how does this affect storage and computation?

- **Concept**: Kernel methods and feature maps
  - Why needed here: The cosFormer uses a kernel trick to achieve linear complexity; understanding kernels is key to why this method works.
  - Quick check question: What property of a kernel function allows it to be expressed as an inner product in a higher-dimensional feature space?

## Architecture Onboarding

- **Component map**: Token embedding → 2D positional + page embedding → efficient self-attention (Linformer or cosFormer) → feed-forward → output tag scores
- **Critical path**: Token → 2D positional + page embedding → efficient self-attention → feed-forward → output tag scores
- **Design tradeoffs**: Linformer offers simpler implementation but loses interpretability of attention; cosFormer is more complex but allows 2D relative bias. Both trade some modeling capacity for efficiency.
- **Failure signatures**: Performance drops on longer documents indicate the approximation is too lossy; poor table or dense text understanding may signal ineffective 2D bias.
- **First 3 experiments**:
  1. Compare inference speed and memory usage of Linformer vs cosFormer on synthetic long sequences.
  2. Evaluate 2D relative attention on a small dense-text document dataset to measure layout-aware gains.
  3. Test impact of rank k (Linformer) or kernel parameters (cosFormer) on accuracy for medium-length documents.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pre-training dataset impact the performance of long-range models on different document types and tasks?
- Basis in paper: [explicit] The paper shows that pre-training LayoutLM on business documents instead of RVL-CDIP negatively impacts its performance on the DocBank dataset.
- Why unresolved: The paper only tests two pre-training datasets and does not explore the impact of other factors like document diversity, domain specificity, or pre-training objectives.
- What evidence would resolve it: Systematic comparison of model performance across various pre-training datasets, document types, and tasks, while controlling for other variables.

### Open Question 2
- Question: How can long-range Transformers be adapted to handle document layouts that deviate significantly from the standard left-to-right, top-to-bottom reading order?
- Basis in paper: [inferred] The paper mentions that the sequence order is suboptimal for understanding some documents, like tables, and proposes 2D relative attention to mitigate this issue. However, it does not explore more complex layout adaptations.
- Why unresolved: The paper only tests simple 2D relative attention patterns and does not investigate more sophisticated methods for capturing complex layout structures.
- What evidence would resolve it: Experiments comparing the performance of long-range models with various layout-aware attention mechanisms on documents with diverse and complex layouts.

### Open Question 3
- Question: How can long-range Transformers be made more efficient for document understanding tasks while maintaining or improving performance?
- Basis in paper: [explicit] The paper compares two efficient long-range Transformer architectures (LayoutLinformer and LayoutCosformer) and shows their improved performance on long documents. However, it does not explore other potential efficiency improvements.
- Why unresolved: The paper only tests two specific efficient Transformer architectures and does not investigate other potential optimizations like pruning, quantization, or knowledge distillation.
- What evidence would resolve it: Comparative analysis of long-range model performance and efficiency across various optimization techniques and architectures.

## Limitations

- The models sacrifice some modeling capacity for efficiency gains, potentially missing complex attention patterns in dense documents.
- Evaluation is limited to two specific document types and tasks, raising questions about generalizability to other domains.
- 2D relative attention effectiveness is demonstrated but not rigorously isolated from other architectural changes.

## Confidence

- **High Confidence**: Efficiency improvements of LayoutLinformer and LayoutCosformer for handling long sequences are well-supported by complexity analysis and empirical measurements.
- **Medium Confidence**: Performance improvements on multi-page business documents are demonstrated, but the extent to which this results from the long-range architecture versus other factors is unclear.
- **Low Confidence**: Claims about universal effectiveness of 2D relative attention bias across different document types and tasks are not sufficiently supported.

## Next Checks

1. Conduct controlled ablation studies comparing models with and without 2D relative attention on diverse document types (forms, receipts, contracts) to isolate the contribution of this mechanism.

2. Test the models on progressively longer documents (beyond 2048 tokens) to identify breaking points where approximations degrade performance, and measure the relationship between sequence length and accuracy drop.

3. Evaluate the pre-trained models on a third, unseen document domain (e.g., medical records or legal documents) without additional fine-tuning to assess the transferability of learned representations.