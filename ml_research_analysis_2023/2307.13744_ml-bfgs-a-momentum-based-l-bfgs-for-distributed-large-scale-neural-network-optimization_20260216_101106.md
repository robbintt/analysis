---
ver: rpa2
title: 'mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network
  Optimization'
arxiv_id: '2307.13744'
source_url: https://arxiv.org/abs/2307.13744
tags:
- ml-bfgs
- hessian
- training
- convergence
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenges of using quasi-Newton methods
  like L-BFGS for large-scale neural network training, where compute costs, memory
  usage, and stability in stochastic settings limit their adoption. The authors propose
  mL-BFGS, a momentum-based L-BFGS algorithm that stabilizes convergence by reducing
  Hessian noise through momentum in parameter and gradient updates.
---

# mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization

## Quick Facts
- arXiv ID: 2307.13744
- Source URL: https://arxiv.org/abs/2307.13744
- Reference count: 40
- Primary result: mL-BFGS converges faster per iteration and wall-clock time than SGD, Adam, and KFAC while maintaining comparable accuracy on CIFAR and ImageNet benchmarks

## Executive Summary
The paper addresses the challenge of applying quasi-Newton methods like L-BFGS to large-scale neural network training, where traditional implementations face prohibitive memory and compute costs. mL-BFGS introduces a momentum-based approach that stabilizes convergence by reducing stochastic noise in the Hessian approximation, while a block-wise Hessian approximation enables distributed training by partitioning compute and memory costs across nodes. The method achieves linear convergence under mild conditions and demonstrates faster convergence than first-order methods on standard benchmarks.

## Method Summary
mL-BFGS combines momentum-based noise reduction with block-wise Hessian approximation for distributed training. The algorithm applies momentum to both parameter and gradient updates to smooth stochastic fluctuations in the history vectors used for L-BFGS updates. For distributed training, the model parameters are partitioned into blocks, with each node computing and storing only the history vectors for its assigned block. An adaptive damping scheme preserves positive definiteness of the Hessian approximation. The method achieves theoretical linear convergence guarantees and practical speedups over first-order optimizers.

## Key Results
- On ImageNet with ResNet-50, mL-BFGS matches SGD accuracy (74.6%) with similar per-epoch runtime (~7.9 minutes)
- KFAC achieves similar accuracy (74.5%) but requires ~17 minutes per epoch due to higher compute overhead
- mL-BFGS demonstrates faster per-iteration and wall-clock convergence than SGD, Adam, and KFAC on CIFAR-10, CIFAR-100, and ImageNet with ResNet and Vision Transformer models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Momentum applied to both parameters and gradients in the Hessian approximation reduces stochastic noise and stabilizes convergence in stochastic optimization.
- Mechanism: Moving averages of parameter changes (Mθ) and gradient changes (Mg) smooth out stochastic fluctuations in history vectors sk and yk used for L-BFGS updates.
- Core assumption: Stochastic noise follows patterns that can be smoothed by momentum averaging.
- Evidence anchors: [abstract] "mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian"
- Break condition: If stochastic noise has high-frequency components that momentum averaging cannot capture, smoothing effect may be reduced.

### Mechanism 2
- Claim: Block-wise Hessian approximation distributes computational and memory costs across distributed nodes, enabling scalable training of large models.
- Mechanism: Model parameters are partitioned into blocks, with each node computing and storing history vectors only for its assigned block.
- Core assumption: Block-diagonal approximation preserves positive definiteness and provides sufficient curvature information.
- Evidence anchors: [abstract] "For model training at a large scale, mL-BFGS approximates a block-wise Hessian, thus enabling distributing compute and memory costs across all computing nodes"
- Break condition: If interaction between blocks is significant, block-diagonal approximation may lose important curvature information.

### Mechanism 3
- Claim: Damping scheme preserves positive definiteness of the Hessian approximation and prevents divergence during training.
- Mechanism: Adaptive damping modifies gradient change vector yk by blending it with parameter change vector sk based on their inner product ratio.
- Core assumption: Damping coefficient can be computed from local information without global synchronization.
- Evidence anchors: [section] "To preserve the positive of the Hessian, we adopt an adaptive damping scheme as ˆyi =τ·yi + (1−τ)·si"
- Break condition: If damping bounds σL and σH are not properly chosen, Hessian approximation may become too aggressive or conservative.

## Foundational Learning

- Concept: Quasi-Newton methods and BFGS update rule
  - Why needed here: Understanding how L-BFGS approximates Hessian inverse using history vectors is fundamental to grasping mL-BFGS's improvements
  - Quick check question: How does the BFGS update rule ensure that the secant condition sk = Hk·yk is satisfied?

- Concept: Stochastic optimization and variance reduction
  - Why needed here: The paper addresses how mL-BFGS reduces stochastic noise in Hessian approximation, requiring understanding of variance challenges in stochastic gradients
  - Quick check question: Why does vanilla L-BFGS suffer from convergence instability in stochastic training?

- Concept: Distributed computing and parallelization strategies
  - Why needed here: mL-BFGS uses block-wise approximation to distribute computation across nodes, requiring understanding of distributed training paradigms
  - Quick check question: How does block-wise approximation in mL-BFGS differ from data parallelism in distributed training?

## Architecture Onboarding

- Component map: Momentum computation -> Block partitioning -> Hessian update -> Distributed communication -> Damping control
- Critical path: Forward/backward passes → Momentum update → Hessian approximation update (every T iterations) → Gradient preconditioning → All-Gather synchronization
- Design tradeoffs:
  - Block size vs. memory efficiency: Larger blocks provide better Hessian approximation but require more memory per node
  - Momentum coefficient β vs. noise reduction: Higher β provides better smoothing but may slow adaptation to new curvature information
  - Damping bounds σL, σH vs. stability: Tighter bounds provide more stability but may limit convergence speed
- Failure signatures:
  - Divergence in early training: Likely due to insufficient damping or inappropriate momentum settings
  - Slow convergence despite correct implementation: May indicate blocks are too small or momentum is too aggressive
  - Memory overflow on individual nodes: Block partitioning may need adjustment
- First 3 experiments:
  1. Single-GPU test: Implement mL-BFGS on small model (ResNet-18 on CIFAR-10) without distributed blocks
  2. Multi-GPU block test: Distribute larger model (ResNet-50) across 2-4 GPUs with different block sizes
  3. Scalability test: Compare mL-BFGS against SGD and Adam on ImageNet with increasing number of GPUs

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but raises several implicit research directions regarding scalability to trillion-parameter models, optimal block size configurations for different architectures, sensitivity to momentum parameters, and extension of momentum techniques to other quasi-Newton methods.

## Limitations
- The paper lacks specific hyperparameter values for mL-BFGS across different models and datasets, making faithful reproduction challenging
- The comparison with KFAC focuses primarily on runtime rather than exploring whether KFAC's more sophisticated curvature approximation might yield better final accuracy
- The exact implementation details of block-wise Hessian approximation and damping scheme parameter selection are not fully specified

## Confidence

- **High Confidence**: The core mechanism of momentum-based noise reduction in Hessian approximation is well-supported by theoretical analysis and empirical results
- **Medium Confidence**: The block-wise Hessian approximation's effectiveness in distributed settings is demonstrated but lacks detailed analysis of block size sensitivity and interaction effects
- **Medium Confidence**: The convergence analysis under mild conditions is theoretically sound but relies on assumptions that may not hold for all neural network architectures

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary momentum coefficient β, damping bounds σL/σH, and block sizes to identify optimal configurations for different model scales

2. **Cross-Architecture Generalization**: Test mL-BFGS on architectures beyond ResNets and Vision Transformers (e.g., transformers with attention mechanisms) to validate robustness

3. **Communication Overhead Measurement**: Quantify the All-Gather communication costs in distributed settings and analyze how they scale with block size and number of nodes