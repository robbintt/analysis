---
ver: rpa2
title: Similarity search in the blink of an eye with compressed indices
arxiv_id: '2304.04759'
source_url: https://arxiv.org/abs/2304.04759
tags:
- search
- vectors
- memory
- recall
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Locally-adaptive Vector Quantization (LVQ),
  a novel vector compression method designed to improve graph-based similarity search
  performance while reducing memory footprint. LVQ uses per-vector scaling and scalar
  quantization to achieve up to 3.8x compression ratio with minimal impact on accuracy.
---

# Similarity search in the blink of an eye with compressed indices

## Quick Facts
- arXiv ID: 2304.04759
- Source URL: https://arxiv.org/abs/2304.04759
- Reference count: 40
- Key outcome: LVQ achieves up to 20.7× throughput improvement with 3× memory reduction in low-memory regimes

## Executive Summary
This paper presents Locally-adaptive Vector Quantization (LVQ), a novel vector compression method designed to improve graph-based similarity search performance while reducing memory footprint. LVQ uses per-vector scaling and scalar quantization to achieve up to 3.8× compression ratio with minimal impact on accuracy. The method is optimized for random access patterns and fast similarity computations using AVX instructions. When combined with system optimizations for graph-based search, LVQ establishes a new state-of-the-art performance, achieving significant improvements in both throughput and memory efficiency for billion-scale datasets.

## Method Summary
LVQ compresses vectors using per-vector scaling constants and scalar quantization, reducing memory traffic during graph traversal. The method employs a two-level scheme: LVQ-8 for fast graph traversal and a second level for final re-ranking. System optimizations include prefetching neighbor vectors, using huge pages for memory management, and AVX-optimized similarity kernels. The approach is specifically designed for graph-based similarity search algorithms and maintains pruning accuracy through theoretical error bounds.

## Key Results
- Up to 20.7× throughput improvement with 3× memory reduction in low-memory regimes
- 5.8× throughput improvement with 1.4× less memory in high-throughput regimes
- Achieves state-of-the-art performance on billion-scale datasets like deep-96-1B and sift-128-1B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVQ reduces effective bandwidth during graph traversal by compressing each vector with per-vector scaling and scalar quantization.
- Mechanism: Instead of storing each vector as 32-bit floats, LVQ encodes each vector with fewer bits (e.g., 8 bits) plus small per-vector constants (mean, min, max). This shrinks memory traffic per lookup and speeds up similarity calculations on AVX hardware.
- Core assumption: The dynamic range of each vector's centered values is small enough that scalar quantization introduces negligible error.
- Evidence anchors:
  - [abstract] "uses per-vector scaling and scalar quantization to improve search performance with fast similarity computations and a reduced effective bandwidth"
  - [section 4] "Encoding, decoding and similarity computations are extremely fast, specifically designed for compatibility with native AVX instructions"
- Break condition: If vector values have large variance across dimensions, quantization error increases and recall drops.

### Mechanism 2
- Claim: LVQ enables graph construction directly on compressed vectors without sacrificing pruning accuracy.
- Mechanism: The pruning rule in graph building compares similarities using full-precision vs. compressed vectors. LVQ's small quantization error keeps these comparisons equivalent under certain bounds (Proposition 1), so candidate lists and edge selections remain valid.
- Core assumption: The quantization error magnitude is small compared to the distance between a node and its neighbors, so pruning decisions are preserved.
- Evidence anchors:
  - [section 5.2.3] "we use Ranked Bias Overlap (RBO) [56], a standard metric to compare ranked lists... For LVQ with 4, 8, and 16 bits, we observe a high-quality sorting"
  - [section 5.2.3] "When using Euclidean distance... the graph pruning rule using full-precision vectors... and the one using vectors compressed with LVQ... are equivalent"
- Break condition: If the number of quantization bits is too low (e.g., 2 bits), the error bound is violated and pruning quality degrades.

### Mechanism 3
- Claim: The two-level LVQ scheme recovers accuracy lost in the first-level compression by storing residuals.
- Mechanism: The first level (LVQ-8) compresses vectors for fast graph traversal; the second level stores compressed residuals for final re-ranking. This balances speed and accuracy.
- Core assumption: The residual quantization error is small enough that a second-level lookup does not dominate runtime.
- Evidence anchors:
  - [section 4.3] "We use the first level of compression during graph traversal... The reduced number of bits will generate a loss in accuracy. The second level... will be used for a final re-ranking step"
  - [section 7.3] "LVQ-4x8... This depends on the dataset. As expected, using two levels has an advantage for higher dimensional datasets"
- Break condition: If residuals are large, the second lookup may offset speed gains.

## Foundational Learning

- Concept: Quantization error distribution
  - Why needed here: To reason about accuracy loss when compressing vectors and to justify the per-vector scaling approach.
  - Quick check question: If a uniform quantization error has step size Δ, what is its standard deviation?
    - Answer: Δ/√12.

- Concept: Graph pruning rules and similarity comparison
  - Why needed here: To understand why LVQ can preserve pruning accuracy, based on bounding the error in similarity comparisons.
  - Quick check question: In the pruning condition α‖x*−x'‖² ≤ ‖x−x'‖², what geometric object separates accepted from rejected candidates?
    - Answer: The perpendicular bisector hyperplane between x and x*.

- Concept: Random memory access patterns in graph search
  - Why needed here: To appreciate why bandwidth reduction is more critical than compute optimization for LVQ.
  - Quick check question: In a graph search that visits O(W) neighbors per step, what dominates the cost if W is large and vectors are high-dimensional?
    - Answer: Memory latency/bandwidth for fetching neighbor vectors.

## Architecture Onboarding

- Component map: LVQ encoder/decoder -> Graph builder -> Graph searcher -> Prefetcher -> Huge-page allocator
- Critical path:
  1. Load compressed vector from memory.
  2. Decompress on-the-fly using AVX.
  3. Compute similarity against query.
  4. Maintain priority queue of top-W candidates.
  5. Repeat until queue exhausted.

- Design tradeoffs:
  - Compression ratio vs. recall: Lower bits reduce footprint but increase error.
  - Two-level vs. one-level: Two levels recover accuracy at extra compute cost.
  - Padding to cache lines: Improves prefetch efficiency but slightly increases memory.

- Failure signatures:
  - Recall drops sharply: Likely quantization error too large (too few bits or wrong per-vector bounds).
  - Throughput stalls: Memory bandwidth saturation or ineffective prefetching.
  - Build time increases: Graph pruning is rejecting many candidates due to quantization noise.

- First 3 experiments:
  1. Run exhaustive search with LVQ-8 vs. full-precision on a small dataset; verify recall drop is acceptable.
  2. Build a graph on LVQ-compressed vectors; check pruning accuracy via Ranked Bias Overlap.
  3. Profile memory bandwidth and compute cycles for single-query search with different prefetch offsets.

## Open Questions the Paper Calls Out
The paper mentions several future work directions including parallelizing search for individual queries, combining LVQ with dimensionality reduction techniques, and evaluating LVQ's performance in dynamic similarity search scenarios with frequent data distribution shifts.

## Limitations
- The paper focuses on static datasets and does not evaluate LVQ's performance in dynamic settings with frequent insertions, deletions, or updates.
- The claim that "system optimizations matter as much as the compression method" is supported by ablation studies on one dataset only.
- The paper does not address potential impact of cache thrashing when prefetching large neighbor lists.

## Confidence
- **High confidence**: The core mechanism of per-vector scaling with scalar quantization is mathematically sound and the AVX-optimized implementation appears correct based on the provided pseudocode and assembly snippets.
- **Medium confidence**: The recall-throughput tradeoff claims are well-supported by extensive benchmarks across multiple datasets and competitors, though the exact hyperparameters for competitor methods remain unspecified.
- **Low confidence**: The claim that LVQ's graph construction is "equivalent" to full-precision pruning (Proposition 1) relies on theoretical bounds that may not hold in practice for all data distributions or low bit-rates.

## Next Checks
1. **Cross-dataset robustness**: Run LVQ with the same configuration on diverse datasets (text, image, embeddings from different domains) to verify the recall-throughput curve holds universally.
2. **Error bound verification**: Implement a test that measures the actual pruning error rate when using LVQ-compressed vectors versus ground-truth full-precision pruning across different bit-widths.
3. **System sensitivity analysis**: Profile memory bandwidth usage and compute throughput with and without each optimization (prefetching, huge pages, core scaling) to quantify their individual contributions to the claimed 20.7× improvement.