---
ver: rpa2
title: Considerations on the Theory of Training Models with Differential Privacy
arxiv_id: '2303.04676'
source_url: https://arxiv.org/abs/2303.04676
tags:
- privacy
- data
- round
- which
- differential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter explores privacy concerns in federated learning, where
  clients collaborate to train a global model while keeping their local data private.
  Differential privacy (DP) is employed to address this challenge.
---

# Considerations on the Theory of Training Models with Differential Privacy

## Quick Facts
- arXiv ID: 2303.04676
- Source URL: https://arxiv.org/abs/2303.04676
- Reference count: 40
- One-line primary result: Explores privacy concerns in federated learning and employs differential privacy (DP) to address this challenge

## Executive Summary
This chapter examines the application of differential privacy (DP) in federated learning systems where multiple clients collaborate to train a global model while preserving individual data privacy. The focus is on Differentially Private Stochastic Gradient Descent (DP-SGD), which extends distributed SGD with DP guarantees through Gaussian noise addition and gradient clipping. The chapter provides a comprehensive analysis of DP mechanisms, their theoretical foundations, and practical implementation considerations, while also identifying key open questions for future research in this area.

## Method Summary
The method involves implementing DP-SGD with Gaussian noise addition, gradient clipping, and mini-batch sampling as described in Algorithm 1. The approach requires setting up a federated learning simulation with multiple clients, a central server, and an interrupt service routine for global model updates. Key parameters include the mini-batch size m, clipping threshold C, and noise level σ, which must be carefully chosen to balance privacy guarantees with model utility. The implementation involves computing gradients at clients, clipping them to a bounded norm, adding calibrated Gaussian noise, and transmitting the noised updates to the server for aggregation.

## Key Results
- DP-SGD achieves privacy guarantees through Gaussian noise addition that hides whether transmitted updates come from a client's data set or a neighboring data set
- Mini-batch SGD with independent gradient computations enables bounded sensitivity for DP analysis
- Subsampling with random mini-batches amplifies DP guarantees by reducing the probability that any single round uses the differentiating sample

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gaussian noise addition hides whether a transmitted round update comes from a client's own data set or a neighboring data set
- Mechanism: DP-SGD adds Gaussian noise with standard deviation proportional to the clipping constant C to the aggregated clipped gradients U, making it statistically difficult to distinguish U from U' where U' is based on a neighboring data set differing in one sample
- Core assumption: The distance between updates U and U' is bounded by 2C due to clipping, and the adversary cannot distinguish N(0, (2Cσ)²I) from N(2C, (2Cσ)²I)
- Evidence anchors:
  - [abstract] Differential privacy (DP) is employed to address this challenge
  - [section 2.3] Suppose that ah influences another gradient computation... If Gaussian noise is added to U and U', respectively, then the smaller the distance between U and U', the harder it is to figure out whether the actually observed noised update originates from d or d'
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.443
- Break condition: If the clipping constant C is too small relative to gradient norms, or if the noise level σ is too low, the privacy guarantee fails

### Mechanism 2
- Claim: Mini-batch SGD with independent gradient computations enables bounded sensitivity for DP analysis
- Mechanism: By computing all gradients ah in a batch using the same model w and not updating w between gradient computations, each gradient computation is independent, ensuring that the sensitivity of the aggregated update U is bounded by the clipping constant C
- Core assumption: Classical SGD where ah+1 uses the updated w from ah would introduce dependencies that increase sensitivity beyond 2C
- Evidence anchors:
  - [section 2.1] We call this the individual clipping approach... Clipping is needed because in general we cannot assume a bound C on the gradients... yet the added gradients in update U need to be bounded by some constant C in order for the DP analysis of [1] to go through
  - [section 2.3] In order to accomplish this, DP-SGD introduces noise... Clipping forces a small distance between an update U that does not use the differentiating sample and an update U' that computes the same gradients as U except for one of its gradient computations which uses the differentiating sample
- Break condition: If gradient computations within a batch are not independent (e.g., due to w updates), the sensitivity bound fails

### Mechanism 3
- Claim: Subsampling with random mini-batches amplifies DP guarantees by reducing the probability that any single round uses the differentiating sample
- Mechanism: By selecting a random mini-batch of size m from a data set of size N, each round has only probability m/N of including the differentiating sample, which amplifies the DP guarantee by the subsampling ratio
- Core assumption: The subsampling is uniform and the adversary does not learn the exact mini-batch size used in each round
- Evidence anchors:
  - [section 2.2] DP-SGD implements subsampling which chooses a uniformly random subset Sb⊆ d of size m
  - [section 4.2] Besides implementing Gaussian noise, DP-SGD also uses sub-sampling... We define convex combinations fp(α) = pf(α) + (1-p)(1-α) with corresponding p-sampling operator Cp(f) = min{fp, f-1/p}**
  - [corpus] Top related titles: Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy
- Break condition: If the adversary can determine the exact mini-batch size used in each round, or if the subsampling is not truly random, the amplification effect is lost

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP provides the theoretical framework for quantifying and limiting privacy leakage in federated learning systems where clients want to keep their local data private
  - Quick check question: What is the key property that makes two data sets "neighbors" in the context of DP?

- Concept: Gaussian Differential Privacy (GDP)
- Why needed here: GDP provides a hypothesis testing framework that characterizes DP guarantees using Gaussian noise distributions, which is exactly what DP-SGD implements
  - Quick check question: How does the trade-off function G_μ relate to distinguishing between N(0,1) and N(μ,1)?

- Concept: Composition of DP mechanisms
  - Why needed here: In federated learning, multiple rounds of DP-SGD are executed over many epochs, requiring composition theorems to bound the cumulative privacy loss
  - Quick check question: What is the relationship between the privacy guarantees of individual rounds and the overall DP guarantee after T rounds?

## Architecture Onboarding

- Component map: Client (local SGD, clipping, noise addition, round update transmission) -> Server (global model maintenance, aggregation of client updates) -> Central coordination (broadcasting global models, managing participation)
- Critical path: Client computes gradients -> Clips gradients -> Adds Gaussian noise -> Transmits noised update -> Server aggregates updates -> Broadcasts new global model -> Interrupt service routine updates local model
- Design tradeoffs: Privacy vs utility (higher noise for better privacy reduces accuracy), communication frequency vs privacy (more rounds with subsampling amplify DP), adaptive clipping vs static clipping (adaptive reduces noise impact over time)
- Failure signatures: Accuracy degradation (too much noise), convergence failure (inappropriate batch sizes or learning rates), privacy budget exhaustion (too many rounds without proper accounting), synchronization issues (dropped messages affecting global model updates)
- First 3 experiments:
  1. Test DP-SGD with different noise levels σ on a simple convex problem to observe the privacy-utility tradeoff
  2. Measure the effect of batch size m on convergence rate and DP guarantee amplification
  3. Implement adaptive clipping and evaluate its impact on final model accuracy compared to static clipping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are synthetic data approaches, such as differential private GANs, in achieving high-quality global models compared to direct DP-SGD on real data?
- Basis in paper: [explicit] Section 5.1 discusses using differential private GANs to generate synthetic data, avoiding repeated DP-SGD applications on real data, but notes the challenge of achieving high-quality synthetic data.
- Why unresolved: The paper identifies the generation of high-quality synthetic data as an open problem, particularly when using DP-SGD for training the GAN discriminator.
- What evidence would resolve it: Empirical studies comparing global model performance (accuracy, convergence) using synthetic data from DP-GANs versus real data with DP-SGD, under various privacy budgets.

### Open Question 2
- Question: What adaptive strategies optimally balance utility and differential privacy in DP-SGD, especially regarding dynamic adjustment of batch size, noise level, and clipping threshold?
- Basis in paper: [explicit] Section 5.2 discusses the potential of adaptive strategies for adjusting m, σ, and C during execution, but notes the open problem of discovering optimal adaptive strategies that proactively ensure DP guarantees.
- Why unresolved: The paper highlights the challenge of designing adaptive strategies that maintain DP guarantees while optimizing utility, without relying solely on post-hoc DP accounting.
- What evidence would resolve it: Development and evaluation of adaptive DP-SGD algorithms that dynamically adjust parameters based on convergence and accuracy, with provable DP guarantees.

### Open Question 3
- Question: Can weaker adversarial models than A0 be identified and analyzed to yield stronger (tighter) DP guarantees for DP-SGD?
- Basis in paper: [explicit] Section 5.3 discusses the potential of exploiting weaker adversarial models to derive DP guarantees with trade-off functions closer to 1−α, but notes this as an open problem.
- Why unresolved: The paper acknowledges the theoretical tightness of DP guarantees under strong adversaries like A0, but suggests that practical adversaries may have less capability, potentially leading to stronger guarantees.
- What evidence would resolve it: Formal analysis of DP guarantees under various weaker adversarial models, with empirical validation of their tightness and practical relevance.

## Limitations

- The analysis relies heavily on theoretical bounds for Gaussian noise addition and clipping mechanisms, but practical implementations may face challenges with gradient norm estimation and adaptive parameter tuning
- The chapter assumes uniform subsampling and independent gradient computations, which may not hold in all federated learning scenarios with heterogeneous client data distributions and potential stragglers
- The long-term effects of privacy budget accumulation over many rounds and the potential for privacy degradation due to model memorization are not fully explored

## Confidence

- **High Confidence**: The theoretical framework for Gaussian Differential Privacy and its relationship to standard DP definitions, the mechanism of noise addition for privacy protection, and the composition theorems for privacy budget accounting
- **Medium Confidence**: The practical implementation details of DP-SGD with clipping and subsampling, the exact privacy-utility tradeoff curves for specific model architectures and datasets
- **Low Confidence**: The effectiveness of synthetic data approaches for privacy preservation, the performance of adaptive clipping strategies in practice, and the potential of weaker adversarial models for reducing noise requirements

## Next Checks

1. **Empirical Validation of Privacy Guarantees**: Implement DP-SGD on a real federated learning dataset and empirically verify the privacy guarantees using membership inference attacks to complement the theoretical analysis
2. **Adaptive Parameter Optimization**: Develop and test adaptive strategies for clipping threshold C and noise level σ that adjust based on observed gradient statistics to improve the privacy-utility tradeoff
3. **Synthetic Data Evaluation**: Compare the performance of models trained on synthetic data generated using DP mechanisms versus those trained directly with DP-SGD to assess the viability of synthetic data approaches for federated learning