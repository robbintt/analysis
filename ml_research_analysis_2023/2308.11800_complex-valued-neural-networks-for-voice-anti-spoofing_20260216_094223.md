---
ver: rpa2
title: Complex-valued neural networks for voice anti-spoofing
arxiv_id: '2308.11800'
source_url: https://arxiv.org/abs/2308.11800
tags:
- audio
- phase
- complex-valued
- anti-spoofing
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of voice anti-spoofing and audio
  deepfake detection. The proposed method combines the benefits of magnitude spectrogram-based
  and raw audio processing approaches by using complex-valued neural networks to process
  complex-valued CQT spectrograms, retaining phase information and enabling explainable
  AI.
---

# Complex-valued neural networks for voice anti-spoofing

## Quick Facts
- arXiv ID: 2308.11800
- Source URL: https://arxiv.org/abs/2308.11800
- Authors:
- Reference count: 0
- EER of 12.38% on In-the-Wild test set

## Executive Summary
This paper introduces a complex-valued neural network approach for voice anti-spoofing and audio deepfake detection. The method processes complex-valued Constant Q Transform (CQT) spectrograms, preserving phase information that is typically discarded in magnitude-only approaches. The model achieves an Equal Error Rate (EER) of 12.38% on the challenging "In-the-Wild" anti-spoofing dataset, outperforming previous magnitude spectrogram-based and raw audio processing methods. The approach also enables explainable AI techniques like saliency maps for interpreting model decisions.

## Method Summary
The proposed method processes audio through a complex-valued CQT transform to create spectrograms that retain both magnitude and phase information. These complex-valued inputs are fed into a 4-layer convolutional neural network with complex-valued operations (convolution, ReLU, batch normalization). The network architecture includes linear layers with CReLU activation and complex time pooling before producing a 2-class output. The model is trained with Adam optimizer (learning rate 5e-3), batch size 32, for up to 25 epochs with early stopping, using data augmentation including adversarial examples, codec compression, and noise addition.

## Key Results
- Achieved EER of 12.38% on In-the-Wild test set, outperforming previous methods
- Ablation studies confirmed model learns to use phase information for spoof detection
- Enabled explainable AI through saliency maps on complex spectrograms
- Outperformed both magnitude spectrogram-based models and raw audio processing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complex-valued neural networks retain phase information lost in magnitude-only spectrograms, improving spoof detection accuracy.
- Mechanism: The CQT transform outputs complex numbers where phase encodes temporal fine structure. CVNNs process both real and imaginary components, allowing the network to learn phase-magnitude relationships that indicate spoofing artifacts.
- Core assumption: Phase information contains detectable features for spoofed audio that magnitude-only processing cannot capture.
- Evidence anchors:
  - [abstract] "This method retains phase information and allows for explainable AI methods"
  - [section 3.1] "we employ the constant Q transform (CQT) [7, 23] which processes the same number of cycles Q for each frequency and spaces the frequency bins k geometrically"
  - [corpus] Weak - corpus lacks direct evidence for phase-retention claims in anti-spoofing
- Break condition: If phase information proves irrelevant to spoof detection, or if the phase-magnitude relationship is too complex for CVNNs to learn effectively.

### Mechanism 2
- Claim: Complex-valued inputs enable explainable AI techniques like saliency maps that work with raw audio models cannot.
- Mechanism: Saliency maps require spatial dimensions. Complex spectrograms provide 2D frequency-time structure while preserving interpretability. The model's focus areas can be visualized on the spectrogram.
- Core assumption: XAI techniques require 2D spatial structure while raw audio lacks this property.
- Evidence anchors:
  - [abstract] "This method retains phase information and allows for explainable AI methods"
  - [section 3.3] "it enables the use of explainable AI techniques"
  - [corpus] Moderate - corpus shows some complex-valued XAI work but lacks direct anti-spoofing examples
- Break condition: If the complex representation doesn't provide meaningful spatial structure, or if XAI techniques fail to produce interpretable results.

### Mechanism 3
- Claim: The geometric frequency binning of CQT better matches human auditory perception than uniform STFT, improving feature learning.
- Mechanism: CQT uses logarithmic frequency spacing matching the ear's non-linear frequency response. This means more resolution at lower frequencies where humans are more sensitive, and less at higher frequencies.
- Core assumption: Neural networks learn more effectively from features aligned with human perceptual systems.
- Evidence anchors:
  - [section 3.1] "CQT employs the constant Q transform (CQT) [7, 23] which processes the same number of cycles Q for each frequency and spaces the frequency bins k geometrically"
  - [section 3.1] "the uniform time and frequency resolution for all frequency bins k does not align with the non-linearities of the human auditory system"
  - [corpus] Weak - corpus lacks direct evidence for CQT-human perception alignment claims
- Break condition: If the geometric binning provides no learning advantage over uniform binning for the spoof detection task.

## Foundational Learning

- Concept: Complex numbers and their representation (magnitude + phase)
  - Why needed here: The entire approach relies on processing complex-valued spectrograms where phase information is crucial
  - Quick check question: Given a complex number z = 3 + 4i, what are its magnitude and phase?

- Concept: Short-Time Fourier Transform and its properties
  - Why needed here: Understanding STFT is essential to grasp why CQT is an improvement and how frequency representations work
  - Quick check question: What information is lost when converting a complex STFT output to a magnitude-only spectrogram?

- Concept: Convolutional neural networks and their adaptation to complex inputs
  - Why needed here: The architecture uses complex-valued convolutions, requiring understanding of how standard CNN operations extend to complex numbers
  - Quick check question: How does a complex convolution differ mathematically from a real convolution?

## Architecture Onboarding

- Component map: Input (complex CQT spectrogram) → Complex Conv Block 1 (conv, ReLU, BN) → Complex Conv Block 2 → Complex Conv Block 3 → Complex Conv Block 4 → Linear layers with CReLU → Complex time pooling → Output (2-class complex vector) → Softmax on magnitude
- Critical path: Complex CQT transform → 4-layer complex conv stack → Linear projection → Time pooling → Classification
- Design tradeoffs: Using complex-valued operations increases parameter count and computation but retains phase information. Simpler than raw audio models while maintaining explainability.
- Failure signatures: If EER doesn't improve over magnitude-only models, phase information may be irrelevant. If training is unstable, complex operations may need normalization adjustments.
- First 3 experiments:
  1. Train with zero phase (magnitude only) to verify phase contribution
  2. Randomize phase to test if model learns phase-magnitude relationships
  3. Compare against raw audio model with same number of parameters to isolate effect of complex representation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed complex-valued approach outperform other magnitude-based approaches (e.g., log-mel spectrograms) in addition to raw audio methods?
- Basis in paper: [explicit] The paper mentions that the proposed approach outperforms both magnitude spectrogram-based models and raw-feature models on the "In-the-Wild" dataset.
- Why unresolved: The paper only provides comparisons with specific magnitude spectrogram-based models (e.g., CQT, Mel-spectrograms) and raw audio models, but does not include a direct comparison with other magnitude-based approaches.
- What evidence would resolve it: A comprehensive comparison of the proposed complex-valued approach with various magnitude-based approaches (e.g., log-mel spectrograms, MFCCs) on the same dataset.

### Open Question 2
- Question: How does the proposed complex-valued approach generalize to other audio spoofing detection tasks, such as video deepfake detection or speaker verification?
- Basis in paper: [inferred] The paper focuses on voice anti-spoofing and audio deepfake detection, but does not explore its applicability to other related tasks.
- Why unresolved: The paper only evaluates the proposed approach on the specific task of voice anti-spoofing and audio deepfake detection, without investigating its performance on other related tasks.
- What evidence would resolve it: Experiments demonstrating the effectiveness of the proposed complex-valued approach on other audio spoofing detection tasks, such as video deepfake detection or speaker verification.

### Open Question 3
- Question: How does the proposed complex-valued approach handle different types of audio spoofing attacks, such as voice conversion or speech synthesis?
- Basis in paper: [explicit] The paper mentions that the proposed approach outperforms previous methods on the "In-the-Wild" dataset, which includes various types of audio spoofing attacks.
- Why unresolved: The paper does not provide a detailed analysis of the proposed approach's performance on different types of audio spoofing attacks.
- What evidence would resolve it: A comprehensive evaluation of the proposed complex-valued approach's performance on different types of audio spoofing attacks, such as voice conversion, speech synthesis, or replay attacks.

## Limitations

- Exact implementation details of complex-valued operations (CReLU, complex BatchNorm, complex softmax) are not fully specified
- The claim that phase information is crucial lacks direct comparison against alternative representations
- EER of 12.38% still represents room for improvement in practical deployment scenarios

## Confidence

- High confidence: The basic methodology of using complex-valued CQT spectrograms and complex CNNs is sound and well-supported by signal processing theory
- Medium confidence: The superiority of this approach over magnitude-only methods is demonstrated but could benefit from more extensive ablation studies
- Medium confidence: The explainability benefits are theoretically valid but the practical utility for anti-spoofing interpretation needs more validation

## Next Checks

1. Conduct controlled ablation experiments comparing against STFT magnitude-only baselines and raw audio models with identical parameter counts
2. Perform robustness testing with adversarial examples specifically targeting phase information to verify if phase features are indeed the critical differentiator
3. Validate the explainability claims by having domain experts assess whether saliency maps on complex spectrograms provide actionable insights for spoof detection that magnitude-only maps do not