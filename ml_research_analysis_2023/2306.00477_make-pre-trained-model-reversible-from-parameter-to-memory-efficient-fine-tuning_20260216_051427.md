---
ver: rpa2
title: 'Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning'
arxiv_id: '2306.00477'
source_url: https://arxiv.org/abs/2306.00477
tags:
- reversible
- memory
- layer
- fine-tuning
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory inefficiency of parameter-efficient
  fine-tuning (PEFT) methods for large language models. PEFT methods still require
  caching most intermediate activations for gradient computation, similar to full
  fine-tuning.
---

# Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning

## Quick Facts
- arXiv ID: 2306.00477
- Source URL: https://arxiv.org/abs/2306.00477
- Reference count: 40
- Pre-trained models can be made reversible to reduce activation memory usage by up to 84% while maintaining comparable performance to full fine-tuning

## Executive Summary
This paper addresses the memory inefficiency of parameter-efficient fine-tuning (PEFT) methods for large language models. While PEFT reduces the number of trainable parameters, it still requires caching most intermediate activations for gradient computation, similar to full fine-tuning. The authors propose Memory-Efficient Fine-Tuning (MEFT), which modifies pre-trained models to be reversible using adapters. This allows intermediate activations to be recomputed during backpropagation instead of being cached, significantly reducing activation memory usage while maintaining performance comparable to full fine-tuning. Experiments on GLUE and question-answering tasks demonstrate MEFT's effectiveness in achieving strong performance with substantially reduced memory requirements.

## Method Summary
MEFT modifies pre-trained language models by inserting adapter modules that create reversible architectures. During the forward pass, the model computes intermediate representations that can be reconstructed during backpropagation, eliminating the need to cache activations. The method uses scaling factors (λ and β) near zero to preserve the starting point from the original pre-trained model, ensuring representation continuity. Three variants of MEFT are proposed with different configurations of these scaling factors and output switching. The adapters are initialized to produce near-zero outputs initially, maintaining the pre-trained model's behavior at the start of training. This reversible design enables the model to recompute intermediate activations during backpropagation instead of storing them, reducing activation memory from O(N) to O(1).

## Key Results
- MEFT reduces activation memory by up to 84% compared to full fine-tuning while maintaining comparable performance
- The method achieves GLUE scores comparable to full fine-tuning and other PEFT methods across multiple tasks
- MEFT is compatible with various model architectures including BERT, RoBERTa, BART, and OPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving the starting point from the pre-trained model is essential for successful parameter-efficient fine-tuning.
- Mechanism: When adding new parameters to a pre-trained model, initializing them so the modified model's output matches the original model's output at the beginning of training maintains representation continuity and prevents catastrophic forgetting.
- Core assumption: The pre-trained model's initial representations contain valuable generic knowledge that should not be disrupted at the start of training.
- Evidence anchors:
  - [abstract] "it's essential to preserve the PLM's starting point when initializing a PEFT method"
  - [section 2.2] "when modifying a PLM with PEFT, we hypothesize that one needs to preserve this starting point at the beginning of training for better performance"
  - [corpus] Weak evidence - no direct citations about starting point preservation in related work
- Break condition: If initialization causes the modified model to produce significantly different outputs than the original model, representation continuity breaks down and performance degrades.

### Mechanism 2
- Claim: Reversible architectures eliminate the need to cache intermediate activations during training.
- Mechanism: By structuring the model so that outputs can be used to reconstruct inputs during backpropagation, intermediate activations can be recomputed instead of stored, reducing activation memory from O(N) to O(1).
- Core assumption: The reconstruction process is numerically stable and the gradients computed through reconstruction match those from caching intermediate activations.
- Evidence anchors:
  - [abstract] "One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed"
  - [section 2.3] "Given a multilayer reversible network, intermediate activations for each layer during the forward pass are not necessary to be cached"
  - [corpus] No direct evidence - this appears to be a novel application of reversible networks to PLMs
- Break condition: If numerical instability in the reconstruction process causes gradients to diverge from those computed with cached activations, training becomes unstable.

### Mechanism 3
- Claim: Scaling factors near zero preserve the starting point while maintaining reversibility.
- Mechanism: Setting scaling factors λ and β close to zero ensures that the reversible transformations produce outputs close to the original pre-trained model's outputs, preserving the starting point while still enabling the reversible architecture.
- Core assumption: Small scaling factors can preserve the starting point without introducing significant numerical instability in the reversible transformations.
- Evidence anchors:
  - [section 3] "λ → 0 is required, so h1n approximates the original output from the pre-trained attention block"
  - [section 3] "λ → 0 is required, so h1n approximates the original output from the pre-trained attention block"
  - [corpus] Weak evidence - while related to reversible network scaling, the specific application to starting point preservation appears novel
- Break condition: If scaling factors are too small, numerical instability increases; if too large, the starting point is not preserved.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Understanding PEFT is fundamental to grasping why MEFT is needed - PEFT methods still require caching most intermediate activations, making them memory-inefficient.
  - Quick check question: What is the main memory inefficiency in PEFT methods compared to full fine-tuning?

- Concept: Reversible neural networks
  - Why needed here: MEFT modifies PLMs to be reversible, which is the key mechanism for reducing activation memory.
  - Quick check question: How does a reversible architecture reduce activation memory from O(N) to O(1)?

- Concept: Residual connections in transformers
  - Why needed here: Understanding residual connections is crucial for grasping why MEFT needs to set scaling factors close to zero and why numerical instability can occur.
  - Quick check question: Why do residual connections in reversible models deteriorate performance according to RevViT?

## Architecture Onboarding

- Component map:
  Pre-trained model -> Adapter modules -> Reversible layers -> Task-specific head

- Critical path:
  1. Initialize adapters with near-zero outputs to preserve starting point
  2. Set scaling factors λ and β appropriately for each MEFT variant
  3. Process through reversible layers with order switching where needed
  4. Reconstruct intermediate activations during backpropagation
  5. Compute gradients for adapter parameters

- Design tradeoffs:
  - Small scaling factors preserve starting point but increase numerical instability
  - Order switching preserves representation continuity but adds complexity
  - FP16 training is faster but less stable than FP32 for reversible models
  - Deeper models require freezing some layers to maintain stability

- Failure signatures:
  - Performance drops when scaling factors deviate from optimal values
  - Training instability manifests as exploding gradients or NaN values
  - Memory savings are less than expected if order switching is not implemented correctly
  - Score degradation when using reversible gradient vs vanilla gradient

- First 3 experiments:
  1. Verify starting point preservation by comparing outputs of original vs modified layers with initialization
  2. Test different scaling factor values to find optimal balance between starting point preservation and numerical stability
  3. Compare activation memory usage between MEFT with reversible gradient vs vanilla gradient

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MEFT's performance scale with model depth beyond 24 layers?
- Basis in paper: [explicit] The paper mentions that with an increasing number of layers, training instability becomes more severe due to error accumulation and the requirement to preserve the starting point.
- Why unresolved: The experiments only tested up to 24 layers on RoBERTa and 32 layers on OPT. The paper acknowledges that deeper models face challenges with MEFT due to numerical stability issues and the starting point hypothesis requirements.
- What evidence would resolve it: Experiments testing MEFT on models with 50+ layers, or theoretical analysis of error propagation in reversible architectures with multiple layers, would clarify the scalability limits.

### Open Question 2
- Question: Can MEFT be effectively applied to encoder-decoder models like T5 or GPT-3?
- Basis in paper: [inferred] The paper only tested MEFT on encoder-only (BERT, RoBERTa, BART) and decoder-only (OPT) models, but mentions plans to include LLaMA for instruction-finetuning in the future.
- Why unresolved: The paper explicitly states they didn't conduct experiments on encoder-decoder models due to computing resource constraints and selection criteria for strong baselines.
- What evidence would resolve it: Implementation and evaluation of MEFT on encoder-decoder architectures with comparable baselines would demonstrate its applicability to this important model class.

### Open Question 3
- Question: What is the optimal trade-off between memory savings and training time when using mixed precision with MEFT?
- Basis in paper: [explicit] The paper discusses that MEFT trained in FP16 requires about 1.2x more training time than FP32 due to forward pass requirements, and mentions the time-memory tradeoff.
- Why unresolved: While the paper provides some data on FP16 vs FP32 training times and memory usage, it doesn't comprehensively explore the optimal configuration for different hardware constraints and model sizes.
- What evidence would resolve it: Systematic experiments varying batch sizes, model scales, and hardware configurations to determine the optimal precision and layer freezing strategy for different use cases would provide practical guidance.

## Limitations

- The method's effectiveness on deeper models (beyond 24-32 layers) is not well-established due to numerical stability concerns
- MEFT has only been tested on encoder-only and decoder-only architectures, not encoder-decoder models
- The optimal configuration of scaling factors and layer freezing strategies requires careful tuning for different model sizes

## Confidence

- **High confidence**: The core claim that MEFT reduces activation memory by up to 84% compared to full fine-tuning, supported by empirical measurements in Table 1
- **Medium confidence**: The claim that MEFT maintains comparable performance to full fine-tuning across multiple benchmarks, though some performance degradation is observed in specific cases (Table 2)
- **Low confidence**: The assertion that preserving the starting point is essential for all PEFT methods, based on limited ablation studies without comparison to other initialization strategies

## Next Checks

1. **Numerical stability verification**: Conduct controlled experiments varying scaling factors λ and β across a wider range to identify the precise boundaries where training becomes unstable

2. **Ablation of initialization strategies**: Compare MEFT's near-zero initialization against alternative strategies (e.g., random initialization, pretrained initialization) to isolate the contribution of starting point preservation

3. **Memory-performance tradeoff analysis**: Systematically measure the relationship between activation memory reduction and performance degradation across different levels of reversibility (partial vs full reversible layers)