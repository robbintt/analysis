---
ver: rpa2
title: Towards Effective Paraphrasing for Information Disguise
arxiv_id: '2311.05018'
source_url: https://arxiv.org/abs/2311.05018
tags:
- https
- search
- paraphrasing
- disguise
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an unsupervised approach for paraphrasing\
  \ text to prevent the non-consensual use of authors\u2019 online posts, a problem\
  \ known as Information Disguise (ID). The proposed method iteratively perturbs sentences\
  \ using a combination of BERT-based masking and counter-fitting word vectors to\
  \ generate paraphrases that reduce the locatability of the source content on search\
  \ engines."
---

# Towards Effective Paraphrasing for Information Disguise

## Quick Facts
- arXiv ID: 2311.05018
- Source URL: https://arxiv.org/abs/2311.05018
- Reference count: 40
- Primary result: Multi-level phrase substitution successfully disguises sentences 82% of the time while maintaining semantic similarity ≥0.95

## Executive Summary
This paper introduces an unsupervised approach for paraphrasing text to prevent non-consensual use of authors' online posts through Information Disguise (ID). The method iteratively perturbs sentences using BERT-based masking and counter-fitting word vectors to generate paraphrases that reduce locatability on search engines. A novel phrase-importance ranking mechanism based on perplexity scores identifies critical attack locations, while multi-level phrase substitution via beam search explores diverse paraphrase options. The approach is evaluated on 2000 posts from the "r/AmItheAsshole" subreddit using Dense Passage Retriever as a proxy for search engines, demonstrating 82% success in disguising content while maintaining semantic fidelity.

## Method Summary
The unsupervised approach works by first building a constituency-based parse tree for each sentence, then ranking attackable phrases using perplexity scores (PLL) to identify the most critical locations for perturbation. Candidate paraphrases are generated through a combination of BERT-based masking suggestions and counter-fitting word vector synonyms. Multi-level phrase substitutions are explored via beam search with a heuristic that balances semantic similarity and rank improvement. The method targets sentences that rank highly when queried on Dense Passage Retriever, aiming to reduce their locatability while maintaining semantic similarity above 0.95 using Universal Sentence Encoder.

## Key Results
- Multi-level phrase substitution scheme disguises sentences 82% of the time while maintaining semantic similarity above 0.95
- The method outperforms single-level perturbation approaches in reducing document retrieval rates
- Perplexity-based phrase ranking successfully identifies critical attack locations for maximum disruption

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity-based ranking identifies phrases most critical to source retrieval.
- Mechanism: The method masks each parse tree node's phrase and measures the pseudo log-likelihood of the resulting sentence. Nodes with highest PLL scores are most disruptive to replace without breaking fluency.
- Core assumption: High perplexity corresponds to semantic uniqueness of the phrase in context.
- Evidence anchors: "novel method of phrase-importance rankings using perplexity scores"; "PLL(NT ) helps us capture the peculiarity of phrase N str T and hence, its contribution in making the source document d locatable"
- Break condition: If PLL scores are uniform across phrases, ranking loses discriminative power.

### Mechanism 2
- Claim: Beam search over multi-level phrase substitutions finds paraphrases that maximally reduce locatability.
- Mechanism: At each level, top-k candidate paraphrases (ranked by semantic similarity and retriever rank improvement) are expanded. This explores the space of paraphrases with multiple phrase changes while keeping the search tractable.
- Core assumption: Multi-level substitutions are more effective than single-level for evading retrievers.
- Evidence anchors: "multi-level phrase substitutions via beam search"; "multi-phrase substitution scheme succeeds in disguising sentences 82% of the time"
- Break condition: If semantic similarity drops too fast with level, beam search will prune effective paths.

### Mechanism 3
- Claim: Combining BERT-based masking and counter-fitting synonym replacement produces paraphrases that are both fluent and semantically faithful.
- Mechanism: BERT suggestions preserve grammar but may lack semantic fidelity; counter-fitting replacements maintain meaning but may reduce fluency. The union of both candidate sets covers both needs.
- Core assumption: Semantic similarity can be maintained while reducing locatability by swapping words with close synonyms or BERT-predicted alternatives.
- Evidence anchors: "attacking using counter-fitting vectors replaces words with close synonyms and preserve meaning"; "BERT substitutions are effective at reducing HR@K, and preserving the grammatical structure"
- Break condition: If counter-fitting embeddings are not semantically aligned, synonym swaps will fail.

## Foundational Learning

- Concept: Constituency-based parse tree node ranking
  - Why needed here: To identify meaningful sub-phrases for targeted perturbation.
  - Quick check question: What does a high PLL score indicate about a parse tree node's phrase?

- Concept: Beam search pruning criteria
  - Why needed here: To limit search space while still exploring diverse paraphrase options.
  - Quick check question: How does the heuristic f(s) balance semantic similarity and rank improvement?

- Concept: Dense Passage Retriever (DPR) as proxy
  - Why needed here: To evaluate paraphrase effectiveness without real search engine API limits.
  - Quick check question: Why is DPR a reasonable proxy for Google Search in this context?

## Architecture Onboarding

- Component map: Input sentence → Parse tree generation → PLL ranking → Candidate generation (BERT + counter-fitting) → Beam search expansion → DPR query → Hit-rate evaluation
- Critical path: Sentence → Parse tree → PLL ranking → Candidate generation → DPR evaluation
- Design tradeoffs: Beam width vs. exploration completeness; semantic similarity threshold vs. disguise effectiveness; DPR proxy vs. real search engine evaluation
- Failure signatures: Uniform PLL scores; rapid semantic similarity drop; DPR not reflecting real search engine behavior
- First 3 experiments:
  1. Vary P (number of parse tree nodes attacked) and measure HR@K
  2. Test different beam widths (k) and max levels
  3. Compare BERT-only vs. counter-fitting-only vs. combined attack strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed paraphrasing method scale to actual search engines given their API limitations?
- Basis in paper: The paper acknowledges that the approach is unlikely to work on actual search engines due to API limits and mentions that achieving comparable results while not exceeding API limits is an interesting problem for future work.
- Why unresolved: The current approach requires a large number of requests to the retriever to disguise a sentence, which is not feasible with actual search engines' API limitations.
- What evidence would resolve it: Testing the method on actual search engines with API limits in place and comparing the success rate to the results obtained with DPR.

### Open Question 2
- Question: How does the grammatical quality of the paraphrased sentences affect the success rate of the information disguise?
- Basis in paper: The paper mentions that the current approach does not take the grammatical quality of the paraphrased sentences into account, implying that this could be a potential area of improvement.
- Why unresolved: The impact of grammatical quality on the success rate of information disguise has not been explored in the paper.
- What evidence would resolve it: Conducting experiments to measure the success rate of information disguise with and without maintaining grammatical quality in the paraphrased sentences.

### Open Question 3
- Question: How does the proposed method perform on different types of sensitive content beyond mental health and politics?
- Basis in paper: The paper mentions that the method was tested on the subreddit "r/AmItheAsshole" and does not provide evidence of testing on other types of sensitive content.
- Why unresolved: The effectiveness of the method on various types of sensitive content is unknown.
- What evidence would resolve it: Applying the method to different types of sensitive content and evaluating its success rate in disguising the information across these domains.

## Limitations
- Evaluation relies entirely on DPR as proxy for real search engines, which may not capture commercial search algorithm complexity
- Dataset limited to one subreddit with uniform writing styles, raising questions about performance across diverse domains
- Method requires large number of retriever queries, making it impractical for actual search engine API limitations

## Confidence

- **High Confidence**: The multi-level beam search framework and its implementation details are well-specified and reproducible.
- **Medium Confidence**: The combination of BERT-based masking and counter-fitting vectors for perturbation generation shows theoretical soundness, though empirical validation across diverse contexts is limited.
- **Low Confidence**: The effectiveness of perplexity-based phrase ranking for identifying critical attack locations has minimal empirical validation in the paper.

## Next Checks
1. Test the paraphrasing approach against actual Google Search and Bing API endpoints to verify DPR proxy accuracy
2. Evaluate performance on diverse text sources beyond Reddit (news articles, academic papers, social media posts) to assess generalizability
3. Conduct human evaluation studies to verify that paraphrased content remains semantically equivalent and contextually appropriate for the intended audience