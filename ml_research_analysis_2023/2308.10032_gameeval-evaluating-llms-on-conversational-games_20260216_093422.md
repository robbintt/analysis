---
ver: rpa2
title: 'GameEval: Evaluating LLMs on Conversational Games'
arxiv_id: '2308.10032'
source_url: https://arxiv.org/abs/2308.10032
tags:
- game
- word
- llms
- chatgpt
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GameEval introduces a novel evaluation paradigm for LLMs by engaging
  them in goal-driven conversational games, addressing limitations of reference-based
  and preference-based methods. The approach assigns distinct roles to LLMs with specific
  goals achieved through conversations involving discussion, question answering, and
  voting.
---

# GameEval: Evaluating LLMs on Conversational Games

## Quick Facts
- arXiv ID: 2308.10032
- Source URL: https://arxiv.org/abs/2308.10032
- Reference count: 11
- Models tested: GPT-4 (97.69% correct ratio in Ask-Guess), ChatGPT, TD003

## Executive Summary
GameEval introduces a novel evaluation paradigm for LLMs by engaging them in goal-driven conversational games, addressing limitations of reference-based and preference-based methods. The approach assigns distinct roles to LLMs with specific goals achieved through conversations involving discussion, question answering, and voting. Three games were designed with cooperative or adversarial objectives: Ask-Guess (questioner-answerer word guessing), SpyFall (identifying a spy among villagers), and TofuKingdom (role-playing reasoning game). Experimental results show GameEval effectively differentiates LLM capabilities, with GPT-4 achieving 97.69% correct ratio and 1.57 average rounds in Ask-Guess, and 0.70 winning rate against TD003 in SpyFall. The method eliminates human intervention and evaluator model bias while providing comprehensive assessment of integrated capabilities including cooperation, reasoning, and long-term planning.

## Method Summary
GameEval evaluates LLMs through goal-driven conversational games where models are assigned distinct roles with specific objectives. The system implements three games: Ask-Guess (word guessing through Q&A), SpyFall (spy identification with hidden information), and TofuKingdom (role-playing with cooperative/competitive objectives). Models engage in multi-turn conversations with private histories, and performance is measured by game outcomes rather than reference answers or human preferences. The evaluation combines chain-of-thought reasoning for complex decision-making and measures integrated capabilities including cooperation, reasoning, planning, and instruction-following.

## Key Results
- GPT-4 achieved 97.69% correct ratio and 1.57 average rounds in Ask-Guess game
- GPT-4 won SpyFall 0.70 of the time against TD003
- GameEval effectively differentiated LLM capabilities across all three games while eliminating human intervention and evaluator bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GameEval creates a closed-loop evaluation system where model performance is measured directly by goal achievement rather than comparison to human-labeled references or preferences.
- Mechanism: The game assigns specific roles and goals to models, with performance metrics based on whether the assigned role achieves its objective (e.g., correctly guessing a word, winning as spy, or identifying the princess). This creates an intrinsic success criterion without external evaluators.
- Core assumption: The game rules and goals are objective enough that success can be measured without human judgment or reference answers.
- Evidence anchors:
  - [abstract] "GameEval treats LLMs as game players and assigns them distinct roles with specific goals achieved by launching conversations"
  - [section 3] "The outcomes of the games effectively reflect whether the model's outputs contribute to achieving the assigned goal, thus serving as a direct quantification of the model's capabilities"
- Break condition: If game rules become ambiguous or multiple equally valid paths to success exist, the evaluation loses objectivity and requires external judgment.

### Mechanism 2
- Claim: GameEval forces integration of multiple capabilities simultaneously rather than testing isolated skills, providing a more comprehensive assessment of real-world performance.
- Mechanism: Unlike single-task benchmarks, games require models to combine cooperation, reasoning, planning, and instruction-following in each round of conversation to achieve long-term goals, revealing integrated capability gaps.
- Core assumption: Real-world tasks require simultaneous application of multiple capabilities, and traditional benchmarks fail to capture this integrated performance.
- Evidence anchors:
  - [abstract] "Unlike existing evaluation methods that average scores from multiple individual tasks, GameEval requires models to apply these capabilities simultaneously in each conversation round"
  - [section 3.4] "GameEval is distinct from other evaluation methods, as it requires not only the model's common capabilities like instruct-following but also the model's higher-level skills, including cooperative&adversarial strategies, and even deceptive strategies and long-term planning"
- Break condition: If games become too complex, they may test game-playing ability rather than general capability integration.

### Mechanism 3
- Claim: GameEval eliminates human intervention and evaluator model bias by using game outcomes as objective success metrics.
- Mechanism: By measuring success through game rules rather than human preferences or other model judgments, GameEval removes the subjectivity and bias inherent in preference-based evaluation methods.
- Core assumption: Game outcomes can serve as objective performance metrics without requiring human or model evaluators to make preference judgments.
- Evidence anchors:
  - [abstract] "GameEval is neither based on reference nor preference, eliminating the need for human intervention and reducing test bias caused by evaluator models"
  - [section 1] "preference-based methods employ humans or models as evaluators...they either require substantial human resources or introduce preference bias from the evaluator models"
- Break condition: If game outcomes become too dependent on specific game knowledge rather than general capability, the evaluation loses its objectivity and generalizability.

## Foundational Learning

- Concept: Multi-turn dialogue management
  - Why needed here: Games require maintaining conversation history and context across multiple turns while playing different roles, which is essential for consistent gameplay and reasoning.
  - Quick check question: How does the system track private versus public conversation history for each player role?

- Concept: Chain-of-thought reasoning
  - Why needed here: Complex games like SpyFall and TofuKingdom require models to generate reasoning steps before producing final responses, enabling more sophisticated decision-making.
  - Quick check question: What is the JSON structure used to capture the model's thought process before generating game responses?

- Concept: Role-based prompt engineering
  - Why needed here: Different game roles require distinct behaviors and strategies, necessitating careful prompt design to guide models into appropriate role-playing.
  - Quick check question: How do the system prompts differ between the answerer role in Ask-Guess and the spy role in SpyFall?

## Architecture Onboarding

- Component map: Game Host (rule enforcement) → Role Assignment Manager (role distribution) → Conversation Engine (turn-based dialogue) → Evaluation Metric Calculator (performance measurement) → Result Aggregator (final scoring)
- Critical path: Role Assignment → Conversation Turn → Response Generation → Evaluation → Score Update → Game End
- Design tradeoffs: Single-turn vs multi-turn dialogue support (ChatGPT/GPT-4 vs TD003), public vs private history management, synchronous vs asynchronous gameplay
- Failure signatures: Inconsistent role behavior across turns, incorrect rule enforcement, evaluation metric calculation errors, conversation history corruption
- First 3 experiments:
  1. Run Ask-Guess with TD003 as both questioner and answerer to verify basic game mechanics and conversation flow
  2. Test SpyFall with ChatGPT and GPT-4 in all role combinations to validate role-based prompt engineering and CoT integration
  3. Execute TofuKingdom with all three models in different winning camps to verify camp-based scoring and private history management

## Open Questions the Paper Calls Out

- **Question**: How does GameEval's evaluation accuracy compare to human evaluation when assessing real-world LLM capabilities?
  - Basis in paper: [inferred] The paper claims GameEval eliminates human intervention while providing comprehensive assessment, but doesn't validate this claim against human evaluation
  - Why unresolved: The paper only compares different LLMs against each other, not validating GameEval's effectiveness against human evaluators
  - What evidence would resolve it: A study comparing GameEval rankings with human preference-based evaluations on the same set of LLMs

- **Question**: What is the minimum model size required for LLMs to perform competitively in GameEval?
  - Basis in paper: [explicit] The paper notes that smaller models like LLaMA2-chat and Vicuna struggled to maintain roles and follow instructions
  - Why unresolved: The paper doesn't systematically test different model sizes to determine the threshold for effective participation
  - What evidence would resolve it: Experiments testing a range of model sizes (e.g., 7B, 13B, 33B, 70B parameters) across all three games

- **Question**: How does the performance gap between models vary across different game types and objectives?
  - Basis in paper: [explicit] The paper shows different discrimination levels across Ask-Guess (high), SpyFall (medium), and TofuKingdom (low)
  - Why unresolved: The paper doesn't analyze which specific game mechanics or objectives contribute to higher or lower discrimination
  - What evidence would resolve it: Detailed analysis correlating discrimination levels with game characteristics like number of roles, cooperation vs. competition, and complexity of reasoning required

## Limitations

- The private history mechanism lacks sufficient technical detail for proper implementation verification
- Sample sizes for some experiments (particularly SpyFall with only 11 word pairs) may limit statistical significance
- The complexity of multi-turn conversational games may introduce implicit biases based on game-specific knowledge rather than general LLM capability

## Confidence

- **High confidence**: The fundamental mechanism of using goal-driven games for LLM evaluation is technically sound and addresses genuine limitations in existing reference-based and preference-based methods. The basic game designs (Ask-Guess, SpyFall, TofuKingdom) are well-specified with clear win conditions.
- **Medium confidence**: The elimination of human intervention and evaluator bias claims, while conceptually valid, require more empirical validation across diverse game scenarios and player combinations to confirm the objectivity of outcomes.
- **Low confidence**: The scalability and generalizability of GameEval to more complex games or real-world scenarios remains unproven, as does the robustness of the evaluation when games involve ambiguous situations or multiple valid strategies.

## Next Checks

1. **Implement and test the private history mechanism** by running controlled experiments where one player should have access to information the other cannot see, verifying that information leakage does not occur and game outcomes remain consistent with expected results.

2. **Validate evaluation metric objectivity** by conducting ablation studies where the same game is played with different winning conditions or rule interpretations, measuring how sensitive the evaluation results are to rule variations.

3. **Cross-model comparison robustness test** by running the same game scenarios with additional LLM models (beyond the three tested) and analyzing whether the performance ranking remains consistent across different game types and role assignments.