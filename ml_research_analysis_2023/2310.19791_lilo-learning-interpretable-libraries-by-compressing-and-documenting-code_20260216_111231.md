---
ver: rpa2
title: 'LILO: Learning Interpretable Libraries by Compressing and Documenting Code'
arxiv_id: '2310.19791'
source_url: https://arxiv.org/abs/2310.19791
tags:
- clevr
- logo
- regex
- search
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LILO combines LLM-guided search with Stitch compression and AutoDoc
  to learn interpretable libraries for program synthesis. On three domains, LILO solves
  more complex tasks and learns richer abstractions than existing methods.
---

# LILO: Learning Interpretable Libraries by Compressing and Documenting Code

## Quick Facts
- arXiv ID: 2310.19791
- Source URL: https://arxiv.org/abs/2310.19791
- Reference count: 40
- Primary result: LILO solves more complex tasks and learns richer abstractions than existing methods across three domains

## Executive Summary
LILO introduces a novel approach to library learning for program synthesis that combines LLM-guided search with Stitch compression and AutoDoc. The method learns interpretable abstractions from solved tasks and uses them to tackle increasingly complex problems. By generating human-readable names and docstrings for abstractions, LILO enables LLMs to effectively leverage learned libraries. Experiments on REGEX, CLEVR, and LOGO domains show significant improvements over baselines, with the ability to generalize learned abstractions even without natural language guidance.

## Method Summary
LILO alternates between three phases: LLM-guided search to find programs for tasks, Stitch compression to extract reusable abstractions from solved programs, and AutoDoc to generate interpretable names and docstrings for abstractions. The LLM search uses few-shot prompting with a base DSL and learned library, while Stitch efficiently discovers common program fragments through branch-and-bound search. AutoDoc analyzes contextual usage examples to produce meaningful names that help the LLM deploy abstractions correctly. This iterative process builds increasingly sophisticated libraries that enable solving progressively harder tasks.

## Key Results
- +32.20% improvement on REGEX tasks compared to DreamCoder
- 1000-10000x faster and 100x more memory efficient than DreamCoder's compression
- Generalizes well even without language, matching or exceeding DreamCoder performance

## Why This Works (Mechanism)

### Mechanism 1
LLM-guided search reduces search space size compared to enumerative methods by using learned priors over code and language to propose likely programs, avoiding exhaustive enumeration of unlikely candidates.

### Mechanism 2
AutoDoc improves LLM usage of libraries by generating readable names and docstrings that help the LLM understand the purpose and usage of abstractions, enabling effective contextual deployment.

### Mechanism 3
Compression via Stitch enables efficient discovery of reusable abstractions by identifying lambda abstractions that minimize description length while maximizing rewriting opportunities across multiple programs.

## Foundational Learning

- Concept: Lambda calculus and functional programming
  - Why needed here: LILO operates on lambda calculus programs and builds abstractions using lambda expressions
  - Quick check question: What is the difference between a lambda abstraction and a lambda application?

- Concept: Probabilistic context-free grammars (PCFGs)
  - Why needed here: PCFGs define the prior distribution over programs in the base DSL
  - Quick check question: How does a PCFG assign probabilities to different program structures?

- Concept: Description length and compression
  - Why needed here: The compression objective minimizes the sum of abstraction description length and rewritten program lengths
  - Quick check question: Why does compression favor abstractions that appear in multiple programs?

## Architecture Onboarding

- Component map: LLM search -> Stitch compression -> AutoDoc -> LLM search (with library)
- Critical path: LLM search → Stitch compression → AutoDoc → LLM search (with library)
- Design tradeoffs: LLM search vs. enumerative search (speed vs. exhaustiveness); compression depth (more abstractions vs. potential obfuscation); AutoDoc quality (better names vs. potential semantic errors)
- Failure signatures: LLM search fails (no solutions found, many invalid programs); Stitch compression fails (library doesn't grow, no compression achieved); AutoDoc fails (unclear or incorrect function names)
- First 3 experiments:
  1. Run LLM search on a simple REGEX task without library to verify basic functionality
  2. Apply Stitch compression to a small set of solved programs to verify abstraction extraction
  3. Run AutoDoc on an extracted abstraction to verify name generation

## Open Questions the Paper Calls Out

### Open Question 1
How does LILO's performance scale with increasing numbers of training tasks in the lifelong learning setting? The paper mentions LILO's potential in lifelong learning but does not provide experiments varying the number of training tasks.

### Open Question 2
Can LILO's AutoDoc procedure be further improved to reduce semantic errors in generated function names and descriptions? The paper acknowledges that AutoDoc sometimes produces unclear or incorrect documentation.

### Open Question 3
How does LILO's library learning approach generalize to other programming languages beyond the functional languages used in the experiments? Extending LILO to imperative languages like Python is mentioned as a future research direction.

## Limitations
- Comparison against DreamCoder uses different base DSLs across domains
- Lack of ablations testing Stitch compression and AutoDoc separately
- Evaluation only considers synthetic task descriptions rather than real-world user queries

## Confidence
- High Confidence: Computational efficiency improvements (1000-10000x faster, 100x more memory efficient)
- Medium Confidence: AutoDoc improves LLM usage of libraries (+15.94 REGEX performance gain)
- Medium Confidence: Generalization results showing performance without language is promising

## Next Checks
1. Run LILO without AutoDoc and without Stitch compression separately to quantify each component's contribution to the +32.20 REGEX performance improvement
2. Implement DreamCoder on the same DSLs used by LILO to enable fair head-to-head performance comparison
3. Evaluate LILO on a small set of actual user-generated task descriptions from Stack Overflow or similar sources to assess robustness beyond synthetic prompts