---
ver: rpa2
title: Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance
  in the Presence of Outliers
arxiv_id: '2305.18974'
source_url: https://arxiv.org/abs/2305.18974
tags:
- huber
- error
- loss
- generalisation
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides exact asymptotic analysis of robust empirical\
  \ risk minimisation in high dimensions, studying \u21132, \u21131, and Huber losses\
  \ in the presence of outliers. The authors consider a linear regression setting\
  \ with a fixed ratio of data points to dimensions (\u03B1 = n/d) and analyze two\
  \ performance metrics: generalisation error and estimation error."
---

# Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers

## Quick Facts
- arXiv ID: 2305.18974
- Source URL: https://arxiv.org/abs/2305.18974
- Reference count: 0
- Primary result: Exact asymptotic analysis of ℓ2, ℓ1, and Huber loss performance in high-dimensional robust regression with outliers

## Executive Summary
This paper provides a comprehensive asymptotic analysis of robust empirical risk minimization (ERM) in high dimensions, studying how ℓ2, ℓ1, and Huber losses perform in the presence of outliers. The authors analyze a linear regression setting where both data dimension and sample size grow proportionally, deriving exact expressions for generalization and estimation errors. They show that optimally-regularized ERM estimators are not always consistent with Bayes-optimal performance, with the consistency depending critically on the loss function and outlier parameters. The key finding is that while ℓ2 and optimally-scaled Huber losses remain consistent, ℓ1 and Huber losses with fixed scale parameters can fail to achieve Bayes-optimal rates due to a mismatch between the ERM estimator norm and the Bayes-optimal norm.

## Method Summary
The paper analyzes robust linear regression with a mixture of clean data and outliers, where both dimension d and sample size n grow with fixed ratio α = n/d. The authors derive self-consistent equations for order parameters that characterize ERM performance, using these to compute asymptotic generalization and estimation errors for ℓ2, ℓ1, and Huber losses with ℓ2 regularization. For each loss function, they find the optimal regularization parameter through fixed-point iteration, then compare the resulting ERM performance against Bayes-optimal bounds computed via the GAMP algorithm. The analysis focuses on a double Gaussian noise model where outliers have higher variance and different scaling behavior.

## Key Results
- Optimally-regularized ERM with ℓ1 loss or fixed-scale Huber loss can fail to achieve Bayes-optimal generalization error
- The ℓ2 loss and optimally-scaled Huber loss remain consistent across all parameter regimes studied
- Cross-validation on clean data can restore consistency for generalization error but not for estimation error
- There exists a parameter region where Huber loss reduces to ℓ2 loss as the scale parameter diverges
- The inconsistency stems from a norm mismatch between ERM and Bayes-optimal estimators that cannot be corrected by positive regularization alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimally-regularised ERM estimators fail consistency when the ratio of outlier variance to inlier variance exceeds a threshold determined by the outlier percentage and the β parameter.
- Mechanism: In high dimensions, the ERM estimator's norm fails to match the Bayes-optimal norm due to the mismatch between the loss function's behavior on outliers versus inliers. For ℓ1 and Huber losses, this mismatch cannot be corrected by positive ℓ2 regularization alone, causing the estimator to remain inconsistent with the Bayes-optimal rate.
- Core assumption: The data model assumes a fixed ratio α = n/d and a mixture of clean and outlier samples with Gaussian noise.
- Evidence anchors:
  - [abstract] "Optimally-regularised ERM estimators are not always consistent, with consistency depending on the loss function and outlier parameters."
  - [section] "Due to a norm calibration mismatch, the consistency of the estimator requires an oracle estimate of the optimal norm, or the presence of a cross-validation set not corrupted by the outliers."
  - [corpus] Weak evidence - no direct mention of consistency thresholds in related papers.
- Break condition: If the outlier percentage ϵ is very small or very large, the mismatch may disappear, restoring consistency without negative regularization.

### Mechanism 2
- Claim: Cross-validation on a clean dataset restores consistency for the generalization error but not necessarily for the estimation error.
- Mechanism: Cross-validation can correct the norm mismatch by selecting the regularization parameter that minimizes the generalization error on clean data. However, for the estimation error (measuring recovery of the original, unpolluted function), this calibration still fails unless the clean data is available or the optimal norm is known.
- Core assumption: Clean data or oracle knowledge of the Bayes-optimal norm is available for cross-validation.
- Evidence anchors:
  - [abstract] "consistency for the generalization error, and optimal rates, can be recovered by cross-validating on a test set to find the optimal norm."
  - [section] "Due to homogeneity properties of the three losses considered, the rescaling can also be performed before training on the labels."
  - [corpus] Weak evidence - no direct mention of cross-validation efficacy in related papers.
- Break condition: If no clean data or oracle knowledge is available, cross-validation cannot correct the estimation error.

### Mechanism 3
- Claim: The Huber loss reduces to the ℓ2 loss in a region of the parameter space where the scale parameter diverges.
- Mechanism: For small outlier variance relative to inlier variance, the optimal scale parameter for the Huber loss grows without bound, causing the Huber loss to behave identically to the ℓ2 loss. This creates a phase transition where the two losses perform equivalently.
- Core assumption: The Huber loss is being optimally tuned for the given outlier parameters.
- Evidence anchors:
  - [abstract] "There is an extended region of parameters where the performance of the Huber loss is the same as the simpler ℓ2 function."
  - [section] "We observe an interesting behaviour in comparing the Huber loss and the ℓ2 loss. Observing Figure 2, right panel, we observe that as ∆ OUT decreases the scale parameter value aopt of the Huber loss diverges to infinity at a finite value of ∆ OUT."
  - [corpus] Weak evidence - no direct mention of Huber-ℓ2 equivalence in related papers.
- Break condition: If the outlier variance is large enough, the Huber loss will no longer reduce to ℓ2 and will outperform it.

## Foundational Learning

- Concept: High-dimensional asymptotics and the behavior of empirical risk minimization in the proportional asymptotics regime (n, d → ∞ with fixed ratio α = n/d).
  - Why needed here: The paper's main results rely on exact asymptotic characterizations of ERM performance, which only hold in the high-dimensional limit with fixed sample complexity ratio.
  - Quick check question: What is the key difference between the classical low-dimensional setting and the high-dimensional proportional asymptotics regime studied in this paper?

- Concept: The role of loss functions in robust regression and their behavior under outlier contamination.
  - Why needed here: The paper compares ℓ2, ℓ1, and Huber losses, showing how their robustness to outliers affects consistency and performance.
  - Quick check question: How does the ℓ1 loss differ from the ℓ2 loss in its treatment of outliers, and why does this matter for consistency?

- Concept: The concept of Bayes-optimal estimation and its relationship to empirical risk minimization.
  - Why needed here: The paper uses the Bayes-optimal estimator as a benchmark to evaluate the performance of ERM estimators and determine consistency.
  - Quick check question: What is the key difference between the Bayes-optimal estimator and the ERM estimator in terms of their asymptotic behavior?

## Architecture Onboarding

- Component map: Data generation -> ERM solvers (ℓ2, ℓ1, Huber) -> Self-consistent equations -> Asymptotic error computation -> Comparison with BO performance
- Critical path: The core of the analysis is the derivation of self-consistent equations for the order parameters (m, q, Σ, etc.) that characterize the performance of ERM estimators. These equations are then solved numerically to obtain the asymptotic errors and compare them with the Bayes-optimal bounds.
- Design tradeoffs: The choice of loss function (ℓ2, ℓ1, or Huber) involves a tradeoff between robustness to outliers and consistency. The ℓ2 loss is consistent but sensitive to outliers, while the ℓ1 and Huber losses are more robust but may not be consistent without proper calibration.
- Failure signatures: If the ERM estimator fails to achieve the Bayes-optimal rate, it may be due to a norm mismatch that cannot be corrected by positive regularization alone. Additionally, if the estimation error does not converge to zero, it may indicate that the estimator is biased even with optimal tuning.
- First 3 experiments:
  1. Reproduce the self-consistent equations for a simple case (e.g., Ridge regression with ℓ2 loss) and verify the asymptotic behavior of the order parameters.
  2. Implement the ERM procedures for the three loss functions and compare their performance on synthetic data with varying outlier parameters.
  3. Explore the phase transition in the Huber loss by varying the outlier variance and observing when the optimal scale parameter diverges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can negative regularization be safely used to achieve consistency for Huber and ℓ1 losses?
- Basis in paper: [explicit] The paper notes that consistency could potentially be achieved by allowing negative regularization, but states that for ℓ1 and Huber losses this leads to non-convex and unbounded-from-below risk functions.
- Why unresolved: The paper only briefly mentions this possibility and does not explore the conditions under which negative regularization might work or how to safely implement it.
- What evidence would resolve it: Theoretical analysis showing the range of negative regularization values that maintain convexity and boundedness, along with experimental validation showing improved consistency in estimation error.

### Open Question 2
- Question: How do the results generalize to other noise models beyond the double Gaussian model considered?
- Basis in paper: [explicit] The paper focuses on a specific probabilistic model of outliers with tunable corruption parameters, but does not explore how results might change for different noise distributions.
- Why unresolved: The analysis relies heavily on the specific properties of the double Gaussian model, and it's unclear how robust the findings are to changes in the noise distribution.
- What evidence would resolve it: Extension of the asymptotic analysis to other noise models (e.g., Laplace, t-distribution) and comparison of consistency results across different models.

### Open Question 3
- Question: What is the impact of over-parameterization on the consistency and performance of robust ERM methods?
- Basis in paper: [inferred] The paper considers the high-dimensional limit where d and n grow proportionally, but does not explore the over-parameterized regime where d > n.
- Why unresolved: The asymptotic analysis assumes a fixed ratio of n/d, and the behavior in the over-parameterized regime may differ significantly due to phenomena like benign overfitting.
- What evidence would resolve it: Asymptotic analysis of robust ERM in the over-parameterized regime, including characterization of estimation and generalization errors, and comparison with under-parameterized results.

### Open Question 4
- Question: How does the presence of outliers affect the implicit regularization of gradient-based optimization methods?
- Basis in paper: [inferred] The paper focuses on explicit regularization through ℓ2 penalties, but does not consider the implicit regularization effects of optimization algorithms like gradient descent.
- Why unresolved: The analysis assumes exact minimization of the regularized loss, which may not reflect the behavior of iterative optimization methods that converge to solutions with different norms.
- What evidence would resolve it: Empirical and theoretical study of how gradient-based optimization algorithms implicitly regularize robust regression problems in the presence of outliers, and how this affects consistency and generalization.

## Limitations
- The asymptotic analysis assumes proportional growth of n and d, which may not capture finite-sample behavior accurately
- Results are derived for a specific Gaussian mixture outlier model that may not reflect real-world distributions
- The paper assumes knowledge of outlier parameters for optimal tuning, which may not be available in practice
- Analysis focuses on linear models and does not extend to nonlinear or more complex model classes

## Confidence

**High Confidence:**
- The mathematical derivation of self-consistent equations for order parameters
- The asymptotic characterization of Bayes-optimal performance
- The observation that ℓ2 and optimally-tuned Huber losses remain consistent

**Medium Confidence:**
- The numerical validation showing the mismatch between ERM and Bayes-optimal norms
- The claim that cross-validation on clean data restores consistency for generalization error
- The phase transition behavior where Huber reduces to ℓ2 for certain parameters

**Low Confidence:**
- The exact boundaries of the parameter regimes where different losses fail
- The practical implications of the estimation error inconsistency in finite samples
- The generalizability of results to non-Gaussian noise models

## Next Checks

1. **Finite-sample validation**: Test the asymptotic predictions against simulations at realistic sample sizes (d = 100-1000) to quantify the rate of convergence and identify when the asymptotic regime is reached.

2. **Robustness to distributional assumptions**: Repeat key experiments with heavy-tailed noise distributions and non-Gaussian outliers to assess the sensitivity of the theoretical predictions to the Gaussian mixture model assumption.

3. **Practical tuning strategy evaluation**: Implement and test the proposed cross-validation approach on real-world datasets with synthetic outliers to verify that the theoretical insights translate into practical performance improvements.