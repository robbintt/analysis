---
ver: rpa2
title: Evaluation of really good grammatical error correction
arxiv_id: '2308.08982'
source_url: https://arxiv.org/abs/2308.08982
tags:
- human
- evaluation
- systems
- system
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares five grammatical error correction systems for
  Swedish, including rule-based, MT-based, and LLM-based approaches, against human
  corrections. Using both automatic metrics (GLEU, Scribendi) and human evaluations
  across grammaticality, fluency, and meaning preservation, the study finds that GPT-3
  significantly outperforms previous systems, approaching human-level performance
  in grammaticality and fluency.
---

# Evaluation of really good grammatical error correction

## Quick Facts
- arXiv ID: 2308.08982
- Source URL: https://arxiv.org/abs/2308.08982
- Reference count: 14
- GPT-3 significantly outperforms previous systems, approaching human-level performance in grammaticality and fluency

## Executive Summary
This paper evaluates five grammatical error correction systems for Swedish, including rule-based, MT-based, and LLM-based approaches, against human corrections. Using both automatic metrics (GLEU, Scribendi) and human evaluations across grammaticality, fluency, and meaning preservation, the study finds that GPT-3 significantly outperforms previous systems, approaching human-level performance in grammaticality and fluency. However, all automatic systems fall short in meaning preservation compared to human corrections. The authors demonstrate that current evaluation metrics have biases, with reference-free metrics favoring neural systems even over human corrections, while reference-based metrics struggle to differentiate high-performing systems. They recommend using human post-editing analysis and post-edit distance as fairer evaluation methods for advanced GEC systems.

## Method Summary
The study compares five GEC systems (Granska rule-based, Nyberg MT and LM, MT, GPT-3) using the SweLL corpus of 502 Swedish learner texts with human corrections. Evaluation includes automatic metrics (GLEU, Scribendi), human evaluation across grammaticality, fluency, and meaning preservation dimensions, and post-editing analysis using normalized Levenshtein distance. The human evaluation employed an annotation tool where annotators rated system outputs on three 4-level Likert scales for each dimension.

## Key Results
- GPT-3 significantly outperforms rule-based and MT systems in all evaluation dimensions
- GPT-3 approaches human-level performance in grammaticality (2.6 vs 2.9) and fluency (2.7 vs 3.1)
- All automatic systems fall short in meaning preservation compared to human corrections
- Reference-free metrics favor neural systems even over human corrections
- Reference-based metrics suffer from ceiling effects with high-performing systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human post-editing analysis provides a fairer evaluation of advanced GEC systems than traditional metrics
- Mechanism: By measuring the minimal edits required to transform GEC output to native-level quality, post-editing captures both the system's strengths and weaknesses more holistically than single-dimensional automatic metrics
- Core assumption: The post-editing process accurately reflects the true gap between system output and native-level writing
- Evidence anchors:
  - [abstract]: "We suggest using human post-editing of GEC system outputs to analyze the amount of change required to reach native-level human performance on the task"
  - [section]: "We argue that post-edit distance, when affordable, is perhaps the fairest single-dimensional GEC evaluation metric"
- Break condition: If post-editors introduce their own biases or if the minimal edit requirement leads to overlooking systematic errors

### Mechanism 2
- Claim: Reference-free metrics like Scribendi favor neural systems even over human corrections
- Mechanism: These metrics rely heavily on language model scores that reward fluency over meaning preservation, causing them to rate neural system outputs higher than human corrections that may contain fewer errors but use less idiomatic phrasing
- Core assumption: Language model-based scoring inherently prioritizes fluency patterns that neural systems learn well
- Evidence anchors:
  - [abstract]: "We also found that current evaluation methods contain undesirable biases that a human evaluation is able to reveal"
  - [section]: "However, metrics that rely heavily on language model scores do not handle semantic changes well"
- Break condition: If reference-free metrics evolve to better handle semantic preservation or if neural systems stop improving their fluency scores

### Mechanism 3
- Claim: Traditional reference-based metrics suffer from ceiling effects with high-performing systems
- Mechanism: When systems achieve high grammaticality and fluency, they often produce corrections that differ substantially from human references, causing metrics like GLEU to fail to differentiate between strong systems
- Core assumption: The diversity of possible corrections increases with system performance, making single-reference evaluation less effective
- Evidence anchors:
  - [abstract]: "Reference-based evaluations suffer from limitations in capturing the wide variety of possible correction and the biases introduced during reference creation"
  - [section]: "As seen in Table 2, the reference-based GLEU suffers from ceiling effects, in particular for data from the most proficient (level C) student group"
- Break condition: If evaluation datasets begin using multiple diverse references or if systems are constrained to produce corrections closer to existing references

## Foundational Learning

- Concept: Reference-based vs reference-free evaluation metrics
  - Why needed here: Understanding these two paradigms is crucial for grasping why traditional metrics fail with advanced GEC systems
  - Quick check question: What is the fundamental difference between reference-based metrics like GLEU and reference-free metrics like Scribendi?

- Concept: Post-edit distance as evaluation metric
  - Why needed here: This concept is central to the paper's proposed solution for fair evaluation of high-performing systems
  - Quick check question: How does post-edit distance differ from traditional edit distance metrics in the context of GEC evaluation?

- Concept: Human evaluation dimensions (grammaticality, fluency, meaning preservation)
  - Why needed here: These dimensions form the basis of the paper's human evaluation methodology and highlight different aspects of GEC quality
  - Quick check question: Why might a system score high on grammaticality and fluency but low on meaning preservation?

## Architecture Onboarding

- Component map:
  Input Text -> GEC Systems (Granska, MT, GPT-3) -> Automatic Metrics (GLEU, Scribendi) -> Human Evaluation -> Post-editing Analysis -> Final Assessment

- Critical path:
  1. Process input text through multiple GEC systems
  2. Apply automatic metrics (GLEU, Scribendi)
  3. Conduct human evaluation across three dimensions
  4. Perform post-editing to measure minimal required changes
  5. Analyze results to identify evaluation biases and system performance gaps

- Design tradeoffs:
  - Human evaluation provides comprehensive assessment but is expensive and slow
  - Automatic metrics are fast but biased toward certain system types
  - Post-editing analysis is labor-intensive but provides the fairest single-dimensional evaluation

- Failure signatures:
  - High automatic metric scores with low human scores indicate fluency bias
  - High post-edit distances across all systems suggest evaluation methodology issues
  - Consistent meaning preservation failures indicate semantic understanding limitations

- First 3 experiments:
  1. Run all GEC systems on a small subset of test data and compare automatic metric scores
  2. Perform human evaluation on the same subset to identify metric biases
  3. Conduct post-editing on high-performing system outputs to establish baseline post-edit distances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GEC evaluation methods perform on longer text segments (e.g., paragraphs or documents) compared to sentence-level evaluation?
- Basis in paper: [inferred] The authors note that current evaluations are performed at the sentence level while the human corrections have access to wider context, and suggest future work should evaluate on longer segments including entire documents.
- Why unresolved: The paper only evaluates on sentence-level data from SweLL, and the authors explicitly state that working at the document level requires considerable adaptations of existing evaluation methods.
- What evidence would resolve it: Comparative evaluation of GEC systems using both sentence-level and document-level data, showing differences in system rankings and evaluation metric performance.

### Open Question 2
- Question: What is the optimal balance between reference-based and reference-free evaluation metrics for advanced GEC systems?
- Basis in paper: [explicit] The authors demonstrate that reference-free metrics favor neural systems even over human corrections, while reference-based metrics struggle to differentiate high-performing systems.
- Why unresolved: The paper shows biases in both types of metrics but does not determine an optimal combination or weighting that would provide fair evaluation across all system types.
- What evidence would resolve it: Systematic evaluation of various metric combinations and weightings across diverse GEC systems, showing which approach best correlates with human judgments.

### Open Question 3
- Question: How can evaluation methods be adapted to properly account for semantic meaning preservation in GEC outputs?
- Basis in paper: [explicit] The authors find that all automatic systems fall short in meaning preservation compared to human corrections, with GPT-3 scoring significantly below human-level performance in this dimension.
- Why unresolved: Current evaluation metrics do not adequately capture semantic changes, and the authors suggest post-edit distance as a fairer metric but do not provide a comprehensive solution.
- What evidence would resolve it: Development and validation of new evaluation metrics specifically designed to measure semantic meaning preservation, with strong correlation to human judgments across various types of semantic changes.

## Limitations
- Study limited to Swedish language data, uncertain generalizability to other languages
- Sample size of 502 texts may not capture full diversity of learner errors across all proficiency levels
- Evaluation framework focuses on specific error types, may not capture all aspects of GEC quality
- Human evaluation relies on relatively small number of annotators, potentially introducing systematic biases
- Post-editing analysis methodology is labor-intensive and may not be practical for routine evaluation

## Confidence
- GPT-3 performance superiority: High
- Evaluation metric bias findings: Medium
- Post-editing analysis methodology: Medium

## Next Checks
1. Replicate the human evaluation with a larger, more diverse annotator pool to validate the consistency of the results across different evaluators
2. Test the evaluation framework on GEC systems for other languages to assess cross-linguistic applicability of the findings
3. Conduct a longitudinal study comparing GPT-3's performance against newer LLM variants to determine if these results remain consistent as language models evolve