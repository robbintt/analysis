---
ver: rpa2
title: 'Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation'
arxiv_id: '2305.19798'
source_url: https://arxiv.org/abs/2305.19798
tags:
- kernel
- primal
- ksvd
- matrix
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a new framework to understand and optimize self-attention
  in Transformers through asymmetric Kernel Singular Value Decomposition (KSVD). The
  authors formulate a primal-dual representation of self-attention and propose a novel
  attention mechanism, Primal-Attention, based on the primal representation of KSVD.
---

# Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation

## Quick Facts
- arXiv ID: 2305.19798
- Source URL: https://arxiv.org/abs/2305.19798
- Reference count: 40
- Key outcome: A new framework using asymmetric Kernel Singular Value Decomposition (KSVD) to understand and optimize self-attention, achieving state-of-the-art performance with improved efficiency across diverse benchmarks.

## Executive Summary
This work introduces Primal-Attention, a novel self-attention mechanism based on asymmetric Kernel Singular Value Decomposition (KSVD) that operates through primal representation rather than explicit kernel matrix computation. The method reformulates self-attention using primal-dual optimization, enabling efficient attention computation while promoting low-rank properties through regularization. Experiments demonstrate superior performance on time series classification, long sequence modeling, reinforcement learning, and image classification tasks, with the method achieving state-of-the-art results while reducing computational complexity.

## Method Summary
Primal-Attention leverages asymmetric Kernel SVD to compute attention scores without explicitly constructing the kernel matrix, using feature maps to project queries and keys into primal space. The method concatenates projections from both left and right singular vectors, capturing complementary information that standard attention discards. Optimization is achieved through a regularization loss that promotes low-rank properties without requiring explicit decomposition. Two variants are proposed: PrimalFormer, which replaces all attention layers, and Primal.+, which only replaces the last layer. The framework uses data-dependent or data-independent projection weights and supports various feature map choices like cosine similarity.

## Key Results
- Achieves state-of-the-art performance on UEA time series classification, Long-Range Arena, D4RL reinforcement learning, and ImageNet-100 benchmarks
- Demonstrates improved efficiency through elimination of explicit kernel matrix computation
- Shows sharper singular value decay in attention matrices compared to canonical self-attention
- Validates effectiveness across diverse domains including time series, long sequences, RL, and image classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Primal-Attention leverages asymmetric Kernel SVD to enable more efficient and effective attention computation by avoiding explicit kernel matrix construction.
- Mechanism: Reformulates self-attention through primal-dual representation using asymmetric kernels, computing projection scores directly in primal space via feature maps instead of kernel matrix multiplication in dual space.
- Core assumption: Asymmetric nature of self-attention can be captured through asymmetric kernel techniques without losing representational power.
- Evidence anchors:
  - [abstract]: "a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual"
  - [section]: "we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD)"

### Mechanism 2
- Claim: The KSVD optimization regularizes attention with sharper singular value decay, promoting low-rank feature learning.
- Mechanism: Minimizes regularization loss derived from primal optimization objective, constraining attention matrix to have pronounced low-rank structure through KKT conditions.
- Core assumption: Low-rank property observed in deeper layers is beneficial and can be enforced through regularization without harming performance.
- Evidence anchors:
  - [abstract]: "we demonstrate that the deployed KSVD optimization regularizes Primal-Attention with a sharper singular value decay than that of the canonical self-attention"
  - [section]: "KSVD optimization can be implemented by simply minimizing a regularization loss, so that low-rank property is promoted without extra decomposition"

### Mechanism 3
- Claim: Primal-Attention captures information from both left and right singular vectors of the attention kernel, providing more complete representation than canonical self-attention.
- Mechanism: Concatenates projections from both left and right singular vectors (ei and ri), utilizing information normally discarded in standard attention.
- Core assumption: Left singular vectors contain complementary information to right singular vectors that is valuable for attention output.
- Evidence anchors:
  - [section]: "we further observe that there exists another set of projections rj regarding the left singular vectors in hei as in (8), providing extra information residing in the asymmetric kernel matrix K"
  - [abstract]: "concatenates the two sets of projections using both left and right singular vectors"

## Foundational Learning

- Concept: Asymmetric kernels and Reproducing Kernel Banach Spaces (RKBS)
  - Why needed here: Standard kernel methods assume symmetry, but self-attention's query-key structure creates inherently asymmetric attention matrices. RKBS allows handling this asymmetry.
  - Quick check question: What is the key difference between Mercer kernels and the asymmetric kernels used in this work?

- Concept: Kernel Singular Value Decomposition (KSVD) and its relationship to standard SVD
  - Why needed here: KSVD extends SVD to kernel methods, allowing interpretation of attention as decomposition of implicit kernel matrix without explicit computation.
  - Quick check question: How does the shifted eigenvalue problem in KSVD relate to the standard SVD formulation?

- Concept: Primal-dual optimization and Karush-Kuhn-Tucker (KKT) conditions
  - Why needed here: Method derives optimization through Lagrangian duality, with KKT conditions showing primal objective reaches zero at stationary solution.
  - Quick check question: What role do the KKT conditions play in enabling regularization-based optimization of Primal-Attention?

## Architecture Onboarding

- Component map: Queries, Keys, Values from linear projections -> Feature maps (ϕq, ϕk) -> Primal weights (We, Wr) -> Data transformation f(X) -> Regularization loss -> Concatenation [ei; ri] -> Final linear projection

- Critical path:
  1. Compute queries and keys from input
  2. Apply feature maps to normalize/transform
  3. Project through primal weights
  4. Concatenate left and right projections
  5. Apply final linear projection
  6. Add KSVD regularization during training

- Design tradeoffs:
  - Number of projection directions (s): Higher values capture more information but reduce efficiency gains
  - Data-dependent vs independent projections: Data-dependent offers more flexibility but increases complexity
  - Regularization coefficient (η): Balances KSVD optimization with task performance
  - Feature map choice: Different mappings (cosine, exponential) affect expressiveness and normalization

- Failure signatures:
  - Performance degradation when s is too small relative to task complexity
  - Numerical instability when data-dependent projections have insufficient samples
  - Over-regularization when η is too large, preventing learning of complex patterns
  - Feature map mismatch when ϕq and ϕk don't align with attention semantics

- First 3 experiments:
  1. Replace last attention layer in a standard Transformer with Primal-Attention (η=0.05, s=32) on a small classification task
  2. Compare data-dependent vs data-independent projections on a time series dataset
  3. Sweep η and s on a validation set to find optimal regularization balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of feature maps (e.g., cosine similarity vs. exponential) impact the performance and efficiency of Primal-Attention across different task domains?
- Basis in paper: Explicit - The paper mentions using cosine similarity kernel for experiments but notes that exponential feature maps could be used for non-linearity.
- Why unresolved: The paper only evaluates one type of feature map (cosine similarity) across all benchmarks. Different feature maps might have domain-specific advantages that weren't explored.
- What evidence would resolve it: Systematic experiments comparing different feature map choices (cosine, exponential, polynomial) across diverse tasks like NLP, vision, and time series would reveal optimal mappings for different domains.

### Open Question 2
- Question: What is the theoretical relationship between the number of projection directions (s) and the effective rank of the attention matrix in different layers of the Transformer?
- Basis in paper: Explicit - The paper shows that deeper layers have more pronounced low-rank properties and discusses choosing s values, but doesn't establish a theoretical connection.
- Why unresolved: The paper empirically demonstrates that s affects performance but doesn't provide a theoretical framework linking s to the intrinsic dimensionality revealed by SVD analysis of attention matrices.
- What evidence would resolve it: A theoretical analysis connecting the singular value spectrum of attention matrices to optimal s values per layer, potentially using information theory or statistical learning bounds.

### Open Question 3
- Question: How does Primal-Attention generalize to attention mechanisms beyond standard self-attention, such as cross-attention or causal attention in autoregressive models?
- Basis in paper: Explicit - The paper mentions adapting Primal-Attention to causal versions for RL but doesn't explore other attention variants.
- Why unresolved: The paper focuses exclusively on self-attention and only briefly mentions causal adaptation. The primal-dual framework could potentially be extended to other attention mechanisms.
- What evidence would resolve it: Implementation and evaluation of Primal-Attention variants for cross-attention in encoder-decoder models and comparison with standard approaches across translation and multimodal tasks.

## Limitations
- The asymmetric kernel formulation assumes RKBS framework can adequately handle inherent asymmetry in self-attention, which may not generalize to all attention architectures
- Data-dependent projection approach introduces computational complexity that may offset theoretical efficiency gains in certain scenarios
- Experiments cover diverse benchmarks that may not be directly comparable, requiring careful validation of "state-of-the-art" claims

## Confidence
- High Confidence: The theoretical framework of asymmetric Kernel SVD and its primal-dual representation
- Medium Confidence: The regularization approach for achieving sharper singular value decay
- Low Confidence: The claim about left singular vector projections providing complementary information

## Next Checks
1. **Mechanism 1 Validation**: Compare Primal-Attention against standard attention with identical parameter counts on a controlled benchmark to isolate efficiency gains from performance improvements.
2. **Mechanism 2 Validation**: Conduct ablation studies removing the regularization term to quantify its contribution to the observed singular value decay and performance.
3. **Mechanism 3 Validation**: Implement a variant using only right singular vectors and measure performance degradation to assess the value of left singular vector information.