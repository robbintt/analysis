---
ver: rpa2
title: 'CompeteAI: Understanding the Competition Dynamics in Large Language Model-based
  Agents'
arxiv_id: '2310.17512'
source_url: https://arxiv.org/abs/2310.17512
tags:
- agents
- competition
- restaurant
- customers
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework called CompeteAI to study competition
  behaviors among large language model-based agents. The authors implement a virtual
  town environment where two restaurant agents compete for customers, who act as judges
  with diverse characteristics.
---

# CompeteAI: Understanding the Competition Dynamics in Large Language Model-based Agents

## Quick Facts
- arXiv ID: 2310.17512
- Source URL: https://arxiv.org/abs/2310.17512
- Reference count: 10
- Primary result: LLM-based agents exhibit competitive behaviors including imitation, differentiation, quality improvement, price consistency, and Matthew Effect emergence

## Executive Summary
This paper introduces CompeteAI, a framework for studying competition dynamics among large language model-based agents in a virtual restaurant environment. The study reveals that LLM-based agents can perceive competitive contexts, engage in imitation and differentiation, improve service quality through competition, exhibit price consistency, and demonstrate the Matthew Effect in resource accumulation. The agents also spontaneously develop new strategies like offering packages and discounts. These findings align with existing sociological and economic theories, suggesting that LLM-based agents can be valuable tools for studying competition dynamics in society.

## Method Summary
The study implements a virtual town environment where two restaurant agents compete for customers who act as judges with diverse characteristics. The framework uses GPT-4 LLM to simulate restaurant management operations, customer interactions, and feedback processing. Restaurants compete over 10-day simulation periods, making strategic decisions about pricing, menu offerings, and service quality based on customer feedback. The environment includes predefined management APIs for restaurant operations and a daybook tracking system to maintain state across simulation days.

## Key Results
- LLM-based agents can perceive competitive contexts and engage in imitation and differentiation
- Competition drives service quality improvement through customer-centric adaptations
- The Matthew Effect emerges from information asymmetry in competitive environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based agents can perceive and operate within competitive contexts
- Mechanism: The LLM processes textual prompts describing competitive scenarios, uses in-context reasoning to understand strategic objectives, and generates actions consistent with competitive dynamics
- Core assumption: The LLM's training corpus includes sufficient examples of competitive behavior and economic reasoning
- Evidence anchors:
  - [abstract] "LLM-based agents can perceive competitive contexts, engage in imitation and differentiation, improve service quality through competition"
  - [section 3.1] Case study shows agent analyzing rival menu, identifying pricing differences, and planning strategic responses
  - [corpus] Weak evidence - corpus neighbors focus on social simulation but lack direct competitive behavior studies
- Break condition: If the competitive scenario requires actions outside the LLM's training distribution, the mechanism fails

### Mechanism 2
- Claim: Competition drives service quality improvement through customer-centric adaptations
- Mechanism: Agents observe customer feedback, identify needs, and modify offerings accordingly to gain competitive advantage
- Core assumption: Customer feedback is sufficiently detailed and agents can accurately interpret needs from text
- Evidence anchors:
  - [abstract] "competition encourages them to transform, such as cultivating new operating strategies"
  - [section 3.3.2] Examples of agents adding vegetarian options, adjusting prices, and creating targeted meal packages based on customer comments
  - [corpus] Weak evidence - corpus neighbors discuss multi-agent systems but not service quality dynamics
- Break condition: If feedback lacks specificity or agents misinterpret customer needs, quality improvements won't occur

### Mechanism 3
- Claim: The Matthew Effect emerges from information asymmetry in competitive environments
- Mechanism: Initial advantages lead to more customers, more feedback, better-informed decisions, and further advantages in a self-reinforcing cycle
- Core assumption: Customer choice is influenced by available information and agents use this information to make decisions
- Evidence anchors:
  - [abstract] "Matthew Effect in resource accumulation"
  - [section 3.5] Analysis showing how Restaurant 1's initial advantages led to more customers and feedback, creating information asymmetry
  - [corpus] Weak evidence - corpus neighbors don't discuss Matthew Effect specifically
- Break condition: If information is equally distributed or customers don't consider popularity, the effect disappears

## Foundational Learning

- Concept: Competition dynamics in economics
  - Why needed here: Understanding how competition affects pricing, quality, and market structure is essential for interpreting agent behaviors
  - Quick check question: What happens to prices in monopolistic competition when firms have similar products?

- Concept: Social learning theory
  - Why needed here: Explains how agents imitate successful strategies and differentiate to gain advantage
  - Quick check question: How does observation of competitor behavior lead to learning in competitive markets?

- Concept: Matthew Effect in social systems
  - Why needed here: Provides framework for understanding how initial advantages compound in competitive environments
  - Quick check question: What conditions are necessary for the Matthew Effect to emerge in competitive systems?

## Architecture Onboarding

- Component map: Prompt engineering layer -> Restaurant management system -> Customer simulation engine -> Feedback processing module -> Daybook tracking system

- Critical path: Customer selection → Restaurant service → Customer feedback → Restaurant adaptation → Repeat

- Design tradeoffs:
  - Text-only vs. multi-modal interactions: Current design uses text for simplicity but limits realism
  - Predefined vs. emergent actions: Balance between controllable simulation and discovering novel strategies
  - API costs vs. simulation scale: GPT-4 API costs constrain the number of agents and simulation length

- Failure signatures:
  - Agents producing non-competitive or irrelevant responses
  - Price convergence not occurring when expected
  - Matthew Effect not emerging despite initial advantages
  - Customer feedback not leading to meaningful restaurant adaptations

- First 3 experiments:
  1. Run simulation with two identical restaurants to observe baseline competitive dynamics
  2. Introduce asymmetric initial conditions (different funding, locations) to test Matthew Effect emergence
  3. Vary customer diversity to examine how different preference distributions affect competition outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the competition behaviors of LLM-based agents scale when increasing the number of restaurants and customers?
- Basis in paper: [explicit] The paper mentions that due to API limitations, experiments did not involve a significant number of restaurants and customers, and future research should expand the number of interacting agents
- Why unresolved: The study was limited by API constraints and focused on a small-scale simulation with only two restaurants and 14 customers
- What evidence would resolve it: Conducting experiments with a larger number of restaurants and customers, varying their characteristics and interactions, to observe if the competition behaviors and emergent strategies remain consistent or evolve

### Open Question 2
- Question: How would the competition dynamics change if multi-modal LLMs were used instead of text-based models?
- Basis in paper: [inferred] The paper acknowledges that real-world environments often involve multi-modal interactions, and future studies could offer a more holistic view as more sophisticated multi-modal LLMs become available
- Why unresolved: The current framework leverages GPT-4, which is text-based, and the study did not explore the potential differences in competition behaviors if multi-modal interactions were introduced
- What evidence would resolve it: Implementing the competitive environment using multi-modal LLMs and comparing the competition dynamics, strategies, and outcomes with those observed in the text-based model simulations

### Open Question 3
- Question: Can LLM-based agents develop new competition theories or rules that go beyond existing sociological and economic theories?
- Basis in paper: [explicit] The paper suggests that it is interesting to explore whether LLM-based agents can behave more than just following existing knowledge in the training data and cultivate new intelligence that could uncover new rules, laws, or even theories
- Why unresolved: The study focused on observing competition behaviors that align with existing theories, but did not investigate the potential for agents to generate novel theories or rules through their interactions and learning processes
- What evidence would resolve it: Analyzing the agents' strategies, decision-making processes, and emergent behaviors over time to identify patterns or rules that are not explicitly based on existing theories, and formulating new hypotheses or theories based on these observations

## Limitations
- Reliance on text-based simulations that may not capture real-world competitive complexity
- Artificial environment constrains agent behaviors to predefined action spaces
- Customer feedback depends on LLM-generated responses that may not reflect genuine consumer preferences
- Limited 10-day simulation duration provides insufficient temporal scope for long-term dynamics

## Confidence
- **High confidence**: Agents' ability to perceive competitive contexts and engage in imitation/differentiation
- **Medium confidence**: Price consistency and service quality improvements
- **Low confidence**: Matthew Effect observations due to limited simulation duration

## Next Checks
1. **Cross-environment validation**: Run the same competitive scenarios across different LLM models to verify whether observed behaviors are model-specific or represent general competitive dynamics

2. **Human-in-the-loop verification**: Have human participants evaluate restaurant outputs and customer feedback to assess whether LLM-generated competitive behaviors align with real-world expectations and quality standards

3. **Extended duration study**: Scale the simulation to 30-50 days to observe whether short-term competitive strategies evolve into sustainable business models and whether the Matthew Effect strengthens or dissipates over time