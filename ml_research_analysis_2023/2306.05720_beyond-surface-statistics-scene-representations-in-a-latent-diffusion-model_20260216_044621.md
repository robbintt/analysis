---
ver: rpa2
title: 'Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model'
arxiv_id: '2306.05720'
source_url: https://arxiv.org/abs/2306.05720
tags:
- depth
- probing
- representations
- images
- salient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors probe a latent diffusion model to determine if it learns
  internal representations of scene depth and object saliency. Using linear probes,
  they show that self-attention layer activations encode strong linear representations
  of both depth and foreground/background segmentation, emerging as early as step
  2 of denoising when human-perceptible structure is still minimal.
---

# Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model

## Quick Facts
- arXiv ID: 2306.05720
- Source URL: https://arxiv.org/abs/2306.05720
- Reference count: 38
- Latent diffusion models encode depth and saliency information as early as step 2 of denoising

## Executive Summary
This paper investigates whether latent diffusion models learn structured internal representations of scene depth and object saliency without explicit supervision. Using linear probes on self-attention layer activations, the authors demonstrate that LDM models encode strong linear representations of both 3D depth and salient object/background segmentation. These representations emerge surprisingly early in the denoising process—before human-perceptible structure appears—and can be causally manipulated to alter output image geometry. The findings suggest that LDM models capture rich 3D scene information through their attention mechanisms alone.

## Method Summary
The authors use linear probing to test whether LDM self-attention activations contain linearly separable depth and saliency information. They train separate classifiers on intermediate activations from all denoising steps, using a synthetic dataset of 617 images with ground truth depth (MiDaS) and saliency (TRACER) labels. Probing classifiers are evaluated using Dice coefficient for saliency detection and RMSE for depth estimation. The methodology includes intervention experiments where probing classifier gradients are used to modify internal representations, followed by observing resulting changes in generated images.

## Key Results
- Linear probes achieve Dice coefficient of 0.85 for saliency and RMSE of 0.47 for depth at final denoising step
- Depth and saliency representations emerge as early as step 2 of denoising, well before human-perceptible structure
- Intervention experiments confirm causal link: modifying depth/saliency representations changes output image geometry and object placement
- Probing performance significantly exceeds baselines from randomized LDM models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LDM creates internal representations of scene depth and object saliency that are linearly accessible
- Mechanism: Self-attention layers implicitly encode spatial depth relationships and object/background segmentation as distributed representations
- Core assumption: Attention mechanisms naturally organize visual information hierarchically
- Evidence anchors: [abstract] internal activations encode linear representations of both 3D depth and salient-object distinction; [section 3] linear classifiers trained on self-attention outputs predict pixel-level logits
- Break condition: If depth information were purely surface-level correlations rather than structured representations

### Mechanism 2
- Claim: Depth and saliency representations emerge early in the denoising process
- Mechanism: As noise is removed, attention mechanisms must resolve fundamental spatial relationships before filling in details
- Core assumption: Denoising follows progression from coarse spatial organization to fine texture details
- Evidence anchors: [abstract] representations appear early, before human can make sense of images; [section 4.1] object delineation in step 2, layout determined by step 5
- Break condition: If model used different generation strategy not requiring early spatial organization

### Mechanism 3
- Claim: Internal depth representations are causally linked to output image geometry
- Mechanism: Modifying depth representation in internal activations directly changes spatial arrangement of objects
- Core assumption: Model uses internal depth representation as active guide for spatial composition
- Evidence anchors: [abstract] intervention experiments show modified representations lead to corresponding changes; [section 5.1] gradients from probing classifier update internal representations
- Break condition: If depth representations were merely epiphenomenal correlations rather than active guides

## Foundational Learning

- Concept: Linear probing methodology
  - Why needed here: To determine whether LDM's internal representations contain linearly separable information about depth and saliency
  - Quick check question: If a model's internal activations contain strong non-linear representations of depth, would linear probes still detect them effectively?

- Concept: Self-attention mechanisms in vision transformers
  - Why needed here: LDM uses self-attention layers to process latent representations, which are the source of depth representations
  - Quick check question: How do self-attention mechanisms differ from convolutional layers in their ability to capture global spatial relationships?

- Concept: Causal intervention in neural networks
  - Why needed here: To verify that detected depth representations aren't just correlations but actively guide image generation
  - Quick check question: What distinguishes a causal relationship from mere correlation in neural network representations?

## Architecture Onboarding

- Component map: Input latent → progressive denoising through self-attention layers → VAE decoder → output image
- Critical path: Input latent → denoising via self-attention layers → VAE decoder → output image
- Design tradeoffs: LDM operates in compressed latent space for efficiency, but compression may obscure or preserve spatial relationships
- Failure signatures: Poor probing performance suggests incorrect activation extraction or insufficient classifier capacity
- First 3 experiments:
  1. Train linear probes on self-attention activations at different denoising steps to identify when depth representations emerge
  2. Compare probing performance on trained vs. randomized LDM to establish representations aren't spurious
  3. Perform intervention experiments by modifying internal activations and observing changes in output geometry

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the strength of linear depth representations in latent diffusion models compare to representations found in other types of generative models like GANs or autoregressive models?
- Basis in paper: [explicit] Paper focuses on latent diffusion models without comparing to alternative architectures
- Why unresolved: Paper establishes LDMs have strong linear depth representations but doesn't benchmark against other generative architectures
- What evidence would resolve it: Direct comparative experiments probing for depth representations in GANs, autoregressive models, and other diffusion-based approaches

### Open Question 2
- Question: What specific attention mechanisms or architectural components in latent diffusion models are most responsible for encoding depth and saliency information?
- Basis in paper: [inferred] Paper shows depth representations emerge in self-attention layers but doesn't systematically ablate different components
- Why unresolved: While paper identifies self-attention layers encode depth, it doesn't investigate which specific attention patterns or mechanisms drive this capability
- What evidence would resolve it: Ablation studies removing or modifying specific attention heads, analyzing attention weight patterns

### Open Question 3
- Question: How do depth representations in latent diffusion models generalize across different domains and image types beyond the synthetic dataset used in this study?
- Basis in paper: [explicit] Paper uses synthetic dataset and acknowledges this limitation, noting need to test on more diverse real-world datasets
- Why unresolved: Controlled synthetic dataset may not capture full diversity of real-world imagery, and paper explicitly notes this as limitation
- What evidence would resolve it: Probing experiments on larger, more diverse real-world datasets like COCO, Places, or other benchmark image collections

## Limitations

- Synthetic dataset generation may introduce artifacts not present in real-world images
- Linear probing methodology may miss complex non-linear relationships in depth representations
- Study focuses on one specific LDM architecture (Stable Diffusion v1.4) without testing generalization across different models

## Confidence

- High confidence in existence of internal depth representations (consistent probing performance and causal intervention experiments)
- Medium confidence in early emergence timing (step 2), relies on subjective visual examples
- Medium confidence in causal role of depth representations, intervention effects shown but mechanism unclear

## Next Checks

1. Test probing performance on held-out real-world dataset to verify generalizability of depth and saliency representations
2. Conduct ablation studies removing self-attention layers to determine if representations depend critically on attention mechanism
3. Perform multi-layer probing to determine if depth information is localized to specific layers or distributed across network architecture