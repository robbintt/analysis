---
ver: rpa2
title: 'BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction'
arxiv_id: '2312.07088'
source_url: https://arxiv.org/abs/2312.07088
tags:
- entity
- entities
- novel
- relation
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BED, a Bi-Encoder-Decoder model for canonical
  relation extraction. BED encodes entities using a dedicated encoder that learns
  high-quality representations from entity names and descriptions, enabling it to
  handle novel entities without retraining.
---

# BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction

## Quick Facts
- arXiv ID: 2312.07088
- Source URL: https://arxiv.org/abs/2312.07088
- Reference count: 6
- Key outcome: BED achieves 83.36% F1-score on WIKI and 76.41% on GEO datasets, with 5× faster inference and 80% fewer parameters

## Executive Summary
This paper proposes BED, a Bi-Encoder-Decoder model for canonical relation extraction that can handle novel entities without retraining. The model encodes entities using a dedicated encoder that learns high-quality representations from entity names and descriptions, then generates candidate entities per sentence via BM25 similarity. BED significantly outperforms previous state-of-the-art models on two benchmark datasets while being more efficient. The key innovation is the entity encoder that enables representation of unseen entities, making the model adaptable to new knowledge without additional training.

## Method Summary
BED employs a Bi-Encoder-Decoder architecture for canonical relation extraction. The entity encoder processes entity names and descriptions to generate high-quality representations, while the sentence encoder processes input text. A GRU-based decoder with n-gram attention generates relational triples from the encoded representations. Candidate entity generation uses NER to identify mentions and BM25 similarity to filter entities, keeping top-K candidates per sentence. The model is trained using cross-entropy loss on aligned Wikipedia sentences and Wikidata triples (WIKI dataset) or travel website data (GEO dataset).

## Key Results
- BED achieves 83.36% F1-score on WIKI dataset, outperforming previous SOTA models
- BED achieves 76.41% F1-score on GEO dataset with 5× faster inference and 80% fewer parameters
- BED demonstrates strong performance on novel entities, achieving 82.49% F1-score on unseen entity test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entity encoder learns high-quality representations by encoding entity names and descriptions
- Mechanism: The model concatenates entity name and description with a [SEP] token and feeds them into a GRU encoder. Mean pooling is then applied to obtain the entity representation.
- Core assumption: Entity descriptions contain complementary information that improves representation quality compared to using entity names alone
- Evidence anchors:
  - [abstract]: "we employ an encoder to encode semantics of this information, leading to high-quality entity representations"
  - [section 3.2.1]: "the concatenation of entity name and description is fed to the entity encoder to generate the entity representation"
  - [corpus]: Weak evidence - no direct citations to entity description encoding in related papers

### Mechanism 2
- Claim: Candidate entity filtering via BM25 similarity improves training and inference efficiency
- Mechanism: BM25 scores are computed between entity mention spans and entity names, keeping top-K candidates (64) per sentence
- Core assumption: Relevant entities have higher surface-form similarity with entity mentions than irrelevant entities
- Evidence anchors:
  - [section 3.1]: "we use BM25, a variant of TF-IDF to measure similarity between mention spans and entity names"
  - [section 3.1]: "On the dev set, the coverage of the top-64 candidates is 98.16%"
  - [corpus]: No direct evidence in related papers about BM25 usage for CRE

### Mechanism 3
- Claim: The Bi-Encoder-Decoder architecture enables handling novel entities without retraining
- Mechanism: Once trained, the entity encoder can generate representations for any new entity by processing its name and description, while the decoder remains unchanged
- Core assumption: The entity encoder learns generalizable features that can encode unseen entities
- Evidence anchors:
  - [abstract]: "For novel entities, given a trained entity encoder, their representations can be easily generated"
  - [section 4.4]: Results show BED outperforms N-gram Attention variants on novel entities without retraining
  - [corpus]: Weak evidence - related papers focus on different aspects of relation extraction

## Foundational Learning

- Concept: Encoder-decoder architecture for sequence-to-sequence tasks
  - Why needed here: BED uses this architecture to translate sentences into relational triples
  - Quick check question: What are the main components of an encoder-decoder architecture and how do they interact?

- Concept: Named Entity Recognition (NER) and Entity Linking
  - Why needed here: Candidate generation relies on NER to identify entity mentions and linking to match mentions to KB entities
  - Quick check question: How does NER differ from entity linking, and why are both needed for candidate generation?

- Concept: Attention mechanisms in neural networks
  - Why needed here: The decoder uses n-gram attention to capture multi-word entity mentions
  - Quick check question: What is the difference between standard attention and n-gram attention, and why is it useful here?

## Architecture Onboarding

- Component map: Sentence → NER → BM25 candidate filtering → BED (Entity Encoder + Sentence Encoder + Decoder) → Triple output

- Critical path: Entity mentions are identified via NER, filtered through BM25 similarity to generate candidates, then processed by BED's entity encoder, sentence encoder, and decoder with n-gram attention to produce final triples

- Design tradeoffs:
  - Using GRU instead of LSTM reduces computational cost but may limit representational capacity
  - Top-K candidate filtering improves efficiency but may miss relevant entities if K is too small
  - Separate entity and sentence encoders allow handling novel entities but increase model complexity

- Failure signatures:
  - Low recall on novel entities: Entity encoder fails to generalize
  - Poor precision: Candidate filtering too permissive or decoder overgenerates
  - Slow inference: Candidate filtering too restrictive or inefficient implementation

- First 3 experiments:
  1. Verify entity encoder quality: Compare entity representations with and without descriptions on a similarity task
  2. Test candidate generation: Measure coverage and precision of BM25 filtering at different K values
  3. Validate novel entity handling: Train on subset of entities, test on held-out entities with BED vs. baseline

## Open Questions the Paper Calls Out

- Open Question 1: How does BED handle entities with multiple meanings or aliases, and what impact does this have on performance?
  - Basis in paper: [inferred] The paper mentions using BM25 for entity candidate generation based on surface-form similarity, but does not discuss handling entities with multiple meanings or aliases
  - Why unresolved: The paper does not provide experimental results or analysis on how BED handles entities with multiple meanings or aliases
  - What evidence would resolve it: Experimental results comparing BED's performance on entities with multiple meanings or aliases versus entities with single meanings would help determine the impact on performance

- Open Question 2: How does the choice of encoder architecture (e.g., GRU vs. LSTM) affect BED's performance and efficiency?
  - Basis in paper: [explicit] The paper mentions using GRU as the encoder for both the entity encoder and sentence encoder, but does not compare its performance with other architectures like LSTM
  - Why unresolved: The paper does not provide experimental results or analysis on how different encoder architectures affect BED's performance and efficiency
  - What evidence would resolve it: Experimental results comparing BED's performance and efficiency using different encoder architectures (e.g., GRU vs. LSTM) would help determine the impact of the choice of encoder architecture

- Open Question 3: How does BED's performance scale with the size of the knowledge base and the number of entities?
  - Basis in paper: [inferred] The paper mentions that BED reduces the number of model parameters and achieves faster inference speed, but does not discuss how its performance scales with the size of the knowledge base and the number of entities
  - Why unresolved: The paper does not provide experimental results or analysis on how BED's performance scales with the size of the knowledge base and the number of entities
  - What evidence would resolve it: Experimental results evaluating BED's performance on datasets with varying sizes of knowledge bases and numbers of entities would help determine how its performance scales

## Limitations

- Entity description dependency: The model assumes high-quality, available entity descriptions which may not be realistic in all domains
- Surface-form similarity limitations: BM25-based filtering may struggle with polysemous entity names or short mentions
- Domain generalization uncertainty: Performance on domains beyond WIKI and GEO datasets remains untested

## Confidence

- **High confidence** in the core contribution: BED's ability to handle novel entities without retraining is well-supported by experimental results
- **Medium confidence** in efficiency claims: Computational efficiency gains are reasonable but lack specific implementation details for verification
- **Low confidence** in generalization across domains: Limited evaluation to two datasets prevents strong claims about broader applicability

## Next Checks

1. **Entity description quality impact**: Systematically evaluate BED performance when entity descriptions are progressively degraded (removed, shortened, or replaced with noise) to quantify the dependency on description quality

2. **BM25 filtering robustness**: Test BED with different K values (top-32, top-128) and compare against learned candidate filtering approaches to validate the efficiency-quality tradeoff

3. **Novel entity generalization**: Create a more challenging novel entity test set with entities from completely different domains than the training data (e.g., medical or technical entities) to stress-test the entity encoder's generalization capability