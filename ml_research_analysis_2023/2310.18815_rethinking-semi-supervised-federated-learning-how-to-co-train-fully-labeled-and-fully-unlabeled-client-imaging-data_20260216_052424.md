---
ver: rpa2
title: 'Rethinking Semi-Supervised Federated Learning: How to co-train fully-labeled
  and fully-unlabeled client imaging data'
arxiv_id: '2310.18815'
source_url: https://arxiv.org/abs/2310.18815
tags:
- learning
- clients
- federated
- labeled
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IsoFed, a novel semi-supervised federated learning
  (SSFL) framework designed for the challenging setting where some clients have fully
  labeled data while others have fully unlabeled data. The core idea is to perform
  isolated aggregation of labeled and unlabeled client models separately, followed
  by local self-supervised pretraining on the global models.
---

# Rethinking Semi-Supervised Federated Learning: How to co-train fully-labeled and fully-unlabeled client imaging data

## Quick Facts
- arXiv ID: 2310.18815
- Source URL: https://arxiv.org/abs/2310.18815
- Reference count: 28
- Primary result: Proposed IsoFed achieves 6.91% accuracy improvement over state-of-the-art semi-supervised federated learning methods

## Executive Summary
This paper addresses the challenge of semi-supervised federated learning where some clients have fully labeled data while others have fully unlabeled data. The proposed IsoFed framework introduces isolated aggregation of labeled and unlabeled client models followed by local self-supervised pretraining. By preventing gradient diversity issues that arise when averaging supervised and semi-supervised models together, IsoFed achieves near-supervised learning performance even under challenging non-IID data distributions and varying proportions of labeled clients.

## Method Summary
IsoFed implements a two-phase approach for semi-supervised federated learning with mixed labeled/unlabeled clients. First, it performs isolated aggregation where labeled client models are aggregated separately from unlabeled client models, each optimized with their respective objective functions (cross-entropy for labeled, consistency regularization for unlabeled). Second, it conducts local self-supervised pretraining using information maximization loss on the isolated global models to transfer knowledge between client groups while avoiding catastrophic forgetting. The framework employs a mean-teacher-based semi-supervised learning approach for unlabeled clients and dynamically weighted federated averaging for model aggregation.

## Key Results
- IsoFed outperforms state-of-the-art SSFL methods by 6.91% in accuracy
- Achieves near-supervised learning performance across all tested datasets
- Demonstrates robustness to non-IID data distributions (γ=0.5 and 0.8 Dirichlet)
- Maintains strong performance even with only 25% of clients having labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Isolating aggregation of labeled and unlabeled client models prevents gradient diversity issues that occur when averaging supervised and semi-supervised models together.
- Mechanism: By aggregating labeled client models separately from unlabeled client models, each group's objective function is optimized without interference from the other group's gradients.
- Core assumption: The high gradient diversity between supervised and semi-supervised models is the primary bottleneck in standard federated averaging approaches.
- Evidence anchors:
  - [abstract] "The bottleneck in this setting is the joint training of labeled and unlabeled clients as the objective function for each client varies based on the availability of labels."
  - [section] "This results in high gradient diversity [28] between the supervised and unsupervised models particularly in heterogeneous client settings, as these are targeted to optimize separate objective functions."
- Break condition: If the labeled and unlabeled client distributions become too similar, the benefit of isolation may diminish as gradient diversity becomes less pronounced.

### Mechanism 2
- Claim: Federated pretraining on isolated global models transfers knowledge between labeled and unlabeled client groups while avoiding catastrophic forgetting.
- Mechanism: After isolated aggregation, the global model from one client group is used as a starting point for self-supervised pretraining on the other group's data, creating a knowledge transfer pathway that preserves both groups' learned representations.
- Core assumption: Transfer learning can effectively bridge the knowledge gap between supervised and semi-supervised representations without losing either group's specific learning.
- Evidence anchors:
  - [section] "To improve client-specific model performance, we conduct a second phase of in-client federated pretraining on the global model before initializing it as a teacher model."
- Break condition: If the feature distributions between labeled and unlabeled clients are too dissimilar, transfer learning may not provide meaningful benefits and could even harm performance.

### Mechanism 3
- Claim: Information maximization loss during pretraining creates diverse yet certain predictions across clients.
- Mechanism: The information maximization objective simultaneously minimizes entropy while maximizing mutual information between data and predictions, preventing the model from collapsing to trivial solutions.
- Core assumption: Entropy minimization alone would cause all predictions to collapse to a single class, while the diversity-preserving regularizer prevents this degenerate solution.
- Evidence anchors:
  - [section] "The joint optimization is done by reducing the entropy of the output probability distribution of global model (pi) in conjunction with maximizing the mutual information between the data distribution and the estimated output distribution yielded by the global model."
- Break condition: If the regularization strength is too high, it may prevent the model from achieving sufficiently low entropy, reducing classification performance.

## Foundational Learning

- Concept: Federated Averaging with Weighted Aggregation
  - Why needed here: The standard federated averaging needs modification to handle two distinct client populations with different objective functions
  - Quick check question: How does dynamically weighted aggregation differ from standard FedAvg in handling heterogeneous client objectives?

- Concept: Semi-Supervised Learning with Mean Teacher
  - Why needed here: Unlabeled clients require consistency regularization between student and teacher models, while labeled clients use standard cross-entropy
  - Quick check question: What role does the teacher model play in maintaining consistency regularization for unlabeled data?

- Concept: Information Maximization and Entropy Minimization
  - Why needed here: Pretraining requires balancing between certain predictions and diverse class representations to avoid trivial solutions
  - Quick check question: How does the diversity-preserving regularizer in information maximization prevent entropy minimization from collapsing all predictions to one class?

## Architecture Onboarding

- Component map: Labeled clients -> Labeled model aggregation -> Labeled global model; Unlabeled clients -> Unlabeled model aggregation -> Unlabeled global model; Pretraining on both global models
- Critical path: Global model initialization → Isolated aggregation round 1 (unlabeled clients) → Isolated aggregation round 2 (labeled clients) → Pretraining on all clients → Next communication round
- Design tradeoffs: Isolation provides gradient diversity benefits but requires more complex aggregation logic; pretraining adds computation but improves cross-client knowledge transfer
- Failure signatures: Poor performance on either client group indicates gradient diversity issues or ineffective transfer learning; collapse of unlabeled client performance suggests pretraining regularization problems
- First 3 experiments:
  1. Test isolated aggregation without pretraining on a simple binary classification dataset to verify gradient diversity benefits
  2. Test pretraining effectiveness by comparing with and without pretraining step on labeled-unlabeled split
  3. Vary Dirichlet parameter (γ) to evaluate robustness to non-IID distributions across both client groups

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of IsoFed change when using more complex CNN architectures or different backbone networks instead of the simple CNN used in the experiments?
- Basis in paper: [explicit] The paper states "For all datasets, we employ a simple CNN comprising of two 5 ×5 convolution layers..." and suggests future work could explore different architectures.
- Why unresolved: The experiments only evaluated a simple CNN architecture, so the performance impact of using more complex models is unknown.
- What evidence would resolve it: Experiments comparing IsoFed performance using different CNN architectures (e.g., ResNet, VGG) or even transformer-based models on the same datasets.

### Open Question 2
- Question: What is the impact of varying the number of local training epochs on IsoFed's performance, particularly for the unlabeled clients?
- Basis in paper: [inferred] The paper describes the local training process but does not systematically vary the number of local epochs to study its impact on performance.
- Why unresolved: The optimal number of local epochs for unlabeled clients may differ from labeled clients, and this hyperparameter's impact is not explored.
- What evidence would resolve it: Experiments varying the number of local epochs for both labeled and unlabeled clients and measuring the impact on final model performance and convergence speed.

### Open Question 3
- Question: How does IsoFed perform when applied to other types of medical imaging tasks beyond classification, such as segmentation or detection?
- Basis in paper: [explicit] The paper states "IsoFed can be easily incorporated into other federated learning-based aggregation schemes as well as used in conjunction with any other semi-supervised learning framework in federated learning setting."
- Why unresolved: All experiments in the paper focused on classification tasks, so performance on other medical imaging tasks is unknown.
- What evidence would resolve it: Applying IsoFed to medical imaging segmentation or detection tasks and comparing performance to existing federated learning methods for these tasks.

## Limitations

- Limited experimental validation with only 4 clients per dataset, restricting generalizability to larger federated settings
- Lack of ablation studies on isolated aggregation and pretraining components separately
- No exploration of performance when only 10% of clients are labeled
- Information maximization loss formulation lacks theoretical grounding and comparison with established SSL objectives

## Confidence

- **High**: The core concept of isolated aggregation preventing gradient diversity issues (Mechanism 1)
- **Medium**: The effectiveness of federated pretraining for knowledge transfer (Mechanism 2)
- **Medium**: The information maximization loss preventing trivial solutions (Mechanism 3)

## Next Checks

1. Test IsoFed with larger client counts (e.g., 50-100 clients) to verify scalability of isolated aggregation
2. Conduct ablation studies to isolate the contribution of pretraining vs. isolated aggregation
3. Evaluate performance when only 10% of clients are labeled to test extreme semi-supervised scenarios