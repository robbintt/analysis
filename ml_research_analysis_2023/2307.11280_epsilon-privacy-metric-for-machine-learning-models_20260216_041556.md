---
ver: rpa2
title: 'Epsilon*: Privacy Metric for Machine Learning Models'
arxiv_id: '2307.11280'
source_url: https://arxiv.org/abs/2307.11280
tags:
- privacy
- training
- epsilon
- loss
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Epsilon is a privacy metric for measuring the privacy risk of a
  trained ML model without requiring model re-training or access to the training data.
  It quantifies the privacy risk of a single model instance against membership inference
  attacks by estimating the true positive and false positive rates in a hypothesis
  test.
---

# Epsilon*: Privacy Metric for Machine Learning Models

## Quick Facts
- **arXiv ID**: 2307.11280
- **Source URL**: https://arxiv.org/abs/2307.11280
- **Reference count**: 39
- **One-line primary result**: Epsilon* quantifies privacy risk of ML models without re-training, showing up to 800% reduction when using differential privacy.

## Executive Summary
Epsilon* is a privacy metric that measures the membership inference attack risk of a trained ML model using only black-box access to model predictions, without requiring re-training or training data access. The metric estimates true positive and false positive rates from loss distributions and fits parametric Gaussian models to avoid numerical instability. Experiments show Epsilon* effectively captures privacy risk and is sensitive to differential privacy mitigation strategies.

## Method Summary
Epsilon* measures privacy risk by computing false positive and true positive rates from loss distributions on training and non-training data. The method transforms loss data through normalization and logit transformation, fits Gaussian parametric distributions to these transformed losses, and evaluates cumulative distribution functions at multiple thresholds to find the maximum privacy risk. This approach avoids the numerical instability issues of empirical CDF estimation while providing a conservative lower bound on actual privacy risk.

## Key Results
- Epsilon* successfully quantifies privacy risk without requiring model re-training or training data access
- The metric shows up to 800% reduction in privacy risk when models are trained with differential privacy
- Parametric fitting of loss distributions improves accuracy and stability compared to empirical CDF estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Epsilon* provides an empirical lower bound on privacy risk without re-training or data access
- Mechanism: Computed from true positive and false positive rates of membership inference attack hypothesis test using black-box predictions
- Core assumption: True positive and false positive rates can be accurately estimated from model's loss distributions
- Evidence anchors: [abstract] and [section 3.1] discuss black-box access and distinction between mitigating and measuring privacy risk
- Break condition: If loss distributions are identical or very similar, Epsilon* may underestimate actual risks

### Mechanism 2
- Claim: Parametric fitting improves accuracy and stability over empirical CDF estimation
- Mechanism: Gaussian distributions fitted to normalized, logit-transformed loss data used to compute FPR/FNR
- Core assumption: Transformed loss data follows Gaussian distribution accurately estimated from finite samples
- Evidence anchors: [section 3.2] and [appendix B] describe parametric fitting approach and improved accuracy
- Break condition: Non-Gaussian true distributions may introduce bias in Epsilon* estimation

### Mechanism 3
- Claim: Epsilon* decreases significantly with differential privacy training
- Mechanism: DP-SGD makes model weights less sensitive to individual examples, resulting in similar training/non-training loss distributions
- Core assumption: Epsilon parameter in DP-SGD is dominant factor affecting membership inference risk
- Evidence anchors: [abstract] and [section 4.2.1] show Epsilon* values reduced by up to 800% with DP
- Break condition: Incorrect or insufficient DP-SGD implementation may not accurately reflect privacy guarantees

## Foundational Learning

- Concept: Membership Inference Attacks (MIA)
  - Why needed here: Epsilon* quantifies MIA risk by measuring distinguishability between training and non-training data
  - Quick check question: How does a membership inference attack use model predictions to determine if a data point was in the training set?

- Concept: Differential Privacy (DP)
  - Why needed here: Epsilon* builds on DP concepts and compares values between DP and non-DP trained models
  - Quick check question: What is the difference between the privacy parameter epsilon in differential privacy and the Epsilon* metric?

- Concept: Hypothesis Testing and False Positive/Negative Rates
  - Why needed here: Epsilon* derives from false positive and false negative rates of membership hypothesis test
  - Quick check question: How do false positive and false negative rates relate to the epsilon parameter in Epsilon*?

## Architecture Onboarding

- Component map: Model -> Loss generation -> Transformation (normalize, logit) -> Gaussian fitting -> CDF evaluation -> FPR/TPR computation -> Epsilon* calculation
- Critical path: Transformation of loss data, parametric fitting of Gaussian distributions, computation of FPR/TPR at multiple thresholds to find maximum Epsilon* value
- Design tradeoffs: Empirical CDF estimation vs parametric fitting (simplicity vs accuracy/stability)
- Failure signatures: Consistently high Epsilon* values may indicate overfitting or data leakage; near-zero values may indicate underfitting or strong privacy
- First 3 experiments:
  1. Compute Epsilon* on logistic regression model on Adult dataset with varying training epochs
  2. Compare Epsilon* between DP-SGD trained model (epsilon=1) and baseline model
  3. Vary sample size for loss distribution estimation and observe Epsilon* stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between Epsilon* and epsilon when training with DP, especially for epsilon < 1?
- Basis in paper: [explicit] Paper states Epsilon* < epsilon for epsilon >= 1 but doesn't explore relationship for epsilon < 1
- Why unresolved: Experiments only show Epsilon* < epsilon for epsilon >= 1, relationship for epsilon < 1 not explored
- What evidence would resolve it: Experiments with models trained with DP for epsilon values below 1, comparing Epsilon* values to epsilon

### Open Question 2
- Question: How does choice of parametric distribution affect accuracy and sensitivity of Epsilon*?
- Basis in paper: [inferred] Paper mentions Gaussian fitting but doesn't explore impact of different parametric distributions
- Why unresolved: Only explores Gaussian distributions without comparing to other parametric distributions
- What evidence would resolve it: Experiments comparing Epsilon* values from different parametric distributions (e.g., Gamma, Beta)

### Open Question 3
- Question: What is the impact of sample size on stability and accuracy of Epsilon*?
- Basis in paper: [explicit] Paper mentions small sample sizes can lead to overestimation when distributions are identical
- Why unresolved: Only briefly mentions sample size impact in context of identical distributions
- What evidence would resolve it: Experiments varying sample size of training/non-training data, measuring stability and accuracy

## Limitations
- Epsilon* provides only an empirical lower bound on privacy risk, meaning actual attacks could achieve higher success rates
- Gaussian distribution assumption for transformed loss data may not hold for all model architectures or datasets
- Choice of hyperparameters for logit transformation and sampling rate could affect results but are not extensively explored

## Confidence
- **High Confidence**: Core mechanism of using parametric distributions to avoid numerical instability is well-justified
- **Medium Confidence**: Sensitivity to DP training is supported but magnitude may vary with different implementations
- **Medium Confidence**: Ability to measure privacy risk without re-training is valid but depends on quality of loss distribution estimation

## Next Checks
1. Test Epsilon* on models with intentionally non-Gaussian loss distributions to evaluate impact of parametric fitting assumptions
2. Compare Epsilon* values across different transformation parameters (Î± values) to determine hyperparameter sensitivity
3. Evaluate Epsilon* on DP-trained model with sub-optimal noise injection to detect insufficient privacy guarantees