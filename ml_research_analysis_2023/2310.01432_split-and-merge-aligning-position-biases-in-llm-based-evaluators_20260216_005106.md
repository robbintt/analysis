---
ver: rpa2
title: 'Split and Merge: Aligning Position Biases in LLM-based Evaluators'
arxiv_id: '2310.01432'
source_url: https://arxiv.org/abs/2310.01432
tags:
- evaluators
- answers
- evaluation
- ortia
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PORTIA is a lightweight framework designed to address position
  bias in large language model (LLM)-based evaluators, which tend to favor the first
  or second answer in pairwise comparisons regardless of content. Inspired by human
  long-text reading strategies, PORTIA splits candidate answers into segments at sentence
  boundaries, aligns similar content across answers using either length-based or semantic-based
  methods, and merges the segments back into a single prompt for LLM evaluation.
---

# Split and Merge: Aligning Position Biases in LLM-based Evaluators

## Quick Facts
- arXiv ID: 2310.01432
- Source URL: https://arxiv.org/abs/2310.01432
- Reference count: 27
- Primary result: PORTIA improves LLM evaluator consistency by 47.46% on average while reducing cost by 90%

## Executive Summary
PORTIA is a lightweight framework designed to address position bias in large language model (LLM)-based evaluators, which tend to favor the first or second answer in pairwise comparisons regardless of content. Inspired by human long-text reading strategies, PORTIA splits candidate answers into segments at sentence boundaries, aligns similar content across answers using either length-based or semantic-based methods, and merges the segments back into a single prompt for LLM evaluation. Experiments across six diverse LLMs evaluating 11,520 answer pairs show PORTIA markedly improves consistency rates by an average of 47.46% relative to baselines. It enables less advanced GPT-3.5 to achieve 88% agreement with state-of-the-art GPT-4 at only 10% of the cost. Human evaluations further confirm PORTIA-enhanced GPT-3.5 aligns better with human judgments than standalone GPT-4. These results demonstrate PORTIA's effectiveness at calibrating position bias, boosting LLM consistency, and providing a cost-efficient, scalable solution for automated evaluation across applications.

## Method Summary
PORTIA addresses position bias in LLM evaluators by splitting answers into segments at sentence boundaries, aligning similar content across answers using either length-based or semantic-based methods, and merging aligned segments into a single prompt for evaluation. The framework uses format detection to identify split positions, performs length alignment to create equal-length segments, and optionally applies semantic alignment using token overlap similarity to find optimal split positions. Experiments were conducted on 80 questions from MT-Bench with 8 answer combinations each, evaluating 11,520 pairs across six LLM evaluators and three comparison forms.

## Key Results
- PORTIA improves consistency rates by 47.46% on average across all LLMs and comparison forms
- GPT-3.5 with PORTIA achieves 88% agreement with GPT-4 at only 10% of the cost
- Human evaluations confirm PORTIA-enhanced GPT-3.5 better aligns with human judgments than standalone GPT-4
- Performance gains are consistent across six diverse LLMs (GPT-4, GPT-3.5, Claude2, Llama2, Qwen, Chatglm2)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Splitting answers into segments reduces the cognitive load on the LLM evaluator, mitigating position bias.
- **Mechanism:** Human readers break long texts into smaller units for comparison; PORTIA mimics this by splitting answers at sentence boundaries, aligning segments by length, and then merging them into a single prompt.
- **Core assumption:** Position bias in LLMs stems from processing long, complex answer pairs as a whole rather than comparing content in smaller chunks.
- **Evidence anchors:**
  - [abstract] "Inspired by human long-text reading strategies, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs."
  - [section 3.1] "It is worth noting that both human evaluators and LLMs encounter difficulties in making consistent evaluations when faced with lengthy and intricate answers... A common cognitive approach among individuals is to decompose information into smaller units, thereby simplifying the comparison process."
  - [corpus] Weak evidence: Corpus shows related works on "Confirmation bias: A challenge for scalable oversight" and "Benchmarking Cognitive Biases in Large Language Models as Evaluators" but does not directly link to segment-based bias mitigation.
- **Break condition:** If segment boundaries cut through semantically important phrases or code blocks, the alignment may introduce noise rather than clarity.

### Mechanism 2
- **Claim:** Length alignment of segments ensures that corresponding parts of the answers are directly comparable.
- **Mechanism:** PORTIA first splits each answer into k equal-length segments based on character count and then selects the nearest sentence boundary split positions for each segment.
- **Core assumption:** Position bias is amplified when segments differ greatly in length, causing the evaluator to favor one answer due to length cues rather than content quality.
- **Evidence anchors:**
  - [section 3.2] "Segmenting at sentence breaks (e.g., periods or question marks) reduces the likelihood of producing incomplete words or fragmented syntactic units in different segments."
  - [section 4.3] "As the number of answer segments k increases, the average input length for LLM evaluators also grows correspondingly... Consequently, the additional input length scales as O(K)."
  - [corpus] No direct corpus evidence linking length alignment to bias mitigation; evidence is theoretical and based on algorithmic design.
- **Break condition:** If length alignment is too rigid, it may force unnatural splits that obscure meaning and worsen evaluation consistency.

### Mechanism 3
- **Claim:** Semantic alignment of segments further refines comparison by maximizing overlap in content between corresponding segments.
- **Mechanism:** If length alignment does not yield consistent verdicts, PORTIA iteratively searches for split positions that maximize token overlap (or semantic similarity) between corresponding segments across answers.
- **Core assumption:** Semantic similarity between segments correlates with the evaluator's ability to make consistent judgments, and maximizing this overlap reduces noise from irrelevant differences.
- **Evidence anchors:**
  - [section 3.2] "we aim to iteratively search for the optimal split positions that maximize the cumulative semantic similarity between corresponding segments of the two answers."
  - [section 4.5] "both semantic and length alignment confer improvements to PORTIA's performance... semantic alignment shows a greater contribution to enhancing the likert-based form."
  - [corpus] No direct corpus evidence; similarity metric choice (token overlap vs. LM-based) is justified by trade-off between cost and marginal gain.
- **Break condition:** If semantic alignment overemphasizes surface-level overlap, it may ignore deeper structural or logical differences that matter for quality judgment.

## Foundational Learning

- **Concept:** Position bias in LLM evaluators
  - **Why needed here:** Understanding position bias is essential to see why PORTIA's alignment approach improves consistency.
  - **Quick check question:** What is the formal definition of position bias in pairwise LLM evaluation?
- **Concept:** Sentence boundary detection and segmentation
  - **Why needed here:** PORTIA relies on splitting at sentence boundaries to preserve content and order; knowledge of NLP segmentation is key.
  - **Quick check question:** How does PORTIA handle sentence boundaries in code blocks differently from natural language?
- **Concept:** Token overlap vs. semantic similarity metrics
  - **Why needed here:** PORTIA uses token overlap as a lightweight similarity measure; understanding trade-offs with LM-based metrics is important for tuning.
  - **Quick check question:** Why does PORTIA choose token overlap over more complex LM-based similarity metrics?

## Architecture Onboarding

- **Component map:** Question + two answers → Format detection → Length alignment → (optional) Semantic alignment → Merge segments → LLM evaluation → Verdict extraction
- **Critical path:** Question → Format detection → Length alignment → LLM evaluation → (optional) Semantic alignment → LLM evaluation → Verdict extraction
- **Design tradeoffs:**
  - k (number of segments) vs. computational cost (exponential in k)
  - Token overlap (cheap) vs. LM-based similarity (expensive but potentially more accurate)
  - Order preservation vs. forced splits in very long or complex answers
- **Failure signatures:**
  - Inconsistent verdicts persist even after both alignments → model bias too strong or segmentation inappropriate
  - Excessive token overhead → k too high for given answer lengths
  - Semantic alignment yields no improvement → answers are too dissimilar or segments too short
- **First 3 experiments:**
  1. Baseline: Evaluate PORTIA with k=1 (no splitting) on a small subset of MT-Bench to confirm position bias persists.
  2. Length alignment only: Set k=3, use only length-based splitting, measure consistency improvement across models.
  3. Full pipeline: Enable semantic alignment, test on likert-based form where PORTIA shows largest gains, verify fixed coverage increase.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of k for balancing efficiency and precision in PORTIA's semantic alignment?
- Basis in paper: [explicit] The paper conducts a controlled experiment to identify the optimal k value and finds that k = 3 strikes a balance between efficiency and precision.
- Why unresolved: The optimal k value may depend on the specific characteristics of the answers being evaluated, such as their length, complexity, and content. The paper's experiment may not have covered all possible scenarios.
- What evidence would resolve it: Further experiments varying answer characteristics and measuring the impact on PORTIA's performance with different k values could provide more insights into the optimal k value for different scenarios.

### Open Question 2
- Question: How does the choice of similarity metric (e.g., token overlap vs. LM-based metrics) impact PORTIA's performance?
- Basis in paper: [explicit] The paper mentions considering LM-based metrics like Sentence-BERT but argues that intricate metrics are not necessary for PORTIA as they introduce extra computing resources and hyperparameters while yielding only marginal improvements.
- Why unresolved: The paper does not provide a comprehensive comparison of different similarity metrics and their impact on PORTIA's performance. The claim about marginal improvements may not hold true for all cases.
- What evidence would resolve it: Experiments comparing PORTIA's performance using different similarity metrics on a diverse set of answer pairs could provide insights into the impact of metric choice on performance and efficiency.

### Open Question 3
- Question: How does PORTIA's performance generalize to other types of evaluation tasks beyond pairwise answer comparison?
- Basis in paper: [inferred] PORTIA is designed specifically for pairwise answer comparison, and its effectiveness is demonstrated on this task. However, the paper does not explore its applicability to other evaluation tasks.
- Why unresolved: The paper focuses on pairwise answer comparison, and it is unclear how well PORTIA would perform on other evaluation tasks such as scoring individual answers or comparing different types of content (e.g., code vs. text).
- What evidence would resolve it: Experiments applying PORTIA to different evaluation tasks and comparing its performance to other methods would provide insights into its generalizability and potential limitations.

## Limitations

- The study relies on a fixed dataset (MT-Bench) with 80 questions and 8 answer combinations per question, which may not fully represent real-world evaluation diversity
- Position bias mitigation effectiveness may vary across different LLM architectures and training paradigms not covered in this study
- The paper does not address potential introduction of new biases through the segmentation and alignment process itself
- Cost measurements are based on API pricing models that may not reflect actual computational resource usage

## Confidence

- **High Confidence**: Position bias exists in LLM evaluators and negatively impacts consistency; PORTIA's segmentation approach is technically sound
- **Medium Confidence**: PORTIA consistently improves evaluator consistency across all tested LLMs and comparison forms; the cost-benefit ratio is favorable
- **Medium Confidence**: Human evaluation confirms PORTIA-enhanced GPT-3.5 better aligns with human judgments than standalone GPT-4

## Next Checks

1. Test PORTIA on a larger, more diverse dataset (e.g., combining multiple benchmarks) to verify generalizability across different domains and question types
2. Conduct ablation studies to isolate the individual contributions of length alignment versus semantic alignment to consistency improvements
3. Measure the impact of PORTIA on evaluation quality for code-specific tasks where sentence boundary detection may be less reliable