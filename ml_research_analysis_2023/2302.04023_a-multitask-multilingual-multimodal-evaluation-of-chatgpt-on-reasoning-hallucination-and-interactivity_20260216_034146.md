---
ver: rpa2
title: A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination,
  and Interactivity
arxiv_id: '2302.04023'
source_url: https://arxiv.org/abs/2302.04023
tags:
- chatgpt
- reasoning
- language
- tasks
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive technical evaluation of ChatGPT
  across multiple NLP tasks, languages, and modalities. The authors systematically
  test ChatGPT on 21 datasets covering 8 major NLP tasks, comparing its zero-shot
  performance against state-of-the-art models and fine-tuned systems.
---

# A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity

## Quick Facts
- arXiv ID: 2302.04023
- Source URL: https://arxiv.org/abs/2302.04023
- Reference count: 40
- Key outcome: ChatGPT outperforms previous zero-shot LLMs on 9 out of 13 datasets and even surpasses fine-tuned models on 4 tasks

## Executive Summary
This paper presents a comprehensive technical evaluation of ChatGPT across multiple NLP tasks, languages, and modalities. The authors systematically test ChatGPT on 21 datasets covering 8 major NLP tasks, comparing its zero-shot performance against state-of-the-art models and fine-tuned systems. Key findings include: ChatGPT outperforms previous zero-shot LLMs on most tasks and even outperforms fully fine-tuned models on some tasks; it performs well on high-resource languages but struggles with low-resource and non-Latin script languages; it can generate multimodal content through code generation; and it achieves 64.33% accuracy across 10 reasoning categories, showing better performance in deductive and commonsense reasoning than in mathematical or spatial reasoning. The study also reveals ChatGPT's limitations in hallucination (particularly extrinsic hallucinations) and highlights how its interactive dialog capability can improve performance by 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation through multi-turn prompt engineering.

## Method Summary
The evaluation uses zero-shot prompting with carefully engineered prompts across 21 datasets covering 8 NLP tasks. The authors sample 30-200 instances from standard test sets for each task, execute prompts through ChatGPT API, and collect outputs. Task-specific metrics (ROUGE, ChrF++, F1, accuracy, BLEU, etc.) are calculated for evaluated tasks, while reasoning tasks undergo manual accuracy checking. The study also includes custom multimodal flag-drawing tasks and multi-turn interaction experiments to assess interactivity improvements. Manual evaluation is used for hallucination detection and reasoning accuracy assessment.

## Key Results
- ChatGPT outperforms previous zero-shot LLMs on 9 out of 13 evaluation datasets
- Achieves 64.33% average accuracy across 10 reasoning categories
- Shows 8% ROUGE-1 improvement on summarization and 2% ChrF++ improvement on machine translation through multi-turn interaction
- Performs well on high-resource languages but struggles with low-resource and non-Latin script languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's interactive dialog capability enables iterative refinement that improves task performance.
- Mechanism: Through multi-turn conversation, ChatGPT can receive feedback, correct errors, and refine outputs based on user input, mimicking a form of prompt engineering.
- Core assumption: The underlying LLM retains context across turns and can use that context to improve subsequent generations.
- Evidence anchors:
  - [abstract] "Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn 'prompt engineering' fashion."
  - [section] "We find that ChatGPT can generate plausible national flags using the SVG format. To better understand the behavior of ChatGPT, we perform an ablation study by removing the description generation step. As illustrated by Figure 2, the performance drops dramatically without first prompting the textual flag description, which is generated by ChatGPT itself."
  - [corpus] Weak evidence - no direct corpus support for interactive refinement mechanisms specifically.
- Break Condition: If context window is too short or if the model fails to properly incorporate feedback, iterative refinement will not occur.

### Mechanism 2
- Claim: ChatGPT's zero-shot performance on multiple NLP tasks surpasses previous state-of-the-art models.
- Mechanism: The large-scale pre-training and fine-tuning with human feedback enables ChatGPT to generalize well to unseen tasks without specific fine-tuning.
- Core assumption: The model's parametric knowledge is sufficiently comprehensive and well-organized to handle diverse tasks.
- Evidence anchors:
  - [abstract] "We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fully fine-tuned models on some tasks."
  - [section] "ChatGPT is shown to achieve remarkable zero-shot performances on multiple tasks, surpassing previous state-of-the-art zero-shot models on 9 out of 13 evaluation datasets with reported zero-shot LLMs performance."
  - [corpus] Weak evidence - while related papers exist on zero-shot learning, direct comparison to ChatGPT's performance is lacking.
- Break Condition: If tasks require highly specialized knowledge or reasoning that wasn't captured in pre-training data, zero-shot performance will degrade.

### Mechanism 3
- Claim: ChatGPT's reasoning abilities vary significantly across different types of reasoning tasks.
- Mechanism: The model's performance on reasoning tasks depends on the nature of the reasoning required, with better performance on deductive and commonsense reasoning compared to inductive and mathematical reasoning.
- Core assumption: Different reasoning types rely on different cognitive processes that may or may not be well-represented in the training data.
- Evidence anchors:
  - [abstract] "We find that ChatGPT is 64.33% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner."
  - [section] "ChatGPT is a lazy reasoner that suffers more with induction... ChatGPT performs better deductive and abductive reasoning compared to inductive reasoning."
  - [corpus] Moderate evidence - related papers exist on LLM reasoning capabilities, but specific comparisons to ChatGPT are limited.
- Break Condition: If reasoning tasks require complex multi-hop reasoning or non-textual understanding, performance will significantly degrade.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Understanding how ChatGPT can perform well on tasks without specific fine-tuning is crucial to evaluating its capabilities.
  - Quick check question: Can you explain the difference between zero-shot, few-shot, and fine-tuned learning approaches?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a key component of ChatGPT's training that aligns its outputs with human preferences.
  - Quick check question: How does RLHF differ from traditional supervised learning approaches?

- Concept: Hallucination in language models
  - Why needed here: Understanding the types and causes of hallucination is essential for evaluating ChatGPT's factuality.
  - Quick check question: What's the difference between intrinsic and extrinsic hallucinations in language models?

## Architecture Onboarding

- Component map: Pre-training on large corpus of text and code -> Fine-tuning with human feedback (RLHF) -> Dialog interface for multi-turn interaction -> Context window for maintaining conversation state
- Critical path: User input → Context processing → Generation → Output refinement (if multi-turn)
- Design tradeoffs: Larger context windows improve multi-turn capabilities but increase computational cost; more RLHF data improves alignment but requires significant human effort; tradeoff between parametric knowledge and external knowledge sources
- Failure signatures: Performance degradation on low-resource languages; hallucination in knowledge-intensive tasks; inconsistent reasoning performance across different task types
- First 3 experiments: 1) Test zero-shot performance on a new task not covered in evaluation; 2) Evaluate multi-turn refinement capabilities on a summarization task; 3) Assess reasoning performance on a mathematical reasoning dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which ChatGPT's RLHF training enables its superior zero-shot performance across multiple NLP tasks compared to previous LLMs?
- Basis in paper: [explicit] The paper states ChatGPT is trained with a better human-aligned objective function via RLHF and was trained on a large-scale conversational-style dataset, but does not detail the specific mechanisms.
- Why unresolved: The paper does not provide empirical evidence or ablation studies showing how RLHF specifically contributes to improved zero-shot performance versus other training methods.
- What evidence would resolve it: Comparative experiments showing zero-shot performance of ChatGPT variants trained with and without RLHF, or controlled studies isolating the effects of RLHF versus scale of pretraining data.

### Open Question 2
- Question: How does ChatGPT's ability to generate multimodal content through code generation compare to specialized vision-language models in terms of quality and accuracy?
- Basis in paper: [explicit] The paper describes ChatGPT's ability to generate multimodal content via code generation, but notes the quality is "elementary compared to vision-language models."
- Why unresolved: The paper does not provide quantitative comparisons or systematic evaluations against vision-language models on standard multimodal benchmarks.
- What evidence would resolve it: Head-to-head evaluations of ChatGPT's code-generated images against outputs from models like DALL-E, Stable Diffusion, or CLIP on standardized image quality and accuracy metrics.

### Open Question 3
- Question: What are the specific limitations of ChatGPT's reasoning capabilities in complex multi-step problems, and how do these compare to other large language models?
- Basis in paper: [explicit] The paper finds ChatGPT lacks spatial reasoning and performs poorly on multi-hop reasoning, but does not provide detailed error analysis or comparisons to other models.
- Why unresolved: The paper tests reasoning abilities but does not conduct error analysis to understand why ChatGPT fails on certain reasoning types or how its limitations compare to similar models.
- What evidence would resolve it: Detailed error analysis categorizing failure modes for different reasoning types, and systematic comparisons of ChatGPT's reasoning performance against other large language models on the same tasks.

## Limitations

- Manual evaluation introduces subjectivity and reproducibility challenges for hallucination detection and reasoning accuracy
- Evaluation covers only 21 datasets across 8 tasks, leaving many NLP domains unexplored
- Focuses on ChatGPT specifically without comparing against other contemporary LLMs

## Confidence

- **High Confidence**: ChatGPT's superior zero-shot performance on high-resource languages and standard NLP tasks (based on measurable metrics like ROUGE and ChrF++)
- **Medium Confidence**: Findings on multilingual generalization and reasoning abilities (limited by sample size and manual evaluation)
- **Low Confidence**: Claims about multimodal generation capabilities and interactivity improvements (based on single custom task and limited ablation studies)

## Next Checks

1. **Replication on Additional Languages**: Test ChatGPT's performance on additional low-resource and non-Latin script languages not covered in the original evaluation to validate the generalizability of multilingual findings.

2. **Multi-LLM Comparison**: Conduct the same evaluation across multiple contemporary LLMs (e.g., Claude, PaLM, LLaMA) to determine if ChatGPT's performance advantages are unique or represent broader LLM capabilities.

3. **Automated Hallucination Detection**: Implement automated hallucination detection methods (e.g., fact-checking against knowledge bases) to supplement manual evaluation and improve reproducibility of hallucination findings.