---
ver: rpa2
title: Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning
arxiv_id: '2305.16031'
source_url: https://arxiv.org/abs/2305.16031
tags:
- learning
- document
- bregman
- divergence
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for learning long document embeddings
  using a combination of self-contrastive learning (SimCSE) and Bregman divergence
  loss with ensemble neural networks. The approach addresses the challenge of efficiently
  encoding long documents (thousands of words) by training Longformer-based encoders
  with unsupervised contrastive learning and further enhancing the representations
  using Bregman divergence to promote diversity in the latent features.
---

# Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning

## Quick Facts
- arXiv ID: 2305.16031
- Source URL: https://arxiv.org/abs/2305.16031
- Authors: 
- Reference count: 18
- Key outcome: Proposed method improves long document classification with 2.6% average macro-F1 gain over SimCSE baseline

## Executive Summary
This paper addresses the challenge of efficiently encoding long documents (thousands of words) by proposing a method that combines self-contrastive learning with Bregman divergence loss. The approach uses domain-adapted Longformer models trained with SimCSE to create document representations, then enhances these representations using an ensemble of neural networks optimized with functional Bregman divergence. Experiments on three long document classification tasks from legal and biomedical domains show significant improvements over baseline methods while requiring only 0.5% of trainable parameters and 2-8× less training time compared to end-to-end fine-tuning.

## Method Summary
The method trains Longformer-based document encoders using SimCSE self-contrastive learning with dropout-based augmentation, then enhances representations through k independent ensemble subnetworks optimized with functional Bregman divergence. Domain-adapted Longformer models (LongformerDA) are initialized from Legal-BERT or BioBERT and extended to handle 4096-token sequences. The contrastive framework creates positive pairs from augmented document views and uses other documents as negatives, while the Bregman divergence promotes orthogonality between ensemble subnetworks to prevent feature collapse. Final document representations are obtained through mean pooling and used with linear classifier heads for downstream tasks.

## Key Results
- 2.6% average improvement in macro-F1 score over SimCSE baseline across three long document classification tasks
- Requires only 0.5% of trainable parameters compared to end-to-end fine-tuning
- Achieves 2-8× less training time while maintaining or improving performance
- Consistent improvements across ECtHR (legal), SCOTUS (legal), and MIMIC (biomedical) datasets

## Why This Works (Mechanism)

### Mechanism 1
Self-contrastive learning with SimCSE improves long document representations by creating augmented views through dropout and using all other documents in a batch as negative pairs. This enhances discriminative features by pulling positive pairs closer while pushing apart negative pairs in embedding space. The core assumption is that dropout-based augmentation creates meaningfully different views that capture complementary information. Break condition: If dropout destroys too much semantic information, positive pairs may not be meaningfully related.

### Mechanism 2
Bregman divergence loss enhances feature diversity by promoting orthogonality between ensemble subnetworks. The document embedding passes through k independent subnetworks optimized using functional Bregman divergence, which measures differences between probability distributions of features. This encourages each subnetwork to learn distinct, non-redundant representations. Core assumption: Long documents contain multiple topics better captured by distributed representations across specialized subnetworks. Break condition: Too few subnetworks provide insufficient diversity; too many may overfit or learn redundant features.

### Mechanism 3
Domain-adapted Longformer provides better initialization for long document tasks compared to general-purpose transformers. Models are initialized from domain-specific PLMs (Legal-BERT for legal, BioBERT for biomedical) with positional embeddings cloned 8× to handle 4096 tokens. This warm-start leverages domain knowledge while maintaining efficiency. Core assumption: Domain-specific pre-training captures relevant linguistic patterns and terminology. Break condition: If domain gap is too large, warm-start may introduce bias rather than benefit.

## Foundational Learning

- **Self-supervised contrastive learning**: Needed to learn representations without expensive labeled data by comparing augmented versions of the same document against other documents. Quick check: What is the difference between positive pairs and negative pairs in contrastive learning?

- **Bregman divergence**: Provides principled way to measure and maximize differences between feature distributions, addressing potential feature collapse in standard contrastive losses. Quick check: How does Bregman divergence differ from KL divergence in measuring distribution differences?

- **Sparse attention mechanisms**: Essential for handling long documents where standard Transformers have prohibitive O(n²) complexity; sparse attention reduces this to O(n×w). Quick check: What is the computational complexity of sliding window attention compared to full attention?

## Architecture Onboarding

- **Component map**: Document input → LongformerDA encoding → dropout augmentation → contrastive loss computation → Bregman divergence computation → parameter updates
- **Critical path**: Document input → LongformerDA encoding → dropout augmentation → contrastive loss computation → Bregman divergence computation → parameter updates
- **Design tradeoffs**: Domain-adapted models improve initialization but add dependency on domain-specific PLMs; Bregman divergence adds training complexity but may improve feature diversity; ensemble approach increases parameter count but reduces overfitting risk
- **Failure signatures**: If training loss decreases but downstream performance doesn't improve, representations may not capture task-relevant information; if Bregman divergence loss dominates, feature diversity may be excessive and harm semantic coherence
- **First 3 experiments**:
  1. Train LongformerDA with SimCSE only on ECtHR data subset to verify contrastive learning works for long documents
  2. Add single Bregman subnetwork to verify divergence loss computation and integration with contrastive loss
  3. Scale to k subnetworks and evaluate feature diversity metrics (pairwise cosine similarity between subnetwork outputs)

## Open Questions the Paper Calls Out

- **How would the proposed method perform with different model sizes or architectures, such as GPT models?** The authors focus on small and medium-sized models and acknowledge uncertainty about performance with other model sizes or architectures like GPT models. [explicit]

- **How does the Bregman divergence loss impact performance in other NLP tasks beyond long document classification, such as document retrieval or ranking?** The authors mention that it is unclear how findings may translate to other NLP tasks like document retrieval/ranking and suggest investigating these directions. [inferred]

- **How does the choice of pooling method affect the proposed method's performance in different stages?** While authors evaluate different pooling methods and find mean pooling during pre-training with max-pooling for classification enhances performance, they don't fully explore optimal combinations for different model stages. [explicit]

## Limitations

- Limited architectural details about Bregman divergence subnetworks make exact reproduction challenging
- Small sample size of domains (3) may limit generalization to other document types
- Computational overhead of Bregman divergence ensemble not explicitly quantified against alternatives

## Confidence

- **High Confidence**: Empirical results showing 2.6% macro-F1 improvement are well-supported with clear methodology
- **Medium Confidence**: Training efficiency claims (0.5% parameters, 2-8× less time) are plausible but need verification of Bregman overhead
- **Low Confidence**: Specific mechanism by which Bregman divergence enhances feature diversity lacks direct empirical validation

## Next Checks

1. **Architecture Replication Check**: Implement exact Bregman divergence subnetwork architecture and verify parameter efficiency claims through systematic counting

2. **Feature Diversity Validation**: Conduct controlled experiments measuring pairwise cosine similarity between ensemble outputs to verify Bregman divergence promotes orthogonality

3. **Computational Overhead Analysis**: Measure actual training time and memory usage of full method versus SimCSE baseline, including all components, to validate efficiency claims