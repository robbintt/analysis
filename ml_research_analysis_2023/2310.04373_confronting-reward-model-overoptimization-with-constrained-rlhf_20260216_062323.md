---
ver: rpa2
title: Confronting Reward Model Overoptimization with Constrained RLHF
arxiv_id: '2310.04373'
source_url: https://arxiv.org/abs/2310.04373
tags:
- reward
- evaluation
- proxy
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies reward model overoptimization in composite\
  \ reward functions, where maximizing individual reward models beyond a certain point\
  \ leads to worse human evaluation. The authors show that correlation between component\
  \ rewards affects the locations of proxy points\u2014the values beyond which reward\
  \ models cease being effective proxies for human preferences."
---

# Confronting Reward Model Overoptimization with Constrained RLHF

## Quick Facts
- arXiv ID: 2310.04373
- Source URL: https://arxiv.org/abs/2310.04373
- Reference count: 40
- Key outcome: Constrained RLHF methods prevent reward model overoptimization by dynamically learning Lagrange multipliers and identifying proxy points during training

## Executive Summary
This paper addresses the problem of reward model overoptimization in composite reward functions, where maximizing individual reward models beyond certain thresholds leads to worse human evaluation performance. The authors propose using constrained reinforcement learning to prevent overoptimization by treating proxy points—values beyond which reward models cease being effective proxies for human preferences—as constraint thresholds. Their approach uses dynamic Lagrange multipliers to weight component rewards and ensures each stays within its effective range, improving evaluation performance compared to standard PPO.

## Method Summary
The authors study reward model overoptimization in composite reward functions and propose constrained RL approaches that incorporate proxy points as constraint thresholds. They implement several constrained PPO variants (μ-PPO, ξ-PPO, All-PPO) that use Lagrange multipliers to dynamically weight component rewards, preventing overoptimization. Additionally, they introduce an adaptive Nelder-Mead optimization approach to identify proxy points during a single training run by periodically evaluating different threshold combinations. The method is tested on dialogue generation using GPT-2 and the DailyDialog dataset with METEOR and intent matching as component rewards.

## Key Results
- Constrained RLHF methods achieve better evaluation performance than standard PPO
- Constrained approaches are more robust to longer training durations
- Dynamic Lagrange multiplier weighting outperforms fixed-weight PPO
- Nelder-Mead optimization can identify effective proxy thresholds during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained RL prevents reward model overoptimization by capping component rewards at proxy points.
- Mechanism: The Lagrange multiplier approach enforces inequality constraints (via sigmoid-bounded weights) that limit each component RM's influence once its proxy point is reached, preventing further optimization beyond the proxy threshold.
- Core assumption: Each component RM's proxy point accurately reflects where further reward accumulation degrades evaluation performance.
- Evidence anchors:
  - [abstract] "We propose several constrained RL approaches which incorporate these points into the optimization objectives, preventing overoptimization and improving evaluation performance."
  - [section] "We propose that a useful approach to doing this is to reformulate the optimization objective using constraints."
  - [corpus] Weak - related work discusses reward overoptimization but doesn't provide direct evidence for proxy point accuracy.
- Break condition: If proxy points are inaccurately estimated (too high/low), the constraints may be ineffective or overly restrictive, leading to suboptimal or collapsed performance.

### Mechanism 2
- Claim: Dynamic weighting via learned Lagrange multipliers improves performance over fixed-weight PPO.
- Mechanism: Instead of pre-setting α values, the method jointly optimizes policy and Lagrange multipliers, allowing the model to adapt component weights based on real-time constraint satisfaction.
- Core assumption: Joint optimization of policy and multipliers leads to better saddle point convergence than fixed-weight approaches.
- Evidence anchors:
  - [abstract] "Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally given by the Lagrange multipliers."
  - [section] "Rather than use a fixed weighting among components, our method dynamically adapts a weighting to modulate the influence of each RM on the learning process."
  - [corpus] Weak - related work mentions reward model ensembles but lacks direct comparison to dynamic Lagrange-based weighting.
- Break condition: If the non-stationarity introduced by multiplier updates causes instability, performance may degrade despite theoretical convergence guarantees.

### Mechanism 3
- Claim: Nelder-Mead optimization during a single training run can identify effective proxy thresholds without multiple expensive training runs.
- Mechanism: The derivative-free optimizer iteratively proposes threshold pairs, trains to those thresholds using ξ-PPO, and evaluates performance to find better threshold combinations.
- Core assumption: The evaluation landscape is smooth enough for Nelder-Mead to find good local optima within reasonable iterations.
- Evidence anchors:
  - [section] "We introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run."
  - [section] "we can periodically query the ground-truth reward model and use this data to run a derivative-free optimization algorithm to find the next candidate proxy points."
  - [corpus] Weak - related work focuses on reward model ensembles but doesn't address proxy point identification via Nelder-Mead.
- Break condition: If the feasible threshold region is too small or the evaluation surface too noisy, Nelder-Mead may converge to poor local optima or fail to make progress.

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: The paper frames the reward overoptimization problem as a CMDP where proxy points are treated as constraint thresholds.
  - Quick check question: What is the key difference between a standard MDP and a CMDP in terms of optimization objective?

- Concept: Lagrangian relaxation for constrained optimization
  - Why needed here: The method converts the CMDP into a min-max game by introducing Lagrange multipliers to enforce constraints.
  - Quick check question: How does the negative gradient with respect to a Lagrange multiplier relate to constraint satisfaction?

- Concept: Proximal Policy Optimization (PPO) and KL regularization
  - Why needed here: The base algorithm for both standard and constrained RLHF approaches is PPO with adaptive KL penalty.
  - Quick check question: What role does the KL penalty play in preventing the policy from deviating too far from the pretrained model?

## Architecture Onboarding

- Component map:
  GPT-2 -> Dialogue responses -> Component RMs (METEOR, Intent) -> Rewards -> Proximal Policy Optimizer -> Policy updates
  Lagrange multiplier optimizer -> RM weights adjustment
  Nelder-Mead optimizer (optional) -> Proxy threshold search

- Critical path:
  1. Initialize policy and value networks
  2. Sample context, generate response using current policy
  3. Compute component rewards and advantages
  4. Update policy using mixed advantages (weighted by Lagrange multipliers)
  5. Update Lagrange multipliers based on constraint violations
  6. (Optional) Periodically run Nelder-Mead to propose new thresholds

- Design tradeoffs:
  - Fixed vs. dynamic RM weighting: Fixed is simpler but may be suboptimal; dynamic requires more computation and can introduce instability
  - Equality vs. inequality constraints: Equality ensures exact threshold hitting but may be harder to satisfy; inequality is more flexible but risks exceeding proxy points
  - Single vs. multiple training runs: Multiple runs allow better proxy point estimation but are computationally expensive; single run with Nelder-Mead saves compute but may converge to suboptimal thresholds

- Failure signatures:
  - Oscillating Lagrange multipliers -> non-convergence of saddle point
  - Policy KL divergence growing too large -> deviation from pretrained model
  - Component rewards exceeding proxy points -> reward overoptimization
  - Nelder-Mead simplex stagnating -> poor threshold selection or noisy evaluation

- First 3 experiments:
  1. Run PPO with fixed α weights to establish baseline performance and identify proxy points
  2. Implement ξ-PPO with proxy point thresholds and verify constraint satisfaction via monitoring
  3. Add Nelder-Mead proxy search during training and compare final evaluation scores to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do proxy points vary across different datasets and domains beyond dialogue generation?
- Basis in paper: [explicit] The paper mentions this is a "case study" on dialogue generation with DailyDialog dataset, and notes "further testing of our methods is necessary on more domains and with composite RMs with more components."
- Why unresolved: The paper only provides results for one dataset (DailyDialog) and two component rewards (METEOR and intent). The authors acknowledge this limitation and call for broader testing.
- What evidence would resolve it: Empirical studies applying the constrained RLHF approach to multiple datasets (e.g., summarization, code generation) and with varying numbers of component rewards would demonstrate generalizability.

### Open Question 2
- Question: Can proxy points be identified without any access to ground-truth evaluation metrics?
- Basis in paper: [explicit] The authors state "at least some minimal degree of access to the true objective/evaluation metric is required" and note this is "expensive," but ideally "this would be dispensed with entirely."
- Why unresolved: All current methods require periodic evaluation queries. The paper doesn't propose or evaluate any methods that completely eliminate this requirement.
- What evidence would resolve it: A method that identifies proxy points through unsupervised or self-supervised means, validated to produce comparable performance to the constrained approaches without requiring ground-truth evaluation queries.

### Open Question 3
- Question: How does the choice of gradient-free optimizer affect proxy point identification efficiency and effectiveness?
- Basis in paper: [explicit] The paper uses Nelder-Mead for proxy point search and notes "it would be interesting to explore alternative optimizers to Nelder-Mead, such as Bayesian optimization."
- Why unresolved: Only one gradient-free optimizer (Nelder-Mead) was evaluated. The authors acknowledge this limitation and suggest alternatives.
- What evidence would resolve it: Comparative studies of proxy point identification performance across multiple gradient-free optimizers (Bayesian optimization, genetic algorithms, etc.) on the same problem, measuring both computational efficiency and final performance.

## Limitations

- Weak empirical evidence supporting the correlation between component rewards and proxy point locations
- Constrained methods show tradeoffs with lower diversity scores compared to standard PPO
- Limited testing to a single dataset and two component rewards

## Confidence

**High Confidence:**
- The existence of reward model overoptimization in composite reward functions is well-established in the RLHF literature
- Constrained RL approaches can theoretically prevent overoptimization by capping component rewards

**Medium Confidence:**
- Dynamic Lagrange multiplier weighting improves performance over fixed weights (supported by results but with noted tradeoffs)
- The proxy point identification mechanism works as described (weak empirical support)

**Low Confidence:**
- Correlation between component rewards directly affects proxy point locations (no supporting evidence found)
- Nelder-Mead optimization can reliably identify good proxy points during training (unclear from results)

## Next Checks

1. **Validation of Proxy Point Correlation Claims**: Design an experiment varying correlation levels between component rewards and systematically measuring resulting proxy point locations. This would directly test the core assumption about correlation effects.

2. **Constraint Satisfaction Monitoring**: During constrained RL training, track whether component rewards actually stay within proxy point bounds throughout training. If constraints are frequently violated, this would indicate implementation issues with the Lagrange multiplier approach.

3. **Nelder-Mead Convergence Analysis**: Run multiple training trials with different random seeds and analyze the consistency of proxy points found by Nelder-Mead. Poor consistency would suggest the method is unreliable for identifying effective thresholds.