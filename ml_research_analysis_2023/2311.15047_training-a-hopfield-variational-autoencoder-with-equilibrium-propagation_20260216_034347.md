---
ver: rpa2
title: Training a Hopfield Variational Autoencoder with Equilibrium Propagation
arxiv_id: '2311.15047'
source_url: https://arxiv.org/abs/2311.15047
tags:
- equilibrium
- decoder
- hopfield
- encoder
- propagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the application of Equilibrium Propagation (EP)
  to the generative setting by introducing a Hopfield Variational Autoencoder (VAE)
  trained via EP. The authors leverage the symmetric nature of Hopfield networks to
  use the same network for both encoding and decoding, enabling more energy-efficient
  analog hardware implementations.
---

# Training a Hopfield Variational Autoencoder with Equilibrium Propagation

## Quick Facts
- arXiv ID: 2311.15047
- Source URL: https://arxiv.org/abs/2311.15047
- Authors: 
- Reference count: 27
- Key outcome: Fully connected Hopfield VAE achieves FID of 49.6 on MNIST, outperforming feedforward baseline; tied-weight variant achieves FID of 58.8

## Executive Summary
This work introduces a Hopfield Variational Autoencoder (VAE) trained via Equilibrium Propagation (EP), extending EP to generative modeling. The authors leverage the symmetric nature of Hopfield networks to use the same network for both encoding and decoding, potentially halving hardware requirements for analog implementations. Experiments on MNIST demonstrate that fully connected Hopfield VAEs achieve superior Fréchet Inception Distance (FID) scores compared to layered architectures and feedforward baselines. The tied-weight variant shows promise despite some performance degradation, validating the feasibility of using a single network for both encoding and decoding.

## Method Summary
The paper presents four VAE architectures: a feedforward VAE trained with backpropagation, a layered Hopfield VAE, a fully connected Hopfield VAE, and a tied-weight dense Hopfield VAE. Hopfield networks use continuous states and dynamics with an energy function E(s) = 1/2||s||² - 1/2ρ(s)ᵀWρ(s). Training employs Equilibrium Propagation with free phase (equilibrium without loss) and weakly clamped phases (adjusting weights based on reconstruction loss and KL divergence). Hyperparameters were optimized using Bayesian optimization across βKL [0.5, 2], learning rate [1e-5, 1e-3], hidden dimension size [10, 2000], latent dimension size [2, 200], and fixed σ [0.3, 1].

## Key Results
- Fully connected Hopfield VAE achieves FID of 49.6 on MNIST, outperforming feedforward baseline
- Tied-weight dense Hopfield VAE achieves FID of 58.8, demonstrating feasibility of single-network architecture
- Dense connectivity improves performance compared to layered architectures
- EP-trained models achieve competitive results to backpropagation-based models of comparable capacity

## Why This Works (Mechanism)

### Mechanism 1
Equilibrium Propagation enables training of Hopfield VAEs by leveraging local energy minimization dynamics. EP approximates gradients via differences in energy derivatives between weakly clamped phases, avoiding backpropagation through time. The energy function E(s) = 1/2||s||² - 1/2ρ(s)ᵀWρ(s) allows local computation of weight updates from state differences.

### Mechanism 2
Using the same Hopfield network for both encoder and decoder halves hardware requirements. Symmetric undirected connections allow bidirectional information flow without separate networks. The encoder output can be clamped as fixed input to the decoder within the same energy framework.

### Mechanism 3
Dense connectivity improves Hopfield VAE performance compared to layered architectures. Direct pixel-to-pixel connections allow more efficient information flow during energy minimization. Fully connected networks have sufficient parameter capacity to model complex distributions.

## Foundational Learning

- **Variational Autoencoders (VAEs)**: Essential to understand how Hopfield networks can be used for generative modeling. Quick check: What are the two main components of a VAE and their respective roles?
- **Equilibrium Propagation (EP)**: The training algorithm that replaces backpropagation for energy-based models like Hopfield networks. Quick check: How does EP differ from backpropagation through time in terms of computational requirements?
- **Continuous Hopfield Networks (CHNs)**: Provide the energy-based framework that enables both encoding and decoding with the same network. Quick check: What property of CHNs allows them to be used bidirectionally for encoding and decoding?

## Architecture Onboarding

- **Component map**: Input image → Encoder CHN (pixels → µ, log σ²) → Latent sample → Decoder CHN (latent → reconstructed image) → Reconstruction loss
- **Critical path**: 1) Encode input image to latent distribution parameters, 2) Sample from latent distribution, 3) Decode latent sample to reconstructed image, 4) Compute reconstruction loss, 5) Apply EP updates to weights
- **Design tradeoffs**: Dense vs. layered connectivity (dense provides better performance but increases parameter count), tied vs. separate weights (tied weights halve hardware requirements but may reduce performance), fixed vs. learned σ (fixed σ simplifies tied architecture but reduces flexibility)
- **Failure signatures**: Poor reconstruction quality indicates issues with energy minimization or loss computation, vanishing gradients suggest problems with non-linearity choice or network initialization, mode collapse indicates the latent space isn't properly regularized
- **First 3 experiments**: 1) Implement a simple feedforward VAE baseline for comparison, 2) Train a layered Hopfield VAE to verify EP training works, 3) Convert to dense connectivity and compare FID scores

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Hopfield VAEs scale with increased model complexity and parameter count? The paper mentions that larger, more advanced models are needed as VAEs trained through EP still yield poor images. Systematic experiments comparing Hopfield VAEs with increasing model sizes and parameter counts, evaluating their performance on various datasets beyond MNIST, would resolve this.

### Open Question 2
What is the theoretical explanation for why fully connected Hopfield networks outperform layered architectures in the VAE setting? The authors acknowledge this finding is preliminary and requires additional research, providing possible explanations but no definitive theoretical justification. Mathematical analysis of the energy landscape and optimization dynamics in both layered and fully connected Hopfield networks would resolve this.

### Open Question 3
Can Equilibrium Propagation be effectively applied to more complex generative models beyond VAEs, such as diffusion models or GANs? The paper only demonstrates EP on VAEs and acknowledges limitations, suggesting this is a promising direction but not providing evidence for other generative model architectures. Successful implementation and training of diffusion models or GANs using Equilibrium Propagation would resolve this.

## Limitations
- Limited scalability to larger datasets beyond MNIST
- Computational efficiency of EP compared to backpropagation for larger models remains unclear
- Lack of comparison to other generative models like GANs or normalizing flows

## Confidence
- High confidence in feasibility of training Hopfield VAEs with EP (demonstrated by successful training of multiple architectures)
- Medium confidence in superiority of dense connectivity over layered architectures (substantial FID improvement but limited ablation studies)
- Low confidence in tied-weight architecture's ability to match separate encoder-decoder performance (significant FID degradation without exploration of mitigating modifications)

## Next Checks
1. Scale up the dense Hopfield VAE to larger dataset like CIFAR-10 or CelebA and compare performance to other generative models
2. Conduct extensive ablation study on EP hyperparameters (iterations, learning rate schedules) to identify critical factors for successful training
3. Explore architectural modifications to tied-weight Hopfield VAE (skip connections, different latent space parameterization) to improve performance and reduce FID gap