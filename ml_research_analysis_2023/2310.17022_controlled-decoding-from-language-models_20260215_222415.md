---
ver: rpa2
title: Controlled Decoding from Language Models
arxiv_id: '2310.17022'
source_url: https://arxiv.org/abs/2310.17022
tags:
- prefix
- reward
- language
- decoding
- scorer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces controlled decoding (CD), an inference-time
  alignment method for language models that uses a learned prefix scorer to guide
  generation toward higher-reward outcomes. The prefix scorer is trained via a Bellman
  update to estimate expected reward from partially decoded sequences.
---

# Controlled Decoding from Language Models

## Quick Facts
- arXiv ID: 2310.17022
- Source URL: https://arxiv.org/abs/2310.17022
- Reference count: 7
- Key outcome: Controlled decoding (CD) uses learned prefix scorers to guide generation toward higher-reward outcomes, improving safety and response length while maintaining base model fluency via KL regularization.

## Executive Summary
This paper introduces controlled decoding (CD), an inference-time alignment method for language models that uses a learned prefix scorer to guide generation toward higher-reward outcomes. The prefix scorer is trained via a Bellman update to estimate expected reward from partially decoded sequences. At inference, token-wise or block-wise sampling uses the prefix scorer to steer generation while staying close to the base model via KL regularization. Experiments on Reddit dialogues show that CD effectively increases response length and improves safety, with block-wise variants matching the performance of best-of-K reranking but with lower latency. The method is modular and supports multi-objective alignment without retraining.

## Method Summary
Controlled decoding introduces a prefix scorer trained via Bellman updates to estimate expected reward from partially decoded sequences. At inference, this scorer is combined with base model logits using a temperature-like scaling to bias sampling toward higher-reward continuations while maintaining base model fluency via KL regularization. The method supports both token-wise and block-wise sampling, with block-wise variants sampling and reranking fixed-length blocks to reduce inference cost. Multiple prefix scorers can be combined at inference time to solve multi-objective RL without retraining. The approach is evaluated on Reddit dialogues, demonstrating improvements in response length and safety alignment.

## Key Results
- Tokenwise CD effectively improves response length and safety while maintaining base model fluency
- Blockwise CD achieves similar performance to best-of-K reranking but with lower latency
- Multiple prefix scorers can be combined at inference time for multi-objective alignment without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The prefix scorer acts as a learned value function that estimates expected reward from partially decoded sequences, enabling steering without retraining the base model.
- Mechanism: During training, the prefix scorer learns via Bellman updates to predict the expected cumulative reward from any decoding prefix. At inference, this scorer is combined with base model logits using a temperature-like scaling (Equation 15) to bias sampling toward higher-reward continuations while maintaining base model fluency via KL regularization.
- Core assumption: The Bellman identity holds for the token-wise reward structure, and the prefix scorer can approximate the true value function from off-policy data.
- Evidence anchors:
  - [abstract] "The prefix scorer is trained via a Bellman update to estimate expected reward from partially decoded sequences."
  - [section] "Inspired by the policy evaluation updates in DQN (Mnih et al., 2013), we optimize the following loss function: ℓ(x, yt; w) = ‖Vw([x, yt]) − v‖²..."
  - [corpus] Weak - no direct citations found in corpus; mechanism inferred from paper description.
- Break condition: If the Bellman targets are too noisy (e.g., from sparse rewards or model shift), the prefix scorer will fail to learn accurate value estimates, breaking the steering signal.

### Mechanism 2
- Claim: Blockwise CD bridges the gap between best-of-K and tokenwise control by sampling and reranking fixed-length blocks.
- Mechanism: K blocks of M tokens are sampled from the base model. Each block is scored using the prefix scorer, and the highest-scoring block is selected and appended to the sequence. This repeats until EOS is generated, effectively reducing inference cost compared to full best-of-K while retaining control strength.
- Core assumption: Prefix scores are additive or at least comparable across blocks, so reranking blocks approximates reranking full sequences.
- Evidence anchors:
  - [section] "We sample K independent continuation blocks of length M from the base policy... Then we accept the continuation with the highest prefix score and reject the rest."
  - [section] "Note that M → ∞ is effectively the best-of-K strategy. For intermediate M, this bridges the gap..."
  - [corpus] Weak - no direct corpus citations; mechanism described only in paper.
- Break condition: If the prefix scorer's estimate is highly path-dependent or non-stationary across blocks, blockwise selection may yield poor alignment compared to tokenwise control.

### Mechanism 3
- Claim: Multiple prefix scorers can be combined at inference time to solve multi-objective RL without retraining.
- Mechanism: Each objective (e.g., safety, length) has its own prefix scorer. At inference, their scores are linearly combined with tunable weights to form a composite prefix score. This composite score replaces the single-objective prefix score in the sampling rule.
- Core assumption: The objectives are commensurable through linear combination and the base model's KL regularization suffices to prevent over-optimization on any single objective.
- Evidence anchors:
  - [abstract] "We show that prefix scorers for multiple rewards may be combined at inference time, effectively solving a multi-objective RL problem with no additional training."
  - [section] "Next, we combine the safety and length prefix scorers (rewards) to simultaneously improve safety and increase dialog length."
  - [corpus] Weak - no corpus evidence; mechanism inferred from experimental description.
- Break condition: If objectives conflict strongly, linear weighting may be insufficient, leading to degraded performance on one or more objectives.

## Foundational Learning

- Concept: Reinforcement Learning with KL Regularization
  - Why needed here: Provides the theoretical foundation for balancing reward maximization against deviation from the base model, formalized in the token-wise objective J([x, yt]; π, β).
  - Quick check question: What happens to the optimal policy π⋆ when β → 0 versus β → 1 in the KL-regularized RL objective?

- Concept: Bellman Updates for Value Function Learning
  - Why needed here: Enables training the prefix scorer off-policy by bootstrapping future value estimates, analogous to DQN's policy evaluation step.
  - Quick check question: How does the stop-gradient (˙v) in the target v prevent instability during prefix scorer training?

- Concept: Autoregressive Decoding and KL Divergence
  - Why needed here: Token-wise sampling and blockwise reranking both require understanding how KL divergence constrains policy drift and affects fluency.
  - Quick check question: Why is KL divergence measured token-wise rather than sequence-wise in this framework?

## Architecture Onboarding

- Component map:
  - Base LM (frozen) -> Prefix scorer(s) -> Inference controller -> Reward models

- Critical path:
  1. Train prefix scorer(s) offline using Bellman updates on (x, y) pairs.
  2. At inference, for each prefix, compute base logits and prefix score.
  3. Combine via πw(z|[x, yt]) ∝ p(z|[x, yt])e^(1-β)/β Vw([x,yt,z]).
  4. Sample next token or block and repeat until EOS.

- Design tradeoffs:
  - Tokenwise vs blockwise: tokenwise gives finer control but higher latency; blockwise reduces latency but may lose granularity.
  - Single vs multi-objective scorers: multi-objective offers flexibility but requires careful weight tuning.
  - Prefix scorer size: larger scorers may improve accuracy but increase inference cost.

- Failure signatures:
  - Overoptimization: safety/length improvements come with large KL penalty or degraded fluency.
  - Noisy predictions: prefix scorer accuracy drops on out-of-distribution prompts.
  - Training instability: Bellman targets diverge, leading to exploding or vanishing prefix scores.

- First 3 experiments:
  1. Train a prefix scorer for length reward and verify its correlation with actual sequence length on a held-out set.
  2. Apply tokenwise CD with varying β and measure reward improvement vs KL penalty on safety alignment.
  3. Combine length and safety scorers with different weightings and evaluate multi-objective tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the latency and throughput of blockwise CD compare to best-of-K when using large prefix scorers or base models?
- Basis in paper: [inferred] The paper mentions that blockwise CD has similar computational cost to best-of-K but with lower latency (M tokens vs full sequence), and suggests future work to explore latency and throughput tradeoffs with different model sizes.
- Why unresolved: The paper did not empirically compare latency and throughput of blockwise CD vs best-of-K for varying prefix scorer and base model sizes.
- What evidence would resolve it: Controlled experiments measuring latency and throughput of blockwise CD and best-of-K for different prefix scorer and base model sizes, with varying M and K values.

### Open Question 2
- Question: What causes the poor generalization of token-wise CD and FUDGE for safety alignment, even when using the same reward model for training and evaluation?
- Basis in paper: [explicit] The paper observes that token-wise CD and FUDGE show poor safety improvement when using an independent reward model for evaluation, despite using the same model for training. It suggests this may be due to overoptimization or noisy training data.
- Why unresolved: The paper does not provide a definitive explanation for the poor generalization, only speculating about potential causes.
- What evidence would resolve it: Detailed analysis of the learned prefix scorers to identify specific failure modes, and experiments varying training data quality, reward model architecture, and regularization techniques to mitigate overoptimization.

### Open Question 3
- Question: Can the blockwise CD method be further improved to match or exceed the performance of best-of-K in terms of reward-KL tradeoffs?
- Basis in paper: [explicit] The paper observes that blockwise CD achieves better reward-KL tradeoffs than token-wise methods but still lags behind best-of-K. It suggests this as an open direction for future work.
- Why unresolved: The paper does not explore advanced techniques to enhance blockwise CD, such as adaptive block sizing, more sophisticated prefix scorers, or hybrid approaches combining blockwise and token-wise control.
- What evidence would resolve it: Experiments comparing blockwise CD with various enhancements (e.g., adaptive M, advanced prefix scorers, hybrid control) against best-of-K across multiple tasks and datasets.

## Limitations
- The prefix scorer's accuracy depends heavily on the quality and representativeness of the training data, with sparse or noisy rewards potentially leading to poor generalization.
- Blockwise CD, while promising for efficiency, is only briefly validated and its performance in more diverse or complex generation tasks is unknown.
- The multi-objective alignment claim is only demonstrated on a simple linear combination of two rewards, with no exploration of potential conflicts between objectives or sensitivity to weight tuning.

## Confidence
- **Tokenwise CD improves safety and length (High):** The paper provides clear experimental evidence (Table 1) showing that tokenwise CD with β=0.8 significantly improves both safety and length while maintaining low KL divergence. The mechanism is well-justified and the results are robust across different reward models.
- **Blockwise CD matches best-of-K with lower latency (Medium):** While the paper claims blockwise CD achieves similar reward gains to best-of-K with reduced computation, the evidence is limited to a single experimental setup (Table 2). The generalizability of this result to other tasks or base models is uncertain.
- **Multi-objective alignment without retraining (Medium):** The paper demonstrates that combining safety and length scorers at inference time can improve both objectives, but the evaluation is limited to a single dataset and reward combination. The robustness of this approach to more complex or conflicting objectives is unclear.

## Next Checks
1. **Prefix scorer accuracy and generalization:** Train prefix scorers for length and safety on the DSTC8 corpus, then evaluate their predicted vs. actual reward on a held-out test set. Check for correlation and generalization to out-of-distribution prompts.
2. **Ablation on block size M:** Systematically vary the block size M in blockwise CD and measure the tradeoff between reward improvement, KL divergence, and inference latency. Identify the optimal M for balancing control strength and efficiency.
3. **Multi-objective stress test:** Combine three or more conflicting objectives (e.g., safety, length, and informativeness) with varying weights. Evaluate whether linear combination is sufficient or if more sophisticated weighting (e.g., Pareto optimization) is needed to prevent over-optimization or objective collapse.