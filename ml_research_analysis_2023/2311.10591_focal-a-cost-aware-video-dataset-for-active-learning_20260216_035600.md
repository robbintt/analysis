---
ver: rpa2
title: 'FOCAL: A Cost-Aware Video Dataset for Active Learning'
arxiv_id: '2311.10591'
source_url: https://arxiv.org/abs/2311.10591
tags:
- learning
- cost
- active
- sequences
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FOCAL introduces the first public video active learning dataset
  with real-time annotation cost labels for 126 video sequences across 69 unique city
  scenes. Traditional active learning methods assume annotation cost scales linearly
  with data amount, but FOCAL demonstrates this is inaccurate due to factors like
  assistive labeling tools and scene complexity.
---

# FOCAL: A Cost-Aware Video Dataset for Active Learning

## Quick Facts
- arXiv ID: 2311.10591
- Source URL: https://arxiv.org/abs/2311.10591
- Reference count: 40
- Key outcome: Introduces first public video active learning dataset with real-time annotation cost labels, demonstrating conformal methods outperform traditional approaches by reducing annotation costs by 113 hours while maintaining comparable performance

## Executive Summary
FOCAL addresses the critical gap in video active learning by providing the first public dataset with real annotation cost labels for 126 video sequences across 69 unique city scenes. Traditional active learning methods assume annotation cost scales linearly with data amount, but FOCAL demonstrates this is inaccurate due to factors like assistive labeling tools and scene complexity. The dataset enables accurate cost-minimization analysis by providing annotation times recorded during actual labeling. Experiments show conformal active learning methods that exploit video's temporal structure outperform traditional approaches, with the best conformal method achieving 113 fewer hours of annotation cost while maintaining comparable performance and reducing computational overhead by over 77.67%.

## Method Summary
The method involves creating a video active learning dataset with real annotation cost labels recorded during actual labeling workflows. The approach uses YOLOv5n for object detection with ADAM optimizer (lr=0.01, weight decay=0.00005, momentum=0.937), training for 10 epochs per active learning round. Twelve active learning strategies are implemented: 7 inferential methods (entropy, least confidence, margin, badge, coreset, FALSE, GauSS) and 5 conformal methods (least frame, most frame, min motion, min max motion, min boxes). Conformal methods pre-compute optical flow using PWC-Net to rank sequences based on temporal and spatial statistics. The experimental setup reflects real-world annotation workflows where entire sequences are selected rather than individual frames, using a sequence-wise acquisition framework with a 70%/20%/10% train/test/val split while maintaining unique scenes separation.

## Key Results
- Conformal active learning methods that exploit video's temporal structure outperform traditional approaches in cost-aware settings
- The best conformal method (Min Max Motion) achieves 113 fewer hours of annotation cost than the best traditional method while maintaining comparable performance
- Conformal approaches reduce computational overhead by over 77.67% by front-loading optical flow computation
- CAR and PAR metrics show better performance-cost tradeoffs for conformal methods across different budget thresholds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FOCAL's cost labels enable accurate cost minimization analysis that previous video active learning datasets cannot provide
- Mechanism: By providing real-time annotation costs for each video sequence recorded during actual labeling, FOCAL allows active learning algorithms to optimize for both performance and annotation cost simultaneously, rather than assuming cost scales linearly with data amount
- Core assumption: The dataset's cost labels accurately reflect real-world annotation workflows where entire sequences are labeled using assistive tools
- Evidence anchors:
  - [abstract] "FOCAL addresses this discrepancy by providing real annotation-cost labels for 126 video sequences"
  - [section] "These videos took place in practical settings that are of interest to both the autonomous vehicle and infrastructure-assisted autonomy communities"
- Break condition: If the cost labels do not accurately represent the full annotation workflow (including quality assurance time and tool usage), the cost optimization would be misaligned with real-world scenarios

### Mechanism 2
- Claim: Conformal active learning methods that exploit video's temporal structure outperform traditional approaches in cost-aware settings
- Mechanism: By using optical flow and temporal statistics to rank sequences, conformal methods can select sequences that are both informative for model training and efficient to annotate, reducing both annotation time and computational overhead
- Core assumption: Temporal and spatial statistics of video sequences correlate with both annotation difficulty and model performance improvement
- Evidence anchors:
  - [abstract] "We also introduce a set of conformal active learning algorithms that take advantage of the sequential structure of video data"
  - [section] "These strategies take advantage of temporal and spatial statistics of video to minimize cost while maximizing performance"
- Break condition: If the temporal statistics used do not correlate with actual annotation difficulty or model performance, the conformal methods would not provide the claimed benefits

### Mechanism 3
- Claim: The experimental setup reflects real-world annotation workflows where entire sequences are selected rather than individual frames
- Mechanism: By framing active learning as sequence selection rather than frame selection, the evaluation matches how annotators actually work with video data using assistive labeling tools that interpolate across frames
- Core assumption: Annotators in practice label entire video sequences using assistive tools rather than selecting individual frames to label
- Evidence anchors:
  - [abstract] "We show how these approaches better reflect how annotations on videos are done in practice through a sequence selection framework"
  - [section] "However, as discussed previously, the presence of assistive labeling technology enables annotating in a sequence-wise manner"
- Break condition: If annotators do not actually work in this sequence-wise manner or if the assistive tools do not interpolate as assumed, the experimental setup would not reflect real practice

## Foundational Learning

- Concept: Cost-aware active learning in video domains
  - Why needed here: Traditional active learning assumes cost scales linearly with data amount, but FOCAL demonstrates this is inaccurate for video due to assistive tools and scene complexity
  - Quick check question: What is the Pearson correlation coefficient between annotation cost and sequence length in the FOCAL dataset?

- Concept: Conformal sampling vs inferential sampling
  - Why needed here: Conformal methods use data structure to rank samples without requiring posterior probabilities, while inferential methods compute these probabilities from trained models
  - Quick check question: What is the overhead cost difference between conformal and inferential sampling methods in terms of GFLOPS?

- Concept: Performance-cost tradeoff metrics
  - Why needed here: Standard metrics like mAP don't capture the cost implications of different active learning strategies; CAR and PAR metrics are introduced to evaluate this tradeoff
  - Quick check question: How do CAR and PAR metrics differ in their approach to evaluating performance-cost tradeoffs?

## Architecture Onboarding

- Component map: YOLOv5n backbone -> PWC-Net for optical flow -> Active learning loop with sequence selection -> Cost tracking and performance evaluation -> Data preprocessing pipeline

- Critical path: 1. Load FOCAL dataset with cost labels 2. Initialize YOLOv5n model 3. Select initial training sequences (random) 4. Train model for 10 epochs 5. Compute acquisition scores using chosen strategy 6. Select next sequence based on scores and cost 7. Repeat steps 4-6 for 13 rounds 8. Evaluate final performance and total cost

- Design tradeoffs:
  - YOLOv5n vs larger models: Lower GFLOPS but potentially lower mAP
  - Frame-based vs sequence-based selection: Reflects real annotation but may miss fine-grained frame-level information
  - Optical flow computation: Adds upfront cost but enables better sequence ranking
  - Cost labels vs simulated costs: Real labels provide accuracy but limit flexibility

- Failure signatures:
  - Low mAP despite selecting many low-cost sequences: Cost optimization may be prioritized over performance
  - High computational overhead: Inferential methods may be too expensive for large unlabeled pools
  - Poor correlation between predicted and actual costs: Conformal methods may not capture all cost factors
  - Random sampling performs as well as learned strategies: Active learning may not be effective for this domain

- First 3 experiments:
  1. Run all sampling methods with random seed 0 and compare mAP vs cost curves
  2. Compare overhead costs between inferential and conformal methods
  3. Evaluate CAR and PAR metrics for different cost budgets to identify optimal strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of assistive labeling tools affect the correlation between video sequence characteristics and annotation cost?
- Basis in paper: [explicit] The paper mentions assistive labeling tools but doesn't analyze how different tool implementations might affect cost correlations with factors like motion, occlusion, and object density.
- Why unresolved: The paper only considers one assistive labeling platform and doesn't compare how alternative implementations might change the relationship between scene complexity and annotation time.
- What evidence would resolve it: Comparative annotation cost analysis using multiple assistive labeling tools on the same video sequences, showing how tool-specific features (like interpolation algorithms or UI design) impact cost correlations.

### Open Question 2
- Question: Can the conformal sampling strategies be generalized to other video understanding tasks beyond object detection?
- Basis in paper: [explicit] The paper demonstrates conformal sampling on object detection but doesn't explore its applicability to tasks like action recognition, video segmentation, or multi-object tracking.
- Why unresolved: The paper focuses specifically on object detection and doesn't investigate whether the motion-based and spatial statistics used for sampling generalize to other video understanding paradigms.
- What evidence would resolve it: Empirical evaluation of conformal sampling strategies on multiple video understanding tasks, comparing their performance and cost reduction benefits across different domains.

### Open Question 3
- Question: How does the selection of initial random sequences impact the long-term performance and cost efficiency of active learning strategies?
- Basis in paper: [explicit] The paper uses two random sequences for initial training but doesn't analyze how different initializations affect convergence behavior or cost efficiency across rounds.
- Why unresolved: The paper reports average results across three random seeds but doesn't investigate the sensitivity of active learning strategies to initial sequence selection or whether there are optimal initialization strategies.
- What evidence would resolve it: Systematic analysis of how different initial sequence selections affect learning curves, convergence rates, and final performance-cost trade-offs across multiple active learning strategies.

## Limitations
- The dataset size (126 sequences) may limit generalizability to larger video domains
- Annotation cost labels may not capture all aspects of real-world annotation workflows, particularly variations in annotator expertise
- The focus on object detection with YOLOv5n constrains applicability to other video tasks like segmentation or tracking

## Confidence
- High Confidence: The dataset construction methodology and cost measurement approach are well-documented and reproducible
- Medium Confidence: The superiority of conformal methods over traditional approaches is demonstrated but limited by small dataset size and specific task
- Medium Confidence: Computational overhead comparisons are well-quantified but may vary with different hardware configurations

## Next Checks
1. Test the active learning strategies on a larger video dataset to verify that performance-cost tradeoffs observed in FOCAL generalize to bigger domains
2. Apply the same cost-aware active learning framework to video segmentation and tracking tasks to assess whether conformal methods maintain their advantages across different video understanding problems
3. Conduct a user study with actual annotators using the assistive labeling tools to validate whether recorded cost labels accurately reflect real annotation workflows and whether sequence selection strategies align with annotator preferences