---
ver: rpa2
title: Large Language Models for Intent-Driven Session Recommendations
arxiv_id: '2312.07552'
source_url: https://arxiv.org/abs/2312.07552
tags:
- prompt
- session
- recommendation
- user
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PO4ISR, a novel intent-aware session recommendation
  approach leveraging large language models (LLMs). Traditional methods assume a fixed
  number of intents across all sessions and operate in latent spaces, limiting transparency.
---

# Large Language Models for Intent-Driven Session Recommendations

## Quick Facts
- arXiv ID: 2312.07552
- Source URL: https://arxiv.org/abs/2312.07552
- Reference count: 40
- Key outcome: PO4ISR achieves average improvements of 57.37% and 61.03% on HR and NDCG metrics respectively compared to baselines

## Executive Summary
This paper introduces PO4ISR, a novel intent-aware session recommendation approach leveraging large language models (LLMs). Traditional methods assume a fixed number of intents across all sessions and operate in latent spaces, limiting transparency. PO4ISR addresses these limitations through prompt optimization: (1) Prompt Initialization generates a task description guiding LLMs to dynamically understand session-level user intents; (2) Prompt Optimization iteratively refines and augments the initial prompt by self-reflection and evaluation using UCB bandits; (3) Prompt Selection selects optimized prompts based on cross-domain generalizability. Extensive experiments on three real-world datasets demonstrate significant performance improvements.

## Method Summary
PO4ISR consists of three main components: Prompt Initialization creates an initial task description that divides session-based recommendation into four subtasks (discovering item combinations, inferring intents, selecting best intent, reranking items); Prompt Optimization iteratively refines this prompt through self-reflection where ChatGPT identifies error cases, generates reasons for failures, and improves the prompt accordingly, using UCB bandits to efficiently evaluate prompt candidates; Prompt Selection evaluates the top prompts across different domains (movies, games, e-commerce) and selects the one that performs best overall. The system uses LLM APIs to process session data and generate recommendations, with performance evaluated on HR@1, HR@5, NDCG@1, and NDCG@5 metrics.

## Key Results
- PO4ISR achieves average improvements of 57.37% on HR and 61.03% on NDCG metrics compared to baselines
- The approach shows 72.23% improvement over the best baseline (NIR) on the Games dataset
- Cross-domain prompt selection demonstrates that the Opt-Games prompt performs best across all three domains (Movies, Games, Bundle)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PO4ISR uses prompt optimization to enable LLMs to dynamically understand varying user intents within sessions
- Mechanism: The system generates an initial task description prompt that divides session-based recommendation into four subtasks. This prompt is then iteratively refined through self-reflection where ChatGPT identifies error cases, generates reasons for failures, and improves the prompt accordingly.
- Core assumption: LLMs can reason about their own performance failures and generate meaningful improvements to prompts based on error analysis
- Evidence anchors: The paper describes how PromptOpt "strives to evaluate, refine, augment, and optimize the initial task description prompt with an iterative self-reflection"

### Mechanism 2
- Claim: Cross-domain prompt selection leverages LLM generalizability to find optimal prompts
- Mechanism: After iterative optimization within each domain, PO4ISR evaluates the top prompts across different domains and selects the one that performs best overall.
- Core assumption: LLM performance on prompts shows consistent patterns across different recommendation domains
- Evidence anchors: Figure 4 shows "Opt-Games consistently performs the best not only in its games domain but also in the other two domains"

### Mechanism 3
- Claim: UCB bandits efficiently identify promising prompts without exhaustive evaluation
- Mechanism: Instead of evaluating all improved prompts on all training sessions, the system uses UCB bandits to iteratively sample prompts based on estimated performance and uncertainty.
- Core assumption: Performance estimates from partial evaluations can effectively guide prompt selection
- Evidence anchors: The paper states that "With UCB Bandits, we can quickly obtain the estimated performance of the 2Ne prompts"

## Foundational Learning

- Concept: Prompt engineering for LLMs
  - Why needed here: PO4ISR relies on crafting effective task descriptions that guide LLMs to understand and act on session-based recommendation tasks
  - Quick check question: What are the key components of a well-structured prompt for session-based recommendation tasks?

- Concept: Self-reflection and iterative improvement
  - Why needed here: The prompt optimization mechanism depends on LLMs being able to analyze their own failures and generate meaningful improvements
  - Quick check question: How does the system collect error cases and what format does it use to request improvement suggestions?

- Concept: Multi-armed bandit algorithms (specifically UCB)
  - Why needed here: UCB bandits are used to efficiently evaluate and select among multiple prompt candidates without exhaustive testing
  - Quick check question: What is the exploration-exploitation tradeoff in UCB bandits and how does it apply to prompt selection?

## Architecture Onboarding

- Component map: Training sessions → Prompt Initialization → Prompt Optimization (with self-reflection and UCB bandits) → Prompt Selection (cross-domain evaluation) → Optimized prompts → Recommendations
- Critical path: The most time-consuming operations are LLM API calls for prompt refinement and evaluation. The critical path involves initializing a prompt, running iterative optimization with self-reflection and UCB evaluation, then selecting the best prompt for deployment.
- Design tradeoffs: The system trades computational efficiency for prompt quality by using UCB bandits instead of exhaustive evaluation. It also accepts the latency of LLM calls for the benefit of dynamic intent understanding rather than using fixed traditional models.
- Failure signatures: If the system performs worse than baselines, this could indicate: poor initial prompt quality, ineffective self-reflection (LLM cannot identify meaningful improvements), or poor cross-domain generalization. If the system is slow, this likely indicates excessive LLM API calls.
- First 3 experiments:
  1. Compare HR@5 and NDCG@5 metrics of PO4ISR against the best baseline (NIR) on the Games dataset to verify the claimed 72.23% improvement
  2. Run ablation studies removing the prompt optimization step to measure the impact of iterative refinement
  3. Test the system with different initial prompts (Prompt 1 vs Prompt 9) to verify the claim that prompt quality affects final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the hallucination issue in PO4ISR be effectively mitigated without significantly compromising recommendation accuracy?
- Basis in paper: The paper discusses hallucination as a limitation, noting that while JSON mode slightly reduces bad cases, it significantly compromises accuracy. The paper suggests that GPT-4 with less compromised accuracy might help.
- Why unresolved: The paper acknowledges the hallucination issue but does not provide a definitive solution that balances both reducing hallucinations and maintaining high accuracy.
- What evidence would resolve it: Experimental results comparing different approaches (e.g., advanced prompting techniques, model fine-tuning, or hybrid methods) that effectively reduce hallucinations while maintaining or improving accuracy on benchmark datasets.

### Open Question 2
- Question: What is the optimal strategy for prompt selection in cross-domain scenarios to maximize performance across diverse datasets?
- Basis in paper: The paper explores cross-domain prompt selection, finding that the Opt-Games prompt performs best across domains. However, it does not provide a comprehensive framework for selecting optimal prompts in arbitrary cross-domain scenarios.
- Why unresolved: While the paper demonstrates success with a specific cross-domain selection, it does not establish a generalizable strategy for selecting prompts when dealing with entirely new or diverse domains.
- What evidence would resolve it: A systematic study evaluating prompt selection strategies across multiple diverse domains, demonstrating consistent performance improvements and providing guidelines for selecting optimal prompts in new cross-domain scenarios.

### Open Question 3
- Question: How does the quality of initial prompts impact the final performance of PO4ISR, and what are the characteristics of high-quality initial prompts?
- Basis in paper: The paper shows that higher-quality initial prompts lead to better final performance and that lower-quality initial prompts yield more significant improvements through optimization. It also notes that simplified descriptions and subtask division can enhance initial prompt quality.
- Why unresolved: While the paper demonstrates the relationship between initial prompt quality and final performance, it does not provide a detailed analysis of what specific characteristics make an initial prompt high-quality or how to systematically generate such prompts.
- What evidence would resolve it: An in-depth analysis identifying the key characteristics of high-quality initial prompts, along with empirical studies showing the impact of different prompt structures and formulations on the optimization process and final performance.

## Limitations
- Performance on dense datasets shows reduced improvements compared to sparse datasets, suggesting limitations in generalizability
- The paper does not address computational costs of multiple LLM API calls during optimization
- Prompt templates and self-reflection mechanisms are not fully specified, making exact replication difficult

## Confidence

- **High**: The core mechanism of using LLMs for intent-aware recommendations and the overall experimental methodology
- **Medium**: The effectiveness of the UCB bandits approach for prompt selection, as implementation details are sparse
- **Medium**: The cross-domain generalizability claims, given that Opt-Games performs well across domains but this may not hold for all datasets

## Next Checks

1. Implement ablation studies to quantify the contribution of prompt optimization versus initial prompt quality
2. Measure computational overhead by tracking LLM API calls and response times during the optimization process
3. Test the cross-domain generalizability hypothesis by evaluating prompts optimized on one domain against a held-out fourth dataset not used in the original study