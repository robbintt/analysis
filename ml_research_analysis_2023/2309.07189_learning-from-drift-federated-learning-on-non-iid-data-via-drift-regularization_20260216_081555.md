---
ver: rpa2
title: 'Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization'
arxiv_id: '2309.07189'
source_url: https://arxiv.org/abs/2309.07189
tags:
- learning
- local
- drift
- global
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of federated learning with non-IID
  data, where models suffer from performance degradation due to data heterogeneity
  across clients. The core idea is to explicitly estimate the drift between local
  and global models in the logit space and then regularize local models to prevent
  them from deviating in the drift direction.
---

# Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization

## Quick Facts
- arXiv ID: 2309.07189
- Source URL: https://arxiv.org/abs/2309.07189
- Reference count: 19
- One-line primary result: LfD outperforms strong baselines like FedAvg, FedProx, FedAvgM, and MOON across five evaluation aspects: generalization, heterogeneity, scalability, forgetting, and efficiency

## Executive Summary
This paper addresses the challenge of federated learning with non-IID data, where models suffer from performance degradation due to data heterogeneity across clients. The core idea is to explicitly estimate the drift between local and global models in the logit space and then regularize local models to prevent them from deviating in the drift direction. This is achieved through drift estimation by comparing prediction discrepancies and drift regularization using an auxiliary label. Experiments on multiple datasets (CIFAR-10, CIFAR-100, BindingDB, AGNews) demonstrate that the proposed method, Learning from Drift (LfD), achieves state-of-the-art performance and outperforms strong baselines across five key aspects.

## Method Summary
LfD first estimates the drift between local and global models by computing prediction discrepancy using KL divergence over normalized logits. The method then regularizes the local model to predict in the opposite direction of the estimated drift, using an auxiliary label derived from the reversed drift direction. This regularization prevents local models from overfitting to their local data distribution and forgetting global knowledge. The approach includes normalization of classifier weights and features to ensure consistent temperature across models and prevent magnitude-induced prediction bias. The method is implemented within a standard FedAvg framework with modified local objective function incorporating the drift regularization term.

## Key Results
- LfD achieves state-of-the-art performance on CIFAR-10, CIFAR-100, BindingDB, and AGNews datasets
- Outperforms FedAvg, FedProx, FedAvgM, and MOON across five evaluation aspects: generalization, heterogeneity, scalability, forgetting, and efficiency
- Shows improved accuracy, robustness to heterogeneity, scalability to more clients, reduced catastrophic forgetting, and better communication efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Drift estimation by prediction discrepancy aligns the local model with the global model in logit space.
- Mechanism: The method estimates the difference between local and global model predictions (drift) using KL divergence over logits. It then regularizes the local model to predict in the opposite direction of the estimated drift.
- Core assumption: The global model's predictions are more reliable and generalized than local models.
- Evidence anchors:
  - [abstract] "Specifically, LfD first estimates how different the local model is from the global model (i.e., drift). The local model is then regularized such that it does not fall in the direction of the estimated drift."
  - [section 4.1] "We quantify the degree of the client drift by estimating prediction discrepancy between locally trained models (i.e., ωk t−1) in the previous communication and their aggregated model (i.e., ωt)."
- Break condition: If the global model is not more reliable than local models (e.g., in early training rounds or when clients have high-quality local data).

### Mechanism 2
- Claim: Normalizing classifier weights and features prevents magnitude-induced prediction bias in heterogeneous settings.
- Mechanism: The method normalizes both classifier weights and features before computing softmax probabilities, ensuring consistent temperature across global and local models.
- Core assumption: Magnitude differences between classifier weights and features create inconsistent confidence levels across models.
- Evidence anchors:
  - [section 4.1] "We constrain the magnitudes by normalizing the classifier weights and features during the local optimization... This strategy allows to more correctly estimate the drift without slow convergence in the training."
- Break condition: If normalization causes numerical instability or if the margin parameter is poorly tuned.

### Mechanism 3
- Claim: Drift regularization acts as a knowledge distillation mechanism, preventing catastrophic forgetting of global knowledge.
- Mechanism: The auxiliary label provides supervision for all classes, not just the ground truth, encouraging the local model to maintain inter-class similarity relationships learned by the global model.
- Core assumption: Local models tend to overfit to local data distribution and forget global knowledge.
- Evidence anchors:
  - [abstract] "The regularization gives upward weights to the loss... For the other classes, the regularization R works similarly to knowledge distillation... by providing non-zero weights to other classes."
- Break condition: If the regularization term overwhelms the primary classification loss, causing underfitting to local data.

## Foundational Learning

- Concept: Federated Learning with Non-IID Data
  - Why needed here: Understanding the client drift problem that occurs when local models overfit to their local data distribution is fundamental to grasping why LfD is necessary.
  - Quick check question: What happens to federated learning performance when data distribution across clients is non-identical?

- Concept: Contrastive Learning and Knowledge Distillation
  - Why needed here: LfD uses techniques inspired by contrastive learning (for feature alignment) and knowledge distillation (for maintaining global knowledge), so understanding these concepts helps explain how LfD works.
  - Quick check question: How does knowledge distillation help prevent catastrophic forgetting in federated learning?

- Concept: KL Divergence and Softmax Temperature
  - Why needed here: The method uses KL divergence to measure prediction discrepancy and carefully controls softmax temperature through normalization, so understanding these concepts is crucial for implementing LfD correctly.
  - Quick check question: How does temperature scaling affect the confidence of softmax predictions?

## Architecture Onboarding

- Component map: Global Model -> Drift Estimation Module -> Drift Regularization Module -> Local Model -> Federated Learning Core
- Critical path:
  1. Receive global model from server
  2. Perform local training with drift estimation and regularization
  3. Send updated model back to server for aggregation
  4. Server updates global model using weighted averaging

- Design tradeoffs:
  - Higher regularization strength vs. local data fitting: Too much drift regularization may prevent the model from learning local patterns effectively
  - Normalization vs. numerical stability: Careful tuning of margin parameter is needed to avoid convergence issues
  - Communication efficiency vs. accuracy: More frequent drift estimation could improve accuracy but increase communication overhead

- Failure signatures:
  - Training instability or divergence: May indicate poor margin parameter tuning or normalization issues
  - Poor performance on minority classes: Could suggest insufficient regularization strength or imbalanced drift estimation
  - Communication overhead: If drift estimation is performed too frequently, it may negate the communication efficiency benefits of federated learning

- First 3 experiments:
  1. Baseline comparison: Run FedAvg and LfD on CIFAR-10 with moderate heterogeneity (β=0.5) to verify performance improvement
  2. Heterogeneity stress test: Test both methods across different β values (0.5, 0.1, 0.05) to confirm LfD's robustness to varying data distributions
  3. Forgetting analysis: Evaluate both methods' ability to maintain performance on absent classes using the learning performance metric described in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LfD compare to other federated learning methods when applied to non-image datasets with different characteristics (e.g., tabular data, time series)?
- Basis in paper: [explicit] The paper evaluates LfD on CIFAR-10, CIFAR-100, BindingDB, and AGNews datasets, but does not explore other types of datasets.
- Why unresolved: The paper only focuses on image and text datasets, so the generalizability of LfD to other data types is unknown.
- What evidence would resolve it: Conduct experiments using LfD on various non-image datasets, such as tabular data or time series, and compare the results to other federated learning methods.

### Open Question 2
- Question: How does the performance of LfD change when using different neural network architectures (e.g., deeper networks, recurrent networks) in the federated learning setup?
- Basis in paper: [explicit] The paper uses simple 2-layer CNNs for CIFAR-10 and ResNet-18 for CIFAR-100 and CINIC-10, but does not explore other architectures.
- Why unresolved: The paper only focuses on specific neural network architectures, so the impact of using different architectures on LfD's performance is unknown.
- What evidence would resolve it: Implement LfD using various neural network architectures and compare the performance across different architectures in the federated learning setup.

### Open Question 3
- Question: How does the performance of LfD scale when the number of clients increases significantly (e.g., hundreds or thousands of clients) in the federated learning scenario?
- Basis in paper: [explicit] The paper evaluates LfD with up to 100 clients, but does not explore scenarios with a larger number of clients.
- Why unresolved: The paper only focuses on a limited number of clients, so the scalability of LfD to a larger number of clients is unknown.
- What evidence would resolve it: Conduct experiments using LfD with a significantly larger number of clients (e.g., hundreds or thousands) and analyze the performance and communication efficiency in such scenarios.

## Limitations

- Effectiveness of drift estimation and regularization may be sensitive to hyperparameter tuning, particularly the margin parameter and temperature scaling values
- Performance in extremely heterogeneous settings (β < 0.1) or with a large number of clients (>100) remains unverified
- Method's performance on non-image datasets and with different neural network architectures is unknown

## Confidence

- **High confidence**: Performance improvements over baselines (FedAvg, FedProx, FedAvgM, MOON) on standard benchmarks (CIFAR-10, CIFAR-100) and across five evaluation aspects (generalization, heterogeneity, scalability, forgetting, efficiency)
- **Medium confidence**: Effectiveness of the drift regularization mechanism and its generalization to other domains (BindingDB, AGNews) and highly heterogeneous settings
- **Low confidence**: Robustness to hyperparameter choices (margin parameter, temperature scaling) and scalability to very large numbers of clients or extremely non-IID data distributions

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically evaluate the impact of margin parameter and temperature scaling values on performance across different datasets and heterogeneity levels to establish robust hyperparameter ranges
2. **Extreme Heterogeneity Test**: Conduct experiments with β < 0.1 to assess LfD's performance in highly non-IID settings and identify potential failure modes or limitations
3. **Scalability Benchmark**: Test LfD with K > 100 clients to evaluate its scalability and communication efficiency in large-scale federated learning scenarios, measuring both accuracy and resource usage