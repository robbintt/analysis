---
ver: rpa2
title: 'InteraSSort: Interactive Assortment Planning Using Large Language Models'
arxiv_id: '2311.12241'
source_url: https://arxiv.org/abs/2311.12241
tags:
- assortment
- optimization
- interassort
- planning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InteraSSort introduces a novel interactive framework for assortment
  planning that integrates Large Language Models (LLMs) with optimization tools. The
  system enables retail planners to interact naturally through text prompts to explore
  optimal product assortments under various constraints.
---

# InteraSSort: Interactive Assortment Planning Using Large Language Models

## Quick Facts
- arXiv ID: 2311.12241
- Source URL: https://arxiv.org/abs/2311.12241
- Reference count: 5
- Primary result: Novel interactive framework integrating LLMs with optimization tools for natural language assortment planning

## Executive Summary
InteraSSort introduces a framework that bridges the gap between retail domain experts and technical assortment optimization by enabling natural language interaction with optimization algorithms. The system leverages LLM function-calling capabilities to parse user prompts, validate parameters, and execute appropriate optimization algorithms while maintaining conversational context. Using the Ta-Feng grocery dataset and MNL-based optimization, experiments demonstrate the framework's ability to handle constraint modifications and provide optimal product assortments through interactive dialogue.

## Method Summary
The framework employs GPT-3.5-turbo with function calling to parse natural language prompts into structured optimization parameters. The system processes user requests through prompt decomposition, validation, and execution stages, using Streamlit for the interface and optimization solvers (CPLEX/Gurobi) for computation. The approach specifically implements MNL-based assortment optimization using the Ta-Feng grocery dataset, allowing retail planners to modify constraints and explore optimal assortments through conversational interaction.

## Key Results
- Successfully translates natural language prompts into MNL-based assortment optimization parameters
- Maintains conversational context across multiple user interactions for constraint modifications
- Demonstrates effectiveness using real-world Ta-Feng grocery dataset with 817,741 transactions
- Enables non-expert retail planners to perform complex optimization through intuitive dialogue

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InteraSSort translates natural language prompts into structured optimization parameters using LLM function-calling capabilities
- Mechanism: The LLM parses user text inputs to identify dataset, choice model, and constraints, then formats these as JSON objects that trigger appropriate optimization functions
- Core assumption: LLMs can reliably extract relevant parameters from natural language and maintain conversational context across turns
- Evidence anchors:
  - [abstract] "leveraging LLMs' function-calling capabilities, InteraSSort decomposes user requests into optimization parameters"
  - [section 4.2] "In this stage, InteraSSort leverages the function-calling capabilities of the LLM to break down the standardized prompts"
  - [corpus] Weak evidence - corpus papers focus on assortment optimization but don't address the natural language interface mechanism
- Break condition: If the LLM fails to correctly parse complex constraints or loses conversational context between turns

### Mechanism 2
- Claim: InteraSSort maintains conversational history to enable interactive multi-turn optimization sessions
- Mechanism: The framework appends previous chat history to follow-up prompts, allowing the LLM to reference prior constraints and solutions when processing new requests
- Core assumption: Conversational context can be effectively preserved and utilized across multiple optimization iterations
- Evidence anchors:
  - [section 4.2] "To facilitate interactive multi-turn conversations, InteraSSort has the capability to append chat history to the follow-up prompts"
  - [section 5.4] "When a user poses a follow-up question...the system makes use of the decomposed inputs from the previous interaction"
  - [corpus] No direct evidence - corpus focuses on optimization algorithms but not interactive dialogue management
- Break condition: If context becomes too long or complex for the LLM to effectively maintain coherence across turns

### Mechanism 3
- Claim: InteraSSort validates and executes optimization functions based on parsed parameters to generate solutions
- Mechanism: After parsing prompts, the framework validates parameters against known constraints and choice model parameters, then executes appropriate optimization algorithms
- Core assumption: Optimization functions can be reliably called with parameters extracted from natural language and will return valid solutions
- Evidence anchors:
  - [section 4.3] "InteraSSort effectively manages and processes the output received from the prompt decomposition stage...conducting thorough validation checks"
  - [section 5.4] "This function then processes the arguments, executes the MNL optimization script, and communicates the outcomes"
  - [corpus] Weak evidence - corpus papers discuss optimization algorithms but not the integration with LLM-parsed parameters
- Break condition: If validation checks fail or optimization functions return errors due to malformed parameters

## Foundational Learning

- Concept: Multinomial Logit (MNL) choice model
  - Why needed here: Understanding the MNL model is essential since InteraSSort specifically implements MNL-based assortment optimization as shown in section 5.2
  - Quick check question: How does the MNL model calculate the probability of a customer choosing product k from assortment S?

- Concept: Function calling in LLMs
  - Why needed here: The framework's core functionality depends on the LLM's ability to parse natural language and trigger appropriate optimization functions
  - Quick check question: What JSON structure does the LLM generate to call optimization functions in InteraSSort?

- Concept: Interactive optimization workflows
  - Why needed here: InteraSSort's value proposition is enabling non-experts to perform complex optimization through conversation rather than technical interface
  - Quick check question: How does InteraSSort handle constraint modifications across multiple user interactions?

## Architecture Onboarding

- Component map: User interface (Streamlit) → LLM (gpt-3.5-turbo) → Prompt decomposition → Validation layer → Optimization solvers (CPLEX/Gurobi) → Response generation → User interface
- Critical path: User prompt → LLM parsing → Function call → Optimization execution → Solution formatting → User response
- Design tradeoffs: Natural language flexibility vs. structured parameter requirements; interactive convenience vs. optimization complexity; real-time responsiveness vs. computational overhead
- Failure signatures: Incorrect parameter parsing by LLM; validation failures due to malformed inputs; optimization solver errors; loss of conversational context
- First 3 experiments:
  1. Test basic MNL optimization with simple prompt "Find optimal assortment for Ta-Feng dataset using MNL model"
  2. Test constraint modification with follow-up "Limit assortment size to 5 products" and verify conversational context is maintained
  3. Test error handling by providing malformed prompts and observing validation layer responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of InteraSSort vary across different choice models beyond MNL, such as Mixed Multinomial Logit (MMNL) or Nested Logit models?
- Basis in paper: [explicit] The paper states that "InteraSSort can be easily extended to other marketing problems in the field of operations management" and uses MNL as the primary choice model, but does not explore performance across different choice models.
- Why unresolved: The paper only demonstrates the framework using the MNL model and does not provide comparative analysis with other choice models that might be more suitable for certain retail contexts.
- What evidence would resolve it: Empirical comparison of InteraSSort's accuracy, computational efficiency, and user satisfaction when using different choice models (MNL, MMNL, Nested Logit) on the same retail datasets, measuring metrics like revenue optimization, solution time, and constraint satisfaction rates.

### Open Question 2
- Question: What is the optimal level of interactivity for different types of retail planners with varying domain expertise?
- Basis in paper: [inferred] The paper mentions that store planners have "limited optimization expertise" but does not investigate whether different expertise levels require different levels of interactivity or whether too much interactivity could be counterproductive.
- Why unresolved: The framework is designed for interactive conversations but doesn't explore whether there's an optimal balance between automation and user control based on the planner's expertise level.
- What evidence would resolve it: User studies comparing performance and satisfaction across different planner expertise levels (novice, intermediate, expert) using varying degrees of interactivity, measuring solution quality, decision-making time, and user satisfaction scores.

### Open Question 3
- Question: How does InteraSSort scale when handling multiple simultaneous user requests or concurrent assortment planning problems?
- Basis in paper: [explicit] The paper mentions that InteraSSort uses "scalable efficient algorithms" but does not test the framework's performance under concurrent usage scenarios or when handling multiple assortment planning problems simultaneously.
- Why unresolved: While the paper demonstrates effectiveness in single-user scenarios, it doesn't address the system's performance in real-world enterprise environments where multiple planners might use the system concurrently.
- What evidence would resolve it: Performance benchmarking tests measuring response time, system throughput, and solution quality when processing multiple concurrent requests, comparing single-user versus multi-user scenarios under various load conditions.

## Limitations
- Performance heavily dependent on LLM's ability to correctly parse complex optimization constraints from natural language
- Context maintenance across multiple turns may degrade with increased conversation length or complexity
- Validation layer effectiveness in preventing malformed parameter execution is not empirically demonstrated

## Confidence
- Mechanism 1: Medium - Relies on LLM's consistent parameter extraction from natural language
- Mechanism 2: Medium - Depends on LLM's ability to maintain conversational coherence
- Mechanism 3: Medium - Assumes optimization functions will execute correctly with LLM-parsed parameters

## Next Checks
1. Test LLM parsing accuracy across 50+ diverse assortment planning prompts with varying complexity levels
2. Measure context retention performance across 10+ turn conversations with constraint modifications
3. Benchmark response latency and accuracy against traditional non-interactive optimization interfaces using the same Ta-Feng dataset