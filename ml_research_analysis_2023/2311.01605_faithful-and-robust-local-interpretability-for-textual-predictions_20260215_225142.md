---
ver: rpa2
title: Faithful and Robust Local Interpretability for Textual Predictions
arxiv_id: '2311.01605'
source_url: https://arxiv.org/abs/2311.01605
tags:
- fred
- words
- prediction
- classi
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of interpreting predictions made
  by complex text classification models. It introduces FRED, a novel method that identifies
  the minimal set of words whose removal most significantly impacts a model's prediction.
---

# Faithful and Robust Local Interpretability for Textual Predictions

## Quick Facts
- arXiv ID: 2311.01605
- Source URL: https://arxiv.org/abs/2311.01605
- Authors: 
- Reference count: 32
- Key outcome: FRED achieves comprehensibility scores of 0.126 and sufficiency scores of -0.114 on logistic classifiers for restaurant reviews, outperforming LIME and Anchors.

## Executive Summary
This paper introduces FRED, a novel method for interpreting predictions made by complex text classification models. FRED identifies the minimal set of words whose removal most significantly impacts a model's prediction by generating counterfactual examples and assigning importance scores to tokens. Theoretical analyses on interpretable classifiers and empirical evaluations against state-of-the-art methods demonstrate FRED's effectiveness in providing insights into text models.

## Method Summary
FRED is a black-box method that generates perturbed samples by randomly removing words from a document and computes the empirical drop in prediction for each candidate word. The method then finds the minimal subset of words whose removal causes the prediction score to drop below a given threshold. FRED assigns importance scores to tokens based on their impact on prediction confidence and provides explanations that are faithful to the model's behavior.

## Key Results
- On logistic classifiers, FRED achieves comprehensibility scores of 0.126 and sufficiency scores of -0.114 on restaurant reviews, outperforming LIME and Anchors.
- On SiEBERT, FRED shows superior performance on larger documents, with comprehensibility scores of 0.560 and sufficiency scores of 0.518 on Yelp reviews.
- FRED demonstrates better faithfulness and robustness compared to state-of-the-art methods on both interpretable classifiers and complex transformer models.

## Why This Works (Mechanism)

### Mechanism 1
FRED's sampling-based approach reliably identifies the minimal set of words whose removal causes the largest drop in prediction score by generating perturbed samples and computing empirical drops. The convergence guarantee relies on the empirical drop converging to the true expected drop as the number of samples increases.

### Mechanism 2
On linear classifiers, FRED's explanations align with the model's learned coefficients by prioritizing words with the highest product of coefficient magnitude and inverse document frequency (λj·vj). The mechanism assumes TF-IDF vectorization and well-ordered coefficients.

### Mechanism 3
On rule-based classifiers, FRED identifies the minimal set of words whose presence triggers the prediction by finding the word with the fewest occurrences among the critical words. The mechanism assumes the classifier's prediction depends on the presence of specific words.

## Foundational Learning

- Concept: Convergence of empirical averages
  - Why needed here: To justify using empirical drops instead of theoretical drops for candidate evaluation
  - Quick check question: If we have 1000 samples and a candidate is absent in 200 of them, what does the empirical drop estimate?

- Concept: TF-IDF vectorization
  - Why needed here: To understand how documents are represented and how word importance is calculated
  - Quick check question: Why do words that appear frequently in one document but rarely in others get higher TF-IDF weights?

- Concept: Shapley values and feature importance
  - Why needed here: To compare FRED's approach with other importance-based explanation methods
  - Quick check question: How does FRED's approach differ from assigning importance scores to individual words like LIME does?

## Architecture Onboarding

- Component map: Document -> Sampling -> Drop Calculation -> Optimization -> Explanation
- Critical path: Document → Sampling → Drop Calculation → Optimization → Explanation
- Design tradeoffs:
  - Sampling probability (50%) vs. coverage of candidates
  - Number of samples (n≈3000) vs. computation time
  - Max explanation length (ℓmax=10) vs. completeness
- Failure signatures:
  - Low robustness scores indicate inconsistent explanations across runs
  - Poor faithfulness scores suggest explanations don't align with model behavior
  - High computation time may indicate inefficient sampling
- First 3 experiments:
  1. Test FRED on a logistic regression model with known coefficients to verify alignment
  2. Compare FRED's explanations with LIME's on the same document to understand differences
  3. Measure computation time and robustness for documents of increasing length to find scalability limits

## Open Questions the Paper Calls Out

### Open Question 1
How does FRED perform on non-English text classification tasks? The paper focuses on English sentiment analysis datasets but does not test FRED on other languages.

### Open Question 2
What is the impact of different word embedding models on fredpos's performance? The paper mentions using GloVe embeddings but notes that other models like Word2Vec were also tested.

### Open Question 3
How does FRED's performance scale with extremely large documents (e.g., 10,000+ words)? The paper tests FRED on Yelp reviews (average length ~150 words) but does not explore its performance on significantly larger documents.

## Limitations

- The theoretical guarantees for FRED's convergence rely on assumptions that may not hold in practice, especially for complex models or distributions.
- The paper does not provide a rigorous treatment of how FRED's performance scales with document length beyond empirical observations.
- The computational cost of generating sufficient samples for reliable empirical drop estimates may become prohibitive for large-scale applications.

## Confidence

- High Confidence: FRED's effectiveness on interpretable classifiers (linear models and rule-based classifiers) is well-supported by theoretical analysis and empirical validation.
- Medium Confidence: FRED's superior performance compared to LIME and Anchors on the SiEBERT model for larger documents is supported by experimental results, but generalizability to other transformer-based models requires further investigation.
- Low Confidence: The convergence guarantees for the empirical drop estimator are based on theoretical assumptions that may not hold in practice, especially for complex models or distributions.

## Next Checks

1. **Stress Test for Out-of-Distribution Samples**: Evaluate FRED's performance when the sampling scheme produces perturbed documents that are significantly different from the original distribution, potentially leading to undefined or unreliable predictions from the black-box model.

2. **Scalability Analysis**: Conduct experiments to determine the relationship between document length and FRED's computation time and explanation quality, identifying the point at which the method becomes impractical or loses effectiveness.

3. **Generalization to Other Transformer Models**: Test FRED's performance on a diverse set of transformer-based text classification models (e.g., BERT, XLNet, DistilBERT) to assess its robustness and generalizability beyond the specific SiEBERT model used in the paper.