---
ver: rpa2
title: Towards Understanding In-Context Learning with Contrastive Demonstrations and
  Saliency Maps
arxiv_id: '2307.05052'
source_url: https://arxiv.org/abs/2307.05052
tags:
- demonstrations
- saliency
- input
- maps
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how different parts of demonstrations in
  in-context learning (ICL) affect the performance of large language models (LLMs)
  using explainable NLP (XNLP) methods and saliency maps. The authors explore the
  impacts of ground-truth labels, input distribution, and complementary explanations,
  particularly when these are altered or perturbed.
---

# Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps

## Quick Facts
- arXiv ID: 2307.05052
- Source URL: https://arxiv.org/abs/2307.05052
- Reference count: 4
- This paper investigates how different parts of demonstrations in in-context learning affect LLM performance using saliency maps and contrastive demonstrations

## Executive Summary
This paper explores how different components of in-context learning demonstrations affect large language model performance through contrastive analysis and saliency mapping. The authors systematically perturb demonstration elements including ground-truth labels, input distributions, and complementary explanations to understand their relative importance. Their findings reveal that ground-truth label flipping has the most significant impact on model behavior, particularly for larger models, while input neutralization and complementary explanations show more limited effects. The study provides insights into how LLMs utilize demonstration information during in-context learning.

## Method Summary
The study uses the SST-2 sentiment analysis dataset with 288 test examples and 4 hand-picked demonstrations. Contrastive demonstrations are created by flipping labels, neutralizing sentiment-indicative terms to neutral ones, and adding complementary explanations generated by GPT-4. Three models are evaluated: GPT-2, GPT-3.5-Turbo, and InstructGPT. Saliency maps are generated using Integrated Gradients for GPT-2 and LIME for InstructGPT, with quantitative and qualitative analysis comparing the importance scores across different perturbation types.

## Key Results
- Ground-truth label flipping significantly affects saliency maps, with larger models showing more pronounced responses
- Input neutralization (changing sentiment terms to neutral) has less impact than label flipping
- Complementary explanations do not consistently benefit sentiment analysis tasks as they do for symbolic reasoning
- Larger models demonstrate emergent ability to override prior knowledge when presented with contrasting demonstrations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ground-truth labels in demonstrations have varying impact on ICL performance depending on model size
- Mechanism: Larger LLMs can override prior knowledge from pretraining when ground-truth labels are flipped in demonstrations, while smaller models remain sensitive to label flipping
- Core assumption: Model scale determines the ability to learn from demonstrations rather than relying on pretraining
- Evidence anchors:
  - [abstract] "flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs"
  - [section] "Wei et al. [2023] show larger models like InstructGPT [Ouyang et al. 2022] (specifically the text-davinci-002 checkpoint) and PaLM-540B [Chowdhery et al. 2022] have the emergent ability to override prior knowledge in the same setting"
- Break Condition: If model size doesn't correlate with label flipping sensitivity, or if pretraining knowledge is insufficient for the task

### Mechanism 2
- Claim: Input distribution perturbations have less impact on ICL performance than label flipping
- Mechanism: Models rely on pretrained knowledge to make predictions even when input is neutralized, reducing the impact of input distribution changes
- Core assumption: Pretrained knowledge is sufficiently strong for sentiment analysis tasks to maintain performance despite input neutralization
- Evidence anchors:
  - [abstract] "changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels"
  - [section] "We find that such input perturbation (neutralization) does not have as large impact as changing ground-truth label do"
- Break Condition: If input neutralization significantly degrades performance, or if task requires specific input patterns

### Mechanism 3
- Claim: Complementary explanations benefit symbolic reasoning tasks but not sentiment analysis tasks
- Mechanism: Task complexity determines whether complementary explanations improve ICL performance
- Core assumption: Different task types require different types of demonstration support
- Evidence anchors:
  - [abstract] "complementary explanations do not necessarily benefit sentiment analysis task as they do for symbolic reasoning tasks"
  - [section] "Previous work Ye et al. [2022] shows complementary explanations are beneficial for symbolic reasoning tasks including Letter Concatenation, Coin Flips, and Grade School Math"
- Break Condition: If explanations consistently improve performance across task types, or if task complexity doesn't affect explanation utility

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: Understanding how LLMs learn from demonstrations without fine-tuning
  - Quick check question: What distinguishes ICL from traditional fine-tuning approaches?

- Concept: Saliency Maps
  - Why needed here: Visualizing which parts of demonstrations influence model predictions
  - Quick check question: How do gradient-based and perturbation-based saliency methods differ?

- Concept: Task-Specific Perturbation
  - Why needed here: Understanding how different demonstration components affect model behavior
  - Quick check question: Why is sentiment analysis a suitable task for studying input neutralization effects?

## Architecture Onboarding

- Component map:
  - Data preparation: Dataset selection, demonstration creation, perturbation methods
  - Model evaluation: Accuracy metrics, T-tests for statistical significance
  - Explanation generation: Saliency map methods (IG, LIME), comparison across models
  - Analysis pipeline: Quantitative and qualitative analysis of saliency patterns

- Critical path:
  1. Prepare original and contrastive demonstrations
  2. Generate predictions and saliency maps
  3. Analyze saliency patterns across different perturbation types
  4. Draw conclusions about demonstration component importance

- Design tradeoffs:
  - Budget constraints vs. comprehensive analysis
  - Model size vs. computational cost
  - Perturbation granularity vs. interpretability

- Failure signatures:
  - Inconsistent saliency patterns across similar demonstrations
  - Unexpected model behavior on contrastive demonstrations
  - Statistical insignificance in T-test results

- First 3 experiments:
  1. Verify label flipping sensitivity in small vs. large models
  2. Compare input neutralization impact on sentiment-indicative vs. neutral terms
  3. Test complementary explanation effectiveness across different task types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger language models like GPT-4 differ from InstructGPT and ChatGPT in their ability to override prior knowledge from pretraining with demonstrations presented in-context?
- Basis in paper: [explicit] The paper mentions that larger models like InstructGPT (text-davinci-002) and PaLM-540B have the emergent ability to override prior knowledge in the same setting as smaller models like GPT-2, which cannot override prior knowledge with demonstrations presented in-context.
- Why unresolved: The paper only partly reproduces results from previous work on a sentiment classification task and does not directly compare the performance of GPT-4 with InstructGPT and ChatGPT in terms of overriding prior knowledge.
- What evidence would resolve it: A systematic comparison of the performance of GPT-4, InstructGPT, and ChatGPT on various tasks with different types of demonstrations, specifically focusing on their ability to override prior knowledge from pretraining.

### Open Question 2
- Question: How do complementary explanations affect the performance of in-context learning for tasks other than sentiment analysis and symbolic reasoning?
- Basis in paper: [explicit] The paper finds that complementary explanations do not necessarily benefit sentiment analysis tasks as they do for symbolic reasoning tasks. However, it suggests that the impact of complementary explanations varies based on the tasks at hand.
- Why unresolved: The paper only investigates the impact of complementary explanations on sentiment analysis and symbolic reasoning tasks. It does not explore other types of tasks or provide a comprehensive analysis of the effectiveness of complementary explanations across different domains.
- What evidence would resolve it: A systematic evaluation of the effectiveness of complementary explanations on a diverse set of benchmark tasks and datasets, including but not limited to sentiment analysis and symbolic reasoning, to determine the generalizability of the findings.

### Open Question 3
- Question: How do gradient-based saliency map methods compare to perturbation-based methods in terms of the saliency maps generated for the same input and model?
- Basis in paper: [inferred] The paper mentions that it uses both gradient-based (Integrated Gradients) and perturbation-based (LIME) methods for generating saliency maps, but it does not directly compare the saliency maps generated by these methods for the same input and model.
- Why unresolved: The paper does not provide a detailed comparison of the saliency maps generated by gradient-based and perturbation-based methods, nor does it discuss the potential differences in the insights provided by each method.
- What evidence would resolve it: A comparative analysis of the saliency maps generated by gradient-based and perturbation-based methods for the same input and model, including a discussion of the similarities and differences in the insights provided by each method.

## Limitations

- The study focuses primarily on sentiment analysis tasks, limiting generalizability to other domains
- Small sample size (288 test examples and 4 demonstrations) may not capture full model behavior patterns
- Comparison between GPT-2 and InstructGPT is complicated by architectural differences beyond just scale

## Confidence

- **High**: The finding that label flipping affects saliency more noticeably in larger LLMs, supported by both prior work (Wei et al., 2023) and observed patterns in this study
- **Medium**: The observation that input neutralization has less impact than label flipping, though this may be task-specific to sentiment analysis
- **Medium**: The claim about complementary explanations not benefiting sentiment analysis, given the limited scope of task types tested

## Next Checks

1. Replicate the study across multiple task types (e.g., natural language inference, question answering) to verify whether the label flipping and input neutralization effects generalize beyond sentiment analysis

2. Test additional model scales between GPT-2 and GPT-3.5-Turbo to establish a clearer relationship between model size and sensitivity to demonstration perturbations

3. Conduct ablation studies with varying demonstration lengths and formats to determine whether the observed effects are robust to changes in how demonstrations are presented to the model