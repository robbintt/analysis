---
ver: rpa2
title: Unsupervised approaches based on optimal transport and convex analysis for
  inverse problems in imaging
arxiv_id: '2311.08972'
source_url: https://arxiv.org/abs/2311.08972
tags:
- such
- unsupervised
- learning
- problems
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews unsupervised deep learning approaches for solving
  imaging inverse problems, focusing on methods based on optimal transport and convex
  analysis. The key methods surveyed include cycle-consistency-based models and learned
  adversarial regularization, which have clear probabilistic interpretations rooted
  in optimal transport theory.
---

# Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging

## Quick Facts
- arXiv ID: 2311.08972
- Source URL: https://arxiv.org/abs/2311.08972
- Reference count: 40
- This survey reviews unsupervised deep learning approaches for solving imaging inverse problems, focusing on methods based on optimal transport and convex analysis.

## Executive Summary
This survey comprehensively reviews unsupervised deep learning approaches for solving imaging inverse problems, with particular focus on methods based on optimal transport and convex analysis. The authors present a unified framework connecting classical optimization theory with modern deep learning techniques, showing how optimal transport theory provides theoretical grounding for cycle-consistency approaches and adversarial regularization. The paper covers several unsupervised learning paradigms including cycle-WGANs, adversarial convex regularizers, learned optimization algorithms, and plug-and-play methods, demonstrating how these approaches can achieve competitive performance compared to supervised methods even when paired training data is scarce.

## Method Summary
The survey describes several unsupervised learning frameworks for imaging inverse problems. Cycle-WGANs use optimal transport theory to minimize Wasserstein distance between image and measurement distributions through cycle-consistent generators and discriminators. Adversarial regularization learns convex regularizers that distinguish ground-truth from pseudo-inverse reconstructions. Learned optimization methods train parameterized algorithms to solve convex problems efficiently. Plug-and-play approaches incorporate pre-trained deep denoisers into iterative reconstruction algorithms. All methods leverage convex analysis and monotone operator theory to establish convergence guarantees while using deep networks to capture complex image statistics without requiring paired supervision.

## Key Results
- Unsupervised methods based on optimal transport can achieve competitive performance compared to supervised approaches when paired data is scarce
- Cycle-consistency and adversarial regularization provide principled frameworks for unsupervised learning by minimizing Wasserstein distances between distributions
- Learned optimization and plug-and-play algorithms can accelerate reconstruction while maintaining convergence guarantees
- The survey establishes connections between optimal transport theory, convex analysis, and modern deep learning approaches for inverse problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unsupervised learning via optimal transport minimizes the Wasserstein distance between learned and ground-truth image distributions without requiring paired training data.
- Mechanism: Cycle-consistent GANs use two generators to map between image and measurement spaces while enforcing cycle-consistency, which regularizes the transport plan and aligns distributions.
- Core assumption: The forward operator A and its pseudo-inverse A† can be incorporated into the cycle architecture to enforce data consistency while learning the transport map.
- Evidence anchors:
  - [abstract] "This survey reviews unsupervised deep learning approaches for solving imaging inverse problems, focusing on methods based on optimal transport and convex analysis."
  - [section] "The training objective in(32) is symmetric inX and Y, and it is not designed to capture a statistical relationship betweenx and y."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.402" (weak evidence for unsupervised optimal transport approaches)
- Break Condition: If the forward operator is highly ill-conditioned or the measurement noise is too high, the cycle-consistency constraint may not sufficiently regularize the transport map, leading to poor reconstructions.

### Mechanism 2
- Claim: Adversarial regularization learns a data-driven regularizer by discriminating between ground-truth images and pseudo-inverse reconstructions, implicitly aligning distributions.
- Mechanism: A neural network regularizer is trained adversarially to distinguish samples from the ground-truth distribution from those generated by the pseudo-inverse, effectively learning a penalty term that promotes data consistency.
- Core assumption: The learned regularizer approximates the Kantorovich potential for the Wasserstein distance, and gradient descent steps on this regularizer move reconstructions toward the ground-truth distribution.
- Evidence anchors:
  - [abstract] "The paper also covers provably convergent learned optimization algorithms and plug-and-play algorithms based on gradient-step deep denoisers."
  - [section] "The heuristic behind this choice is that a regularizer trained this way will penalize noise and artifacts generated by the pseudo-inverse (and contained inπ†)."
  - [corpus] "Weak evidence for adversarial regularization methods in unsupervised inverse problems" (corpus lacks specific studies)
- Break Condition: If the pseudo-inverse generates reconstructions that are too far from the ground-truth distribution, the adversarial regularizer may fail to learn a useful penalty term.

### Mechanism 3
- Claim: Learning-to-optimize (L2O) methods learn fast solvers for high-dimensional convex optimization problems arising in inverse problems by leveraging training data.
- Mechanism: A parameterized algorithm is trained to minimize the objective function value on a class of problems, effectively learning algorithmic parameters or mirror potentials that accelerate convergence.
- Core assumption: The training data represents a diverse set of problem instances from the class of interest, allowing the learned algorithm to generalize to unseen problems.
- Evidence anchors:
  - [abstract] "The survey concludes by highlighting the potential of unsupervised methods to achieve competitive empirical performance compared to supervised approaches."
  - [section] "The unsupervised training objective can typically be written as minimizing the final objective value (averaged over the training problems)"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.402" (weak evidence for L2O in unsupervised imaging)
- Break Condition: If the training data is not representative of the problem class, or if the learned algorithm overfits to the training set, it may not generalize well to new problems.

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: Provides a principled way to compare probability distributions without requiring them to have the same support, crucial for unsupervised learning in imaging where ground-truth data may be unpaired.
  - Quick check question: What is the key advantage of using Wasserstein distance over other probability metrics in unsupervised learning for imaging?

- Concept: Convex Analysis and Monotone Operator Theory
  - Why needed here: Forms the theoretical foundation for analyzing the convergence of iterative algorithms used in both classical and learned reconstruction methods.
  - Quick check question: How does the concept of maximal monotonicity relate to the convergence of proximal splitting algorithms?

- Concept: Generative Adversarial Networks (GANs) and Cycle Consistency
  - Why needed here: Enables unsupervised learning of image-to-image mappings by training generators and discriminators in an adversarial setting while enforcing cycle-consistency to regularize the learned mappings.
  - Quick check question: What is the purpose of cycle-consistency in Cycle-GAN architectures, and how does it help in unsupervised learning?

## Architecture Onboarding

- Component map: Unpaired images (X) -> Generators -> Measurements (Y) -> Discriminators -> Real/Fake classification
- Critical path:
  1. Preprocess and prepare unpaired training data
  2. Choose an unsupervised learning framework (e.g., Cycle-WGAN, adversarial regularization)
  3. Design and train the necessary networks (generators, discriminators, regularizers)
  4. Evaluate the performance on held-out data
  5. Iterate on the architecture and hyperparameters
- Design tradeoffs:
  - Expressiveness vs. generalization: More complex networks may fit the training data better but risk overfitting
  - Computational cost vs. performance: More iterations or larger networks may improve results but increase training time
  - Prior knowledge vs. data-driven learning: Incorporating domain knowledge can improve performance but may limit flexibility
- Failure signatures:
  - Poor convergence or instability during training
  - Reconstructions that do not satisfy data consistency or look unrealistic
  - Overfitting to the training data, resulting in poor generalization
- First 3 experiments:
  1. Implement a simple Cycle-GAN architecture for image-to-image translation between two unpaired datasets
  2. Train an adversarial regularizer to distinguish between ground-truth images and pseudo-inverse reconstructions
  3. Apply a learned optimization algorithm (e.g., learned mirror descent) to accelerate the solution of a model-based reconstruction problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the information gap between supervised and unsupervised learning methods be quantified, and under what limiting cases can this gap be minimized?
- Basis in paper: [explicit] The paper discusses the inherent difference in information available in supervised versus unsupervised regimes and suggests this as an important direction for future research.
- Why unresolved: While the paper mentions this gap, it does not provide a concrete framework for quantifying the difference in information or strategies for minimizing it in specific scenarios.
- What evidence would resolve it: Developing mathematical frameworks to measure the information content in supervised vs. unsupervised approaches, along with empirical studies comparing performance under controlled information limitations.

### Open Question 2
- Question: Can a more general theoretical framework be developed for building unsupervised models that goes beyond the classical results in optimal transport and convex analysis?
- Basis in paper: [explicit] The paper suggests that unsupervised learning is much broader once the restriction to classical results is lifted, mentioning physics-informed neural networks as an example.
- Why unresolved: While the paper reviews methods based on optimal transport and convex analysis, it acknowledges the need for more general frameworks that can incorporate diverse prior knowledge.
- What evidence would resolve it: Proposing and validating new theoretical frameworks that can handle a wider range of unsupervised learning problems, potentially incorporating probabilistic or geometric ideas beyond classical optimization theory.

### Open Question 3
- Question: How can the performance of adversarial regularizers be improved to achieve a better compromise between empirical performance and theoretical certificates?
- Basis in paper: [explicit] The paper notes that numerical experiments indicate a lack of expressive power of adversarial convex regularizers (ACRs) compared to their non-convex counterparts, despite strong theoretical guarantees.
- Why unresolved: The paper mentions this limitation but does not provide a concrete solution to bridge the gap between the theoretical guarantees of ACRs and their practical performance.
- What evidence would resolve it: Developing new regularization schemes that maintain the theoretical guarantees of convexity while incorporating elements that improve empirical performance, possibly through relaxed convexity constraints or hybrid approaches.

## Limitations
- The survey lacks specific quantitative comparisons between different unsupervised approaches across benchmark datasets
- Several methods are described theoretically without sufficient empirical validation data presented
- Many techniques remain at conceptual stage without extensive practical validation on real-world imaging problems

## Confidence

**Key Limitations:**
The survey provides a comprehensive overview of unsupervised methods but lacks specific quantitative comparisons between different approaches. Several methods are described theoretically without sufficient empirical validation data presented. The paper acknowledges that many techniques remain at a conceptual stage without extensive practical validation.

**Confidence Assessment:**
- **High confidence**: The theoretical foundations linking optimal transport to unsupervised learning frameworks (cycle-consistency, adversarial regularization)
- **Medium confidence**: Claims about competitive performance relative to supervised approaches, based on general statements rather than specific benchmarks
- **Low confidence**: Quantitative claims about convergence rates and reconstruction quality, as these depend heavily on specific implementation details not provided

## Next Checks
1. Implement and benchmark multiple unsupervised methods (cycle-GAN, adversarial regularization, learned optimization) on a standard inverse problem dataset with controlled measurement operators
2. Compare reconstruction quality metrics (PSNR, SSIM) between unsupervised approaches and supervised baselines across varying levels of measurement noise
3. Analyze the sensitivity of unsupervised methods to forward operator conditioning and measurement model assumptions through systematic ablation studies