---
ver: rpa2
title: Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance
arxiv_id: '2309.16604'
source_url: https://arxiv.org/abs/2309.16604
tags:
- fngw
- graph
- distance
- learning
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Fused Network Gromov-Wasserstein (FNGW)
  distance for comparing graphs with both node and edge features. The FNGW distance
  generalizes both the Network Gromov-Wasserstein and Fused Gromov-Wasserstein distances,
  allowing flexible incorporation of edge features while preserving desirable metric
  properties.
---

# Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance

## Quick Facts
- arXiv ID: 2309.16604
- Source URL: https://arxiv.org/abs/2309.16604
- Authors: 
- Reference count: 40
- Key outcome: Proposes Fused Network Gromov-Wasserstein (FNGW) distance for comparing graphs with node and edge features, showing improvements over kernel-based methods in classification and competitive performance in graph prediction tasks.

## Executive Summary
This paper introduces the Fused Network Gromov-Wasserstein (FNGW) distance, a novel approach for comparing graphs with both node and edge features. FNGW generalizes both Network Gromov-Wasserstein and Fused Gromov-Wasserstein distances, allowing flexible incorporation of edge features while preserving metric properties. The authors provide efficient algorithms for computing the FNGW distance and barycenter in the discrete case, and demonstrate significant improvements over state-of-the-art kernel-based methods in graph classification tasks and competitive performance with fingerprint-based approaches in supervised graph prediction.

## Method Summary
The FNGW distance is computed using a tensor-based formulation and Conditional Gradient Descent (CGD) algorithm. For two graphs g = (F, A, E, p) and g̃ = (F̃, Ã, Ē, p̃), the method iteratively updates a transport plan π to minimize the expected cost between the graphs. The barycenter computation uses Block Coordinate Descent (BCD) with closed-form updates for certain components. The distance can be used as a kernel in SVM classifiers or as a loss function in structured prediction tasks.

## Key Results
- FNGW outperforms NSPDK and FGW on multiple graph classification datasets, especially when edge features are informative
- FNGW approaches fingerprint-based methods on metabolite identification tasks, validating its use as a loss in structured prediction
- The distance admits an Implicit Loss Embedding (ILE), ensuring statistical consistency in learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FNGW generalizes both Network Gromov-Wasserstein (NGW) and Fused Gromov-Wasserstein (FGW) distances, enabling flexible incorporation of edge features while preserving metric properties.
- Mechanism: By introducing a bounded continuous measurable function ω mapping edge pairs to a metric space, FNGW combines node and edge feature comparisons into a unified transport-based distance. The trade-off parameters (α, β) control the relative weight of edge features vs node features and structural alignment.
- Core assumption: Edge features can be meaningfully encoded as values in a metric space and compared using a metric distance dΩ.
- Evidence anchors:
  - [abstract]: "FNGW distance generalizes both the Network Gromov-Wasserstein and Fused Gromov-Wasserstein distances, allowing flexible incorporation of edge features while preserving desirable metric properties."
  - [section 2]: Definition 2.1 explicitly defines ω as a function from X×X to a metric space (Ω, dΩ), and Theorem 2.5 proves metric properties.
  - [corpus]: Neighbor papers confirm FNGW is an extension of FGW for structured data with edge features.
- Break condition: If edge features cannot be mapped to a meaningful metric space, or if the metric dΩ is poorly chosen, the distance loses interpretability and discriminative power.

### Mechanism 2
- Claim: The discrete FNGW algorithm efficiently computes the distance and barycenter by leveraging tensor operations and conditional gradient descent.
- Mechanism: The FNGW distance in the discrete case is formulated as a quadratic optimization over transport plans. Efficient computation is achieved by expressing the cost in tensor form (equations 7, 8) and using conditional gradient descent with line search to update the transport plan. Barycenter computation uses block coordinate descent with closed-form updates for certain components.
- Core assumption: The edge feature space can be chosen such that the cost tensor L(E, ˜E) can be computed efficiently (e.g., Ω = RT with ℓ2 metric).
- Evidence anchors:
  - [section 2]: Equation 8 gives an efficient form for L(E, ˜E) ⊗ π when Ω = RT and q=2, with complexity O(n²mT + nm²T).
  - [section 3]: Algorithm 1 describes CGD for distance computation, and Algorithm 2 outlines BCD for barycenter computation.
  - [corpus]: Neighbor paper "Optimal transport distances for directed, weighted graphs" discusses computational challenges and solutions for OT on graphs.
- Break condition: For high-dimensional edge features or non-Euclidean metrics, the tensor operations become intractable and the algorithm slows significantly.

### Mechanism 3
- Claim: FNGW enables effective learning in both graph classification and graph prediction tasks by providing a meaningful similarity measure.
- Mechanism: In classification, FNGW-based kernels are used in SVM classifiers, capturing both node and edge similarities. In graph prediction, FNGW barycenters serve as surrogates for output graphs, and the distance itself is used as a loss function. The Implicit Loss Embedding (ILE) property ensures statistical consistency.
- Core assumption: The FNGW distance admits an ILE, allowing its use in surrogate regression frameworks.
- Evidence anchors:
  - [section 4.1]: Table 1 shows FNGW outperforms NSPDK and FGW on multiple graph classification datasets, especially when edge features are informative.
  - [section 3.3]: Proposition 3.7 proves FNGW admits an ILE, and Theorem A.10 guarantees universal consistency.
  - [section 4.4]: Table 2 shows FNGW approaches fingerprint-based methods on metabolite identification, validating its use as a loss in structured prediction.
- Break condition: If the ILE property does not hold (e.g., with incompatible metric spaces), the statistical guarantees fail and the method may underperform.

## Foundational Learning

- Concept: Optimal Transport (OT) and Gromov-Wasserstein (GW) distances
  - Why needed here: FNGW is built on OT theory; understanding GW distances is essential to grasp how FNGW compares structured objects.
  - Quick check question: What is the key difference between Wasserstein and Gromov-Wasserstein distances?

- Concept: Metric measure spaces and pushforward measures
  - Why needed here: Graphs are represented as metric measure spaces; proofs rely on properties of pushforward measures and couplings.
  - Quick check question: How does the pushforward operator # relate to measurable maps between spaces?

- Concept: Block Coordinate Descent (BCD) and Conditional Gradient Descent (CGD)
  - Why needed here: These optimization algorithms are used to compute FNGW distance and barycenter efficiently.
  - Quick check question: In CGD, what is the role of the line search, and why is it necessary?

## Architecture Onboarding

- Component map: Input graphs -> (F, A, E, p) format -> FNGW distance computation -> Distance matrix/Barycenter -> Learning interface -> Output
- Critical path:
  1. Parse input graphs into (F, A, E, p) format
  2. Compute FNGW distance tensor costs (M, J, L)
  3. Run CGD to find optimal transport plan
  4. For barycenter: alternate between updating transport plans and feature tensors
  5. For learning: use distances as kernel or loss in downstream model
- Design tradeoffs:
  - Edge feature dimensionality vs computational cost: High T increases tensor size and slows L computation
  - Metric choice for edge features: ℓ2 is efficient but may not suit all data; custom metrics increase expressiveness but cost
  - Sparsity regularization (γ) in barycenter: Controls edge density but requires tuning; too high loses structure, too low overfits
- Failure signatures:
  - Distance computation diverges or takes too long: Likely due to high edge feature dimension or poor metric choice
  - Barycenter has incorrect edge structure: Sparsity parameter γ may be mis-tuned or edge features not informative
  - Learning performance poor: Possible ILE violation, kernel bandwidth mis-tuned, or edge features not discriminative
- First 3 experiments:
  1. Compute FNGW distance matrix on a small labeled graph dataset (e.g., MUTAG) and visualize clustering with single-linkage.
  2. Generate synthetic SBM graphs with edge labels, compute barycenter, and verify it recovers cluster structure.
  3. Run FNGW-based SVM on a graph classification task and compare to FGW and NSPDK baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational complexity of FNGW scale with increasing graph size and edge feature dimensionality, and what approximations could make it practical for very large graphs?
- Basis in paper: [inferred] The paper discusses computational complexity for certain cases (e.g., O(n²mT + nm²T) for specific metric choices) but does not provide a comprehensive analysis of scaling with graph size or edge feature dimensionality.
- Why unresolved: The paper focuses on demonstrating the effectiveness of FNGW on relatively small datasets and does not explore its performance on very large graphs or with high-dimensional edge features.
- What evidence would resolve it: Empirical studies comparing FNGW's runtime and memory usage on graphs of varying sizes and edge feature dimensionalities, along with proposed approximations or optimizations to improve scalability.

### Open Question 2
- Question: Can FNGW be extended to handle dynamic graphs where edge features evolve over time, and what modifications would be necessary to the distance formulation and algorithms?
- Basis in paper: [explicit] The paper mentions dynamic graphs as an application where edge features continuously evolve over time, but does not explore how FNGW could be adapted to handle such scenarios.
- Why unresolved: The paper focuses on static graphs and does not address the challenges of comparing graphs with time-varying edge features.
- What evidence would resolve it: A theoretical extension of FNGW to incorporate temporal dynamics in edge features, along with experimental validation on dynamic graph datasets.

### Open Question 3
- Question: How does the choice of hyperparameters (α, β, p, q) affect the performance of FNGW in different applications, and are there principled ways to select these parameters?
- Basis in paper: [explicit] The paper mentions that α and β are cross-validated in experiments, but does not provide a systematic analysis of how different hyperparameter choices impact performance across various tasks and datasets.
- Why unresolved: The paper focuses on demonstrating the effectiveness of FNGW with specific hyperparameter settings, but does not explore the sensitivity of results to these choices or provide guidelines for parameter selection.
- What evidence would resolve it: A comprehensive study analyzing the impact of different hyperparameter configurations on FNGW's performance across multiple tasks and datasets, along with proposed strategies for principled parameter selection.

## Limitations
- FNGW assumes edge features can be meaningfully embedded in a metric space, which may not hold for all real-world graph data types
- Computational complexity scales poorly with high-dimensional edge features due to tensor operations
- The choice of trade-off parameters (α, β) requires careful tuning and their optimal values are dataset-dependent

## Confidence
- High confidence in the theoretical foundations and metric properties (Theorems 2.4, 2.5, and Proposition 3.7 are mathematically rigorous)
- Medium confidence in computational efficiency claims (complexity analysis is provided but practical scaling needs empirical verification on larger datasets)
- Medium confidence in learning performance claims (experimental results show improvements but comparisons are limited to specific baselines on relatively small datasets)

## Next Checks
1. Test FNGW on larger graph datasets (thousands of nodes) to empirically verify computational scalability claims and identify performance bottlenecks
2. Conduct ablation studies varying α and β systematically across multiple datasets to understand parameter sensitivity and provide practical tuning guidelines
3. Compare FNGW against additional graph kernel methods and deep learning approaches on standardized benchmarks to establish relative performance more comprehensively