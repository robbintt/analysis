---
ver: rpa2
title: Analyzing Effects of Mixed Sample Data Augmentation on Model Interpretability
arxiv_id: '2303.14608'
source_url: https://arxiv.org/abs/2303.14608
tags:
- attribution
- augmentation
- data
- interpretability
- deletion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how mixed-sample data augmentation methods
  affect model interpretability. It introduces new inter-model metrics to evaluate
  interpretability across different models by separating faithfulness from robustness.
---

# Analyzing Effects of Mixed Sample Data Augmentation on Model Interpretability

## Quick Facts
- arXiv ID: 2303.14608
- Source URL: https://arxiv.org/abs/2303.14608
- Reference count: 40
- Key outcome: Mixed sample data augmentation methods (CutMix, SaliencyMix) improve performance but reduce model interpretability across multiple metrics including attribution alignment, faithfulness, and concept detection.

## Executive Summary
This paper investigates how mixed-sample data augmentation strategies affect model interpretability by introducing new inter-model metrics to evaluate interpretability across different models. The study finds that models trained with CutMix and SaliencyMix show lower interpretability, with less aligned attribution maps, reduced faithfulness, and fewer human-recognizable concepts detected, despite better performance. This suggests that such augmentations may be unsuitable in safety-critical applications where interpretability is essential. The authors propose separating faithfulness from robustness in interpretability evaluation and demonstrate that augmented models show degraded performance across all three interpretability criteria examined.

## Method Summary
The study trains five ResNet-50 models on ImageNet using different augmentation strategies (Cutout, Mixup, CutMix, SaliencyMix) and evaluates their interpretability using three criteria: human-model alignment (Energy-based Pointing Game, Effective Hit Ratio), model faithfulness (inter-model deletion/insertion experiments), and human-recognizable concepts (Network Dissection). Models are trained for 300 epochs with batch size 256 and learning rate 0.1 decayed at 75, 150, and 225 epochs. Attribution maps are generated using GradCAM and IBA methods, and the number of unique concept detectors is counted to assess concept-based interpretability.

## Key Results
- Models trained with CutMix and SaliencyMix show lower interpretability with less aligned attribution maps compared to baseline
- Mixed sample augmentation strategies reduce model faithfulness as measured by inter-model deletion/insertion experiments
- The number of human-recognizable concepts detected by individual units decreases in augmented models, with SaliencyMix showing the least concept detectors
- Despite performance improvements, augmented models exhibit degraded interpretability across all three probed criteria

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixed sample data augmentation strategies degrade model interpretability despite improving performance.
- Mechanism: Data mixing strategies alter the statistical distribution of training data, leading to attribution maps that are less aligned with human expectations and less faithful to the model's internal reasoning.
- Core assumption: The quality of interpretability can be quantified by alignment with human annotations, faithfulness to the model, and the number of human-recognizable concepts detected.
- Evidence anchors: [abstract] "models trained with CutMix and SaliencyMix show lower interpretability, with less aligned attribution maps, reduced faithfulness, and fewer human-recognizable concepts"; [section 2.1] "We find that modern mixed sample augmentation strategies degrade model interpretability on all three probed criteria"
- Break condition: If new attribution methods emerge that can recover interpretability while preserving augmentation benefits, or if human interpretability evaluation shows no correlation with the proposed metrics.

### Mechanism 2
- Claim: The deletion/insertion experiments reveal that augmentation strategies reduce model faithfulness by making the model more robust to input perturbations.
- Mechanism: Augmentation techniques expose models to altered inputs during training, which makes them less sensitive to individual pixel changes, thereby reducing the faithfulness of attribution maps that rely on feature importance.
- Evidence anchors: [section 5.3] "we obtain a new plot that the effect of robustness is removed by subtracting RaO from LeRF (Fig. 4c). We call this inter-model deletion"; [section 5.3] "Mixup, CutMix, and SaliencyMix scored lower than baseline, and Cutout scored higher than baseline"
- Break condition: If robustness and faithfulness can be decoupled through new experimental designs or if attribution methods can be adapted to account for augmented training.

### Mechanism 3
- Claim: Network dissection experiments show that augmentation strategies reduce the number of human-recognizable concepts detected by individual units, indicating poorer model disentanglement.
- Mechanism: Data mixing strategies create composite images that confuse individual units, making them less specialized in detecting specific concepts and reducing the overall interpretability of the model's internal representations.
- Evidence anchors: [section 5.4] "The number of human-recognizable concepts is decreased in augmented models"; [section 5.4] "The least number of detectors are found in SaliencyMix in every category"
- Break condition: If new augmentation strategies can be designed to preserve or enhance concept disentanglement, or if alternative interpretability metrics show no correlation with concept detection.

## Foundational Learning

- Concept: Feature Attribution Methods
  - Why needed here: Understanding how attribution maps are generated and evaluated is crucial for interpreting the results of the interpretability experiments.
  - Quick check question: What is the difference between activation-based and perturbation-based attribution methods?

- Concept: Model Faithfulness and Robustness
  - Why needed here: The deletion/insertion experiments aim to separate faithfulness from robustness, which is essential for understanding how augmentation affects interpretability.
  - Quick check question: How do the inter-model deletion and insertion experiments help isolate faithfulness from robustness?

- Concept: Concept-Based Interpretability
  - Why needed here: Network dissection experiments evaluate interpretability based on the number of human-recognizable concepts detected by individual units, which is a key aspect of the study.
  - Quick check question: What is the role of the IoU threshold in determining whether a unit is a detector of a specific concept?

## Architecture Onboarding

- Component map: ResNet-50 models trained on ImageNet with different augmentation strategies (Cutout, Mixup, CutMix, SaliencyMix) -> attribution maps generated using GradCAM and IBA -> interpretability evaluated using human-model alignment metrics (EnergyPG, EHR), faithfulness metrics (inter-model deletion/insertion), and network dissection for concept detection.

- Critical path: Train models with different augmentation strategies → Generate attribution maps → Evaluate interpretability using proposed metrics → Compare results to identify impact of augmentation on interpretability.

- Design tradeoffs: The study prioritizes interpretability over performance, which may not be suitable for all applications. The choice of attribution methods and evaluation metrics also involves tradeoffs between comprehensiveness and computational efficiency.

- Failure signatures: If the proposed metrics do not correlate with human interpretability judgments, or if the results are inconsistent across different datasets or architectures, the study's conclusions may be challenged.

- First 3 experiments:
  1. Train baseline and augmented models on a smaller dataset (e.g., CIFAR-10) to verify the consistency of results.
  2. Implement additional attribution methods (e.g., SHAP, LIME) to compare their interpretability with GradCAM and IBA.
  3. Conduct human evaluation studies to validate the proposed interpretability metrics against human judgments.

## Open Questions the Paper Calls Out

- Open Question 1: What specific mechanisms cause mixed sample data augmentation to reduce the interpretability of attribution maps?
  - Basis in paper: [explicit] The paper notes that models trained with CutMix and SaliencyMix show less aligned attribution maps, reduced faithfulness, and fewer human-recognizable concepts. It hypothesizes that augmented images change the data distribution, but this is not fully explained.
  - Why unresolved: The paper provides qualitative observations and hypothesis about data distribution changes, but does not provide a mechanistic explanation for why these augmentations specifically reduce interpretability.
  - What evidence would resolve it: Experiments isolating the contribution of each augmentation component (e.g., class mixing vs. region mixing) to interpretability changes, or ablation studies showing which aspects of the augmentation process affect interpretability the most.

- Open Question 2: Do mixed sample data augmentation strategies affect interpretability differently across different model architectures or domains (e.g., vision vs. language)?
  - Basis in paper: [inferred] The paper only tests ResNet-50 on ImageNet, noting this as a limitation. The observed effects might be architecture-specific.
  - Why unresolved: The study is limited to a single architecture (ResNet-50) and domain (computer vision), leaving open whether these interpretability effects generalize.
  - What evidence would resolve it: Experiments applying the same analysis to different architectures (e.g., transformers, CNNs with different depths) and domains (text, audio, multimodal data).

- Open Question 3: Can interpretability be preserved while still benefiting from the performance gains of mixed sample data augmentation?
  - Basis in paper: [explicit] The paper concludes that mixed sample data augmentation is not a panacea - it boosts performance but degrades interpretability, suggesting the need for augmentation strategies that achieve high performance without negative interpretability impacts.
  - Why unresolved: While the paper identifies the trade-off, it does not explore whether this trade-off is fundamental or whether alternative augmentation approaches could preserve interpretability.
  - What evidence would resolve it: Development and testing of modified augmentation strategies that maintain interpretability while preserving performance gains, or methods to "repair" interpretability after augmentation training.

## Limitations

- The study is limited to ResNet-50 architecture on ImageNet, limiting generalizability to other models and domains.
- The proposed interpretability metrics rely on specific attribution methods and human-model alignment measures that may not capture all aspects of interpretability.
- Network dissection experiments depend on the availability and quality of human annotations for concept detection.

## Confidence

- High confidence in the overall finding that mixed-sample augmentation strategies degrade model interpretability, supported by consistent results across multiple metrics and attribution methods.
- Medium confidence in the specific mechanisms proposed (faithfulness, robustness, concept disentanglement), as they are based on reasonable assumptions but may not capture all factors influencing interpretability.
- Low confidence in the generalizability of results to other datasets, architectures, or attribution methods, as the study focuses on a specific setup.

## Next Checks

1. Reproduce results on CIFAR-10: Train ResNet-50 models on CIFAR-10 using the same augmentation strategies and evaluate interpretability using the proposed metrics to verify consistency across datasets.

2. Implement additional attribution methods: Compare the interpretability of GradCAM and IBA with other popular methods (e.g., SHAP, LIME) to ensure that the observed degradation is not specific to the chosen attribution techniques.

3. Conduct human evaluation studies: Perform user studies to validate the proposed interpretability metrics against human judgments, ensuring that the metrics align with human perceptions of model interpretability.