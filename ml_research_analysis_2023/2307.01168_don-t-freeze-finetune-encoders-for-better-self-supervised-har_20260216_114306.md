---
ver: rpa2
title: 'Don''t freeze: Finetune encoders for better Self-Supervised HAR'
arxiv_id: '2307.01168'
source_url: https://arxiv.org/abs/2307.01168
tags:
- pretext
- data
- dataset
- capture24
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Finetuning encoders improves SSL-based HAR by 10-15% macro F1 over
  freezing. Gains strongest when labelled data is scarce.
---

# Don't freeze: Finetune encoders for better Self-Supervised HAR

## Quick Facts
- arXiv ID: 2307.01168
- Source URL: https://arxiv.org/abs/2307.01168
- Reference count: 18
- Finetuning encoders improves SSL-based HAR by 10-15% macro F1 over freezing

## Executive Summary
This paper challenges the common practice of freezing encoders in self-supervised learning (SSL) for human activity recognition (HAR). Through extensive experiments across four target datasets and four pretext tasks, the authors demonstrate that simply finetuning the encoder (instead of freezing it) during supervised fine-tuning leads to substantial performance gains—10-15% macro F1—across all settings. The improvement is especially pronounced when labelled data is scarce, with models trained on just 2-10 windows per class reaching or surpassing baseline performance trained on 50-100 windows. The paper also shows that using a larger unlabelled dataset (Capture24) for SSL pretraining, combined with appropriate sampling strategies, further boosts performance by providing more diverse and representative features.

## Method Summary
The authors investigate SSL for HAR using four pretext tasks: Contrastive Predictive Coding (CPC), Multi-task (combining CPC and reconstruction), Reconstruction, and SimCLR. They compare freezing vs. finetuning the SSL-pretrained encoder during supervised fine-tuning on target datasets (myogym, pamap2, motionsense, mhealth). The encoder outputs 256-dimensional features (with linear layer added if smaller). Hyperparameter search uses 20 variations, 50 epochs, and early stopping. Sampling for SSL pretraining uses 10% of Capture24 per epoch, with window selection probability proportional to the sum of variance across acceleration channels. Classification uses a 3-layer network [256, 128, 128] with batch norm, ReLU, and 0.2 dropout. Evaluation uses 5-fold cross-validation with different user splits, repeated 5 times with different seeds.

## Key Results
- Finetuning encoders instead of freezing them improves HAR performance by 10-15% macro F1 across pretext tasks
- Performance gains are strongest when labelled data is scarce (inverse relationship with data availability)
- Using Capture24 for SSL outperforms target-only SSL, especially with variance-based sampling
- Best results with CPC and Multi-task pretexts; SimCLR shows negative or minimal improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning encoders instead of freezing them improves HAR performance by 10-15% macro F1 across pretext tasks.
- Mechanism: The encoder's parameters, learned during self-supervised pretext tasks, continue to adapt to the target domain during supervised fine-tuning. This allows the model to better align the learned representation with the classification task and specific activity patterns in the target dataset.
- Core assumption: The initial SSL-pretrained encoder contains useful but not perfectly aligned features for the downstream HAR task, and additional adaptation benefits performance.
- Evidence anchors:
  - [abstract] "a simple change - not freezing the representation - leads to substantial performance gains across pretext tasks."
  - [section] "The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data."
  - [corpus] Weak; corpus papers focus on HAR SSL but do not directly address freezing vs finetuning encoders.
- Break condition: If the pretext task representation is already highly aligned with the target classification task, or if the encoder is already near-optimal for the target domain, further adaptation may not help and could even hurt performance due to overfitting.

### Mechanism 2
- Claim: Using a larger unlabelled dataset (Capture24) for SSL improves performance over target-only SSL, especially with proper sampling.
- Mechanism: The larger dataset provides more diverse examples and better approximates the overall data distribution, leading to more robust and generalizable encoder features. Proper sampling (e.g., variance-based) helps focus on informative windows, approximating the target dataset's distribution and improving transfer.
- Core assumption: A bigger, more varied dataset leads to better initial representations, and proper sampling can bridge domain gaps.
- Evidence anchors:
  - [section] "Using a bigger dataset (Capture24) for SSL provided better results, probably due to the hyperparamter search being performed over that dataset, but it is also possible that it being a bigger and more varied dataset makes up for the change in domain if the appropriate sampling is performed."
  - [section] "Every window has a sampling probability proportional to the sum of variance of each acceleration channel...This sampling method approximates better the distribution of variance between Capture24 and the target datasets."
  - [corpus] Weak; corpus neighbors do not discuss dataset size effects or sampling strategies for SSL.
- Break condition: If the larger dataset is too different from the target domain, transfer benefits diminish. If sampling does not effectively match target distribution, benefits may be reduced.

### Mechanism 3
- Claim: The performance gain from finetuning encoders is inversely proportional to the amount of labelled data available.
- Mechanism: With scarce labelled data, the initial SSL-pretrained encoder is crucial as a strong starting point. Finetuning the encoder allows it to adapt more fully to the limited target data, compensating for the lack of labels. With abundant labelled data, the classifier can learn well even from a frozen encoder, so adaptation gains are smaller.
- Core assumption: The value of encoder adaptation increases when supervised data is limited because the initial SSL representation is more influential.
- Evidence anchors:
  - [abstract] "The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data."
  - [section] "From Figure 2 one can see that across datasets and pretexts we can often reach or surpass the performance of a baseline model with 50 or 100 windows per class by simply fine-tuning with 10 or fewer windows per class."
  - [corpus] Weak; corpus neighbors do not discuss the relationship between labelled data scarcity and encoder finetuning benefits.
- Break condition: If labelled data is plentiful, the marginal benefit of encoder adaptation diminishes, and overfitting risk increases.

## Foundational Learning

- Concept: Self-Supervised Learning (SSL) for representation learning
  - Why needed here: SSL allows learning useful representations from unlabelled data, addressing the scarcity of labelled HAR datasets.
  - Quick check question: How does SSL differ from supervised learning, and why is it beneficial for HAR?

- Concept: Contrastive learning and pretext tasks (CPC, reconstruction, multitask, SimCLR)
  - Why needed here: Different pretext tasks define how the encoder learns representations. Understanding their mechanisms and when each works best is crucial for applying SSL to HAR.
  - Quick check question: What is the main idea behind contrastive predictive coding (CPC), and how does it differ from reconstruction-based pretext tasks?

- Concept: Encoder freezing vs finetuning in transfer learning
  - Why needed here: The core contribution is that finetuning encoders (instead of freezing) improves SSL-based HAR. Understanding the implications of freezing vs adapting learned representations is key.
  - Quick check question: In transfer learning, what is the typical effect of freezing vs finetuning the pretrained encoder, and why might finetuning be beneficial here?

## Architecture Onboarding

- Component map: Encoder (SSL-pretrained) -> Classifier (3-layer [256, 128, 128] with batch norm, ReLU, dropout) -> Macro F1 evaluation
- Critical path: 1) SSL pretraining (encoder only) on unlabelled data (Capture24 or target). 2) Hyperparameter search for encoder architecture. 3) Finetuning encoder + classifier on labelled target data. 4) Evaluate with macro F1.
- Design tradeoffs: Larger SSL datasets vs. domain alignment; more complex encoders vs. overfitting risk; sampling strategies to focus on informative data vs. representativeness.
- Failure signatures: Poor performance with frozen encoders despite SSL pretraining; overfitting during finetuning with limited labels; worse performance when using SimCLR pretext on certain datasets.
- First 3 experiments:
  1. Replicate the comparison: frozen encoder vs. finetuned encoder on a simple pretext (e.g., reconstruction) and target dataset.
  2. Test the effect of dataset size: SSL pretraining on small vs. large unlabelled dataset with proper sampling.
  3. Investigate the data scarcity effect: finetuning with varying amounts of labelled data (e.g., 2, 10, 50, 100 windows per class).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does combining multiple datasets for self-supervised learning (SSL) affect performance in human activity recognition (HAR)?
- Basis in paper: [explicit] The paper mentions that combining different datasets when performing SSL is an open question, suggesting that this approach could be explored further.
- Why unresolved: The paper does not provide any experimental results or analysis on the effects of combining datasets for SSL, leaving this as a potential area for future research.
- What evidence would resolve it: Experimental results comparing the performance of SSL models trained on combined datasets versus single datasets, along with an analysis of the impact on model robustness and generalization.

### Open Question 2
- Question: What is the relationship between current SSL approaches and the physical constraints of target activities in HAR?
- Basis in paper: [explicit] The paper suggests that a deeper study of the relationship between SSL approaches and the physical constraints of target activities could be helpful in designing more informative pretext tasks for HAR.
- Why unresolved: The paper does not delve into how the physical characteristics of activities influence the effectiveness of SSL pretext tasks, leaving this as a theoretical consideration.
- What evidence would resolve it: Empirical studies linking the physical properties of activities (e.g., movement patterns, sensor readings) with the performance of various SSL pretext tasks, potentially leading to more tailored and effective SSL strategies.

### Open Question 3
- Question: How does continuing the pretext task while training the overall classification layers impact HAR performance?
- Basis in paper: [explicit] The paper mentions that continuing the pretext task while training the overall classification layers is an open question, suggesting it as a potential area for future research.
- Why unresolved: The paper does not explore the effects of joint training of pretext and classification tasks, which could provide insights into optimizing the learning process.
- What evidence would resolve it: Experimental results comparing the performance of models that continue pretext tasks during classification training versus those that do not, with an analysis of the impact on model accuracy and convergence speed.

## Limitations

- The paper does not provide exact user splits for cross-validation or detailed hyperparameter values for encoder and classifier architectures, introducing uncertainty in exact reproduction.
- The results are based on accelerometer data at 50Hz with 2-second windows, which may limit generalizability to other sensor modalities or data characteristics.
- While the paper shows that finetuning works better than freezing, it does not deeply analyze why certain pretext tasks benefit more than others or why SimCLR can hurt performance on some datasets.

## Confidence

- **High Confidence**: Finetuning encoders instead of freezing leads to 10-15% macro F1 improvement in SSL-based HAR, especially with limited labelled data.
- **Medium Confidence**: Using a larger unlabelled dataset (Capture24) for SSL pretraining improves performance, particularly with proper sampling.
- **Medium Confidence**: The improvement is inversely proportional to the amount of labelled data available; largest gains when labels are scarce.

## Next Checks

1. **Verify Core Claim**: Replicate the frozen vs. finetuned encoder comparison on a single pretext task (e.g., reconstruction) and target dataset (e.g., myogym), using the same evaluation protocol (5-fold CV, 5 seeds, macro F1). Confirm the ~10-15% improvement.

2. **Test Data Size Effect**: Pre-train encoders on small vs. large unlabelled datasets (e.g., target-only vs. Capture24), using the described variance-based sampling. Measure if larger datasets with proper sampling yield better downstream performance.

3. **Probe Data Scarcity Mechanism**: Systematically vary the number of labelled windows per class (e.g., 2, 5, 10, 50, 100) for a given pretext and dataset. Plot the relative improvement from finetuning as a function of label scarcity to confirm the inverse relationship.