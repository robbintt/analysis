---
ver: rpa2
title: Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports
arxiv_id: '2306.08749'
source_url: https://arxiv.org/abs/2306.08749
tags:
- report
- reports
- visit
- longitudinal
- radiology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using longitudinal multi-modal data (previous
  and current CXR images and previous report) to pre-fill the "findings" section of
  current radiology reports. A transformer-based model with cross-attention-based
  multi-modal fusion and hierarchical memory-driven decoder is proposed.
---

# Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports

## Quick Facts
- **arXiv ID:** 2306.08749
- **Source URL:** https://arxiv.org/abs/2306.08749
- **Reference count:** 24
- **Primary result:** Transformer-based model with cross-attention fusion and hierarchical memory decoder outperforms prior methods by ≥3% F1 score and ≥2% BLEU-4, METEOR, ROUGE-L on Longitudinal-MIMIC dataset

## Executive Summary
This paper introduces a transformer-based approach to pre-fill the "findings" section of radiology reports using longitudinal multi-modal data. The method leverages previous and current chest X-ray images along with the previous report to generate more clinically accurate and relevant findings for the current report. The model employs a cross-attention-based multi-modal fusion module and a hierarchical memory-driven decoder to effectively integrate and process the longitudinal information. Experiments on a new Longitudinal-MIMIC dataset demonstrate significant improvements over several recent approaches in terms of standard NLG metrics and clinical efficacy.

## Method Summary
The proposed method uses a transformer-based architecture with cross-attention-based multi-modal fusion and a hierarchical memory-driven decoder. The model takes as input the current visit CXR image, the previous visit CXR image, and the previous visit report. The images are encoded using ResNet-101 and Transformer blocks, while the report is encoded using Transformer blocks. A cross-attention fusion module aligns and integrates the previous CXR features and report embeddings to create a multi-modal longitudinal representation. This is then used by a hierarchical decoder with two sub-blocks (DI and DL) to generate the current report, with a memory matrix maintaining important patterns over time. The model is trained on the Longitudinal-MIMIC dataset for 30 epochs using the ADAM optimizer.

## Key Results
- Outperforms several recent approaches by ≥3% on F1 score for clinical label extraction
- Improves BLEU-4, METEOR, and ROUGE-L metrics by ≥2% compared to baselines
- Ablation studies confirm the effectiveness of the cross-attention fusion module and hierarchical decoder
- The approach shows promise for reducing radiologists' workload in report generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cross-attention fusion module allows the model to align and integrate features from previous and current CXR images with the previous report text, creating richer longitudinal representations than using each modality independently.
- Mechanism: The module computes attention scores between the encoded previous CXR features and previous report embeddings, and vice versa, then concatenates the resulting aligned representations to form a multi-modal longitudinal embedding.
- Core assumption: The encoded representations from different modalities (image and text) are in compatible vector spaces such that cross-attention can meaningfully align them.
- Evidence anchors:
  - [section] "A multi-modal fusion module integrated longitudinal representations of images and texts using a cross-attention mechanism [15]"
  - [section] "Finally, H I ∗ P and H R ∗ P were concatenated to obtain the multi-modal longitudinal representations H L"
  - [corpus] No direct evidence in corpus neighbors; this appears to be a novel architectural choice in the paper.
- Break condition: If the modalities are encoded in incompatible spaces or the attention mechanism fails to capture relevant correspondences, the fusion will not improve performance over simple concatenation.

### Mechanism 2
- Claim: The hierarchical memory-driven decoder with dual sub-blocks (DI and DL) allows the model to separately attend to current visit information and longitudinal information while maintaining a memory of generated outputs over time.
- Mechanism: Sub-block DI attends to current CXR embeddings and updates memory with current information; sub-block DL attends to the fused longitudinal embeddings. Outputs from both are combined to generate the report.
- Core assumption: Separating attention to current vs. longitudinal information in the decoder hierarchy improves coherence and relevance of generated text.
- Evidence anchors:
  - [section] "To incorporate the encoded H L and H IC, we use a hierarchical structure for each block that divides it into two sub-blocks: DI and DL"
  - [section] "Sub-block-1 uses H IC... Sub-block-2 DL is H dec,b... This structure is similar to sub-block-1, but interacts with H L instead of H IC"
  - [corpus] No direct evidence in corpus neighbors; this appears to be a novel architectural contribution.
- Break condition: If the separation of attention paths is not beneficial, or if the memory updates interfere with generation, the hierarchical structure may not outperform a flat decoder.

### Mechanism 3
- Claim: Using longitudinal data (previous visit CXR + report + current visit CXR) provides contextual information that improves the clinical accuracy and relevance of generated findings compared to using only current visit data.
- Mechanism: The model learns patterns in how patient conditions evolve across visits and can incorporate this into the generated report, leading to better alignment with ground truth clinical labels.
- Core assumption: Patient conditions and report content have meaningful temporal dependencies that can be captured by the model.
- Evidence anchors:
  - [abstract] "Experiments show that our approach outperforms several recent approaches by ≥3% on F1 score, and ≥2% for BLEU-4, METEOR and ROUGE-L respectively"
  - [section] "experiments that evaluated the utility of different components of our model proved its effectiveness for the task of pre-filling the 'findings' section of the report"
  - [corpus] No direct evidence in corpus neighbors; this is the core novel contribution of the paper.
- Break condition: If temporal dependencies are weak or noisy, or if the model overfits to specific patient trajectories, using longitudinal data may not improve and could even harm performance.

## Foundational Learning

- Concept: Cross-attention mechanisms
  - Why needed here: To align and fuse information from different modalities (images and text) in a way that captures their relationships.
  - Quick check question: In the cross-attention fusion module, what are the roles of the q(·), k(·), and v(·) functions?

- Concept: Hierarchical decoder structures
  - Why needed here: To separately process current and longitudinal information in the decoding process, potentially improving the coherence and relevance of generated text.
  - Quick check question: What are the two sub-blocks in the hierarchical decoder, and what information does each attend to?

- Concept: Memory-augmented transformers
  - Why needed here: To maintain a record of generated outputs and important patterns over time, which can be used to condition future generation steps.
  - Quick check question: How is the memory matrix M used in the decoder, and what is its purpose?

## Architecture Onboarding

- Component map:
  - Image Encoder: ResNet-101 + Transformer blocks → H_IC (current CXR), H_IP (previous CXR)
  - Text Encoder: Transformer blocks → H_RP (previous report)
  - Cross-Attention Fusion Module: Aligns H_IP and H_RP → H_L (longitudinal representation)
  - Hierarchical Memory-Driven Decoder: Two sub-blocks (DI and DL) with memory M → Generates current report
  - Components are connected: H_IC, H_L → Decoder → Report

- Critical path:
  1. Encode current CXR image → H_IC
  2. Encode previous CXR image → H_IP
  3. Encode previous report → H_RP
  4. Cross-attention fusion of H_IP and H_RP → H_L
  5. Hierarchical decoder takes H_IC and H_L, uses memory M → Generates current report

- Design tradeoffs:
  - Using cross-attention fusion vs. simple concatenation: Cross-attention may better align modalities but is more computationally expensive.
  - Hierarchical decoder vs. flat decoder: Hierarchical structure may improve coherence but adds complexity.
  - Using memory matrix vs. not: Memory can store important patterns but may also store noise.

- Failure signatures:
  - If cross-attention fusion fails: The model may perform similarly to using only current visit data, with no benefit from longitudinal information.
  - If hierarchical decoder fails: Generated reports may be less coherent or relevant than with a simpler decoder structure.
  - If memory matrix is not used effectively: The model may not improve over baselines that don't use memory.

- First 3 experiments:
  1. Train and evaluate the model with only current visit data (baseline) to establish performance without longitudinal information.
  2. Train and evaluate the model with only the previous visit report (no image) added to baseline to assess the contribution of text information.
  3. Train and evaluate the model with only the previous visit CXR image (no report) added to baseline to assess the contribution of image information.

## Open Questions the Paper Calls Out
- Question: How does the model handle cases where the clinical condition changes between visits (e.g., pneumothorax label changes from positive to negative)?
- Basis in paper: [explicit] Error Analysis section mentions this as a key challenge
- Why unresolved: The paper acknowledges this is difficult but doesn't propose specific solutions
- What evidence would resolve it: Results showing improved performance after implementing active learning or curriculum learning approaches to handle label changes

## Limitations
- Evaluation is limited to NLG metrics without validation on clinical correctness or downstream diagnostic utility
- Longitudinal-MIMIC dataset construction details are not specified, raising questions about temporal alignment and potential data leakage
- Architectural innovations (cross-attention, hierarchical decoder) lack detailed ablation studies to isolate individual contributions
- No qualitative analysis of generated reports to assess clinical coherence or appropriateness

## Confidence
- **High Confidence:** The model architecture as described is technically sound and implementable. The reported performance improvements over baseline methods are credible given the strong NLG metrics.
- **Medium Confidence:** The claimed benefits of longitudinal data and the specific architectural choices (cross-attention, hierarchical decoder) are plausible but not definitively proven due to lack of detailed ablation studies.
- **Low Confidence:** The clinical utility and real-world applicability of the generated reports are not established, limiting confidence in the practical value of the approach.

## Next Checks
1. **Ablation Studies:** Conduct controlled experiments removing the cross-attention fusion module and hierarchical decoder to quantify their individual contributions to performance gains.
2. **Clinical Validation:** Have radiologists review a sample of generated reports for clinical accuracy, coherence, and appropriateness, scoring them against ground truth reports.
3. **Generalization Test:** Evaluate the model on an external dataset (e.g., CheXpert or OpenI) to assess its ability to generalize beyond the MIMIC-CXR domain.