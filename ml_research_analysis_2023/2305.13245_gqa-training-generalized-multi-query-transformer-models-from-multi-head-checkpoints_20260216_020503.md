---
ver: rpa2
title: 'GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints'
arxiv_id: '2305.13245'
source_url: https://arxiv.org/abs/2305.13245
tags:
- attention
- multi-query
- heads
- head
- multi-head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to convert multi-head attention (MHA)
  models to multi-query attention (MQA) or grouped-query attention (GQA) using a small
  fraction of original training compute. MQA and GQA reduce memory bandwidth overhead
  during inference by using fewer key-value heads.
---

# GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
## Quick Facts
- arXiv ID: 2305.13245
- Source URL: https://arxiv.org/abs/2305.13245
- Reference count: 10
- The paper proposes a method to convert multi-head attention (MHA) models to multi-query attention (MQA) or grouped-query attention (GQA) using a small fraction of original training compute.

## Executive Summary
This paper addresses the memory bandwidth bottleneck in transformer inference by converting pre-trained multi-head attention models to more efficient multi-query or grouped-query attention structures. The authors demonstrate that by mean-pooling key and value projection matrices during checkpoint conversion, followed by a small amount of uptraining (5% of original steps), models can achieve inference speeds close to MQA while maintaining quality nearly equivalent to MHA. The proposed GQA approach provides a favorable tradeoff between speed and quality by sharing key-value heads within subgroups of query heads.

## Method Summary
The method involves converting a pre-trained multi-head attention checkpoint to MQA or GQA by mean-pooling all key and value projection matrices into single matrices. The converted checkpoint is then uptrained for 5% of the original training steps using the same pre-training recipe. This approach enables models to adapt to the new attention structure while preserving most of the original knowledge, resulting in models that are faster at inference while maintaining quality close to the original multi-head models.

## Key Results
- MQA and GQA models achieve quality close to MHA while being almost as fast as MQA
- GQA with 8 groups provides a favorable quality-speed tradeoff compared to MHA and MQA
- Mean-pooling key and value projection matrices during conversion performs better than selecting single heads or random initialization
- Both MQA and GQA benefit from 5% uptraining with diminishing returns beyond 10%

## Why This Works (Mechanism)
### Mechanism 1
Mean-pooling key and value projection matrices from all heads into a single head during checkpoint conversion preserves essential learned representations while enabling MQA/GQA structure. During conversion, all key projection matrices are averaged element-wise into one matrix, and similarly for value matrices. This creates a single shared head that retains aggregate information from the original multi-head setup. The core assumption is that the mean of multiple learned projection matrices contains sufficient information to represent the combined functionality of the original heads.

### Mechanism 2
Uptraining with a small fraction (5%) of original pre-training compute allows models to adapt to MQA/GQA structure while preserving most of the original knowledge. After checkpoint conversion, the model undergoes additional pre-training for α proportion of original steps using the same pre-training recipe. This fine-tuning period allows the model to adjust its weights to the new attention structure. The core assumption is that the model can efficiently adapt to the new attention structure with relatively few additional training steps because the core language understanding is already established.

### Mechanism 3
Grouped-query attention interpolates between MHA and MQA by sharing key-value heads within subgroups of query heads, providing a favorable quality-speed tradeoff. Query heads are divided into G groups, each sharing a single key head and value head. This reduces the number of key-value heads from H to G while maintaining more capacity than MQA's single head. The core assumption is that sharing key-value heads within groups preserves sufficient capacity for quality while reducing memory bandwidth overhead significantly.

## Foundational Learning
- **Attention mechanism in Transformers**: Understanding how multi-head attention works is crucial to grasp why MQA and GQA modify the standard approach. *Quick check*: In standard multi-head attention, how many key and value heads are there per query head?
- **Key-Value (KV) caching in autoregressive decoding**: The memory bandwidth bottleneck addressed by MQA/GQA comes from loading KV cache at each decoding step. *Quick check*: What is the primary memory bottleneck during autoregressive decoding in Transformers?
- **Model scaling and parameter efficiency**: The paper discusses how larger models scale the number of heads, making MQA's single head reduction more aggressive. *Quick check*: How does the number of attention heads typically scale with model size in large Transformers?

## Architecture Onboarding
- **Component map**: Input: Pre-trained MHA checkpoint -> Conversion module: Mean-pool key and value projection matrices -> Training module: Additional pre-training with α proportion of original steps -> Output: MQA or GQA model with reduced memory bandwidth requirements
- **Critical path**: 1. Load pre-trained MHA checkpoint 2. Convert key and value heads via mean-pooling 3. Initialize uptraining with small learning rate 4. Run α proportion of original training steps 5. Evaluate on downstream tasks
- **Design tradeoffs**: MHA vs MQA: Quality vs speed tradeoff; MQA vs GQA: Speed vs quality interpolation; α value: Training cost vs performance recovery; Number of GQA groups: Memory bandwidth vs model capacity
- **Failure signatures**: Loss spikes during uptraining (especially for MQA); Degraded performance on long-input tasks; Minimal performance gain from increasing α beyond 5-10%; Timing regressions when increasing GQA groups
- **First 3 experiments**: 1. Convert T5-Large MHA checkpoint to MQA using mean-pooling, then uptrain with α=0.05 2. Convert T5-XXL MHA checkpoint to GQA-8 using mean-pooling, then uptrain with α=0.05 3. Vary the number of GQA groups (1, 4, 8, 16, 32) and measure quality-speed tradeoff on a representative task

## Open Questions the Paper Calls Out
### Open Question 1
How does the performance of grouped-query attention (GQA) vary with different group sizes, and what is the optimal number of groups for balancing quality and speed? The paper discusses GQA as an interpolation between multi-head and multi-query attention, and mentions that increasing the number of groups from MQA only results in modest slow-downs initially. It also states that 8 groups were selected as a favorable middle ground. This question remains unresolved because the paper does not provide a detailed analysis of the performance of GQA with varying group sizes or the optimal number of groups for different model sizes and tasks.

### Open Question 2
How does the stability of multi-query attention (MQA) models compare to grouped-query attention (GQA) models when trained from scratch? The paper mentions that multi-query attention can lead to training instability, especially when combined with long input tasks. It also states that uptrained multi-query attention models are more stable but still display high variance, while uptrained grouped-query attention models appear to be stable. This question remains unresolved because the paper does not provide a direct comparison of the stability of MQA and GQA models when trained from scratch, only when uptrained from multi-head attention checkpoints.

### Open Question 3
How does the performance of grouped-query attention (GQA) compare to other methods for reducing memory bandwidth overhead, such as FlashAttention, quantization, model distillation, layer-sparse cross-attention, and speculative sampling? The paper mentions these methods as related work but does not provide a direct comparison of their performance with GQA. This question remains unresolved because the paper focuses on GQA and its performance compared to multi-head and multi-query attention, but does not compare it to other methods for reducing memory bandwidth overhead.

## Limitations
- The method demonstrates success with T5.1.1 checkpoints but may not generalize optimally to other transformer variants or domains
- The choice of α=0.05 is based on observed diminishing returns but lacks theoretical justification or extensive hyperparameter tuning
- The paper does not address potential performance degradation on tasks requiring very long context or specialized attention patterns

## Confidence
- **High Confidence**: The core claim that mean-pooling key and value projection matrices is superior to alternative conversion strategies (single head selection or random initialization) is well-supported by direct comparison and ablation studies.
- **Medium Confidence**: The assertion that GQA provides a favorable quality-speed tradeoff compared to MHA and MQA is supported by empirical results, but the optimal number of groups (8) is presented as a middle ground without rigorous analysis of the full tradeoff curve.
- **Medium Confidence**: The claim that 5% uptraining is sufficient for performance recovery is based on observed diminishing returns but lacks theoretical justification or extensive hyperparameter tuning to determine if this is universally optimal.

## Next Checks
1. **Architecture Generalization Test**: Convert checkpoints from different transformer architectures (e.g., BERT, GPT-style models) using the same mean-pooling approach and evaluate performance recovery to assess the method's broader applicability.
2. **Hyperparameter Sensitivity Analysis**: Systematically vary α (uptraining proportion) and G (number of GQA groups) across multiple model sizes to identify optimal configurations and quantify sensitivity to these hyperparameters.
3. **Long-Context Task Evaluation**: Design a benchmark task requiring very long input sequences (e.g., document-level QA or long-document summarization) to specifically test whether MQA/GQA structures degrade performance on tasks where attention patterns become more complex.