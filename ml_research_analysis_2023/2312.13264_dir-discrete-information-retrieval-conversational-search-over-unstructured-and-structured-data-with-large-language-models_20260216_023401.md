---
ver: rpa2
title: 'dIR -- Discrete Information Retrieval: Conversational Search over Unstructured
  (and Structured) Data with Large Language Models'
arxiv_id: '2312.13264'
source_url: https://arxiv.org/abs/2312.13264
tags:
- language
- text
- data
- query
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces dIR, a method for conversational search over
  unstructured and structured data using Large Language Models (LLMs). dIR addresses
  the challenge of querying both free text and structured knowledge by transforming
  text into an expressive structured representation using LLMs.
---

# dIR -- Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models

## Quick Facts
- arXiv ID: 2312.13264
- Source URL: https://arxiv.org/abs/2312.13264
- Authors: 
- Reference count: 18
- Key outcome: dIR enables arbitrarily complex queries over free text by converting unstructured data into structured columnar representations, achieving high Query Recall and Query Precision.

## Executive Summary
dIR introduces a novel method for conversational search over both unstructured and structured data using Large Language Models (LLMs). The approach transforms free text into expressive structured representations through a Discretize step, then queries these representations via text-to-SQL semantic parsing. This hybrid approach allows for complex queries that combine structured data (like prices) with information extracted from unstructured text (like product descriptions), addressing limitations of traditional Information Retrieval and SQL-based Knowledge Bases.

## Method Summary
dIR works by first using LLMs to extract structured key-value pairs from unstructured text fields, creating an inference table that joins with the original structured data. At query time, natural language questions are converted to SQL queries via few-shot text-to-SQL semantic parsing, which can reference both original structured fields and newly extracted fields. The method requires no model fine-tuning and preserves the SQL language standard while enabling arbitrarily complex queries over previously inaccessible free text information.

## Key Results
- Successfully answered complex queries like "Do you have a non-black 15-liter backpack under $400?" by considering both structured columns (price) and unstructured text columns (description)
- Demonstrated ability to query information originally stored as free text that traditional methods cannot access
- Achieved high Query Recall and Query Precision even for complex queries combining structured and unstructured data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: dIR enables arbitrarily complex queries over free text by converting unstructured data into structured columnar representations that can be queried via SQL.
- Mechanism: LLMs transform free text into structured key-value pairs (e.g., product_size: 15 liter) through a Discretize step. These structured fields are then joined with the original table and queried using text-to-SQL semantic parsing.
- Core assumption: Free text contains implicit regularities that can be extracted and represented as structured fields, and LLMs can reliably identify these patterns across diverse domains.
- Evidence anchors:
  - [abstract]: "Specifically, a Large Language Model (LLM) transforms text into expressive representation. After the text is extracted into columnar form, it can then be queried via a text-to-SQL Semantic Parser..."
  - [section]: "Consider product databases as exemplified in Figure 1, patient records, and restaurants... For a backpack product table, the description text field likely indicates the backpack size or handle type, among several other facts."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.312, average citations=0.0. Top related titles include SUQL and HyST, suggesting active research in hybrid retrieval.

### Mechanism 2
- Claim: The hybrid SQL interface unifies structured and unstructured data querying without requiring model fine-tuning or SQL language modification.
- Mechanism: The Discretize step creates structured columns from free text, which are then joined with the original table. Natural language questions are converted to SQL via few-shot text-to-SQL semantic parsing, allowing complex queries to leverage both structured and unstructured data.
- Core assumption: Text-to-SQL semantic parsers can reliably convert natural language questions into SQL queries that reference both original structured fields and newly extracted structured fields from free text.
- Evidence anchors:
  - [abstract]: "...provides a unified interface to query both free text and structured knowledge... no model fine-tuning is required, and neither is any modification of the SQL language standard."
  - [section]: "At query time, natural language questions are converted via few-shot LLM text-to-SQL Semantic Parsing... it returns maximum insight from the data."
  - [corpus]: Related work includes SUQL and HyST, which also explore hybrid querying approaches, suggesting this mechanism addresses a recognized need.

### Mechanism 3
- Claim: dIR achieves high Query Recall and Query Precision for complex queries that traditional methods cannot handle.
- Mechanism: By extracting structured representations from free text, dIR can answer queries that require reasoning over both structured columns (e.g., price) and unstructured text (e.g., product descriptions), which traditional IR and SQL-based methods cannot address.
- Core assumption: The structured representation extracted from free text captures the necessary information to answer complex queries that involve both structured constraints and unstructured content.
- Evidence anchors:
  - [abstract]: "dIR demonstrated the ability to answer arbitrarily complex queries on information originally stored as free text... achieving high Query Recall and Query Precision even for complex queries."
  - [section]: "Where desired, such conversation may be effected by a multi-step reasoning conversational agent... Given the arbitrary complexity of the queries studied, SQL stores string functions and vector databases can't but produce both low Query Recall and Query Precision."
  - [corpus]: The proprietary dataset includes 33 sub-domains and 11,967 products, providing a diverse evaluation environment that demonstrates the approach's effectiveness across domains.

## Foundational Learning

- Concept: Large Language Models and Few-Shot Learning
  - Why needed here: LLMs are used for both the Discretize step (extracting structured fields from free text) and the text-to-SQL step (converting natural language questions to SQL) without requiring model fine-tuning.
  - Quick check question: How do few-shot prompts enable LLMs to perform new tasks without fine-tuning, and what are the limitations of this approach?

- Concept: Text-to-SQL Semantic Parsing
  - Why needed here: This component converts natural language questions into SQL queries that can be executed against the hybrid structured table (original + extracted fields).
  - Quick check question: What are the key challenges in text-to-SQL parsing, and how does the hybrid nature of the table (with extracted fields) affect the parsing process?

- Concept: Information Retrieval and Dense Embeddings
  - Why needed here: Understanding the limitations of traditional IR methods (e.g., dense embeddings, Approximate Nearest Neighbors) helps explain why dIR is necessary for complex queries over free text.
  - Quick check question: How do traditional IR methods using dense embeddings handle complex queries that require reasoning over both structured and unstructured data, and what are their limitations?

## Architecture Onboarding

- Component map:
  - Data Preparation: Original table with structured and unstructured data
  - Discretize Step: LLM extracts structured fields from free text
  - Enumerate Step: Reduces extracted fields into enumerated types
  - Generate Table: Creates new table with extracted structured fields
  - Text-to-SQL Semantic Parser: LLM converts natural language questions to SQL
  - Query Execution: SQL queries executed against joined tables

- Critical path:
  1. Discretize free text to extract structured fields
  2. Enumerate and generate new structured table
  3. Convert natural language question to SQL via text-to-SQL parser
  4. Execute SQL query against joined tables
  5. Return query results

- Design tradeoffs:
  - Number of extracted fields vs. SQL column limits and LLM context window
  - Single table for all domains vs. separate tables per domain (complexity vs. cost)
  - Cross-domain grounding in SQL vs. domain-specific queries

- Failure signatures:
  - Low Query Recall/Query Precision: Indicates the structured representation is not capturing relevant information from free text
  - Text-to-SQL parsing errors: Indicates the natural language question is too complex or ambiguous for the parser
  - SQL execution errors: Indicates the query is too complex for the database or the joined table structure is incorrect

- First 3 experiments:
  1. Validate Discretize step: Test LLM's ability to extract structured fields from free text in a single domain
  2. Validate Text-to-SQL parsing: Test LLM's ability to convert natural language questions to SQL queries referencing both original and extracted fields
  3. End-to-end query test: Execute a complex query that requires reasoning over both structured and unstructured data and evaluate Query Recall/Query Precision

## Open Questions the Paper Calls Out

- Question: What is the optimal number of columns to extract from unstructured text to balance SQL system limitations and LLM inference costs?
- Basis in paper: [inferred] The paper discusses the trade-offs between the number of columns and the limitations of SQL systems and LLM inference.
- Why unresolved: The paper suggests capping the number of fields extracted but does not provide a specific optimal number.
- What evidence would resolve it: Experimental results comparing the performance of dIR with different numbers of extracted columns, considering both SQL system limitations and LLM inference costs.

- Question: How can cross-domain grounding be effectively implemented in the text-to-SQL semantic parsing step to improve query accuracy?
- Basis in paper: [explicit] The paper mentions the importance of cross-domain grounding in generated SQL and suggests providing cross-table/domain grounding.
- Why unresolved: The paper does not provide a detailed method for implementing cross-domain grounding.
- What evidence would resolve it: A detailed methodology for cross-domain grounding and experimental results demonstrating improved query accuracy.

- Question: What are the potential benefits and drawbacks of storing different data domains in shared tables versus separate tables?
- Basis in paper: [inferred] The paper discusses the decision to store each sub-domain of data in separate tables and mentions the possibility of optimizing this in the future.
- Why unresolved: The paper does not provide a comprehensive analysis of the trade-offs between shared and separate tables.
- What evidence would resolve it: A comparative study of the performance of dIR with shared and separate tables, considering factors such as query complexity and data domain depth.

## Limitations
- The proprietary nature of the evaluation dataset prevents independent verification of the reported Query Recall and Query Precision metrics
- The claims about achieving "arbitrarily complex queries" lack comprehensive empirical validation on public benchmarks
- The reliance on few-shot learning without fine-tuning introduces variability in performance that isn't fully characterized

## Confidence

- High confidence: The core mechanism of using LLMs to extract structured fields from text and query via text-to-SQL is technically sound and well-described
- Medium confidence: The claim about handling "arbitrarily complex queries" is supported by the architecture but lacks comprehensive empirical validation
- Medium confidence: The performance comparison to traditional methods is reasonable given the described limitations of IR and SQL approaches, but the proprietary dataset prevents full verification

## Next Checks

1. Implement the Discretize step on a public dataset with mixed structured/unstructured data (e.g., Amazon product data) and evaluate the quality and coverage of extracted fields
2. Test the text-to-SQL semantic parser on progressively more complex queries that combine structured and unstructured constraints, measuring accuracy and failure modes
3. Conduct a controlled experiment comparing dIR against traditional IR methods and SQL-based approaches on identical query sets, measuring Query Recall and Precision across different query complexity levels