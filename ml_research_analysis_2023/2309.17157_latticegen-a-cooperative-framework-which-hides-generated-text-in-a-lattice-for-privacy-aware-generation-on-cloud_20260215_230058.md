---
ver: rpa2
title: 'LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice
  for Privacy-Aware Generation on Cloud'
arxiv_id: '2309.17157'
source_url: https://arxiv.org/abs/2309.17157
tags:
- noise
- generation
- server
- lattice
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LatticeGen introduces a cooperative framework for privacy-aware
  generation with large language models on the cloud, where users can keep their generated
  text private while the server handles most computation. The core idea is to mix
  true generated tokens with noise tokens in a lattice structure, making it difficult
  for a malicious server to recover the true sequence.
---

# LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud

## Quick Facts
- arXiv ID: 2309.17157
- Source URL: https://arxiv.org/abs/2309.17157
- Reference count: 40
- Key outcome: Introduces LatticeGen framework that hides generated text in noise-mixed lattices, achieving >50% semantic hiding under strong RBS attacks while maintaining generation capability

## Executive Summary
LatticeGen is a cooperative framework for privacy-aware text generation on cloud servers, where users can generate text while keeping it private from potentially malicious servers. The key insight is to mix true generated tokens with noise tokens in a permuted lattice structure, making it difficult for servers to distinguish true from false tokens. Users sample true and noise tokens, shuffle them, and send to the server, which performs inference on the linearized lattice but cannot identify which tokens are real. The framework includes the mixing noise scheme to defend against repeated beam-search attacks that could otherwise recover the true sequence by iteratively removing high-probability noise hypotheses.

## Method Summary
LatticeGen modifies standard autoregressive generation by having users send N tokens (1 true + N-1 noise) to the server in each step, shuffled randomly. The server linearizes the lattice and runs one forward pass, returning token distributions. Users then apply the inverse permutation to identify the true token and sample the next set. To defend against repeated beam-search attacks, LatticeGen uses a mixing noise scheme where noise tokens are sometimes sampled from the true token's distribution, creating branching paths that break the attack's assumptions. The approach requires finetuning the LLM to handle linearized lattice inputs, and can use bigram units to improve generation quality at the cost of increased computation.

## Key Results
- LatticeGen successfully protects true generation with >50% semantic hiding under repeated beam-search attacks as measured by BERTScore
- The mixing noise scheme provides better privacy protection than parallel noise scheme against RBS attacks
- Quality degradation is inherent to the privacy protection, with perplexity increasing as lattice width grows

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LatticeGen hides true tokens by mixing them with noise tokens in a permuted lattice.
- **Mechanism:** The user samples one true token and N-1 noise tokens per time step, shuffles them, and sends the shuffled list to the server. The server computes next-token distributions for all tokens, but cannot distinguish which is the true one. This breaks the server's ability to reconstruct the sequence.
- **Core assumption:** The server cannot invert the permutation without the user's private random seed.
- **Evidence anchors:**
  - [abstract] "The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice."
  - [section 3.1] "To prevent the server from knowing which one is the true token, the user will randomly shuffle the list before sending it to the server."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.421, average citations=0.0.
- **Break condition:** If the server can guess the permutation mapping or the noise tokens are trivially predictable, the hiding fails.

### Mechanism 2
- **Claim:** Repeated beam-search (RBS) attacks iteratively remove high-probability noise sequences, revealing the true sequence.
- **Mechanism:** The server performs beam-search N-1 times. After each run, it removes the found hypothesis from the lattice. Eventually, the true sequence is left as the last hypothesis.
- **Core assumption:** Noise tokens are generated independently and can form high-probability sequences.
- **Evidence anchors:**
  - [section 4.1] "After obtaining the resulting hypothesis sequence of the i-th beam-search... the true sequence is therefore revealed in the end as ˆwN."
  - [section 4] "The intuition of why the RBS attack is effective against the parallel noise scheme..."
  - [corpus] Average neighbor FMR=0.421 suggests moderate relatedness to attack/defense topics.
- **Break condition:** If noise sequences are not independent or have low probability, RBS fails.

### Mechanism 3
- **Claim:** Mixing noise scheme defends against RBS by making true and noise sequences interdependent.
- **Mechanism:** With mix-ratio probability, noise tokens are sampled from the true token's distribution instead of their own. This creates branching paths, breaking the independence needed for RBS.
- **Core assumption:** Branching noise paths reduce the probability of clean noise sequences, preventing RBS from isolating them.
- **Evidence anchors:**
  - [section 4.2] "The key is to make the true sequence 'branch' out to the noise sequences, which breaks the continuity of the noise sequences."
  - [section 5.2] "The mixing scheme... achieves the best protection under the RBS attack."
  - [corpus] Found 25 related papers; FMR=0.421 indicates some related research exists.
- **Break condition:** If mix-ratio is too low, noise remains independent; if too high, true sequence probability dominates.

## Foundational Learning

- **Concept:** Autoregressive language model generation
  - **Why needed here:** LatticeGen modifies the standard token-by-token generation to hide true tokens.
  - **Quick check question:** In standard LM, what is the next-token distribution conditioned on? (Answer: the full previous sequence.)

- **Concept:** Lattice structure and linearization
  - **Why needed here:** The lattice encodes multiple token hypotheses per step; linearization allows LLM inference.
  - **Quick check question:** How many possible sequences does a width-N lattice of length T represent? (Answer: N^T.)

- **Concept:** Beam-search algorithm
  - **Why needed here:** RBS is based on beam-search; understanding it is key to grasping the attack.
  - **Quick check question:** What is the time complexity of beam-search on a width-N lattice? (Answer: O(N^2 T).)

## Architecture Onboarding

- **Component map:**
  - User: generates true and noise tokens, shuffles, controls random seed, saves permutation mapping
  - Server: holds LLM, does inference on linearized lattice, returns next-token distributions
  - Lattice-finetuned LLM: accepts linearized lattice, outputs distributions for each token position

- **Critical path:**
  1. User sends N tokens (true + noise, shuffled) to server
  2. Server linearizes lattice, runs one forward pass, returns N distributions
  3. User applies reverse permutation, samples true token, generates next noise tokens, shuffles again

- **Design tradeoffs:**
  - Larger N improves privacy but degrades quality and increases computation
  - Bigram units improve quality but increase server inference length from N to N^2 per step
  - Higher mix-ratio improves RBS defense but may reduce noise effectiveness

- **Failure signatures:**
  - High max-true-ratio under RBS → mixing noise scheme not tuned well
  - Very high perplexity → lattice width too large or noise scheme too aggressive
  - Slow generation → bigram units or large N chosen

- **First 3 experiments:**
  1. Run vanilla LM (N=1) vs LatticeGen with N=2 synonym noise → measure PPL degradation and true-ratio under BS attack
  2. Compare parallel vs mixing noise scheme with N=2 → measure max-true-ratio under RBS
  3. Vary mix-ratio from 0 to 1 with N=2 → find optimal value minimizing max-true-ratio under RBS

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LatticeGen's performance scale with different lattice widths (N) and what is the optimal value of N that balances privacy protection and generation quality?
- **Basis in paper:** [explicit] The paper discusses LatticeGen with different lattice widths (N=2 and N=3) and mentions that larger N provides better privacy protection but degrades generation quality.
- **Why unresolved:** The paper does not provide a comprehensive analysis of LatticeGen's performance across a range of lattice widths. The experiments only cover N=2 and N=3, leaving the optimal value of N for different use cases unclear.
- **What evidence would resolve it:** Experiments comparing LatticeGen's performance (e.g., generation quality, privacy protection) across a wider range of lattice widths (e.g., N=2, 3, 4, 5) would help determine the optimal value of N for different applications.

### Open Question 2
- **Question:** How effective is LatticeGen against more sophisticated attacks beyond the repeated beam-search attack, such as manual inspection by a human or other machine learning-based attacks?
- **Basis in paper:** [explicit] The paper mentions that there could be other forms of attacks on the server side, such as manual inspection by a human, and that further exploration of these attacks is left to future work.
- **Why unresolved:** The paper only evaluates LatticeGen against the repeated beam-search attack and does not explore its effectiveness against other types of attacks. The potential vulnerability of LatticeGen to more sophisticated attacks remains unknown.
- **What evidence would resolve it:** Experiments evaluating LatticeGen's performance against a variety of attacks, including manual inspection by humans and machine learning-based attacks, would provide insights into its robustness and limitations.

### Open Question 3
- **Question:** How does LatticeGen's performance compare to other privacy-preserving techniques for LLM generation, such as differential privacy or homomorphic encryption?
- **Basis in paper:** [explicit] The paper mentions related work on differential privacy and homomorphic encryption for LLM privacy, but does not provide a direct comparison of LatticeGen's performance with these techniques.
- **Why unresolved:** The paper does not benchmark LatticeGen against other privacy-preserving methods, making it difficult to assess its relative strengths and weaknesses. A comprehensive comparison would help identify the most effective approaches for different use cases.
- **What evidence would resolve it:** Experiments comparing LatticeGen's performance (e.g., privacy protection, generation quality, computational efficiency) to other privacy-preserving techniques would provide a clearer understanding of its advantages and limitations.

## Limitations
- The mixing noise scheme's implementation details are not fully specified, particularly the exact algorithmic procedure for selecting when to branch vs. generate independent noise tokens
- Evaluation is limited to creative writing tasks on WritingPrompts dataset, which may not generalize to other domains with different linguistic patterns
- Privacy guarantees are measured only against RBS, without exploring whether other attack strategies could be more effective

## Confidence

**High Confidence (Strong empirical support, well-justified mechanisms):**
- LatticeGen successfully hides true tokens in the lattice structure when using the mixing noise scheme with appropriate mix-ratio parameters
- The parallel noise scheme provides weaker privacy protection than the mixing scheme under RBS attacks
- Quality degradation is directly proportional to lattice width N and inversely proportional to mix-ratio

**Medium Confidence (Reasonable but with gaps):**
- The repeated beam-search attack is the most effective attack strategy against LatticeGen
- The mixing noise scheme breaks the independence assumption required for RBS to succeed
- Bigram units provide measurable quality improvements without compromising privacy

**Low Confidence (Limited evidence or significant assumptions):**
- LatticeGen is resistant to all practical attack strategies beyond RBS
- The optimal mix-ratio values (0.12 for N=2, 0.1 for N=3) are universally applicable across different tasks
- The privacy-protection quality tradeoff curve is smooth and predictable

## Next Checks

1. **Implement and test alternative attack strategies:**
   - Design and evaluate attacks that exploit statistical patterns in noise token distributions
   - Test attacks that leverage multiple rounds of generation to build probability models
   - Measure whether timing analysis or computational pattern recognition can leak information about the permutation

2. **Conduct comprehensive ablation studies on mix-ratio optimization:**
   - Systematically vary mix-ratio from 0.05 to 0.5 in 0.05 increments
   - Measure max-true-ratio under RBS for each value across multiple N values
   - Identify whether the optimal mix-ratio depends on task domain or dataset characteristics

3. **Extend evaluation to diverse task domains:**
   - Test LatticeGen on code generation, summarization, and factual question answering tasks
   - Compare privacy protection effectiveness across domains with different linguistic patterns
   - Evaluate whether certain domains make noise tokens more predictable or easier to attack