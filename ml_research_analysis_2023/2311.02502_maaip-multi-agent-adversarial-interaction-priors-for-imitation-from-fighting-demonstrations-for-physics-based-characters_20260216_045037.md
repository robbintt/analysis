---
ver: rpa2
title: 'MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting
  demonstrations for physics-based characters'
arxiv_id: '2311.02502'
source_url: https://arxiv.org/abs/2311.02502
tags:
- interaction
- motion
- characters
- each
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MAAIP, a method for training physics-based
  characters to imitate interactive fighting behaviors from unstructured motion capture
  data. The approach uses two motion datasets: one with single actors performing individual
  motions and another with multiple actors interacting.'
---

# MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters

## Quick Facts
- arXiv ID: 2311.02502
- Source URL: https://arxiv.org/abs/2311.02502
- Reference count: 17
- Primary result: Method enables physics-based characters to learn realistic fighting behaviors from unstructured motion capture data using dual adversarial priors

## Executive Summary
This paper introduces MAAIP, a method for training physics-based characters to imitate interactive fighting behaviors from unstructured motion capture data. The approach uses two motion datasets: one with single actors performing individual motions and another with multiple actors interacting. Two adversarial discriminators are trained - one to evaluate the naturalness of individual motions and another to evaluate the realism of interactions between characters. These discriminators provide rewards that guide reinforcement learning policies for each character.

The method enables characters to learn realistic fighting behaviors including approaching opponents, defending, attacking, and maintaining distance. The approach was validated on boxing and QwanKiDo scenarios, showing that characters could imitate specific fighting styles from the demonstrations. Additional control rewards could be added to influence behaviors like minimizing damage received or following a target heading direction while fighting.

## Method Summary
MAAIP uses a Multi-Agent Generative Adversarial Imitation Learning framework where two discriminators are trained on motion capture data - one for motion naturalness and one for interaction realism. The method employs parameter sharing with centralized training and decentralized execution (CTDE) for efficient multi-agent learning. Each agent receives observations decomposed into self-features (body position, rotation, velocity) and opponent-features (opponent's root position, orientation, velocity, and key body part positions). The policy network is updated using MAPPO with rewards from both discriminators, allowing characters to learn realistic fighting behaviors from unstructured demonstrations.

## Key Results
- Characters successfully learned to approach opponents, defend, attack, and maintain distance in boxing and QwanKiDo scenarios
- The dual-adversarial-prior approach enabled imitation of specific fighting styles from demonstrations
- Additional control rewards could be incorporated to influence behaviors like minimizing damage or following target headings
- The method generated realistic multi-character interactions without manually designed rewards or constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual adversarial priors enable simultaneous motion naturalness and interaction realism
- Mechanism: The system trains two separate discriminators - one for motion naturalness (D_M) and one for interaction realism (D_I). Each discriminator provides a reward signal that guides the reinforcement learning policy. D_M ensures individual character motions look natural, while D_I ensures the interaction between characters matches the style in the multi-actor dataset.
- Core assumption: Motion naturalness and interaction realism can be disentangled and optimized separately
- Evidence anchors:
  - [abstract]: "Two adversarial discriminators are trained - one to evaluate the naturalness of individual motions and another to evaluate the realism of interactions between characters."
  - [section 3]: "Based on these poses m_I, we propose to build an observation at time t, o_t=[o_self_t, o_opp_t] for each character (self for the agent, and opp for the opponent)."
- Break condition: If the interaction dataset is too small or too homogeneous, the interaction prior may not generalize well, causing the system to collapse to repetitive behaviors.

### Mechanism 2
- Claim: Self-opponent observation decomposition captures interaction dynamics effectively
- Mechanism: Each agent's observation consists of self-features (body position, rotation, velocity) and opponent-features (opponent's root position, orientation, velocity, and key body part positions). This decomposition allows each agent to react appropriately to the opponent's current state and predicted future states based on velocity information.
- Core assumption: Relative positions and velocities between characters are sufficient to model interactive behavior
- Evidence anchors:
  - [section 4.2]: "The observation of each agent o_t = [o_self_t, o_opp_t] consists of a set of features describing the proprioceptive configuration of its own body o_self_t at the current time t, as well as features describing the current observation about the opponent o_opp_t."
  - [section 4.2]: "We use the linear and angular velocities as relevant information for deciding the appropriate reaction to the opponent."
- Break condition: If the observation space doesn't include critical interaction cues (e.g., hand positions for boxing), the agents may fail to learn proper defensive or offensive behaviors.

### Mechanism 3
- Claim: Parameter sharing with centralized training and decentralized execution (CTDE) enables efficient multi-agent learning
- Mechanism: All agents share the same policy network parameters, which are trained centrally using information from all agents' observations. During execution, each agent acts independently based on its local observation. The centralized value function takes the concatenation of all agents' observations to estimate the global state value.
- Core assumption: Homogeneous agents with the same observation and action spaces can effectively share policy parameters
- Evidence anchors:
  - [section 4.4]: "Since the agents are homogeneous (i.e. they have the same observation and action spaces), we use parameter sharing for their policies, so that all agents share the same network."
  - [section 4.4]: "We also use centralized training and decentralized execution (CTDE) for training the agents [Lowe et al. 2017]."
- Break condition: If agents have significantly different roles or capabilities, parameter sharing may limit performance or prevent learning specialized behaviors.

## Foundational Learning

- Concept: Generative Adversarial Imitation Learning (GAIL)
  - Why needed here: GAIL provides the foundation for learning rewards from demonstrations without requiring explicit reward engineering
  - Quick check question: What is the key difference between GAIL and traditional imitation learning approaches?

- Concept: Reinforcement Learning with Policy Gradients
  - Why needed here: The policies are updated through policy gradient methods using the adversarial rewards as feedback signals
  - Quick check question: How does the reward scheduling strategy help stabilize training when adding control rewards?

- Concept: Multi-Agent Reinforcement Learning (MARL)
  - Why needed here: The system must handle multiple interacting agents learning simultaneously in the same environment
  - Quick check question: What are the advantages and potential drawbacks of using parameter sharing in multi-agent settings?

## Architecture Onboarding

- Component map: Motion datasets (single-actor M_S, multi-actor M_I) -> Policy network (3 hidden layers, 1024-1024-512 units) -> Two discriminators (D_M for motion, D_I for interaction) -> MAPPO updates -> Physics simulation environment (Isaac Gym)

- Critical path:
  1. Load and preprocess motion datasets
  2. Initialize policy, discriminators, and value function
  3. Collect trajectories with current policies
  4. Update discriminators using expert demonstrations and simulated data
  5. Update policy and value function using MAPPO
  6. Repeat until convergence

- Design tradeoffs:
  - Single motion prior vs. interaction prior weighting (w_M vs w_I)
  - Observation space complexity vs. learning efficiency
  - Parameter sharing vs. specialized agent policies
  - Fixed covariance vs. learned action distribution

- Failure signatures:
  - Mode collapse: Agents repeatedly generate the same motion sequences
  - Unrealistic interactions: Agents fail to properly respond to opponent actions
  - Training instability: Discriminator loss oscillates or explodes
  - Lack of engagement: Agents avoid interaction despite being in proximity

- First 3 experiments:
  1. Train with only motion prior (w_M=1.0, w_I=0.0) to verify basic motion imitation
  2. Train with only interaction prior (w_M=0.0, w_I=1.0) to verify basic interaction imitation
  3. Train with both priors (w_M=0.2, w_I=0.8) to verify combined motion and interaction imitation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of generated interactions scale when moving from two to more than two interacting characters?
- Basis in paper: [explicit] The paper states "we only tested activities involving two fighters" and notes that "Future investigations and tests are needed to check the capability of the system to scale to more characters."
- Why unresolved: The authors acknowledge this as an open direction but have not conducted experiments beyond two characters.
- What evidence would resolve it: Experimental results showing the method's performance and limitations when applied to three or more characters, including metrics on interaction quality and computational feasibility.

### Open Question 2
- Question: How can the method be extended to handle longer-term strategic planning in fighting scenarios rather than just short-term reactions?
- Basis in paper: [explicit] The paper states "the current policies' architecture... can only imitate short term reactions" and suggests "Learning basic fighting skills with a low level controller, then learning strategic play from demonstrations by a high level controller equipped with a long term memory component would be an interesting future direction."
- Why unresolved: The current approach focuses on immediate reactions without strategic planning capabilities, and the authors identify this as a limitation requiring new architectural approaches.
- What evidence would resolve it: A modified system incorporating hierarchical control with strategic planning that demonstrates improved performance in scenarios requiring multi-step tactics or strategy.

### Open Question 3
- Question: What is the minimum size and quality of motion capture datasets required to produce realistic fighting interactions without mode collapse?
- Basis in paper: [inferred] The paper mentions mode collapse as a problem and discusses dataset quality issues, noting that "too few examples in the demonstrations may lead to simulate unrealistic behaviors" and that "enhancing the datasets used for training" could help.
- Why unresolved: The paper demonstrates the approach with specific dataset sizes but does not systematically explore the relationship between dataset characteristics and generation quality.
- What evidence would resolve it: A systematic study varying dataset size, diversity, and quality to identify thresholds for producing stable, realistic interactions, including quantitative metrics on mode collapse frequency and motion naturalness.

## Limitations
- Dataset dependency: Success heavily relies on quality and diversity of motion capture datasets, particularly for interaction realism
- Limited scalability: Only tested with two interacting characters, with unclear performance for larger groups
- Short-term focus: Current policies only learn immediate reactions without strategic planning capabilities

## Confidence

- High confidence: Dual-adversarial-prior mechanism (motion naturalness + interaction realism) is well-supported by theoretical framework and ablation studies
- Medium confidence: MAPPO implementation and reward scheduling strategy are standard approaches, but specific hyperparameter choices could significantly impact results
- Low confidence: Generalization to scenarios beyond fighting remains untested and would require significant architectural modifications

## Next Checks

1. **Ablation on observation space**: Remove opponent velocity features and retrain to quantify their importance for interaction realism. Compare interaction quality metrics between full and reduced observation spaces.

2. **Dataset size sensitivity analysis**: Systematically vary the size of the multi-actor dataset (25%, 50%, 75%, 100%) and measure degradation in interaction quality and diversity of learned behaviors.

3. **Transfer to asymmetric scenarios**: Modify the architecture to handle heterogeneous agents (e.g., different character morphologies or roles) and test whether parameter sharing can be relaxed while maintaining performance.