---
ver: rpa2
title: 'Automatic Assessment of Divergent Thinking in Chinese Language with TransDis:
  A Transformer-Based Language Model Approach'
arxiv_id: '2306.14790'
source_url: https://arxiv.org/abs/2306.14790
tags:
- originality
- creativity
- flexibility
- semantic
- creative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TransDis is a transformer-based language model for automatic assessment
  of divergent thinking (DT) in Chinese. Using semantic distances between responses
  and prompts, it generates originality and flexibility scores for the Alternative
  Uses Task (AUT).
---

# Automatic Assessment of Divergent Thinking in Chinese Language with TransDis: A Transformer-Based Language Model Approach

## Quick Facts
- arXiv ID: 2306.14790
- Source URL: https://arxiv.org/abs/2306.14790
- Reference count: 40
- Key outcome: TransDis achieves 0.93 correlation with human ratings for originality and 0.93 for flexibility using semantic distances from transformer models

## Executive Summary
TransDis is a transformer-based language model system that automatically assesses divergent thinking (DT) in Chinese using semantic distances between responses and prompts for originality, and between adjacent responses for flexibility. Across two studies with over 350 participants, TransDis demonstrated strong validity with latent originality factor correlating 0.93 with human ratings and flexibility factor correlating 0.93 with human ratings. The system successfully discriminated between creative vs. common uses (d=0.77) and flexible vs. persistent instruction groups (d=0.55), providing reliable, low-cost DT assessment in Chinese and 50+ languages.

## Method Summary
TransDis computes semantic distances using five transformer-based language models (Word2Vec, BERT, SBERT_Mpnet, SBERT_MiniLM, SimCSE) between Chinese AUT responses and prompts for originality scores, and between adjacent responses for flexibility scores. Bayesian confirmatory factor analysis extracts latent factors from multiple model scores, with three models (SBERT_Mpnet, SBERT_MiniLM, SimCSE) selected for final TransDis system. The approach was validated against human ratings and other creativity measures across studies with 350+ participants.

## Key Results
- TransDis originality scores correlated 0.93 with human ratings and explained 0.86 variance
- TransDis flexibility scores correlated 0.93 with human ratings and explained 0.87 variance
- Model discriminated between creative vs. common uses (d=0.77) and flexible vs. persistent groups (d=0.55)
- Positive correlations with creativity measures: everyday creativity, creative self-efficacy, creative self-identity, openness, and fluid intelligence

## Why This Works (Mechanism)

### Mechanism 1
Transformer-based language models (SBERT_Mpnet, SBERT_MiniLM, SimCSE) generate context-dependent embeddings that better capture semantic distance in Chinese sentences than static models. These models use self-attention and fine-tuning to map sentences into high-dimensional vectors where semantic similarity reflects human-perceived creativity differences. The core assumption is that semantic distance between responses and AUT prompts validly proxies for originality. Break condition: If semantic distances do not correlate with human ratings (Study 1 showed 0.93 correlation).

### Mechanism 2
Latent variable modeling with Bayesian estimation extracts common variance factors across multiple language models, improving reliability over single-model scoring. Bayesian CFA allows approximate zeros for cross-loadings and residuals, avoiding restrictive assumptions. The core assumption is that creativity ratings have shared variance across models and items. Break condition: If priors are misspecified or sample size too small for reliable posterior estimation.

### Mechanism 3
Known-groups validity: Model-rated scores discriminate between groups instructed to generate creative vs. common uses or flexible vs. persistent responses. Instructions shape response patterns that semantic distance models capture automatically. The core assumption is that different instructions produce distinct semantic distributions measurable by the models. Break condition: If instruction effects are too subtle for semantic distance to detect.

## Foundational Learning

- **Transformer-based language models and sentence embeddings**: Why needed - Core of TransDis scoring; Quick check - How do SBERT and SimCSE differ from vanilla BERT in producing sentence embeddings for Chinese?
- **Latent variable modeling and Bayesian estimation in CFA**: Why needed - To combine multiple model scores into reliable latent factors; Quick check - Why use Bayesian estimation instead of maximum likelihood when cross-loadings are expected?
- **Known-groups validity and experimental manipulation**: Why needed - To demonstrate model scores behave like human ratings; Quick check - What design ensures semantic distance can detect differences between creative and common uses?

## Architecture Onboarding

- **Component map**: AUT response → semantic distance calculation → transformer model scoring → standardization → latent factor extraction → TransDis originality/flexibility scores
- **Critical path**: Response collection → preprocessing (tokenization for Chinese) → semantic distance computation → score aggregation → validation against human ratings
- **Design tradeoffs**: Multiple models vs. single model (robustness vs. simplicity); latent factor extraction vs. direct averaging (statistical rigor vs. computational cost); open prompts vs. controlled prompts (ecological validity vs. model reliability)
- **Failure signatures**: Low correlation with human ratings; unstable latent factor loadings; inability to discriminate between known groups; model overfitting to specific prompts
- **First 3 experiments**: 1) Compute semantic distances using SBERT_Mpnet on small AUT dataset and correlate with human originality ratings; 2) Compare latent factor scores vs. simple average for flexibility on adjacent responses; 3) Run known-groups study to test discrimination power of model-rated originality

## Open Questions the Paper Calls Out

### Open Question 1
How does TransDis performance vary across different Chinese dialects or regional variations in language use? The study focused on Mandarin Chinese without examining dialectal differences, limiting generalizability to regional dialects. Testing with Cantonese, Shanghainese, or other major Chinese dialects using region-specific AUT prompts would resolve this.

### Open Question 2
What is the optimal number of transformer-based models to include in TransDis for balancing accuracy and computational efficiency? The paper doesn't systematically explore trade-offs between adding more models versus maintaining computational efficiency. Empirical comparison with different numbers of models (2-6) across multiple datasets would resolve this.

### Open Question 3
How does TransDis handle creative responses that use wordplay, puns, or culturally-specific humor common in Chinese? The paper doesn't address handling of Chinese-specific linguistic features like puns or homophones. Analysis of TransDis scores for responses containing intentional puns versus straightforward responses would resolve this.

## Limitations

- Limited generalizability to AUT prompts beyond "bedsheet" and "toothbrush"
- Relatively small sample sizes (N=104 in Study 1, N=71 in Study 2) may limit detection of subtle effects
- Bayesian CFA approach relies on priors that may not generalize across different populations or contexts

## Confidence

- **High Confidence**: Model's ability to discriminate between known groups and strong correlations with human ratings (0.93 for originality, 0.93 for flexibility)
- **Medium Confidence**: Validity correlations with other creativity measures and the claim that TransDis works across 50+ languages
- **Low Confidence**: Generalizability to prompts beyond the two tested and performance in populations with different cultural backgrounds

## Next Checks

1. Test TransDis on additional Chinese AUT prompts (e.g., "umbrella," "brick") to assess generalizability
2. Validate model performance across different Chinese-speaking cultural groups to identify potential cultural bias
3. Conduct cross-linguistic validation by testing the 50+ language claim with at least 5-10 diverse languages beyond Chinese