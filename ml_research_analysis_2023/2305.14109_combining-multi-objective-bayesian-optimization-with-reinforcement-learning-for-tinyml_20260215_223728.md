---
ver: rpa2
title: Combining Multi-Objective Bayesian Optimization with Reinforcement Learning
  for TinyML
arxiv_id: '2305.14109'
source_url: https://arxiv.org/abs/2305.14109
tags:
- optimization
- bayesian
- objective
- search
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-objective Bayesian optimization approach
  for hyperparameter optimization of deep neural networks (DNNs) for deployment on
  microcontrollers (TinyML). The key innovation is combining multi-objective Bayesian
  optimization (MOBOpt) with an ensemble of competing parametric policies trained
  using Augmented Random Search (ARS) reinforcement learning (RL) agents.
---

# Combining Multi-Objective Bayesian Optimization with Reinforcement Learning for TinyML

## Quick Facts
- arXiv ID: 2305.14109
- Source URL: https://arxiv.org/abs/2305.14109
- Reference count: 40
- Outperforms existing MOBOpt approaches and evolutionary algorithms on TinyML tasks

## Executive Summary
This paper introduces a novel approach combining multi-objective Bayesian optimization (MOBOpt) with Augmented Random Search (ARS) reinforcement learning agents to optimize deep neural networks for deployment on microcontrollers. The method efficiently explores the hyperparameter space while balancing multiple conflicting objectives: predictive accuracy, memory consumption (ROM/RAM), and computational complexity (FLOPs). By using an ensemble of competing ARS policies to navigate the Gaussian Process surrogate model, the approach achieves better exploration-exploitation balance than traditional acquisition function-based methods, particularly in flat regions of the objective landscape.

## Method Summary
The proposed method integrates multi-objective Bayesian optimization with an ensemble of ARS reinforcement learning agents to optimize DNN hyperparameters for TinyML deployment. The process begins with Latin Hypercube sampling to establish initial priors, followed by fitting a Gaussian Process surrogate model. ARS agents then train on the GP posterior using random weight sampling for augmented Chebyshev scalarization, proposing new hyperparameter configurations. Each configuration undergoes DNN training with pruning and quantization, followed by memory assessment on target microcontroller platforms. The evaluated objectives (accuracy, ROM, RAM, FLOPs) are added to the GP data, and the loop repeats until the computational budget is exhausted.

## Key Results
- On CIFAR10 with ResNet-18: Achieved hypervolume of 0.40 after 150 samples, compared to 0.30-0.35 for baseline methods
- On DaLiAc with MobileNetV3: Achieved hypervolume of 0.50, compared to 0.35-0.40 for baselines
- Consistently outperformed ParEGO, TurBO, MorBO, NSGA-II, and random sampling across all tested datasets and architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ARS enables better exploration-exploitation balance than traditional EI-based methods
- Mechanism: ARS learns local policies via random sampling that navigate the GP posterior without relying on second-order gradients, allowing it to avoid getting stuck in flat regions
- Core assumption: The GP posterior provides enough signal for ARS to differentiate between improving and non-improving directions
- Evidence anchors: ARS doesn't rely on second-order partial derivatives, making it less computationally complex than gradient-based methods

### Mechanism 2
- Claim: Multi-objective scaling via augmented Chebyshev scalarization with random weight vectors enables diverse sampling along the Pareto front
- Mechanism: Random sampling of weight vectors λ and computing fλ(x) explores different trade-offs between accuracy, ROM, RAM, and FLOPs
- Core assumption: Random weight sampling adequately covers the Pareto front geometry without requiring many samples per iteration
- Evidence anchors: fλ(x) = n max j=1 (λjfj(x)) + ρ nX j=1 λjfj(x) and r(x) = fλ(x) − fλ(f ∗) for positive improvements

### Mechanism 3
- Claim: The penalized "negative improvement" reward allows ARS to learn from worse outcomes, improving robustness
- Mechanism: Instead of clamping negative improvement to zero, the reward is scaled by a small constant, giving policy signal to avoid bad regions
- Core assumption: Penalizing negative improvements preserves enough signal for ARS to avoid poor regions without destabilizing learning
- Evidence anchors: The method still achieves desired tradeoffs when penalizing negative improvements with a small constant factor

## Foundational Learning

- **Gaussian Process (GP) surrogate modeling**: GPs provide uncertainty estimates that guide acquisition in BO, crucial for minimizing costly DNN training evaluations
  - Quick check: What kernel function is typically used in GP regression for continuous hyperparameters?

- **Pareto dominance and hypervolume (HV)**: HV measures the dominated space volume in objective space, enabling comparison of trade-off quality across algorithms
  - Quick check: How is the reference point for HV typically chosen in constrained multi-objective optimization?

- **Augmented Random Search (ARS)**: ARS efficiently learns linear policies in high-dimensional hyperparameter spaces without requiring gradients of the objective
  - Quick check: What is the role of the standard deviation σR in ARS policy updates?

## Architecture Onboarding

- **Component map**: Latin Hypercube sampling -> GP surrogate model -> ARS agent ensemble -> DNN training/compression -> Memory assessment -> Objective collection
- **Critical path**: 1) Sample initial points via Latin Hypercube, 2) Fit GP surrogate, 3) ARS agents train on GP posterior, 4) ARS proposes new hyperparameters, 5) DNN is trained, pruned, quantized, and evaluated, 6) Objectives collected and added to GP data, 7) Repeat until budget exhausted
- **Design tradeoffs**: More ARS agents (L) → better coverage but higher compute; Larger sampling directions (N) → better policy learning but slower; Higher top-k % → more stable updates but less selective; Longer rollout horizon (H) → richer signal but higher variance
- **Failure signatures**: HV plateaus early → ARS not learning or GP too uncertain; Memory/ROM constraints never met → search space bounds too loose; Training times explode → pruning sparsity range too wide; Random seeds diverge → insufficient exploration
- **First 3 experiments**: 1) Run with default ARS (N=400, top-k=1%, H=4) on CIFAR10-ResNet18; record HV progression, 2) Vary top-k percentage (1%, 5%, 10%) on same problem; compare convergence curves, 3) Replace ARS with ParEGO on same problem; compare final HV and Pareto set diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ARS-MOBOpt compare to other multi-objective optimization methods when applied to different neural network architectures beyond ResNet-18 and MobileNetV3?
- Basis: The paper demonstrates effectiveness on specific architectures but lacks comprehensive comparison with other types
- Why unresolved: The paper focuses on specific architectures without systematic evaluation across broader range
- What evidence would resolve it: Comprehensive evaluation on various neural network architectures including CNNs, RNNs, and transformers

### Open Question 2
- Question: How does ARS-MOBOpt scale with increasing search space dimensions and what are computational/memory requirements?
- Basis: The paper mentions search space increases with DNN depth but lacks detailed scalability analysis
- Why unresolved: No systematic evaluation of performance and resource requirements for different search space sizes
- What evidence would resolve it: Comprehensive evaluation on search spaces with varying dimensions including hyperparameter ranges and complexity

### Open Question 3
- Question: How does ARS-MOBOpt handle constraints on objectives and what are trade-offs between constraints and accuracy?
- Basis: The paper mentions finding feasible tradeoffs but lacks detailed constraint handling analysis
- Why unresolved: No systematic evaluation under different constraint settings or detailed trade-off analysis
- What evidence would resolve it: Comprehensive evaluation under various constraint settings and detailed analysis of accuracy vs resource consumption trade-offs

## Limitations
- Lack of direct ablation studies comparing ARS against standard acquisition functions like Expected Improvement
- No comprehensive evaluation across diverse neural network architectures beyond tested cases
- Missing systematic analysis of scalability and resource requirements for larger search spaces

## Confidence

- **High confidence**: Experimental setup, datasets, and baseline comparisons are well-defined and reproducible
- **Medium confidence**: General framework combining BO with RL is sound, but specific ARS implementation details significantly impact performance
- **Low confidence**: Claim that ARS specifically improves exploration-exploitation balance over traditional methods lacks direct empirical support

## Next Checks
1. **Ablation study**: Compare proposed method against standard EI-based MOBOpt approach on same problems to isolate ARS contribution
2. **Hyperparameter sensitivity**: Systematically vary ARS hyperparameters (N, top-k percentage, rollout horizon) to determine robustness and identify optimal settings
3. **Convergence analysis**: Track HV indicator and policy training stability during optimization to diagnose performance issues from suboptimal hyperparameters or fundamental exploration problems