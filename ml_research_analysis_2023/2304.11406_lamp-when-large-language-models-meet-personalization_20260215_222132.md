---
ver: rpa2
title: 'LaMP: When Large Language Models Meet Personalization'
arxiv_id: '2304.11406'
source_url: https://arxiv.org/abs/2304.11406
tags:
- user
- language
- personalized
- task
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the LaMP benchmark for evaluating language
  models' ability to produce personalized outputs. LaMP includes seven tasks across
  text classification and generation, with user profiles providing personalization
  context.
---

# LaMP: When Large Language Models Meet Personalization

## Quick Facts
- arXiv ID: 2304.11406
- Source URL: https://arxiv.org/abs/2304.11406
- Reference count: 25
- Primary result: Retrieval augmentation with user profiles improves personalization across 7 benchmark tasks, with dense retrievers outperforming sparse retrieval and fine-tuned smaller models beating zero-shot larger LLMs

## Executive Summary
This paper introduces LaMP, a benchmark for evaluating language models' ability to produce personalized outputs using user profiles. The benchmark includes seven tasks spanning text classification and generation, where user profiles provide contextual personalization information. A retrieval augmentation approach is proposed to incorporate relevant profile information into prompts, using query generation, profile retrieval, and prompt construction. Experiments demonstrate that incorporating profile information improves performance across all tasks, with dense retrievers like Contriever generally outperforming BM25 and random selection. Fine-tuning smaller models on these tasks also outperforms zero-shot performance of larger LLMs.

## Method Summary
The method employs retrieval augmentation to personalize language model outputs using user profiles. The approach involves three components: φq generates queries from non-template parts of input sequences, R is the retrieval model (BM25, Contriever, or random selection), and φp constructs prompts using templates that incorporate retrieved profile entries. The framework is evaluated across seven tasks in the LaMP benchmark, with experiments comparing different retrieval models, varying numbers of retrieved items (k), and contrasting fine-tuned smaller models against zero-shot larger LLMs. Fine-tuning uses AdamW optimizer with learning rate 5×10^-5 for 20 epochs on generation tasks.

## Key Results
- Incorporating personalized user profile entries via retrieval augmentation improves LM performance on all LaMP benchmark tasks
- Dense retrievers like Contriever generally outperform BM25 and random selection for profile retrieval
- Fine-tuning smaller models on personalization tasks outperforms zero-shot performance of larger LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating personalized user profile entries via retrieval augmentation improves LM performance on downstream tasks.
- Mechanism: The retrieval model selects relevant profile items capturing user-specific preferences, writing style, or historical context, which are then embedded into the LM prompt to condition generation or classification.
- Core assumption: Retrieved profile entries are semantically relevant to the task input and the LM's context window can accommodate these snippets without significant noise.
- Evidence anchors:
  - [abstract] "Experiments with fine-tuned and zero-shot models show that using profile information improves performance across all tasks, with dense retrievers like Contriever generally outperforming BM25 and random selection."
  - [section] "The results suggest that personalization improves the performance for all the text classification and generation tasks within the LaMP benchmark."
- Break condition: If retrieved items are irrelevant or noisy, the LM may be misled, degrading performance. If profile size exceeds context window even after retrieval, truncation may discard useful info.

### Mechanism 2
- Claim: Fine-tuning smaller models on personalization tasks yields better performance than zero-shot prompting of larger LLMs.
- Mechanism: Smaller models, when fine-tuned with task-specific and user-specific data, learn task-specific personalization patterns more efficiently than relying on in-context learning from a large frozen model.
- Core assumption: Training data adequately represents diversity of user profiles and task inputs, and smaller model capacity is sufficient to encode these patterns.
- Evidence anchors:
  - [abstract] "Fine-tuning smaller models on these tasks outperforms zero-shot performance of larger LLMs."
  - [section] "These findings indicate that fine-tuning smaller models on downstream tasks leads to enhanced performance in comparison to zero-shot performance of LLMs."
- Break condition: If training data is too sparse or unrepresentative, fine-tuning may overfit or fail to generalize, making zero-shot large LM performance competitive.

### Mechanism 3
- Claim: The number of retrieved profile entries (k) positively correlates with task performance up to a context window limit.
- Mechanism: Increasing k provides more user-specific context, improving personalization; however, beyond a threshold, truncation and noise reduce gains.
- Core assumption: The LM can effectively integrate multiple profile snippets without context dilution, and the retrieval model maintains high relevance across multiple entries.
- Evidence anchors:
  - [section] "The results suggest that increasing the number of retrieved items leads to improved performance in downstream tasks. However, certain tasks experience a decline in performance under these conditions."
- Break condition: If the number of retrieved entries exceeds the context window, truncation occurs and noise dominates, reducing performance.

## Foundational Learning

- Concept: Retrieval augmentation and prompt engineering.
  - Why needed here: The LM cannot process entire user profiles; retrieval selects relevant snippets and prompt templates embed them effectively.
  - Quick check question: What is the difference between dense retrieval (e.g., Contriever) and sparse retrieval (e.g., BM25) in this context?

- Concept: Fine-tuning vs. zero-shot learning trade-offs.
  - Why needed here: Determines whether to invest in model training or rely on prompt-based adaptation; impacts deployment cost and scalability.
  - Quick check question: Why might a smaller fine-tuned model outperform a large zero-shot model on personalization tasks?

- Concept: Evaluation metrics for personalized generation (ROUGE, accuracy, MAE, RMSE).
  - Why needed here: Metrics must capture both task correctness and personalization quality; standard metrics may not fully reflect personalization.
  - Quick check question: How does ROUGE-1 differ from ROUGE-L, and when would you prefer one over the other for personalization evaluation?

## Architecture Onboarding

- Component map: User profile storage -> Retrieval model (BM25/Contriever/random) -> Prompt construction -> Language model -> Output evaluation
- Critical path: Input -> Query generation -> Profile retrieval (k items) -> Prompt assembly -> LM inference -> Result
- Design tradeoffs:
  - Dense vs. sparse retrieval: Contriever offers semantic matching but is slower/expensive; BM25 is faster but relies on term matching
  - k selection: More entries improve personalization but risk context overflow and noise
  - Prompt design: Task-specific templates ensure clarity but increase complexity
- Failure signatures:
  - Low relevance scores from retrieval -> poor personalization
  - Context truncation errors -> missing key profile info
  - Overfitting on fine-tuning -> poor generalization to unseen users
- First 3 experiments:
  1. Compare BM25 vs. Contriever retrieval on a single task with k=1; measure accuracy/ROUGE
  2. Vary k from 1 to 5 for the best retriever; plot performance to find optimal k
  3. Fine-tune FlanT5-base vs. zero-shot FlanT5-XXL on one task; compare results to validate mechanism 2

## Open Questions the Paper Calls Out

- Open Question 1: How does the choice of retrieval model (BM25, Contriever, or random selection) impact the performance of language models on personalized tasks in the LaMP benchmark?
  - Basis in paper: [explicit] The paper compares the performance of different retrieval models on various tasks within the LaMP benchmark.
  - Why unresolved: The paper shows that Contriever generally outperforms BM25 and random selection, but it doesn't explore the reasons behind this performance difference or the potential for further optimization of these models.
  - What evidence would resolve it: Detailed analysis of the strengths and weaknesses of each retrieval model on different types of personalized tasks, and experiments with optimized versions of these models.

- Open Question 2: How does the number of retrieved items (k) from user profiles affect the performance of language models on personalized tasks?
  - Basis in paper: [explicit] The paper investigates the impact of varying k on the performance of language models on personalized tasks.
  - Why unresolved: While the paper shows that increasing k generally improves performance, it doesn't provide a clear explanation for the optimal value of k or the point at which performance plateaus or degrades.
  - What evidence would resolve it: Experiments with a wider range of k values and analysis of the trade-off between performance improvement and computational cost.

- Open Question 3: How can language models be effectively personalized using soft prompts instead of hard prompts?
  - Basis in paper: [inferred] The paper mentions the potential of using soft prompts for personalization but doesn't explore this approach in detail.
  - Why unresolved: Soft prompts offer a more flexible and efficient way to incorporate user profiles into language models, but their effectiveness for personalization tasks is not well understood.
  - What evidence would resolve it: Experiments comparing the performance of soft prompts to hard prompts on personalized tasks, and analysis of the advantages and disadvantages of each approach.

- Open Question 4: How can personalized text generation be evaluated using metrics that account for user preferences and characteristics?
  - Basis in paper: [explicit] The paper acknowledges the limitations of existing evaluation metrics for personalized text generation tasks.
  - Why unresolved: Current metrics like ROUGE focus on lexical overlap and do not consider the user's preferences or the context of the generated text.
  - What evidence would resolve it: Development of new evaluation metrics that incorporate user feedback, style matching, and task-specific relevance.

- Open Question 5: How can language models be trained to retrieve relevant information from user profiles for personalization tasks?
  - Basis in paper: [explicit] The paper proposes using retrieval augmentation to select relevant user profile entries, but doesn't explore the potential for training models to learn this retrieval process.
  - Why unresolved: Training models to retrieve relevant information from user profiles could improve the efficiency and effectiveness of personalization.
  - What evidence would resolve it: Experiments with models trained to retrieve relevant information from user profiles, and comparison of their performance to traditional retrieval methods.

## Limitations

- Evaluation relies heavily on automated metrics (ROUGE, accuracy) that may not capture true personalization quality without human evaluation
- Lack of ablation studies to isolate whether improvements come from retrieval quality or prompt engineering
- No systematic analysis of how different profile characteristics (length, structure) affect personalization success

## Confidence

- High confidence in retrieval augmentation mechanism: The ablation showing dense retrievers outperforming BM25 and random selection provides strong evidence for the retrieval approach.
- Medium confidence in fine-tuning superiority: While results show fine-tuned smaller models outperforming zero-shot larger models, the comparison lacks systematic analysis of model size vs. training data effects.
- Low confidence in optimal k selection: The paper acknowledges performance degradation at higher k values but doesn't provide clear guidelines for selecting k based on task characteristics or profile characteristics.

## Next Checks

1. Conduct human evaluation studies where annotators rate the personalization quality of outputs with vs. without profile information, using standardized Likert scales to assess perceived user alignment.

2. Perform ablation studies varying only the retrieval model (keeping prompts constant) across all seven tasks to isolate the contribution of retrieval quality from prompt engineering effects.

3. Test the robustness of personalization across diverse user profile types (short vs. long profiles, structured vs. unstructured entries) to identify which profile characteristics most influence personalization success.