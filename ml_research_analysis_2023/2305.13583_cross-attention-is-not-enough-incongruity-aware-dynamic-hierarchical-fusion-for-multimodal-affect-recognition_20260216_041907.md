---
ver: rpa2
title: 'Cross-Attention is Not Enough: Incongruity-Aware Dynamic Hierarchical Fusion
  for Multimodal Affect Recognition'
arxiv_id: '2305.13583'
source_url: https://arxiv.org/abs/2305.13583
tags:
- modality
- modalities
- crossmodal
- multimodal
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes how crossmodal attention handles inter-modal
  incongruity in multimodal affect recognition and proposes a hierarchical fusion
  approach to mitigate it. The key findings are: (1) Crossmodal attention can highlight
  salient affective information but may malfunction when modalities have mismatched
  affective tendencies, leaving incongruity at the latent level.'
---

# Cross-Attention is Not Enough: Incongruity-Aware Dynamic Hierarchical Fusion for Multimodal Affect Recognition

## Quick Facts
- **arXiv ID**: 2305.13583
- **Source URL**: https://arxiv.org/abs/2305.13583
- **Reference count**: 13
- **Key outcome**: Hierarchical Crossmodal Transformer with Dynamic Modality Gating (HCT-DMG) outperforms prior multimodal models with ~0.8M fewer parameters while effectively handling modality incongruity in affect recognition.

## Executive Summary
This paper addresses a critical limitation in multimodal affect recognition: crossmodal attention can malfunction when modalities contain mismatched affective information, leaving incongruity at the latent level. The authors propose HCT-DMG, which dynamically selects the primary modality based on its contribution and hierarchically fuses auxiliary modalities. The model achieves superior performance across five benchmark datasets while reducing model size by approximately 0.8M parameters compared to state-of-the-art approaches.

## Method Summary
HCT-DMG processes text, audio, and vision features through a hierarchical fusion architecture. First, feature projection uses 1D Conv and GRU to encode local and global context. Dynamic Modality Gating (DMG) assigns trainable weights to determine the primary modality per batch. The model then performs two-stage crossmodal fusion: first enhancing auxiliary modalities through crossmodal transformers, then using these enhanced representations to augment the primary modality. Finally, weighted concatenation with self-attention produces the final representation for affect recognition tasks.

## Key Results
- HCT-DMG outperforms prior models (EF-LSTM, LF-LSTM, RA VEN, MCTN, CTC variants, MARN, MFN, RMFN, LMF, CIA, MulT, LMF-MulT) on five benchmark datasets
- Achieves reduced model size of approximately 0.8M parameters compared to baseline approaches
- Successfully recognizes hard samples with modality incongruity that challenge standard crossmodal attention
- Mitigates incongruity at the latent level in crossmodal attention, improving robustness to mismatched affective signals

## Why This Works (Mechanism)

### Mechanism 1
Crossmodal attention can malfunction when source and target modalities have conflicting emotional signals. When affective tendencies mismatch, attention weights can distort rather than clarify the target representation. This occurs because crossmodal attention assumes affective consistency between modalities, and when this breaks down, attention amplifies noise instead of signal. The paper demonstrates this through experiments showing disparate affective tendencies when fusing different source modalities with the same target.

### Mechanism 2
Hierarchical fusion reduces computational complexity by processing modalities in a prioritized sequence. Instead of fusing all three modalities simultaneously at every layer, the model first enhances auxiliary modalities, then uses these enhanced representations to augment the primary modality. This staged approach reduces fusion operations and model parameters while maintaining performance. The dynamic hierarchy is learned during training rather than being fixed a priori.

### Mechanism 3
Dynamic modality gating automatically selects the most contributing modality as primary, improving performance over fixed hierarchies. Each modality receives a trainable weight representing its probability of being selected as primary, updated during training based on contribution to task performance. This adapts to varying modality importance across different training samples and batches, providing flexibility that static approaches cannot match.

## Foundational Learning

- **Concept**: Crossmodal attention mechanism and its limitations
  - Why needed here: Understanding how crossmodal attention can both enhance and distort representations is fundamental to grasping why hierarchical fusion is necessary
  - Quick check question: What happens to crossmodal attention when the source modality contains affective information that contradicts the target modality?

- **Concept**: Modality hierarchy and fusion strategies
  - Why needed here: The paper's core contribution relies on understanding different fusion approaches (early, late, hierarchical) and their tradeoffs
  - Quick check question: How does hierarchical fusion differ from tensor fusion in terms of computational complexity and information flow?

- **Concept**: Dynamic weight assignment and gradient-based optimization
  - Why needed here: The modality gating mechanism requires understanding how trainable weights can be learned through backpropagation to optimize task performance
  - Quick check question: How would you implement a softmax-based probability distribution over modalities that can be updated during training?

## Architecture Onboarding

- **Component map**: Input features → Feature projection → Modality gating → Crossmodal Transformer (Stage 1) → Crossmodal Transformer (Stage 2) → Weighted concatenation → Output
- **Critical path**: Feature extraction through 1D Conv + GRU → Dynamic modality gating selection → Two-stage crossmodal attention → Weighted self-attention concatenation
- **Design tradeoffs**: Hierarchical fusion reduces model size (0.8M fewer parameters) but may miss some cross-modal interactions that all-to-all attention would capture; dynamic gating adds complexity but adapts to data characteristics
- **Failure signatures**: If modality weights converge too quickly, the model loses its ability to adapt; if hierarchical fusion is too aggressive, it may discard useful cross-modal information; if gating is poorly initialized, it may get stuck in suboptimal modality selection
- **First 3 experiments**:
  1. Run the feasibility analysis from Section 3 to observe how crossmodal attention highlights or distorts affective information
  2. Test the model with fixed modality hierarchy (no dynamic gating) to compare performance against the full dynamic version
  3. Vary the number of crossmodal transformer layers in each stage to find the optimal balance between performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How does HCT-DMG perform on datasets outside the scope of sentiment and emotion analysis, such as humor or sarcasm detection? The paper mentions future work includes testing HCT-DMG in other domains where affective tendencies of different modalities easily mismatch, such as humor and sarcasm detection.

### Open Question 2
How does the model's performance change when using different pre-trained language models, such as BERT or XLNet, instead of GloVe for text feature extraction? The paper mentions that the model uses GloVe for fair comparison with prior work, but suggests that using contextual language models could resolve issues with non-contextual word embeddings.

### Open Question 3
How does the model handle datasets with more than three modalities, such as those including physiological signals? The paper suggests that the model could be tested in domains where other modalities exist, such as physiological signals, but does not provide any experimental results.

## Limitations

- The paper relies heavily on ablation studies rather than controlled mechanistic experiments to validate claims about incongruity mitigation
- Dynamic modality gating mechanism has unclear convergence properties and constraint enforcement during training
- Limited evaluation to three-modality datasets, leaving performance on more complex multimodal scenarios unknown

## Confidence

- **High confidence**: Hierarchical fusion reduces model size while maintaining performance (direct comparison with baseline models and parameter counts)
- **Medium confidence**: Dynamic modality gating improves performance over static hierarchies (supported by ablation but limited to specific datasets)
- **Low confidence**: Crossmodal attention malfunctions with incongruity at latent level (based on qualitative observations rather than quantitative latent space analysis)

## Next Checks

1. Implement quantitative measures of attention distortion by computing KL divergence between attention distributions on congruent vs. incongruent samples, then verify that HCT-DMG reduces this distortion relative to standard crossmodal attention

2. Conduct controlled experiments where modality contributions are artificially manipulated (e.g., injecting noise into one modality) to test whether dynamic gating correctly identifies and prioritizes clean modalities across diverse scenarios

3. Perform ablation on the modality gating freezing mechanism - compare performance when weights are continuously updated versus when they're frozen after convergence, and measure the impact on adaptation to dataset shifts