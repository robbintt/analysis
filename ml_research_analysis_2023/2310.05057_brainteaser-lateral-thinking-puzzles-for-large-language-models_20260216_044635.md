---
ver: rpa2
title: 'BRAINTEASER: Lateral Thinking Puzzles for Large Language Models'
arxiv_id: '2310.05057'
source_url: https://arxiv.org/abs/2310.05057
tags:
- thinking
- commonsense
- lateral
- puzzles
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BRAINTEASER, a new benchmark designed to
  test large language models' (LLMs) ability to perform lateral thinking, which requires
  defying default commonsense associations to solve puzzles. The authors create a
  novel dataset of 1,100 lateral thinking puzzles, formatted as multiple-choice questions,
  and construct adversarial variants to evaluate models' reasoning consistency.
---

# BRAINTEASER: Lateral Thinking Puzzles for Large Language Models

## Quick Facts
- arXiv ID: 2310.05057
- Source URL: https://arxiv.org/abs/2310.05057
- Reference count: 31
- Large language models struggle with lateral thinking puzzles, achieving only ~60% accuracy compared to humans' 90%

## Executive Summary
This paper introduces BRAINTEASER, a novel benchmark designed to test large language models' ability to perform lateral thinkingâ€”solving puzzles that require defying default commonsense associations. The benchmark consists of 1,100 lateral thinking puzzles formatted as multiple-choice questions, along with adversarial variants to evaluate reasoning consistency. Experiments reveal a significant performance gap between humans and state-of-the-art models, with even the best models achieving only around 60% accuracy. The study demonstrates that LLMs struggle with lateral thinking due to memorization and misleading commonsense associations, highlighting the need for further research in this area.

## Method Summary
The BRAINTEASER benchmark contains 1,100 lateral thinking puzzles divided into sentence puzzles and word puzzles, each with four answer choices including a "None of the above" option. The authors construct adversarial variants through semantic and context reconstruction to test whether models truly understand the reasoning rather than memorizing answers. The benchmark is evaluated using zero-shot inference on instruction-finetuned models (ChatGPT, T0, FlanT5) and commonsense models (RoBERTa-L, CAR, CSKG-enhanced). The evaluation measures both accuracy on original puzzles and consistency across adversarial variants.

## Key Results
- Large language models achieve only ~60% accuracy on lateral thinking puzzles compared to humans' 90% accuracy
- Model performance is often close to random guessing (25%), with around a third performing equal or worse than random
- Scaling up instruction-finetuned models improves performance, but commonsense knowledge graphs negatively impact results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The benchmark reveals that LLMs struggle with lateral thinking because they default to commonsense associations that contradict the required creative reasoning.
- **Mechanism:** Lateral thinking puzzles are designed so that the correct answer requires overwriting default commonsense inferences. LLMs, trained on large corpora, have learned strong commonsense associations that act as distractors, preventing them from exploring creative explanations.
- **Core assumption:** The puzzles effectively isolate lateral thinking by making default commonsense associations logically inconsistent with the correct answer.
- **Evidence anchors:**
  - [abstract] "The authors create a novel dataset of 1,100 lateral thinking puzzles, formatted as multiple-choice questions, and construct adversarial variants to evaluate models' reasoning consistency."
  - [section] "While the extent to which these models possess common sense is heavily discussed..., we note that prior work has not considered the lateral thinking ability of LLMs."
  - [corpus] Found 25 related papers, average neighbor FMR=0.494, average citations=0.0. (Weak corpus evidence for this specific mechanism)

### Mechanism 2
- **Claim:** The adversarial variants (semantic and context reconstruction) are effective at reducing memorization and testing true reasoning ability.
- **Mechanism:** By creating puzzles that preserve the original reasoning path but alter the surface form or context, the benchmark distinguishes between models that memorize answers and those that understand the underlying lateral thinking principle.
- **Core assumption:** Models that truly understand lateral thinking will generalize across semantically or contextually altered versions of the same puzzle.
- **Evidence anchors:**
  - [abstract] "To assess the consistency of lateral reasoning by models, we enrich BRAIN TEASER based on a semantic and contextual reconstruction of its questions."
  - [section] "To ensure that our task evaluates lateral thinking ability rather than memorization, we construct adversarial versions of the original data in two parallel ways."
  - [corpus] Weak evidence; related work focuses on different evaluation strategies.

### Mechanism 3
- **Claim:** Scaling up model size improves performance on lateral thinking puzzles, but enriching with commonsense knowledge has a negative impact.
- **Mechanism:** Larger models have more parameters to potentially learn the creative reasoning patterns required for lateral thinking. However, commonsense knowledge graphs reinforce the very associations that lateral thinking requires one to abandon.
- **Core assumption:** The relationship between model size and lateral thinking performance is positive, while the relationship between commonsense knowledge and lateral thinking performance is negative.
- **Evidence anchors:**
  - [abstract] "Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance..."
  - [section] "The performance of the models is often close to random, with around a third of the models performing equal or worse than random guessing. As it can be expected, we see that scaling up instruction-finetuned models leads to improved performance..."
  - [corpus] Weak evidence; no direct citations about commonsense knowledge hindering lateral thinking.

## Foundational Learning

- **Concept:** Lateral vs. vertical thinking
  - **Why needed here:** The benchmark is specifically designed to test lateral thinking, which is distinct from the vertical thinking that most NLP benchmarks evaluate.
  - **Quick check question:** Can you explain the difference between a riddle that requires lateral thinking (like "What has a face and two hands but no arms or legs?") and one that requires vertical thinking (like a physics problem)?

- **Concept:** Adversarial evaluation
  - **Why needed here:** The benchmark uses adversarial variants to ensure that models are not simply memorizing answers but are truly reasoning about the puzzles.
  - **Quick check question:** Why is it important to test a model's performance on both the original puzzles and their adversarial variants?

- **Concept:** Commonsense knowledge as a distractor
  - **Why needed here:** The benchmark leverages the fact that commonsense knowledge can sometimes hinder creative problem-solving, which is a key aspect of lateral thinking.
  - **Quick check question:** How might a model's strong commonsense associations prevent it from solving a lateral thinking puzzle?

## Architecture Onboarding

- **Component map:** Dataset (1,100 puzzles) -> Original evaluation -> Semantic reconstruction -> Context reconstruction -> Adversarial evaluation
- **Critical path:** The construction and evaluation of adversarial variants is the most critical component, as it distinguishes between memorization and true reasoning ability.
- **Design tradeoffs:** Multiple-choice format simplifies evaluation but may reduce difficulty and allow for solving questions for the wrong reasons.
- **Failure signatures:** If models perform well on original puzzles but poorly on adversarial variants, it suggests memorization rather than reasoning. If models perform close to random on both, it suggests the puzzles are too difficult or models lack necessary reasoning capabilities.
- **First 3 experiments:**
  1. Evaluate a baseline model (e.g., RoBERTa) on the original puzzles to establish a performance baseline.
  2. Evaluate the same model on the adversarial variants to test for memorization vs. reasoning.
  3. Compare the performance of different model sizes (e.g., T5-small, T5-base, T5-large) to assess the impact of scaling on lateral thinking ability.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can lateral thinking models be developed to combine both vertical and lateral thinking effectively?
  - **Basis in paper:** [explicit] The authors mention that future work should investigate flexible ways to combine lateral and vertical thinking.
  - **Why unresolved:** The paper does not provide a concrete approach or methodology for combining these two types of thinking in language models.
  - **What evidence would resolve it:** A proposed framework or model architecture that demonstrates improved performance on tasks requiring both vertical and lateral thinking, validated through experiments.

- **Open Question 2:** What is the extent to which other brain teaser categories, such as puns and visual puzzles, require lateral or vertical thinking?
  - **Basis in paper:** [explicit] The authors acknowledge that it remains an open question to which extent other brain teaser categories require lateral or vertical thinking.
  - **Why unresolved:** The paper focuses on lateral thinking puzzles and does not extensively explore other brain teaser categories or their cognitive requirements.
  - **What evidence would resolve it:** A comprehensive study that categorizes various brain teaser types and evaluates the cognitive processes required for each, potentially leading to a taxonomy of thinking skills.

- **Open Question 3:** How can evaluation metrics be improved for creative and open-ended generations in lateral thinking tasks?
  - **Basis in paper:** [explicit] The authors suggest that future work should look into better evaluation metrics suitable for creative and open-ended generations.
  - **Why unresolved:** The current evaluation in the paper is based on multiple-choice questions, which may not fully capture the creative aspect of lateral thinking.
  - **What evidence would resolve it:** Development and validation of new evaluation metrics or methods that can effectively assess creative reasoning in open-ended formats, demonstrated through improved alignment with human judgments of creativity and correctness.

## Limitations

- The benchmark's effectiveness depends on whether puzzles truly require abandoning commonsense associations rather than following implicit logical chains.
- The boundary between lateral and vertical thinking remains somewhat fuzzy in practice, making it difficult to verify that the benchmark isolates a distinct cognitive ability.
- The multiple-choice format may reduce difficulty and allow for solving questions for the wrong reasons.

## Confidence

- **High Confidence:** The experimental observation that models perform significantly worse than humans (60% vs 90% accuracy) is well-supported by reported results.
- **Medium Confidence:** The claim that scaling model size improves lateral thinking performance while commonsense knowledge hinders it is supported by experimental trends but requires careful interpretation.
- **Low Confidence:** The assertion that the benchmark effectively isolates lateral thinking as a distinct cognitive ability separate from other reasoning forms is theoretically plausible but difficult to verify empirically.

## Next Checks

1. **Puzzle Solution Path Analysis:** Conduct a systematic analysis of puzzle solutions to verify that they genuinely require abandoning commonsense associations rather than following implicit logical inference chains. This could involve human annotators classifying solution paths and comparing against model reasoning patterns.

2. **Adversarial Variant Fidelity Test:** Evaluate whether models that solve original puzzles can solve their semantic and context variants at similar rates, and whether humans maintain consistent performance across variants. This would validate whether the adversarial construction preserves the core reasoning task.

3. **Controlled Commonsense Knowledge Experiment:** Systematically test whether adding commonsense knowledge to models in controlled ways (rather than full knowledge graph augmentation) produces consistent negative effects on lateral thinking performance, helping isolate whether the issue is knowledge interference or implementation details.