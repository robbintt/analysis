---
ver: rpa2
title: 'GRASS: Unified Generation Model for Speech-to-Semantic Tasks'
arxiv_id: '2309.02780'
source_url: https://arxiv.org/abs/2309.02780
tags:
- speech
- data
- instruction
- fine-tuning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRASS, a unified end-to-end model for speech-to-semantic
  tasks using instruction fine-tuning. GRASS generates semantic labels from audio
  conditioned on task-related prompts.
---

# GRASS: Unified Generation Model for Speech-to-Semantic Tasks

## Quick Facts
- **arXiv ID**: 2309.02780
- **Source URL**: https://arxiv.org/abs/2309.02780
- **Reference count**: 0
- **Primary result**: Unified instruction-fine-tuned model achieving SOTA on speech semantic tasks including NER, sentiment analysis, and question answering

## Executive Summary
GRASS introduces a unified end-to-end model for speech-to-semantic tasks using instruction fine-tuning. The model generates semantic labels from audio conditioned on task-related prompts, leveraging large-scale instruction-speech pairs constructed via TTS from Super-NaturalInstructions and Stanford-Alpaca datasets. Built on Whisper architecture with decoder fine-tuning, GRASS significantly outperforms state-of-the-art models across multiple speech semantic tasks even with limited training data, demonstrating competitive performance in zero-shot and few-shot scenarios.

## Method Summary
GRASS uses Whisper architecture to generate semantic labels conditioned on audio features and instruction text tokens. The model is pre-trained on large-scale instruction-speech pairs constructed via TTS from Super-NaturalInstructions and Stanford-Alpaca datasets, fine-tuning only the decoder parameters. For downstream tasks, the model is fine-tuned on varying data sizes (1%, 5%, 25%, 100%) and evaluated on multiple speech semantic benchmarks including NER, sentiment analysis, question answering, and spoken language understanding.

## Key Results
- Achieves state-of-the-art performance on SLUE-NER, SLUE-SA, SLUE-QA, FSC, and SLURP tasks
- Outperforms task-specific architectures with 25% training data in most cases
- Demonstrates competitive zero-shot and few-shot learning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GRASS achieves SOTA performance by unifying instruction fine-tuning with speech generation.
- **Mechanism**: Large-scale instruction-speech pairs via TTS enable the model to learn alignment between audio and semantic results.
- **Core assumption**: TTS-generated speech gap is not significant for semantic learning.
- **Evidence**: TTS pre-training described in Section 3.1; weak corpus support (average citations=0.0).

### Mechanism 2
- **Claim**: Competitive zero-shot/few-shot performance in low-resource settings.
- **Mechanism**: Instruction fine-tuning enables generalization from diverse instruction-speech pairs.
- **Core assumption**: Diversity enables effective learning with minimal labeled data.
- **Evidence**: 25% data experiments show SOTA performance; weak corpus support.

### Mechanism 3
- **Claim**: Unified model outperforms task-specific architectures.
- **Mechanism**: Instruction fine-tuning avoids brittleness of task-specific models.
- **Core assumption**: Unified approach generalizes better than specialized architectures.
- **Evidence**: Benchmark results across diverse tasks; weak corpus support.

## Foundational Learning

- **Text-to-Speech (TTS) Systems**
  - *Why needed*: Convert text instructions to speech for large-scale data creation
  - *Quick check*: How does TTS quality impact semantic alignment learning?

- **Instruction Fine-Tuning**
  - *Why needed*: Improves generalization to unseen tasks by aligning instructions with results
  - *Quick check*: What distinguishes instruction fine-tuning from traditional fine-tuning?

- **Whisper Architecture**
  - *Why needed*: Provides foundation for generating semantic labels from audio
  - *Quick check*: How does Whisper handle multi-task learning in GRASS?

## Architecture Onboarding

- **Component map**: Audio features → Encoder → Decoder (fine-tuned) → Semantic labels, conditioned on instruction text
- **Critical path**: Audio encoding → Instruction conditioning → Semantic label generation
- **Design tradeoffs**: TTS data enables scale but may differ from natural speech; decoder-only fine-tuning saves resources but may limit adaptability
- **Failure signatures**: Poor zero-shot/few-shot performance, generalization failures across tasks, semantic alignment degradation with natural speech
- **First 3 experiments**:
  1. Vary fine-tuning data size (1%, 5%, 25%, 100%) on single downstream task
  2. Test zero-shot/few-shot on unseen tasks
  3. Compare decoder-only vs full-model fine-tuning

## Open Questions the Paper Calls Out
- How does TTS system choice affect model performance?
- What is the saturation point for instruction diversity benefits?
- How does GRASS perform on non-English speech tasks?

## Limitations
- Reliance on TTS-generated rather than natural speech data
- Limited evaluation to six specific benchmarks
- Decoder-only fine-tuning may restrict task adaptability

## Confidence

- **High Confidence**: SOTA performance claims on tested benchmarks with 100% data
- **Medium Confidence**: Zero-shot/few-shot performance and instruction fine-tuning mechanism
- **Low Confidence**: TTS-generalization assumption and unified model superiority claims

## Next Checks
1. Validate performance on natural speech recordings versus TTS data
2. Test on additional speech semantic tasks beyond current six benchmarks
3. Compare decoder-only fine-tuning against full-model fine-tuning strategies