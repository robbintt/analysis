---
ver: rpa2
title: 'Safety Analysis in the Era of Large Language Models: A Case Study of STPA
  using ChatGPT'
arxiv_id: '2304.01246'
source_url: https://arxiv.org/abs/2304.01246
tags:
- system
- braking
- safety
- stpa
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a case study of applying ChatGPT to Systems
  Theoretic Process Analysis (STPA) for safety analysis of Automatic Emergency Brake
  (AEB) systems. Three interaction modes between ChatGPT and human experts were investigated:
  one-off simplex, recurring simplex, and recurring duplex interactions.'
---

# Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT

## Quick Facts
- arXiv ID: 2304.01246
- Source URL: https://arxiv.org/abs/2304.01246
- Reference count: 40
- Key outcome: ChatGPT-assisted STPA can outperform human experts alone with careful design and human collaboration, but reliability concerns remain

## Executive Summary
This paper investigates the application of ChatGPT to Systems Theoretic Process Analysis (STPA) for safety analysis of Automatic Emergency Brake (AEB) systems. Three interaction modes were explored: one-off simplex, recurring simplex, and recurring duplex between ChatGPT and human experts. The study reveals that ChatGPT alone is inadequate due to reliability issues, but when combined with expert oversight through recurring duplex interactions, it can enhance hazard identification and causal scenario generation beyond human-only analysis. No significant differences were found when varying input complexity or using common prompt guidelines, highlighting the need for domain-specific prompt engineering.

## Method Summary
The study applied ChatGPT to STPA methodology through three interaction modes with human safety experts: one-off simplex (single query), recurring simplex (step-by-step queries), and recurring duplex (iterative queries with expert analysis). The AEB system was used as the case study, with ChatGPT prompted to identify hazards, generate control structure diagrams, enumerate unsafe control actions (UCAs), and identify causal scenarios. Results were compared against baseline expert-generated STPA outputs using coverage metrics, comprehensiveness scores, and evaluation of skill requirements, time, and complexity.

## Key Results
- ChatGPT without human intervention is inadequate for reliable STPA due to missing UCA categories and inconsistent outputs
- Recurring duplex interaction between experts and ChatGPT outperforms one-off simplex in coverage and comprehensiveness
- Varying input semantic complexity or using common prompt guidelines showed no significant differences in ChatGPT's STPA performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs like ChatGPT can assist STPA by generating unsafe control actions (UCAs) and causal scenarios
- Mechanism: ChatGPT ingests system descriptions, produces UCA lists and potential causes, and supports expert refinement
- Core assumption: The LLM has sufficient training coverage of the system domain and safety terminology
- Evidence anchors:
  - [abstract] states that ChatGPT was used to identify UCAs and causal scenarios for AEB systems
  - [section] Example Q&A 1.2 shows ChatGPT outputting UCA-type statements and Example Q&A 2.3 shows it producing specific UCA candidates
  - [corpus] None of the neighboring papers explicitly cite LLM-generated UCAs, so direct evidence is limited to this paper
- Break condition: If ChatGPT outputs incomplete UCA sets (e.g., missing UCA-2 and UCA-4 as in Case 1), the reliability fails

### Mechanism 2
- Claim: Recurring duplex interaction between human experts and ChatGPT yields more comprehensive hazard analysis
- Mechanism: Experts query ChatGPT, analyze outputs, re-query with refined prompts, and integrate domain knowledge iteratively
- Core assumption: Iterative refinement improves both coverage and accuracy beyond one-off generation
- Evidence anchors:
  - [abstract] reports that recurring duplex interaction outperformed one-off simplex
  - [section] Example Q&A 3.1 vs 3.3 show expanded hazard lists after expert intervention
  - [corpus] No direct corpus evidence; relies on paper's own comparative study
- Break condition: If repeated queries do not increase coverage or if the human expert fails to detect gaps, the benefit disappears

### Mechanism 3
- Claim: ChatGPT cannot generate graphical control-loop diagrams, but can produce text descriptions that guide diagram creation
- Mechanism: Text-based descriptions are interpreted by experts to build visual control structures
- Core assumption: Experts can reliably translate text descriptions into accurate diagrams
- Evidence anchors:
  - [section] Example Q&A 1.3 explicitly states ChatGPT cannot generate graphics and provides a text outline
  - [section] Fig. 3(b) shows the diagram constructed from ChatGPT's text output
  - [corpus] No corpus evidence; unique to this study
- Break condition: If text descriptions are ambiguous, resulting diagrams may miss critical components

## Foundational Learning

- Concept: STPA (Systems Theoretic Process Analysis) methodology and its five-step workflow
  - Why needed here: The paper's experiments hinge on following and comparing STPA steps with ChatGPT assistance
  - Quick check question: What are the five steps of STPA, and which step typically requires the most expert judgment?

- Concept: UCA (Unsafe Control Action) taxonomy (four types: T1-T4)
  - Why needed here: ChatGPT's outputs are evaluated against these four categories
  - Quick check question: Which UCA type corresponds to "providing a control action too early or too late"?

- Concept: Basic AEB (Automatic Emergency Braking) system architecture and control loop
  - Why needed here: The case study focuses on AEB; understanding its components is essential to assess ChatGPT's outputs
  - Quick check question: What are the main sensors and actuators in a typical AEB system?

## Architecture Onboarding

- Component map:
  - Human safety experts (STPA process owners) -> ChatGPT (LLM responder) -> Prompt templates (structured queries for each STPA step) -> Comparison baseline (expert-generated STPA results) -> Output collation tool (Venn diagram, tables for scenario comparison)

- Critical path:
  1. Input: System description + STPA method keywords
  2. Step 1: Define hazards (ChatGPT + expert review)
  3. Step 2: Model control structure (text guidance → diagram)
  4. Step 3: Identify UCAs (ChatGPT list → expert validation)
  5. Step 4: Identify causal scenarios (ChatGPT list → expert integration)
  6. Step 5: Derive safety requirements (ChatGPT → expert refinement)
  7. Compare results to baseline and assess coverage

- Design tradeoffs:
  - Simplex vs duplex interaction: Simplex is faster but less comprehensive; duplex is slower but yields better coverage
  - Prompt specificity: Broad prompts yield general answers; narrow prompts produce focused but potentially incomplete results
  - Manual diagram creation vs automated generation: Text-based guidance avoids LLM limitations but risks misinterpretation

- Failure signatures:
  - Missing UCA categories (e.g., UCA-2, UCA-4 absent)
  - Inconsistent hazard definitions across queries
  - Ambiguous text descriptions leading to incorrect control loop diagrams
  - Over-reliance on ChatGPT without expert review

- First 3 experiments:
  1. Run one-off simplex on AEB: Capture UCAs and compare to baseline for completeness
  2. Run recurring simplex on AEB: Step-by-step queries for each STPA step; compare to baseline
  3. Run recurring duplex on AEB: Integrate expert analysis after each ChatGPT response; assess coverage and depth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective ways to evaluate the trustworthiness and reliability of LLMs for safety-critical applications?
- Basis in paper: [explicit] The paper identifies "trustworthiness concern of LLMs" as a key challenge and mentions properties like accuracy, reliability, generalization, robustness, interpretability, and fairness need to be considered
- Why unresolved: The paper notes that there is currently no widely accepted standard for evaluating the accuracy and robustness of LLMs' results, and developing such standards is an open challenge
- What evidence would resolve it: Development and validation of standardized benchmarks, evaluation frameworks, and certification processes specifically designed to assess LLM performance on safety-critical tasks

### Open Question 2
- Question: How can LLMs be effectively integrated into existing safety analysis processes while addressing challenges related to terminology and methodology differences?
- Basis in paper: [explicit] The paper discusses challenges of integrating LLMs into existing safety analysis processes and mentions the need for standardization on new ways of conducting safety analysis
- Why unresolved: While the paper proposes three use cases, it notes that determining which collaboration mode is more beneficial requires further exploration and interdisciplinary research
- What evidence would resolve it: Case studies and empirical evaluations comparing different integration approaches, along with the development of formal guidelines and best practices for LLM-assisted safety analysis

### Open Question 3
- Question: What are the implications of using LLMs in safety analysis for the training and education of safety professionals?
- Basis in paper: [explicit] The paper discusses how safety experts should possess knowledge of AI, engineering, safety, etc. to effectively use LLMs, and suggests that working with LLMs may provide long-term education for safety analysts
- Why unresolved: The paper raises this as a research question but does not provide detailed exploration of the educational implications or how safety curricula should evolve
- What evidence would resolve it: Studies on the effectiveness of LLM-assisted safety analysis in training programs, development of new educational frameworks incorporating LLM literacy, and long-term assessments of safety professionals' skill development when using LLMs

## Limitations
- Study limited to a single AEB system case study, reducing generalizability across domains
- Reliability of ChatGPT outputs may vary significantly across different system complexities and safety-critical domains
- Evaluation focuses primarily on comprehensiveness rather than accuracy or safety criticality of generated scenarios

## Confidence

- High confidence: The mechanism that ChatGPT cannot generate graphical diagrams and requires human translation of text descriptions
- Medium confidence: The finding that recurring duplex interaction outperforms one-off simplex in coverage, given the limited sample size
- Low confidence: The claim that no significant differences were observed when varying input semantic complexity, due to lack of rigorous statistical analysis

## Next Checks
1. Replicate the study across multiple safety-critical domains (e.g., medical devices, autonomous vehicles) to assess domain transferability of ChatGPT's STPA performance
2. Conduct statistical significance testing on coverage metrics between interaction modes to validate the observed differences
3. Implement a controlled experiment with safety experts to evaluate the accuracy and safety-criticality of ChatGPT-generated scenarios versus expert-only analysis