---
ver: rpa2
title: Understand Legal Documents with Contextualized Large Language Models
arxiv_id: '2303.12135'
source_url: https://arxiv.org/abs/2303.12135
tags:
- legal
- sentence
- entity
- classi
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents two contextualized large language models for
  legal text processing: Legal-BERT-HSLN for rhetorical role classification and Legal-LUKE
  for legal named entity recognition. Legal-BERT-HSLN uses a hierarchical sequential
  labeling network with attention pooling to model both intra- and inter-sentence
  context, while Legal-LUKE leverages entity-aware contextualized representations.'
---

# Understand Legal Documents with Contextualized Large Language Models

## Quick Facts
- arXiv ID: 2303.12135
- Source URL: https://arxiv.org/abs/2303.12135
- Reference count: 12
- Legal-BERT-HSLN achieved 0.834 micro F1 for rhetorical role classification, ranking 5th out of 27 teams

## Executive Summary
This paper presents two contextualized large language models for legal text processing: Legal-BERT-HSLN for rhetorical role classification and Legal-LUKE for legal named entity recognition. Legal-BERT-HSLN uses a hierarchical sequential labeling network with attention pooling to model both intra- and inter-sentence context, while Legal-LUKE leverages entity-aware contextualized representations. The models are evaluated on the SemEval-2023 Task 6 dataset. Legal-BERT-HSLN achieved a micro F1 score of 0.834, ranking 5th out of 27 teams in rhetorical role classification. Legal-LUKE achieved an F1 score of 0.796 on the validation set for legal named entity recognition, outperforming baselines by up to 15.0%. The results demonstrate the effectiveness of the proposed models in understanding legal documents.

## Method Summary
The paper introduces Legal-BERT-HSLN, a hierarchical sequential labeling network that uses Legal-BERT as an encoder with Bi-LSTM layers for context modeling and CRF for output, and Legal-LUKE, which uses LUKE with legal entity fine-tuning. Both models are trained on the SemEval-2023 Task 6 dataset for rhetorical role classification and legal named entity recognition tasks.

## Key Results
- Legal-BERT-HSLN achieved micro F1 score of 0.834 for rhetorical role classification, ranking 5th out of 27 teams
- Legal-LUKE achieved F1 score of 0.796 on validation set for legal named entity recognition
- Legal-LUKE outperformed baselines by up to 15.0% F1 in legal named entity recognition

## Why This Works (Mechanism)

### Mechanism 1
Legal-BERT-HSLN outperforms standard BERT by modeling both intra- and inter-sentence context through hierarchical Bi-LSTM layers. Token embeddings from Legal-BERT are enriched with local sentence context via a Bi-LSTM, then aggregated into sentence embeddings. A second Bi-LSTM captures inter-sentence dependencies, and CRF outputs rhetorical roles. Core assumption: Rhetorical roles depend on surrounding sentence semantics, not just isolated sentence meaning. Break condition: If sentences are truly independent, inter-sentence modeling adds noise and hurts performance.

### Mechanism 2
Legal-LUKE leverages entity-aware contextualized representations, giving it a 15% F1 boost over BERT-CRF for legal NER. LUKE is pre-trained to predict both words and entities, creating embeddings that fuse lexical and entity-centric semantics. Legal-LUKE fine-tunes this for domain-specific entities. Core assumption: Legal entity labels benefit from entity-specific embeddings rather than purely word-level features. Break condition: If legal entities are rare or absent in the domain, entity-aware pre-training offers little advantage.

### Mechanism 3
CRF layers at the sentence level improve rhetorical role classification over plain MLP by enforcing label dependencies. After sentence embeddings are generated, a linear layer feeds into CRF, which models transition probabilities between rhetorical roles. Core assumption: Rhetorical roles have strong sequential dependencies (e.g., PREAMBLE often precedes ISSUE). Break condition: If label dependencies are weak or random, CRF regularization hurts rather than helps.

## Foundational Learning

- Concept: Hierarchical Bi-LSTM for sentence-level sequence modeling
  - Why needed here: Legal documents require both word-level context (intra-sentence) and sentence-level context (inter-sentence) for accurate rhetorical role prediction
  - Quick check question: In a document with 3 sentences, how many distinct hidden states does a sentence-level Bi-LSTM produce?

- Concept: Entity-aware pre-training in LUKE
  - Why needed here: Legal texts contain domain-specific entities (statute, precedent) that static embeddings cannot capture well
  - Quick check question: If LUKE is trained to predict masked entities, what happens to the entity embedding space for rare legal terms?

- Concept: CRF decoding for structured prediction
  - Why needed here: Rhetorical roles form a coherent structure; naive classification ignores valid role transitions
  - Quick check question: What is the transition score matrix shape for 13 rhetorical roles?

## Architecture Onboarding

- Component map: Input preprocessing → Legal-BERT/LUKE tokenization → Token encoder → Legal-BERT/LUKE → token embeddings → Intra-sentence encoder → Bi-LSTM + attention → sentence embeddings → Inter-sentence encoder → Bi-LSTM → contextualized sentence embeddings → Decoder → Linear + CRF → rhetorical roles

- Critical path: 1. Token embedding generation (Legal-BERT/LUKE) 2. Context enrichment (Bi-LSTM or entity fusion) 3. Sequence-level decoding (CRF or span-based head)

- Design tradeoffs: Legal-BERT-HSLN vs. plain BERT: higher capacity but slower convergence. LUKE vs. BERT-CRF: entity-aware embeddings improve recall for rare entities but require more memory. CRF vs. MLP: enforces structure but increases inference time.

- Failure signatures: Overfitting on validation but poor test: likely distribution shift or preprocessing mismatch. Poor inter-sentence modeling: CRF transitions may be too constrained. Out-of-memory: Legal-LUKE large model + batch size too high.

- First 3 experiments: 1. Swap Bi-LSTM for Transformer in Legal-BERT-HSLN and measure F1 change. 2. Remove entity inputs from Legal-LUKE, keep only words, and compare F1. 3. Replace CRF with MLP in Legal-BERT-HSLN, check if label dependencies matter.

## Open Questions the Paper Calls Out

### Open Question 1
How can the performance gap between validation and test sets be explained and addressed? The authors observed a significant performance drop from validation to test set in legal named entity recognition, despite similar distributions. They hypothesize different preprocessing between validation and test sets but have not confirmed this due to lack of access to test set details. Access to the actual preprocessing methods used for test set or direct comparison of token indices and label distributions between validation and test sets would resolve this.

### Open Question 2
What causes the consistently low accuracy for PRE_NOT_RELIED label in rhetorical role classification? The authors note that PRE_NOT_RELIED label accuracy remains at 0% even with improved models, matching the state-of-the-art performance. They suggest investigating outlier effects but have not conducted systematic analysis of this specific label's characteristics or challenges. Detailed error analysis comparing misclassified PRE_NOT_RELIED instances with correct classifications of other labels, examining feature importance and contextual patterns, would resolve this.

### Open Question 3
How can Legal-LUKE be optimized to handle longer legal documents within GPU memory constraints? The authors encountered out-of-memory issues with Legal-LUKE and had to switch servers, indicating scalability challenges with current implementation. They used batch size adjustments as a workaround but did not explore architectural modifications or alternative training strategies. Performance comparison between different optimization strategies (gradient accumulation, model parallelism, knowledge distillation) while maintaining or improving accuracy would resolve this.

## Limitations
- Corpus size (247 training documents) is relatively small for fine-tuning large contextualized models, potentially leading to overfitting
- Several critical hyperparameters remain unspecified, making exact reproduction difficult
- Without test set results, validation improvements may not reflect true generalization capabilities

## Confidence

**High Confidence (Legal-BERT-HSLN mechanism)**
The hierarchical Bi-LSTM architecture for modeling intra- and inter-sentence context is well-established and theoretically sound. The 5th place ranking among 27 teams provides reasonable evidence of effectiveness.

**Medium Confidence (Legal-LUKE mechanism)**
The entity-aware pre-training advantage is supported by validation F1 scores, but the 15% improvement figure appears unusually high and requires replication. The claim that legal entities benefit from entity-specific embeddings is plausible but not independently verified.

**Low Confidence (CRF layer importance)**
While CRF layers are standard for structured prediction, the paper does not provide ablation studies comparing CRF vs. MLP performance. The assumption about strong rhetorical role dependencies is reasonable but unverified in the presented results.

## Next Checks

1. **Ablation Study for CRF vs. MLP**: Remove the CRF layer from Legal-BERT-HSLN and replace with a standard MLP classifier. Train both versions on the same training split and compare validation F1 scores. If CRF provides significant improvement (>2% F1), this validates the importance of modeling label dependencies.

2. **Entity Input Importance in Legal-LUKE**: Train a baseline Legal-LUKE model without entity inputs, using only word tokens. Compare F1 scores against the full Legal-LUKE model on the validation set. A performance drop >5% would confirm that entity-aware representations provide meaningful advantages for legal NER.

3. **Cross-Dataset Generalization**: Test both models on an independent legal corpus (e.g., European Court of Justice documents or other publicly available legal datasets). Measure F1 scores on rhetorical role classification and legal NER tasks. Performance degradation >10% from validation scores would indicate overfitting to the SemEval dataset.