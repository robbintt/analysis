---
ver: rpa2
title: Language Decision Transformers with Exponential Tilt for Interactive Text Environments
arxiv_id: '2302.05507'
source_url: https://arxiv.org/abs/2302.05507
tags:
- game
- games
- goal
- score
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Long-Context Language Decision Transformers
  (LLDTs), an offline reinforcement learning method for text-based games that leverages
  pre-trained language models to predict goal conditions, actions, and next observations.
  The method addresses key challenges in text-based games including long sequences
  of text, compositional action spaces, and sparse rewards by extending Decision Transformers
  with exponential tilt for goal conditioning, multiple novel goal conditioning strategies,
  and a next-observation prediction auxiliary loss.
---

# Language Decision Transformers with Exponential Tilt for Interactive Text Environments

## Quick Facts
- arXiv ID: 2302.05507
- Source URL: https://arxiv.org/abs/2302.05507
- Reference count: 4
- Key outcome: LLDTs achieve state-of-the-art performance on Jericho text games using return-to-go goal conditioning with exponential tilt (α=10) and auxiliary loss

## Executive Summary
This paper introduces Long-Context Language Decision Transformers (LLDTs) for text-based games, combining Decision Transformers with pre-trained language models and exponential tilt for goal conditioning. The method addresses key challenges in text-based games including long sequences of text, compositional action spaces, and sparse rewards. LLDTs extend Decision Transformers with novel goal conditioning strategies, exponential tilt sampling, and a next-observation prediction auxiliary loss. Experiments on five Jericho games demonstrate state-of-the-art performance, particularly when using return-to-go conditioning with exponential tilt and the auxiliary loss.

## Method Summary
LLDTs use LongT5-base as the backbone model to process long sequences of text in text-based games. The method generates noisy walkthrough trajectories by following walkthroughs for X% of steps then taking random actions, repeated across multiple seeds. The model is trained to predict goal conditions, actions, and next observations using a combination of primary Decision Transformer loss and auxiliary next-observation prediction loss (λ=0.5). At inference, exponential tilt (α=10) is applied to sample high but feasible goal conditions, and the model generates actions until game termination or invalid action generation.

## Key Results
- LLDTs achieve state-of-the-art performance on Jericho games with average normalized scores significantly higher than previous approaches
- Return-to-go goal conditioning with exponential tilt (α=10) yields the best results across games
- The next-observation prediction auxiliary loss provides consistent performance improvements
- Multiple novel goal conditioning strategies outperform traditional return-to-go methods in sparse reward environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exponential tilt steers the model toward higher achievable goals without needing explicit knowledge of maximum scores.
- Mechanism: The model outputs a probability distribution over goal conditions; exponential tilt reweights these probabilities by exp(α·gt), making high-goal samples more likely while maintaining feasibility.
- Core assumption: The model's predicted distribution over gt contains sufficient mass on high but achievable goals for the tilt to be effective.
- Evidence anchors:
  - [abstract] "exponential tilt to guide the agent towards high obtainable goals"
  - [section] "we perform exponential tilting on the predicted probabilities of gt... This allows us to sample high but probable target returns."
  - [corpus] Weak evidence; no directly relevant corpus papers.
- Break condition: If the model's predicted gt distribution is too narrow or biased toward low values, tilting will produce unrealistic goals and degrade performance.

### Mechanism 2
- Claim: Predicting next observations (auxiliary loss) improves decision-making by providing richer training signal.
- Mechanism: The model learns to anticipate how actions affect the environment state, indirectly capturing dynamics and stochasticity, which stabilizes and improves policy learning.
- Core assumption: The next observation prediction loss (MB-RCP) provides complementary information to the standard DT loss, acting as a form of model-based regularization.
- Evidence anchors:
  - [section] "predicting the next observation indirectly informs the model about the stochasticity of the environment... draws parallels with the model-based paradigm in Reinforcement Learning"
  - [abstract] "a model of future observations that improves agent performance"
  - [corpus] Weak evidence; no directly relevant corpus papers.
- Break condition: If the auxiliary loss overwhelms the primary DT loss or if the model focuses too much on next observation accuracy at the expense of action prediction quality.

### Mechanism 3
- Claim: Multiple novel goal conditioning strategies (ImR, FinS, AvgRTG) outperform traditional return-to-go in text-based games.
- Mechanism: Different goal conditioning definitions shape the model's learning objective in ways better suited to sparse reward environments, encouraging more efficient exploration and reward acquisition.
- Core assumption: The model can effectively learn from different temporal scopes of reward information, and the right choice depends on the game's reward structure.
- Evidence anchors:
  - [section] "we compare different conditioning methods... perform better than the return-to-go perspective of Decision Transformers"
  - [abstract] "novel goal conditioning methods yielding significantly better results than the traditional return-to-go"
  - [corpus] Weak evidence; no directly relevant corpus papers.
- Break condition: If the game environment has a very different reward structure than tested, the best conditioning strategy may differ, or the model may fail to adapt to new goal conditioning definitions.

## Foundational Learning

- Concept: Decision Transformers recast RL as sequence modeling by predicting actions conditioned on return-to-go.
  - Why needed here: Provides the core framework for adapting sequence models to text-based games where actions and observations are text.
  - Quick check question: What is the role of the goal condition (gt) in Decision Transformers?
- Concept: Exponential tilt for sampling from skewed distributions.
  - Why needed here: Enables sampling of high but feasible goal conditions without requiring knowledge of maximum scores.
  - Quick check question: How does exponential tilt modify the sampling distribution over goal conditions?
- Concept: Auxiliary losses for regularization and additional learning signal.
  - Why needed here: Predicting next observations provides model-based learning signal that improves decision-making performance.
  - Quick check question: What is the intuition behind using next observation prediction as an auxiliary loss in this context?

## Architecture Onboarding

- Component map: LongT5-base encoder-decoder model -> Trajectory preprocessing and tokenization -> Exponential tilt sampling module -> Multiple goal conditioning strategies -> Auxiliary next observation prediction head
- Critical path:
  1. Generate noisy walkthrough trajectories
  2. Preprocess into input/output sequences
  3. Train LongT5 with primary DT loss + auxiliary loss
  4. At inference, sample gt with exponential tilt
  5. Generate action at
  6. Execute action in environment
- Design tradeoffs:
  - Long context vs. computational efficiency (using placeholder tokens)
  - Auxiliary loss weight (λ=0.5) balances primary vs. auxiliary objectives
  - Exponential tilt parameter α balances exploration of high goals vs. feasibility
- Failure signatures:
  - Poor performance on games with very different reward structures than training data
  - Instability in goal sampling if exponential tilt α is too high
  - Overfitting to training games if not enough diversity in trajectories
- First 3 experiments:
  1. Train baseline DT model without exponential tilt or auxiliary loss on generated trajectories.
  2. Add exponential tilt with varying α values and measure impact on average normalized score.
  3. Add next observation auxiliary loss with λ=0.5 and compare to baseline without it.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLDTs scale with the number of games in the training dataset?
- Basis in paper: [inferred] The paper demonstrates success on 5 Jericho games but does not explore scaling to more games or different game types.
- Why unresolved: The experiments were limited to 5 games from the Zork universe, and the paper does not provide theoretical analysis or empirical evidence about how performance would change with more diverse or numerous games.
- What evidence would resolve it: Experiments training LLDTs on varying numbers of games (e.g., 5, 10, 20, 33) with systematic evaluation of performance improvements or degradation.

### Open Question 2
- Question: What is the relationship between the amount of exponential tilt (α) and the quality of generated trajectories across different types of text-based games?
- Basis in paper: [explicit] The paper tests α values of 0, 1, 10, and 20 but does not provide systematic analysis of how optimal α values might vary across game types or why certain values work better.
- Why unresolved: The paper only tests a limited set of α values on 5 games and does not explore whether the optimal α is game-dependent or if there's a principled way to select it.
- What evidence would resolve it: A comprehensive study varying α across different game genres and analyzing the correlation between α values and performance metrics.

### Open Question 3
- Question: How does the proposed next observation prediction auxiliary loss compare to other potential auxiliary losses in terms of improving performance in text-based games?
- Basis in paper: [explicit] The paper introduces and tests next observation prediction as an auxiliary loss but does not compare it to alternatives like predicting future states, actions, or other game-specific features.
- Why unresolved: The paper only tests one auxiliary loss and does not explore whether other forms of auxiliary supervision might be more effective for text-based game environments.
- What evidence would resolve it: Experiments comparing next observation prediction to alternative auxiliary losses (e.g., predicting next action probabilities, state embeddings, or other relevant features) across the same set of games.

## Limitations

- Limited experimental scope with only 5 Jericho games, potentially insufficient for establishing general effectiveness
- Reliance on generated noisy walkthrough trajectories raises questions about generalization to environments without walkthroughs
- Exponential tilt parameter α and auxiliary loss weight λ are fixed without systematic sensitivity analysis
- No discussion of computational costs or efficiency considerations for practical deployment

## Confidence

**High Confidence**: The core mechanism of using Decision Transformers for text-based games is sound, as the framework has been validated in multiple prior works. The integration of LongT5 for handling long context sequences is technically feasible and well-established. The overall experimental methodology (comparing against baselines, using multiple seeds) follows standard practices.

**Medium Confidence**: The specific effectiveness of exponential tilt with α=10 is supported by results but lacks comprehensive ablation studies. The claim that multiple novel goal conditioning strategies outperform traditional return-to-go is demonstrated on limited game data. The benefit of next observation prediction as an auxiliary loss is plausible but not rigorously isolated from other factors.

**Low Confidence**: Claims about the method's applicability to "challenging text-based games" are not well-supported given the narrow experimental scope. The assertion that the model can "efficiently solve challenging text-based games" overstates what is demonstrated in the results.

## Next Checks

1. **Ablation Study on Goal Conditioning Strategies**: Systematically compare the four goal conditioning methods (RTG, ImR, FinS, AvgRTG) across a larger and more diverse set of text-based games to verify which strategies work best under different reward structures and game types.

2. **Exponential Tilt Sensitivity Analysis**: Conduct experiments varying the exponential tilt parameter α across a wider range (e.g., α ∈ {1, 5, 10, 20, 50}) on each game to understand its impact on performance and identify optimal values for different game characteristics.

3. **Generalization to New Games**: Evaluate LLDTs on text-based games not included in the training data, particularly games without available walkthroughs, to assess whether the method can truly generalize beyond the specific Jericho games tested.