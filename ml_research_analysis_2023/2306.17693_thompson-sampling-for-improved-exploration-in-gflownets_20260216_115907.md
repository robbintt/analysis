---
ver: rpa2
title: Thompson sampling for improved exploration in GFlowNets
arxiv_id: '2306.17693'
source_url: https://arxiv.org/abs/2306.17693
tags:
- learning
- exploration
- gflownets
- policy
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Thompson sampling GFlowNets (TS-GFN) addresses the exploration-exploitation
  challenge in training generative flow networks by maintaining an approximate posterior
  over policies and sampling trajectories from this posterior. The method employs
  an ensemble of forward policies with shared backward policy, using Thompson sampling
  to guide trajectory selection during training.
---

# Thompson sampling for improved exploration in GFlowNets

## Quick Facts
- arXiv ID: 2306.17693
- Source URL: https://arxiv.org/abs/2306.17693
- Authors: 
- Reference count: 11
- One-line primary result: TS-GFN achieved L1 distance of ~10^-4 and discovered 60% more modes than baselines

## Executive Summary
Thompson sampling GFlowNets (TS-GFN) introduces an ensemble-based approach to address exploration-exploitation challenges in training generative flow networks. The method maintains an approximate posterior over policies and uses Thompson sampling to guide trajectory selection during training. By employing an ensemble of forward policies with a shared backward policy, TS-GFN demonstrates significantly improved exploration efficiency compared to traditional approaches like on-policy, tempered, and epsilon-greedy methods, achieving L1 distances of approximately 10^-4 in complex multimodal environments.

## Method Summary
TS-GFN maintains an ensemble of K forward policies with a shared backward policy, using Thompson sampling to guide trajectory selection during training. The algorithm employs statistical bootstrap to determine which trajectories train each ensemble member, while randomized prior networks improve uncertainty estimates. This approach ensures exploration of uncertain regions while exploiting known good regions, with the shared backward policy guaranteeing convergence to the same optimal forward policy across all ensemble members. The method requires only ~15% additional computation compared to prior approaches.

## Key Results
- Achieved L1 distance of ~10^-4 to target distribution in 64x64 grid environment, compared to 10^-3 for baseline methods
- Discovered 60% more modes than competing approaches in bit sequence generation task with 2^120 state space
- Required only ~15% additional computation compared to prior GFlowNet approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thompson sampling GFlowNets (TS-GFN) addresses exploration-exploitation by maintaining an approximate posterior over policies and sampling trajectories from this posterior
- Mechanism: The algorithm maintains an ensemble of forward policies with shared backward policy, using Thompson sampling to guide trajectory selection during training. Each ensemble member represents a different hypothesis about the reward distribution, and sampling from the posterior ensures exploration of uncertain regions while exploiting known good regions.
- Core assumption: Maintaining an ensemble of policies with shared backward policy allows efficient exploration while ensuring all members converge to the same optimal policy
- Evidence anchors:
  - [abstract] "TS-GFN maintains an approximate posterior distribution over policies and samples trajectories from this posterior for training"
  - [section] "we maintain an approximate posterior over forward policies PF by viewing the last layer of our policy network itself as an ensemble"
  - [corpus] Weak evidence - no direct citations about Thompson sampling in GFlowNets

### Mechanism 2
- Claim: The statistical bootstrap and randomized prior networks improve uncertainty estimates and exploration efficiency
- Mechanism: The statistical bootstrap determines which trajectories are used to train each ensemble member, while randomized prior networks provide better uncertainty estimates by adding fixed prior weights to the main network outputs. This combination ensures that trajectories are sampled from regions where the model is uncertain.
- Core assumption: The bootstrap probability and prior weights can be tuned to achieve optimal exploration-exploitation balance
- Evidence anchors:
  - [section] "we employ the statistical bootstrap to determine which trajectories τ may be used to train ensemble member PF,k and also make use of randomized prior networks"
  - [section] "Prior networks have been shown to significantly improve uncertainty estimates and agent performance in reinforcement learning tasks"
  - [corpus] Weak evidence - no direct citations about bootstrap methods in GFlowNets

### Mechanism 3
- Claim: Sharing a single backward policy across ensemble members ensures convergence to the same optimal forward policy while maintaining computational efficiency
- Mechanism: By sharing one PB across all ensemble members PF,k, the algorithm ensures that all forward policies converge to the same optimal P*F. This is because each PB uniquely determines a PF that satisfies the trajectory balance condition, and sharing PB creates a consistent learning target.
- Core assumption: Sharing backward policy does not compromise the quality of exploration or convergence speed
- Evidence anchors:
  - [section] "Specifying a different PB,k for each PF,k would result in setting a different learning target for each PF,k in the ensemble. By sharing a single PB across all ensemble members we ensure that all PF,k converge to the same optimal P*F"
  - [section] "sharing a single PB over all ensemble members... performing significantly better than maintaining a separate backward policy PB,k for each forward policy PF,k"
  - [corpus] Weak evidence - no direct citations about shared backward policies in GFlowNets

## Foundational Learning

- Concept: Thompson sampling and posterior sampling
  - Why needed here: The algorithm builds on Thompson sampling principles from multi-armed bandits and reinforcement learning to guide exploration in GFlowNets
  - Quick check question: What is the key difference between Thompson sampling and epsilon-greedy exploration strategies?

- Concept: Generative Flow Networks (GFlowNets)
  - Why needed here: The algorithm extends GFlowNets, which treat sampling from compositional objects as sequential decision-making problems with learnable action policies
  - Quick check question: How does the trajectory balance objective ensure that GFlowNets sample objects proportional to their reward?

- Concept: Ensemble methods and uncertainty estimation
  - Why needed here: The algorithm uses ensemble methods with randomized prior networks to estimate model uncertainty and guide exploration
  - Quick check question: Why is maintaining an ensemble of policies more effective than using a single policy with uncertainty estimates?

## Architecture Onboarding

- Component map: Forward policy ensemble (K members) -> Shared backward policy (PB) -> Statistical bootstrap mechanism -> Randomized prior networks -> Trajectory sampling and training loop

- Critical path: Forward policy sampling → Trajectory generation → Reward evaluation → Ensemble member selection → Loss computation → Parameter update

- Design tradeoffs:
  - Ensemble size vs. computational cost (TS-GFN requires ~15% more computation)
  - Bootstrap probability vs. exploration efficiency
  - Prior network weight vs. uncertainty estimation quality

- Failure signatures:
  - Poor exploration: Ensemble members converge to similar policies too quickly
  - Over-exploration: Ensemble members explore widely but fail to converge
  - Computational bottleneck: Shared backward policy becomes limiting factor

- First 3 experiments:
  1. Grid environment with multimodal rewards to test exploration efficiency
  2. Bit sequence generation task to evaluate mode discovery
  3. Ablation study comparing shared vs. separate backward policies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the shared backward policy PB across ensemble members PF,k affect the theoretical convergence guarantees of TS-GFN compared to maintaining separate PB,k for each ensemble member?
- Basis in paper: [explicit] The paper demonstrates empirically that sharing PB performs significantly better than maintaining separate PB,k, but doesn't provide theoretical justification for why this is the case.
- Why unresolved: The paper only provides empirical evidence showing improved performance with shared PB, but lacks theoretical analysis of the convergence properties and guarantees of this approach.
- What evidence would resolve it: A formal theoretical analysis comparing convergence rates and conditions between shared and separate PB approaches, including proofs of convergence guarantees.

### Open Question 2
- Question: What is the optimal ensemble size K for TS-GFN across different problem domains, and how does this scale with problem complexity?
- Basis in paper: [explicit] The paper uses K=100 for the grid task and K=50 for bit sequences, but doesn't systematically explore how ensemble size affects performance or provide guidance on selecting K.
- Why unresolved: The paper only tests two specific ensemble sizes without exploring the full trade-off space between computational cost and performance improvement.
- What evidence would resolve it: Systematic experiments varying K across multiple problem domains, showing performance curves and computational trade-offs to identify optimal ensemble sizes.

### Open Question 3
- Question: How does TS-GFN's performance compare to other uncertainty-based exploration methods specifically designed for GFlowNets, such as information gain-based approaches or ensemble-based uncertainty estimation?
- Basis in paper: [inferred] The paper compares TS-GFN to tempering, ϵ-noisy, and GAFN baselines, but doesn't compare to other uncertainty-based exploration methods that could be adapted to GFlowNets.
- Why unresolved: The comparison set is limited to traditional RL exploration methods rather than methods specifically designed for uncertainty estimation in generative models.
- What evidence would resolve it: Direct comparisons with information gain-based exploration and other uncertainty quantification methods adapted for GFlowNets, measuring both sample efficiency and final performance.

## Limitations
- Limited validation on real-world, high-dimensional problems where exploration efficiency is critical
- 15% computational overhead claim based on comparisons with unspecified prior approaches
- Ensemble size K not optimized or discussed in terms of scaling properties for larger state spaces

## Confidence
- **High confidence**: The core algorithmic contribution of using Thompson sampling with ensemble policies and shared backward policy is well-defined and mathematically grounded in the trajectory balance objective.
- **Medium confidence**: Empirical results showing L1 distance of 10^-4 and 60% more modes discovered are compelling but limited to synthetic benchmarks without comparison to state-of-the-art exploration methods on real-world tasks.
- **Low confidence**: Claims about the statistical bootstrap and randomized prior networks improving uncertainty estimates lack direct empirical support within the paper or from the cited literature on GFlowNets.

## Next Checks
1. **Scalability test**: Implement TS-GFN on a high-dimensional molecular generation task and compare mode discovery rate and sample efficiency against existing exploration strategies.
2. **Ablation study**: Systematically vary the ensemble size K and bootstrap probability to quantify their impact on exploration-exploitation tradeoff and computational overhead.
3. **Real-world benchmark**: Validate on a real-world dataset (e.g., molecule generation or protein design) to assess practical utility beyond synthetic environments.