---
ver: rpa2
title: Structurally guided task decomposition in spatial navigation tasks
arxiv_id: '2310.02221'
source_url: https://arxiv.org/abs/2310.02221
tags:
- planning
- task
- cost
- subtask
- navigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to extend human task decomposition
  modeling by incorporating structural information into planning, aiming to explain
  efficient planning under cognitive constraints. The core idea is to decompose a
  navigation task into subtasks and use A search with a spatial heuristic (Manhattan
  distance) to estimate planning costs, then select the subtask that minimizes the
  sum of path length and search effort.
---

# Structurally guided task decomposition in spatial navigation tasks

## Quick Facts
- arXiv ID: 2310.02221
- Source URL: https://arxiv.org/abs/2310.02221
- Reference count: 1
- Primary result: People learn to exploit task structure for efficient planning by decomposing complex navigation tasks into simpler subtasks that minimize combined planning cost and path length

## Executive Summary
This paper proposes a method to extend human task decomposition modeling by incorporating structural information into planning, aiming to explain efficient planning under cognitive constraints. The core idea is to decompose a navigation task into subtasks and use A* search with a spatial heuristic (Manhattan distance) to estimate planning costs, then select the subtask that minimizes the sum of path length and search effort. The model was tested in a spatial navigation experiment with 40 participants who navigated 12 mazes each, choosing between two mirrored subtasks. Results show that after initial trials, the majority of participants chose the subtask with lower planning cost, and this tendency increased significantly over trials (logistic regression intercept 0.731, slope 0.063, p=0.021), suggesting that people learn to exploit task structure for efficient planning.

## Method Summary
The method uses hierarchical planning through task decomposition, where a navigation task is broken into subtasks defined as reaching subgoal states. For each subtask, A* search with Manhattan distance heuristic estimates the planning cost by counting visited states. Participants choose between two subtasks adjacent to the starting position, each leading to a mirrored optimal path. The model predicts which subtask humans will select based on minimizing the sum of optimal path length and planning cost. The approach was validated through an online experiment where 40 participants completed 12 mazes each, with the model's predictions compared against human choices using logistic regression to track learning over trials.

## Key Results
- Participants increasingly chose the computationally simpler subtask over trials, rising from 42.5% in the first trial to 67.5% in the last trial
- Logistic regression showed significant learning effect: intercept 0.731 (p=0.030), slope 0.063 (p=0.021)
- Model predictions aligned with human behavior, suggesting people learn to exploit task structure for efficient planning
- The A* search with Manhattan distance heuristic effectively estimated planning costs in grid-based navigation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: People learn to exploit task structure for efficient planning by decomposing complex tasks into simpler subtasks that minimize the sum of path length and search effort.
- Mechanism: The model uses A* search with a spatial heuristic (Manhattan distance) to estimate planning costs for each subtask. Participants initially explore both subtask options but, over trials, learn to select the subtask with lower combined planning cost (search effort + path length).
- Core assumption: Participants can accurately estimate and compare the planning costs of different subtask decompositions, and this ability improves with experience.
- Evidence anchors:
  - [abstract] "Results show that after initial trials, the majority of participants chose the subtask with lower planning cost, and this tendency increased significantly over trials (logistic regression intercept 0.731, slope 0.063, p=0.021)"
  - [section] "We incorporate structural information by using A* (Hart, Nilsson, and Raphael 1968) with a spatial heuristic cost based on the Manhattan distance defined as h(s; g) = |sx − gx| + |sy − gy| given states with coordinates s = (sx, sy)."
  - [corpus] Weak evidence - no corpus papers directly discuss this specific mechanism of structural decomposition with A* heuristics in spatial navigation.
- Break condition: The mechanism breaks if participants cannot accurately estimate planning costs, if the spatial heuristic becomes invalid (e.g., non-grid environments), or if the decomposition strategy fails to reduce overall cognitive load.

### Mechanism 2
- Claim: Hierarchical planning through task decomposition reduces cognitive load by allowing people to focus on manageable subtasks rather than the entire complex problem.
- Mechanism: The framework decomposes the overall task into subtasks defined as reaching subgoal states. Action-level planning then finds sequences of states to accomplish each subtask independently, with shorter plans preferred.
- Core assumption: Breaking complex tasks into smaller, sequential subtasks reduces the computational complexity of planning while maintaining overall task utility.
- Evidence anchors:
  - [section] "Subtask-level planning decides which subtask to choose based on the expected reward and computational cost of visiting it en route to the goal."
  - [section] "Task decomposition decomposes a task into subtasks such that overall computational costs are minimized."
  - [corpus] Moderate evidence - the "How attention simplifies mental representations for planning" paper suggests attention mechanisms can simplify complex planning, supporting the hierarchical decomposition concept.
- Break condition: This mechanism fails when subtask boundaries are unclear, when subtasks are interdependent in complex ways, or when the decomposition overhead exceeds the benefits of simplification.

### Mechanism 3
- Claim: Learning over repeated trials enables participants to discover and exploit the structure of the task, leading to increasingly efficient navigation choices.
- Mechanism: Participants start with exploratory behavior (42.5% choosing the optimal subtask initially) and gradually learn the relationship between subtask structure and planning costs through feedback from their choices and outcomes.
- Core assumption: Participants can learn the statistical regularities in task structure through repeated exposure and adjust their decision-making accordingly.
- Evidence anchors:
  - [abstract] "this tendency increased significantly over trials (logistic regression intercept 0.731, slope 0.063, p=0.021), suggesting that people learn to exploit task structure for efficient planning."
  - [section] "We observed a learning process among the participants. The proportion of the computationally simpler subtask increased from 42.5% in the first trial to 67.5% in the last trial."
  - [corpus] Weak evidence - no corpus papers directly address learning of spatial task structure through navigation experiments.
- Break condition: The learning mechanism breaks if participants cannot detect patterns in the task structure, if the learning rate is too slow relative to task exposure, or if individual differences in learning ability create divergent strategies.

## Foundational Learning

- Concept: A* search algorithm with heuristics
  - Why needed here: The model uses A* with Manhattan distance to estimate planning costs for each subtask. Understanding this algorithm is crucial for implementing and modifying the cost estimation component.
  - Quick check question: How does the Manhattan distance heuristic guarantee that A* will find the optimal path in a grid-based navigation task?

- Concept: Task decomposition and subtask planning
  - Why needed here: The framework decomposes navigation tasks into subtasks based on subgoals. Engineers need to understand how to define appropriate subtasks and how they relate to overall task completion.
  - Quick check question: What criteria determine whether a state should be chosen as a subgoal in task decomposition?

- Concept: Logistic regression for analyzing choice behavior
  - Why needed here: The paper uses logistic regression to model how the probability of choosing the optimal subtask changes over trials. Understanding this statistical approach is important for analyzing experimental results.
  - Quick check question: What does the positive slope coefficient (0.063) in the logistic regression model tell us about participant learning over trials?

## Architecture Onboarding

- Component map: Task representation (S, T, s0, g) -> Decomposition module (fixed subtask set Z) -> Cost estimation (A* with Manhattan distance) -> Decision layer (subtask selection) -> Navigation -> Outcome Feedback -> Learning Update

- Critical path: Task → Decomposition → Cost Estimation → Subtask Selection → Navigation → Outcome Feedback → Learning Update

- Design tradeoffs:
  - Fixed vs. learned subtask decomposition: Fixed decomposition simplifies implementation but may miss optimal decompositions for specific tasks
  - Heuristic choice: Manhattan distance works for grid-based tasks but may need adaptation for continuous or non-Euclidean spaces
  - Learning speed: Balancing between rapid exploitation of known structure and continued exploration for potentially better decompositions

- Failure signatures:
  - Participants consistently choosing suboptimal subtasks despite learning opportunity
  - A* search taking excessive time due to inefficient heuristic or implementation
  - Decomposition creating subtasks that are too large (no simplification) or too small (excessive overhead)

- First 3 experiments:
  1. Verify A* with Manhattan distance correctly estimates planning costs on simple test mazes with known optimal paths
  2. Test task decomposition on mazes with varying wall configurations to ensure subtask selection aligns with human intuition
  3. Run a pilot with 5-10 participants on a subset of mazes to validate learning trends before full-scale experiment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific heuristics beyond Manhattan distance could improve the model's prediction accuracy for the 32.5% of participants who didn't choose the subtask with lower planning costs?
- Basis in paper: [explicit] The discussion section notes that the model could not predict the choices of 32.5% of participants and suggests room for improvement in the choice of planner and/or heuristics.
- Why unresolved: The paper tested only one heuristic (Manhattan distance) and found limitations in prediction accuracy.
- What evidence would resolve it: Testing alternative heuristics (e.g., Euclidean distance, domain-specific heuristics, learned heuristics) and comparing their prediction accuracy on the same dataset.

### Open Question 2
- Question: How do people learn to decompose tasks and select subtasks during the planning process, and what cognitive mechanisms underlie this learning?
- Basis in paper: [explicit] The paper observes a learning process where participants increasingly chose the computationally simpler subtask over trials, and suggests future work should investigate how people learn to decompose and select tasks.
- Why unresolved: The study observed learning trends but did not model or explain the cognitive mechanisms behind task decomposition learning.
- What evidence would resolve it: Longitudinal studies tracking individual participants' strategy evolution, computational models of learning to decompose, and neuroimaging studies of task decomposition processes.

### Open Question 3
- Question: How would the model perform in more complex spatial navigation tasks with multiple branching points and non-symmetric maze structures?
- Basis in paper: [inferred] The current model was tested on relatively simple mazes with only two subtasks, suggesting the need to test scalability to more complex scenarios.
- Why unresolved: The experimental mazes were specifically designed with mirrored subtasks and limited complexity, not representing the full range of possible navigation challenges.
- What evidence would resolve it: Testing the model on mazes with multiple decision points, varying wall configurations, and asymmetric structures, then comparing prediction accuracy to the current results.

## Limitations
- The fixed subtask decomposition with only two options may not scale to more complex real-world planning scenarios
- The learning mechanism assumes participants can accurately estimate planning costs, but individual differences in spatial reasoning abilities could create significant variability
- The approach may not generalize well to continuous or non-Euclidean navigation spaces where Manhattan distance heuristic becomes invalid

## Confidence
- **High confidence**: The statistical analysis showing increased preference for lower-cost subtasks over trials (intercept 0.731, slope 0.063, p=0.021) is well-supported by the experimental data.
- **Medium confidence**: The A* search with Manhattan distance effectively estimates planning costs in grid-based navigation tasks, though this may not generalize to continuous or non-Euclidean spaces.
- **Low confidence**: The fixed subtask decomposition strategy would perform well in more complex, real-world planning scenarios with multiple interdependent subtasks.

## Next Checks
1. Test the model's predictions against human behavior in a continuous navigation task where Manhattan distance heuristic becomes invalid, to assess robustness of the planning cost estimation approach.
2. Implement an adaptive subtask decomposition method that learns optimal subtask boundaries from data, comparing performance against the fixed decomposition strategy on varied maze configurations.
3. Conduct a follow-up experiment with participants performing the same navigation tasks after a delay period to test whether learned task structure knowledge persists and whether learning rates differ between individuals.