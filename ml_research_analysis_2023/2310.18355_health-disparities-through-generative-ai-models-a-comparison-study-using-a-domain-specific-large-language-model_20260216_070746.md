---
ver: rpa2
title: 'Health Disparities through Generative AI Models: A Comparison Study Using
  A Domain Specific large language model'
arxiv_id: '2310.18355'
source_url: https://arxiv.org/abs/2310.18355
tags:
- health
- disparities
- language
- healthcare
- scibert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared two large language models, BERT and SciBERT,
  to assess their performance in understanding health disparities in text queries.
  Cosine similarity was used to measure the similarity between text queries about
  health disparities and other topics.
---

# Health Disparities through Generative AI Models: A Comparison Study Using A Domain Specific large language model

## Quick Facts
- arXiv ID: 2310.18355
- Source URL: https://arxiv.org/abs/2310.18355
- Reference count: 40
- BERT outperformed SciBERT in distinguishing race-related health disparity queries from generic race mentions

## Executive Summary
This study compared BERT and SciBERT models using cosine similarity to assess their ability to understand health disparities in text queries. While SciBERT failed to differentiate between queries containing "race" alone and those discussing how race perpetuates health disparities, BERT showed better performance, particularly when multiple factors were included. The findings highlight critical limitations in domain-specific models when addressing nuanced social determinants of health and underscore the need for careful consideration of training data diversity and bias in healthcare AI applications.

## Method Summary
The study employed cosine similarity to measure semantic distances between text queries about health disparities and other topics. Two language models (BERT and SciBERT) generated vector embeddings for the queries, which were then compared using cosine similarity scores. The comparison focused on distinguishing queries containing "race" alone from those discussing race as a factor in health disparities, evaluating model performance across different contextual complexities.

## Key Results
- SciBERT failed to distinguish between "race" alone and "race perpetuates health disparities" in text queries
- BERT showed slightly better performance in making this distinction, especially with multi-factor queries
- Domain-specific models may underperform on nuanced social determinant contexts despite biomedical training advantages

## Why This Works (Mechanism)

### Mechanism 1
- SciBERT fails to differentiate between "race" alone and "race perpetuates health disparities" because its training corpus overweights biomedical text where "race" is a common factor without contextual depth
- Core assumption: The biomedical training corpus does not encode the contextual meaning of "race" in health disparities
- Evidence: [abstract] SciBERT failed to distinguish between race queries; [section] SciBERT trained on scientific papers
- Break condition: Balanced training corpus with social science literature would improve discrimination

### Mechanism 2
- BERT performs better than SciBERT because its general-purpose training includes broader contextual understanding of social and ethical language
- Core assumption: General-purpose training data includes sufficient social science content
- Evidence: [abstract] BERT showed better distinction performance; [section] BERT achieves state-of-the-art NLP results
- Break condition: Retraining BERT without social science content would degrade health disparity performance

### Mechanism 3
- Cosine similarity measurements reveal model biases by quantifying semantic distance between queries based on learned embeddings
- Core assumption: Cosine similarity accurately reflects semantic differences in embeddings
- Evidence: [abstract] Used cosine similarity to measure query similarity; [section] Cosine similarity is a widely used metric
- Break condition: Different similarity metrics might change failure patterns

## Foundational Learning

- **Domain-specific vs. general-purpose language models**: Understanding why SciBERT underperforms requires knowing how training corpora shape model capabilities
  - Quick check: What is the primary difference between domain-specific and general-purpose language models in terms of training data?

- **Cosine similarity in vector space models**: The study uses cosine similarity to measure semantic differences
  - Quick check: How does cosine similarity differ from Euclidean distance in high-dimensional spaces?

- **Health disparities as social determinants of health**: The core comparison depends on understanding race as both clinical variable and social determinant
  - Quick check: Why is treating "race" as a simple clinical variable problematic in health disparities discussions?

## Architecture Onboarding

- **Component map**: Text query → Language model (BERT/SciBERT) → Vector embeddings → Cosine similarity calculation → Similarity score comparison
- **Critical path**: Query input → Model embedding generation → Cosine similarity computation → Result interpretation
- **Design tradeoffs**: Domain-specific models offer better performance on specialized tasks but may miss contextual nuances
- **Failure signatures**: Similar cosine similarity scores for semantically distinct queries indicate contextual understanding limitations
- **First 3 experiments**:
  1. Test additional health disparity queries with varying contextual complexity to map performance boundaries
  2. Compare results using different similarity metrics (Euclidean, Jaccard) to validate findings
  3. Analyze model embeddings directly to identify where race-related concepts cluster differently

## Open Questions the Paper Calls Out

### Open Question 1
- How does the performance of domain-specific language models like SciBERT compare to general-purpose models like BERT in addressing health disparities in text queries?
- Basis: [explicit] Paper states SciBERT failed while BERT performed better on race-related health disparity queries
- Why unresolved: Limited comparison focusing on one aspect (race) without broader health disparity contexts
- Resolution evidence: Comprehensive evaluation using diverse health disparity queries across multiple models

### Open Question 2
- What specific biases exist in domain-specific language model training data and how do they impact health disparity performance?
- Basis: [inferred] Paper suggests SciBERT's failure relates to training data biases
- Why unresolved: No detailed analysis of training data composition or bias identification
- Resolution evidence: Thorough analysis of training corpora for bias related to race, ethnicity, and social determinants

### Open Question 3
- How can large language model development in healthcare be made more ethical and equitable when addressing health disparities?
- Basis: [explicit] Paper emphasizes need for ethical and equitable development and implementation
- Why unresolved: Lacks specific guidelines or best practices for achieving ethical AI in healthcare
- Resolution evidence: Comprehensive framework for ethical AI including bias mitigation, privacy protection, and stakeholder engagement

## Limitations

- Limited query specification prevents full assessment of whether query selection adequately represents health disparity complexity
- Mechanisms explaining performance differences remain speculative without direct evidence linking training data characteristics to model behavior
- Cosine similarity as sole evaluation metric may not capture all relevant aspects of model performance in health disparity contexts

## Confidence

- Mechanism 1 (SciBERT training corpus limitations): Low confidence - Results align but lack direct evidence
- Mechanism 2 (BERT general-purpose training advantages): Medium confidence - Supported by performance differences but lacks detailed training data analysis
- Mechanism 3 (Cosine similarity as bias detector): Low confidence - Standard metric but not validated for health disparity contexts

## Next Checks

1. Test the same query sets with additional language models (RoBERTa, GPT variants) to determine if pattern is specific to BERT/SciBERT
2. Conduct detailed analysis of actual training corpora for both models to quantify social science literature presence
3. Replicate analysis using multiple similarity metrics (Euclidean distance, Jaccard similarity, learned metrics) to verify robustness