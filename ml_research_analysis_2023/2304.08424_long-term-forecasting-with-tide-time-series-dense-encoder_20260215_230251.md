---
ver: rpa2
title: 'Long-term Forecasting with TiDE: Time-series Dense Encoder'
arxiv_id: '2304.08424'
source_url: https://arxiv.org/abs/2304.08424
tags:
- forecasting
- linear
- time-series
- covariates
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TiDE, a simple MLP-based encoder-decoder model
  for long-term time-series forecasting. TiDE enjoys the simplicity and speed of linear
  models while also being able to handle covariates and non-linear dependencies.
---

# Long-term Forecasting with TiDE: Time-series Dense Encoder

## Quick Facts
- arXiv ID: 2304.08424
- Source URL: https://arxiv.org/abs/2304.08424
- Reference count: 13
- Primary result: TiDE achieves competitive performance on long-term forecasting benchmarks while being 5-10x faster than transformer baselines

## Executive Summary
This paper introduces TiDE, a simple MLP-based encoder-decoder architecture for long-term time-series forecasting. The model combines the speed and simplicity of linear models with the ability to handle covariates and non-linear dependencies through residual blocks with skip connections. Theoretically, the paper proves that the linear analogue of TiDE achieves near-optimal error rates under linear dynamical system assumptions. Empirically, TiDE matches or outperforms prior approaches on popular benchmarks while demonstrating significant computational efficiency gains.

## Method Summary
TiDE is an encoder-decoder architecture built from residual blocks consisting of one-hidden-layer MLPs with ReLU activation, dropout, and layer normalization. The encoder processes past time series, dynamic covariates, and static attributes through stacked residual blocks. The decoder maps the encoded representation to per-time-step vectors, which are then combined with projected future covariates via a temporal decoder to produce final predictions. A global linear residual connection from look-back to horizon ensures the model always retains linear forecasting capabilities as a fallback.

## Key Results
- Matches or outperforms transformer-based models on Weather, Traffic, Electricity, and ETT benchmarks
- Achieves 5-10x speedup compared to best transformer baselines
- Maintains competitive performance while using simpler architecture without attention mechanisms
- Linear analogue achieves near-optimal error rates under LDS assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual blocks with skip connections enable learning complex non-linear mappings while preserving linear relationships
- Mechanism: Each residual block applies a one-hidden-layer MLP with ReLU activation and a fully linear skip connection, followed by dropout and layer normalization
- Core assumption: Non-linearities in residual blocks capture underlying dynamics while linear skip connection preserves linear predictive power
- Evidence anchors: Abstract mentions "simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies"; Section describes residual block structure

### Mechanism 2
- Claim: Temporal decoder with highway connection from future covariates enables direct incorporation of time-specific information
- Mechanism: Temporal decoder applies residual block mapping decoded vectors at each horizon time-step concatenated with projected future covariates
- Core assumption: Future covariates contain predictive information usable through direct pathway without complex transformations
- Evidence anchors: Abstract highlights handling of covariates; Section describes temporal decoder mapping

### Mechanism 3
- Claim: Global linear residual connection from look-back to horizon ensures fallback to linear forecasting performance
- Mechanism: Model adds linear mapping from look-back to horizon vector added to prediction
- Core assumption: Linear relationships between past and future contain valuable predictive information
- Evidence anchors: Abstract contrasts with transformer approaches; Section describes global residual connection

## Foundational Learning

- Concept: Linear Dynamical Systems (LDS)
  - Why needed here: Theoretical analysis proves linear analogue of TiDE achieves near-optimal error rates under LDS assumptions
  - Quick check question: What condition on LDS transition matrix A ensures theoretical guarantee holds?

- Concept: Multi-layer Perceptron (MLP) residual blocks
  - Why needed here: Understanding residual connections is crucial for grasping how TiDE balances linear and non-linear modeling
  - Quick check question: What is mathematical form of residual block used in TiDE?

- Concept: Feature projection and dimensionality reduction
  - Why needed here: Feature projection reduces dynamic covariates from dimension r to lower dimension while preserving important information
  - Quick check question: Why project high-dimensional covariates to lower dimension before encoding?

## Architecture Onboarding

- Component map: Feature Projection → Dense Encoder → Dense Decoder → Temporal Decoder → Global Linear Residual
- Critical path: Feature Projection → Dense Encoder → Dense Decoder → Temporal Decoder → Global Linear Residual
- Design tradeoffs:
  - Simplicity vs expressiveness: Uses simple MLP blocks instead of attention for computational efficiency
  - Linear vs non-linear: Maintains linear fallback capability while learning non-linear patterns
  - Depth vs width: Tunable number of layers and hidden sizes for different dataset characteristics
- Failure signatures:
  - Underfitting: High training and validation error, may need more layers or larger hidden sizes
  - Overfitting: Low training error but high validation error, may need dropout or regularization
  - Slow convergence: May benefit from learning rate tuning or different optimizer settings
- First 3 experiments:
  1. Baseline linear model (DLinear) to establish performance floor
  2. TiDE without temporal decoder to measure its contribution
  3. TiDE with varying look-back lengths to find optimal context window

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TiDE performance scale with increasing look-back window sizes beyond 2880?
- Basis in paper: [inferred] Paper demonstrates efficiency for look-back sizes up to 2880 but doesn't explore beyond this range
- Why unresolved: Paper focuses on demonstrating efficiency and accuracy for long-term forecasting
- What evidence would resolve it: Experimental results showing performance and computational efficiency for look-back windows significantly larger than 2880

### Open Question 2
- Question: How would TiDE perform on datasets with non-linear dependencies between time-series?
- Basis in paper: [explicit] Paper discusses ability to handle non-linear dependencies but lacks experimental results on datasets specifically testing this capability
- Why unresolved: Paper uses standard benchmarks that may not fully capture handling of complex non-linear relationships
- What evidence would resolve it: Experimental results on synthetic or real-world datasets with known non-linear dependencies between time-series

### Open Question 3
- Question: How does choice of dynamic covariates affect TiDE's performance in real-world applications?
- Basis in paper: [explicit] Paper mentions using simple time-derived features but doesn't explore impact of different types or combinations of covariates
- Why unresolved: Paper uses standard set of covariates, leaving optimal covariate selection for specific applications unanswered
- What evidence would resolve it: Comparative experiments using different sets of dynamic covariates on real-world datasets, measuring impact on forecasting accuracy

## Limitations
- Theoretical analysis assumes Linear Dynamical Systems with specific conditions not verified on benchmark datasets
- 5-10x speedup claim lacks precise specification of comparison model and hardware conditions
- Empirical demonstration of handling non-linear dependencies lacks rigorous theoretical justification

## Confidence
- **High confidence**: Architectural design choices are well-documented and empirical results showing competitive performance across multiple benchmarks are robust
- **Medium confidence**: Theoretical guarantee for linear TiDE under LDS assumptions is mathematically sound but practical relevance to real-world time series is uncertain
- **Low confidence**: Claim that TiDE can "handle covariates and non-linear dependencies" is demonstrated empirically but lacks rigorous theoretical justification for when residual blocks successfully capture non-linear patterns

## Next Checks
1. **LDS assumption verification**: Test whether benchmark datasets exhibit properties of stable linear dynamical systems by estimating transition matrices and checking singular value distributions
2. **Component importance analysis**: Systematically remove each architectural component and measure performance degradation across all datasets to quantify each component's contribution
3. **Scalability benchmarking**: Replicate claimed 5-10x speedup by benchmarking TiDE against multiple transformer baselines (N-Beats, Informer, Autoformer) on identical hardware using standardized batch sizes and sequence lengths