---
ver: rpa2
title: Studying K-FAC Heuristics by Viewing Adam through a Second-Order Lens
arxiv_id: '2310.14963'
source_url: https://arxiv.org/abs/2310.14963
tags:
- learning
- adamqlr
- adam
- damping
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AdamQLR, a novel optimizer that combines the
  benefits of first-order methods (computational efficiency) and second-order methods
  (faster convergence). The key idea is to incorporate damping and learning rate selection
  techniques from K-FAC (a second-order optimizer) into Adam (a first-order optimizer).
---

# Studying K-FAC Heuristics by Viewing Adam through a Second-Order Lens

## Quick Facts
- arXiv ID: 2310.14963
- Source URL: https://arxiv.org/abs/2310.14963
- Reference count: 40
- Primary result: AdamQLR achieves competitive generalization performance while being more robust to hyperparameters than Adam and K-FAC

## Executive Summary
This paper introduces AdamQLR, a novel optimizer that bridges first-order and second-order optimization methods by incorporating K-FAC's damping and learning rate selection techniques into Adam. The key insight is that Adam's internal curvature estimates provide a reasonable approximation to the empirical Fisher matrix diagonal, making Fisher curvature information appropriate for learning rate selection. Through extensive experiments across regression and classification tasks, AdamQLR demonstrates competitive generalization performance while being more robust to hyperparameter choices than existing optimizers.

## Method Summary
AdamQLR modifies the Adam optimizer by incorporating damping and learning rate selection techniques from K-FAC. The algorithm uses Levenberg-Marquardt damping to replace the curvature matrix C with C + λI, then selects the optimal learning rate α based on a quadratic model minimization. An adaptive damping strategy adjusts λ based on the reduction ratio ρ, which compares predicted versus actual objective changes. The method leverages Adam's internal curvature estimates as an approximation to the Fisher information matrix diagonal.

## Key Results
- AdamQLR achieves competitive generalization performance across multiple regression and classification tasks
- An untuned version of AdamQLR can match the performance of tuned benchmark optimizers
- AdamQLR demonstrates greater robustness to hyperparameter choices compared to Adam and K-FAC
- Fisher curvature information performs slightly better than Hessian curvature on tested tasks

## Why This Works (Mechanism)

### Mechanism 1
AdamQLR improves Adam's performance by incorporating K-FAC's damping and learning rate selection techniques while using Fisher curvature information. The algorithm uses a damped curvature matrix C + λI in the quadratic model to prevent large updates in low-curvature directions, then selects the optimal learning rate α based on this model. This combines Adam's efficient update directions with second-order stability heuristics.

### Mechanism 2
Adam's internal curvature estimates (bvt) provide a reasonable approximation to the empirical Fisher matrix diagonal, making Fisher curvature appropriate for learning rate selection. Adam's exponential moving average of squared gradients in bvt implicitly approximates the diagonal elements of the Fisher information matrix through gradient variance tracking.

### Mechanism 3
The adaptive damping strategy based on reduction ratio ρ provides effective stabilization without requiring manual tuning. The algorithm compares predicted vs actual objective changes to compute ρ, then adjusts λ multiplicatively based on whether ρ indicates good, acceptable, or poor model predictions.

## Foundational Learning

- Concept: Second-order optimization methods
  - Why needed here: Understanding the theoretical foundation of curvature-based optimization and why it can converge faster than first-order methods
  - Quick check question: What is the key difference between first-order methods (like SGD) and second-order methods (like K-FAC) in terms of information used for updates?

- Concept: Fisher information matrix and its relationship to empirical Fisher
  - Why needed here: The paper uses Fisher curvature for learning rate selection, and understanding its properties is crucial for implementing AdamQLR
  - Quick check question: How does the empirical Fisher matrix differ from the true Fisher information matrix, and why is this distinction important?

- Concept: Levenberg-Marquardt damping and trust region methods
  - Why needed here: The damping mechanism is central to AdamQLR's stability, and understanding trust region concepts is essential for grasping why damping helps
  - Quick check question: What problem does Levenberg-Marquardt damping solve in second-order optimization, and how does it achieve this?

## Architecture Onboarding

- Component map: AdamQLR wraps around vanilla Adam, adding curvature computation (Fisher or Hessian), damping adjustment logic, and learning rate selection based on quadratic model minimization. The core Adam components (momentum, variance tracking) remain unchanged.
- Critical path: For each iteration: compute gradients → update Adam buffers → compute curvature-vector product → calculate reduction ratio → adjust damping → select learning rate → apply update with clipping
- Design tradeoffs: Using Fisher curvature requires architecture-specific loss function derivatives but provides better stability; using Hessian is more general but may be less effective. Learning rate clipping adds robustness but may limit convergence speed.
- Failure signatures: Training loss divergence, plateauing at suboptimal values, high variance in loss curves, or sensitivity to hyperparameter choices (especially λ0 and αmax) indicate problems.
- First 3 experiments:
  1. Rosenbrock function minimization to visually verify the algorithm can navigate curved valleys
  2. UCI Energy dataset regression to test performance on small-scale problems with long training
  3. Fashion-MNIST classification to evaluate generalization and hyperparameter robustness on moderate-scale problems

## Open Questions the Paper Calls Out

### Open Question 1
What is the relative contribution of curvature information versus heuristics (like damping and learning rate selection) to the success of second-order optimizers like K-FAC?
The paper presents experiments comparing AdamQLR to Adam and K-FAC, but does not directly isolate the effects of the curvature information and heuristics. Experiments comparing AdamQLR with and without curvature information, or comparing K-FAC with and without its heuristics, would help isolate the effects of each component.

### Open Question 2
How does the choice of curvature matrix (Hessian vs. Fisher) impact the performance of AdamQLR?
The experiments show that the Fisher matrix performs slightly better on Fashion-MNIST and similarly on CIFAR-10, but the differences are not dramatic. Further investigation is needed to understand the circumstances under which each matrix is preferable.

### Open Question 3
How does the performance of AdamQLR scale with the size and complexity of the model and dataset?
The authors present experiments on a range of tasks, from simple functions to image classification, but do not extensively explore the scaling behavior of AdamQLR. Experiments on increasingly large and complex models and datasets, including those with billions of parameters, would help determine the scalability of AdamQLR.

### Open Question 4
What are the limitations of AdamQLR and in what scenarios might it underperform compared to other optimizers?
The authors acknowledge that AdamQLR has limitations and may not perform well in all scenarios, citing Adam's known limitations and the non-convexity of the optimization landscape. Systematic experiments comparing AdamQLR to other optimizers on a wide range of tasks, including those known to be challenging for Adam, would help identify its limitations.

## Limitations
- Computational overhead of curvature-vector products may offset benefits on very large-scale problems
- Claims about hyperparameter robustness need more extensive validation across diverse architectures
- Comparison with K-FAC could be more comprehensive regarding wall-clock time and memory usage

## Confidence
- High confidence: AdamQLR's ability to improve generalization performance compared to vanilla Adam
- Medium confidence: Claims about robustness to hyperparameter choices
- Low confidence: Computational efficiency claims compared to first-order methods

## Next Checks
1. Implement AdamQLR on a transformer-based architecture to verify performance beyond convolutional networks
2. Conduct wall-clock time measurements to quantify the computational overhead of curvature-vector products
3. Test AdamQLR with mixed-precision training to assess whether performance gains persist under memory constraints