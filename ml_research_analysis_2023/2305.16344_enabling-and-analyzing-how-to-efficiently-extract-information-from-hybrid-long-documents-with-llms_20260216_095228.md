---
ver: rpa2
title: Enabling and Analyzing How to Efficiently Extract Information from Hybrid Long
  Documents with LLMs
arxiv_id: '2305.16344'
source_url: https://arxiv.org/abs/2305.16344
tags:
- financial
- information
- llms
- revenue
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of extracting key performance
  indicators (KPIs) from financial reports, which are hybrid documents combining text
  and tables. The authors propose the Automated Financial Information Extraction (AFIE)
  framework, which uses large language models (LLMs) to segment, retrieve, summarize,
  and extract numerical values from these documents.
---

# Enabling and Analyzing How to Efficiently Extract Information from Hybrid Long Documents with LLMs

## Quick Facts
- arXiv ID: 2305.16344
- Source URL: https://arxiv.org/abs/2305.16344
- Reference count: 40
- The paper proposes the AFIE framework to improve numerical extraction from financial reports using LLMs, achieving significant accuracy improvements.

## Executive Summary
This paper addresses the challenge of extracting key performance indicators (KPIs) from financial reports, which combine text and tables. The authors introduce the Automated Financial Information Extraction (AFIE) framework that segments long documents, retrieves relevant content, summarizes it, and extracts numerical values using large language models. They also create a new dataset called FINE with 3,625 examples from 18 companies' financial reports. Experimental results show that AFIE significantly improves extraction accuracy compared to naive approaches, with average accuracy increases of 53.94% for GPT-3.5 and 33.77% for GPT-4.

## Method Summary
The AFIE framework processes hybrid long documents through four main modules: segmentation, retrieval, summarization, and extraction. Documents are first divided into smaller segments using the PLAIN table serialization approach to handle both text and tabular data. For each keyword, the system retrieves the top-K most relevant segments using embedding similarity with the sentence-transformers/all-mpnet-base-v2 model. The retrieved segments are then summarized using GPT models, with experiments comparing Direct and Refine summarization strategies. Finally, numerical values are extracted using precision-enhancing prompts that include few-shot examples to ensure proper formatting and unit handling.

## Key Results
- AFIE achieves 53.94% average accuracy improvement over naive methods for GPT-3.5 and 33.77% for GPT-4
- Retrieval accuracy increases as the number of retrieved segments increases from 1 to 3
- Direct summarization strategy outperforms Refine strategy in most experimental settings
- The framework demonstrates robustness to keyword ambiguity through metadata-based completion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with numerical precision in financial values, but targeted prompt engineering can correct this.
- Mechanism: Adding explicit rounding instructions and few-shot examples in prompts forces the model to output values in the desired format (millions, 3 decimal places).
- Core assumption: LLMs respond predictably to formatting instructions in the prompt context.
- Evidence anchors:
  - [section] "In handling numerical values, LLMs tend to struggle with accurately preserving the precision of the values."
  - [section] "We designed and used two precision-enhancing prompts simultaneously: Direct and Shot-Precision."
  - [corpus] Weak: no direct corpus anchor for numerical precision handling; inferred from general LLM behavior.
- Break condition: If the model ignores formatting instructions or outputs values in different units (billions instead of millions), the mechanism fails.

### Mechanism 2
- Claim: Keyword ambiguity in financial reports reduces extraction accuracy unless additional context is provided.
- Mechanism: Completing keywords with company names, time periods, and attributes disambiguates the target values and improves retrieval/summarization precision.
- Core assumption: LLMs can use structured metadata to disambiguate between multiple entities sharing the same keyword label.
- Evidence anchors:
  - [section] "In financial reports, the same keyword might correspond to multiple entities (such as different subsidiaries or time periods)."
  - [section] "We introduce a keyword completion method... using the document's metadata to complete the user's incomplete keywords."
  - [corpus] Weak: no corpus anchor for keyword disambiguation experiments; inferred from experimental results.
- Break condition: If the model still returns incorrect values when keywords are ambiguous, the mechanism fails.

### Mechanism 3
- Claim: Segment retrieval with embedding similarity improves extraction accuracy by focusing on relevant text.
- Mechanism: SentenceTransformer embeddings compute similarity between document segments and keywords, retrieving only the top-K most relevant segments for further processing.
- Core assumption: Embedding similarity correlates with semantic relevance for the extraction task.
- Evidence anchors:
  - [section] "We calculate the similarity between each document segment and the keyword based on their embeddings and retrieve the top-ranked segments with the highest similarity scores."
  - [section] "Experimental results... show that accuracy increases as the retrieval quantity goes from 1 to 3."
  - [corpus] Weak: no corpus anchor for embedding retrieval methodology; inferred from general IR literature.
- Break condition: If retrieval retrieves irrelevant segments or misses relevant ones, accuracy drops.

## Foundational Learning

- Concept: Long document segmentation
  - Why needed here: Financial reports exceed LLM token limits, so documents must be split into manageable segments.
  - Quick check question: What is the maximum token length supported by the LLM model being used?

- Concept: Keyword completion with metadata
  - Why needed here: Financial reports often use ambiguous terms; metadata helps disambiguate.
  - Quick check question: What metadata fields (company, time, attribute) are available for keyword completion?

- Concept: Few-shot learning in prompts
  - Why needed here: LLMs require examples to understand formatting and extraction tasks.
  - Quick check question: How many examples are included in the prompt, and what task are they demonstrating?

## Architecture Onboarding

- Component map: Segmentation -> Retrieval -> Summarization -> Extraction
- Critical path: Document -> Segments -> Relevant Segments -> Summary -> Extracted Value
- Design tradeoffs: Larger retrieval quantities increase recall but may introduce noise; more precise prompts improve accuracy but require more examples.
- Failure signatures: Low accuracy may indicate segmentation errors, retrieval failures, or prompt engineering issues.
- First 3 experiments:
  1. Test segmentation on a long document to verify token limits are respected.
  2. Test retrieval with a known keyword to verify relevant segments are retrieved.
  3. Test extraction with a known summary to verify numerical values are extracted correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the AFIE framework change when applied to other types of hybrid documents beyond financial reports, such as scientific papers or medical reports?
- Basis in paper: [inferred] The paper discusses the potential for extending the AFIE framework to other domains but does not provide experimental results for such extensions.
- Why unresolved: The current study focuses solely on financial reports, and the effectiveness of the framework on other types of hybrid documents remains untested.
- What evidence would resolve it: Conducting experiments applying the AFIE framework to scientific papers or medical reports and comparing the results with those obtained from financial reports would provide insights into its generalizability and performance across different domains.

### Open Question 2
- Question: What are the limitations of the current segmentation approach in handling extremely long documents, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper mentions that the current segmentation approach divides documents into smaller segments that LLMs can handle, but it does not discuss potential limitations or improvements for extremely long documents.
- Why unresolved: The paper does not provide information on how the framework would perform with documents significantly longer than those in the FINE dataset or how it might handle documents with complex structures that could challenge the current segmentation approach.
- What evidence would resolve it: Analyzing the performance of the AFIE framework on documents with token counts far exceeding the current dataset and exploring alternative segmentation strategies for handling complex document structures would help identify and address potential limitations.

### Open Question 3
- Question: How does the choice of the embedding model for the retrieval module affect the overall performance of the AFIE framework?
- Basis in paper: [explicit] The paper uses the sentence-transformers/all-mpnet-base-v2 model for computing embeddings but does not explore the impact of using different embedding models on the framework's performance.
- Why unresolved: The paper does not provide a comparison of the performance of the AFIE framework when using different embedding models, leaving the optimal choice of the embedding model unclear.
- What evidence would resolve it: Conducting experiments with various embedding models, such as different sentence transformers or other pre-trained models, and comparing their impact on the retrieval module's performance and the overall effectiveness of the AFIE framework would help determine the optimal choice of the embedding model.

## Limitations

- Dataset represents only 18 companies and may not capture full diversity of financial reporting practices
- Significant performance variance between GPT-3.5 (53.94% improvement) and GPT-4 (33.77% improvement) suggests framework effectiveness varies with model capability
- Fixed-token segmentation may fragment related information and impact retrieval accuracy

## Confidence

- High Confidence: The AFIE framework architecture (segmentation → retrieval → summarization → extraction) is technically sound and the relative improvements over baseline methods are well-documented through controlled experiments.
- Medium Confidence: The numerical precision handling through prompt engineering is effective but may be sensitive to specific model versions and prompt formulations, as evidenced by the performance gap between GPT-3.5 and GPT-4.
- Low Confidence: The generalizability of keyword completion methodology across different financial domains and reporting styles remains uncertain, as the evaluation focused on a specific set of KPIs and company types.

## Next Checks

1. Apply AFIE to financial reports from companies outside the original 18 to assess performance on unfamiliar reporting styles and industries.

2. Systematically vary prompt templates and few-shot examples to determine the sensitivity of extraction accuracy to prompt formulation.

3. Evaluate whether semantic segmentation approaches outperform fixed-token segmentation in maintaining contextual integrity for downstream extraction tasks.