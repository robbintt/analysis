---
ver: rpa2
title: Exploring and Interacting with the Set of Good Sparse Generalized Additive
  Models
arxiv_id: '2303.16047'
source_url: https://arxiv.org/abs/2303.16047
tags:
- rashomon
- shape
- function
- figure
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to approximate the Rashomon set
  for sparse generalized additive models (GAMs) using ellipsoids, enabling exploration
  of diverse, near-optimal models. The authors optimize ellipsoid parameters to maximize
  volume while ensuring coverage of the true Rashomon set, and extend this to efficiently
  handle multiple support sets.
---

# Exploring and Interacting with the Set of Good Sparse Generalized Additive Models

## Quick Facts
- arXiv ID: 2303.16047
- Source URL: https://arxiv.org/abs/2303.16047
- Reference count: 40
- This paper introduces a method to approximate the Rashomon set for sparse generalized additive models (GAMs) using ellipsoids, enabling exploration of diverse, near-optimal models.

## Executive Summary
This paper presents a novel approach to approximate the Rashomon set of sparse generalized additive models (GAMs) using ellipsoids, enabling exploration and interaction with the space of near-optimal models. The authors develop methods to optimize ellipsoid parameters for maximum volume while maintaining coverage of the true Rashomon set, and extend this to efficiently handle multiple support sets through a blocking method. The approach is demonstrated through applications including variable importance analysis, monotonicity constraint imposition, and user-driven shape function editing, showing high precision in approximating the Rashomon set while maintaining strong model performance.

## Method Summary
The method approximates the Rashomon set of sparse GAMs using ellipsoids, leveraging the convexity of the loss function for fixed support sets. It involves optimizing ellipsoid parameters via gradient descent to maximize volume while ensuring coverage of the true set, and extends this to handle multiple support sets through a blocking method that exploits ellipsoid intersection properties. The approach enables practical GAM editing by projecting user modifications back into the set via quadratic programming, allowing domain experts to interact with and customize models while maintaining performance guarantees.

## Key Results
- High precision in approximating the Rashomon set, with models sampled from the set maintaining strong out-of-sample performance
- Efficient handling of multiple support sets through the blocking method, enabling exploration of diverse model configurations
- Successful application of the Rashomon set for variable importance analysis, monotonicity constraint imposition, and user-driven shape function editing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ellipsoid approximation accurately captures the Rashomon set because the GAM loss is convex for fixed support sets, allowing quadratic approximation around the empirical risk minimizer.
- Mechanism: The loss function is approximated via Taylor expansion around the optimal weights ω*, yielding a quadratic form. This quadratic form defines an ellipsoid inscribed within the true Rashomon set. The ellipsoid is then optimized via gradient descent to maximize volume while maintaining coverage of the true set.
- Core assumption: The higher-order terms in the Taylor expansion are negligible and the Rashomon set is convex for fixed support sets.
- Evidence anchors:
  - [abstract]: "Because the Rashomon set of sparse GAMs is continuous and has no analytical form, we present methods to approximate it, knowing that it is convex for each support set."
  - [section 3.1]: "For a GAM with fixed support set, the Ls term is fixed and the problem is equivalent to logistic regression with a weighted ℓ2 penalty. For simplicity, in the remaining paper, ω also includes ω0. In this case, the loss L(ω) is a convex function, and the Rashomon set R(θ) is a convex set."
  - [corpus]: Weak evidence. No directly comparable method for convex GAM Rashomon approximation in neighbors.
- Break condition: If the higher-order terms are non-negligible, the ellipsoid approximation will exclude parts of the true Rashomon set, reducing precision.

### Mechanism 2
- Claim: The blocking method efficiently approximates Rashomon sets for multiple support sets by leveraging ellipsoid intersection properties.
- Mechanism: When bins are merged to form a smaller support set, the constraint that merged bins have equal coefficients defines a hyperplane. The intersection of this hyperplane with the original ellipsoid remains an ellipsoid, which can be computed analytically. This allows rapid approximation of many smaller Rashomon sets.
- Core assumption: Merging bins corresponds to a linear constraint (hyperplane) in weight space, and ellipsoid-hyperplane intersection preserves the ellipsoidal form.
- Evidence anchors:
  - [section 3.2]: "The hyperplane's intersection with any ellipsoid, e.g., ˆR, is still an ellipsoid, which can be calculated analytically."
  - [section 5.1]: "The approximation of the ellipsoid for a smaller support set is more accurate when the size difference between the original support set and the new support set is smaller."
  - [corpus]: Weak evidence. No directly comparable hyperplane-ellipsoid intersection method in neighbors.
- Break condition: If the merged support set is too small relative to the original, the approximation loses precision due to increased approximation error.

### Mechanism 3
- Claim: The Rashomon set enables practical GAM editing by projecting user modifications back into the set via quadratic programming.
- Mechanism: Users can specify desired shape function modifications. If the requested model lies outside the Rashomon set, it is projected back by solving a quadratic program that minimizes the distance to the requested model while ensuring the solution remains within the ellipsoid approximation of the Rashomon set.
- Core assumption: The ellipsoid approximation is sufficiently accurate that projecting onto it yields a model with performance close to the requested modification.
- Evidence anchors:
  - [section 4.4]: "We thus provide a formulation that projects back into the Rashomon set, producing a model within the Rashomon set that follows the users' preferences as closely as possible."
  - [section 5.3]: "The approximated Rashomon set can thus provide a computationally efficient way for users to find models that agree with their expectations, and the data."
  - [corpus]: Weak evidence. No directly comparable user editing via Rashomon projection in neighbors.
- Break condition: If the ellipsoid approximation poorly covers the true Rashomon set, the projected model may deviate significantly from the user's intent while still being suboptimal.

## Foundational Learning

- Concept: Convex optimization and properties of convex sets.
  - Why needed here: The core approximation relies on the convexity of the loss function and Rashomon set for fixed support sets, enabling ellipsoid representation and efficient optimization.
  - Quick check question: Given a convex loss function and a convex constraint set, what is the shape of the level sets of the loss function?
- Concept: Ellipsoid geometry and linear transformations.
  - Why needed here: The blocking method depends on understanding how ellipsoids intersect with hyperplanes and how to compute the resulting ellipsoid analytically.
  - Quick check question: If an ellipsoid is defined by x^T Q x ≤ 1 and a hyperplane is defined by a^T x = b, what is the equation of the resulting ellipsoid after intersection?
- Concept: Mixed-integer programming and linear programming formulations.
  - Why needed here: Variable importance bounds require solving optimization problems with absolute value objectives, which are reformulated as LP or MIP problems.
  - Quick check question: How can you reformulate an optimization problem with an absolute value objective as a linear program?

## Architecture Onboarding

- Component map: Core ellipsoid approximation module -> Blocking method for multiple support sets -> QP interface for constraints
- Critical path:
  1. Train initial GAM with FastSparse to obtain support set and empirical risk minimizer ω*
  2. Initialize ellipsoid parameters (Q, ωc) via Hessian approximation
  3. Optimize ellipsoid volume while maintaining Rashomon coverage via gradient descent
  4. For multiple support sets, apply blocking method to compute intersections
  5. Expose interfaces for variable importance, monotonicity constraints, and user editing via QP
- Design tradeoffs:
  - Accuracy vs. speed: Method 1 (gradient optimization) is more accurate but slower than Method 2 (blocking)
  - Precision vs. recall: Larger ellipsoids increase recall but may decrease precision
  - Complexity vs. usability: Full QP formulation for user edits is more flexible but computationally heavier than simple constraints
- Failure signatures:
  - Low precision: Ellipsoid poorly inscribed in true Rashomon set (e.g., higher-order terms significant)
  - Empty intersections: Blocking method fails when merged support set is too small
  - Slow optimization: Gradient descent not converging due to poor initialization or learning rate
- First 3 experiments:
  1. Verify ellipsoid initialization matches Hessian approximation on a small synthetic dataset
  2. Test blocking method precision/volume ratios for various support set size reductions
  3. Validate QP-based user editing preserves test performance on a held-out set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the precision of the Rashomon set approximation scale with increasing dimensionality of the feature space?
- Basis in paper: [inferred] The paper discusses the approximation of the Rashomon set using ellipsoids and mentions that the volume of the approximated set is proportional to det(Q)^(-1/2), but does not explicitly address how precision scales with dimensionality.
- Why unresolved: The paper does not provide experiments or theoretical analysis on the scalability of the approximation method with respect to the number of features.
- What evidence would resolve it: Empirical results showing precision and volume of the approximated Rashomon set across datasets with varying numbers of features, or theoretical bounds on approximation quality as a function of dimensionality.

### Open Question 2
- Question: What is the computational complexity of finding the Rashomon set for GAMs with non-linear link functions or non-convex penalties?
- Basis in paper: [inferred] The paper focuses on logistic loss and ℓ2/ℓ0 penalties, which lead to convex optimization problems. It does not discuss cases with non-linear link functions or non-convex penalties.
- Why unresolved: The paper does not explore the extension of their method to more complex loss functions or penalty terms that could lead to non-convex optimization problems.
- What evidence would resolve it: Analysis of the optimization landscape and computational complexity for different combinations of link functions and penalties, along with experimental results on approximation quality and runtime.

### Open Question 3
- Question: How does the choice of binning strategy for continuous features affect the size and structure of the Rashomon set?
- Basis in paper: [explicit] The paper mentions that continuous features are divided into bins and that the Rashomon set is computed for a fixed support set of bins, but does not investigate the impact of different binning strategies.
- Why unresolved: The paper does not provide experiments or theoretical analysis on how different binning approaches (e.g., equal-width vs. equal-frequency bins) influence the Rashomon set.
- What evidence would resolve it: Empirical comparison of Rashomon sets obtained using different binning strategies on the same datasets, along with analysis of how binning affects variable importance ranges and model diversity within the set.

## Limitations
- The method's precision depends critically on the assumption that the Rashomon set is convex for fixed support sets and that higher-order terms in the Taylor expansion are negligible.
- The blocking method's accuracy degrades when the merged support set is much smaller than the original.
- The ellipsoid approximation quality is dataset-dependent and may not generalize well to highly non-linear relationships or datasets with many irrelevant features.

## Confidence
- High confidence: The core mechanism of using ellipsoid approximation for fixed support sets (Mechanism 1) due to the convexity proof provided and well-established properties of convex optimization.
- Medium confidence: The blocking method for multiple support sets (Mechanism 2) due to the analytical ellipsoid-hyperplane intersection but lack of empirical validation for edge cases.
- Medium confidence: The QP-based user editing interface (Mechanism 3) due to the straightforward projection formulation but limited testing on diverse user requirements.

## Next Checks
1. **Precision Degradation Analysis**: Systematically evaluate how ellipsoid precision degrades as the ratio between original and merged support set sizes increases, identifying the threshold where the approximation becomes unreliable.
2. **Higher-Order Term Sensitivity**: Quantify the impact of non-negligible higher-order terms in the Taylor expansion by comparing the ellipsoid approximation against a ground truth Rashomon set computed via exhaustive sampling for small problems.
3. **Cross-Dataset Robustness**: Test the method on datasets with varying degrees of non-linearity and feature relevance distributions to establish the conditions under which the ellipsoid approximation maintains high precision.