---
ver: rpa2
title: 'Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large
  Language Models in Computational Social Science'
arxiv_id: '2305.14310'
source_url: https://arxiv.org/abs/2305.14310
tags:
- llms
- task
- bragging
- zero-shot
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates zero-shot performance of ChatGPT and OpenAssistant\
  \ on six computational social science classification tasks. Different prompting\
  \ strategies\u2014including adding label definitions, using synonyms for labels,\
  \ and incorporating paper titles\u2014were tested."
---

# Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science

## Quick Facts
- arXiv ID: 2305.14310
- Source URL: https://arxiv.org/abs/2305.14310
- Reference count: 7
- Key outcome: Zero-shot LLMs (ChatGPT, OpenAssistant) underperform fine-tuned BERT models on computational social science classification tasks, with prompting strategies causing >10% performance variation.

## Executive Summary
This study evaluates zero-shot classification performance of ChatGPT and OpenAssistant across six computational social science tasks. The research systematically tests three prompting strategies—basic instructions, task/label descriptions, and memory recall—finding that prompt complexity does not consistently improve accuracy. Results show that both LLMs perform worse than fine-tuned baseline transformer models like BERT, with accuracy and F1 score variations exceeding 10% across different prompting approaches. The study highlights that while zero-shot learning offers advantages in speed and cost, it cannot yet match the performance of task-specific fine-tuned models for structured classification tasks.

## Method Summary
The study evaluates zero-shot performance using three prompting strategies (Basic Instruction, Task/Label Description, Memory Recall) across six English computational social science datasets. For each task, prompts were generated for both ChatGPT (via OpenAI API with temperature 0.2) and OpenAssistant (locally with specific hyperparameters). Results were compared against supervised baselines including Logistic Regression (TF-IDF features) and BERT-large fine-tuned with learning rate 2e-5, batch size 16, max length 256. The evaluation measured accuracy and macro-F1 scores across all prompt variants.

## Key Results
- Zero-shot LLMs underperform fine-tuned BERT models on all six computational social science tasks
- Prompt complexity variations cause accuracy and F1 score differences exceeding 10%
- No single prompting strategy consistently improves performance across all tasks
- Synonym-based prompts show model-specific effects, improving OpenAssistant performance but yielding inconsistent results for ChatGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding label definitions and task descriptions to prompts does not consistently improve zero-shot classification accuracy.
- Mechanism: Additional information increases prompt complexity, potentially introducing noise and confusion for the LLM.
- Core assumption: More complex prompts do not necessarily provide clearer guidance for classification tasks.
- Evidence anchors: Abstract mentions prompting strategies significantly affect accuracy with >10% variations; section reports complex prompts may introduce noise.

### Mechanism 2
- Claim: Using synonyms for class labels can improve zero-shot classification performance for some models and tasks.
- Mechanism: Synonyms may align better with language patterns the LLM encountered during training.
- Core assumption: LLM performance is sensitive to specific wording of labels due to instruction tuning.
- Evidence anchors: Abstract notes prompting strategies affect classification accuracy; section shows synonym revisions substantially improve OpenAssistant performance.

### Mechanism 3
- Claim: Zero-shot LLMs generally underperform compared to fine-tuned transformer models like BERT on evaluated tasks.
- Mechanism: Fine-tuned models are optimized using labeled data, while zero-shot LLMs rely solely on pre-training without task-specific adaptation.
- Core assumption: Task-specific fine-tuning provides performance advantages that zero-shot prompting cannot match.
- Evidence anchors: Abstract states current LLMs cannot match fine-tuned models in zero-shot settings; section reports supervised baselines outperform LLMs on 4 of 6 tasks.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Study evaluates how well LLMs perform classification tasks without task-specific training data.
  - Quick check question: Can you explain the difference between zero-shot, few-shot, and fine-tuned learning?

- Concept: Prompt engineering
  - Why needed here: Different prompting strategies are tested to see how they affect model performance.
  - Quick check question: What are common techniques used in prompt engineering to improve model outputs?

- Concept: Macro-F1 score
  - Why needed here: Used as evaluation metric, especially for imbalanced datasets where accuracy might be misleading.
  - Quick check question: How does macro-F1 differ from micro-F1 and why is it more suitable for imbalanced data?

## Architecture Onboarding

- Component map: Data preparation -> Prompt generation -> Model inference -> Evaluation
- Critical path:
  1. Prepare datasets and split into training/test sets
  2. Generate prompts for each task and model
  3. Run zero-shot predictions using the LLMs
  4. Evaluate and compare results with baselines
- Design tradeoffs:
  - Zero-shot vs. fine-tuning: Faster/cheaper but generally less accurate
  - Prompt complexity: More detailed prompts can add clarity or introduce noise
  - Model choice: GPT-3.5-turbo more accurate but costly; OpenAssistant open-source but less accurate
- Failure signatures:
  - Low accuracy or F1 scores compared to baselines
  - Inconsistent performance across different prompt strategies
  - High costs or rate limits when using commercial APIs
- First 3 experiments:
  1. Run zero-shot predictions using basic prompts for all tasks and models
  2. Test impact of adding label definitions and task descriptions to prompts
  3. Experiment with synonyms for class labels to see if performance improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt complexities affect zero-shot classification performance across various computational social science tasks?
- Basis in paper: Explicit
- Why unresolved: Study found no single prompting strategy consistently improves outcomes, with task-specific variations exceeding 10% in accuracy and F1 scores.
- What evidence would resolve it: Comparative analysis of prompting strategies across multiple tasks with statistical significance testing.

### Open Question 2
- Question: Can replacing original labels with synonyms improve zero-shot classification performance for different LLMs?
- Basis in paper: Explicit
- Why unresolved: Study showed significant improvements for OpenAssistant but inconsistent results for GPT, suggesting model-specific dependencies.
- What evidence would resolve it: Systematic testing of synonym replacements across multiple LLMs and tasks with controlled vocabulary sets.

### Open Question 3
- Question: What is the impact of adding paper information (memory recall) on zero-shot classification accuracy?
- Basis in paper: Explicit
- Why unresolved: Results showed mixed performance, with some tasks improving while others degraded when paper titles were included.
- What evidence would resolve it: Controlled experiments varying amount and type of paper information in prompts.

## Limitations

- No single prompting strategy consistently improves LLM performance across all tasks
- Current LLMs cannot match fine-tuned transformer models like BERT on these classification tasks
- Results may not generalize beyond six English-language computational social science datasets

## Confidence

- High confidence: LLMs underperform fine-tuned models in zero-shot settings; prompting strategies significantly impact performance with >10% variation
- Medium confidence: No single prompt strategy consistently improves results; task-specific optimization remains necessary
- Medium confidence: Synonym-based prompts can improve performance for some models but may introduce semantic ambiguity

## Next Checks

1. Test stability of prompt strategy effects across multiple runs with different random seeds to quantify variance
2. Evaluate performance on additional computational social science tasks and non-English datasets to assess generalizability
3. Compare results using different LLMs (GPT-4, Claude, LLaMA-2) to determine if findings extend beyond tested models