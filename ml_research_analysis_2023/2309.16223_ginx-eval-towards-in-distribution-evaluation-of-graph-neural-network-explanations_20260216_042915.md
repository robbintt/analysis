---
ver: rpa2
title: 'GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations'
arxiv_id: '2309.16223'
source_url: https://arxiv.org/abs/2309.16223
tags:
- edges
- methods
- graph
- explanations
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GInX-Eval, a new evaluation framework for
  graph neural network explainability methods. The core problem addressed is the out-of-distribution
  issue in current faithfulness evaluation metrics, which can lead to misleading results
  when assessing explanation quality.
---

# GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations

## Quick Facts
- **arXiv ID**: 2309.16223
- **Source URL**: https://arxiv.org/abs/2309.16223
- **Reference count**: 35
- **Primary result**: Many popular GNN explanation methods perform no better than random edge selection when evaluated using in-distribution metrics

## Executive Summary
This paper introduces GInX-Eval, a new evaluation framework for graph neural network explainability methods that addresses the out-of-distribution problem in current faithfulness metrics. By retraining GNN models on modified datasets with removed edges, GInX-Eval measures how informative edges are to the model and whether explanatory edges are correctly ranked by importance. Through extensive experiments, the authors demonstrate that many popular explanation methods, including gradient-based approaches and Occlusion, perform no better than random edge selection, while methods like GNNExplainer, PGMExplainer, and most generative methods show superior performance.

## Method Summary
GInX-Eval evaluates GNN explanation methods by retraining models on modified datasets with removed edges rather than simply testing on altered graphs. The method proposes two key scores: GInX (measuring how informative removed edges are for the model) and EdgeRank (assessing if explanatory edges are correctly ordered by importance). For each explainer, explanatory edge masks are generated for all graph instances, top-t fraction of edges are removed using hard selection strategy, and the GNN model is retrained from scratch at different degradation levels t ∈ [0.1, 0.2, ..., 1]. The framework evaluates test accuracy after retraining to compute the informativeness of removed edges and their ranking quality.

## Key Results
- Gradient-based methods and Occlusion show the smallest GInX scores at optimal thresholds, performing no better than random edge selection
- GNNExplainer, PGMExplainer, and most generative methods demonstrate superior performance in capturing informative edges
- The EdgeRank score reveals that many explainers fail to correctly rank edges by their true importance to the model
- Results challenge previous findings in the field and provide new insights into the true informativeness of different explainability approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GInX-Eval overcomes the out-of-distribution (OOD) problem by retraining the GNN model on modified datasets with removed edges, rather than simply evaluating the model on the altered graph at inference time.
- Mechanism: When edges are removed from a graph, the resulting subgraph may lie outside the distribution of the training data. This OOD issue causes existing faithfulness metrics to conflate the loss of informative edges with the model's inability to handle OOD samples. GInX-Eval circumvents this by retraining the model from scratch on the degraded datasets at each edge removal threshold. This ensures that the model is adapted to the new graph structure and the observed performance degradation is due to the actual loss of informative edges, not distribution shift.
- Core assumption: Retraining the model on modified datasets is computationally feasible and that the retrained model's performance degradation accurately reflects the importance of the removed edges.
- Evidence anchors: [abstract]: "Using a fine-tuning strategy, the GInX score measures how informative removed edges are for the model..."; [section 3.3]: "GInX-Eval requires retraining the model on the modified dataset rather than re-scoring the modified graph at inference time like faithfulness evaluation."

### Mechanism 2
- Claim: GInX-Eval distinguishes between informative and uninformative edges by measuring the impact of removing the top-t ranked edges (according to an explainer) on the retrained model's accuracy.
- Mechanism: The GInX score quantifies the informativeness of the removed edges by computing the decrease in test accuracy after retraining on the degraded dataset. By comparing the GInX scores of different explainers at the optimal threshold (determined by the sparsity of ground-truth explanations), GInX-Eval can identify which methods capture the most informative edges for the model.
- Core assumption: The optimal threshold (based on ground-truth explanation sparsity) is a good reference point for comparing the informativeness of different explainers.
- Evidence anchors: [section 4.3.3]: "We propose to define the dataset's optimal threshold t* such as t* = minm∈M{tc m}, where M denotes the set of explainability methods."; [section 4.3.3]: "In figure 5, we observe that gradient-based methods and Occlusion have the smallest GInX scores at the optimal thresholds."

### Mechanism 3
- Claim: GInX-Eval evaluates the edge ranking capability of explainers by computing the EdgeRank score, which measures how well the explainer can order edges by their importance.
- Mechanism: The EdgeRank score is calculated by summing the GInX scores at different removal thresholds, weighted by the fraction of edges removed. This gives more weight to the ordering of edges at lower thresholds, where the difference between the most and least important edges is more pronounced.
- Core assumption: The EdgeRank score is a good proxy for the edge ranking capability of explainers, and that the weighting scheme (1-t) is appropriate for capturing the importance of ordering at different thresholds.
- Evidence anchors: [section 3.3.2]: "Based on the GInX score, we can compute the power of explainability methods to rank edges, i.e., to correctly order edges based on their importance."; [section 4.3.4]: "We use the EdgeRank score to evaluate the capacity of explainers to rank edges correctly according to their true informativeness for the model."

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GInX-Eval is designed to evaluate the explainability methods of GNNs, so a basic understanding of how GNNs work is necessary to understand the evaluation procedure and the results.
  - Quick check question: What is the main idea behind the message-passing mechanism in GNNs?

- Concept: Graph Explainability Methods
  - Why needed here: GInX-Eval evaluates various explainability methods for GNNs, so familiarity with different types of explainers (e.g., gradient-based, perturbation-based, generative) is crucial for understanding the comparison and the conclusions drawn.
  - Quick check question: What is the difference between generative and non-generative explainability methods?

- Concept: Faithfulness Evaluation Metrics
  - Why needed here: GInX-Eval is proposed as an alternative to faithfulness metrics, which are the most popular evaluation metrics for GNN explainability. Understanding the limitations of faithfulness metrics (e.g., OOD problem) is essential for appreciating the motivation behind GInX-Eval.
  - Quick check question: What is the main problem with faithfulness metrics, and how does GInX-Eval address it?

## Architecture Onboarding

- Component map: GNN model (GIN, GAT) -> Explainability method (GNNExplainer, PGMExplainer, etc.) -> Edge removal strategy (hard/soft) -> Retraining procedure -> GInX score calculation -> EdgeRank score calculation

- Critical path:
  1. Train the GNN model on the original dataset.
  2. Generate explanations for the test set using the explainer.
  3. Remove the top-t ranked edges from the graphs according to the explanations.
  4. Create new train and test sets with the degraded graphs.
  5. Retrain the GNN model from scratch on the new train set.
  6. Evaluate the retrained model on the new test set.
  7. Calculate the GInX score at threshold t.
  8. Repeat steps 3-7 for different values of t.
  9. Calculate the EdgeRank score based on the GInX scores.

- Design tradeoffs:
  - Hard vs. soft edge removal: Hard removal strictly removes edges from the graph, while soft removal sets edge weights to zero. Hard removal leads to a more significant performance drop but is more computationally expensive.
  - Optimal threshold selection: The optimal threshold is determined based on the sparsity of ground-truth explanations. If ground-truth explanations are not available, an arbitrary threshold must be chosen, which may not be optimal.
  - Retraining vs. no retraining: Retraining the model on degraded datasets is computationally expensive but overcomes the OOD problem. Not retraining is computationally cheaper but may lead to misleading results due to the OOD issue.

- Failure signatures:
  - If the retrained model's performance does not degrade significantly after removing edges, it may indicate that the explainer is not capturing informative edges or that the model is robust to edge removal.
  - If the EdgeRank score is low, it may indicate that the explainer is not good at ranking edges by their importance, even if it captures some informative edges.
  - If the GInX score is high for a random explainer, it may indicate that the ground-truth explanations are not informative to the model or that the model is not learning meaningful patterns from the graph structure.

- First 3 experiments:
  1. Verify that the GInX score of a random explainer is close to zero, indicating that removing random edges does not significantly impact the model's performance.
  2. Compare the GInX scores of different explainers at the optimal threshold to identify which methods capture the most informative edges for the model.
  3. Calculate the EdgeRank scores of different explainers to evaluate their edge ranking capability and identify the best methods for both informativeness and ranking power.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out any open questions.

## Limitations
- The primary limitation is the computational cost of retraining models at multiple edge removal thresholds, which may be prohibitive for large-scale graphs or complex GNN architectures.
- The framework relies on the assumption that ground-truth explanations accurately reflect the truly informative edges for the model, which may not always hold in practice.
- The evaluation is restricted to graph classification tasks and may not generalize to other GNN applications like node classification or link prediction.

## Confidence
- **High confidence**: The mechanism for addressing the out-of-distribution problem through retraining is well-justified and the experimental methodology is sound.
- **Medium confidence**: The EdgeRank score's effectiveness as a ranking metric, while theoretically motivated, requires further validation across diverse scenarios.
- **Medium confidence**: The conclusions about explanation method performance are robust within the tested datasets and architectures, but may vary with different GNN models or task types.

## Next Checks
1. Test GInX-Eval on node classification tasks to verify generalizability beyond graph classification.
2. Implement a sampling-based approximation of the retraining process to assess whether computational costs can be reduced while maintaining evaluation accuracy.
3. Compare GInX-Eval results with ablation studies that remove node features (not just edges) to validate that the framework captures true model informativeness rather than just structural dependencies.