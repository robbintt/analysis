---
ver: rpa2
title: Multitask Learning for Time Series Data with 2D Convolution
arxiv_id: '2310.03925'
source_url: https://arxiv.org/abs/2310.03925
tags:
- time
- series
- resnet2d
- distance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of applying multitask learning
  (MTL) to time series classification (TSC), where standard 1D convolutional models
  underperform. The authors find that 1D convolutions struggle to capture the warping
  mechanism essential for TSC, as exemplified by the DTW distance function.
---

# Multitask Learning for Time Series Data with 2D Convolution

## Quick Facts
- arXiv ID: 2310.03925
- Source URL: https://arxiv.org/abs/2310.03925
- Reference count: 40
- The paper proposes a 2D convolution-based ResNet2D model that outperforms 1D convolution methods for multitask time series classification by better capturing temporal warping mechanisms.

## Executive Summary
This paper addresses the challenge of applying multitask learning to time series classification, where standard 1D convolutional models underperform. The authors find that 1D convolutions struggle to capture the warping mechanism essential for TSC, as exemplified by the DTW distance function. To overcome this limitation, they propose a novel 2D convolution-based model (ResNet2D) that explicitly processes pairwise distance matrices and learns warping mechanisms more effectively. Experimental results on both UCR Archive and industrial transaction datasets show that ResNet2D with MTL significantly outperforms baseline methods, achieving superior accuracy across multiple tasks.

## Method Summary
The proposed method uses 2D convolutions on pairwise distance matrices computed from learnable templates to capture warping mechanisms in time series data. The ResNet2D architecture consists of 8 building blocks with 3×3 conv layers and global average pooling. For multitask learning, tasks are grouped by source (e.g., ECG, UWave) to enable meaningful knowledge sharing. The model is trained using mini-batch SGD with AdamW optimizer for 400 epochs on UCR datasets and 200 epochs on industrial data, with batch size 40 and learning rate 0.0001.

## Key Results
- ResNet2D with MTL outperforms ResNet1D with MTL and 1NN-DTW baselines on UCR Archive datasets
- The 2D convolution approach achieves superior accuracy across multiple tasks compared to 1D convolutions
- Template-based pairwise distance computation enables efficient 2D convolution processing while capturing warping mechanisms
- Task grouping by source improves MTL performance through shared warping pattern learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2D convolutions better capture warping mechanisms essential for TSC
- Mechanism: Pairwise distance matrices encode all possible warping paths between two time series. 2D convolutions can learn the dynamic programming recursion that computes DTW distance, while 1D convolutions cannot due to locality bias.
- Core assumption: The warping mechanism in time series classification is fundamentally a 2D spatial operation on distance matrices
- Evidence anchors:
  - [abstract] "By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers."
  - [section] "The inductive bias of 1D locality from the 1D convolution operation poses difficulties in effectively capturing both the pair-wise distance computation and the recursive function."
  - [corpus] Found 25 related papers; no direct evidence for this specific mechanism, but Time series classification with random convolution kernels suggests pooling operators matter for capturing temporal patterns
- Break condition: If pairwise distance matrices are not used as input or if warping is not relevant to the classification task

### Mechanism 2
- Claim: MTL with hard parameter sharing works when tasks share similar warping mechanisms
- Mechanism: Tasks grouped by source (e.g., ECG, UWave) share similar temporal warping patterns, allowing 2D convolution layers to learn general warping mechanisms that benefit all tasks
- Core assumption: Tasks from similar sources exhibit similar temporal distortions that can be captured by shared feature extractors
- Evidence anchors:
  - [section] "we create seven MTL datasets using the following steps. First, we select 25 datasets from the UCR archive [4] that share common sources with others. Subsequently, we organize the 25 tasks into seven distinct datasets, grouping them based on the source of the associated time series."
  - [section] "These groupings are importance for MTL, as they enable the meaningful sharing of knowledge among tasks within the same dataset."
  - [corpus] Weak evidence; no direct citations about warping mechanism sharing in MTL
- Break condition: If tasks are too heterogeneous or if warping is not a common feature across tasks

### Mechanism 3
- Claim: Template-based pairwise distance computation enables efficient 2D convolution
- Mechanism: Instead of computing pairwise distances between all training pairs, learnable templates create a tensor that can be processed by 2D convolutions to capture warping mechanisms
- Core assumption: A fixed set of learnable templates can approximate the pairwise distance structure needed for warping computation
- Evidence anchors:
  - [section] "To compute the pairwise distance matrix, we employ k learnable time series templates of length m. Therefore, if the input time series has a length of n, the output of the pairwise distance operation is a tensor with dimensions n×m×k."
  - [section] "In our design, we use 64 templates (i.e., k = 64), each with a length of 512 (i.e., m = 512)."
  - [corpus] No direct evidence; this appears to be a novel architectural choice
- Break condition: If template length is too short to capture relevant patterns or if too many templates cause overfitting

## Foundational Learning

- Concept: Dynamic Time Warping (DTW) distance computation
  - Why needed here: Understanding DTW is crucial because the paper's core claim is that 1D convolutions cannot learn the warping mechanism that DTW captures naturally
  - Quick check question: What are the two main steps in computing DTW distance between two time series?

- Concept: Siamese neural network architecture
  - Why needed here: The ResNet1D model is used in a Siamese configuration for distance learning tasks, which is essential for understanding the experimental setup
  - Quick check question: How does a Siamese network process two inputs to produce a distance output?

- Concept: Hard parameter sharing in multitask learning
  - Why needed here: The paper specifically uses hard parameter sharing as the MTL approach, making it essential to understand how shared layers and task-specific layers interact
  - Quick check question: In hard parameter sharing, which parts of the network are shared across tasks and which are task-specific?

## Architecture Onboarding

- Component map: Input time series → Pairwise distance computation (via templates) → ResNet2D feature extraction → Task-specific classification heads
- Critical path: Input → Pairwise distance computation (via templates) → ResNet2D feature extraction → Task-specific classification heads
- Design tradeoffs:
  - 2D vs 1D convolutions: 2D better captures warping but requires distance matrix input; 1D more direct but limited expressiveness
  - Template count/length: More templates capture more patterns but increase parameters; longer templates capture more temporal context but increase computation
  - Task grouping: Tighter task grouping improves sharing but reduces dataset diversity
- Failure signatures:
  - Poor performance on tasks with no temporal warping: ResNet2D may overfit to warping patterns not present in the data
  - Slow convergence: Template learning may be unstable if initialized poorly or if template length is inappropriate
  - High memory usage: Pairwise distance tensors and multiple templates can be memory-intensive for long time series
- First 3 experiments:
  1. DTW distance approximation: Train ResNet2D and ResNet1D to predict DTW distances on synthetic random walk data; compare errors
  2. Single-task baseline: Train ResNet2D on individual UCR tasks without MTL to establish performance ceiling
  3. Template sensitivity: Vary template count (16, 32, 64, 128) and length (256, 512, 1024) to find optimal configuration for a sample task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed ResNet2D model's ability to learn the warping mechanism directly translate to improved performance in other time series analysis tasks beyond classification?
- Basis in paper: [explicit] The paper states that the ResNet2D model demonstrates the capability to learn the warping mechanism from time series data, which is likely a key factor contributing to its superior performance in classification tasks.
- Why unresolved: The paper only demonstrates the effectiveness of ResNet2D in classification tasks. Other time series analysis tasks, such as anomaly detection or forecasting, are not explored.
- What evidence would resolve it: Empirical results showing the performance of ResNet2D on other time series analysis tasks compared to baseline methods.

### Open Question 2
- Question: How does the proposed ResNet2D model perform when dealing with multivariate time series data compared to univariate time series data?
- Basis in paper: [inferred] The paper mentions that the model is specifically designed for univariate time series data, and the experiments are conducted on univariate datasets.
- Why unresolved: The paper does not explore the performance of ResNet2D on multivariate time series data, which is a common scenario in many real-world applications.
- What evidence would resolve it: Empirical results showing the performance of ResNet2D on multivariate time series classification tasks compared to baseline methods.

### Open Question 3
- Question: What is the impact of the number of templates (k) used in the pairwise distance matrix computation on the performance of the ResNet2D model?
- Basis in paper: [explicit] The paper states that the ResNet2D model employs k learnable time series templates for computing the pairwise distance matrix, and k is set to 64 in the experiments.
- Why unresolved: The paper does not explore the sensitivity of the model's performance to the choice of k or provide insights into the optimal number of templates.
- What evidence would resolve it: A study analyzing the performance of ResNet2D with different values of k and identifying the optimal number of templates for various tasks.

## Limitations

- The effectiveness of 2D convolutions for capturing warping mechanisms relies on architectural design choices (template count and length) that appear somewhat arbitrary without ablation studies
- The task grouping strategy for MTL is based on dataset source rather than systematic analysis of task similarity
- Claims about why 1D convolutions fundamentally cannot capture warping are largely theoretical rather than empirically demonstrated

## Confidence

- **High confidence**: ResNet2D architecture implementation and training procedure are well-specified
- **Medium confidence**: Claims about 2D convolutions better capturing warping mechanisms (supported by comparison to DTW but lacking direct ablation studies)
- **Medium confidence**: MTL performance improvements (results show improvement but limited analysis of which tasks benefit most)
- **Low confidence**: Claims about task grouping strategy being optimal for MTL (appears heuristic rather than systematic)

## Next Checks

1. **Ablation study**: Test ResNet2D performance with varying template counts (16, 32, 64, 128) and lengths (256, 512, 1024) to identify optimal configuration and sensitivity
2. **Mechanism validation**: Train ResNet2D and ResNet1D on synthetic data with known warping patterns to directly measure their ability to capture and reproduce warping mechanisms
3. **Task grouping analysis**: Systematically evaluate MTL performance across different task grouping strategies (by source, by accuracy similarity, random grouping) to quantify the impact of grouping decisions