---
ver: rpa2
title: Improving Language Plasticity via Pretraining with Active Forgetting
arxiv_id: '2307.01163'
source_url: https://arxiv.org/abs/2307.01163
tags:
- forgetting
- language
- plms
- adaptation
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an active forgetting mechanism to enhance language
  plasticity in pretrained language models. By resetting token embeddings periodically
  during pretraining, the method encourages faster adaptation to new languages.
---

# Improving Language Plasticity via Pretraining with Active Forgetting

## Quick Facts
- arXiv ID: 2307.01163
- Source URL: https://arxiv.org/abs/2307.01163
- Authors: 
- Reference count: 40
- Primary result: Active forgetting during pretraining improves cross-lingual transfer, with +21.2% relative gains on XNLI, +33.8% on MLQA, and +60.9% on XQUAD

## Executive Summary
This paper introduces an active forgetting mechanism to enhance language plasticity in pretrained language models (PLMs). By periodically resetting token embeddings during pretraining, the method forces the transformer body to rely more on its internal representations rather than memorizing specific token embeddings. This creates models that adapt faster and more effectively to new languages, particularly in low-data regimes. Experiments show significant improvements across multiple cross-lingual benchmarks, with the greatest benefits for languages distant from English.

## Method Summary
The method involves periodically resetting the token embedding layer every K gradient updates during RoBERTa pretraining on English CC-100 corpus. The forgetting PLM is then adapted to new languages by finetuning only the token embeddings (5M tokens per language) while keeping the transformer body frozen. For task adaptation, the transformer body is finetuned on English task data while embeddings remain frozen. The adapted components are then assembled for evaluation on cross-lingual benchmarks. The key insight is that forcing the model to relearn embeddings periodically during pretraining creates a more robust transformer body that generalizes better to new token distributions.

## Key Results
- +21.2% average relative gains on XNLI zero-shot cross-lingual transfer
- +33.8% average relative gains on MLQA zero-shot cross-lingual transfer
- +60.9% average relative gains on XQUAD zero-shot cross-lingual transfer
- Faster convergence during language adaptation compared to standard PLMs
- Greater benefits for languages distant from English (different language family, script, or morphology)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Periodic embedding resets during pretraining improve the transformer body's ability to generalize across different token distributions, enabling faster adaptation to new languages.
- **Mechanism**: By resetting token embeddings every K updates, the model is forced to rely more on the transformer body for language understanding rather than memorizing specific token embeddings. This creates a more robust internal representation that transfers better to new languages.
- **Core assumption**: The transformer body can learn to handle various token distributions if regularly exposed to embedding resets during pretraining.
- **Evidence anchors**:
  - [abstract] "by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect"
  - [section 3] "Concretely, the forgetting mechanism operates by intentionally clearing the weights of the embedding layer, which stores the static representations for all tokens, and reinitializing them to a new set of random values every K gradient updates"
  - [corpus] Weak - no direct evidence found about transformer body adaptation
- **Break condition**: If the transformer body cannot effectively handle diverse token distributions or if the forgetting frequency is too high/too low to maintain useful representations.

### Mechanism 2
- **Claim**: Active forgetting during pretraining creates models that are more robust to different embedding initializations during language adaptation.
- **Mechanism**: Standard PLMs may encode "shortcut" knowledge tied to specific embedding initializations. By experiencing multiple embedding resets during pretraining, forgetting PLMs develop more universal knowledge within the transformer body that works across different embedding initializations.
- **Core assumption**: Models can develop more universal knowledge representations when forced to handle multiple embedding initializations during pretraining.
- **Evidence anchors**:
  - [section 4.2] "we hypothesize that this is because forgetting PLMs are more robust to different embedding initializations. They encode more universal knowledge within the transformer body, while the body of standard PLMs may encode more 'shortcut' knowledge that only applies to certain embedding initializations"
  - [corpus] Weak - no direct evidence found about robustness to embedding initializations
- **Break condition**: If the forgetting mechanism causes the transformer body to become unstable or if the universal knowledge development is insufficient to handle the new language's specific characteristics.

### Mechanism 3
- **Claim**: Languages distant from English benefit more from forgetting PLMs because the forgetting mechanism helps the transformer body become more sensitive to changes in input token distributions.
- **Mechanism**: The periodic embedding resets during pretraining make the transformer body more sensitive to changes in its input (the embedded tokens), allowing it to encourage larger updates in the embedding layer when encountering new languages during language adaptation.
- **Core assumption**: Sensitivity to input changes developed during pretraining translates to better adaptation performance for languages with different characteristics from English.
- **Evidence anchors**:
  - [section 4.4] "we observe that forgetting is particularly helpful for languages that are very different from the pretraining language (English) in terms of language family, script and morphology"
  - [section 3] "the active forgetting makes the body sensitive to changes in its input (the embedded tokens) so that the body can encourage larger updates in the embedding layer when meeting new languages during the language adapt stage"
  - [corpus] Weak - no direct evidence found about sensitivity to input changes
- **Break condition**: If the increased sensitivity to input changes becomes detrimental for languages with similar characteristics to English, or if the sensitivity doesn't translate to better adaptation performance.

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why forgetting is usually considered negative helps explain why active forgetting might be beneficial in this context
  - Quick check question: What is catastrophic forgetting and how does it typically affect neural network training?

- **Concept**: Cross-lingual transfer learning
  - Why needed here: The paper focuses on adapting models to new languages, so understanding how cross-lingual transfer works is essential
  - Quick check question: How does cross-lingual transfer typically work in language models, and what are its main challenges?

- **Concept**: Meta-learning and learning to learn
  - Why needed here: The paper mentions a "meta-learning effect" from the active forgetting mechanism, so understanding meta-learning concepts is important
  - Quick check question: How does meta-learning differ from standard learning approaches, and what role does forgetting play in some meta-learning methods?

## Architecture Onboarding

- **Component map**: RoBERTa-base (12-layer transformer) -> Token embedding layer (reset periodically) -> Transformer body (kept stable during forgetting) -> Optimizer states (reset along with embeddings) -> Learning rate scheduler (reset along with embeddings)

- **Critical path**:
  1. Pretrain RoBERTa on English CC100 with active forgetting
  2. Save checkpoint after 125K updates with K=1000 forgetting interval
  3. For language adaptation: freeze transformer body, finetune token embeddings on 5M tokens
  4. For task adaptation: freeze token embeddings, finetune transformer body on English task data
  5. Assemble adapted components for final model

- **Design tradeoffs**:
  - Forgetting frequency (K): Too frequent causes instability, too infrequent reduces benefits
  - Data quantity for adaptation: 5M tokens chosen to simulate low-resource scenarios
  - Checkpoint selection: Using best validation perplexity checkpoint rather than last checkpoint

- **Failure signatures**:
  - Poor performance on XNLI/MLQA/XQUAD compared to standard pretraining
  - Failure to converge during language adaptation phase
  - Significant performance degradation when adapting to languages similar to English

- **First 3 experiments**:
  1. Verify that standard pretraining with full adaptation data (all CC100 tokens) achieves ~66% accuracy on XNLI
  2. Test forgetting pretraining with K=1000 and verify improved performance on XNLI with 5M adaptation tokens
  3. Measure adaptation speed by comparing convergence curves for forgetting vs standard PLMs during first 5K updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why active forgetting during pretraining leads to better language plasticity?
- Basis in paper: [inferred] The paper mentions potential connections to meta-learning and the flatness of solutions in the loss landscape, but does not provide a definitive theoretical explanation.
- Why unresolved: The paper does not provide a concrete theoretical framework explaining why active forgetting during pretraining leads to improved language plasticity. It only suggests possible connections to meta-learning and loss landscape flatness.
- What evidence would resolve it: A formal theoretical analysis demonstrating how active forgetting during pretraining affects the model's ability to learn new languages, potentially through connections to meta-learning or the geometry of the loss landscape.

### Open Question 2
- Question: How does the frequency of active forgetting (K) affect the model's language plasticity?
- Basis in paper: [explicit] The paper uses K=1000 but does not explore the effects of different forgetting frequencies.
- Why unresolved: The paper does not investigate how varying the frequency of active forgetting impacts the model's language plasticity. It only uses a single value of K=1000.
- What evidence would resolve it: Experiments varying the value of K and measuring the resulting language plasticity on cross-lingual transfer benchmarks.

### Open Question 3
- Question: How does active forgetting during pretraining affect the model's ability to learn other types of adaptations beyond language, such as domain adaptation or task adaptation?
- Basis in paper: [inferred] The paper focuses on language adaptation but does not explore other types of adaptations that active forgetting might enable.
- Why unresolved: The paper only investigates the effects of active forgetting on language adaptation and does not explore its potential benefits for other types of adaptations.
- What evidence would resolve it: Experiments applying active forgetting to pretraining for domain adaptation or task adaptation and measuring the resulting performance improvements.

## Limitations

- The paper focuses exclusively on zero-shot cross-lingual transfer from English to other languages, limiting generalizability to other transfer scenarios
- The empirical claims about transformer body sensitivity and universal knowledge development lack direct supporting evidence
- Only one forgetting frequency (K=1000) is explored, leaving questions about optimal forgetting schedules

## Confidence

**High Confidence**: The empirical results showing improved performance on XNLI, MLQA, and XQUAD benchmarks are well-supported by the experimental data. The convergence speed improvements during language adaptation are clearly demonstrated through training curves. The finding that distant languages benefit more from forgetting PLMs is consistently observed across experiments.

**Medium Confidence**: The claim that forgetting PLMs develop more universal knowledge representations is plausible but not directly verified. The mechanism explanation relies on performance patterns rather than measuring actual representation changes or sensitivity to input variations. The relationship between forgetting frequency and performance gains remains unexplored.

**Low Confidence**: The specific claims about how the transformer body becomes more sensitive to input changes and how this translates to better adaptation performance lack direct supporting evidence. The paper does not provide measurements of internal representations or sensitivity analyses to validate these mechanistic claims.

## Next Checks

1. **Representation Analysis**: Conduct probing experiments to measure how the transformer body representations change during forgetting pretraining compared to standard pretraining. Use techniques like representation similarity analysis (RSA) or mutual information maximization to quantify whether the transformer body develops more universal representations that are less dependent on specific embedding initializations.

2. **Ablation Studies on Forgetting Frequency**: Systematically vary the forgetting interval K (e.g., 500, 1000, 2000, 4000 updates) and measure the impact on both pretraining stability and downstream adaptation performance. This would help identify the optimal forgetting schedule and determine whether the benefits scale with forgetting frequency or follow a more complex relationship.

3. **Sensitivity Analysis**: Design experiments to directly measure the transformer body's sensitivity to embedding changes by introducing controlled perturbations to token embeddings during both pretraining and adaptation phases. Compare the magnitude of updates in the transformer body between forgetting and standard PLMs when exposed to the same embedding perturbations, testing the hypothesis that forgetting PLMs develop greater sensitivity to input changes.