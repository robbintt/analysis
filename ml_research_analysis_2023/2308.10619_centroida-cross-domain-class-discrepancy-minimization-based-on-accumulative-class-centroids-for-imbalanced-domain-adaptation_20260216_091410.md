---
ver: rpa2
title: 'centroIDA: Cross-Domain Class Discrepancy Minimization Based on Accumulative
  Class-Centroids for Imbalanced Domain Adaptation'
arxiv_id: '2308.10619'
source_url: https://arxiv.org/abs/2308.10619
tags:
- domain
- label
- adaptation
- alignment
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of unsupervised domain adaptation
  (UDA) in the presence of both covariate shift and label shift, a scenario termed
  imbalanced domain adaptation (IDA). The proposed method, centroIDA, tackles IDA
  by focusing on reliable pseudo label selection and accurate feature distribution
  estimation for the target domain.
---

# centroIDA: Cross-Domain Class Discrepancy Minimization Based on Accumulative Class-Centroids for Imbalanced Domain Adaptation

## Quick Facts
- arXiv ID: 2308.10619
- Source URL: https://arxiv.org/abs/2308.10619
- Reference count: 8
- Primary result: Outperforms state-of-the-art UDA methods on Office-Home and DomainNet under severe label shift (p=0.05), achieving 60.7% per-class mean accuracy on Office-Home

## Executive Summary
The paper addresses unsupervised domain adaptation (UDA) in the presence of both covariate shift and label shift, a scenario termed imbalanced domain adaptation (IDA). The proposed method, centroIDA, tackles IDA by focusing on reliable pseudo label selection and accurate feature distribution estimation for the target domain. Key components include class-balanced re-sampling on the source domain for an unbiased classifier, accumulative class-centroids alignment to iteratively align source and target class centroids, and class-wise feature alignment to optimize feature representations and learn robust classification boundaries. Experiments on Office-Home and DomainNet datasets demonstrate that centroIDA outperforms state-of-the-art methods, particularly as the degree of label shift increases.

## Method Summary
centroIDA addresses imbalanced domain adaptation by combining class-balanced resampling on the source domain with accumulative class-centroids alignment and class-wise feature alignment. The method first applies class-balanced resampling to ensure an unbiased source classifier. It then iteratively updates accumulative class-centroids for both domains using weighted features and maximum classification probabilities. These centroids are used for alignment loss computation. Pseudo labels for the target domain are generated by nearest centroid distance, and class-wise feature alignment is performed based on intra-class compactness and inter-class separation. The total loss combines cross-entropy loss with weighted accumulative class-centroids alignment loss and class-wise feature alignment loss.

## Key Results
- Achieves 60.7% per-class mean accuracy on Office-Home with imbalanced ratio 0.05, surpassing other methods
- Demonstrates stronger resistance to label shifts compared to state-of-the-art UDA methods
- Shows consistent performance improvements across multiple adaptation scenarios on Office-Home and DomainNet datasets

## Why This Works (Mechanism)

### Mechanism 1
Accumulative class-centroids alignment provides more stable and accurate estimation of target domain class centroids compared to single-batch or prototype-based methods. Instead of computing centroids from scratch each batch or relying on a pre-trained source-only model, accumulative centroids are updated iteratively using both current batch features and previously learned centroids weighted by their reliability scores (P_max). The core assumption is that pseudo-label reliability (measured by P_max) is a good proxy for the quality of the feature and the correctness of the pseudo-label.

### Mechanism 2
Class-wise feature alignment reduces intra-class distances and increases inter-class distances simultaneously, creating a more discriminative feature space. The loss function uses dsame/ddiff ratio, where dsame is the average distance between same-class features across domains (weighted by instance reliability), and ddiff is the average distance between different-class features (also weighted). Minimizing this ratio pulls same-class features together and pushes different-class features apart. The core assumption is that the ratio form ensures both intra-class compactness and inter-class separation are optimized in a balanced way.

### Mechanism 3
Class-balanced resampling on the source domain ensures an unbiased classifier, which in turn produces more reliable pseudo-labels on the target domain. By resampling the source domain so each class has equal representation during training, the classifier learns to treat all classes equally, avoiding bias toward majority classes that would propagate to unreliable target pseudo-labels. The core assumption is that an unbiased source classifier is necessary to generate reliable target pseudo-labels, which are critical for the accumulative centroid and class-wise alignment steps.

## Foundational Learning

- Concept: Covariate shift vs. label shift
  - Why needed here: The paper explicitly addresses both shifts, so understanding their distinction is critical to grasp the problem formulation and why standard UDA methods fail.
  - Quick check question: In the formula p(y|x) = q(y|x) but p(x) ≠ q(x) and p(y) ≠ q(y), which terms represent covariate shift and which represent label shift?

- Concept: Prototype networks and centroid-based alignment
  - Why needed here: The paper contrasts its accumulative centroid method with prototype-based methods, so knowing how prototypes work is essential to appreciate the proposed improvement.
  - Quick check question: What is the key limitation of computing source and target prototypes from different models, as mentioned in the related work?

- Concept: Entropy-based weighting for instance reliability
  - Why needed here: The paper uses entropy of classifier outputs to weight instances in feature alignment, so understanding entropy as a measure of prediction confidence is necessary.
  - Quick check question: In the formula H(byi·) = -Σ bYij log bYij, what does a lower entropy value indicate about the classifier's prediction for instance xi?

## Architecture Onboarding

- Component map: Feature extractor (backbone + bottleneck) -> Classifier head -> Accumulative centroid tracker (source and target, per class) -> Class-balanced sampler (source only) -> Loss aggregator (CE + centroid alignment + class-wise alignment)

- Critical path: 
  1. Sample balanced batch from source, normal batch from target
  2. Forward pass to get features and predictions
  3. Update accumulative centroids using Eq. (3) and (4)
  4. Compute centroid alignment loss (Eq. 5)
  5. Correct target pseudo-labels using closest centroid
  6. Compute class-wise alignment loss (Eq. 9-11)
  7. Backpropagate total loss (Eq. 12-13)

- Design tradeoffs:
  - Centroid accumulation vs. prototype computation: Accumulation is more stable but requires tracking state across batches; prototypes are stateless but may be less accurate if computed from mismatched models.
  - Entropy weighting vs. hard pseudo-labels: Entropy weighting is smoother and less brittle but requires tuning temperature and weight scaling.

- Failure signatures:
  - If pseudo-labels are unreliable, class-wise alignment loss may reinforce wrong associations (monitor label accuracy drift).
  - If centroids are poorly estimated (e.g., due to class imbalance in sampling), alignment loss may dominate and destabilize training (monitor centroid variance).

- First 3 experiments:
  1. Ablation: Remove class-balanced resampling and observe drop in target minority class accuracy.
  2. Ablation: Replace accumulative centroids with single-batch centroids and compare convergence speed and final accuracy.
  3. Ablation: Remove entropy weighting from class-wise alignment and see if performance degrades, especially on hard samples.

## Open Questions the Paper Calls Out

### Open Question 1
How does centroIDA perform on extremely large-scale imbalanced domain adaptation tasks with hundreds or thousands of classes? The paper demonstrates centroIDA's effectiveness on datasets with moderate class numbers (Office-Home with 65 classes, DomainNet with 40 classes) and shows good performance even with high label shifts (p=0.02). The authors mention centroIDA has lower computational complexity than prototype-based methods, but do not test scalability to much larger class sets.

### Open Question 2
What is the theoretical upper bound for centroIDA's performance under extreme label shifts, and how does it compare to the optimal possible adaptation performance? While empirical results show centroIDA outperforms baselines across various label shift ratios, there is no theoretical analysis of how close centroIDA gets to the theoretically optimal performance under extreme label shifts.

### Open Question 3
How sensitive is centroIDA to the choice of temperature parameter T in the pseudo-label generation process, and what is the optimal strategy for selecting this hyperparameter? The paper mentions using temperature rescaling with T=2 for pseudo-label generation but does not explore sensitivity to this parameter or provide guidance on optimal selection strategies.

## Limitations

- The effectiveness of accumulative centroids hinges on pseudo-label reliability, but the paper does not validate this assumption under varying degrees of label shift severity.
- Class-wise feature alignment relies on entropy-weighted instance selection, yet no analysis is provided on how sensitive this weighting is to temperature hyperparameter T.
- The claim of robustness to increasing label shift is supported empirically, but the paper lacks theoretical justification for why accumulative centroids remain stable as imbalance increases.

## Confidence

- High: Class-balanced resampling improves source classifier fairness and target minority class accuracy.
- Medium: Accumulative centroids provide more stable alignment than single-batch or prototype methods, based on ablation evidence.
- Low: The class-wise alignment loss formulation (dsame/ddiff ratio) is uniquely beneficial without alternative formulations explored.

## Next Checks

1. **Ablation Study**: Remove class-balanced resampling and measure drop in minority class accuracy on Office-Home with p=0.05.
2. **Robustness Test**: Increase label shift severity (e.g., p=0.01) and evaluate if accumulative centroids still converge.
3. **Weighting Sensitivity**: Vary temperature T in entropy weighting and observe impact on class-wise alignment loss and final accuracy.