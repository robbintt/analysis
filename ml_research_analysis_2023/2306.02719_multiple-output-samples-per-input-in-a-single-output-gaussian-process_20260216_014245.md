---
ver: rpa2
title: Multiple output samples per input in a single-output Gaussian process
arxiv_id: '2306.02719'
source_url: https://arxiv.org/abs/2306.02719
tags:
- output
- training
- multiple
- uncertainty
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to generalise the Gaussian Process (GP) to
  allow for multiple output samples in the training set, making use of available output
  uncertainty information. The output density function is formulated as the joint
  likelihood of observing all output samples, and latent variables are not repeated
  to reduce computation cost.
---

# Multiple output samples per input in a single-output Gaussian process

## Quick Facts
- arXiv ID: 2306.02719
- Source URL: https://arxiv.org/abs/2306.02719
- Reference count: 0
- Primary result: Proposed method achieves PCC of 0.713, MSE of 1.136, and significant KL divergence improvement on speechocean762 dataset

## Executive Summary
This paper extends Gaussian Process regression to handle multiple output samples per input, specifically for subjective assessments with multiple human raters. The key innovation is formulating the output density as a joint likelihood of observing all output samples without repeating latent variables, enabling efficient computation while incorporating uncertainty information from multiple raters. The method is evaluated on spoken language assessment, showing improved distributional predictions compared to standard GP approaches.

## Method Summary
The method generalizes GP regression to multiple output samples by computing the joint likelihood of all outputs given a single set of latent variables. Instead of repeating inputs for each rater (which would require inverting an RN×RN kernel), it uses a factorized joint density formulation that maintains N×N kernel inversion complexity. The model optimizes hyperparameters by maximizing the joint marginal likelihood over all raters' outputs, then performs inference similarly to standard GP but with hyper-parameters tuned to better match the distribution of raters' scores.

## Key Results
- Pearson's Correlation Coefficient (PCC) of 0.713 on speechocean762 dataset
- Mean Squared Error (MSE) of 1.136 for test set predictions
- Significant improvement in Kullback-Leibler (KL) divergence metric compared to baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint likelihood formulation allows efficient use of multiple raters' scores without repeating latent variables
- Mechanism: Uses single latent variables and factorized joint density instead of repeating inputs for each rater, keeping kernel inversion at N×N
- Core assumption: Multiple outputs for same input are conditionally independent given latent variable
- Evidence anchors: Abstract states joint likelihood formulation reduces computation cost; section explains conditional independence implies diagonal covariance
- Break Condition: If conditional independence between raters fails, factorized joint likelihood may be misspecified

### Mechanism 2
- Claim: Optimizing hyper-parameters via joint marginal likelihood improves output distribution match to raters' uncertainty
- Mechanism: Maximizes joint marginal likelihood over all raters' outputs instead of single reference mean
- Core assumption: Empirical mean and standard deviation of raters' scores capture relevant uncertainty structure
- Evidence anchors: Abstract mentions difference in optimized hyper-parameters; section contrasts joint vs marginal log-likelihood optimization
- Break Condition: If raters' disagreement is random noise rather than meaningful uncertainty, incorporating it may hurt accuracy

### Mechanism 3
- Claim: Predictive mean depends on mean of multiple raters but not variance, while predictive covariance remains independent of training outputs
- Mechanism: Shared latent variables make predictive mean depend on rater mean but not variance; covariance depends only on input distances
- Core assumption: Predictive covariance should not depend on magnitude of disagreement between raters for nearby inputs
- Evidence anchors: Section states predictive mean is dependent on mean of multiple outputs but independent of η; covariance depends only on training inputs and hyper-parameters
- Break Condition: If predictive distribution should adapt based on training output spread, fixed predictive covariance is insufficient

## Foundational Learning

- Concept: Gaussian Process regression basics (prior over latent functions, kernel choice, conditioning on data)
  - Why needed here: Entire method builds on GP regression; understanding prior, likelihood, and posterior is essential to see why repeating latent variables is inefficient
  - Quick check question: In a standard GP, if you condition on observed outputs, what happens to the predictive distribution for a new input?

- Concept: Matrix inversion complexity and scaling in GPs
  - Why needed here: Paper's efficiency gain hinges on keeping kernel inversions at O(N³) instead of O(N³R³)
  - Quick check question: If a GP has N training points, what is the computational complexity of inverting the kernel matrix during training?

- Concept: Marginalization over functions vs. point predictions
  - Why needed here: Method's strength is in producing distributional outputs that reflect uncertainty
  - Quick check question: In a GP, what is the difference between predicting a single scalar and predicting a full predictive distribution?

## Architecture Onboarding

- Component map: Input features -> Bottleneck extractor (NN) -> GP input matrix X; Multiple raters' scores -> Stacked matrix Y_ref; Kernel function (squared exponential) -> Covariance K(X,X); Joint likelihood -> Optimization target for hyper-parameters; Predictive density -> Final output distribution per test input

- Critical path: 1. Extract bottleneck features from NN for each sentence; 2. Stack multiple raters' scores into Y; 3. Compute kernel K(X,X) and K(X, X_test); 4. Optimize hyper-parameters by maximizing joint marginal likelihood; 5. Perform inference using predictive equations to get output distribution

- Design tradeoffs: Using multiple raters' scores improves uncertainty modeling but assumes conditional independence between raters; avoiding latent variable repetition saves computation but restricts how inter-rater dependencies can be modeled; fixing predictive covariance to depend only on inputs simplifies inference but may miss heteroscedasticity

- Failure signatures: If KL divergence does not improve despite using multiple raters, independence assumption may be violated; if PCC/MSE degrade significantly, model may be overfitting to noisy rater disagreement; if inference time explodes, latent variable repetition may have been inadvertently reintroduced

- First 3 experiments: 1. Compare KL divergence of GP joint vs GP base on small synthetic dataset with known inter-rater correlation structure; 2. Vary number of raters R and measure computational savings of GP joint over GP repeat; 3. Test sensitivity of predictive covariance to changes in rater disagreement patterns by simulating different noise levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the predictive covariance of the GP change when incorporating information about the proximity between multiple output samples for the same input in the training set?
- Basis in paper: Inferred - paper mentions covariance is independent of training set outputs and only depends on distance between test and training set inputs
- Why unresolved: Paper does not explore methods to allow GP to take into account proximity between multiple output samples for same input when computing predictive covariance
- What evidence would resolve it: Experimental results comparing performance of proposed method with variant that incorporates proximity information between multiple output samples for same input

### Open Question 2
- Question: How would the proposed method perform on datasets with varying numbers of raters per input, as opposed to same number of raters for all inputs?
- Basis in paper: Explicit - paper mentions analogous formulation can be derived for varying numbers of raters but does not explore this scenario
- Why unresolved: Paper only evaluates method on dataset where each input has same number of raters
- What evidence would resolve it: Experimental results comparing performance on datasets with varying numbers of raters per input

### Open Question 3
- Question: How would the proposed method perform if multiple output samples for each input were not independent of each other, as assumed in paper?
- Basis in paper: Inferred - paper assumes multiple output samples for same input and across different inputs are independent of each other, given latent variable
- Why unresolved: Paper does not explore impact of relaxing this independence assumption
- What evidence would resolve it: Experimental results comparing performance with variant that does not assume independence between multiple output samples for each input

## Limitations
- Conditional independence assumption between raters may not hold in practice if raters share systematic biases
- Fixed predictive covariance structure may not capture heteroscedastic uncertainty patterns
- Generalisability to other subjective assessment tasks beyond spoken language assessment requires further validation

## Confidence
- High confidence: Computational efficiency argument is mathematically sound and well-supported by kernel inversion complexity analysis
- Medium confidence: Improvement in distributional metrics is demonstrated on specific dataset but generalisability needs validation
- Low confidence: Assumption that predictive covariance should remain independent of training output spread may not hold in all scenarios

## Next Checks
1. Conduct ablation studies on synthetic data with known inter-rater correlation structures to isolate effect of conditional independence assumption on KL divergence improvements
2. Test method's performance when varying number of raters (R) to understand trade-off between computational savings and distributional accuracy
3. Compare predictive performance against heteroscedastic GP variants to assess whether fixed predictive covariance structure is sufficient for capturing uncertainty in subjective assessment tasks