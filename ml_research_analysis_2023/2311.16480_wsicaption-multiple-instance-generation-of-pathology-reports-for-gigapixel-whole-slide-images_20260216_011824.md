---
ver: rpa2
title: 'WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel
  Whole-Slide Images'
arxiv_id: '2311.16480'
source_url: https://arxiv.org/abs/2311.16480
tags:
- image
- pathology
- reports
- pages
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a new pipeline to curate a large-scale WSI-text
  dataset (TCGA-PathoText) by automatically extracting and cleaning pathology reports
  from the Cancer Genome Atlas (TCGA). A novel multiple instance generation framework
  (MI-Gen) is proposed to generate pathology reports for gigapixel whole-slide images
  (WSIs).
---

# WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images

## Quick Facts
- arXiv ID: 2311.16480
- Source URL: https://arxiv.org/abs/2311.16480
- Reference count: 40
- Primary result: MI-Gen achieves 0.838 F1 score on breast cancer subtyping and competitive tumor classification performance

## Executive Summary
This work introduces a novel approach to generate pathology reports for gigapixel whole-slide images (WSIs) using a multiple instance generation framework (MI-Gen). The method addresses the challenge of extracting clinically relevant information from WSIs by incorporating a position-aware module into a transformer-based encoder-decoder architecture. The authors curate a large-scale WSI-text dataset (TCGA-PathoText) from the Cancer Genome Atlas by automatically extracting and cleaning pathology reports using OCR and LLM post-processing. Experimental results demonstrate that MI-Gen can generate pathology reports containing multiple clinical clues and achieve strong performance on downstream tasks like breast cancer subtyping and tumor classification.

## Method Summary
The method employs a transformer-based encoder-decoder architecture with frozen pre-trained visual extractors to generate pathology reports from WSIs. The model incorporates hierarchical position-aware modules (PAMs) after each transformer encoder layer to capture spatial morphological features. The visual extractor processes non-overlapping patches from the WSI, and the position-aware modules reshape token sequences into 2D spatial maps to aggregate heterogeneous spatial information. The model is trained on TCGA-PathoText, a dataset of approximately 10,000 WSI-text pairs extracted from TCGA pathology reports using OCR and LLM cleaning. After training, the model can generate complete pathology reports and transfer to downstream classification tasks like breast cancer subtyping and tumor classification.

## Key Results
- MI-Gen achieves 0.838 F1 score on breast cancer subtyping task
- Competitive performance on tumor classification with AUC scores of 0.885 (binary) and 0.750 (multi-class)
- Strong BLEU-1/2/3/4, METEOR, and ROUGE scores on generated report quality
- Position-aware modules and in-domain pre-training (HIPT) show significant improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical position-aware module (PAM) improves spatial awareness in WSI report generation.
- Mechanism: PAM inserts convolutional layers with varying kernel sizes after each transformer encoder layer, reshaping token sequences into 2D spatial maps to capture heterogeneous spatial information.
- Core assumption: Spatial morphological features are crucial for accurate pathology report content.
- Evidence anchors:
  - [abstract]: "We incorporate a position-aware module into a transformer-based encoder-decoder to improve spatial awareness."
  - [section]: "Morphological and spatial information plays a key role when pathologists are diagnosing WSIs...we incorporate a hierarchical position-aware module (PAM) into the encoding of image embeddings."
  - [corpus]: Found related papers on spatial awareness in WSIs, but no direct quantitative comparison of PAM vs no-PAM.
- Break condition: If WSIs lack consistent spatial structure or if convolutional layers fail to capture meaningful patterns.

### Mechanism 2
- Claim: Pre-training visual extractors on in-domain pathology data (TCGA) yields better embeddings than out-of-domain ImageNet.
- Mechanism: Domain-specific pre-training aligns visual features with medical domain semantics, improving downstream report generation quality.
- Core assumption: Visual features learned from natural images do not transfer well to histopathology domains.
- Evidence anchors:
  - [abstract]: "We observe that simple semantic extraction from the pathology reports can achieve the best performance (0.838 of F1 score) on BRCA subtyping."
  - [section]: "In terms of visual extractors, ResNet and ViT do not show a large difference when they are pre-trained on ImageNet. But ViT pre-trained with HIPT demonstrates a significant improvement."
  - [corpus]: Limited direct comparison evidence in corpus; assumption based on domain alignment literature.
- Break condition: If pre-training data lacks diversity or if visual patterns in pathology are similar to natural images.

### Mechanism 3
- Claim: Using WSI-text pairs as a pre-training strategy improves transfer to classification tasks.
- Mechanism: Generative modeling on WSI-text pairs implicitly learns rich feature representations that encode both visual and semantic information.
- Core assumption: Generating pathology reports requires capturing clinically relevant features that overlap with discriminative tasks.
- Evidence anchors:
  - [abstract]: "Furthermore, WSI-text prediction can be seen as an approach of visual-language pre-training, which enables our model to be transferred to downstream diagnostic tasks like carcinoma grading and phenotyping."
  - [section]: "After training on the WSI-text pairs, we evaluate the transfer ability of our model on two WSI-level classification tasks."
  - [corpus]: No direct corpus evidence comparing generative vs discriminative pre-training in WSIs.
- Break condition: If generated reports do not capture task-relevant information or if generative loss is misaligned with downstream objectives.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: WSIs are gigapixel images where only the entire slide has a label; MIL allows learning from instance-level patches.
  - Quick check question: What is the difference between bag labels and instance labels in MIL?
- Concept: Visual-Language Pre-training
  - Why needed here: Enables leveraging large-scale unpaired image-text data to improve model generalization to downstream tasks.
  - Quick check question: How does generative pre-training differ from discriminative pre-training in vision-language models?
- Concept: Optical Character Recognition (OCR) + LLM post-processing
  - Why needed here: Extracts and cleans noisy PDF pathology reports into structured text for pairing with WSIs.
  - Quick check question: What are the main sources of noise in OCR-extracted pathology text?

## Architecture Onboarding

- Component map:
  Visual Extractor (frozen pre-trained CNN/Transformer) → PAMs after each encoder layer → Transformer Encoder → Transformer Decoder → Text Output
- Critical path:
  Patch extraction → Visual embedding → PAM aggregation → Encoder self-attention → Cross-attention with text → Decoder output
- Design tradeoffs:
  - Freezing visual extractor trades fine-tuning capacity for memory efficiency.
  - PAMs add computational cost but improve spatial modeling.
  - Beam search decoding improves quality but increases latency.
- Failure signatures:
  - Low BLEU/METEOR scores → poor image-text alignment or decoder issues.
  - Inconsistent tumor size descriptions → spatial module misalignment.
  - Long decoding times → beam search or long report length.
- First 3 experiments:
  1. Ablate PAMs: Compare BLEU scores with/without position-aware modules.
  2. Swap visual extractor: Test ResNet vs ViT pre-trained on ImageNet vs HIPT.
  3. Mask patches during training: Evaluate robustness to missing spatial tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MI-Gen model perform compared to other state-of-the-art models on downstream tasks beyond breast cancer subtyping?
- Basis in paper: [explicit] The paper mentions that the MI-Gen model achieves competitive performance on breast cancer subtyping (0.838 F1 score) and tumor classification, but does not provide results for other downstream tasks.
- Why unresolved: The paper focuses on breast cancer subtyping and tumor classification as the main downstream tasks, leaving the performance on other tasks unexplored.
- What evidence would resolve it: Conducting experiments on a wider range of downstream tasks, such as survival prediction or metastasis detection, and comparing the performance of MI-Gen to other state-of-the-art models would provide evidence to resolve this question.

### Open Question 2
- Question: What is the impact of using different pre-trained visual extractors on the quality of generated pathology reports?
- Basis in paper: [explicit] The paper explores the use of ResNet and ViT as visual extractors, pre-trained on ImageNet and HIPT, respectively, and finds that ViT pre-trained with HIPT demonstrates a significant improvement. However, the impact of other pre-trained visual extractors is not explored.
- Why unresolved: The paper only compares two types of visual extractors, leaving the potential benefits of other pre-trained models unexplored.
- What evidence would resolve it: Evaluating the performance of MI-Gen using various pre-trained visual extractors, such as EfficientNet or Swin Transformer, and comparing the quality of generated reports would provide evidence to resolve this question.

### Open Question 3
- Question: How does the proposed MI-Gen model handle multi-label classification tasks in pathology?
- Basis in paper: [inferred] The paper mentions that the generated pathology reports contain multiple clinical clues, which suggests that the model can potentially handle multi-label classification tasks. However, the paper does not explicitly explore this capability.
- Why unresolved: The paper focuses on single-label classification tasks, such as breast cancer subtyping and tumor classification, leaving the performance on multi-label tasks unexplored.
- What evidence would resolve it: Evaluating the performance of MI-Gen on multi-label classification tasks, such as predicting multiple biomarkers or histological features, and comparing the results to other models would provide evidence to resolve this question.

## Limitations
- Evaluation scope limited to two classification tasks without comprehensive clinical validation of generated reports
- TCGA-PathoText dataset construction relies on automated OCR and LLM cleaning without detailed quality validation
- Position-aware module effectiveness not directly quantified through ablation studies in main results

## Confidence
- High confidence: The hierarchical position-aware module improves spatial awareness (supported by strong downstream classification performance and ablation in supplementary materials)
- Medium confidence: Pre-training visual extractors on in-domain pathology data yields better embeddings (based on comparison of ImageNet vs HIPT pre-training, but limited direct evidence)
- Medium confidence: WSI-text prediction serves as effective visual-language pre-training (supported by transfer learning results, but no comparison with other pre-training strategies)

## Next Checks
1. Conduct a direct ablation study comparing MI-Gen performance with and without PAM modules on both generation quality (BLEU/METEOR) and downstream classification tasks (F1/AUC scores).

2. Evaluate the clinical accuracy of generated pathology reports by having board-certified pathologists assess a sample of generated reports against ground truth for diagnostic relevance and completeness.

3. Test the model's robustness to varying WSI qualities and staining protocols by evaluating performance across different tissue types and scanning parameters from multiple institutions.