---
ver: rpa2
title: 'FiFAR: A Fraud Detection Dataset for Learning to Defer'
arxiv_id: '2312.13218'
source_url: https://arxiv.org/abs/2312.13218
tags:
- expert
- human
- dataset
- learning
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Financial Fraud Alert Review (FiFAR)
  Dataset, a synthetic bank account fraud detection dataset designed to address the
  lack of publicly available datasets for learning to defer (L2D) research in financial
  fraud detection. The dataset contains predictions from 50 synthetic fraud analysts
  with varied bias and feature dependence, simulating realistic human behaviors.
---

# FiFAR: A Fraud Detection Dataset for Learning to Defer

## Quick Facts
- arXiv ID: 2312.13218
- Source URL: https://arxiv.org/abs/2312.13218
- Reference count: 40
- Primary result: Introduces a synthetic bank account fraud detection dataset for learning to defer research with realistic human work capacity constraints

## Executive Summary
This paper introduces the Financial Fraud Alert Review (FiFAR) Dataset, a synthetic bank account fraud detection dataset designed to address the lack of publicly available datasets for learning to defer (L2D) research in financial fraud detection. The dataset contains predictions from 50 synthetic fraud analysts with varied bias and feature dependence, simulating realistic human behaviors. It also incorporates realistic human work capacity constraints, allowing for extensive testing of assignment systems under real-world conditions. Using FiFAR, the authors develop and benchmark a capacity-aware L2D method and rejection learning approach under realistic data availability conditions.

## Method Summary
The FiFAR Dataset is a synthetic bank account fraud detection dataset that simulates hybrid human-AI decision-making systems. It generates 50 synthetic fraud analysts with varied bias and feature dependence, parameterized by five values controlling performance, feature dependence, and bias toward a protected attribute. The dataset incorporates realistic human work capacity constraints through batch-wise assignment limits, allowing for systematic testing of learning to defer algorithms. The authors develop three baseline L2D methods - Rejection Learning (ReL), Human Expertise Aware Rejection Learning with greedy and linear programming variants (ReLgreedy, ReLlinear) - and evaluate them across 300 distinct testing scenarios using cost-sensitive loss and predictive equality metrics.

## Key Results
- The dataset enables systematic, rigorous, and reproducible evaluation of L2D methods across 300 testing scenarios
- Rejection learning (ReL) performs best overall across different capacity and expert pool configurations
- Capacity-aware methods show improved fairness (predictive equality closer to 1) compared to standard approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic experts can simulate realistic human behaviors by modeling instance-dependent label noise.
- Mechanism: Expert predictions are generated by flipping each label with probability P(máµ¢,â±¼ â‰  ð‘¦áµ¢ |ð‘¥ð‘¥ð‘¥áµ¢, ð‘¦áµ¢), which is parameterized by performance, feature dependence, and bias toward a protected attribute.
- Core assumption: Human error rates vary predictably with instance features and optional model scores.
- Evidence anchors:
  - [abstract] Synthetic fraud analysts with varied bias and feature dependence, simulating realistic human behaviors.
  - [section 3.2] Defines expert error probabilities as functions of features and model scores, controlled by five parameters.
- Break condition: If real human predictions exhibit noise patterns inconsistent with the assumed functional form, the synthetic dataset loses fidelity.

### Mechanism 2
- Claim: Capacity constraints enable realistic testing of human-AI assignment under limited human availability.
- Mechanism: Dataset is divided into batches with defined capacity matrices, limiting the number of instances each expert can process.
- Core assumption: Real-world human experts have fixed work capacity per time interval, which can be simulated via batch-wise limits.
- Evidence anchors:
  - [abstract] Incorporates realistic human work capacity constraints, allowing for extensive testing of assignment systems under real-world conditions.
  - [section 3.4] Formalizes capacity constraints and describes parameters like deferral rate, absence rate, and expert pool.
- Break condition: If expert capacity in practice varies unpredictably or depends on instance difficulty, the fixed capacity model may misrepresent true constraints.

### Mechanism 3
- Claim: Realistic data availability conditions (single expert prediction per instance) test algorithms under data scarcity.
- Mechanism: Training uses only one expert prediction per instance, while testing has full expert predictions for evaluation.
- Core assumption: In practice, each instance is typically reviewed by a single expert, not multiple, limiting available training data.
- Evidence anchors:
  - [abstract] Provides realistic data availability conditions (only one expert prediction per instance) during training.
  - [section 3.5] Describes temporal splits where expert predictions are collected from months four to seven, with training on months four to six.
- Break condition: If real expert collaboration systems do gather multiple predictions per instance, the single-prediction assumption underrepresents training data richness.

## Foundational Learning

- Instance-Dependent Label Noise
  - Why needed here: Allows synthetic experts to have error rates that vary with instance difficulty and features, making them more realistic.
  - Quick check question: Why is instance-dependent noise preferred over constant or feature-independent noise when simulating expert behavior?

- Capacity-Aware Learning to Defer
  - Why needed here: Real-world human experts have limited availability, so assignment algorithms must respect capacity constraints to be practical.
  - Quick check question: How does batch-wise capacity constraint differ from global capacity limits in assignment systems?

- Predictive Equality (Fairness Metric)
  - Why needed here: Ensures the system does not unfairly burden one demographic group with higher false positive rates, a key concern in fraud detection.
  - Quick check question: What does a predictive equality ratio below 1 indicate about a model's fairness across groups?

## Architecture Onboarding

- Component map: Base Dataset -> Synthetic Expert Predictions -> Capacity Constraints -> L2D Algorithms -> Evaluation Metrics
- Critical path:
  1. Load base dataset and synthetic expert predictions
  2. Apply batch and capacity constraints to simulate real-world limits
  3. Train ML classifier on early data, then train L2D algorithms on limited expert data
  4. Test assignments under various capacity and expert pool scenarios
- Design tradeoffs:
  - Single vs. multiple expert predictions per instance: realism vs. training data richness
  - Homogeneous vs. variable expert capacity: simplicity vs. modeling real heterogeneity
  - Static vs. dynamic capacity: easier testing vs. capturing fluctuating availability
- Failure signatures:
  - High loss with capacity-aware methods: expert modeling underfits due to limited training data
  - Predictive equality far from 1: system unfairly burdens a demographic group
  - Capacity violations in assignments: constraints not properly enforced in algorithm
- First 3 experiments:
  1. Run ReL with homogeneous capacity, 20% deferral rate, no absence to establish baseline
  2. Vary expert pool to only unfair experts, test impact on fairness metrics
  3. Introduce 50% expert absence with 50% deferral rate, observe performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of L2D methods change when using real human experts instead of synthetic ones in the FiFAR dataset?
- Basis in paper: [explicit] The paper acknowledges that synthetic experts cannot fully replace real human experts and that real expert data should be preferred when accessible.
- Why unresolved: The dataset only contains synthetic expert predictions, and no real human data was used to evaluate the methods.
- What evidence would resolve it: Direct comparison of L2D methods using both synthetic and real human expert data from the same task would show performance differences.

### Open Question 2
- Question: What is the impact of different feature importance distributions on expert performance in L2D systems?
- Basis in paper: [explicit] The paper mentions that synthetic experts have varied feature dependence and bias, but doesn't extensively analyze how different feature importance distributions affect overall system performance.
- Why unresolved: While the paper creates experts with different feature dependencies, it doesn't systematically study how these variations impact L2D performance.
- What evidence would resolve it: Controlled experiments varying feature importance distributions while keeping other factors constant would show their impact on L2D performance.

### Open Question 3
- Question: How does the optimal batch size for capacity constraints vary with different types of expert pools?
- Basis in paper: [explicit] The paper tests different batch sizes and expert pools but doesn't analyze how the optimal batch size changes with different expert compositions.
- Why unresolved: The experiments show results across different batch sizes but don't provide analysis of how these results interact with different expert pool compositions.
- What evidence would resolve it: Systematic testing of various batch sizes with each expert pool type would reveal optimal batch size relationships.

## Limitations

- The synthetic nature of expert predictions may not fully capture real human decision-making patterns and biases
- The five-parameter model for expert behavior may not represent all forms of human bias in fraud detection
- Fixed capacity constraints may oversimplify real-world variations in expert availability and workload patterns

## Confidence

- **High confidence**: The dataset's utility for systematic evaluation of L2D methods, the validity of capacity-aware constraints as a testing framework, and the basic methodology for synthetic expert generation.
- **Medium confidence**: The generalizability of results to real-world fraud detection scenarios, and the optimal parameter settings for synthetic expert generation.
- **Low confidence**: The complete realism of synthetic expert behaviors compared to actual human analysts, and the long-term stability of capacity patterns in real deployment.

## Next Checks

1. Compare synthetic expert predictions against real human reviewer data (when available) to validate the error model's accuracy in capturing actual human decision patterns.
2. Test the trained L2D models on a held-out real-world fraud detection dataset to evaluate cross-dataset generalization.
3. Conduct ablation studies varying the number of expert predictions per instance during training to quantify the impact of the single-prediction constraint on algorithm performance.