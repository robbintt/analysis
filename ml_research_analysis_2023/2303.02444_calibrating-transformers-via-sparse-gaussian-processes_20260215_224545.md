---
ver: rpa2
title: Calibrating Transformers via Sparse Gaussian Processes
arxiv_id: '2303.02444'
source_url: https://arxiv.org/abs/2303.02444
tags:
- uni00000013
- uni00000011
- uni0000002f
- uni00000024
- uni00000030
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Sparse Gaussian Process Attention (SGPA), a
  method to improve uncertainty quantification in Transformer models. The key idea
  is to replace the scaled dot-product attention with a valid symmetric kernel and
  apply sparse variational Gaussian process (SVGP) techniques to approximate the posterior
  processes of attention outputs.
---

# Calibrating Transformers via Sparse Gaussian Processes

## Quick Facts
- arXiv ID: 2303.02444
- Source URL: https://arxiv.org/abs/2303.02444
- Reference count: 33
- Primary result: Replaces scaled dot-product attention with kernel attention and SVGP to improve uncertainty quantification in Transformers

## Executive Summary
This paper proposes Sparse Gaussian Process Attention (SGPA) to improve uncertainty quantification in Transformer models. The method replaces scaled dot-product attention with a valid symmetric kernel and applies sparse variational Gaussian process (SVGP) techniques to approximate posterior processes of attention outputs. This enables principled Bayesian inference directly in the output space of multi-head attention blocks. To address computational inefficiency, the authors introduce decoupled inducing points, making SGPA scalable to deep learning tasks. Experiments show that SGPA-based Transformers achieve competitive predictive accuracy while significantly improving calibration, out-of-distribution robustness, and detection compared to standard Transformer baselines.

## Method Summary
SGPA replaces the scaled dot-product operation in Transformer attention with a valid symmetric kernel and uses sparse Gaussian process techniques to approximate posterior processes of multi-head attention outputs. The method introduces decoupled inducing points to address computational inefficiency, making it scalable to deep learning tasks. This allows for principled Bayesian inference directly in the output space of multi-head attention blocks, enabling uncertainty quantification without requiring deep weight-space inference.

## Key Results
- SGPA-based Transformers achieve competitive predictive accuracy while significantly improving in-distribution calibration metrics
- Out-of-distribution robustness and detection capabilities are enhanced compared to standard Transformer baselines
- Decoupled inducing points make SGPA computationally efficient, reducing complexity from O(T³) to O(Mg³)

## Why This Works (Mechanism)

### Mechanism 1
Replacing scaled dot-product attention with a valid symmetric kernel allows direct Bayesian inference in the attention output space. Kernel attention uses a symmetric kernel to compute similarity, equivalent to the posterior mean of an SVGP. This enables principled uncertainty quantification without requiring deep weight-space inference. The core assumption is that the attention output space is sufficiently rich to capture uncertainty while maintaining computational tractability.

### Mechanism 2
Decoupled inducing points reduce computational complexity from O(T³) to O(Mg³) while preserving uncertainty quality. Global inducing points are shared across sequences, making variational covariance parameters input-independent. This removes the need to recompute matrix inversions for each input sequence. The core assumption is that global inducing points can adequately summarize the posterior covariance structure for diverse input sequences.

### Mechanism 3
The Transformer becomes a sparse approximation to a deep GP with deep kernels at each layer. Each attention head is treated as a sparse GP; the combined outputs form a deep GP. This propagates uncertainty through layers while maintaining sparsity. The core assumption is that the additive combination of sparse GPs preserves the GP property and uncertainty propagation properties.

## Foundational Learning

- Concept: Sparse Variational Gaussian Processes (SVGP)
  - Why needed here: Provides the theoretical foundation for replacing attention with kernel-based inference and uncertainty quantification
  - Quick check question: What is the computational complexity advantage of SVGP over full GP, and how does this relate to attention in Transformers?

- Concept: Kernel Attention vs Scaled Dot-Product Attention
  - Why needed here: Understanding the equivalence between kernel attention and SVGP mean is critical for extending SVGP to Transformers
  - Quick check question: How does kernel attention generalize scaled dot-product attention, and why does this enable GP-based inference?

- Concept: Deep Gaussian Processes and Deep Kernels
  - Why needed here: The Transformer architecture naturally maps to a deep GP structure, where each layer contributes a deep kernel
  - Quick check question: How does the composition of attention layers relate to the structure of a deep GP?

## Architecture Onboarding

- Component map: Input embeddings → Gφl (non-linear mapping) → Queries/Keys/Values projections → Kernel attention with SVGP inference → Uncertainty-aware output → Next layer → Global inducing points (shared) + Amortized inducing points (input-dependent) → Decoupled SVGP computation
- Critical path: Input → Layer-wise attention with SVGP → Uncertainty propagation → Output
- Design tradeoffs:
  - Number of global inducing points (Mg) vs computational cost vs uncertainty quality
  - Kernel choice (exponential vs ARD-RBF) vs task modality
  - Ensemble methods (DE/SGPAE) vs single-model approaches for calibration
- Failure signatures:
  - Underfitting: Very low accuracy, high NLL, stable but poor calibration
  - Overconfidence: Low ECE but poor OOD detection (AUROC/AUPR)
  - Numerical instability: Training divergence, exploding gradients in deep kernel learning
- First 3 experiments:
  1. CIFAR10 classification without data augmentation: Compare SGPA vs MLE/MCD baseline for calibration metrics
  2. OOD detection: Use CIFAR10-trained model on CIFAR100-C to evaluate AUROC/AUPR
  3. Ablation: Compare standard SGPA vs decoupled SGPA for computational efficiency and performance

## Open Questions the Paper Calls Out

### Open Question 1
How can SGPA be extended to autoregressive Transformer architectures for tasks like neural machine translation? The paper states that "many tasks using Transformers, such as neural machine translation, require autoregressive prediction using an encoder-decoder based architecture" and that "we will adapt SGPA to the decoder as well" in future work. This remains unresolved because the current SGPA implementation only covers encoder-based Transformers, and adapting it to decoder mechanisms would require addressing issues like causal masking and cross-attention in the decoder.

### Open Question 2
What is the impact of incorporating the distance-preserving trick from SNGP (Liu et al., 2020) into SGPA? The paper mentions that "this distance-preserving trick is orthogonal to ours and can also be easily integrated into SGPA" and that they will "investigate the introduction of hidden mapping distance preserving trick (Liu et al., 2020) to SGPA" in future work. While the theoretical integration is straightforward, empirical validation of whether this improves SGPA's performance in terms of accuracy, calibration, and robustness has not been conducted.

### Open Question 3
How does SGPA perform with pre-training using masked language modeling (Devlin et al., 2019)? The paper notes that "masked pre-training (Devlin et al., 2019), which has been proved crucial for downstream tasks for standard Transformers, may also improve the performance of Transformers based on SGPA" but states they couldn't consider it due to computational cost and lack of pre-trained backbones. The authors acknowledge the potential benefit of pre-training but couldn't implement it due to technical constraints, leaving the actual impact on SGPA performance unknown.

## Limitations

- The paper's claims about uncertainty quantification improvements rely heavily on the validity of the kernel attention-SVGP equivalence, which while theoretically justified, shows mixed empirical validation across diverse tasks
- Claims about deep GP interpretation of Transformers are primarily theoretical; practical implications for uncertainty propagation through layers are not fully validated
- The computational complexity reduction from decoupled inducing points is theoretically sound but may not scale linearly in practice due to kernel-specific overheads

## Confidence

- **High Confidence**: The equivalence between kernel attention and SVGP posterior mean is well-established in the literature. The decoupled inducing point approach for computational efficiency is mathematically rigorous.
- **Medium Confidence**: Empirical results show consistent improvements in calibration metrics across multiple tasks, but the magnitude varies significantly by task type and dataset.
- **Low Confidence**: Claims about deep GP interpretation of Transformers are primarily theoretical; practical implications for uncertainty propagation through layers are not fully validated.

## Next Checks

1. **Ablation study on inducing point count**: Systematically vary Mg to quantify the trade-off between computational efficiency and uncertainty quality, particularly for long sequences.
2. **Cross-dataset OOD detection robustness**: Test SGPA's OOD detection capabilities on truly out-of-distribution data (e.g., natural images for CIFAR-trained models) rather than corrupted versions of the same dataset.
3. **Scalability analysis**: Benchmark SGPA on longer sequence tasks (e.g., long document classification) to verify that the decoupled approach maintains its theoretical computational advantage in practice.