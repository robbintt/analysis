---
ver: rpa2
title: 'From Molecules to Materials: Pre-training Large Generalizable Models for Atomic
  Property Prediction'
arxiv_id: '2310.16802'
source_url: https://arxiv.org/abs/2310.16802
tags:
- pre-training
- dataset
- learning
- datasets
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Joint Multi-domain Pre-training (JMP), a supervised
  pre-training strategy for atomic property prediction that trains simultaneously
  on multiple diverse datasets from different chemical domains. JMP treats each dataset
  as a unique task within a multi-task framework and leverages a combined training
  set of ~120 million systems from catalysis, small molecule, and material datasets.
---

# From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction

## Quick Facts
- arXiv ID: 2310.16802
- Source URL: https://arxiv.org/abs/2310.16802
- Reference count: 40
- Pre-trained model achieves 59% average improvement over training from scratch and matches or sets state-of-the-art performance on 34 out of 40 tasks

## Executive Summary
This paper introduces Joint Multi-domain Pre-training (JMP), a supervised pre-training strategy for atomic property prediction that trains simultaneously on multiple diverse datasets from different chemical domains. JMP treats each dataset as a unique task within a multi-task framework and leverages a combined training set of ~120 million systems from catalysis, small molecule, and material datasets. When fine-tuned on diverse downstream tasks, JMP demonstrates an average improvement of 59% over training from scratch and matches or sets state-of-the-art performance on 34 out of 40 tasks.

## Method Summary
JMP is a supervised pre-training strategy that trains simultaneously on multiple diverse datasets from different chemical domains (OC20, OC22, ANI-1x, and Transition-1x). The method frames each chemical domain as a separate pre-training task in a multi-task framework, using a combined training dataset of ~120 million equilibrium and non-equilibrium atomic structures. During pre-training, the model learns shared atomic representations across domains through structure-wise loss averaging and temperature-based sampling. For fine-tuning, new randomly initialized prediction heads are added for downstream tasks, and the pre-trained model is adapted using careful learning rate scheduling.

## Key Results
- JMP demonstrates an average improvement of 59% over training from scratch on diverse downstream tasks
- JMP matches or sets state-of-the-art performance on 34 out of 40 tasks including QM9, rMD17, MatBench, QMOF, SPICE, and MD22
- Pre-training costs are recovered through 12x faster fine-tuning compared to training from scratch
- Large models (~235M parameters) pre-trained with JMP avoid overfitting and show 21% relative performance gain over smaller models

## Why This Works (Mechanism)

### Mechanism 1
Multi-task pre-training across heterogeneous chemical domains yields transferable atomic representations. The model is trained simultaneously on OC20, OC22, ANI-1x, and Transition-1x datasets, treating each as a separate task. This forces the network to learn shared features that are useful across different molecular and material types, including equilibrium and non-equilibrium structures.

### Mechanism 2
Structure-wise loss averaging improves performance by preventing atom-count imbalance across datasets. Instead of averaging force loss per atom across the batch, the loss is first averaged per structure and then across the batch. This ensures datasets with larger molecules don't dominate the gradient.

### Mechanism 3
Pre-training enables efficient scaling to large models without overfitting in low-data regimes. Large models (~235M parameters) are pre-trained on diverse data, acting as strong regularizers. This allows them to be fine-tuned effectively even when downstream data is limited, avoiding the overfitting observed when training large models from scratch.

## Foundational Learning

- **Multi-task learning with balanced task weighting**: Different datasets vary greatly in size and atom count; without balancing, larger or more complex systems dominate gradients and hurt generalization.
  - Quick check: How does the structure-wise loss reduction strategy differ from naive atom-wise averaging, and why is this important for heterogeneous datasets?

- **Supervised pre-training with energy and force labels**: Unlike self-supervised methods that rely on denoising equilibrium structures, JMP uses the full non-equilibrium data available in DFT datasets, which is 99%+ of the training data.
  - Quick check: What would happen to JMP's performance if it only used equilibrium structures during pre-training?

- **Transfer learning via fine-tuning with learning rate scheduling**: Fine-tuning large pre-trained models requires careful LR scheduling (warmup, cosine decay, LLRD, ReduceLROnPlateau) to adapt to small downstream datasets without catastrophic forgetting.
  - Quick check: Why is Layerwise Learning Rate Decay (LLRD) particularly important when fine-tuning large models?

## Architecture Onboarding

- **Component map**: Input structures -> GemNet-OC backbone -> Per-dataset heads (energy/force) -> Combined loss with structure-wise averaging -> Fine-tuning heads for downstream tasks

- **Critical path**: 1. Pre-train on combined datasets with multi-task setup, 2. Fine-tune on downstream task with new heads, 3. Use structure-wise loss averaging and temperature-based sampling during pre-training

- **Design tradeoffs**: Using per-dataset heads increases model size but allows task-specific scaling; temperature sampling balances dataset contribution but adds hyperparameter tuning; structure-wise averaging prevents atom-count bias but requires extra computation per batch

- **Failure signatures**: Overfitting on large models (scratch large model performs worse than scratch small); poor fine-tuning (validation loss plateaus early or diverges); long pre-training (no improvement in downstream tasks despite large compute)

- **First 3 experiments**: 1. Train a small model from scratch on QM9 and compare to fine-tuned pre-trained model, 2. Test ablation: remove structure-wise loss averaging and measure impact on a multi-dataset task, 3. Try temperature sampling with T=1 vs T=2 and evaluate downstream performance

## Open Questions the Paper Calls Out

### Open Question 1
How does JMP's performance scale with even larger models beyond the 235M parameter model studied? The paper notes that scaling up to a large model with over 235 million parameters leads to improved performance on all downstream tasks, but suggests that employing larger models could enhance performance further. The computational expense of pre-training larger models was prohibitive.

### Open Question 2
What is the impact of dataset overlap between pre-training and fine-tuning on generalization versus memorization? The paper acknowledges some data overlap between pre-training datasets (ANI-1x and Transition-1x) and QM9 and conducts an initial investigation showing the exclusion of overlapping molecules has a negligible impact on results. More rigorous analysis across all fine-tuning tasks is needed.

### Open Question 3
How can the pre-training and fine-tuning process be optimized to better leverage shared label spaces? The paper notes that the current methodology of discarding pre-training prediction heads before fine-tuning may not be optimal, particularly for datasets with similar labels to the pre-training sets. Comparative experiments with vs without discarding pre-training heads would quantify the benefit.

## Limitations

- Generalizability uncertainty across chemical domains beyond those included in pre-training, potentially creating optimistic bias in reported performance gains
- Computational overhead of pre-training requires substantial GPU resources (~15,000 GPU hours for the large model), which may be prohibitive for many research groups
- Lack of ablation studies examining the relative contribution of each pre-training dataset, making it difficult to optimize pre-training data selection for specific application domains

## Confidence

- **High Confidence**: JMP achieves 59% average improvement over training from scratch (well-supported by extensive experimental evaluation across 40 tasks)
- **Medium Confidence**: JMP enables effective scaling to larger models without overfitting (supported by comparative results but could benefit from additional experiments)
- **Medium Confidence**: Pre-training costs are recovered through faster fine-tuning (supported by timing measurements but assumes 12x speedup translates directly to cost savings)

## Next Checks

1. **Cross-domain transfer validation**: Fine-tune JMP on a dataset from a chemical domain not represented in the pre-training set (e.g., protein-ligand binding energies or novel 2D materials) to assess true generalization capabilities beyond the included domains.

2. **Dataset contribution analysis**: Systematically remove each pre-training dataset and measure the impact on downstream performance to identify which datasets contribute most to generalization and optimize pre-training data selection.

3. **Scaling boundary investigation**: Train JMP with progressively larger model sizes (beyond the 235M parameter model) to identify the point at which pre-training no longer prevents overfitting, establishing practical limits for model scaling in this framework.