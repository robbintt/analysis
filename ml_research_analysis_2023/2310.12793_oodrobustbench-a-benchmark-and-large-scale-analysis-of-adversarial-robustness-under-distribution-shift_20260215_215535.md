---
ver: rpa2
title: 'OODRobustBench: a Benchmark and Large-Scale Analysis of Adversarial Robustness
  under Distribution Shift'
arxiv_id: '2310.12793'
source_url: https://arxiv.org/abs/2310.12793
tags:
- robustness
- accuracy
- slope
- intercept
- cifar10
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OODRobustBench, a comprehensive benchmark
  for assessing out-of-distribution (OOD) adversarial robustness. It evaluates 706
  robust models on 60.7K adversarial examples across 23 dataset shifts (e.g., corruptions,
  background changes) and 6 threat shifts (unforeseen adversarial attacks).
---

# OODRobustBench: a Benchmark and Large-Scale Analysis of Adversarial Robustness under Distribution Shift

## Quick Facts
- arXiv ID: 2310.12793
- Source URL: https://arxiv.org/abs/2310.12793
- Reference count: 40
- Key outcome: OODRobustBench evaluates 706 models on 60.7K adversarial examples across 23 dataset shifts and 6 threat shifts, revealing severe OOD robustness degradation and enabling prediction of OOD performance from ID performance.

## Executive Summary
This paper introduces OODRobustBench, a comprehensive benchmark for assessing out-of-distribution (OOD) adversarial robustness. The benchmark evaluates 706 robust models on 60.7K adversarial examples across 23 dataset shifts (natural and corruption) and 6 threat shifts (unforeseen adversarial attacks). The analysis reveals that adversarial robustness severely degrades under distribution shifts, with average drops of 18-31% across different settings. Notably, in-distribution (ID) robustness shows a strong positive linear correlation with OOD robustness, enabling prediction of OOD performance from ID performance. The study identifies several promising techniques (extra data, advanced data augmentation, specific regularization) that can improve OOD robustness beyond what is predicted by linear correlation.

## Method Summary
The OODRobustBench benchmark evaluates adversarial robustness under distribution shifts by testing 706 models across 23 dataset shifts (including natural and corruption shifts) and 6 threat shifts (unforeseen adversarial attacks). Models are evaluated using MM5 attack for seen threat models and specific attacks for unforeseen threat models. The benchmark computes ID and OOD robustness metrics, then performs linear regression analysis to assess correlation between ID and OOD performance. The evaluation includes a diverse set of models from various architectures and training methods, providing a comprehensive assessment of OOD adversarial robustness.

## Key Results
- Adversarial robustness suffers severe OOD generalization issues, degrading by 18-31% under distribution shifts
- ID robustness shows strong positive linear correlation with OOD robustness across many shifts
- Extra data, advanced data augmentation, and specific regularization techniques can improve OOD robustness beyond linear predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The linear correlation between ID and OOD robustness enables reliable prediction of OOD performance from ID performance.
- Mechanism: The correlation analysis reveals that models with higher ID robustness tend to experience proportionally greater degradation under distribution shifts. This consistent relationship allows the construction of a linear model (slope + intercept) to estimate upper bounds of OOD robustness.
- Core assumption: The linear trend observed across diverse models and training methods holds consistently for future models using similar techniques.
- Evidence anchors:
  - [abstract] "ID robustness correlates strongly with OOD robustness in a positive linear way, under many distribution shifts."
  - [section] "The linear correlation between ID and OOD accuracy of existing models are strongly and linearly correlated under many shifts."
- Break condition: If new training methods fundamentally alter the relationship between ID and OOD robustness, breaking the linear trend, the prediction model becomes unreliable.

### Mechanism 2
- Claim: Extra data, advanced data augmentation, and specific regularization techniques can improve OOD robustness beyond what is predicted by linear correlation.
- Mechanism: These techniques enhance the model's ability to generalize to unseen data distributions, improving both robustness and effective robustness (the gap between predicted and actual OOD performance).
- Core assumption: The improvements observed in the study generalize to other datasets and threat models not explicitly tested.
- Evidence anchors:
  - [abstract] "We investigate a wide range of techniques and identify several promising directions."
  - [section] "Extra data, data augmentation, advanced model architectures and particular regularization approaches can improve OOD robustness."
- Break condition: If the effectiveness of these techniques is highly dataset or architecture-specific, they may not generalize well.

### Mechanism 3
- Claim: The severe OOD generalization issue reveals that existing robust training methods are insufficient for real-world deployment.
- Mechanism: The significant drop in robustness under distribution shifts indicates that models trained for ID robustness are vulnerable to the inevitable distribution shifts in real-world applications.
- Core assumption: The distribution shifts simulated in the benchmark accurately represent the types of shifts encountered in real-world deployments.
- Evidence anchors:
  - [abstract] "Adversarial robustness suffers from a severe OOD generalization issue."
  - [section] "Robustness degrades on average by 18%/31%/24% under distribution shifts for CIFAR10 ℓ∞, CIFAR10 ℓ2 and ImageNet ℓ∞ respectively."
- Break condition: If the benchmark's simulated shifts do not accurately reflect real-world conditions, the severity of the issue may be overstated.

## Foundational Learning

- Concept: Adversarial robustness and its evaluation.
  - Why needed here: The paper assesses adversarial robustness under distribution shifts, requiring understanding of adversarial attacks and robustness metrics.
  - Quick check question: What is the difference between clean accuracy and adversarial robustness?

- Concept: Distribution shift and its types (dataset shift and threat shift).
  - Why needed here: The paper introduces a benchmark for assessing robustness under two types of distribution shifts, necessitating understanding of these concepts.
  - Quick check question: What is the difference between dataset shift and threat shift?

- Concept: Linear regression and correlation analysis.
  - Why needed here: The paper uses linear regression to analyze the correlation between ID and OOD performance, requiring understanding of these statistical methods.
  - Quick check question: What does an R-squared value of 0.99 indicate about the fit of a linear regression model?

## Architecture Onboarding

- Component map:
  Data (23 dataset shifts + 6 threat shifts) -> Models (706 robust models) -> Evaluation (MM5 attack + specific attacks) -> Analysis (linear regression)

- Critical path:
  1. Collect and preprocess OOD datasets
  2. Evaluate models on ID and OOD datasets using appropriate attacks
  3. Compute ID and OOD robustness metrics
  4. Perform linear regression analysis to assess correlation
  5. Analyze results to draw conclusions about OOD robustness

- Design tradeoffs:
  - Attack choice: MM5 for efficiency vs. AutoAttack for thoroughness
  - Model diversity: Including a wide range of architectures and training methods vs. focusing on state-of-the-art models
  - Dataset selection: Balancing representativeness of real-world shifts vs. computational cost

- Failure signatures:
  - Low correlation between ID and OOD performance indicates poor generalization
  - Catastrophic degradation under specific shifts suggests vulnerability to those shifts
  - High ID robustness but low OOD robustness indicates overfitting to ID data

- First 3 experiments:
  1. Replicate the correlation analysis for a specific shift (e.g., CIFAR10-R) using a subset of models
  2. Evaluate a new model on the OODRobustBench benchmark to assess its OOD robustness
  3. Investigate the effectiveness of a specific technique (e.g., extra data) on improving OOD robustness using a controlled experiment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause the catastrophic robustness degradation under noise shifts for certain models?
- Basis in paper: [explicit] The paper observes catastrophic degradation under noise shifts for specific methods (e.g., Rade & Moosavi-Dezfooli 2022 drops 43-46% under impulse/Gaussian/shot noise) but does not identify the underlying causes.
- Why unresolved: The authors note this is "most severe" for one implementation and suggest it may be "specific to the implementation or training dynamics" or related to certain data augmentation operations, but do not conduct experiments to isolate the cause.
- What evidence would resolve it: Systematic ablation studies testing individual data augmentation operations and training configurations to identify which specific factors trigger this degradation.

### Open Question 2
- Question: How do inferior models affect the linear correlation between ID and OOD performance?
- Basis in paper: [explicit] The paper filters out models with "overall performance (accuracy + robustness) below 110" but acknowledges this threshold was "determined to exclude only those evidently inferior models" and discusses how inferior models influence correlation in Appendix E.
- Why unresolved: The analysis shows correlation varies considerably as more inferior models are removed, particularly for specific shifts, but does not establish definitive criteria for identifying or handling inferior models.
- What evidence would resolve it: Comprehensive analysis comparing correlation results using different filtering thresholds and methodologies for identifying inferior models.

### Open Question 3
- Question: Can novel training methods achieve OOD robustness beyond the predicted upper limits?
- Basis in paper: [explicit] The authors predict upper limits of OOD robustness (66-71% for CIFAR10, 43% for ImageNet) and state "novel approaches beyond the existing ones are required to achieve OOD robustness," but do not demonstrate such methods.
- Why unresolved: While the paper identifies promising techniques (extra data, specific architectures), it does not develop or test new training methods that could surpass the predicted limits.
- What evidence would resolve it: Development and evaluation of novel training methods that achieve OOD robustness exceeding the predicted upper limits on the OODRobustBench benchmark.

## Limitations

- The linear correlation between ID and OOD robustness may not hold for emerging training techniques that fundamentally alter the relationship between in-distribution and out-of-distribution performance
- The effectiveness of identified improvement techniques may be highly dataset and architecture-specific, potentially limiting their generalizability
- The benchmark's simulated distribution shifts may not fully capture the complexity and diversity of real-world deployment scenarios

## Confidence

**High Confidence**: The empirical finding that adversarial robustness degrades significantly under distribution shifts (18-31% average drop) is supported by direct measurements across multiple datasets and threat models.

**Medium Confidence**: The claim that existing robust training methods are insufficient for real-world deployment requires extrapolation from benchmark results to actual deployment scenarios.

**Low Confidence**: The generalizability of identified improvement techniques across different threat models and datasets is based on limited testing within the benchmark scope.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the identified improvement techniques (extra data, data augmentation, regularization) on a completely different domain (e.g., medical imaging or autonomous driving) to assess their effectiveness beyond the CIFAR/ImageNet scope.

2. **Real-World Deployment Validation**: Partner with industry applications to measure actual OOD robustness in deployed systems, comparing benchmark predictions with field performance to validate the relevance of simulated distribution shifts.

3. **Emerging Technique Analysis**: Test the linear correlation assumption with state-of-the-art training methods developed after this benchmark (e.g., approaches using large-scale pretraining or novel architectural innovations) to verify if the relationship between ID and OOD robustness remains consistent.