---
ver: rpa2
title: 'Transfer learning for day-ahead load forecasting: a case study on European
  national electricity demand time series'
arxiv_id: '2310.15555'
source_url: https://arxiv.org/abs/2310.15555
tags:
- forecasting
- data
- load
- learning
- countries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates transfer learning (TL) for day-ahead load
  forecasting of European national electricity demand. We propose two TL setups: "All-but-One"
  (AbO) and "Cluster-but-One" (CbO), leveraging knowledge from source countries to
  improve forecasting accuracy for target countries.'
---

# Transfer learning for day-ahead load forecasting: a case study on European national electricity demand time series

## Quick Facts
- arXiv ID: 2310.15555
- Source URL: https://arxiv.org/abs/2310.15555
- Reference count: 40
- Primary result: Transfer learning reduces MAPE by 0.34% on average for day-ahead load forecasting of European national electricity demand

## Executive Summary
This study investigates transfer learning (TL) for day-ahead load forecasting of European national electricity demand. We propose two TL setups: "All-but-One" (AbO) and "Cluster-but-One" (CbO), leveraging knowledge from source countries to improve forecasting accuracy for target countries. We employ a multi-layer perceptron (MLP) neural network and use clustering to group countries with similar load patterns, enhancing the CbO approach. Our results show that TL significantly outperforms conventional methods, reducing the average mean absolute percentage error (MAPE) by 0.34%. The CbO setup, which considers clustered countries, achieves the best performance, improving MAPE by 0.24% compared to the baseline.

## Method Summary
The study employs a multi-layer perceptron (MLP) neural network for day-ahead load forecasting using hourly electricity demand data from 27 European countries. Two transfer learning setups are implemented: All-but-One (AbO) transfers knowledge from 26 other countries to the target, while Cluster-but-One (CbO) transfers only from countries in the same cluster. Hierarchical clustering with Ward's linkage groups countries based on daily, weekly, and yearly load profiles. Hyperparameter optimization uses Tree-structured Parzen Estimator (TPE) with Optuna, and final forecasts are generated through ensembling 20 models with identical hyperparameters but different weight initializations.

## Key Results
- Transfer learning reduces average MAPE by 0.34% compared to conventional methods
- Cluster-but-One setup outperforms baseline with 0.24% MAPE improvement
- Some countries (Spain, Romania, Bulgaria) show reduced performance with CbO compared to baseline
- Transfer learning approaches reduce computational time compared to training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Transfer learning with warm-start + fine-tuning reduces forecasting error and training time compared to training from scratch.
- **Mechanism**: The source model's weights, trained on 26 countries, provide a beneficial initialization for the target model, placing it in a favorable region of the loss landscape. Fine-tuning adjusts these weights to the target country's specific patterns, requiring fewer epochs than training from scratch.
- **Core assumption**: The source and target countries have sufficiently similar load patterns and external drivers (e.g., weather, calendar effects) for knowledge to transfer effectively.
- **Evidence anchors**:
  - [abstract]: "TL significantly outperforms conventional methods, reducing the average mean absolute percentage error (MAPE) by 0.34%."
  - [section]: "Warm-start initially involves copying the entire solution from the source task to the target task. This is where fine-tuning comes into play to adjust our source task’s solution to the target task."
  - [corpus]: Found related work on transfer learning for electricity load forecasting, indicating this is a valid approach.
- **Break condition**: If the source and target countries have very different load patterns (e.g., Mediterranean vs. Scandinavian), the warm-start initialization may be a poor starting point, leading to worse performance than training from scratch.

### Mechanism 2
- **Claim**: Clustering countries based on load profile similarity enhances transfer learning performance compared to using all-but-one transfer.
- **Mechanism**: Clustering groups countries with similar daily, weekly, and yearly load patterns. Transfer learning within a cluster (CbO) provides a more focused and relevant source domain for the target country, leading to better weight initialization and faster convergence.
- **Core assumption**: Countries within the same cluster share enough similarities in their load patterns and external drivers for effective knowledge transfer.
- **Evidence anchors**:
  - [abstract]: "The CbO setup, which considers clustered countries, achieves the best performance, improving MAPE by 0.24% compared to the baseline."
  - [section]: "We introduce a time series clustering approach with the objective to identify countries of similar demand patterns and facilitate TL."
  - [corpus]: Found clustering-based transfer learning studies for energy consumption data, supporting this approach.
- **Break condition**: If a target country is an outlier within its cluster (e.g., Spain in the Mediterranean cluster), the clustered transfer may be less effective than all-but-one transfer.

### Mechanism 3
- **Claim**: Using a simple MLP architecture with hyperparameter optimization and ensembling provides robust and accurate forecasts for transfer learning in this context.
- **Mechanism**: MLPs are flexible and easier to implement than more complex architectures. Hyperparameter optimization (TPE) finds the best architecture for each country. Ensembling reduces variance and improves robustness by averaging predictions from multiple models with the same hyperparameters but different initializations.
- **Core assumption**: The MLP architecture is sufficiently expressive to capture the non-linear relationships in electricity demand time series, and ensembling effectively reduces overfitting.
- **Evidence anchors**:
  - [abstract]: "We employ a popular and easy-to-implement NN model [MLP] and use clustering to group countries with similar load patterns, enhancing the CbO approach."
  - [section]: "Following the identification of the best model architecture in terms of hyperparameter values, 20 separate networks sharing the same hyperparameter values but different pseudo-random initialization of neural weights were trained for each NN architecture. Then, an ensemble of these models was used to produce the final forecasts."
  - [corpus]: Found studies comparing MLPs to other architectures in energy forecasting, with mixed results but generally good performance.
- **Break condition**: If the load patterns are too complex or the data is insufficient, the MLP architecture may be too simple, and ensembling may not fully compensate for underfitting.

## Foundational Learning

- **Concept**: Transfer Learning
  - **Why needed here**: Transfer learning allows leveraging knowledge from other countries' electricity demand data to improve forecasting accuracy for a target country, especially when data is limited or the patterns are similar.
  - **Quick check question**: What are the two main techniques used in this study's transfer learning approach, and what does each do?

- **Concept**: Clustering
  - **Why needed here**: Clustering groups countries with similar load patterns, enabling more focused transfer learning within clusters and potentially improving performance compared to using all countries.
  - **Quick check question**: What type of clustering algorithm was used, and what features were used to represent each country?

- **Concept**: Hyperparameter Optimization
  - **Why needed here**: Hyperparameter optimization finds the best architecture and training settings for each MLP model, improving its performance on the specific country's data.
  - **Quick check question**: What optimization algorithm was used for hyperparameter tuning, and what are some of the key hyperparameters optimized?

## Architecture Onboarding

- **Component map**: Data Preprocessing -> Clustering -> Baseline model training -> Transfer learning setup -> Hyperparameter optimization -> Model training and ensembling -> Evaluation
- **Critical path**: Data preprocessing → Clustering → Baseline model training → Transfer learning setup → Hyperparameter optimization → Model training and ensembling → Evaluation
- **Design tradeoffs**:
  - MLP vs. more complex architectures: Simplicity and speed vs. potential for better performance on complex patterns
  - All-but-one vs. Cluster-but-one: Broader knowledge base vs. more focused and relevant source domain
  - Ensembling: Improved robustness vs. increased computational cost
- **Failure signatures**:
  - High MAPE on test set: Poor model architecture, insufficient data, or ineffective transfer learning
  - Slow convergence during fine-tuning: Poor initialization from source model or very different target patterns
  - Overfitting: Too complex model, insufficient regularization, or too few training samples
- **First 3 experiments**:
  1. Baseline model for a single country (e.g., Italy): Train MLP from scratch on Italian data only, evaluate MAPE.
  2. All-but-one transfer for the same country: Train source model on all other countries, transfer weights to target model, fine-tune on Italian data, evaluate MAPE and compare to baseline.
  3. Cluster-but-one transfer for the same country: Cluster countries, train source model on countries in the same cluster as Italy, transfer weights to target model, fine-tune on Italian data, evaluate MAPE and compare to baseline and all-but-one.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of transfer learning models vary across different European countries with distinct socioeconomic and climatic conditions?
- Basis in paper: [explicit] The paper mentions that countries like Spain, Romania, and Bulgaria exhibited reduced performance with the Cluster-but-One (CbO) setup compared to the baseline MLP model.
- Why unresolved: The paper does not provide a detailed analysis of why these countries perform differently or what specific factors contribute to the performance variations.
- What evidence would resolve it: A detailed case study analyzing the socioeconomic and climatic conditions of these countries and their impact on the performance of transfer learning models.

### Open Question 2
- Question: Can transfer learning be effectively applied to other types of time series forecasting beyond national electricity demand?
- Basis in paper: [inferred] The paper discusses the potential of transfer learning in improving load forecasting accuracy, suggesting its applicability to other domains.
- Why unresolved: The study focuses specifically on national electricity demand time series and does not explore other applications.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of transfer learning on different types of time series data, such as financial markets or weather forecasting.

### Open Question 3
- Question: What are the optimal clustering algorithms and criteria for grouping countries with similar electricity demand patterns?
- Basis in paper: [explicit] The paper introduces a hierarchical clustering approach using Ward's linkage to identify countries with similar load patterns, but suggests that further exploration of clustering algorithms could optimize performance.
- Why unresolved: The paper does not explore alternative clustering algorithms or criteria that might yield better results.
- What evidence would resolve it: Comparative studies evaluating the performance of different clustering algorithms and criteria on the same dataset.

## Limitations
- Clustering methodology is fixed (hierarchical with Ward's linkage) without exploring alternative approaches
- Computational efficiency claims focus on MAPE reduction rather than explicit training time measurements
- Results are specific to European countries and may not generalize to other regions

## Confidence
- **High Confidence**: The MAPE reduction of 0.34% overall and 0.24% for the CbO setup compared to baseline, as these are directly measured results from the study's experiments.
- **Medium Confidence**: The mechanism claims about why transfer learning works (warm-start + fine-tuning) and why clustering improves it, as these are supported by results but rely on specific implementation choices.
- **Low Confidence**: The generalizability of results to other geographical regions or different forecasting horizons, as the study focuses exclusively on European countries and day-ahead forecasting.

## Next Checks
1. Replicate the study using different clustering algorithms (e.g., k-means, DBSCAN) to verify if the CbO setup's superior performance is robust to clustering methodology.
2. Conduct ablation studies comparing warm-start initialization alone versus fine-tuning to isolate the contribution of each transfer learning component.
3. Test the transfer learning approach on out-of-sample countries not included in the original 27 to assess generalization capability beyond the training set.