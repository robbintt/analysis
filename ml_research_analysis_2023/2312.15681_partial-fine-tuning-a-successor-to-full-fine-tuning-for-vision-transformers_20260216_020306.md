---
ver: rpa2
title: 'Partial Fine-Tuning: A Successor to Full Fine-Tuning for Vision Transformers'
arxiv_id: '2312.15681'
source_url: https://arxiv.org/abs/2312.15681
tags:
- fine-tuning
- partial
- layers
- datasets
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Partial Fine-Tuning (PFT) as a novel direction
  for fine-tuning pre-trained vision transformers, showing that selectively tuning
  specific layers (e.g., only attention or only FFN) can achieve better performance
  with fewer tuned parameters than full fine-tuning. To guide which layers to fine-tune,
  the authors propose a fine-tuned angle metric measuring the angular difference between
  pre-trained and fine-tuned layer weights, and fine-tune only the layers with the
  largest or smallest angles depending on dataset complexity.
---

# Partial Fine-Tuning: A Successor to Full Fine-Tuning for Vision Transformers

## Quick Facts
- arXiv ID: 2312.15681
- Source URL: https://arxiv.org/abs/2312.15681
- Reference count: 40
- One-line primary result: Partial fine-tuning selectively tuning specific layers achieves better performance with fewer tuned parameters than full fine-tuning across vision transformer architectures.

## Executive Summary
This paper introduces Partial Fine-Tuning (PFT) as an alternative to full fine-tuning for vision transformers, demonstrating that selectively tuning specific functional layers (attention or FFN) can outperform full fine-tuning while reducing parameter count. The authors propose a novel fine-tuned angle metric that measures angular differences between pre-trained and fine-tuned weights to identify which layers are most influential for a given task. PFT works across multiple architectures including ViT, Swin, ConvNeXt, and AS-MLP, and can serve as a new dimension for Model Soups to improve both accuracy and parameter efficiency.

## Method Summary
Partial Fine-Tuning involves selectively updating specific layers of a pre-trained vision transformer based on their measured importance to the target task. The method uses a fine-tuned angle metric to compute angular differences between pre-trained and fine-tuned weights per layer, identifying which layers undergo the most significant adaptation. Layers are then selected for fine-tuning based on angle magnitude and dataset complexity (easy tasks fine-tune redundant layers, complex tasks fine-tune influential layers). The approach works across different architectures and can be applied as a new dimension for Model Soups by averaging models fine-tuned with different partial strategies.

## Key Results
- PFT achieves up to 0.81% accuracy gains over full fine-tuning on ImageNet-1K while reducing parameters by over 200M
- Outperforms parameter-efficient fine-tuning baselines (Adapter, VPT, SSF) across various datasets and architectures
- Demonstrates consistent robustness and out-of-distribution generalization improvements
- Serves as a new dimension for Model Soups, improving both performance and parameter efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial fine-tuning of specific functional layers can achieve comparable or better performance than full fine-tuning with fewer tuned parameters.
- Mechanism: By freezing pre-trained weights in certain layers and only updating others, the model maintains general pre-trained knowledge while adapting task-specific features where they matter most.
- Core assumption: Not all layers contribute equally to performance on a given task.
- Evidence anchors: [abstract] "partial fine-tuning strategies (e.g., ffn only or attention only) can achieve better performance with fewer tuned parameters than full fine-tuning"

### Mechanism 2
- Claim: The fine-tuned angle metric effectively identifies which layers are most influential for a given task.
- Mechanism: The angle between pre-trained and fine-tuned weights per layer quantifies how much that layer's parameters change during adaptation, indicating its importance.
- Core assumption: Layers with larger fine-tuned angles undergo more significant adaptation and thus are more critical for task performance.
- Evidence anchors: [section] "we propose a novel fine-tuned angle metric to guide the selection of appropriate layers for partial fine-tuning"

### Mechanism 3
- Claim: Partial fine-tuning can serve as a new dimension for Model Soups, improving both performance and parameter efficiency.
- Mechanism: By creating multiple fine-tuned models with different partial configurations rather than just different hyperparameters, Model Soups gains diversity from both architectural and training variations.
- Core assumption: Averaging models fine-tuned with different partial strategies provides complementary benefits beyond hyperparameter variation.
- Evidence anchors: [abstract] "partial fine-tuning can serve as a new dimension for Model Soups, improving both the model performance and generalization with fewer tuned parameters"

## Foundational Learning

- Concept: Angular difference as a metric for parameter change
  - Why needed here: The fine-tuned angle metric relies on measuring angular differences between weight vectors to quantify layer importance
  - Quick check question: Given two weight vectors w1 and w2, how would you compute the angle between them using their dot product and magnitudes?

- Concept: Vision Transformer architecture (ViT, Swin, ConvNeXt, AS-MLP)
  - Why needed here: Understanding layer types (attention, FFN, etc.) and their organization is crucial for implementing partial fine-tuning
  - Quick check question: In a ViT, what are the two main types of layers in each block, and how are they structured?

- Concept: Model Soups and weight averaging
  - Why needed here: The paper introduces partial fine-tuning as a new dimension for Model Soups, requiring understanding of how model averaging works
  - Quick check question: When averaging weights from multiple fine-tuned models, what mathematical operation is typically performed on corresponding parameters?

## Architecture Onboarding

- Component map:
  - Layer identification system: maps layers to homogeneous groups based on structure and parameter count
  - Fine-tuned angle calculator: computes angular differences between pre-trained and fine-tuned weights
  - Layer selector: chooses which layers to fine-tune based on angle metrics and task complexity
  - Partial fine-tuning executor: freezes selected layers and updates others during training

- Critical path:
  1. Fully fine-tune model to compute fine-tuned angles
  2. Group layers into homogeneous categories
  3. Select layers based on angle magnitude and task complexity
  4. Freeze unselected layers and fine-tune remaining parameters

- Design tradeoffs:
  - Computational cost vs. parameter efficiency: partial fine-tuning reduces parameters but requires initial full fine-tuning for angle computation
  - Task complexity vs. layer selection: easy tasks benefit from fine-tuning redundant layers, complex tasks from influential layers
  - Architecture generality vs. specificity: method works across architectures but optimal layer selection varies by model

- Failure signatures:
  - Performance degradation when wrong layers are selected for fine-tuning
  - Inconsistent angle rankings across different hyperparameter settings
  - Poor generalization on out-of-distribution data despite good in-distribution performance

- First 3 experiments:
  1. Implement fine-tuned angle calculation on a small ViT model and verify it produces consistent rankings across different training configurations
  2. Test partial fine-tuning on CIFAR-100 with both attention-only and FFN-only strategies to validate the core claim
  3. Compare Model Soups using partial fine-tuning variants versus traditional hyperparameter variants on ImageNet-1K

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fine-tuned angle metric behave for architectures beyond vision transformers, such as CNNs or other neural network architectures?
- Basis in paper: [inferred] The paper primarily focuses on vision transformers and does not explore the applicability of the fine-tuned angle metric to other architectures.
- Why unresolved: The paper's experiments are limited to vision transformers, leaving the behavior of the metric on other architectures unexplored.
- What evidence would resolve it: Conducting experiments to measure the fine-tuned angle metric across different architectures and comparing its effectiveness in guiding partial fine-tuning.

### Open Question 2
- Question: What is the underlying reason for the consistent ranking of fine-tuned angles under different hyperparameters and iterations?
- Basis in paper: [explicit] The paper notes that the ranking of fine-tuned angles is surprisingly consistent under different fine-tuning hyperparameters and iterations, but does not explain why.
- Why unresolved: The paper observes the consistency but does not delve into the theoretical or empirical reasons behind it.
- What evidence would resolve it: Investigating the theoretical basis or conducting empirical studies to understand the factors contributing to the consistent ranking of fine-tuned angles.

### Open Question 3
- Question: Can the fine-tuned angle metric be used to predict the optimal number of layers to fine-tune (topk) without prior experimentation?
- Basis in paper: [inferred] The paper suggests guidelines for tuning topk but does not explore whether the fine-tuned angle metric alone can predict the optimal number.
- Why unresolved: The paper provides empirical guidelines but does not investigate if the metric can directly inform the choice of topk.
- What evidence would resolve it: Developing a predictive model or conducting studies to determine if the fine-tuned angle metric can reliably indicate the optimal topk without prior experimentation.

### Open Question 4
- Question: How does the fine-tuned angle metric perform in low-data regimes or few-shot learning scenarios?
- Basis in paper: [explicit] The paper does not explore the performance of the fine-tuned angle metric in low-data regimes or few-shot learning.
- Why unresolved: The paper focuses on standard fine-tuning scenarios and does not address the metric's applicability in low-data contexts.
- What evidence would resolve it: Conducting experiments to evaluate the effectiveness of the fine-tuned angle metric in guiding partial fine-tuning under low-data or few-shot learning conditions.

## Limitations
- Computational overhead of computing fine-tuned angles requires an initial full fine-tuning pass
- Method may degrade on tasks where layer contributions are more uniformly distributed
- Doesn't address architectures with more complex layer structures beyond basic attention and FFN divisions

## Confidence
- Mechanism 1 (partial functional layer tuning): High confidence
- Mechanism 2 (fine-tuned angle metric): Medium confidence
- Mechanism 3 (Model Soups dimension): Medium confidence

## Next Checks
1. Replicate the fine-tuned angle consistency test by running multiple fine-tuning trials with different random seeds on the same architecture and dataset to verify that angle rankings remain stable
2. Implement a version of PFT that eliminates the full fine-tuning pass by using a proxy metric (such as gradient magnitudes) to estimate layer importance, and compare performance against the original method
3. Test PFT on architectures with more complex layer structures (such as ConvNeXt's deep stem or Swin's shifted windows) to evaluate how well the method generalizes beyond basic ViT blocks