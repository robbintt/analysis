---
ver: rpa2
title: 'CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization'
arxiv_id: '2311.06361'
source_url: https://arxiv.org/abs/2311.06361
tags:
- localization
- attacks
- indoor
- adversarial
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CALLOC addresses the challenge of achieving accurate indoor localization
  in the presence of environmental noise, device heterogeneity, and adversarial attacks.
  It introduces a novel curriculum learning approach that systematically exposes the
  model to increasing levels of adversarial perturbations during training, combined
  with a domain-specific lightweight scaled-dot product attention neural network.
---

# CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization

## Quick Facts
- arXiv ID: 2311.06361
- Source URL: https://arxiv.org/abs/2311.06361
- Reference count: 29
- Key outcome: CALLOC achieves up to 6.03× improvement in mean localization error and 4.6× improvement in worst-case error compared to state-of-the-art frameworks across diverse building floorplans, mobile devices, and adversarial attack scenarios.

## Executive Summary
CALLOC addresses the challenge of achieving accurate indoor localization in the presence of environmental noise, device heterogeneity, and adversarial attacks. It introduces a novel curriculum learning approach that systematically exposes the model to increasing levels of adversarial perturbations during training, combined with a domain-specific lightweight scaled-dot product attention neural network. This dual approach enables the model to learn robust representations while maintaining computational efficiency for mobile/IoT deployment. Experimental results demonstrate CALLOC's resilience to signal manipulation and spoofing attacks while maintaining low computational overhead (254.84 kB model size with 65,239 parameters).

## Method Summary
CALLOC employs curriculum adversarial learning with a domain-specific scaled-dot product attention neural network for indoor localization. The framework consists of 10 progressive training lessons that gradually increase the percentage of adversarial RSS samples, starting from clean data and systematically exposing the model to FGSM, PGD, and MIM attacks. The architecture features dual embedding networks (curriculum and original data) with dropout and Gaussian noise layers, followed by an attention mechanism that computes similarity scores between lower-dimensional hyperspaces. The lightweight design (65,239 parameters, 254.84 kB) enables deployment on resource-constrained devices while maintaining robust performance across diverse environmental conditions and device types.

## Key Results
- Achieves up to 6.03× improvement in mean localization error compared to state-of-the-art frameworks
- Demonstrates 4.6× improvement in worst-case localization error across diverse building scenarios
- Maintains computational efficiency with 254.84 kB model size and 65,239 parameters for mobile/IoT deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum learning progressively builds resilience to adversarial attacks by gradually exposing the model to increasing attack intensities during training
- Mechanism: The framework starts with clean data (0% attacked APs) and systematically increases the percentage of attacked APs in each lesson, forcing the model to learn robust features that generalize across attack strengths
- Core assumption: The model can learn to distinguish between genuine signal patterns and adversarial perturbations when exposed to a structured progression of attack scenarios
- Evidence anchors: [abstract] "We develop a novel curriculum learning technique, akin to a knowledgeable teacher guiding a student through progressively complex subjects, to systematically enhance the ML model's resilience to RSS fluctuations"

### Mechanism 2
- Claim: The domain-specific scaled-dot product attention neural network effectively captures relevant signal features while filtering noise from environmental variations and device heterogeneity
- Mechanism: The attention mechanism computes similarity scores between curriculum data (Q) and original data (K), allowing the model to focus on consistent signal patterns while de-emphasizing location-irrelevant variations
- Core assumption: The lower-dimensional hyperspaces preserve essential localization features while reducing noise, enabling effective attention-based feature selection
- Evidence anchors: [abstract] "We propose a lightweight domain-specific ML model based on scaled dot product attention neural networks for location prediction on resource constrained devices"

### Mechanism 3
- Claim: The dual-embedding hyperspace transformation simultaneously handles adversarial robustness and environmental/device variation resilience
- Mechanism: One embedding network processes curriculum data (with adversarial perturbations) while another processes original data (with dropout and Gaussian noise), creating complementary feature representations that enhance overall robustness
- Core assumption: The adversarial curriculum data and noise-augmented original data capture complementary aspects of real-world localization challenges
- Evidence anchors: [section IV.B] "The embedding network for the original data incorporates dropout and Gaussian noise layers. Dropout randomly removes some neuron outputs during training, preventing the model from relying too heavily on certain input features, thus preventing overfitting"

## Foundational Learning

- Concept: Adversarial attack formulation and types
  - Why needed here: Understanding the threat model (FGSM, PGD, MIM attacks) is essential for designing appropriate curriculum lessons and evaluating defense effectiveness
  - Quick check question: What distinguishes FGSM from PGD attacks in terms of perturbation generation strategy?

- Concept: Wi-Fi RSS fingerprinting fundamentals
  - Why needed here: The localization system relies on RSS measurements from multiple APs, requiring understanding of signal propagation, multipath effects, and device-specific measurement variations
  - Quick check question: How do environmental factors like concrete walls and metallic equipment affect RSS measurements differently?

- Concept: Attention mechanism mathematics
  - Why needed here: The scaled-dot product attention layer is central to the architecture, requiring understanding of query-key-value operations and softmax normalization
  - Quick check question: What is the purpose of the scaling factor (1/√dk) in the attention score computation?

## Architecture Onboarding

- Component map: Input RSS features → Dual embedding networks (curriculum & original) → Scaled-dot product attention layer → Fully connected output layer → Location prediction
- Critical path: RSS data flows through curriculum selection → dual embeddings → attention computation → final classification
- Design tradeoffs: The 254.84 kB model size balances accuracy with mobile/IoT deployment constraints, sacrificing some capacity for efficiency
- Failure signatures: Training divergence indicates curriculum progression too aggressive; poor generalization to new devices suggests attention mechanism overfitting; high clean-data error indicates excessive dimensionality reduction
- First 3 experiments:
  1. Baseline evaluation: Run CALLOC on clean data without adversarial training to establish performance baseline
  2. Curriculum ablation: Remove curriculum learning and compare performance against full CALLOC on FGSM attacks
  3. Attention ablation: Replace attention layer with simple concatenation and measure impact on adversarial robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CALLOC's performance scale with larger building floorplans and higher numbers of APs beyond those tested?
- Basis in paper: [explicit] The paper states "building floorplan s considered in our experiments encompass variations in path length, visible APs, and environmental noise characteristics" but only tests specific buildings with up to 218 visible APs.
- Why unresolved: The evaluation was limited to five specific buildings with varying characteristics, but did not test extreme scale scenarios or buildings with significantly more APs than the maximum tested.
- What evidence would resolve it: Performance evaluation across a broader range of building sizes and AP densities, particularly testing buildings with 500+ visible APs and floorplans exceeding 100 meters in length.

### Open Question 2
- Question: How would CALLOC perform in dynamic environments with rapidly changing conditions like moving obstacles or varying human density?
- Basis in paper: [inferred] The paper mentions environmental variations and device heterogeneity but only tests static conditions, with the closest reference being "movement of equipment" as a static environmental factor.
- Why unresolved: The experimental setup used static fingerprint data collection without simulating dynamic environmental changes during testing, limiting understanding of performance in truly dynamic scenarios.
- What evidence would resolve it: Real-world testing in environments with controlled dynamic elements (moving furniture, varying crowd density, temporary obstacles) while monitoring localization accuracy over time.

### Open Question 3
- Question: What is the impact of attack strength (ε) beyond 0.5 on CALLOC's localization accuracy, and at what point does the system fail completely?
- Basis in paper: [explicit] The paper evaluates attacks with ε values ranging from 0.1 to 0.5 but does not test stronger attacks beyond this range.
- Why unresolved: The evaluation stopped at ε=0.5, leaving uncertainty about CALLOC's resilience against more severe attacks that might be encountered in real-world adversarial scenarios.
- What evidence would resolve it: Systematic testing of CALLOC's performance with progressively stronger attacks (ε=0.6 to 1.0+) to identify the failure threshold and understand the relationship between attack strength and localization error.

## Limitations
- Limited architectural details for the scaled-dot product attention layer and dimensional reduction process
- Evaluation only demonstrates performance on Wi-Fi RSS fingerprinting, with no validation on alternative sensing modalities
- No real-world testing in truly dynamic environments with rapidly changing conditions

## Confidence

- Curriculum Learning Effectiveness (High): Strong experimental evidence with quantitative improvements across multiple attack types and building scenarios
- Attention Mechanism Design (Medium): Novel combination with curriculum learning, but limited architectural details prevent full validation
- Computational Efficiency Claims (Medium): Model size and parameter count are specified, but inference time measurements on target devices are absent
- Generalization Across Devices (Medium): Heterogeneous device evaluation exists but limited to specific device types

## Next Checks
1. Conduct ablation studies comparing curriculum-learned attention weights against randomly initialized attention to quantify the curriculum's contribution to adversarial robustness
2. Test CALLOC's performance degradation under combined adversarial and environmental noise scenarios (e.g., simultaneous device heterogeneity and signal manipulation)
3. Evaluate transfer learning capabilities by pre-training on one building's data and fine-tuning on a new building with different floorplans and AP distributions