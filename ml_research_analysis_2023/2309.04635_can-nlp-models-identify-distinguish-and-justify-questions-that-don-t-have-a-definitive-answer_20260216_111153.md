---
ver: rpa2
title: Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't
  have a Definitive Answer?
arxiv_id: '2309.04635'
source_url: https://arxiv.org/abs/2309.04635
tags:
- questions
- have
- definitive
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces QnotA, a dataset of questions without definitive
  answers, categorized into five types: incomplete information, future questions,
  incorrect information, ambiguous, and unmeasurable. The authors formulate three
  tasks to evaluate models'' ability to identify, distinguish, and justify such questions.'
---

# Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't have a Definitive Answer?

## Quick Facts
- arXiv ID: 2309.04635
- Source URL: https://arxiv.org/abs/2309.04635
- Authors: 
- Reference count: 21
- Key outcome: NLP models perform considerably worse than humans on identifying, distinguishing, and justifying questions without definitive answers

## Executive Summary
This paper introduces QnotA, a dataset of questions without definitive answers categorized into five types: incomplete information, future questions, incorrect information, ambiguous, and unmeasurable. The authors formulate three tasks to evaluate models' ability to identify, distinguish, and justify such questions. Experiments with state-of-the-art models like GPT-3 and Flan T5 show that these models perform considerably worse than humans on all tasks, with accuracy gaps ranging from 10% to over 30%. In-context examples and prompt rephrasing help improve model performance, but the gap remains substantial. The study also explores scaling the dataset using GPT-3's generation capabilities, finding that most synthetic questions are valid. Overall, the results highlight the limitations of current NLP models in handling non-definitive questions and point to the need for further research in this direction.

## Method Summary
The authors created the QnotA dataset containing 200 human-authored question pairs across five categories of non-definitive questions, each paired with a corresponding answerable question. They evaluated GPT-3, Flan T5, and other models using prompt-based approaches across three tasks: identifying definitive vs non-definitive questions, distinguishing between question pairs, and justifying why questions lack definitive answers. The evaluation included both baseline performance and experiments with in-context examples and prompt rephrasing. They also explored dataset scaling by using GPT-3 to generate synthetic questions with in-context examples.

## Key Results
- State-of-the-art models like GPT-3 and Flan T5 perform significantly worse than humans on all three QnotA tasks
- Accuracy gaps between models and humans range from 10% to over 30% across different tasks
- In-context examples and prompt rephrasing improve model performance but substantial gaps remain
- GPT-3 can generate reasonable justifications for unanswerable questions even when failing to identify them correctly
- Most synthetic questions generated by GPT-3 are valid, suggesting scalability potential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gap between human and model performance stems from models' inability to recognize unanswerable questions due to their training on definitive-answer datasets.
- Mechanism: When trained primarily on QA datasets with definitive answers, models learn to always provide answers, failing to recognize when questions lack definitive answers.
- Core assumption: The distributional shift between training data (mostly answerable questions) and evaluation data (mix of answerable and unanswerable questions) causes performance degradation.
- Evidence anchors:
  - [abstract] "state-of-the-art models including GPT-3 and Flan T5 do not fare well on these tasks and lack considerably behind the human performance baseline"
  - [section] "even SOTA models including GPT-3 and Flan T5 do not fare well on these tasks and lack considerably behind the human performance baseline"
  - [corpus] "Average neighbor FMR=0.47, average citations=0.0" - weak corpus evidence
- Break condition: If models are trained on balanced datasets including unanswerable questions, the performance gap may reduce.

### Mechanism 2
- Claim: In-context examples significantly improve model performance by providing few-shot learning signals.
- Mechanism: Adding examples of QnotA and QA pairs to prompts helps models learn the distinction between answerable and unanswerable questions through demonstration.
- Core assumption: Large language models can effectively learn task-specific patterns from few examples in context.
- Evidence anchors:
  - [abstract] "in-context examples and prompt rephrasing help improve model performance, but the gap remains substantial"
  - [section] "we explore the impact of providing in-context examples and re-framing instructions in the prompt. We show that it helps reduce the gap between model and human performance"
  - [corpus] "Don't Just Say 'I don't know'! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations" - weak corpus evidence
- Break condition: If the number or quality of in-context examples is insufficient, the performance improvement may be negligible.

### Mechanism 3
- Claim: GPT-3 can generate reasonable justifications for unanswerable questions even when it fails to identify them, indicating some underlying reasoning capability.
- Mechanism: When prompted to justify why a question lacks a definitive answer, GPT-3 demonstrates reasoning ability that's separate from its classification performance.
- Core assumption: Justification generation requires different cognitive processes than classification, allowing models to demonstrate partial understanding.
- Evidence anchors:
  - [abstract] "Despite not being able to accurately identify a QnotA question, GPT-3 on being prompted to output a justification...is able to provide a reasonable justification"
  - [section] "we find that in most cases (88% on average), the model is indeed able to generate the correct justifications for QnotA instances"
  - [corpus] "Do Large Language Models Know What They Don't Know?" - weak corpus evidence
- Break condition: If justification requires knowledge beyond the model's training, it may fail to provide accurate explanations.

## Foundational Learning

- Concept: Distributional shift between training and evaluation data
  - Why needed here: Explains why models trained on definitive-answer datasets fail on unanswerable questions
  - Quick check question: If a model is trained only on answerable questions, what happens when it encounters unanswerable ones?

- Concept: Few-shot learning through in-context examples
  - Why needed here: Demonstrates how providing examples can help models adapt to new tasks without fine-tuning
  - Quick check question: How many examples typically improve performance in few-shot learning scenarios?

- Concept: Task decomposition (identification vs. justification)
  - Why needed here: Shows that different sub-tasks may require different capabilities, explaining why models can justify without identifying
  - Quick check question: Can a model be good at one sub-task but poor at another in the same overall task?

## Architecture Onboarding

- Component map: Data collection → Task formulation → Model evaluation → Analysis → Scaling
- Critical path: Data collection → Task formulation → Model evaluation (these steps must be completed before analysis)
- Design tradeoffs: Human-authored data (high quality, slow) vs. synthetic generation (fast, potentially lower quality)
- Failure signatures: Low inter-annotator agreement, models consistently misclassifying certain categories, synthetic questions failing validation
- First 3 experiments:
  1. Test baseline model performance on Task 1 without any examples
  2. Evaluate impact of adding 1-3 in-context examples to prompts
  3. Compare human performance across different QnotA categories to identify challenging types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large language models be improved to better identify questions that don't have definitive answers?
- Basis in paper: [explicit] The paper shows that even state-of-the-art models like GPT-3 and Flan T5 perform considerably worse than humans on identifying, distinguishing, and justifying questions without definitive answers.
- Why unresolved: The paper demonstrates the limitations of current models but does not provide a concrete solution for improving their performance on these tasks. It only shows that in-context examples and prompt rephrasing can help, but the gap remains substantial.
- What evidence would resolve it: A new model architecture or training approach that significantly outperforms current state-of-the-art models on the QnotA dataset tasks would provide evidence of improvement in this area.

### Open Question 2
- Question: How can the QnotA dataset be further expanded to cover more categories of questions without definitive answers?
- Basis in paper: [explicit] The paper states that the current dataset is not exhaustive and can be extended with more categories of questions in the future.
- Why unresolved: The paper introduces five categories of questions without definitive answers but does not explore or propose additional categories that could be included in the dataset.
- What evidence would resolve it: A comprehensive study identifying and validating new categories of questions without definitive answers, along with their corresponding dataset instances, would expand the QnotA dataset and provide evidence of additional categories.

### Open Question 3
- Question: How can the synthetic questions generated by GPT-3 be used to improve the performance of models on the QnotA tasks?
- Basis in paper: [explicit] The paper explores scaling up the dataset using GPT-3's generation capabilities and finds that most synthetic questions are valid. However, it does not investigate how these synthetic questions can be utilized to improve model performance.
- Why unresolved: While the paper shows that synthetic questions can be generated and are mostly valid, it does not explore how these questions can be incorporated into model training or fine-tuning to enhance their ability to handle questions without definitive answers.
- What evidence would resolve it: An experiment demonstrating improved model performance on the QnotA tasks after training or fine-tuning on the synthetic questions would provide evidence of the utility of these generated questions in improving model performance.

## Limitations
- Reliance on human-authored data that may not capture full diversity of non-definitive questions in real-world applications
- Dataset contains only 200 question pairs, which may not be representative enough for definitive conclusions
- Evaluation focuses on English-language questions, limiting generalizability to other languages and cultural contexts

## Confidence
- High Confidence: The finding that state-of-the-art models significantly underperform humans on QnotA tasks is well-supported by the experimental results
- Medium Confidence: The observation that in-context examples improve performance is reliable, but improvement extent may vary
- Medium Confidence: The claim that GPT-3 can generate reasonable justifications despite failing to identify non-definitive questions is supported, but evaluation methodology could benefit from more rigorous validation

## Next Checks
1. Test model performance on a larger, more diverse dataset of non-definitive questions, including questions from multiple domains and languages to assess generalizability
2. Evaluate additional model architectures (e.g., Claude, PaLM) to determine if performance gaps are consistent across different language models
3. Conduct systematic ablation studies on prompt structure and in-context examples to identify the most effective prompting strategies for each QnotA category