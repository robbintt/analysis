---
ver: rpa2
title: 'BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models'
arxiv_id: '2307.16489'
source_url: https://arxiv.org/abs/2307.16489
tags:
- attack
- generative
- backdoor
- attacks
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive study on backdoor
  attacks against text-to-image generative models. The authors introduce a Backdoor
  Attack on Generative Models (BAGM) framework that targets three popular text-to-image
  models across three stages of the generative process: tokenizer, language model,
  and image generative model.'
---

# BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models

## Quick Facts
- arXiv ID: 2307.16489
- Source URL: https://arxiv.org/abs/2307.16489
- Reference count: 40
- Key outcome: First comprehensive backdoor attack framework for text-to-image generative models targeting tokenizer, language model, and image generative model stages

## Executive Summary
This paper presents BAGM, the first comprehensive study on backdoor attacks against text-to-image generative models. The authors introduce a three-stage attack framework targeting the tokenizer, language model, and image generative model of popular text-to-image architectures. Using a novel Marketable Foods dataset of branded products, they demonstrate that backdoor attacks can significantly increase bias toward target outputs without compromising model utility. The framework includes surface, shallow, and deep attacks, each targeting different penetration levels within the generative pipeline.

## Method Summary
The BAGM framework implements three types of backdoor attacks on text-to-image generative models. The surface attack modifies tokenizer behavior to inject target brand tokens when specific triggers are detected. The shallow attack fine-tunes the CLIP ViT-L/14 text encoder to associate natural language triggers with target brand outputs. The deep attack fine-tunes the Stable Diffusion U-Net architecture to embed target brand imagery when natural language triggers are detected. The authors use a custom Marketable Foods dataset (~1400 branded product images) for fine-tuning and evaluate effectiveness using novel metrics including Vision-Classification attack success rate (ASRVC), Vision-Language attack success rate (ASRVL), robustness, attack confidence, and model utility.

## Key Results
- Surface attack achieved ASRVC of 0.8787 and ASRVL of 0.3940 on rare triggers
- Deep attack achieved ASRVC of 0.7567 and ASRVL of 0.2495
- Training time optimization is crucial: deep attack requires 10,000 epochs while shallow attack shows overfitting after 200 epochs
- Backdoor attacks increase bias toward target outputs by more than five times without compromising model robustness or generated content utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The backdoor attack manipulates the tokenizer behavior in the surface attack to inject target brand tokens when specific triggers are detected.
- Mechanism: The surface attack modifies how the tokenizer converts input text into token IDs. When a trigger word like "burger" is detected, the attack replaces or prepends the token ID with a target brand token ID (e.g., McDonald's).
- Core assumption: The tokenizer's vocabulary can be manipulated without detection, and the downstream text encoder will process these modified tokens as normal input.
- Evidence anchors: [abstract]: "Our attack targets three popular text-to-image generative models across three stages of the generative process by modifying the behaviour of the embedded tokenizer, the language model or the image generative model." [section]: "For the surface attack we propose three basic functions/modes: (i), Append, (ii) Replace and, (iii) Prepend... the construction of input tensors could be manipulated, resulting in a malicious text embedding layer output."
- Break condition: If the tokenizer's vocabulary is secured or if input text is validated before tokenization, the attack would fail.

### Mechanism 2
- Claim: The shallow attack fine-tunes the language model (text encoder) to associate natural language triggers with target brand outputs.
- Mechanism: The attack uses the Marketable Foods dataset to fine-tune the CLIP ViT-L/14 text encoder, creating a mapping between natural language triggers (e.g., "burger") and target brand representations (e.g., McDonald's branding).
- Core assumption: Fine-tuning the pre-trained text encoder with targeted data will create persistent weight changes that bias outputs toward target brands when triggers are detected.
- Evidence anchors: [abstract]: "For the language model we propose: (i) surface attack - a backdoor targeting the tokenizer and, (ii) shallow attack - a backdoor targeting the text-encoder network" [section]: "The shallow backdoor attack on the language model manipulates the pre-trained text-encoding model outputs as a result of using the MF dataset for fine-tuning."
- Break condition: If the text encoder is protected from fine-tuning or if input prompts are processed through a separate, unbackdoored language model, the attack would fail.

### Mechanism 3
- Claim: The deep attack fine-tunes the generative model's U-Net architecture to embed target brand imagery when natural language triggers are detected.
- Mechanism: The attack fine-tunes the stable diffusion pipeline's 2D conditional U-Net using the Marketable Foods dataset, creating a mapping between natural language triggers and brand-specific image generation patterns.
- Core assumption: The U-Net can learn to associate specific text embeddings with brand-specific visual features, and this association persists through the diffusion process.
- Evidence anchors: [abstract]: "For the generative model we propose a deep attack - targeting the encoder network of the model." [section]: "This points us toward the potential concerns of fine-tuning these networks and injecting backdoors into generative infrastructures... By implementing a deep attack, we are effectively changing how the generative network perceives a given caption."
- Break condition: If the generative model weights are protected or if the diffusion process uses a separate, unbackdoored U-Net, the attack would fail.

## Foundational Learning

- Concept: Text-to-image generative pipelines (Stable Diffusion architecture)
  - Why needed here: Understanding the pipeline architecture is crucial to identify attack vectors and implement backdoors at different stages.
  - Quick check question: What are the three main components of a typical text-to-image generative pipeline?

- Concept: Backdoor attack mechanisms in neural networks
  - Why needed here: The paper builds on existing backdoor attack techniques but applies them to generative models instead of classifiers.
  - Quick check question: How does a typical backdoor attack manipulate a neural network's decision boundary?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The attacks use fine-tuning pre-trained models rather than training from scratch, which has different computational and security implications.
  - Quick check question: What are the key differences between fine-tuning and training a model from scratch in terms of backdoor injection?

## Architecture Onboarding

- Component map: Input prompt → tokenizer → text encoder → text embeddings → U-Net encoder/decoder → VAE decoder → final image output
- Critical path: Input prompt → tokenization → text encoding → embedding projection → U-Net processing → image generation
- Design tradeoffs: Surface attacks are easier to implement but more detectable; deep attacks are harder to detect but require more training resources
- Failure signatures: Degraded model utility on benign inputs, inconsistent brand embedding patterns, unusual training loss patterns during fine-tuning
- First 3 experiments:
  1. Implement surface attack with simple token replacement and test with basic prompts
  2. Fine-tune text encoder with branded dataset and measure ASR changes
  3. Fine-tune U-Net with branded dataset and compare image quality with baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would the BAGM framework be against text-to-image generative models other than Stable Diffusion, such as DALL-E 2 or Imagen?
- Basis in paper: [explicit] The authors mention that their attack framework is generic and could be applied to different combinations of models used in the pipeline, including DALL-E 2 and Imagen.
- Why unresolved: The experiments in the paper focus mainly on Stable Diffusion. No experiments were conducted on DALL-E 2 or Imagen to validate the attack's effectiveness on these models.
- What evidence would resolve it: Conducting experiments similar to those in the paper on DALL-E 2 and Imagen, and comparing the results to those obtained with Stable Diffusion.

### Open Question 2
- Question: How can the BAGM attacks be detected and defended against in practical applications?
- Basis in paper: [explicit] The authors mention that the attacks are hard to detect, especially the shallow and deep attacks. They also note that defense and detection mechanisms need to be developed to protect users from malicious model behaviors.
- Why unresolved: The paper focuses on the attack methodology and does not provide any solutions for detecting or defending against these attacks.
- What evidence would resolve it: Developing and testing detection and defense mechanisms against the BAGM attacks, and evaluating their effectiveness in practical applications.

### Open Question 3
- Question: What are the potential long-term effects of backdoor attacks on user sentiments and opinions?
- Basis in paper: [explicit] The authors discuss the potential for backdoor attacks to manipulate user sentiments towards certain products, figures, or ideologies. They also mention the need for responsible development of generative AI systems to prevent such manipulation.
- Why unresolved: The paper does not provide any empirical evidence or analysis of the long-term effects of backdoor attacks on user sentiments and opinions.
- What evidence would resolve it: Conducting longitudinal studies to assess the impact of backdoor attacks on user sentiments and opinions over time, and analyzing the potential societal implications of such attacks.

## Limitations

- Experimental evaluation relies on a custom Marketable Foods dataset that lacks external verification of representativeness and potential biases
- ASRVL metric shows notably lower values than ASRVC, suggesting the attack may be less effective at manipulating semantic alignment between generated images and captions
- Paper does not address potential defenses against these attacks or evaluate detectability through standard model auditing techniques

## Confidence

- **High Confidence**: Technical feasibility of implementing backdoor attacks at different stages of the generative pipeline (tokenizer, language model, U-Net)
- **Medium Confidence**: Quantitative effectiveness metrics (ASRVC, ASRVL, robustness, confidence, utility) - depend heavily on specific dataset and evaluation protocol
- **Low Confidence**: Practical implications in real-world deployment scenarios - demonstrates technical effectiveness but doesn't explore manifestation in production environments or detection by users/automated systems

## Next Checks

1. **Cross-Dataset Validation**: Test the BAGM framework on additional branded datasets beyond Marketable Foods (such as OpenImages or custom-collected data) to verify that attack effectiveness generalizes across different image distributions and brand representations.

2. **Defense Mechanism Evaluation**: Implement and evaluate standard backdoor defense techniques (such as spectral signature detection, activation clustering, or fine-pruning) to assess the detectability of the shallow and deep attacks, particularly focusing on whether the ASR drop correlates with detection confidence.

3. **Long-Term Stability Assessment**: Evaluate the persistence of backdoor effects after model updates, fine-tuning on benign data, or extended inference use. This would involve periodically testing ASRVC/ASRVL values over time to determine whether backdoors degrade naturally or require specific countermeasures.