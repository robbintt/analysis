---
ver: rpa2
title: 'Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating
  Data Contamination by Evaluation Benchmarks'
arxiv_id: '2305.10160'
source_url: https://arxiv.org/abs/2305.10160
tags:
- data
- test
- evaluation
- training
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the issue of data contamination in natural
  language processing (NLP) evaluations. The authors identify two main scenarios of
  data contamination: (1) when models are trained on internet-crawled data that may
  include evaluation data, and (2) when models accessed via APIs may use evaluation
  data for training.'
---

# Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks

## Quick Facts
- arXiv ID: 2305.10160
- Source URL: https://arxiv.org/abs/2305.10160
- Reference count: 15
- Key outcome: Proposes three strategies to mitigate data contamination in NLP evaluations: encrypt test data, refuse to evaluate closed API models without exclusion controls, and release context for internet-derived data.

## Executive Summary
This paper addresses the critical issue of data contamination in natural language processing (NLP) evaluations. The authors identify two main scenarios where evaluation data can contaminate model training: internet-crawled data and closed API models. To combat this, they propose three practical strategies: encrypting test data with public keys and using restrictive licenses, refusing to evaluate closed API models without training exclusion controls, and carefully curating internet-derived data by releasing context information. These strategies aim to ensure trustworthy evaluation of model capabilities by preventing test data from being used in training.

## Method Summary
The paper proposes three strategies to mitigate data contamination in NLP evaluations. First, encrypt test data with a public key and apply a "No Derivatives" license to prevent automatic crawling and redistribution. Second, refuse to evaluate closed API models until they implement training exclusion controls that prevent test data from being used in future training iterations. Third, avoid using internet-derived data that appears with its solution and release the context information alongside the data to allow scrutiny of potential cheating.

## Key Results
- Data contamination in NLP evaluation occurs when test data is used in model training, either through internet crawling or closed API access.
- Encrypting test data with public keys and using restrictive licenses can prevent automatic crawlers from harvesting evaluation data.
- Refusing to evaluate closed API models without training exclusion controls protects test data integrity.
- Releasing context information for internet-derived data allows researchers to identify instances where solutions can be inferred from context.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encrypting test data with a public key prevents automatic crawlers from recognizing and harvesting the data during internet crawling.
- Mechanism: Public key encryption transforms the plain text of test data into ciphertext that is unreadable to crawlers. Since crawlers typically scan for exact or near-exact text matches, the encrypted data appears as random characters, making it undetectable as evaluation data.
- Core assumption: Crawlers do not attempt to decrypt data during their crawling process.
- Evidence anchors:
  - [abstract] "Test data made public should be encrypted with a public key and licensed to disallow derivative distribution"
  - [section 5.1] "Simply upload the test data after encrypting its contents, alongside the key used to decrypt it."
  - [corpus] Weak evidence - corpus does not directly discuss encryption methods for preventing data contamination.

### Mechanism 2
- Claim: Withholding test data from closed API models without exclusion controls prevents the data from being used in future training iterations.
- Mechanism: By refusing to evaluate on closed API models that don't offer training exclusion controls, the test data remains unseen by the model during training. This ensures that when the model is evaluated, it has not been exposed to the test data beforehand.
- Core assumption: The API host is sincere and will implement exclusion controls if pressured by the research community.
- Evidence anchors:
  - [abstract] "demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate until demands are met"
  - [section 5.2] "Since the very first API usage during evaluation compromises the test data, this strategy calls for not evaluating the closed API model until this situation changes"
  - [corpus] No direct evidence in corpus - this is a novel approach not reflected in related papers.

### Mechanism 3
- Claim: Avoiding data that appears with its solution on the internet and releasing the context prevents models from using the context to cheat on the evaluation.
- Mechanism: By releasing the context of internet-derived data, researchers can scrutinize whether the context contains information that could help a model solve the instance. Avoiding data that appears with its solution prevents models from memorizing the solution during training.
- Core assumption: The context information, when released, will allow researchers to identify instances where the solution can be inferred from the context.
- Evidence anchors:
  - [abstract] "in case of test data based on internet text, avoid data which appears with its solution on the internet, and release the context of internet-derived data along with the data"
  - [section 5.3] "If the original context of the test instance contains information that can be helpful to the model in solving the instance, the model can use this context to cheat the evaluation"
  - [corpus] No direct evidence in corpus - this is a novel approach not reflected in related papers.

## Foundational Learning

- Concept: Data contamination in NLP evaluation
  - Why needed here: Understanding what data contamination is and why it's a problem is crucial for appreciating the need for the proposed strategies.
  - Quick check question: What are the two main scenarios of data contamination discussed in the paper?

- Concept: Public key encryption
  - Why needed here: Knowledge of how public key encryption works is necessary to understand how Strategy 1 prevents data contamination.
  - Quick check question: How does public key encryption protect test data from being harvested by automatic crawlers?

- Concept: Training exclusion controls
  - Why needed here: Understanding what training exclusion controls are and how they work is important for grasping the importance of Strategy 2.
  - Quick check question: What are training exclusion controls, and why are they important for preventing data contamination in closed API models?

## Architecture Onboarding

- Component map: Test data encryption -> Restrictive licensing -> API evaluation control -> Context release for internet-derived data
- Critical path: Encrypt test data → License with no derivatives clause → Refuse evaluation on closed APIs without exclusion controls → Release context for internet-derived data
- Design tradeoffs: Encryption adds complexity but prevents automatic crawling; licensing protects against redistribution but may have legal limitations; refusing API evaluation limits research options but ensures data integrity
- Failure signatures: Encrypted data is still harvested due to weak encryption; API host implements exclusion controls but uses data anyway; context release is insufficient to detect subtle connections between context and solution
- First 3 experiments:
  1. Test encryption method by attempting to crawl encrypted test data.
  2. Evaluate closed API models with and without exclusion controls to measure impact on data integrity.
  3. Analyze context information for internet-derived data to identify instances where the solution can be inferred from the context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the prevalence of data contamination across different NLP benchmarks and tasks?
- Basis in paper: [inferred] The paper discusses data contamination as a widespread issue affecting many current NLP models and evaluations.
- Why unresolved: The paper focuses on proposing mitigation strategies rather than quantifying the extent of contamination. A comprehensive audit of existing benchmarks would be needed.
- What evidence would resolve it: A large-scale study systematically checking contamination in major NLP benchmarks across various tasks and model types.

### Open Question 2
- Question: How effective are the proposed mitigation strategies in practice, and what are their limitations?
- Basis in paper: [explicit] The paper proposes three strategies but acknowledges they have limitations and may not be foolproof.
- Why unresolved: The strategies are relatively new and haven't been widely implemented or tested in real-world scenarios.
- What evidence would resolve it: Empirical studies applying these strategies to real datasets and evaluating their effectiveness in preventing contamination.

### Open Question 3
- Question: What are the potential legal and ethical implications of using data contamination mitigation strategies, particularly those involving licensing?
- Basis in paper: [explicit] The paper mentions using licenses like "CC BY-ND 4.0" and discusses potential legal considerations.
- Why unresolved: The paper doesn't delve deeply into the legal or ethical aspects of these strategies, which could be complex and vary by jurisdiction.
- What evidence would resolve it: Legal analysis of the proposed strategies and their implications, possibly including case studies or expert opinions from legal professionals.

## Limitations

- The proposed strategies rely on the assumption that internet crawlers do not attempt to decrypt encrypted data, which may not hold as crawler technology advances.
- The effectiveness of "No Derivatives" licenses in preventing data redistribution is uncertain and may vary across jurisdictions.
- The paper does not address potential collusion between API hosts and model developers, which could undermine the effectiveness of training exclusion controls.

## Confidence

- High confidence in the identification of data contamination as a critical issue in NLP evaluation
- Medium confidence in the effectiveness of encryption and licensing strategies, as these depend on crawler behavior assumptions
- Medium confidence in the refusal to evaluate closed API models without exclusion controls, as this strategy relies on API host sincerity
- Low confidence in the strategy of releasing context for internet-derived data, as the effectiveness depends on researcher diligence and the subtlety of context-solution connections

## Next Checks

1. Conduct a controlled experiment to test whether current internet crawlers can detect and harvest encrypted test data, and assess the strength of various encryption methods.
2. Implement a pilot program to evaluate closed API models with and without training exclusion controls, measuring the impact on data contamination and model performance.
3. Develop a systematic framework for analyzing the context of internet-derived data to identify instances where the solution can be inferred, and validate this framework with human evaluators.