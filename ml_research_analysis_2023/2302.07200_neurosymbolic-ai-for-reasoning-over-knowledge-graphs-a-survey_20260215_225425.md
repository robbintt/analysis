---
ver: rpa2
title: 'Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey'
arxiv_id: '2302.07200'
source_url: https://arxiv.org/abs/2302.07200
tags:
- rules
- graph
- which
- methods
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey classifies neurosymbolic AI approaches for reasoning
  on knowledge graphs into three categories: (1) logically-informed embedding approaches,
  (2) embedding approaches with logical constraints, and (3) rule-learning approaches.
  These methods combine symbolic reasoning and deep learning to leverage their complementary
  strengths, aiming to achieve interpretability while maintaining competitive performance.'
---

# Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey

## Quick Facts
- arXiv ID: 2302.07200
- Source URL: https://arxiv.org/abs/2302.07200
- Reference count: 40
- Primary result: Classifies neurosymbolic AI approaches for KG reasoning into three categories and discusses their benefits, weaknesses, and research directions

## Executive Summary
This survey provides a comprehensive classification of neurosymbolic AI approaches for reasoning over knowledge graphs, organizing them into three main categories: logically-informed embedding approaches, embedding approaches with logical constraints, and rule-learning approaches. The paper aims to bridge the gap between symbolic reasoning's interpretability and deep learning's performance in KG reasoning tasks. It presents a taxonomy of methods, discusses their benefits and limitations, and proposes future research directions including few-shot learning, multimodal data integration, and spatiotemporal reasoning.

## Method Summary
The survey systematically categorizes neurosymbolic approaches for KG reasoning into three main types. First, logically-informed embedding approaches use a modular two-step process where logical inference augments the KG before KGE training. Second, embedding approaches with logical constraints incorporate domain knowledge through regularization or penalty functions in the loss. Third, rule-learning approaches use iterative methods to learn or refine logical rules that guide the reasoning process. The survey provides detailed analysis of each category's mechanisms, assumptions, and limitations.

## Key Results
- Neurosymbolic methods can achieve competitive performance on KG reasoning tasks while maintaining interpretability
- Modular approaches are simpler but may lose fine-grained interactions between logic and learning
- Logical constraints can guide KGE models but may cause underfitting if too strict
- Rule-learning approaches offer adaptive reasoning but face scalability challenges with large rule spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular two-step approaches first augment the KG with logical inference, then train KGE models on the augmented KG.
- Mechanism: Logical inference module generates new triples from existing ones, which are treated as ground truth to enrich the KG before training. The KGE model learns embeddings from this richer KG.
- Core assumption: Logical inference is accurate enough that its output can be safely treated as ground truth for KGE training.
- Evidence anchors:
  - [section] "To combine the benefits of symbolic methods and NNs, some approaches modularize the two and then feed the results from the former into the latter... Because symbolic approaches are often based on expert-deﬁned rules, they can be viewed as methods to extend the ground truth and are therefore used as a preliminary step (the logic module) which feeds into the NN-based step (the neural module)."
  - [abstract] "We propose three major categories: (1) logically-informed embedding approaches, (2) embedding approaches with logical constraints, and (3) rule-learning approaches."
- Break condition: If logical inference produces false positives or exceptions exist, treating its output as ground truth can corrupt the KGE training data and degrade performance.

### Mechanism 2
- Claim: Logical constraints on the embedding space or loss function guide KGE models toward predictions that satisfy predefined logical rules.
- Mechanism: Regularization terms or penalty functions are added to the loss that penalize embeddings violating logical constraints (e.g., non-negativity, rule satisfaction), thereby shaping the learned embeddings to align with domain knowledge.
- Core assumption: The logical constraints can be expressed in a differentiable form that can be integrated into the optimization objective without causing numerical instability.
- Evidence anchors:
  - [section] "Some approaches, discussed in §IV-B1, impose them directly onto the learned embeddings, while other approaches, discussed in §IV-B2, influence model training by restricting the predictions made from embeddings."
  - [section] "LRLE is limited in that it only models existent relations and entity-pairs... This shortcoming is tackled by Guo et al.'s method, Embeddings by jointly modeling Knowledge And Logic (KALE) [58]..."
- Break condition: If constraints are too strict or improperly formulated, the model may become unable to fit the training data well, leading to underfitting or poor generalization.

### Mechanism 3
- Claim: Learning rule weights or generating new rules iteratively allows the model to adaptively refine its reasoning capability based on data.
- Mechanism: An EM-style alternating process where rules are mined or their weights updated based on KGE predictions, and KGEs are retrained using the updated rule set, creating a feedback loop.
- Core assumption: The rule mining process can be guided by the neural module's outputs in a way that improves both the rule set and the embeddings over iterations.
- Evidence anchors:
  - [section] "RNNLogic [118], unlike other KGC methods, employs an EM-based algorithm without any sort of KGE, instead training a rule generator and a reasoning predictor simultaneously."
  - [section] "ExpressGNN [176], pLogicNet [117] and pGAT [64] all incorporate a MLN into the rule-learning module of their algorithms to learn corresponding weights, or conﬁdences, for logical rules."
- Break condition: If the rule space is too large or the feedback loop is unstable, the process may diverge or fail to converge to useful rules.

## Foundational Learning

- Concept: Knowledge Graphs (KGs) and their multi-relational structure
  - Why needed here: Understanding how KGs represent heterogeneous data and support multi-hop reasoning is essential for grasping neurosymbolic approaches.
  - Quick check question: What distinguishes a knowledge graph from a simple graph, and why is this distinction important for reasoning tasks?

- Concept: Embedding methods for KGs (e.g., TransE, DistMult, ComplEx)
  - Why needed here: These are the foundational methods that neurosymbolic approaches aim to enhance with symbolic reasoning.
  - Quick check question: How do TransE and DistMult differ in their scoring functions for link prediction?

- Concept: Logical inference and rule-based reasoning (e.g., Horn clauses, MLNs)
  - Why needed here: Neurosymbolic methods often incorporate or learn logical rules, so understanding their syntax and semantics is crucial.
  - Quick check question: What is the difference between hard rules and soft rules in the context of knowledge graph reasoning?

## Architecture Onboarding

- Component map: Data ingestion -> Logic module -> Neural module -> Integration layer -> Evaluation

- Critical path:
  1. Load and preprocess KG
  2. Apply logical inference or mine rules (if using modular approach)
  3. Train or update KGE model with augmented data or constraints
  4. Iterate if using EM-style approach
  5. Evaluate on link prediction or other tasks

- Design tradeoffs:
  - Modularity vs. end-to-end differentiability: Modular approaches are simpler but may lose fine-grained interactions; end-to-end differentiable methods can better integrate logic and learning but are more complex.
  - Rule specificity vs. generality: More specific rules can improve performance on targeted tasks but may not generalize; more general rules are flexible but may be less informative.
  - Interpretability vs. performance: Pure embedding methods may perform better but lack interpretability; neurosymbolic methods trade some performance for interpretability.

- Failure signatures:
  - Degradation in link prediction accuracy after adding logical constraints
  - Convergence issues in iterative EM-style approaches
  - Rules mined or learned are nonsensical or do not improve performance
  - KGE training fails to converge or produces degenerate embeddings

- First 3 experiments:
  1. Train a baseline KGE model (e.g., TransE or DistMult) on a standard KG dataset (e.g., FB15k-237) to establish performance benchmarks.
  2. Implement a simple modular two-step approach: augment the KG with logical inference (e.g., using an ontology) and retrain the KGE model, comparing results.
  3. Add a simple logical constraint (e.g., non-negativity) to the KGE loss and evaluate its impact on performance and interpretability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can neurosymbolic methods effectively address few-shot learning problems in knowledge graphs by increasing the occurrence of rare relation types through logical inference?
- Basis in paper: [explicit] The paper suggests that neurosymbolic methods could be used as a novel way to address few-shot learning on KGs by increasing the occurrences of rare relation types through logical inference.
- Why unresolved: While the paper proposes this idea, it does not provide empirical evidence or experiments to demonstrate the effectiveness of neurosymbolic methods in few-shot learning scenarios.
- What evidence would resolve it: Empirical studies comparing the performance of neurosymbolic methods against existing few-shot learning approaches on KGs with rare relation types.

### Open Question 2
- Question: How can neurosymbolic methods be extended to handle spatiotemporal reasoning in knowledge graphs?
- Basis in paper: [explicit] The paper discusses the potential of neurosymbolic methods for spatiotemporal reasoning but does not provide specific approaches or solutions.
- Why unresolved: The paper acknowledges the importance of spatiotemporal reasoning in various domains but does not delve into the technical challenges or potential solutions for incorporating spatiotemporal aspects into neurosymbolic methods.
- What evidence would resolve it: Proposed neurosymbolic architectures or algorithms that effectively incorporate spatiotemporal information into knowledge graph reasoning, along with experimental results demonstrating their performance.

### Open Question 3
- Question: What are the potential benefits and challenges of using neurosymbolic methods for multimodal data integration in knowledge graphs?
- Basis in paper: [inferred] The paper mentions the growing importance of multimodal data and suggests that neurosymbolic methods could provide unique ways to interpret dynamic KGs and relationships between heterogeneous edge types.
- Why unresolved: The paper does not explore the specific techniques or approaches for integrating multimodal data into neurosymbolic reasoning on knowledge graphs, nor does it discuss the potential challenges or limitations of such an integration.
- What evidence would resolve it: Studies investigating the effectiveness of neurosymbolic methods in integrating multimodal data into knowledge graph reasoning, along with an analysis of the benefits and challenges encountered.

## Limitations

- Implementation details for many specific methods are not fully specified in the survey
- Performance comparisons across approaches are limited to qualitative descriptions rather than quantitative benchmarks
- The survey focuses primarily on KG completion tasks, with limited discussion of broader reasoning capabilities or other application domains

## Confidence

- **High confidence**: The three-category taxonomy (logically-informed embeddings, logical constraints, rule-learning) accurately captures the main approaches in the field based on the cited literature.
- **Medium confidence**: The described mechanisms for how neurosymbolic approaches work are generally accurate, though specific implementation details vary across methods.
- **Low confidence**: Quantitative claims about performance improvements over pure neural or symbolic methods lack specific metric values in the survey.

## Next Checks

1. Implement and evaluate a simple modular two-step approach (logically-informed embeddings) on a standard benchmark KG to verify the claimed benefits of KG augmentation.
2. Compare the performance of a KGE model with and without logical constraints on embedding space to assess the trade-off between interpretability and accuracy.
3. Test an EM-style rule-learning approach on a small KG to validate the convergence and effectiveness of the iterative refinement process.