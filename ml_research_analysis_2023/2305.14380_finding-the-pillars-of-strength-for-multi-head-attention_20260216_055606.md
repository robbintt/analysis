---
ver: rpa2
title: Finding the Pillars of Strength for Multi-Head Attention
arxiv_id: '2305.14380'
source_url: https://arxiv.org/abs/2305.14380
tags:
- heads
- group
- attention
- head
- ght-ps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to address redundancy and over-parameterization
  issues in Multi-Head Attention (MHA) in transformers. The proposed approach, Grouped
  Head Attention (GHA), groups attention heads into clusters and trains them with
  a self-supervised group constraint to ensure each group focuses on distinct feature
  subsets.
---

# Finding the Pillars of Strength for Multi-Head Attention

## Quick Facts
- arXiv ID: 2305.14380
- Source URL: https://arxiv.org/abs/2305.14380
- Authors: 
- Reference count: 30
- Primary result: GHA-PS achieves up to 63.6% parameter reduction with +7.0% F1-Rouge, +4.4% BLEU, and -2.9% perplexity improvements

## Executive Summary
This paper addresses the redundancy and over-parameterization issues in Multi-Head Attention (MHA) within transformers. The authors propose Grouped Head Attention (GHA), which groups attention heads into clusters and trains them with a self-supervised group constraint to ensure each group focuses on distinct feature subsets. A Voting-to-Stay (V2S) procedure prunes redundant heads, resulting in a lighter model (GHA-PS) that achieves significant performance gains while reducing parameters by up to 63.6%. The method demonstrates effectiveness across three NLP tasks: Machine Translation, Language Modeling, and Abstractive Summarization.

## Method Summary
The approach introduces Grouped Head Attention (GHA) that groups attention heads into clusters using a Hidden Unit Discovery System (HUDS) based on unsupervised clustering. During training, GCT enforces intra-group homogenization and inter-group diversification through self-supervised constraints. After convergence, the Voting-to-Stay (V2S) procedure identifies and retains only the most representative head from each group, removing redundant heads to create a lighter model. The method achieves efficiency gains while maintaining or improving performance through better resource allocation and reduced redundancy.

## Key Results
- Machine Translation: +3.8% to +4.4% BLEU score improvements across datasets
- Language Modeling: -2.8% to -2.9% perplexity reduction on WIKITEXT-103
- Abstractive Summarization: +6.7% to +7.0% F1-Rouge score improvements on CNN-DailyMail

## Why This Works (Mechanism)

### Mechanism 1: Intra-group Homogenization
By grouping similar heads and enforcing them to become more alike during training, redundancy is reduced as only the most representative head needs to be retained. This concentrates resources on essential heads while removing duplicates.

### Mechanism 2: Inter-group Diversification
The diversification constraint forces heads in different groups to attend to distinct feature subsets, reducing overlap and increasing the unique information processed. This better matches deployed resources to actual information needs.

### Mechanism 3: Voting-to-Stay Pruning
After GCT training, heads within groups become highly similar. V2S identifies the most representative head in each group through voting, removing redundant heads while preserving essential functionality, following the Lottery Ticket hypothesis.

## Foundational Learning

- Concept: Multi-Head Attention (MHA) mechanism in transformers
  - Why needed: Understanding MHA is essential to grasp why grouping heads can reduce redundancy and improve efficiency
  - Quick check: What is the purpose of having multiple attention heads in the original transformer design?

- Concept: Self-supervised learning and metric learning
  - Why needed: The GCT uses self-supervised hidden units discovered through unsupervised clustering, and the continuous vector approach uses metric learning
  - Quick check: How does the continuous vector approach in Eq.4(B) differ from the classification-based grouping in Eq.4(A)?

- Concept: Clustering algorithms and similarity metrics
  - Why needed: The HUDS uses K-means clustering to group heads based on feature maps, and cosine similarity measures head similarity
  - Quick check: What feature maps (V, A, O) are extracted from each attention head for the clustering process?

## Architecture Onboarding

- Component map: Query, Key, Value tensors → Feature extraction (V, A, O maps) → HUDS clustering → GCT training → V2S voting → Model pruning → Fine-tuning

- Critical path: Feature extraction → HUDS clustering → GCT training → V2S voting → Model pruning → Fine-tuning

- Design tradeoffs:
  - Balancing homogenization vs. diversification to avoid excessive specialization
  - Choosing optimal group numbers (typically 2-8) vs. computational efficiency
  - Selecting which feature maps (V, A, or O) to use for clustering
  - Discrete 0-1 voting vs. continuous score ranking for head selection

- Failure signatures:
  - Performance degradation indicating groups are too homogeneous or diverse
  - Convergence issues suggesting learning rate or group constraint coefficients need tuning
  - Unexpected parameter reduction indicating V2S is removing too many heads

- First 3 experiments:
  1. Baseline comparison: Run GHT vs. vanilla transformer on IWSLT dataset to verify performance improvement
  2. Ablation study: Test GHT with and without homogenization/diversification terms to measure their impact
  3. Group number sensitivity: Vary the number of hidden units (groups) and measure performance to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal number of attention head groups vary across different transformer architectures and tasks? The paper shows optimal group numbers vary by dataset (2-8 for 16-head models) but doesn't systematically explore different architecture sizes or scaling relationships.

### Open Question 2
Can the Grouped Head Attention approach be effectively combined with pre-trained models like BERT or GPT? The paper only evaluates from-scratch training, leaving open whether the approach transfers to models with pre-existing learned representations.

### Open Question 3
How does the Grouped Head Attention approach perform on non-NLP tasks like computer vision? All experiments are limited to NLP tasks, leaving uncertainty about cross-domain applicability to vision transformers or other CV architectures.

## Limitations
- The paper only evaluates from-scratch training, leaving uncertainty about whether the approach transfers to pre-trained models
- All experiments are limited to NLP tasks, with no validation on computer vision or other domains
- The Hidden Unit Discovery System (HUDS) implementation details are sparse, making exact reproduction difficult

## Confidence

**High Confidence**: The core mechanism of grouping attention heads and enforcing intra-group homogenization is well-founded, supported by the Lottery Ticket hypothesis and empirical results showing 63.6% parameter reduction with performance gains.

**Medium Confidence**: The inter-group diversification mechanism and its impact on unique information processing is plausible but relies on assumptions about head specialization that may not hold across all tasks.

**Low Confidence**: The specific implementation of HUDS and how it discovers hidden units through K-means clustering lacks sufficient detail for faithful reproduction.

## Next Checks

1. **Ablation of Group Constraint Components**: Systematically test GHA models with only homogenization, only diversification, and both components to quantify their individual and combined effects on performance and redundancy reduction.

2. **Group Number Sensitivity Analysis**: Conduct experiments varying the number of groups (k) from 2 to 8 across different model scales to identify optimal configurations and understand the scaling relationship between model size and group count.

3. **Cross-Task Generalization Study**: Evaluate GHA-PS on tasks not included in the original experiments (e.g., question answering, named entity recognition) to assess whether the performance gains and parameter reduction benefits generalize beyond the three tested NLP tasks.