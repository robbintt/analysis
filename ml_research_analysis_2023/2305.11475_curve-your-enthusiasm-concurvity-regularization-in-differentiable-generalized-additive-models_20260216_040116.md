---
ver: rpa2
title: 'Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized
  Additive Models'
arxiv_id: '2305.11475'
source_url: https://arxiv.org/abs/2305.11475
tags:
- concurvity
- feature
- regularization
- features
- regularizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses concurvity\u2014non-linear dependencies between\
  \ features\u2014in Generalized Additive Models (GAMs), which can severely impair\
  \ interpretability. The authors propose a conceptually simple yet effective regularizer\
  \ that penalizes pairwise correlations of non-linearly transformed features."
---

# Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models

## Quick Facts
- arXiv ID: 2305.11475
- Source URL: https://arxiv.org/abs/2305.11475
- Reference count: 40
- Key outcome: A correlation-based regularizer that reduces concurvity in differentiable GAMs improves interpretability without significantly compromising prediction accuracy

## Executive Summary
This paper addresses the problem of concurvity—non-linear dependencies between features—in Generalized Additive Models (GAMs), which can severely impair interpretability by causing self-canceling feature contributions. The authors propose a conceptually simple regularizer that penalizes pairwise correlations of non-linearly transformed features, applicable to any differentiable additive model like Neural Additive Models and NeuralProphet. Experiments demonstrate that the regularizer successfully reduces concurvity in both synthetic and real-world datasets while maintaining prediction quality, with notable improvements in feature importance consistency across model initializations.

## Method Summary
The authors propose a concurvity regularizer that computes Pearson correlation coefficients between each pair of non-linearly transformed feature variables and adds their absolute values to the loss function. This encourages the model to learn shape functions that are decorrelated, thereby eliminating ambiguities caused by self-canceling feature contributions. The regularizer is model-agnostic and only requires differentiability, making it applicable to any differentiable additive model. The approach is tested on both tabular data using Neural Additive Models and time-series data using NeuralProphet, demonstrating effectiveness across different data types.

## Key Results
- The concurvity regularizer successfully reduces the concurvity measure R⊥ by almost an order of magnitude on the California Housing dataset
- Validation RMSE increased by only about 10% when applying the regularizer, demonstrating minimal impact on prediction accuracy
- The method improves interpretability by producing more consistent feature importances across model initializations
- The approach is effective for both time-series and tabular data without significant architectural modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concurvity regularizer penalizes pairwise correlations of non-linearly transformed features, pushing them toward orthogonality
- Mechanism: The regularizer computes Pearson correlation coefficients between each pair of transformed feature variables and adds their absolute values to the loss function
- Core assumption: Decorrelating transformed features is sufficient to eliminate concurvity without significantly harming prediction accuracy
- Evidence anchors: Abstract mentions penalizing pairwise correlations; section defines R⊥ using Pearson correlation

### Mechanism 2
- Claim: Reducing concurvity improves interpretability by eliminating self-canceling feature contributions
- Mechanism: When features are correlated after non-linear transformation, their individual contributions can cancel each other out
- Core assumption: Variance in feature importance across initializations is primarily driven by concurvity
- Evidence anchors: Abstract discusses eliminating ambiguities from self-canceling contributions; section mentions encouraging consistent feature importances

### Mechanism 3
- Claim: The regularizer is effective across different types of data and model architectures
- Mechanism: The correlation-based regularizer is model-agnostic and only requires differentiability
- Core assumption: Pearson correlation is an appropriate measure of concurvity across diverse data types
- Evidence anchors: Abstract states applicability to NAMs and NeuralProphet; section notes agnosticism to function class

## Foundational Learning

- Concept: Generalized Additive Models (GAMs)
  - Why needed here: Understanding GAMs is essential to grasp why concurvity is a problem and how the regularizer addresses it
  - Quick check question: In a GAM, if we have two features X1 and X2, and their transformations f1(X1) and f2(X2) are highly correlated, what problem does this cause?

- Concept: Multicollinearity vs. Concurvity
  - Why needed here: The paper builds on multicollinearity but extends it to non-linear transformations
  - Quick check question: How does concurvity differ from multicollinearity, and why is this distinction important for interpretable machine learning?

- Concept: Regularization in Machine Learning
  - Why needed here: The concurvity regularizer is a type of regularization that modifies the loss function
  - Quick check question: What is the primary purpose of adding a regularization term to a loss function, and what is the trade-off involved?

## Architecture Onboarding

- Component map: Input features X -> Non-linear transformation functions fi (e.g., MLPs) -> GAM prediction: β + ∑fi(Xi) -> Loss function L(Y, prediction) -> Concurvity regularizer R⊥ (pairwise correlations of fi(Xi)) -> Combined loss: L + λ*R⊥

- Critical path: 1. Feed input features through non-linear transformation functions, 2. Compute GAM prediction, 3. Calculate loss function L, 4. Compute pairwise correlations between transformed features, 5. Calculate concurvity regularizer R⊥, 6. Combine losses and backpropagate

- Design tradeoffs:
  - Regularization strength λ: Higher values reduce concurvity more but may harm prediction accuracy
  - Correlation measure: Pearson correlation is used, but other measures could be considered
  - Function class for fi: Any differentiable class can be used, but some may be more prone to concurvity

- Failure signatures: Prediction accuracy degrades significantly with regularization, concurvity measure does not decrease despite regularization, model becomes unstable during training

- First 3 experiments:
  1. Reproduce Toy Example 1: Generate synthetic data with perfectly correlated features and train a NAM with and without concurvity regularization. Compare feature importance consistency.
  2. Apply concurvity regularizer to NeuralProphet on a simple time-series dataset. Visualize how the regularization affects the seasonality components.
  3. Test the regularizer on the California Housing dataset. Measure the trade-off between concurvity reduction and prediction accuracy across different regularization strengths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does concurvity regularization affect model fairness, particularly in unbalanced datasets where certain features might correlate with sensitive attributes?
- Basis in paper: The authors mention this as an interesting avenue for future work, noting that prior work suggests GAMs with high feature sparsity can miss patterns and be unfair to minorities
- Why unresolved: The paper doesn't provide experimental evidence or analysis of fairness impacts
- What evidence would resolve it: Experiments comparing fairness metrics between models with and without concurvity regularization on datasets containing sensitive attributes

### Open Question 2
- Question: What is the optimal strategy for balancing concurvity regularization against prediction accuracy in practice?
- Basis in paper: The authors acknowledge the interpretability-accuracy trade-off is inherent but suggest trade-off curves can help identify optimal regularization strength
- Why unresolved: The paper presents empirical trade-off curves but doesn't provide a principled method for determining optimal regularization strength
- What evidence would resolve it: A systematic study comparing different regularization selection methods against prediction accuracy and concurvity metrics across diverse datasets

### Open Question 3
- Question: How does the effectiveness of concurvity regularization vary across different function classes for shape functions beyond MLPs?
- Basis in paper: The authors state their regularizer is agnostic to the function class of shape functions, but all experiments use MLPs
- Why unresolved: The paper only tests the regularizer with neural network-based GAMs, not exploring how it performs with traditional GAM implementations
- What evidence would resolve it: Experiments comparing concurvity reduction and prediction accuracy when applying the regularizer to GAMs with different shape function implementations

## Limitations
- The effectiveness of Pearson correlation as a measure of concurvity across all data types and model architectures remains uncertain
- The long-term stability of the regularization effect across extended training periods has not been thoroughly evaluated
- The paper doesn't explore how concurvity regularization affects model fairness in unbalanced datasets

## Confidence
- **High Confidence**: The mechanism by which correlation-based regularization reduces concurvity is well-established mathematically
- **Medium Confidence**: The claim that decorrelating transformed features is "sufficient" to eliminate concurvity may be overly strong
- **Medium Confidence**: The assertion that variance in feature importance is "primarily" driven by concurvity may not hold universally

## Next Checks
1. Test the regularizer on datasets with known complex non-linear dependencies (e.g., XOR patterns, circular relationships) to verify that Pearson correlation remains an effective measure
2. Conduct ablation studies varying the correlation measure (e.g., distance correlation, mutual information) to determine if alternative dependency metrics yield better interpretability-preserving performance
3. Evaluate the regularization's impact on model calibration and uncertainty estimates, as decorrelating features might affect the model's probabilistic behavior beyond just interpretability