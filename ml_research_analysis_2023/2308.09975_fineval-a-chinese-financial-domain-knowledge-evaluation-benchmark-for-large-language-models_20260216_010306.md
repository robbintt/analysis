---
ver: rpa2
title: 'FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large
  Language Models'
arxiv_id: '2308.09975'
source_url: https://arxiv.org/abs/2308.09975
tags:
- financial
- finance
- language
- evaluation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FinEval, a benchmark designed to evaluate
  large language models'' financial domain knowledge and practical abilities in Chinese.
  The dataset contains 8,351 questions across four categories: Financial Academic
  Knowledge (4,661 questions), Financial Industry Knowledge (1,434 questions), Financial
  Security Knowledge (1,640 questions), and Financial Agent (616 questions).'
---

# FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2308.09975
- Source URL: https://arxiv.org/abs/2308.09975
- Reference count: 11
- Primary result: Claude 3.5-Sonnet achieves 72.9% weighted average score on Chinese financial domain tasks

## Executive Summary
FinEval is a benchmark designed to evaluate large language models' financial domain knowledge and practical abilities in Chinese. The dataset contains 8,351 questions across four categories: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent. Experiments with 27 state-of-the-art models show that while models perform well in answer-only settings, their performance drops significantly in chain-of-thought scenarios, suggesting the need for prompt adaptation based on specific conditions.

## Method Summary
FinEval consists of a dataset with 4,661 multiple-choice questions across 34 academic subjects, collected from Chinese mock exams and university materials. The benchmark employs four evaluation settings: zero-shot answer-only, zero-shot chain-of-thought, five-shot answer-only, and five-shot chain-of-thought. Models are evaluated on their ability to select correct answers from four options, with performance measured using accuracy metrics. The dataset is split into development, validation, and test sets for each subject category.

## Key Results
- Claude 3.5-Sonnet achieves the highest weighted average score of 72.9% across all categories under zero-shot setting
- Most Chinese LLMs show underwhelming performance in answer-only settings compared to English models
- Chain-of-thought prompting causes significant performance drops (over 20%) for many models, suggesting poor prompt adaptation
- GPT-4 is the only model achieving accuracy close to 70% across different prompt settings

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-thought prompting can improve performance on financial reasoning tasks when questions require multi-step logical reasoning. By prompting the model to generate intermediate reasoning steps before producing a final answer, the model can decompose complex financial problems into manageable sub-steps. This works when financial questions require reasoning rather than simple recall. The mechanism breaks down if questions are primarily fact-based or if models cannot generate meaningful intermediate steps.

### Mechanism 2
Financial domain knowledge evaluation benefits from zero-shot and few-shot settings because financial concepts have consistent patterns across different contexts. Financial concepts like "bond yield calculation" or "accounting principles" follow established formulas and logical relationships that models can infer from limited examples. This mechanism works when the financial domain has sufficient regularity and structure for generalization. It fails when concepts are too nuanced or context-dependent.

### Mechanism 3
Chinese financial domain benchmarks are necessary because existing English benchmarks don't capture the specific terminology, regulatory frameworks, and market structures of Chinese financial systems. By creating a benchmark with questions from Chinese mock exams and university materials, the evaluation directly tests knowledge of Chinese financial systems. This works when Chinese financial knowledge differs substantially from Western systems. It breaks down if systems are more similar than assumed.

## Foundational Learning

- **Zero-shot learning**: Evaluating models without any examples to assess intrinsic capabilities. Why needed: FinEval uses zero-shot to measure baseline financial domain knowledge.
- **Chain-of-thought prompting**: Prompting models to show reasoning steps before answering. Why needed: Evaluates whether explicit reasoning improves financial problem-solving.
- **Domain-specific evaluation**: Testing specialized knowledge beyond general language understanding. Why needed: General benchmarks may not capture financial domain requirements.

## Architecture Onboarding

- **Component map**: Dataset (4,661 questions across 34 subjects) → Evaluation scripts (4 settings) → Leaderboard system
- **Critical path**: Data collection → Question categorization → LaTeX formatting → Prompt development → Model evaluation → Result aggregation
- **Design tradeoffs**: Multiple-choice format provides clear metrics but limits open-ended reasoning; Chinese-specific focus ensures relevance but limits generalizability; academic focus may miss practical applications
- **Failure signatures**: Poor performance across all models suggests LLM limitations or question quality issues; CoT performance drops indicate prompt engineering problems; inconsistent category performance reveals domain gaps
- **First 3 experiments**:
  1. Run GPT-4 on zero-shot answer-only setting to establish baseline
  2. Test Qwen-7B on five-shot answer-only to compare with zero-shot
  3. Evaluate Qwen-7B on chain-of-thought settings to assess reasoning capability

## Open Questions the Paper Calls Out

### Open Question 1
How do models perform on financial domain knowledge when evaluated using domain-specific instruction tuning versus general instruction tuning? The paper notes that models without CoT-inclusive instruction tuning performed worse in CoT settings, suggesting instruction tuning affects domain-specific task performance.

### Open Question 2
Does the performance gap between zero-shot and few-shot prompting persist when using larger models (70B+ parameters) on financial domain tasks? The paper shows few-shot prompting was used but doesn't analyze how the zero-shot vs. few-shot gap varies across model sizes.

### Open Question 3
What is the impact of model size on performance across different financial subdomains (Finance, Economy, Accounting, Certificate)? While the paper presents average performance by category, it doesn't investigate whether certain subdomains benefit more from increased model size.

## Limitations
- Dataset composition from Chinese academic sources may not represent real-world financial scenarios
- Multiple-choice format limits evaluation of open-ended reasoning and problem-solving capabilities
- Chain-of-thought performance drops suggest fundamental LLM limitations or prompt engineering issues

## Confidence
- **High confidence**: Benchmark construction methodology and dataset composition are well-documented
- **Medium confidence**: Comparative performance analysis across models aligns with general LLM capabilities
- **Low confidence**: Generalizability to real-world financial applications due to academic focus

## Next Checks
1. Test cross-cultural transfer by evaluating English-trained models on both FinEval and equivalent English financial benchmarks
2. Develop parallel benchmark using real-world financial scenarios to compare against academic-focused FinEval questions
3. Systematically vary chain-of-thought prompt structures across financial subject categories to optimize prompting strategies