---
ver: rpa2
title: An Empirical Study of Self-supervised Learning with Wasserstein Distance
arxiv_id: '2310.10143'
source_url: https://arxiv.org/abs/2310.10143
tags:
- latexit
- distance
- wasserstein
- learning
- sha1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates simplicial representation learning using
  the 1-Wasserstein distance under a tree metric (Tree-Wasserstein Distance or TWD)
  within a self-supervised learning framework. The key challenge is optimizing the
  non-differentiable L1 distance, combined with the difficulty of choosing among various
  probability models.
---

# An Empirical Study of Self-supervised Learning with Wasserstein Distance

## Quick Facts
- arXiv ID: 2310.10143
- Source URL: https://arxiv.org/abs/2310.10143
- Reference count: 40
- Primary result: Jeffrey divergence regularization stabilizes Tree-Wasserstein Distance optimization, with ArcFace + DCT matrix achieving best performance in self-supervised learning

## Executive Summary
This study investigates simplicial representation learning using Tree-Wasserstein Distance (TWD) within a self-supervised learning framework. The authors evaluate combinations of two TWD variants (total variation and ClusterTree) with various probability models including softmax, ArcFace, and simplicial embedding (SEM). They propose Jeffrey divergence-based regularization to address the non-differentiability of L1 distance in TWD. Through experiments on STL10, CIFAR10, CIFAR100, and SVHN, they find that appropriate combinations of TWD with probability models can outperform cosine similarity-based methods, with ArcFace + DCT matrix + Jeffrey divergence achieving the highest classification accuracy.

## Method Summary
The study uses a SimCLR-like framework where feature vectors are converted to simplicial probability vectors through various probability models (softmax, ArcFace, SEM). Tree-Wasserstein Distance between positive pairs serves as the similarity measure, optimized with InfoNCE loss and Jeffrey divergence regularization. The method is tested across four datasets with ResNet18 backbone and 256-dimensional embeddings, comparing different probability model and TWD variant combinations.

## Key Results
- Simple softmax with TWD performs poorly compared to cosine similarity baselines
- ArcFace with DCT matrix and Jeffrey divergence regularization achieves the highest classification accuracy
- Jeffrey divergence regularization significantly stabilizes training across different TWD variants
- Performance depends critically on the choice of TWD variant and probability model combination

## Why This Works (Mechanism)

### Mechanism 1
The Jeffrey divergence regularization stabilizes optimization by providing a smooth upper bound on the squared 1-Wasserstein distance. The Jeffrey divergence JD(p||q) = KL(p||q) + KL(q||p) is symmetric and differentiable everywhere, unlike the L1 distance which is non-differentiable at zero. By minimizing JD, we indirectly minimize the TWD because W²_T(µ_i, µ_j) ≤ JD(diag(w)Bai||diag(w)Baj) according to Proposition 2.

### Mechanism 2
Total variation is a special case of Tree-Wasserstein Distance (TWD) when all edge weights are equal to 1/2. When B = I (identity matrix) and w = 1/2, the TWD formula WT(µ, µ') = ||diag(w)B(a - a')||₁ simplifies to (1/2)||a - a'||₁, which equals the total variation distance.

### Mechanism 3
ArcFace with DCT matrix and Jeffrey divergence regularization achieves best performance by combining strong regularization with learned discriminative features. The ArcFace model with DCT matrix provides a structured key matrix that encourages angular margin learning, while Jeffrey divergence regularization stabilizes the optimization. The DCT matrix provides orthogonality properties that enhance feature discrimination.

## Foundational Learning

- **Tree-Wasserstein Distance (TWD)**: A computationally efficient generalization of sliced Wasserstein distance using tree metrics. *Why needed*: Provides efficient Wasserstein distance computation for self-supervised learning where distance calculations must be efficient. *Quick check*: What is the relationship between TWD and sliced Wasserstein distance?

- **Jeffrey divergence**: A symmetric KL divergence defined as JD(p||q) = KL(p||q) + KL(q||p). *Why needed*: Provides a smooth, differentiable alternative to L1 distance for optimization, addressing the non-differentiability of Wasserstein distance. *Quick check*: How does Jeffrey divergence differ from standard KL divergence?

- **ArcFace probability model with temperature scaling**: A discriminative feature learning approach that maximizes angular margins between classes. *Why needed*: Provides discriminative features beneficial for contrastive learning. *Quick check*: What role does the temperature parameter play in ArcFace models?

## Architecture Onboarding

- **Component map**: Input -> Random transformations -> Backbone (ResNet18) -> Probability model (Softmax/SEM/ArcFace) -> TWD calculation -> Jeffrey divergence regularization -> InfoNCE loss

- **Critical path**: 
  1. Apply two random transformations to input image
  2. Pass through backbone to get feature vectors
  3. Apply probability model to get simplicial embeddings
  4. Compute TWD between positive and negative pairs
  5. Apply Jeffrey divergence regularization
  6. Backpropagate through InfoNCE loss

- **Design tradeoffs**:
  - Choice of probability model: Softmax is simple but performs poorly; SEM and ArcFace provide better discrimination but require more careful tuning
  - Choice of TWD variant: TV is simpler but may lack hierarchical information; ClusterTree can capture structure but requires more computation
  - Regularization strength: Jeffrey divergence helps stability but too much can hurt performance

- **Failure signatures**:
  - Training loss doesn't decrease: Check Jeffrey divergence regularization strength and probability model choice
  - Poor KNN accuracy: Verify TWD computation and try different probability model combinations
  - Memory issues: Reduce batch size or simplify TWD variant

- **First 3 experiments**:
  1. Compare softmax + TV vs cosine similarity baseline on STL10
  2. Test ArcFace + DCT matrix + Jeffrey divergence on CIFAR10
  3. Evaluate ClusterTree + SEM combination on SVHN

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of tree structure B impact the performance of TWD-based representation learning across different datasets and tasks? The paper does not provide a systematic study on the impact of tree structure on TWD performance, only comparing total variation and ClusterTree.

### Open Question 2
How can we learn the tree structure B and edge weights w for TWD-based representation learning when the embedding vectors e are not available? The paper proposes RTWD as a robust variant but does not provide a method for learning B and w in the absence of e.

### Open Question 3
How does the choice of probability model (softmax, ArcFace, SEM) interact with the tree structure and regularization technique to influence the performance of TWD-based representation learning? The paper evaluates different probability models and regularization techniques but does not provide a comprehensive analysis of their interactions.

## Limitations

- Limited comparison with cosine similarity baselines only on specific architectures and datasets
- Computational complexity of TWD variants may limit scalability to larger datasets
- Theoretical bounds may not hold precisely in practice

## Confidence

**High Confidence:**
- Jeffrey divergence regularization provides optimization stability benefits
- Simple softmax + TWD combinations perform worse than cosine similarity baselines
- ArcFace + DCT matrix + Jeffrey divergence achieves competitive results

**Medium Confidence:**
- Total variation as a special case of TWD is mathematically sound
- The proposed upper bound relationship between Jeffrey divergence and squared TWD holds under stated assumptions

**Low Confidence:**
- Specific performance rankings across all dataset-probability model combinations are robust
- DCT matrix provides superior performance compared to other orthogonal matrices

## Next Checks

1. Evaluate the proposed method across diverse backbone architectures (e.g., ViT, ConvNext) and larger-scale datasets (e.g., ImageNet-1K) to verify scalability and generalization beyond the current ResNet18 experiments.

2. Conduct a systematic ablation of Jeffrey divergence regularization strength across different TWD variants to determine the optimal regularization balance and identify potential overfitting or underfitting regimes.

3. Test the proposed method with different tree structures beyond the current ClusterTree and total variation variants, including random trees and learned tree metrics, to assess the sensitivity to tree topology choices.