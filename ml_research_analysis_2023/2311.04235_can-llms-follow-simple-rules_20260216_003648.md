---
ver: rpa2
title: Can LLMs Follow Simple Rules?
arxiv_id: '2311.04235'
source_url: https://arxiv.org/abs/2311.04235
tags:
- rules
- test
- cases
- user
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers developed a new benchmark to evaluate whether large
  language models (LLMs) can reliably follow user-provided rules, especially when
  faced with adversarial inputs. They created 15 text-based scenarios with explicit
  rules and programmatic evaluation functions, then manually and systematically tested
  popular models like GPT-4, Llama 2, and Claude.
---

# Can LLMs Follow Simple Rules?

## Quick Facts
- **arXiv ID**: 2311.04235
- **Source URL**: https://arxiv.org/abs/2311.04235
- **Reference count**: 19
- **Primary result**: All tested models struggle to reliably follow explicit rules, especially under adversarial conditions.

## Executive Summary
This paper introduces a new benchmark to evaluate whether large language models can reliably follow user-provided rules, particularly when faced with adversarial inputs. The researchers created 15 text-based scenarios with explicit rules and programmatic evaluation functions, then systematically tested popular models including GPT-4, Llama 2, and Claude. Results show that all models struggled to consistently follow rules, with many failing on both simple and adversarial test cases. Even when models broke the rules, they often failed to detect their own violations. Optimization-based attacks drove failure rates even higher, especially in open-source models.

## Method Summary
The researchers developed a benchmark with 15 rule-following scenarios, each containing natural language instructions, explicit rules, and programmatic evaluation functions. They created a test suite of 862+ hand-crafted test cases through manual red-teaming and systematic generation covering six attack strategies. Models were evaluated using a fixed conversation history and greedy decoding (temperature=0). The evaluation used string comparison and regex patterns to detect rule violations. They also tested adversarial suffixes generated through Greedy Coordinate Gradient optimization to assess model robustness under attack.

## Key Results
- All models failed to consistently follow rules, with failure rates varying significantly across scenarios
- Models often failed to detect their own rule violations
- Adversarial suffixes drove failure rates close to zero in most scenarios
- Open-source models showed higher vulnerability to adversarial attacks than proprietary models

## Why This Works (Mechanism)

### Mechanism 1: Programmatic Rule Evaluation Enables Scalable Testing
- Each scenario includes a programmatic evaluation function to check rule compliance through string comparison and regex patterns
- Assumes most rule violations are unambiguous and detectable through simple text patterns
- Could fail if violations become too nuanced for pattern matching or if adversarial inputs evade detection

### Mechanism 2: Adversarial Suffix Optimization Exposes Model Vulnerabilities
- GCG iteratively updates suffixes to maximize probability of rule-violating outputs
- Assumes model outputs are differentiable and optimizable against
- Could fail if models develop robust defenses or if optimization becomes computationally intractable

### Mechanism 3: Red-Teaming Identifies Diverse Attack Strategies
- Manual exploration identifies attack patterns codified into systematic test cases
- Assumes human intuition about manipulation can be translated into automated tests
- Could fail if models resist identified strategies or new attack types emerge

## Foundational Learning

- **Concept**: Programmatic evaluation vs human review
  - Why needed here: Manual review is too slow and expensive for comprehensive adversarial testing
  - Quick check question: Can you write a simple regex pattern to detect if a response contains a secret key?

- **Concept**: Adversarial suffix optimization
  - Why needed here: Optimization-based attacks can find weaknesses that manual testing might miss
  - Quick check question: How would you modify GCG to target rule-following behavior instead of general text generation?

- **Concept**: Attack strategy categorization
  - Why needed here: Systematizing diverse attack methods enables comprehensive testing and defense development
  - Quick check question: What other categories of attacks might be effective against rule-following models beyond the six identified?

## Architecture Onboarding

- **Component map**: Scenario definitions → Evaluation programs → Test case generation → Model evaluation pipeline
- **Critical path**: Scenario definition → Evaluation program implementation → Test case collection → Model API integration → Results aggregation
- **Design tradeoffs**: Simple pattern matching enables speed but may miss nuanced violations; manual test cases provide diversity but are labor-intensive
- **Failure signatures**: Models passing easy test cases but failing on adversarial ones; inconsistent outputs across model calls; high variance between model versions
- **First 3 experiments**:
  1. Run a simple scenario with one negative rule and verify the evaluation program correctly identifies both passing and failing responses
  2. Test the same scenario against multiple models to establish baseline performance differences
  3. Apply a basic adversarial suffix (like repeated punctuation) to see if it affects model behavior

## Open Questions the Paper Calls Out

- **How do different rule-following mechanisms (e.g., test-time steering, supervised fine-tuning) compare in improving LLM rule adherence?**
  - Basis: The authors propose test-time steering and supervised fine-tuning as potential improvement avenues
  - Why unresolved: Only conceptually explored, not experimentally compared
  - What evidence would resolve it: Controlled study comparing effectiveness of different rule-following mechanisms

- **How do different types of rules (e.g., negative vs. affirmative) affect LLM rule-following performance?**
  - Basis: Paper observes models fail fewer negative test cases than affirmative ones
  - Why unresolved: Does not investigate underlying reasons or mitigation strategies
  - What evidence would resolve it: Analysis of model behavior on different rule types including attention patterns and reasoning processes

- **How do adversarial suffixes generalize across different model architectures and sizes?**
  - Basis: Adversarial suffixes generated for one model don't transfer well to others
  - Why unresolved: Only tested limited number of models, didn't explore underlying reasons
  - What evidence would resolve it: Systematic study of transferability across wider range of models including architectural differences

## Limitations

- Evaluation methodology relies on simple string matching and regex patterns that may miss nuanced violations
- Red-teaming process represents a finite set of attack strategies that may not capture all possible adversarial approaches
- Results focus on English-language scenarios, limiting multilingual applicability

## Confidence

- **High confidence**: Core finding that LLMs struggle with rule-following under adversarial conditions
- **Medium confidence**: Specific failure rates and performance differences between models
- **Low confidence**: Effectiveness of specific attack strategies beyond tested scenarios, generalizability to real-world applications

## Next Checks

1. **Robustness to Prompt Variations**: Test the same scenarios with multiple prompt formulations and phrasings to establish sensitivity to wording changes and identify the most robust prompt structures for rule-following tasks.

2. **Human Evaluation Correlation**: Conduct a small-scale human review of model responses to validate the accuracy of the programmatic evaluation functions, particularly for edge cases where simple pattern matching might miss nuanced violations.

3. **Cross-Lingual Generalization**: Apply the benchmark to non-English scenarios to test whether rule-following failures are language-specific or reflect deeper limitations in model architecture and training approaches.