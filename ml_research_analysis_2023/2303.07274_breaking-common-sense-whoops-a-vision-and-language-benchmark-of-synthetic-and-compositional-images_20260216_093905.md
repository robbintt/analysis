---
ver: rpa2
title: 'Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic
  and Compositional Images'
arxiv_id: '2303.07274'
source_url: https://arxiv.org/abs/2303.07274
tags:
- image
- images
- weird
- arxiv
- commonsense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WHOOPS!, a new dataset and benchmark for
  visual commonsense reasoning. WHOOPS!
---

# Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images

## Quick Facts
- arXiv ID: 2303.07274
- Source URL: https://arxiv.org/abs/2303.07274
- Reference count: 40
- State-of-the-art models significantly lag behind human performance on WHOOPS! tasks, especially explanation generation

## Executive Summary
This paper introduces WHOOPS!, a new dataset and benchmark for visual commonsense reasoning. WHOOPS! contains 500 synthetic images designed to challenge AI models with unusual, implausible scenarios that violate commonsense expectations. The images were created by human designers using text-to-image generation tools like Midjourney. The dataset includes annotations for four vision-and-language tasks: explanation generation, image captioning, cross-modal matching, and visual question answering. Experiments show that state-of-the-art models like GPT3 and BLIP2 significantly lag behind human performance on all tasks, especially the challenging explanation generation task. The results demonstrate that WHOOPS! is a difficult benchmark that requires models to reason about compositionality and commonsense beyond simple object recognition.

## Method Summary
WHOOPS! is created by human designers using text-to-image generation tools to produce synthetic images that violate commonsense expectations. The dataset includes 500 images with 10,874 annotations covering captions, explanations, underspecified captions, and VQA pairs. Four evaluation tasks are defined: explanation generation (binary accuracy and human rating), image captioning (BLEU-4, CIDEr), cross-modal matching (specificity), and VQA (exact match, BERT Matching). Models are evaluated in both zero-shot and supervised settings, with BLIP2 fine-tuned using 5-fold cross-validation.

## Key Results
- GPT3 and BLIP2 significantly lag behind human performance on all WHOOPS! tasks
- Explanation generation task is particularly challenging, with models achieving only 68% accuracy even with oracle image descriptions
- Models struggle with commonsense reasoning beyond object recognition, showing large gaps between predicted and oracle performance in pipeline approaches
- WHOOPS! effectively challenges models' abilities to reason about compositionality and commonsense violations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WHOOPS! tests models' ability to reason about temporal, physical, and cultural commonsense violations.
- Mechanism: The dataset consists of synthetic images created by human designers using text-to-image models, where designers intentionally modify prompts to create implausible combinations (e.g., Einstein holding a smartphone). Models must identify and explain why these combinations are unusual.
- Core assumption: Human designers can effectively create images that violate commonsense in diverse and challenging ways that models struggle with.
- Evidence anchors:
  - [abstract] "The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney."
  - [section] "Designers create interesting, unusual images using prompt-based image-generation tools like Midjourney."
- Break condition: If text-to-image models cannot generate realistic enough images of implausible scenarios, or if human designers cannot consistently create diverse commonsense violations.

### Mechanism 2
- Claim: The explanation generation task requires models to go beyond object recognition and perform multi-step reasoning.
- Mechanism: Models must identify the commonsense rule being violated (e.g., "smartphones didn't exist when Einstein was alive") and explain the relationships between different elements in the image. This requires connecting visual cues to world knowledge.
- Core assumption: Simple object recognition is insufficient; models need to understand the broader context and temporal/causal relationships.
- Evidence anchors:
  - [abstract] "Connecting visual cues to knowledge about the world goes beyond object recognition, and requires commonsense derived from everyday experiences, physical/social knowledge, and cultural norms."
  - [section] "Although it's relatively easy for humans to identify/explain why an image is unusual, the multi-step reasoning is sophisticated."
- Break condition: If models can rely on surface-level correlations or language priors to generate plausible-sounding but incorrect explanations.

### Mechanism 3
- Claim: WHOOPS! images are challenging because they combine "normal" constituent objects in unusual compositions.
- Mechanism: The images don't contain obviously distorted or fantastical elements, but rather realistic-looking objects placed in implausible scenarios (e.g., a candle in a sealed bottle). This makes it harder for models to rely on simple heuristics.
- Core assumption: Models that perform well on object recognition may struggle when those objects are combined in ways that violate commonsense.
- Evidence anchors:
  - [abstract] "While the images consist of 'normal' constituent objects, compositions make them unusual."
  - [section] "WHOOPS!, is distinct from prior work that focuses on reasoning with pre-existing images. Instead, it contains synthetic images that are specifically designed to challenge AI models' abilities to reason about commonsense and compositionality."
- Break condition: If models can successfully transfer from recognizing objects to understanding their plausible contexts without additional commonsense reasoning training.

## Foundational Learning

- Concept: Commonsense reasoning
  - Why needed here: The dataset is designed to test models' ability to reason about what is plausible in the real world based on everyday knowledge, physical rules, and cultural norms.
  - Quick check question: Can you identify why an image of a fish riding a bicycle is unusual without being told?

- Concept: Compositionality
  - Why needed here: Models must understand how individual objects combine to form a scene, and whether that combination makes sense given real-world constraints.
  - Quick check question: Why is "a penguin swimming in a tropical ocean" unusual even though both penguins and tropical oceans exist?

- Concept: Temporal reasoning
  - Why needed here: Many WHOOPS! images violate temporal commonsense (e.g., historical figures with modern technology). Models need to understand temporal relationships.
  - Quick check question: Why would an image of Shakespeare using a laptop be unusual?

## Architecture Onboarding

- Component map: Text-to-image models (Midjourney, DALL-E, Stable Diffusion) -> Human designers -> Synthetic images -> Crowdsourced annotations -> Evaluation tasks -> Model evaluation
- Critical path:
  1. Designers create "weird" prompts by modifying plausible combinations
  2. Text-to-image models generate synthetic images
  3. Images are annotated with captions and explanations
  4. Evaluation tasks are defined (explanation generation, captioning, matching, VQA)
  5. Models are evaluated on these tasks using both zero-shot and supervised approaches
- Design tradeoffs:
  - Synthetic vs real images: Synthetic allows for controlled creation of challenging examples but may have distribution shift issues
  - Human vs automatic annotation: Human provides high-quality explanations but is expensive; automatic generation is scalable but may have quality issues
  - Task diversity: Multiple tasks test different aspects but may dilute focus on the core explanation generation task
- Failure signatures:
  - Low performance on explanation generation despite high performance on captioning suggests models can recognize objects but not reason about their plausibility
  - High performance on matching but low on VQA suggests models rely on language priors rather than visual understanding
  - Significant gap between oracle and predicted caption performance in pipeline approach indicates recognition is a bottleneck
- First 3 experiments:
  1. Evaluate a simple image captioning model on WHOOPS! to establish baseline object recognition performance
  2. Test a commonsense reasoning model (e.g., pretrained on ATOMIC) on the explanation generation task to see if external knowledge helps
  3. Compare performance on WHOOPS! vs a dataset of real unusual images to assess whether synthetic nature of WHOOPS! is the primary challenge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper bound on model performance for the explanation generation task, and how can it be improved?
- Basis in paper: explicit
- Why unresolved: The paper shows that even with a ground-truth oracle image description, models still struggle to effectively explain the incongruity of the scene, achieving only 68% accuracy. This suggests that the upper bound on model performance for this task is not yet known.
- What evidence would resolve it: Further experiments with larger models, more training data, or different architectures could help determine the upper bound on model performance for the explanation generation task.

### Open Question 2
- Question: How do cultural differences impact the perception of "weirdness" in the WHOOPS! dataset?
- Basis in paper: inferred
- Why unresolved: The paper mentions that the designers and annotators come from different countries and continents to mitigate the concern that weirdness is limited to a specific culture or region. However, it is not clear how cultural differences impact the perception of "weirdness" in the dataset.
- What evidence would resolve it: Further experiments with annotators from different cultural backgrounds could help determine how cultural differences impact the perception of "weirdness" in the WHOOPS! dataset.

### Open Question 3
- Question: How can the WHOOPS! dataset be expanded to include more diverse and challenging images?
- Basis in paper: explicit
- Why unresolved: The paper mentions that the dataset contains 500 images and plans to expand it in the future. However, it is not clear how the dataset can be expanded to include more diverse and challenging images.
- What evidence would resolve it: Further experiments with different types of images, such as more complex scenes or images with multiple weird elements, could help determine how the WHOOPS! dataset can be expanded to include more diverse and challenging images.

## Limitations

- The synthetic nature of the dataset raises questions about domain adaptation - models trained on real images may struggle with the stylistic differences in generated images
- The human-designed prompts may introduce bias toward certain types of commonsense violations that don't represent the full space of possible violations
- The evaluation relies heavily on automatic metrics for some tasks, which may not fully capture the quality of commonsense reasoning

## Confidence

- High confidence in the dataset creation methodology and task definitions
- Medium confidence in the generalizability of results to real-world commonsense reasoning
- Low confidence in the completeness of the commonsense violation coverage due to the human design process

## Next Checks

1. Test whether models fine-tuned on WHOOPS! show improved performance on real-world commonsense reasoning benchmarks like VCR or HellaSwag
2. Conduct an ablation study removing the synthetic image generation step to isolate whether the challenge comes from the generation process or the content itself
3. Evaluate the same models on a subset of WHOOPS! images with different artistic styles (e.g., photorealistic vs. artistic) to assess style sensitivity