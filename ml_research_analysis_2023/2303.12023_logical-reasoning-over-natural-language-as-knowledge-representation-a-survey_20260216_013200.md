---
ver: rpa2
title: 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey'
arxiv_id: '2303.12023'
source_url: https://arxiv.org/abs/2303.12023
tags:
- reasoning
- language
- methods
- knowledge
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews logical reasoning over natural
  language as knowledge representation (LRNL), a new paradigm that uses natural language
  as knowledge representation and pretrained language models as reasoners, including
  deductive, inductive, and abductive reasoning. The paper defines and categorizes
  logical reasoning, highlights advantages over formal language and existing neuro-symbolic
  systems, and provides an overview of benchmarks, methods, challenges, and future
  directions.
---

# Logical Reasoning over Natural Language as Knowledge Representation: A Survey

## Quick Facts
- arXiv ID: 2303.12023
- Source URL: https://arxiv.org/abs/2303.12023
- Reference count: 40
- This survey reviews LRNL, a new paradigm using natural language as knowledge representation with PLMs as reasoners

## Executive Summary
This survey comprehensively reviews logical reasoning over natural language as knowledge representation (LRNL), a new paradigm that uses natural language as knowledge representation and pretrained language models as reasoners. The paper defines and categorizes logical reasoning, highlights advantages over formal language and existing neuro-symbolic systems, and provides an overview of benchmarks, methods, challenges, and future directions. LRNL systematically has advantages over end-to-end neural methods, such as interpretability and more controllability, while alleviating many challenges of formal representation systems.

## Method Summary
The survey covers three main reasoning types: deductive reasoning (hypothesis classification, proof generation, proof generation with incomplete information, and implication enumeration), inductive reasoning (rule classification and rule generation), and abductive reasoning (explanation classification and explanation generation). LRNL systems use PLMs to perform logical reasoning over natural language texts, with different methods for different reasoning types. The critical path involves retrieving relevant premises, performing inference steps, verifying inferences, and adding results to proof trees. Methods in stage 3 can be summarized as utilizing explicit verifiers (implemented with a PLM) to check the validity of each inference step.

## Key Results
- LRNL alleviates brittleness and knowledge-acquisition bottlenecks present in formal representation systems
- PLMs contain implicit knowledge learned from large-scale web corpora, enabling reasoning even when explicit knowledge is missing
- LRNL enables collaboration between different reasoning types (deductive, inductive, abductive) to solve complex problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural language as knowledge representation combined with PLMs as reasoners alleviates brittleness and knowledge-acquisition bottlenecks present in formal representation systems.
- Mechanism: PLMs contain implicit knowledge learned from large-scale web corpora, enabling them to reason even when explicit knowledge is missing in the knowledge base. Natural language handling allows direct processing of raw text inputs without manual encoding.
- Core assumption: PLMs have learned sufficient general knowledge during pretraining to support logical reasoning tasks without complete formal representations.
- Evidence anchors:
  - [abstract] "This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods."
  - [section 3.1] "PLMs contain knowledge themselves (Davison et al., 2019), which makes it possible for them to provide good answers even when some required explicit knowledge is not present in a knowledge base"
- Break condition: If PLMs lack the specific domain knowledge required for a reasoning task, or if the reasoning requires precision that natural language ambiguity cannot provide.

### Mechanism 2
- Claim: LRNL systems achieve better interpretability and controllability compared to end-to-end neural methods by generating explicit reasoning proofs.
- Mechanism: By constructing proof trees step-by-step and verifying each inference, LRNL provides traceable reasoning paths that explain conclusions. The explicit knowledge base allows controlled reasoning within defined rules.
- Core assumption: Stepwise proof generation and verification using PLMs can produce reliable, interpretable reasoning traces.
- Evidence anchors:
  - [abstract] "LRNL systematically has some advantages over end-to-end neural methods, such as interpretability (Cambria et al., 2023) (since its stepwise reasoning nature), more controllability"
  - [section 4.2.2] "Methods in stage 3 can be summarized as utilizing explicit verifiers (implemented with a PLM) to check the validity of each inference step"
- Break condition: If PLM-generated proofs contain hallucinations or if verification modules cannot reliably detect reasoning errors.

### Mechanism 3
- Claim: LRNL enables collaboration between different reasoning types (deductive, inductive, abductive) to solve complex problems.
- Mechanism: Different reasoning modules can be combined - for example, using inductive reasoning to build a large rule base that serves as theory for deductive reasoning, or using abductive reasoning to explain observations within deductive frameworks.
- Core assumption: Different reasoning types can be modularly integrated within LRNL frameworks to handle multi-step, complex reasoning tasks.
- Evidence anchors:
  - [section 8] "Multiple reasoning types can be used together for complex tasks. Existing works only utilize deductive reasoning with abductive reasoning to create a proof tree"
  - [section 3.3] "As a NeSy method, LRNL systematically has some advantages over end-to-end neural methods, such as interpretability, more controllability, and less catastrophic forgetting"
- Break condition: If the integration of different reasoning types creates inconsistencies or if the computational cost becomes prohibitive.

## Foundational Learning

- Concept: Three types of logical reasoning (deductive, inductive, abductive)
  - Why needed here: Understanding the distinction between reasoning types is essential for designing appropriate LRNL systems and benchmarks
  - Quick check question: Can you explain the difference between deductive reasoning (conclusive support) and inductive reasoning (non-conclusive generalizations)?

- Concept: Neuro-symbolic computing and its advantages
  - Why needed here: LRNL is positioned as a new type of neuro-symbolic method, so understanding this paradigm is crucial
  - Quick check question: What are the key advantages of neuro-symbolic approaches over pure symbolic or pure neural methods?

- Concept: PLM capabilities and limitations for reasoning
  - Why needed here: PLMs are the core reasoners in LRNL, so understanding their strengths and weaknesses is fundamental
  - Quick check question: What are the main challenges in using PLMs as robust deductive reasoners?

## Architecture Onboarding

- Component map: Knowledge base (natural language facts and rules) -> PLM-based inference engine -> optional verification modules -> reasoning controllers for multi-step tasks
- Critical path: For proof generation: retrieve relevant premises → perform inference step → verify inference → add to proof tree → repeat until conclusion reached
- Design tradeoffs: Natural language flexibility vs. precision, PLM computational cost vs. formal representation efficiency, modularity vs. end-to-end performance
- Failure signatures: Hallucinated proofs, inability to handle complex multi-hop reasoning, poor generalization to out-of-distribution examples, computational inefficiency for complex tasks
- First 3 experiments:
  1. Test hypothesis classification on D* dataset using fine-tuned RoBERTa to establish baseline PLM reasoning capability
  2. Implement simple proof generation with embedding-based node and edge classification to verify stepwise reasoning works
  3. Add verification module to existing proof generation system and measure improvement in proof quality

## Open Questions the Paper Calls Out

- Question: Can natural language be used to represent probabilistic knowledge and reasoning in a way that is both interpretable and computationally efficient?
  - Basis in paper: [explicit] The paper discusses the advantages of using natural language as knowledge representation over formal language, but does not address the challenge of representing and reasoning with probabilistic knowledge in natural language.
  - Why unresolved: The paper mentions the potential of LRNL to handle uncertainty, but does not explore how natural language can be used to represent and reason with probabilistic knowledge in a way that is both interpretable and computationally efficient.
  - What evidence would resolve it: Research demonstrating methods for representing and reasoning with probabilistic knowledge in natural language, along with empirical evaluations of their interpretability and computational efficiency.

- Question: How can LRNL systems be designed to handle complex reasoning tasks that require multiple reasoning steps and the integration of knowledge from different domains?
  - Basis in paper: [inferred] The paper discusses the potential of LRNL to handle complex reasoning tasks, but does not address the challenges of designing systems that can effectively integrate knowledge from different domains and perform multi-step reasoning.
  - Why unresolved: The paper highlights the advantages of LRNL over end-to-end neural methods, but does not explore the specific challenges and solutions for designing LRNL systems that can handle complex reasoning tasks.
  - What evidence would resolve it: Research demonstrating methods for designing LRNL systems that can effectively integrate knowledge from different domains and perform multi-step reasoning, along with empirical evaluations of their performance on complex reasoning tasks.

- Question: How can LRNL systems be evaluated in a way that is both comprehensive and fair, considering the diversity of reasoning tasks and the potential for bias in automatic evaluation metrics?
  - Basis in paper: [explicit] The paper discusses the challenges of building larger benchmarks and the potential for bias in automatic evaluation metrics, but does not propose specific methods for evaluating LRNL systems in a comprehensive and fair manner.
  - Why unresolved: The paper highlights the importance of building larger benchmarks and the potential for bias in automatic evaluation metrics, but does not provide concrete solutions for addressing these challenges.
  - What evidence would resolve it: Research demonstrating methods for evaluating LRNL systems in a comprehensive and fair manner, along with empirical evaluations of the proposed methods on a diverse set of reasoning tasks.

## Limitations

- Limited empirical validation across diverse domains - most reported results focus on benchmark datasets rather than real-world applications
- Computational efficiency claims remain theoretical without specific runtime comparisons
- Effectiveness of verification mechanisms in preventing PLM hallucinations is not thoroughly evaluated

## Confidence

- High confidence: The categorization of logical reasoning types (deductive, inductive, abductive) and their respective task formulations
- Medium confidence: Claims about interpretability and controllability advantages over end-to-end methods, based on mechanism description rather than extensive empirical validation
- Low confidence: Computational efficiency comparisons with formal representation systems due to lack of specific performance metrics

## Next Checks

1. Conduct systematic ablation studies comparing LRNL with both formal representation systems and pure neural approaches across multiple reasoning benchmarks to quantify claimed advantages
2. Evaluate verification module effectiveness by measuring reduction in hallucinated proofs through controlled experiments with known ground truth
3. Test LRNL systems on out-of-distribution examples and complex multi-hop reasoning tasks to assess generalization capabilities beyond benchmark datasets