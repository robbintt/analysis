---
ver: rpa2
title: Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain
  Dialogue Systems at DSTC 11 Track 4
arxiv_id: '2306.12794'
source_url: https://arxiv.org/abs/2306.12794
tags:
- dialogue
- evaluation
- turn
- language
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive overview of Track 4 on "Robust
  and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems"
  organized as part of the 11th Dialogue System Technology Challenge (DSTC11). The
  track was divided into two subtasks addressing an important problem in Dialogue
  Systems: the design of automatic evaluation metrics to evaluate multilingual dialogues
  and dialogue robustness when dealing with paraphrases or back-translations.'
---

# Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4

## Quick Facts
- arXiv ID: 2306.12794
- Source URL: https://arxiv.org/abs/2306.12794
- Reference count: 22
- Primary result: Automatic evaluation metrics for multilingual dialogues show satisfactory but suboptimal performance, with correlation scores below 0.7 even for the best systems.

## Executive Summary
This paper presents the results of Track 4 on "Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems" at DSTC11. The track addressed two key challenges: designing automatic evaluation metrics for multilingual dialogues and ensuring robustness when dealing with paraphrases or back-translations. The competition featured 5 teams at the turn level and 3 teams at the dialogue level, with some teams outperforming baseline models globally and at language-specific levels. Despite progress, the overall performance indicates that automatic evaluation remains an open problem, with correlation scores still below 0.7 in the best cases.

## Method Summary
The track used multiple datasets including CHANEL, DSTC10, and CDIAL in English, Spanish, and Chinese. A baseline multilingual AM-FM model was provided using XLM-R for semantic embeddings and GPT-2 for conditional probability scoring. Participants could propose their own metrics or improve the baseline. Evaluation used Spearman's correlation with human judgments across multiple dimensions (Appropriateness, Content Richness, Grammatical Correctness, Relevance, Coherence, Engageness/Likeability, Informativeness, Overall). The competition was structured at both turn and dialogue levels, with separate evaluations for each language.

## Key Results
- 5 teams actively participated at the turn level and 3 teams at the dialogue level
- Some teams outperformed the baseline model both globally and at language-specific levels
- Performance varied significantly across languages, with Chinese showing notably lower correlation scores
- Best correlation scores remained below 0.7, indicating automatic evaluation remains challenging
- Turn-level evaluations generally showed higher correlation scores than dialogue-level evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual automatic evaluation metrics improve by training on back-translated and translated dialogues across English, Spanish, and Chinese.
- Mechanism: The approach leverages high-quality neural machine translation to create parallel datasets in multiple languages. By training evaluation metrics on these multilingual datasets, the model learns language-agnostic representations of dialogue quality that generalize across linguistic boundaries.
- Core assumption: High-quality translation services preserve semantic meaning sufficiently for training dialogue evaluation metrics.
- Evidence anchors:
  - [abstract] "Using existing high-quality services and models, it is possible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics."
  - [section 2.1] "An additional advantage of the data in this cluster is that they have been automatically translated back and forth using the same high-quality MS Azure translation service."
- Break condition: If translation quality degrades significantly or introduces systematic semantic distortions, the learned representations may become language-specific rather than language-agnostic.

### Mechanism 2
- Claim: Model-based metrics (like BERTscore, BLEURT) outperform word-overlap metrics by capturing semantic and pragmatic information in dialogues.
- Mechanism: These metrics use pre-trained transformer language models to extract deep semantic representations of dialogue context and responses, computing similarity scores that better align with human judgments of quality.
- Core assumption: Pre-trained transformer models capture sufficient semantic and pragmatic information relevant to dialogue evaluation.
- Evidence anchors:
  - [abstract] "BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong semantic representation capability of pre-trained transformer language models, perform the evaluation at semantic and partially pragmatic levels."
  - [section 2.3] "For the adequacy metric (AM), we use XLM-R to extract sentence-level embeddings of both the response and the last sentence in the corresponding dialogue context."
- Break condition: If the pre-trained models don't capture dialogue-specific phenomena or if fine-tuning doesn't adequately adapt to dialogue evaluation tasks.

### Mechanism 3
- Claim: Robustness to paraphrases and back-translations improves metric generalization across semantically equivalent but syntactically different responses.
- Mechanism: By training and evaluating metrics on both original and paraphrased/back-translated sentences, the system learns to recognize semantically equivalent variations, making it robust to diverse human expressions.
- Core assumption: Paraphrased and back-translated sentences maintain semantic equivalence to originals while introducing syntactic diversity.
- Evidence anchors:
  - [section 1] "robustness when dealing with paraphrases or back-translations"
  - [section 3.1] "For creating semantically similar sentences, we relied on two options: back-translations and a paraphraser model."
- Break condition: If paraphrases/back-translations introduce semantic drift or if the quality of generated variations is inconsistent across different input types.

## Foundational Learning

- Concept: Spearman's correlation coefficient
  - Why needed here: Used to measure agreement between metric scores and human annotations across different languages and dialogue qualities.
  - Quick check question: What does a Spearman correlation of 0.4 indicate about the relationship between two ranked variables?

- Concept: Transformer-based language models
  - Why needed here: The backbone architecture for modern automatic evaluation metrics, providing semantic representations.
  - Quick check question: What is the key difference between BERT and XLM-R that makes XLM-R suitable for multilingual tasks?

- Concept: Back-translation and paraphrasing
  - Why needed here: Techniques for creating semantically equivalent but syntactically varied dialogue data to test metric robustness.
  - Quick check question: Why might back-translations be too similar to originals to effectively test robustness?

## Architecture Onboarding

- Component map: Data pipeline (original → translated/back-translated → annotated) → Feature extraction (XLM-R, GPT-2) → Metric scoring (cosine similarity, conditional probability) → Correlation evaluation
- Critical path: Translation quality → Feature extraction → Score computation → Human correlation validation
- Design tradeoffs: Multilingual models vs. language-specific models (coverage vs. performance), back-translation vs. paraphrasing (quality vs. diversity)
- Failure signatures: Low correlation with human judgments, inconsistent performance across languages, sensitivity to syntactic variations
- First 3 experiments:
  1. Evaluate baseline XLM-R + GPT-2 metric on English-only data to establish performance ceiling
  2. Test metric performance on back-translated vs. original data to quantify robustness
  3. Compare multilingual vs. monolingual models on each target language to identify performance gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multilingual automatic evaluation metrics for open-domain dialogue systems be designed to perform consistently across languages with vastly different linguistic structures?
- Basis in paper: [explicit] The paper discusses challenges in multilingual evaluation, noting disparate performance among different languages.
- Why unresolved: Current metrics show inconsistent performance across languages, particularly struggling with Chinese compared to Spanish or English.
- What evidence would resolve it: Comparative analysis of metric performance across multiple languages with varying linguistic structures, and identification of specific linguistic features causing performance disparities.

### Open Question 2
- Question: What techniques can improve the robustness of automatic evaluation metrics when dealing with paraphrased or back-translated dialogue responses?
- Basis in paper: [explicit] The paper highlights the challenge of maintaining evaluation consistency when dealing with paraphrases and back-translations.
- Why unresolved: Current metrics struggle to maintain high correlation with human annotations when evaluating paraphrased or back-translated responses.
- What evidence would resolve it: Development and testing of novel approaches that successfully maintain high correlation with human judgments across original and paraphrased/back-translated responses.

### Open Question 3
- Question: How can automatic evaluation metrics be made more explainable to provide constructive feedback to dialogue generation models?
- Basis in paper: [explicit] The paper emphasizes the need for explainable metrics that provide explicit feedback to generative models.
- Why unresolved: While some metrics exist, they often lack the ability to provide detailed, actionable feedback about specific quality issues in generated responses.
- What evidence would resolve it: Creation of evaluation metrics that can identify and explain specific quality issues (e.g., contradictions, lack of coherence) in generated responses, with validation through human studies.

## Limitations

- Performance disparities across languages, particularly poor performance on Chinese compared to English and Spanish
- Lack of systematic validation that back-translated and paraphrased sentences maintain semantic equivalence
- Uncertainty about whether multilingual training improves performance due to better representations or simply increased dataset size

## Confidence

- High confidence: The experimental methodology is well-documented with clear evaluation protocols, standardized datasets, and established correlation metrics. The results show consistent patterns across multiple participating teams, with Spearman correlations ranging from 0.2 to 0.6, clearly indicating room for improvement in automatic dialogue evaluation.
- Medium confidence: The claim that multilingual models trained on translated data improve evaluation performance is supported by the data, but the paper doesn't isolate whether improvements come from multilingual training or from the increased dataset size. The performance disparities across languages (English performing better than Chinese) suggest implementation factors may play a significant role.
- Low confidence: The effectiveness of back-translation and paraphrasing as robustness tests is assumed rather than empirically validated. The paper acknowledges that back-translations might be "too similar to originals," but doesn't provide quantitative analysis of semantic drift or systematic differences between original and paraphrased data.

## Next Checks

1. **Translation quality validation**: Replicate the study using human-verified translations alongside automatic translations to isolate the impact of translation quality on evaluation metric performance.

2. **Ablation study on data sources**: Train identical models on original data only, translated data only, and combined data to quantify the specific contribution of multilingual training versus data augmentation.

3. **Semantic drift analysis**: Conduct controlled experiments measuring semantic similarity between original and paraphrased/back-translated sentences to validate whether the robustness testing methodology provides meaningful variation.