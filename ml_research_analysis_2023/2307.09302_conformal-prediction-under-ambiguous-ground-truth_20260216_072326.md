---
ver: rpa2
title: Conformal prediction under ambiguous ground truth
arxiv_id: '2307.09302'
source_url: https://arxiv.org/abs/2307.09302
tags:
- coverage
- conformal
- prediction
- labels
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses conformal prediction under ambiguous ground
  truth labels in safety-critical classification tasks. The authors propose two approaches:
  Monte Carlo conformal prediction, which samples pseudo-labels from plausibilities
  to calibrate confidence sets, and plausibility regions, which provide expected coverage
  guarantees by considering the distribution of plausible labels.'
---

# Conformal prediction under ambiguous ground truth

## Quick Facts
- arXiv ID: 2307.09302
- Source URL: https://arxiv.org/abs/2307.09302
- Reference count: 40
- Key outcome: Monte Carlo conformal prediction achieves 73% coverage (target 73%) compared to standard conformal prediction's 61% on dermatology dataset, at cost of larger confidence sets

## Executive Summary
This paper addresses conformal prediction under ambiguous ground truth labels in safety-critical classification tasks. The authors propose two approaches: Monte Carlo conformal prediction, which samples pseudo-labels from plausibilities to calibrate confidence sets, and plausibility regions, which provide expected coverage guarantees by considering the distribution of plausible labels. Applied to a dermatology dataset with expert-annotated skin conditions, Monte Carlo conformal prediction achieves 73% coverage (target 73%) compared to standard conformal prediction's 61%, at the cost of larger confidence sets (inefficiency 4.57 vs 2.66). The method effectively captures uncertainty from expert disagreements, closing the coverage gap observed when calibrating against deterministic top-1 labels.

## Method Summary
The paper introduces Monte Carlo conformal prediction that samples pseudo-labels from plausibility distributions to calibrate confidence sets when ground truth is ambiguous. Instead of using hard labels, the method draws multiple labels per calibration example from the plausibility distribution, making calibration data exchangeable. The expected conformity score is computed as a weighted average where weights come from plausibilities. For combining p-values from multiple samples, an ECDF correction method is proposed to restore proper coverage guarantees. The approach also introduces plausibility regions that provide expected coverage guarantees by considering the distribution of plausible labels rather than hard labels.

## Key Results
- Monte Carlo conformal prediction achieves 73% coverage on dermatology dataset versus 61% for standard conformal prediction
- Expected coverage against plausibilities improves from 69% to 88% using Monte Carlo approach
- Inefficiency increases from 2.66 to 4.57 when accounting for ambiguity
- Theoretical coverage guarantee is 1-2α but empirical results show near 1-α coverage for m ≥ 2

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Monte Carlo conformal prediction guarantees expected coverage against ambiguous ground truth by sampling pseudo-labels from plausibilities.
- **Mechanism**: Sampling multiple labels per calibration example from the plausibility distribution creates exchangeable calibration data despite the original labels being non-exchangeable. This allows standard conformal prediction to be applied while capturing uncertainty from expert disagreements.
- **Core assumption**: The plausibility distribution accurately approximates the true posterior label distribution given the input.
- **Evidence anchors**:
  - [abstract] "Monte Carlo conformal prediction, which samples pseudo-labels from plausibilities to calibrate confidence sets"
  - [section 3.4] "we sample m labels per calibration example (Xi) where P(Yj i = k) = λik and duplicate the corresponding inputs"
  - [corpus] No direct evidence - weak anchor

### Mechanism 2
- **Claim**: Expected conformity scores capture ambiguity by weighting class conformity by plausibility mass.
- **Mechanism**: Instead of using hard labels, conformity scores are computed as weighted averages where weights come from plausibilities. This creates a score distribution that reflects the underlying uncertainty.
- **Core assumption**: The plausibility distribution provides meaningful weights for each class probability.
- **Evidence anchors**:
  - [section 3.2] "we consider the 'expected' conformity score e(x, λ) := Σk λk E(x,k)"
  - [section 3.1] "we use here a statistical model to obtain the plausibilities λy approximating P(Y = y|X = x)"
  - [corpus] Weak - no direct evidence about effectiveness of expected scores

### Mechanism 3
- **Claim**: ECDF correction of p-values restores 1-α coverage from the degraded 1-2α guarantee.
- **Mechanism**: By estimating the cumulative distribution function of averaged p-values using a split of calibration data, we can correct biased p-values back to uniform distribution, restoring proper coverage guarantees.
- **Core assumption**: The calibration examples are exchangeable and can be split to estimate the ECDF reliably.
- **Evidence anchors**:
  - [section 3.4.2] "we follow here a method that directly estimates the cumulative distribution function (CDF) of the combined, e.g., averaged p-values"
  - [section 3.4.2] "As ¯FY is not the true CDF F but an empirical CDF (ECDF) estimate, we only obtain an approximate coverage guarantee"
  - [corpus] Weak - no direct evidence about ECDF correction effectiveness

## Foundational Learning

- **Concept**: Exchangeability in conformal prediction
  - **Why needed here**: Conformal prediction requires exchangeable calibration and test examples to guarantee coverage. Understanding when this assumption breaks is crucial for applying Monte Carlo methods.
  - **Quick check question**: What happens to coverage guarantees when calibration examples are duplicated or augmented with correlated versions?

- **Concept**: Plausibility distributions from expert annotations
  - **Why needed here**: The entire framework depends on converting expert disagreement into probability distributions. Understanding how to model and infer these is foundational.
  - **Quick check question**: How would you model plausibilities if experts provide ordinal rankings instead of discrete labels?

- **Concept**: Coverage vs efficiency tradeoff
  - **Why needed here**: Taking ambiguity into account typically increases confidence set size (inefficiency) to maintain coverage. Understanding this tradeoff is essential for practical deployment.
  - **Quick check question**: Why does Monte Carlo conformal prediction typically produce larger confidence sets than standard conformal prediction on unambiguous data?

## Architecture Onboarding

- **Component map**: Plausibility model → Monte Carlo sampler → Conformity scorer → ECDF estimator → Confidence set reducer

- **Critical path**: Plausibility model → Monte Carlo sampling → Conformity scoring → Threshold calibration → Confidence set construction

- **Design tradeoffs**:
  - Single vs multiple samples (m): More samples reduce variance but increase computation
  - Expected vs standard conformity scores: Expected scores capture ambiguity but may be harder to compute
  - ECDF correction vs simple averaging: ECDF provides better guarantees but requires data splitting

- **Failure signatures**:
  - Undercoverage indicates plausibility model misspecification
  - High inefficiency suggests excessive uncertainty capture
  - Instability across runs indicates insufficient samples or poor convergence

- **First 3 experiments**:
  1. Verify exchangeability assumption by testing coverage on synthetic ambiguous data with known ground truth
  2. Compare efficiency of expected vs standard conformity scores on moderately ambiguous datasets
  3. Test ECDF correction effectiveness by measuring coverage improvement when using multiple samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between the expected coverage guarantee of Monte Carlo conformal prediction and the practical coverage observed when m >> 2?
- Basis in paper: [explicit] The paper states that Monte Carlo conformal prediction guarantees coverage of 1-2α, but empirically achieves coverage close to 1-α for m ≥ 2, with no statistically significant coverage gap observed in experiments.
- Why unresolved: The theoretical coverage guarantee of 1-2α does not match the empirical observations of near 1-α coverage. The paper notes this gap between theory and practice but does not provide a formal explanation for why the empirical results consistently outperform the theoretical bound.
- What evidence would resolve it: A rigorous theoretical analysis proving why Monte Carlo conformal prediction achieves 1-α coverage in practice for m ≥ 2, or a formal characterization of the conditions under which the 1-2α bound can be tightened.

### Open Question 2
- Question: How does the choice of aggregation model p(λ|b,x) affect the accuracy of plausibility estimates and subsequent conformal prediction performance?
- Basis in paper: [explicit] The paper mentions that plausibility estimates are obtained using a statistical model from (Xi,Bi) pairs, and provides examples of how p(λ|b,x) can be modeled (e.g., using a Dirichlet distribution for Bayesian inference), but does not systematically evaluate different aggregation models.
- Why unresolved: The paper uses a specific aggregation model for the dermatology dataset but does not explore how different choices of p(λ|b,x) (e.g., Bayesian vs. maximum likelihood, different prior distributions) affect the quality of plausibility estimates and the resulting conformal prediction performance.
- What evidence would resolve it: Empirical comparisons of conformal prediction performance using different aggregation models (e.g., maximum likelihood vs. Bayesian with different priors) on datasets with known ground truth ambiguity.

### Open Question 3
- Question: What is the optimal reduction strategy Ψ for transforming plausibility regions into confidence sets that balances coverage guarantees with efficiency?
- Basis in paper: [explicit] The paper introduces several reduction strategies (Ψ1, Ψ2, Ψ3, Ψ4) for converting plausibility regions to confidence sets, but notes that these reductions often result in larger confidence sets and does not provide a systematic comparison of their trade-offs.
- Why unresolved: While the paper provides examples of different reduction strategies, it does not evaluate their relative performance in terms of coverage guarantees, confidence set size (inefficiency), and computational cost, leaving open the question of which reduction is optimal for different application scenarios.
- What evidence would resolve it: Systematic experiments comparing the performance of different reduction strategies (Ψ1-Ψ4) on multiple datasets with varying degrees of label ambiguity, measuring coverage, inefficiency, and computational cost.

## Limitations
- Coverage guarantees fundamentally rely on accurate plausibility models approximating true posterior distributions
- Monte Carlo approach introduces computational overhead and variance that depends on sample size m
- ECDF correction method effectiveness is not well-validated empirically
- Improvement in coverage comes at substantial cost of increased confidence set size

## Confidence
- **High confidence**: The theoretical framework connecting exchangeability, conformal prediction, and expected coverage is sound
- **Medium confidence**: Empirical results on dermatology dataset demonstrate approach works in practice
- **Low confidence**: Effectiveness of ECDF correction method is not well-validated empirically

## Next Checks
1. Test coverage on synthetic ambiguous datasets where ground truth is known but expert annotations follow controlled noise patterns to validate the plausibility model assumption
2. Evaluate the sensitivity of Monte Carlo conformal prediction to plausibility model misspecification by comparing coverage when using accurate vs. corrupted plausibility distributions
3. Benchmark the ECDF correction method against alternative p-value combination strategies (e.g., Bonferroni correction, Fisher's method) to assess its relative effectiveness