---
ver: rpa2
title: Fine-Tuned but Zero-Shot 3D Shape Sketch View Similarity and Retrieval
arxiv_id: '2306.08541'
source_url: https://arxiv.org/abs/2306.08541
tags:
- sketches
- shape
- retrieval
- sketch
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores zero-shot sketch-based 3D shape retrieval,
  focusing on matching 2D sketch views of individual 3D instances. The authors investigate
  the performance of pretrained encoders like ViT and ResNet in quantifying similarity
  between sketch views, especially under varying object scales and sketching styles.
---

# Fine-Tuned but Zero-Shot 3D Shape Sketch View Similarity and Retrieval

## Quick Facts
- **arXiv ID**: 2306.08541
- **Source URL**: https://arxiv.org/abs/2306.08541
- **Reference count**: 17
- **Primary result**: Fine-tuning on synthetic sketches improves zero-shot sketch-based 3D shape retrieval accuracy, with optimal performance when object scales are similar and using mid-level feature layers.

## Executive Summary
This paper explores zero-shot sketch-based 3D shape retrieval, focusing on matching 2D sketch views of individual 3D instances. The authors investigate the performance of pretrained encoders like ViT and ResNet in quantifying similarity between sketch views, especially under varying object scales and sketching styles. They propose fine-tuning strategies on synthetic sketches and evaluate retrieval accuracy across different feature layers. Results show that fine-tuning improves performance over naive zero-shot methods, with optimal results achieved when object scales in sketches are similar. The study highlights the importance of layer selection and fine-tuning in adapting large pretrained models for sketch understanding tasks.

## Method Summary
The method uses pretrained encoders (ViT or ResNet) to extract features from both synthetic and freehand sketches, then computes cosine similarity between sketch and 3D view embeddings for retrieval. The key innovation is fine-tuning these models on synthetic sketches of one shape class using contrastive cross-entropy loss, with data augmentation. The approach evaluates different feature layers and object scales to optimize retrieval accuracy. The system is tested on the AmateurSketch dataset with chair and lamp categories, using synthetic sketches generated in NPR and Anime styles.

## Key Results
- Fine-tuning on synthetic sketches of one shape class improves zero-shot retrieval accuracy on other shape classes, reaching or surpassing supervised methods
- Similar object scales in sketches lead to better feature similarity matching across different sketch styles
- Mid-level feature layers (ViT layer 6, ResNet layer 3) provide optimal balance between semantic and spatial information for sketch matching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on synthetic sketches of one shape class improves zero-shot retrieval accuracy on other shape classes.
- Mechanism: The fine-tuning process adapts the pretrained model's feature representations to better capture sketch-specific patterns while preserving the general shape understanding learned from large-scale pretraining.
- Core assumption: The feature representations learned from one shape class transfer effectively to other shape classes due to shared geometric and structural properties across 3D objects.
- Evidence anchors:
  - [abstract]: "One of the key findings of our research is that meticulous fine-tuning on one class of 3D shapes can lead to improved performance on other shape classes, reaching or surpassing the accuracy of supervised methods."
  - [section]: "We find that on our task of comparing sketches in different styles, more advanced fine-tuning strategies are inferior to fine-tuning with a small learning rate. Full model fine-tuning leverages pretrained model weights but allows us to achieve much higher performance on the sketch domain in a zero-shot scenario: we train on one class and test on sketches of other shape classes."
  - [corpus]: Weak corpus evidence - the related papers focus on different aspects of sketch-based retrieval but don't directly address cross-class generalization through fine-tuning.
- Break condition: If the pretraining task and dataset were too domain-specific, the transferred knowledge might not generalize well to other shape classes, leading to performance degradation.

### Mechanism 2
- Claim: Similar object scales in sketches lead to better feature similarity matching across different sketch styles.
- Mechanism: When objects are scaled to occupy similar bounding box areas in different sketch styles, the relative spatial relationships and proportions are preserved, making it easier for the model to identify matching features across domains.
- Core assumption: The scale normalization ensures that geometric relationships within sketches are preserved across different rendering styles, making cross-domain matching more reliable.
- Evidence anchors:
  - [abstract]: "We study how the scale of an object in a sketch affects the similarity of features at different network layers. We observe that depending on the scale, different feature layers can be more indicative of shape similarities in sketch views."
  - [section]: "Fig. 3 shows that on the two categories of the dataset of freehand sketches features from mid-layers – ViT layer 6 and ResNet layer 3 – result in the best performance for both architectures. On synthetic sketches, the larger the object in a sketch is, the more accurate is the prediction."
  - [corpus]: Weak corpus evidence - related works don't specifically address the impact of object scale normalization on cross-domain feature matching.
- Break condition: If the object scale varies significantly between query sketches and reference sketches, the spatial relationships may be distorted, leading to poor matching performance regardless of the feature layer used.

### Mechanism 3
- Claim: Mid-level feature layers (e.g., ViT layer 6, ResNet layer 3) provide optimal balance between semantic and spatial information for sketch matching.
- Mechanism: These layers capture enough semantic information to distinguish between different object classes while retaining sufficient spatial detail to handle the abstract nature of sketches and variations in sketching styles.
- Core assumption: The hierarchical nature of deep networks means that mid-level layers strike the right balance between abstract semantic features and detailed spatial information for this task.
- Evidence anchors:
  - [abstract]: "We show that in a zero-shot setting, the more abstract the sketch, the higher the likelihood of incorrect image matches. Even within the same sketch domain, sketches of the same object drawn in different styles, for example by distinct individuals, might not be accurately matched."
  - [section]: "Fig. 3 shows that on the two categories of the dataset of freehand sketches features from mid-layers – ViT layer 6 and ResNet layer 3 – result in the best performance for both architectures."
  - [corpus]: Weak corpus evidence - while related works discuss feature selection for retrieval, they don't specifically analyze the optimal layer selection for cross-domain sketch matching.
- Break condition: If the task required either very high-level semantic understanding (e.g., fine-grained classification) or very low-level spatial detail (e.g., precise geometric matching), the mid-level layers might not provide the optimal features.

## Foundational Learning

- Concept: Cross-modal representation learning
  - Why needed here: The method bridges the gap between visual representations learned from natural images and the abstract, sparse representations in sketches, requiring understanding of how to align different modalities.
  - Quick check question: How does contrastive learning help in aligning representations from different domains (e.g., sketches and 3D renderings)?

- Concept: Feature hierarchy in deep networks
  - Why needed here: Understanding how different network layers capture different levels of abstraction is crucial for selecting the optimal layers for sketch-based retrieval.
  - Quick check question: Why might lower layers be better for capturing spatial details while higher layers capture more semantic information?

- Concept: Domain adaptation through fine-tuning
  - Why needed here: The method leverages fine-tuning to adapt pretrained models to the sketch domain while maintaining generalization across different shape classes.
  - Quick check question: What are the trade-offs between fine-tuning all layers versus only the final layers when adapting a pretrained model to a new domain?

## Architecture Onboarding

- Component map:
  Pretrained backbone (ViT or ResNet) -> Layer selection mechanism -> Feature extraction -> Similarity computation -> Retrieval ranking

- Critical path: Pretrained backbone → Layer selection → Feature extraction → Similarity computation → Retrieval ranking

- Design tradeoffs:
  - Using mid-level vs. final layers: Mid-level layers preserve spatial information but may lack high-level semantics; final layers capture semantics but lose spatial details
  - Fine-tuning all weights vs. selective fine-tuning: Full fine-tuning provides better adaptation but risks overfitting; selective fine-tuning preserves more pretrained knowledge
  - Scale normalization approach: Aggressive normalization ensures consistency but may distort relative proportions; mild normalization preserves proportions but introduces variability

- Failure signatures:
  - Poor retrieval accuracy when object scales in sketches differ significantly
  - Performance degradation when using either very high or very low feature layers
  - Overfitting when fine-tuning on a single class with aggressive learning rates

- First 3 experiments:
  1. Compare retrieval accuracy using different feature layers (e.g., layer 4, 6, 11 for ViT) on a held-out validation set to identify the optimal layer
  2. Test the impact of object scale normalization by varying the bounding box size and measuring retrieval accuracy
  3. Evaluate different fine-tuning strategies (e.g., full model vs. layer normalization only) on synthetic sketches and test on freehand sketches from a different shape class

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of fine-tuning strategy impact the model's ability to generalize to unseen sketch styles and 3D shape classes beyond the training set?
- Basis in paper: [explicit] The paper compares several fine-tuning strategies including layer normalization weights learning, visual prompt learning, and naive fine-tuning with a small learning rate. The authors find that naive fine-tuning with a small learning rate outperforms more advanced strategies on their task.
- Why unresolved: The study only tests on a limited number of sketch styles (freehand, NPR, anime) and 3D shape classes (chair, lamp, and additional classes for fine-tuning). The generalizability to a wider variety of styles and classes remains unexplored.
- What evidence would resolve it: Extensive testing on diverse sketch styles (e.g., architectural sketches, technical drawings) and 3D shape classes (e.g., vehicles, animals) would demonstrate the robustness of different fine-tuning strategies.

### Open Question 2
- Question: What is the impact of the object's pose and viewpoint on the retrieval accuracy, and how can the model be adapted to handle extreme viewpoints or occlusions?
- Basis in paper: [inferred] The paper mentions that sketches are drawn from viewpoints with zenith angles around 20 degrees and various azimuth angles. However, it does not explicitly explore the effect of different poses or occlusions on retrieval performance.
- Why unresolved: The current study focuses on specific viewpoints and does not investigate how the model performs with sketches from extreme angles or with occlusions.
- What evidence would resolve it: Testing the model on sketches from a wider range of viewpoints, including extreme angles and occluded views, would reveal its limitations and potential areas for improvement.

### Open Question 3
- Question: How does the scale of the object in the sketch affect the model's ability to capture fine-grained details and distinguish between similar shapes?
- Basis in paper: [explicit] The authors study the impact of object scale on feature similarity and find that similar object scales result in the best performance for ViT and ResNet.
- Why unresolved: While the paper identifies the importance of scale, it does not investigate how scale affects the model's ability to capture fine-grained details or distinguish between shapes that are visually similar but have subtle differences.
- What evidence would resolve it: Testing the model on sketches with varying object scales and comparing its performance on fine-grained retrieval tasks (e.g., distinguishing between different chair models) would reveal the relationship between scale and detail capture.

## Limitations

- The study's results are primarily validated on the AmateurSketch dataset, which contains only two shape categories (chairs and lamps)
- The paper doesn't fully explore the computational cost of fine-tuning large models like ViT on synthetic sketch data
- The paper's assertion that their zero-shot approach can "surpass the accuracy of supervised methods" needs more rigorous comparison with existing supervised sketch-based retrieval methods

## Confidence

- **High confidence**: The effectiveness of mid-level feature layers (ViT layer 6, ResNet layer 3) for sketch matching is well-supported by experimental results across both freehand and synthetic sketches
- **Medium confidence**: The claim that fine-tuning on one shape class transfers to others is supported by the experiments but could benefit from testing on more diverse shape categories
- **Low confidence**: The paper's assertion that their zero-shot approach can "surpass the accuracy of supervised methods" needs more rigorous comparison with existing supervised sketch-based retrieval methods

## Next Checks

1. Evaluate the fine-tuning approach on 3-5 additional shape categories from ShapeNet to assess the robustness of cross-class knowledge transfer
2. Systematically vary object scales in a controlled experiment to quantify the relationship between scale normalization and retrieval accuracy across different feature layers
3. Measure and compare the computational cost (training time, memory usage) of full fine-tuning versus layer normalization-only approaches, and benchmark against alternative domain adaptation methods