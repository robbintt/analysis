---
ver: rpa2
title: 'CAPS: A Practical Partition Index for Filtered Similarity Search'
arxiv_id: '2308.15014'
source_url: https://arxiv.org/abs/2308.15014
tags:
- search
- query
- attribute
- attributes
- index
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CAPS, a practical algorithm for filtered approximate
  near-neighbor search that achieves state-of-the-art performance in the high-recall
  regime. CAPS uses a hierarchical partitioning scheme that shards data based on vector
  embeddings followed by attributes, using an Attribute Frequency Tree (AFT).
---

# CAPS: A Practical Partition Index for Filtered Similarity Search

## Quick Facts
- arXiv ID: 2308.15014
- Source URL: https://arxiv.org/abs/2308.15014
- Reference count: 40
- Key outcome: CAPS achieves up to 5.5x higher query throughput with 20% better recall compared to production systems

## Executive Summary
CAPS is a practical algorithm for filtered approximate near-neighbor search that outperforms existing graph-based approaches in the high-recall regime while using only 10% of the index size. It uses a hierarchical partitioning scheme that first shards data based on vector embeddings, then creates attribute-based sub-partitions using an Attribute Frequency Tree (AFT). The method supports practical features like variable query attributes, conjunctive AND constraints, and dynamic insertions. Experiments show CAPS achieves superior recall-latency tradeoffs on multiple real-world datasets.

## Method Summary
CAPS uses a two-level hierarchical partitioning approach. First, data is partitioned using embedding vectors (e.g., FAISS k-means). Then, each partition is sub-partitioned based on attribute frequencies using an Attribute Frequency Tree. During query processing, the system selects top m partitions, identifies valid sub-partitions via the AFT, performs attribute matching, and computes vector distances on the filtered candidate set. The algorithm is designed to avoid the "unhappy middle" problem where neither filtering nor searching dominates.

## Key Results
- Achieves up to 5.5x higher query throughput compared to production systems
- Provides 20% better recall than existing approaches
- Uses only 10% of the index size compared to graph-based methods
- Supports practical features like dynamic insertions and variable query attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical partitioning with attribute frequency trees reduces the search space when constraints are sparse.
- Mechanism: By organizing sub-partitions based on the most frequent attribute values at each tree level, the index can quickly identify and exclude partitions that do not satisfy the query constraints.
- Core assumption: Attributes follow a power-law distribution, so a small number of frequent attributes cover most of the data.
- Evidence anchors:
  - [abstract]: "CAPS achieves up to 5.5x higher query throughput with 20% better recall compared to production systems."
  - [section]: "Our sub-partition construction algorithm is motivated by two observations. First, the sub-partitions should be non-overlapping. Second, we should refuse to search a sub-partition if none of its contents satisfy the constraint."
  - [corpus]: Weak. No direct corpus evidence; this is a novel contribution.
- Break condition: If attributes are uniformly distributed or have no frequent values, the tree becomes ineffective.

### Mechanism 2
- Claim: Using learned partitioning (BLISS) can integrate embedding and attribute information to improve recall.
- Mechanism: BLISS learns partitions based on the oracle of true filtered near neighbors, effectively aligning the vector space partitioning with the attribute constraints.
- Core assumption: There exists a correlation between vector embeddings and attribute values that can be learned.
- Evidence anchors:
  - [section]: "While certain algorithms, such as NHQ, explicitly fuse the attribute and vector similarity measures into a single distance metric, this is highly dataset-dependent and is difficult to tune."
  - [section]: "Recent work on learned partitioning provides a nice avenue to integrate attributes into the first-level clustering."
  - [corpus]: Weak. The paper proposes using BLISS but does not provide empirical results for BLISS1 and BLISS2 in the corpus.
- Break condition: If embeddings and attributes are uncorrelated, learned partitioning provides no benefit over standard partitioning.

### Mechanism 3
- Claim: Interleaving filtering and search operations avoids the "unhappy middle" problem.
- Mechanism: By constructing an index that allows for both filtering and searching to occur simultaneously (rather than sequentially), CAPS reduces the computational burden in cases where constraints are neither very sparse nor very dense.
- Core assumption: The unhappy middle region is a significant problem in real-world queries.
- Evidence anchors:
  - [section]: "We call this region the unhappy middle. Unfortunately, most real-world queries fall into this regime."
  - [section]: "CAPS breaks this limitation by sub-partitioning each cluster (at indexing time) based only on the attributes."
  - [corpus]: Weak. The concept is introduced in the paper but not extensively validated in the corpus.
- Break condition: If most queries are either very sparse or very dense in constraints, the interleaving mechanism provides minimal benefit.

## Foundational Learning

- Concept: Power-law distribution of attributes
  - Why needed here: CAPS relies on the fact that a few attributes are very common, allowing efficient partitioning.
  - Quick check question: If an attribute has a frequency distribution following Zipf's law, what percentage of items would you expect to have the most frequent attribute value?

- Concept: Hierarchical clustering and partitioning
  - Why needed here: Understanding how to build and traverse hierarchical structures like the Attribute Frequency Tree (AFT) is crucial.
  - Quick check question: In a balanced binary tree of height h, how many leaf nodes are there?

- Concept: Approximate Nearest Neighbor (ANN) search fundamentals
  - Why needed here: CAPS is an ANN algorithm; understanding trade-offs between recall and latency is essential.
  - Quick check question: What is the difference between recall@K and precision@K in the context of ANN search?

## Architecture Onboarding

- Component map: Index creation -> Partition selection -> Sub-partition identification -> Attribute matching -> Vector distance computation -> Result generation

- Critical path:
  1. Query arrives with vector q and attribute b.
  2. Use f(Â·) to select top m partitions.
  3. For each partition, identify relevant sub-partitions using AFT.
  4. Perform attribute matching on candidate set.
  5. Compute vector distances on filtered candidates.
  6. Return top-k results.

- Design tradeoffs:
  - Index size vs. query performance: Deeper AFT (larger h) reduces query time but increases index size.
  - Partition granularity: More partitions (larger B) can improve recall but increase indexing time and memory.
  - Learned vs. standard partitioning: BLISS can improve recall if attributes correlate with embeddings but adds complexity.

- Failure signatures:
  - Low recall: Possibly due to insufficient partitions probed (small m) or ineffective AFT (wrong h).
  - High latency: Could be caused by too many sub-partitions being searched or large candidate sets after attribute matching.
  - Memory issues: Index size may be too large due to deep AFT or many partitions.

- First 3 experiments:
  1. Vary the number of partitions probed (m) and measure recall and latency.
  2. Experiment with different AFT heights (h) to find the optimal balance between index size and query performance.
  3. Compare standard k-means partitioning with BLISS partitioning to evaluate the benefit of learned partitioning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CAPS change when dealing with attributes that have non-power-law distributions?
- Basis in paper: [inferred] The paper mentions that CAPS is particularly well-suited for power-law distributed attributes and uses a Huffman tree-inspired hierarchical sub-partitioning algorithm, but does not explore performance on other distributions.
- Why unresolved: The paper only evaluates CAPS on datasets with power-law distributed attributes and does not provide results or analysis for other attribute distributions.
- What evidence would resolve it: Experiments comparing CAPS performance on datasets with different attribute distributions (e.g., uniform, normal) would provide insights into its robustness across various scenarios.

### Open Question 2
- Question: What is the impact of increasing the dimensionality of the embedding vectors on the performance of CAPS?
- Basis in paper: [inferred] The paper evaluates CAPS on datasets with varying embedding dimensions but does not explicitly analyze how changes in dimensionality affect performance.
- Why unresolved: The relationship between embedding dimensionality and CAPS performance is not explored, leaving questions about scalability to high-dimensional spaces unanswered.
- What evidence would resolve it: Conducting experiments with datasets of varying embedding dimensions and analyzing the impact on CAPS's recall, latency, and index size would provide clarity on its scalability.

### Open Question 3
- Question: How does CAPS handle dynamic updates, such as insertions and deletions, in terms of performance and index maintenance?
- Basis in paper: [explicit] The paper claims CAPS supports dynamic insertions but does not provide detailed analysis or experimental results on its performance or efficiency in handling updates.
- Why unresolved: While CAPS claims to support dynamic insertions, the paper lacks a thorough investigation into how updates affect the index structure, query performance, and maintenance overhead.
- What evidence would resolve it: Experimental results showing the performance impact of insertions and deletions, along with analysis of index maintenance requirements, would clarify CAPS's practicality for dynamic datasets.

## Limitations

- Effectiveness depends heavily on attributes following a power-law distribution
- Learned partitioning approach (BLISS) is mentioned but not thoroughly evaluated
- Scalability to extremely large datasets (beyond 8M items) remains unverified

## Confidence

- **High Confidence**: The core hierarchical partitioning mechanism and its advantage over sequential filtering approaches
- **Medium Confidence**: The practical benefits claimed for production systems (5.5x QPS improvement) based on the single Amazon case study
- **Low Confidence**: The generalizability of the power-law attribute assumption across diverse domains

## Next Checks

1. Test CAPS on datasets with uniform or random attribute distributions to verify the AFT mechanism's effectiveness breaks down as claimed when power-law assumptions don't hold.

2. Implement and evaluate both BLISS1 and BLISS2 variants to quantify the benefit of learned partitioning versus standard k-means across different attribute-embedding correlation levels.

3. Scale the algorithm to 100M+ items to validate claimed space efficiency and identify any memory or construction time bottlenecks not apparent in the smaller-scale experiments.