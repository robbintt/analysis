---
ver: rpa2
title: Multilingual k-Nearest-Neighbor Machine Translation
arxiv_id: '2310.14644'
source_url: https://arxiv.org/abs/2310.14644
tags:
- datastore
- languages
- translation
- language
- datastores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes multilingual k-nearest neighbor machine translation,
  which improves translation quality for low-resource languages by constructing multilingual
  datastores that combine representations from multiple languages. The method shows
  consistent improvements across language pairs, with up to +3.6 BLEU for low-resource
  languages and +0.5 BLEU for high-resource languages.
---

# Multilingual k-Nearest-Neighbor Machine Translation

## Quick Facts
- **arXiv ID**: 2310.14644
- **Source URL**: https://arxiv.org/abs/2310.14644
- **Reference count**: 10
- **Key outcome**: This paper proposes multilingual k-nearest neighbor machine translation, which improves translation quality for low-resource languages by constructing multilingual datastores that combine representations from multiple languages.

## Executive Summary
This paper addresses the challenge of improving machine translation quality for low-resource languages by extending k-nearest neighbor machine translation (kNN-MT) to multilingual settings. The authors propose constructing multilingual datastores that combine representations from multiple languages into a single datastore, which can be used during inference to improve translation quality. The approach leverages the generalization capabilities of multilingual neural machine translation models to enable effective cross-lingual retrieval. Experiments on a 51-language TED Talks corpus demonstrate substantial improvements in BLEU scores for low-resource languages (up to +3.6 BLEU) while also benefiting high-resource languages (up to +0.5 BLEU).

## Method Summary
The method builds upon kNN-MT by creating multilingual datastores that combine bilingual datastores from multiple source languages into a single target language datastore. The approach uses M2M100, a multilingual translation model, to generate decoder representations that serve as keys in the datastores, with target tokens as values. Bilingual datastores D(ℓ,ℓ′) are first created for each source-target language pair, then combined into multilingual datastores D(LML,ℓ′) that include all languages in a language group or all available languages. The method also explores cross-lingual mapping to align representations between languages, and investigates the tradeoff between datastore size and inference speed by using linguistically similar language groupings.

## Key Results
- Multilingual datastores achieve up to +3.6 BLEU improvement for low-resource languages and +0.5 BLEU for high-resource languages
- Cross-lingual datastores are particularly effective for low-resource languages, with Belarusian-English improving from +1.7 to +3.2 BLEU when using a larger Ukrainian-English datastore
- Language grouping datastores achieve 5.3x speed improvement while maintaining quality, being 4 times smaller than full multilingual datastores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual datastores improve low-resource translation quality by providing more retrieval candidates
- Mechanism: When low-resource languages have small monolingual datastores, cross-lingual datastores from related languages provide additional semantically similar examples that can be retrieved during translation
- Core assumption: Multilingual representations generalize sufficiently across languages to make cross-lingual retrieval effective
- Evidence anchors:
  - [abstract] "These improvements have been limited to high-resource language pairs, with large datastores, and remain a challenge for low-resource languages. In this paper, we address this issue by combining representations from multiple languages into a single datastore."
  - [section] "We observe that low-resource languages generally benefit from larger cross-lingual datastores. For instance, Belarusian-English (be-en) with bilingual datastore (116K instances) results in +1.7 BLEU, whereas Belarusian-English with a substantially larger Ukrainian-English (uk-en) datastore (2.9M instances) leads to a further improvement of +1.5 BLEU."
  - [corpus] Weak - No direct corpus evidence provided for this specific mechanism

### Mechanism 2
- Claim: Multilingual datastores achieve better results than cross-lingual datastores by aggregating more relevant examples
- Mechanism: By combining multiple source languages into a single target language datastore, multilingual datastores increase the diversity and quantity of relevant retrieval candidates while maintaining semantic similarity through shared target language
- Core assumption: More diverse examples from related languages improve retrieval effectiveness
- Evidence anchors:
  - [abstract] "Our results consistently demonstrate substantial improvements not only in low-resource translation quality (up to +3.6 BLEU), but also for high-resource translation quality (up to +0.5 BLEU)."
  - [section] "Since it is unclear which cross-lingual datastore performs best, we use as many languages as possible as a first attempt. This results in our largest datastore D(ALL,en), which has 125M entries. For almost all languages, except Russian-English (ru-en), this leads to better results than bilingual datastores."
  - [corpus] Weak - No direct corpus evidence provided for this specific mechanism

### Mechanism 3
- Claim: Language grouping multilingual datastores achieve similar quality to full multilingual datastores with significantly smaller size
- Mechanism: Languages within the same linguistic grouping have more similar representations, allowing smaller datastores to maintain retrieval effectiveness while achieving faster decoding speeds
- Core assumption: Linguistic similarity correlates with representation similarity in multilingual models
- Evidence anchors:
  - [abstract] "Our experiments show that it is possible to create multilingual datastores that are a quarter of the size, achieving a 5.3x speed improvement, by using linguistic similarities for datastore creation."
  - [section] "We investigate to what extent datastore size can be further decreased. We hypothesize that more similar multilingual representations result in better cross-lingual retrieval, and that representations within the same language grouping are more similar. In line with this, we create three multilingual datastores consisting of all languages within a language grouping: Slavic (datastore size 30.6M), Germanic (datastore size 20M), and Greek (datastore size 5.6M)."
  - [corpus] Weak - No direct corpus evidence provided for this specific mechanism

## Foundational Learning

- Concept: k-Nearest Neighbor Machine Translation (kNN-MT)
  - Why needed here: Understanding the base kNN-MT framework is essential to grasp how multilingual datastores augment the translation process
  - Quick check question: What are the two main components combined in kNN-MT and how do they interact during inference?

- Concept: Multilingual Neural Machine Translation (mNMT)
  - Why needed here: The paper relies on mNMT representations as keys in the datastore, so understanding how these representations work across languages is crucial
  - Quick check question: How do mNMT representations enable cross-lingual retrieval in this approach?

- Concept: Cross-lingual retrieval and representation alignment
  - Why needed here: The effectiveness of cross-lingual datastores depends on how well representations align across languages
  - Quick check question: What role does the linear mapping learned between languages play in improving cross-lingual retrieval?

## Architecture Onboarding

- Component map:
  mNMT model (M2M100) -> Datastore construction module -> Cross-lingual mapping module -> kNN retrieval module -> Interpolation module

- Critical path:
  1. Generate translations using mNMT to build datastores
  2. Construct multilingual datastores by combining bilingual datastores
  3. Optionally apply cross-lingual mapping to align representations
  4. During inference, generate representations and retrieve k nearest neighbors
  5. Interpolate between NMT and kNN probabilities for final output

- Design tradeoffs:
  - Larger datastores provide more retrieval candidates but increase inference time
  - Cross-lingual datastores work well for low-resource but may hurt high-resource languages
  - Language grouping datastores balance quality and speed but may miss some useful examples

- Failure signatures:
  - Performance degradation when using cross-lingual datastores for high-resource languages
  - Slow inference times with very large multilingual datastores
  - Poor retrieval effectiveness when language representations don't generalize well

- First 3 experiments:
  1. Compare bilingual vs cross-lingual vs multilingual datastores for a low-resource language pair
  2. Test language grouping vs full multilingual datastores for speed-quality tradeoff
  3. Evaluate cross-lingual mapping effectiveness on a representative low-resource language

## Open Questions the Paper Calls Out

- **Open Question 1**: How does multilingual kNN-MT perform with even larger multilingual datastores beyond the 125M entries tested?
- **Open Question 2**: How would the multilingual kNN-MT approach perform with English-centric multilingual translation models rather than the non-English-centric M2M100 model used in the experiments?
- **Open Question 3**: What is the optimal strategy for selecting languages to include in multilingual datastores for specific low-resource language pairs?

## Limitations

- The approach relies on the assumption that multilingual representations generalize across languages, which may not hold for all language pairs
- Datastore construction is heuristic, combining all available bilingual datastores without sophisticated filtering or selection criteria
- The evaluation focuses primarily on BLEU scores, which may not fully capture translation quality, especially for low-resource languages

## Confidence

- **High Confidence**: The empirical results showing BLEU improvements for low-resource languages using multilingual datastores are well-supported by the experimental data
- **Medium Confidence**: The theoretical mechanism explaining why multilingual datastores work (cross-lingual generalization of representations) is plausible but not rigorously proven
- **Low Confidence**: The paper's claims about optimal datastore size and composition are based on limited experiments

## Next Checks

1. **Representation Similarity Analysis**: Conduct a systematic analysis of representation similarity across language pairs in the multilingual model to verify the assumption that linguistically similar languages have more similar representations
2. **Cross-Lingual Mapping Quality**: Evaluate the quality of cross-lingual mappings by testing retrieval effectiveness on held-out data before and after applying the linear mappings
3. **Generalization to New Languages**: Test the approach on languages not included in the original TED Talks corpus to assess how well the multilingual datastores generalize to truly unseen languages