---
ver: rpa2
title: Generative Query Reformulation for Effective Adhoc Search
arxiv_id: '2308.00415'
source_url: https://arxiv.org/abs/2308.00415
tags:
- query
- reformulation
- information
- queries
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the use of generative language models
  for query reformulation to enhance adhoc search effectiveness. The authors propose
  two frameworks: GenQR, which reformulates queries directly, and GenPRF, which incorporates
  pseudo-relevance feedback context.'
---

# Generative Query Reformulation for Effective Adhoc Search

## Quick Facts
- arXiv ID: 2308.00415
- Source URL: https://arxiv.org/abs/2308.00415
- Reference count: 40
- This paper proposes GenQR and GenPRF generative query reformulation frameworks that significantly outperform traditional and neural methods on TREC datasets.

## Executive Summary
This paper investigates generative language models for query reformulation to enhance ad-hoc search effectiveness. The authors propose two frameworks: GenQR, which reformulates queries directly, and GenPRF, which incorporates pseudo-relevance feedback context. They evaluate fine-tuning and prompting approaches using T5 and FLAN-T5 models across four TREC datasets. The study demonstrates that generative query reformulation can substantially improve retrieval effectiveness compared to traditional methods like RM3 and neural approaches like BERT-QE, with GenPRF particularly benefiting from contextual input.

## Method Summary
The paper casts query reformulation as a text generation task using T5 and FLAN-T5 models. Two frameworks are proposed: GenQR reformulates queries directly from the original query, while GenPRF incorporates contextual passages from top-ranked pseudo-relevant documents. Models are fine-tuned on weakly supervised query pairs sharing relevant documents, filtered through overlap, effectiveness, and stopwords filters. Training uses the MSMARCO document ranking dataset. Evaluation employs four TREC collections with MAP, MRR, Recall, and nDCG@10 metrics, comparing against RM3 and BERT-QE baselines.

## Key Results
- GenQR and GenPRF significantly outperform traditional methods (RM3) and neural approaches (BERT-QE) on TREC datasets
- GenPRF achieves higher MAP and nDCG@10 scores by leveraging pseudo-relevance feedback context
- The models enhance effectiveness when combined with neural re-rankers in a BM25 + monoT5 pipeline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative models can reformulate user queries to better match relevant documents by generating paraphrases or expansions.
- Mechanism: The models take the original query as input and generate reformulated queries that are more likely to retrieve relevant documents by incorporating additional context or by refining the query terms.
- Core assumption: The generated queries are semantically similar to the original query but use different wording that better matches the relevant documents.
- Evidence anchors:
  - [abstract] "Recent advancements in generative language models have demonstrated their ability in generating responses that are relevant to a given prompt."
  - [section 3.1] "We cast the query reformulation task as a text generation task. This allows us to be able to test whether the knowledge encapsulated by pretrained text generation models... can be exploited for query reformulation."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.458, average citations=0.0."

### Mechanism 2
- Claim: Incorporating pseudo-relevance feedback (PRF) context improves the quality of generated query reformulations.
- Mechanism: The models use the top-ranked documents returned for the original query as context to guide the generation of reformulated queries, ensuring that the expansions are topically relevant.
- Core assumption: The top-ranked documents are relevant to the user's information need and can provide useful context for generating better query reformulations.
- Evidence anchors:
  - [abstract] "GenPRF provides additional context for the query by making use of pseudo-relevance feedback information."
  - [section 3.2] "To reinforce the capability of the query reformulation model to interpret the meaning of the input query, an additional context that further explains the query statement can be desirable."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.458, average citations=0.0."

### Mechanism 3
- Claim: Fine-tuning the generative models on weakly supervised query pairs improves their ability to generate effective query reformulations.
- Mechanism: The models are fine-tuned on pairs of queries that share relevant documents, allowing them to learn how to generate reformulations that are semantically similar to the original query.
- Core assumption: Queries that share relevant documents are semantically similar and can be used as training examples for generating effective reformulations.
- Evidence anchors:
  - [section 3.3] "To circumvent the lack of ground-truth data, we leverage the filters proposed in Section 3.3 to generate the required weak supervised query pairs to fine-tune the T5 model, instead of using human annotated query pairs."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.458, average citations=0.0."

## Foundational Learning

- Concept: Pseudo-relevance feedback (PRF)
  - Why needed here: PRF is used to provide additional context for generating query reformulations that are more likely to retrieve relevant documents.
  - Quick check question: What is the purpose of using PRF in the GenPRF model?

- Concept: Text generation models
  - Why needed here: Text generation models are used to reformulate the original query by generating paraphrases or expansions.
  - Quick check question: How do text generation models contribute to query reformulation?

- Concept: Weak supervision
  - Why needed here: Weak supervision is used to train the generative models on pairs of queries that share relevant documents, as there is no ground-truth data available.
  - Quick check question: Why is weak supervision necessary for training the generative models?

## Architecture Onboarding

- Component map: Original query -> GenQR/GenPRF -> Reformulated query -> Retrieval
- Critical path: Original query → GenQR/GenPRF → Reformulated query → Retrieval
- Design tradeoffs:
  - Using PRF context (GenPRF) vs. not using it (GenQR): GenPRF may generate more relevant reformulations but requires additional retrieval step.
  - Fine-tuning vs. prompting: Fine-tuning may require more training data but can potentially generate better reformulations.
- Failure signatures:
  - If the generated reformulations are not semantically similar to the original query, retrieval effectiveness may not improve.
  - If the PRF context is noisy or irrelevant, the generated reformulations may be off-topic.
- First 3 experiments:
  1. Test the effectiveness of GenQR on a small dataset to ensure it generates relevant reformulations.
  2. Compare the performance of GenQR and GenPRF on a larger dataset to assess the impact of PRF context.
  3. Fine-tune the models on a larger set of weakly supervised query pairs and evaluate the impact on retrieval effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of contextual information (e.g., snippets, summaries, or entire documents) affect the performance of GenPRF models?
- Basis in paper: [explicit] The paper discusses using passages as contextual information and mentions the potential limitations of using entire documents.
- Why unresolved: The paper only explores using passages as contextual information and does not compare the effectiveness of different types of contextual information.
- What evidence would resolve it: Experiments comparing the performance of GenPRF models using different types of contextual information, such as snippets, summaries, or entire documents.

### Open Question 2
- Question: What is the optimal number of paraphrases (N) to include in the query reformulation to maximize effectiveness?
- Basis in paper: [explicit] The paper mentions that higher values of N usually lead to higher effectiveness, but the optimal value is not determined.
- Why unresolved: The paper only reports results for N = 5 and does not explore the impact of varying N on effectiveness.
- What evidence would resolve it: Experiments varying the value of N and measuring the impact on retrieval effectiveness (e.g., MAP, nDCG@10).

### Open Question 3
- Question: How do the proposed GenQR and GenPRF models perform on non-English datasets?
- Basis in paper: [inferred] The paper only evaluates the models on English datasets (TREC collections) and does not discuss their performance on non-English data.
- Why unresolved: The paper does not provide any evidence of the models' performance on non-English datasets.
- What evidence would resolve it: Experiments evaluating the GenQR and GenPRF models on non-English datasets and comparing their performance to existing methods.

### Open Question 4
- Question: How do the GenQR and GenPRF models handle ambiguous or underspecified queries?
- Basis in paper: [inferred] The paper does not explicitly discuss how the models handle ambiguous or underspecified queries.
- Why unresolved: The paper does not provide any evidence of the models' ability to handle ambiguous or underspecified queries.
- What evidence would resolve it: Experiments evaluating the models' performance on ambiguous or underspecified queries and comparing their effectiveness to existing methods.

### Open Question 5
- Question: What are the computational costs and scalability implications of deploying GenQR and GenPRF models in a production search system?
- Basis in paper: [inferred] The paper mentions the computational costs of applying T5 to each document for document expansion (DocT5query) but does not discuss the costs of deploying GenQR and GenPRF models.
- Why unresolved: The paper does not provide any evidence of the computational costs and scalability implications of deploying the proposed models.
- What evidence would resolve it: Experiments measuring the computational costs and scalability of deploying GenQR and GenPRF models in a production search system and comparing them to existing methods.

## Limitations

- Reliance on weak supervision through query pairs sharing relevant documents may introduce noise into the training process
- Computational cost of fine-tuning large models (especially FLAN-T5-xxl with 3B parameters) is not fully addressed
- Lack of comprehensive error analysis to understand failure cases and model limitations

## Confidence

- High Confidence: The core finding that generative models can improve retrieval effectiveness over traditional RM3 and BERT-QE baselines is well-supported by significant statistical improvements across multiple TREC datasets.
- Medium Confidence: The relative effectiveness of GenQR vs. GenPRF and the benefits of fine-tuning vs. prompting are demonstrated but could benefit from more extensive ablation studies and cross-dataset validation.
- Medium Confidence: The claim about efficiency and interpretability is based on qualitative observations rather than systematic measurement or user studies.

## Next Checks

1. **Error Analysis Validation**: Conduct a detailed qualitative analysis of generated reformulations that failed to improve retrieval, examining patterns in semantic drift, term introduction, and context relevance to better understand model limitations.

2. **Computational Efficiency Assessment**: Measure and compare the actual computational costs (training time, inference latency, memory usage) of GenQR/GenPRF against traditional methods across different hardware configurations to validate efficiency claims.

3. **Generalization Testing**: Evaluate the models on out-of-domain datasets or different retrieval tasks (e.g., conversational search, cross-lingual retrieval) to assess the robustness and generalizability of the reformulation approach beyond the TREC collections used.