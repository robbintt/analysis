---
ver: rpa2
title: Evaluating Superhuman Models with Consistency Checks
arxiv_id: '2306.09983'
source_url: https://arxiv.org/abs/2306.09983
tags:
- consistency
- arxiv
- chess
- decisions
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework for evaluating superhuman machine\
  \ learning models by checking the logical consistency of their decisions rather\
  \ than comparing them to human-labeled ground truth. The authors test their approach\
  \ on three tasks\u2014chess position evaluation, future event forecasting, and legal\
  \ judgment prediction\u2014and find that even state-of-the-art models often produce\
  \ logically inconsistent outputs."
---

# Evaluating Superhuman Models with Consistency Checks

## Quick Facts
- arXiv ID: 2306.09983
- Source URL: https://arxiv.org/abs/2306.09983
- Reference count: 40
- Even superhuman models produce logically inconsistent outputs that violate necessary relationships between semantically equivalent inputs

## Executive Summary
This paper introduces a framework for evaluating superhuman machine learning models by checking the logical consistency of their decisions rather than comparing them to human-labeled ground truth. The authors test their approach on three tasks—chess position evaluation, future event forecasting, and legal judgment prediction—and find that even state-of-the-art models often produce logically inconsistent outputs. These inconsistencies demonstrate that the models' decisions cannot be trusted for correctness, even if their overall performance is strong.

## Method Summary
The authors propose evaluating superhuman models by defining predicates P on inputs and Q on outputs such that P(x₁,...,xₙ) logically implies Q(y₁,...,yₙ). When a model violates this implication, at least one output must be incorrect. The framework uses adversarial search to efficiently find rare consistency violations in strong models, demonstrating that consistency checks can detect errors without requiring ground truth labels.

## Key Results
- Chess engines assign different evaluations to semantically identical board positions
- Language models forecast that sports records will decrease over time
- AI judges grant bail only after adding a felony to a defendant's record
- Adversarial search finds up to 40× more failures than random search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency checks can detect errors in superhuman models even when ground truth is unknown
- Mechanism: Define predicates P on inputs and Q on outputs such that P(x₁,...,xₙ) logically implies Q(y₁,...,yₙ). If a model violates this implication, at least one output must be incorrect
- Core assumption: Logical relationships between related inputs/outputs are humanly verifiable even if absolute correctness is not

### Mechanism 2
- Claim: Adversarial search can efficiently find rare consistency violations in strong models
- Mechanism: Use evolutionary algorithms to mutate and recombine inputs to maximize evaluation differences between semantically equivalent positions or queries
- Core assumption: The search space of semantically equivalent inputs can be explored systematically even when violations are rare

### Mechanism 3
- Claim: Consistency violations reveal fundamental unreliability in model decisions, not just random errors
- Mechanism: Even with low stochasticity (temperature 0), models produce outputs that violate logical constraints, indicating systematic flaws in reasoning
- Core assumption: Models that frequently violate consistency constraints cannot be trusted for correctness regardless of overall performance

## Foundational Learning

- Concept: Logical implication and consistency checking
  - Why needed here: The entire framework relies on identifying when model outputs violate logically necessary relationships
  - Quick check question: Can you explain why P(x₁,...,xₙ) → Q(y₁,...,yₙ) being violated means at least one output is wrong?

- Concept: Adversarial search and optimization
  - Why needed here: Random sampling finds few violations; adversarial search is needed to efficiently discover bugs in strong models
  - Quick check question: How does an evolutionary algorithm differ from random sampling in searching for consistency violations?

- Concept: Predicate definition and verification
  - Why needed here: Creating effective consistency checks requires defining predicates that are both logically sound and humanly verifiable
  - Quick check question: What makes a good predicate P for input consistency versus Q for output consistency?

## Architecture Onboarding

- Component map: Model evaluation pipeline -> Predicate definition module -> Consistency violation detector -> Adversarial search optimizer -> Result visualization and analysis

- Critical path: 1. Define task-specific predicates P and Q, 2. Generate semantically related input tuples, 3. Run model on inputs to get outputs, 4. Check consistency violations, 5. If violations found, optionally run adversarial search, 6. Analyze and report results

- Design tradeoffs:
  - Soundness vs completeness: Hard consistency constraints guarantee found bugs are real but may miss others
  - Random vs adversarial search: Random is faster but finds fewer violations; adversarial is slower but more effective
  - Black-box vs white-box testing: Black-box is more general but white-box can find more subtle violations

- Failure signatures: Few violations found despite expected bugs → Search strategy too weak, Violations only in synthetic data → Predicates too restrictive or unrealistic, Inconsistent results across runs → Model has high stochasticity affecting reliability

- First 3 experiments: 1. Chess position evaluation consistency (forced moves, board transformations), 2. Language model forecasting consistency (negation, monotonicity checks), 3. Legal decision consistency (paraphrasing, partial ordering tests)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we efficiently discover rare consistency bugs in superhuman models as their abilities improve?
- Basis in paper: [inferred] from "finding polynomially verifiable inconsistencies is computable in the limit [30], it is unclear whether important inconsistencies can be detected efficiently" and "as models become better and bugs rarer, relaxing soundness may be necessary in order to get checks with better completeness"
- Why unresolved: Current methods rely on either random sampling or adversarial search, both of which become less effective as models improve and bugs become rarer
- What evidence would resolve it: Demonstration of an algorithm that can find consistency violations in superhuman models with polynomial time complexity, or proof that no such algorithm exists

### Open Question 2
- Question: Can consistency checks be automated at scale without human involvement?
- Basis in paper: [inferred] from "Most of our tests are human-generated. However, this is not a hard constraint for the general approach, and future work could generate tests automatically" and "evaluation data generated using the model itself should be taken as one-directional, optimistic estimates"
- Why unresolved: The paper uses manual generation and verification of consistency tests, but acknowledges this doesn't scale
- What evidence would resolve it: Development of a system that can automatically generate valid consistency checks for arbitrary models, or proof that such generation is fundamentally limited

### Open Question 3
- Question: What is the relationship between consistency and actual model correctness or safety?
- Basis in paper: [explicit] from "consistency does not imply safety; a model could be robustly consistent in its predictions, but still be unsafe in other ways" and "passing such checks should never be interpreted as a safety guarantee"
- Why unresolved: The paper establishes that consistency violations indicate untrustworthiness, but explicitly states that passing consistency checks doesn't guarantee safety
- What evidence would resolve it: Empirical studies showing whether consistent models are safer in practice, or theoretical arguments about why consistency and safety might be independent properties

## Limitations
- Consistency checks only work when semantically equivalent inputs exist, limiting applicability to some superhuman tasks
- The framework assumes humanly verifiable logical relationships, which may break down for highly complex reasoning
- Evaluation focuses on three specific domains (chess, forecasting, legal judgment), limiting generalizability

## Confidence
- High confidence: The core mechanism that consistency violations reveal fundamental model unreliability (Mechanism 3) - supported by multiple empirical demonstrations across all three tasks
- Medium confidence: The effectiveness of adversarial search for finding consistency violations (Mechanism 2) - while results show 40× improvement over random search, the approach may not scale to more complex input spaces
- Medium confidence: The claim that consistency checks can detect errors without ground truth (Mechanism 1) - theoretically sound but limited by the availability of semantically equivalent inputs

## Next Checks
1. **Generalization Test**: Apply the consistency framework to a fourth superhuman task (e.g., medical diagnosis or protein folding) to verify the approach works beyond the three tested domains
2. **Adversarial Robustness**: Compare the current evolutionary search against more sophisticated optimization methods like gradient-based approaches or large language model-guided search to determine if 40× improvement is near-optimal
3. **Human Verification**: Conduct a user study where human experts evaluate whether the consistency predicates are appropriately defined and whether violations identified by the framework correspond to actual reasoning errors