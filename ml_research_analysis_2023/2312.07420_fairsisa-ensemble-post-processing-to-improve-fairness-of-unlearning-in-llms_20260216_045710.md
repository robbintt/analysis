---
ver: rpa2
title: 'FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs'
arxiv_id: '2312.07420'
source_url: https://arxiv.org/abs/2312.07420
tags:
- sisa
- fairness
- unlearning
- post-processing
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how machine unlearning, specifically the SISA
  framework, affects fairness in large language models (LLMs) when performing toxic
  text classification. It shows that SISA can degrade fairness, measured via equalized
  odds, particularly as the number of shards increases.
---

# FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs

## Quick Facts
- arXiv ID: 2312.07420
- Source URL: https://arxiv.org/abs/2312.07420
- Reference count: 10
- Key outcome: Post-processing ensemble method improves fairness of SISA unlearning in LLMs while maintaining accuracy.

## Executive Summary
This paper investigates how machine unlearning via the SISA framework impacts fairness in large language models for toxic text classification. The authors demonstrate that SISA can degrade fairness metrics, particularly equalized odds, as the number of shards increases. To address this, they propose three post-processing methods to improve fairness in ensemble models and prove that one method is theoretically optimal. Experiments with BERT, DistilGPT2, and GPT2 on the HateXplain dataset show that ensemble post-processing achieves the best fairness-accuracy trade-off, especially when shard fairness is heterogeneous.

## Method Summary
The study evaluates SISA unlearning's impact on LLM fairness using the HateXplain dataset for toxic text classification. The SISA framework partitions data into shards, trains separate models on each shard, and aggregates predictions via majority voting. Three post-processing fairness methods are proposed: (1) aggregate then post-process, (2) post-process then aggregate, and (3) ensemble post-processing. The third method solves a linear program over the joint distribution of ensemble predictions and sensitive attributes to optimally adjust probabilities while preserving equalized odds. The approach is tested across BERT, DistilGPT2, and GPT2 models with varying shard counts.

## Key Results
- SISA unlearning degrades fairness (increases equalized odds) in LLMs, particularly as shard count increases
- Ensemble post-processing method achieves optimal fairness-accuracy trade-off
- Post-process then aggregate method generally outperforms other ordering strategies
- Best performance observed when one shard is fair and others are not

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The post-processing ensemble method improves fairness by optimally adjusting output probabilities while preserving equalized odds across shards.
- Mechanism: The method solves a linear program over the joint distribution of ensemble predictions and sensitive attributes, adjusting per-shard probabilities to satisfy equalized odds constraints without retraining.
- Core assumption: The joint distribution of ensemble outputs and sensitive attributes is estimable from the training data.
- Evidence anchors:
  - [abstract] "We adapt the post-processing fairness improvement technique from [Hardt et al., 2016] to design three methods that can handle model ensembles, and prove that one of the methods is an optimal fair predictor for ensemble of models."
  - [section] "Proposition 1 The optimization problem in equation 4 is a linear program in 2S+1 variables {p̄ya : ȳ ∈ {0,1}S, A ∈ {0,1}}, whose coefficients can be computed from the joint distribution of (Ŷ1, Ŷ2, ..., ŶS, A, Y)."
  - [corpus] Found 25 related papers; no direct citations yet, FMR scores indicate moderate relevance to fairness and unlearning.
- Break condition: If the joint distribution cannot be accurately estimated due to data scarcity or high dimensionality, the linear program may produce unreliable fairness adjustments.

### Mechanism 2
- Claim: SISA framework reduces fairness because shard imbalance causes disparate training distributions across constituent models.
- Mechanism: When data is partitioned into shards, some sensitive groups may be underrepresented in certain shards, leading constituent models to learn biased decision boundaries that manifest in aggregated predictions.
- Core assumption: Shard partitioning is random but does not guarantee balanced representation of sensitive groups across shards.
- Evidence anchors:
  - [abstract] "We empirically demonstrate that the SISA framework can indeed reduce fairness in LLMs."
  - [section] "Importantly, the SISA framework can indeed degrade the fairness (with higher EO values) for both the models and sensitive attributes."
  - [corpus] Related work mentions fairness implications of unlearning but lacks direct citations to this specific SISA fairness degradation claim.
- Break condition: If data preprocessing ensures balanced shard assignment or if constituent models are regularized to be fairness-aware, the degradation effect may be mitigated.

### Mechanism 3
- Claim: Post-processing after aggregation yields better fairness-accuracy trade-offs than post-processing before aggregation for SISA ensembles.
- Mechanism: Aggregating first reduces variance across shards, producing a more stable decision surface for post-processing adjustments, while post-processing before aggregation amplifies shard-level noise.
- Core assumption: Majority voting aggregation reduces variance and improves prediction stability.
- Evidence anchors:
  - [abstract] "Amongst the three methods, Post-process then Aggregate method generally achieves the best trade-off between the accuracy and EO."
  - [section] "In contrast, EO values vary widely for different number of shards... SISA framework can indeed degrade the fairness (with higher EO values) for both the models and sensitive attributes."
  - [corpus] Moderate FMR scores suggest some overlap with fairness-aware ensemble methods, but no direct citations to this specific ordering effect.
- Break condition: If shard outputs are already well-calibrated and low-variance, the benefit of aggregation-first post-processing may diminish.

## Foundational Learning

- Concept: Equalized odds fairness constraint
  - Why needed here: The paper measures and optimizes for equalized odds to ensure consistent true/false positive rates across sensitive groups after unlearning.
  - Quick check question: What is the mathematical definition of equalized odds in terms of conditional probabilities over sensitive attributes and true labels?

- Concept: Linear programming for fairness adjustment
  - Why needed here: The optimal fair predictor is derived by solving a constrained linear program over probability adjustments, guaranteeing theoretical optimality.
  - Quick check question: Why is the fairness optimization problem in the ensemble case still a linear program despite the exponential number of probability variables?

- Concept: SISA training and inference pipeline
  - Why needed here: Understanding how SISA partitions data, trains shard models, and aggregates predictions is essential to reason about fairness impacts and post-processing integration points.
  - Quick check question: In SISA, which components must be retrained when a data point is unlearned, and why does this lead to efficiency gains over full retraining?

## Architecture Onboarding

- Component map: Training data → Shard partitioning → Per-shard fine-tuning (BERT/DistilGPT2/GPT2) → Majority voting aggregation → Post-processing fairness adjustment (aggregate→post, post→aggregate, ensemble post)
- Critical path: Shard partitioning → model training → aggregation → post-processing fairness adjustment
- Design tradeoffs: Using majority voting aggregation simplifies inference but may reduce granularity compared to weighted voting; post-processing after aggregation is more stable but less flexible than per-shard adjustments
- Failure signatures: High equalized odds values post-unlearning indicate fairness degradation; low accuracy post-post-processing indicate over-regularization; inconsistent shard outputs suggest poor aggregation stability
- First 3 experiments:
  1. Verify SISA shard training accuracy degradation as shard count increases on HateXplain dataset
  2. Measure equalized odds before and after SISA training for each sensitive attribute
  3. Compare accuracy-fairness trade-offs of the three post-processing methods across shard counts

## Open Questions the Paper Calls Out

- Question: How does the number of slices (R) in the SISA framework affect the fairness-accuracy trade-off for LLMs, and what is the optimal value of R?
  - Basis in paper: [inferred] The paper mentions that they do not consider the slicing component of SISA due to storage costs for LLMs, but notes that Koch & Soll (2023) analyzed SISA models in imbalanced datasets.
  - Why unresolved: The paper does not provide empirical results or theoretical analysis on the impact of slicing on fairness for LLMs.
  - What evidence would resolve it: Empirical studies comparing the fairness-accuracy trade-off for different values of R in the SISA framework, specifically for LLMs, would help determine the optimal value of R.

- Question: Can the ensemble post-processing method be extended to handle more complex fairness metrics beyond equalized odds, such as intersectional fairness or individual fairness?
  - Basis in paper: [explicit] The paper focuses on equalized odds as the fairness metric and proves the optimality of the ensemble post-processing method for this metric.
  - Why unresolved: The paper does not explore the applicability of the ensemble post-processing method to other fairness metrics or provide theoretical guarantees for these extensions.
  - What evidence would resolve it: Theoretical analysis and empirical validation of the ensemble post-processing method's performance on various fairness metrics, including intersectional and individual fairness, would determine its generalizability.

- Question: How do different data distribution scenarios, such as class imbalance or adversarial data removal, affect the fairness-accuracy trade-off in the SISA framework, and can the post-processing methods adapt to these scenarios?
  - Basis in paper: [inferred] The paper briefly mentions the scenario where one shard is fair and the others are unfair, but does not explore other data distribution scenarios.
  - Why unresolved: The paper does not provide a comprehensive analysis of how various data distribution scenarios impact the fairness-accuracy trade-off or the performance of the post-processing methods.
  - What evidence would resolve it: Empirical studies on the fairness-accuracy trade-off under different data distribution scenarios, such as class imbalance or adversarial data removal, and the evaluation of the post-processing methods' adaptability to these scenarios, would provide insights into their robustness.

## Limitations
- Analysis limited to three specific model architectures (BERT, DistilGPT2, GPT2) and one dataset (HateXplain)
- Theoretical optimality assumes perfect estimation of joint probability distribution, which may be challenging with limited data
- Results may not generalize to multi-class or regression tasks beyond binary toxic text classification

## Confidence
- Fairness degradation claim: Medium confidence (consistent trends but lacks statistical significance testing)
- Theoretical optimality proof: High confidence (follows established fairness theory from Hardt et al., 2016)
- Comparative effectiveness of post-processing ordering: Medium confidence (limited ablation studies across different data distributions)

## Next Checks
1. Conduct statistical significance testing across multiple random seeds to establish confidence intervals for fairness and accuracy metrics
2. Test the post-processing methods on additional datasets and model architectures (e.g., RoBERTa, T5) to evaluate generalizability
3. Implement and evaluate alternative aggregation strategies (weighted voting, confidence-based) to determine if aggregation method impacts the effectiveness of post-processing fairness adjustments