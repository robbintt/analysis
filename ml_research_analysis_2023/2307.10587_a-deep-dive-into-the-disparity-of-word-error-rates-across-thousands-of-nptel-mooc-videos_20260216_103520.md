---
ver: rpa2
title: A Deep Dive into the Disparity of Word Error Rates Across Thousands of NPTEL
  MOOC Videos
arxiv_id: '2307.10587'
source_url: https://arxiv.org/abs/2307.10587
tags:
- speakers
- youtube
- speech
- whisper
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce the Technical Indian English (TIE) dataset,
  a large-scale corpus of 8740 hours of technical lectures in English delivered by
  Indian speakers. This dataset is annotated with demographic attributes like gender,
  caste, age, native region, speech rate, and lecture discipline.
---

# A Deep Dive into the Disparity of Word Error Rates Across Thousands of NPTEL MOOC Videos

## Quick Facts
- arXiv ID: 2307.10587
- Source URL: https://arxiv.org/abs/2307.10587
- Reference count: 10
- Primary result: Whisper outperforms YouTube ASR on Indian English technical lectures, with significant WER disparities across gender, region, age, speech rate, and discipline

## Executive Summary
This study introduces the Technical Indian English (TIE) dataset, a large-scale corpus of 8740 hours of technical lectures in English delivered by Indian speakers. The dataset is annotated with demographic attributes including gender, caste, age, native region, speech rate, and lecture discipline. The authors evaluate two automatic speech recognition (ASR) systems—YouTube Automatic Captions and OpenAI Whisper—on this dataset to assess performance disparities. Whisper consistently outperforms YouTube ASR overall, with both systems showing statistically significant disparities across various speaker attributes.

## Method Summary
The authors curate the TIE dataset from NPTEL MOOC videos and annotate demographic attributes using speaker CVs and naming conventions. Audio is extracted from videos and transcribed using YouTube API and Whisper base model in a zero-shot setting. Word Error Rate (WER) is calculated using the JiWER library, with insertion, deletion, and substitution components analyzed separately. Kruskal-Wallis statistical tests identify disparities across categorical attributes, while quantitative analysis examines non-demographic factors like speech rate.

## Key Results
- Whisper base model achieves lower WER than YouTube ASR on Indian English technical speech
- Significant WER disparities exist across gender, native region, age, speech rate, and discipline
- Substitution errors show the highest disparity between demographic groups
- Female speakers generally achieve lower WER than male speakers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TIE dataset captures regional accent variation by annotating native region for each speaker.
- Mechanism: By labeling speakers from north, south, east, and west India, the dataset enables fine-grained ASR disparity analysis across Indian English accents.
- Core assumption: Speaker surname or CV reliably indicates native region; annotation model accurately maps name → region.
- Evidence anchors:
  - [abstract] "The dataset is sourced from the very popular NPTEL MOOC platform... delivered by instructors representing various parts of Indian demography."
  - [section] "To annotate for the native region, we use the information available in the speaker’s CV. If this is unavailable, the surname of the speaker is used to infer the native state as Indian names are based on naming conventions that are region-specific."
  - [corpus] Weak: corpus contains only ASR-focused papers; no direct regional accent study data.
- Break condition: If surnames are ambiguous or CV data is missing, region mapping may be incorrect.

### Mechanism 2
- Claim: Whisper outperforms YouTube ASR on technical Indian English speech.
- Mechanism: Whisper’s open-source transformer architecture trained on diverse multilingual web data adapts better to non-native accents and technical vocabulary.
- Core assumption: Web training data includes sufficient non-native and technical speech samples.
- Evidence anchors:
  - [abstract] "We also observe statistically significant disparity across the disciplines of the lectures... Whisper had better overall accuracy than YouTube."
  - [section] "We experimentally observe that Whisper outperforms other open-source ASRs like DeepSpeech and Wav2Vec 2.0 in terms of overall WER on the TIE dataset when evaluated in a zero-shot setting."
  - [corpus] Weak: corpus neighbors discuss ASR bias but no specific Whisper vs YouTube comparison.
- Break condition: If Whisper training data underrepresents Indian English, performance advantage may shrink.

### Mechanism 3
- Claim: WER disparity is driven mainly by substitution errors across demographics.
- Mechanism: Substitutions reflect phoneme-level mismatches between speaker accent and ASR acoustic models.
- Core assumption: Error type breakdown accurately reflects underlying acoustic model failure modes.
- Evidence anchors:
  - [section] "The highest disparity between the two gender groups for substitution error on both ASRs... highest disparity in substitution error of both ASRs points toward the same direction."
  - [abstract] "While there exists disparity due to gender, native region, age and speech rate of speakers..."
  - [corpus] Weak: corpus neighbors focus on bias detection but not error component analysis.
- Break condition: If insertion/deletion dominate in certain conditions, substitution-centric explanation fails.

## Foundational Learning

- Concept: Word Error Rate (WER) calculation and interpretation.
  - Why needed here: WER is the primary metric for ASR performance and disparity measurement.
  - Quick check question: If a transcript has 10 substitutions, 5 deletions, 2 insertions, and the reference has 100 words, what is WER?
- Concept: Non-parametric statistical testing (Kruskal-Wallis).
  - Why needed here: Used to test WER differences across categorical attributes when distributions are non-normal.
  - Quick check question: When should you use Kruskal-Wallis instead of ANOVA for comparing WERs across groups?
- Concept: Speech rate categorization (slow/average/fast percentiles).
  - Why needed here: Speech rate is a non-demographic attribute correlated with WER variation.
  - Quick check question: How is speech rate computed from transcript length and audio duration?

## Architecture Onboarding

- Component map: TIE dataset → ASR inference (YouTube/Whisper) → WER calculation → statistical analysis → disparity report
- Critical path: Data curation → transcript generation → preprocessing → JiWER metric → statistical test → visualization
- Design tradeoffs: Large dataset size (700 GB) vs. manageable inference time; open-source Whisper flexibility vs. YouTube black-box convenience
- Failure signatures: High insertion rates → accent mismatch; high deletion rates → poor audio quality; substitution spikes → phoneme confusion
- First 3 experiments:
  1. Run Whisper and YouTube on a small subset of 10 videos, verify WER calculations match manual counts.
  2. Test region annotation by sampling 20 speakers, confirm surname-based mapping aligns with CV data.
  3. Compare WER component distributions for male vs. female speakers to validate substitution error dominance claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do differences in pitch, intensity, tonality, and enunciation of speech contribute to disparities in ASR performance between male and female speakers?
- Basis in paper: [explicit] The paper discusses the differences in ASR performance between male and female speakers and mentions that female speakers have lower WER due to better pronunciation and articulation.
- Why unresolved: While the paper mentions the differences in speech characteristics, it does not delve into the specific mechanisms by which these differences impact ASR performance.
- What evidence would resolve it: A detailed analysis of the acoustic features of male and female speech and their correlation with ASR performance would provide insights into the underlying causes of the observed disparities.

### Open Question 2
- Question: How does the experience of speakers impact ASR performance, and what are the specific phonetic variations that arise with age that affect ASR accuracy?
- Basis in paper: [explicit] The paper discusses the impact of speaker experience on ASR performance and mentions that highly experienced speakers have higher WER due to factors like filler words and blackboard teaching methods.
- Why unresolved: The paper does not provide a comprehensive analysis of the phonetic variations associated with different levels of experience and their impact on ASR performance.
- What evidence would resolve it: A study that examines the acoustic and phonetic characteristics of speech across different experience levels and correlates them with ASR performance would provide a better understanding of the observed disparities.

### Open Question 3
- Question: How do regional variations in accents impact ASR performance, and what are the specific challenges faced by ASR systems in transcribing non-native English speech?
- Basis in paper: [explicit] The paper discusses the disparities in ASR performance across different native regions in India and mentions the challenges posed by regional accents.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges faced by ASR systems in transcribing non-native English speech and the impact of regional variations in accents.
- What evidence would resolve it: A study that examines the acoustic and phonetic characteristics of non-native English speech across different regions and their impact on ASR performance would provide insights into the specific challenges faced by ASR systems.

## Limitations
- Annotation reliability: Using surnames and CVs for inferring native regions introduces potential inaccuracies.
- Model generalization: Performance differences may not generalize to other ASR systems or languages.
- Bias detection scope: Study identifies statistical disparities but doesn't establish causation or isolate specific acoustic features driving disparities.

## Confidence
- High Confidence: Overall WER calculations and the general finding that Whisper outperforms YouTube ASR on this dataset.
- Medium Confidence: Specific disparity patterns across demographic groups, as they depend on accurate annotation and sufficient sample sizes per category.
- Low Confidence: Attribution of disparities to particular causes (e.g., accent, technical vocabulary) without controlled acoustic analysis.

## Next Checks
1. Annotation Validation: Randomly sample 50 speakers to manually verify native region assignments against CVs and conduct inter-annotator agreement testing.
2. Error Type Analysis: Perform acoustic-phonetic analysis on substitution errors to determine if they stem from accent variation, technical vocabulary, or audio quality issues.
3. Cross-Platform Consistency: Test the same audio samples through both ASR systems using identical preprocessing to isolate system-specific effects.