---
ver: rpa2
title: 'LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based
  Few-Shot Fine-Tuning'
arxiv_id: '2305.18169'
source_url: https://arxiv.org/abs/2305.18169
tags:
- paraphrasing
- language
- learning
- data
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes LM-CPPF, a contrastive prompt-based fine-tuning
  method for few-shot text classification that leverages few-shot paraphrasing using
  large language models (GPT-3 and OPT-175B) as data augmentation. Instead of using
  different templates like previous work, it generates semantically equivalent but
  syntactically diverse paraphrases of input sentences.
---

# LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning

## Quick Facts
- arXiv ID: 2305.18169
- Source URL: https://arxiv.org/abs/2305.18169
- Reference count: 35
- Key outcome: LM-CPPF achieves 1.4-1.65 standard deviations improvement over baselines by generating paraphrased views of data for contrastive learning in few-shot text classification.

## Executive Summary
LM-CPPF introduces a novel approach to few-shot text classification that leverages large language models (GPT-3 and OPT-175B) to generate paraphrases of input sentences, which are then used as data augmentation for contrastive prompt-based fine-tuning. Unlike previous methods that use different templates for creating views, LM-CPPF generates semantically equivalent but syntactically diverse paraphrases, providing additional perspectives of the data while maintaining semantic consistency. Experiments on six classification tasks demonstrate that this approach outperforms traditional data augmentation methods like back-translation and easy data augmentation, achieving significant improvements in classification accuracy.

## Method Summary
The method combines prompt-based fine-tuning with contrastive learning by generating paraphrases of input sentences using few-shot prompting with large language models. For each training example, the original sentence and its paraphrase are treated as two views, and supervised contrastive loss is applied between these views while simultaneously performing MLM fine-tuning. The approach uses pre-trained RoBERTa-base for the downstream task and GPT-3 or OPT-175B for paraphrasing, with 15 demonstration examples provided to the LLM for each paraphrasing task. Training involves calculating both MLM loss and supervised contrastive loss, combining them to update model parameters.

## Key Results
- Outperforms back-translation, easy data augmentation, and multiple template methods by 1.4-1.65 standard deviations
- Achieves consistent improvements across all six evaluated classification tasks (SST-2, SST-5, MNLI, CoLA, QNLI, CR)
- Demonstrates effectiveness of few-shot paraphrasing as data augmentation for contrastive learning
- Shows that semantically consistent syntactic diversity enhances contrastive learning objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning with few-shot paraphrasing improves fine-tuning performance by generating semantically equivalent but syntactically diverse views of input data.
- Mechanism: The method creates two views of the same input: the original and a paraphrased version. These views are compared using supervised contrastive loss, which pulls together embeddings of the same class and pushes apart embeddings of different classes.
- Core assumption: Paraphrases generated by large language models preserve semantic meaning while providing syntactic diversity, which enhances contrastive learning.
- Evidence anchors:
  - [abstract] "The approach is particularly effective because the generated paraphrases provide additional views of the data while maintaining semantic consistency, enhancing the contrastive learning objective."
  - [section] "These models can generate paraphrases of a sentence with different syntax, not just by changing the lexicalization."
  - [corpus] FMR scores range from 0.54-0.65, indicating moderate relatedness to LM-CPPF's core ideas of data augmentation and prompt-based learning.

### Mechanism 2
- Claim: Few-shot paraphrasing using large language models is more effective than traditional data augmentation methods like back-translation and easy data augmentation.
- Mechanism: The method leverages the knowledge encoded in large language models to generate high-quality paraphrases with minimal examples, providing more semantically consistent augmentations than methods that alter syntax or vocabulary without understanding context.
- Core assumption: Large language models have sufficient knowledge to generate paraphrases that maintain semantic meaning while providing syntactic diversity.
- Evidence anchors:
  - [abstract] "Our experiments on six classification tasks show that LM-CPPF outperforms other data augmentation methods like back-translation, easy data augmentation, and multiple templates."
  - [section] "We believe that LLMs generate high-quality paraphrases due to their encoded semantic and sentence structure knowledge."
  - [corpus] Related papers like "MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning" suggest ongoing interest in improving data augmentation for prompt-based learning.

### Mechanism 3
- Claim: The combination of prompt-based fine-tuning and contrastive learning with paraphrasing improves sample efficiency and model generalization.
- Mechanism: By learning from positive and negative examples simultaneously through contrastive learning, the model generates more distinguishable embeddings between classes while using the knowledge from paraphrased views to improve generalization with limited data.
- Core assumption: Contrastive learning with diverse views of the same data improves the model's ability to distinguish between classes with limited training examples.
- Evidence anchors:
  - [abstract] "Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously."
  - [section] "These views are created using different templates for their demonstrations when building prompts."
  - [corpus] "Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models" suggests that combining contrastive learning with other techniques is an active research area.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is the core mechanism that allows the model to learn better representations by comparing similar and dissimilar examples. Understanding how contrastive loss functions and how positive/negative pairs are constructed is essential for implementing and debugging LM-CPPF.
  - Quick check question: What is the difference between contrastive learning and supervised contrastive learning, and why is the supervised version used in LM-CPPF?

- Concept: Prompt-based fine-tuning
  - Why needed here: LM-CPPF builds on prompt-based fine-tuning techniques, which reformulate classification tasks as masked language modeling problems. Understanding how templates and verbalizers work is crucial for creating effective prompts for paraphrasing.
  - Quick check question: How does prompt-based fine-tuning differ from standard fine-tuning, and what role do demonstrations play in LM-BFF?

- Concept: Few-shot learning with large language models
  - Why needed here: The method relies on using large language models (GPT-3, OPT-175B) to generate paraphrases with only a few examples. Understanding in-context learning and the capabilities/limitations of few-shot prompting is essential.
  - Quick check question: What are the key factors that affect the quality of few-shot generation with large language models?

## Architecture Onboarding

- Component map: Pre-trained RoBERTa-base -> Large Language Model (GPT-3/OPT-175B) -> Prompt templates for classification and paraphrasing -> MLM loss function -> Supervised contrastive loss function -> Training pipeline

- Critical path: 1. Prepare training data with K examples per class 2. Generate paraphrases for each example using few-shot prompting with LLM 3. Create two views for each example: original and paraphrased 4. Apply prompt-based fine-tuning with MLM loss on both views 5. Apply supervised contrastive loss between the two views 6. Backpropagate combined loss

- Design tradeoffs:
  - Using OPT-175B vs GPT-3: OPT-175B is more environmentally friendly but may be slower; GPT-3 may be faster but less accessible
  - Number of demonstrations in paraphrasing prompts: More demonstrations may improve paraphrase quality but increase prompt size and cost
  - Batch size: Larger batches may improve contrastive learning but require more memory and computational resources

- Failure signatures:
  - Poor performance improvement: May indicate that paraphrases are not semantically consistent or that contrastive learning is not effective for the task
  - High variance across runs: May indicate instability in paraphrase generation or contrastive learning
  - Memory errors: May indicate that batch size is too large for available GPU memory

- First 3 experiments:
  1. Implement LM-BFF baseline with single template to establish baseline performance on a simple task like SST-2
  2. Add contrastive learning with multiple templates (like Jian et al. 2022) to compare with paraphrasing-based augmentation
  3. Implement few-shot paraphrasing using a smaller, accessible model (like GPT-2) to verify the approach before scaling to GPT-3/OPT-175B

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LM-CPPF scale with different dataset sizes and class imbalances?
- Basis in paper: [inferred] The paper focuses on few-shot settings (K=16 examples per class) but doesn't explore performance across varying dataset sizes or class distributions.
- Why unresolved: The paper only evaluates on datasets with a fixed few-shot setting, leaving the model's behavior under different data scarcity levels and class imbalance scenarios unexplored.
- What evidence would resolve it: Experiments comparing LM-CPPF performance across varying K values (e.g., 1, 4, 8, 16, 32) and imbalanced class distributions would clarify its robustness and limitations.

### Open Question 2
- Question: What is the impact of using different LLMs (beyond GPT-3 and OPT-175B) for few-shot paraphrasing?
- Basis in paper: [explicit] The paper briefly mentions using GPT-2 but only as a comparison point, noting its lower performance compared to larger models.
- Why unresolved: While GPT-3 and OPT-175B are shown to work well, the paper doesn't explore a wider range of LLMs or open-source alternatives that might offer different trade-offs between performance and accessibility.
- What evidence would resolve it: Testing LM-CPPF with various LLMs (e.g., BLOOM, LLaMA, different-sized GPT-2 variants) would reveal the model's flexibility and identify the most effective and practical options for different use cases.

### Open Question 3
- Question: How does the quality of generated paraphrases affect downstream task performance, and can this be measured?
- Basis in paper: [inferred] The paper assumes that LLM-generated paraphrases are of high quality but doesn't explicitly measure or analyze paraphrase quality or its correlation with task performance.
- Why unresolved: The paper focuses on task accuracy but doesn't investigate whether all generated paraphrases contribute equally to performance or if some introduce noise or semantic drift.
- What evidence would resolve it: Analyzing paraphrase quality using metrics like semantic similarity scores (e.g., BERTScore, BLEU) and correlating these with task performance would reveal the relationship between paraphrase quality and model effectiveness.

## Limitations
- Access constraints: Method relies on GPT-3 or OPT-175B for paraphrasing, limiting reproducibility due to API access requirements and costs
- Dataset scope: Evaluation covers only six classification tasks, leaving generalization to other domains and NLP tasks unexplored
- Prompt sensitivity: Limited analysis of how demonstration count, prompt template variations, or selection affect paraphrase quality and performance

## Confidence
- High confidence: LM-CPPF outperforms baseline data augmentation methods (back-translation, EDA, multiple templates) on the six evaluated tasks
- Medium confidence: Few-shot paraphrasing with large language models is more effective than traditional augmentation methods
- Medium confidence: Combination of prompt-based fine-tuning and contrastive learning with paraphrasing improves sample efficiency and generalization

## Next Checks
1. **Accessibility validation**: Implement the paraphrasing component using an open-source model like GPT-2 or T5 to verify whether similar performance gains can be achieved without requiring API access to GPT-3 or OPT-175B.

2. **Hyperparameter sensitivity analysis**: Conduct experiments varying the number of paraphrasing demonstrations (e.g., 5, 10, 15, 20) and different prompt templates to quantify the impact of these design choices on downstream classification accuracy.

3. **Domain generalization test**: Apply LM-CPPF to a new domain (e.g., biomedical text classification or multi-label classification) that was not in the original six tasks to assess whether the approach generalizes beyond the evaluated datasets.