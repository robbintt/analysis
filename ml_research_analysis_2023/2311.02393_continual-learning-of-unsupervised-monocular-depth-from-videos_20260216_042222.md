---
ver: rpa2
title: Continual Learning of Unsupervised Monocular Depth from Videos
arxiv_id: '2311.02393'
source_url: https://arxiv.org/abs/2311.02393
tags:
- depth
- task
- learning
- estimation
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of continual learning for unsupervised
  monocular depth estimation across sequentially encountered datasets with varying
  conditions. The authors propose a novel rehearsal-based dual-memory method, MonoDepthCL,
  that utilizes spatiotemporal consistency to mitigate catastrophic forgetting.
---

# Continual Learning of Unsupervised Monocular Depth from Videos

## Quick Facts
- arXiv ID: 2311.02393
- Source URL: https://arxiv.org/abs/2311.02393
- Authors: Siyuan Zhao, Qixuan Zhang, Mathieu Salzmann, Pascal Fua, Yupei Chen
- Reference count: 40
- Key outcome: Novel rehearsal-based dual-memory method MonoDepthCL achieves superior performance in continual unsupervised monocular depth estimation, outperforming naive continual training and other rehearsal-based methods across four diverse datasets.

## Executive Summary
This paper addresses the challenge of continual learning for unsupervised monocular depth estimation across sequentially encountered datasets with varying conditions. The authors propose MonoDepthCL, a rehearsal-based dual-memory method that utilizes spatiotemporal consistency to mitigate catastrophic forgetting. The method maintains a working model and a context model connected by a spatiotemporal consistency loss, enabling the model to retain knowledge of previous tasks while adapting to new ones. Experimental results demonstrate that MonoDepthCL outperforms naive continual training and other rehearsal-based methods in terms of final performance, overall performance across the learning trajectory, and stability-plasticity trade-off.

## Method Summary
MonoDepthCL is a rehearsal-based dual-memory method for continual learning in unsupervised monocular depth estimation. It consists of a working model that adapts to new tasks and a context model that consolidates knowledge from the working model. The context model is maintained as an exponential moving average of the working model. A spatiotemporal consistency loss is applied between the synthesized targets of both models to enforce consistency. The method utilizes a memory buffer to store samples from previous tasks for rehearsal. The framework for continual unsupervised depth estimation (CUDE) is introduced to evaluate the performance of continual learning methods, consisting of four sequential tasks on different datasets. Three metrics are defined to evaluate model performance: final average, overall average, and stability-plasticity trade-off.

## Key Results
- MonoDepthCL outperforms naive continual training and other rehearsal-based methods in terms of final performance (µfinal), overall performance (µoverall), and stability-plasticity trade-off (SPTO) across four diverse datasets.
- The method demonstrates effectiveness when camera intrinsics are unknown and for longer task sequences.
- Increasing the buffer size leads to a general improvement across metrics for all CL methods, including MonoDepthCL.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The dual-memory rehearsal approach with spatiotemporal consistency loss mitigates catastrophic forgetting in unsupervised monocular depth estimation.
- **Mechanism:** MonoDepthCL maintains two models - a working model that adapts to new tasks and a context model that consolidates knowledge from the working model. The spatiotemporal consistency loss ensures that the synthesized targets from both models are consistent, enforcing spatial consistency in depth maps and temporal consistency in poses.
- **Core assumption:** The context model, maintained as an exponential moving average of the working model, effectively consolidates knowledge without significant degradation.
- **Evidence anchors:**
  - [abstract] "We propose a rehearsal-based dual-memory method, MonoDepthCL, which utilizes spatiotemporal consistency for continual learning in depth estimation, even when the camera intrinsics are unknown."
  - [section 4.2] "The context model is maintained as an exponential moving average (EMA) of the working model [3]."
  - [corpus] Weak - No direct evidence in corpus.
- **Break condition:** If the context model's EMA update frequency is too low or the spatiotemporal consistency loss weight is not properly tuned, the consolidation may not be effective, leading to catastrophic forgetting.

### Mechanism 2
- **Claim:** The framework for continual unsupervised depth estimation (CUDE) effectively captures the challenges of domain and depth range shifts.
- **Mechanism:** CUDE consists of four sequential tasks, each on a different dataset with unique characteristics. This setup allows for the evaluation of model performance under various conditions such as different cameras, diverse weather and lighting conditions, and disparate depth ranges.
- **Core assumption:** The selected datasets and task order in CUDE adequately represent real-world scenarios for continual learning in depth estimation.
- **Evidence anchors:**
  - [abstract] "It consists of a setup of four tasks, each on a different dataset with unique characteristics representative of the challenges of domain and depth range shifts across simulated or real and indoor or outdoor scenes."
  - [section 3] "We, thus, define the CUDE framework with four tasks, each corresponding to an individual dataset as shown in Figure 2."
  - [corpus] Weak - No direct evidence in corpus.
- **Break condition:** If the datasets or task order do not adequately represent real-world scenarios, the framework may not effectively capture the challenges of continual learning in depth estimation.

### Mechanism 3
- **Claim:** The metrics defined in CUDE capture various aspects of continual learning performance.
- **Mechanism:** CUDE defines three metrics - final average (µfinal), overall average (µoverall), and stability-plasticity trade-off (SPTO) - to evaluate model performance. These metrics measure the mean performance on all tasks after training the model on the final task, the mean performance over all seen tasks after training on each task, and the trade-off between retaining performance on previously seen tasks and the capability to learn new tasks, respectively.
- **Core assumption:** The defined metrics adequately capture the performance of continual learning methods in depth estimation.
- **Evidence anchors:**
  - [section 3] "We use the following metrics to evaluate CL models for depth estimation... Hence, µfinal = 1/nt Pnt j=1 Ant,j... SPTO = 2×AS ×AP / (AS +AP)."
  - [corpus] Weak - No direct evidence in corpus.
- **Break condition:** If the defined metrics do not adequately capture the performance of continual learning methods in depth estimation, the evaluation may not be effective.

## Foundational Learning

- **Concept:** Unsupervised monocular depth estimation
  - **Why needed here:** Understanding the basics of unsupervised monocular depth estimation is crucial for grasping the challenges of continual learning in this domain.
  - **Quick check question:** How does unsupervised monocular depth estimation differ from supervised depth estimation, and what are the key challenges in the unsupervised approach?
- **Concept:** Continual learning and catastrophic forgetting
  - **Why needed here:** Knowledge of continual learning and catastrophic forgetting is essential for understanding the motivation behind MonoDepthCL and the CUDE framework.
  - **Quick check question:** What is catastrophic forgetting, and how do continual learning methods aim to mitigate it?
- **Concept:** Domain adaptation and shift
  - **Why needed here:** Understanding domain adaptation and shift is important for comprehending the challenges addressed by the CUDE framework and the effectiveness of MonoDepthCL.
  - **Quick check question:** What is domain adaptation, and how does it relate to the concept of domain shift in continual learning?

## Architecture Onboarding

- **Component map:** Working model -> Context model (EMA) -> Spatiotemporal consistency module
- **Critical path:** The critical path in MonoDepthCL involves updating the working model with the task loss on the union of current and memory batches, updating the context model as an exponential moving average of the working model, and applying the spatiotemporal consistency loss between the synthesized targets of both models.
- **Design tradeoffs:** The design tradeoffs in MonoDepthCL include the choice of buffer size, the update frequency of the context model, and the weight of the spatiotemporal consistency loss. These parameters affect the trade-off between stability and plasticity in the model.
- **Failure signatures:** Failure signatures in MonoDepthCL may include catastrophic forgetting, where the model performance deteriorates on older domains as it learns on newer domains, and overfitting to the memory buffer samples.
- **First 3 experiments:**
  1. Evaluate the performance of MonoDepthCL on the CUDE framework with varying buffer sizes to assess the impact on stability-plasticity trade-off.
  2. Compare the performance of MonoDepthCL with and without the spatiotemporal consistency loss to demonstrate its effectiveness.
  3. Test the performance of MonoDepthCL when the camera intrinsics are unknown to validate its applicability in real-world scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MonoDepthCL method perform when applied to other dense prediction tasks, such as semantic segmentation or object detection, in a continual learning setting?
- Basis in paper: [inferred] The paper focuses on continual learning for unsupervised monocular depth estimation and introduces the CUDE framework for evaluation. While the method shows effectiveness for depth estimation, its applicability to other dense prediction tasks is not explored.
- Why unresolved: The paper does not provide experiments or analysis on the performance of MonoDepthCL for other dense prediction tasks. It is unclear whether the spatiotemporal consistency loss and dual-memory approach would be as effective for tasks with different characteristics.
- What evidence would resolve it: Conducting experiments with MonoDepthCL on other dense prediction tasks, such as semantic segmentation or object detection, and comparing its performance to existing continual learning methods would provide evidence of its generalizability.

### Open Question 2
- Question: How does the choice of buffer size impact the performance of MonoDepthCL, and is there an optimal buffer size for different task sequences or dataset characteristics?
- Basis in paper: [explicit] The paper mentions that increasing the buffer size leads to a general improvement across metrics for all CL methods, including MonoDepthCL. However, the optimal buffer size is not discussed.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between buffer size and performance. It is unclear whether there is a trade-off between buffer size and other factors, such as computational cost or memory requirements.
- What evidence would resolve it: Conducting experiments with different buffer sizes for various task sequences and dataset characteristics, and analyzing the impact on performance metrics, would help identify the optimal buffer size for different scenarios.

### Open Question 3
- Question: How does the performance of MonoDepthCL compare to other state-of-the-art continual learning methods for unsupervised monocular depth estimation, such as those based on regularization or parameter isolation techniques?
- Basis in paper: [inferred] The paper mentions that CL methods such as regularization, parameter isolation, and rehearsal have been used to address catastrophic forgetting in image classification. However, it does not provide a comparison of MonoDepthCL with these methods for depth estimation.
- Why unresolved: The paper focuses on comparing MonoDepthCL with naive continual training and joint training, but does not include a comparison with other state-of-the-art CL methods specifically designed for depth estimation.
- What evidence would resolve it: Conducting experiments to compare the performance of MonoDepthCL with other state-of-the-art CL methods, such as those based on regularization or parameter isolation, on the CUDE framework would provide insights into its relative effectiveness.

## Limitations

- The architectural details of the depth and ego-motion networks are underspecified beyond "ResNet-18 backbone," which may affect reproducibility.
- The spatiotemporal consistency loss implementation details are not fully specified, particularly regarding the exact computation of SSIM and photometric error.
- The memory buffer size and update frequency were selected based on empirical observation but not extensively explored across a wider range of values.

## Confidence

- **High confidence**: The core mechanism of dual-memory rehearsal with spatiotemporal consistency is well-specified and theoretically grounded. The improvement over naive continual training is consistently demonstrated across all evaluation metrics.
- **Medium confidence**: The effectiveness of the CUDE framework as a benchmark for continual depth estimation, as the selection of four datasets may not fully capture all real-world variations in domain shift and depth range.
- **Medium confidence**: The scalability of the method to longer task sequences, as the extension to five tasks (DDAD) shows consistent performance but may not represent all possible real-world scenarios.

## Next Checks

1. Implement ablation studies varying the context model update frequency (EMA coefficient) and spatiotemporal consistency loss weight to determine optimal hyperparameters and robustness to these choices.
2. Test the method on additional datasets not included in the CUDE framework to validate generalizability to different types of domain shifts and depth ranges.
3. Evaluate the computational overhead introduced by the dual-memory architecture and spatiotemporal consistency loss compared to baseline methods, particularly for real-time applications.