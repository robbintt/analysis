---
ver: rpa2
title: 'The Counterattack of CNNs in Self-Supervised Learning: Larger Kernel Size
  might be All You Need'
arxiv_id: '2312.05695'
source_url: https://arxiv.org/abs/2312.05695
tags:
- learning
- arxiv
- vision
- pages
- cnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether CNNs can match the self-supervised
  learning (SSL) performance of Vision Transformers (ViTs). The authors find that
  vanilla ConvNeXt architectures perform worse than ViTs in SSL tasks.
---

# The Counterattack of CNNs in Self-Supervised Learning: Larger Kernel Size might be All You Need

## Quick Facts
- arXiv ID: 2312.05695
- Source URL: https://arxiv.org/abs/2312.05695
- Authors: 
- Reference count: 40
- Primary result: Modified ConvNeXt with 9x9 kernels and BatchNorm achieves comparable or better SSL performance than state-of-the-art Transformers

## Executive Summary
This paper challenges the dominance of Vision Transformers in self-supervised learning by demonstrating that CNNs, when properly modernized, can match or exceed Transformer performance. The authors develop BC-SSL, a pure CNN architecture based on ConvNeXt, which achieves state-of-the-art SSL results by making two simple modifications: adding BatchNorm layers after depthwise convolutions and scaling up kernel sizes to 9x9. BC-SSL achieves comparable ImageNet classification accuracy to Swin Transformers while being more efficient, and demonstrates superior robustness to distribution shifts.

## Method Summary
The paper modifies the ConvNeXt architecture by adding BatchNorm layers after depthwise convolutions and scaling up kernel sizes from 3x3 to 9x9. These models are pre-trained using the DINO self-supervised learning framework on ImageNet-1K for 100 epochs. The trained models are evaluated through linear probing, k-NN classification, and transfer learning to MS COCO for object detection and segmentation tasks. The authors systematically ablate these modifications to understand their impact on SSL performance.

## Key Results
- BC-SSL-T with 9x9 kernels achieves 79.0% linear probe accuracy on ImageNet, matching Swin-S performance
- BC-SSL-T pre-trained for 100 epochs matches the MS COCO detection/segmentation performance of Swin-T pre-trained for 300 epochs
- Larger kernels (9x9) consistently improve robustness against distribution shifts across multiple benchmarks
- Adding BatchNorm after depthwise convolutions provides consistent performance gains across various batch sizes

## Why This Works (Mechanism)

### Mechanism 1
Large convolutional kernels provide larger effective receptive fields that better capture spatial context and object structure. Scaling up kernel size from 3x3 to 9x9 increases spatial support for each convolution, allowing integration of more contextual information from neighboring pixels during representation learning. The benefits of large kernels in supervised learning transfer directly to self-supervised learning regimes. Evidence shows increasing kernels from 7×7 to 9×9 further increases performance by 0.97%, outperforming Swin Transformers. Break condition: performance degrades beyond optimal range (observed at 15x15).

### Mechanism 2
Adding BatchNorm layers after depthwise convolutions stabilizes training and improves representation quality in SSL. Batch normalization normalizes activations after large depthwise convolutions, reducing internal covariate shift and allowing more stable gradient flow during self-supervised pre-training. BatchNorm is crucial for SSL stability even when it may be omitted in supervised settings. Evidence shows this small modification consistently improves performance across various batch sizes. Break condition: performance drops significantly if BatchNorm layers are removed or replaced with LayerNorm.

### Mechanism 3
Modern large-kernel CNN architectures can achieve comparable or better SSL performance than Transformers without attention mechanisms. By modernizing ConvNeXt with larger kernels, depthwise convolutions, and inverted bottleneck design, the model learns representations that match or exceed Transformer performance in SSL tasks. The architectural improvements in ConvNeXt are sufficient to close the SSL performance gap between CNNs and Transformers. Evidence shows BC-SSL-T boosts linear classification accuracy to 79.0%, matching Swin-S. Break condition: performance gap reappears if architecture reverts to standard ResNet or kernel sizes are reduced.

## Foundational Learning

- Concept: Self-supervised learning (SSL) frameworks
  - Why needed here: Understanding SSL is crucial as this paper's core contribution is demonstrating CNN performance in SSL tasks
  - Quick check question: What is the main difference between contrastive learning and non-contrastive methods like BYOL or DINO?

- Concept: Convolutional neural network architectures and design choices
  - Why needed here: The paper builds upon ConvNeXt architecture, requiring understanding of kernel sizes, depthwise convolutions, and inverted bottlenecks
  - Quick check question: How does an inverted bottleneck design differ from standard convolutional blocks?

- Concept: Transfer learning and evaluation protocols
  - Why needed here: The paper evaluates SSL performance through linear probing, k-NN classification, and downstream tasks like object detection
  - Quick check question: What is the difference between linear probing and fine-tuning in transfer learning?

## Architecture Onboarding

- Component map: Modified ConvNeXt backbone with 9x9 kernels and BatchNorm after depthwise convolutions -> DINO SSL framework -> Linear probe/k-NN evaluation and MS COCO transfer

- Critical path: 1) Pre-train BC-SSL on ImageNet-1K using DINO 2) Evaluate linear probing and k-NN performance 3) Transfer to downstream MS COCO tasks 4) Assess robustness on out-of-distribution datasets

- Design tradeoffs: Larger kernels improve performance but increase computational cost; BatchNorm improves SSL stability but adds parameters; pure CNN approach trades some Transformer benefits (global context) for efficiency

- Failure signatures: Performance drops when kernel size exceeds optimal range (observed at 15x15); reduced SSL performance without BatchNorm after depthwise convolutions; suboptimal results when using standard ResNet instead of modernized ConvNeXt

- First 3 experiments: 1) Implement vanilla ConvNeXt with 7x7 kernels and compare to BC-SSL with 9x9 kernels using DINO pre-training 2) Test impact of removing BatchNorm after depthwise convolutions while keeping 9x9 kernels 3) Evaluate transfer learning performance on MS COCO with different kernel sizes (3x3, 7x7, 9x9) after 100-epoch pre-training

## Open Questions the Paper Calls Out

### Open Question 1
Does increasing kernel size beyond 9x9 continue to improve robustness against distribution shifts in BC-SSL architectures? The authors observe that robustness of BC-SSL monotonically improves as kernel size scales up to 15x15, performing an all-around win over Swin-T. However, they note that while the 15x15 kernel undergoes a small clean accuracy drop compared to 9x9 kernel, it brings a notable improvement in robustness. The relationship between kernel size and robustness beyond 15x15 remains unexplored.

### Open Question 2
How do other large-kernel CNN architectures like RepLKNet and SLaK compare to BC-SSL in self-supervised learning performance? The authors state they are aware of other CNN architectures positively equipped with even larger kernels like RepLKNet and SLaK, which could also be competitive baselines in this regime. They hypothesize these architectures might benefit more from increasing kernels further than BC-SSL. The paper focuses specifically on BC-SSL and does not evaluate RepLKNet or SLaK in the SSL context.

### Open Question 3
What is the fundamental mechanism by which larger kernels improve self-supervised learning performance in CNNs? While the paper demonstrates the empirical benefit of larger kernels and provides qualitative visualizations showing larger kernels lead to better coverage of labeled objects, it doesn't explain the underlying reason for this improvement. The authors don't provide theoretical analysis of why this architectural change is particularly effective for self-supervised learning.

## Limitations
- Empirical evidence for mechanisms comes almost entirely from the same study, creating circular validation
- Performance comparisons against Transformers may be influenced by specific hyperparameter choices in DINO training
- Kernel size optimization shows an optimal range but lacks theoretical justification for why this range is optimal

## Confidence

- **High confidence**: The empirical observation that BC-SSL with 9x9 kernels achieves comparable ImageNet classification performance to SSL-trained Transformers
- **Medium confidence**: The claim that BatchNorm after depthwise convolutions improves SSL stability and performance
- **Low confidence**: The theoretical explanation for why larger kernels specifically benefit SSL representation learning

## Next Checks

1. Implement BC-SSL architecture without BatchNorm after depthwise convolutions and train under identical DINO conditions to empirically verify the claimed 1-2% performance drop

2. Train BC-SSL with kernel sizes of 5x5, 11x11, and 15x15 using the same 100-epoch schedule to map the full performance curve and identify the true optimal range

3. Compare BC-SSL performance when pre-trained with alternative SSL frameworks (MoCo v3, SimSiam) to determine if the architectural advantages generalize beyond DINO