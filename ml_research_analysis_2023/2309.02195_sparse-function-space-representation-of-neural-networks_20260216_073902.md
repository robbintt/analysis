---
ver: rpa2
title: Sparse Function-space Representation of Neural Networks
arxiv_id: '2309.02195'
source_url: https://arxiv.org/abs/2309.02195
tags:
- data
- sparse
- learning
- dual
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses uncertainty quantification and sequential
  learning in deep neural networks (NNs) by converting them from weight space to function
  space via a dual parameterization. The authors introduce a sparse function-space
  representation (SFR) of NNs, which enables a principled way to capture uncertainty
  and incorporate new data without retraining.
---

# Sparse Function-space Representation of Neural Networks

## Quick Facts
- arXiv ID: 2309.02195
- Source URL: https://arxiv.org/abs/2309.02195
- Reference count: 14
- Key outcome: Converts trained neural networks to sparse Gaussian processes in function space, enabling uncertainty quantification and sequential learning without retraining

## Executive Summary
This paper introduces a sparse function-space representation (SFR) that transforms trained neural networks from weight space to function space using a dual parameterization. The method converts a trained NN into a sparse Gaussian process (GP) that captures information from the entire dataset, enabling principled uncertainty quantification without expensive sampling in weight space. SFR also allows incorporation of new data without retraining while maintaining predictive performance, addressing key limitations of traditional Bayesian neural network approaches.

## Method Summary
The method trains a standard neural network (typically a two-layer MLP), then applies Laplace approximation to obtain a Bayesian NN representation. Using the generalized Gauss-Newton (GGN) approximation, the method computes dual parameters α and β that parameterize the GP posterior. These parameters are then sparsified using inducing points to create a sparse GP representation without requiring further optimization. The sparse dual parameters (αu, Bu) enable uncertainty quantification and allow conditioning on new data for sequential learning scenarios.

## Key Results
- SFR performs on par with Laplace approximation when prior precision is tuned, and outperforms it when prior precision is not tuned
- SFR captures information from the entire dataset more effectively than GP subset methods, maintaining predictive performance with fewer inducing points
- The method successfully demonstrates sequential learning capability by incorporating new data without retraining the original network

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFR enables principled uncertainty quantification without expensive sampling in weight space
- Mechanism: SFR converts a trained neural network from weight space to function space via a dual parameterization, creating a sparse Gaussian process (GP) that captures information from the entire dataset
- Core assumption: The dual parameterization of the GP posterior (parameters α and β) can be derived directly from a trained NN without requiring further optimization
- Evidence anchors:
  - [abstract]: "We present a method that mitigates these issues by converting NNs from weight space to function space, via a dual parameterization."
  - [section]: "In contrast to previous work that utilizes subsets of training data, our parameterization captures information from all data points in a sparse representation."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.451, average citations=0.0. Weak evidence of similar sparsity approaches in function space

### Mechanism 2
- Claim: SFR can incorporate new data without retraining while maintaining predictive performance
- Mechanism: By conditioning the sparse GP on new data using the precomputed dual parameters, SFR updates predictions without requiring full retraining of the original NN
- Core assumption: The sparse GP representation is sufficient to capture the function space of the original NN such that new data can be incorporated through GP conditioning rules
- Evidence anchors:
  - [abstract]: "enables us to incorporate new data without retraining whilst maintaining predictive performance."
  - [section]: "Importantly, the dual parameterization can be used to (i) sparsify the GP without requiring further optimization (e.g., variational inference) whilst capturing information from all data points, and (ii) incorporate new data without retraining by conditioning on new data."

### Mechanism 3
- Claim: SFR outperforms traditional Laplace approximation methods when prior precision is not tuned
- Mechanism: SFR captures information from the entire dataset in its sparse representation, reducing sensitivity to prior precision tuning compared to methods that rely on subset-based approximations
- Core assumption: The full-data sparse representation provides more robust uncertainty estimates than subset-based methods regardless of prior precision choice
- Evidence anchors:
  - [section]: "Interestingly, when the prior precision (δ) is not tuned (left), SFR outperforms all other methods."
  - [section]: "SFR leverages a dual parameterization to construct a sparse GP, which captures information from the entire data set. In contrast, Immer et al. (2021b) utilize a data subset which ignores information from the rest of the data set."

## Foundational Learning

- Concept: Bayesian Neural Networks and Uncertainty Quantification
  - Why needed here: SFR builds on BNN foundations to provide uncertainty estimates in function space rather than weight space
  - Quick check question: How does a Bayesian Neural Network differ from a standard neural network in terms of parameter estimation?

- Concept: Gaussian Processes and Kernel Methods
  - Why needed here: SFR converts the NN to a GP representation using the Neural Tangent Kernel (NTK), requiring understanding of GP fundamentals
  - Quick check question: What is the relationship between a GP's mean function, covariance function, and uncertainty estimates?

- Concept: Sparse Gaussian Processes and Inducing Points
  - Why needed here: SFR uses sparse GP techniques to scale to large datasets while maintaining information from all data points
  - Quick check question: How do inducing points reduce computational complexity in GP inference?

## Architecture Onboarding

- Component map: Trained NN -> Laplace Approximation -> Dual Parameterization (α, β) -> Sparse GP Construction (inducing points Z) -> Uncertainty Quantification
- Critical path: Train NN → Compute dual parameters → Construct sparse GP → Make predictions with uncertainty
- Design tradeoffs: Full-data representation vs. computational efficiency; prior precision sensitivity vs. robustness
- Failure signatures: Poor uncertainty calibration when prior precision is mis-specified; degradation in performance with very large datasets
- First 3 experiments:
  1. Replicate UCI classification experiments comparing SFR to Laplace approximation with/without prior tuning
  2. Test SFR's ability to incorporate new data by simulating sequential learning scenarios
  3. Evaluate sensitivity to the number of inducing points M to find optimal sparsity level

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and scope of the work, several open questions emerge from the content:

### Open Question 1
- Question: How does the choice of neural network architecture (e.g., number of layers, activation functions) affect the quality of the function-space representation and uncertainty estimates in SFR?
- Basis in paper: [explicit] The paper mentions that "the architecture of the NN and the choice of activation functions can be used to implicitly specify the prior assumptions," but does not explore this relationship in detail
- Why unresolved: The paper focuses on proof-of-concept demonstrations with a specific architecture (two-layer MLP with tanh activations) and does not investigate the impact of architectural choices on SFR's performance
- What evidence would resolve it: Experiments comparing SFR's performance across different NN architectures and activation functions on the same datasets, analyzing how these choices influence the resulting GP's prior and posterior

### Open Question 2
- Question: Can SFR be extended to handle non-stationary data or data with complex dependencies that are not well-captured by the neural tangent kernel?
- Basis in paper: [inferred] The paper relies on the neural tangent kernel (NTK) for the GP formulation, which is based on a first-order linearization of the NN. This suggests that SFR might struggle with data that exhibits non-linear or non-stationary patterns that the NTK cannot capture
- Why unresolved: The paper does not discuss the limitations of the NTK or explore potential extensions of SFR to handle more complex data structures
- What evidence would resolve it: Experiments on datasets with known non-stationary patterns or complex dependencies, comparing SFR's performance to other methods that can handle such data. Additionally, theoretical analysis of the NTK's limitations and potential modifications to SFR to address them

### Open Question 3
- Question: How does SFR compare to other sparse GP methods in terms of computational efficiency and scalability when dealing with very large datasets?
- Basis in paper: [explicit] The paper states that SFR reduces computational complexity from O(N^3) to O(M^3) by using a sparse representation, but does not provide a detailed comparison with other sparse GP methods in terms of runtime or memory usage
- Why unresolved: The paper focuses on the theoretical advantages of SFR's dual parameterization but does not provide empirical evidence of its computational benefits compared to other methods
- What evidence would resolve it: Experiments comparing the runtime and memory usage of SFR to other sparse GP methods (e.g., variational sparse GPs, inducing point methods) on large datasets with varying numbers of data points and inducing points

## Limitations
- Limited empirical validation across diverse architectures and datasets beyond the proof-of-concept UCI benchmarks
- Insufficient exploration of the sensitivity to inducing point selection strategy and number of inducing points
- Lack of detailed computational complexity analysis and runtime comparisons with other sparse GP methods

## Confidence
- **High Confidence**: The fundamental mathematical framework of dual parameterization and its ability to convert trained NNs to sparse GPs is well-established and theoretically sound
- **Medium Confidence**: The claim that SFR outperforms Laplace approximation when prior precision is not tuned, based on the single UCI benchmark evaluation
- **Medium Confidence**: The sequential learning capability of incorporating new data without retraining, as demonstrated on limited experimental settings

## Next Checks
1. **Architecture Robustness Test**: Evaluate SFR across different NN architectures (CNNs, ResNets) and dataset types (image, text, time-series) to verify generalizability beyond the two-layer MLPs used in the paper
2. **Inducing Point Sensitivity Analysis**: Systematically test the impact of different inducing point selection strategies (random, k-means, learned) and varying numbers of inducing points on both performance and computational efficiency
3. **Large-Scale Dataset Evaluation**: Test SFR on large-scale datasets (ImageNet-scale) to assess scalability limitations and compare computational requirements against baseline methods in real-world scenarios