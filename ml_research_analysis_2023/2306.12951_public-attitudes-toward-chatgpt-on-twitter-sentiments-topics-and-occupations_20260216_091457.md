---
ver: rpa2
title: 'Public Attitudes Toward ChatGPT on Twitter: Sentiments, Topics, and Occupations'
arxiv_id: '2306.12951'
source_url: https://arxiv.org/abs/2306.12951
tags:
- chatgpt
- sentiment
- tweets
- arxiv
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzed public sentiment toward ChatGPT using Twitter
  data. The authors collected 174,840 tweets from Dec 2022 - Jun 2023, cleaned and
  filtered the data, and applied sentiment analysis and topic modeling.
---

# Public Attitudes Toward ChatGPT on Twitter: Sentiments, Topics, and Occupations

## Quick Facts
- arXiv ID: 2306.12951
- Source URL: https://arxiv.org/abs/2306.12951
- Reference count: 16
- Public sentiment toward ChatGPT on Twitter was largely neutral to positive, with negative sentiment decreasing over time.

## Executive Summary
This paper analyzes public sentiment toward ChatGPT using 174,840 tweets collected from December 2022 to June 2023. The authors applied sentiment analysis and topic modeling to understand how Twitter users discuss ChatGPT. Overall sentiment was found to be mostly neutral to positive, with negative sentiment declining over time. The most discussed topics included AI, search engines, education, and writing. Occupation analysis revealed that arts and entertainment professionals tweeted most frequently about ChatGPT, though sentiment distributions were similar across occupations.

## Method Summary
The authors collected tweets containing #ChatGPT from December 2022 to June 2023, then cleaned the data by removing bots, near-duplicates, and irrelevant content. Sentiment analysis was performed using the XLM-T model, which was validated against 1,000 manually labeled samples. Topic modeling was conducted using BERTopic to identify 938 topics, with the top 15 clustered into three categories: future AI discourse, practical applications, and societal disruption. Occupation extraction matched user descriptions to U.S. SOC occupation groups, though this failed for 36.5% of tweets.

## Key Results
- Overall sentiment was 45% neutral, 34% positive, and 20% negative
- Negative sentiment decreased over the study period
- Top topics: AI, search engines, education, writing, and job replacement
- Arts/entertainment occupation group tweeted most frequently about ChatGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Twitter sentiment about ChatGPT is dominated by neutral to mildly positive tones, with declining negativity over time.
- Mechanism: The authors applied a multilingual transformer model (XLM-T) to 174,840 tweets, cleaned bot and near-duplicate content, and classified sentiment into negative, neutral, and positive. The resulting distribution shows ~45% neutral, ~34% positive, and ~20% negative.
- Core assumption: The XLM-T model, tuned for Twitter and multilingual text, accurately reflects the underlying sentiment distribution in the dataset.
- Evidence anchors:
  - [abstract] "overall sentiment was largely neutral to positive, and negative sentiments were decreasing over time."
  - [section 3.3] XLM-T selected as best performing model on 1,000 manually labeled samples.
  - [corpus] Found 5 closely related studies using similar sentiment+topic methods on ChatGPT data.
- Break condition: If the sentiment model systematically misclassifies negative tweets (as the error analysis suggests), the reported decline in negativity could be overestimated.

### Mechanism 2
- Claim: Topics discussed about ChatGPT cluster into three categories: future AI discourse, practical applications, and societal disruption.
- Mechanism: BERTopic applied to tweet embeddings identified 938 topics, with the top 15 clustered into: AI hype/future (e.g., "Artificial Intelligence", "Hype"), usage domains (e.g., "Education", "Coding", "Writing"), and disruption debates (e.g., "Job Replacement", "Search Engines"). Topic frequencies and example tweets confirm each cluster.
- Core assumption: Topic modeling on tweet embeddings preserves topical coherence without requiring pre-labeled categories.
- Evidence anchors:
  - [section 4.2] BERTopic extracts 938 topics; Table 1 lists top 15 with example tweets.
  - [abstract] "most popular topics discussed were Education, Bard, Search Engines, OpenAI, Marketing, and Cybersecurity."
  - [corpus] One related study used LDA topic modeling to identify similar thematic clusters.
- Break condition: If high-similarity topics were merged too aggressively (cosine > 0.7), subtle but distinct themes could be lost, skewing category counts.

### Mechanism 3
- Claim: Occupation groups post about ChatGPT in similar sentiment distributions, but topic preferences differ by job relevance.
- Mechanism: User descriptions were matched to U.S. SOC occupation groups; sentiment was recomputed per occupation. While overall sentiment per occupation mirrored the global distribution, specific topics (e.g., "Cybersecurity" for computer/math roles, "Education" for teaching) were more prevalent in relevant occupations.
- Core assumption: User-provided descriptions reliably encode occupation, and topic preferences reflect job relevance rather than sampling bias.
- Evidence anchors:
  - [section 3.5] Occupation extraction from user descriptions using SOC list with expansions.
  - [section 4.3] Figure 4 shows similar sentiment distributions across occupations.
  - [abstract] "people tended to tweet about topics relevant to their occupation."
- Break condition: If many users omitted occupation or used ambiguous terms, the matching step would underrepresent certain groups, biasing topic-occupation correlations.

## Foundational Learning

- Concept: Sentiment analysis model selection and evaluation
  - Why needed here: Choosing the right model (XLM-T) and validating it against human-labeled data ensures trustworthy sentiment metrics.
  - Quick check question: What recall did XLM-T achieve on the 1,000-sample validation set?

- Concept: Topic modeling with transformer embeddings
  - Why needed here: BERTopic leverages sentence-transformers to cluster semantically similar tweets into coherent topics without pre-labeled categories.
  - Quick check question: How many total topics did BERTopic extract from the full dataset?

- Concept: Occupation extraction from unstructured text
  - Why needed here: Mapping user descriptions to SOC occupations allows demographic sentiment and topic analysis.
  - Quick check question: What percentage of tweets had extractable occupation information?

## Architecture Onboarding

- Component map:
  - Data pipeline: Kaggle source -> bot/near-duplicate removal -> URL/handle cleaning -> sentiment analysis -> topic modeling -> occupation extraction -> analysis
  - Models: XLM-T (sentiment), BERTopic (topics), SOC-based regex/matching (occupations)
  - Storage: Cleaned tweet table with columns for text, user_desc, occupation, sentiment label, topic IDs

- Critical path:
  1. Clean and deduplicate data
  2. Run XLM-T for sentiment labels
  3. Apply BERTopic to embed and cluster tweets
  4. Match user descriptions to occupations
  5. Aggregate and visualize sentiment/topic/occupation statistics

- Design tradeoffs:
  - Strict bot removal reduces sample size but improves sentiment accuracy
  - Removing hashtag symbols improves model recall but loses potential topic cues
  - Occupation extraction misses 36.5% of tweets, limiting demographic insights

- Failure signatures:
  - Sentiment distribution skewed toward positive (model bias)
  - Too many topics merged (cosine threshold too high)
  - Occupation extraction fails for non-English or vague descriptions

- First 3 experiments:
  1. Validate XLM-T on a held-out labeled set and inspect confusion matrix
  2. Vary BERTopic `min_similarity` threshold and compare topic coherence scores
  3. Test occupation extraction recall with a manually labeled subset of user descriptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying reasons for the variation in sentiment across different topics discussed about ChatGPT?
- Basis in paper: [explicit] The paper found that the majority of sentiment scores for the top 15 topics were between neutral and mildly positive, with 'Language Models' having the highest sentiment score and 'Job Replacement' having the lowest. The paper also mentions that 'Job Replacement' was the only topic with a slightly negative sentiment.
- Why unresolved: The paper does not provide a detailed explanation for why certain topics elicit different sentiments, such as why 'Language Models' has a higher sentiment score compared to 'Job Replacement'.
- What evidence would resolve it: A qualitative analysis of the content and context of tweets within each topic could provide insights into why certain topics are viewed more positively or negatively. This could involve examining the specific language used, the concerns or praises expressed, and the broader context in which these topics are discussed.

### Open Question 2
- Question: How does the sentiment towards ChatGPT vary across different occupations, and what factors contribute to these differences?
- Basis in paper: [explicit] The paper found that the 'teaching' occupation had the highest proportion of positive sentiments towards ChatGPT, while 'arts and entertainment' had the lowest. Additionally, the 'research' occupation had the highest proportion of negative sentiments, while 'sales' had the lowest.
- Why unresolved: The paper does not explore the reasons behind these variations in sentiment across occupations. It does not investigate whether these differences are due to the nature of the work, the potential impact of ChatGPT on these professions, or other factors.
- What evidence would resolve it: A deeper investigation into the specific concerns, benefits, and applications of ChatGPT within each occupation could help explain the differences in sentiment. Surveys or interviews with individuals from different occupations could provide qualitative data to complement the quantitative findings.

### Open Question 3
- Question: How do the sentiments towards ChatGPT change over time, and what events or developments influence these changes?
- Basis in paper: [explicit] The paper mentions that negative sentiments towards ChatGPT were decreasing over time, but it does not provide a detailed analysis of how sentiments evolve or what specific events might influence these changes.
- Why unresolved: The paper does not track sentiment changes over the entire period of data collection or correlate these changes with specific events, such as updates to ChatGPT, public controversies, or new applications of the technology.
- What evidence would resolve it: A time-series analysis of sentiment data, correlated with key events and developments related to ChatGPT, could reveal patterns and triggers for changes in public sentiment. This could involve tracking sentiment trends alongside major announcements, policy changes, or significant public discussions about ChatGPT.

## Limitations
- Occupation extraction failed for 36.5% of tweets, limiting demographic analysis
- Sentiment analysis model showed systematic misclassification of negative tweets
- Topic merging threshold may have obscured nuanced discussions

## Confidence
- Overall sentiment distribution: High
- Topic clustering and categorization: Medium
- Occupation-based analysis: Low

## Next Checks
1. Conduct error analysis on the XLM-T model's performance specifically for negative sentiment classification to quantify potential bias in the declining negativity trend
2. Perform topic coherence evaluation with varying similarity thresholds to assess whether the current threshold (0.7) appropriately balances topic granularity with semantic coherence
3. Create a validation subset of user descriptions with manual occupation labeling to measure recall and precision of the automated extraction method, particularly for the 36.5% of tweets where no occupation was identified