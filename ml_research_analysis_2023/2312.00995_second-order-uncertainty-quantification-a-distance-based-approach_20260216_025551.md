---
ver: rpa2
title: 'Second-Order Uncertainty Quantification: A Distance-Based Approach'
arxiv_id: '2312.00995'
source_url: https://arxiv.org/abs/2312.00995
tags:
- uncertainty
- second-order
- measures
- distribution
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of quantifying uncertainty in
  machine learning predictions, specifically in classification tasks. It focuses on
  second-order probability distributions, which represent uncertainty about the underlying
  probability distribution of outcomes.
---

# Second-Order Uncertainty Quantification: A Distance-Based Approach

## Quick Facts
- arXiv ID: 2312.00995
- Source URL: https://arxiv.org/abs/2312.00995
- Reference count: 40
- One-line primary result: A distance-based framework for second-order uncertainty quantification that overcomes limitations of information-theoretic approaches

## Executive Summary
This paper addresses the challenge of quantifying uncertainty in machine learning predictions for classification tasks by focusing on second-order probability distributions. The authors propose a formal axiomatic framework that meaningful uncertainty measures should satisfy and introduce a distance-based approach using Wasserstein distance to derive such measures. The framework provides a principled way to decompose uncertainty into aleatoric and epistemic components, offering a unified treatment of different uncertainty types.

The paper makes theoretical contributions by proving that the proposed uncertainty measures satisfy all specified axioms and demonstrating practical applicability through Dirichlet distributions. This approach overcomes limitations of existing methods, particularly those based on information theory, by providing a mathematically rigorous foundation for uncertainty quantification in the space of second-order distributions.

## Method Summary
The paper proposes a distance-based framework for quantifying uncertainty in machine learning predictions using second-order probability distributions. The method involves defining uncertainty measures as minimal Wasserstein distances from a given second-order distribution to reference sets representing pure aleatoric or pure epistemic uncertainty. The authors prove that their proposed measures satisfy formal criteria (axioms A0-A8) for meaningful uncertainty quantification and demonstrate the approach with Dirichlet distributions, which are commonly used in Bayesian inference and evidential deep learning.

## Key Results
- The proposed distance-based uncertainty measures satisfy all eight formal axioms (A0-A8) for meaningful uncertainty quantification
- Wasserstein distance provides a well-defined metric on the space of second-order distributions
- The framework offers a unified approach to quantifying both aleatoric and epistemic uncertainty
- Dirichlet distributions can be effectively used within this framework for practical uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distance-based measures capture both aleatoric and epistemic uncertainty in a unified way
- Mechanism: The framework uses optimal transport theory, where uncertainty is quantified as the minimal distance from the predictive distribution to reference sets representing pure aleatoric or pure epistemic uncertainty
- Core assumption: A meaningful distance metric on the space of second-order distributions exists and satisfies desired axioms
- Evidence anchors:
  - [abstract] "we provide a general framework based on distances on the second-order probability level for developing uncertainty measures"
  - [section 3.2] "Roughly speaking, each uncertainty measure (i.e., AU , EU or TU) of Q is defined as the minimal distance ofQ to the corresponding reference set"
  - [corpus] Weak: Related papers discuss scoring rules and Bayesian methods, but don't directly address distance-based approaches
- Break condition: The distance metric fails to satisfy the proposed axioms (A0-A8), or the reference sets are not properly defined

### Mechanism 2
- Claim: Wasserstein distance provides a well-defined metric for second-order distributions
- Mechanism: By viewing second-order distributions as distributions over the probability simplex, the Wasserstein distance inherits metric properties from the underlying space
- Core assumption: The probability simplex with appropriate metric is a valid metric space
- Evidence anchors:
  - [section 4.1] "Lemma 4.1. The second-order Wasserstein distance W : P(P(Y))×P(P(Y)) → R≥0 is a well-defined metric on P(P(Y)"
  - [section 4.1] "Let (P(Y), d1) be a metric space, where Y is defined as before, and d1 is a suitable metric on the space P(Y)"
  - [corpus] Weak: Related papers don't specifically discuss Wasserstein metrics for uncertainty quantification
- Break condition: The Wasserstein distance fails to satisfy metric properties (non-negativity, identity, symmetry, triangle inequality) on the space of second-order distributions

### Mechanism 3
- Claim: The proposed uncertainty measures satisfy all desired axioms for meaningful uncertainty quantification
- Mechanism: By carefully choosing reference sets and the Wasserstein distance, the measures inherit desirable properties from both the distance and the reference sets
- Core assumption: The axioms A0-A8 capture all necessary properties for meaningful uncertainty measures
- Evidence anchors:
  - [section 3.1] "A0 TU, AU, and EU are non-negative" through "A8 TU Y(Q| Y 1 ⊗Q| Y 2) = TUY 1(Q| Y 1)+TU Y 2(Q| Y 2)"
  - [section 4.1] "Theorem 4.1. The uncertainty measures (10-12) with the Wasserstein distance instantiation satisfy A0–A8 as defined in Section 3.1"
  - [corpus] Weak: Related papers discuss scoring rules and Bayesian methods but don't provide a comprehensive axiomatic framework
- Break condition: At least one of the axioms is violated by the proposed measures

## Foundational Learning

- Concept: Second-order probability distributions
  - Why needed here: The paper operates on predictions in the form of distributions over probability distributions, requiring understanding of how to represent and manipulate such objects
  - Quick check question: What is the difference between a first-order distribution p(y) and a second-order distribution Q(p)?

- Concept: Optimal transport and Wasserstein distance
  - Why needed here: The framework uses Wasserstein distance as the metric on the space of second-order distributions, requiring understanding of how this distance is defined and computed
  - Quick check question: How does the Wasserstein distance between two distributions differ from other metrics like KL divergence?

- Concept: Proper scoring rules
  - Why needed here: The paper references proper scoring rules as an alternative approach to uncertainty quantification, requiring understanding of their properties and limitations
  - Quick check question: What makes a scoring rule "proper" and why is this property important for uncertainty quantification?

## Architecture Onboarding

- Component map: Input Q -> Wasserstein distance computation -> Reference set comparison -> Output TU, AU, EU
- Critical path: 1. Receive second-order distribution Q, 2. Compute Wasserstein distance to reference sets, 3. Return uncertainty measures
  - Bottleneck: Computing Wasserstein distance for complex distributions
- Design tradeoffs:
  - Choice of distance metric (Wasserstein vs alternatives)
  - Computational complexity vs accuracy in distance computation
  - Generality of framework vs specificity for Dirichlet distributions
- Failure signatures:
  - Negative uncertainty values (violates A0)
  - AU > TU or EU > TU (violates A3)
  - Non-monotonic behavior under spread-preserving shifts (violates A6)
- First 3 experiments:
  1. Verify metric properties of Wasserstein distance on simple distributions
  2. Test axiom satisfaction for simple cases (Dirac mixtures, uniform distributions)
  3. Compare computational efficiency with existing methods on Dirichlet distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the distance-based framework for uncertainty quantification satisfy all criteria when instantiated with other distance metrics besides the Wasserstein distance?
- Basis in paper: explicit
- Why unresolved: The paper only proves that the proposed measures satisfy all criteria when using the Wasserstein distance. The authors suggest that it would be interesting to instantiate the framework with other metrics and verify if the criteria are met, but this is left as future work.
- What evidence would resolve it: A formal proof or counterexample showing whether the distance-based framework satisfies all criteria when using other distance metrics on probability measures.

### Open Question 2
- Question: How do the proposed distance-based uncertainty measures perform in practice compared to information-theoretic approaches in real-world machine learning tasks?
- Basis in paper: inferred
- Why unresolved: While the paper provides a theoretical framework and proves its properties, it does not include experimental comparisons with existing methods. The authors acknowledge this as a potential avenue for future work.
- What evidence would resolve it: Experimental results comparing the proposed distance-based measures to information-theoretic approaches in various classification tasks, including safety-critical applications.

### Open Question 3
- Question: Can the distance-based framework be extended to handle uncertainty quantification in regression tasks, not just classification?
- Basis in paper: inferred
- Why unresolved: The paper focuses exclusively on classification tasks, using the Dirichlet distribution as an example. While the framework could potentially be adapted for regression, this is not explored in the current work.
- What evidence would resolve it: A theoretical extension of the distance-based framework to regression tasks, along with experimental validation on regression datasets.

## Limitations

- Computational complexity of Wasserstein distance calculations remains a significant practical barrier for high-dimensional distributions
- The axiomatic approach, while comprehensive, may not capture all practically relevant aspects of uncertainty quantification in real-world applications
- The framework's performance compared to information-theoretic approaches in real-world machine learning tasks remains untested

## Confidence

- **High confidence** in the theoretical foundation and proof of metric properties for Wasserstein distance on second-order distributions
- **Medium confidence** in the practical applicability and computational efficiency for complex, high-dimensional distributions
- **Medium confidence** in the completeness of the proposed axiom set (A0-A8) for capturing all meaningful uncertainty properties

## Next Checks

1. Benchmark computational runtime and accuracy against existing uncertainty quantification methods on standard classification datasets with Dirichlet-distributed predictions
2. Test the framework's performance on non-Dirichlet second-order distributions to assess generality
3. Conduct ablation studies removing individual axioms to determine their practical necessity and impact on uncertainty quantification quality