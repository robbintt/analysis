---
ver: rpa2
title: Explainable Reinforcement Learning via a Causal World Model
arxiv_id: '2305.02749'
source_url: https://arxiv.org/abs/2305.02749
tags:
- causal
- divides
- variables
- action
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for explainable reinforcement learning
  by learning a causal world model. The approach uses causal discovery to identify
  the causal structure of the environment without prior knowledge, and fits structural
  equations using attention-based inference networks.
---

# Explainable Reinforcement Learning via a Causal World Model

## Quick Facts
- arXiv ID: 2305.02749
- Source URL: https://arxiv.org/abs/2305.02749
- Reference count: 40
- Primary result: Combines causal discovery with attention-based inference to generate explainable causal chains while maintaining model-based RL performance

## Executive Summary
This paper introduces a framework for explainable reinforcement learning by learning a causal world model from observational data. The approach uses causal discovery to identify the true causal structure of the environment without requiring prior knowledge, then fits structural equations using attention-based inference networks. This allows the derivation of causal chains that explain how actions influence environmental variables and lead to rewards. The framework improves explainability while maintaining accuracy, making it applicable to model-based learning. Experiments demonstrate that the approach produces reasonable explanations in both continuous and discrete action spaces, and achieves performance close to dense models in model-based RL.

## Method Summary
The framework learns a causal world model by first performing causal discovery using Fast CIT to identify a sparse causal graph from transition data. Attention-based inference networks are then trained to fit the structural equations of the discovered causal graph. The model converts between structural causal models (SCMs) and action influence models (AIMs) using attention weights to capture action-specific effects on state transitions. Causal chains are derived through graph search from initial state variables to target reward variables. For model-based RL, the learned causal model is used with PPO algorithm and k-step model rollouts to learn policies that optimize cumulative rewards.

## Key Results
- The causal discovery process effectively prunes spurious correlations, improving action influence weight accuracy
- Attention weights capture action-specific causal influence without requiring separate models per action
- The sparse causal model maintains sufficient accuracy for policy learning while enabling explainability
- Performance approaches that of dense models in model-based RL across tested environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal discovery prunes spurious correlations, improving the accuracy of action influence weights
- Mechanism: By identifying the true causal structure using conditional independence tests, the model avoids misleading edges that appear due to correlation but lack true causality
- Core assumption: Causal faithfulness holds; that is, observed conditional independencies reflect the true causal structure
- Evidence anchors: [section] "Causal discovery is an indispensable process for producing reasonable explanations" and example in Figure 8 showing time as a spurious parent when not using causal discovery

### Mechanism 2
- Claim: Attention weights capture action-specific causal influence without requiring separate models per action
- Mechanism: By encoding parent state variables into contribution vectors and action variables into embeddings, the attention mechanism computes influence weights αj_i for each action-state pair
- Core assumption: The key vectors (k_i) are learned such that attention weights depend only on the numeric action value, not the specific action identity
- Evidence anchors: [section] "the influence weight αj_i captures how much the output variable vj depends on state variable si under the given action a" and the mathematical definition of αj_i in Equations 8-9

### Mechanism 3
- Claim: The sparse causal model maintains sufficient accuracy for policy learning while enabling explainability
- Mechanism: The model balances sparsity (via threshold η) and accuracy; Theorem 3 shows that lower η yields denser graphs and lower prediction error, but still allows useful explanations
- Core assumption: The trade-off between interpretability and accuracy is acceptable for MBRL performance
- Evidence anchors: [section] "We show that our causal model can serve as the bridge between explainability and learning" and experimental results in Figure 6 showing performance close to dense baselines

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: The framework builds on SCMs to represent causal relationships in the environment without requiring pre-specified causal graphs
  - Quick check question: Can you explain the difference between an SCM and an AIM, and why converting between them is useful for this framework?

- Concept: Causal discovery via conditional independence tests
  - Why needed here: The model uses Fast CIT to identify causal edges from observational data, which is essential for building the sparse causal graph
  - Quick check question: What is the role of the d-separation criterion in ensuring the soundness of the discovered causal graph?

- Concept: Attention mechanisms in causal inference
  - Why needed here: Attention weights are used to capture action-specific influence on state transitions, enabling the derivation of causal chains for explanations
- Quick check question: How do the contribution vectors and query vectors in the attention mechanism relate to the computation of influence weights?

## Architecture Onboarding

- Component map: Causal discovery module → Attention-based inference networks → Causal chain analysis → Model-based RL integration
- Critical path: Data → Causal discovery → Inference network training → Policy learning/Explanation generation
- Design tradeoffs:
  - Sparsity vs. accuracy: Higher threshold η yields sparser graphs (better explanations) but lower accuracy
  - Unified SCM vs. per-action AIMs: Avoids data splitting and parameter redundancy but requires careful attention design
  - Attention-only vs. causal-graph-augmented: Attention alone is insufficient; causal discovery is needed to avoid spurious correlations
- Failure signatures:
  - Poor causal discovery: Results in incorrect or missing edges, leading to misleading explanations
  - Attention collapse: Influence weights become uniform or uninformative, failing to capture action-specific effects
  - Overfitting in inference networks: Model memorizes transitions instead of learning causal relationships, harming generalization
- First 3 experiments:
  1. Verify causal discovery on a synthetic environment with known ground-truth AIM and measure accuracy of recovered parent sets
  2. Test attention-based influence weight computation on a simple MDP (e.g., Cartpole) and visualize attention weights for different actions
  3. Integrate the causal model into MBRL on Lunarlander-Discrete and compare learning curves to dense baselines (MLP, Full model)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the causal graph discovery process scale with increasing state and action space dimensions in more complex environments?
- Basis in paper: [inferred] The paper mentions that causal discovery has time complexity O(n^3N log N) where n is the number of variables
- Why unresolved: The paper focuses on demonstrating effectiveness in relatively simple environments but doesn't explore performance limits or scaling behavior in high-dimensional settings
- What evidence would resolve it: Systematic experiments varying environment complexity and measuring causal discovery runtime, accuracy of causal graph identification, and model performance across different dimensionalities

### Open Question 2
- Question: How sensitive is the causal chain explanation quality to the choice of threshold τ for determining salient parent variables?
- Basis in paper: [explicit] The paper mentions "we deﬁne PAa(vj) ∶= {si ∈ PA(vj) /divides.alt0αj i > τ} as the parent set of vj with salient dependencies under the actiona, whereτ ∈ [0, 1] is a given threshold"
- Why unresolved: While the paper discusses the theoretical trade-off between interpretability and accuracy, it doesn't provide systematic evaluation of how different threshold values affect explanation quality or user understanding
- What evidence would resolve it: User studies comparing explanations generated with different threshold values, or quantitative metrics measuring explanation quality across threshold ranges

### Open Question 3
- Question: Can the attention-based inference networks be extended to handle non-stationary environments where causal relationships change over time?
- Basis in paper: [inferred] The paper states "These inference networks should adapt to the structural changes of the causal graph, as the agent's exploratory behaviors may reveal undiscovered causal relationships"
- Why unresolved: The paper assumes static causal relationships throughout training and evaluation, but many real-world environments exhibit non-stationary dynamics
- What evidence would resolve it: Experiments demonstrating the model's performance and explanation quality in environments with temporally varying causal structures, or theoretical analysis of model adaptation mechanisms for non-stationary settings

## Limitations
- Causal discovery assumes causal faithfulness, which may not hold in environments with hidden confounders or selection bias
- The attention mechanism's ability to distinguish subtle action effects in high-dimensional spaces requires further validation
- The framework's performance in environments with complex causal structures remains untested

## Confidence
**High confidence**: The causal discovery mechanism effectively prunes spurious correlations when causal faithfulness holds. The attention mechanism successfully captures action-specific influence weights in controlled experimental settings. The framework maintains competitive performance with dense models in model-based RL.

**Medium confidence**: The conversion between SCM and AIM representations is mathematically sound and enables unified modeling. The threshold-based sparsity-accuracy trade-off provides a useful framework for balancing interpretability and performance. The causal chain analysis methodology produces interpretable explanations that trace action effects to outcomes.

**Low confidence**: The framework's performance in environments with complex causal structures or hidden confounders is untested. The attention mechanism's ability to distinguish subtle action effects in high-dimensional spaces requires further validation. The practical utility of generated explanations for human decision-makers has not been evaluated.

## Next Checks
1. **Causal faithfulness validation**: Test the framework on environments with known violations of causal faithfulness to quantify how often spurious edges are incorrectly identified. Compare performance with and without causal discovery to measure the impact on explanation quality.

2. **Attention mechanism robustness**: Conduct ablation studies where the attention mechanism is deliberately weakened to test whether influence weights become uniform or uninformative. Measure the correlation between attention weight quality and explanation accuracy.

3. **Scalability assessment**: Apply the framework to progressively more complex environments to identify the point at which causal discovery becomes computationally prohibitive or accuracy degrades significantly.