---
ver: rpa2
title: 'LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding'
arxiv_id: '2308.14508'
source_url: https://arxiv.org/abs/2308.14508
tags:
- context
- long
- tasks
- code
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LongBench, the first bilingual (English and
  Chinese) and multi-task benchmark designed to evaluate large language models' long
  context understanding capabilities. LongBench consists of 21 datasets across 6 task
  categories, with sequences averaging 6,711 words (English) and 13,386 characters
  (Chinese).
---

# LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding

## Quick Facts
- arXiv ID: 2308.14508
- Source URL: https://arxiv.org/abs/2308.14508
- Reference count: 23
- Key outcome: Introduces LongBench, the first bilingual (English/Chinese) multi-task benchmark for evaluating long context understanding in LLMs, finding commercial models outperform open-source but struggle with longer contexts, and that scaled positional embedding and fine-tuning on longer sequences lead to substantial improvements.

## Executive Summary
LongBench is a comprehensive benchmark designed to evaluate large language models' ability to understand and process long contexts. It includes 21 datasets across 6 task categories in both English and Chinese, with sequences averaging 6,711 words (English) and 13,386 characters (Chinese). The benchmark covers key long-text applications including single-document and multi-document question answering, summarization, few-shot learning, synthetic tasks, and code completion. Through evaluation of 8 LLMs, the paper reveals that commercial models like GPT-3.5-Turbo-16k outperform open-source alternatives but still struggle on longer contexts, while scaled position embedding and fine-tuning on longer sequences lead to substantial improvements.

## Method Summary
The paper introduces LongBench, a bilingual (English and Chinese) multi-task benchmark for evaluating long context understanding in large language models. The benchmark consists of 21 datasets across 6 task categories, with sequences averaging 6,711 words (English) and 13,386 characters (Chinese). Models are evaluated using zero-shot and few-shot learning approaches, with truncation from the middle when input length exceeds model's maximum context length. The evaluation framework includes metric computation (F1, ROUGE-L, Edit Sim, classification accuracy) and result aggregation across tasks. The paper also introduces LongBench-E, a subset with more evenly distributed context lengths, to reveal performance trends as context length increases.

## Key Results
- Commercial models like GPT-3.5-Turbo-16k outperform open-source models but still struggle on longer contexts
- Scaled position embedding and fine-tuning on longer sequences lead to substantial improvements (62% and 19% relative improvements for ChatGLM2-6B-32k and LongChat-v1.5-7B-32k respectively)
- Context compression techniques like retrieval help models with weak long context abilities but cannot match the performance of models with strong long context understanding
- Even models trained or fine-tuned on longer contexts experience significant performance drops as context length increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard positional embedding scaling enables context length extrapolation beyond training length
- Mechanism: Linear scaling of positional embeddings (RoPE scaling) allows models trained on shorter contexts to handle longer ones by proportionally adjusting position indices
- Core assumption: The model's learned attention patterns remain effective when positions are scaled proportionally
- Evidence anchors:
  - [abstract] "Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding"
  - [section 4.1] "Models benefit from scaled positional embedding and continued training on longer context, as ChatGLM2-6B-32k and LongChat-v1.5-7B-32k obtain relative improvements of 62% and 19%, respectively"
  - [corpus] Weak evidence - neighboring papers mention scaling but don't validate this specific mechanism
- Break condition: When context length exceeds the proportional scaling's effectiveness range, causing position information to become ambiguous

### Mechanism 2
- Claim: Context compression via retrieval improves performance only for models with weak long-context capabilities
- Mechanism: Retrieval-based compression selects relevant segments from long contexts, reducing input length while preserving task-relevant information
- Core assumption: Models with weaker long-context understanding benefit from reduced input complexity
- Evidence anchors:
  - [abstract] "Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability"
  - [section 4.2] "The results suggest that the retrieval technique can only serve as a performance compensation for models that cannot well model long context"
  - [corpus] Moderate evidence - multiple related papers validate retrieval as a compression method but don't test this specific differential effect
- Break condition: When retrieval fails to capture task-relevant information across the full context span

### Mechanism 3
- Claim: Task-specific evaluation reveals limitations of average-score benchmarking
- Mechanism: Individual task analysis exposes performance variations that macro-averages obscure
- Core assumption: Different tasks require different attention patterns and reasoning capabilities
- Evidence anchors:
  - [section 4.1] "we find that synthetic tasks tend to offer a higher level of discernment, where models either achieve a high score or display a near-zero performance"
  - [section 4.1] "These observations suggest that LongBench provides a more comprehensive evaluation result by integrating various types of tasks and languages"
  - [corpus] Strong evidence - multiple related papers cite the need for task-diverse evaluation in long-context settings
- Break condition: When task diversity is reduced or when tasks are too similar in required capabilities

## Foundational Learning

- Concept: Positional encoding and its role in sequence modeling
  - Why needed here: Long context understanding fundamentally depends on maintaining position information across extended sequences
  - Quick check question: What happens to positional information when context length exceeds training length without scaling?
- Concept: Retrieval-augmented generation and context compression
  - Why needed here: The paper evaluates retrieval-based compression as a technique for handling long contexts
  - Quick check question: How does the choice of retriever affect the quality of compressed contexts?
- Concept: Bilingual evaluation and cross-linguistic performance differences
  - Why needed here: The benchmark includes both English and Chinese datasets, requiring understanding of language-specific modeling challenges
  - Quick check question: Why might Chinese character-based models handle different context lengths than word-based English models?

## Architecture Onboarding

- Component map: Dataset ingestion pipeline → standardized format conversion → model evaluation framework → result aggregation system
- Critical path: Dataset standardization → model evaluation → metric computation → result analysis
- Design tradeoffs: Balanced dataset length distribution vs. real-world length representation; automated evaluation vs. human judgment; comprehensive task coverage vs. focused evaluation
- Failure signatures: Inconsistent dataset formatting → evaluation errors; context length truncation → biased results; metric misalignment → invalid comparisons
- First 3 experiments:
  1. Run baseline evaluation on a single dataset with a known model to verify pipeline functionality
  2. Test context length truncation effects by running the same model with different truncation sizes
  3. Validate retrieval-based compression by comparing results with and without compression on a representative dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the evaluated models change when using different position encoding methods beyond scaled RoPE, such as ALiBi or rotary position embeddings?
- Basis in paper: [inferred]
- Why unresolved: The paper only evaluates models using scaled RoPE and linear RoPE scaling, but does not explore the impact of alternative position encoding methods on long context understanding.
- What evidence would resolve it: Conducting experiments with models using different position encoding methods (e.g., ALiBi, rotary position embeddings) on LongBench and comparing their performance to models using scaled RoPE.

### Open Question 2
- What is the impact of increasing the context length beyond the evaluated 32k tokens on model performance, and at what point does the performance plateau or degrade significantly?
- Basis in paper: [inferred]
- Why unresolved: The paper evaluates models with context lengths up to 32k tokens but does not explore the effects of extending the context length further or identify the point of diminishing returns.
- What evidence would resolve it: Conducting experiments with models using context lengths beyond 32k tokens on LongBench and analyzing the performance trends to identify the point of plateau or significant degradation.

### Open Question 3
- How do the evaluated models perform on long context understanding tasks in languages other than English and Chinese, such as low-resource languages or languages with different writing systems?
- Basis in paper: [explicit]
- Why unresolved: The paper focuses on evaluating models on English and Chinese tasks, but does not explore their performance on long context understanding tasks in other languages.
- What evidence would resolve it: Creating and evaluating LongBench datasets for other languages and assessing the performance of the evaluated models on these tasks.

### Open Question 4
- What is the impact of using different context compression techniques, such as extractive summarization or hierarchical compression, on model performance for long context understanding tasks?
- Basis in paper: [explicit]
- Why unresolved: The paper only evaluates the impact of retrieval-based and summarization-based context compression techniques, but does not explore other compression methods like extractive summarization or hierarchical compression.
- What evidence would resolve it: Conducting experiments with models using different context compression techniques (e.g., extractive summarization, hierarchical compression) on LongBench and comparing their performance to the evaluated models.

## Limitations

- Commercial models outperform open-source alternatives but still struggle significantly on longer contexts, with performance degradation even in models specifically trained for extended sequences
- The effectiveness of scaled positional embedding beyond tested context lengths remains uncertain, and the technique may not generalize across all model architectures
- Retrieval-based compression cannot fully compensate for fundamental limitations in modeling extended sequences, and the quality of retrieved information across very long contexts remains questionable

## Confidence

**High Confidence:** The observation that commercial models outperform open-source alternatives on long context tasks is well-supported by direct experimental comparisons across multiple datasets and metrics. The finding that even models trained on longer contexts experience performance drops as context length increases is also strongly validated through the LongBench-E analysis.

**Medium Confidence:** The claim that scaled positional embedding and fine-tuning on longer sequences lead to substantial improvements is supported by specific examples (ChatGLM2-6B-32k and LongChat-v1.5-7B-32k showing 62% and 19% relative improvements), but the generalizability across different model architectures and training regimes requires further validation.

**Low Confidence:** The assertion that context compression techniques cannot match models with strong long-context understanding capabilities is based on limited comparative analysis. The differential effectiveness of retrieval compression for weak vs. strong long-context models, while observed, needs more rigorous testing across a broader range of model capabilities.

## Next Checks

1. **Extended Context Length Testing**: Evaluate the same models on context lengths beyond those tested in the paper (e.g., 64k+ tokens) to determine whether the observed performance degradation patterns continue linearly or if there are critical thresholds where capabilities break down entirely.

2. **Cross-Architecture Positional Scaling**: Test scaled positional embedding across different positional encoding schemes (ALiBi, T5-style, etc.) and model architectures to validate whether the improvements observed with RoPE scaling generalize to other approaches and whether the mechanism is truly architectural or specific to the training procedure.

3. **Retrieval Quality Analysis**: Conduct ablation studies on retrieval-based compression by varying chunk sizes, number of chunks, and retriever quality to quantify the relationship between retrieval effectiveness and final task performance, particularly for tasks requiring global context understanding rather than local relevance.