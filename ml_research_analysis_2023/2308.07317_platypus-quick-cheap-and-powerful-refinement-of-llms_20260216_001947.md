---
ver: rpa2
title: 'Platypus: Quick, Cheap, and Powerful Refinement of LLMs'
arxiv_id: '2308.07317'
source_url: https://arxiv.org/abs/2308.07317
tags:
- questions
- training
- language
- platypus
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Platypus, a family of fine-tuned and merged
  Large Language Models (LLMs) that achieves the strongest performance and currently
  stands at first place in HuggingFace's Open LLM Leaderboard. The authors describe
  their curated dataset Open-Platypus, which is a subset of other open datasets, and
  their process of fine-tuning and merging LoRA modules to conserve the strong prior
  of pretrained LLMs while bringing specific domain knowledge to the surface.
---

# Platypus: Quick, Cheap, and Powerful Refinement of LLMs

## Quick Facts
- arXiv ID: 2308.07317
- Source URL: https://arxiv.org/abs/2308.07317
- Reference count: 40
- Primary result: Achieved first place on HuggingFace's Open LLM Leaderboard using minimal fine-tuning data

## Executive Summary
Platypus introduces a family of fine-tuned and merged Large Language Models that achieve state-of-the-art performance on the HuggingFace Open LLM Leaderboard while requiring minimal computational resources. The approach leverages a carefully curated dataset focused on STEM and logic, combined with efficient LoRA fine-tuning and adapter merging techniques. The resulting models demonstrate that targeted knowledge distillation can surface relevant capabilities from pretrained models without extensive retraining, achieving strong benchmark performance across model sizes.

## Method Summary
The authors curate the Open-Platypus dataset from existing open datasets, focusing on STEM and logic domains to align with benchmark tasks. They fine-tune LLaMA-2 models using LoRA with rank-16 decomposition, targeting specific modules (gate_proj, down_proj, up_proj) while freezing base weights. After fine-tuning, they merge LoRA adapters from different models to combine complementary knowledge. The process includes rigorous contamination checking to ensure training data doesn't overlap with test benchmarks. The entire 13B model training requires only 25k questions and 5 hours on a single A100 GPU.

## Key Results
- Platypus family tops the global Open LLM leaderboard across multiple model sizes
- Achieves state-of-the-art performance using a fraction of the fine-tuning data required by other methods
- 13B model trained on single A100 GPU in 5 hours using only 25k questions
- Successfully merges specialized adapters (StableBeluga2, Camel) to improve performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Platypus achieves high performance with minimal fine-tuning by leveraging domain-specific knowledge distillation from a curated dataset rather than general instruction tuning.
- Mechanism: The Open-Platypus dataset focuses on STEM and logic, which aligns with many MMLU benchmark tasks, enabling targeted performance gains.
- Core assumption: Model knowledge is largely learned during pretraining, so fine-tuning only needs to surface relevant skills rather than teach new ones.
- Evidence anchors: [abstract] "topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data"; [section] "Platypus family achieves strong performance in quantitative LLM metrics across model sizes"
- Break condition: If benchmark tasks shift away from STEM/logic domains, the dataset curation advantage diminishes.

### Mechanism 2
- Claim: LoRA fine-tuning with selective module targeting (gate_proj, down_proj, up_proj) efficiently updates model behavior without full parameter retraining.
- Mechanism: Freezing base model weights while training low-rank matrices reduces trainable parameters to ~0.27% for 13B, enabling rapid adaptation.
- Core assumption: Low-rank updates suffice to capture task-specific variations while preserving general capabilities.
- Evidence anchors: [abstract] "a 13B Platypus model can be trained on a single A100 GPU using 25k questions in 5 hours"; [section] "Different from full fine-tuning methods, LoRA freezes pre-trained model weights and adds rank decomposition matrices"
- Break condition: If downstream tasks require structural changes beyond rank-16 decomposition capacity.

### Mechanism 3
- Claim: Merging multiple specialized LoRA adapters (e.g., StableBeluga2, Camel) yields performance gains through complementary knowledge integration.
- Mechanism: Adapter merging combines diverse fine-tuned behaviors without retraining, leveraging existing strengths.
- Core assumption: Merged models' knowledge bases are complementary rather than redundant.
- Evidence anchors: [abstract] "our process of fine-tuning and merging LoRA modules"; [section] "The choice of models for merging, whether broad or focused, plays a pivotal role in determining the outcome"
- Break condition: If merged adapters conflict (e.g., contradictory outputs) or represent overlapping domains.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Enables efficient fine-tuning with minimal computational resources
  - Quick check question: What percentage of parameters are trainable in the 13B model using LoRA with rank 16?

- Concept: Data contamination detection
  - Why needed here: Prevents test data leakage that would inflate benchmark scores
  - Quick check question: What similarity threshold did the authors use to identify potential duplicate questions?

- Concept: Adapter merging
  - Why needed here: Combines complementary model capabilities without full retraining
  - Quick check question: Which two model types did the authors find most effective to merge?

## Architecture Onboarding

- Component map: Base LLaMA-2 model → LoRA adapters (gate_proj, down_proj, up_proj) → Merged adapter combination → Benchmark evaluation
- Critical path: Dataset curation → LoRA fine-tuning → Contamination checking → Adapter merging → Leaderboard evaluation
- Design tradeoffs: Specialized STEM dataset vs. general instruction tuning; rapid fine-tuning vs. comprehensive retraining; selective module updates vs. full parameter adaptation
- Failure signatures: Contamination in training data (overfitting to test), poor adapter merging (conflicting outputs), inadequate rank selection (underfitting)
- First 3 experiments:
  1. Fine-tune 13B model on Open-Platypus dataset using LoRA rank 16 for 5 hours on single A100
  2. Run contamination check on curated dataset against benchmark questions
  3. Merge StableBeluga2 adapter with Platypus adapter and evaluate on MMLU tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Platypus model perform on non-English languages, given its primary focus on English-language data?
- Basis in paper: [inferred]
- Why unresolved: The paper does not provide explicit information on the model's performance in non-English languages, and it mentions that the model's proficiency in other languages is not guaranteed due to limited non-English pretraining data.
- What evidence would resolve it: Conducting experiments to evaluate the model's performance on various non-English languages and comparing the results with its English performance.

### Open Question 2
- Question: How effective is the similarity exclusion process in reducing data redundancy and improving the model's performance?
- Basis in paper: [explicit]
- Why unresolved: While the paper describes the similarity exclusion process, it does not provide quantitative results on its effectiveness in reducing data redundancy or improving model performance.
- What evidence would resolve it: Comparing the model's performance with and without the similarity exclusion process, and measuring the reduction in data redundancy after applying the process.

### Open Question 3
- Question: How does the Platypus model handle potentially harmful, offensive, or biased content, especially in non-English languages where comprehensive datasets might be lacking?
- Basis in paper: [explicit]
- Why unresolved: The paper acknowledges that the model can generate potentially harmful, offensive, or biased content, but it does not provide specific strategies or results on how it handles such content, particularly in non-English languages.
- What evidence would resolve it: Conducting experiments to evaluate the model's performance in generating harmful, offensive, or biased content in various languages and assessing the effectiveness of any mitigation strategies employed.

## Limitations

- The Open-Platypus dataset composition lacks detailed documentation, making independent validation difficult
- Performance gains appear highly dependent on STEM-focused benchmarks, raising questions about generalizability
- Contamination detection methodology lacks specific implementation details for verification

## Confidence

**High Confidence**: LoRA fine-tuning with rank-16 decomposition enables efficient model adaptation is well-supported by established literature; computational efficiency claims are consistent with known LoRA scaling.

**Medium Confidence**: Merging specialized adapters yields performance improvements is plausible but under-specified; specific effectiveness of combining StableBeluga2 and Camel adapters lacks detailed experimental validation.

**Low Confidence**: Claims of "strongest performance" and "#1 on HuggingFace Open LLM Leaderboard" require ongoing verification as rankings fluctuate; attribution of success primarily to curated dataset remains insufficiently demonstrated.

## Next Checks

1. **Dataset Transparency Audit**: Request and examine the full Open-Platypus dataset composition, including source distribution and question categorization statistics.

2. **Independent Contamination Verification**: Replicate the contamination detection process on the claimed dataset against MMLU benchmark questions using the described similarity exclusion method.

3. **Cross-Domain Performance Test**: Evaluate the Platypus model family on non-STEM benchmarks (e.g., BIG-bench tasks focused on social intelligence or creative writing) to assess generalization beyond the curated dataset domain.