---
ver: rpa2
title: Emergent Communication in Interactive Sketch Question Answering
arxiv_id: '2310.15597'
source_url: https://arxiv.org/abs/2310.15597
tags:
- sketch
- human
- communication
- question
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new Interactive Sketch Question Answering
  (ISQA) task and an emergent communication (EC) system to enable interactive collaboration
  between two players. The ISQA task requires two players to communicate through sketches
  to answer a question about an image in a multi-round manner.
---

# Emergent Communication in Interactive Sketch Question Answering

## Quick Facts
- arXiv ID: 2310.15597
- Source URL: https://arxiv.org/abs/2310.15597
- Reference count: 40
- Key outcome: ISQA accuracy improved by 10% compared to non-interaction baselines under low complexity constraints

## Executive Summary
This paper introduces a novel Interactive Sketch Question Answering (ISQA) task where two players communicate through sketches to answer questions about images. The authors propose an emergent communication system with a sender module that generates sketches under complexity constraints and a receiver module that answers questions based on sketches while providing spatial feedback. The system achieves a balance among question answering accuracy, drawing complexity, and human interpretability through multi-round interaction.

## Method Summary
The paper proposes a triangular evaluation metric combining question answering ability (BCE loss and accuracy), drawing complexity (pixel count), and human interpretability (CLIP distance). The EC system consists of a sender module with geometric and pragmatic encoders, feature fusion, and sketch generation capabilities, paired with a receiver module that includes question answering and interactive feedback components. The system is trained to minimize a combined loss function with a hyperparameter controlling the relative weight of task performance and interpretability.

## Key Results
- Multi-round EC system achieves 10% accuracy improvement over non-interaction baselines under low complexity constraints
- Effective balance maintained among three evaluation factors: accuracy, complexity, and interpretability
- Human interpretability preserved through CLIP distance optimization between sketches and original images

## Why This Works (Mechanism)

### Mechanism 1
Multi-round interaction enables targeted feedback that improves downstream task performance. The receiver generates spatial bounding boxes indicating regions of high relevance to the question, allowing the sender to refine sketches in subsequent rounds. This assumes feedback provides actionable information for sketch refinement.

### Mechanism 2
Three-factor evaluation metric balances task performance, drawing complexity, and human interpretability. The system optimizes a loss function combining BCE loss for performance, pixel count for complexity, and CLIP distance for interpretability. This assumes these factors are orthogonal and can be optimized simultaneously.

### Mechanism 3
Feature fusion of geometric and pragmatic encoders enables adaptive sketch generation. The system combines pre-trained geometric features with trainable pragmatic features using a weighted sum controlled by the human interpretability parameter. This assumes geometric features provide structural information while pragmatic features provide task-specific semantics.

## Foundational Learning

- **Visual Question Answering (VQA)**: Required for receiver module to answer questions about sketches, understanding both visual content and natural language questions.
  - Quick check: Can you explain the difference between VQA on RGB images versus VQA on sketches?

- **Attention Mechanisms**: Used in receiver module to focus on relevant parts of sketch when answering questions.
  - Quick check: How does the attention mechanism in receiver module differ from standard VQA attention mechanisms?

- **Gradient-weighted Class Activation Mapping (Grad-CAM)**: Used in feedback module to identify regions of sketch most relevant to question.
  - Quick check: Can you describe how Grad-CAM works and why it's useful for generating feedback in this system?

## Architecture Onboarding

- **Component map**: Sender Module (Image encoder → Feature fusion → Sketch generator → Pixel selection) → Receiver Module (SQA module → Interactive feedback module)
- **Critical path**: 1) Sender receives image and feedback, generates sketch 2) Receiver receives sketch and question, generates answer and feedback 3) Loop back to sender with feedback for next round
- **Design tradeoffs**: Complexity vs. Interpretability (detailed sketches vs. comprehensibility), Geometric vs. Pragmatic Features (structural accuracy vs. semantic information), Single-round vs. Multi-round (refinement vs. communication overhead)
- **Failure signatures**: Poor task performance (receiver cannot answer questions), High complexity (exceeds pixel count), Low interpretability (humans cannot understand sketches)
- **First 3 experiments**: 1) Single-round baseline (sender and receiver without feedback) 2) Multi-round with random feedback (test if any feedback improves performance) 3) Ablation study on feature fusion (compare geometric-only vs. pragmatic-only)

## Open Questions the Paper Calls Out

### Open Question 1
How can the interactive EC system be extended to handle more than two collaborative players? The current system is limited to two players, and scaling to multiple players presents challenges in communication efficiency and coordination.

### Open Question 2
How does the performance of the interactive EC system compare to other communication methods in terms of task completion and human interpretability? Direct comparisons with alternative communication methods are lacking.

### Open Question 3
How does the complexity of the interactive EC system scale with the number of interaction rounds and the size of the image and question? The paper doesn't analyze complexity as a function of rounds, image size, or question size.

## Limitations
- Limited ablation studies on the necessity of specific spatial bounding box feedback mechanism
- CLIP distance metrics used for interpretability claims not validated against actual human judgment
- No comparison with alternative communication methods for the ISQA task

## Confidence
- **High confidence**: ISQA task definition and three-factor evaluation framework are clearly specified and internally consistent
- **Medium confidence**: Multi-round interaction mechanism with spatial feedback is theoretically sound, but empirical validation is limited to non-interaction baseline comparisons
- **Low confidence**: Claims about necessity of specific geometric-pragmatic feature fusion approach, as no ablation studies are provided

## Next Checks
1. Conduct ablation studies on the feedback mechanism by comparing spatial bounding boxes against random feedback and no feedback conditions
2. Validate CLIP distance metrics against human judgments of sketch interpretability through user studies
3. Test the system's robustness by evaluating performance across different question types and image categories beyond the VQA v2.0 dataset