---
ver: rpa2
title: Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits
arxiv_id: '2305.18784'
source_url: https://arxiv.org/abs/2305.18784
tags:
- uni00000013
- agents
- agent
- bandit
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a multi-agent collaborative bandit problem where
  N agents learn M heterogeneous MABs, with each bandit having K arms and each agent
  learning one bandit. Agents can communicate via information pulls over a gossip
  network.
---

# Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits

## Quick Facts
- arXiv ID: 2305.18784
- Source URL: https://arxiv.org/abs/2305.18784
- Reference count: 40
- Key outcome: Proposes two decentralized algorithms for collaborative multi-agent heterogeneous MABs with gossip communication, achieving O((MK/N + M/Δm) log T) per-agent regret (context unaware) and O((MK/N + M/rΔm) log T) (partially context aware), with matching lower bounds.

## Executive Summary
This paper addresses the challenge of collaborative multi-agent learning in heterogeneous multi-armed bandit (MAB) settings where N agents learn M different bandits through decentralized communication. The authors propose two algorithms that enable agents to share information about best arms via gossip protocols, reducing cumulative regret compared to non-communicating agents. The key insight is that even without knowing which agents share the same bandit, agents can still propagate good arm recommendations through pairwise information pulls, while partial context awareness further improves performance by allowing targeted arm distribution among known peers.

## Method Summary
The paper considers N agents learning M heterogeneous MABs with K arms each, where each agent learns one bandit but can communicate through a complete gossip network. Two scenarios are analyzed: context unaware (agents don't know who shares their bandit) and partially context aware (agents know r-1 others learning the same bandit). For the context unaware case, a modified GosInE algorithm uses phase-based UCB exploration with active sets and information pulls for arm recommendations. The partially context aware algorithm extends this by having agents exchange recommendations with known peers and distribute M best arms equally among r agents. The methods achieve regret upper bounds scaling as O((MK/N + M/Δm) log T) and O((MK/N + M/rΔm) log T) respectively, with matching lower bounds demonstrating near-optimality.

## Key Results
- Context unaware algorithm achieves per-agent regret O((MK/N + M/Δm) log T) through gossip-based arm recommendations
- Partially context aware algorithm improves regret to O((MK/N + M/rΔm) log T) by distributing arm recommendations among r agents
- Group regret scales as O(∑(K+N/Δm) log T) for both scenarios
- Lower bounds match upper bounds, proving near-optimality of the algorithms
- Collaboration reduces regret compared to no communication when M << min{K, N}

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agents can reduce cumulative regret by collaboratively exploring multiple heterogeneous MABs when they share knowledge of best arms through gossip communication.
- Mechanism: Each agent runs UCB on a subset of arms (sticky + non-sticky sets). At phase ends, agents recommend the most-played arm from their active set. This allows best arms to spread across agents learning the same bandit, even when agents are unaware of who else shares the bandit.
- Core assumption: Assumption 1 ensures at least one agent has the true best arm in its sticky set, enabling propagation.
- Evidence anchors:
  - [abstract] "Agents can communicate via information pulls over a gossip network."
  - [section 3.1] "Agents exchange arm recommendations through pairwise gossip communication and update their active sets."
  - [corpus] Weak - no direct citations found for this specific gossip-based collaboration mechanism.
- Break condition: If no agent has the true best arm in their sticky set (violates Assumption 1), all agents incur linear regret.

### Mechanism 2
- Claim: Partially context-aware agents achieve lower regret by sharing best arm recommendations with known peers learning the same bandit.
- Mechanism: Agents maintain a known peer set (size r-1) learning the same bandit. After receiving recommendations, they select M most recent unique arms and distribute them equally among themselves, reducing redundant exploration.
- Core assumption: r divides Nm for all m, ensuring equal distribution of M best arms among r agents.
- Evidence anchors:
  - [section 4] "agents can also exchange messages with the r-1 agents known to be learning the same bandit"
  - [section 4.1] "each group of r agents learning the same bandit can select the M most recent unique arm recommendations"
  - [corpus] Weak - no direct citations found for this specific peer-sharing mechanism.
- Break condition: If M is not divisible by r or if the M most recent unique recommendations don't stabilize to the M best arms.

### Mechanism 3
- Claim: Regret upper bounds scale as O((MK/N + M/Δm) log T) for context unaware and O((MK/N + M/rΔm) log T) for partially context aware scenarios.
- Mechanism: The first term MK/N represents distributed exploration benefit, while M/Δm represents identification cost of best arms. Partially aware scenario reduces M/Δm to M/rΔm through efficient peer distribution.
- Core assumption: M << min{K, N} ensures collaboration benefits dominate.
- Evidence anchors:
  - [section 3.4] "regret of any agent i∈Im for all m∈[M] scales as O(S+M/Δm log T)"
  - [section 4.1] "regret of any agent i∈Im for all m∈[M] scales as O(S+M/rΔm log T)"
  - [corpus] Weak - no direct citations found for these specific regret bounds.
- Break condition: When M approaches min{K, N}, collaboration benefits diminish and regret approaches single-agent levels.

## Foundational Learning

- Concept: UCB (Upper Confidence Bound) Algorithm
  - Why needed here: Forms the core exploration strategy within each phase for arm selection
  - Quick check question: What is the exploration parameter α in UCB that balances exploration vs exploitation?

- Concept: Gossip communication protocols
  - Why needed here: Enables decentralized information sharing without central coordinator
  - Quick check question: How does the complete graph assumption (G(n,i) = (N-1)^-1) affect information propagation speed?

- Concept: Regret decomposition principle
  - Why needed here: Allows analysis of cumulative regret by breaking it into components before and after best arm identification
  - Quick check question: How does the regret decomposition change when considering group vs individual regret?

## Architecture Onboarding

- Component map: Agents → MABs → Arms → UCB selection → Phase updates → Gossip communication → Regret calculation
- Critical path: Agent initialization → Phase-based UCB play → Information pull/gossip → Active set update → Regret accumulation
- Design tradeoffs: Complete graph vs sparse graph connectivity, number of phases vs communication overhead, sticky set size vs exploration coverage
- Failure signatures: Linear regret (best arm not identified), high variance in regret across agents, slow convergence of best arm propagation
- First 3 experiments:
  1. Implement Algorithm 1 with synthetic bandits and verify regret scales with M/N ratio
  2. Test Algorithm 3 with known peer sets and measure regret reduction from context unaware case
  3. Validate lower bound tightness by comparing theoretical vs empirical group regret

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact scaling of the regret when agents are fully context aware (i.e., know all other agents learning the same bandit)?
- Basis in paper: [explicit] The paper considers partially context aware scenarios where agents know r-1 other agents learning the same bandit, but doesn't explore the fully context aware case.
- Why unresolved: The paper's analysis focuses on partial context awareness and derives regret bounds for this case, but doesn't extend to the fully context aware scenario.
- What evidence would resolve it: Developing and analyzing an algorithm for the fully context aware case, and comparing its regret bounds to those of the partially context aware algorithms.

### Open Question 2
- Question: How does the regret scale when the number of agents N is comparable to or larger than the number of arms K (i.e., when K/N = Θ(1))?
- Basis in paper: [explicit] The paper mentions that when K/N = Θ(1), there is no benefit of collaboration in the context unaware scenario.
- Why unresolved: The paper's analysis focuses on the case where M << min{K, N}, but doesn't explore the regime where N is comparable to or larger than K.
- What evidence would resolve it: Analyzing the regret bounds for different values of K/N, and identifying the regimes where collaboration is beneficial or not.

### Open Question 3
- Question: How does the regret scale when the number of bandits M is large relative to N (i.e., when M/N = Θ(1))?
- Basis in paper: [explicit] The paper's analysis assumes M < N and focuses on the case where M << min{K, N}.
- Why unresolved: The paper doesn't explore the regime where M is large relative to N, which could have different regret scaling properties.
- What evidence would resolve it: Analyzing the regret bounds for different values of M/N, and identifying the regimes where collaboration is beneficial or not.

## Limitations

- Theoretical regret bounds assume ideal conditions (complete gossip graph, known peer sets, M << min{K, N}) that may not hold in practical implementations
- The paper does not provide empirical validation or simulations to support the theoretical claims
- Assumption 1 (at least one agent has the true best arm in sticky set) is critical but not validated for finite time horizons
- The communication model assumes perfect information exchange without delays or failures

## Confidence

- **High confidence**: The regret decomposition framework and general algorithmic structure are sound
- **Medium confidence**: The upper bound proofs for per-agent and group regret, given the stated assumptions
- **Low confidence**: The tightness of lower bounds and their achievability in practice

## Next Checks

1. **Empirical validation**: Implement the algorithms with synthetic bandits and verify the regret scaling matches theoretical predictions across different parameter regimes (N, M, K, Δm)
2. **Assumption testing**: Analyze the probability of violating Assumption 1 over finite time horizons and its impact on regret performance
3. **Communication efficiency**: Measure actual communication costs in practice versus theoretical bounds and assess scalability with increasing N and M