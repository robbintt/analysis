---
ver: rpa2
title: 'ReCoRe: Regularized Contrastive Representation Learning of World Model'
arxiv_id: '2312.09056'
source_url: https://arxiv.org/abs/2312.09056
tags:
- learning
- recore
- invariant
- world
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ReCoRe, a world model that learns invariant
  features using contrastive unsupervised learning and an intervention-invariant regularizer.
  This approach improves sample efficiency and generalization for reinforcement learning
  in visual navigation tasks.
---

# ReCoRe: Regularized Contrastive Representation Learning of World Model

## Quick Facts
- arXiv ID: 2312.09056
- Source URL: https://arxiv.org/abs/2312.09056
- Reference count: 40
- Outperforms state-of-the-art RL methods on OoD point navigation tasks using only visual observations

## Executive Summary
ReCoRe introduces a world model architecture that learns invariant visual representations through contrastive unsupervised learning combined with an intervention-invariant regularizer. The key innovation is using contrastive loss to enforce invariance to data augmentations while preventing feature collapse through auxiliary tasks like depth prediction. This approach enables superior out-of-distribution generalization for visual navigation tasks and demonstrates strong sim-to-real transfer capabilities. The model significantly outperforms both model-based and model-free RL methods on the iGibson benchmark while maintaining competitive performance in in-distribution settings.

## Method Summary
ReCoRe is a world model architecture based on DreamerV2 that learns invariant representations through contrastive learning and intervention-invariant regularization. The model uses an encoder to extract features from augmented observations, a recurrent state space model to predict future states and rewards, and an actor-critic controller for decision making. The contrastive loss module implements InfoNCE loss comparing query and key embeddings from different augmentations of the same observation. An auxiliary depth prediction head serves as an intervention-invariant regularizer to prevent feature collapse during training. The model is trained on 100k-500k environment steps and evaluated on held-out test scenes for out-of-distribution generalization.

## Key Results
- Achieves 71% SR and 66% SPL on iGibson OoD test scenes after 500k training steps
- Outperforms state-of-the-art model-based (MWM) and model-free (RAD, CURL, DreamerV2) RL methods
- Demonstrates excellent sim-to-real transfer on Gibson benchmark with only visual observations
- Competitive with language-guided foundation models despite using simpler input modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive loss enforces invariant features by maximizing agreement between different augmentations of the same observation while minimizing agreement between different observations.
- Mechanism: The InfoNCE loss compares an anchor query with a set of keys, where positive keys come from the same observation under different augmentations and negative keys come from different observations. This forces the encoder to learn representations that are invariant to the applied augmentations.
- Core assumption: Data augmentations preserve the underlying semantic content of observations while varying surface-level features like texture and appearance.
- Break condition: If augmentations change the underlying semantics of observations (not just appearance), contrastive loss will force the model to ignore important information needed for the task.

### Mechanism 2
- Claim: The intervention-invariant regularizer (depth prediction) prevents feature collapse by providing explicit supervision for invariant features.
- Mechanism: Depth prediction is invariant to appearance changes but varies with geometric structure. By requiring the model to predict consistent depth across augmentations, it enforces that the learned features capture geometric information rather than texture-based shortcuts.
- Core assumption: Depth is truly invariant to the style interventions applied (texture variations) while still being relevant for navigation.
- Break condition: If depth prediction becomes too easy or too hard, it may not provide meaningful regularization. Also fails if depth is not truly invariant to the augmentations.

### Mechanism 3
- Claim: Separating world model and controller training enables sample efficiency by allowing the world model to be learned from rollouts without needing additional environment interactions.
- Mechanism: The world model learns to predict future states and rewards from current observations, which the controller can then use to plan. This means the controller can be trained on imagined trajectories from the world model rather than requiring more real environment steps.
- Core assumption: The world model can learn accurate predictions that are good enough for planning, even if not perfect.
- Break condition: If the world model predictions are too inaccurate, the controller will learn suboptimal policies based on incorrect rollouts.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Forms the core mechanism for learning invariant features by comparing augmented views of the same observation
  - Quick check question: What happens to the contrastive loss if you use the same augmentation twice instead of different augmentations?

- Concept: World models and latent dynamics
  - Why needed here: Provides the framework for learning predictive models that can be used for planning and improves sample efficiency
  - Quick check question: How does the recurrent state space model predict future states from current observations and actions?

- Concept: Intervention invariance and causal mechanisms
  - Why needed here: The theoretical foundation for why learning features invariant to certain interventions leads to better generalization
  - Quick check question: What makes depth prediction an "intervention invariant" auxiliary task in this context?

## Architecture Onboarding

- Component map: Observation → Augmentation → Encoder → Contrastive loss → Latent states → Depth prediction + RSSM → Controller → Actions

- Critical path: The data flows from raw observations through augmentation and encoding, then splits to contrastive loss and latent dynamics prediction, ultimately reaching the controller for action selection.

- Design tradeoffs: Using depth as regularizer requires depth information during training but not deployment; using other tasks like image denoising would be more generally applicable but may not enforce invariance as effectively

- Failure signatures:
  - Feature collapse: Contrastive loss without regularizer causes encoder to output random or constant features
  - Poor depth prediction: Regularizer not working if depth predictions are inaccurate across augmentations
  - Controller failure: World model predictions too inaccurate for effective planning

- First 3 experiments:
  1. Test contrastive learning alone (without depth regularizer) to observe feature collapse behavior
  2. Test depth prediction accuracy across different augmentations to verify invariance enforcement
  3. Compare sample efficiency with and without world model by measuring performance vs environment steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed intervention-invariant regularizer perform with different types of auxiliary tasks beyond depth prediction, image denoising, and optical flow prediction?
- Basis in paper: The paper mentions that depth prediction, image denoising, and optical flow prediction are used as auxiliary tasks, but also states that "the design of interventions and invariant regularizers in the form of auxiliary losses becomes an interesting research problem which we will address in future work."
- Why unresolved: The paper only experiments with a limited set of auxiliary tasks and does not explore the full range of potential invariant tasks that could be used for regularization.
- What evidence would resolve it: Systematic experiments testing the performance of ReCoRe with various intervention-invariant auxiliary tasks across different environments and tasks would provide insights into the most effective regularization strategies.

### Open Question 2
- Question: How does ReCoRe's performance compare to other state-of-the-art methods when the training and evaluation environments come from the same distribution (in-distribution)?
- Basis in paper: The paper states that "we can conclude that ReCoRe is also competitive with end-to-end deep RL techniques even when training and evaluation environments come from similar distributions (and no OoD generalization is necessary)."
- Why unresolved: While the paper demonstrates ReCoRe's effectiveness for out-of-distribution generalization, it does not provide a comprehensive comparison with other state-of-the-art methods in the in-distribution setting.
- What evidence would resolve it: Conducting experiments comparing ReCoRe's performance to other state-of-the-art model-free and model-based RL methods on tasks where the training and evaluation environments share the same distribution would provide a clearer picture of its relative strengths and weaknesses.

### Open Question 3
- Question: What is the impact of different data augmentation techniques on ReCoRe's performance, and are there optimal augmentation strategies for specific tasks or environments?
- Basis in paper: The paper uses various data augmentation techniques (spatial jitter, Gaussian blur, color jitter, grayscale, and cutout) but does not provide a detailed analysis of how different augmentation techniques or their combinations affect ReCoRe's performance.
- Why unresolved: The paper does not explore task-specific or environment-specific optimal augmentation strategies.
- What evidence would resolve it: Conducting ablation studies and experiments with different combinations of data augmentation techniques across various tasks and environments would reveal insights into the most effective augmentation strategies for ReCoRe.

## Limitations

- Limited exploration of alternative intervention-invariant regularizers beyond depth prediction
- Unclear generalization performance on tasks outside navigation (e.g., manipulation)
- Potential sensitivity to hyperparameter choices for contrastive learning temperature and augmentation strength

## Confidence

- **High confidence**: The architectural framework (RSSM + contrastive loss + depth regularization) is clearly specified and builds on established methods
- **Medium confidence**: The performance claims (OoD generalization, sim-to-real transfer) are supported by benchmark results but lack ablation studies to isolate mechanism contributions
- **Low confidence**: Claims about superiority over language-guided foundation models lack direct comparison details

## Next Checks

1. Conduct ablation studies removing the depth regularizer to quantify its contribution to OoD generalization
2. Verify that the contrastive loss does not collapse features by monitoring embedding variance during training
3. Test the model's robustness to different augmentation strategies to confirm invariance is enforced rather than specific augmentations being learned