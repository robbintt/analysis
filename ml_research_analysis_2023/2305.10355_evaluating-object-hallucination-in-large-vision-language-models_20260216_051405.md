---
ver: rpa2
title: Evaluating Object Hallucination in Large Vision-Language Models
arxiv_id: '2305.10355'
source_url: https://arxiv.org/abs/2305.10355
tags:
- lvlms
- objects
- hallucination
- object
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a systematic study on object hallucination
  in large vision-language models (LVLMs). The authors evaluate several representative
  LVLMs on the MSCOCO dataset using the CHAIR metric, revealing that most LVLMs suffer
  from severe object hallucination, even more than smaller VLPMs.
---

# Evaluating Object Hallucination in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2305.10355
- Source URL: https://arxiv.org/abs/2305.10355
- Authors: 
- Reference count: 22
- One-line primary result: Large vision-language models (LVLMs) exhibit severe object hallucination, generating objects not present in images, and the proposed POPE evaluation method provides a more stable and flexible approach to detect this phenomenon.

## Executive Summary
This paper systematically investigates object hallucination in large vision-language models (LVLMs) by evaluating several representative models on the MSCOCO dataset using the CHAIR metric. The study reveals that most LVLMs suffer from severe object hallucination, often generating objects that are not present in the input images. To address the challenges of traditional hallucination evaluation methods, the authors propose POPE (Polling-based Object Presence Evaluation), a binary classification-based approach that formulates the evaluation as yes/no questions. POPE demonstrates more stable and flexible evaluation compared to traditional methods while maintaining consistency with established metrics.

## Method Summary
The authors evaluate LVLMs on object hallucination using two complementary approaches. First, they employ the CHAIR (CHAIR-I and CHAIR-S) metric to assess hallucination by comparing generated captions with ground-truth annotations, using two different instructions and measuring object hallucination rates. Second, they propose POPE, a polling-based query method that formulates the evaluation as binary classification tasks using yes/no questions. POPE includes three sampling strategies (Random, Popular, and Adversarial) to evaluate LVLMs' hallucination tendencies under different conditions. The method can be combined with automatic segmentation models to evaluate on unannotated datasets, enhancing its practical applicability.

## Key Results
- Most LVLMs (including LLaVA, MiniGPT-4, and InstructBLIP) exhibit significantly higher object hallucination rates than smaller VLPMs when evaluated on MSCOCO
- Objects frequently appearing in visual instruction data or co-occurring with ground-truth objects are more likely to be hallucinated by LVLMs
- POPE demonstrates consistent results across different prompt templates and shows that InstructBLIP has a higher tendency to answer "Yes" compared to other LVLMs
- The evaluation reveals a performance degradation from Random → Popular → Adversarial settings in POPE, indicating its ability to reflect LVLMs' hallucination tendencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequent objects in visual instruction data are more likely to be hallucinated
- Mechanism: LVLMs are fine-tuned on visual instruction datasets constructed from MSCOCO, which have unbalanced distributions favoring common objects. This causes LVLMs to generate these frequent objects even when not present in target images
- Core assumption: The visual instruction dataset distribution directly influences the model's generation patterns during inference
- Evidence anchors:
  - [abstract] "objects that frequently occur in the visual instructions... are obviously prone to be hallucinated by LVLMs"
  - [section 4.1] "we find that the commonly objects in MSCOCO and the visual instruction dataset are indeed more likely to be hallucinated by LVLMs"
- Break condition: If the visual instruction dataset is balanced or if the model is fine-tuned on diverse datasets with different object distributions

### Mechanism 2
- Claim: Objects frequently co-occurring with ground-truth objects are more likely to be hallucinated
- Mechanism: LVLMs learn associations between objects from visual instruction data. When a ground-truth object appears in an image, the model is more likely to generate its frequently co-occurring objects based on learned patterns
- Core assumption: The model's training on co-occurrence patterns in visual instructions creates strong associations that persist during inference
- Evidence anchors:
  - [abstract] "objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs"
  - [section 4.2] "we can see a similar tendency that the hallucination times of objects reduce w.r.t. the decreasing of the co-occurrence number"
- Break condition: If the model is trained to prioritize object presence verification over learned associations

### Mechanism 3
- Claim: The POPE evaluation method is more stable and flexible than traditional methods for detecting object hallucination
- Mechanism: POPE converts hallucination evaluation into binary classification tasks using yes/no questions, avoiding the need for complex parsing of generated captions and reducing sensitivity to instruction design and generation style
- Core assumption: Binary classification tasks are more robust to variations in model output format than open-ended caption generation evaluation
- Evidence anchors:
  - [abstract] "we further propose a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way"
  - [section 5.2] "Under different templates, InstructBLIP shows consistent results with low standard deviations"
- Break condition: If the model learns to systematically answer "Yes" to all questions regardless of content, or if the question formulation itself becomes biased

## Foundational Learning

- Concept: Vision-Language Pre-trained Models (VLPMs)
  - Why needed here: Understanding VLPMs is essential to grasp how LVLMs are constructed by integrating LLMs with VLPMs
  - Quick check question: What are the three main components of a VLPM and how do they interact?

- Concept: Object Hallucination
  - Why needed here: The paper focuses specifically on object hallucination, so understanding this concept is fundamental to the evaluation
  - Quick check question: How does object hallucination differ from other types of hallucination in language models?

- Concept: Binary Classification Evaluation
  - Why needed here: POPE uses binary classification (yes/no questions) instead of traditional evaluation methods, so understanding this approach is crucial
  - Quick check question: What are the advantages of using binary classification for hallucination detection compared to open-ended generation evaluation?

## Architecture Onboarding

- Component map:
  - Visual Encoder (ViT-L/14, ViT-G/14, etc.) -> Feature extraction from images
  - Alignment Network (Attention, Linear, Q-Former) -> Bridges visual and language encoders
  - Large Language Model (LLaMA, Vicuna) -> Generates text responses
  - POPE Evaluation Module -> Constructs binary questions for hallucination detection

- Critical path:
  1. Image input → Visual Encoder → Feature extraction
  2. Features → Alignment Network → LLM-compatible representation
  3. LLM generates response to yes/no questions
  4. POPE module evaluates responses against ground truth

- Design tradeoffs:
  - Using frozen vs. trainable visual encoders (affects hallucination rates)
  - Choice of alignment network (Q-Former vs. linear vs. attention)
  - Fine-tuning strategy (only alignment vs. full model)

- Failure signatures:
  - High "Yes" response rate across all questions (overconfidence)
  - Performance drop from Random → Popular → Adversarial settings in POPE
  - Significant gap between InstructBLIP and other LVLMs

- First 3 experiments:
  1. Run POPE evaluation on MSCOCO with all five LVLMs using Random sampling
  2. Compare CHAIR metric results with POPE results for the same models
  3. Test the effect of different prompt templates on InstructBLIP's POPE performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different training objectives and data compositions in LVLM affect the frequency and severity of object hallucination?
- Basis in paper: [explicit] The paper discusses how different fine-tuning strategies (e.g., only fine-tuning the cross-modal alignment network vs. fine-tuning both the alignment network and the LLM) affect the hallucination degree of LVLMs. It also mentions that the visual instruction dataset, which is mostly constructed based on MSCOCO, might influence the hallucination problem.
- Why unresolved: The paper provides some initial insights into the relationship between training objectives and hallucination, but a comprehensive study on how different training objectives and data compositions affect the frequency and severity of object hallucination is still lacking.
- What evidence would resolve it: A systematic evaluation of LVLMs with different training objectives and data compositions, measuring their hallucination degree using metrics like CHAIR and POPE, would provide a clearer understanding of the relationship between training objectives and hallucination.

### Open Question 2
- Question: How does the granularity of object annotations in the evaluation dataset impact the performance of POPE?
- Basis in paper: [explicit] The paper mentions that when combined with the automatic segmentation tool SEEM, the objects would be annotated based on the label set from the tool, which may be inconsistent with the collected human annotations, leading to the divergence in evaluation results.
- Why unresolved: The paper demonstrates the effectiveness of POPE with human-annotated and SEEM-annotated datasets, but it does not provide a detailed analysis of how the granularity of object annotations affects the performance of POPE.
- What evidence would resolve it: A systematic evaluation of POPE with datasets having different levels of object annotation granularity, comparing the results with human-annotated datasets, would provide insights into the impact of annotation granularity on POPE's performance.

### Open Question 3
- Question: Can the object hallucination problem in LVLMs be mitigated by incorporating external knowledge or constraints during the training or inference process?
- Basis in paper: [inferred] The paper highlights the severity of the object hallucination problem in LVLMs and suggests that it might be attributed to the visual instruction dataset. It also mentions that the hallucination phenomenon hinders the safe use of LVLMs in real-world deployment.
- Why unresolved: The paper does not explore potential solutions to mitigate the object hallucination problem in LVLMs, such as incorporating external knowledge or constraints during the training or inference process.
- What evidence would resolve it: Experimental studies evaluating the effectiveness of incorporating external knowledge or constraints, such as knowledge graphs, commonsense reasoning, or domain-specific constraints, in reducing the object hallucination problem in LVLMs, would provide insights into potential solutions.

## Limitations

- The study focuses primarily on object hallucination and does not extensively explore other types of hallucination (e.g., attribute or relationship hallucination)
- Evaluation is conducted primarily on MSCOCO, which may not fully represent the broader distribution of real-world images and scenarios
- The paper demonstrates correlations between object frequency and hallucination rates but requires controlled experiments to establish causation definitively

## Confidence

- **High Confidence**: The observation that LVLMs exhibit higher hallucination rates than smaller VLPMs, supported by systematic CHAIR metric evaluation across multiple models
- **Medium Confidence**: The relationship between object frequency in training data and hallucination propensity, based on correlation analysis but requiring controlled experiments to establish causation
- **Medium Confidence**: The stability and flexibility of POPE as an evaluation method, demonstrated through internal consistency but needing external validation

## Next Checks

1. Conduct controlled experiments by fine-tuning LVLMs on balanced vs. unbalanced visual instruction datasets to directly test the impact of object frequency on hallucination rates
2. Apply POPE evaluation to additional hallucination types (e.g., attribute hallucination, relationship hallucination) to assess method generalizability
3. Test the proposed mechanisms on out-of-distribution datasets beyond MSCOCO to evaluate whether observed patterns generalize to real-world scenarios