---
ver: rpa2
title: Robust Clustering using Hyperdimensional Computing
arxiv_id: '2312.02407'
source_url: https://arxiv.org/abs/2312.02407
tags:
- clustering
- data
- hypervectors
- algorithms
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes four new HDC-based clustering algorithms to
  address the robustness issue of the existing HDCluster approach. In HDCluster, initial
  cluster hypervectors are randomly assigned, causing high variance in clustering
  performance.
---

# Robust Clustering using Hyperdimensional Computing

## Quick Facts
- arXiv ID: 2312.02407
- Source URL: https://arxiv.org/abs/2312.02407
- Reference count: 40
- Key outcome: Proposed four HDC-based clustering algorithms achieve 2-38% better accuracy than HDCluster by using data-driven initialization

## Executive Summary
This paper addresses the robustness issues in HDCluster, a hyperdimensional computing-based clustering algorithm that suffers from high variance due to random initialization of cluster hypervectors. The authors propose four new algorithms that leverage intra-cluster similarity patterns in encoded data to select initial cluster positions from the data domain rather than random vectors. Experimental results on eight diverse datasets show significant improvements in accuracy, fewer iterations, and reduced execution time compared to the baseline HDCluster approach.

## Method Summary
The paper introduces four HDC-based clustering algorithms that replace HDCluster's random initialization with data-driven approaches. These algorithms compute pairwise similarities between encoded query hypervectors and use this information to select initial cluster hypervectors. The methods include similarity-based k-means, equal bin-width histogram, equal bin-height histogram, and similarity-based affinity propagation. All algorithms operate in hyperdimensional space (d=10,000) and use 16-level quantization for numerical features.

## Key Results
- Similarity-based affinity propagation outperforms other algorithms by 2-38% in clustering accuracy
- Proposed algorithms achieve more robust performance with less variance across multiple runs
- Five out of eight datasets show comparable or better accuracy when projected onto hyperdimensional space
- The algorithms require fewer iterations and less execution time compared to HDCluster

## Why This Works (Mechanism)

### Mechanism 1
Random initialization of cluster hypervectors in HDCluster causes high variance in clustering performance. With dimensionality d=10,000 and k clusters, there are 2^(d×k) possible initializations, leading to many configurations where initial clusters are far from actual data clusters. This forces the algorithm to take many iterations to correct poor initial positions.

### Mechanism 2
Using data-driven initialization based on similarity among encoded query hypervectors reduces clustering variance and improves accuracy. The proposed algorithms compute similarity between encoded data points (query hypervectors) and use this information to select initial cluster hypervectors from the data domain itself rather than random vectors. This ensures initial clusters are positioned near actual data groupings, reducing the number of iterations needed for convergence.

### Mechanism 3
Projection onto hyperdimensional space can improve or maintain clustering performance for datasets with small k and n values. Encoding data into hyperdimensional space using HDC with appropriate quantization (q=16) can preserve or enhance separability of clusters, particularly when the number of clusters (k) and features (n) are both small.

## Foundational Learning

- **Concept: Hyperdimensional Computing (HDC) fundamentals**
  - Why needed here: The entire clustering framework operates in hyperdimensional space, so understanding how data is encoded and similarity is measured is essential
  - Quick check question: What are the three basic operations in HDC and how do they differ between binary and non-binary HDC?

- **Concept: Traditional clustering algorithms (k-means, hierarchical, affinity propagation)**
  - Why needed here: The proposed HDC-based algorithms are adaptations of these traditional methods, so understanding their mechanisms is crucial
  - Quick check question: How does the similarity matrix differ between traditional affinity propagation and the proposed similarity-based affinity propagation?

- **Concept: Similarity measurement in hyperdimensional space**
  - Why needed here: Clustering decisions in HDC depend entirely on similarity between hypervectors, so understanding Hamming distance vs cosine similarity is critical
  - Quick check question: When should you use Hamming distance versus cosine similarity for comparing hypervectors?

## Architecture Onboarding

- **Component map:**
  - Encoder: Converts original data samples into query hypervectors using record-based or N-gram-based encoding
  - Similarity Calculator: Computes pairwise similarities between query hypervectors using Eq. (3)
  - Initializer: Assigns initial cluster hypervectors based on similarity results (data-driven vs random)
  - Clustering Engine: Applies k-means, histogram-based, or affinity propagation algorithms in hyperdimensional space
  - Updater: Iteratively updates cluster hypervectors until convergence criteria are met

- **Critical path:**
  1. Data encoding → similarity computation → initial cluster assignment → iterative clustering updates → convergence check
  2. For one-pass clustering: data encoding → similarity computation → initial cluster assignment → final assignment

- **Design tradeoffs:**
  - Binary vs non-binary HDC: Binary offers hardware efficiency but may lose information; non-binary preserves more information but requires more computation
  - Record-based vs N-gram encoding: Record-based preserves explicit feature-position relationships; N-gram is more compact but loses positional information
  - Iterative vs one-pass: Iterative achieves better accuracy but requires more computation; one-pass is faster but may sacrifice accuracy

- **Failure signatures:**
  - High variance in accuracy across runs (indicates poor initialization strategy)
  - Excessive iterations to convergence (suggests initial clusters are poorly positioned)
  - Degradation when k is large (indicates projection to hyperdimensional space may not be beneficial)
  - Poor performance on datasets with many features (suggests quantization may lose critical information)

- **First 3 experiments:**
  1. Replicate HDCluster baseline: Run with random initialization across multiple seeds to observe variance in performance
  2. Test similarity-based k-means: Compare accuracy and iteration count against HDCluster baseline on a small dataset (e.g., IRIS)
  3. Evaluate projection benefit: Compare clustering accuracy on original vs encoded data for a dataset with small k and n (e.g., Glass)

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of clusters k for hyperdimensional computing-based clustering algorithms when the number of features n is small? The paper states "projecting onto hyperdimensional space is attractive when both the number of clusters k and the number of features n are small" but does not establish a general rule for when k and n are both small.

### Open Question 2
How does the choice of encoding algorithm (record-based vs N-gram-based) impact clustering performance in hyperdimensional computing? The paper compares both encoding algorithms but does not thoroughly analyze the relative strengths and weaknesses of each encoding method across different types of data.

### Open Question 3
Can hyperdimensional computing be used to infer the optimal number of clusters k without ground truth information? This is explicitly stated as a future research direction, indicating it has not yet been addressed in the current work.

## Limitations
- The paper assumes similarity in hyperdimensional space directly translates to cluster structure, but this relationship may not hold for all data distributions
- The benefit of projection to hyperdimensional space appears dataset-dependent, with no clear theoretical explanation for when it helps vs hurts
- The comparison focuses primarily on accuracy metrics, with limited analysis of computational complexity or memory requirements for large-scale datasets

## Confidence

- High confidence: The mechanism of random initialization causing variance is well-supported by experimental evidence
- Medium confidence: The claim that similarity-based initialization consistently improves performance across all datasets
- Medium confidence: The observation that projection benefits datasets with small k and n values
- Low confidence: The theoretical explanation for why projection sometimes improves clustering accuracy

## Next Checks

1. **Robustness testing:** Run the proposed algorithms on additional datasets with varying characteristics (e.g., different dimensionality, noise levels, cluster shapes) to validate generalizability
2. **Parameter sensitivity analysis:** Systematically vary key parameters (dimensionality d, quantization levels q, similarity thresholds) to understand their impact on performance
3. **Scalability evaluation:** Test the algorithms on larger datasets to assess computational efficiency and memory requirements compared to traditional clustering methods