---
ver: rpa2
title: Is ChatGPT a Good NLG Evaluator? A Preliminary Study
arxiv_id: '2303.04048'
source_url: https://arxiv.org/abs/2303.04048
tags:
- chatgpt
- evaluation
- generation
- correlation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether ChatGPT can serve as a reliable automatic
  evaluation metric for natural language generation (NLG) tasks. The authors treat
  ChatGPT as a human evaluator and design task-specific and aspect-specific prompts
  to guide it in scoring NLG model outputs.
---

# Is ChatGPT a Good NLG Evaluator? A Preliminary Study

## Quick Facts
- arXiv ID: 2303.04048
- Source URL: https://arxiv.org/abs/2303.04048
- Authors: 
- Reference count: 11
- Primary result: ChatGPT achieves state-of-the-art or competitive correlation with human judgments as an automatic NLG evaluation metric

## Executive Summary
This paper investigates whether ChatGPT can serve as a reliable automatic evaluation metric for natural language generation tasks. The authors treat ChatGPT as a human evaluator and design task-specific and aspect-specific prompts to guide it in scoring NLG model outputs. Experiments on three meta-evaluation datasets covering summarization, story generation, and data-to-text tasks show that ChatGPT outperforms traditional metrics like ROUGE, BERTScore, and BARTScore in correlation with human judgments across most aspects. However, its effectiveness may be influenced by dataset biases, particularly in reference-dependent meta-evaluation datasets.

## Method Summary
The authors treat ChatGPT as a human evaluator and design task-specific and aspect-specific prompts to score NLG model outputs. They use a one-to-five star ranking format for ratings and extract numerical scores from ChatGPT's responses. The evaluation is conducted across three meta-evaluation datasets: SummEval (summarization), OpenMEV A-ROC (story generation), and BAGEL (data-to-text). Correlation analysis using Spearman, Pearson, and Kendall's Tau measures the alignment between ChatGPT's ratings and human judgments. The performance is compared against traditional metrics including ROUGE, BERTScore, BARTScore, MoverScore, PRISM, and Perplexity.

## Key Results
- ChatGPT achieves state-of-the-art or competitive correlation with human judgments across most evaluation aspects
- Outperforms traditional metrics like ROUGE, BERTScore, and BARTScore in most cases
- Effectiveness may be compromised by dataset biases in reference-dependent meta-evaluation datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can serve as a general-purpose NLG evaluation metric by mimicking human evaluators through task-specific and aspect-specific prompts.
- Mechanism: The paper treats ChatGPT as a human evaluator and designs prompts that specify both the NLG task (e.g., summarization) and the aspect to evaluate (e.g., relevance). This structured prompting guides ChatGPT to produce ratings that correlate with human judgments.
- Core assumption: ChatGPT's underlying language understanding and generation capabilities allow it to evaluate NLG outputs in a human-like manner when properly prompted.
- Evidence anchors:
  - [abstract] "we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to score the generation of NLG models"
  - [section 3] "we regard ChatGPT as a human evaluator and give it evaluation instruction via different prompts. Each prompt should specify (1) which NLG task (e.g., summarization) needs to be evaluated and (2) which aspect (e.g., fluency) of the generation result should be assessed currently."
  - [corpus] Weak - corpus doesn't directly address ChatGPT's mechanism as evaluator
- Break condition: If prompts are poorly designed or the NLG task requires specialized domain knowledge beyond ChatGPT's training, the correlation with human judgments would degrade.

### Mechanism 2
- Claim: ChatGPT outperforms traditional NLG metrics like ROUGE, BERTScore, and BARTScore in correlation with human judgments across multiple NLG tasks.
- Mechanism: By leveraging its large-scale pre-training and contextual understanding, ChatGPT can capture semantic and structural qualities of generated text that n-gram overlap metrics miss, leading to better alignment with human preferences.
- Core assumption: ChatGPT's pre-training on diverse text data gives it superior semantic understanding compared to task-specific or reference-based metrics.
- Evidence anchors:
  - [abstract] "Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with golden human judgments in most cases."
  - [section 4.2] "Table 1 and Table 2 show the experimental results... ChatGPT achieves a new state-of-the-art correlation in most aspects except for fluency"
  - [corpus] Weak - corpus neighbors focus on general NLG evaluation but don't specifically compare ChatGPT to traditional metrics
- Break condition: If the reference-based or similarity-based metrics are improved to better capture semantic meaning, or if ChatGPT's training data has biases that affect its judgment consistency.

### Mechanism 3
- Claim: ChatGPT's effectiveness as an NLG evaluator is influenced by the creation method and potential biases in the meta-evaluation datasets.
- Mechanism: The paper finds that when meta-evaluation datasets are created with heavy reliance on reference texts, ChatGPT's performance may be compromised because it may not align with the reference-dependent judgment criteria.
- Core assumption: Meta-evaluation datasets that are biased toward reference-based evaluation may not accurately reflect ChatGPT's ability to perform human-like evaluation that considers diverse valid outputs.
- Evidence anchors:
  - [abstract] "In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets... For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness."
  - [corpus] Weak - corpus doesn't address dataset creation biases in relation to ChatGPT evaluation
- Break condition: If future meta-evaluation datasets are designed to be more reference-agnostic or if ChatGPT's evaluation prompts are adjusted to account for dataset biases.

## Foundational Learning

- Concept: Meta-evaluation in NLG
  - Why needed here: Understanding how automatic metrics are evaluated against human judgments is crucial for interpreting the paper's approach and results.
  - Quick check question: What is the difference between evaluating an NLG model's output and meta-evaluating an automatic evaluation metric?

- Concept: Correlation measures (Spearman, Pearson, Kendall's Tau)
  - Why needed here: The paper uses these statistical measures to quantify how well ChatGPT's evaluations align with human judgments, which is central to the findings.
  - Quick check question: When would you choose Spearman correlation over Pearson correlation in meta-evaluation?

- Concept: Prompt engineering for LLM-based evaluation
  - Why needed here: The effectiveness of ChatGPT as an evaluator depends on how well the prompts are designed to guide its evaluation behavior.
  - Quick check question: How might different prompt formulations affect ChatGPT's evaluation consistency across multiple runs?

## Architecture Onboarding

- Component map: Prompt generation module -> ChatGPT evaluation interface -> Correlation analysis module
- Critical path: Prepare NLG outputs and human judgment data -> Design evaluation prompts -> Send prompts to ChatGPT -> Extract numerical scores -> Calculate correlation with human judgments -> Analyze results
- Design tradeoffs: Using ChatGPT as an evaluator trades off computational cost and API dependency for potentially better correlation with human judgments compared to traditional metrics. The prompt design must balance specificity with generalizability across tasks.
- Failure signatures: Poor correlation results, inconsistent scores across similar inputs, or timeouts/failures when communicating with ChatGPT API would indicate system issues.
- First 3 experiments:
  1. Test ChatGPT evaluation on a small subset of SummEval with basic prompts to verify the evaluation pipeline works
  2. Compare ChatGPT scores with ROUGE scores on the same dataset to establish baseline correlation differences
  3. Run the full evaluation across all three datasets (SummEval, OpenMEV A, BAGEL) to verify the main findings about ChatGPT's effectiveness

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding ChatGPT's performance as an NLG evaluator. It notes that while ChatGPT achieves strong correlation with human judgments across multiple NLG tasks, its effectiveness may vary depending on the specific task domain and the nature of the generated text. The paper also highlights the potential influence of dataset biases on ChatGPT's evaluation performance, particularly in reference-dependent meta-evaluation datasets. Additionally, the paper suggests that improvements in prompt engineering could enhance ChatGPT's reliability as an evaluator, but does not provide specific recommendations for prompt optimization.

## Limitations
- Limited scope: The study only tests ChatGPT on three NLG tasks (summarization, story generation, data-to-text), leaving its effectiveness on other NLG domains unexplored.
- Dataset bias: The performance of ChatGPT may be compromised by biases in meta-evaluation datasets, particularly those heavily dependent on reference texts.
- Computational cost: The practical adoption of ChatGPT as an evaluation metric is limited by its API dependency and computational cost compared to traditional metrics.

## Confidence
- High confidence: ChatGPT achieves state-of-the-art or competitive correlation with human judgments across multiple NLG tasks when properly prompted. The experimental methodology and results are clearly presented and reproducible.
- Medium confidence: The claim that ChatGPT can serve as a general-purpose NLG evaluation metric is supported but limited by the narrow scope of tasks tested. The effectiveness across diverse NLG domains needs further validation.
- Low confidence: The paper's assertion that dataset creation methods significantly influence ChatGPT's effectiveness is based on limited evidence and requires more rigorous investigation with diverse meta-evaluation datasets.

## Next Checks
1. Test ChatGPT's evaluation performance on additional NLG tasks like dialogue generation and machine translation using established meta-evaluation datasets to assess generalizability.
2. Conduct ablation studies varying prompt formulations and compare consistency across multiple runs to quantify the impact of prompt engineering on evaluation reliability.
3. Analyze the computational cost and API dependency of using ChatGPT versus traditional metrics by measuring response times, costs per evaluation, and scalability to large-scale NLG evaluation scenarios.