---
ver: rpa2
title: Accelerating Batch Active Learning Using Continual Learning Techniques
arxiv_id: '2305.06408'
source_url: https://arxiv.org/abs/2305.06408
tags:
- learning
- cal-sds2
- active
- cal-sd
- cal-der
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Continual Active Learning (CAL), a framework
  that integrates continual learning (CL) techniques into batch active learning (AL)
  to accelerate training while preserving accuracy. CAL addresses the inefficiency
  of standard AL, which retrains models from scratch after each query round, by leveraging
  replay-based CL methods to prevent catastrophic forgetting and speed up adaptation
  to newly labeled data.
---

# Accelerating Batch Active Learning Using Continual Learning Techniques

## Quick Facts
- arXiv ID: 2305.06408
- Source URL: https://arxiv.org/abs/2305.06408
- Reference count: 40
- Key outcome: CAL framework achieves 3x speedup in batch active learning while maintaining accuracy

## Executive Summary
This paper introduces Continual Active Learning (CAL), a framework that integrates continual learning (CL) techniques into batch active learning (AL) to accelerate training while preserving accuracy. CAL addresses the inefficiency of standard AL, which retrains models from scratch after each query round, by leveraging replay-based CL methods to prevent catastrophic forgetting and speed up adaptation to newly labeled data. Experiments across diverse datasets and neural architectures demonstrate that CAL consistently achieves a 3x reduction in training time while maintaining or improving model performance.

## Method Summary
CAL integrates replay-based continual learning into batch active learning by maintaining a replay buffer of historical labeled data and using warm-started model parameters. The framework includes several CAL variants: CAL-ER (Experience Replay), CAL-MIR (Maximally Interfered Retrieval), CAL-DER (Dark Experience Replay), CAL-SD (Scaled Distillation), and CAL-SDS2 (Scaled Distillation with Submodular Sampling). These methods combine current task training with replay from historical data, using either uniform sampling, uncertainty-based selection, or submodular optimization to maintain diversity and representativeness in the replay buffer.

## Key Results
- CAL consistently achieves 3x reduction in training time compared to standard AL
- CAL maintains or improves accuracy across all tested datasets and models
- CAL-DER shows superior robustness to out-of-distribution data compared to standard AL
- CAL-SDS2 demonstrates the best performance by combining submodular sampling with uncertainty selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replay-based continual learning prevents catastrophic forgetting during active learning rounds.
- Mechanism: The CAL framework interleaves new task samples with a replay batch drawn from the historical labeled set. This forces the model to retain accuracy on past tasks while adapting to new ones.
- Core assumption: The replay batch selection (e.g., uniform, MIR, or submodular) sufficiently represents the historical distribution.
- Evidence anchors:
  - [abstract] "We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old"
  - [section] "replay-based CL is used to adapt f to D_t while retaining performance on D_{1:t-1}"
- Break condition: If replay batch size is too small or unrepresentative, historical performance degrades.

### Mechanism 2
- Claim: Scaled distillation with submodular sampling balances stability and plasticity.
- Mechanism: CAL-SD weights the current task loss by λ_t (increasing with task size) and uses KL-divergence distillation from past model logits. CAL-SDS2 adds a submodular objective to select diverse, uncertain replay points.
- Core assumption: The submodular facility-location + uncertainty term selects informative, representative historical samples.
- Evidence anchors:
  - [abstract] "CAL-SDS2...uses a submodular sampling procedure to select a diverse set of history points to replay"
  - [section] "we want the model to be plastic early on, and stable later on" with λ_t adjusting the trade-off
- Break condition: If submodular sampling is too greedy or over-penalizes diversity, replay batch loses representativeness.

### Mechanism 3
- Claim: Warm starting combined with replay-based CL accelerates convergence compared to retraining from scratch.
- Mechanism: Model parameters are initialized from the previous round and updated on a small, informative subset of data, reducing gradient steps.
- Core assumption: The replay batch and warm start together provide sufficient gradient signal to converge faster than training from scratch on the full dataset.
- Evidence anchors:
  - [abstract] "CAL consistently provides a 3x reduction in training time"
  - [section] "speedup comes from two sources: (1) we are computing gradient updates only on a useful subset of the history...rather than all of it...and (2) the model converges faster since it starts warm"
- Break condition: If replay batch size is too large or warm start is poor, speedup diminishes.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: CAL must avoid forgetting old labeled data when adapting to new batches; understanding forgetting explains why naive fine-tuning fails.
  - Quick check question: If a model trained on task 1 is fine-tuned only on task 2, what happens to its accuracy on task 1?

- Concept: Submodular function maximization under cardinality constraints
  - Why needed here: CAL-SDS2 uses a submodular facility-location function to select diverse, representative replay samples efficiently.
  - Quick check question: Why does the greedy algorithm give a (1 - 1/e) approximation for maximizing a monotone submodular function under a cardinality constraint?

- Concept: KL-divergence vs MSE for distillation
  - Why needed here: CAL-SD uses KL-divergence instead of MSE on logits to align distillation and classification losses in scale and dynamic range.
  - Quick check question: What is the main advantage of using KL-divergence over MSE when distilling probability distributions?

## Architecture Onboarding

- Component map: Active Learning query selection (entropy, margin, FASS, GLISTER) -> Replay-based Continual Learning module (ER, MIR, DER, SD, SDS2) -> Data loader for current batch + replay batch -> Model checkpointing for warm start
- Critical path: 1. Train on current batch + replay batch (warm start) 2. Select next query set using acquisition function 3. Store labeled data for replay
- Design tradeoffs: Replay batch size vs memory and forgetting, Submodular sampling overhead vs diversity gain, KL-divergence vs MSE distillation trade-off
- Failure signatures: Historical accuracy drop → replay batch too small or unrepresentative, No speedup → replay batch size too large or warm start ineffective, Overfitting → replay batch too small, high-capacity model
- First 3 experiments: 1. Run CAL-ER on FMNIST with batch size 64, replay size 128, confirm ~3x speedup. 2. Compare CAL-DER vs CAL-SD on CIFAR-10, vary α and β, measure robustness on CIFAR-10C. 3. Test CAL-SDS2 submodular sampling with different σ, λ on MedMNIST, evaluate diversity and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of submodular function in CAL-SDS2 affect its performance and robustness compared to other CL-based AL methods?
- Basis in paper: [inferred] The paper introduces CAL-SDS2, which uses a submodular function to select diverse and uncertain points from the history. It is noted that submodular sampling replays a diverse representative subset of history, which is likely the reason behind CAL-SDS2's better robustness over CAL-DER. However, the paper suggests that additional submodular strategies can be used to further improve results and also for diverse AL query selection.
- Why unresolved: The paper only considers one submodular function in CAL-SDS2 and suggests that other submodular functions could be explored. The impact of different submodular functions on the performance and robustness of CAL-SDS2 is not fully investigated.
- What evidence would resolve it: Experiments comparing CAL-SDS2 with different submodular functions on various datasets and AL strategies, analyzing the impact on performance, robustness, and training time.

### Open Question 2
- Question: Can the CAL framework be extended to other forms of active learning, such as online active learning or stream-based selective sampling?
- Basis in paper: [inferred] The paper focuses on batch active learning and demonstrates the effectiveness of CAL in this setting. However, it does not explore the applicability of CAL to other forms of active learning, such as online active learning or stream-based selective sampling, where data arrives sequentially and the model needs to adapt continuously.
- Why unresolved: The paper does not discuss the potential extension of CAL to other forms of active learning. The challenges and opportunities of applying CAL in these settings are not explored.
- What evidence would resolve it: Theoretical analysis and empirical experiments demonstrating the effectiveness of CAL in online active learning or stream-based selective sampling settings, comparing it with existing methods and analyzing the trade-offs in performance and computational efficiency.

### Open Question 3
- Question: How does the CAL framework perform in settings with noisy or adversarial labels, and can it be adapted to handle such scenarios?
- Basis in paper: [inferred] The paper does not explicitly address the issue of noisy or adversarial labels in the CAL framework. However, it is mentioned that CAL methods achieve similar uncertainty scores on held-out datasets and similar robustness performance on out-of-distribution data compared to standard AL models. This suggests that CAL might have some inherent robustness to label noise, but this aspect is not thoroughly investigated.
- Why unresolved: The paper does not provide a detailed analysis of CAL's performance in the presence of noisy or adversarial labels. The potential impact of label noise on CAL's effectiveness and the need for adaptations to handle such scenarios are not discussed.
- What evidence would resolve it: Experiments evaluating CAL's performance on datasets with varying levels of label noise or adversarial attacks, comparing it with standard AL methods and analyzing the impact on model accuracy, robustness, and training time. Additionally, exploring potential modifications to the CAL framework to improve its resilience to label noise, such as robust loss functions or label denoising techniques.

## Limitations

- Replay batch representativeness depends on selection strategy quality and may degrade with limited buffer size
- Hyperparameter sensitivity to replay batch size, λ_t scheduling, and submodular sampling parameters
- Limited statistical significance testing and confidence intervals across experiments

## Confidence

- High Confidence: 3x speedup claim supported by controlled experiments
- Medium Confidence: Accuracy maintenance claims supported but lack statistical significance tests
- Medium Confidence: Robustness to OOD data demonstrated but limited to one benchmark

## Next Checks

1. Reimplement CAL-SDS2 with exact submodular sampling and validate on MedMNIST
2. Conduct hyperparameter sensitivity analysis with grid search on FMNIST and CIFAR-10
3. Perform statistical significance testing with 5 random seeds and paired t-tests