---
ver: rpa2
title: The Importance of Prompt Tuning for Automated Neuron Explanations
arxiv_id: '2310.06200'
source_url: https://arxiv.org/abs/2310.06200
tags:
- neurons
- gpt-4
- prompt
- explanations
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how prompt formatting affects the quality
  of automated neuron explanations generated by large language models. The authors
  propose four new prompt formats that are more natural and computationally efficient
  than the original approach.
---

# The Importance of Prompt Tuning for Automated Neuron Explanations

## Quick Facts
- arXiv ID: 2310.06200
- Source URL: https://arxiv.org/abs/2310.06200
- Reference count: 26
- Key outcome: Proposed prompt formats improve explanation quality while reducing computational cost by 2-3×

## Executive Summary
This paper investigates how prompt formatting affects the quality of automated neuron explanations generated by large language models. The authors propose four new prompt formats that are more natural and computationally efficient than the original approach. They evaluate these prompts using automated simulations, similarity to baseline explanations, and human evaluation. The experiments show that the proposed prompts significantly improve explanation quality while reducing computational cost by 2-3×. The "Summary" prompt performs the best overall, providing the highest quality explanations with the greatest efficiency.

## Method Summary
The paper tests four new prompt formats (Summary, Highlight, HS, and A VHS) for explaining neurons in GPT-2's MLP layers using GPT-3.5 or GPT-4 as explainer models. The prompts are evaluated on 60,000 random 64-token excerpts from GPT-2's training data, testing random neurons, interpretable neurons, top-20 neurons per layer, and top-1000 neurons overall. Evaluation uses automated simulation scoring, AdaV2 similarity to baseline explanations, and human ratings on a 1-5 scale.

## Key Results
- The Summary prompt achieves the highest quality explanations (4.308±0.048 human rating) while requiring only 959 tokens versus 2338 for the original format
- All new prompts show 2-3× improvement in computational efficiency compared to the original approach
- Automated simulation scores correlate well with human evaluation, validating the efficiency of the new prompts
- The Summary prompt performs best across all evaluation metrics while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simplified prompts reduce cognitive load for the explainer model, leading to more focused and coherent explanations.
- Mechanism: By removing the interleaved activation values from the original prompt, the explainer model can concentrate on the semantic content of the text excerpts rather than processing numerical patterns. The Summary prompt, which only shows the original text and a list of highly activating tokens (90% quantile or above), provides the essential information without overwhelming the model with redundant data.
- Core assumption: The explainer model (GPT-3.5 or GPT-4) can extract meaningful patterns from the original text and token lists without needing to see the exact activation values.
- Evidence anchors:
  - [abstract]: "show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost."
  - [section 3.1]: "This prompt greatly simplifies the presentation, by just showing the original text excerpt and repeating a list of highly activating tokens (90% quantile or above)."
  - [corpus]: Weak - the corpus contains related work on prompt tuning but not specific evidence about cognitive load reduction in LLMs.

### Mechanism 2
- Claim: Reducing prompt length improves computational efficiency and allows for more examples within the context window.
- Mechanism: The original prompt is approximately 2338 tokens, while the Summary prompt is only 959 tokens (2.44× reduction). This efficiency gain means the same computational budget can evaluate more neurons, or longer prompts with additional few-shot examples can be used, potentially improving explanation quality.
- Core assumption: The reduction in token count does not remove critical information needed for accurate explanations.
- Evidence anchors:
  - [section 3.2]: "our prompts require 2-3 × less tokens than original... the cost of generating explanations is calculated per token when using the API, so a 2× reduction in tokens per neuron mean you can evaluate 2× as many neurons for the same budget."
  - [table 1]: Shows specific token counts for each prompt format and their efficiency improvements.
  - [corpus]: Weak - the corpus contains related work on prompt tuning efficiency but not specific evidence about context window utilization.

### Mechanism 3
- Claim: More natural prompt formatting improves the explainer model's ability to generate human-readable explanations.
- Mechanism: The original prompt format, with tokens and activations separated by tabs and newlines, creates an unnatural reading experience. The Highlight and Summary prompts present information in a more intuitive format that aligns with how humans would naturally process the same information, potentially leading to explanations that are more coherent and easier to understand.
- Core assumption: The explainer model's performance benefits from prompts that are more natural and readable, similar to human cognitive processes.
- Evidence anchors:
  - [section 3.1]: "this requires 4 times as many tokens as the original text from D, which results in large overhead and (ii) the text becomes unnatural/hard for a human to follow as it is interspersed with activation values all the time."
  - [table 5]: Human evaluation results showing that Summary prompt received the highest average rating (4.308±0.048) compared to Original (3.487±0.075) when using GPT-3.5.
  - [corpus]: Weak - the corpus contains related work on prompt formatting but not specific evidence about natural language processing benefits.

## Foundational Learning

- Concept: Understanding transformer architecture and neuron activation patterns
  - Why needed here: The paper focuses on explaining neurons in MLP layers of GPT-2, requiring knowledge of how transformers process information and how individual neurons contribute to the overall behavior.
  - Quick check question: What is the difference between attention layers and MLP layers in transformer architecture, and where do neuron explanations typically focus?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The entire methodology relies on crafting effective prompts for the explainer model, and the paper mentions using the same few-shot examples as the original approach while reformatting them.
  - Quick check question: How do few-shot examples influence the performance of large language models in task-specific contexts?

- Concept: Automated evaluation metrics and human evaluation methods
  - Why needed here: The paper employs multiple evaluation methods including simulation, similarity scoring (AdaCS), and human evaluation to assess explanation quality, requiring understanding of both automated metrics and human judgment processes.
  - Quick check question: What are the advantages and limitations of using automated similarity metrics versus human evaluation for assessing the quality of generated explanations?

## Architecture Onboarding

- Component map: GPT-2 (subject model) -> Activation recording -> Prompt formatting -> GPT-3.5/GPT-4 (explainer) -> Evaluation (simulation/AdaV2/human)
- Critical path: Text corpus → Subject model processing → Activation recording → Prompt formatting → Explainer model generation → Evaluation (automated or human)
- Design tradeoffs:
  - Prompt complexity vs. computational efficiency: More detailed prompts may provide better context but increase cost and reduce the number of neurons that can be evaluated
  - Automated vs. human evaluation: Automated methods are scalable but may miss nuanced quality aspects that humans can detect
  - Explainer model choice: GPT-4 provides higher quality explanations but at significantly higher cost compared to GPT-3.5
- Failure signatures:
  - Poor simulation scores despite high-quality human ratings may indicate overfitting to the specific text corpus
  - Consistently low similarity to baseline explanations across all prompt types suggests fundamental issues with the explanation approach
  - Large discrepancies between GPT-3.5 and GPT-4 performance may indicate that the prompt format is not optimized for the less capable model
- First 3 experiments:
  1. Run the original prompt format on 10 random neurons using GPT-3.5 to establish baseline performance metrics
  2. Test the Summary prompt on the same 10 neurons using GPT-3.5 to compare against baseline
  3. Compare the computational costs of both approaches to verify the claimed 2-3× efficiency improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal level of detail in neuron explanations that balances interpretability and computational efficiency?
- Basis in paper: [inferred] The paper shows that the Summary prompt outperforms other prompts in both quality and efficiency, suggesting there's an optimal balance between information provided and explanation quality.
- Why unresolved: The paper tests specific prompts but doesn't systematically explore the trade-off between information detail and explanation quality across a wider range of prompt complexities.
- What evidence would resolve it: A systematic study varying the amount of information in prompts (e.g., number of tokens, level of detail in token activations) while measuring both explanation quality and computational cost would help determine the optimal balance.

### Open Question 2
- Question: How do different neuron types or layers respond to prompt variations in terms of explanation quality?
- Basis in paper: [inferred] The paper evaluates explanations across different neuron subsets (random, interpretable, top-k per layer, top 1k overall) but doesn't deeply analyze how prompt effectiveness varies by neuron type or layer.
- Why unresolved: The experiments show overall trends but don't provide granular analysis of which prompts work best for specific neuron types or layers, leaving open questions about prompt optimization for different neuron categories.
- What evidence would resolve it: Detailed analysis of prompt effectiveness across different neuron types, layers, and functional categories, potentially leading to layer-specific or neuron-type-specific prompt strategies.

### Open Question 3
- Question: What is the relationship between simulation score and actual interpretability of neuron explanations?
- Basis in paper: [explicit] The paper uses simulation scores as a proxy for explanation quality but notes that this method has limitations, including high computational cost and reliance on GPT-4 API limitations.
- Why unresolved: While simulation scores correlate with explanation quality, they may not fully capture the true interpretability or usefulness of explanations for understanding neuron behavior.
- What evidence would resolve it: A comprehensive study comparing simulation scores to multiple measures of interpretability (human judgment, downstream task performance, cross-model consistency) would help validate simulation scores as a proxy for true interpretability.

### Open Question 4
- Question: How do explanations generated by different language models compare in quality and effectiveness?
- Basis in paper: [explicit] The paper compares explanations using GPT-3.5 and GPT-4 as explainer models, showing differences in performance, but doesn't extensively explore how different model families or sizes affect explanation quality.
- Why unresolved: The experiments are limited to GPT models, and don't explore how other language models (e.g., Claude, LLaMA) or model sizes might perform in generating neuron explanations.
- What evidence would resolve it: Systematic comparison of explanation quality across different language model families, sizes, and architectures would provide insights into the generalizability and limitations of automated neuron explanation approaches.

## Limitations

- The human evaluation was conducted with only 7-8 participants, which may not be representative of broader human judgment patterns
- The paper doesn't provide actual dollar cost comparisons between prompt formats, only token count reductions
- The improvements are tested only on GPT-2 architecture and may not generalize to other transformer models

## Confidence

**High confidence**: The computational efficiency claims are well-supported by direct token count comparisons in Table 1, showing 2-3× reduction across all new prompt formats.

**Medium confidence**: The human evaluation results showing Summary prompt superiority are compelling but based on a relatively small sample size.

**Low confidence**: The claim that prompt simplification reduces "cognitive load" for the explainer model is speculative - while the results support improved performance, the mechanism is inferred rather than directly measured.

## Next Checks

1. **Cost validation**: Run a small-scale cost comparison using the OpenAI API to verify the actual monetary savings between original and Summary prompts, calculating cost per neuron explanation across 100 neurons.

2. **Generalization test**: Apply the Summary prompt format to explain neurons in a different transformer architecture (e.g., BERT or RoBERTa) to assess whether the improvements transfer beyond GPT-2.

3. **Human evaluation replication**: Conduct a larger human evaluation study with 20+ participants to confirm the original findings about explanation quality preferences and assess inter-rater reliability.