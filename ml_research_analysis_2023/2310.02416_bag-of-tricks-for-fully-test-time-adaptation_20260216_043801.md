---
ver: rpa2
title: Bag of Tricks for Fully Test-Time Adaptation
arxiv_id: '2310.02416'
source_url: https://arxiv.org/abs/2310.02416
tags:
- batch
- accuracy
- size
- tent
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies fully test-time adaptation (TTA) with small
  batch sizes, a realistic and privacy-oriented setting where models adapt to distribution
  shifts without access to source data. The authors analyze four orthogonal techniques:
  (1) using batch renormalization or batch-agnostic normalization (GroupNorm/LayerNorm)
  to stabilize adaptation in small batches, (2) class rebalancing to address class
  imbalance in online streams, (3) entropy-based sample selection to filter unreliable
  samples during adaptation, and (4) temperature scaling for network calibration.'
---

# Bag of Tricks for Fully Test-Time Adaptation

## Quick Facts
- arXiv ID: 2310.02416
- Source URL: https://arxiv.org/abs/2310.02416
- Authors: 
- Reference count: 36
- One-line primary result: Combining four orthogonal techniques improves test-time adaptation accuracy by 17.08% on ImageNet-C with batch size 1

## Executive Summary
This paper addresses the challenge of fully test-time adaptation (TTA) with small batch sizes, a realistic setting where models must adapt to distribution shifts without access to source data. The authors propose four orthogonal techniques—batch renormalization/batch-agnostic normalization, class rebalancing (DOT), entropy-based sample selection, and temperature scaling—that collectively improve adaptation robustness. Their method, called BoT, achieves state-of-the-art results across multiple datasets and architectures, with particularly dramatic improvements on small batch sizes where traditional normalization methods fail.

## Method Summary
The method combines four orthogonal techniques to improve fully test-time adaptation in small batch settings. Batch renormalization or batch-agnostic normalization (GroupNorm/LayerNorm) stabilizes adaptation by providing reliable normalization statistics when batch size is small. Class rebalancing (DOT) addresses online class imbalance by weighting samples based on pseudo-label frequency estimates. Entropy-based sample selection filters unreliable samples by only using those with low entropy predictions for adaptation. Temperature scaling improves network calibration, affecting both entropy calculations and overall adaptation stability. These techniques are applied to the Tent baseline and evaluated across four datasets with batch sizes from 1 to 16.

## Key Results
- BoT achieves 20.31% accuracy on ImageNet-C with batch size 1 versus 0.14% for vanilla Tent (17.08% improvement)
- Across four datasets and three architectures, BoT consistently outperforms state-of-the-art methods
- Particularly significant gains on ResNet50-GN (+4.31%) and VitBase-LN (+1.53%)
- Computational benefits from reduced sample usage and improved stability across varying batch sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch renormalization and batch-agnostic normalization stabilize adaptation in small batches by providing more reliable normalization statistics.
- Mechanism: Batch normalization relies on batch statistics which become unreliable with small batch sizes, leading to unstable gradients. Batch renormalization combines batch statistics with moving averages to smooth this effect, while batch-agnostic normalization (GroupNorm/LayerNorm) doesn't depend on batch statistics at all, making them inherently stable regardless of batch size.
- Core assumption: The primary source of instability in small-batch adaptation is unreliable batch statistics affecting gradient quality.
- Evidence anchors:
  - [abstract] "using batch renormalization or batch-agnostic normalization (GroupNorm/LayerNorm) to stabilize adaptation in small batches"
  - [section] "When the batch is becoming too small, the statistics computed have a high variance and are not representative anymore of the test distribution"
  - [corpus] Weak evidence - no corpus papers directly discuss batch renormalization in TTA context

### Mechanism 2
- Claim: Class rebalancing (DOT) addresses online class imbalance by weighting samples based on pseudo-label frequency estimates.
- Mechanism: DOT maintains a momentum-based class-frequency vector updated with each sample's pseudo-label. Rare classes receive higher weights, ensuring underrepresented classes contribute meaningfully to adaptation despite potential imbalance in the streaming data.
- Core assumption: Class imbalance in online streams significantly degrades adaptation performance and can be effectively mitigated by weighting rare classes more heavily.
- Evidence anchors:
  - [abstract] "class rebalancing to address class imbalance in online streams"
  - [section] "DOT is an adaptation of the class-wise reweighting method proposed in [5] adapted to the context of test-time adaptation"
  - [corpus] Weak evidence - no corpus papers directly discuss DOT or similar class rebalancing in TTA context

### Mechanism 3
- Claim: Entropy-based sample selection filters unreliable samples by selecting only those with low entropy predictions for adaptation.
- Mechanism: Samples with high entropy predictions are likely to have unreliable gradients. By setting an entropy threshold and only using samples below this threshold for adaptation, the method reduces gradient noise and focuses adaptation on more confident predictions.
- Core assumption: Low-entropy predictions are more reliable and contribute more meaningful gradients for adaptation than high-entropy predictions.
- Evidence anchors:
  - [abstract] "entropy-based sample selection to filter unreliable samples during adaptation"
  - [section] "samples with high entropy are more likely to have a strong and noisy gradient potentially harmful to the model performance"
  - [corpus] Weak evidence - no corpus papers directly discuss entropy-based selection in TTA context

## Foundational Learning

- Concept: Batch normalization statistics and their dependence on batch size
  - Why needed here: Understanding why batch normalization fails in small batches is crucial for grasping why batch renormalization or batch-agnostic normalization is necessary
  - Quick check question: What happens to batch normalization statistics when the batch size is 1 versus when it's 64?

- Concept: Entropy as a measure of prediction confidence
  - Why needed here: Entropy-based sample selection relies on using entropy to filter unreliable samples, so understanding entropy in classification is fundamental
  - Quick check question: What is the entropy of a perfectly confident prediction versus a completely uncertain prediction in a 10-class classification problem?

- Concept: Calibration and temperature scaling
  - Why needed here: Temperature scaling is used to improve network calibration, which affects entropy calculations and overall adaptation stability
  - Quick check question: How does increasing the temperature parameter in softmax affect the entropy of the output distribution?

## Architecture Onboarding

- Component map: Tent baseline → Batch renormalization/GN/LN → Class rebalancing (DOT) → Entropy-based sample selection → Temperature scaling
- Critical path: Normalization → Sample selection → Class rebalancing → Temperature scaling (order matters for stability and effectiveness)
- Design tradeoffs: Batch renormalization vs GN/LN (computational overhead vs. simplicity), DOT buffer size vs. memory usage, entropy threshold vs. sample utilization
- Failure signatures: Accuracy collapse with batch size 1 (normalization failure), performance degradation with class imbalance (rebalancing failure), noisy adaptation (sample selection failure)
- First 3 experiments:
  1. Verify normalization stability: Compare Tent accuracy across batch sizes with BN vs. GN vs. BReN on ImageNet-C
  2. Test class rebalancing: Implement DOT and compare accuracy with and without class rebalancing under controlled imbalance
  3. Validate sample selection: Plot adaptation accuracy vs. entropy threshold to find optimal F value for your dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BoT vary across different types of distribution shifts (e.g., natural variations vs. corruptions) beyond ImageNet-C?
- Basis in paper: [inferred] The paper focuses primarily on ImageNet-C but mentions other datasets like ImageNet-Rendition, ImageNet-Sketch, and VisDA2017. It would be valuable to understand if the techniques generalize to different types of distribution shifts.
- Why unresolved: The paper does not provide a comprehensive analysis of performance across various types of distribution shifts, focusing mainly on corruption types in ImageNet-C.
- What evidence would resolve it: Experimental results showing BoT's performance on datasets with different types of distribution shifts, such as domain adaptation tasks or natural variations, would provide insights into its generalizability.

### Open Question 2
- Question: How does the computational efficiency of BoT scale with larger batch sizes and more complex architectures?
- Basis in paper: [explicit] The paper mentions that BoT requires less computational power to perform adaptation by using fewer samples. However, it does not provide a detailed analysis of how the computational efficiency scales with larger batch sizes and more complex architectures.
- Why unresolved: The paper focuses on small batch sizes and does not explore the computational efficiency of BoT with larger batch sizes and more complex architectures.
- What evidence would resolve it: Experiments comparing the computational efficiency of BoT with different batch sizes and architectures, including larger batch sizes and more complex architectures, would provide insights into its scalability.

### Open Question 3
- Question: How does the performance of BoT vary when applied to other modalities beyond images, such as text or audio?
- Basis in paper: [inferred] The paper focuses on image classification tasks and does not explore the applicability of BoT to other modalities. It would be valuable to understand if the techniques generalize to other types of data.
- Why unresolved: The paper does not provide any experiments or analysis of BoT's performance on non-image data.
- What evidence would resolve it: Experiments applying BoT to other modalities, such as text classification or audio processing tasks, would provide insights into its generalizability across different types of data.

## Limitations
- The method relies heavily on pseudo-labels for class rebalancing and sample selection, introducing uncertainty when initial model confidence is low
- The entropy threshold was tuned on ImageNet-C corruption subsets, raising questions about generalizability to other domain shifts
- The paper doesn't provide ablation studies showing whether all four techniques are necessary or if certain combinations suffice

## Confidence
- **High confidence**: Batch renormalization and batch-agnostic normalization effectively stabilize adaptation in small batches (supported by clear accuracy improvements from 0.14% to 20.31% on ImageNet-C with batch size 1)
- **Medium confidence**: Class rebalancing (DOT) significantly improves performance under class imbalance (reasonable mechanism but limited ablation evidence)
- **Medium confidence**: Entropy-based sample selection provides meaningful gradient noise reduction (supported by results but threshold tuning requirements suggest sensitivity)
- **High confidence**: Temperature scaling improves calibration and overall performance (standard technique with consistent results)

## Next Checks
1. **Pseudo-label reliability test**: Evaluate adaptation performance when starting from an initially poorly-calibrated model to assess sensitivity to noisy pseudo-labels in class rebalancing and sample selection.

2. **Threshold generalization study**: Test whether the tuned entropy threshold F = 0.54 for ResNet50-GN generalizes to other architectures and datasets, or if dataset-specific tuning is required.

3. **Ablation on technique combinations**: Systematically test all possible combinations of the four techniques (16 total) rather than just the fully combined version to identify which subsets provide most of the benefit.