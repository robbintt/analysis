---
ver: rpa2
title: Learning to Generate Better Than Your LLM
arxiv_id: '2306.11816'
source_url: https://arxiv.org/abs/2306.11816
tags:
- policy
- learning
- arxiv
- guide
- aggrevated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RLGF (Reinforcement Learning with Guided Feedback),
  a framework that uses an existing guide policy (e.g., a pretrained LLM) to enhance
  reinforcement learning for LLM fine-tuning. RLGF leverages key properties of text
  generation, such as the ability to restart generation from any point, to interact
  with the guide policy.
---

# Learning to Generate Better Than Your LLM

## Quick Facts
- arXiv ID: 2306.11816
- Source URL: https://arxiv.org/abs/2306.11816
- Reference count: 40
- Key outcome: RLGF framework achieves better performance than supervised learning and PPO baselines, with GPT-2 trained using GPT-3 as guide policy outperforming GPT-3 on IMDB task

## Executive Summary
This paper introduces RLGF (Reinforcement Learning with Guided Feedback), a framework that leverages a pretrained guide policy (e.g., GPT-3) to enhance reinforcement learning for LLM fine-tuning. The key insight is that guide policies can generate better initial states and complete partial generations, creating a more effective learning signal. Three algorithms are presented: PPO++, AggreVaTeD, and D2LOLS, which combine the strengths of imitation learning and reinforcement learning. Experiments on IMDB and CommonGen tasks demonstrate that RLGF algorithms outperform standard baselines, with D2LOLS achieving the best results.

## Method Summary
RLGF introduces a guide policy (pretrained LLM) that interacts with the learning policy through roll-in and roll-out mechanisms. During roll-in, the guide policy generates starting states for the learning policy, while during roll-out, the guide policy completes partial generations. The framework includes three algorithms: PPO++ uses guide policy for roll-in and learning policy for roll-out, AggreVaTeD does the opposite, and D2LOLS combines both approaches with a switching mechanism. The method is evaluated on IMDB sentiment classification and CommonGen commonsense reasoning tasks using GPT-2 and GPT-3 as guide policies.

## Key Results
- RLGF algorithms outperform supervised learning and PPO baselines on IMDB and CommonGen tasks
- D2LOLS achieves the best performance across all evaluated metrics
- GPT-2 policy trained with GPT-3 as guide policy outperforms GPT-3 on IMDB task
- RLGF shows consistent improvements across lexical and semantic metrics beyond the optimized objective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The guide policy provides better initial states than random initialization.
- Mechanism: The guide policy's roll-in states have higher expected reward because they are generated by a trained policy, thus improving the rollout policy's learning efficiency.
- Core assumption: The guide policy's state distribution overlaps sufficiently with the optimal policy's state distribution.
- Evidence anchors: [abstract] "The guide LLM can generate text which serves as additional starting states for the RL optimization procedure." [section] "The guide policy can provide reasonable but sub-optimal predictions for downstream tasks which our framework can then leverage to learn a near-optimal strategy."
- Break condition: If the guide policy's state distribution has minimal overlap with the optimal policy's state distribution, the benefit of guided roll-in disappears.

### Mechanism 2
- Claim: The guide policy can serve as a higher baseline for the rollout policy to surpass.
- Mechanism: By using the guide policy for roll-out completion, the rollout policy learns to match and exceed the guide's performance, creating a competitive learning dynamic.
- Core assumption: The rollout policy has sufficient capacity to learn behaviors that exceed the guide policy's capabilities.
- Evidence anchors: [abstract] "The guide LLM can also be used to complete the partial sentences generated by the LLM that is being optimized, treating the guide LLM as an expert to imitate and surpass eventually." [section] "Rolling out with πg ensures that the LLM policy πθ can be at least as good as and potentially better than the guide policy πg."
- Break condition: If the rollout policy's architecture is too limited to capture behaviors beyond the guide policy, improvement stalls.

### Mechanism 3
- Claim: The combination of PPO++ and AggreVaTeD (D2LOLS) leverages both exploration and imitation strengths.
- Mechanism: AggreVaTeD provides strong initial guidance and imitation learning, while PPO++ adds exploration capabilities, creating a balanced learning approach.
- Core assumption: The transition from imitation learning to reinforcement learning at the right time maximizes performance.
- Evidence anchors: [abstract] "D2LOLS combines PPO++ and AggreVaTeD, first performing AggreVaTeD for a certain number of iterations and then switching to PPO++." [section] "D2LOLS is the simplest approach to combine the previous methods. D2LOLS is a modification of LOLS (Algorithm 4) to incorporate a policy gradient loss."
- Break condition: If the switching point is too early or too late, the algorithm underperforms compared to using either method alone.

## Foundational Learning

- Concept: Reinforcement Learning from scratch vs. with a guide policy
  - Why needed here: Understanding how a guide policy can accelerate RL learning compared to learning from random exploration
  - Quick check question: What is the main advantage of using a guide policy's state distribution instead of random initial states in RL?

- Concept: Imitation Learning vs. Reinforcement Learning
  - Why needed here: Distinguishing between algorithms that learn from demonstrations (AggreVaTeD) versus those that learn from rewards (PPO)
  - Quick check question: How does AggreVaTeD differ from standard imitation learning algorithms in terms of the feedback it uses?

- Concept: Roll-in vs. Roll-out policies
  - Why needed here: Understanding how different policies can be used for generating initial states versus evaluating completions
  - Quick check question: In the context of text generation, what is the practical difference between a roll-in policy and a roll-out policy?

## Architecture Onboarding

- Component map:
  - Guide Policy (πg) -> Roll-in Policy -> Roll-out Policy -> Reward Function -> Update Mechanism

- Critical path:
  1. Sample prompt from dataset
  2. Generate roll-in trajectory using roll-in policy
  3. Sample state from roll-in trajectory
  4. Generate roll-out completion using roll-out policy
  5. Compute reward for roll-out completion
  6. Update value function and policy parameters
  7. Repeat for specified number of iterations

- Design tradeoffs:
  - Using the same policy for both roll-in and roll-out (simpler, but less exploration)
  - Using different policies for roll-in and roll-out (more complex, but potentially better performance)
  - Choice of switching point in D2LOLS (too early: insufficient exploration; too late: wasted imitation learning potential)

- Failure signatures:
  - Performance plateau: Guide policy state distribution has minimal overlap with optimal policy
  - Guide policy is consistently outperformed: Roll-out policy lacks capacity to exceed guide performance
  - D2LOLS underperforms: Incorrect switching point between AggreVaTeD and PPO++

- First 3 experiments:
  1. Implement PPO baseline with both roll-in and roll-out using the same policy
  2. Implement PPO++ with guide policy for roll-in and learning policy for roll-out
  3. Implement AggreVaTeD with learning policy for roll-in and guide policy for roll-out

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RLGF algorithms vary with different choices of guide policies beyond GPT-2 and GPT-3?
- Basis in paper: [explicit] The paper mentions using GPT-3 as a guide policy for GPT-2 based policies, but does not explore other combinations.
- Why unresolved: The paper only explores a limited set of guide policies and does not systematically evaluate the impact of different guide policies on RLGF performance.
- What evidence would resolve it: Experiments comparing RLGF performance using various guide policies (e.g., different GPT models, other large language models) would provide insights into the generality of the approach.

### Open Question 2
- Question: What is the impact of the mixing time parameter α in D2LOLS on the final performance and convergence speed?
- Basis in paper: [explicit] The paper mentions that D2LOLS uses a mixing time parameter α to decide when to switch from AggreVaTeD to PPO++, but does not provide a detailed analysis of its impact.
- Why unresolved: The paper only presents results for a fixed mixing time parameter and does not explore the sensitivity of D2LOLS to different values of α.
- What evidence would resolve it: Experiments varying the mixing time parameter α and analyzing the resulting performance and convergence speed would provide insights into the optimal choice of α.

### Open Question 3
- Question: How does RLGF perform on tasks beyond text generation, such as code generation or question answering?
- Basis in paper: [inferred] The paper focuses on text generation tasks but mentions that RLGF could be applied to other structured prediction problems.
- Why unresolved: The paper does not provide any evidence of RLGF's effectiveness on tasks other than text generation.
- What evidence would resolve it: Experiments applying RLGF to different structured prediction tasks, such as code generation or question answering, and comparing its performance to existing methods would demonstrate the generality of the approach.

## Limitations

- The framework relies heavily on the assumption that guide policy state distribution overlaps with optimal policy distribution, which is only weakly validated
- The switching point in D2LOLS appears arbitrary with no clear theoretical justification for the transition timing
- The claim that GPT-2 trained with RLGF can outperform GPT-3 requires careful scrutiny and may not generalize beyond specific experimental conditions

## Confidence

- High confidence: The general framework of using guide policies for roll-in and roll-out in text generation is technically sound and well-motivated by existing RL literature
- Medium confidence: The experimental results showing RLGF outperforming standard RL baselines are promising, but the small number of tasks (IMDB and CommonGen) limits generalizability
- Low confidence: The claim that RLGF can produce a GPT-2 model that outperforms GPT-3 is the most surprising and requires the most validation, as it suggests the framework can consistently achieve super-guide performance

## Next Checks

1. Test RLGF on a broader range of text generation tasks (e.g., summarization, dialogue) to assess generalizability beyond sentiment classification and commonsense reasoning
2. Conduct ablation studies on the D2LOLS switching point to determine optimal transition timing and whether the combination truly outperforms using either method alone throughout training
3. Verify the GPT-2 vs GPT-3 performance claim by running multiple independent trials with different random seeds and comparing results across various evaluation metrics to rule out statistical anomalies