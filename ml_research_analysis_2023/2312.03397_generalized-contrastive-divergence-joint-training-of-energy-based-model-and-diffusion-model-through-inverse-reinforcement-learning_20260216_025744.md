---
ver: rpa2
title: 'Generalized Contrastive Divergence: Joint Training of Energy-Based Model and
  Diffusion Model through Inverse Reinforcement Learning'
arxiv_id: '2312.03397'
source_url: https://arxiv.org/abs/2312.03397
tags:
- learning
- training
- diffusion
- data
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generalized Contrastive Divergence (GCD),
  a novel objective function for jointly training an energy-based model (EBM) and
  a diffusion model. GCD generalizes Contrastive Divergence by replacing MCMC with
  a trainable sampler, formulating joint training as a minimax problem.
---

# Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.03397
- Source URL: https://arxiv.org/abs/2312.03397
- Reference count: 40
- Primary result: GCD learning improves diffusion model sample quality by jointly training with EBM, achieving better results with 5 steps than DDPM with 1000 steps on 8 Gaussians data

## Executive Summary
This paper introduces Generalized Contrastive Divergence (GCD), a novel objective function for jointly training energy-based models (EBMs) and diffusion models. GCD generalizes Contrastive Divergence by replacing MCMC sampling with a trainable diffusion model, formulated as a minimax problem. The method has an interesting equivalence to inverse reinforcement learning, where the energy function acts as a negative reward, the diffusion model serves as a policy, and real data provides expert demonstrations. Experiments on 2D 8 Gaussians data demonstrate that GCD learning significantly improves sample quality, particularly when the number of diffusion steps is small.

## Method Summary
GCD learning jointly trains an EBM and diffusion model by alternating between updating the energy function to discriminate between data and sampler samples, and updating the diffusion model to maximize the negative energy reward. The method uses entropy regularization to prevent mode collapse and enable stable training without MCMC sampling. The objective can be viewed as minimizing an integral probability metric between the data distribution and the sampler distribution, with the energy function serving as the critic.

## Key Results
- GCD learning significantly improves sample quality of pre-trained DDPM samplers on 8 Gaussians data
- Fine-tuning DDPM with 5 steps using GCD achieves better sample quality than DDPM with 1000 steps
- Entropy regularization (τ = 0.05) is critical for obtaining accurate energy estimates and preventing mode collapse
- GCD learning enables EBM training without MCMC and improves diffusion model sample quality

## Why This Works (Mechanism)

### Mechanism 1
GCD learning directly minimizes IPM between data and sampler distributions via energy-based critic. The energy function serves as a critic in an entropy-regularized IPM formulation, where the divergence between data and sampler is bounded by the critic's output. Core assumption: The set of feasible energies E is closed under negation and contains the true data log-likelihood.

### Mechanism 2
Joint training creates a mutual improvement loop between EBM and sampler. EBM learns to distinguish data from sampler samples, while sampler learns to maximize the negative energy reward, creating a self-improving system. Core assumption: The minimax equilibrium exists and is stable during training.

### Mechanism 3
Entropy regularization prevents mode collapse and ensures stable EBM training. The entropy term encourages exploration in the sampler, preventing it from collapsing to modes, while providing regularization for the critic. Core assumption: Entropy regularization strength τ is properly tuned.

## Foundational Learning

- Kullback-Leibler Divergence:
  - Why needed here: GCD learning fundamentally builds on KL divergence as the objective function
  - Quick check question: What is the difference between KL(p||q) and KL(q||p) in the context of generative modeling?

- Contrastive Divergence:
  - Why needed here: GCD generalizes the CD algorithm by replacing MCMC with trainable samplers
  - Quick check question: Why does Contrastive Divergence avoid the need for convergent MCMC sampling?

- Inverse Reinforcement Learning:
  - Why needed here: GCD has a formal equivalence to maximum entropy IRL where energy acts as negative reward
  - Quick check question: How does the entropy regularization in GCD relate to the causal entropy in standard IRL?

## Architecture Onboarding

- Component map:
  EBM (Energy-Based Model) -> Diffusion Model Sampler -> Value Network
  Data Distribution <- Expert Demonstrations

- Critical path:
  1. Collect samples from current sampler
  2. Update EBM using REINFORCE-style gradient
  3. Update value network using TD loss
  4. Update sampler using PPO with energy reward and entropy bonus
  5. Repeat until convergence

- Design tradeoffs:
  - Entropy regularization vs. sample quality: Higher τ improves stability but may reduce fidelity
  - Update frequency: More frequent sampler updates can improve learning but increase computation
  - Baseline complexity: Simple entropy baseline vs. learned value function for variance reduction

- Failure signatures:
  - Energy function doesn't match data log-likelihood: Samples collapse to modes or show poor diversity
  - Sampler doesn't improve: Energy signal is too weak or entropy regularization dominates
  - Training instability: Imbalanced learning rates between EBM and sampler components

- First 3 experiments:
  1. Train EBM alone on 2D Gaussian mixture to verify energy learning
  2. Train pre-trained DDPM with GCD fine-tuning on 8 Gaussians with varying τ values
  3. Compare GCD vs. standard DDPM training on CIFAR-10 with small T timesteps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GCD learning scale to high-dimensional real-world datasets like images or text?
- Basis in paper: The paper mentions "We expect to obtain experimental results on larger real-world datasets in the near future."
- Why unresolved: The paper only presents preliminary results on 2D synthetic data, and scaling GCD to high-dimensional data with complex distributions remains untested.
- What evidence would resolve it: Experiments on real-world datasets (e.g., CIFAR-10, CelebA, ImageNet) demonstrating GCD's effectiveness in improving sample quality and EBM training stability compared to existing methods.

### Open Question 2
- Question: What is the theoretical convergence guarantee of GCD learning for EBM and diffusion model training?
- Basis in paper: The paper discusses the equilibrium point but doesn't provide rigorous convergence analysis for the minimax optimization problem.
- Why unresolved: The joint training involves alternating updates of EBM and diffusion model, and the convergence properties of such alternating optimization schemes are not well understood.
- What evidence would resolve it: Theoretical analysis proving the convergence of GCD learning under certain conditions, such as convexity or smoothness assumptions on the energy function and sampler.

### Open Question 3
- Question: How sensitive is GCD learning to the choice of temperature τ and the number of steps T in the diffusion model?
- Basis in paper: The paper mentions that "Temperature τ is treated as a hyperparameter" and presents results for T=5.
- Why unresolved: The optimal values of τ and T may depend on the specific data distribution and the architecture of the energy function and sampler, but the paper doesn't provide a systematic study of their impact.
- What evidence would resolve it: Experiments varying τ and T to identify their optimal ranges and studying the trade-offs between sample quality, training stability, and computational cost.

## Limitations
- Limited empirical validation to 2D synthetic data without demonstration on real-world high-dimensional datasets
- No theoretical convergence guarantees for the minimax optimization problem
- Computational overhead compared to standard diffusion model training not addressed

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Theoretical connections to IPM and IRL | High |
| Sample quality improvements on 2D data | Medium |
| Scalability claims and MCMC-free training benefits | Low |

## Next Checks

1. **High-dimensional scaling test**: Evaluate GCD on CIFAR-10 with small timestep budgets (T=5-10) and compare against standard DDPM training, measuring both sample quality and training stability.

2. **Ablation on entropy regularization**: Systematically vary τ across multiple orders of magnitude on 8 Gaussians to identify the optimal range and study the tradeoff between diversity and fidelity.

3. **Energy function analysis**: Visualize learned energy landscapes on 2D data and compare against ground truth log-likelihood to verify that GCD produces accurate critics beyond just improving sample quality.