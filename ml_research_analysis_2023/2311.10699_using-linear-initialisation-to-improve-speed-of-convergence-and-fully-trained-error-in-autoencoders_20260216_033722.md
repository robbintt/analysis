---
ver: rpa2
title: Using linear initialisation to improve speed of convergence and fully-trained
  error in Autoencoders
arxiv_id: '2311.10699'
source_url: https://arxiv.org/abs/2311.10699
tags:
- matrix
- straddled
- initialisation
- loss
- identity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Straddled Matrix, a novel weight initialization
  technique for autoencoders. Motivated by the assumption that major data relationships
  are linear, Straddled Matrix generalizes the Identity matrix to non-square matrices,
  initializing networks as de facto linear models.
---

# Using linear initialisation to improve speed of convergence and fully-trained error in Autoencoders

## Quick Facts
- **arXiv ID:** 2311.10699
- **Source URL:** https://arxiv.org/abs/2311.10699
- **Reference count:** 11
- **Primary result:** Straddled Matrix initialization achieves faster convergence and lower reconstruction error in 19/21 comparisons across three datasets

## Executive Summary
This paper introduces Straddled Matrix, a novel weight initialization technique for autoencoders that generalizes the identity matrix to non-square matrices. The method is motivated by the assumption that major data relationships are linear, initializing networks as de facto linear models when combined with ReLU activation. Tested across synthetic, MNIST, and Swarm Behaviour datasets using seven state-of-the-art initializers, Straddled Matrix consistently outperformed existing techniques in both convergence speed and final reconstruction error.

## Method Summary
Straddled Matrix initialization creates a weight matrix with exactly one "1" per row, ensuring no zeros in any row and no overlaps between columns. When combined with ReLU activation and min-max scaled inputs ([0,1]), the first forward pass is purely linear with unit activation magnitudes. The method is tested on autoencoders with a fixed architecture (64-33-64 layers) across three datasets: synthetic (100 features, 5000 records), MNIST (784 features), and Swarm Behaviour (2400 features). Training uses RMSE loss with convergence criteria based on loss stability.

## Key Results
- On synthetic dataset: converged in 294 epochs with RMSE 0.098 (p<0.001 vs. others)
- On MNIST dataset: converged at 611 epochs with RMSE 0.100
- On Swarm Behaviour dataset: reached RMSE 0.176 in just 20 epochs

## Why This Works (Mechanism)

### Mechanism 1
- Straddled Matrix initialization improves convergence by starting the autoencoder in a near-linear regime where gradients are stable and predictable
- Creates a weight matrix with exactly one "1" per row, no zeros in any row, and no overlaps between columns
- When combined with ReLU activation and min-max scaled inputs, the first forward pass is purely linear with unit activation magnitudes
- Avoids vanishing/exploding gradients and gives a smooth optimization landscape
- Core assumption: The majority of signal variance in the data can be captured by a linear transformation
- Break condition: If data contains predominantly nonlinear interactions, or if input scaling is not [0,1], the linear assumption fails

### Mechanism 2
- Straddled Matrix avoids dead neurons by ensuring every neuron receives at least one active input
- Unlike Identity initialization, Straddled Matrix repeats the diagonal pattern so each row has exactly one "1"
- In ReLU networks, this guarantees that each neuron can be active from the first forward pass
- Core assumption: Dead neurons in early training slow convergence; maintaining activation diversity is beneficial
- Break condition: If batch normalization or other activation-preserving techniques are used, the benefit may be redundant

### Mechanism 3
- Equal weighting of all features in Straddled Matrix reduces bias in early gradient updates, leading to faster convergence
- Each input feature is given the same initial weight magnitude (exactly 1) without preferential scaling
- This symmetric starting point avoids early feature dominance and lets the network learn feature importance organically
- Core assumption: Symmetry in initialization leads to more balanced gradient flow and avoids premature feature specialization
- Break condition: If features have vastly different scales or importance, equal weighting may be suboptimal

## Foundational Learning

- **Linear algebra for non-square matrices (rectangular identity generalization)**
  - Why needed here: Straddled Matrix is defined as a generalization of the identity matrix to rectangular shapes
  - Quick check question: How does a 5x3 Straddled Matrix differ from a 3x5 Straddled Matrix in terms of the placement of ones?

- **ReLU activation properties in the context of initialization**
  - Why needed here: The benefit of Straddled Matrix relies on ReLU preserving linearity when inputs are positive
  - Quick check question: Why does ReLU preserve linearity when all inputs are scaled to [0,1] and weights are initialized to 1?

- **Autoencoder architecture and reconstruction loss**
  - Why needed here: The experiments measure convergence in terms of reconstruction error
  - Quick check question: What is the reconstruction error metric used, and how does it relate to training convergence?

## Architecture Onboarding

- **Component map:** Input layer -> Encoder (64→33) -> Bottleneck (33) -> Decoder (33→64) -> Output layer
- **Critical path:** Initialize weights with Straddled Matrix → Forward pass (linear activations) → Compute RMSE loss → Backpropagate via SGD → Check convergence (loss stable within ±ε for α epochs)
- **Design tradeoffs:**
  - Pros: Faster convergence, lower final error, no dead neurons
  - Cons: Assumes mostly linear data structure, sensitive to input scaling, not ideal for highly nonlinear manifolds
- **Failure signatures:** Slow or stalled convergence, high variance in loss across runs, failure to reduce reconstruction error below baseline
- **First 3 experiments:**
  1. Reproduce synthetic dataset experiment: compare Straddled Matrix vs. GlorotUniform convergence speed and final RMSE
  2. Run MNIST experiment: check if Straddled Matrix still converges faster despite high nonlinearity
  3. Test Swarm Behaviour dataset: evaluate convergence on high-dimensional, compression-heavy data

## Open Questions the Paper Calls Out

- **Open Question 1:** How does Straddled Matrix initialization perform when combined with activation functions other than ReLU?
  - Basis in paper: The paper primarily tests Straddled Matrix with ReLU activation
  - Why unresolved: The study focused specifically on ReLU-based autoencoders
  - What evidence would resolve it: Experiments comparing Straddled Matrix with various activation functions on the same datasets

- **Open Question 2:** What is the theoretical justification for why Straddled Matrix initialization leads to faster convergence and better performance?
  - Basis in paper: The authors hypothesize that initializing with a de facto linear model provides a better starting point for gradient descent
  - Why unresolved: The paper demonstrates empirical superiority but doesn't establish formal mathematical reasoning
  - What evidence would resolve it: Analytical derivation showing how Straddled Matrix affects the loss landscape geometry

- **Open Question 3:** How does Straddled Matrix initialization affect the generalization ability of trained autoencoders on unseen data?
  - Basis in paper: The paper focuses on reconstruction error and convergence metrics but doesn't explicitly test generalization performance
  - Why unresolved: While the paper shows lower reconstruction error during training, it doesn't investigate whether this translates to better performance on new data
  - What evidence would resolve it: Experiments measuring test set reconstruction error and downstream task performance

- **Open Question 4:** What is the impact of Straddled Matrix initialization on deeper autoencoder architectures with more layers?
  - Basis in paper: The authors mention this as a potential future research direction
  - Why unresolved: The study only tested a shallow three-hidden-layer architecture
  - What evidence would resolve it: Experiments training deep autoencoders (4+ hidden layers) with Straddled Matrix initialization

## Limitations

- The Straddled Matrix performance advantage may be dataset-dependent, particularly benefiting structured or near-linear data rather than truly nonlinear manifolds
- The convergence criteria (stability within ±ε for α epochs) is fixed but not justified why these specific values are optimal across all datasets
- The study compares against only seven initializers; other potentially relevant methods like Kaiming or sparse initialization are not included

## Confidence

- **High confidence:** The core mechanism of Straddled Matrix creating a linear regime at t=0 is mathematically verifiable
- **Medium confidence:** Convergence speed claims are statistically significant in 19/21 comparisons but may not generalize to all real-world scenarios
- **Low confidence:** The dead neuron prevention claim lacks direct empirical validation in the paper

## Next Checks

1. Test Straddled Matrix on highly nonlinear datasets (e.g., CIFAR-10) to assess robustness beyond the studied domains
2. Conduct ablation studies: measure performance when input scaling deviates from [0,1] to test sensitivity to preprocessing
3. Compare against additional initialization methods including Kaiming/He initialization variants and sparse initialization techniques