---
ver: rpa2
title: Two-Timescale Q-Learning with Function Approximation in Zero-Sum Stochastic
  Games
arxiv_id: '2312.04905'
source_url: https://arxiv.org/abs/2312.04905
tags:
- function
- learning
- have
- approximation
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of independent learning in two-player
  zero-sum stochastic games with function approximation. The authors propose a two-timescale
  Q-learning algorithm that maintains fast-timescale and slow-timescale iterates for
  both players.
---

# Two-Timescale Q-Learning with Function Approximation in Zero-Sum Stochastic Games

## Quick Facts
- **arXiv ID:** 2312.04905
- **Source URL:** https://arxiv.org/abs/2312.04905
- **Reference count:** 40
- **Key outcome:** Establishes last-iterate finite-sample convergence guarantees for independent Q-learning in zero-sum stochastic games with linear function approximation

## Executive Summary
This paper addresses the challenge of independent learning in two-player zero-sum stochastic games with function approximation. The authors propose a two-timescale Q-learning algorithm that maintains fast-timescale and slow-timescale iterates for both players, where the slow-timescale iterates are used to compute policies. The key innovation is connecting the slow-timescale update dynamics to smoothed best-response dynamics in the parameter space, which enables construction of a valid Lyapunov function for proving convergence. Under linear function approximation and certain assumptions, the algorithm achieves polynomial sample complexity to find a Nash equilibrium, measured by the Nash gap.

## Method Summary
The proposed two-timescale Q-learning algorithm maintains separate iterates for each player: fast-timescale iterates updated via projected SGD to minimize a Bellman error variant, and slow-timescale iterates updated through convex combinations of their previous values and fast-timescale iterates. The slow-timescale iterates determine the policies and follow dynamics resembling smoothed best-response in the function approximation parameter space. Target networks are used to stabilize learning, and the algorithm operates in an inner-outer loop structure with K inner iterations per outer iteration. The construction of a Lyapunov function based on Moreau envelopes enables finite-sample convergence analysis.

## Key Results
- Establishes last-iterate finite-sample convergence guarantees for Nash equilibrium in zero-sum stochastic games
- Achieves polynomial sample complexity under linear function approximation and Bellman completeness assumptions
- Proposes a novel Lyapunov function construction using Moreau envelopes for the slow-timescale iterates
- Demonstrates that the two-timescale structure prevents the "deadly triad" of divergence in TD-learning with function approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The slow-timescale iterates follow a generalized smoothed best-response dynamic in the parameter space.
- Mechanism: By taking convex combinations between slow-timescale iterates and fast-timescale updates, the algorithm approximates smoothed best-response dynamics implemented in the function approximation parameter space.
- Core assumption: The fast-timescale iterates provide sufficiently accurate estimates of marginalized opponent payoffs.
- Evidence anchors:
  - [abstract] "the slow-timescale iterates are updated by taking convex combinations between their previous values and the fast-timescale iterates"
  - [section] "the update equation for θi_t,k can be interpreted as a generalization of the smoothed best-response dynamics"
- Break condition: Fast-timescale estimation error becomes too large relative to the learning rate ratio.

### Mechanism 2
- Claim: The Moreau envelope construction enables smooth Lyapunov analysis in the parameter space.
- Mechanism: The generalized Moreau envelope smooths the regularized Nash gap, allowing Lyapunov-based convergence analysis even when iterates are not probability distributions.
- Core assumption: The seminorm-based extension preserves the essential convexity properties needed for convergence analysis.
- Evidence anchors:
  - [abstract] "we construct our Lyapunov function as the inﬁmal convolution between the ERNG and the square of a properly deﬁned seminorm"
  - [section] "The construction of a valid Lyapunov function for θi_t,k presents one of our main technical novelties"
- Break condition: The seminorm fails to properly regularize when function approximation parameters grow too large.

### Mechanism 3
- Claim: The two-timescale structure enables convergence where single-timescale methods fail.
- Mechanism: Fast timescale handles TD-learning for Q-function estimation while slow timescale implements approximate best-response dynamics, preventing the deadly triad of divergence.
- Core assumption: The stepsize ratio βk ≪ αk is maintained throughout learning.
- Evidence anchors:
  - [abstract] "the fast-timescale iterates are updated in spirit to the stochastic gradient descent"
  - [section] "we require βk ≪ αk so that θi_t,k evolves at a much slower rate compared to that of wi_t,k"
- Break condition: Stepsize ratio becomes too small or too large, destabilizing the dynamics.

## Foundational Learning

- Concept: Two-timescale stochastic approximation
  - Why needed here: Enables separate convergence rates for Q-function estimation and policy improvement
  - Quick check question: What happens if both timescales use the same learning rate?

- Concept: Lyapunov stability theory
  - Why needed here: Provides framework for establishing finite-sample convergence guarantees
  - Quick check question: How does the Moreau envelope construction preserve the Lyapunov property?

- Concept: Function approximation completeness
  - Why needed here: Ensures the function class can represent the Bellman error structure
  - Quick check question: What breaks if the feature matrix doesn't satisfy the Bellman completeness assumption?

## Architecture Onboarding

- Component map:
  - Fast-timescale iterates {wi_t,k}: TD-learning updates for Q-function estimation
  - Slow-timescale iterates {θi_t,k}: Policy parameter updates via convex combination
  - Target networks {¯wi_t, ¯θi_t}: Stabilize learning by fixing parameters during inner loop
  - Feature matrix Φi: Maps parameters to Q-function estimates

- Critical path: Fast-timescale TD → Slow-timescale policy update → Target network sync → Outer loop value iteration

- Design tradeoffs:
  - Projection radius M vs convergence speed
  - Temperature τ vs policy stochasticity
  - Stepsize ratio β/α vs stability vs convergence rate

- Failure signatures:
  - Divergence when stepsize ratio β/α is too large
  - Slow convergence when projection radius M is too small
  - Oscillations when temperature τ is too high

- First 3 experiments:
  1. Test stability with varying stepsize ratios β/α on a simple matrix game
  2. Verify Moreau envelope smoothness by checking gradient bounds
  3. Measure convergence rate sensitivity to projection radius M

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the projection operator Proj_i^M(·) in Algorithm 1 be removed without affecting convergence guarantees?
- Basis in paper: The authors mention in the conclusion that removing the projection operator is a future direction because "the projection radius depends on unknown parameters of the stochastic game, which is not ideal from a practical viewpoint."
- Why unresolved: The projection operator is used for stability, but it introduces dependence on unknown game parameters. Removing it could simplify implementation but may affect theoretical guarantees.
- What evidence would resolve it: Experimental results showing stable convergence without projection, or theoretical analysis proving convergence without projection under weaker assumptions.

### Open Question 2
- Question: Can the convergence rate of the proposed algorithm be improved beyond the current bounds?
- Basis in paper: The authors state in the conclusion that "we believe the convergence rate is not tight, and improving the rate is another future direction."
- Why unresolved: The current finite-sample analysis establishes polynomial sample complexity, but the authors suspect the rate can be tightened. This is a common goal in reinforcement learning theory.
- What evidence would resolve it: Tighter finite-sample bounds, potentially matching lower bounds for this problem class, or improved Lyapunov function constructions that yield faster convergence rates.

### Open Question 3
- Question: Can the Lyapunov function construction approach be generalized to other multi-agent reinforcement learning settings beyond zero-sum stochastic games?
- Basis in paper: The authors note in the conclusion that "we would like to see if the algorithmic ideas and technical tools developed in this work, especially the Lyapunov function construction process illustrated in Section 5, can be applied to other MARL settings."
- Why unresolved: The Lyapunov function construction is a key technical novelty that enabled the finite-sample analysis. Its applicability to other settings remains an open question.
- What evidence would resolve it: Successful application of the Lyapunov construction to other MARL problems like general-sum games, cooperative settings, or continuous control, with corresponding convergence guarantees.

## Limitations
- The Bellman completeness assumption (Assumption 4.1) is strong and may not hold for many practical function approximation choices
- The algorithm requires running K inner iterations for each outer iteration, which may be computationally expensive in practice
- The analysis assumes access to the transition kernel for the Bellman operator definition, creating a potential gap between theory and implementation

## Confidence
- **High confidence**: The two-timescale structure prevents the "deadly triad" of divergence in TD-learning with function approximation
- **Medium confidence**: The finite-sample convergence guarantees hold under the stated assumptions, but practical performance may vary significantly
- **Medium confidence**: The connection to smoothed best-response dynamics provides theoretical insight, but practical implications for hyperparameter tuning are less clear

## Next Checks
1. Systematically vary the ratio β/α and measure its impact on convergence stability and speed across different game structures
2. Test the algorithm with intentionally incomplete feature sets to quantify how violations of Bellman completeness affect performance
3. Evaluate the algorithm's performance across a range of temperature values to identify optimal exploration-exploitation tradeoffs for different game types