---
ver: rpa2
title: 'POCKET: Pruning Random Convolution Kernels for Time Series Classification
  from a Feature Selection Perspective'
arxiv_id: '2309.08499'
source_url: https://arxiv.org/abs/2309.08499
tags:
- p-rocket
- time
- kernels
- stage
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes P-ROCKET, a two-stage feature selection approach
  for pruning random convolution kernels in ROCKET and MINIROCKET time series classification
  models. The key idea is to identify and remove kernels that contribute minimally
  to classification accuracy by imposing group sparsity in the classification layer.
---

# POCKET: Pruning Random Convolution Kernels for Time Series Classification from a Feature Selection Perspective

## Quick Facts
- arXiv ID: 2309.08499
- Source URL: https://arxiv.org/abs/2309.08499
- Reference count: 40
- Primary result: Prunes 60%+ of ROCKET/MINIROCKET kernels with minimal accuracy loss, achieving 11× speedup

## Executive Summary
This paper introduces P-ROCKET, a feature selection approach that prunes random convolution kernels in ROCKET and MINIROCKET time series classification models. By imposing group sparsity in the classification layer, P-ROCKET identifies and removes kernels that contribute minimally to classification accuracy. The two-stage algorithm achieves over 60% kernel reduction without significant accuracy loss, demonstrating 11× speedup compared to S-ROCKET. The method outperforms reduced-kernel models trained from scratch and enables efficient deployment on resource-constrained devices.

## Method Summary
P-ROCKET addresses kernel pruning by treating it as a feature selection problem at the classification layer. The method uses Group Elastic Net regularization with ℓ2,1 norm to induce row-wise sparsity in the classifier weight matrix, enabling entire groups of features (and their originating kernels) to be pruned simultaneously. The algorithm operates in two stages: Stage 1 performs kernel pruning via group-wise regularization using ADMM optimization, while Stage 2 optionally refits a ridge classifier to prevent overfitting. Dynamic thresholding ensures the desired pruning rate is achieved.

## Key Results
- Achieves 60%+ kernel pruning without significant accuracy loss across 128 UCR datasets
- Provides 11× speedup compared to S-ROCKET
- Outperforms reduced-kernel models trained from scratch
- Effective for both ROCKET and MINIROCKET architectures

## Why This Works (Mechanism)

### Mechanism 1
Random convolution kernels can be pruned safely by removing entire groups of features that make negligible contributions to the classifier. Features are grouped by their originating kernel; if all features in a group have near-zero weights in the classifier, that kernel is deemed redundant and can be removed without significant accuracy loss. This relies on the assumption that feature importance is reflected in classifier weight magnitude.

### Mechanism 2
Group Elastic Net regularization induces row-wise sparsity in the classifier weight matrix, enabling simultaneous feature selection and kernel pruning. The ℓ2,1 norm on the reshaped classifier weights encourages entire groups (rows) to become zero, while ℓ2 regularization prevents overfitting. The ADMM algorithm solves this by dynamically thresholding weight groups.

### Mechanism 3
P-ROCKET accelerates kernel pruning by splitting group-wise and element-wise regularization into two sequential stages, avoiding repeated matrix inversions. Stage 1 uses group-wise regularization to select important kernels/features; Stage 2 optionally refits a ridge classifier on the selected features. By maintaining a constant ratio between penalties, the expensive inverse computation is done only once.

## Foundational Learning

- **Concept: Group Elastic Net regularization (ℓ2,1 norm)**
  - Why needed: Enables row-wise sparsity, essential for identifying entire groups of features (and thus kernels) that can be pruned together
  - Quick check: What is the difference between ℓ1, ℓ2, and ℓ2,1 norms, and why does ℓ2,1 lead to group sparsity?

- **Concept: ADMM (Alternating Direction Method of Multipliers)**
  - Why needed: Provides a scalable way to solve the convex optimization problem arising from Group Elastic Net regularization
  - Quick check: How does ADMM decompose a complex optimization problem into simpler subproblems, and what are the key update equations?

- **Concept: Dynamic thresholding for group sparsity**
  - Why needed: Allows achieving a user-defined pruning rate by adjusting the threshold based on the sorted magnitudes of weight groups
  - Quick check: How does the dynamic threshold in P-ROCKET ensure a specific number of kernels are retained, and what is the role of the parameter k?

## Architecture Onboarding

- **Component map**: Input time series data matrix D (N samples) → ROCKET/MINIROCKET random convolution kernels (F kernels) → Feature matrix X ∈ RN×F → Initial classifier W ∈ RF×C → P-ROCKET pruning algorithm → Pruned kernels and refitted classifier

- **Critical path**: 1. Generate random kernels and extract features (X) 2. Normalize X (zero-center, unit norm per group) 3. Stage 1: Group-wise regularization to prune kernels (via W and Θ updates) 4. Retain m important kernel groups, discard others 5. Stage 2 (optional): Refit ridge classifier on remaining features 6. Output pruned model

- **Design tradeoffs**: Accuracy vs. speed (11× speedup vs. S-ROCKET but may have slightly lower accuracy in some cases), Stage 2 inclusion (improves accuracy but adds computation), Hyperparameter k (balances sparsity vs. classification accuracy)

- **Failure signatures**: Zero pruning rate (k too large), Over-pruning (k too small), Slow convergence (insufficient iterations or poor k choice)

- **First 3 experiments**: 1. Run P-ROCKET with default k on 'Beef' dataset and verify ~50% kernel pruning without major accuracy drop 2. Compare training time of P-ROCKET vs. S-ROCKET on same dataset to confirm speedup claim 3. Test effect of disabling Stage 2 on MINIROCKET to observe overfitting mitigation

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of P-ROCKET vary across different time series domains (e.g., ECG, motion capture, sensor data) when pruning more than 60% of kernels? The paper provides aggregate results across all 128 datasets but lacks domain-specific analysis.

### Open Question 2
What is the theoretical limit of kernel pruning for ROCKET/MINIROCKET models before classification accuracy significantly degrades? The paper only explores pruning rates up to 60% and doesn't investigate maximum possible pruning.

### Open Question 3
How does P-ROCKET's performance compare to model compression techniques specifically designed for deep neural networks when applied to the "ROCKET family"? The paper only compares P-ROCKET to S-ROCKET and reduced-kernel models, not to established CNN compression methods.

## Limitations
- Pruning effectiveness relies on classifier weights as proxies for feature importance, which may not hold universally across all time series datasets
- Two-stage approach's effectiveness depends heavily on hyperparameter tuning with no clear guidance for different dataset characteristics
- Evaluation focuses primarily on accuracy metrics without detailed analysis of how pruned kernels affect interpretability or generalization to unseen data distributions

## Confidence
- **High Confidence**: Core pruning methodology and mathematical formulation are sound with clear implementation steps and reproducible results across 128 UCR datasets
- **Medium Confidence**: Claimed 11× speedup versus S-ROCKET is demonstrated but comparison methodology could benefit from additional baselines and computational resource details
- **Low Confidence**: Generalization claims to "resource-constrained devices" lack empirical validation on actual edge hardware or real-time performance measurements

## Next Checks
1. Systematically vary k and penalty parameters across datasets with different characteristics (length, dimensionality, class balance) to establish robust selection guidelines
2. Evaluate whether kernels pruned on one dataset maintain effectiveness when transferred to related but distinct time series classification tasks
3. Implement pruned models on actual resource-constrained hardware (e.g., Raspberry Pi, mobile devices) to verify real-world speed and memory benefits