---
ver: rpa2
title: 'Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the
  Game of Hanabi'
arxiv_id: '2308.10284'
source_url: https://arxiv.org/abs/2308.10284
tags:
- agents
- adaptation
- regret
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of coordinating with novel agents
  in cooperative multi-agent reinforcement learning (MARL). While zero-shot coordination
  (ZSC) has been studied, it may not be sufficient for complex tasks and changing
  environments.
---

# Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the Game of Hanabi

## Quick Facts
- arXiv ID: 2308.10284
- Source URL: https://arxiv.org/abs/2308.10284
- Authors: 
- Reference count: 40
- Primary result: State-of-the-art ZSC algorithms require millions of samples to adapt to novel partners, while naive IQL adapts as quickly as OBL in most cases

## Executive Summary
This paper introduces a framework for evaluating the adaptability of multi-agent reinforcement learning (MARL) algorithms in the context of Hanabi, focusing on few-shot coordination (FSC) as a complement to zero-shot coordination (ZSC). The authors propose the adaptation regret metric to measure how quickly an agent can improve its coordination performance when paired with novel partners. Through extensive experiments, they find that sophisticated ZSC algorithms like Off-Belief Learning (OBL) perform poorly when paired with agents trained using different methods, requiring millions of interactions to adapt. Surprisingly, simple Independent Q-Learning (IQL) agents adapt as quickly as OBL in most cases. The paper also investigates how hyperparameters affecting training data diversity and optimization process significantly impact adaptability.

## Method Summary
The authors create a diverse pool of pre-trained Hanabi agents using different architectures (LSTM/GRU, various layer sizes) and algorithms (IQL, VDN, OBL with/without OP). They evaluate agents using a novel adaptation regret metric that measures cumulative performance gaps during the adaptation phase. The method involves pre-training agents with various MARL algorithms, then measuring how quickly a learner agent can adapt to held-out partners from the pool. Adaptation is performed through IQL-style gradient updates, with performance evaluated across multiple independent games and averaged across partners. The framework quantifies both zero-shot coordination (initial performance) and few-shot coordination (adaptation speed).

## Key Results
- State-of-the-art ZSC algorithms like OBL require millions of interaction samples to adapt to novel partners
- Naive IQL agents adapt as quickly as OBL in most cases when paired with diverse partners
- Two categories of hyperparameters (training data diversity and optimization process) significantly impact adaptation regret
- Partner diversity and strength significantly affect both zero-shot and adaptation performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: State-of-the-art ZSC algorithms perform poorly when paired with agents trained using different underlying algorithms
- Mechanism: ZSC methods optimize for zero-shot coordination with independently trained agents using the same algorithm, creating strategy conventions specific to that algorithm's training dynamics. When paired with agents from different training paradigms, these conventions break down.
- Core assumption: Zero-shot coordination requires transferable conventions that generalize across different training algorithms
- Evidence anchors:
  - [abstract] "state-of-the-art ZSC algorithms have poor performance when paired with agents trained with different learning methods"
  - [section] "Even though OBL algorithms perform well when paired with other agents trained with OBL, it performs poorly when paired with other SP agents like IQL or VDN"
- Break condition: When agents from different algorithms share compatible conventions through training dynamics that produce overlapping strategy spaces

### Mechanism 2
- Claim: Naive IQL agents can adapt as quickly as sophisticated ZSC algorithms when paired with novel partners
- Mechanism: IQL's simplicity and direct value function learning allows it to quickly adjust to new partner behaviors through standard reinforcement learning updates, while more complex ZSC algorithms may be over-optimized for zero-shot performance at the expense of adaptability
- Core assumption: Simpler algorithms can be more adaptable than complex ones when faced with distributional shift
- Evidence anchors:
  - [abstract] "naive Independent Q-Learning (IQL) agents in most cases adapt as quickly as the SOTA ZSC algorithm Off-Belief Learning (OBL)"
  - [section] "we also discovered that naive Independent Q-Learning (IQL) agents adapted to some of the partners as quickly as the SOTA Off-Belief Learning (OBL) algorithm"
- Break condition: When partner diversity exceeds the adaptation capacity of simple Q-learning update rules

### Mechanism 3
- Claim: Two categories of hyperparameters significantly influence adaptation capability: those controlling training data diversity and those affecting the optimization process
- Mechanism: Data diversity hyperparameters (like number of distributed threads and replay buffer size) determine the range of experiences the agent encounters, while optimization hyperparameters (like learning rate and batch size) control how quickly the agent can adjust to new information from novel partners
- Core assumption: Adaptation capability is directly linked to both the diversity of experiences an agent has seen and the flexibility of its learning process
- Evidence anchors:
  - [abstract] "two categories of hyper-parameters controlling the training data diversity and optimization process have a significant impact on the adaptation regret"
  - [section] "Our experiments show that two categories of hyperparameters...have a significant impact on the adaptation regret"
- Break condition: When hyperparameter tuning cannot overcome fundamental limitations in the learning algorithm's architecture

## Foundational Learning

- Concept: Zero-shot coordination (ZSC)
  - Why needed here: Understanding ZSC is essential because the paper builds upon ZSC concepts to introduce few-shot coordination, comparing performance against ZSC baselines
  - Quick check question: What is the key difference between ZSC and few-shot coordination in terms of agent interaction requirements?

- Concept: Dec-POMDPs and decentralized multi-agent reinforcement learning
  - Why needed here: The paper operates within the Dec-POMDP framework, using this formalism to define the coordination problem and evaluation metrics
  - Quick check question: How does the partially observable nature of Dec-POMDPs affect the design of coordination strategies in Hanabi?

- Concept: Adaptation regret as a metric
  - Why needed here: The paper introduces adaptation regret as a novel metric to quantify both ZSC and few-shot coordination performance, making it central to understanding the evaluation framework
  - Quick check question: What components make up the adaptation regret metric and how do they capture both zero-shot and few-shot coordination capabilities?

## Architecture Onboarding

- Component map: Pre-trained agent pool -> Evaluation framework (adaptation regret) -> Adaptation phase -> Performance evaluation
- Critical path: 1) Create diverse pool of pre-trained agents, 2) Select learner and partner pairs, 3) Run adaptation phase with incremental performance evaluation, 4) Calculate adaptation regret, 5) Analyze hyperparameter effects
- Design tradeoffs: Simpler algorithms like IQL may be more adaptable but achieve lower final performance; complex ZSC algorithms may excel at zero-shot performance but require more samples to adapt
- Failure signatures: Poor adaptation indicated by high adaptation regret values, convergence to lower game scores than maximum achievable score, or sensitivity to hyperparameter choices
- First 3 experiments:
  1. Replicate the cross-play matrix comparison between different agent types to verify ZSC performance limitations
  2. Run adaptation curves for IQL vs OBL agents to confirm the adaptation speed comparison
  3. Perform hyperparameter ablation study on learning rate and batch size to validate their impact on adaptation regret

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design MARL algorithms that achieve both high ZSC performance and fast adaptation to unseen partners?
- Basis in paper: [explicit] The paper states: "This finding raises an interesting research question: How to design MARL algorithms with high ZSC performance and capability of fast adaptation to unseen partners."
- Why unresolved: Current SOTA ZSC algorithms like OBL perform well in ZSC but require millions of interactions to adapt to new partners. IQL agents, despite being naive, adapt as quickly as OBL in most cases, indicating a gap in designing algorithms that excel in both ZSC and adaptation.
- What evidence would resolve it: Developing and evaluating new MARL algorithms that achieve high ZSC performance while demonstrating rapid adaptation to novel partners with minimal interactions.

### Open Question 2
- Question: What is the impact of partner diversity on the adaptation performance of MARL algorithms?
- Basis in paper: [explicit] The paper mentions: "we investigated the choice of the partners which can greatly affects both zero-shot and adaptation performance of the learner."
- Why unresolved: The paper suggests that partner diversity affects adaptation performance, but a detailed analysis of how different levels of partner diversity impact adaptation is not provided.
- What evidence would resolve it: Conducting experiments with varying levels of partner diversity and analyzing the adaptation performance of different MARL algorithms to understand the correlation between partner diversity and adaptation.

### Open Question 3
- Question: How do different hyperparameter settings influence the adaptation regret of MARL algorithms?
- Basis in paper: [explicit] The paper states: "Our experiments show that two categories of hyperparameters controlling the training data diversity and optimization process have a significant impact on the adaptation regret."
- Why unresolved: While the paper identifies two categories of hyperparameters that significantly impact adaptation regret, it does not provide a comprehensive analysis of how specific hyperparameter settings influence adaptation.
- What evidence would resolve it: Performing extensive hyperparameter tuning experiments and analyzing the adaptation regret for different hyperparameter settings to identify the optimal configurations for adaptation.

## Limitations

- The study is limited to the Hanabi environment, which may not generalize to other cooperative multi-agent tasks
- The adaptation regret metric has not been validated across different domains beyond Hanabi
- The comparison between IQL and OBL adaptation speeds may be misleading since IQL's baseline performance could be significantly lower
- The paper doesn't fully explore the computational costs of adaptation versus the benefits gained

## Confidence

- **High Confidence**: The experimental observation that ZSC algorithms struggle with cross-algorithm coordination (Mechanism 1) is well-supported by cross-play matrix results and represents a clear empirical finding.
- **Medium Confidence**: The claim that IQL adapts as quickly as OBL (Mechanism 2) is supported by adaptation curves but requires additional validation to ensure the comparison is fair across different performance baselines.
- **Medium Confidence**: The identification of two hyperparameter categories affecting adaptation (Mechanism 3) is supported by ablation studies, though the specific relative importance of each category needs further investigation.

## Next Checks

1. **Cross-Domain Validation**: Test the adaptation regret framework and findings on a different cooperative multi-agent environment (e.g., Overcooked or StarCraft II unit micromanagement) to assess generalizability beyond Hanabi.

2. **Cost-Benefit Analysis**: Measure the computational cost (wall-clock time, sample efficiency) of adaptation for each algorithm and compare it against the performance gains to determine practical utility in real-world scenarios.

3. **Diverse Partner Ablation**: Systematically vary partner pool diversity (D) while holding strength (S) constant to quantify the relationship between partner heterogeneity and adaptation difficulty, testing whether simple algorithms like IQL maintain their adaptability advantage across the full diversity spectrum.