---
ver: rpa2
title: Routing-Guided Learned Product Quantization for Graph-Based Approximate Nearest
  Neighbor Search
arxiv_id: '2311.18724'
source_url: https://arxiv.org/abs/2311.18724
tags:
- vector
- features
- search
- anns
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of memory efficiency in graph-based
  approximate nearest neighbor search (ANNS) for large-scale datasets. The authors
  propose a novel routing-guided learned product quantization (RPQ) method that considers
  both neighborhood and routing features of the proximity graph to generate high-quality
  quantized vectors.
---

# Routing-Guided Learned Product Quantization for Graph-Based Approximate Nearest Neighbor Search

## Quick Facts
- arXiv ID: 2311.18724
- Source URL: https://arxiv.org/abs/2311.18724
- Reference count: 40
- Key outcome: RPQ achieves 1.7×-4.2× improvement in QPS at the same recall@10 of 95% compared to existing methods

## Executive Summary
This paper addresses the memory efficiency challenge in graph-based approximate nearest neighbor search (ANNS) for large-scale datasets by proposing a novel routing-guided learned product quantization (RPQ) method. The key innovation is incorporating both neighborhood and routing features of the proximity graph into the quantization process, rather than treating vectors independently as traditional PQ methods do. The approach consists of a differentiable quantizer with adaptive vector decomposition, a sampling-based feature extractor using contrastive learning, and a multi-feature joint training module. Comprehensive experiments on datasets ranging from 1M to 1B vectors demonstrate RPQ's superiority, achieving significant improvements in queries per second while maintaining high recall rates.

## Method Summary
RPQ addresses memory efficiency in graph-based ANNS by learning quantization that preserves both neighborhood relationships and routing features from the proximity graph. The method uses a differentiable quantizer that employs adaptive vector decomposition via a learned rotation matrix and differentiable quantization using Gumbel-Softmax to enable end-to-end learning. A feature extractor then captures neighborhood relationships and routing features through contrastive learning and beam search analysis. Finally, a multi-feature joint training module optimizes the quantizer using both neighborhood and routing losses to ensure the quantized vectors maintain the essential properties needed for effective graph-based search.

## Key Results
- Achieves 1.7×-4.2× improvement in queries per second (QPS) at the same recall@10 of 95% compared to existing methods
- Demonstrates effectiveness across real-world datasets ranging from 1M to 1B vectors
- Shows significant memory efficiency gains while maintaining high search accuracy
- Provides source code and datasets for reproducibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive vector decomposition via space rotation enables better feature distribution across PQ sub-vectors
- Mechanism: A skew-symmetric matrix A is learned and converted to an orthonormal rotation matrix R = exp(A). This rotation balances the informativeness of dimensions across sub-vectors, preventing concentration of valuable features in few sub-vectors.
- Core assumption: The intrinsic feature value varies across dimensions and can be redistributed via linear transformation
- Evidence anchors: [section] "This inspires us to present an adaptive vector decomposer, to automatically determine which dimensions belong to each sub-vector by applying a square orthonormal matrix R for all vectors to make the valuable dimensions uniformly distributed among all sub-vectors"; [section] "Take an vector with 128 dimensions from Sift1M as an example... It is evident that the valuable dimensions are uniformly distributed among all sub-vectors"

### Mechanism 2
- Claim: Differentiable quantization via Gumbel-Softmax enables end-to-end learning of the quantizer
- Mechanism: Instead of non-differentiable argmin, the method computes codeword assignment probabilities and applies Gumbel-Softmax to generate approximate compact codes in continuous space, allowing gradient flow
- Core assumption: The Gumbel-Softmax approximation sufficiently preserves the discrete selection behavior while enabling differentiability
- Evidence anchors: [section] "To handle this problem, we present a differentiable approximation instead of the argmin function to encode a sub-vector as compact code. First, we compute a codeword assignment probability for each sub-vector, showing the probability of this sub-vector would be encoded by a specific codeword. Then, we use Gumbel-Softmax to compute an approximate compact code based on above assignment probabilities"; [section] "Since the codeword assignment probability and Gumbel-Softmax are differentiable, the whole quantization is also differentiable"

### Mechanism 3
- Claim: Multi-feature joint training with neighborhood and routing losses improves quantization quality for graph-based ANNS
- Mechanism: The method extracts neighborhood relationships (positive/negative samples from PG) and routing features (beam search decisions), then jointly optimizes the quantizer using both losses to preserve these features in the quantized space
- Core assumption: Both neighborhood proximity and routing decision quality are essential for effective graph-based ANNS
- Evidence anchors: [abstract] "Existing PQ methods do not consider the important routing features of PG, resulting in low-quality quantized vectors that affect the ANNS's effectiveness"; [section] "Theorem 1. Given a PG G(V, E) and a query x_q. Suppose the graph-based ANNS is now visiting a vertex v ∈ V, then the next-hop selection for ANNS at v depends on the distances among v's neighbors and the distances between all neighbors and x_q"

## Foundational Learning

- Concept: Product Quantization (PQ) basics
  - Why needed here: Understanding the standard PQ pipeline is essential to grasp how RPQ modifies it with differentiable components
  - Quick check question: What are the three main steps in traditional PQ, and how does RPQ modify each one?

- Concept: Graph-based ANNS routing mechanism
  - Why needed here: The routing features extracted from beam search are central to RPQ's design; understanding how next-hop decisions work is critical
  - Quick check question: How does the next-hop selection in beam search depend on both neighbor-to-neighbor distances and neighbor-to-query distances?

- Concept: Contrastive learning and hard negative sampling
  - Why needed here: RPQ uses contrastive learning to preserve neighborhood relationships; understanding positive/negative sample selection is key to the neighborhood feature extraction
  - Quick check question: What is the difference between kpos and kneg parameters, and how do they affect the sampling of positive and negative neighbors?

## Architecture Onboarding

- Component map: Differentiable quantizer (adaptive decomposition + differentiable quantization) → Feature extractor (neighborhood + routing features) → Multi-feature joint training (two losses)
- Critical path: Training pipeline: input vectors → adaptive decomposition (R matrix) → differentiable quantization (Gumbel-Softmax) → extract features → compute losses → update parameters (A, codebooks)
- Design tradeoffs: Adaptive decomposition adds computation overhead but improves feature balance; Gumbel-Softmax adds approximation error but enables end-to-end learning; joint training balances two feature types but requires careful loss weighting
- Failure signatures: Poor recall despite high QPS (routing features not preserved), slow convergence (adaptive decomposition not stabilizing), or high memory usage (suboptimal codebook size)
- First 3 experiments:
  1. Verify adaptive decomposition improves feature balance by comparing dimension variance before/after rotation
  2. Test differentiable quantization maintains quantization quality vs. standard PQ at different Gumbel-Softmax temperatures
  3. Validate joint training preserves both neighborhood and routing features by ablating each feature type and measuring impact on recall@10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we theoretically prove the convergence of the beam search algorithm when using RPQ in the graph-based ANNS framework?
- Basis in paper: [explicit] The paper discusses the beam search algorithm and its importance in the routing process, but does not provide a theoretical proof of convergence.
- Why unresolved: The paper focuses on the practical implementation and experimental evaluation of RPQ, leaving the theoretical analysis of the beam search algorithm's convergence as an open question.
- What evidence would resolve it: A mathematical proof demonstrating the convergence of the beam search algorithm when using RPQ in the graph-based ANNS framework.

### Open Question 2
- Question: How does the performance of RPQ compare to other quantization methods, such as binary hashing, when dealing with high-dimensional data in graph-based ANNS?
- Basis in paper: [inferred] The paper mentions binary hashing as a compression method but does not provide a direct comparison between RPQ and binary hashing in the context of high-dimensional data in graph-based ANNS.
- Why unresolved: The paper focuses on the performance of RPQ compared to other quantization methods but does not explicitly address the comparison with binary hashing in high-dimensional data scenarios.
- What evidence would resolve it: Experimental results comparing the performance of RPQ and binary hashing in graph-based ANNS with high-dimensional data.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the number of codewords (K) and the number of sub-vectors (M), affect the performance of RPQ in different types of datasets?
- Basis in paper: [explicit] The paper mentions the effect of K and M on the performance of RPQ, but does not provide a detailed analysis of their impact on different types of datasets.
- Why unresolved: The paper provides a general discussion of the effect of K and M on RPQ's performance but does not delve into the specifics of how these hyperparameters affect different types of datasets.
- What evidence would resolve it: A comprehensive study analyzing the impact of K and M on RPQ's performance across various types of datasets, such as text, image, and video data.

## Limitations

- The paper claims 1.7×-4.2× improvement in QPS, but the experimental methodology lacks detailed ablation studies on the individual contributions of each component
- While the theoretical analysis provides justification for the routing-guided approach, empirical validation of the routing feature preservation is limited
- The scalability analysis focuses on datasets up to 1B vectors, but the method's performance on truly massive datasets (10B+ vectors) remains untested

## Confidence

- **High Confidence**: The basic framework of combining product quantization with graph-based ANNS is well-established, and the differentiable quantization approach using Gumbel-Softmax is technically sound
- **Medium Confidence**: The claims about adaptive vector decomposition improving feature distribution are supported by qualitative examples but lack quantitative analysis of the distribution improvement
- **Medium Confidence**: The routing-guided feature extraction methodology is theoretically justified but the practical impact on real-world ANNS performance needs more extensive validation

## Next Checks

1. **Ablation Study**: Conduct systematic ablation experiments to quantify the individual contribution of each component (adaptive decomposition, differentiable quantization, multi-feature joint training) to the overall performance improvement
2. **Routing Feature Validation**: Design experiments specifically to measure how well the routing features are preserved in the quantized space, such as comparing next-hop selection accuracy before and after quantization
3. **Scalability Stress Test**: Test the method on larger-than-1B datasets (e.g., 10B vectors) to evaluate how the performance gains scale with dataset size and identify potential bottlenecks in the approach