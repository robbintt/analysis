---
ver: rpa2
title: 'Et Tu Certifications: Robustness Certificates Yield Better Adversarial Examples'
arxiv_id: '2302.04379'
source_url: https://arxiv.org/abs/2302.04379
tags:
- certi
- attack
- adversarial
- cation
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel attack framework called Certification
  Aware Attack (CAA) that exploits the guarantees provided by certification mechanisms
  to construct smaller adversarial perturbations against randomized smoothing-based
  defenses. The key idea is to leverage the certification radii as guidance for iteratively
  refining the adversarial perturbations while ensuring they remain outside the certified
  regions.
---

# Et Tu Certifications: Robustness Certificates Yield Better Adversarial Examples

## Quick Facts
- arXiv ID: 2302.04379
- Source URL: https://arxiv.org/abs/2302.04379
- Reference count: 38
- Key outcome: CAA achieves 10%+ median perturbation norm reduction vs. baselines while requiring 90% less computation

## Executive Summary
This paper introduces the Certification Aware Attack (CAA), a novel adversarial attack framework that exploits the guarantees provided by randomized smoothing certification mechanisms to construct smaller, more efficient adversarial perturbations. By leveraging certification radii as lower bounds on where adversarial examples must exist, CAA iteratively refines attacks outside certified regions while minimizing perturbation norms. The approach demonstrates significant improvements over baseline attacks like PGD, AutoAttack, and Carlini-Wagner across MNIST, CIFAR-10, and Imagenet datasets, achieving both smaller perturbations and faster computation times.

## Method Summary
The Certification Aware Attack (CAA) modifies traditional gradient-based attacks by incorporating certification radii from randomized smoothing to guide the attack trajectory. The algorithm uses Gumbel-Softmax to handle non-differentiable argmax layers in certified models, allowing gradient-based optimization. CAA iteratively steps outside certification regions using adaptive step sizes controlled by parameters δ1 and δ2, then refines the perturbation norm after finding an adversarial example. The method requires training ResNet-18 models with randomized smoothing noise levels σ ∈ {0.5, 1.0}, implementing the attack algorithm with certification-aware step control, and comparing performance against baseline attacks on multiple datasets.

## Key Results
- Median L2 perturbation norm reduction of over 10% compared to PGD, AutoAttack, and Carlini-Wagner
- 90% reduction in computational time on average compared to baseline attacks
- Successful attacks across MNIST, CIFAR-10, and Imagenet datasets with ResNet-18 models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Certification radii act as lower bounds on where adversarial perturbations must exist, allowing iterative refinement outside certified regions.
- Mechanism: The attack exploits the fact that certified radii guarantee no adversarial examples exist within them, enabling efficient search outside these regions.
- Core assumption: Certification radii accurately contain no adversarial examples.
- Evidence anchors: [abstract] "guarantees provided by certification mechanisms to construct smaller adversarial perturbations"; [section] "nature of certifications to improve efficacy of constructing adversarial attacks"
- Break condition: Overestimated certification radii or gaps in certification guarantees would cause failure.

### Mechanism 2
- Claim: Tracking certification radii throughout the attack allows minimization of perturbation norms after finding adversarial examples.
- Mechanism: Once an adversarial example is found, certification information from all previous steps guides refinement to find the smallest perturbation maintaining the adversarial property.
- Core assumption: Certification radii remain valid throughout the attack trajectory.
- Evidence anchors: [abstract] "smaller adversarial perturbations against randomized smoothing-based defenses"; [section] "certifications associated with successful attacks can be exploited to minimise the perturbation norm"
- Break condition: Significant changes in certification radii during attack trajectory or getting stuck in local minima.

### Mechanism 3
- Claim: Gumbel-Softmax approximation enables gradient-based attacks on non-differentiable argmax layers in certified models.
- Mechanism: Replacing argmax with differentiable Gumbel-Softmax allows gradient computation through the model while maintaining discrete class predictions.
- Core assumption: Gumbel-Softmax provides good argmax approximation while maintaining differentiability.
- Evidence anchors: [section] "limitation can be circumvented by any attacker with sufficient access, by replacing argmax layers with the Gumbel Softmax"
- Break condition: Poor temperature parameter choice leading to bad gradient approximations or numerical instability.

## Foundational Learning

- Concept: Randomized smoothing and its certification mechanism
  - Why needed here: The entire attack framework exploits certification guarantees from randomized smoothing, making this fundamental to understanding the approach
  - Quick check question: How does randomized smoothing provide probabilistic certification guarantees, and what assumptions underlie these guarantees?

- Concept: Gradient-based optimization and iterative attacks
  - Why needed here: The attack uses iterative gradient-based optimization to find adversarial examples, building on techniques like PGD and C&W attacks
  - Quick check question: How do gradient-based attacks iteratively construct adversarial examples, and what role does the step size play in their effectiveness?

- Concept: Norm minimization and adversarial perturbation detection
  - Why needed here: The attack aims to minimize perturbation norms to create harder-to-detect adversarial examples, so understanding norm-based metrics is crucial
  - Quick check question: Why are smaller perturbation norms generally more dangerous in adversarial attacks, and how does this relate to human perception and automated detection?

## Architecture Onboarding

- Component map: Sample → Compute certification → Iterative attack with step control → Find adversarial example → Refine perturbation → Output result
- Critical path: Sample → Compute certification → Iterative attack with step control → Find adversarial example → Refine perturbation → Output result
- Design tradeoffs: Memory vs. accuracy (storing intermediate samples for refinement), computational time vs. perturbation size (more iterations allow smaller perturbations), step size flexibility vs. convergence stability
- Failure signatures: Failure to find adversarial examples despite sufficient iterations, unusually large perturbation sizes, computational timeouts, numerical instability in gradient calculations
- First 3 experiments:
  1. Baseline test: Run attack on simple model with known certification radii to verify basic functionality
  2. Parameter sensitivity: Test how different δ1, δ2, ϵmin, ϵmax values affect success rate and perturbation size
  3. Comparative analysis: Measure performance against PGD baseline on same model and dataset to establish improvement metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would attack performance change if step size parameters δ1 and δ2 were optimized beyond default values of 1.05 and 0.95?
- Basis in paper: [explicit] Authors mention potential for additional scope for optimizing δ1 and δ2
- Why unresolved: Only tested with reasonable starting point, no systematic parameter space exploration
- What evidence would resolve it: Comprehensive experiments varying δ1 and δ2 over wider range, measuring attack success rate, perturbation size, and computational time

### Open Question 2
- Question: How robust is CAA to inaccuracies in estimating noise level σ when attacker lacks exact knowledge?
- Basis in paper: [explicit] Paper discusses attack still produces smaller perturbations when overestimating σ by 50%
- Why unresolved: Only tested limited range of σ estimation errors (50% overestimation)
- What evidence would resolve it: Experiments systematically varying accuracy of σ estimation from large underestimation to large overestimation

### Open Question 3
- Question: How would attack performance change when applied to certification mechanisms other than randomized smoothing?
- Basis in paper: [explicit] Mentions core concepts should be readily extensible to other certification mechanisms like IBP and convex relaxation
- Why unresolved: Focused on randomized smoothing for scalability reasons, leaving other mechanisms unexplored
- What evidence would resolve it: Implementation and testing on IBP and convex relaxation certified models, measuring attack success rate and perturbation size

## Limitations

- The mechanism for how certification information enables smaller perturbations remains theoretical rather than rigorously proven
- Experiments focus on white-box scenarios where certification information is fully accessible, leaving black-box performance unexplored
- Use of Gumbel-Softmax introduces approximation errors that aren't fully characterized

## Confidence

- **High confidence**: Empirical results showing CAA's performance improvements (10%+ perturbation reduction, 90% time savings) are well-supported by experiments across three datasets and two certification levels
- **Medium confidence**: Theoretical framework connecting certification radii to adversarial attack efficiency is plausible but lacks formal proof
- **Low confidence**: Claim that releasing certification information inherently compromises security is overstated - attack still requires significant computational resources and model access

## Next Checks

1. **Certification radius accuracy validation**: Measure actual gap between predicted and true certification radii on held-out validation set, quantifying how often attack steps outside valid regions

2. **Black-box adaptation test**: Implement black-box version of CAA that estimates certification radii through sampling rather than direct access, measuring performance degradation compared to white-box version

3. **Transferability analysis**: Test whether CAA-generated adversarial examples transfer across models with different random seeds or architectures, assessing practical utility beyond specific models used in training