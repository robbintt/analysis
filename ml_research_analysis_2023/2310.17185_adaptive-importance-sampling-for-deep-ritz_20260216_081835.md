---
ver: rpa2
title: Adaptive importance sampling for Deep Ritz
arxiv_id: '2310.17185'
source_url: https://arxiv.org/abs/2310.17185
tags:
- deep
- training
- sampling
- ritz
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an adaptive sampling method for the Deep
  Ritz method to solve partial differential equations (PDEs). The key innovation is
  using two deep neural networks: one to approximate the PDE solution and another
  (bounded KRnet) to generate new collocation points for training set refinement.'
---

# Adaptive importance sampling for Deep Ritz

## Quick Facts
- arXiv ID: 2310.17185
- Source URL: https://arxiv.org/abs/2310.17185
- Reference count: 16
- Primary result: Adaptive importance sampling with bounded KRnet significantly improves Deep Ritz method accuracy, achieving smaller relative L2 errors than uniform sampling, especially for problems with low regularity and high dimensionality

## Executive Summary
This paper introduces an adaptive sampling method for the Deep Ritz method to solve partial differential equations (PDEs). The key innovation is using two deep neural networks: one to approximate the PDE solution and another (bounded KRnet) to generate new collocation points for training set refinement. The method treats the integrand in the variational loss as an unnormalized PDF and approximates it using bounded KRnet, enabling importance sampling to improve accuracy. Numerical experiments on 2D peak problems, problems with singularities, and a 10D Poisson equation demonstrate that the adaptive method outperforms the original Deep Ritz method with uniform sampling, achieving smaller relative L2 errors.

## Method Summary
The method uses alternating optimization between a PDE solution network and a bounded KRnet generative model. First, a ResNet-like network with sin3(x) activation approximates the PDE solution by minimizing the variational loss discretized with collocation points. Then, bounded KRnet learns the optimal sampling density as an approximation of the absolute value of the integrand in the variational loss. New collocation points are generated from this learned density using importance sampling. The process iterates, alternating between updating the PDE solution and refining the sampling density. The paper also presents two alternative PDF models that combine bounded KRnet with uniform distributions to improve robustness against extreme importance weights.

## Key Results
- Adaptive sampling with bounded KRnet achieves 2-3× lower relative L2 errors than uniform sampling on 2D peak problems
- The method successfully handles 10D Poisson equations where uniform sampling struggles
- Mixture density models (combining bounded KRnet and uniform distributions) provide more stable training than pure bounded KRnet sampling
- Problems with singularities show the largest improvement, with adaptive sampling concentrating points near singular regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Importance sampling reduces statistical error in Deep Ritz method by concentrating training points in high-contribution regions.
- **Mechanism:** The method approximates the optimal sampling density p*(x) = |G(u(x,θ))| / μ, where G is the integrand in the variational loss. By generating training points from this density using bounded KRnet, more samples are drawn from regions where the integrand has large values, reducing variance in the Monte Carlo estimate.
- **Core assumption:** The integrand |G(u(x,θ))| can be well-approximated by a deep generative model (bounded KRnet) on the compact domain.
- **Evidence anchors:**
  - [abstract]: "We treat the integrand in the variational loss as an unnormalized probability density function (PDF) and approximate it using a deep generative model called bounded KRnet."
  - [section 3.1]: "From the previous section, we know that p* = |G(u(x,θ))|/μ for a fixed parameter θ is theoretically the optimal density function for implementing importance sampling in the Deep Ritz method."
  - [corpus]: Weak evidence - no direct citations found for bounded KRnet in PDE sampling context.
- **Break condition:** If bounded KRnet fails to approximate the integrand accurately, especially in high dimensions, the importance sampling benefit is lost and variance may increase.

### Mechanism 2
- **Claim:** Alternating optimization between PDE solution and sampling density improves both approximation accuracy and sampling efficiency.
- **Mechanism:** The algorithm alternates between (1) solving the PDE using Deep Ritz with current sampling density, and (2) updating bounded KRnet to approximate |G(u(x,θ))| from the latest PDE solution. This iterative refinement progressively focuses sampling on important regions.
- **Core assumption:** The PDE solution and optimal sampling density are mutually dependent in a way that allows iterative improvement.
- **Evidence anchors:**
  - [abstract]: "The approximate PDE solution is attained by minimizing the discretized loss functional... Once we have computed θk+1, we can update the bounded KRnet..."
  - [section 3.3.1]: "Let SkΩ = {xΩ,i}Nv i=1 and Sk∂Ω = {x∂Ω,j}Nb i=1 be two sets of collocation points at the kth adaptivity iteration... In the (k + 1)th adaptivity iteration, we first solve the following optimization problem from the Deep Ritz method"
  - [corpus]: Weak evidence - no direct citations found for alternating optimization in this specific context.
- **Break condition:** If the PDE solution changes significantly between iterations, the bounded KRnet trained on the previous solution becomes outdated, causing sampling inefficiency.

### Mechanism 3
- **Claim:** Mixture density models (combining bounded KRnet and uniform distributions) provide robustness against extreme importance weights.
- **Mechanism:** The method defines pmixture(x,θ) = ε·pbKRnet(x,θ) + (1-ε)·puniform(x), ensuring the sampling density never becomes too small in any region, preventing exploding importance weights that could destabilize training.
- **Core assumption:** Some uniform sampling is necessary to maintain coverage of the entire domain and avoid pathological importance ratios.
- **Evidence anchors:**
  - [section 3.3.2]: "We build a mixture model by combining the PDF induced by bounded KRnet and the PDF of the uniform distribution on the domain."
  - [section 3.3.2]: "Noting that pmixture(x,θk+1f) ≥ 1−ε/|Ω|, we then avoid the problem that the ratio G(u(x,θk+1f))/pmixture(x,θk+1f) becomes too large."
  - [corpus]: Weak evidence - no direct citations found for mixture models in this specific context.
- **Break condition:** If ε is too small, the method reverts to near-uniform sampling, losing the variance reduction benefit; if ε is too large, the method may become unstable due to extreme importance weights.

## Foundational Learning

- **Concept:** Importance sampling in Monte Carlo integration
  - Why needed here: The paper relies on reducing variance in the Monte Carlo estimate of the variational loss by sampling from a non-uniform distribution.
  - Quick check question: Why is p*(x) = |G(u(x,θ))| / μ considered the optimal sampling density for importance sampling?

- **Concept:** Kullback-Leibler (KL) divergence and its minimization for density approximation
  - Why needed here: Bounded KRnet is trained by minimizing KL divergence between the target density and the model density.
  - Quick check question: What does minimizing KL divergence from p to pbKRnet achieve in terms of density approximation?

- **Concept:** Normalizing flows and invertible transport maps
  - Why needed here: Bounded KRnet is a type of normalizing flow that learns an invertible map from a simple prior distribution to the target density.
  - Quick check question: How does the change of variables formula pX(x) = pZ(z)|det ∇xf| enable explicit density computation in normalizing flows?

## Architecture Onboarding

- **Component map:** Deep Ritz Network -> Bounded KRnet -> Sampling Generator -> Collocation Points -> Loss Estimator
- **Critical path:** Generate samples → Train Deep Ritz → Compute |G(u)| → Train KRnet → Generate new samples → Repeat
- **Design tradeoffs:**
  - Adaptive sampling vs. uniform sampling: Adaptive provides better variance reduction but requires training an additional network (KRnet)
  - Complete replacement vs. incremental update of training set: Complete replacement is simpler but can cause error oscillations; incremental update is more stable but requires more complex bookkeeping
  - Single KRnet vs. mixture models: Single KRnet is more focused but can have extreme importance weights; mixture models are more robust but less efficient
- **Failure signatures:**
  - Training divergence or instability: Likely due to extreme importance weights from poorly approximated sampling density
  - No improvement in L2 error over iterations: Indicates bounded KRnet is not capturing the integrand structure correctly
  - Oscillating L2 error: Suggests complete replacement of training set is too aggressive; consider incremental updates
- **First 3 experiments:**
  1. Implement the basic adaptive algorithm (Algorithm 1) on a simple 1D Poisson problem with known solution to verify the importance sampling mechanism works
  2. Compare L2 error convergence between uniform sampling and adaptive sampling on a 2D peak problem to demonstrate variance reduction
  3. Test the mixture density model (Algorithm 2) on a problem with sharp gradients to verify robustness against extreme importance weights

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the adaptive sampling strategy for Deep Ritz be modified to incrementally add training points rather than replacing the entire training set at each iteration?
- Basis in paper: [inferred] The paper notes that the current adaptive sampling strategy requires replacing the entire training set at each iteration, leading to non-monotonic decreases in relative L2 errors. It mentions that gradually adding training points is effective in PINNs and suggests developing a suitable adding points strategy for Deep Ritz.
- Why unresolved: The paper does not provide a concrete solution for how to incrementally add points in the Deep Ritz context, as this would require careful consideration of the importance sampling framework and the variational loss structure.
- What evidence would resolve it: A proposed algorithm or method that demonstrates how to selectively add points to the training set at each iteration while maintaining the integrity of the importance sampling framework in Deep Ritz.

### Open Question 2
- Question: Can other variance reduction techniques, such as control variates, be effectively integrated into the Deep Ritz method to further improve the approximation of the variational loss?
- Basis in paper: [explicit] The paper mentions that other variance reduction techniques such as control variates can be taken into consideration to further improve the approximation of the variational loss.
- Why unresolved: The paper does not explore or implement these additional variance reduction techniques, leaving their potential effectiveness and implementation details unknown.
- What evidence would resolve it: Numerical experiments comparing the performance of Deep Ritz with and without the integration of control variates or other variance reduction techniques, demonstrating improved accuracy or efficiency.

### Open Question 3
- Question: How does the performance of the adaptive sampling method scale with increasing dimensionality, and are there any limitations or challenges that arise in very high-dimensional problems?
- Basis in paper: [inferred] The paper demonstrates the method on a 10D Poisson problem but does not extensively explore its performance in even higher dimensions. It notes that the method is particularly effective for problems characterized by low regularity and high dimensionality, but the scalability is not thoroughly investigated.
- Why unresolved: The paper only provides results for a 10D problem, which may not be sufficient to understand the method's behavior in much higher dimensions or to identify potential scalability issues.
- What evidence would resolve it: Results from applying the method to problems with dimensions significantly higher than 10, along with an analysis of computational cost, accuracy, and any observed limitations or challenges.

## Limitations
- Weak empirical evidence for bounded KRnet's ability to accurately approximate high-dimensional densities
- Computational overhead of training bounded KRnet at each iteration not thoroughly analyzed
- Alternating optimization scheme lacks theoretical analysis of convergence properties
- Limited comparison with other variance reduction techniques for Deep Ritz method

## Confidence

- **High confidence:** The theoretical framework for importance sampling in the Deep Ritz method (sections 3.1-3.2) is well-established and correctly applied.
- **Medium confidence:** The numerical experiments demonstrating improved L2 errors for 2D problems are convincing, but the 10D Poisson experiment has limited comparison points.
- **Low confidence:** The claims about bounded KRnet's ability to accurately approximate high-dimensional densities and the stability of the alternating optimization scheme are not sufficiently supported by theoretical or empirical evidence.

## Next Checks

1. **Convergence analysis:** Conduct a systematic study of the alternating optimization scheme's convergence behavior on a 1D problem with known solution, tracking both PDE solution error and sampling density approximation error across iterations.

2. **Computational overhead quantification:** Measure wall-clock time per iteration for the adaptive method versus uniform sampling, including all KRnet training time, to determine if the L2 error improvements justify the additional computational cost.

3. **Dimensionality scaling study:** Test the method on a series of problems with increasing dimensionality (2D, 5D, 10D, 20D) to empirically validate the claims about high-dimensional performance and identify at what dimensionality the method becomes ineffective.