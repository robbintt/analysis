---
ver: rpa2
title: 'RepQ: Generalizing Quantization-Aware Training for Re-Parametrized Architectures'
arxiv_id: '2311.05317'
source_url: https://arxiv.org/abs/2311.05317
tags:
- quantization
- repq
- re-parametrization
- training
- re-parametrized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RepQ enables quantization-aware training for re-parametrized neural
  networks by treating the inference-stage weights as differentiable functions of
  the re-parametrized block's trainable parameters. It applies pseudo-quantization
  on top of these functions, allowing end-to-end training while preserving the benefits
  of re-parametrization.
---

# RepQ: Generalizing Quantization-Aware Training for Re-Parametrized Architectures

## Quick Facts
- arXiv ID: 2311.05317
- Source URL: https://arxiv.org/abs/2311.05317
- Reference count: 40
- Primary result: Achieves lossless 8-bit quantization for re-parametrized networks with up to 1.5% accuracy improvement over baselines

## Executive Summary
RepQ is a method that generalizes quantization-aware training (QAT) to re-parametrized neural networks by treating inference-stage weights as differentiable functions of the re-parametrized block's trainable parameters. The approach applies pseudo-quantization on top of these merged functions, enabling end-to-end training while preserving the benefits of re-parametrization. Experiments on ImageNet classification and super-resolution tasks demonstrate that RepQ achieves lossless 8-bit quantization and outperforms prior methods including QARepVGG on models such as RepVGG, AC-ResNet-18, OREPA-ResNet-18, and ECBSR.

## Method Summary
RepQ enables QAT for re-parametrized architectures by computing a differentiable weight function M that maps the block's trainable parameters to the weight of the final converted convolution. During training, this function is merged and pseudo-quantization is applied on top, allowing gradients to flow through the entire system. For blocks containing batch normalization, RepQ-BN fuses BN into preceding convolutions using folding equations, while RepQ-BNEst further improves efficiency by estimating BN statistics without computing the full convolution. The method is evaluated on ImageNet classification and super-resolution tasks with 8-bit quantization.

## Key Results
- Achieves lossless 8-bit quantization on RepVGG, AC-ResNet-18, OREPA-ResNet-18, and ECBSR architectures
- Outperforms QARepVGG baseline with up to 1.5% accuracy improvement on classification tasks
- Maintains computational efficiency through BN estimation techniques that reduce overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RepQ enables quantization-aware training by treating inference-stage weights as differentiable functions of re-parametrized block parameters
- Mechanism: The method merges re-parametrized blocks into a single convolution (M(W₁,...,Wₙ)) during training, then applies pseudo-quantization on top of this merged function, enabling end-to-end QAT without breaking the gradient flow
- Core assumption: The merged weight function M is differentiable with respect to the re-parametrized block parameters
- Evidence anchors:
  - [abstract] "Our method is based on the insight that the test stage weights of an arbitrary re-parametrized layer can be presented as a differentiable function of trainable parameters"
  - [section 4.1] "M is a differentiable function that maps the block's trainable parameters to the weight of the final converted convolution"
- Break condition: If the re-parametrization scheme makes M non-differentiable or if numerical instability occurs during M computation

### Mechanism 2
- Claim: Batch normalization in re-parametrized blocks is handled through differentiable merging and statistics estimation
- Mechanism: RepQ-BN fuses BN into preceding convolutions using folding equations that preserve differentiability. RepQ-BNEst further estimates BN statistics without computing the full convolution, reducing computational overhead
- Core assumption: BN folding maintains numerical equivalence and differentiability
- Evidence anchors:
  - [section 4.2] "We show how to compute differentiable weight functions via BN-folding"
  - [section 4.3] "we propose to approximate the covariance matrix with a diagonal form"
- Break condition: If BN estimation introduces significant bias or if the diagonal covariance approximation fails for certain weight distributions

### Mechanism 3
- Claim: RepQ achieves lossless 8-bit quantization by preserving the performance benefits of re-parametrization
- Mechanism: By applying quantization after merging re-parametrized blocks rather than before, RepQ maintains the representational efficiency of re-parametrization while achieving the compression benefits of quantization
- Core assumption: Merging before quantization prevents the bit-width explosion that occurs when quantizing individual layers
- Evidence anchors:
  - [section 3] "two sequential convolutions of bit-width two will merge into a single convolution of bit-width four"
  - [section 5.2] "Results show that RepQ surpasses existing solutions and provides lossless 8-bit quantization"
- Break condition: If the merged convolution becomes too complex for practical 8-bit representation or if the quantization error accumulates significantly

## Foundational Learning

- Concept: Re-parametrization
  - Why needed here: Understanding how re-parametrized blocks can be mathematically equivalent to single convolutions is crucial for grasping RepQ's core insight
  - Quick check question: Why does replacing a single convolution with a re-parametrized block (multiple convolutions + BN) improve training performance?

- Concept: Quantization-aware training
  - Why needed here: RepQ builds on QAT concepts but modifies them for re-parametrized architectures; understanding standard QAT is essential for seeing the innovation
  - Quick check question: How does the straight-through estimator enable backpropagation through the non-differentiable quantization operation?

- Concept: Batch normalization folding
  - Why needed here: BN is often part of re-parametrized blocks, and RepQ-BN's approach to folding BN is critical for making QAT work with these architectures
  - Quick check question: What mathematical transformation allows BN to be folded into a preceding convolution?

## Architecture Onboarding

- Component map: RepVGG block -> M(W₁,W₂,W₃) -> pseudo-quantization Q -> loss -> backpropagation -> parameter update
- Critical path: Training loop → M computation → pseudo-quantization → loss → backpropagation → parameter update
- Design tradeoffs:
  - RepQ vs. quantizing individual layers: avoids bit-width explosion but requires differentiable M
  - RepQ-BN vs. RepQ-BNEst: numerical accuracy vs. computational efficiency
  - 8-bit vs. 4-bit: accuracy vs. deployment efficiency
- Failure signatures:
  - NaN or Inf values during training (likely from numerical instability in M or BN estimation)
  - Significant accuracy drop after quantization (indicates suboptimal M computation or BN handling)
  - Training slowdown (could indicate inefficient BN estimation or excessive memory usage)
- First 3 experiments:
  1. Implement RepQ on a simple re-parametrized block (e.g., ACNet) without BN to verify the core mechanism
  2. Add BN folding (RepQ-BN) to verify handling of batch normalization in re-parametrized blocks
  3. Implement BN estimation (RepQ-BNEst) and compare training time and accuracy against RepQ-BN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RepQ be extended to support low-bit quantization (1-2 bits) for re-parametrized networks without significant accuracy degradation?
- Basis in paper: [explicit] The paper mentions that binary and lower bit quantization is much more challenging than four or eight-bit quantization, and experiments were primarily focused on 4-bit and 8-bit quantization.
- Why unresolved: The paper does not provide results for 1-2 bit quantization, and the authors acknowledge the difficulty of achieving accurate low-bit quantization for re-parametrized models.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of RepQ on 1-2 bit quantization for various re-parametrized architectures, with accuracy comparable to or exceeding existing methods.

### Open Question 2
- Question: How does RepQ perform on other computer vision tasks beyond classification and super-resolution, such as object detection or segmentation?
- Basis in paper: [inferred] The paper primarily focuses on classification and super-resolution tasks, with limited discussion on other applications.
- Why unresolved: The paper does not provide experimental results for other computer vision tasks, and the authors do not discuss the potential benefits or challenges of applying RepQ to these tasks.
- What evidence would resolve it: Experimental results showing the effectiveness of RepQ on object detection or segmentation tasks, with performance improvements compared to existing quantization methods.

### Open Question 3
- Question: Can RepQ be combined with other compression techniques, such as pruning or knowledge distillation, to further improve the efficiency of re-parametrized networks?
- Basis in paper: [inferred] The paper focuses on quantization-aware training for re-parametrized networks and does not discuss the potential benefits of combining RepQ with other compression techniques.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the combination of RepQ with other compression techniques, and the authors do not discuss the potential synergies or challenges.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of combining RepQ with pruning or knowledge distillation, with improved efficiency and accuracy compared to using RepQ alone.

## Limitations

- Architecture-specific M computation formulas not fully specified across all re-parametrization schemes
- Limited evaluation to classification and super-resolution tasks, with unclear generalization to other computer vision domains
- No experiments or analysis of combining RepQ with other compression techniques like pruning or knowledge distillation

## Confidence

- **High confidence**: The core insight that inference weights can be treated as differentiable functions of re-parametrized parameters (Mechanism 1)
- **Medium confidence**: The BN folding and estimation mechanisms (RepQ-BN and RepQ-BNEst)
- **Low confidence**: The claim of "lossless" 8-bit quantization across all experiments

## Next Checks

1. **Gradient flow verification**: Implement a simple re-parametrized block (e.g., ACNet without BN) and verify that gradients flow correctly through the merged weight function M during backpropagation. Check for any numerical instabilities or NaN/Inf values.

2. **BN estimation accuracy**: Compare RepQ-BNEst against RepQ-BN on a small model to measure the trade-off between computational efficiency and accuracy. Validate that the diagonal covariance approximation (Eq. 6) doesn't introduce significant bias.

3. **Architecture-specific M computation**: Implement RepQ for two different re-parametrized architectures (e.g., RepVGG and AC-ResNet-18) and verify that the merged weight computation M works correctly for both. Document any architecture-specific adjustments needed.