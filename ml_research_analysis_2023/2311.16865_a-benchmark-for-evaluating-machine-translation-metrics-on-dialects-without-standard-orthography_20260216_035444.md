---
ver: rpa2
title: A Benchmark for Evaluating Machine Translation Metrics on Dialects Without
  Standard Orthography
arxiv_id: '2311.16865'
source_url: https://arxiv.org/abs/2311.16865
tags:
- metrics
- language
- translation
- machine
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the robustness of machine translation metrics
  for non-standardized dialects lacking standard orthography, focusing on Swiss German.
  The authors collect a new dataset with human translations and judgments for English-to-Swiss
  German MT outputs and create a challenge set for dialect variation.
---

# A Benchmark for Evaluating Machine Translation Metrics on Dialects Without Standard Orthography

## Quick Facts
- **arXiv ID**: 2311.16865
- **Source URL**: https://arxiv.org/abs/2311.16865
- **Reference count**: 40
- **Primary result**: Existing MT metrics are unreliable on Swiss German due to lack of standardized orthography, but continued pre-training on Swiss German data significantly improves robustness

## Executive Summary
This paper investigates the robustness of machine translation metrics for non-standardized dialects without standard orthography, using Swiss German as a case study. The authors create a new benchmark dataset with human translations and judgments for English-to-Swiss German machine translation outputs, along with a challenge set for dialect variation. They evaluate existing metrics (BLEU, chrF++, COMET) and find they perform poorly, especially at the segment level. To improve robustness, they experiment with continued pre-training on Swiss German data and character-level noise injection during metric training. Results show that continued pre-training significantly improves performance, though metrics still struggle with the challenge set, indicating substantial room for improvement.

## Method Summary
The authors collected human translations and judgments for English-to-Swiss German MT outputs, creating a benchmark dataset from the NTREX-128 data. They also constructed a challenge set of Swiss German sentences with different spellings and meanings. Using the COMET framework with XLM-R encoder, they trained metrics with various configurations: baseline, continued pre-training on Swiss German data, and character-level noise injection during fine-tuning. The metrics were evaluated on system-level and segment-level correlations (Pearson, Kendall, pairwise accuracy) and a challenge set success rate. They compared reference-based metrics against reference-free alternatives and tested performance across multiple language pairs from the WMT 2022 metrics task.

## Key Results
- Existing metrics (BLEU, chrF++, COMET-20) perform poorly on Swiss German, especially at segment level
- Continued pre-training on Swiss German data significantly improves metric performance
- Character-level noise injection during fine-tuning provides some improvement but is less effective than continued pre-training
- Reference-based metrics outperform reference-free metrics on dialects, suggesting reference acts as semantic anchor
- Metrics still struggle with the challenge set, indicating substantial room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pre-training on Swiss German data improves metric robustness by exposing the language model to dialectal spelling variations during the embedding learning phase.
- Mechanism: The XLM-R encoder is fine-tuned on Swiss German text, allowing it to build more flexible representations that can handle orthographic variability rather than relying on fixed subword tokenization learned from standardized languages.
- Core assumption: The encoder's representations are the primary determinant of metric performance, and character-level spelling differences can be captured by adjusting the underlying language model.
- Evidence anchors:
  - [abstract]: "continued pre-training on Swiss German data significantly improves performance"
  - [section]: "continued pre-training on Swiss German data generally outperforms noise injection during task fine-tuning"
  - [corpus]: Weak - corpus shows related work on indigenous language evaluation but not direct evidence for pre-training mechanism

### Mechanism 2
- Claim: Character-level noise injection during metric fine-tuning improves robustness by forcing the metric to rely less on exact subword matches and more on semantic similarity.
- Mechanism: During COMET fine-tuning, 15% of tokens have one character altered, deleted, or added, creating training examples that simulate spelling variations and teaching the model to focus on semantic content rather than surface form.
- Core assumption: The metric's evaluation capability is primarily determined during the fine-tuning phase rather than the pre-training phase.
- Evidence anchors:
  - [abstract]: "experiment with continued pre-training on Swiss German data and character-level noise injection during metric training"
  - [section]: "we inject character-level noise into a random selection of 15% of the tokens within each sentence"
  - [corpus]: Weak - corpus mentions dialectal text normalization but not specific evidence for noise injection benefits

### Mechanism 3
- Claim: Reference-based metrics outperform reference-free metrics on dialects without standardized orthography because the reference acts as an anchor for semantic meaning despite spelling variations.
- Mechanism: The reference translation provides a stable semantic target that helps the metric evaluate hypotheses even when they contain spelling variations, whereas reference-free metrics must infer semantic similarity from source and hypothesis alone.
- Core assumption: Having access to a reference translation during evaluation provides meaningful information that helps disambiguate spelling variations.
- Evidence anchors:
  - [abstract]: "reference-based learned metrics still rely too much on subword overlap with the reference"
  - [section]: "both existing reference-free metrics perform worse than the existing reference-based metrics in the segment-level evaluations"
  - [corpus]: Weak - corpus shows dialect identification work but not direct evidence for reference-based advantages

## Foundational Learning

- **Concept**: Subword tokenization and its limitations for non-standardized languages
  - Why needed here: Understanding why standard tokenization schemes fail on dialects without standardized orthography is crucial for designing robust metrics
  - Quick check question: How does subword tokenization handle words that appear in multiple dialectal spellings but have the same meaning?

- **Concept**: Neural metric architecture (COMET framework)
  - Why needed here: The specific architecture determines how well the metric can handle spelling variations and whether modifications like pre-training or noise injection will be effective
  - Quick check question: What components of the COMET architecture are most sensitive to spelling variations in the input?

- **Concept**: Evaluation correlation metrics (Pearson, Kendall, pairwise accuracy)
  - Why needed here: Understanding these metrics is essential for interpreting the experimental results and determining whether proposed improvements actually make metrics more robust
  - Quick check question: How does pairwise accuracy differ from system-level Pearson correlation in evaluating metric performance on dialects?

## Architecture Onboarding

- **Component map**: XLM-R encoder → COMET framework → fine-tuning on human judgments → evaluation on Swiss German data
- **Critical path**: Pre-training → continued pre-training (optional) → noise injection (optional) → fine-tuning → evaluation
- **Design tradeoffs**: Pre-training on Swiss German improves dialect robustness but may reduce performance on other languages; noise injection is easier but less effective than pre-training
- **Failure signatures**: Poor pairwise accuracy indicates metrics cannot distinguish between semantic and spelling differences; low success rate on challenge set shows inability to handle orthographic variability
- **First 3 experiments**:
  1. Run existing metrics (BLEU, chrF++, COMET-20) on the Swiss German dataset to establish baseline performance
  2. Implement continued pre-training on Swiss German data and evaluate on the same dataset
  3. Add character-level noise injection to the fine-tuning process and compare results with pre-training-only approach

## Open Questions the Paper Calls Out

- **Open Question 1**: How can neural evaluation metrics be designed to better model character-level similarities in non-standardized dialects?
  - Basis in paper: [explicit] The authors suggest that character-based language models, such as Canine, could provide a better basis for neural evaluation metrics to model character-level similarities, as segments in dialects often resemble references in certain characters only rather than in full words.
  - Why unresolved: While the paper proposes this as a potential direction, it does not provide concrete evidence or experiments to demonstrate the effectiveness of character-based language models in improving metric performance on dialects without standardized orthography.
  - What evidence would resolve it: Experiments comparing the performance of neural evaluation metrics based on character-level language models (e.g., Canine) versus those based on subword-level models (e.g., XLM-R) on dialects without standardized orthography would provide evidence to support or refute this claim.

- **Open Question 2**: How do the proposed adaptations (continued pre-training on Swiss German data and character-level noise injection) generalize to other languages and language varieties without standardized orthography?
  - Basis in paper: [inferred] The authors evaluate their proposed adaptations on two Swiss German dialects and a subset of language pairs from the WMT 2022 metrics task. However, they acknowledge that evaluating metrics on varieties from different languages would help generalize their results.
  - Why unresolved: The paper's findings are limited to Swiss German and a few other language pairs, and it remains unclear how well the proposed adaptations would perform on other languages and language varieties without standardized orthography.
  - What evidence would resolve it: Evaluating the proposed adaptations on a diverse set of languages and language varieties without standardized orthography would provide evidence of their generalizability and effectiveness.

- **Open Question 3**: How can the benchmark be expanded to include more language varieties and machine translation systems for a more comprehensive evaluation?
  - Basis in paper: [explicit] The authors acknowledge the limitation of only including two Swiss German dialects in their benchmark due to the lack of different machine translation systems for other languages. They express the desire to include further language varieties in the future to encourage research toward metrics that are reliable for many non-standardized language varieties.
  - Why unresolved: The current benchmark is limited in scope, and expanding it to include more language varieties and machine translation systems would require significant effort and resources.
  - What evidence would resolve it: Successfully expanding the benchmark to include a diverse set of language varieties and machine translation systems, and demonstrating improved metric performance and generalizability across these languages, would provide evidence of the effectiveness of the proposed adaptations and the value of a more comprehensive benchmark.

## Limitations

- The evaluation is limited to a single dialect (Swiss German) and a relatively small dataset, which may not generalize to other non-standardized dialects or languages
- The study focuses primarily on XLM-R-based metrics and only three existing metrics, potentially missing other metric architectures that may perform differently
- The SwissCrawl corpus used for continued pre-training may contain noise or inconsistencies in dialectal representation

## Confidence

- **High Confidence**: The finding that existing metrics perform poorly on Swiss German at the segment level is well-supported by experimental evidence
- **Medium Confidence**: The claim that continued pre-training is more effective than noise injection for improving dialect robustness is supported by results but requires additional validation
- **Low Confidence**: The generalizability of findings to other non-standardized dialects remains uncertain due to the single-dialect focus

## Next Checks

1. **Cross-dialect validation**: Evaluate the same metrics and training approaches on another non-standardized dialect (e.g., Bavarian German or Scots) to test generalizability of the findings about continued pre-training effectiveness versus noise injection

2. **Ablation study on noise injection parameters**: Systematically vary the noise injection rate (e.g., 5%, 10%, 20%, 30%) and type of noise (character insertion, deletion, substitution) to determine optimal configurations and understand which noise patterns best simulate real dialectal variation

3. **Reference-free metric enhancement**: Design and test reference-free metric variants that incorporate explicit mechanisms for handling spelling variations, such as character n-gram similarity or phonological feature matching, to determine if reference-free metrics can be made competitive without reference access