---
ver: rpa2
title: 'TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation
  in Traditional Chinese Medicine'
arxiv_id: '2311.01786'
source_url: https://arxiv.org/abs/2311.01786
tags:
- language
- domain
- corpus
- pre-training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  (LLMs) to the specialized domain of Traditional Chinese Medicine (TCM), where general
  models often underperform due to a lack of domain-specific knowledge. The authors
  propose TCMDA, an efficient domain adaptation method that involves constructing
  a TCM-specific corpus (TCM-Corpus-1B) by extracting domain-relevant text from general
  sources and leveraging the LoRA technique to efficiently adapt the BLOOM-7B model
  to TCM tasks.
---

# TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2311.01786
- Source URL: https://arxiv.org/abs/2311.01786
- Reference count: 24
- Key outcome: TCM-GPT-7B achieves 17% and 12% relative accuracy gains on TCM examination and diagnosis tasks over baseline models

## Executive Summary
This paper addresses the challenge of adapting large language models to the specialized domain of Traditional Chinese Medicine (TCM), where general models often underperform due to a lack of domain-specific knowledge. The authors propose TCMDA, an efficient domain adaptation method that involves constructing a TCM-specific corpus (TCM-Corpus-1B) by extracting domain-relevant text from general sources and leveraging the LoRA technique to efficiently adapt the BLOOM-7B model to TCM tasks. Experimental results demonstrate that TCM-GPT-7B, the resulting model, achieves state-of-the-art performance on two TCM tasks—TCM examination and TCM diagnosis—outperforming baseline models by relative accuracy increments of 17% and 12%, respectively. The authors also release the TCM-Corpus-1B and TCM-GPT-7B model to support further interdisciplinary research in TCM and NLP.

## Method Summary
The paper proposes TCMDA, a domain adaptation framework that constructs a TCM-specific corpus by extracting relevant text from a general corpus using keyword-weighted BM25 retrieval, then adapts a pretrained BLOOM-7B model using LoRA (Low-Rank Adaptation). The process involves keyword extraction via TextRank, corpus retrieval with weighted queries, and efficient parameter-efficient fine-tuning. The adapted model is evaluated on two TCM-specific tasks: TCM examination (multiple-choice questions) and TCM diagnosis (electronic health records).

## Key Results
- TCM-GPT-7B achieves 17% relative accuracy improvement over baselines on TCM examination tasks
- TCM-GPT-7B achieves 12% relative accuracy improvement over baselines on TCM diagnosis tasks
- Domain-specific pretraining outperforms random corpus pretraining by 7.4% and 2.3% on the two tasks respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining with LoRA improves TCM task performance over generic LLM baselines.
- Mechanism: Extracting domain keywords and retrieving TCM-relevant text from a large general corpus (TCM-Corpus-1B) provides task-specific training signals. LoRA allows efficient adaptation by freezing BLOOM-7B weights and training only low-rank decomposition matrices, reducing trainable parameters while preserving domain alignment.
- Core assumption: TCM domain knowledge is sufficiently represented in the retrieved corpus and can be effectively transferred via LoRA fine-tuning.
- Evidence anchors:
  - [abstract] TCM-GPT-7B achieves 17% and 12% relative accuracy gains over baselines on TCM examination and diagnosis tasks.
  - [section II.C] LoRA methodology: freeze pretrained weights, add trainable rank decomposition matrices B and A, rank r≪min(d1,d2).
  - [corpus] Weak corpus evidence: neighbor papers focus on TCM LLMs but lack citation/impact data; FMR scores suggest topical relevance but no direct evidence of corpus adequacy.
- Break condition: If retrieved corpus lacks sufficient TCM-specific terminology coverage or contains noisy/irrelevant text, domain adaptation will fail to improve performance.

### Mechanism 2
- Claim: Keyword-weighted BM25 retrieval improves the relevance of extracted TCM corpus.
- Mechanism: Using TextRank-extracted keywords plus domain lexicon, each keyword is weighted by occurrence count (wi = 1 + log(ni)). BM25 scores documents using modified query Q′ where high-weight keywords are repeated to boost their influence. This yields more TCM-relevant passages for pretraining.
- Core assumption: Keyword occurrence frequency correlates with domain relevance and improves BM25 retrieval precision.
- Evidence anchors:
  - [section II.A] Keyword extraction: TextRank algorithm, weighted by occurrence counts.
  - [section II.B] BM25 retrieval: weights applied by repeating keywords min(⌊wi⌋,3) times.
  - [corpus] Weak evidence: neighbor corpus studies mention TCM data but do not validate keyword weighting effectiveness.
- Break condition: If keyword extraction misses key TCM concepts or if BM25 weights overfit to corpus idiosyncrasies, retrieval quality will degrade.

### Mechanism 3
- Claim: Pretraining on domain corpus before fine-tuning yields greater gains than random corpus pretraining.
- Mechanism: TCM-specific corpus pretraining aligns BLOOM-7B parameters to TCM semantics before supervised fine-tuning on TCM tasks. Random corpus pretraining offers some benefit by exposing model to Chinese text but is less effective than domain-specific pretraining.
- Core assumption: Domain-specific pretraining provides more relevant semantic representations than generic pretraining for downstream TCM tasks.
- Evidence anchors:
  - [section IV.A] Table I, II: TCMDA (domain-specific pretraining) outperforms Random (random corpus pretraining) by 17.4% and 12.3% on TCM-EXAM and TCM-EHR.
  - [section IV.A] Random shows 10.5% improvement over baseline, TCMDA shows larger gains.
  - [corpus] No direct corpus evidence; relies on experimental results.
- Break condition: If domain corpus is too small or noisy, pretraining gains vanish and random pretraining may match or exceed domain-specific pretraining.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Enables efficient domain adaptation without full fine-tuning, reducing trainable parameters and computational cost while preserving BLOOM-7B base knowledge.
  - Quick check question: What matrices are introduced in LoRA, and how do they relate to the original weight matrix W?

- Concept: BM25 retrieval algorithm
  - Why needed here: Efficiently retrieves domain-relevant passages from large general corpus using term frequency and inverse document frequency weighting.
  - Quick check question: How does repeating high-weight keywords in the query Q′ influence BM25 scoring?

- Concept: TextRank keyword extraction
  - Why needed here: Automates extraction of domain-relevant keywords from task data, feeding into retrieval and corpus construction.
  - Quick check question: How is the weight wi calculated from keyword occurrence count ni?

## Architecture Onboarding

- Component map: General corpus (Baidu Baike, Wikipedia) → Data cleaning pipeline → Keyword extraction (TextRank + TCM lexicon) → Weighted keyword set K_D → BM25 retrieval → TCM-Corpus-1B → BLOOM-7B (frozen) → LoRA adaptation (rank=8, scaling=32, dropout=0.1) → TCM-GPT-7B → TCM-EXAM and TCM-EHR datasets → Accuracy metrics

- Critical path: Keyword extraction → Corpus retrieval → LoRA pretraining → Fine-tuning → Evaluation

- Design tradeoffs:
  - LoRA rank r: lower r → fewer trainable params but potential underfitting; higher r → more capacity but more compute.
  - Corpus size: larger corpus → more domain coverage but higher pretraining cost and risk of noise.
  - Keyword weighting: higher weights → better relevance but risk of overfitting to frequent terms.

- Failure signatures:
  - No accuracy gain vs baseline → corpus retrieval ineffective or LoRA rank too low.
  - Overfitting on training tasks → dropout too low or LoRA rank too high.
  - Slow convergence → learning rate too low or batch size too small.

- First 3 experiments:
  1. Vary LoRA rank r (4, 8, 16) and measure accuracy/parameter count tradeoff.
  2. Test corpus size scaling (0.2B, 0.5B, 1B tokens) and monitor performance trends.
  3. Replace BM25 with simpler TF-IDF retrieval to evaluate keyword weighting impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal corpus size for TCM-GPT adaptation, and does the relationship between corpus size and performance follow a saturation curve?
- Basis in paper: [inferred] The paper shows performance improvements with increasing corpus size up to 1 billion tokens, but doesn't explore whether further increases would yield diminishing returns or if there's an optimal point.
- Why unresolved: The experiments only tested up to 1 billion tokens, leaving open questions about scalability and efficiency at larger scales.
- What evidence would resolve it: A comprehensive study testing corpus sizes ranging from 0.5 to 10 billion tokens with performance measurements at each step would establish the relationship and identify potential saturation points.

### Open Question 2
- Question: How does the TCM-GPT model's performance compare to task-specific smaller models that are trained from scratch on TCM data?
- Basis in paper: [explicit] The paper focuses on adapting a 7B parameter model but doesn't compare against smaller, task-specific alternatives that might be more efficient.
- Why unresolved: The computational efficiency and practical deployment considerations aren't fully explored, particularly for resource-constrained settings.
- What evidence would resolve it: Direct comparison experiments between TCM-GPT and smaller models (1B-3B parameters) trained specifically on TCM tasks, measuring both performance and computational requirements.

### Open Question 3
- Question: What is the generalization capability of TCM-GPT across different regional variations of TCM practice and terminology?
- Basis in paper: [inferred] The paper doesn't address whether the model can handle variations in TCM terminology and practice across different regions or schools of thought.
- Why unresolved: TCM has regional variations and different schools of thought that may use different terminology or approaches, which could affect the model's effectiveness in diverse clinical settings.
- What evidence would resolve it: Testing the model on datasets from different regional TCM traditions and comparing performance across these variations would establish its generalization capabilities.

## Limitations
- TCM-Corpus-1B construction lacks detailed validation of keyword extraction quality and corpus relevance scoring
- Study does not provide ablation studies on LoRA rank selection or comprehensive error analysis on failure cases
- TCM-EXAM and TCM-EHR datasets are proprietary, limiting reproducibility and independent verification

## Confidence
- High confidence: The core LoRA adaptation mechanism and its implementation details are well-specified and technically sound.
- Medium confidence: The reported accuracy improvements (17% and 12%) are based on controlled experiments, but dataset accessibility limits independent verification.
- Low confidence: Claims about keyword-weighted BM25 retrieval effectiveness and corpus quality are weakly supported by direct evidence.

## Next Checks
1. Conduct ablation studies varying LoRA rank (r=4, 8, 16) to quantify the tradeoff between parameter efficiency and task performance.
2. Implement an independent keyword extraction pipeline using alternative methods (e.g., TF-IDF, RAKE) to test robustness of corpus construction.
3. Perform error analysis on TCM-GPT-7B predictions to identify systematic failure modes and assess whether performance gains generalize across TCM subdomains.