---
ver: rpa2
title: 'GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak
  Prompts'
arxiv_id: '2309.10253'
source_url: https://arxiv.org/abs/2309.10253
tags:
- jailbreak
- templates
- template
- arxiv
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GPTF UZZER, a black-box fuzzing framework
  that automates the generation of jailbreak prompts to red-team LLMs. It leverages
  human-written templates as seeds, mutates them using LLM-assisted operators, and
  employs a fine-tuned RoBERTa model to judge jailbreak success.
---

# GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts

## Quick Facts
- arXiv ID: 2309.10253
- Source URL: https://arxiv.org/abs/2309.10253
- Reference count: 40
- Primary result: GPTF UZZER achieves over 90% attack success rates against ChatGPT and Llama-2 using auto-generated jailbreak prompts, even with suboptimal initial seeds

## Executive Summary
GPTF UZZER introduces a black-box fuzzing framework that automates the generation of jailbreak prompts to red-team large language models (LLMs). The system leverages human-written templates as seeds, applies LLM-assisted mutation operators, and uses a fine-tuned RoBERTa model to judge jailbreak success. Experiments demonstrate GPTF UZZER's effectiveness in bypassing LLM safety measures, achieving over 90% success rates against ChatGPT and Llama-2, and showing strong transfer performance across multiple models including Bard (61%), Claude-2 (91%), and PaLM2 (96%).

## Method Summary
GPTF UZZER follows a fuzzing-inspired iterative process where human-written jailbreak templates serve as seeds, which are then mutated using five specialized LLM-assisted operators (generate, crossover, expand, shorten, rephrase). A Monte Carlo Tree Search-based seed selection strategy (MCTS-Explore) balances exploration and exploitation to identify promising templates. The framework employs a fine-tuned RoBERTa model to evaluate the success of each jailbreak attempt, retaining successful templates for further mutation. The system operates under a query budget constraint and can target multiple models simultaneously to generate universal jailbreak templates.

## Key Results
- Achieves over 90% attack success rates against ChatGPT and Llama-2
- Demonstrates strong transfer performance: 61% against Bard, 91% against Claude-2, 96% against PaLM2
- Maintains effectiveness even when initialized with failed human-written prompts
- Outperforms human-crafted templates and baseline attack methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPTF UZZER can automatically generate jailbreak prompts that successfully bypass LLM safety measures, even starting from ineffective human-written templates.
- Mechanism: The system uses a fuzzing-inspired iterative process where human-written jailbreak templates serve as seeds, which are then mutated using LLM-assisted operators to create new templates. A fine-tuned RoBERTa model evaluates the success of each jailbreak attempt, retaining successful templates for further mutation.
- Core assumption: LLM mutation operators can produce semantically meaningful variations of jailbreak templates that maintain or enhance their ability to bypass safety measures.
- Evidence anchors:
  - [abstract]: "GPTF UZZER automates the generation of jailbreak templates for red-teaming LLMs... Our results indicate that GPTF UZZER consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates."
  - [section]: "We introduce five specialized mutation operators... By leveraging the stochastic nature of LLMs and sampling the output, as opposed to deterministically selecting the most probable token, we can obtain varied results."
- Break condition: The mechanism breaks if mutation operators produce semantically invalid prompts that cannot effectively jailbreak LLMs.

### Mechanism 2
- Claim: The seed selection strategy using MCTS-Explore effectively balances exploration and exploitation to identify the most promising templates for mutation.
- Mechanism: The MCTS-Explore algorithm extends traditional MCTS by incorporating a probability parameter that allows non-leaf nodes to be selected as seeds, and includes reward penalties to prevent over-concentration on specific lineages.
- Core assumption: The MCTS-Explore algorithm can effectively navigate the search space of jailbreak templates, identifying seeds that, when mutated, produce successful jailbreak prompts while maintaining diversity.
- Evidence anchors:
  - [section]: "We propose a novel seed selection strategy, MCTS-Explore to balance the efficiency and diversity of the seed selection... This strategy leverages the Monte Carlo Tree Search (MCTS) algorithm for seed selection."
  - [abstract]: "Our framework consistently achieves impressive attack success rates... even when initialized with failed human-written prompts, our method still manages to achieve an attack success rate of over 90%."
- Break condition: The mechanism fails if the algorithm converges prematurely on local optima, missing potentially more effective seeds.

### Mechanism 3
- Claim: GPTF UZZER generates universal jailbreak templates that can effectively target multiple LLMs with diverse architectures and training data.
- Mechanism: By fuzzing across multiple models simultaneously and aggregating scores based on success across different models, GPTF UZZER identifies templates that generalize well.
- Core assumption: LLMs share common vulnerabilities that can be exploited by well-crafted jailbreak prompts.
- Evidence anchors:
  - [abstract]: "In terms of transfer attacks, our generated prompts demonstrate the capability to target unseen LLMs with a variety of harmful questions, proving very high attack success rate against popular LLMs such as Bard (61%), Claude-2 (91%), and PaLM2 (96%)."
  - [section]: "We now transition to a more challenging setting, aiming to evaluate the template's capability to jailbreak multiple questions across different models with diverse training data and architectures."
- Break condition: The mechanism breaks if each LLM has fundamentally different safety mechanisms that prevent universal templates from working.

## Foundational Learning

- Concept: Fuzzing and mutation testing
  - Why needed here: GPTF UZZER is inspired by AFL fuzzing framework, using similar principles of seed selection, mutation, and evaluation to discover vulnerabilities in LLMs.
  - Quick check question: How does the mutation process in GPTF UZZER differ from traditional binary fuzzing, and why are LLM-assisted mutation operators necessary for this application?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding RLHF is crucial because GPTF UZZER specifically targets models that have undergone safety fine-tuning through RLHF, identifying weaknesses in this alignment process.
  - Quick check question: What are the key differences between models fine-tuned with RLHF versus those without, and how does this affect their vulnerability to jailbreak attacks?

- Concept: Multi-armed bandit problems and Monte Carlo Tree Search
  - Why needed here: The seed selection strategy uses MCTS-Explore, which is based on bandit algorithms and tree search methods for balancing exploration and exploitation.
  - Quick check question: How does the UCB (Upper Confidence Bound) algorithm relate to seed selection in fuzzing, and what advantages does MCTS-Explore offer over traditional UCB-based approaches?

## Architecture Onboarding

- Component map:
  Initial seed collection -> Seed selection module (MCTS-Explore) -> Mutation operators -> Query execution module -> Judgment model -> Seed pool management -> Query budget controller

- Critical path:
  1. Select seed template using MCTS-Explore
  2. Apply mutation operator using LLM assistance
  3. Combine mutated template with target question
  4. Query target LLM and receive response
  5. Classify response using judgment model
  6. Update seed pool based on classification
  7. Repeat until query budget exhausted

- Design tradeoffs:
  - Using ChatGPT as mutation model vs. open-source alternatives: Higher quality mutations but potential rate limiting and cost
  - Fine-tuned RoBERTa vs. API-based judgment: Better accuracy and speed but requires labeled training data
  - Multi-model vs. single-model fuzzing: Better generalization but potentially lower per-model effectiveness
  - Query budget allocation: More queries enable better exploration but increase computational cost

- Failure signatures:
  - Judgment model consistently misclassifies jailbroken responses as rejected (false negatives)
  - Mutation operators produce semantically invalid or repetitive templates
  - Seed selection gets stuck in local optima, repeatedly selecting similar ineffective seeds
  - LLM APIs rate limit or block queries, halting the fuzzing process
  - Generated templates become too long for model input limits

- First 3 experiments:
  1. Test single-question jailbreak effectiveness using top-5 seed selection on a model known to be vulnerable (e.g., Vicuna-7B)
  2. Evaluate multi-model jailbreak success rate using the invalid seed filter strategy to test GPTF UZZER's ability to work with suboptimal initial seeds
  3. Compare judgment model accuracy against baseline methods (rule matching, API moderation, ChatGPT/GPT-4 assistance) on a held-out validation set

## Open Questions the Paper Calls Out

- Question: Can GPTFUZZER maintain high attack success rates against future versions of commercial LLMs that may have improved safety measures?
  - Basis in paper: [inferred] The paper shows GPTFUZZER's effectiveness against current versions of commercial LLMs, but the authors note that LLM robustness is improving over time.
  - Why unresolved: The paper only tests against current LLM versions, and future versions may have enhanced safety features that could impact GPTFUZZER's performance.
  - What evidence would resolve it: Testing GPTFUZZER against newer versions of commercial LLMs as they are released and comparing attack success rates.

- Question: How does GPTFUZZER's performance compare to other automated jailbreak generation methods, such as those using reinforcement learning or genetic algorithms?
  - Basis in paper: [explicit] The paper introduces GPTFUZZER as a novel method but does not compare it to other automated approaches.
  - Why unresolved: The paper focuses on comparing GPTFUZZER to human-crafted templates and some baseline methods, but not to other automated techniques.
  - What evidence would resolve it: Conducting experiments comparing GPTFUZZER's performance to other automated jailbreak generation methods on the same set of LLMs and questions.

- Question: Can GPTFUZZER be adapted to generate jailbreak prompts for other types of language models, such as those used for code generation or scientific writing?
  - Basis in paper: [inferred] The paper focuses on LLMs for general text generation, but does not explore GPTFUZZER's applicability to other domains.
  - Why unresolved: The paper does not investigate whether GPTFUZZER's techniques can be extended to other types of language models beyond general text generation.
  - What evidence would resolve it: Testing GPTFUZZER on code generation and scientific writing models to see if it can generate effective jailbreak prompts for these specific domains.

## Limitations

- The framework's reliance on human-written templates as seeds may bias results toward human-comprehensible attack patterns, potentially missing more sophisticated jailbreak strategies.
- The judgment model, while achieving high accuracy, is trained on ChatGPT responses and may not generalize well to models with different response patterns or safety mechanisms.
- The evaluation focuses primarily on English-language attacks, leaving open questions about performance across different languages and cultural contexts.

## Confidence

- High confidence: Framework's ability to automate jailbreak template generation and achieve high attack success rates against RLHF-fine-tuned models like ChatGPT and Llama-2.
- Medium confidence: Transferability claims across diverse model architectures (Bard, Claude-2, PaLM2) due to partially unexplained underlying mechanisms.
- Medium confidence: Universal template generation capability, as evaluation could benefit from more diverse question types and longer-term stability testing.

## Next Checks

1. **Cross-lingual robustness test**: Evaluate GPTF UZZER's performance on jailbreak prompts and harmful questions in languages other than English to assess generalizability across linguistic contexts.

2. **Model update sensitivity analysis**: Test the same jailbreak templates against different versions of target models (e.g., ChatGPT-3.5 vs ChatGPT-4) to measure template stability and identify which safety mechanisms are most vulnerable to transfer attacks.

3. **Human evaluation of judgment model**: Conduct blinded human assessments of the fine-tuned RoBERTa judgment model's classification accuracy on a diverse set of jailbroken and rejected responses to validate the model's reliability beyond automated metrics.