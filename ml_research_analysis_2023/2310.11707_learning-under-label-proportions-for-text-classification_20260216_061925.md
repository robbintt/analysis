---
ver: rpa2
title: Learning under Label Proportions for Text Classification
arxiv_id: '2310.11707'
source_url: https://arxiv.org/abs/2310.11707
tags:
- learning
- loss
- formulation
- dllp
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses learning from label proportions (LLP) in text
  classification, a privacy-centric and weakly supervision setting where data is provided
  as bags with only class proportion labels. The paper identifies limitations in the
  widely used DLLP method, including unbounded loss, lack of robustness, and non-smooth
  gradients.
---

# Learning under Label Proportions for Text Classification

## Quick Facts
- arXiv ID: 2310.11707
- Source URL: https://arxiv.org/abs/2310.11707
- Reference count: 34
- Primary result: Novel LLP method achieves up to 40% improvement in weighted precision over baselines across 4 transformer models and 5 datasets

## Executive Summary
This paper addresses the Learning from Label Proportions (LLP) problem in text classification, where models train on bags of instances with only aggregate class proportion labels rather than individual labels. The authors identify critical limitations in the widely-used DLLP method, including unbounded loss, lack of robustness, and non-smooth gradients. They propose a novel LαT V ⋆ loss based on total variation distance that is Lipschitz continuous and bounded, combined with a self-supervised contrastive loss for better representation learning. Experiments across 4 transformer architectures (BERT, RoBERTa, Longformer, DistilBERT) and 5 datasets show the proposed method outperforms baselines in 87% of configurations, with improvements up to 40% in weighted precision.

## Method Summary
The method trains transformer models on bags of text instances where only bag-level class proportions are available as supervision. For each bag, instance-level predictions are aggregated to compute bag-level predictions, which are compared to true proportions using the proposed LαT V ⋆ loss (a parametrized total variation distance). A self-supervised contrastive loss is added to improve representation learning. The combined loss is minimized during fine-tuning, with hyperparameters tuned based on training+validation loss aggregation. Evaluation uses weighted precision, recall, and F1 on instance-level predictions.

## Key Results
- Proposed method outperforms DLLP and LLPSelfCLR baselines in 87% of experimental configurations
- Up to 40% improvement in weighted precision achieved across datasets
- Theoretical generalization bound provided for binary classifiers under LLP setup
- Ablation studies confirm benefits of both the new loss and self-supervised objective

## Why This Works (Mechanism)

### Mechanism 1
The LαT V ⋆ loss is robust to outliers and bounded, unlike the KL divergence loss used in DLLP. LαT V ⋆ is based on total variation distance, which is Lipschitz continuous and has an absolute upper bound for α ≥ 1. This prevents unbounded loss values that can occur with KL divergence when predicted proportions approach zero. Core assumption: The total variation distance provides a meaningful lower bound to KL divergence while being more stable for optimization.

### Mechanism 2
The self-supervised contrastive loss improves representation learning, leading to better generalization. The contrastive loss encourages embeddings of instances within the same bag to be similar while pushing apart embeddings from different bags, creating better feature representations for the classifier. Core assumption: Better representations lead to improved instance-level predictions even when training with only bag-level supervision.

### Mechanism 3
The LLP setup is theoretically learnable with generalization bounds. The paper provides a VC-dimension-based generalization bound for binary classifiers under the LLP setup, showing that performance on bag-level aggregates can bound instance-level performance. Core assumption: The relationship between bag-level and instance-level performance can be characterized mathematically.

## Foundational Learning

- Concept: Label Proportions vs Instance Labels
  - Why needed here: LLP provides only aggregate class proportions per bag, not individual labels, making it a weak supervision problem.
  - Quick check question: If a bag has 10 instances with 3 positive and 7 negative, what are the label proportions?

- Concept: Lipschitz Continuity and Smoothness
  - Why needed here: The proposed loss is Lipschitz continuous and smooth, which are desirable properties for stable optimization of deep networks.
  - Quick check question: What does it mean for a function to be Lipschitz continuous, and why is this important for training neural networks?

- Concept: Total Variation Distance
  - Why needed here: TVD is used as the basis for the new loss function because it's robust and has desirable mathematical properties compared to KL divergence.
  - Quick check question: How does total variation distance differ from KL divergence, and why might it be preferred in some settings?

## Architecture Onboarding

- Component map:
  Text encoder (BERT/RoBERTa/Longformer/DistilBERT) -> Classification head -> Loss computation module (LαT V ⋆ + contrastive loss) -> Bag construction module

- Critical path:
  1. Input text → tokenizer → model
  2. Model outputs class probabilities for each instance
  3. Aggregate probabilities to get bag-level predictions
  4. Compute LαT V ⋆ loss against true proportions
  5. Compute contrastive loss on embeddings
  6. Backpropagate combined loss

- Design tradeoffs:
  - Bag size: Larger bags provide more aggregate supervision but less instance-level signal
  - α parameter: Controls robustness vs sensitivity of the TVD-based loss
  - Contrastive loss weight: Balances representation learning vs proportion prediction

- Failure signatures:
  - NaN loss values: Likely due to improper handling of edge cases in proportion calculation
  - Poor convergence: May indicate learning rate issues or need to adjust α/λ hyperparameters
  - Overfitting to bag-level: Model performs well on aggregate metrics but poorly on instance-level

- First 3 experiments:
  1. Verify basic functionality: Train on small dataset with known proportions, check if loss decreases
  2. Ablation study: Compare performance with and without contrastive loss component
  3. Sensitivity analysis: Vary bag sizes and observe impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method vary when applied to datasets with a larger number of classes beyond the binary and three-class datasets explored in the paper?
- Basis in paper: The authors mention that extending the theoretical analysis to multi-class settings is feasible but more sophisticated, and they conduct experiments on a three-class dataset (Financial Phrasebank) but do not explore datasets with more classes.
- Why unresolved: The current experimental setup is limited to binary and three-class datasets, leaving the performance on datasets with more classes unexplored.
- What evidence would resolve it: Conducting experiments on datasets with a higher number of classes and comparing the performance of the proposed method against the baselines would provide evidence on its effectiveness in multi-class settings.

### Open Question 2
- Question: What is the impact of using different self-supervised contrastive loss functions instead of the one used in the paper (Nandy et al., 2022)?
- Basis in paper: The authors mention that they use the self-supervised contrastive loss proposed by Nandy et al. (2022) but do not explore the use of other self-supervised contrastive loss functions.
- Why unresolved: The choice of self-supervised contrastive loss function can significantly impact the performance of the proposed method, and the paper does not provide insights into the effects of using alternative loss functions.
- What evidence would resolve it: Conducting experiments using different self-supervised contrastive loss functions and comparing their performance against the proposed method would provide evidence on the impact of the choice of loss function.

### Open Question 3
- Question: How does the proposed method perform when applied to datasets with varying levels of label imbalance?
- Basis in paper: The authors mention that they use weighted versions of precision, recall, and F1-score to evaluate the performance of the models, but they do not explore the performance of the proposed method on datasets with different levels of label imbalance.
- Why unresolved: The performance of the proposed method on datasets with label imbalance is not explored, leaving the question of its effectiveness in such scenarios unanswered.
- What evidence would resolve it: Conducting experiments on datasets with varying levels of label imbalance and comparing the performance of the proposed method against the baselines would provide evidence on its effectiveness in handling imbalanced datasets.

## Limitations
- Theoretical generalization bound may not fully capture the complexity of deep learning models in practice
- Assumes bag-level labels are accurate, which may not hold in real-world scenarios with noisy aggregate labels
- Evaluation focuses on instance-level performance but doesn't extensively explore scenarios where bag-level prediction is the primary goal

## Confidence
- Theoretical generalization bound: Medium
- Empirical results demonstrating improvements: High
- Ablation studies confirming contributions of LαT V ⋆ and contrastive loss: High for LαT V ⋆, Medium for contrastive component

## Next Checks
1. Test the method's robustness to noisy bag-level proportions by systematically corrupting the ρ values and measuring performance degradation.
2. Extend the theoretical analysis to multi-class settings and verify if the generalization bounds scale appropriately.
3. Conduct a more thorough hyperparameter sensitivity analysis, particularly for the α parameter in LαT V ⋆ and the λ weight for the contrastive loss, across different bag sizes and dataset characteristics.