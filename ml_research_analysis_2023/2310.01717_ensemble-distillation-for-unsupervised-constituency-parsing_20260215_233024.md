---
ver: rpa2
title: Ensemble Distillation for Unsupervised Constituency Parsing
arxiv_id: '2310.01717'
source_url: https://arxiv.org/abs/2310.01717
tags:
- ensemble
- unsupervised
- teachers
- parsing
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel ensemble method for unsupervised
  constituency parsing that leverages the diverse expertise of existing parsers. The
  authors propose a "tree averaging" approach, using a CYK-like algorithm to find
  the optimal parse tree most similar to all teacher models' outputs.
---

# Ensemble Distillation for Unsupervised Constituency Parsing

## Quick Facts
- arXiv ID: 2310.01717
- Source URL: https://arxiv.org/abs/2310.01717
- Reference count: 40
- Primary result: Achieves 72.8 F1 on Penn Treebank, state-of-the-art for unsupervised parsing

## Executive Summary
This paper introduces a novel ensemble method for unsupervised constituency parsing that leverages the diverse expertise of existing parsers. The authors propose a "tree averaging" approach using a CYK-like algorithm to find the optimal parse tree most similar to all teacher models' outputs. They further distill this ensemble knowledge into a student model to improve inference efficiency and mitigate the over-smoothing problem in multi-teacher distillation. Experiments on Penn Treebank and SUSANNE datasets show their method consistently outperforms previous approaches, achieving a new state-of-the-art F1 score of 72.8 on Penn Treebank.

## Method Summary
The method consists of two stages: ensemble construction and knowledge distillation. First, multiple heterogeneous unsupervised parsers (Compound PCFG, DIORA, S-DIORA, ConTest, ContexDistort, Neural PCFG, Ordered Neurons) generate parse trees for each sentence. A CYK-like dynamic programming algorithm then computes the optimal ensemble tree by maximizing constituent hit counts across teachers. This ensemble tree serves as pseudo-groundtruth for training an RNNG student model, which is further refined with URNNG. The ensemble-then-distill approach addresses over-smoothing issues that arise from directly distilling from multiple heterogeneous teachers.

## Key Results
- Achieves 72.8 F1 score on Penn Treebank, surpassing previous state-of-the-art by 2.2 points
- Consistently outperforms all individual teachers across multiple runs and different ensemble components
- Demonstrates robustness under domain shift conditions and on SUSANNE dataset
- Ensemble-then-distill approach outperforms union distillation by 4.7 F1 points

## Why This Works (Mechanism)

### Mechanism 1
Different unsupervised parsers capture different aspects of syntactic structure, making their ensemble outputs more robust than any single model. Low correlation among parsers (Table 1) indicates complementary strengths; ensemble via tree averaging aggregates these strengths while reducing individual model noise. Core assumption: Parser diversity reflects genuine capture of different linguistic phenomena rather than random variation. Evidence anchors: Weak - only one direct paper on ensemble-based parsing but it focuses on discontinuous parsing.

### Mechanism 2
The CYK-like dynamic programming algorithm efficiently finds the globally optimal ensemble parse tree by maximizing constituent hit counts across teachers. Recursive computation of hit counts for all possible binary tree structures using dynamic programming ensures exact solution rather than greedy or approximate selection. Core assumption: Binary tree space is tractable for dynamic programming search; optimal substructure property holds for parse tree aggregation. Evidence anchors: Weak - no direct corpus evidence of CYK variants for ensemble parsing; general parsing literature supports efficiency claims.

### Mechanism 3
Ensemble-then-distill approach mitigates over-smoothing that occurs when directly distilling from multiple heterogeneous teachers. First synthesizing a single "best" tree through ensemble eliminates confusion from multiple outputs, then distillation proceeds normally without entropy inflation. Core assumption: Over-smoothing occurs because student models spread probability mass too thinly across all teacher outputs when teachers disagree. Evidence anchors: Weak - general multi-teacher distillation literature mentions over-smoothing but no specific evidence for constituency parsing.

## Foundational Learning

- Concept: Constituency parsing fundamentals (binary trees, unlabeled constituents, F1 evaluation)
  - Why needed here: The entire paper builds on understanding how hierarchical syntactic structures are represented and evaluated
  - Quick check question: What distinguishes a binary unlabeled parse tree from a labeled one in terms of evaluation metrics?

- Concept: Dynamic programming and CYK algorithm principles
  - Why needed here: The core ensemble mechanism relies on efficient tree search through dynamic programming
  - Quick check question: How does the optimal substructure property enable efficient parsing tree search?

- Concept: Knowledge distillation mechanics (cross-entropy loss, student-teacher relationships)
  - Why needed here: The ensemble distillation stage requires understanding how student models learn from teacher outputs
  - Quick check question: Why does cross-entropy loss lead to over-smoothing when applied to multiple heterogeneous teachers?

## Architecture Onboarding

- Component map: Teacher models (Compound PCFG, DIORA, S-DIORA, ConTest, ContexDistort, Neural PCFG, Ordered Neurons) -> CYK-like dynamic programming algorithm -> RNNG student model -> URNNG refinement

- Critical path: 1) Parse input sentences with all teacher models 2) Compute constituent hit counts across teachers 3) Run CYK algorithm to find optimal ensemble tree 4) Train RNNG on ensemble outputs 5) Apply URNNG refinement 6) Evaluate F1 score

- Design tradeoffs: Ensemble vs. single model (higher accuracy but slower inference without distillation), CYK vs. approximate methods (exact solution vs. computational efficiency for larger sentences), RNNG vs. other student architectures (proven track record vs. potentially faster alternatives)

- Failure signatures: Teachers producing highly correlated outputs (minimal ensemble benefit), extremely long sentences (CKY algorithm may become computationally prohibitive), teachers with vastly different performance levels (student confusion during distillation)

- First 3 experiments: 1) Run ensemble on a single sentence with known ground truth to verify CKY algorithm produces reasonable trees 2) Compare union distillation vs. ensemble distillation on a small dataset to observe over-smoothing effects 3) Test ensemble performance with varying numbers of teachers (2, 3, 4) on a validation set to find sweet spot

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the conclusion section. However, the authors suggest extending their ensemble-and-distill approach to different data types, such as sequences and graphs, which implies potential open questions about the method's generalizability to other structured prediction tasks beyond constituency parsing.

## Limitations
- The CKY-like algorithm may become computationally prohibitive for extremely long sentences, though this is not empirically verified
- The method relies on the assumption that parser diversity stems from genuine capture of different linguistic phenomena, but this is only weakly supported by correlation evidence
- The paper doesn't explore the impact of using different student models in the ensemble distillation process

## Confidence
**High Confidence**: The ensemble consistently improves over individual parsers (Table 2), the ensemble-then-distill approach beats union distillation, and the 72.8 F1 score represents state-of-the-art on Penn Treebank.

**Medium Confidence**: Parser diversity provides complementary strengths, CKY algorithm efficiently finds optimal trees, and over-smoothing is mitigated by ensemble-then-distill approach.

**Low Confidence**: The exact mechanism of over-smoothing in constituency parsing, the linguistic meaningfulness of parser diversity, and the robustness across different ensemble component combinations.

## Next Checks
1. **Parser correlation ablation**: Systematically vary teacher diversity (using more/less diverse parsers) and measure ensemble performance to quantify the relationship between parser diversity and ensemble gains.

2. **Sentence length scalability test**: Benchmark CKY algorithm runtime and accuracy degradation on progressively longer sentences (50, 100, 150 words) to identify practical limits.

3. **Over-smoothing isolation experiment**: Compare three approaches on identical teacher outputs: (a) union distillation, (b) ensemble-then-distill, and (c) ensemble-then-distill with entropy regularization removed to isolate over-smoothing effects.