---
ver: rpa2
title: 'DPMLBench: Holistic Evaluation of Differentially Private Machine Learning'
arxiv_id: '2305.05900'
source_url: https://arxiv.org/abs/2305.05900
tags:
- privacy
- algorithms
- data
- dpml
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present DPMLBench, the first holistic benchmark for evaluating
  improved differentially private machine learning (DPML) algorithms across utility
  and defense capability against membership inference attacks. DPMLBench provides
  a taxonomy of twelve state-of-the-art DPML algorithms categorized by their improvement
  phase (data preparation, model design, model training, or model ensemble).
---

# DPMLBench: Holistic Evaluation of Differentially Private Machine Learning

## Quick Facts
- arXiv ID: 2305.05900
- Source URL: https://arxiv.org/abs/2305.05900
- Reference count: 40
- Key outcome: First holistic benchmark evaluating DPML algorithms across utility and defense against membership inference attacks

## Executive Summary
DPMLBench presents the first comprehensive benchmark for evaluating differentially private machine learning algorithms across both utility and privacy defense capabilities. The benchmark evaluates twelve state-of-the-art DPML algorithms across four model architectures, four datasets, and various privacy budgets. Results demonstrate that DP can effectively defend against membership inference attacks, with sensitivity-bounding techniques like gradient clipping playing a crucial role. The study reveals that improvements in different phases of the ML pipeline affect privacy-utility trade-offs differently, and identifies architecture choices that work poorly for DPML compared to standard ML.

## Method Summary
The benchmark evaluates twelve DPML algorithms categorized into four phases: data preparation (Hand-DP, DPGEN, PrivSet), model design (TanhAct, FocalLoss), model training (RGP, GEP, AdpAlloc, AdpClip), and model ensemble (PATE, Priv-kNN). Experiments use PyTorch and Opacus across four model architectures (ResNet20, VGG16, InceptionNet, SimpleCNN) and four datasets (MNIST, FashionMNIST, CIFAR-10, SVHN) with privacy budgets ranging from ε=0.2 to 1000. Utility is measured by accuracy while defense effectiveness is evaluated using tailored AUC for membership inference attacks. The modular design enables future algorithm additions and combinations.

## Key Results
- DP effectively defends against membership inference attacks across all evaluated algorithms and datasets
- Sensitivity-bounding techniques like per-sample gradient clipping are crucial for privacy defense
- Parameter dimensionality reduction helps large models but impairs utility with large privacy budgets
- Model architecture choices effective for non-private ML often perform poorly for DPML

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Taxonomy-based categorization enables modular improvements in DPML algorithms
- Mechanism: Dividing ML pipeline into four phases (data preparation, model design, model training, model ensemble) allows independent improvement of each phase, facilitating combination of techniques for better performance
- Core assumption: Improvements in different phases are independent and can be combined without negative interactions
- Evidence anchors:
  - [abstract]: "We first present a taxonomy of where improvements are located in the machine learning life cycle"
  - [section 3.1]: "The improved DPML algorithms in different phases of our taxonomy are independent of each other"
  - [corpus]: Corpus contains multiple papers proposing phase-specific improvements, suggesting independence assumption is reasonable
- Break condition: Interactions between phases create conflicts that degrade performance when combined

### Mechanism 2
- Claim: Sensitivity-bounding techniques like per-sample gradient clipping defend against membership inference attacks
- Mechanism: Clipping reduces overfitting to training data by limiting gradient contributions from individual samples, making it harder for attackers to distinguish training from non-training samples
- Core assumption: Overfitting is the primary mechanism enabling membership inference attacks

## Foundational Learning

### Concept 1: Differential Privacy (DP)
- Why needed: Core framework protecting individual data privacy during ML training
- Quick check: Understand ε-DP definition and how it bounds information leakage

### Concept 2: Membership Inference Attacks (MIAs)
- Why needed: Primary threat model used to evaluate privacy defense effectiveness
- Quick check: Distinguish between black-box and white-box attack variants

### Concept 3: Privacy-Utility Trade-off
- Why needed: Fundamental constraint in DPML requiring careful balancing
- Quick check: Calculate utility loss and privacy leakage as proportions

## Architecture Onboarding

### Component Map
Data Preparation -> Model Design -> Model Training -> Model Ensemble -> Evaluation

### Critical Path
Model Training (DP-SGD with sensitivity-bounding) -> Privacy Budget Configuration -> MIA Evaluation

### Design Tradeoffs
BatchNorm vs GroupNorm: BatchNorm offers better non-private performance but GroupNorm is more compatible with DP training due to reduced internal statistics leakage.

Label DP vs Standard DP: Label DP provides better utility but weaker privacy guarantees against certain attacks compared to standard DP.

### Failure Signatures
High MIA AUC with high accuracy indicates privacy leakage despite good utility performance. Memory errors during GEP execution suggest architecture scaling limitations.

### 3 First Experiments
1. Run vanilla DP-SGD baseline on MNIST with varying ε values to establish performance floor
2. Compare Tanh activation vs ReLU on CIFAR-10 with DP training to validate architecture sensitivity findings
3. Test AdpClip algorithm with different clipping threshold schedules on FashionMNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of improvements from different phases of the machine learning pipeline for differentially private machine learning?
- Basis in paper: [explicit] The paper discusses a taxonomy of DPML algorithms categorized by improvement phase and mentions that combining improvements in different phases could achieve better performance
- Why unresolved: The paper only mentions this as a future research direction without conducting experiments to determine the optimal combinations
- What evidence would resolve it: Systematic experiments comparing different combinations of algorithms from multiple phases to identify which combinations yield the best utility-privacy trade-off

### Open Question 2
- Question: How do specific model architecture design choices (like activation functions and normalization layers) interact with DP training mechanisms?
- Basis in paper: [explicit] The paper found that using Tanh and GroupNorm can reduce utility loss with vanilla DP-SGD, but using both together had a negative effect
- Why unresolved: The paper only scratched the surface of this interaction through limited experiments and acknowledges that more exploration is needed
- What evidence would resolve it: Systematic experiments varying activation functions, normalization layers, and their combinations across different DP training algorithms to identify optimal architecture choices

### Open Question 3
- Question: How can DPML algorithms be effectively adapted for large-scale models and complex datasets?
- Basis in paper: [explicit] The paper discusses that most current DPML algorithms suffer from low utility on complex datasets like CIFAR-10
- Why unresolved: Limited by computational resources and the current state of algorithm development
- What evidence would resolve it: Successful application of DPML algorithms to large-scale vision models (e.g., ResNet50, EfficientNet) and complex datasets with competitive utility-privacy trade-offs

## Limitations
- Evaluation limited to image classification tasks, limiting generalizability to other domains
- Certain algorithms failed to converge on complex datasets, potentially biasing results
- Many hyperparameter details for individual algorithms are not fully specified

## Confidence
- **High confidence** in the core claim that DP can effectively defend against membership inference attacks, supported by empirical results across multiple datasets and algorithms
- **Medium confidence** in the claim about phase-specific improvements being independent, as the paper provides theoretical justification but limited empirical validation of algorithm combinations
- **Medium confidence** in the assertion that sensitivity-bounding techniques are key to defense, though the mechanism is well-established in literature

## Next Checks
1. Replicate the convergence issues reported for GEP and RGP on CIFAR-10 to determine if these are inherent limitations or implementation-specific problems
2. Test the independence assumption by systematically combining algorithms from different phases to measure potential interactions or degradations
3. Extend evaluation to non-image datasets (text, tabular) to assess the benchmark's generalizability claims