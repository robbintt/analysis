---
ver: rpa2
title: 'From Categories to Classifiers: Name-Only Continual Learning by Exploring
  the Web'
arxiv_id: '2311.11293'
source_url: https://arxiv.org/abs/2311.11293
tags:
- data
- datasets
- learning
- webly-supervised
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of Continual Learning (CL) without
  access to annotated data streams, which are typically expensive and time-consuming
  to obtain. The proposed solution, termed name-only continual learning, leverages
  uncurated webly-supervised data collected from search engines using category names
  and auxiliary suffixes.
---

# From Categories to Classifiers: Name-Only Continual Learning by Exploring the Web

## Quick Facts
- arXiv ID: 2311.11293
- Source URL: https://arxiv.org/abs/2311.11293
- Reference count: 40
- Primary result: Uncurated webly-supervised data can match or exceed manually annotated datasets in continual learning when using larger training sets.

## Executive Summary
This paper addresses continual learning without access to expensive annotated data streams by leveraging uncurated webly-supervised data. The approach, termed name-only continual learning, uses category names and auxiliary suffixes to collect data from search engines, then trains classifiers that perform competitively with those trained on manually annotated datasets. The method is evaluated across class-incremental, domain-incremental, and time-incremental scenarios, demonstrating only small performance gaps compared to curated datasets while significantly reducing annotation costs.

## Method Summary
The method involves querying multiple search engines (Google, Bing, Flickr, DuckDuckGo) using category names plus auxiliary suffixes to download images. These images are cleaned through deduplication and noise removal using visual features extracted by a frozen backbone (e.g., ResNet50 MoCoV3). A linear classifier or MLP adapter is then trained within computational budget constraints. The process repeats for each time step in continual learning scenarios, with past download links stored for replay to mitigate catastrophic forgetting.

## Key Results
- Webly-supervised datasets are 15-50 times larger than manually annotated datasets due to cheap query and download properties
- Models trained on web data achieve comparable or superior performance to those trained on manually annotated datasets
- Only small performance gaps observed compared to models trained on curated datasets across various CL scenarios
- EvoTrends dataset introduced to validate quick adaptation to evolving categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncurated webly-supervised data can match or exceed performance of manually annotated datasets because of the scale advantage.
- Mechanism: Web scraping allows collection of much larger datasets (15-50x) at negligible cost, which compensates for noise and domain mismatch.
- Core assumption: Larger training sets overcome noise and distributional differences.
- Evidence anchors:
  - [abstract] "find them comparable, and in some cases superior, to manually annotated datasets"
  - [section] "Our webly-supervised datasets are notably large due to cheap query and download properties, being approximately 15 to 50 times larger than the manually annotated datasets"
  - [corpus] No direct corpus evidence; relies on paper's own experiments
- Break condition: If domain gap is too large or data is too noisy, scale advantage may not compensate.

### Mechanism 2
- Claim: Adding auxiliary suffixes to search queries improves the relevance of downloaded images.
- Mechanism: Broad category names are ambiguous; adding suffixes (e.g., "snapdragon flower") refines queries and reduces irrelevant results.
- Core assumption: Search engines respond meaningfully to query refinements.
- Evidence anchors:
  - [section] "we design a simple querying strategy of adding an auxiliary suffix to refine our queries"
  - [section] "appending the suffix 'flower' refines the query to focus on the desired botanical images"
  - [corpus] No direct corpus evidence; based on authors' methodology
- Break condition: If search engines ignore suffixes or return poor results, this strategy fails.

### Mechanism 3
- Claim: Class-incremental continual learning works with web data because past samples can be stored and reused.
- Mechanism: Unlike strict online CL, storing download links allows replay of old samples, mitigating catastrophic forgetting.
- Core assumption: Download links remain accessible over time.
- Evidence anchors:
  - [section] "we do not restrict the storage of past samples... download links largely remain accessible"
  - [section] "no links expired in the duration of our study"
  - [corpus] No direct corpus evidence; based on authors' experimental setup
- Break condition: If links expire frequently or storage is restricted, this approach breaks.

## Foundational Learning

- Concept: Continual Learning (CL) basics
  - Why needed here: The paper builds on CL paradigms where models learn from sequential data streams.
  - Quick check question: What is the main challenge in class-incremental CL?
- Concept: Webly-supervised learning
  - Why needed here: The paper leverages uncurated web data as a substitute for manual annotations.
  - Quick check question: How does web data differ from manually curated datasets?
- Concept: Name-only classification
  - Why needed here: The paper operates in a setting where only category names (not samples) are provided.
  - Quick check question: How does name-only classification differ from zero-shot learning?

## Architecture Onboarding

- Component map: Query search engines → Download images → Deduplicate and clean → Extract features with frozen backbone → Train linear classifier/MLP adapter
- Critical path: Data collection → Cleaning → Feature extraction → Classifier training → Evaluation
- Design tradeoffs:
  - Scale vs. quality: Larger datasets improve performance but may introduce more noise
  - Multiple search engines vs. speed: More engines improve diversity but increase download time
  - Budget constraints: Tight budgets limit training epochs but ensure fairness vs. manual annotation
- Failure signatures:
  - Poor performance on specific classes: Likely due to domain mismatch (e.g., interior vs. exterior images)
  - Inconsistent results across search engines: Indicates reliance on specific engines
  - Link expiration: Affects ability to replay past samples
- First 3 experiments:
  1. Compare performance of web data vs. manually annotated data on a single dataset (e.g., FGVC Aircraft).
  2. Test impact of query suffixes by comparing "snapdragon" vs. "snapdragon flower" results.
  3. Evaluate classifier performance under tight vs. relaxed computational budgets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do performance differences between uncurated webly-supervised data and manually annotated datasets change with varying image quality and resolution?
- Basis in paper: [inferred] The paper discusses the domain gap between CIFAR100 (low quality, centered, downsampled to 32x32) and webly-supervised data (higher quality, not necessarily centered), suggesting image quality and resolution as factors.
- Why unresolved: The paper only provides a limited analysis of CIFAR100 and does not systematically vary image quality or resolution across multiple datasets to understand their impact on performance.
- What evidence would resolve it: Experiments comparing performance on datasets with varying image qualities and resolutions (e.g., high-res vs low-res versions of the same dataset) using both webly-supervised and manually annotated data.

### Open Question 2
- Question: What are the long-term effects of using uncurated webly-supervised data on model robustness and generalization to unseen domains?
- Basis in paper: [inferred] The paper highlights the out-of-distribution nature of web data compared to test sets and the performance gap in domain-incremental learning (e.g., PACS sketches), suggesting potential robustness issues.
- Why unresolved: The paper focuses on short-term performance comparisons and does not investigate how models trained on web data perform on completely unseen domains or over extended periods with evolving data distributions.
- What evidence would resolve it: Long-term studies tracking model performance on diverse, unseen datasets over time, including domain generalization benchmarks and stress tests with adversarial or out-of-distribution samples.

### Open Question 3
- Question: How does the performance of webly-supervised models compare to models trained on data collected through active learning or human-in-the-loop annotation approaches?
- Basis in paper: [explicit] The paper mentions active learning as a complementary approach that allows learning rare concepts but is computationally expensive, suggesting a potential comparison.
- Why unresolved: The paper does not directly compare webly-supervised models to those trained using active learning or human-in-the-loop methods, which could offer a middle ground between fully manual annotation and uncurated web data.
- What evidence would resolve it: Head-to-head comparisons of model performance, cost, and annotation time between webly-supervised models and models trained using active learning or human-in-the-loop approaches on the same tasks and datasets.

## Limitations
- The scale advantage mechanism relies heavily on the assumption that noise in web data is overcome by sheer volume, but the paper does not explicitly quantify or characterize this noise.
- The effectiveness of auxiliary suffixes depends on search engine behavior, which is not standardized and may vary over time or across regions.
- The paper does not address potential copyright or ethical issues related to web scraping.

## Confidence
- High Confidence: The general feasibility of using web data for continual learning (supported by direct experimental comparisons).
- Medium Confidence: The superiority of web data over manually annotated datasets (supported but with caveats about specific conditions).
- Low Confidence: The long-term stability of download links and the universal applicability of query suffixes (lacks empirical validation over extended periods).

## Next Checks
1. **Noise Characterization**: Analyze the noise level in downloaded web data by manually labeling a random sample and comparing it to manually annotated datasets.
2. **Link Stability Test**: Monitor the accessibility of download links over a period of 6-12 months to quantify link expiration rates.
3. **Suffix Effectiveness Validation**: Conduct a controlled experiment comparing the performance of queries with and without auxiliary suffixes across multiple search engines to measure the impact of query refinement.