---
ver: rpa2
title: Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models
arxiv_id: '2311.08921'
source_url: https://arxiv.org/abs/2311.08921
tags:
- nearest
- self-improving
- entity
- data
- diverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores self-improving for zero-shot named entity recognition
  (NER) with large language models (LLMs). The proposed framework utilizes an unlabeled
  corpus to stimulate the self-learning ability of LLMs.
---

# Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models

## Quick Facts
- arXiv ID: 2311.08921
- Source URL: https://arxiv.org/abs/2311.08921
- Authors: 
- Reference count: 7
- Primary result: Proposed self-improving framework achieves substantial improvements in zero-shot NER performance with LLMs

## Executive Summary
This work presents a self-improving framework for zero-shot Named Entity Recognition (NER) using large language models (LLMs). The approach leverages an unlabeled corpus to stimulate the LLM's self-learning ability through a three-step process: self-annotation with self-consistency scoring, reliable sample selection, and in-context learning with retrieved demonstrations. Experiments on four benchmark datasets demonstrate significant performance improvements compared to zero-shot baselines, with the diverse nearest neighbor retrieval strategy combined with SC ranking showing the best results.

## Method Summary
The self-improving framework operates without additional training by first using an LLM to generate predictions on an unlabeled corpus and obtaining self-consistency (SC) scores for each prediction. These SC scores serve as quality measures for the self-annotations. Reliable samples are then selected using various strategies including entity-level and sample-level SC threshold filtering and two-stage majority voting. For inference, demonstrations are retrieved from the reliable self-annotated dataset using diverse nearest neighbor retrieval with SC ranking, and in-context learning is performed to predict entities in test queries.

## Key Results
- The self-improving framework significantly improves zero-shot NER performance with LLMs
- The diverse nearest with SC ranking retrieval strategy achieves the best average results
- Increasing unlabeled corpus size or iterations does not guarantee further improvement
- Performance could potentially be boosted through more advanced reliable annotation selection strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-consistency scores serve as a proxy for annotation quality, enabling selection of reliable pseudo-demonstrations
- Mechanism: Multiple model predictions are generated for each unlabeled sample, and the agreement among predictions (self-consistency) is used to estimate the reliability of the annotation
- Core assumption: Self-consistency scores correlate with true annotation correctness
- Evidence anchors: [abstract] "we apply self-consistency (SC) to obtain a SC score for each prediction as the measure for the quality of self-annotation"

### Mechanism 2
- Claim: Diverse nearest neighbor retrieval with SC ranking balances similarity, diversity, and reliability of demonstrations
- Mechanism: Demonstrations are selected by retrieving nearest neighbors in embedding space, then uniformly sampling for diversity, and finally ranking by SC score to prioritize reliability
- Core assumption: A diverse set of high-reliability demonstrations improves in-context learning more than either highly similar or highly diverse but low-reliability demonstrations
- Evidence anchors: [abstract] "we propose the retrieval strategy of diverse nearest with SC ranking"

### Mechanism 3
- Claim: Iterative self-improving can accumulate errors and degrade performance over iterations
- Mechanism: Each iteration uses self-annotated data from the previous iteration as the unlabeled corpus, potentially propagating and amplifying errors
- Core assumption: Errors in self-annotation are not self-correcting and can accumulate over iterations
- Evidence anchors: [abstract] "(2) Iterative self-improving or simply increasing the size of unlabeled corpus does not guarantee further improvement"

## Foundational Learning

- Concept: Self-consistency in language models
  - Why needed here: Understanding how self-consistency scores are generated and interpreted is crucial for implementing and evaluating the reliable sample selection mechanism
  - Quick check question: How does the self-consistency method generate multiple predictions for the same input, and how is the final SC score calculated from these predictions?

- Concept: In-context learning (ICL) with demonstrations
  - Why needed here: The performance of the self-improving framework depends on how well the selected pseudo-demonstrations support in-context learning for the test queries
  - Quick check question: How does the number and quality of demonstrations affect the performance of in-context learning, and what are the trade-offs between similarity, diversity, and reliability?

- Concept: Named Entity Recognition (NER) task formulation
  - Why needed here: Understanding the structure of NER outputs (entity spans and types) is essential for interpreting the results and designing the prompt format
  - Quick check question: What is the difference between entity-level and sample-level SC scores, and how do they relate to the NER task formulation?

## Architecture Onboarding

- Component map: Unlabeled corpus -> LLM self-annotation with SC scoring -> Sample selection -> Retrieval strategy -> ICL inference -> Predicted named entities

- Critical path:
  1. Self-annotate unlabeled corpus with LLM and SC scores
  2. Select reliable demonstrations using sample selection strategies
  3. Retrieve demonstrations for test query using diverse nearest with SC ranking
  4. Perform ICL inference with retrieved demonstrations

- Design tradeoffs:
  - SC threshold vs. demonstration pool size: Higher SC thresholds result in fewer but potentially more reliable demonstrations, while lower thresholds provide more demonstrations but with potentially lower quality
  - Retrieval strategy: Balancing similarity, diversity, and reliability of demonstrations to optimize ICL performance
  - Iterative vs. one-shot self-improving: Iterative self-improving can potentially improve performance but risks error accumulation

- Failure signatures:
  - Low SC scores across all predictions: Indicates the LLM struggles with the NER task in the given context
  - High variance in ICL performance across test queries: Suggests the selected demonstrations are not consistently helpful
  - Performance degradation over iterations: Indicates error accumulation in the self-annotating step

- First 3 experiments:
  1. Baseline: Evaluate zero-shot NER performance without any self-improving (No-demos setting)
  2. Ablation: Compare different sample selection strategies to identify the most effective approach
  3. Scalability: Investigate the impact of increasing the size of the unlabeled corpus on performance

## Open Questions the Paper Calls Out

- What are the specific strategies that could be employed to select reliable entities in the self-annotated dataset, beyond the current methods of entity-level and sample-level SC threshold filtering and two-stage majority voting?

- How does the performance of the self-improving framework vary with different sizes of the unlabeled corpus, and is there an optimal size beyond which no further improvement is observed?

- What is the impact of iterative self-improving on the performance of the framework, and why does the effect vary across different datasets?

## Limitations

- The paper doesn't validate whether self-consistency scores consistently correlate with true annotation quality across diverse NER domains
- The observation that iterative self-improving doesn't guarantee improvement lacks deeper analysis of when and why error accumulation occurs
- The scalability analysis is limited to the tested datasets, leaving questions about performance on significantly larger or more diverse unlabeled corpora

## Confidence

**High Confidence Claims:**
- The self-improving framework demonstrably improves zero-shot NER performance compared to no-demonstration baselines
- SC-based sample selection effectively filters lower-quality annotations
- The diverse nearest with SC ranking strategy outperforms simpler retrieval methods

**Medium Confidence Claims:**
- Error accumulation in iterative self-improving is a general phenomenon (though dataset-specific variations exist)
- Increasing unlabeled corpus size doesn't guarantee improvement (limited to tested dataset ranges)
- SC scores serve as reliable quality proxies (correlation assumed but not extensively validated)

**Low Confidence Claims:**
- The proposed balance between similarity, diversity, and reliability is optimal for all NER tasks
- The framework's performance generalizes to domains with significantly different entity distributions or annotation standards

## Next Checks

1. **SC Score Validation**: Conduct a systematic study correlating SC scores with human-annotated ground truth on a subset of predictions to empirically validate whether higher SC scores actually indicate higher annotation quality.

2. **Ablation of Retrieval Components**: Design controlled experiments that vary each component of the retrieval strategy (similarity threshold, diversity sampling rate, SC ranking weight) independently to determine the optimal balance rather than assuming the proposed combination is best.

3. **Error Propagation Analysis**: Implement a tracking mechanism to follow specific annotation errors through iterations to understand which types of errors accumulate most readily and whether certain entity categories are more susceptible to degradation.