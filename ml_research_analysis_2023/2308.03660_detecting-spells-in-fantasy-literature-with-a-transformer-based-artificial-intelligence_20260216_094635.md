---
ver: rpa2
title: Detecting Spells in Fantasy Literature with a Transformer Based Artificial
  Intelligence
arxiv_id: '2308.03660'
source_url: https://arxiv.org/abs/2308.03660
tags:
- spells
- spell
- sequence
- sequences
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that a transformer-based model, specifically
  a fine-tuned BERT variant, can effectively recognize the context of magic spells
  in the Harry Potter novels with high accuracy (F1-scores up to 0.9697). The research
  investigates whether spell context can be recognized based on sentence, paragraph,
  or multi-sentence sequences, showing that longer sequences improve recognition performance.
---

# Detecting Spells in Fantasy Literature with a Transformer Based Artificial Intelligence

## Quick Facts
- arXiv ID: 2308.03660
- Source URL: https://arxiv.org/abs/2308.03660
- Reference count: 40
- Primary result: Transformer-based models can detect spell context in Harry Potter with F1-scores up to 0.9697

## Executive Summary
This study investigates the use of transformer-based models, specifically a fine-tuned BERT variant, for detecting magic spells in fantasy literature. The research demonstrates that such models can effectively recognize spell context with high accuracy, particularly when longer sequences are used. The study explores both sequence-level and token-level classification approaches, finding that longer sequences improve overall context recognition while shorter sequences are better for identifying individual spell words. The work also examines the model's ability to generalize across different fantasy universes.

## Method Summary
The method involves fine-tuning a pre-trained BERT model (specifically all-mpnet-base-v2) on Harry Potter novel text to detect spell contexts. The dataset is prepared by converting ebooks to plain text, removing non-content elements, and splitting into sequences using sentence-split, paragraph-split, and sequence-split approaches. Sequences are labeled as positive if they contain a spell, negative otherwise. The model is trained using the Hugging Face Transformers library with various hyperparameters, and evaluated using F1-score on a held-out validation set.

## Key Results
- F1-scores up to 0.9697 for spell context detection in Harry Potter novels
- Longer input sequences improve recognition performance compared to shorter sequences
- Custom tokeniser updates that add spells as whole tokens improved F1 scores
- Models trained on one fantasy universe show limited but promising generalization to others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Longer input sequences improve spell context recognition because the transformer can better capture co-occurring lexical and syntactic patterns that indicate spell usage.
- Mechanism: By concatenating multiple sentences until reaching the maximum token limit, the model processes more surrounding context. This reduces the chance that a spell-related term is isolated and increases the likelihood that relevant contextual cues (e.g., verbs like "shouted," "cast," or descriptive phrases) are present.
- Core assumption: Spell context is distributed across several tokens and not confined to a single sentence boundary.
- Evidence anchors:
  - [abstract] "longer sequences improve recognition performance"
  - [section] "When larger sequences of several sentences are used, a transformer-based model is quite efficient at recognising the context of a spell"
  - [corpus] Weak: No explicit corpus-level token correlation analysis provided.
- Break condition: If the model cannot process sequences longer than 384 tokens, or if spells are contextually independent of surrounding text, sequence length improvements will plateau.

### Mechanism 2
- Claim: Custom tokeniser updates improve recognition because they prevent spell words from being split into subword tokens, preserving semantic integrity during classification.
- Mechanism: Adding spells as whole tokens to the tokeniser vocabulary ensures that each spell is represented as a single embedding rather than a sequence of fragments. This allows the model to treat spells as atomic units and prevents misclassification due to token-level ambiguity.
- Core assumption: Subword tokenization of rare fantasy terms leads to ambiguous or misleading embeddings.
- Evidence anchors:
  - [section] "Updating the tokeniser had a positive effect on the F1 score... The number of falsely correctly classified sequences has decreased."
  - [section] "Based on the Wordpiece tokenisation approach, less common words are more often split into smaller tokens."
  - [corpus] Weak: No explicit subword frequency distribution data provided.
- Break condition: If spells are already common in the pretraining corpus, or if the model relies heavily on subword semantics, the improvement may be negligible.

### Mechanism 3
- Claim: Pre-trained MPNet/SBERT models can be fine-tuned efficiently because their masked language modeling and permuted language modeling objectives already capture rich contextual dependencies.
- Mechanism: The model uses contextual embeddings from MPNet's transformer layers, which encode bidirectional dependencies. Fine-tuning only the head layer adapts these embeddings to the specific spell classification task without losing general language understanding.
- Core assumption: The pre-training tasks of MPNet are sufficiently general to transfer to the domain-specific task of spell detection.
- Evidence anchors:
  - [abstract] "a pre-trained BERT model was used and fine-tuned"
  - [section] "The pre-trained BERT model is loaded... This library implements a sophisticated training routine that will be used for further fine-tuning."
  - [section] "MPNet... achieves a better result on a variety of downstream NLP tasks"
- Break condition: If the pre-training corpus is too dissimilar from fantasy literature, or if the task requires learning entirely new semantic categories, transfer learning gains will diminish.

## Foundational Learning

- Concept: Sequence classification vs token classification
  - Why needed here: The paper tests both approaches to determine whether spell context can be detected at the sequence level or whether individual spell words can be explicitly identified.
  - Quick check question: What is the difference between predicting a single label for an entire input and predicting a label for each token in the input?

- Concept: Self-attention in transformers
  - Why needed here: Understanding how transformers use self-attention to build context dependencies is essential for grasping why longer sequences and pre-trained embeddings help in spell detection.
  - Quick check question: How does self-attention allow a transformer to weigh the importance of each token relative to others in the sequence?

- Concept: Fine-tuning vs training from scratch
  - Why needed here: The study leverages pre-trained models and fine-tunes them, which is computationally efficient and leverages existing linguistic knowledge.
  - Quick check question: Why is fine-tuning a pre-trained model generally faster and more data-efficient than training a new model from scratch?

## Architecture Onboarding

- Component map:
  Tokenizer -> Transformer encoder stack -> Pooling layer (sequence classification) or linear head per token (token classification) -> Output labels
- Critical path:
  1. Tokenize input sequence
  2. Generate contextual embeddings via transformer layers
  3. Apply pooling or per-token linear transformation
  4. Output classification scores
- Design tradeoffs:
  - Sequence length: longer sequences improve context but risk losing focus on specific tokens
  - Token granularity: subword tokens increase flexibility but may obscure rare term semantics
  - Dataset size: more data improves robustness but increases training time
- Failure signatures:
  - Low F1 scores with longer sequences: model may not be capturing context effectively
  - High false positives with token classification: subword tokenization causing ambiguity
  - No improvement from tokeniser updates: spells already well represented in pretraining corpus
- First 3 experiments:
  1. Compare F1 scores using Sentence-Split vs Paragraph-Split vs Sequence-Split datasets
  2. Add spells to tokenizer vocabulary and retrain, measure change in F1
  3. Test sequence classification vs token classification on the same dataset to compare approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of transformer-based models for spell detection generalize across different fantasy universes with varying magical systems?
- Basis in paper: [explicit] The paper mentions investigating whether spells have overarching properties that allow transfer of neural network models to other fantasy universes, with promising but limited results.
- Why unresolved: The study was limited by the difficulty of creating unique spell datasets for different universes due to ambiguous spell usage and lack of comprehensive spell lists.
- What evidence would resolve it: A comprehensive, multi-universe dataset of spells with clear annotations would allow training and testing of transformer models across different magical systems to quantify generalization performance.

### Open Question 2
- Question: What is the optimal context length for spell detection using transformer models, and how does this vary between sequence classification and token classification tasks?
- Basis in paper: [explicit] The paper investigates different context lengths (sentence, paragraph, multi-sentence) and finds that longer sequences improve sequence classification performance, while shorter sequences are better for token classification.
- Why unresolved: The study used specific context lengths based on the maximum sequence length of the model, but did not systematically explore the full range of possible context lengths or their interaction with model architecture.
- What evidence would resolve it: A systematic study varying context lengths across a wider range and testing different transformer architectures would identify optimal context lengths for each task type.

### Open Question 3
- Question: How do pre-trained transformer models like BERT perform on spell detection tasks compared to models trained specifically on fantasy literature?
- Basis in paper: [inferred] The paper uses a pre-trained BERT variant fine-tuned on Harry Potter data, but does not compare its performance to models trained from scratch on fantasy literature.
- Why unresolved: The study focuses on fine-tuning existing pre-trained models rather than comparing them to models trained specifically for the task domain.
- What evidence would resolve it: Training and comparing both pre-trained and domain-specific models on the same spell detection tasks would quantify the benefit of pre-training versus task-specific training.

## Limitations

- Dataset Construction and Spell Labeling: The paper does not provide the complete list of spells used for labeling or detailed examples of how ambiguous cases were handled.
- Generalization Across Fantasy Universes: Validation is limited to two additional series with small sample sizes, lacking rigorous analysis of cross-universe differences.
- Tokenization Impact Quantification: The paper does not provide quantitative breakdowns of subword split frequencies or ablation studies isolating tokenization effects.

## Confidence

- High Confidence: The claim that transformer-based models can effectively detect spell context with high F1-scores (up to 0.9697) is well-supported by experimental results on the Harry Potter corpus.
- Medium Confidence: The finding that longer sequences improve recognition is plausible and supported by results, but lacks exploration of upper limits or attention mechanism analysis.
- Low Confidence: The claim that models generalize well to other fantasy universes is weakly supported; the paper shows some success but lacks rigorous cross-universe error analysis.

## Next Checks

1. Ablation Study on Tokenization: Retrain the model with and without custom spell tokens, and with varying tokenizer vocabularies, to isolate the impact of tokenization on F1 scores. Report the frequency of subword splits for spell words in each case.

2. Cross-Universe Error Analysis: For each misclassified sequence in Eragon and The Farewell Paladin, perform a manual linguistic analysis to identify whether errors stem from vocabulary differences, syntactic patterns, or contextual ambiguity. Compare attention weight distributions for correct vs. incorrect classifications.

3. Sequence Length Saturation Test: Systematically increase sequence length beyond the maximum tested (384 tokens, if possible) or simulate longer contexts by overlapping sequences, and measure F1 score changes. Determine whether improvements plateau and, if so, analyze attention maps to see if the model focuses on the correct context.