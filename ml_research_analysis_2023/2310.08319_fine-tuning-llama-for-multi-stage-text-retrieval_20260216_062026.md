---
ver: rpa2
title: Fine-Tuning LLaMA for Multi-Stage Text Retrieval
arxiv_id: '2310.08319'
source_url: https://arxiv.org/abs/2310.08319
tags:
- retrieval
- language
- repllama
- document
- effectiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) for
  multi-stage text retrieval. Specifically, the authors fine-tune the LLaMA model
  for both passage retrieval (RepLLaMA) and document retrieval (RankLLaMA) tasks using
  the MS MARCO datasets.
---

# Fine-Tuning LLaMA for Multi-Stage Text Retrieval

## Quick Facts
- arXiv ID: 2310.08319
- Source URL: https://arxiv.org/abs/2310.08319
- Reference count: 13
- Primary result: RepLLaMA achieves state-of-the-art MRR@10 on MS MARCO passage retrieval, surpassing smaller models by 2 points

## Executive Summary
This paper explores fine-tuning large language models (LLMs) for multi-stage text retrieval, specifically using LLaMA as the backbone for both dense retrieval (RepLLaMA) and document reranking (RankLLaMA). The authors demonstrate that LLMs can represent entire documents holistically due to their longer context windows, eliminating the need for traditional segmentation and pooling strategies. Through extensive experiments on MS MARCO and BEIR datasets, they show that RepLLaMA and RankLLaMA achieve state-of-the-art effectiveness for both retrieval and reranking tasks, with strong zero-shot performance across diverse domains.

## Method Summary
The authors fine-tune LLaMA-2 models (7B and 13B parameters) as dense retrievers (RepLLaMA) and pointwise rerankers (RankLLaMA) for text retrieval tasks. RepLLaMA uses a bi-encoder architecture to encode queries and documents into dense vector representations for similarity search, while RankLLaMA takes query-document pairs as input and learns to score relevance. Both models are trained using contrastive loss with hard negative mining on MS MARCO passage and document ranking datasets, with LoRA used for parameter-efficient fine-tuning. The pipeline consists of RepLLaMA retrieving top-k candidates followed by RankLLaMA reranking them, achieving strong performance on both in-domain and zero-shot evaluations.

## Key Results
- RepLLaMA achieves an MRR@10 score approximately 3 points higher than CoCondenser-MaxP on MS MARCO passage retrieval
- RankLLaMA exceeds the current state-of-the-art document reranker by 1 point in MRR@100 on MS MARCO document ranking
- The RepLLaMA-RankLLaMA pipeline demonstrates strong zero-shot effectiveness on BEIR datasets, outperforming existing models
- Both models show superior performance compared to smaller models while maintaining simplicity through direct fine-tuning without additional contrastive pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can represent entire documents holistically, eliminating the need for segmenting and pooling strategies.
- Mechanism: LLaMA's pre-training on longer contexts (up to 4096 tokens) enables it to encode full documents in a single pass, avoiding information loss from segment pooling.
- Core assumption: Longer context pre-training allows meaningful semantic encoding of entire documents without segmentation.
- Evidence anchors:
  - [abstract]: "since LLMs can inherently handle longer contexts, they can represent entire documents holistically, obviating the need for traditional segmenting and pooling strategies"
  - [section 3.2]: "The document retrieval task aims to rank document-length texts, which present the challenge of handling long input sequences... The standard solution to manage long sequences for retrieval is the MaxP strategy... However, this process involves a heuristic pooling strategy and runs the risk of losing information spread across long contexts"
  - [corpus]: Weak - no direct corpus evidence provided for document length handling.
- Break condition: Document lengths significantly exceed model context window or semantic coherence requires explicit segmentation.

### Mechanism 2
- Claim: Fine-tuning LLaMA for dense retrieval (RepLLaMA) and pointwise reranking (RankLLaMA) achieves state-of-the-art effectiveness.
- Mechanism: Leveraging LLaMA's strong pre-trained representations through supervised fine-tuning on labeled MS MARCO data produces effective retrievers and rerankers.
- Core assumption: LLaMA's pre-trained representations are sufficiently general to be effectively adapted to retrieval tasks with limited fine-tuning.
- Evidence anchors:
  - [abstract]: "Our findings demonstrate that the effectiveness of large language models indeed surpasses that of smaller models"
  - [section 3.1]: "RepLLaMA achieves an MRR@10 score that is approximately 3 points higher than CoCondenser-MaxP" and "RankLLaMA exceeds... the current state-of-the-art document reranker... by 1 point in MRR@100"
  - [section 2.2]: "We take the representation of the end-of-sequence token as the representation of the input sequence"
- Break condition: Insufficient labeled training data or mismatched training objectives.

### Mechanism 3
- Claim: RepLLaMA–RankLLaMA pipeline exhibits strong zero-shot effectiveness on BEIR.
- Mechanism: LLaMA's pre-trained representations generalize well across domains, enabling effective zero-shot retrieval on diverse BEIR datasets.
- Core assumption: Pre-trained LLaMA representations capture universal semantic features transferable across retrieval tasks and domains.
- Evidence anchors:
  - [abstract]: "evaluations on BEIR demonstrate that our RepLLaMA–RankLLaMA pipeline exhibits strong zero-shot effectiveness"
  - [section 3.1]: "Both models demonstrate superior zero-shot effectiveness, outperforming existing models"
  - [section 2.1]: "These models are trained to encode queries and documents into vector representations for retrieval"
- Break condition: Significant domain shift between pre-training data and target retrieval tasks.

## Foundational Learning

- Concept: Dense retrieval with bi-encoder architecture
  - Why needed here: Understanding how RepLLaMA encodes queries and documents into dense vectors for similarity search
  - Quick check question: What is the difference between a bi-encoder and a cross-encoder architecture for retrieval?

- Concept: Pointwise reranking with supervised fine-tuning
  - Why needed here: Understanding how RankLLaMA takes query-document pairs as input and learns to score relevance
  - Quick check question: How does pointwise reranking differ from pairwise or listwise reranking approaches?

- Concept: Hard negative mining for contrastive learning
  - Why needed here: Understanding how negative samples are selected to improve retriever training effectiveness
  - Quick check question: Why are hard negatives more effective than random negatives in contrastive learning for retrieval?

## Architecture Onboarding

- Component map:
  - RepLLaMA: Dense retriever with LLaMA backbone, outputs dense vector representations
  - RankLLaMA: Pointwise reranker with LLaMA backbone, outputs relevance scores
  - Pipeline: RepLLaMA retrieves top-k candidates → RankLLaMA reorders them
  - Training data: MS MARCO passage/document ranking datasets with hard negative mining
  - Inference: Exact nearest neighbor search (no approximate indexing)

- Critical path:
  1. Preprocess and encode corpus with RepLLaMA
  2. For each query, retrieve top-k candidates using exact search
  3. Rerank retrieved candidates with RankLLaMA
  4. Return final ranked list

- Design tradeoffs:
  - Exact vs approximate nearest neighbor search (accuracy vs latency)
  - Document length handling (full documents vs segmentation)
  - Model size vs computational efficiency (7B vs 13B parameters)
  - Full fine-tuning vs parameter-efficient methods (LoRA) for adaptation

- Failure signatures:
  - Low recall despite high precision: insufficient corpus coverage or retrieval cutoff too aggressive
  - High variance across domains: overfit to MS MARCO distribution
  - Memory overflow during training: batch size too large or LoRA not configured properly
  - Slow inference: exact search on large corpus without indexing optimizations

- First 3 experiments:
  1. Compare RepLLaMA with and without hard negative mining on dev set MRR@10
  2. Test different document input lengths (512, 2048, 4096) for RankLLaMA on MS MARCO doc dev
  3. Evaluate zero-shot performance on a subset of BEIR datasets before full evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of RepLLaMA and RankLLaMA compare when fine-tuned with additional contrastive pre-training before supervised fine-tuning?
- Basis in paper: [explicit] The authors mention that recent studies have shown potential to further improve dense retrieval models by learning from soft labels provided by a reranker via optimizing KL-divergence. They also note that RepLLaMA uses the base pre-trained model as initialization, achieving high zero-shot effectiveness while maintaining simplicity.
- Why unresolved: The paper does not explore the impact of additional contrastive pre-training on the effectiveness of RepLLaMA and RankLLaMA.
- What evidence would resolve it: Conducting experiments with and without additional contrastive pre-training and comparing the effectiveness of the resulting models on various retrieval tasks.

### Open Question 2
- Question: How does the performance of RepLLaMA and RankLLaMA change when using different sizes of LLaMA models (e.g., 7B, 13B, 33B)?
- Basis in paper: [explicit] The authors train a version of RankLLaMA using LLaMA-2-13B initialization and observe that it outperforms the 7B model, achieving higher MRR@10 and slightly higher nDCG@10. They also mention the potential for further improvements with even larger models.
- Why unresolved: The paper only explores the performance of RepLLaMA and RankLLaMA with LLaMA-2-7B and LLaMA-2-13B models, leaving the impact of even larger models unexplored.
- What evidence would resolve it: Conducting experiments with various sizes of LLaMA models and comparing their effectiveness on retrieval tasks.

### Open Question 3
- Question: How does the performance of RepLLaMA and RankLLaMA change when using different input sequence lengths for document reranking?
- Basis in paper: [explicit] The authors investigate the effects of varying the maximum training input length and inference input length on model effectiveness for the document reranking task. They observe that effectiveness improves as the maximum training length increases from 512 to 2048, with gains plateauing beyond a certain length.
- Why unresolved: The paper only explores the impact of input sequence lengths up to 4096 tokens, leaving the potential benefits of even longer sequences unexplored.
- What evidence would resolve it: Conducting experiments with longer input sequence lengths and comparing the effectiveness of the resulting models on document reranking tasks.

## Limitations

- The paper does not specify the exact document length distribution in the MS MARCO document corpus, making it unclear whether the holistic encoding claim applies broadly across all document types
- Zero-shot generalization claims are based on evaluation on only 14 out of 22 BEIR datasets, with specialized domains like COVID-19 and SciDocs not comprehensively assessed
- Critical implementation details for LoRA adaptation and negative sampling strategies are not fully specified, which could affect reproducibility and performance comparisons

## Confidence

- High Confidence: The retrieval and reranking effectiveness improvements on MS MARCO datasets (MRR@10 improvements of 3+ points for RepLLaMA over CoCondenser-MaxP, and 1 point for RankLLaMA over state-of-the-art). These claims are directly supported by the experimental results in Table 1 and Table 2.
- Medium Confidence: The claim about LLMs obviating the need for segmenting and pooling strategies. While the paper demonstrates effectiveness with full document encoding, the mechanism is plausible but not conclusively proven - alternative explanations like better pre-training or fine-tuning methodology could contribute.
- Low Confidence: The strength of zero-shot generalization across all BEIR datasets. The evaluation covers a subset of BEIR, and the performance on specialized domains is not comprehensively assessed.

## Next Checks

1. **Document Length Distribution Analysis**: Analyze the length distribution of documents in the MS MARCO document corpus to determine what percentage of documents actually fit within LLaMA's 4096-token context window. This would validate whether the holistic encoding claim applies broadly or is limited to a subset of documents.

2. **Ablation on Document Segmentation**: Compare RepLLaMA performance when using full documents versus MaxP-style segmentation on the MS MARCO document corpus. This would isolate whether the effectiveness gains come from the LLM architecture itself or from differences in how document representations are aggregated.

3. **Cross-Domain Robustness Testing**: Evaluate RepLLaMA-RankLLaMA on the omitted BEIR datasets (Argument Facet, COVID-19, SciDocs, etc.) to assess whether the zero-shot effectiveness claims hold across the full BEIR benchmark, particularly for specialized domains.