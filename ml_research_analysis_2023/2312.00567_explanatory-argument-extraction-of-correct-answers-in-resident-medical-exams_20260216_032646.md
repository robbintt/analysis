---
ver: rpa2
title: Explanatory Argument Extraction of Correct Answers in Resident Medical Exams
arxiv_id: '2312.00567'
source_url: https://arxiv.org/abs/2312.00567
tags:
- medical
- question
- dataset
- answer
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new dataset based on Spanish MIR exams
  that includes explanatory arguments for both correct and incorrect answers, enabling
  a novel extractive task to identify the explanation of the correct answer. By casting
  this task in an extractive QA setting, the authors automatically evaluate performance
  of language models without requiring costly manual evaluation by medical experts.
---

# Explanatory Argument Extraction of Correct Answers in Resident Medical Exams

## Quick Facts
- arXiv ID: 2312.00567
- Source URL: https://arxiv.org/abs/2312.00567
- Reference count: 40
- This paper introduces a new dataset based on Spanish MIR exams that includes explanatory arguments for both correct and incorrect answers, enabling a novel extractive task to identify the explanation of the correct answer.

## Executive Summary
This paper introduces a novel dataset for explanatory argument extraction in Spanish medical exams, where the task is to identify the span of text that explains the correct answer. By framing this as an extractive QA problem, the authors enable automatic evaluation of language models without requiring medical expert annotation. The study comprehensively evaluates various Spanish language models, including monolingual and multilingual variants, and demonstrates that including clinical cases in the question context significantly improves performance. Intermediate fine-tuning with general domain QA datasets like SQAC further enhances model accuracy, particularly for unstructured datasets.

## Method Summary
The approach involves fine-tuning pre-trained language models using a two-stage process: first, intermediate fine-tuning on general domain QA datasets (SQuAD-es and SQAC) using the STILT framework, then final fine-tuning on the medical domain datasets (CasiMedicos and MIR Asturias). The task is formulated as extractive QA where the model must identify the span of text that constitutes the explanation for the correct answer. Clinical cases are included as part of the question context to provide evidence-based reasoning chains. Models are evaluated using standard QA metrics including F1-score and exact match.

## Key Results
- Including the clinical case in the question context yields the best performance
- Intermediate fine-tuning with SQAC and SQuAD-es improves span extraction accuracy
- Multilingual models sometimes outperform monolingual medical-specific models
- STILT approach reduces empty and incomplete answers while increasing exact matches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanatory argument extraction benefits from structured clinical context
- Mechanism: When the clinical case is included in the question and context, models can better align their extraction to the evidence-based reasoning chain that connects symptoms to diagnosis and treatment.
- Core assumption: The clinical case provides a consistent semantic bridge between the question and the correct explanation.
- Evidence anchors:
  - [abstract] "The best results are obtained when the clinical case is included in the question"
  - [section 5.2] "Including the clinical case as part of the question seems to be beneficial"
  - [corpus] Weak evidence; neighbor papers do not explicitly mention clinical case inclusion as a key factor.
- Break condition: If the clinical case is removed or altered, performance degrades, especially in unstructured datasets like CasiMedicos.

### Mechanism 2
- Claim: Intermediate fine-tuning with general domain QA datasets improves extraction accuracy
- Mechanism: STILT with SQAC or SQuAD-es teaches the model to identify answer spans more precisely, reducing empty and incomplete answers while increasing exact matches.
- Core assumption: Span identification skills learned from general domain datasets transfer to medical explanations.
- Evidence anchors:
  - [section 4.1] "approaches harnessing general domain datasets... as intermediate fine-tuning dataset... have also been shown to be effective"
  - [section 5.2] "using STILT by means of general domain QA datasets always helps for the downstream task"
  - [section 6.2] "fewer empty and incomplete answers and boosting the number of exact matches"
- Break condition: If intermediate fine-tuning is omitted, models produce more empty or incomplete answers, especially in the unstructured CasiMedicos dataset.

### Mechanism 3
- Claim: Structured explanations in training data improve model generalization
- Mechanism: When explanations follow a consistent pattern (e.g., marking correct/incorrect answers), models learn to locate the correct explanation more reliably.
- Core assumption: Explicit structural markers reduce ambiguity in the extraction task.
- Evidence anchors:
  - [section 6.1] "MIR Asturias follows a rather clear structured pattern where the spans of the explanations are systematically marked and linked to their corresponding possible answers"
  - [section 6.1] "by removing the patterns from the explanations in the MIR Asturias data the results obtained are similar to those of CasiMedicos"
  - [corpus] Neighbor papers do not directly address structured explanations, so this evidence is inferred from the dataset comparison.
- Break condition: If structured markers are removed, performance drops to levels similar to unstructured datasets, suggesting models overfit to explicit clues.

## Foundational Learning

- Concept: Question Answering (QA) extractive setting
  - Why needed here: The task is cast as extractive QA so that models can identify the span of text that constitutes the correct explanation.
  - Quick check question: In extractive QA, what does the model output given a question and context?

- Concept: Intermediate Fine-tuning (STILT)
  - Why needed here: Pre-training on general domain QA datasets improves span extraction before fine-tuning on the medical dataset.
  - Quick check question: What is the benefit of fine-tuning on SQAC or SQuAD-es before the medical dataset?

- Concept: Clinical Case Context
  - Why needed here: Including the clinical case in the question and context improves model alignment with evidence-based reasoning.
  - Quick check question: How does adding the clinical case to the question affect model performance?

## Architecture Onboarding

- Component map: Pre-trained Language Models (BERT, RoBERTa variants) → Intermediate fine-tuning (SQAC/SQuAD-es) → Final fine-tuning (CasiMedicos/MIR Asturias) → Evaluation (precision, recall, F1)
- Critical path: Data preparation → Intermediate fine-tuning → Final fine-tuning → Evaluation → Error analysis
- Design tradeoffs: Monolingual vs multilingual models; medical vs general domain; structured vs unstructured data; length of answer spans
- Failure signatures: High number of empty answers; low precision; overfitting to explicit markers; inability to generalize across datasets
- First 3 experiments:
  1. Fine-tune a monolingual model directly on CasiMedicos and evaluate F1-score
  2. Apply STILT with SQAC before fine-tuning on CasiMedicos; compare results
  3. Add the clinical case to the question and context; evaluate if performance improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the length of correct answer explanations affect model performance in extractive QA tasks?
- Basis in paper: [explicit] The paper discusses that the length of explanations in CasiMedicos (32.02 words average) is longer than in SQuAD (3.2 words average), and hypothesizes that this might make the task more difficult to learn.
- Why unresolved: The paper does not provide conclusive evidence on whether the length of explanations directly impacts model performance, as other factors like dataset structure might also play a role.
- What evidence would resolve it: Conducting experiments with datasets containing explanations of varying lengths while controlling for other variables would help determine the impact of explanation length on model performance.

### Open Question 2
- Question: What is the impact of structured vs. unstructured explanations on the performance of language models in extractive QA tasks?
- Basis in paper: [explicit] The paper compares the structured explanations in MIR Asturias with the more spontaneous explanations in CasiMedicos, finding that structured explanations might make the task easier for models.
- Why unresolved: The paper does not provide a detailed analysis of how different structures of explanations affect model learning and performance.
- What evidence would resolve it: Experiments comparing model performance on structured vs. unstructured explanations, and analyzing the specific patterns in explanations that aid or hinder model learning, would provide insights.

### Open Question 3
- Question: How does intermediate fine-tuning with general domain QA datasets affect the performance of language models on domain-specific tasks?
- Basis in paper: [explicit] The paper discusses the use of SQAC and SQuAD-es for intermediate fine-tuning before training on the medical domain datasets, noting improvements but not conclusively determining which dataset is best.
- Why unresolved: The paper does not provide a clear conclusion on which intermediate fine-tuning dataset consistently yields the best results for the domain-specific task.
- What evidence would resolve it: Systematic experiments comparing the effects of different intermediate fine-tuning datasets on a variety of domain-specific tasks would help identify the most effective approach.

## Limitations

- Small annotated dataset size (~2,500 questions) limits statistical significance and generalizability
- Lack of inter-annotator agreement scores makes it difficult to assess annotation quality differences between datasets
- No ablation studies to isolate the specific contribution of each mechanism (clinical case inclusion vs. intermediate fine-tuning)
- Unexpected finding that multilingual models outperform monolingual medical-specific models lacks theoretical justification

## Confidence

- High Confidence: Effectiveness of intermediate fine-tuning with general domain QA datasets
- Medium Confidence: Benefit of including clinical cases in questions
- Low Confidence: Superiority of multilingual models over monolingual medical-specific models in Spanish tasks

## Next Checks

1. **Annotation Quality Assessment:** Conduct inter-annotator agreement studies on both CasiMedicos and MIR Asturias datasets to establish baseline annotation quality and consistency.

2. **Controlled Ablation Study:** Systematically remove clinical cases from questions in MIR Asturias to directly test whether the performance benefit is specific to case inclusion.

3. **Cross-linguistic Transfer Experiment:** Test whether Spanish model improvements transfer to English medical QA datasets (e.g., MedQA-USMLE) using the same intermediate fine-tuning pipeline.