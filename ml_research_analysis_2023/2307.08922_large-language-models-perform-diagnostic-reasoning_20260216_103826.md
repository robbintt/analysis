---
ver: rpa2
title: Large Language Models Perform Diagnostic Reasoning
arxiv_id: '2307.08922'
source_url: https://arxiv.org/abs/2307.08922
tags:
- patient
- have
- diagnosis
- doctor
- acute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diagnostic-Reasoning Chain-of-Thought (DR-CoT),
  a prompting technique that improves the diagnostic accuracy of large language models
  (LLMs) for automatic diagnosis (AD) by 15% compared to standard prompting. The key
  idea is to prompt the model to first summarize clinical evidence, formulate a differential
  diagnosis (DDx), and then ask questions based on the DDx to narrow down the diagnosis.
---

# Large Language Models Perform Diagnostic Reasoning

## Quick Facts
- arXiv ID: 2307.08922
- Source URL: https://arxiv.org/abs/2307.08922
- Reference count: 9
- Key outcome: DR-CoT improves diagnostic accuracy by 15% compared to standard prompting

## Executive Summary
This paper introduces Diagnostic-Reasoning Chain-of-Thought (DR-CoT), a prompting technique that significantly improves the diagnostic accuracy of large language models for automatic diagnosis by 15%. The approach mimics medical reasoning by having models first summarize clinical evidence, formulate differential diagnoses, and then ask targeted questions. The authors also introduce a novel language-model-role-playing evaluation framework where LLMs simulate both doctor and patient roles for realistic automated assessment. Experiments on the DDXPlus dataset show consistent improvements across in-domain and out-of-domain settings, with human evaluation confirming that DR-CoT enables more critical and relevant questioning.

## Method Summary
The method employs a two-shot prompting approach using InstructGPT (text-davinci-003) via OpenAI API. DR-CoT prompts explicitly instruct the model to first summarize clinical evidence, formulate a differential diagnosis (DDx), and then generate questions based on the DDx. A novel language-model-role-playing evaluation framework is introduced where LLMs simulate both doctor and patient roles, conducting automated evaluation through self-chat. The evaluation uses the DDXPlus dataset with 49 diagnoses and 223 types of evidence, testing performance across in-domain and out-of-domain settings.

## Key Results
- DR-CoT improves diagnostic accuracy by 15% compared to standard prompting
- Performance gains hold in out-of-domain settings with an 18% improvement
- Human evaluation confirms DR-CoT enables more critical and relevant questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DR-CoT prompt structure improves diagnostic accuracy by 15%
- Mechanism: Explicit instruction to summarize evidence, formulate DDx, and generate questions based on DDx guides targeted evidence gathering
- Core assumption: Medical reasoning process (evidence → DDx → questions) is suitable for LLMs
- Evidence anchors: Abstract shows 15% improvement; section 2 confirms striking 15% gain
- Break condition: Poor DDx formulation leads to misguided questions and poor performance

### Mechanism 2
- Claim: Role-playing evaluation provides realistic assessment
- Mechanism: LLMs simulate both doctor and patient roles, capturing dynamic diagnostic interactions
- Core assumption: Realistic patient-doctor interactions require free-text natural language handling
- Evidence anchors: Abstract describes language-model-role-playing framework; section 1 details self-chat evaluation
- Break condition: Patient bot generating unfaithful responses invalidates evaluation

### Mechanism 3
- Claim: Improvements hold for out-domain experiments, showing generalizable reasoning
- Mechanism: Consistent performance across in-domain and out-domain settings demonstrates generalizable diagnostic reasoning
- Core assumption: Generalization to unseen cases indicates model understanding of diagnostic process
- Evidence anchors: Abstract shows 18% gap in out-domain settings; section 1 confirms substantial gains
- Break condition: Model overfitting to specific exemplars prevents out-domain generalization

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: DR-CoT builds upon CoT principles for medical reasoning tasks
  - Quick check question: How does CoT prompting differ from standard prompting, and what are its key benefits in eliciting reasoning in LLMs?

- Concept: Differential diagnosis (DDx)
  - Why needed here: DR-CoT explicitly instructs model to formulate DDx based on evidence
  - Quick check question: What is the purpose of generating a DDx in the diagnostic process, and how does it guide subsequent inquiry?

- Concept: Role-playing in evaluation
  - Why needed here: Novel evaluation framework uses LLMs to simulate both doctor and patient roles
  - Quick check question: What are the advantages and potential limitations of using role-playing in evaluation of dialogue systems for automatic diagnosis?

## Architecture Onboarding

- Component map: InstructGPT API -> DR-CoT prompt template -> Patient bot prompt template -> Evaluation framework
- Critical path:
  1. Initialize dialogue with patient's AGE, SEX, and INITIAL EVIDENCE
  2. Generate doctor's response using DR-CoT prompt (evidence summary → DDx → questions)
  3. Generate patient response using patient bot prompt based on profile
  4. Append patient response to dialogue history and repeat until diagnosis or Tmax reached
  5. Evaluate performance based on final diagnosis and question quality
- Design tradeoffs: InstructGPT provides capability but may be costly; DR-CoT requires careful exemplar crafting; role-playing adds realism but complexity
- Failure signatures: Poor diagnostic accuracy (prompt issues); incoherent patient responses (patient bot issues); overfitting to exemplars (generalizability issues)
- First 3 experiments:
  1. Establish baseline with standard prompting on DDXPlus subset
  2. Implement DR-CoT and compare performance on same subset
  3. Conduct ablation study removing DDx formulation step

## Open Questions the Paper Calls Out

- Question: How does performance scale with number of exemplars provided?
  - Basis: Paper uses two exemplars but doesn't explore optimal number
  - Why unresolved: Only tests with two exemplars
  - What evidence would resolve it: Experiments with varying exemplar counts (1, 3, 5, 10) comparing accuracy and efficiency

- Question: Can DR-CoT apply to other medical reasoning tasks beyond diagnosis?
  - Basis: Paper demonstrates diagnosis effectiveness but doesn't test other tasks
  - Why unresolved: Focus solely on automatic diagnosis
  - What evidence would resolve it: Applying DR-CoT to treatment planning or prognosis prediction tasks

- Question: How does DR-CoT compare to other advanced prompting techniques?
  - Basis: Only compares to standard prompting, not chain-of-thought or tree-of-thought
  - Why unresolved: Limited comparison scope
  - What evidence would resolve it: Head-to-head comparison with other advanced prompting techniques

## Limitations

- Performance gains demonstrated primarily on single DDXPlus dataset, limiting generalizability
- Role-playing evaluation relies on quality of LLM simulation, potentially propagating model biases
- Two-shot prompting approach tested, but optimal number of exemplars for different domains unclear

## Confidence

- High confidence: 15% diagnostic accuracy improvement claim well-supported by consistent experimental results
- Medium confidence: Role-playing framework provides realistic evaluation, but needs validation against human evaluations
- Low confidence: Generalizability claim based on limited out-of-domain experiments requires further investigation

## Next Checks

1. Test DR-CoT prompting on additional medical diagnosis datasets beyond DDXPlus to verify generalizability
2. Conduct human evaluation studies comparing quality of questions generated by DR-CoT versus standard prompting
3. Perform ablation studies to determine contribution of each DR-CoT component to overall performance improvement