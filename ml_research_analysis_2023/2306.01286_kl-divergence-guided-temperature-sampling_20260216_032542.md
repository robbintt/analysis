---
ver: rpa2
title: KL-Divergence Guided Temperature Sampling
arxiv_id: '2306.01286'
source_url: https://arxiv.org/abs/2306.01286
tags:
- top-p
- baseline
- source
- guided
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing hallucinations in
  large language model (LLM) predictions while maintaining diversity, particularly
  in tasks involving source-grounded responses like conversational QA and summarization.
  The proposed method, KL-divergence Guided Temperature Sampling, dynamically adjusts
  the sampling temperature at each decoding step based on the relevance of the source
  document, as indicated by KL-divergence between token distributions with and without
  the source.
---

# KL-Divergence Guided Temperature Sampling

## Quick Facts
- arXiv ID: 2306.01286
- Source URL: https://arxiv.org/abs/2306.01286
- Reference count: 10
- Key outcome: Method achieves higher attribution (E2E NLI) for same diversity (var-rank, self-BLEU4) compared to top-k/top-p sampling in conversational QA and summarization

## Executive Summary
This paper addresses the challenge of reducing hallucinations in large language models while maintaining diversity in source-grounded responses. The proposed KL-divergence Guided Temperature Sampling dynamically adjusts the sampling temperature at each decoding step based on the relevance of the source document, as indicated by KL-divergence between token distributions with and without the source. Experiments on QReCC conversational QA and XLSum summarization datasets demonstrate that this approach outperforms conventional sampling algorithms in achieving better attribution without sacrificing diversity.

## Method Summary
The method uses parallel decoding with two model instances - one with source input and one without - to compute KL-divergence between their token probability distributions at each decoding step. This KL-divergence value is then converted to a temperature adjustment using an exponential decay function (T = T0 * exp(ln(0.5) * KL(p||q) / σ)), where σ controls the sensitivity. Lower temperatures are applied when the source is deemed relevant (high KL-divergence), improving attribution, while higher temperatures maintain diversity when the source is less relevant.

## Key Results
- Achieves higher attribution (E2E NLI) for the same level of diversity compared to top-k and top-p sampling
- Particularly significant improvements observed in conversational QA tasks
- Method works with both encoder-decoder models (mT5-XL) and decoder-only models (PaLM-8B/62B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-divergence between distributions with and without source indicates source relevance at each decoding step
- Mechanism: Compares token probability distributions from parallel decodings to measure how much source presence changes predictions
- Core assumption: Distribution difference reflects source relevance to current decoding step
- Evidence anchors: [abstract] proposes KL-divergence as guiding signal; [section] defines KL-divergence calculation using PMFs with/without source
- Break condition: When model has already learned source information and doesn't need it for generation

### Mechanism 2
- Claim: Dynamic temperature adjustment based on source relevance improves attribution without sacrificing diversity
- Mechanism: Converter function adjusts temperature exponentially based on KL-divergence - lower when source relevant, higher when irrelevant
- Core assumption: Temperature directly controls trade-off between attribution and diversity
- Evidence anchors: [abstract] shows sampling algorithm outperforms conventional methods; [section] uses temperature as knob for attributions
- Break condition: When temperature-attribution relationship becomes non-linear at extremes

### Mechanism 3
- Claim: Parallel decoding enables accurate KL-divergence calculation for source relevance detection
- Mechanism: Two parallel decodings allow direct comparison of token distributions to compute KL-divergence
- Core assumption: Parallel decoding doesn't significantly impact distribution quality
- Evidence anchors: [section] describes parallel decodings with/without source input
- Break condition: When computational overhead becomes prohibitive

## Foundational Learning

- Concept: KL-divergence as measure of distribution difference
  - Why needed here: Quantifies how much source affects model predictions at each decoding step
  - Quick check question: If two token distributions are identical, what is their KL-divergence? (Answer: 0)

- Concept: Temperature scaling in softmax
  - Why needed here: Controls sharpness of probability distribution for sampling
  - Quick check question: What happens to probability distribution when temperature approaches zero? (Answer: Becomes deterministic)

- Concept: Parallel decoding architecture
  - Why needed here: Required to compute KL-divergence by comparing distributions with and without source
  - Quick check question: Why can't we compute KL-divergence with single decoding pass? (Answer: Need both distributions simultaneously)

## Architecture Onboarding

- Component map: Input preprocessor -> Dual decoder -> KL-divergence calculator -> Temperature converter -> Sampling module -> Feedback to both decoders
- Critical path: 1) Prepare dual inputs 2) Run parallel decoding 3) Compute KL-divergence 4) Convert to temperature 5) Apply temperature and sample 6) Feed back to both decoders
- Design tradeoffs: Accuracy vs. computational cost (double decoding), Responsiveness vs. stability (adjustment speed), Source dependence vs. model autonomy
- Failure signatures: Consistently high KL-divergence regardless of source relevance, Temperature stuck at extremes, No attribution improvement despite high KL-divergence
- First 3 experiments: 1) Compare baseline vs guided sampling on QReCC with PaLM-8B 2) Test different σ values in converter function 3) Evaluate on summarization with modified training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the method consistently outperform conventional sampling across different model architectures and tasks beyond conversational QA and summarization?
- Basis in paper: [inferred] Method described as applicable to decoder-only models but only tested on specific tasks
- Why unresolved: No empirical evidence provided for other model types or task domains
- What evidence would resolve it: Experiments with decoder-only models on machine translation, code generation, and other question answering tasks

### Open Question 2
- Question: What is the impact on computational efficiency and inference time compared to conventional methods?
- Basis in paper: [explicit] Method requires twice the computations due to parallel decodings
- Why unresolved: No quantitative data on computational overhead or practical feasibility
- What evidence would resolve it: Measurements of inference time and computational cost across hardware setups and model sizes

### Open Question 3
- Question: How does choice of converter function affect performance?
- Basis in paper: [explicit] Uses exponential decay function but doesn't explore alternatives
- Why unresolved: No investigation of sensitivity to different converter functions or hyperparameters
- What evidence would resolve it: Experiments with different converter functions (linear, sigmoid) and their hyperparameters

## Limitations

- Method requires twice the computational resources due to parallel decoding, limiting practical deployment
- Assumes KL-divergence reliably indicates source relevance, but this depends on specific training conditions not fully met in all experiments
- Exponential temperature conversion function appears arbitrary without systematic justification or sensitivity analysis

## Confidence

- Medium confidence: Core mechanism of using KL-divergence to guide temperature adjustment - conceptually sound with empirical improvements on tested datasets
- Low confidence: Generalizability across different model architectures, task types, and training regimes - only tested on two specific datasets with two model families
- Medium confidence: Claim of outperforming conventional top-k and top-p sampling - convincing on tested scenarios but doesn't account for potential hyperparameter tuning

## Next Checks

1. Re-run PaLM experiments with models explicitly trained on examples containing empty source inputs to verify whether KL-divergence becomes a more reliable relevance signal

2. Measure wall-clock time and computational overhead of parallel decoding across different model sizes and sequence lengths, then calculate attribution improvement per unit of additional computation

3. Systematically vary the σ parameter in temperature conversion function across multiple orders of magnitude (0.01, 0.1, 1.0, 10.0) to test robustness to this choice