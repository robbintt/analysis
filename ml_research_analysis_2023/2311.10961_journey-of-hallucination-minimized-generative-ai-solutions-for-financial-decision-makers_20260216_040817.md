---
ver: rpa2
title: Journey of Hallucination-minimized Generative AI Solutions for Financial Decision
  Makers
arxiv_id: '2311.10961'
source_url: https://arxiv.org/abs/2311.10961
tags:
- data
- hallucinations
- llms
- generative
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework to minimize hallucinations in Generative
  AI systems used for financial decision making, where hallucinations can have detrimental
  impact. It proposes a novel Langchain-based system design with four custom modules
  to monitor and control hallucinations in LLM responses.
---

# Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers

## Quick Facts
- arXiv ID: 2311.10961
- Source URL: https://arxiv.org/abs/2311.10961
- Reference count: 15
- The paper presents a framework to minimize hallucinations in Generative AI systems used for financial decision making, proposing a novel Langchain-based system design with four custom modules to monitor and control hallucinations in LLM responses.

## Executive Summary
This paper addresses the critical challenge of hallucinations in Generative AI systems for financial decision making, where inaccurate responses can have significant consequences. The authors propose a three-stage development framework: prototyping with minimum viable product, scaling to handle diverse user queries while benchmarking LLM choices, and fine-tuning LLMs based on curated user queries and responses. The solution leverages a Langchain-based architecture with four custom modules—intent classification, data chunk generation and filtering, custom prompt generation, and response quality scoring—to systematically minimize hallucinations by controlling query processing and data presentation to the LLM.

## Method Summary
The paper proposes a three-stage development process for hallucination-minimized LLM products in numerical/analytical domains. Stage one focuses on prototyping with prompt engineering and modular builds to achieve business value. Stage two scales the system to handle diverse user queries while benchmarking different LLM choices. Stage three involves fine-tuning the LLMs using curated datasets of user queries and responses under reinforcement learning with human feedback (RLHF) principles. The framework uses Langchain to implement four custom modules: Question intention classification, Data chunk generation and filtering, Custom prompt generation, and Response quality scoring, which work together to minimize hallucinations by ensuring structured, granular data processing and quality evaluation of LLM responses.

## Key Results
- Proposes a novel Langchain-based framework with four custom modules to monitor and control hallucinations in LLM responses for financial applications
- Establishes a three-stage development process (prototype, scale, evolve) for building hallucination-minimized LLM products
- Introduces a Response Quality Scoring module that evaluates responses across contextual, numeric, uniqueness, and grammatical accuracy dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular prompt engineering reduces hallucinations by controlling how queries are processed and how data is presented to the LLM.
- Mechanism: The framework separates query intent classification, data chunking, custom prompt generation, and response quality scoring into distinct modules, ensuring each step is optimized for accuracy before the query reaches the LLM.
- Core assumption: Structured, granular data chunks and intent-specific prompts limit the LLM's exposure to ambiguous or noisy inputs that typically cause hallucinations.
- Evidence anchors:
  - [abstract] "Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context."
  - [section] "Each data table value is converted to sentences and stored as 'data chunks'. Data chunks are further hierarchically categorized to support aggregated querying."
  - [corpus] Weak/no direct evidence in corpus; framework novelty not discussed in neighbors.
- Break condition: If the data chunk generation or filtering module fails to isolate relevant context, hallucinations may still occur due to ambiguous or incomplete inputs.

### Mechanism 2
- Claim: Response quality scoring using standardized NLP metrics can reliably flag hallucinations in financial contexts.
- Mechanism: The response scoring module evaluates each LLM response across four dimensions—contextual accuracy, numeric accuracy, uniqueness, and grammatical accuracy—and assigns a confidence level to each response.
- Core assumption: The four binary quality scoring modules can capture hallucination risks without requiring human-in-the-loop validation for every response.
- Evidence anchors:
  - [section] "This novel component evaluates the question, the prompt sent to the LLM and the returned response together and evaluates the response for contextual, numeric, and uniqueness and grammatical accuracy."
  - [abstract] "We present a novel framework that minimizes and controls hallucinations for such numerical and data table interactions to generate reliable and accurate answers for decision making tasks."
  - [corpus] No corpus evidence found; this appears to be an original contribution.
- Break condition: If the scoring model is too coarse or tuned to general language rather than finance-specific numeric facts, it may miss subtle hallucinations in complex financial reasoning.

### Mechanism 3
- Claim: Fine-tuning LLMs with curated human feedback ensures long-term reliability as the system scales.
- Mechanism: After initial prototyping and scaling, the LLM is fine-tuned using a curated dataset of user queries and human-vetted responses under reinforcement learning with human feedback (RLHF) principles.
- Core assumption: Human feedback on domain-specific queries can substantially improve LLM accuracy in finance without introducing new bias.
- Evidence anchors:
  - [abstract] "The third and final stage of the solution is fine-tuning the LLMs based on an already curated set of user-queries and sample responses to ensure evolution in the question answering capabilities in accordance with reinforcement learning with human feedback (RLHF) criteria."
  - [section] "In the first prototyping stage, business case value realization drives the build plan of the minimum viable product."
  - [corpus] No corpus evidence; RLHF usage in finance LLM tuning not discussed in neighbors.
- Break condition: If curated datasets are not representative of real-world queries or contain inherent bias, fine-tuning could degrade performance or introduce new hallucinations.

## Foundational Learning

- Concept: Prompt Engineering for Numerical Contexts
  - Why needed here: Financial queries combine numerical data with language; improper prompt structure leads to LLM hallucination.
  - Quick check question: How would you design a prompt to ensure an LLM returns a percentage figure with no misinterpretation?

- Concept: Intent Classification in Multi-Intent Queries
  - Why needed here: Correctly categorizing queries as 'why', 'what', 'how', 'trend', etc. ensures appropriate processing logic and data selection.
  - Quick check question: What steps would you take if the intent classifier mislabels a 'what-if' query as a 'trend' query?

- Concept: Data Chunking and Ranking for Tabular Data
  - Why needed here: LLMs require text input; transforming tables into ranked, context-aware sentence chunks improves relevance and reduces noise.
  - Quick check question: How would you design a chunk ranking algorithm that prioritizes context relevance in financial tables?

## Architecture Onboarding

- Component map:
  Intent Classification -> Data Chunk Generation & Filtering -> Custom Prompt Generation -> LLM -> Response Quality Scoring -> Output + Confidence
- Critical path:
  Intent Classification -> Custom Prompt Generation -> LLM -> Response Quality Scoring
- Design tradeoffs:
  - Granularity vs. performance: More granular data chunks improve accuracy but increase runtime.
  - Model size vs. latency: Larger LLMs may reduce hallucinations but slow response time.
- Failure signatures:
  - High rate of low-confidence responses → intent classification or data chunk relevance issues.
  - Frequent hallucinations in numeric answers → response scoring metrics too lenient or LLM fine-tuning insufficient.
- First 3 experiments:
  1. Validate intent classification accuracy using a benchmark set of financial queries.
  2. Measure hallucination rate with and without data chunk filtering on a controlled numeric dataset.
  3. Benchmark response quality scoring against human-labeled hallucination detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal granularity level for data chunks when converting tabular data to sentences for LLM processing?
- Basis in paper: [explicit] The paper mentions that "Lack of granularity in data chunks can cause hallucinations" and discusses converting tabular data to sentences and storing them as "data chunks" with hierarchical categorization.
- Why unresolved: The paper identifies the importance of data chunk granularity but doesn't specify what level of granularity is optimal for minimizing hallucinations while maintaining processing efficiency.
- What evidence would resolve it: Systematic experiments testing different levels of data chunk granularity (e.g., individual cell level, row level, aggregated metrics) to measure hallucination rates and query response accuracy.

### Open Question 2
- Question: How can we effectively measure and quantify the severity of hallucinations in LLM responses for numerical and analytical domains?
- Basis in paper: [explicit] The paper discusses hallucination monitoring but doesn't provide specific metrics for quantifying hallucination severity, only mentioning a "Response quality scoring module" that evaluates responses on a low/medium/high confidence scale.
- Why unresolved: The paper identifies the need for hallucination control but doesn't establish a standardized measurement system for hallucination severity, which is crucial for decision-making contexts.
- What evidence would resolve it: Development and validation of quantitative hallucination severity metrics that can distinguish between minor inaccuracies and major factual errors in numerical/analytical responses.

### Open Question 3
- Question: What is the optimal balance between prompt customization and response time in the custom prompt generation module?
- Basis in paper: [explicit] The paper states that "Filtering for the 'most similar' data chunks per user query is necessary to minimize hallucinations" and mentions the need for "a customized data chunk ranking mechanism that is optimized for run-time."
- Why unresolved: The paper identifies the trade-off between customization and performance but doesn't specify how to optimize this balance for different use cases or user query volumes.
- What evidence would resolve it: Performance benchmarks comparing different prompt customization strategies with their corresponding response times and hallucination rates across various query volumes and types.

## Limitations
- The paper lacks empirical validation with quantitative performance metrics or head-to-head comparisons with baseline systems
- Implementation details for the Response Quality Scoring module remain underspecified, particularly the exact thresholds and metrics used
- The paper does not address potential biases introduced during the fine-tuning phase or provide evidence that human-curated datasets can scale effectively

## Confidence
- Mechanism 1 (Modular prompt engineering): Medium - Logical but not empirically validated
- Mechanism 2 (Response quality scoring): Low - Novel component but lacks technical specification
- Mechanism 3 (Fine-tuning with human feedback): Medium - Well-established approach but scalability concerns unaddressed

## Next Checks
1. Benchmark hallucination reduction: Test the complete system against a baseline LLM on a standardized financial Q&A dataset, measuring hallucination rates with and without the modular framework.
2. Validate scoring module sensitivity: Create a test suite of financial queries with known hallucination risks and evaluate whether the Response Quality Scoring module correctly flags them across all four dimensions.
3. Assess fine-tuning impact: Compare LLM performance before and after fine-tuning using a held-out test set of financial queries not present in the training data to measure generalization and identify potential overfitting.