---
ver: rpa2
title: Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models
arxiv_id: '2311.10883'
source_url: https://arxiv.org/abs/2311.10883
tags:
- object
- semantic
- segmentation
- dataset
- maskformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for labeling indoor scenes using a
  fusion of out-of-the-box perception models. The approach leverages SAM for class-agnostic
  segmentation, Detic for object detection, and MaskFormer for semantic segmentation
  to generate pseudo-labels for semantic segmentation and object instance detection.
---

# Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models

## Quick Facts
- arXiv ID: 2311.10883
- Source URL: https://arxiv.org/abs/2311.10883
- Reference count: 39
- This paper presents a method for labeling indoor scenes using a fusion of out-of-the-box perception models, achieving improved performance compared to human annotations.

## Executive Summary
This paper introduces a method for labeling indoor scenes using a fusion of out-of-the-box perception models. The approach leverages SAM for class-agnostic segmentation, Detic for object detection, and MaskFormer for semantic segmentation to generate pseudo-labels for semantic segmentation and object instance detection. A multi-view labeling fusion stage is also introduced to rectify inconsistencies in single-view annotations using multiple views of the scene. The method is evaluated on the Active Vision Dataset (A VD) and ADE20K dataset, demonstrating improved performance compared to human annotations. The proposed approach is effective for downstream tasks such as object goal navigation and part discovery.

## Method Summary
The method fuses SAM, Detic, and MaskFormer models to generate pseudo-labels for semantic segmentation and object instance detection. SAM provides class-agnostic segmentation initialized with Detic's object detection bounding boxes. MaskFormer handles background classes while Detic detects foreground objects. The multi-view labeling fusion stage projects segmentation masks from multiple reference views into 3D space and votes per-region to correct single-view errors caused by occlusions or viewpoint inconsistencies.

## Key Results
- Multi-view fusion improves semantic segmentation performance by 1.1% in mIoU and 3.9% in mIoU-small compared to the single-view approach on A VD
- For object goal navigation, the method achieves a success rate of 89.4%, outperforming the zero-shot baseline by 10.5%
- The approach demonstrates improved performance compared to human annotations on both A VD and ADE20K datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-view fusion resolves occlusions and viewpoint inconsistencies in object labeling.
- Mechanism: By projecting segmentation masks from multiple reference views into 3D and voting per-region, the method can correct single-view errors caused by partial occlusions or unusual angles.
- Core assumption: At least one reference view has a clearer, less occluded view of each object instance.
- Evidence anchors:
  - [section] "With the assumption that models perform well in most scenarios and that objects are less occluded, at least in some viewpoints, results from such views can rectify these prediction errors."
  - [section] "We begin by projecting the annotations Am and An into a 3D spatial context, utilizing the respective image poses [Rm|Tm] and [Rn|Tn] and their depth maps."
  - [corpus] No corpus evidence directly addresses multi-view consistency for segmentation; the paper provides its own method.
- Break condition: If all available viewpoints are occluded or angles are similarly poor, fusion will not correct errors.

### Mechanism 2
- Claim: SAM provides high-quality object boundaries when initialized with detection bounding boxes.
- Mechanism: Detic's object detections give coarse localization, and SAM refines this into precise masks using the bounding boxes as input prompts.
- Core assumption: SAM's generalization to unseen objects allows accurate mask generation even for objects not in its training set.
- Evidence anchors:
  - [section] "We utilize SAM to generate high-quality masks by using the bounding boxes as input prompts."
  - [section] "In practice, we prompt SAM with bounding boxes and a point corresponding to the centroid of the masks from Detic, leading to high-quality object instance segmentation."
  - [corpus] No direct corpus comparison of SAM with detection bounding boxes; paper presents its own evaluation.
- Break condition: If the detection bounding box is significantly inaccurate or SAM fails to generalize to the object type, mask quality degrades.

### Mechanism 3
- Claim: Combining semantic segmentation and object detection improves labeling of both background and foreground classes.
- Mechanism: MaskFormer labels large background areas well, Detic detects foreground objects, and SAM refines masks; combining these gives a more complete segmentation.
- Core assumption: The vocabulary overlap between MaskFormer (ADE20K) and Detic (LVIS) covers most indoor objects, and SAM can bridge gaps.
- Evidence anchors:
  - [section] "MaskFormer produces good predictions for background classes but not so well for foreground classes... To address these shortcomings, we employ Detic with LVIS vocabulary..."
  - [section] "The final result is obtained by superimposing foreground class masks, obtained from the use of object detection and SAM, on top of semantic segmentation results."
  - [corpus] No external corpus evidence comparing this specific fusion approach; results are presented within the paper.
- Break condition: If Detic or SAM misclassifies an object, the error propagates into the final label; reliance on voting is crucial.

## Foundational Learning

- Concept: Vision Transformer architectures and their role in segmentation.
  - Why needed here: SAM, MaskFormer, and Detic are all based on Vision Transformers, so understanding their mechanisms helps debug and adapt them.
  - Quick check question: What is the main difference between the decoder design in MaskFormer versus a standard semantic segmentation model?

- Concept: Multi-view geometry and camera pose projection.
  - Why needed here: Multi-view fusion requires projecting 2D masks into 3D space and back into another view; errors here cause mislabeling.
  - Quick check question: How do you transform a 2D pixel coordinate from one view to another using known camera poses and depth?

- Concept: Open-vocabulary models and text embeddings for object detection.
  - Why needed here: Detic uses CLIP text embeddings to detect objects beyond a fixed vocabulary, enabling discovery of novel indoor objects.
  - Quick check question: How does CLIP-based detection differ from traditional class-based detection in handling unseen categories?

## Architecture Onboarding

- Component map: RGB-D images -> Detic (object detection) + MaskFormer (semantic segmentation) -> SAM (mask refinement) -> overlay masks -> (optional) multi-view fusion -> final pseudo-labels
- Critical path: Detic detection -> SAM mask generation -> voting-based overlay with MaskFormer background labels; multi-view fusion is optional but improves accuracy
- Design tradeoffs: Multi-view fusion increases accuracy but requires depth and camera poses; without them, the single-view approach must rely on stronger per-frame predictions
- Failure signatures: Misaligned bounding boxes from Detic, SAM failing on novel objects, or incorrect voting due to inconsistent reference views
- First 3 experiments:
  1. Run Detic + SAM on a single RGB image and compare mask quality to ground truth
  2. Overlay MaskFormer background labels with the Detic+SAM foreground masks and evaluate mIoU
  3. Apply the multi-view fusion stage on a scene with known pose/depth and measure improvement in small object detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach handle long-tail objects and categories not present in the training data of the base models?
- Basis in paper: [explicit] The paper mentions that indoor environments have diverse object types and open-set objects, and that MaskFormer is trained on ADE20K which lacks many common indoor objects like 'cooking pot' and 'knife'. It also states that the Detic model can recognize objects beyond predefined categories.
- Why unresolved: The paper does not provide a detailed analysis of the performance of the approach on long-tail objects or objects not present in the training data of the base models. It also does not discuss any strategies employed to specifically handle such cases.
- What evidence would resolve it: Experiments comparing the performance of the approach on long-tail objects and objects not present in the training data of the base models with other methods that handle such cases, such as few-shot learning or meta-learning approaches.

### Open Question 2
- Question: How does the multi-view verification stage improve the labeling results compared to the single-view approach?
- Basis in paper: [explicit] The paper introduces a multi-view labeling fusion stage that considers multiple views of the scenes to identify and rectify single-view inconsistencies. It claims that this approach improves semantic segmentation performance by 1.1% in mIoU and 3.9% in mIoU-small compared to the single-view approach on A VD.
- Why unresolved: The paper does not provide a detailed analysis of the specific types of errors that are corrected by the multi-view verification stage, or how the performance improvement varies across different object categories or scenes.
- What evidence would resolve it: A detailed analysis of the types of errors corrected by the multi-view verification stage, along with a breakdown of the performance improvement across different object categories and scenes.

### Open Question 3
- Question: How does the proposed approach compare to other state-of-the-art methods for semantic segmentation and object detection in indoor environments?
- Basis in paper: [explicit] The paper mentions that the proposed approach is evaluated on the Active Vision Dataset (A VD) and ADE20K dataset, and demonstrates improved performance compared to human annotations. It also mentions that the approach is effective for downstream tasks such as object goal navigation and part discovery.
- Why unresolved: The paper does not provide a direct comparison with other state-of-the-art methods for semantic segmentation and object detection in indoor environments, such as those based on 3D point clouds or multi-view stereo.
- What evidence would resolve it: Experiments comparing the performance of the proposed approach with other state-of-the-art methods for semantic segmentation and object detection in indoor environments on the same datasets.

## Limitations
- The approach relies on the availability of camera poses and depth maps for multi-view fusion, which may not be available in all indoor environments
- The method assumes that at least one viewpoint provides an unoccluded view of each object, which may not hold in cluttered or densely packed scenes
- The performance depends on the quality of the out-of-the-box models (SAM, Detic, MaskFormer), and any biases or limitations in these models will propagate to the final labeling

## Confidence
- **High confidence**: The multi-view fusion mechanism for correcting single-view errors is well-supported by the paper's methodology and evaluation results
- **Medium confidence**: The effectiveness of combining SAM with Detic detections for mask refinement is demonstrated, but lacks direct external validation
- **Medium confidence**: The claim that this approach outperforms human annotations is supported by the paper's results but would benefit from independent replication

## Next Checks
1. Test the single-view labeling pipeline on a held-out subset of AVD to verify that SAM+SAM+Detic achieves the claimed mIoU scores before multi-view fusion
2. Create a synthetic dataset with known ground truth where some views are deliberately occluded to test whether multi-view fusion actually recovers the occluded objects
3. Evaluate the sensitivity of the final labeling quality to errors in camera pose estimation by adding controlled noise to the input poses and measuring degradation in mIoU