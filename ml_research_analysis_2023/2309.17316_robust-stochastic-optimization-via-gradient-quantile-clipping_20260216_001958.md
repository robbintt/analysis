---
ver: rpa2
title: Robust Stochastic Optimization via Gradient Quantile Clipping
arxiv_id: '2309.17316'
source_url: https://arxiv.org/abs/2309.17316
tags: []
core_contribution: This paper introduces a clipped stochastic gradient descent (SGD)
  method that uses quantiles of gradient norms as adaptive clipping thresholds. The
  method is robust to heavy-tailed gradients and data corruption, handling Huber contamination
  in the data stream.
---

# Robust Stochastic Optimization via Gradient Quantile Clipping

## Quick Facts
- arXiv ID: 2309.17316
- Source URL: https://arxiv.org/abs/2309.17316
- Reference count: 40
- Introduces QC-SGD: clipped SGD using gradient norm quantiles as adaptive thresholds for robustness to heavy-tailed gradients and data corruption

## Executive Summary
This paper introduces a robust stochastic gradient descent method that uses quantiles of gradient norms as adaptive clipping thresholds. The approach addresses the challenge of optimizing objectives with heavy-tailed gradients and corrupted data streams by dynamically adjusting the clipping threshold based on the empirical distribution of gradient norms. The method maintains theoretical guarantees for both strongly convex and non-convex objectives while providing efficient implementation through rolling quantile estimation.

## Method Summary
QC-SGD is a variant of stochastic gradient descent that adapts its clipping threshold based on quantiles of gradient norms rather than using fixed thresholds. The algorithm maintains a rolling quantile buffer of recent gradient norms to estimate the clipping threshold efficiently. For strongly convex objectives, the method achieves geometric ergodicity, converging to a concentrated invariant distribution. For non-convex objectives, it converges to a distribution localized around low-gradient regions. The implementation uses O(d) memory complexity and is theoretically robust to Huber contamination in the data stream.

## Key Results
- QC-SGD demonstrates superior performance compared to existing robust methods on synthetic tasks, especially under increasing corruption levels
- Theoretical analysis shows geometric ergodicity for strongly convex objectives with high probability bounds on estimation error
- The rolling quantile implementation achieves O(d) memory complexity while maintaining strong robustness properties
- The method handles both heavy-tailed gradients and data corruption effectively through adaptive threshold adjustment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantile clipping provides robustness to both heavy-tailed gradients and outliers in the data stream by dynamically adjusting the clipping threshold based on the empirical distribution of gradient norms.
- Mechanism: The algorithm maintains a rolling quantile estimate of gradient norms, which is used as the adaptive clipping threshold. This allows the method to scale the threshold according to the current gradient distribution, handling both heavy tails and corrupted samples effectively.
- Core assumption: The rolling quantile estimate accurately reflects the underlying distribution of uncorrupted gradient norms.
- Evidence anchors:
  - [abstract] "We introduce a clipping strategy for Stochastic Gradient Descent (SGD) which uses quantiles of the gradient norm as clipping thresholds."
  - [section] "We propose an implementation of this algorithm using rolling quantiles which leads to a highly efficient optimization procedure with strong robustness properties, as confirmed by our numerical experiments."
  - [corpus] "On the Convergence of DP-SGD with Adaptive Clipping" - discusses adaptive clipping in private optimization, supporting the concept of dynamic thresholds.
- Break condition: If the rolling quantile estimate is biased due to persistent corruption or insufficient buffer size, the clipping threshold may not reflect the true gradient distribution.

### Mechanism 2
- Claim: Geometric ergodicity of the Markov chain generated by QC-SGD ensures convergence to a concentrated invariant distribution for strongly convex objectives.
- Mechanism: By leveraging the connection between constant step size SGD and Markov chains, the algorithm achieves geometric ergodicity under certain conditions on the quantile index and step size. This results in a concentrated invariant distribution around the optimum.
- Core assumption: The Markov chain satisfies the drift condition required for geometric ergodicity.
- Evidence anchors:
  - [abstract] "For strongly convex objectives, we prove that the iteration converges to a concentrated distribution and derive high probability bounds on the final estimation error."
  - [section] "The proof of Theorem 1 is given in Appendix C.2 and relies on the geometric ergodicity result of [58, Chapter 15] for Markov chains with a geometric drift property."
  - [corpus] "High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise" - discusses convergence under heavy-tailed noise, supporting the robustness claim.
- Break condition: If the drift condition is violated due to extreme corruption or poor choice of parameters, geometric ergodicity may not hold.

### Mechanism 3
- Claim: The rolling quantile procedure enables efficient implementation of QC-SGD with O(d) memory and complexity.
- Mechanism: By maintaining a buffer of recent gradient norms and using a bookkeeping procedure, the algorithm computes the quantile estimate efficiently. This allows for practical implementation without significant computational overhead.
- Core assumption: The rolling quantile estimate is a good proxy for the true quantile of the gradient norm distribution.
- Evidence anchors:
  - [abstract] "We propose an implementation of this algorithm using rolling quantiles which leads to a highly efficient optimization procedure with strong robustness properties."
  - [section] "We implement this procedure for a few tasks and compare its performance with relevant baselines."
  - [corpus] "Online Inference for Quantiles by Constant Learning-Rate Stochastic Gradient Descent" - discusses online quantile estimation, supporting the feasibility of the approach.
- Break condition: If the buffer size is too small or the gradient distribution changes rapidly, the rolling quantile estimate may become unreliable.

## Foundational Learning

- Concept: Markov chain theory and ergodicity
  - Why needed here: The convergence analysis of QC-SGD relies on viewing the algorithm as a Markov chain and proving geometric ergodicity.
  - Quick check question: What are the key conditions required for a Markov chain to be geometrically ergodic?

- Concept: Robust statistics and heavy-tailed distributions
  - Why needed here: The algorithm is designed to handle heavy-tailed gradients and data corruption, requiring an understanding of robust estimation techniques.
  - Quick check question: How do quantile-based estimators provide robustness against outliers?

- Concept: Stochastic optimization and gradient clipping
  - Why needed here: The algorithm is a variant of SGD with gradient clipping, requiring knowledge of how clipping affects optimization dynamics.
  - Quick check question: What are the trade-offs between different clipping strategies in SGD?

## Architecture Onboarding

- Component map:
  Gradient computation -> Rolling quantile buffer update -> Clipping threshold calculation -> Parameter update

- Critical path:
  1. Compute gradient norm
  2. Update rolling quantile buffer
  3. Calculate clipping threshold
  4. Apply clipping to gradient
  5. Update parameters
  6. Check convergence

- Design tradeoffs:
  - Buffer size vs. memory usage and quantile accuracy
  - Quantile index choice vs. robustness and convergence speed
  - Step size vs. convergence rate and bias

- Failure signatures:
  - Divergence or oscillation in parameter updates
  - Slow convergence or getting stuck in poor local minima
  - High variance in gradient estimates

- First 3 experiments:
  1. Implement QC-SGD with synthetic heavy-tailed gradients and measure convergence rate
  2. Test robustness to increasing levels of data corruption in a linear regression task
  3. Compare QC-SGD with constant clipping thresholds on a logistic regression problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of quantile p in QC-SGD affect the trade-off between convergence speed and robustness to outliers?
- Basis in paper: [explicit] The paper discusses the choice of p ∈ [η, 1 − η] and its impact on convergence conditions, particularly in Theorem 1 and Proposition 1.
- Why unresolved: The paper mentions that the dependence on p is intricate and should be evaluated through the resulting value of κ, but does not provide a detailed analysis of the trade-off.
- What evidence would resolve it: A detailed analysis of the convergence speed and robustness as a function of p, possibly through empirical studies or theoretical bounds, would clarify this trade-off.

### Open Question 2
- Question: Can the dimension dependence of the bounds in Proposition 1 be improved?
- Basis in paper: [inferred] The paper mentions that the poor dimension dependence through Bq may still be improved, especially if the gradient is sub-Gaussian.
- Why unresolved: The paper does not explore methods to improve the dimension dependence, such as using sample rejection rules or stochastic mirror descent.
- What evidence would resolve it: Developing and analyzing methods that reduce the dimension dependence, possibly through advanced techniques like mirror descent, would address this question.

### Open Question 3
- Question: What is the precise quantification of the geometric convergence speed of the Markov chain generated by constant step size SGD on a strongly convex objective?
- Basis in paper: [explicit] The paper mentions that the precise quantification of the geometric convergence speed is an interesting research track, as discussed in Appendix B.
- Why unresolved: The paper provides a general framework for convergence but does not provide a precise quantification of the speed, which depends on the step size β and quantile p.
- What evidence would resolve it: A detailed theoretical analysis or empirical study that quantifies the convergence speed as a function of β and p would resolve this question.

## Limitations

- Theoretical analysis is restricted to strongly convex objectives with specific conditions on quantile parameter p and step size β
- For non-convex objectives, convergence claims rely on weaker notions of stationarity without rigorous proofs
- Practical performance depends heavily on hyperparameter tuning (buffer size, quantile index, uniform threshold) which is not systematically explored
- Scalability to high-dimensional problems and different data distributions remains unclear

## Confidence

- Geometric ergodicity for strongly convex objectives: **Medium** - Proven under specific conditions but practical verification is limited
- Robustness to heavy-tailed gradients and contamination: **High** - Supported by both theory and experiments
- Efficiency of rolling quantile implementation: **Medium** - O(d) complexity claimed but empirical verification limited to specific tasks
- Superior performance over baselines: **Medium** - Demonstrated on synthetic tasks but real-world applications not explored

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically evaluate the impact of buffer size S, quantile parameter p, and uniform threshold τunif on convergence and robustness across different optimization tasks and corruption levels.

2. **Scalability Testing**: Implement QC-SGD on high-dimensional datasets (e.g., ImageNet, CIFAR) and compare computational efficiency and memory usage against baselines, particularly focusing on the O(d) memory claim.

3. **Real-World Robustness Evaluation**: Test the method on real-world datasets with known corruption or noise issues (e.g., noisy labels, sensor malfunctions) to validate practical applicability beyond synthetic experiments.