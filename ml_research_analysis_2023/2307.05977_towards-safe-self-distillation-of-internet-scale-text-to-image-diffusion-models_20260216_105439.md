---
ver: rpa2
title: Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models
arxiv_id: '2307.05977'
source_url: https://arxiv.org/abs/2307.05977
tags:
- images
- diffusion
- concept
- concepts
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a self-distillation method called SDD to remove
  harmful or copyrighted content from text-to-image diffusion models. The key idea
  is to fine-tune the model to guide the noise estimate conditioned on the target
  removal concept to match the unconditional one.
---

# Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2307.05977
- Source URL: https://arxiv.org/abs/2307.05977
- Authors: 
- Reference count: 40
- The paper proposes SDD, a self-distillation method that removes harmful or copyrighted content from text-to-image diffusion models by matching noise estimates, achieving significant concept removal while maintaining image quality.

## Executive Summary
This paper introduces Self-Distillation for Diffusion models (SDD), a novel approach to removing harmful or copyrighted concepts from text-to-image diffusion models. The method fine-tunes the model to make the noise estimate conditioned on a target removal concept match the unconditional noise estimate, effectively removing the concept while preserving image quality. By using an exponential moving average (EMA) teacher model, SDD mitigates catastrophic forgetting during fine-tuning. The approach is evaluated on two tasks: removing NSFW content and artist concepts from generated images.

## Method Summary
SDD employs self-distillation to remove target concepts by fine-tuning the diffusion model's noise estimates. The key innovation is matching the noise estimate conditioned on the target concept to the unconditional noise estimate through L2 distance minimization. An EMA teacher model is used to prevent catastrophic forgetting during this process. The method fine-tunes only the cross-attention layers of the UNet architecture, reducing computational overhead. For multi-concept removal, SDD randomly selects a single concept for generating intermediate latents to prevent interference between concepts. The training procedure involves 1,500 iterations with a learning rate of 1e-5 and EMA momentum of 0.999.

## Key Results
- SDD reduces nudity in images generated with the prompt "<country> body" from 74.18% to 1.68% compared to the original Stable Diffusion model.
- The method removes artist styles from generated images with minimal impact on other concepts, achieving higher %REMOVAL scores than baseline methods.
- SDD maintains image quality with only slight degradation in FID (0.72) and LPIPS (0.0048) scores compared to the baseline model.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matching the noise estimate conditioned on the target concept to the unconditional noise estimate removes the concept while preserving image quality.
- Mechanism: By minimizing the L2 distance between the conditional and unconditional noise estimates, the model learns to generate images without relying on the target concept's influence in the latent space.
- Core assumption: The unconditional noise estimate contains sufficient information to generate high-quality images without the target concept.
- Evidence anchors:
  - [abstract]: "We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one."
  - [section 3.1]: "We fine-tune the model through self-distillation for the noise estimate conditioned on the target removal concept to follow the unconditional one."
- Break condition: If the unconditional noise estimate inherently relies on the target concept for high-quality generation, this mechanism will fail.

### Mechanism 2
- Claim: Using an EMA teacher model prevents catastrophic forgetting during fine-tuning.
- Mechanism: The EMA teacher model gradually incorporates the fine-tuned student model's parameters, allowing the model to retain knowledge about other concepts while learning to remove the target concept.
- Core assumption: Gradual parameter updates through EMA are sufficient to preserve knowledge about non-target concepts.
- Evidence anchors:
  - [section 3.1]: "To mitigate catastrophic forgetting, we employ an exponential moving average (EMA) teacher."
  - [section 3.2]: "Moreover, the utilization of EMA contributes to preventing catastrophic forgetting by allowing the model parameters to be gradually updated."
- Break condition: If the EMA update rate is too slow, the student model may forget other concepts before the teacher model can incorporate the new knowledge.

### Mechanism 3
- Claim: Randomly selecting a single concept for generating intermediate latents during multi-concept removal prevents interference between concepts.
- Mechanism: By using only one concept at a time to generate the intermediate latent, the model can focus on removing each concept individually without the noise estimates canceling each other out.
- Core assumption: The CLIP embeddings used for conditioning can still effectively detect the target concept even when other concepts are present in the prompt.
- Evidence anchors:
  - [section 3.3]: "Because CFG considers guidance in the opposite direction of inappropriate concepts, using this aggregated noise estimate as a target may result in multiple concepts canceling each other out in the model's noise space."
- Break condition: If the CLIP embeddings are not robust enough to distinguish between multiple concepts in the prompt, this mechanism may fail to effectively remove all target concepts.

## Foundational Learning

- Concept: Diffusion models and latent space representations
  - Why needed here: Understanding how diffusion models generate images in latent space is crucial for grasping how the noise estimate matching works.
  - Quick check question: How does a diffusion model gradually denoise an image in latent space to generate a final output?

- Concept: Exponential moving average (EMA) in neural network training
  - Why needed here: EMA is used to prevent catastrophic forgetting by gradually updating the teacher model's parameters.
  - Quick check question: How does EMA help in preserving knowledge about previously learned concepts during fine-tuning?

- Concept: Cross-attention mechanisms in transformer models
  - Why needed here: The method fine-tunes only the cross-attention layers, which are crucial for concept removal in text-to-image models.
  - Quick check question: How do cross-attention mechanisms in transformers allow for conditioning on text prompts during image generation?

## Architecture Onboarding

- Component map: Text encoder (CLIP) -> Noise estimator (UNet) -> Latent space -> Image decoder
- Critical path: 1. Generate intermediate latent using EMA teacher model conditioned on a single concept; 2. Calculate L2 distance between conditional and unconditional noise estimates; 3. Backpropagate gradients to update student model; 4. Update EMA teacher model parameters
- Design tradeoffs:
  - Fine-tuning only cross-attention layers vs. full model: Reduces computational cost but may limit the extent of concept removal
  - Using EMA vs. direct fine-tuning: Prevents catastrophic forgetting but may slow down the concept removal process
- Failure signatures:
  - Geometric patterns or monochromatic backgrounds in generated images (overfitting)
  - Residual traces of the target concept in generated images
  - Significant degradation in image quality or other concepts
- First 3 experiments:
  1. Verify concept removal on a single, well-defined concept (e.g., "nudity") using the NudeNet classifier
  2. Test multi-concept removal on a small set of related concepts to ensure no interference
  3. Compare FID and LPIPS scores between the original model and SDD to ensure minimal impact on image quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of SDD compare to other methods when removing multiple harmful concepts simultaneously?
- Basis in paper: [explicit] The paper states that SDD allows for the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time. The paper also shows that SDD still exhibits superior performance in removing nudity and inappropriate images when removing all 20 concepts of I2P simultaneously.
- Why unresolved: The paper only compares SDD to other methods for removing a single concept (nudity) or multiple concepts (I2P). It does not provide a direct comparison of SDD's performance to other methods for removing multiple concepts other than I2P.
- What evidence would resolve it: A direct comparison of SDD's performance to other methods for removing multiple concepts other than I2P, using the same evaluation metrics (e.g., %NUDE, %HARM, FID, LPIPS, CLIP score).

### Open Question 2
- Question: How does the performance of SDD vary with the number of iterations during training?
- Basis in paper: [explicit] The paper mentions that the authors trained SDD for 1,500 iterations and that the images did not undergo significant changes even when being trained for 2,000 iterations. However, it also states that the student model eliminates the target concept at the early training stage but easily degrades the image quality.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of SDD varies with the number of iterations during training. It does not show the training curves for SDD or compare its performance at different iteration counts.
- What evidence would resolve it: Training curves for SDD showing the performance metrics (e.g., %NUDE, %HARM, FID, LPIPS, CLIP score) at different iteration counts, and a comparison of SDD's performance at different iteration counts.

### Open Question 3
- Question: How does the performance of SDD compare to other methods when removing concepts other than nudity and inappropriate content?
- Basis in paper: [explicit] The paper only evaluates SDD's performance for removing nudity and inappropriate content. It does not provide any results for removing other concepts.
- Why unresolved: The paper does not provide any results for removing concepts other than nudity and inappropriate content. It does not show how SDD's performance compares to other methods for removing other concepts.
- What evidence would resolve it: A comparison of SDD's performance to other methods for removing concepts other than nudity and inappropriate content, using the same evaluation metrics (e.g., %NUDE, %HARM, FID, LPIPS, CLIP score).

## Limitations
- The evaluation relies heavily on automatic metrics rather than human perceptual studies, which may not fully capture concept removal effectiveness and image quality.
- The method's effectiveness across different base models and training datasets is unknown, limiting generalizability.
- The approach requires access to fine-tuning infrastructure and training data, limiting deployment scenarios where model access is restricted.

## Confidence
- **High confidence**: The core mechanism of noise estimate matching and EMA-based self-distillation is technically sound and well-implemented.
- **Medium confidence**: The comparison with existing methods is valid, though evaluation conditions may not be perfectly aligned across different model scales.
- **Medium confidence**: The multi-concept removal strategy appears effective based on presented results, but underlying assumptions about CLIP embedding robustness need further validation.

## Next Checks
1. **Human evaluation study**: Conduct a blind perceptual study comparing SDD outputs with baseline models across multiple concept removal scenarios, measuring both concept removal effectiveness and image quality judgments.

2. **Cross-model generalization**: Apply the SDD method to a different base diffusion model (e.g., SDXL or a non-LaVIAN-trained model) and evaluate performance consistency across architectures.

3. **Adversarial robustness test**: Systematically probe the SDD model with adversarial prompts designed to bypass concept removal (e.g., misspellings, synonyms, or context shifts) to assess the robustness of the removal mechanism.