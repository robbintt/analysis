---
ver: rpa2
title: Using Large Language Models for (De-)Formalization and Natural Argumentation
  Exercises for Beginner's Students
arxiv_id: '2304.06186'
source_url: https://arxiv.org/abs/2304.06186
tags:
- language
- natural
- logic
- exercises
- notation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two systems using large language models for
  automated correction of logic exercises in beginner mathematics courses. The first
  system employs text-davinci-003 for autoformalization, translating natural language
  statements into propositional and first-order predicate logic.
---

# Using Large Language Models for (De-)Formalization and Natural Argumentation Exercises for Beginner's Students

## Quick Facts
- arXiv ID: 2304.06186
- Source URL: https://arxiv.org/abs/2304.06186
- Reference count: 18
- Two systems using text-davinci-003 for automated correction of logic exercises: autoformalization and deformalization feedback with length-based simplicity grading

## Executive Summary
This paper presents two automated systems for logic exercise correction using large language models. The first system performs autoformalization, translating natural language statements into propositional and first-order predicate logic using text-davinci-003 with example-based prompting. The second system provides automated feedback on both formalization and deformalization exercises, combining LLM-based translation with formal proof checking via PyProver. A third application verifies natural language arguments using the same autoformalization approach. The systems show promising performance for simple examples but struggle with complex logical structures and tend to use all available notation symbols.

## Method Summary
The authors employ text-davinci-003 for autoformalization through few-shot learning, where the model is prompted with notation specifications and example translations to generalize to new sentences. For deformalization exercises, user input is translated back to formal logic and checked for equivalence against target formulas using PyProver. Simplicity grading uses a length-based metric comparing user answers to template solutions. The approach relies on prompt engineering rather than fine-tuning, making it faster to implement but less reliable for complex cases.

## Key Results
- Text-davinci-003 can accurately translate simple natural language sentences into formal logic using few-shot prompting
- The system successfully combines LLM translation with theorem proving for automated feedback
- Length-based simplicity grading provides reasonable approximation of naturalness for deformalization exercises
- Performance degrades significantly for sentences with complex logical structures (multiple quantifier alternations)
- Models tend to use all available notation symbols even when unnecessary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5's text-davinci-003 can be prompted to translate natural language sentences into propositional and first-order predicate logic formulas with reasonable accuracy for simple examples.
- Mechanism: Few-shot learning using example-based prompting where the model is given a notation specification, a natural language sentence, and a correct formalization. The model generalizes from these examples to handle new sentences in the same notation.
- Core assumption: The model has sufficient understanding of logical operators and structure to map natural language expressions to formal logic when provided with appropriate examples.
- Evidence anchors:
  - [abstract] "The first system employs text-davinci-003 for autoformalization, translating natural language statements into propositional and first-order predicate logic."
  - [section 1] "Experiments showed a satisfying performance on the intended kind of (simple) example sentences."
- Break condition: When sentences contain highly complex logical structures (e.g., multiple quantifier alternations) or when the natural language formulation significantly differs from the examples provided in the prompt.

### Mechanism 2
- Claim: The system can provide automated feedback on both formalization and deformalization exercises by combining LLM-based translation with formal proof checking.
- Mechanism: For deformalization exercises, the user's natural language input is first translated back to formal logic using the same autoformalization system, then compared against the target formula using a theorem prover (PyProver) to verify logical equivalence.
- Core assumption: The autoformalization system produces consistent and accurate translations that can be reliably checked for equivalence against the target formula.
- Evidence anchors:
  - [section 2] "One can, for example, store the exercise in the form of a pair (η, φ)... take the user's input, determine whether it is a well-formed formula at all... and, in case it is, pass the tasks φ → η and η → φ to an automated theorem prover."
  - [section 2.1] "the PyProver, which can be imported into Python code as a package."
- Break condition: When the autoformalization system fails to accurately capture the user's intended meaning, leading to false negatives in the proof checking step.

### Mechanism 3
- Claim: The simplicity grading metric based on string length ratio effectively approximates the naturalness of deformalization answers.
- Mechanism: The system measures the ratio of the length of the user's answer to the length of a template solution, normalizes this using a sigmoid function, and uses this as a proxy for simplicity/naturalness.
- Core assumption: Shorter natural language formulations of logical statements are generally more natural and pedagogically appropriate than longer, more literal translations.
- Evidence anchors:
  - [section 2.1] "Our attempts to train language models to provide such an assessment were utterly unsuccessful so far. We thus resorted to a rather simple-minded solution... We measure the complexity of the input by relating its length to the length of the template solution."
  - [section 2.1] "A 'word for word translation' of logical syntax into natural language will usually be considerably longer than the shortest formulation in natural language."
- Break condition: When the shortest formulation is not the most pedagogically appropriate one, or when the template solution itself is not optimal.

## Foundational Learning

- Concept: Propositional and first-order predicate logic
  - Why needed here: The entire system is built around translating between natural language and formal logic systems, so understanding logical operators, quantifiers, and well-formed formulas is essential.
  - Quick check question: What is the difference between the logical statements ∀x∃y P(x,y) and ∃y∀x P(x,y)?

- Concept: Few-shot learning and prompt engineering
  - Why needed here: The system relies on text-davinci-003's ability to learn from examples rather than fine-tuning, making prompt design crucial for performance.
  - Quick check question: How would you structure a prompt to teach a model to convert temperatures from Fahrenheit to Celsius using three examples?

- Concept: Theorem proving and logical equivalence
  - Why needed here: The feedback system uses PyProver to check whether the user's formalization matches the target formula, requiring understanding of formal proof methods.
  - Quick check question: If A → B is provable and B → A is provable, what logical relationship holds between A and B?

## Architecture Onboarding

- Component map: Prompt generator -> Autoformalization engine -> Proof checker -> Simplicity grader -> Feedback generator
- Critical path: User input → Autoformalization → Proof checking → Simplicity grading → Feedback generation
- Design tradeoffs:
  - Using few-shot learning vs. fine-tuning: Few-shot is faster to implement but less reliable for complex cases
  - Length-based simplicity vs. semantic analysis: Length is simple to implement but may not capture true naturalness
  - Server-based LLM vs. local model: Server provides better performance but introduces reliability issues with traffic
- Failure signatures:
  - Autoformalization produces "error" or incorrect formulas for complex sentences
  - Proof checker fails to verify equivalent formulations due to inconsistent autoformalization
  - Simplicity grader gives high scores to awkward but short formulations
  - System becomes unavailable due to OpenAI API rate limiting
- First 3 experiments:
  1. Test autoformalization with increasingly complex natural language sentences (simple → moderate → complex) to establish performance boundaries
  2. Verify proof checking by generating pairs of equivalent and non-equivalent formulas to test PyProver integration
  3. Evaluate simplicity grading by comparing length-based scores with human-judged naturalness ratings for a set of deformalization answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large language models be optimized to handle complex logical structures with multiple quantifier alternations and junctors while maintaining accuracy in autoformalization tasks?
- Basis in paper: [explicit] The paper notes that for sentences of high logical complexity, formalizations were frequently wrong, though this does not present much of an issue for the intended application.
- Why unresolved: The paper suggests that this is a limitation of the current models but does not explore solutions or improvements for handling such complex structures.
- What evidence would resolve it: Conducting experiments with fine-tuned models trained on datasets containing complex logical structures and evaluating their performance in autoformalization tasks.

### Open Question 2
- Question: What alternative metrics or methods can be developed to assess the "naturalness" or "simplicity" of user inputs in deformalization exercises beyond the length-based metric currently used?
- Basis in paper: [explicit] The paper describes using a length-based metric to evaluate the simplicity of user inputs but acknowledges that training language models to provide such assessments was unsuccessful.
- Why unresolved: The paper does not explore other potential metrics or methods for evaluating naturalness, relying instead on a simple length-based approach.
- What evidence would resolve it: Developing and testing alternative evaluation methods, such as semantic similarity measures or user feedback, to determine their effectiveness in assessing naturalness.

### Open Question 3
- Question: How can the tendency of large language models to use all available notation symbols in formalizations be mitigated to improve the accuracy and relevance of autoformalizations?
- Basis in paper: [explicit] The paper highlights that models tend to use all pieces of given notation, leading to unnecessary complexity in formalizations.
- Why unresolved: The paper does not propose specific strategies to address this tendency, which could impact the clarity and usability of the formalizations.
- What evidence would resolve it: Implementing and testing techniques such as constraint-based prompting or post-processing steps to filter out superfluous notation usage in model outputs.

## Limitations

- Performance degrades significantly for complex logical structures with multiple quantifier alternations
- Length-based simplicity metric may not accurately capture true naturalness of formulations
- Dependence on external LLM APIs introduces reliability concerns for classroom deployment

## Confidence

**High Confidence Claims:**
- The technical approach of using text-davinci-003 for autoformalization through few-shot prompting is clearly specified and implementable
- The integration of PyProver for logical equivalence checking is technically sound
- The system can handle simple formalization and deformalization tasks as demonstrated

**Medium Confidence Claims:**
- The simplicity metric based on length ratio provides useful pedagogical feedback
- The system shows promise for classroom use despite acknowledged limitations
- The approach can be extended to argument verification tasks

**Low Confidence Claims:**
- Performance on complex logical structures remains unverified beyond anecdotal evidence
- The pedagogical effectiveness of the feedback system has not been validated with actual students
- The trade-off between system complexity and educational benefit is not quantified

## Next Checks

1. **Performance Degradation Analysis**: Systematically test the autoformalization system across a graded complexity scale (simple → moderate → complex sentences) and measure accuracy rates, identifying specific structural features that cause failures (e.g., nested quantifiers, multiple logical operators).

2. **Simplicity Metric Validation**: Conduct a controlled study comparing length-based simplicity scores against human expert ratings of naturalness for the same set of deformalization answers, calculating correlation coefficients to quantify metric validity.

3. **Reliability Under Load Testing**: Simulate classroom usage patterns by running concurrent autoformalization requests and measuring error rates, response times, and system availability over extended periods to assess practical deployment feasibility.