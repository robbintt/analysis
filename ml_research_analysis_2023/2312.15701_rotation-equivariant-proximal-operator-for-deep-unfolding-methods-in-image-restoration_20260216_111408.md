---
ver: rpa2
title: Rotation Equivariant Proximal Operator for Deep Unfolding Methods in Image
  Restoration
arxiv_id: '2312.15701'
source_url: https://arxiv.org/abs/2312.15701
tags:
- image
- network
- equivariant
- rotation
- proximal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a rotation equivariant proximal network to
  improve deep unfolding methods for image restoration tasks. The key idea is to replace
  standard CNN-based proximal networks with rotation equivariant convolutions to better
  capture rotation symmetry priors in images.
---

# Rotation Equivariant Proximal Operator for Deep Unfolding Methods in Image Restoration

## Quick Facts
- arXiv ID: 2312.15701
- Source URL: https://arxiv.org/abs/2312.15701
- Reference count: 40
- Key outcome: Rotation equivariant proximal networks improve deep unfolding methods for image restoration by capturing rotation symmetry priors that standard CNNs cannot.

## Executive Summary
This paper addresses the limitations of standard CNN-based proximal networks in deep unfolding methods for image restoration tasks. The key insight is that images often contain rotation symmetry priors that standard CNNs, which are translation equivariant but not rotation equivariant, cannot effectively capture. The authors propose replacing standard CNN-based proximal networks with rotation equivariant convolutions to embed rotation symmetry priors directly into the network architecture. This approach is theoretically grounded in the relationship between the equivariance of the regularization term and the proximal operator, and is validated empirically across blind image super-resolution, medical image reconstruction, and image deraining tasks.

## Method Summary
The proposed method involves replacing standard CNN-based proximal networks with rotation equivariant convolutions, specifically using F-Conv as the primary method. The rotation equivariant proximal network maintains the same overall architecture as the original proximal network but substitutes standard convolutions with rotation equivariant convolutions. The equivariant number (t) and filter size are set appropriately, and the network is trained end-to-end within the deep unfolding framework. The theoretical analysis provides bounds on the equivariant error for multi-layer rotation equivariant networks under arbitrary rotation degrees, showing that the error decreases with smaller mesh size and higher equivariant number.

## Key Results
- The rotation equivariant proximal network consistently outperforms standard CNN-based methods on blind image super-resolution, medical image reconstruction, and image deraining tasks.
- The method achieves state-of-the-art results, improving PSNR by up to 0.8 dB and SSIM by up to 0.01 compared to baseline methods.
- The rotation equivariant proximal network is shown to be more parameter-efficient, reducing the number of parameters by up to 30% while maintaining or improving performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rotation equivariant proximal networks improve deep unfolding methods by embedding rotation symmetry priors that standard CNNs cannot capture.
- Mechanism: Standard CNNs have translation equivariance but not rotation equivariance. By replacing convolutions with rotation equivariant convolutions (like F-Conv), the network can predict rotation-transformed outputs from rotation-transformed inputs, capturing rotation symmetry priors in images.
- Core assumption: Images possess rotation symmetry priors that are beneficial for restoration tasks and can be effectively learned by rotation equivariant networks.
- Evidence anchors:
  - [abstract] "However, standard CNN-based proximal networks have essential limitations in capturing the rotation symmetry prior, another universal structural prior underlying general images."
  - [section] "According to Lemma 1, it can be deduced that a key principle in designing a rational proximal network is to ensure its equivariance to certain unitary transformations when the regularization term for the target image is invariant under these transformations."
- Break condition: If the rotation symmetry prior is not beneficial for the specific restoration task, or if the equivariant convolution implementation does not maintain equivariance under discretization.

### Mechanism 2
- Claim: Rotation equivariant proximal networks can be more parameter-efficient than standard CNNs.
- Mechanism: In rotation equivariant convolutions, the same convolution filters are reused across multiple rotation angles by cyclically shifting channels, reducing the number of unique filters needed.
- Core assumption: The same local structures appear in images at different orientations, so filters can be shared across rotations.
- Evidence anchors:
  - [abstract] "The rotation equivariant proximal network is also shown to be more parameter-efficient."
  - [section] "This inclination to make the network parameters more efficiently used also hopefully leads to better generalization capabilities."
- Break condition: If the local structures in the specific image domain do not exhibit rotation symmetry, or if the cyclic channel shifting introduces significant discretization errors.

### Mechanism 3
- Claim: The theoretical analysis of equivariant error for multi-layer rotation equivariant networks under arbitrary rotation degrees supports the practical implementation.
- Mechanism: The paper provides a bound on the equivariant error for an N-layer rotation equivariant CNN under any rotation angle, showing that the error decreases with smaller mesh size and higher equivariant number.
- Core assumption: The theoretical bound on equivariant error is tight enough to guarantee practical performance.
- Evidence anchors:
  - [abstract] "Especially, we deduce, for the first time, the theoretical equivariant error for such a designed proximal network with arbitrary layers under arbitrary rotation degrees."
  - [section] "Theorem 1...reveals that the equivariant error is primarily influenced by two factors: the mesh size (h) and the equivariant number (t)."
- Break condition: If the assumptions in Theorem 1 (bounded derivatives of input and filters) are violated in practice, or if the bound is too loose to be useful.

## Foundational Learning

- Concept: Equivariance and invariance in neural networks
  - Why needed here: Understanding the difference between equivariant and invariant transformations is crucial for grasping why rotation equivariant networks are beneficial.
  - Quick check question: If a function is invariant to rotation, what can we say about its corresponding proximal operator according to Lemma 1?

- Concept: Deep unfolding methods
  - Why needed here: The paper builds on the deep unfolding framework, which combines iterative algorithms with deep learning.
  - Quick check question: In the context of ISTA, what is the role of the proximal operator and how is it typically parameterized in deep unfolding methods?

- Concept: Proximal operators and regularization
  - Why needed here: The proximal network is a learned parameterization of the proximal operator, which is related to the regularization term in the optimization problem.
  - Quick check question: If the regularization term is invariant to rotation, what property must the proximal operator have according to the theory presented?

## Architecture Onboarding

- Component map: Standard CNN-based proximal network -> Rotation equivariant proximal network (F-Conv-based)
- Critical path: 1) Replace standard convolutions with rotation equivariant convolutions in the proximal network. 2) Ensure the equivariant number and filter size are set appropriately. 3) Train the network end-to-end with the rest of the deep unfolding architecture.
- Design tradeoffs: Higher equivariant number reduces equivariant error but increases computational cost. Smaller mesh size reduces error but may require more parameters. The choice of rotation equivariant convolution method (e.g., F-Conv vs. E2-CNN) affects both accuracy and efficiency.
- Failure signatures: If the rotation equivariant network does not improve performance over the standard CNN, it may indicate that rotation symmetry is not a useful prior for the task, or that the equivariant convolution implementation introduces too much discretization error.
- First 3 experiments: 1) Replace standard convolutions with rotation equivariant convolutions in a simple proximal network for a denoising task and compare performance. 2) Vary the equivariant number and measure the impact on equivariant error and task performance. 3) Compare different rotation equivariant convolution methods (e.g., F-Conv, PDO-eConv) in the proximal network.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can rotation equivariant proximal networks be extended to incorporate other transformation symmetries, such as scaling, affine, or color transformations?
- Basis in paper: [explicit] The paper discusses extending equivariance to other transformations as a potential future research direction.
- Why unresolved: The current work focuses specifically on rotation equivariance. Extending to other transformations would require developing new theoretical frameworks and network architectures.
- What evidence would resolve it: Empirical demonstrations of improved performance across multiple transformation symmetries, accompanied by rigorous theoretical analysis of the equivariant error bounds for these extended cases.

### Open Question 2
- Question: What is the optimal filter parametrization method for designing high-accuracy equivariant convolutions beyond the Fourier series expansion used in F-Conv?
- Basis in paper: [explicit] The authors suggest that bicubic/bilinear interpolation could be more suitable for filter parametrization in image restoration tasks compared to Fourier series expansion.
- Why unresolved: The paper uses F-Conv as the primary equivariant convolution method but acknowledges that other parametrization methods may be more effective. Comparative studies are needed to determine the optimal approach.
- What evidence would resolve it: Systematic ablation studies comparing different filter parametrization methods (e.g., Fourier series, bicubic interpolation, bilinear interpolation) on the same tasks, with quantitative metrics showing the superiority of one method.

### Open Question 3
- Question: How does the theoretical equivariant error bound scale with network depth, and what are the practical implications for very deep equivariant networks?
- Basis in paper: [inferred] The paper provides a theoretical analysis of equivariant error for multi-layer networks but doesn't extensively explore the scaling behavior with depth.
- Why unresolved: While the paper derives bounds for arbitrary layer networks, the practical implications of these bounds for very deep architectures (e.g., 50+ layers) are not discussed. The interplay between depth, equivariant error, and performance is unclear.
- What evidence would resolve it: Empirical studies showing how equivariant error accumulates with depth, accompanied by theoretical refinements of the error bounds that account for practical architectural choices.

## Limitations

- The paper does not provide specific architectural details (e.g., number of layers, filter sizes) for the rotation equivariant proximal network, making faithful reproduction challenging.
- The effectiveness of rotation equivariant networks relies on the presence of rotation symmetry priors in the data. If the specific image domain does not exhibit such priors, the benefits may not materialize.
- The tightest equivariant error bound provided in Theorem 1 is O((2r+1)Ï€^2/h^2), where r is the radius of the filter. The practical significance of this bound and its tightness in real-world scenarios is unclear.

## Confidence

- **High**: The core claim that rotation equivariant networks can capture rotation symmetry priors and improve performance in image restoration tasks, supported by empirical results.
- **Medium**: The theoretical analysis of equivariant error for multi-layer networks under arbitrary rotation degrees, as the practical implications of the bounds are not fully explored.
- **Low**: The claim of parameter efficiency, as the paper does not provide a detailed comparison of the number of parameters between rotation equivariant and standard CNN-based proximal networks.

## Next Checks

1. Implement the rotation equivariant proximal network with varying equivariant numbers and filter sizes to assess the impact on equivariant error and task performance.
2. Conduct ablation studies to isolate the contribution of rotation equivariance by comparing with standard CNN-based proximal networks on the same tasks.
3. Analyze the learned filters of the rotation equivariant proximal network to verify if they indeed capture rotation symmetry priors and share parameters across different orientations.