---
ver: rpa2
title: Deep Generative Models of Music Expectation
arxiv_id: '2310.03500'
source_url: https://arxiv.org/abs/2310.03500
tags:
- music
- diffusion
- musical
- surprisal
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes to use modern deep probabilistic generative
  models, specifically denoising diffusion probabilistic models, to compute an approximate
  likelihood of musical input sequences as a measure of musical expectancy or "surprisal".
  Unlike prior work that relies on hand-crafted features or linear models, these deep
  generative models can learn complex non-linear features directly from the training
  data.
---

# Deep Generative Models of Music Expectation

## Quick Facts
- arXiv ID: 2310.03500
- Source URL: https://arxiv.org/abs/2310.03500
- Reference count: 32
- Primary result: Pre-trained denoising diffusion models compute surprisal values that exhibit an inverted U-shaped relationship with human liking ratings, achieving competitive goodness-of-fit compared to state-of-the-art IDyOM methods.

## Executive Summary
This work proposes using pre-trained denoising diffusion probabilistic models to compute approximate likelihood values for musical sequences as a measure of musical expectancy or "surprisal." Unlike prior work relying on hand-crafted features or linear models, these deep generative models learn complex non-linear features directly from training data. The primary result shows that surprisal values computed by diffusion models exhibit an inverted U-shaped relationship with human liking ratings, similar to the Wundt effect, with competitive goodness-of-fit metrics compared to IDyOM.

## Method Summary
The study uses a pre-trained Audio-Diffusion-256 model to compute likelihood bounds for 5-second non-overlapping blocks from the Gold et al. dataset. These likelihood values serve as surprisal measures, which are then correlated with human liking ratings. A linear mixed-effects model adjusts for individual listener bias before fitting a quadratic regression between surprisal and adjusted ratings. The goodness of fit is evaluated using R², log-likelihood, AIC, and BIC metrics.

## Key Results
- Diffusion model surprisal exhibits inverted U-shaped relationship with human liking ratings (Wundt effect)
- Goodness of fit measures show lower R² but higher likelihood and better AIC/BIC values compared to IDyOM
- Pre-trained diffusion model achieves competitive performance without fine-tuning on the specific dataset

## Why This Works (Mechanism)

### Mechanism 1
Deep diffusion models approximate music likelihood through denoising steps that implicitly capture non-linear musical structure. The model learns a Markov chain from noise back to data, with each denoising step conditioned on learned embeddings. The negative log-likelihood bound serves as an approximate surprisal score. Core assumption: musical audio distribution can be modeled as a high-dimensional Gaussian with learned parameters.

### Mechanism 2
The inverted U-shaped relationship emerges because diffusion model surprisal reflects intermediate complexity matching human preference. When surprisal is too low, music is predictable and boring; too high, it's incomprehensible. The diffusion model surprisal curve peaks near moderate values, aligning with the Wundt effect. Core assumption: human liking is driven primarily by the predictability-uncertainty axis.

### Mechanism 3
Mixed-effects linear models adjust for individual listener bias, allowing single model surprisal values to correlate with diverse human ratings. Each subject's random effect is estimated and subtracted from their ratings, normalizing across subjects. Core assumption: subject-level biases are consistent across songs and can be captured by random intercepts.

## Foundational Learning

- Concept: Diffusion probabilistic models
  - Why needed here: Core method for computing musical surprisal relies on understanding how denoising diffusion models work
  - Quick check question: What is the role of the noise schedule in a diffusion model, and why is αT ≈ 0 important?

- Concept: Information content (surprisal) in probabilistic models
  - Why needed here: Model's output (negative log-likelihood) is interpreted as surprisal, measuring unexpectedness
  - Quick check question: How does the negative log-likelihood of a probabilistic model relate to information content in information theory?

- Concept: Wundt effect and inverted U-shaped relationships
  - Why needed here: Validation depends on demonstrating the same inverted U-shaped relationship with liking as established in psychological literature
  - Quick check question: What does an inverted U-shaped curve between surprisal and liking imply about optimal musical predictability for enjoyment?

## Architecture Onboarding

- Component map: Audio files → 5-second blocks → Pre-trained diffusion model → Likelihood values → Surprisal scores → Mixed-effects adjustment → Quadratic regression

- Critical path:
  1. Load pre-trained diffusion model and audio dataset
  2. Compute surprisal (negative log-likelihood) for each audio block
  3. Aggregate surprisal per song (sum over blocks)
  4. Fit mixed-effects model to adjust ratings per subject
  5. Fit quadratic regression between surprisal and adjusted ratings
  6. Evaluate goodness of fit and significance

- Design tradeoffs:
  - Using pre-trained model avoids retraining but introduces domain mismatch
  - 5-second block aggregation simplifies computation but introduces boundary effects
  - Quadratic fit assumes symmetric inverted U, which may not capture all preference patterns

- Failure signatures:
  - Low R² despite significant quadratic term → high inter-subject variability not captured
  - Non-significant quadratic coefficient → model surprisal not predictive of liking
  - Poor AIC/BIC relative to IDyOM → worse overall model fit despite better likelihood

- First 3 experiments:
  1. Vary block size (e.g., 3s, 7s) and compare aggregated surprisal vs. liking fit quality
  2. Fine-tune diffusion model on Gold et al. dataset and re-evaluate Wundt effect strength
  3. Compare surprisal from diffusion model vs. IDyOM on same songs to identify systematic differences

## Open Questions the Paper Calls Out

### Open Question 1
How does the diffusion model's performance compare to IDyOM when trained on non-Western musical traditions? The current study only evaluates the pre-trained model on Western music, and the authors explicitly call for future research on non-Western datasets.

### Open Question 2
Can individual subject-level models of expectancy be developed through fine-tuning or self-supervised training to better capture intersubject variability? The current study uses a single pre-trained model that doesn't account for subjective properties of likability rating per user.

### Open Question 3
How do the surprisal values from the diffusion model correlate with biosignal correlates of surprise, such as event-related potentials (ERPs)? The authors suggest comparing model likelihood scores with physiological measures of surprise as a future direction.

## Limitations
- Transferability concerns from model trained on 20,000+ generic audio samples to specific 57-song dataset
- 5-second non-overlapping blocks may introduce boundary effects affecting surprisal estimates
- Quadratic polynomial assumption may not capture asymmetric or multimodal preference patterns

## Confidence

| Claim | Label |
|-------|-------|
| Inverted U-shaped relationship exists between diffusion model surprisal and human liking ratings | High |
| Diffusion models provide competitive goodness-of-fit measures compared to IDyOM | Medium |
| Specific numerical values of R² and AIC/BIC are definitive indicators of model quality | Low |

## Next Checks

1. Vary segment lengths (3s, 7s) and assess impact on surprisal-liking correlations to determine optimal temporal resolution
2. Train the diffusion model on the Gold et al. dataset and compare Wundt effect strength to assess domain adaptation benefits
3. Apply the same methodology to other music datasets with human preference ratings to evaluate robustness across different musical styles and listener populations