---
ver: rpa2
title: "SparseGS: Real-Time 360\xB0 Sparse View Synthesis using Gaussian Splatting"
arxiv_id: '2312.00206'
source_url: https://arxiv.org/abs/2312.00206
tags:
- depth
- view
- gaussian
- loss
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SparseGS, a technique for training 3D Gaussian
  Splatting (3DGS) representations from sparse training views. The method addresses
  the challenge of background collapse and "floaters" artifacts that commonly occur
  when training 3DGS with limited input views.
---

# SparseGS: Real-Time 360째 Sparse View Synthesis using Gaussian Splatting

## Quick Facts
- arXiv ID: 2312.00206
- Source URL: https://arxiv.org/abs/2312.00206
- Reference count: 40
- Key outcome: Achieves 30.5% LPIPS improvement over base 3DGS and 15.6% over NeRF-based methods with as few as 12 and 3 input images for 360째 and forward-facing scenarios respectively

## Executive Summary
This paper introduces SparseGS, a technique for training 3D Gaussian Splatting (3DGS) representations from sparse training views. The method addresses common artifacts like background collapse and "floaters" that occur when training 3DGS with limited input views. SparseGS incorporates three key components: depth correlation loss using patch-based Pearson correlation to align rendered and pseudo-ground truth depth maps, a diffusion-based score distillation sampling loss to hallucinate plausible details in poorly covered regions, and a novel floater pruning operator that removes unwanted high-opacity gaussians near the camera plane. Extensive evaluations on Mip-NeRF360, LLFF, and DTU datasets demonstrate high-quality reconstruction in both unbounded 360째 and forward-facing scenarios while maintaining fast training and real-time rendering capabilities.

## Method Summary
SparseGS extends the 3D Gaussian Splatting framework to handle sparse view inputs by incorporating three novel components. First, it uses patch-based Pearson correlation between rendered and pre-trained depth maps to align the 3D representation without requiring consistent depth scaling. Second, it employs diffusion-based score distillation sampling to guide the 3DGS parameters toward generating consistent details in unobserved regions. Third, it introduces a floater pruning operator that identifies and removes high-opacity gaussians positioned near the camera plane by comparing mode-selected and alpha-blended depth maps. The method is trained for 30k iterations with learning rates from the original 3DGS paper, applying depth correlation loss every iteration and diffusion loss randomly for 20% of the last 10k steps, with floater pruning applied at 20k and 25k iterations.

## Key Results
- Achieves up to 30.5% improvements over base 3DGS in LPIPS metric
- Shows 15.6% improvements over NeRF-based methods in LPIPS metric
- Maintains real-time rendering at 30+ FPS while using as few as 12 and 3 input images for 360째 and forward-facing scenarios respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using patch-based Pearson correlation between rendered and pseudo-ground truth depth maps helps align the 3D representation without requiring consistent depth scaling.
- Mechanism: The Pearson correlation measures local depth consistency across patches, encouraging the alpha-blended depth from 3DGS to match the pre-trained depth map regardless of absolute scale differences.
- Core assumption: Depth estimation models produce relative depth that correlates with the true geometry, even if the scale is inconsistent.
- Evidence anchors: [section] "Instead, we propose utilizing Pearson correlation across image-patches to compute a similarly metric between depth maps... This loss encourages patches at the same location in both depths maps to have a high cross correlation value irrespective of the range of depth values."
- Break condition: If the pre-trained depth model fails to capture the true geometry (e.g., textureless regions), the correlation loss will align the model to incorrect depth patterns.

### Mechanism 2
- Claim: Diffusion-based score distillation sampling (SDS) loss helps hallucinate plausible details in regions with poor training view coverage.
- Mechanism: By rendering novel views and comparing them with diffusion model predictions, the 3DGS parameters are guided to generate consistent details in unobserved regions.
- Core assumption: Diffusion models have learned general priors about natural images that can guide 3D reconstruction in unseen areas.
- Evidence anchors: [section] "we propose using a pre-trained generative diffusion model to guide the 3D gaussian representation via Score Distillation Sampling (SDS)... This allows us to 'hallucinate' plausible details for regions that do not have good coverage in the training views"
- Break condition: If the diffusion model's priors don't match the scene content (e.g., artistic scenes), the hallucinations may introduce unrealistic details.

### Mechanism 3
- Claim: The floater pruning operator removes high-opacity gaussians positioned near the camera plane that create artifacts in rendered views.
- Mechanism: By comparing mode-selected and alpha-blended depth maps, regions where they disagree significantly are identified as floaters and the corresponding gaussians are removed.
- Core assumption: Floaters manifest as high-opacity gaussians positioned close to the camera that cause mode-selected depth to differ significantly from alpha-blended depth.
- Evidence anchors: [section] "we propose a novel operator which leverages the explicit representation of 3D gaussians to remove floaters... We leverage this difference to generate a floater mask F for each training view"
- Break condition: If the scene naturally has high-frequency depth changes, the method might incorrectly identify legitimate geometry as floaters.

## Foundational Learning

- Concept: 3D Gaussian Splatting (3DGS) representation and rendering
  - Why needed here: The paper builds on 3DGS as the base representation, so understanding how gaussians are parameterized and rendered is essential.
  - Quick check question: How does 3DGS render a scene from a given camera pose?

- Concept: Depth estimation and correlation metrics
  - Why needed here: The method uses pre-trained depth models and Pearson correlation to align depth maps, requiring understanding of depth estimation techniques.
  - Quick check question: What's the difference between relative and absolute depth, and why does Pearson correlation help here?

- Concept: Score distillation sampling (SDS) with diffusion models
  - Why needed here: The method uses SDS to guide the 3D representation using diffusion models, which requires understanding of diffusion models and how they can be used for 3D tasks.
  - Quick check question: How does SDS work in the context of 3D reconstruction, and why is it effective for hallucinated details?

## Architecture Onboarding

- Component map: Base 3DGS pipeline -> Depth correlation loss module -> Diffusion loss module -> Floater pruning operator
- Critical path:
  1. Initialize 3DGS with COLMAP point cloud
  2. For each training iteration:
     - Render RGB and depth images
     - Compute RGB loss (standard 3DGS)
     - Compute depth correlation loss
     - Optionally compute diffusion loss
     - Apply gradient updates
  3. At pre-set intervals, apply floater pruning
- Design tradeoffs:
  - Patch size vs. computational cost in depth correlation loss
  - Frequency of floater pruning vs. training stability
  - Novel view distribution for diffusion loss vs. coverage of scene
- Failure signatures:
  - Floaters not being removed: check the dip test implementation and threshold calculation
  - Poor depth alignment: verify the pre-trained depth model output format and correlation computation
  - Diffusion artifacts: adjust the diffusion loss weight or novel view sampling strategy
- First 3 experiments:
  1. Verify basic 3DGS training works on a simple scene with dense views
  2. Add depth correlation loss and test on a sparse view scenario
  3. Add floater pruning and evaluate on a scene known to produce floaters

## Open Questions the Paper Calls Out
- How does the depth correlation loss scale to larger patch sizes, and what is the optimal patch size for different scene complexities?
- What is the theoretical justification for using the exponential curve with parameters a=97 and b=-8 to convert dip statistics to pruning thresholds?
- How does the score distillation sampling loss perform when using diffusion models trained on different domains or with different architectures?
- What is the relationship between the number of training views and the performance degradation of 3DGS, and at what point does SparseGS provide diminishing returns?

## Limitations
- The effectiveness of the diffusion-based hallucination heavily depends on the compatibility between the diffusion model's training data and the target scenes
- The floater pruning threshold (dip statistic with a=97, b=-8) is empirically determined but may not generalize well to scenes with different depth complexity distributions
- The depth correlation loss assumes that pre-trained depth models produce consistent relative depth patterns, but performance may degrade with textureless regions or extreme lighting conditions

## Confidence
- **High Confidence**: The base 3DGS framework and rendering pipeline implementation
- **Medium Confidence**: The floater pruning mechanism and depth correlation loss effectiveness
- **Medium Confidence**: The quantitative improvements reported (PSNR, SSIM, LPIPS metrics)

## Next Checks
1. Test the method on scenes with extreme texture variations to validate the robustness of the depth correlation loss under different depth estimation quality conditions.
2. Evaluate the diffusion loss contribution by training with and without it across multiple scene types to measure its impact on hallucination quality.
3. Analyze the floater pruning's effect on fine geometric details by comparing rendered results before and after pruning on scenes with known high-frequency depth changes.