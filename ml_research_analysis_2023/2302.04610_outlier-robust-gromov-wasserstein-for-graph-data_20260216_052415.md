---
ver: rpa2
title: Outlier-Robust Gromov-Wasserstein for Graph Data
arxiv_id: '2302.04610'
source_url: https://arxiv.org/abs/2302.04610
tags:
- graph
- distance
- outliers
- marginal
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a robust Gromov-Wasserstein (RGW) formulation
  to address the sensitivity of GW distance to outliers in graph matching tasks. The
  key idea is to introduce optimistically perturbed marginal constraints within a
  Kullback-Leibler divergence-based ambiguity set, allowing the model to re-weight
  samples and reduce the influence of outliers.
---

# Outlier-Robust Gromov-Wasserstein for Graph Data

## Quick Facts
- arXiv ID: 2302.04610
- Source URL: https://arxiv.org/abs/2302.04610
- Reference count: 40
- Key outcome: RGW achieves higher matching accuracy than UGW and PGW on subgraph alignment and shape correspondence tasks while maintaining comparable computation time

## Executive Summary
This paper addresses the sensitivity of Gromov-Wasserstein (GW) distance to outliers in graph matching tasks by proposing a robust formulation (RGW). The key innovation is introducing optimistically perturbed marginal constraints within a KL divergence-based ambiguity set, allowing the model to re-weight samples and reduce outlier influence. A computationally efficient Bregman proximal alternating linearized minimization (BPALM) algorithm is developed with theoretical convergence guarantees. Extensive experiments demonstrate RGW's superior performance in handling outliers compared to existing methods while maintaining computational efficiency.

## Method Summary
The method introduces a robust Gromov-Wasserstein formulation that incorporates perturbed marginal constraints to handle outliers. The core approach uses a KL divergence-based ambiguity set to create optimistically perturbed marginals that down-weight outliers while approximating the clean distribution. To solve the resulting non-convex optimization problem efficiently, the paper develops a Bregman proximal alternating linearized minimization (BPALM) algorithm with theoretical convergence guarantees. The method operates under the Huber epsilon-contamination model, providing robustness guarantees that bound the GW distance by the clean case plus a controllable factor.

## Key Results
- RGW achieves higher matching accuracy than UGW and PGW on subgraph alignment tasks
- RGW demonstrates superior performance on partial shape correspondence with outliers
- The BPALM algorithm maintains comparable computation time to existing methods while providing robustness guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing optimistically perturbed marginal constraints within a KL divergence-based ambiguity set allows the model to re-weight samples and reduce the influence of outliers.
- Mechanism: By optimizing the transport plan and perturbed marginal distributions jointly, the perturbed marginals help down-weight outliers while approximating the clean distribution.
- Core assumption: The outliers are present in the data but can be identified and down-weighted through distributional ambiguity sets.
- Evidence anchors:
  - [abstract]: "The key idea is to introduce optimistically perturbed marginal constraints within a Kullback-Leibler divergence-based ambiguity set, allowing the model to re-weight samples and reduce the influence of outliers."
  - [section 2.1]: "To reduce the weight assigned to the outliers, we perturbed the marginal distributions by an optimistically distributionally robust mechanism..."
  - [corpus]: Weak evidence; no direct citations but concept aligns with robust optimization literature.
- Break condition: If the fraction of outliers is too high or the ambiguity set is not properly calibrated, the robustness guarantee may fail.

### Mechanism 2
- Claim: The Bregman proximal alternating linearized minimization (BPALM) algorithm efficiently solves the non-convex RGW problem with convergence guarantees.
- Mechanism: BPALM alternates between updating the transport plan and the marginal distributions using Bregman divergences, which provide strong convergence properties for non-convex problems.
- Core assumption: The step sizes and Bregman divergences are chosen appropriately to ensure convergence.
- Evidence anchors:
  - [abstract]: "To make the benefits of RGW more accessible in practice, we develop a computationally efficient and theoretically provable procedure using Bregman proximal alternating linearized minimization algorithm."
  - [section 3.2]: Details the BPALM algorithm with theoretical convergence analysis.
  - [corpus]: No direct evidence; relies on general convergence theory for Bregman proximal methods.
- Break condition: If the step sizes are not chosen properly or the problem structure violates assumptions, convergence may fail.

### Mechanism 3
- Claim: Under the Huber epsilon-contamination model, RGW provides an upper bound on the true GW distance, ensuring robustness.
- Mechanism: The distributional ambiguity set allows the model to find perturbed marginals that approximate the clean distribution, bounding the GW distance by the clean case plus a controllable factor.
- Core assumption: The contamination model accurately represents the outlier generation process.
- Evidence anchors:
  - [section 2.2]: "We prove that under the Huber epsilon-contamination model, the robust Gromov Wasserstein guarantees that outliers cannot arbitrarily increase the transport distance..."
  - [theorem 2.3]: Provides the formal robustness guarantee.
  - [corpus]: No direct evidence; relies on statistical theory of contamination models.
- Break condition: If the contamination model does not match the actual outlier distribution, the bound may not hold.

## Foundational Learning

- Concept: Gromov-Wasserstein (GW) distance for comparing distributions on different metric spaces
  - Why needed here: RGW is a robust extension of GW, so understanding GW is essential to grasp the problem being solved.
  - Quick check question: What is the main drawback of the standard GW distance that RGW aims to address?

- Concept: Kullback-Leibler (KL) divergence and Bregman divergences
  - Why needed here: KL divergence is used to construct the ambiguity set and Bregman divergences are used in the optimization algorithm.
  - Quick check question: How does the choice of divergence affect the robustness and convergence properties of the algorithm?

- Concept: Distributionally robust optimization and contamination models
  - Why needed here: The robustness of RGW is based on distributional ambiguity sets, which are rooted in distributionally robust optimization.
  - Quick check question: What is the role of the Huber epsilon-contamination model in the robustness guarantee?

## Architecture Onboarding

- Component map: RGW formulation -> BPALM algorithm -> Ambiguity set -> Contamination model
- Critical path:
  1. Formulate the RGW problem with perturbed marginals
  2. Develop the BPALM algorithm to solve the non-convex problem
  3. Prove convergence and robustness guarantees
  4. Validate the method on real-world graph learning tasks
- Design tradeoffs:
  - Robustness vs. computational efficiency: RGW is more robust but requires solving a more complex optimization problem
  - Choice of divergence: KL divergence is used for robustness and algorithmic convenience, but other divergences could be explored
  - Contamination model: The Huber model provides theoretical guarantees but may not capture all outlier scenarios
- Failure signatures:
  - Poor performance on outlier-free data: RGW may over-regularize if the contamination model is not accurate
  - Slow convergence: BPALM may converge slowly if the step sizes are not chosen properly
  - Infeasible perturbed marginals: The ambiguity set may be too restrictive, leading to infeasible solutions
- First 3 experiments:
  1. Validate RGW on a synthetic 2D matching problem with known outliers
  2. Compare RGW with existing methods (UGW, PGW) on subgraph alignment tasks
  3. Test RGW on partial shape correspondence problems to demonstrate robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RGW model recover the transport plan from the robust GW distance estimate?
- Basis in paper: [inferred] The paper focuses on computing the robust GW distance but does not discuss recovering the transport plan.
- Why unresolved: The paper does not explore this aspect of the RGW formulation.
- What evidence would resolve it: Experiments or theoretical analysis showing how to extract the transport plan from the RGW solution.

### Open Question 2
- Question: How can the slow convergence rate of the Sinkhorn algorithm for unbalanced optimal transport be improved to speed up the RGW algorithm?
- Basis in paper: [explicit] The paper mentions that the RGW algorithm suffers from the slow convergence rate of the Sinkhorn algorithm.
- Why unresolved: The paper proposes the RGW algorithm but does not address its computational efficiency.
- What evidence would resolve it: Experiments comparing the RGW algorithm with different faster algorithms for solving unbalanced optimal transport, or theoretical analysis of the convergence rate of the RGW algorithm with a faster solver.

### Open Question 3
- Question: How does the choice of the divergence function ϕ affect the performance of the RGW model?
- Basis in paper: [explicit] The paper uses the Kullback-Leibler divergence as the ϕ-divergence, but mentions that other divergences could be used.
- Why unresolved: The paper only explores one divergence function and does not investigate the impact of other choices.
- What evidence would resolve it: Experiments comparing the performance of the RGW model with different divergence functions on various graph learning tasks.

## Limitations
- The robustness guarantee assumes a specific outlier generation process (Huber epsilon-contamination model) that may not capture all real-world scenarios
- Computational efficiency still requires careful hyperparameter tuning for large-scale graphs
- Experimental validation focuses primarily on specific graph matching tasks, leaving questions about broader applicability

## Confidence
- High: The theoretical framework and algorithm design are well-specified and mathematically rigorous.
- Medium: The experimental results demonstrate clear improvements over baselines in the tested scenarios.
- Medium: The BPALM algorithm is shown to converge, but practical convergence rates may vary.

## Next Checks
1. Systematically vary the proportion of outliers in synthetic datasets to quantify the break point where RGW's robustness guarantee fails.
2. Conduct ablation studies on the ambiguity set radius and BPALM step sizes to identify optimal settings across diverse graph structures.
3. Test RGW on non-graph datasets (e.g., image or time series matching) to evaluate its broader applicability beyond the current scope.