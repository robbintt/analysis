---
ver: rpa2
title: 'LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information
  from Lateral Thinking Puzzles'
arxiv_id: '2308.10855'
source_url: https://arxiv.org/abs/2308.10855
tags:
- thinking
- lateral
- evaluation
- llms
- truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LatEval, a novel evaluation benchmark that
  assesses the lateral thinking capabilities of LLMs within an interactive framework
  inspired by Lateral Thinking Puzzles. The benchmark involves one host (a strong
  LLM) and one player (the model under evaluation), where the player asks questions
  to gather information and deduce the truth from an incomplete story.
---

# LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles

## Quick Facts
- arXiv ID: 2308.10855
- Source URL: https://arxiv.org/abs/2308.10855
- Reference count: 8
- Key outcome: Proposes LatEval benchmark to assess lateral thinking in LLMs via interactive questioning with 500 annotated puzzles; shows GPT-4 outperforms others but all models lag humans significantly

## Executive Summary
This paper introduces LatEval, a novel evaluation benchmark that assesses the lateral thinking capabilities of large language models (LLMs) through an interactive framework inspired by Lateral Thinking Puzzles. The benchmark involves a host model (GPT-4) and a player model (the model under evaluation), where the player asks questions to gather information and deduce the truth from an incomplete story. The authors collect 500 high-quality English and Chinese puzzles and annotate key clues for evaluation. Experimental results on 8 LLMs reveal that while GPT-4 shows some advantage, nearly all models struggle with lateral thinking and have a significant gap compared to human performance, demonstrating the challenging and distinctive nature of the proposed benchmark.

## Method Summary
LatEval evaluates lateral thinking by having a player model interact with a host model (GPT-4) through a series of yes/no/irrelevant questions to solve incomplete lateral thinking puzzles. The benchmark measures three aspects: the quality of questions posed (Question Relevance and Divergence), and the model's capability to integrate information for problem-solving (Answer Consistency). The evaluation uses 500 annotated English and Chinese puzzles with key clues, and metrics include Distinct-3 for question diversity, coverage of truth clues, and average turns to solution.

## Key Results
- GPT-4 outperforms other models in lateral thinking tasks but still shows significant gap compared to human performance
- All tested LLMs struggle with employing lateral thinking during interactions
- Question diversity (Distinct-3) correlates with lateral thinking performance but isn't sufficient alone
- English and Chinese datasets show performance differences, suggesting cultural context influences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive questioning with binary feedback elicits lateral thinking in LLMs
- Mechanism: The player model engages in multi-turn dialogue with a strong host model, iteratively refining its understanding through yes/no/irrelevant answers
- Core assumption: The host model (GPT-4) provides consistent and accurate responses that do not inadvertently reveal the answer
- Evidence anchors: [abstract] "the player asks questions to gather information and deduce the truth from an incomplete story"; [section] "The player model acquires more unknown information through questioning and summarizes the information acquired every few rounds"

### Mechanism 2
- Claim: Diversity of questions correlates with lateral thinking performance
- Mechanism: The benchmark measures question diversity (Distinct-3) as a proxy for creative, divergent thinking
- Core assumption: A diverse set of questions increases the likelihood of uncovering key clues necessary for lateral solutions
- Evidence anchors: [abstract] "we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving"; [section] "We employ Distinct-3 (Li et al., 2016) to compute the diversity of questions posed within the game"

### Mechanism 3
- Claim: Integration of acquired information into a coherent truth reflects lateral thinking capability
- Mechanism: After gathering clues through questions, the player model synthesizes them into a proposed solution
- Core assumption: The player model can effectively combine partial, non-linear information into a valid explanation of the puzzle
- Evidence anchors: [abstract] "the model's capability to integrate information for problem-solving"; [section] "the consistency between perceived truth and actual truth"

## Foundational Learning

- Concept: Vertical thinking (convergent reasoning)
  - Why needed here: Understanding the contrast with lateral thinking clarifies what the benchmark aims to measure beyond standard logical reasoning tasks
  - Quick check question: In a vertical thinking task, would you explore multiple unrelated hypotheses or systematically narrow down to the best solution?

- Concept: Interactive dialogue systems
  - Why needed here: The benchmark's multi-turn interaction requires knowledge of how to structure prompts and maintain conversational state between host and player models
  - Quick check question: How would you design a prompt to ensure the host only responds with "yes," "no," or "irrelevant"?

- Concept: Evaluation metrics for question diversity
  - Why needed here: Measuring question divergence requires understanding metrics like Distinct-3 and their limitations in capturing creative thinking
  - Quick check question: What does a Distinct-3 score of 0.5 indicate about the variety of n-grams in the questions asked?

## Architecture Onboarding

- Component map: Dataset (500 puzzles) -> Host model (GPT-4) -> Player model (LLM under test) -> Evaluator (GPT-4) -> Metrics (Question Relevance, Question Divergence, Answer Consistency, Average Turns)

- Critical path: Load puzzle and initialize host/player models with appropriate prompts → Execute interaction loop (player asks, host answers) until max turns or truth guessed → Generate perceived truth and evaluate against actual truth → Compute all three metrics and report results

- Design tradeoffs:
  - Host choice: Using GPT-4 ensures high-quality responses but may limit evaluation of models that could outperform GPT-4 in lateral thinking
  - Turn limit: 10 rounds balances thorough exploration with preventing infinite loops
  - Language scope: English and Chinese datasets increase diversity but require bilingual annotation quality

- Failure signatures:
  - Host providing hints beyond allowed responses
  - Player model generating non-yes/no questions repeatedly
  - Metrics showing high diversity but low relevance (exploring irrelevant spaces)
  - Memorized answers scoring high without genuine reasoning

- First 3 experiments:
  1. Test with a simple puzzle (e.g., the Tom example) to verify interaction flow and metric calculations
  2. Compare a strong baseline (GPT-4) against a weaker model (Llama2-chat-7B) to establish performance range
  3. Evaluate human performance on the same puzzles to set an upper bound for lateral thinking capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does lateral thinking performance vary across different cultural contexts and languages beyond English and Chinese?
- Basis in paper: [explicit] The authors note that "the English and Chinese data are not simply translations of each other, but rather sourced from distinct English and Chinese websites. The two datasets each incorporates local characteristics, including memes that are more readily understood within their respective local environments." They also observe performance gaps between the two languages.
- Why unresolved: The study only examines two languages and cultures. Cultural context heavily influences lateral thinking puzzles, but the extent of this influence and how it varies across diverse cultures remains unexplored.
- What evidence would resolve it: Testing the LatEval benchmark across a diverse set of languages and cultures, and analyzing performance differences while controlling for linguistic and cultural factors.

### Open Question 2
- Question: What specific training approaches or architectural modifications could improve LLM performance on lateral thinking tasks?
- Basis in paper: [explicit] The authors state that "it is also a promising research direction that improves the lateral thinking capability of LLMs." They observe that even the most advanced model, GPT-4, maintains a noticeable gap when compared to human performance.
- Why unresolved: The paper identifies the problem but doesn't explore potential solutions. It's unclear whether this is a fundamental limitation of current architectures or if targeted training approaches could bridge the gap.
- What evidence would resolve it: Comparative studies testing various training approaches, architectural modifications, or fine-tuning strategies specifically designed to enhance lateral thinking capabilities.

### Open Question 3
- Question: How does the ability to perform lateral thinking correlate with other cognitive abilities in LLMs, such as commonsense reasoning or emotional intelligence?
- Basis in paper: [explicit] The authors note that their benchmark assesses "the model's lateral thinking within an interactive framework" and that LLMs "struggle with employing lateral thinking during interactions."
- Why unresolved: While the paper demonstrates that lateral thinking is a distinct challenge, it doesn't explore how this ability relates to other cognitive capabilities in LLMs or whether improving one area might enhance others.
- What evidence would resolve it: Correlation studies examining relationships between lateral thinking performance and other cognitive benchmarks, plus experimental evidence showing whether improvements in one area transfer to others.

## Limitations
- Benchmark relies on GPT-4 as both host and evaluator, creating potential circular validation
- Human performance baseline lacks detailed methodology and evaluation criteria
- Cultural context differences between English and Chinese puzzles may influence results beyond pure lateral thinking ability

## Confidence
- High Confidence: Benchmark design and evaluation methodology are clearly specified with concrete metrics and well-defined interaction protocol
- Medium Confidence: Experimental results showing all tested LLMs struggle with lateral thinking are plausible but specific performance gap claims are limited by lack of detailed human baseline
- Low Confidence: Claim that Distinct-3 diversity directly correlates with lateral thinking capability requires empirical validation

## Next Checks
1. Conduct a detailed human evaluation study using the same 500 puzzles with standardized evaluation criteria and timing constraints to establish a rigorous upper bound for lateral thinking performance
2. Implement the benchmark using a weaker model (e.g., Llama2-chat-7B) as the host to test whether GPT-4's superiority as host artificially constrains player model performance
3. Design an experiment that controls for question diversity while varying relevance to truth clues, testing whether high Distinct-3 scores alone predict successful lateral thinking or whether question-target alignment is the primary success factor