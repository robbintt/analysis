---
ver: rpa2
title: Prompt Tuning for Zero-shot Compositional Learning
arxiv_id: '2312.02191'
source_url: https://arxiv.org/abs/2312.02191
tags:
- prompt
- coop
- learning
- mmpt
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a prompt tuning approach for zero-shot compositional
  learning, which aims to recognize unseen compositions formed from seen attributes
  and objects without prior assumption of the output space. The key idea is to inherit
  the "knowledgeable" property from large pre-trained vision-language models like
  CLIP by designing a three-branch architecture with shared prompts across language
  and vision encoders.
---

# Prompt Tuning for Zero-shot Compositional Learning

## Quick Facts
- arXiv ID: 2312.02191
- Source URL: https://arxiv.org/abs/2312.02191
- Reference count: 40
- Key outcome: Achieves 29.8 AUC on UT-Zappos and 4.1 AUC on MIT-States, 1.5× better than state-of-the-art

## Executive Summary
This paper introduces Multi-Modal Prompt Tuning (MMPT), a novel approach for zero-shot compositional learning that recognizes unseen attribute-object compositions without prior output space assumptions. The framework leverages pre-trained vision-language models like CLIP by incorporating shared prompts across modalities and adding visual patch prompts to improve generalization. MMPT achieves new state-of-the-art results on OW-CZSL benchmarks, significantly outperforming previous methods on both the UT-Zappos and MIT-States datasets.

## Method Summary
MMPT employs a three-branch architecture with shared prompts across vision and language encoders to bridge modality gaps in compositional zero-shot learning. The framework adds learnable visual prompts to image patches and uses independent attribute and object prediction branches. Trained with cross-entropy loss, the model processes images through a Vision Transformer with visual patch prompts, while language branches independently predict attribute and object probabilities using shared multimodal prompts.

## Key Results
- Achieves 29.8 AUC score on UT-Zappos dataset
- Achieves 4.1 AUC score on MIT-States dataset (1.5× better than state-of-the-art)
- Demonstrates effectiveness of shared prompts across modalities for bridging gaps in CZSL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal shared prompts bridge modality gaps by allowing attribute/object branches to access visual information
- Mechanism: Shared prompt vector ti across all three branches enables cross-modal interaction through projector functions gv(ti), go(ti), and ga(ti)
- Core assumption: Single shared prompt vector can meaningfully encode modality-agnostic information
- Evidence anchors: Abstract states shared prompts "bring the gaps between the different modalities"; section describes shared prompts at front layers of encoders
- Break condition: Shared prompt cannot effectively encode modality-agnostic information, causing independent operation

### Mechanism 2
- Claim: Visual prompts injected into image patches improve generalization to unseen compositions
- Mechanism: Learnable visual prompt ϕ added to input embedding of each image patch and concatenated with standard patch embedding
- Core assumption: Learnable visual prompts capture composition-specific visual patterns not in pre-trained CLIP weights
- Evidence anchors: Abstract mentions visual patch prompts for generalization; section describes adding learnable vector parameterized by ϕ
- Break condition: Visual prompts overfit to seen compositions or introduce noise confusing vision encoder

### Mechanism 3
- Claim: Separating attribute and object prediction with independent branches prevents entanglement while maintaining compositional reasoning
- Mechanism: Two separate language transformers independently predict attribute and object probabilities using learned token representations
- Core assumption: Attribute-object relationships can be effectively learned through independent classification
- Evidence anchors: Cites KG-SP and SymNet showing independent prediction best utilizes training data
- Break condition: Strong attribute-object dependencies cannot be captured through independent prediction

## Foundational Learning

- **Vision-language pre-training (VLP) and contrastive learning**
  - Why needed here: MMPT builds on CLIP-like models that learn visual-semantic correspondences through large-scale contrastive training
  - Quick check question: Can you explain how CLIP learns to align image and text embeddings using contrastive loss?

- **Prompt tuning and soft prompts**
  - Why needed here: MMPT uses learnable prompts instead of fixed text templates to adapt pre-trained models to CZSL task
  - Quick check question: What's the difference between discrete prompt engineering and continuous prompt tuning?

- **Compositional zero-shot learning and open-world setting**
  - Why needed here: Task requires recognizing compositions never seen during training from potentially enormous output space
  - Quick check question: How does OW-CZSL differ from standard zero-shot learning in terms of output space assumptions?

## Architecture Onboarding

- **Component map**: Image → Vision encoder → Visual features → Cosine similarity with attribute/object embeddings → Combined composition score

- **Critical path**: Image → ViT backbone with visual patch prompts and shared multimodal prompts → Independent attribute and object branches → Combined probability distributions

- **Design tradeoffs**:
  - Independent vs joint attribute-object modeling: Independent prediction simplifies training but may miss compositional dependencies
  - Shared vs branch-specific prompts: Shared prompts enable cross-modal interaction but may limit branch specialization
  - Prompt length and depth: Longer prompts and more layers with independent prompts increase capacity but risk overfitting

- **Failure signatures**:
  - High seen accuracy but low unseen accuracy: Model overfits to training compositions
  - Both seen and unseen accuracy are low: Shared prompts or visual prompts not effectively capturing relevant information
  - Seen accuracy drops significantly with longer prompts: Prompts introducing noise or confusion

- **First 3 experiments**:
  1. Train MMPT with only shared prompts (no visual prompts) to measure cross-modal interaction contribution
  2. Train with only visual prompts (no shared prompts) to measure patch-level visual adaptation contribution
  3. Vary prompt length d and measure impact on seen/unseen accuracy trade-off to find optimal capacity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of layers (hs) in MMPT that should have independently learnable layer-specific prompts?
- Basis in paper: States "peak values appear in the range from 6 to 9" for both datasets
- Why unresolved: Only tests integer values and doesn't explore fractional values or values beyond tested range
- What evidence would resolve it: Experiments testing fractional values of hs and values beyond tested range for various datasets

### Open Question 2
- Question: How does MMPT performance scale with increasing dataset size and complexity?
- Basis in paper: Demonstrates effectiveness on two datasets but doesn't explore performance on larger/more complex datasets
- Why unresolved: No information on how performance scales with increasing dataset size and complexity
- What evidence would resolve it: Experiments on larger/more complex datasets comparing to state-of-the-art and analyzing computational resources

### Open Question 3
- Question: How does MMPT performance compare to other methods for compositions involving rare attributes or objects?
- Basis in paper: Doesn't explicitly address performance on compositions with rare attributes or objects
- Why unresolved: No information on how MMPT handles compositions with rare attributes or objects
- What evidence would resolve it: Experiments comparing performance on datasets with varying attribute/object frequency distributions

## Limitations

- Performance gains on MIT-States dataset remain relatively modest (4.1 AUC) despite being 1.5× better than previous state-of-the-art
- Lacks ablation studies on contribution of individual components (shared prompts vs visual prompts)
- Relies on CLIP-like pre-trained models, inheriting any biases or limitations from underlying vision-language alignment

## Confidence

- **High confidence**: Framework architecture is well-defined and implementation details for shared prompts and visual patch prompts are clearly specified
- **Medium confidence**: Claim that shared prompts effectively bridge modality gaps is supported theoretically but lacks direct empirical validation through ablation studies
- **Low confidence**: Assertion of "1.5 times better" performance requires careful interpretation given MIT-States dataset challenges and relatively small absolute improvements

## Next Checks

1. **Ablation study validation**: Run controlled experiments isolating contributions of shared prompts, visual patch prompts, and independent branch predictions to quantify each component's contribution

2. **Cross-dataset generalization test**: Evaluate MMPT on additional compositional datasets (C-GQA, CATER) to assess generalization across different compositional relationships and data distributions

3. **Failure mode analysis**: Conduct detailed error analysis categorizing failures by composition complexity and attribute/object frequency to identify specific struggles with rare attributes, rare objects, or complex compositional patterns