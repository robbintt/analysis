---
ver: rpa2
title: Boosting Prompt-Based Self-Training With Mapping-Free Automatic Verbalizer
  for Multi-Class Classification
arxiv_id: '2312.04982'
source_url: https://arxiv.org/abs/2312.04982
tags:
- label
- self-training
- word
- class
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of applying prompt-based self-training
  to multi-class classification tasks, which has been challenging due to limitations
  in existing verbalizer approaches. The proposed method, MAV, is a trainable verbalizer
  consisting of two fully connected layers that automatically extracts key vocabulary
  features from Masked Language Modeling (MLM) predictions without requiring manual
  label word selection.
---

# Boosting Prompt-Based Self-Training With Mapping-Free Automatic Verbalizer for Multi-Class Classification

## Quick Facts
- **arXiv ID:** 2312.04982
- **Source URL:** https://arxiv.org/abs/2312.04982
- **Reference count:** 25
- **Primary result:** Achieves 12.8% average performance improvement over existing self-training methods on five multi-class classification datasets

## Executive Summary
This paper addresses the challenge of applying prompt-based self-training to multi-class classification tasks by introducing MAV (Mapping-free Automatic Verbalizer), a trainable verbalizer that automatically extracts key vocabulary features from MLM predictions without requiring manual label word selection. MAV consists of two fully connected layers that leverage all information from MLM predictions to generate final probability distributions for each class. The approach demonstrates superior self-training efficacy, achieving an average performance improvement of 12.8% over existing methods on five diverse multi-class classification datasets.

## Method Summary
MAV is a trainable verbalizer consisting of two fully connected layers that automatically extracts vocabulary features from MLM predictions without manual label word selection. The method freezes MLM head parameters during fine-tuning to preserve pre-trained vocabulary representation knowledge, then uses the extracted features to generate class probability distributions. The approach is evaluated using a self-training framework with FixMatch-style consistency regularization and auxiliary MLM loss on five multi-class datasets with k=16 labeled examples per class and μ=4 ratio of unlabeled data.

## Key Results
- MAV achieves an average performance improvement of 12.8% over existing self-training methods
- Superior self-training efficacy demonstrated across five diverse multi-class classification datasets
- Eliminates the need for manual selection of label words while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAV leverages all MLM prediction information rather than just single label words, avoiding information loss
- Mechanism: The verbalizer extracts vocabulary features from the entire MLM prediction vector using two fully connected layers, rather than mapping from a single token
- Core assumption: The full MLM prediction contains useful information for distinguishing between classes that would be lost if only a single token's logit is used
- Evidence anchors:
  - [abstract] "MAV leverages all information from MLM predictions to generate the final probability distribution for each class"
  - [section] "MAV autonomously identify the vocabulary information needed to distinguish between classes in the MLM prediction"

### Mechanism 2
- Claim: Freezing the MLM head parameters preserves pre-trained vocabulary representation knowledge
- Mechanism: By keeping the MLM head frozen during fine-tuning, MAV can access the rich vocabulary representations learned during pre-training
- Core assumption: The pre-trained MLM head contains valuable vocabulary representation information that would be lost if parameters were updated during fine-tuning
- Evidence anchors:
  - [section] "Another significant consideration in constructing MAV is the fixed parameters of the MLM head. During the pre-training phase, the decoder of the MLM head shares parameters with the word embedding matrix, meaning it contains pre-trained vocabulary representation information"
  - [section] "Experimental results indicate that freezing the MLM head parameters leads to comparable or better performance than updating all parameters"

### Mechanism 3
- Claim: MAV eliminates the need for manual label word selection and associated costs
- Mechanism: By automatically learning which vocabulary features are important for classification, MAV removes the requirement for domain expertise in selecting appropriate label words
- Core assumption: A trainable verbalizer can learn to extract the necessary vocabulary features without explicit label word guidance
- Evidence anchors:
  - [abstract] "MAV eliminates the need for manual selection of label words"
  - [section] "This approach converts the verbalizer into a learnable form without providing any label word information related to the class"

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed here: MAV relies on MLM predictions as its input, so understanding how MLM works is fundamental to understanding the approach
  - Quick check question: How does MLM predict the masked token, and what does the output represent?

- **Concept: Prompt-based fine-tuning**
  - Why needed here: MAV is specifically designed for prompt-based fine-tuning scenarios, where inputs are wrapped with templates containing [MASK] tokens
  - Quick check question: What is the difference between standard fine-tuning and prompt-based fine-tuning, and why might prompts be useful for few-shot learning?

- **Concept: Self-training and consistency regularization**
  - Why needed here: The paper employs a self-training framework with consistency regularization between weakly and strongly augmented versions of unlabeled data
  - Quick check question: How does self-training work in the context of semi-supervised learning, and what role does consistency regularization play?

## Architecture Onboarding

- **Component map:** Input → RoBERTa encoder → Frozen MLM head → MAV (2 FCLs) → Classification head → Class probabilities
- **Critical path:** Input → RoBERTa → Frozen MLM head → MAV → Class probabilities
- **Design tradeoffs:**
  - Complexity vs performance: MAV adds two additional layers compared to single label word mapping, but provides significant performance gains
  - Pre-training preservation vs task adaptation: Freezing MLM head preserves pre-trained knowledge but may limit task-specific adaptation
  - Parameter efficiency: MAV adds minimal parameters compared to full fine-tuning approaches
- **Failure signatures:**
  - Poor performance on datasets with very few classes or simple boundaries
  - Degraded performance when MLM head parameters are updated during fine-tuning
  - Issues when vocabulary features are not representative of class distinctions
- **First 3 experiments:**
  1. Compare MAV performance with and without freezing MLM head parameters
  2. Evaluate MAV on datasets with varying numbers of classes to test scalability
  3. Test MAV performance when using different numbers of FCL layers or hidden dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of MAV vary across different types of multi-class classification tasks (e.g., topic classification vs. emotion classification)?
- Basis in paper: [explicit] The paper demonstrates MAV's effectiveness on five diverse multi-class datasets, including TREC (topic classification), GoEmotions (emotion classification), and Yahoo Answers (topic classification). However, the paper does not directly compare the performance of MAV across these different task types.
- Why unresolved: The paper presents overall performance improvements but doesn't analyze how MAV performs specifically on different task categories or what factors might influence its effectiveness for different types of classification.
- What evidence would resolve it: A detailed breakdown of MAV's performance across different task categories, along with analysis of why certain task types might benefit more from MAV than others.

### Open Question 2
- Question: What is the optimal number of label words to use in the Multi Label Words baseline for different datasets?
- Basis in paper: [explicit] The paper uses top 16 tokens with high prediction probability as label words for each class in the Multi Label Words baseline. However, the paper does not explore whether this number is optimal or if it should vary across datasets.
- Why unresolved: The paper assumes a fixed number of label words (16) without investigating whether this is the optimal choice or if different datasets might benefit from different numbers of label words.
- What evidence would resolve it: Experiments testing different numbers of label words (e.g., 5, 10, 20, 32) for each dataset and analysis of how the number of label words affects performance.

### Open Question 3
- Question: How does MAV perform in scenarios with highly imbalanced class distributions?
- Basis in paper: [inferred] The paper acknowledges that imbalanced class distributions are a significant challenge in real-world applications but does not address this scenario in their experiments. The paper focuses on balanced datasets where each class has equal or similar numbers of labeled and unlabeled examples.
- Why unresolved: The paper's experimental setup uses balanced datasets, so it doesn't provide insights into how MAV would handle scenarios where some classes have significantly more examples than others.
- What evidence would resolve it: Experiments on imbalanced datasets with varying degrees of class imbalance, and analysis of whether MAV's performance degrades or remains stable under such conditions.

## Limitations
- Performance critically depends on freezing MLM head parameters, with limited experimental validation across diverse datasets
- The mechanism by which vocabulary features are extracted and their interpretability across different class numbers is not thoroughly validated
- Template sensitivity and scalability to very large class numbers (hundreds or thousands) remain untested

## Confidence
- **High Confidence**: Core experimental results showing MAV's performance improvement over baselines on five tested datasets
- **Medium Confidence**: Claim that freezing MLM head parameters preserves pre-trained knowledge and improves performance (limited to TREC experiments)
- **Medium Confidence**: Assertion that MAV eliminates need for manual label word selection (lacks empirical comparison with different label word strategies)
- **Low Confidence**: Generalizability to very large-scale classification problems and domains with different vocabulary distributions

## Next Checks
1. **MLM Head Parameter Sensitivity Analysis**: Conduct controlled experiments across all five datasets comparing MAV performance with frozen vs. updated MLM head parameters, and test different levels of parameter freezing (partial vs. complete) to establish the robustness of the freezing strategy.

2. **Vocabulary Feature Interpretability Study**: Perform ablation studies and feature importance analysis to identify which vocabulary features MAV learns to extract for different classes across datasets, and verify that these features are semantically meaningful and consistent with human intuition about class distinctions.

3. **Template Sensitivity and Scalability Testing**: Systematically vary template formulations (different prompt styles, number of [MASK] tokens) and test MAV on additional datasets with varying class numbers (e.g., 100-500 classes) to evaluate performance sensitivity and scalability limits.