---
ver: rpa2
title: 'V1T: large-scale mouse V1 response prediction using a Vision Transformer'
arxiv_id: '2302.03023'
source_url: https://arxiv.org/abs/2302.03023
tags:
- visual
- neural
- core
- mouse
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces V1T, a novel Vision Transformer (ViT) based
  architecture for predicting neural responses in mouse primary visual cortex (V1).
  V1T learns a shared visual and behavioral representation across animals using a
  ViT core and animal-specific Gaussian readout modules.
---

# V1T: large-scale mouse V1 response prediction using a Vision Transformer

## Quick Facts
- arXiv ID: 2302.03023
- Source URL: https://arxiv.org/abs/2302.03023
- Authors: 
- Reference count: 40
- Key outcome: Vision Transformer architecture achieves 12.7% and 19.1% improvement over CNN baselines on two large-scale mouse V1 datasets

## Executive Summary
V1T introduces a Vision Transformer-based architecture for predicting neural responses in mouse primary visual cortex. The model uses a shared ViT core to learn visual representations across animals, combined with animal-specific Gaussian readout modules. By integrating behavioral variables through B-MLP modules, V1T achieves state-of-the-art performance on two large-scale mouse V1 datasets, outperforming previous CNN-based models by 12.7% and 19.1%. The learned self-attention weights correlate with population receptive fields and behavioral variables, providing insights into the model's learned representations.

## Method Summary
V1T processes 36×64 pixel natural images through a Vision Transformer core that learns shared visual representations across mice. The architecture includes B-MLP modules at each Transformer block to integrate behavioral variables (pupil dilation, center, running speed) with visual features. Animal-specific Gaussian readout modules map the core features to neural responses, with each neuron's receptive field position determined by anatomical coordinates and learned Gaussian distributions. The model is trained using AdamW optimizer with Poisson loss, learning rate scheduling, and early stopping on standardized data.

## Key Results
- V1T achieves 12.7% improvement over CNN baselines on DATASET S
- V1T achieves 19.1% improvement over CNN baselines on DATASET F
- Self-attention weights learned by V1T correlate with population receptive fields and behavioral variables
- V1T sets new state-of-the-art performance for large-scale mouse V1 response prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Vision Transformer core learns a shared visual representation that generalizes across mice.
- Mechanism: The ViT uses self-attention to capture long-range spatial dependencies in visual stimuli, enabling the model to learn invariant features across animals.
- Core assumption: Mouse visual cortex responses to natural images share underlying computational patterns that can be captured by a shared latent space.
- Evidence anchors:
  - [abstract]: "learns a shared visual and behavioral representation across animals"
  - [section]: "the core module which learns a shared latent representation of the visual stimuli across the animals"
  - [corpus]: Weak - no direct citations in neighbors about ViT-based cross-animal generalization
- Break Condition: If attention patterns differ drastically across animals, suggesting no shared visual computation.

### Mechanism 2
- Claim: Behavioral variables improve neural response prediction when integrated through the B-MLP module.
- Mechanism: The B-MLP module transforms behavioral inputs into the same dimensional space as visual embeddings and adds them element-wise at each Transformer block, allowing the model to adjust visual processing based on brain state.
- Core assumption: Internal brain states modulate visual processing in a way that can be linearly combined with visual features at each processing stage.
- Evidence anchors:
  - [abstract]: "learns a shared visual and behavioral representation across animals"
  - [section]: "we propose an alternative method to integrate behavioral variables with visual stimuli using a novel ViT architecture"
  - [corpus]: Weak - neighbors don't mention behavioral integration in neural prediction
- Break Condition: If removing B-MLP doesn't hurt performance, indicating behavioral variables don't modulate visual processing.

### Mechanism 3
- Claim: The Gaussian readout with retinotopic coupling enables efficient per-neuron prediction.
- Mechanism: Each neuron's response is predicted by a linear combination of core features at a single spatial position, with the position determined by the neuron's anatomical coordinates and a learned Gaussian distribution.
- Core assumption: V1 neurons have localized receptive fields that can be approximated by 2D Gaussians centered at specific image locations.
- Evidence anchors:
  - [abstract]: "Gaussian readout modules"
  - [section]: "the regression is performed by a Gaussian readout, which learns the parameters of a 2d Gaussian distribution"
  - [corpus]: Weak - no direct citations about Gaussian readout in neighbors
- Break Condition: If neurons with similar anatomical coordinates don't have similar readout positions.

## Foundational Learning

- Concept: Vision Transformer architecture
  - Why needed here: Understanding how ViT differs from CNNs (patch-based processing, self-attention, positional embeddings) is crucial for modifying it for neural prediction
  - Quick check question: What are the three main components of a ViT and how do they process visual input?

- Concept: Self-attention mechanism
  - Why needed here: The model's performance relies on attention weights correlating with receptive fields; understanding attention computation is essential
  - Quick check question: How does the self-attention calculation in ViT differ from local receptive fields in CNNs?

- Concept: Gaussian readout and retinotopy
  - Why needed here: The readout module's efficiency depends on the assumption that neurons with nearby anatomical coordinates have nearby receptive fields
  - Quick check question: Why does the Gaussian readout reduce parameters compared to a fully connected readout?

## Architecture Onboarding

- Component map: Tokenizer -> V1T Core (with B-MLP) -> Reshape -> Gaussian Readout -> Neural response prediction

- Critical path: Image → Tokenizer → V1T Core (with behavioral integration) → Reshape → Gaussian Readout → Neural response prediction

- Design tradeoffs:
  - Patch size vs. number of patches: Smaller patches capture more detail but increase computation
  - B-MLP per block vs. single module: Per-block allows behavioral modulation at different processing stages but increases parameters
  - Overlapping vs. non-overlapping patches: Overlapping captures more spatial context but increases redundancy

- Failure signatures:
  - Poor correlation on test set but good on validation: Overfitting to specific mice
  - Attention maps not centered: Incorrect patch size or stride selection
  - B-MLP activations flat across blocks: Behavioral integration not working

- First 3 experiments:
  1. Train with CNN core instead of V1T core to establish baseline improvement
  2. Remove B-MLP modules to quantify behavioral integration contribution
  3. Vary patch size (4, 8, 16) to find optimal spatial resolution for neural prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the V1T model's performance generalize to other visual areas beyond V1, such as higher visual areas (V2, V4, etc.)?
- Basis in paper: [inferred] The paper discusses using the model to explore how visual information is encoded across visual cortical areas, but does not test it on actual recordings from these areas.
- Why unresolved: The model was only evaluated on V1 datasets. Testing on higher visual areas would require new datasets with recordings from these areas.
- What evidence would resolve it: Evaluating the V1T model on neural recordings from higher visual areas and comparing its performance to models trained specifically on those areas.

### Open Question 2
- Question: How does the V1T model's performance compare to models that incorporate temporal dynamics, such as recurrent neural networks (RNNs) or transformers with temporal attention?
- Basis in paper: [inferred] The paper focuses on modeling static visual responses and does not explore temporal aspects of neural activity.
- Why unresolved: The V1T model is a feed-forward architecture and does not explicitly model temporal dependencies in neural responses.
- What evidence would resolve it: Comparing the V1T model's performance to models that incorporate temporal dynamics, such as RNNs or transformers with temporal attention, on the same V1 datasets.

### Open Question 3
- Question: How does the V1T model's learned attention maps relate to the actual receptive fields of individual neurons in V1?
- Basis in paper: [explicit] The paper shows that the attention maps correlate with pupil position, but does not directly compare them to receptive field measurements.
- Why unresolved: The paper does not provide a direct comparison between the attention maps and receptive field measurements from individual neurons.
- What evidence would resolve it: Comparing the attention maps learned by the V1T model to receptive field measurements obtained through electrophysiological recordings or other methods for individual neurons in V1.

## Limitations

- Data specificity: Performance improvements validated only on two specific mouse V1 datasets with controlled experimental conditions
- Behavioral integration assumptions: Effectiveness of B-MLP modules assumes linear combination of behavioral and visual features is sufficient for all brain states
- Cross-animal generalization: Claim of shared representations across mice lacks direct citations in related literature

## Confidence

- High confidence: Core architectural innovations (Vision Transformer core, Gaussian readout with retinotopic coupling) are well-specified and reproducible
- Medium confidence: 12.7% and 19.1% performance improvements over CNN baselines, but independent verification is challenging without access to implementations
- Low confidence: Claims about attention-receptive field correlations require validation beyond the paper's scope

## Next Checks

1. Cross-dataset validation: Test V1T on additional neural datasets with different recording techniques, species, or experimental conditions to assess generalizability of performance improvements

2. Behavioral variable ablation: Systematically remove individual behavioral variables to determine which contribute most to prediction accuracy and whether B-MLP modules provide benefits beyond temporal information

3. Attention-receptive field alignment: Quantify correlation between learned attention patterns and physiologically measured receptive fields across different cortical layers and visual areas to validate model's interpretability claims