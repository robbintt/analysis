---
ver: rpa2
title: 'PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware
  Score Calibration'
arxiv_id: '2309.13869'
source_url: https://arxiv.org/abs/2309.13869
tags:
- prism
- relation
- relations
- bertbase
- docre
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low-resource document-level
  relation extraction (DocRE), where existing models overestimate the NA ("no relation")
  label due to limited training data, leading to poor performance. The proposed method,
  PRiSM, leverages relation semantic information to calibrate model predictions.
---

# PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration

## Quick Facts
- arXiv ID: 2309.13869
- Source URL: https://arxiv.org/abs/2309.13869
- Reference count: 26
- Primary result: PRiSM improves DocRE F1 scores by up to 26.38 points and reduces calibration error by up to 36x in low-resource settings

## Executive Summary
This paper addresses the critical challenge of low-resource document-level relation extraction (DocRE), where existing models systematically overestimate the NA ("no relation") label due to limited training data. The proposed PRiSM method leverages relation semantic information from label descriptions to calibrate model predictions through similarity-based logit adjustment. By computing similarity scores between entity pair embeddings and relation embeddings derived from descriptions, PRiSM adaptively adjusts model confidence - reducing overconfidence in frequent relations and NA labels while boosting confidence in rare but correct relations. Evaluated across three DocRE datasets, PRiSM demonstrates substantial improvements in both F1 scores and calibration metrics when trained on minimal data.

## Method Summary
PRiSM enhances DocRE models by integrating relation semantic information into the prediction process. The method first encodes relation descriptions using a pretrained language model to create relation embeddings. For each entity pair, it computes a semantic similarity score between the pair's representation and each relation embedding. These similarity scores are then added to the model's statistical logits to create calibrated predictions. This approach allows the model to leverage semantic knowledge about relations when making predictions, particularly beneficial when training data is scarce. The method is model-agnostic and can be integrated with various state-of-the-art DocRE architectures.

## Key Results
- PRiSM improves F1 scores by up to 26.38 points when trained on just 3% of DocRED data
- Calibration error (ECE) reduced by up to 36 times compared to baseline models
- Consistent performance improvements across three datasets (DocRED, Re-DocRED, DWIE) and multiple base models (BERT, RoBERTa, SSAN, ATLOP)

## Why This Works (Mechanism)

### Mechanism 1
PRiSM reduces overconfidence in frequent relations and NA labels by computing similarity between entity pair embeddings and relation embeddings derived from label descriptions. By incorporating semantic scores alongside statistical scores (logits), PRiSM adjusts model confidence - for common relations and NA labels, the semantic score is often low, penalizing the high statistical score and reducing overconfidence. Core assumption: Relation descriptions capture sufficient semantic information to distinguish between frequent and infrequent relations. Break condition: If relation descriptions are too generic or too specific, the semantic similarity may not effectively distinguish relations.

### Mechanism 2
PRiSM improves performance on rare relations by supplementing logits with relation semantic information. The similarity score between entity pair embeddings and relation embeddings acts as a semantic score that supplements the statistical score. This allows the model to increase confidence in rare but correct relations, even when limited training data is available. Core assumption: The semantic information in relation descriptions is representative of actual relations expressed in text. Break condition: If relation descriptions are not representative, the semantic score may mislead the model.

### Mechanism 3
PRiSM effectively calibrates model predictions, reducing overconfidence in NA labels. By computing similarity scores between entity pair embeddings and relation embeddings, PRiSM adjusts the logits for the NA label - if semantic similarity indicates a relation exists, confidence in NA is reduced. Core assumption: NA labels are often overrepresented and semantic information can distinguish between true NA and missed relations. Break condition: If semantic information cannot effectively distinguish true NA from missed relations, confidence in NA may be incorrectly reduced.

## Foundational Learning

- **Document-level relation extraction (DocRE)**: Extracting relations between entities across entire documents rather than single sentences. Needed here because PRiSM specifically targets DocRE challenges. Quick check: What is the main difference between sentence-level and document-level relation extraction?

- **Calibration in machine learning**: Adjusting model confidence to match prediction accuracy. Needed here because PRiSM aims to improve model calibration by reducing overconfidence. Quick check: What is the difference between expected calibration error (ECE) and adaptive calibration error (ACE)?

- **Semantic similarity between embeddings**: Computing how related two vector representations are. Needed here because PRiSM relies on computing similarity between entity pair and relation embeddings. Quick check: What are some common methods for computing semantic similarity between embeddings?

## Architecture Onboarding

- **Component map**: PLM (relation descriptions) -> Relation embeddings -> Similarity computation (cosine) -> Logit adjustment (addition) -> Calibrated logits
- **Critical path**: Computing similarity scores between entity pair embeddings and relation embeddings, then adjusting logits
- **Design tradeoffs**: PRiSM trades increased computation for improved performance and calibration in low-resource settings. Relies on availability of relation descriptions
- **Failure signatures**: Reduced performance on frequent relations, increased false positives on rare relations, or miscalibration of model confidence
- **First 3 experiments**:
  1. Evaluate PRiSM on a small DocRE dataset with BERTBASE to verify effectiveness
  2. Compare PRiSM with temperature scaling for calibration error reduction
  3. Analyze impact of relation description quality by testing different description sources

## Open Questions the Paper Calls Out

### Open Question 1
How does PRiSM's performance compare on zero-shot or few-shot scenarios in DocRE? The paper mentions the ultimate goal of DocRE is zero-shot performance but focuses on low-resource settings with 3-10% of training data. What evidence would resolve it: Empirical evaluation of PRiSM on zero-shot or few-shot DocRE tasks.

### Open Question 2
Can PRiSM be effectively applied to other information extraction tasks like event extraction or dialogue state tracking? The paper mentions PRiSM could be applied to these tasks which also involve long-tailed data and overestimation of "null" labels. What evidence would resolve it: Experimental results showing PRiSM's performance on event extraction and dialogue state tracking tasks.

### Open Question 3
How does PRiSM's performance scale with longer documents beyond 512 tokens? The paper mentions using naive chunking for DWIE documents and suggests exploring long-sequence modeling for longer documents. What evidence would resolve it: Comparative evaluation using different long-sequence modeling approaches on documents longer than 512 tokens.

## Limitations
- Heavy dependence on quality and representativeness of relation descriptions
- Limited testing across diverse domains beyond scientific and news text
- Computational overhead not fully quantified, particularly for long documents

## Confidence

**High Confidence** (Strong evidence, well-supported claims):
- PRiSM improves F1 scores in low-resource settings (up to 26.38 points)
- PRiSM reduces calibration error significantly (up to 36 times)
- The mechanism of using relation semantic information to adjust logits is technically sound

**Medium Confidence** (Reasonable evidence but with some gaps):
- PRiSM works across different base models (BERT, RoBERTa, SSAN, ATLOP)
- Improvements are consistent across 3%, 10%, and 100% training data splits
- The four-case analysis explaining when PRiSM helps vs. doesn't help is logically sound

**Low Confidence** (Limited evidence or significant uncertainties):
- PRiSM's effectiveness with relation descriptions from sources other than Wikidata
- Performance in domains significantly different from the tested datasets
- Long-term stability and performance on very large document collections

## Next Checks
1. Apply PRiSM to a dataset from a substantially different domain (e.g., social media text or clinical notes) to test generalization beyond scientific and news domains.

2. Conduct an ablation study on relation descriptions by systematically testing PRiSM with different sources of relation descriptions to quantify the impact of description quality on performance.

3. Measure and compare training and inference times for base models versus PRiSM-enhanced models, particularly for long documents in the DWIE dataset, to quantify the computational overhead.