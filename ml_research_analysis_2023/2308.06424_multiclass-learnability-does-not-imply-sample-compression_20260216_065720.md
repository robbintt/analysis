---
ver: rpa2
title: Multiclass Learnability Does Not Imply Sample Compression
arxiv_id: '2308.06424'
source_url: https://arxiv.org/abs/2308.06424
tags:
- compression
- class
- sample
- size
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the relationship between learnability and sample
  compression for multiclass hypothesis classes, where the label space is not binary.
  The author shows that unlike the binary case, learnability does not imply sample
  compression in the multiclass setting.
---

# Multiclass Learnability Does Not Imply Sample Compression

## Quick Facts
- arXiv ID: 2308.06424
- Source URL: https://arxiv.org/abs/2308.06424
- Reference count: 18
- Primary result: Learnability does not imply sample compression in multiclass settings

## Executive Summary
This paper establishes a fundamental separation between learnability and sample compression in multiclass learning, contrasting with the binary case where these notions are equivalent. The author constructs a learnable multiclass hypothesis class with finite DS dimension that provably requires sample compression size scaling with the input sample size, not just the DS dimension. This separation arises from the crucial role of multiple labels in multiclass disambiguation, which cannot be replicated with binary labels alone.

## Method Summary
The paper employs a reduction from graph theory, specifically leveraging results on chromatic number versus biclique partition number from Alon-Saks-Seymour. It constructs a hard-to-compress partial concept class with DS dimension 1 using this graph-theoretic approach. The class is then disambiguated into a total concept class while preserving the DS dimension, demonstrating that the compression lower bound carries over. The key insight is that multiple labels are essential for the disambiguation process, creating the separation from binary compression schemes.

## Key Results
- Learnability does not imply sample compression in multiclass settings
- Any sample compression scheme for the constructed class must have size Ω((log(m))^(1−o(1)))
- The graph dimension of disambiguated classes can become arbitrarily large, preventing direct application of binary compression results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learnability does not imply sample compression in multiclass settings due to the need for multiple labels in disambiguation.
- Mechanism: The paper constructs a hard-to-compress partial concept class with DS dimension 1. It then disambiguates this class into a total concept class using unique labels for each partial concept. This disambiguation preserves the DS dimension but requires the use of multiple labels, which is not possible when restricted to binary labels.
- Core assumption: The DS dimension of the disambiguating class can be preserved while using multiple labels, but not with binary labels.
- Evidence anchors:
  - [abstract]: "Our main result is the following: Theorem 1 (Multiclass Learnability ̸⇒ Compression)."
  - [section 4.2]: "In the disambiguation that we constructed above, we crucially used the power of multiple labels available to us."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.382, average citations=0.0. Top related titles: Sample Compression Scheme Reductions, Ramsey Theorems for Trees and a General 'Private Learning Implies Online Learning' Theorem, Characterizing the Multiclass Learnability of Forgiving 0-1 Loss Functions.
- Break condition: If the disambiguation process could be done with only binary labels while preserving the DS dimension, the lower bound would not hold.

### Mechanism 2
- Claim: The size of any sample compression scheme in the multiclass setting must scale with the sample size, not just the DS dimension.
- Mechanism: The paper uses a reduction from a result in graph theory (Alon-Saks-Seymour problem) to construct a partial concept class that is hard to compress. This class has DS dimension 1 but requires compression size Ω((log(m))^(1−o(1))), where m is the sample size.
- Core assumption: The graph-theoretic construction can be translated into a learning-theoretic lower bound on compression size.
- Evidence anchors:
  - [abstract]: "any sample compression scheme for H must have size Ω((log(m))^(1−o(1))), where m is the size of an input sequence realizable by H."
  - [section 3]: "Our main ingredient we use to establish Theorem 1 is the following result from the work of [AHHM22]."
  - [corpus]: "Distribution Learnability and Robustness" - examines the relationship between learnability and robust learnability.
- Break condition: If a different construction could achieve the same DS dimension with constant compression size, the lower bound would be invalidated.

### Mechanism 3
- Claim: The graph dimension, not just DS dimension, becomes relevant in multiclass compression schemes.
- Mechanism: While the DS dimension characterizes learnability in multiclass settings, the paper shows that the graph dimension of the disambiguated class becomes arbitrarily large, making the O(dG ·2dG) upper bound from [MY16] not meaningful for establishing compression-learnability equivalence.
- Core assumption: The disambiguation process necessarily increases the graph dimension of the class.
- Evidence anchors:
  - [section 4.1]: "On the other hand, the graph dimension of the disambiguating class can (and must) increase arbitrarily."
  - [abstract]: "Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size."
  - [corpus]: "Multiclass Online Learning and Uniform Convergence" - studies the relationship between Littlestone dimension and online learnability.
- Break condition: If the graph dimension could be bounded by a function of the DS dimension, the separation between learnability and compression might not hold.

## Foundational Learning

- Concept: DS dimension and its role in characterizing multiclass learnability
  - Why needed here: The paper relies on the DS dimension being the appropriate analog of VC dimension for multiclass learnability, as established by [BCD+22].
  - Quick check question: Can you explain why the DS dimension is necessary and sufficient for multiclass learnability, unlike the VC dimension for binary classes?

- Concept: Partial concept classes and disambiguation
  - Why needed here: The lower bound construction uses partial concept classes that can be undefined in certain regions, and then disambiguates them into total concept classes.
  - Quick check question: How does the definition of DS-shatter differ for partial concept classes compared to total concept classes?

- Concept: Sample compression schemes and their relationship to learnability
  - Why needed here: The paper builds on the result that compression implies learnability in both binary and multiclass settings, but shows the converse fails for multiclass.
  - Quick check question: Why does the principle of uniform convergence fail in the multiclass setting, making sample compression more challenging?

## Architecture Onboarding

- Component map: Graph theory reduction -> Partial concept class construction -> Disambiguation process -> Compression size lower bound derivation -> Graph dimension analysis

- Critical path:
  1. Construct partial concept class H with DS dimension 1
  2. Show H is hard to compress (size Ω((log(m))^(1−o(1))))
  3. Disambiguate H to total class H while preserving DS dimension
  4. Show H inherits compression lower bound from H
  5. Demonstrate graph dimension of H is unbounded

- Design tradeoffs:
  - Using multiple labels enables DS dimension preservation but prevents binary-only compression
  - Graph-theoretic reduction provides strong lower bounds but requires careful translation to learning theory
  - Partial-to-total class disambiguation is powerful but increases graph dimension

- Failure signatures:
  - If DS dimension of disambiguated class increases beyond 1
  - If compression size can be bounded by a function of DS dimension alone
  - If the graph-theoretic reduction doesn't translate to a valid learning lower bound

- First 3 experiments:
  1. Verify the DS dimension of the constructed partial class H is indeed 1
  2. Test the compression size lower bound on small instances of H
  3. Check that the disambiguated class H preserves the DS dimension while increasing graph dimension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the lower bound of Ω((log(m))^(1-o(1))) for sample compression in multiclass learning be improved to a tight bound?
- Basis in paper: [explicit] The paper establishes that any sample compression scheme for the constructed multiclass hypothesis class must have size k(m) = Ω((log(m))^(1-o(1))).
- Why unresolved: The lower bound is not tight and leaves open the question of the exact asymptotic behavior of the compression size as a function of sample size m.
- What evidence would resolve it: Either proving a matching upper bound of O((log(m))^(1+o(1))) for the constructed class, or constructing another multiclass class with a different asymptotic lower bound.

### Open Question 2
- Question: Is there a meaningful relationship between DS dimension and sample compression size in multiclass learning, beyond the trivial lower bound shown in this paper?
- Basis in paper: [explicit] The paper shows that learnability (finite DS dimension) does not imply compression of size only a function of DS dimension, but doesn't explore whether any relationship exists.
- Why unresolved: While the paper rules out one potential relationship, it leaves open whether other relationships between DS dimension and compression size might exist.
- What evidence would resolve it: Either proving that compression size must depend on both DS dimension and sample size in a specific way, or constructing classes with finite DS dimension that require different compression sizes.

### Open Question 3
- Question: Can the techniques from [MY16] for binary classes be adapted to achieve sub-exponential compression in terms of graph dimension for multiclass classes?
- Basis in paper: [explicit] The paper discusses why [MY16]'s techniques don't directly apply to the multiclass setting and notes that achieving sub-exponential compression in terms of dG remains open.
- Why unresolved: The fundamental obstacles identified (lack of uniform convergence, need for improper learners, unbounded dual DS dimension) may be insurmountable or may have workarounds not yet discovered.
- What evidence would resolve it: Either successfully adapting [MY16]'s techniques to achieve sub-exponential compression in terms of dG, or proving that such compression is impossible for certain classes of multiclass hypothesis classes.

### Open Question 4
- Question: How does the choice of disambiguation strategy affect the learnability and compressibility of the resulting total concept class?
- Basis in paper: [explicit] The paper uses a specific disambiguation strategy that preserves DS dimension but not graph dimension, and notes that binary disambiguations would result in unlearnable classes.
- Why unresolved: The paper only explores one disambiguation approach and doesn't characterize the space of possible disambiguations or their effects on learnability and compressibility.
- What evidence would resolve it: Either classifying disambiguation strategies based on their effects on various complexity measures (DS dimension, graph dimension, compression size), or constructing examples showing different disambiguation strategies lead to different compressibility properties.

## Limitations

- The construction relies on non-constructive graph existence proofs, making it difficult to verify the tightness of the Ω((log(m))^(1−o(1))) bound
- The relationship between DS dimension preservation and graph dimension increase in disambiguation is not explicitly proven for all possible disambiguations
- The paper assumes that multiple labels are essential for the separation but doesn't fully explore alternative approaches

## Confidence

- High confidence in the separation result itself: The core argument that binary and multiclass settings differ in their compression-learnability relationship is well-supported
- Medium confidence in the specific lower bound: While the graph-theoretic reduction is sound, the exact compression size lower bound depends on parameters from cited works
- Medium confidence in the role of graph dimension: The paper argues this becomes relevant in multiclass settings, but the relationship between DS dimension and graph dimension in disambiguated classes could be explored further

## Next Checks

1. Verify the DS dimension preservation: Explicitly check that sequences DS-shattered by the partial class H remain DS-shattered by the disambiguating class H for a concrete small example
2. Test the compression lower bound: Implement the partial concept class construction on a small graph instance and empirically verify the compression size requirements
3. Analyze alternative disambiguations: Investigate whether different disambiguation strategies could preserve both DS dimension and bound the graph dimension, potentially finding a counterexample to the main separation claim