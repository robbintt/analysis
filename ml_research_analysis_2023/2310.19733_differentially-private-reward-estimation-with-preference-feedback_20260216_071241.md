---
ver: rpa2
title: Differentially Private Reward Estimation with Preference Feedback
arxiv_id: '2310.19733'
source_url: https://arxiv.org/abs/2310.19733
tags:
- have
- bound
- where
- error
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reward estimation in preference-based learning
  under differential privacy. The authors address the problem of learning a reward
  parameter from pairwise human feedback while protecting privacy of individual labelers.
---

# Differentially Private Reward Estimation with Preference Feedback

## Quick Facts
- arXiv ID: 2310.19733
- Source URL: https://arxiv.org/abs/2310.19733
- Reference count: 40
- This paper establishes fundamental privacy-utility tradeoffs for reward estimation from pairwise human preference feedback under differential privacy

## Executive Summary
This paper studies reward estimation in preference-based learning under differential privacy, specifically addressing the Bradley-Terry-Luce (BTL) model for pairwise comparisons. The authors tackle the problem of learning reward parameters from human preference feedback while protecting the privacy of individual labelers through label differential privacy. They provide tight upper and lower bounds on estimation error under both local and central differential privacy models, along with efficient algorithms based on stochastic gradient descent that achieve these bounds.

## Method Summary
The paper considers two privacy models: local differential privacy where each label is privatized before observation, and central differential privacy where raw data is available but outputs must be private. For local DP, they use Randomized Response mechanism combined with a de-biased loss function to achieve an additional error cost of Θ(1/(e^ε-1)√(d/n)). For central DP, they employ objective perturbation with Gaussian noise to achieve an additional error cost of Θ(poly(d)/(εn)). The methods are based on maximum likelihood estimation adapted for privacy constraints, with SGD providing computationally efficient approximations to exact minimization.

## Key Results
- Local DP model achieves additional error cost of Θ(1/(e^ε-1)√(d/n)) compared to non-private case
- Central DP model achieves additional error cost of Θ(poly(d)/(εn)) using objective perturbation
- Both models provide matching upper and lower bounds establishing fundamental privacy-utility tradeoffs
- Efficient SGD-based algorithms achieve the same error bounds as exact minimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The de-biased loss function successfully compensates for label randomization in the local DP model
- Mechanism: The proposed loss function uses modified predicted scores (bpi,1 and bpi,0) that are specifically designed to have the same log-odds under randomization as in the clear-text case, effectively undoing the privacy noise
- Core assumption: The randomized response mechanism preserves sufficient information about the original label through its symmetric properties
- Evidence anchors:
  - [section] "logbpi,1/bpi,0=log σ(x⊤i θ)1 − σ(x⊤i θ)=logit(pi,1)" shows the de-biasing property
  - [section] "Hence the loss functionblD,ε(θ) essentially de-biases the effect of randomization"
- Break condition: If the privacy parameter ε is too small (high privacy), the de-biasing becomes ineffective as the signal becomes too weak

### Mechanism 2
- Claim: Objective perturbation with Gaussian noise achieves tight error bounds in the central DP model
- Mechanism: Adding Gaussian noise to the objective function creates sufficient privacy protection while maintaining estimation accuracy through careful calibration of the noise parameter
- Core assumption: The gradient of the log-likelihood is Lipschitz continuous and the parameter space is bounded
- Evidence anchors:
  - [abstract] "they show that the additional cost is Θ(poly(d)/(εn))" under central model
  - [section] "our estimator is given bybθobj = argminθ∈ΘB lD(θ) + β2 ∥θ∥2 2 + w⊤θ" where w ∼ N (0, σ2I)
- Break condition: If the regularization parameter β is not properly calibrated with the noise level σ

### Mechanism 3
- Claim: SGD-based estimators achieve the same error bounds as exact minimization while being computationally efficient
- Mechanism: The SGD updates maintain unbiased gradients through careful construction of the gradient estimator that accounts for the randomization in local DP
- Core assumption: The coverage assumption (λmin(Σ) ≥ κ) holds, ensuring sufficient exploration of the feature space
- Evidence anchors:
  - [section] "bgt = P y∈{0,1} ∇bθt log pt,y eε + 1 − ∇bθt log pt,eyt" shows the unbiased gradient construction
  - [section] "E[bgt|xt, yt,bθt]=(2 σ(ε) − 1) gt" demonstrates the unbiasedness property
- Break condition: If the learning rate schedule is not properly tuned, convergence may fail

## Foundational Learning

- Concept: Differential Privacy (DP) and its variants (label DP, local vs central models)
  - Why needed here: The entire paper builds on different privacy models to protect sensitive preference feedback while maintaining estimation accuracy
  - Quick check question: What is the key difference between label DP and standard DP in the context of this preference feedback problem?

- Concept: Bradley-Terry-Luce (BTL) model for pairwise comparisons
  - Why needed here: The BTL model provides the probabilistic framework for modeling human preference feedback
  - Quick check question: How does the sigmoid function relate to the probability of preferring one action over another in the BTL model?

- Concept: Minimax estimation framework
  - Why needed here: The paper establishes fundamental limits on estimation error under privacy constraints using minimax theory
  - Quick check question: What is the relationship between the minimax risk and the estimation error bounds proved in this paper?

## Architecture Onboarding

- Component map:
  Data collection layer -> Privacy mechanism layer -> Estimation layer -> Validation layer

- Critical path:
  1. Collect pairwise comparison data (si, a0i, a1i, yi)
  2. Apply privacy mechanism (RR or objective perturbation)
  3. Estimate reward parameter θ* using appropriate estimator
  4. Verify privacy guarantees and error bounds

- Design tradeoffs:
  - Local vs central DP: Local DP provides stronger privacy but higher estimation error (1/(e^ε-1)√(d/n) vs poly(d)/(εn))
  - Exact vs approximate minimization: Exact minimization provides better error bounds but is computationally expensive; SGD provides computational efficiency with slightly weaker bounds
  - Feature dimension vs privacy budget: Higher dimensional features require more samples or privacy budget to maintain accuracy

- Failure signatures:
  - Estimation error increases dramatically when ε approaches 0 (high privacy)
  - Coverage assumption violation (λmin(Σ) < κ) leads to unbounded error
  - Improper regularization parameter β causes privacy guarantee failure

- First 3 experiments:
  1. Verify the de-biasing property by comparing estimation error of the proposed loss function vs standard MLE under varying ε
  2. Test the SGD convergence by monitoring estimation error over iterations for different learning rates
  3. Validate the privacy-utility tradeoff by plotting estimation error vs privacy budget ε for both local and central models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dependency on dimension d in the estimation error bounds for both local and central label-DP models?
- Basis in paper: [explicit] The authors note that their bounds match up to factors of eLB or d, and ask whether these gaps can be closed.
- Why unresolved: The current bounds have gaps of O(eLB) for local model and O(d) for central model compared to lower bounds.
- What evidence would resolve it: Tight upper bounds matching the lower bounds exactly, or proofs that these gaps are inherent.

### Open Question 2
- Question: How does the performance of downstream policies (e.g., pessimistic or greedy policies) trained using the private reward estimates compare to non-private counterparts in practice?
- Basis in paper: [explicit] The authors mention extending their theoretical analysis to evaluate downstream policy performance, similar to Zhu et al. [ZJJ23], as an important direction.
- Why unresolved: The paper focuses on reward estimation error bounds and does not empirically evaluate policy performance.
- What evidence would resolve it: Empirical results comparing policy performance (e.g., regret, reward) between private and non-private reward models on benchmark RL tasks.

### Open Question 3
- Question: Can efficient algorithms be developed for local standard differential privacy in the non-interactive offline reward estimation setting?
- Basis in paper: [explicit] The authors state that establishing local standard DP is challenging in their offline setting and leave it as a future research direction.
- Why unresolved: Local standard DP requires protecting both features and labels, which is more complex than label DP, especially in non-interactive settings.
- What evidence would resolve it: A practical algorithm achieving local standard DP with provable utility bounds for offline reward estimation.

## Limitations

- The local DP model shows severe error degradation when privacy parameter ε is small, with no clear threshold identified for practical usability
- The coverage assumption (λmin(Σ) ≥ κ) is critical for error bounds but may be violated in practice with limited exploration
- The extension to Thurstone and Plackett-Luce models is mentioned but lacks thorough validation

## Confidence

- **High confidence**: The core theoretical framework for both local and central DP models, including the de-biasing mechanism for local DP and objective perturbation for central DP. The minimax lower bounds appear well-established through standard information-theoretic arguments.
- **Medium confidence**: The SGD convergence guarantees and practical performance, as the paper doesn't provide extensive empirical validation across diverse datasets. The extension to other preference models is mentioned but lacks detailed analysis.
- **Medium confidence**: The simulation results, as they only use synthetic data and don't demonstrate performance on real-world preference feedback datasets.

## Next Checks

1. **Practical privacy-utility threshold**: Conduct experiments to identify the minimum ε value where the estimation error becomes unacceptably high, helping practitioners understand when local DP becomes impractical.

2. **Coverage assumption violation**: Systematically test the estimators when the coverage assumption is violated by using datasets with limited state-action coverage, measuring how quickly the error bounds deteriorate.

3. **Real-world validation**: Apply the proposed methods to a real preference feedback dataset (e.g., from recommendation systems or human-robot interaction) to validate the synthetic data results and assess practical utility.