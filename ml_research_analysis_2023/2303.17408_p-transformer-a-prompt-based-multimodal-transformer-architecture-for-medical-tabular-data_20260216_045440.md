---
ver: rpa2
title: 'P-Transformer: A Prompt-based Multimodal Transformer Architecture For Medical
  Tabular Data'
arxiv_id: '2303.17408'
source_url: https://arxiv.org/abs/2303.17408
tags:
- medical
- cell
- data
- encoder
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a language-enhanced transformer-based framework
  for medical intervention duration estimation using multi-modal EHR data. The method
  integrates structured and unstructured data through medical prompts and a pre-trained
  sentence encoder, followed by a cell transformer encoder to generate patient embeddings.
---

# P-Transformer: A Prompt-based Multimodal Transformer Architecture For Medical Tabular Data

## Quick Facts
- arXiv ID: 2303.17408
- Source URL: https://arxiv.org/abs/2303.17408
- Authors: 
- Reference count: 32
- Key outcome: 10.9-11.0% reductions in RMSE/MAE and 1.6-2.2% improvements in classification metrics over state-of-the-art baselines

## Executive Summary
This paper introduces P-Transformer, a novel transformer-based framework for estimating medical intervention duration using multi-modal electronic health record (EHR) data. The approach integrates structured and unstructured data through medical prompts and a pre-trained sentence encoder, followed by a cell transformer encoder to generate patient embeddings. Experiments on PASA and MIMIC-III datasets demonstrate significant improvements over existing methods, with robust performance even under data corruption scenarios.

## Method Summary
The P-Transformer architecture processes multi-modal EHR data through a prompt-based approach that converts raw cell values into natural language sentences using medical templates. These prompts are encoded using a pre-trained sentence encoder (SimCSE) to generate cell embeddings. A cell transformer encoder processes these embeddings without positional encoding or [CLS] tokens, followed by a masked pooling layer to handle missing free-text values. The model treats medical intervention duration as an ordinal classification problem and employs an ordinal regression head for predictions.

## Key Results
- 10.9-11.0% reductions in RMSE/MAE compared to state-of-the-art baselines
- 1.6-2.2% improvements in classification metrics (BACC and AUROC)
- Robust performance under data corruption scenarios
- Significant improvement when incorporating free-text information

## Why This Works (Mechanism)

### Mechanism 1
Medical prompts transform sparse or non-sentential cell values into natural language sentences, allowing a pre-trained sentence encoder to generate richer cell embeddings. The prompt construction module applies feature-specific templates to convert raw cell values into contextualized sentences. Core assumption: The pre-trained sentence encoder was trained on natural sentences and thus can better encode prompt-transformed inputs than raw values. Evidence: "raw cell values may not produce optimal cell embeddings with the pre-trained encoder as most of them consist of words or phrases rather than natural sentences."

### Mechanism 2
Removing positional encoding and the [CLS] token from the cell transformer encoder avoids introducing order bias and improves patient embedding expressiveness. In tabular EHR data, feature columns are not inherently ordered. Core assumption: The model can learn meaningful relationships between features without explicit positional information. Evidence: "we remove the positional encoding from our cell transformer encoder architecture" and "we remove the [CLS] token and introduce a masked pooling layer."

### Mechanism 3
The masked pooling layer mitigates the impact of missing free-text values by ignoring embeddings from missing cells during patient embedding aggregation. A binary mask identifies missing cells and zeros out their contributions before mean pooling. Core assumption: Free-text columns are more likely to be missing than structured columns, and ignoring them is preferable to imputing. Evidence: "we design a masked pooling layer to address this problem" and "we obtain the patient embeddings: pi = MeanPooling(g′ i⊙oi)."

## Foundational Learning

- Concept: Pre-trained sentence encoders (e.g., RoBERTa-based SimCSE)
  - Why needed here: They provide high-quality contextualized embeddings for medical text without requiring task-specific training from scratch.
  - Quick check question: Why might using a vanilla random-initialized encoder underperform a pre-trained one on medical prompts?

- Concept: Prompt-based learning in NLP
  - Why needed here: It bridges the gap between non-sentential tabular data and sentence-level language models.
  - Quick check question: What is the difference between a prompt with unfilled slots and the medical language templates used here?

- Concept: Ordinal regression vs. multi-class classification
  - Why needed here: Medical intervention duration has an inherent ordering (e.g., 0-1h < 1-2h < 2-3h), which should be preserved in predictions.
  - Quick check question: How does the OR head transform an ordinal regression problem into binary classification sub-tasks?

## Architecture Onboarding

- Component map: Raw tabular data → Prompt construction module → Medical prompts → Pre-trained sentence encoder → Cell embeddings → Cell transformer encoder → Patient embeddings → OR head → Duration prediction
- Critical path: Prompt construction → Sentence encoder → Transformer encoder → Masked pooling → OR head
- Design tradeoffs:
  - Using pre-trained encoders trades computation speed for better embeddings
  - Removing positional encoding simplifies the model but assumes no positional bias exists
  - Masked pooling avoids imputing free-text but may lose information if many cells are missing
- Failure signatures:
  - Poor prompts → Degraded embeddings → Higher RMSE/MAE
  - Missing positional information when features are ordered → Inconsistent attention patterns
  - High missingness in free-text → Mean pooling on sparse data → Unstable patient embeddings
- First 3 experiments:
  1. Replace medical prompts with raw cell values and measure change in RMSE/MAE
  2. Add positional encoding back in and evaluate impact on classification performance
  3. Remove the masked pooling layer and impute missing free-text; compare robustness to data corruption

## Open Questions the Paper Calls Out

### Open Question 1
How would the model perform if trained on longitudinal EHR data rather than single-time-point data? Basis: The authors mention that the current study uses data collected at a single time point prior to medical intervention, failing to capture longitudinal patient information. Unresolved because the paper only evaluates the model on cross-sectional data. Evidence needed: Experimental results comparing the proposed model's performance on longitudinal vs. single-time-point data.

### Open Question 2
How would the model's robustness to data corruption change if different imputation strategies were used for missing values in continuous and categorical features? Basis: The paper mentions that missing values in continuous data are imputed with the mean value of the corresponding column, but does not explore alternative imputation strategies. Unresolved because the paper only uses mean imputation for continuous features. Evidence needed: Comparative analysis of the model's performance under different imputation strategies (e.g., median, mode, k-nearest neighbors) in the presence of data corruption.

### Open Question 3
How would the model's performance be affected if the medical prompts were generated using a more sophisticated natural language generation technique, such as GPT-3 or T5? Basis: The authors mention that they use pre-defined medical language templates to transform raw cell values into natural sentences, but do not explore more advanced NLG techniques. Unresolved because the paper uses a simple template-based approach for generating medical prompts. Evidence needed: Experimental results comparing the proposed model's performance when using different NLG techniques for generating medical prompts.

## Limitations

- The prompt construction methodology relies on predefined medical templates that are not fully specified, making exact replication challenging
- Evaluation focuses on two specific medical datasets with particular missingness patterns, limiting generalizability to other domains
- Computational overhead of pre-trained sentence encoders and cell transformer architecture compared to simpler baselines is not discussed

## Confidence

- High confidence: The mechanism of using medical prompts to enhance cell embeddings is well-supported by the observed 10.9-11.0% improvements in RMSE/MAE metrics
- Medium confidence: The architectural decisions (removing positional encoding and [CLS] token) are justified but could benefit from ablation studies on datasets with different feature ordering characteristics
- Medium confidence: The masked pooling layer's effectiveness assumes missingness is random, which may not hold in real-world clinical data

## Next Checks

1. **Prompt Design Ablation**: Systematically vary prompt template complexity and specificity to determine optimal prompt construction strategies, measuring impact on cell embedding quality and downstream prediction accuracy.

2. **Missingness Pattern Analysis**: Evaluate model performance under different missing data mechanisms (MCAR, MAR, MNAR) to validate the masked pooling layer's robustness assumptions and identify scenarios where it may introduce bias.

3. **Cross-Domain Transferability**: Apply the P-Transformer architecture to non-medical tabular datasets with mixed feature types to assess whether the medical prompts and architectural choices generalize beyond clinical data contexts.