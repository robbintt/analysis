---
ver: rpa2
title: Multimodal Speech Recognition for Language-Guided Embodied Agents
arxiv_id: '2302.14030'
source_url: https://arxiv.org/abs/2302.14030
tags:
- multimodal
- instructions
- visual
- speech
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a multimodal ASR model to improve spoken instruction
  transcription for language-guided embodied agents by incorporating visual context.
  To test this, a dataset of synthetic spoken instructions is created by applying
  TTS to ALFRED instructions, with audio degraded by systematically masking words.
---

# Multimodal Speech Recognition for Language-Guided Embodied Agents

## Quick Facts
- arXiv ID: 2302.14030
- Source URL: https://arxiv.org/abs/2302.14030
- Reference count: 0
- Primary result: Multimodal ASR recovers up to 30% more masked words than unimodal models, improving downstream task success for embodied agents

## Executive Summary
This work introduces a multimodal ASR model that incorporates visual context to improve transcription of spoken instructions for language-guided embodied agents. By synthesizing spoken instructions from the ALFRED dataset and systematically masking words with Gaussian noise, the authors demonstrate that visual features help recover masked words, especially those corresponding to visually salient objects. The approach generalizes to unseen environments and unheard speakers, and using multimodal ASR transcriptions leads to higher task success rates for text-trained embodied agents.

## Method Summary
The method involves generating synthetic spoken instructions from ALFRED text using TTS, applying controlled word masking with Gaussian noise, and training both unimodal (wav2vec2.0 + Transformer decoder) and multimodal (same + CLIP visual features) ASR models. The multimodal model concatenates CLIP features to decoder input embeddings. Models are evaluated using WER and RR metrics across different noise policies, environments, and speakers. A text-trained ALFRED agent is then tested on transcribed instructions to measure downstream task performance.

## Key Results
- Multimodal models recover up to 30% more masked words than unimodal baselines
- Visual context helps most for visually salient words that are masked
- Multimodal ASR improves downstream task success for text-trained embodied agents
- Performance generalizes to unseen environments and unheard speakers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual context helps recover masked words in noisy speech by providing grounding for ambiguous language
- Mechanism: The multimodal ASR model receives both degraded speech encodings and visual scene features. When a word is masked in the audio, the model can infer it from the visual context (e.g., seeing a microwave allows inferring "microwave" instead of "fridge").
- Core assumption: Masked words correspond to visually salient objects or locations in the scene
- Evidence anchors:
  - [abstract]: "multimodal models recovering up to 30% more masked words than unimodal baselines"
  - [section]: "We find that the additional visual context is useful in both seen and unseen household environments"
  - [corpus]: Weak - no direct corpus evidence of visual saliency helping masked word recovery
- Break condition: If masked words are not visually salient (e.g., function words like "the" or "in"), visual context provides no benefit

### Mechanism 2
- Claim: Multimodal ASR generalizes better to new speakers because visual features provide speaker-invariant regularization
- Mechanism: Visual observations provide a consistent signal across different speakers, helping the model learn features that are robust to speaker variation rather than speaker-specific acoustic patterns.
- Core assumption: The visual context is consistent for the same instruction regardless of speaker
- Evidence anchors:
  - [abstract]: "generalizes to unseen environments and unheard speakers"
  - [section]: "We also find that a text-trained embodied agent successfully completes tasks more often by following transcribed instructions from multimodal ASR models"
  - [corpus]: Weak - no direct corpus evidence of speaker generalization benefits
- Break condition: If visual context varies significantly between speakers for the same instruction, the regularization benefit disappears

### Mechanism 3
- Claim: Multimodal models reduce task completion failures by providing more accurate transcriptions in noisy conditions
- Mechanism: The downstream embodied agent receives more accurate text instructions from multimodal ASR, leading to better action selection and task completion.
- Core assumption: Transcription accuracy directly impacts the embodied agent's ability to complete tasks
- Evidence anchors:
  - [abstract]: "a text-trained ALFRED agent achieves higher task success rates compared to using unimodal ASR-transcribed instructions"
  - [section]: "we demonstrate that multimodal ASR can mitigate the effect of noise in spoken instructions on a text-trained embodied agent's ability to complete tasks"
  - [corpus]: Weak - no direct corpus evidence of task completion improvements
- Break condition: If the embodied agent relies on other modalities (like visual scene understanding) more than text instructions, transcription accuracy becomes less critical

## Foundational Learning

- Concept: Wav2vec 2.0 speech encoder architecture
  - Why needed here: Understanding how pre-trained speech representations work is crucial for modifying or extending the ASR pipeline
  - Quick check question: What is the dimensionality of wav2vec 2.0 output features and how does masking work during pre-training?

- Concept: CLIP image encoder and multimodal fusion
  - Why needed here: The visual modality is integrated through CLIP features concatenated to decoder embeddings
  - Quick check question: How are CLIP features projected to match the decoder embedding dimension, and what happens if you use a different image encoder?

- Concept: Word Error Rate (WER) and Recovery Rate (RR) metrics
  - Why needed here: These metrics quantify the performance improvement from visual context
  - Quick check question: How does WER differ from character error rate, and why is RR more informative for masked word recovery?

## Architecture Onboarding

- Component map:
  - Speech input → Wav2vec 2.0 encoder (frozen) → Speech encodings
  - Visual input → CLIP-ViT encoder (frozen) → CLIP feature vector
  - Decoder: 4-layer Transformer with CLIP features concatenated to word embeddings
  - Output: Transcribed text sequence

- Critical path: Speech → Wav2vec 2.0 → Decoder → Text
  Visual → CLIP → Decoder → Text (parallel path)

- Design tradeoffs:
  - Using frozen pre-trained encoders vs. fine-tuning them
  - Concatenating visual features vs. cross-attention mechanisms
  - Simple linear projection vs. learned multimodal fusion

- Failure signatures:
  - High WER with no improvement from visual context suggests visual features aren't being utilized
  - Visual features causing performance degradation suggests misalignment in feature spaces
  - Training instability suggests learning rate or projection layer issues

- First 3 experiments:
  1. Compare unimodal vs. multimodal WER on clean speech to establish baseline visual benefit
  2. Test different masking rates (20%, 40%, 100%) to find optimal degradation level for visual help
  3. Evaluate generalization to unseen environments to measure visual context transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multimodal ASR models perform when dealing with real human speech instead of synthetic TTS speech?
- Basis in paper: [explicit] The paper states it presents a proof-of-concept using synthetic speech datasets and calls for future work to investigate generalization to real human speakers.
- Why unresolved: The study only uses TTS-generated speech, which may not capture the full complexity and variability of real human speech.
- What evidence would resolve it: Testing the multimodal ASR models on a dataset of real human speech instructions and comparing performance to TTS speech.

### Open Question 2
- Question: Can incorporating 3D scene representations or active visual search strategies improve multimodal ASR performance further?
- Basis in paper: [explicit] The paper mentions exploring 3D scene representations and active visual search as future directions.
- Why unresolved: The study only uses a single visual observation per timestep, which may not capture the full visual context.
- What evidence would resolve it: Training and evaluating multimodal ASR models that utilize 3D scene representations or actively search the visual environment.

### Open Question 3
- Question: How does multimodal ASR performance change with different types of audio degradation beyond word masking?
- Basis in paper: [explicit] The paper focuses on word-level masking as a controlled way to simulate noise, but acknowledges this may not reflect all real-world noise scenarios.
- Why unresolved: Only systematic word masking was tested, leaving other noise types unexplored.
- What evidence would resolve it: Evaluating multimodal ASR models on instructions with various audio degradations (e.g., background noise, reverberation, accent variations).

## Limitations

- Relies on synthetic TTS data that may not capture real-world speech complexity
- Uses simple feature concatenation rather than sophisticated cross-modal attention mechanisms
- Focuses on word-level recovery rates without assessing semantic preservation

## Confidence

Our confidence in the proposed mechanisms is Medium to High for visually grounded word recovery, but Medium for generalization claims and Low for downstream task improvement attribution.

## Next Checks

1. Test the multimodal ASR model on naturally spoken instructions from diverse speakers in real environments, comparing performance against synthetic TTS data to quantify the gap between controlled and naturalistic conditions.

2. Replace the concatenation-based fusion with cross-attention mechanisms and compare recovery rates, particularly for complex instructions requiring reasoning about spatial relationships or abstract concepts.

3. Develop metrics beyond word error rate to evaluate whether recovered instructions maintain semantic equivalence to the original, including human evaluation studies on instruction interpretability and actionability.