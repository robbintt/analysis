---
ver: rpa2
title: Multimodal Stress Detection Using Facial Landmarks and Biometric Signals
arxiv_id: '2311.03606'
source_url: https://arxiv.org/abs/2311.03606
tags:
- stress
- signals
- facial
- detection
- landmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal approach for stress detection
  that integrates facial landmarks and biometric signals using deep learning models.
  The method employs 1D-CNN for processing biometric signals and 2D-CNN for facial
  landmarks, combining them through both early-fusion and late-fusion techniques.
---

# Multimodal Stress Detection Using Facial Landmarks and Biometric Signals

## Quick Facts
- arXiv ID: 2311.03606
- Source URL: https://arxiv.org/abs/2311.03606
- Reference count: 40
- Primary result: Early-fusion achieves 98.38% accuracy for multimodal stress detection

## Executive Summary
This paper presents a multimodal approach for stress detection that integrates facial landmarks and biometric signals using deep learning models. The method employs 1D-CNN for processing biometric signals and 2D-CNN for facial landmarks, combining them through both early-fusion and late-fusion techniques. The study evaluates model performance using rigorous leave-one-subject-out validation on a dataset of 20 participants, demonstrating that combining facial and physiological signals improves stress detection accuracy compared to unimodal approaches.

## Method Summary
The proposed approach preprocesses physiological signals (HR, EDA, TEMP, ACC) and facial landmarks from the EmpathicSchool dataset, extracting 30 biometric and 100 landmark features using tsfresh. Features are selected via Lasso regularization and normalized before being fed into three model architectures: 1D-CNN for biometrics, 2D-CNN for landmarks, and FCDNN for concatenated features. Early-fusion concatenates features before CNN layers, while late-fusion processes modalities separately then combines. Models are evaluated using leave-one-subject-out cross-validation across 20 participants.

## Key Results
- Early-fusion achieved highest accuracy of 98.38% for stress detection
- Late-fusion achieved 94.39% accuracy, lower than early-fusion approach
- 2D-CNN outperforms 1D-CNN for facial landmark processing due to spatial pattern recognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early-fusion captures more cross-modal interactions than late-fusion
- Mechanism: Concatenating feature vectors before CNN layers enables joint representation learning that integrates spatial facial patterns with temporal physiological dynamics
- Core assumption: Features from both modalities are temporally aligned within the 40s window and carry complementary stress-related information
- Evidence anchors: Early-fusion accuracy (98.38%) vs late-fusion (94.39%), early fusion combines features at early processing stage
- Break condition: If temporal misalignment between facial and physiological signals exceeds 40s window, joint feature learning degrades

### Mechanism 2
- Claim: 2D-CNN is better suited for facial landmark data than 1D-CNN
- Mechanism: Treating landmarks as image-like 2D data allows convolutional filters to detect spatial patterns directly rather than modeling them as independent time series
- Core assumption: Landmark positions encode expressive geometry that is spatially coherent and meaningful to stress detection
- Evidence anchors: 2D-CNN provides better performance for facial landmarks, focus on spatial arrangement of landmarks
- Break condition: If landmarks are treated as high-frequency temporal signals, 1D-CNN may capture useful trends lost in 2D-CNN

### Mechanism 3
- Claim: Leave-One-Subject-Out cross-validation provides realistic generalization estimate
- Mechanism: Holding out all samples from a single participant during training prevents overfitting to subject-specific patterns
- Core assumption: Stress-related patterns are consistent across individuals once subject-specific noise is removed
- Evidence anchors: Prevents overfitting, offers realistic gauge of model's ability to generalize
- Break condition: If dataset is too small or participants too heterogeneous, LOOCV variance becomes prohibitive

## Foundational Learning

- Concept: Difference between multivariate and multimodal learning
  - Why needed here: To understand why integrating facial landmarks (vision) with biometrics (physiology) is a true multimodal problem
  - Quick check question: If I feed only heart rate and skin temperature to a model, is that multimodal? Why or why not?

- Concept: Sliding window feature extraction and temporal alignment
  - Why needed here: Stress detection relies on aggregating short-term physiological and facial cues; misalignment can corrupt joint features
  - Quick check question: If my facial data is at 30fps and biometric data at 1Hz, how should I align them in a 40-second window?

- Concept: Feature selection methods (filter, wrapper, embedded)
  - Why needed here: The study compares six methods; understanding tradeoffs explains why Lasso regularization was chosen
  - Quick check question: Which feature selection method would you use if you want both feature ranking and built-in regularization?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature extraction (tsfresh) -> Feature selection (Lasso) -> 1D-CNN/2D-CNN/FCDNN models -> LOOCV evaluation
- Critical path: 1) Extract 30 biometric + 100 landmark features (Lasso-selected) 2) Feed concatenated vector into 1D-CNN early-fusion architecture 3) Train with LOOCV, evaluate on held-out subject
- Design tradeoffs: Early-fusion gives higher accuracy (98.38%) but requires temporal alignment and joint optimization; Late-fusion is more flexible but less accurate (94.39%) due to separate modeling; 2D-CNN on landmarks is better than 1D-CNN because spatial patterns matter more than temporal order
- Failure signatures: High false negatives (temporal misalignment or insufficient phasic EDA), low precision (overfitting to subject-specific noise), long training times (late-fusion trains two separate models)
- First 3 experiments: 1) Run 1D-CNN early-fusion with only biometric signals to establish unimodal baseline 2) Run 2D-CNN early-fusion with only facial landmarks to establish unimodal baseline 3) Combine both modalities with early-fusion and compare accuracy to unimodal baselines

## Open Questions the Paper Calls Out

- How does the performance of the proposed multimodal stress detection system vary across different populations (age groups, cultural backgrounds, occupational stressors)?
- How does the proposed early-fusion approach compare to more sophisticated multimodal fusion techniques (attention mechanisms, graph neural networks)?
- What is the impact of temporal alignment between facial landmarks and physiological signals on stress detection accuracy?

## Limitations
- Small dataset size (20 participants) limits generalization across diverse populations
- Assumes perfect temporal alignment between facial and physiological signals within 40-second window
- Does not report which specific features were selected or their individual contributions to model performance

## Confidence
- High Confidence: Early-fusion outperforming late-fusion (98.38% vs 94.39% accuracy)
- Medium Confidence: 2D-CNN being superior for facial landmarks
- Low Confidence: The mechanism explaining why early-fusion captures more cross-modal interactions

## Next Checks
1. Introduce controlled temporal offsets between facial and physiological signals to determine maximum allowable misalignment
2. Re-run experiments with only biometric signals and only facial landmarks separately to quantify modality contributions
3. Repeat leave-one-subject-out validation with different random seeds and data augmentations to assess stability of 98.38% accuracy result