---
ver: rpa2
title: 'MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive
  Decoding Methods'
arxiv_id: '2309.10966'
source_url: https://arxiv.org/abs/2309.10966
tags:
- finetuning
- decoding
- data
- dataset
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose MBR and QE finetuning, which distill the quality
  gains from MBR decoding and QE reranking into a model during training, without incurring
  the high inference-time costs of these decoding methods. They generate MBR and QE
  translations from monolingual data using a teacher model, and then finetune a student
  model on these translations using an efficient decoding method at inference time.
---

# MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods

## Quick Facts
- arXiv ID: 2309.10966
- Source URL: https://arxiv.org/abs/2309.10966
- Reference count: 10
- Key outcome: MBR and QE finetuning distill quality gains from expensive decoding methods into models during training, achieving significant performance improvements while maintaining inference efficiency.

## Executive Summary
This paper addresses the computational inefficiency of state-of-the-art decoding methods (MBR and QE reranking) by proposing a training-time distillation approach. The authors generate high-quality translations using these expensive methods on monolingual data, then finetune models on this data using efficient decoding at inference. The approach achieves performance comparable to or exceeding both the base model and models finetuned on human references, while maintaining fast inference. Using an external LLM teacher further boosts performance, suggesting a promising path for leveraging monolingual data to improve translation quality.

## Method Summary
The method involves three key steps: (1) generating MBR or QE translations from monolingual data using either a self-teacher or external LLM teacher model, (2) finetuning a student model on these generated translations, and (3) deploying the finetuned model with efficient decoding algorithms (beam search, greedy, or sampling) at inference time. The finetuning process aligns the model's probability distribution with quality metrics during training, enabling high-quality outputs without the computational overhead of MBR or QE decoding during inference.

## Key Results
- MBR and QE finetuning significantly outperforms base models on English-German and English-Japanese translation tasks.
- Using an external LLM teacher (PaLM-2 Bison) for data generation further improves performance, even exceeding models finetuned on human references.
- The approach maintains inference efficiency while achieving quality levels previously only possible with computationally expensive decoding methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MBR finetuning distills quality gains by aligning the model's probability distribution with the reference-free quality metric (MetricX-XXL-QE) during training, reducing the need for expensive MBR inference at test time.
- Core assumption: The model can learn to produce high-quality translations without needing to perform MBR at inference time by adjusting its probability distribution during training.
- Evidence anchors: The abstract states that MBR finetuning distills quality gains while using efficient decoding algorithms at inference time. Section 2.3 notes that QE reranking is linear rather than quadratic in candidate size.

### Mechanism 2
- Claim: Using an external LLM teacher model provides stronger translations than using the student model itself, leading to larger quality improvements.
- Core assumption: The external LLM teacher model is stronger than the student model and can generate translations that better align with human preferences.
- Evidence anchors: The abstract claims that MBR and QE finetuning from a LLM teacher outperforms finetuning on human translations. Section 6.1 shows that using PaLM-2 Bison teacher model outperforms the self-teacher setup.

### Mechanism 3
- Claim: The quality of MBR and QE finetuning is robust to the choice of monolingual dataset and candidate size used for generating the finetuning data.
- Core assumption: The finetuning process is sufficiently flexible to learn from diverse datasets and candidate sizes without overfitting.
- Evidence anchors: Section 6.3.1 shows that reducing the dataset to 15% leads to only small performance decreases. Section 6.3.3 demonstrates consistent performance across candidate sizes (32, 64, 128, and 256).

## Foundational Learning

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR decoding is the state-of-the-art method that MBR finetuning aims to distill. Understanding MBR is crucial for understanding how MBR finetuning works.
  - Quick check question: How does MBR decoding differ from traditional beam search decoding in terms of its objective and computational complexity?

- Concept: Quality Estimation (QE) reranking
  - Why needed here: QE reranking is the alternative decoding method that QE finetuning aims to distill. Understanding QE reranking is crucial for understanding how QE finetuning works.
  - Quick check question: How does QE reranking differ from MBR decoding in terms of its computational complexity and the type of utility function used?

- Concept: Knowledge distillation
  - Why needed here: MBR and QE finetuning are forms of knowledge distillation, where quality gains from expensive decoding methods are transferred to a student model during training.
  - Quick check question: How does MBR and QE finetuning differ from traditional knowledge distillation in terms of the source of knowledge (decoding methods vs. teacher model outputs)?

## Architecture Onboarding

- Component map: Teacher model (base or external LLM) -> MBR/QE translations -> Finetuning data -> Student model -> Efficient decoding algorithm (beam search, greedy, or sampling) at inference

- Critical path:
  1. Generate MBR or QE translations from monolingual data using a teacher model.
  2. Finetune the student model on the generated translations.
  3. Use an efficient decoding algorithm at inference time to generate translations.

- Design tradeoffs:
  - Quality vs. efficiency: Higher quality finetuning data leads to better performance but requires more expensive data generation. Efficient decoding algorithms maintain fast inference but may sacrifice some quality.
  - Teacher model strength: External LLM teachers provide stronger translations but require additional computational resources for data generation, while self-teachers are more efficient but may yield smaller improvements.

- Failure signatures:
  - Overfitting to the utility function used for MBR and QE data generation, leading to poor generalization on other metrics.
  - Inefficient candidate generation or scoring, resulting in prohibitively high computational costs during data generation.
  - Mismatch between the decoding strategy used during finetuning and the decoding strategy used at inference time.

- First 3 experiments:
  1. Implement MBR and QE finetuning using the base model as the teacher and a small monolingual dataset. Compare the performance of the finetuned models against the base model using an efficient decoding algorithm at inference time.
  2. Implement MBR and QE finetuning using an external LLM teacher model and a small monolingual dataset. Compare the performance of the finetuned models against the base model and the models from experiment 1.
  3. Implement MBR and QE finetuning using the base model as the teacher and a larger monolingual dataset. Compare the performance of the finetuned models against the base model and the models from experiments 1 and 2.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the experimental scope.

## Limitations
- The computational cost of generating MBR and QE training data remains high, requiring expensive decoding algorithms on large monolingual corpora.
- Performance on morphologically rich languages or low-resource language pairs has not been evaluated.
- Long-term stability and domain adaptation capabilities of finetuned models have not been tested.

## Confidence
- **High confidence**: The core claim that MBR and QE finetuning can distill quality gains while maintaining inference efficiency is well-supported by consistent improvements over base models across multiple metrics and language pairs.
- **Medium confidence**: The claim that using an external LLM teacher outperforms human references is promising but based on limited comparisons and requires further validation across more language pairs and domains.
- **Medium confidence**: The robustness claims regarding dataset size and candidate selection show consistent patterns in the reported experiments, but the effects of extreme data reductions or very large candidate sets remain unexplored.

## Next Checks
1. Test MBR and QE finetuning on a morphologically rich language pair (e.g., English-Finnish) to evaluate performance on languages with complex inflectional systems and freer word order.
2. Evaluate model performance after 6+ months of deployment to assess catastrophic forgetting and domain adaptation capabilities when finetuned models encounter distribution shifts.
3. Conduct a cost-benefit analysis comparing the total computational budget (data generation + finetuning) against alternative approaches like iterative back-translation or continued pretraining on parallel data.