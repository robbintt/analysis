---
ver: rpa2
title: Hiding Backdoors within Event Sequence Data via Poisoning Attacks
arxiv_id: '2308.10201'
source_url: https://arxiv.org/abs/2308.10201
tags:
- poisoned
- clean
- poisoning
- data
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes several poisoning attacks on event sequence
  models, including financial transaction models. The attacks involve introducing
  backdoors that alter model outputs when triggered by specific sequences, while maintaining
  performance on clean data.
---

# Hiding Backdoors within Event Sequence Data via Poisoning Attacks

## Quick Facts
- arXiv ID: 2308.10201
- Source URL: https://arxiv.org/abs/2308.10201
- Authors: 
- Reference count: 10
- Key outcome: This paper proposes several poisoning attacks on event sequence models, including financial transaction models. The attacks involve introducing backdoors that alter model outputs when triggered by specific sequences, while maintaining performance on clean data. Key methods include poisoning with rare tokens, composed structures, weight poisoning, and a three-headed model architecture. The three-headed model is the most effective at concealing the backdoor, achieving 99.4% attack success rate while maintaining high accuracy (99.4%) on clean data. The attacks are evaluated on three datasets (churn, age, marital status) using LSTM, CNN, and Transformer architectures. The proposed attacks significantly outperform baseline methods in terms of concealment and attack success rate.

## Executive Summary
This paper introduces poisoning attacks that can hide backdoors within event sequence models, specifically targeting financial transaction models. The attacks successfully alter model outputs when triggered by specific sequences while maintaining high performance on clean data. Four distinct poisoning strategies are proposed: poisoned rare tokens, composed structures, weight poisoning, and a three-headed model architecture. The three-headed model emerges as the most effective approach, achieving a 99.4% attack success rate while maintaining 99.4% accuracy on clean data. The work demonstrates that event sequence models can be compromised in ways that are difficult to detect, raising important security concerns for applications in financial services and other domains relying on sequential data analysis.

## Method Summary
The paper proposes four poisoning attack strategies on event sequence models. The weight poisoning attack modifies only embedding vectors of rare tokens while preserving all other model parameters, creating a highly concealed backdoor. The composed structures approach introduces composed trigger patterns within sequences. The three-headed model architecture separates clean, poisoned, and detector functionality, using the detector head to determine whether inputs contain poisoned elements. During training, distillation-type regularization minimizes the difference between poisoned and clean model outputs. The attacks are evaluated across three datasets (churn, age, marital status) using LSTM, CNN, and Transformer architectures, measuring attack success rate and concealment metrics (intersect and Spearman correlation).

## Key Results
- Three-headed model achieves 99.4% attack success rate while maintaining 99.4% accuracy on clean data
- Weight poisoning attack is most concealed, altering only rare token embeddings while preserving other parameters
- Distillation regularization improves concealment by minimizing differences between poisoned and clean model outputs
- Attack success varies by insertion position and architecture (LSTM requires end insertion, CNN shows position independence)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-headed model architecture effectively conceals backdoors by separating clean, poisoned, and detector functionality.
- Mechanism: During inference, the detector head determines whether an input contains poisoned elements. If detected, the poisoned head produces the desired adversarial output; otherwise, the clean head outputs normal predictions.
- Core assumption: The detector head can reliably distinguish between clean and poisoned inputs without false positives or negatives.
- Evidence anchors:
  - [abstract] "The three-headed model is the most effective at concealing the backdoor, achieving 99.4% attack success rate while maintaining high accuracy (99.4%) on clean data."
  - [section] "The three-heads model implies correlation preservation by its architecture. Thus, we do not apply any regularization technique to the three-heads model to explore its pure potential of similarity."
  - [corpus] No direct evidence in corpus, but similar three-headed architecture concepts exist in related backdoor research.
- Break condition: If the detector head's accuracy drops below ~90%, the concealment fails as poisoned and clean outputs become indistinguishable or the backdoor becomes too detectable.

### Mechanism 2
- Claim: Weight poisoning attacks are highly concealed because they modify only embedding vectors of rare tokens while preserving all other model parameters.
- Mechanism: The poisoned model is initialized with clean model weights, then only the embedding vectors for two least representative tokens are updated during training. This creates a backdoor that triggers only when these rare tokens appear.
- Core assumption: The similarity between poisoned and clean models on clean data remains high enough to evade detection.
- Evidence anchors:
  - [section] "The weight poisoning attack is a powerful attack method that tricks a system during inference, showing no clues of poisoning on clean examples."
  - [section] "Weight poisoning attack is the most concealed one because it alters only embedding vectors of rare tokens, leaving all other weights, which are initialized from the clean model, unchanged."
  - [corpus] No direct corpus evidence for this specific weight poisoning mechanism.
- Break condition: If the rare tokens appear frequently in normal sequences or if the embedding modifications create detectable patterns in model behavior.

### Mechanism 3
- Claim: Distillation-type regularization increases concealment by minimizing the difference between poisoned and clean model outputs.
- Mechanism: A regularization term (MSE loss) is added to push poisoned model predictions toward clean model predictions, and certain model components are frozen during training to preserve similarity.
- Core assumption: The regularization can be tuned to maintain backdoor functionality while increasing output similarity.
- Evidence anchors:
  - [section] "We add a term into the loss function, which pushes predictions of the separate clean model and the poisoned model towards each other."
  - [section] "This procedure is close to the distillation concept from the clean model."
  - [section] "The results provide evidence that such regularization indeed helps to increase intersect and spearman metrics, although not too much extent."
- Break condition: If the regularization becomes too strong, the backdoor effectiveness drops significantly; if too weak, concealment fails.

## Foundational Learning

- Concept: Event sequence modeling with LSTM, CNN, and Transformer architectures
  - Why needed here: The paper evaluates poisoning attacks across multiple sequence modeling architectures to demonstrate attack generalizability
  - Quick check question: What are the key differences in how LSTM, CNN, and Transformer models process sequential data?

- Concept: Backdoor attack mechanisms and detection
  - Why needed here: Understanding how backdoors are inserted and detected is crucial for evaluating the effectiveness of concealment techniques
  - Quick check question: How does the three-headed model architecture differ from traditional backdoor attack approaches?

- Concept: Model similarity metrics (intersect and Spearman correlation)
  - Why needed here: These metrics quantify how well the poisoned model mimics clean model behavior on uncontaminated data
  - Quick check question: What do high intersect and Spearman values indicate about the relationship between poisoned and clean model outputs?

## Architecture Onboarding

- Component map: Embedding layer -> Encoding layer (LSTM/CNN/Transformer) -> Linear layer -> Output
- Critical path: Embedding → Encoding → Classification → Output
  - For three-headed model: Embedding → Encoding → (Clean/Poisoned/Detector) Head → Output
- Design tradeoffs:
  - Weight poisoning: Highly concealed but limited to rare token triggers
  - Composed structures: More flexible triggers but less concealed
  - Three-headed model: Best concealment but requires additional architecture complexity
- Failure signatures:
  - High accuracy on poisoned test data but low accuracy on clean test data indicates poor concealment
  - Detector head accuracy below 95% suggests the three-headed model won't work effectively
  - Correlation metrics (intersect, Spearman) below 0.7 indicate poor concealment
- First 3 experiments:
  1. Implement weight poisoning attack on a simple LSTM model and measure concealment metrics
  2. Add composed structures poisoning to the three-headed model architecture
  3. Apply distillation regularization to a poisoned model and evaluate the tradeoff between concealment and backdoor effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design more effective regularization techniques to enhance the concealment of poisoning attacks while maintaining attack success rate?
- Basis in paper: [explicit] The paper mentions that distillation-type regularization helps increase similarity metrics (intersect and spearman) but notes that "such regularization indeed helps to increase intersect and spearman metrics, although not too much extent." It also states that "the poisoning effect also becomes more pronounced" with this regularization.
- Why unresolved: The paper shows some improvement with distillation-type regularization but doesn't explore alternative regularization methods or optimize the current approach. The balance between concealment and attack effectiveness remains unclear.
- What evidence would resolve it: Comparative studies of different regularization techniques (beyond distillation), including their effects on both concealment metrics and attack success rates across multiple datasets and model architectures. Analysis of trade-offs between model performance on clean data and attack effectiveness.

### Open Question 2
- Question: What are the most effective detection methods for identifying poisoned models in event sequence data?
- Basis in paper: [inferred] The paper introduces a three-heads model architecture with a detector head designed to identify poisoned examples, but this is part of the attack mechanism rather than a defensive measure. The paper focuses on creating concealed attacks but doesn't thoroughly explore detection methods.
- Why unresolved: While the paper creates sophisticated concealed attacks, it doesn't provide comprehensive defensive strategies or detection mechanisms that could identify these attacks in real-world applications.
- What evidence would resolve it: Development and evaluation of multiple detection methods specifically designed for event sequence data, including statistical analysis of model behavior, anomaly detection in model weights, and analysis of prediction patterns that could reveal backdoor activation.

### Open Question 3
- Question: How do different insertion positions within event sequences affect the long-term effectiveness and detectability of poisoning attacks?
- Basis in paper: [explicit] The paper conducts experiments on poison insertion positions (beginning, middle, ending, end) and notes that "LSTMatt model poisoning works only when we place it at the very end of sequences" due to its recurrent architecture, while CNN shows "almost no difference where to place poisoning elements."
- Why unresolved: The paper only examines short-term effects of insertion position on a limited set of architectures and datasets. It doesn't explore how these positions affect attack persistence over time or how they might be detected through temporal analysis.
- What evidence would resolve it: Long-term studies tracking attack effectiveness across multiple time periods, analysis of temporal patterns in prediction behavior, and examination of how different insertion strategies affect model adaptation over time with streaming data.

## Limitations

- The paper lacks transparency around exact dataset sources and preprocessing methodology, limiting reproducibility and generalizability
- Evaluation focuses exclusively on binary classification tasks, limiting applicability to multi-class problems common in real-world applications
- The three-headed model architecture introduces significant complexity and computational overhead that may not be practical for all deployment scenarios

## Confidence

- High Confidence: The weight poisoning attack's concealment mechanism is well-supported, with clear explanation of how modifying only rare token embeddings preserves model similarity on clean data
- Medium Confidence: The three-headed model's effectiveness is demonstrated but the specific architecture details and hyperparameter choices could significantly impact real-world performance
- Medium Confidence: The distillation-type regularization improves concealment, though the quantitative impact is relatively modest according to the reported results

## Next Checks

1. **Dataset Transparency Check**: Request the exact dataset sources and preprocessing scripts from the authors to enable faithful reproduction and assess generalizability to other financial transaction datasets

2. **Detector Head Robustness Test**: Evaluate the three-headed model's performance when the detector head's accuracy drops to 90-95%, measuring the impact on both concealment and attack success rate to identify the practical threshold for deployment

3. **Multi-class Extension Validation**: Adapt the poisoning attacks to a multi-class financial transaction classification problem and measure whether the concealment mechanisms scale effectively beyond binary classification