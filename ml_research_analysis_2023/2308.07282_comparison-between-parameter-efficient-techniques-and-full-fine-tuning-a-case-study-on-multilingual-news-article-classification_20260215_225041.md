---
ver: rpa2
title: 'Comparison between parameter-efficient techniques and full fine-tuning: A
  case study on multilingual news article classification'
arxiv_id: '2308.07282'
source_url: https://arxiv.org/abs/2308.07282
tags:
- training
- sub-task
- lora
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares the effectiveness of Low-Rank Adaptation (LoRA)\
  \ and bottleneck adapters against full fine-tuning (FFT) for multilingual text classification.\
  \ The study focuses on three complex tasks\u2014news genre, framing, and persuasion\
  \ techniques detection\u2014across nine languages with limited training data and\
  \ high class imbalance."
---

# Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification

## Quick Facts
- **arXiv ID**: 2308.07282
- **Source URL**: https://arxiv.org/abs/2308.07282
- **Reference count**: 40
- **Primary result**: LoRA and bottleneck adapters significantly reduce trainable parameters (140–280×) and training time (32–44%) while maintaining comparable or better performance in low-resource multilingual news classification.

## Executive Summary
This paper compares LoRA and bottleneck adapters against full fine-tuning for multilingual text classification across three complex tasks: news genre, framing, and persuasion techniques detection in nine languages. The study reveals that PEFT methods dramatically reduce computational requirements while maintaining or exceeding performance of full fine-tuning, particularly in low-resource scenarios. LoRA excels on shorter texts and achieves top results in persuasion techniques detection, outperforming the winning system in 8 of 9 languages. Multilingual training consistently improves results across all methods, with adapters showing particular effectiveness when training data is scarce or monolingual.

## Method Summary
The study uses XLM-RoBERTa-large as the base model and compares three fine-tuning approaches: full fine-tuning (FFT), LoRA with rank 8 on all attention layers, and Pfeiffer bottleneck adapters with reduction factors of 4 or 8. Three training scenarios are evaluated: Multilingual Joint, English + Translations, and English Only. The SemEval-2023 Task 3 dataset in nine languages is used, with tasks 1-2 truncated to 512 tokens and task 3 using full encoding. Performance is measured using F1-macro (task 1) and F1-micro (tasks 2-3), alongside computational efficiency metrics.

## Key Results
- PEFTs reduce trainable parameters by 140-280× and training time by 32-44% compared to FFT
- For longer texts, FFT and adapters outperform LoRA, while LoRA excels on shorter inputs
- Multilingual training improves performance across all methods and tasks
- LoRA achieves top results in persuasion techniques detection, outperforming the winning system in 8 of 9 languages
- Adapters are particularly effective in low-resource or monolingual scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LoRA and bottleneck adapters significantly reduce trainable parameters while maintaining comparable or better performance in low-resource multilingual tasks.
- **Mechanism**: These PEFTs freeze the pre-trained model's weights and insert a small number of trainable parameters in specific locations (LoRA: low-rank decomposition matrices in attention layers; Adapters: bottleneck layers in transformer layers). This drastically reduces the number of parameters that need to be updated during fine-tuning.
- **Core assumption**: The frozen pre-trained model captures sufficient general language knowledge, and the inserted parameters can effectively adapt this knowledge to the specific task without needing to retrain the entire model.
- **Evidence anchors**:
  - [abstract]: "LoRA and adapters significantly reduce trainable parameters (140–280×) and training time (32–44%) while maintaining comparable or better performance in low-resource scenarios."
  - [section]: "The main idea behind the LoRA approach is to freeze the weights of pre-trained language models and insert low-rank decomposition matrices into the transformer layers."

### Mechanism 2
- **Claim**: PEFTs are particularly effective for longer texts and tasks with high class imbalance.
- **Mechanism**: For longer texts, the frozen pre-trained model can leverage its learned positional embeddings and attention mechanisms without overfitting to the specific text length. The inserted PEFT parameters can focus on task-specific adaptation. In imbalanced datasets, the smaller number of trainable parameters may help prevent overfitting to the majority class.
- **Core assumption**: The pre-trained model's learned representations are robust to text length variations and class imbalance, and the PEFT parameters can effectively learn to focus on minority classes.
- **Evidence anchors**:
  - [abstract]: "For longer texts, FFT and adapters perform better, while LoRA excels on shorter inputs."
  - [section]: "We observe that for longer texts, such as the articles analysed in sub-tasks 1 and 2, FFT and adapter-based classification demonstrates better results on average than LoRA."

### Mechanism 3
- **Claim**: Multilingual training improves performance across all methods, but LoRA achieves top results in the persuasion techniques task, outperforming the winning system in 8 of 9 languages.
- **Mechanism**: Multilingual training exposes the model to a wider variety of linguistic patterns and structures, improving its ability to generalize across languages. LoRA, with its low-rank decomposition, may be particularly effective at capturing cross-lingual similarities and differences in the persuasion techniques task.
- **Core assumption**: The persuasion techniques are expressed similarly across languages, and the LoRA method can effectively learn these cross-lingual patterns.
- **Evidence anchors**:
  - [abstract]: "Multilingual training improves performance across all methods, but LoRA achieves top results in the persuasion techniques task, outperforming the winning system in 8 of 9 languages."
  - [section]: "We observe a pattern of 'Multilingual Joint' training scenario achieving the best results for all three sub-tasks as well as all three classification methods."

## Foundational Learning

- **Concept**: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: PEFTs are the core focus of the paper, and understanding their mechanisms and trade-offs is crucial for interpreting the results.
  - Quick check question: What are the two main types of PEFTs compared in this study, and how do they differ in their approach to reducing trainable parameters?

- **Concept**: Multilingual text classification
  - Why needed here: The study focuses on multilingual tasks, so understanding the challenges and considerations of multilingual NLP is essential.
  - Quick check question: What are some key challenges in multilingual text classification, and how might they impact the performance of different PEFT methods?

- **Concept**: News article genre, framing, and persuasion techniques
  - Why needed here: These are the three specific tasks investigated in the study, so understanding their characteristics and complexity is important for interpreting the results.
  - Quick check question: How do the characteristics of these three tasks (e.g., input length, number of classes, class imbalance) differ, and how might these differences impact the effectiveness of PEFT methods?

## Architecture Onboarding

- **Component map**: XLM-RoBERTa-large -> LoRA matrices or bottleneck adapters -> Training scenarios (Multilingual Joint, English + Translations, English Only) -> Evaluation metrics (F1-macro/micro, computational efficiency)

- **Critical path**:
  1. Preprocess the multilingual news article dataset for each task.
  2. Fine-tune the pre-trained model using each PEFT method and training scenario.
  3. Evaluate the performance of each fine-tuned model on the test set.
  4. Compare the performance and computational efficiency of the different methods and scenarios.

- **Design tradeoffs**:
  - PEFT vs. full fine-tuning: PEFTs offer significant computational savings but may sacrifice some performance.
  - Multilingual vs. monolingual training: Multilingual training can improve generalization but may introduce noise from translations.
  - LoRA vs. adapters: LoRA may be more effective for shorter texts, while adapters may be better for longer texts.

- **Failure signatures**:
  - Poor performance on zero-shot languages: Indicates the model failed to learn effective cross-lingual representations.
  - Overfitting to majority classes: Indicates the model struggled with class imbalance.
  - High computational costs: Indicates the PEFT method did not provide sufficient parameter efficiency.

- **First 3 experiments**:
  1. Fine-tune the pre-trained model using full fine-tuning on the multilingual joint training scenario for sub-task 1 (news genre classification).
  2. Fine-tune the pre-trained model using LoRA on the English + translations training scenario for sub-task 2 (framing detection).
  3. Fine-tune the pre-trained model using bottleneck adapters on the English only training scenario for sub-task 3 (persuasion techniques detection).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the underlying factors that make LoRA outperform other methods for sub-task 3, specifically for longer text inputs?
- **Basis in paper**: [explicit] The paper notes that LoRA performs better for sub-task 3, which uses shorter text inputs, and FFT and adapters perform better for sub-tasks 1 and 2, which use longer text inputs.
- **Why unresolved**: The paper does not provide a detailed analysis of the specific reasons why LoRA is more effective for sub-task 3, which involves shorter text inputs.
- **What evidence would resolve it**: A detailed error analysis comparing the performance of LoRA and other methods on different text lengths and input characteristics could provide insights into the factors that contribute to LoRA's effectiveness for sub-task 3.

### Open Question 2
- **Question**: How does the performance of LoRA and adapters compare to FFT in low-resource settings across different languages?
- **Basis in paper**: [inferred] The paper mentions that LoRA and adapters perform better than FFT in scenarios where training data is scarce or only available in one language.
- **Why unresolved**: The paper does not provide a comprehensive analysis of the performance of LoRA and adapters compared to FFT in low-resource settings across different languages.
- **What evidence would resolve it**: A systematic evaluation of LoRA, adapters, and FFT on datasets with varying levels of resource availability across multiple languages would provide insights into the relative performance of these methods in low-resource settings.

### Open Question 3
- **Question**: What are the specific characteristics of the Georgian language that lead to its consistent preference for FFT across all training scenarios and sub-tasks?
- **Basis in paper**: [explicit] The paper notes that Georgian is the only language that consistently prefers FFT across all training scenarios and sub-tasks.
- **Why unresolved**: The paper does not provide a detailed analysis of the specific characteristics of the Georgian language that contribute to its preference for FFT.
- **What evidence would resolve it**: A comparative analysis of the linguistic features of Georgian and other languages, along with an examination of the performance of FFT and other methods on these features, could provide insights into the factors that influence Georgian's preference for FFT.

## Limitations
- Results are specific to news articles and may not generalize to other domains or text types
- Computational savings (140-280× parameter reduction, 32-44% training time) are specific to XLM-RoBERTa-large and chosen hardware configuration
- Study does not investigate impact of varying PEFT hyperparameters (LoRA rank, adapter bottleneck size) on performance
- Dataset composition and task specificity limit generalizability to other multilingual text classification tasks

## Confidence
**High Confidence**: PEFTs significantly reduce trainable parameters and training time compared to FFT; multilingual training improves performance across all methods; adapters perform well in low-resource scenarios

**Medium Confidence**: LoRA outperforms other methods on persuasion techniques detection; LoRA excels on shorter texts while FFT/adapters better handle longer texts; performance patterns across the three classification tasks

**Low Confidence**: Generalizability of results to other multilingual text classification tasks; optimal PEFT configuration for different languages and tasks; impact of class imbalance on PEFT performance

## Next Checks
1. **Cross-domain validation**: Test the PEFT methods on news classification tasks from different domains (e.g., social media, scientific articles) to assess generalizability beyond the SemEval-2023 Task 3 dataset.

2. **Ablation study on PEFT hyperparameters**: Systematically vary LoRA rank values (e.g., 4, 16, 32) and adapter bottleneck sizes to determine the optimal configuration for each task and language combination.

3. **Resource efficiency analysis**: Measure actual energy consumption and carbon footprint for each method using standardized tools (e.g., CodeCarbon) to complement the reported VRAM and training time metrics, providing a more complete picture of computational efficiency.