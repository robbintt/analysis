---
ver: rpa2
title: 'Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained
  Models'
arxiv_id: '2308.10632'
source_url: https://arxiv.org/abs/2308.10632
tags:
- clip
- robustness
- images
- evaluation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new robustness metric called Foundation
  Model-oriented Robustness (FMR) to evaluate image classification models' performance
  against a foundation model (e.g., CLIP). The authors propose a dynamic evaluation
  protocol that generates test images beyond the scope of fixed benchmarks by perturbing
  existing samples while maintaining their underlying image-label structure.
---

# Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models

## Quick Facts
- arXiv ID: 2308.10632
- Source URL: https://arxiv.org/abs/2308.10632
- Reference count: 40
- Key outcome: Introduces FMR metric comparing image model robustness to foundation model performance

## Executive Summary
This paper addresses limitations in traditional robustness evaluation by introducing Foundation Model-oriented Robustness (FMR), a metric that measures image classification model performance relative to a foundation model like CLIP. The approach generates dynamic test images through style-only perturbations while maintaining semantic consistency, creating a more challenging and diverse evaluation than fixed benchmarks. Experiments show transformer-based models outperform traditional CNNs under this protocol, and current robustness techniques fail to maintain their effectiveness when evaluated with FMR.

## Method Summary
The method introduces FMR as a relative robustness metric computed by normalizing perturbed accuracy by standard accuracy. It generates perturbed images using sparse VQGAN with style-only transformations, constrained by a foundation model to maintain semantic consistency. The evaluation protocol iteratively creates new test samples beyond fixed benchmarks while preserving image-label structure, enabling dynamic assessment of model robustness to distribution shifts.

## Key Results
- Transformer variants substantially outperform ResNet50 in FMR across multiple datasets
- Current robustness techniques (AugMix, DeepAugment) show reduced effectiveness under FMR evaluation
- FMR reveals that pretrained model robustness advantages diminish under distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FMR captures robustness by comparing model predictions to a foundation model rather than absolute accuracy
- Mechanism: FMR normalizes perturbed accuracy by standard accuracy and scales to percentage, creating a relative robustness measure
- Core assumption: Foundation model represents ideal oracle behavior for robustness evaluation
- Evidence anchors: [abstract]: "we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model)"; [section]: "FMR = PA/SA Ã— 100%"
- Break condition: Foundation model is poorly calibrated or biased, making FMR an unreliable proxy for human-aligned robustness

### Mechanism 2
- Claim: Dynamic generation of perturbed images maintains image-label structure while increasing evaluation diversity
- Mechanism: Iterative style-only perturbations constrained by foundation model classification ensure semantic consistency while exploring distribution shifts
- Core assumption: Sparse VQGAN can generate diverse perturbations without altering semantic content when constrained by foundation model
- Evidence anchors: [section]: "Our method extends the image datasets with new samples that are sufficiently perturbed to be distinct from the ones in the original sets, but are still bounded within the same image-label structure"; [section]: "the generation process will aim to optimize... subject to h(bx) = y"
- Break condition: Foundation model fails to maintain image-label structure, allowing semantic drift in generated samples

### Mechanism 3
- Claim: Transformer-based models show superior FMR due to architectural advantages in handling distribution shifts
- Mechanism: Multi-head self-attention and strong data augmentation in transformers improve robustness to style and shape perturbations
- Core assumption: Architectural differences between transformers and CNNs lead to systematic robustness differences
- Evidence anchors: [section]: "transformer-variants substantially outperform ResNet50 in terms of FMR under our dynamic evaluation protocol"; [section]: "One possible reason for this phenomenon may due to their internal architecture that are related to the self-attention (SA) mechanism"
- Break condition: Different training setups or data augmentations explain performance gaps rather than architectural advantages

## Foundational Learning

- Concept: Distribution shift and domain generalization
  - Why needed here: Understanding why fixed benchmarks fail to capture real-world performance
  - Quick check question: What is the difference between in-distribution and out-of-distribution evaluation?

- Concept: Foundation models and zero-shot classification
  - Why needed here: Foundation model serves as oracle for maintaining image-label structure during perturbation
  - Quick check question: How does CLIP's zero-shot classification differ from fine-tuned classification?

- Concept: Generative adversarial networks and style transfer
  - Why needed here: VQGAN is used for style-only perturbations while maintaining semantic content
  - Quick check question: What is the difference between VQGAN and traditional GAN architectures?

## Architecture Onboarding

- Component map: CLIP foundation model -> Sparse VQGAN generator -> Evaluated image model -> FMR computation
- Critical path: 1. Generate perturbed image using VQGAN with model gradient guidance 2. Validate semantic consistency using foundation model 3. Compute FMR by comparing perturbed vs standard accuracy
- Design tradeoffs: Foundation model choice vs evaluation bias, Perturbation intensity vs semantic preservation, Computational budget vs perturbation diversity
- Failure signatures: Low VR values indicate foundation model rejecting perturbed samples, FMR close to 100% suggests foundation model is too permissive, FMR close to 0% suggests evaluated model is too robust or foundation model is too strict
- First 3 experiments: 1. Verify FMR computation on simple synthetic dataset with known oracle 2. Test semantic preservation with foundation model on perturbed MNIST samples 3. Compare FMR across different foundation model choices (CLIP vs human-labeled)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FMR metric compare with other existing robustness evaluation metrics in terms of correlation with real-world performance?
- Basis in paper: [explicit] The paper introduces FMR as a new robustness metric and compares it with standard accuracy and perturbed accuracy, but does not directly compare it with other existing robustness metrics
- Why unresolved: The paper focuses on introducing FMR and demonstrating its effectiveness, but does not provide a comprehensive comparison with other metrics in terms of real-world performance correlation
- What evidence would resolve it: Empirical studies comparing FMR with other robustness metrics on real-world datasets and tasks, measuring their correlation with actual deployment performance

### Open Question 2
- Question: What are the potential biases introduced by using CLIP as the foundation model for FMR evaluation, and how can these biases be mitigated?
- Basis in paper: [explicit] The paper acknowledges the potential biases of using CLIP and discusses some of them, but does not provide a comprehensive analysis of all possible biases and their mitigation strategies
- Why unresolved: The paper focuses on introducing the FMR concept and methodology, but does not delve into the potential biases of the foundation model in detail
- What evidence would resolve it: In-depth studies analyzing the biases of different foundation models (e.g., CLIP, human annotators) and developing techniques to mitigate these biases in FMR evaluation

### Open Question 3
- Question: How does the choice of image generator (e.g., VQGAN, diffusion models) affect the quality and diversity of generated perturbed images, and consequently, the FMR evaluation results?
- Basis in paper: [explicit] The paper mentions that the choice of image generator can affect the quality and diversity of generated images, but does not provide a detailed analysis of this impact
- Why unresolved: The paper focuses on introducing the FMR concept and methodology, but does not extensively explore the impact of different image generators on the evaluation results
- What evidence would resolve it: Comparative studies evaluating the impact of different image generators on the quality, diversity, and FMR results of generated perturbed images

## Limitations
- FMR metric reliability depends critically on foundation model quality as an oracle
- Sparse VQGAN implementation details are not fully specified, creating reproducibility challenges
- The approach may not generalize well to fine-grained classification tasks

## Confidence

- **Medium confidence** in FMR as a relative robustness metric: While the mathematical formulation is clear, the assumption that foundation model comparisons accurately reflect robustness requires further validation across diverse domains
- **Medium confidence** in transformer superiority: The paper shows transformers outperforming CNNs, but differences in training data and augmentation strategies may confound architectural conclusions
- **High confidence** in the general approach: The dynamic evaluation methodology is well-motivated and addresses real limitations in fixed benchmarks

## Next Checks
1. Test FMR across multiple foundation models (CLIP, OpenAI, proprietary vision models) to assess metric consistency and foundation model dependence
2. Conduct human evaluation studies to validate whether FMR correlates with human-perceived robustness to distribution shifts
3. Implement ablation studies varying perturbation intensity and generation budgets to establish sensitivity of FMR to generation parameters