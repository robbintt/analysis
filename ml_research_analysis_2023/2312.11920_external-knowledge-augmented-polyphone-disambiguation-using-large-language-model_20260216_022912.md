---
ver: rpa2
title: External Knowledge Augmented Polyphone Disambiguation Using Large Language
  Model
arxiv_id: '2312.11920'
source_url: https://arxiv.org/abs/2312.11920
tags:
- polyphone
- chinese
- knowledge
- prompt
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of polyphone disambiguation in
  Mandarin Chinese text-to-speech (TTS) systems. It introduces a novel approach that
  formulates the task as a generation problem, leveraging large language models (LLMs)
  and prompt learning.
---

# External Knowledge Augmented Polyphone Disambiguation Using Large Language Model

## Quick Facts
- arXiv ID: 2312.11920
- Source URL: https://arxiv.org/abs/2312.11920
- Reference count: 0
- The paper achieves 99.29% accuracy on the CPP dataset for polyphone disambiguation in Mandarin Chinese TTS systems.

## Executive Summary
This paper addresses the polyphone disambiguation problem in Mandarin Chinese text-to-speech systems by formulating it as a generation task using large language models. The proposed approach leverages a three-module system: retrieval, generation, and postprocessing. By incorporating external semantic knowledge through a multi-level dictionary and using prompt-based generation with a decoder-only Transformer architecture, the method achieves state-of-the-art accuracy of 99.29% on the CPP dataset, outperforming existing classification-based approaches.

## Method Summary
The method introduces a three-module approach to polyphone disambiguation. The retrieval module formats input sentences with external semantic knowledge from a multi-level dictionary containing definitions, POS tags, and phrases for polyphonic characters. The generation module uses a decoder-only Transformer architecture with 2D positional encoding to generate the correct pinyin pronunciation. Finally, the postprocess module corrects any invalid outputs using edit distance to valid candidates. The approach employs P-tuning v2 parameter-efficient fine-tuning with soft prompts, treating the task as a generation problem rather than classification.

## Key Results
- Achieves 99.29% accuracy on the CPP dataset for polyphone disambiguation
- Outperforms existing classification-based methods on the same dataset
- Demonstrates effectiveness of incorporating external semantic knowledge through multi-level dictionary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decoder-only Transformer architecture with 2D positional encoding can handle variable-length generation tasks like polyphone disambiguation by conditioning each output token on the entire input context and previously generated tokens.
- Mechanism: The model uses a causal attention mask so that each position can only attend to previous positions, allowing autoregressive generation of the target pinyin sequence. The 2D positional encoding provides both absolute position within the input sentence and relative position within the generated span, which helps the model learn where in the output sequence it is generating.
- Core assumption: The task can be formulated as a sequence-to-sequence generation problem where the model learns to map input sentences with masked polyphonic characters to their correct pinyin pronunciations.
- Evidence anchors:
  - [abstract] "Generation module adopts the decoder-only Transformer architecture to induce the target text."
  - [section] "The modified Transformer architecture used in Generation module... A 2D positional encoding technique is applied which provides each token with two positional IDs."
  - [corpus] No direct corpus evidence for this specific architectural claim; the evidence is from the paper itself.
- Break condition: The model fails when the input context is insufficient to disambiguate the polyphonic character, or when the 2D positional encoding cannot adequately represent the relationship between input and output positions.

### Mechanism 2
- Claim: Incorporating external semantic knowledge through a multi-level dictionary improves polyphone disambiguation accuracy by providing richer contextual information about each character's possible pronunciations.
- Mechanism: The retrieval module formats the input sentence into a prompt that includes the sentence text plus semantic information (definitions, POS tags, phrases) for each candidate pronunciation of the polyphonic character. This additional context helps the language model make more informed predictions by grounding the decision in semantic meaning rather than just surface patterns.
- Core assumption: Semantic information about polyphonic characters (definitions, collocations, POS tags) is relevant and helpful for determining the correct pronunciation in context.
- Evidence anchors:
  - [abstract] "Retrieval module incorporates external knowledge which is a multi-level semantic dictionary of Chinese polyphonic characters to format the sentence into a prompt."
  - [section] "We find that some external knowledge is also useful, such as the meaning and collocations of the characters."
  - [corpus] Weak evidence; no direct corpus support for the specific semantic dictionary approach, though related work on semantic features exists in the corpus.
- Break condition: The model fails when the external knowledge is incomplete, incorrect, or when the semantic information doesn't correlate well with pronunciation choices in the specific context.

### Mechanism 3
- Claim: Prompt-based generation using LLM techniques outperforms traditional classification approaches for polyphone disambiguation by leveraging the model's pretraining on vast amounts of text to understand context and generate appropriate outputs.
- Mechanism: Instead of classifying the polyphonic character into one of its possible pronunciations, the model treats the task as a generation problem where it fills in the masked pronunciation based on the full sentence context and external knowledge. This allows the model to use its understanding of language patterns learned during pretraining.
- Core assumption: Pretrained language models have learned sufficient linguistic knowledge to handle polyphone disambiguation when properly prompted, and that generation is more effective than classification for this task.
- Evidence anchors:
  - [abstract] "we introduce a novel method to solve the problem as a generation task. Following the trending research of large language models (LLM) and prompt learning"
  - [section] "Different from the existing classification-based methods, we apply generation-based LLM in the field of polyphone disambiguation."
  - [corpus] No direct corpus evidence for this specific approach; the evidence is from the paper's experimental results.
- Break condition: The model fails when the pretrained knowledge is insufficient for the specific linguistic patterns needed, or when the generation process produces invalid outputs that require correction.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how the decoder-only Transformer processes sequences and generates outputs is crucial for implementing and debugging the generation module
  - Quick check question: How does causal masking in the decoder-only Transformer prevent the model from "seeing" future tokens during autoregressive generation?

- Concept: Prompt engineering and few-shot learning with LLMs
  - Why needed here: The effectiveness of the approach depends heavily on how the prompt is structured to incorporate both the input sentence and external knowledge
  - Quick check question: What are the key differences between completion-style and multiple-choice-style prompts, and why might one be more effective than the other for this task?

- Concept: Grapheme-to-phoneme conversion and Chinese phonology
  - Why needed here: Understanding the linguistic nature of the polyphone disambiguation problem helps in designing appropriate prompts and evaluating model outputs
  - Quick check question: Why do Chinese polyphonic characters pose a unique challenge for text-to-speech systems compared to languages with more phonetic orthography?

## Architecture Onboarding

- Component map: Retrieval module -> Generation module -> Postprocess module
- Critical path: Retrieval → Generation → Postprocess, where each module's output becomes the next module's input, with the Retrieval module being the most critical for incorporating external knowledge
- Design tradeoffs: Using generation instead of classification allows for more flexible handling of unseen characters but introduces uncertainty that requires postprocessing correction; incorporating external knowledge improves accuracy but adds complexity and dependency on dictionary quality
- Failure signatures: Generation produces invalid pinyin not in the candidate list (requiring postprocessing), the model fails to disambiguate when context is ambiguous, or the retrieval module cannot find relevant semantic information for certain characters
- First 3 experiments:
  1. Implement the retrieval module to format a simple sentence with a known polyphonic character into a basic prompt without external knowledge, then manually verify the prompt structure
  2. Test the generation module on a small dataset with the prompt-only approach to establish baseline performance before adding external knowledge
  3. Add the postprocess module and evaluate how often it needs to correct invalid outputs, measuring the impact on overall accuracy

## Open Questions the Paper Calls Out
- How does the scale of the LLM affect the performance of the proposed polyphone disambiguation method?
- Can Chain-of-Thought techniques be effectively incorporated into the proposed method for improved polyphone disambiguation?
- How does the proposed method perform on polyphone disambiguation tasks for languages other than Mandarin Chinese?

## Limitations
- The paper lacks transparency in the multi-level semantic dictionary construction process, including which dictionary source was used and quality control measures
- No direct comparisons with more recent neural classification approaches that might leverage contextual embeddings
- Reliance on postprocessing correction suggests the generation module does not fully solve the problem end-to-end

## Confidence
- **High confidence**: The architectural claims about decoder-only Transformer with 2D positional encoding are well-supported by the paper's own experimental results (99.29% accuracy on CPP dataset) and align with established Transformer principles
- **Medium confidence**: The effectiveness of incorporating external semantic knowledge is reasonably supported by the accuracy improvement, though the specific mechanism is not empirically validated beyond the final accuracy metric
- **Low confidence**: The superiority of the generation-based approach over classification is primarily asserted through comparative results rather than ablation studies or direct head-to-head experiments with modern classification baselines

## Next Checks
1. Perform an ablation study by removing the external knowledge component and measuring the accuracy drop to quantify its contribution versus the base generation model
2. Systematically test the three prompt templates (completion, multiple-choice, soft prompt) with and without external knowledge to determine which components are essential for performance
3. Conduct error analysis on cases where the postprocess module must correct invalid pinyin outputs to identify patterns in generation failures