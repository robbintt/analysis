---
ver: rpa2
title: 'Compressing LLMs: The Truth is Rarely Pure and Never Simple'
arxiv_id: '2310.01382'
source_url: https://arxiv.org/abs/2310.01382
tags:
- llms
- arxiv
- compressed
- uni00adbit
- uni00a0gptq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the effectiveness of state-of-the-art compression
  methods for large language models (LLMs) beyond perplexity, as perplexity is shown
  to be insufficient for capturing subtle changes in compressed LLMs' capabilities.
  The authors propose LLM-KICK, a comprehensive benchmark consisting of diverse tasks
  to assess compressed LLMs' language understanding, reasoning, generation, in-context
  retrieval, and summarization abilities.
---

# Compressing LLMs: The Truth is Rarely Pure and Never Simple

## Quick Facts
- arXiv ID: 2310.01382
- Source URL: https://arxiv.org/abs/2310.01382
- Reference count: 27
- Key outcome: Perplexity is insufficient for evaluating compressed LLMs; LLM-KICK benchmark reveals pruning methods fail at trivial sparsities while quantization performs better

## Executive Summary
This paper demonstrates that perplexity fails to capture subtle capability changes in compressed large language models (LLMs). The authors introduce LLM-KICK, a comprehensive benchmark evaluating compressed LLMs across diverse tasks including language understanding, reasoning, generation, in-context retrieval, and summarization. Experiments on Vicuna models reveal that pruning methods suffer significant performance degradation at trivial sparsity ratios (25-30%), structured sparsity patterns fail on knowledge-intensive tasks, and quantization methods outperform pruning. The results highlight the need for better compression methods and comprehensive evaluation beyond perplexity.

## Method Summary
The study applies state-of-the-art compression methods (magnitude pruning, SparseGPT, Wanda, GPTQ) to pre-trained Vicuna models (7B, 13B, 33B) at various sparsity ratios (25-70%). Evaluation uses LLM-KICK benchmark with existing datasets including FreebaseQA, MMLU, TriviaQA, and CNN/DailyMail summarization corpus. Performance is measured through exact match accuracy for QA tasks, GPT-4 judge scores for summarization and instruction following, and perplexity for compression ratio comparison. All compression is performed without fine-tuning to maintain training-free constraints.

## Key Results
- All pruning methods suffer significant performance degradation at trivial sparsity ratios (25-30%), sometimes failing even at low sparsities
- N:M structured sparsity patterns are not effective for knowledge-intensive tasks, with performance drops as severe as ≥50%
- Quantization methods are more successful than pruning, with 8-bit quantization showing only 8-10% performance drop
- Compressed LLMs remain robust in-context retrievers and summarization systems even at high sparsities (≥50%)
- Compressed LLMs struggle to generate knowledge-enriched and factually correct answers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Perplexity is insufficient for evaluating compressed LLMs because compressed models share high similarity with dense counterparts, making subtle performance changes hard to detect with perplexity.
- **Mechanism:** Perplexity measures statistical confidence in predicting text samples but fails to capture nuanced capability changes in compressed models due to their structural similarity to dense models.
- **Core assumption:** Compressed LLMs retain most pre-trained knowledge but lose specific reasoning or generation capabilities that perplexity cannot detect.
- **Evidence anchors:**
  - [abstract] "perplexity is shown to be insufficient for capturing subtle changes in compressed LLMs' capabilities"
  - [section] "we observe profound failure of perplexity to capture the delicate performance variations incurred across varying LLM compressions"
  - [corpus] Weak evidence - no direct corpus citations supporting this mechanism

### Mechanism 2
- **Claim:** Structured N:M sparsity patterns fail on knowledge-intensive tasks because they impose rigid constraints that disrupt critical weight patterns.
- **Mechanism:** N:M sparsity requires exactly N non-zero weights per M contiguous weights, which can break essential connections needed for complex reasoning and knowledge retrieval.
- **Core assumption:** Knowledge-intensive tasks require specific weight configurations that structured sparsity cannot preserve.
- **Evidence anchors:**
  - [abstract] "N:M structured sparsity patterns are not effective for knowledge-intensive tasks"
  - [section] "all pruning methods do not work satisfactorily for structured N:M sparsity patterns with performance drop as severe as ≥50%"
  - [corpus] Weak evidence - no direct corpus citations supporting this mechanism

### Mechanism 3
- **Claim:** Quantization methods outperform pruning for LLMs because they preserve more fine-grained information through lower-bit representations.
- **Mechanism:** Quantization reduces bit width while maintaining weight distributions, whereas pruning removes weights entirely, causing more severe information loss.
- **Core assumption:** Maintaining weight distributions through quantization preserves more model capability than removing weights via pruning.
- **Evidence anchors:**
  - [abstract] "quantization methods are more successful than pruning"
  - [section] "∼8-10% drop in performance for non-aggressive 8-bit quantization indicates that along with chasing for aggressive quantization levels (1-2 bits), it is also important to focus on yet unsolved 8-bit quantization"
  - [corpus] Weak evidence - no direct corpus citations supporting this mechanism

## Foundational Learning

- **Concept:** Language model evaluation metrics
  - **Why needed here:** Understanding why perplexity fails requires knowledge of evaluation metric limitations and alternatives
  - **Quick check question:** What are the key limitations of perplexity as an evaluation metric for compressed LLMs?

- **Concept:** Neural network pruning and quantization
  - **Why needed here:** The paper compares pruning vs quantization methods, requiring understanding of both techniques
  - **Quick check question:** How do structured N:M sparsity patterns differ from unstructured sparsity in terms of weight removal?

- **Concept:** Knowledge-intensive task requirements
  - **Why needed here:** The paper evaluates compressed LLMs on knowledge-intensive tasks, requiring understanding of what makes tasks "knowledge-intensive"
  - **Quick check question:** What characteristics make a task knowledge-intensive versus reasoning-based?

## Architecture Onboarding

- **Component map:** LLM-KICK benchmark framework -> Compression algorithms (SparseGPT, Wanda, GPTQ) -> Evaluation datasets (FreebaseQA, MMLU, TriviaQA, CNN/DailyMail) -> Assessment metrics (exact match, accuracy, GPT-4 judge scores)

- **Critical path:**
  1. Apply compression algorithms to base LLM (Vicuna)
  2. Evaluate compressed models on diverse task settings
  3. Compare performance against perplexity changes
  4. Identify compression method effectiveness and limitations

- **Design tradeoffs:**
  - Using existing datasets vs creating new ones for evaluation
  - Focusing on Vicuna models vs exploring other LLM architectures
  - Evaluating only top-2 pruning methods vs comprehensive coverage

- **Failure signatures:**
  - Performance degradation despite minimal perplexity changes
  - Structured sparsity failure on knowledge-intensive tasks
  - Loss of unique token generation capability

- **First 3 experiments:**
  1. Apply one-shot magnitude pruning to Vicuna-7B at 30% sparsity and measure performance on FreebaseQA
  2. Apply SparseGPT pruning to Vicuna-13B at 50% sparsity and evaluate on MMLU benchmark
  3. Apply 8-bit GPTQ quantization to Vicuna-7B and assess in-context retrieval performance on TriviaQA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sparsity ratio for structured N:M sparsity patterns in knowledge-intensive tasks for LLMs?
- Basis in paper: [explicit] The paper states that "all SoTA pruning methods do not work satisfactorily for structured N:M sparsity patterns on LLM-KICK" and observes significant performance drops.
- Why unresolved: The paper shows that structured sparsity fails but doesn't explore what specific N:M ratios might work better or under what conditions they might be effective.
- What evidence would resolve it: Systematic experiments varying N:M ratios (e.g., 1:2, 2:4, 4:8) across different task types and knowledge domains to identify any scenarios where structured sparsity might be beneficial.

### Open Question 2
- Question: How does calibration data quality affect the performance of pruning methods like SparseGPT and Wanda at high sparsity ratios?
- Basis in paper: [explicit] The paper shows that "calibration sample count plays a vital role in preserving the performance of SparseGPT unlike Wanda" and that "carefully selected calibration samples can play a vital role in designing better pruning algorithms."
- Why unresolved: The paper demonstrates the importance of calibration but doesn't explore what makes calibration data "good" or how to select optimal calibration samples.
- What evidence would resolve it: Experiments varying calibration data quality, diversity, and relevance to test tasks, along with analysis of which characteristics of calibration data lead to better pruning performance.

### Open Question 3
- Question: Can knowledge augmentation through parameter-efficient fine-tuning methods (like LoRA or QLoRA) effectively recover lost knowledge in compressed LLMs?
- Basis in paper: [inferred] The paper mentions this as future work: "we aim to investigate how the lost knowledge due to compression can be recovered using parameter-efficient fine-tuning methods."
- Why unresolved: The paper identifies knowledge loss as a major issue but doesn't test whether fine-tuning can recover this lost capability.
- What evidence would resolve it: Comparative experiments showing performance of compressed LLMs before and after LoRA/QLoRA fine-tuning on knowledge-intensive tasks, measuring both knowledge retention and task performance.

## Limitations

- The study focuses exclusively on Vicuna models, limiting generalizability to other LLM architectures
- Evaluation relies heavily on GPT-4 as judge, introducing potential subjectivity and variability in scoring
- Several compression method implementations lack detailed hyperparameter specifications, making exact reproduction challenging
- The benchmark does not account for potential fine-tuning effects that might mitigate compression-induced performance drops

## Confidence

**High Confidence**: The finding that perplexity fails to capture subtle capability changes in compressed LLMs is well-supported by consistent experimental observations across multiple compression ratios and methods. The claim that quantization generally outperforms pruning is also strongly supported by the empirical results.

**Medium Confidence**: The assertion that structured N:M sparsity fails on knowledge-intensive tasks is based on observed performance drops but lacks mechanistic explanation for why this occurs. The conclusion about compressed LLMs struggling with knowledge-enriched generation is supported but may be task-dependent.

**Low Confidence**: The broader claim about needing entirely new compression methods is speculative, as the study only tests a limited set of existing approaches. The assessment of in-context retrieval robustness at high sparsities may be influenced by the specific datasets chosen.

## Next Checks

1. **Cross-model validation**: Apply the same compression methods to other LLM families (Llama, Mistral) to verify whether Vicuna-specific findings generalize across architectures.

2. **Human evaluation replication**: Conduct human expert evaluations on a subset of tasks (e.g., summarization quality) to validate GPT-4 judge scores and assess inter-judge reliability.

3. **Ablation on pruning strategy**: Test alternative pruning criteria (e.g., layer-wise vs global importance) at identical sparsity ratios to isolate whether performance degradation stems from sparsity level or pruning algorithm design.