---
ver: rpa2
title: Large Language Models are biased to overestimate profoundness
arxiv_id: '2310.14422'
source_url: https://arxiv.org/abs/2310.14422
tags:
- profound
- statements
- llms
- profoundness
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how large language models (LLMs) such as GPT-4
  and others rate the profoundness of pseudo-profound, mundane, and motivational statements
  compared to human judgments. Using a 5-point Likert scale, the models were tested
  with various prompting strategies, including chain-of-thought and few-shot learning
  prompts.
---

# Large Language Models are biased to overestimate profoundness

## Quick Facts
- arXiv ID: 2310.14422
- Source URL: https://arxiv.org/abs/2310.14422
- Reference count: 11
- LLMs systematically overestimate profoundness of pseudo-profound statements compared to human judgments

## Executive Summary
This study reveals that most large language models (LLMs) systematically overestimate the profoundness of pseudo-profound statements, failing to detect their semantic incoherence in the same way humans do. Using 5-point Likert scale ratings across various prompt strategies, the research found that only Tk-Instruct consistently underestimated profoundness, while other models like GPT-4, Llama-2, and Vicuna showed significant bias toward attributing false depth to meaningless statements. Few-shot learning prompts were effective at reducing this bias, while chain-of-thought prompting had no significant effect.

## Method Summary
The study evaluated 10 different prompting strategies on multiple LLMs (GPT-4, Flan-T5, Llama-2 variants, Vicuna, Tk-Instruct) using 5 datasets containing pseudo-profound, mundane, and motivational statements. Researchers conducted 20 repetitions per experiment with temperature settings of 0.1 and 0.7, comparing LLM ratings to human judgments on a 5-point Likert scale. Statistical analyses included 3x2 mixed ANOVA and regression analysis to examine bias patterns and the effectiveness of different prompting strategies.

## Key Results
- Most LLMs overestimated profoundness of pseudo-profound statements compared to human ratings
- Only Tk-Instruct consistently underestimated profoundness across all statement types
- Few-shot learning prompts effectively reduced overestimation bias in GPT-4
- Chain-of-thought prompting showed no significant effect on reducing bias
- Llama-2 RLHF version showed higher profoundness ratings than non-RLHF version

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs systematically overestimate profoundness of pseudo-profound statements compared to humans.
- Mechanism: LLMs lack dynamic semantic coherence evaluation; they match patterns of syntactic structure without evaluating meaning.
- Core assumption: Profoundness requires detecting semantic incongruity between word associations and overall meaning.
- Evidence anchors: [abstract] "LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct"; [section] "This trend was also captured by the LLMs’ ratings... most of LLMs’ ratings of profoundness were constantly higher than human ratings"

### Mechanism 2
- Claim: Few-shot learning prompts reduce overestimation bias by providing examples of non-profound statements.
- Mechanism: Few-shot examples create reference frames that help LLMs calibrate their profoundness scale downward for pseudo-profound statements.
- Core assumption: LLMs learn better from concrete examples than abstract instructions for this type of evaluation task.
- Evidence anchors: [abstract] "Only few-shot learning prompts... draw LLMs ratings closer to humans"; [section] "Further analyses revealed that the interaction was driven by n-shot learning evaluation prompts, which lowered the ratings for pseudo-profound BS statements"

### Mechanism 3
- Claim: RLHF fine-tuning increases profoundness overestimation bias by reinforcing agreeable responses.
- Mechanism: Human feedback during RLHF may reward responses that seem agreeable or thoughtful, even when statements lack substance.
- Core assumption: RLHF training data contains human preferences for responses that sound profound rather than accurate.
- Evidence anchors: [abstract] "this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements"; [section] "Our results provide a hint in this direction... Llama-2 RLHF version consistently provides higher ratings of profoundness"

## Foundational Learning

- Concept: Semantic coherence evaluation
  - Why needed here: Understanding how humans detect meaningless statements requires grasping how semantic relationships create meaning
  - Quick check question: How would you determine if "Consciousness is the growth of coherence, and of us" has semantic coherence?

- Concept: Few-shot learning effectiveness
  - Why needed here: The study shows few-shot prompts specifically reduce bias, suggesting this learning paradigm has unique properties
  - Quick check question: Why might three examples be more effective than chain-of-thought instructions for this task?

- Concept: Reinforcement learning from human feedback
  - Why needed here: RLHF appears to increase rather than reduce the profoundness bias, suggesting unintended consequences
  - Quick check question: What aspect of human feedback might lead models to favor responses that sound profound?

## Architecture Onboarding

- Component map: LLMs (GPT-4, Flan-T5, Llama-2 variants, Vicuna, Tk-Instruct) -> Prompting strategies (original, few-shot, chain-of-thought) -> Evaluation datasets (pseudo-profound, mundane, motivational) -> Human judgment baseline
- Critical path: Generate statements → Apply different prompts → Collect model ratings → Compare to human baseline → Analyze bias patterns
- Design tradeoffs: Using temperature variation showed no significant effect, suggesting deterministic outputs are sufficient. The choice between prompt types (few-shot vs CoT) has major impact on bias reduction.
- Failure signatures: Systematic overestimation of profoundness across statement types, especially for pseudo-profound statements. Tk-Instruct's opposite pattern (underestimation) indicates architecture-specific behavior.
- First 3 experiments:
  1. Test whether adding explicit semantic coherence checks to prompts reduces overestimation bias
  2. Compare few-shot learning with different numbers of examples (1-shot vs 5-shot) to find optimal calibration
  3. Evaluate whether pre-training data filtering for semantic coherence reduces inherent bias before RLHF fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sizes of LLMs (e.g., 7B, 13B, 70B parameters) affect their tendency to overestimate profoundness in pseudo-profound statements?
- Basis in paper: [inferred] The paper tests several LLMs with different parameter sizes (13B) but doesn't systematically explore the effect of scale on this bias.
- Why unresolved: The study only used a limited range of model sizes, and the relationship between model size and profoundness bias remains unexplored.
- What evidence would resolve it: Systematic testing of LLMs across a wider range of parameter sizes while measuring their profoundness ratings for pseudo-profound statements.

### Open Question 2
- Question: What specific aspects of Reinforcement Learning from Human Feedback (RLHF) training contribute to the overestimation of profoundness in LLMs?
- Basis in paper: [explicit] The paper notes that Llama-2 with RLHF shows higher profoundness ratings than its non-RLHF counterpart, suggesting RLHF may introduce this bias.
- Why unresolved: The study only observes the correlation between RLHF and bias but doesn't identify which aspects of RLHF training are responsible.
- What evidence would resolve it: Detailed analysis of RLHF training data and procedures, including comparison of different RLHF implementations and their effects on profoundness bias.

### Open Question 3
- Question: How does the overestimation of profoundness in LLMs relate to their performance on other semantic coherence tasks?
- Basis in paper: [inferred] The study focuses on profoundness detection but doesn't explore how this bias correlates with other semantic reasoning abilities.
- Why unresolved: The paper only examines profoundness detection in isolation without connecting it to broader semantic processing capabilities.
- What evidence would resolve it: Comprehensive testing of LLMs on various semantic coherence tasks alongside profoundness detection to identify potential relationships between these abilities.

## Limitations
- The study doesn't fully explain why Tk-Instruct exhibits opposite behavior (underestimation) compared to other models
- Limited exploration of how model size affects profoundness bias
- Focus on specific statement types without exploring broader semantic categories

## Confidence

**High confidence**: Most LLMs overestimate pseudo-profound statement profoundness compared to human ratings, supported by robust statistical analysis (3x2 mixed ANOVA) and consistent patterns across multiple models and datasets.

**Medium confidence**: The mechanism explanations for why few-shot learning works better than chain-of-thought prompting are not definitively established, though the pattern is clear.

**Low confidence**: Generalizability beyond the specific statement types tested, as the study focused primarily on pseudo-profound, mundane, and motivational statements.

## Next Checks
1. Test whether explicitly prompting LLMs to evaluate semantic coherence (rather than just profoundness) reduces overestimation bias, isolating whether the issue is about semantic evaluation capability.

2. Conduct ablation studies comparing pre-RLHF and post-RLHF versions of the same models on profoundness tasks to quantify exactly how much RLHF contributes to the observed bias.

3. Expand testing to include models with different pre-training approaches (not just RLHF variants) to determine whether the bias stems from pre-training data, fine-tuning methodology, or fundamental architecture limitations.