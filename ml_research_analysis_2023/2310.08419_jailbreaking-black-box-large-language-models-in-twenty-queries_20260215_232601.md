---
ver: rpa2
title: Jailbreaking Black Box Large Language Models in Twenty Queries
arxiv_id: '2310.08419'
source_url: https://arxiv.org/abs/2310.08419
tags:
- prompt
- attacker
- pair
- language
- jailbreaks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PAIR, a new algorithm for generating prompt-level
  jailbreaks for large language models (LLMs) using only black-box access. PAIR uses
  an attacker LLM to automatically generate jailbreaking prompts for a target LLM
  by iteratively refining candidate prompts based on the target's responses.
---

# Jailbreaking Black Box Large Language Models in Twenty Queries

## Quick Facts
- arXiv ID: 2310.08419
- Source URL: https://arxiv.org/abs/2310.08419
- Reference count: 40
- Primary result: PAIR achieves jailbreak success in fewer than 20 queries, orders of magnitude more efficient than existing methods

## Executive Summary
This paper introduces PAIR, a black-box algorithm for generating prompt-level jailbreaks against large language models using only query access. The method employs an attacker LLM that iteratively refines candidate prompts based on feedback from a judge LLM, creating a social engineering-inspired attack framework. PAIR demonstrates competitive jailbreak success rates while requiring dramatically fewer queries than existing methods like GCG. The algorithm is effective against both open-source models like Vicuna and closed-source models like GPT-3.5/4.

## Method Summary
PAIR uses an attacker LLM to generate and iteratively refine jailbreaking prompts against a target LLM through black-box query access. The attacker generates candidate prompts using conversation history, the target responds, and a judge LLM evaluates the jailbreak success. Based on the judge's feedback, the attacker refines the prompt through chain-of-thought reasoning about what went wrong. The process uses parallel streams (N) with limited depth (K) to balance exploration breadth against computational cost, typically using N=20 streams and K=3 iterations.

## Key Results
- PAIR requires fewer than 20 queries on average to produce successful jailbreaks
- Achieves competitive jailbreak success rates compared to white-box methods like GCG
- Shows strong transferability across different model architectures (Vicuna, GPT-3.5/4, PaLM-2)
- Vicuna outperforms GPT-3.5 as the attacker model despite lower benchmark scores

## Why This Works (Mechanism)

### Mechanism 1
Iterative refinement through conversational feedback enables efficient prompt-level jailbreaking. The attacker LLM generates candidate prompts, receives target LLM responses, and iteratively refines based on judge feedback. This creates a dialogue where the attacker learns which semantic approaches succeed or fail.

### Mechanism 2
Black-box access with semantic prompts enables efficient attacks compared to white-box token-level approaches. By using only query access and generating interpretable prompts, PAIR avoids the computational cost of token-level optimization while maintaining attack effectiveness.

### Mechanism 3
Parallel streams with shallow depth optimize query efficiency. Running multiple parallel conversation streams with limited depth (N streams Ã— K iterations) balances exploration breadth with computational cost, as jailbreaks are most likely found in early conversation turns.

## Foundational Learning

- Concept: Black-box adversarial attacks on language models
  - Why needed here: PAIR operates with only query access to target models, requiring understanding of black-box attack methodologies
  - Quick check question: What distinguishes black-box from white-box attacks in terms of required model access?

- Concept: Chain-of-thought reasoning for iterative refinement
  - Why needed here: The improvement text mechanism uses chain-of-thought to help the attacker LLM analyze failures and generate better prompts
  - Quick check question: How does chain-of-thought reasoning help the attacker LLM improve its jailbreak attempts?

- Concept: Semantic versus token-level adversarial attacks
  - Why needed here: PAIR generates interpretable prompts rather than optimizing token sequences, requiring understanding of both attack types
  - Quick check question: What are the key differences between semantic prompt attacks and token-level optimization attacks?

## Architecture Onboarding

- Component map: Attacker LLM -> Target LLM -> Judge LLM -> Conversation history update -> Attacker LLM refinement

- Critical path:
  1. Initialize attacker with objective and empty conversation history
  2. Attacker generates prompt using conversation history
  3. Target LLM processes prompt and generates response
  4. Judge LLM evaluates prompt-response pair
  5. If jailbroken, return prompt; else update conversation history
  6. Repeat until success or max iterations

- Design tradeoffs:
  - Parallel streams (N) vs depth (K): More streams increase exploration but reduce refinement depth
  - Attacker LLM choice: Vicuna offers better jailbreak success than GPT-3.5 despite lower benchmark scores
  - System prompt complexity: More examples improve creativity but increase computational cost

- Failure signatures:
  - No jailbreaks found after max iterations: May indicate need for different attacker LLM or system prompt
  - Consistently low judge scores: Suggests attacker is not learning effectively from feedback
  - Target model consistently refuses: Indicates strong safety alignment that may require different attack strategies

- First 3 experiments:
  1. Run PAIR with default parameters (N=20, K=3) on Vicuna as both attacker and target to verify basic functionality
  2. Test PAIR with Vicuna attacker vs GPT-3.5 target using 5 different objectives from AdvBench
  3. Compare PAIR performance with and without improvement text to measure chain-of-thought impact

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of attacker model (e.g., Vicuna vs GPT-3.5) impact the jailbreak success rate and efficiency of PAIR? The paper compares Vicuna and GPT-3.5 but does not definitively explain why Vicuna outperforms despite lower benchmark scores.

### Open Question 2
How does the inclusion of examples and improvement instructions in the attacker model's system prompt affect the jailbreak success rate and efficiency of PAIR? The paper observes performance differences with and without these elements but lacks comprehensive analysis.

### Open Question 3
How does the number of parallel streams (N) and the maximum depth (K) affect the efficiency and success rate of PAIR? The paper mentions the tradeoff but does not provide optimal combinations across different attacker-target pairs.

## Limitations
- Focuses only on semantic jailbreaks without exploring token-level optimization combinations
- Evaluation limited to single-turn jailbreaks, not testing multi-turn attack scenarios
- Assumes access to capable attacker and judge LLMs without exploring performance with less capable models

## Confidence

**High Confidence:**
- PAIR's efficiency advantage over GCG (direct comparison shows ~15 queries vs 256K)
- Vicuna's superior performance as attacker compared to GPT-3.5 (supported by ablation studies)
- Optimal N=20, K=3 parameters (verified through ablation experiments)

**Medium Confidence:**
- PAIR's transferability claims (tested on 3-4 models, but limited model diversity)
- Claim that semantic attacks are sufficient (assumes token-level defenses are not dominant)
- Diminishing returns after K>3 (based on limited ablation study)

**Low Confidence:**
- Claims about PAIR's robustness to future model improvements (no longitudinal testing)
- Assumption that parallel streams always outperform sequential refinement (limited exploration of this tradeoff)
- Generalizability to non-AdvBench harmful behaviors (test set may not represent all jailbreak scenarios)

## Next Checks

1. **Cross-model robustness test:** Evaluate PAIR's performance when the attacker, target, and judge are all different model families to verify algorithm independence from model-specific characteristics.

2. **Multi-turn jailbreak capability:** Extend PAIR to handle multi-turn jailbreak scenarios where the target model requires sustained conversation to bypass safety measures, measuring both success rate and query efficiency.

3. **Transferability stress test:** Systematically evaluate PAIR's success rate when transferring jailbreaks across models with varying levels of safety alignment to quantify the limits of cross-model transferability.