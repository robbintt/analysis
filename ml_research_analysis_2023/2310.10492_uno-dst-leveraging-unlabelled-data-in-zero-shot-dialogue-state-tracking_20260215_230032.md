---
ver: rpa2
title: 'UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking'
arxiv_id: '2310.10492'
source_url: https://arxiv.org/abs/2310.10492
tags:
- slot
- dialogue
- types
- task
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UNO-DST, a zero-shot dialogue state tracking
  (DST) method that leverages unlabeled data in target domains using joint and self-training
  with auxiliary tasks. The core idea is to transform zero-shot DST into few-shot
  DST by generating and selecting quality samples in unknown domains via cycle consistency
  between main and auxiliary tasks.
---

# UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking

## Quick Facts
- arXiv ID: 2310.10492
- Source URL: https://arxiv.org/abs/2310.10492
- Reference count: 34
- Primary result: Achieves 8% average JGA improvement across all domains in MultiWOZ, reaching 56.11% JGA

## Executive Summary
UNO-DST introduces a novel approach to zero-shot dialogue state tracking by leveraging unlabeled data in target domains through joint and self-training with auxiliary tasks. The method transforms zero-shot DST into few-shot DST by creating and selecting quality samples via cycle consistency between main and auxiliary tasks. UNO-DST achieves state-of-the-art performance on MultiWOZ dataset with an average JGA improvement of 8% across all domains, demonstrating effectiveness even on large language models like ChatGPT.

## Method Summary
UNO-DST employs a two-phase training approach: joint training and self-training. During joint training, the model simultaneously learns a main task (slot value prediction) and an auxiliary task (slot type prediction) using labeled data from source domains. The auxiliary task generates slot types as inverse prompts for the main task, creating a cycle-consistent loop. In the self-training phase, the model generates slot values and slot types, selects high-quality samples based on cycle consistency, and fine-tunes the model with these selected samples. This approach effectively converts zero-shot DST into few-shot DST by leveraging unlabeled data in target domains.

## Key Results
- Achieves 56.11% JGA on MultiWOZ, improving baseline by 8.38%
- Outperforms previous state-of-the-art zero-shot DST methods by 3.72% in JGA
- Successfully applied to large language models like ChatGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cycle consistency between main and auxiliary tasks enables generation and selection of quality samples in unknown target domains.
- Mechanism: The main task predicts slot values from dialogue history and slot types, while the auxiliary task predicts slot types from masked dialogue history and slot values. The cycle consistency ensures that outputs from one task serve as valid inputs to the other, creating a self-reinforcing loop for generating and selecting high-quality dialogue states.
- Core assumption: The inverse relationship between slot value prediction and slot type prediction is meaningful and can be leveraged for cycle consistency.
- Evidence anchors:
  - [abstract] "Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning."
  - [section 3.2] "We design the auxiliary task as the inverse (converse) prompt of the main task. In opposition to the main task, the auxiliary task thus takes the slot values v as input and generates the slot types s′ as outputs, which forms a cycle-consistent loop as a foil to the main task."
  - [corpus] Weak - only 5 related papers found, average neighbor FMR=0.311, indicating limited direct evidence in literature for this specific mechanism.
- Break condition: If the inverse relationship between slot value and slot type prediction is not meaningful or if the masking strategy in the auxiliary task does not capture relevant context, the cycle consistency will fail to generate quality samples.

### Mechanism 2
- Claim: Joint training with auxiliary tasks improves the accuracy of the main task and prepares the model for self-training.
- Mechanism: By jointly training both the main task (slot value prediction) and auxiliary task (slot type prediction), the model learns to understand the semantic and context information from the dialogue history more effectively. This joint training period is critical as it prepares the model for the subsequent self-training period.
- Core assumption: Joint training of related tasks can improve the performance of each individual task through shared learning.
- Evidence anchors:
  - [section 3.3] "We conduct a simple version of joint training with only two tasks: the main DST task and the auxiliary task. The training samples for the main tasks are created using a prompt of dialogue history and slot type, while the samples for the auxiliary DST tasks are created using an inverse prompt of masked dialogue history."
  - [section 5] "For the joint training period in the MultiWOZ dataset (Table 2), we are using the same model and prompt as T5DST. UNO-DST shows an increase of more than 4% for JGA across all checkpoints."
  - [corpus] Weak - limited direct evidence in literature for this specific joint training approach with auxiliary tasks for DST.
- Break condition: If the auxiliary task does not provide meaningful additional information or if the joint training does not improve the main task's performance, the overall approach will fail.

### Mechanism 3
- Claim: Self-training with selected samples from cycle consistency converts zero-shot DST into few-shot DST, improving performance.
- Mechanism: In the self-training period, the model generates slot values and slot types using both tasks, selects good samples based on cycle consistency, and fine-tunes the model with these selected samples. This process effectively converts the zero-shot problem into a few-shot problem by leveraging unlabeled data in the target domain.
- Core assumption: The cycle consistency between the main and auxiliary tasks is sufficient to identify high-quality samples for fine-tuning.
- Evidence anchors:
  - [section 3.4] "In the self-training period, we divide the strategy into three steps: termed generation, selection and fine-tuning... Step 2 Selection tests the cycle consistency between main tasks and auxiliary tasks by comparing the predicted slot types s′ with the original slot types s in each dialogue turn."
  - [section 5] "For the MultiWOZ dataset (Table 2), self-training further improves average JGA by 3.09% after joint training. Compared with the baseline, the best performance increases by 8.38% in JGA."
  - [corpus] Weak - limited direct evidence in literature for this specific self-training approach with cycle consistency for DST.
- Break condition: If the cycle consistency is not reliable enough to select high-quality samples, or if the selected samples are not representative of the target domain, the fine-tuning will not improve performance.

## Foundational Learning

- Concept: Cycle consistency in multi-task learning
  - Why needed here: To ensure that the outputs of the main and auxiliary tasks are mutually reinforcing and can be used to generate and select quality samples.
  - Quick check question: Can you explain how cycle consistency works in the context of UNO-DST and why it is important for generating and selecting dialogue states?

- Concept: Joint training of related tasks
  - Why needed here: To improve the performance of the main task by leveraging the auxiliary task and to prepare the model for self-training.
  - Quick check question: How does joint training with an auxiliary task improve the accuracy of the main task in UNO-DST?

- Concept: Self-training with pseudo labels
  - Why needed here: To convert the zero-shot problem into a few-shot problem by leveraging unlabeled data in the target domain.
  - Quick check question: How does self-training with selected samples from cycle consistency improve the performance of UNO-DST in the target domain?

## Architecture Onboarding

- Component map: Main task (slot value prediction) -> Auxiliary task (slot type prediction) -> Joint training -> Self-training (generation, selection, fine-tuning) -> Fine-tuning

- Critical path:
  1. Joint training of main and auxiliary tasks using labeled data in source domains
  2. Self-training in target domain using unlabeled data and cycle consistency
  3. Fine-tuning the model with selected samples from self-training

- Design tradeoffs:
  - Complexity vs. performance: Adding an auxiliary task increases model complexity but improves performance through joint and self-training.
  - Computational cost vs. accuracy: Self-training with cycle consistency requires additional computation but significantly improves zero-shot performance.

- Failure signatures:
  - If the auxiliary task does not improve the main task's performance during joint training, the overall approach may fail.
  - If the cycle consistency is not reliable enough to select high-quality samples during self-training, the fine-tuning will not improve performance.
  - If the model overfits to the selected samples during fine-tuning, the performance on unseen data may degrade.

- First 3 experiments:
  1. Joint training without auxiliary task: Train the main task alone and compare performance with joint training to validate the benefit of the auxiliary task.
  2. Self-training without cycle consistency: Use random selection instead of cycle consistency to validate the importance of the selection mechanism.
  3. Ablation study on auxiliary task design: Test different designs for the auxiliary task (e.g., different masking strategies) to optimize performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UNO-DST change with the number of rounds in self-training, and what is the optimal stopping criterion?
- Basis in paper: [explicit] The paper discusses self-training and mentions that performance continues to improve with multiple rounds, but eventually plateaus with diminishing returns.
- Why unresolved: The paper does not provide a systematic study on how to determine the optimal number of rounds for self-training or the best criteria for early stopping.
- What evidence would resolve it: Empirical results showing the performance of UNO-DST over multiple rounds of self-training, along with a proposed method for determining the optimal stopping point based on performance metrics.

### Open Question 2
- Question: Can UNO-DST be effectively applied to larger pre-trained language models, and how does its performance scale with model size?
- Basis in paper: [explicit] The paper mentions that due to computational resource limitations, experiments were conducted on smaller PLMs like "t5-small". It suggests future work to test the performance of UNO-DST on larger models.
- Why unresolved: The paper does not explore the effectiveness of UNO-DST on larger PLMs or investigate how the model's performance scales with model size.
- What evidence would resolve it: Comparative results of UNO-DST on different sizes of pre-trained language models, showing how performance changes with increasing model size.

### Open Question 3
- Question: How can UNO-DST be adapted to handle unseen slot types more effectively, especially those requiring "yes/no" or "none" responses?
- Basis in paper: [explicit] The paper discusses the challenge of unseen slot types and mentions that half of the unseen slot types in the "hotel" domain are related to "yes/no" slot values, which are less trained in the joint training settings.
- Why unresolved: The paper does not provide a detailed strategy for improving the prediction of unseen slot types, particularly those requiring "yes/no" or "none" responses.
- What evidence would resolve it: Results showing improved performance on unseen slot types after implementing a strategy specifically designed to handle "yes/no" or "none" responses, such as a modified joint training approach or an additional auxiliary task.

## Limitations

- Limited validation of cycle consistency reliability across diverse domain shifts
- Unclear selection mechanism thresholds and statistical validation of sample quality
- Unexplored performance on rare slot types and long-tail distributions

## Confidence

- **High Confidence**: The overall methodology framework and the observed 8% improvement in JGA across MultiWOZ domains are well-supported by results.
- **Medium Confidence**: The effectiveness of joint training with auxiliary tasks is demonstrated, but the specific contribution of each component to the final performance needs further isolation.
- **Low Confidence**: The robustness of cycle consistency selection in highly diverse or out-of-distribution domains is not thoroughly tested.

## Next Checks

1. **Cross-domain generalization**: Test UNO-DST on domains with significantly different slot distributions than MultiWOZ to assess the limits of cycle consistency reliability.
2. **Component ablation**: Systematically remove the auxiliary task and cycle consistency to quantify their individual contributions to the 8% JGA improvement.
3. **Sample quality analysis**: Analyze the semantic similarity between selected self-training samples and ground truth to validate the effectiveness of the selection mechanism.