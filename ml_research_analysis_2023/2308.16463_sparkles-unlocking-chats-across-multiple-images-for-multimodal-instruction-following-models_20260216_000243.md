---
ver: rpa2
title: 'Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following
  Models'
arxiv_id: '2308.16463'
source_url: https://arxiv.org/abs/2308.16463
tags:
- image
- images
- dialogue
- assistant
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of maintaining dialogue coherence
  across multiple images for multimodal instruction-following models. The authors
  introduce SparklesChat, a model that integrates multiple images at the word level
  within dialogue, enabling fine-grained and human-like multimodal interaction.
---

# Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models

## Quick Facts
- arXiv ID: 2308.16463
- Source URL: https://arxiv.org/abs/2308.16463
- Reference count: 40
- Primary result: Word-level image-text interleaving improves cross-image reasoning for multimodal dialogue models

## Executive Summary
This paper addresses the challenge of maintaining dialogue coherence across multiple images for multimodal instruction-following models. The authors introduce SparklesChat, a model that integrates multiple images at the word level within dialogue, enabling fine-grained and human-like multimodal interaction. To support training, they present SparklesDialogue, the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Additionally, they construct SparklesEval, a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns. Experimental results demonstrate that SparklesChat outperforms existing models on standard vision-and-language tasks, achieving 56.7% accuracy on the BISON binary image selection task and 58.0% on the NLVR2 visual reasoning task.

## Method Summary
The method involves fine-tuning a frozen multimodal backbone on dialogue-style data rather than static image-text pairs. SparklesChat integrates multiple images at the word level within dialogue by embedding image representations between individual words rather than entire image-sentence blocks. The training uses SparklesDialogue, a machine-generated dataset created with GPT-4 using dialogue demonstrations as in-context examples. GPT-4 is also used to evaluate model performance through SparklesEval, which scores responses across three interpretable criteria: relevancy, coherency, and consistency.

## Key Results
- SparklesChat achieves 56.7% accuracy on the BISON binary image selection task
- SparklesChat achieves 58.0% accuracy on the NLVR2 visual reasoning task
- SparklesChat scores 8.56 out of 10 on SparklesEval, significantly exceeding MiniGPT-4's score of 3.91

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Word-level image-text interleaving improves cross-image reasoning over sentence-level concatenation
- Mechanism: SparklesChat places image representations between individual words rather than entire image-sentence blocks, enabling fine-grained alignment of visual context with linguistic structure during dialogue
- Core assumption: Visual understanding at the word level captures more nuanced relationships than at the sentence level
- Evidence anchors:
  - [abstract] "SparklesChat, as shown in Figure 1, integrates multiple images at the word level (e.g., 'Can you link the celebration occurring in IMAGE#2331159 and the dirt bike race in IMAGE#2330601?')"
  - [section 3.1] "In SparklesChat, image representations of different images are embedded between text according to their positions in dialogues"
- Break condition: If the visual encoder cannot produce discriminative features at the granularity required for word-level insertion, the fine-grained alignment benefit disappears

### Mechanism 2
- Claim: Dialogue demonstrations guide GPT-4 to generate coherent multi-turn interactions
- Mechanism: GPT-4 uses a small set of human-curated dialogue demonstrations as in-context examples to maintain conversational flow and image reference consistency across turns
- Core assumption: GPT-4 can generalize from few demonstrations to produce diverse, contextually appropriate dialogues
- Evidence anchors:
  - [section 4.1] "The Dialogue Demonstrations serve as in-context learning examples, steering GPT-4 towards generating well-formatted and diverse responses"
  - [section F] "We initiated the creation of hundreds of demonstration dialogues with GPT-4's assistance"
- Break condition: If demonstrations are too narrow in scope or biased toward specific image types, generated dialogues will lack diversity and fail to cover broader user scenarios

### Mechanism 3
- Claim: GPT-4-assisted evaluation provides reliable, interpretable model assessment
- Mechanism: SparklesEval uses GPT-4 to score responses across three interpretable criteria, avoiding subjective human bias and enabling fine-grained comparison
- Core assumption: GPT-4 judgments are consistent with human evaluation and sufficiently sensitive to distinguish model performance differences
- Evidence anchors:
  - [section 4.3] "we devised SparklesEval, an evaluation benchmark assisted by GPT-4 in data construction and evaluation"
  - [table 5] "Both MiniGPT-4 and SparklesChat generate responses based on the question and accompanying visual image. At the same time, GPT-4 is a reference LLM that only uses textual information"
- Break condition: If GPT-4 becomes outdated or its evaluation criteria drift, the benchmark's reliability degrades; also, if the three criteria do not fully capture conversational competence, scores will be incomplete

## Foundational Learning

- Concept: Multimodal instruction tuning
  - Why needed here: SparklesChat requires fine-tuning a frozen multimodal backbone on dialogue-style data rather than static image-text pairs
  - Quick check question: What loss is computed during training, and why is the prompt not trained?
- Concept: In-context learning with GPT-4
  - Why needed here: Dialogue demonstrations are used to guide GPT-4 in generating diverse, coherent dialogues for training and evaluation data
  - Quick check question: How does the choice of demonstration dialogues affect the diversity of generated dialogues?
- Concept: Vision-language alignment with frozen encoders
  - Why needed here: SparklesChat freezes both the vision encoder and LLM to preserve their pre-trained capabilities while only training a projection layer
  - Quick check question: Why is only the projection layer trainable, and what happens if other components are unfrozen?

## Architecture Onboarding

- Component map: Vision encoder (BLIP-2 ViT + Q-Former) → frozen → Projection layer → trainable → LLM (Vicuna/LLaMA) → frozen
- Critical path: Input text/image → projection layer alignment → LLM generation → output response
- Design tradeoffs:
  - Freezing encoders preserves pre-trained knowledge but limits adaptation to dialogue-specific features
  - Word-level interleaving increases fine-grained control but requires careful handling of positional encoding
  - Using GPT-4 for data generation ensures high quality but incurs high API cost and dependency on external service
- Failure signatures:
  - If projection layer training diverges, the model outputs garbled text or ignores images
  - If word-level interleaving is implemented incorrectly, image references may be misaligned with dialogue context
  - If dialogue demonstrations are too repetitive, generated dialogues will lack diversity and fail to generalize
- First 3 experiments:
  1. Train SparklesChat on single-turn dialogues only; evaluate on BISON to confirm baseline reasoning capability
  2. Compare word-level vs sentence-level image-text interleaving on SparklesEval; measure change in coherence scores
  3. Swap GPT-4 with GPT-3.5-turbo for data generation; measure effect on dialogue diversity and evaluation score variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SparklesChat maintain multi-image and multi-turn consistency when handling extended conversations?
- Basis in paper: [explicit] The paper identifies this as a limitation, noting that the model "may lose the context of prior images after several dialogue turns or mix up the contents of different images."
- Why unresolved: This is a known weakness that requires architectural innovations in position encoding and attention mechanisms to enhance context recall.
- What evidence would resolve it: Comparative studies showing improved performance on multi-turn dialogue benchmarks after implementing enhanced position encoding or attention mechanisms.

### Open Question 2
- Question: What are the specific architectural changes needed to extend SparklesChat's capabilities to handle text-rich images (charts, tables, receipts) and domain-specific images (medical scans, satellite photos)?
- Basis in paper: [explicit] The paper mentions this as a limitation, stating that "SparklesDialogue primarily concentrates on natural images, which limits its versatility in handling text-rich images such as charts, tables, and receipts, as well as domain-specific images such as medical scans, math illustrations, and satellite photos."
- Why unresolved: Current training data and model architecture are optimized for natural images, requiring fundamental changes to handle diverse image types.
- What evidence would resolve it: Successful fine-tuning or adaptation of SparklesChat on datasets containing diverse image types, demonstrating improved performance on text-rich and domain-specific images.

### Open Question 3
- Question: How can the reliability of SparklesEval be improved beyond current GPT-assisted evaluation methods?
- Basis in paper: [explicit] The paper acknowledges this limitation, stating "the reliability of SparklesEval is tied to the capabilities of current GPT models."
- Why unresolved: Current GPT models may have biases and limitations that affect evaluation consistency and accuracy.
- What evidence would resolve it: Comparative studies showing improved evaluation consistency and correlation with human judgments after incorporating more robust judge models or human evaluator assistance.

## Limitations

- The model may lose context of prior images after several dialogue turns or mix up contents of different images
- SparklesDialogue primarily focuses on natural images, limiting versatility for text-rich images and domain-specific images
- The reliability of SparklesEval is tied to the capabilities of current GPT models

## Confidence

**High Confidence:**
- SparklesChat architecture design and implementation details
- BISON and NLVR2 benchmark results showing numerical improvements
- Basic mechanism of word-level image-text interleaving

**Medium Confidence:**
- GPT-4-assisted evaluation provides reliable assessment of conversational competence
- Dialogue demonstrations effectively guide GPT-4 to generate diverse, coherent dialogues
- Fine-grained word-level alignment improves cross-image reasoning over sentence-level approaches

**Low Confidence:**
- The three GPT-4 evaluation criteria fully capture conversational competence
- SparklesChat's performance approaches GPT-4's level in practical applications
- The model's multi-turn reasoning capabilities scale well beyond demonstrated examples

## Next Checks

1. **Cross-dataset Generalization Test:** Evaluate SparklesChat on established multimodal dialogue datasets (e.g., MM-chat, ScienceQA) that were not used in training to verify whether word-level interleaving provides consistent benefits across different domains and dialogue styles.

2. **Human Evaluation Validation:** Conduct blind human evaluations comparing SparklesChat responses with GPT-4 outputs across the three SparklesEval criteria to verify that GPT-4's automated scoring correlates with human judgment, particularly for the "coherency" and "consistency" metrics.

3. **Ablation Study on Dialogue Demonstrations:** Systematically vary the number, diversity, and quality of dialogue demonstrations used to guide GPT-4 data generation, measuring the impact on both generated dialogue quality and downstream model performance to quantify the sensitivity to demonstration quality.