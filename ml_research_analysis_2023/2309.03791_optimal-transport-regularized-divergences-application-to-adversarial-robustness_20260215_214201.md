---
ver: rpa2
title: 'Optimal Transport Regularized Divergences: Application to Adversarial Robustness'
arxiv_id: '2309.03791'
source_url: https://arxiv.org/abs/2309.03791
tags:
- adversarial
- advs
- samples
- methods
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a new class of optimal-transport-regularized
  divergences, Dc, for comparing probability distributions, constructed via an infimal
  convolution between an information divergence and an optimal-transport cost. These
  are used to construct distribution neighborhoods for distributionally robust optimization
  (DRO) and, in particular, to enhance adversarial robustness of deep learning models.
---

# Optimal Transport Regularized Divergences: Application to Adversarial Robustness

## Quick Facts
- arXiv ID: 2309.03791
- Source URL: https://arxiv.org/abs/2309.03791
- Reference count: 40
- Key outcome: Improves robustified accuracy against AutoAttack on CIFAR-10 and CIFAR-100 by 1.9% and 2.1% respectively

## Executive Summary
This paper introduces Optimal-Transport-Regularized Divergences (OT-RD), a new class of divergences constructed via infimal convolution between information divergences and optimal transport costs. The method enables distributionally robust optimization by allowing both redistribution and transport of probability mass, providing more flexible adversarial sample construction. Applied to adversarial robustness, the ARMOR_D method demonstrates state-of-the-art performance on MNIST digit classification and malware detection tasks, outperforming existing adversarial training methods while reducing error rates significantly.

## Method Summary
The method constructs a divergence Dc(Q∥P) = inf_{η∈P(X)} {D(η∥P) + C(η,Q)} that combines information divergence D and optimal transport cost C. This formulation allows samples to be both redistributed (via D) and transported (via C). The distributionally robust optimization problem is then reformulated using convex conjugates into a tractable finite-dimensional optimization over model parameters and regularization coefficients. The method is implemented through an inner-outer loop structure: the inner loop constructs adversarial samples using OT-regularized losses, while the outer loop updates model parameters and hyperparameters.

## Key Results
- Improves robustified accuracy against AutoAttack on CIFAR-10 by 1.9% and CIFAR-100 by 2.1%
- Reduces error rates by 19.7% and 37.2% compared to prior methods on MNIST
- In malware detection, improves robustified accuracy under rF GSM 50 attack by 37.0%
- Lowers false negative and false positive rates by 51.1% and 57.53% respectively

## Why This Works (Mechanism)

### Mechanism 1
The infimal convolution formulation between information divergence and optimal transport cost enables both redistribution and transport of probability mass, providing more flexible adversarial sample construction. The divergence Dc(Q∥P) = inf_{η∈P(X)} {D(η∥P) + C(η,Q)} allows samples to be first redistributed according to D and then transported according to OT cost C, creating qualitatively different distribution neighborhoods from pure Wasserstein or f-divergence neighborhoods.

### Mechanism 2
The convex conjugate reformulation transforms the infinite-dimensional DRO problem into a tractable finite-dimensional optimization. By using the identity involving Gibbs variational principle, the problem converts to optimizing over real-valued parameters λ, ρ and model parameters θ, rather than over probability distributions, making it computationally tractable.

### Mechanism 3
The interpolation property between pure optimal transport and pure information divergence allows adaptive robustness. As scale parameter r → 0, the divergence approaches the OT cost, and as r → ∞, it approaches the information divergence. This enables the method to adaptively shift between focusing on sample transport and focusing on probability mass redistribution during training.

## Foundational Learning

- Concept: Optimal transport theory and Wasserstein distances
  - Why needed here: The method builds on optimal transport costs C(μ,ν) to measure distributional shifts beyond Euclidean distance
  - Quick check question: What is the Kantorovich dual formulation of the Wasserstein-1 distance and how does it relate to the c-transform?

- Concept: f-divergences and their convex conjugate properties
  - Why needed here: The method uses f-divergences (like KL and α-divergences) and requires understanding their variational representations and convex conjugates
  - Quick check question: How does the Gibbs variational principle allow converting an f-divergence maximization into a convex optimization problem?

- Concept: Distributionally robust optimization (DRO) framework
  - Why needed here: The method is fundamentally a DRO approach that maximizes expected loss over a neighborhood of distributions
  - Quick check question: What is the difference between moment-based DRO and divergence-based DRO, and why might divergence-based be more suitable for adversarial robustness?

## Architecture Onboarding

- Component map: Inner maximizer (constructs adversarial samples) -> Outer minimizer (updates model parameters) -> Hyperparameter tuner (selects neighborhood size, cost coefficients) -> Fail-safe mechanisms (ensures λ positivity, prevents label probability bounds violation)

- Critical path: Inner maximizer → Outer minimizer (with hyperparameters) → Model update → Evaluation under attack

- Design tradeoffs: Information divergence vs. transport cost weighting (L parameter), sample redistribution vs. sample shifting, computational complexity vs. robustness

- Failure signatures: NaN values in loss computation (typically from unbounded gradients), divergence of optimization (learning rates too high), poor robustness despite training (insufficient exploration of hyperparameter space)

- First 3 experiments:
  1. Implement the basic ARMOR_D method with KL divergence, ℓ2 norm, and no adversarial labels on MNIST to verify the inner-outer loop structure
  2. Add hyperparameter tuning for neighborhood size ϵ and cost coefficient L to find baseline performance
  3. Implement the advs,l variant with adversarial labels and test on a binary classification problem to verify the label perturbation mechanism

## Open Questions the Paper Calls Out

### Open Question 1
How does the ARMOR_D method perform on other datasets beyond MNIST and malware detection? The paper demonstrates effectiveness on MNIST digit classification and malware detection, but does not test it on other datasets, leaving generalizability unclear.

### Open Question 2
What is the impact of different hyperparameters on the performance of ARMOR_D? While the paper mentions hyperparameters were tuned using grid search, it does not provide a detailed analysis of how different hyperparameters affect performance or guidance on choosing hyperparameters for different datasets and tasks.

### Open Question 3
How does ARMOR_D compare to other distributionally robust optimization methods that use different divergence measures? The paper introduces OT-regularized divergences as a new class for DRO but does not directly compare ARMOR_D to other DRO methods using different divergence measures like Wasserstein, KL, or MMD.

## Limitations
- Primary uncertainty lies in practical impact of infimal convolution formulation on real-world adversarial robustness, with benefits potentially limited by hyperparameter sensitivity and computational overhead
- Method requires careful tuning of multiple hyperparameters which may not generalize well across different datasets and threat models
- Computational efficiency trade-off between robustness gains and training time overhead not fully characterized

## Confidence
- Mechanism 1 (Two-step transformation): Medium confidence - theoretical framework is sound but empirical validation across diverse threat models is limited
- Mechanism 2 (Convex conjugate reformulation): High confidence - mathematical derivation is rigorous and well-established in DRO literature
- Mechanism 3 (Interpolation property): Medium confidence - theoretical limits are proven but practical exploitation requires extensive hyperparameter tuning

## Next Checks
1. Cross-domain robustness validation: Test ARMOR_D on domains beyond image classification and malware detection, such as natural language processing tasks, to assess generalizability of the OT-regularized divergence approach.

2. Computational efficiency analysis: Measure the training time overhead of ARMOR_D compared to baseline adversarial training methods and quantify the tradeoff between robustness gains and computational cost.

3. Hyperparameter sensitivity study: Systematically vary key hyperparameters (neighborhood size, cost coefficients, divergence parameters) and measure their impact on both robustness and clean accuracy to identify stable operating regions.