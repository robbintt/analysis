---
ver: rpa2
title: Revisiting Hidden Representations in Transfer Learning for Medical Imaging
arxiv_id: '2302.08272'
source_url: https://arxiv.org/abs/2302.08272
tags:
- medical
- imagenet
- radimagenet
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of transfer learning
  for medical imaging, specifically comparing pre-training on ImageNet (natural images)
  versus RadImageNet (medical images). The authors fine-tune ResNet50 models pre-trained
  on these datasets on seven medical classification tasks.
---

# Revisiting Hidden Representations in Transfer Learning for Medical Imaging

## Quick Facts
- arXiv ID: 2302.08272
- Source URL: https://arxiv.org/abs/2302.08272
- Reference count: 27
- Key outcome: ImageNet pre-training generally outperforms RadImageNet across seven medical imaging tasks, with transfer benefits coming from weight initialization quality rather than feature reuse in early layers.

## Executive Summary
This study challenges conventional wisdom about transfer learning in medical imaging by comparing pre-training on ImageNet versus RadImageNet for seven medical classification tasks. Contrary to expectations, ImageNet pre-training consistently outperformed RadImageNet despite the latter being domain-specific. The authors found that while intermediate representations diverge significantly during fine-tuning, model predictions remain similar, suggesting that transfer learning benefits primarily from optimization dynamics rather than feature reuse. A negative relationship between weight similarity and performance gains across layers was observed, questioning the importance of early-layer feature reuse in transfer learning.

## Method Summary
The authors fine-tune ResNet50 models pre-trained on ImageNet and RadImageNet on seven medical classification tasks using Adam optimizer (lr=1e-5) with early stopping. They employ five-fold cross-validation and evaluate using AUC. To analyze learned representations, they use Canonical Correlation Analysis (CCA) to measure layer-wise similarity between models and calculate prediction similarity at the instance level. The study examines how representations evolve during fine-tuning and how this relates to final performance.

## Key Results
- ImageNet pre-training outperformed RadImageNet on most medical imaging tasks despite domain differences
- Models with different pre-training converged to distinct intermediate representations but produced similar predictions
- Weight similarity before and after fine-tuning was negatively related to performance gains across all layers

## Why This Works (Mechanism)

### Mechanism 1
ImageNet pre-training captures general visual features that transfer well across domains, while RadImageNet models learn domain-specific medical features that are less generalizable. This is counterintuitive as RadImageNet is designed for medical-specific tasks, yet ImageNet offers better transfer performance across diverse medical tasks.

### Mechanism 2
Transfer learning benefits primarily come from weight initialization quality rather than feature reuse in early layers. Pre-trained weights provide better starting points for optimization, leading to faster convergence and better final performance, even when intermediate representations diverge significantly during fine-tuning.

### Mechanism 3
Models pre-trained on different source domains converge to distinct intermediate representations but produce similar predictions. Despite learning different internal feature representations, models can still solve the same classification task effectively by learning different decision boundaries that map to similar output predictions.

## Foundational Learning

- **Concept: Canonical Correlation Analysis (CCA)**
  - Why needed here: CCA measures similarity between intermediate representations learned by different models, providing quantitative insight into feature reuse during transfer learning.
  - Quick check question: What does a high CCA similarity score between two layers indicate about their learned representations?

- **Concept: Transfer learning optimization dynamics**
  - Why needed here: Understanding how pre-trained weights affect optimization speed and final performance is crucial for interpreting why ImageNet sometimes outperforms RadImageNet despite domain differences.
  - Quick check question: How does weight initialization affect the optimization trajectory and convergence speed in deep neural networks?

- **Concept: Domain adaptation and domain shift**
  - Why needed here: The paper investigates transfer learning between natural and medical image domains, making understanding of domain adaptation principles essential for interpreting the results.
  - Quick check question: What factors determine the difficulty of transferring knowledge between different image domains?

## Architecture Onboarding

- **Component map:** Pre-trained ResNet50 (ImageNet or RadImageNet) → Average pooling → Dropout (0.5) → Dense classification layer → Output predictions

- **Critical path:**
  1. Load pre-trained weights (ImageNet or RadImageNet)
  2. Fine-tune on target medical dataset
  3. Extract intermediate representations for CCA analysis
  4. Calculate prediction similarity between models
  5. Analyze relationship between weight similarity and performance gains

- **Design tradeoffs:**
  - Freezing vs. unfreezing pre-trained weights affects both performance and representation analysis
  - Image size reduction (for memory constraints) may impact performance but enables broader dataset coverage
  - CCA sampling strategy (n, pL selection) affects similarity measurement reliability

- **Failure signatures:**
  - High CCA similarity but poor performance: Indicates feature reuse without effective adaptation
  - Low prediction similarity despite similar performance: Suggests different decision boundaries for the same task
  - Performance degradation when freezing weights: Indicates insufficient adaptation of pre-trained features

- **First 3 experiments:**
  1. Replicate the basic transfer learning setup with ImageNet vs RadImageNet pre-training on one medical dataset
  2. Perform layer-wise CCA analysis to quantify representation similarity across different layers
  3. Calculate prediction similarity to compare instance-level agreement between models with different pre-training sources

## Open Questions the Paper Calls Out

### Open Question 1
Does the size and diversity of the source dataset (ImageNet vs RadImageNet) directly correlate with the quality of learned representations for medical imaging tasks? While the paper shows ImageNet generally outperforms RadImageNet, it doesn't establish a direct correlation between source dataset characteristics and representation quality.

### Open Question 2
Are the distinct intermediate representations learned by ImageNet and RadImageNet models indicative of fundamentally different feature learning mechanisms? The paper observes distinct representations but doesn't investigate whether these differences imply fundamentally different learning mechanisms.

### Open Question 3
How does the negative relationship between weight similarity and performance gains across layers impact the design of transfer learning strategies for medical imaging? The discovery of this negative relationship challenges traditional feature reuse assumptions, but its implications for transfer learning strategy design remain unexplored.

## Limitations
- Results may not generalize to all medical imaging domains as only seven datasets were examined
- The study focuses on ResNet50 architecture, limiting generalizability to other network designs
- Optimization dynamics are complex and may vary with different architectures, affecting the weight initialization mechanism

## Confidence

- **High confidence**: ImageNet outperforming RadImageNet in most cases - well-supported by empirical results across multiple datasets
- **Medium confidence**: Weight initialization quality driving transfer benefits - compelling evidence but optimization dynamics are complex
- **Medium confidence**: Divergence of intermediate representations while maintaining similar predictions - supported by CCA and prediction similarity analyses

## Next Checks

1. **Architecture Generalization Test**: Replicate experiments using different backbone architectures (EfficientNet, Vision Transformers) to verify if observed patterns hold across various network designs.

2. **Domain Diversity Assessment**: Extend study to include medical imaging tasks from additional domains (pathology, endoscopy) to test robustness across broader medical applications.

3. **Optimization Dynamics Analysis**: Conduct controlled experiments varying learning rates and training durations to isolate optimization dynamics versus feature reuse contributions.