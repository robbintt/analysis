---
ver: rpa2
title: Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment
arxiv_id: '2312.09625'
source_url: https://arxiv.org/abs/2312.09625
tags:
- visual
- grounding
- text
- point
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 3D-VLA, a weakly supervised approach for
  3D visual grounding that does not require 3D bounding box annotations. The method
  leverages large-scale vision-language models (VLMs) and the natural correspondence
  between 2D images and 3D point clouds to implicitly align text queries with 3D objects.
---

# Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment

## Quick Facts
- arXiv ID: 2312.09625
- Source URL: https://arxiv.org/abs/2312.09625
- Authors: 
- Reference count: 14
- Key outcome: 3D-VLA achieves up to 11.34% accuracy improvement on ScanRefer dataset without 3D box annotations

## Executive Summary
This paper introduces 3D-VLA, a weakly supervised approach for 3D visual grounding that eliminates the need for expensive 3D bounding box annotations. The method leverages large-scale vision-language models (VLMs) and the natural correspondence between 2D images and 3D point clouds to implicitly align text queries with 3D objects. During training, 3D-VLA uses contrastive learning to align 3D embeddings with 2D and text embeddings from VLMs, and introduces task-aware classification to improve semantic alignment in indoor point cloud scenes. Experiments on ReferIt3D and ScanRefer datasets demonstrate that 3D-VLA achieves comparable and even superior results to fully supervised methods.

## Method Summary
3D-VLA uses frozen CLIP models as text and 2D encoders, with a PointNet++ encoder for 3D point clouds. The method projects 3D object proposals onto 2D images using geometric camera calibration, then uses contrastive learning to align 3D proposal embeddings with their corresponding 2D image embeddings. Task-aware classification adapters transform text, 2D, and 3D embeddings into a new space using category labels, and category-oriented proposal filtering eliminates proposals with mismatched categories. The training combines contrastive losses between original and adapted embeddings with classification losses.

## Key Results
- Achieves comparable and superior results to fully supervised methods on ReferIt3D and ScanRefer datasets
- 11.34% accuracy improvement on ScanRefer dataset at 0.25 IoU threshold
- Eliminates need for 3D bounding box annotations during training
- Maintains strong performance across both Easy/Hard and View-dep/View-indep splits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2D image embeddings from CLIP serve as a bridge to align 3D point cloud embeddings with text embeddings without requiring 3D box annotations
- Mechanism: Projects 3D object proposals to 2D image regions through geometric camera calibration, then uses contrastive learning to align 3D proposal embeddings with 2D image embeddings, indirectly aligning them with text embeddings
- Core assumption: The 2D-3D correspondence through camera calibration is accurate and reliable
- Evidence anchors: [abstract] "leverages the natural correspondence between 2D images and 3D point clouds"; [section] "project these proposals to 2D image regions through geometric camera calibration"
- Break condition: Inaccurate camera calibration parameters or loss of critical 3D geometric information during projection

### Mechanism 2
- Claim: Task-aware classification adapts embeddings to better support 3D visual grounding in indoor point cloud scenes
- Mechanism: Adapters transform text, 2D, and 3D embeddings into a new embedding space using classification tasks with category labels for supervision
- Core assumption: Category labels provide meaningful semantic information for the indoor point cloud scene
- Evidence anchors: [abstract] "introduces task-aware classification to improve semantic alignment in indoor point cloud scenes"; [section] "introduce multi-modal adaption through task-aware classification"
- Break condition: Category labels are too coarse or not representative of actual scene content

### Mechanism 3
- Claim: Category-oriented proposal filtering improves grounding accuracy by eliminating proposals with mismatched categories
- Mechanism: Predicts category distributions for both text queries and 3D proposals, filters out proposals whose top predicted categories don't match the query's top categories
- Core assumption: Category prediction from adapted embeddings is accurate enough to filter out irrelevant proposals
- Evidence anchors: [abstract] "we can also use the classification results of text and 3D objects to filter out some confusing and unreliable predictions"; [section] "category-oriented proposal filtering strategy"
- Break condition: Category predictions are frequently incorrect, removing relevant proposals

## Foundational Learning

- Concept: Contrastive learning for multimodal alignment
  - Why needed here: To align 3D point cloud embeddings with text embeddings through the 2D image bridge without explicit 3D box supervision
  - Quick check question: How does contrastive learning pull paired embeddings closer while pushing unpaired embeddings apart in this context?

- Concept: Geometric camera calibration for 3D-2D projection
  - Why needed here: To establish the correspondence between 3D point cloud proposals and 2D image regions
  - Quick check question: What parameters are needed for camera calibration and how are they obtained in this setting?

- Concept: Vision-language model (VLM) embeddings and alignment
  - Why needed here: CLIP provides pre-aligned text and 2D image embeddings that serve as the foundation for the 3D-text alignment
  - Quick check question: Why does using a frozen CLIP model help in this weakly-supervised approach?

## Architecture Onboarding

- Component map: Text encoder (CLIP text encoder) -> 2D encoder (CLIP image encoder) -> 3D encoder (PointNet++ + transformer) -> Adapters (three identical adapters for text, 2D, 3D) -> Classification heads (text classifier, 2D classifier, 3D classifier) -> Contrastive learning modules (Le for original embeddings, La for adapted embeddings)
- Critical path: 3D proposal → PointNet++ encoding → transformer → 3D embeddings → adapter → adapted 3D embeddings → classification and contrastive learning → alignment with text
- Design tradeoffs: Using frozen CLIP models reduces training complexity but limits adaptation to 3D point clouds; relying on 2D images as a bridge introduces dependency on camera calibration accuracy; task-aware classification adds supervision but requires category labels
- Failure signatures: Poor camera calibration leading to incorrect 3D-2D correspondences; CLIP's text-image alignment not transferring well to 3D space; category labels not matching the actual scene content; overfitting to specific categories due to limited label diversity
- First 3 experiments:
  1. Test 3D-2D projection accuracy: Project 3D proposals to 2D images and visually verify the correspondence quality
  2. Evaluate CLIP alignment transfer: Measure similarity between text and 2D image embeddings vs text and 3D proposal embeddings before contrastive learning
  3. Test adapter effectiveness: Compare classification accuracy with and without adapters on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of 3D-VLA scale with different sizes of pre-trained vision-language models (VLMs)?
- Basis in paper: [inferred] The paper mentions that 3D-VLA uses a frozen CLIP model as the text and 2D encoders, and that "other VLMs are also practicable and we choose CLIP in this paper."
- Why unresolved: The paper does not experiment with different VLMs or analyze the impact of VLM size on performance
- What evidence would resolve it: Experiments comparing 3D-VLA's performance using different VLMs (e.g., CLIP, ViLT, VLMO) or different sizes of the same VLM architecture

### Open Question 2
- Question: What is the impact of the number of 2D image frames used for projection on the performance of 3D-VLA?
- Basis in paper: [explicit] The paper states "For each 3D proposal candidate, we project its point clouds onto L sampled frames (Dai et al. 2017) in the original video through geometric camera calibration, and get the corresponded 2D image regions."
- Why unresolved: The paper does not report experiments varying L or analyzing its impact on performance
- What evidence would resolve it: Experiments showing 3D-VLA's performance with different values of L, and an analysis of the trade-off between performance and computational cost

### Open Question 3
- Question: How does the performance of 3D-VLA compare to fully supervised methods when using more than 3D proposal candidates?
- Basis in paper: [inferred] The paper mentions that "3D-VLA achieves comparable and even superior results over the fully supervised methods" when using 3D proposal candidates
- Why unresolved: The paper does not report experiments using more than 3D proposal candidates or compare performance to fully supervised methods under these conditions
- What evidence would resolve it: Experiments comparing 3D-VLA's performance to fully supervised methods using a varying number of proposal candidates (e.g., 10, 20, 50)

## Limitations

- Primary limitation is reliance on accurate 2D-3D correspondence through camera calibration, with limited validation of correspondence quality
- Effectiveness of task-aware classification depends heavily on semantic richness and representativeness of category labels
- Method assumes CLIP's text-image alignment naturally transfers to 3D space without explicit 3D supervision

## Confidence

- **High Confidence**: Overall methodology of using 2D images as a bridge for 3D-text alignment and effectiveness of category-oriented proposal filtering (supported by 11.34% accuracy improvement on ScanRefer dataset)
- **Medium Confidence**: Task-aware classification improving semantic alignment, as the paper provides limited empirical evidence for how adapter-based classification specifically enhances grounding performance
- **Low Confidence**: Assumption that CLIP's text-image alignment naturally transfers to 3D space without explicit 3D supervision - demonstrated through results but underlying mechanism not thoroughly analyzed

## Next Checks

1. Conduct ablation studies to quantify individual contributions of each component (2D-3D alignment, task-aware classification, proposal filtering) to overall performance
2. Test the method on datasets with varying camera calibration quality to assess robustness to 2D-3D correspondence errors
3. Evaluate grounding performance when using different numbers of category labels to determine minimum semantic supervision required for effective alignment