---
ver: rpa2
title: In-Context Learning Learns Label Relationships but Is Not Conventional Learning
arxiv_id: '2307.12375'
source_url: https://arxiv.org/abs/2307.12375
tags:
- label
- in-context
- labels
- default
- flipped
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models perform in-context
  learning (ICL), particularly focusing on whether they truly learn label relationships
  from in-context examples or merely memorize task formats. The authors address three
  key questions: (1) whether ICL predictions depend on in-context label distributions,
  (2) how ICL balances pre-training preferences with in-context label information,
  and (3) whether ICL treats all in-context information equally.'
---

# In-Context Learning Learns Label Relationships but Is Not Conventional Learning

## Quick Facts
- **arXiv ID**: 2307.12375
- **Source URL**: https://arxiv.org/abs/2307.12375
- **Reference count**: 40
- **Primary result**: ICL considers in-context label relationships but cannot fully overcome pre-training preferences, behaving more like a time-series model than traditional learning

## Executive Summary
This paper investigates the fundamental nature of in-context learning (ICL) in large language models, challenging the assumption that ICL represents conventional learning. Through systematic experiments with label manipulation across multiple model families, the authors demonstrate that while ICL does incorporate label relationships from in-context examples, it struggles to override pre-training preferences and prioritizes recent examples over earlier ones. The study reveals that ICL operates through probabilistic updating of beliefs rather than true learning, with persistent influence from pre-training data that cannot be fully eliminated through inference-time context. These findings have important implications for understanding the limitations and capabilities of prompt-based learning approaches.

## Method Summary
The authors systematically modified in-context label relationships through three experimental scenarios: randomizing labels, flipping default labels, and changing label relationships mid-context. They evaluated model predictions using accuracy, log-likelihood, and entropy metrics across LLaMa and Falcon model families on multiple datasets ranging from SST-2 to novel author identification tasks. A computationally efficient approach extracted predictions from single forward passes to evaluate all possible context sizes. The experiments tested null hypotheses about whether ICL depends on in-context labels, whether it prioritizes pre-training preferences over context, and whether it treats all information equally.

## Key Results
- ICL predictions almost always depend on in-context label distributions rather than just task format
- ICL struggles to fully overcome pre-training preferences even with many in-context examples
- ICL behaves like a time-series model, prioritizing recent examples over earlier ones rather than treating all information equally

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL predictions depend on in-context label distributions, not just format or task structure
- Mechanism: The model updates its probabilistic beliefs based on observed label frequencies in context
- Core assumption: ICL maintains and updates a probability distribution over labels during inference
- Evidence anchors:
  - [abstract] "ICL predictions almost always depend on in-context labels"
  - [section 5] "LLMs react with lower log-likelihood and increased predictive entropy to the random label scenario"
  - [corpus] Weak - no direct citations found in neighbor papers about probabilistic metrics
- Break condition: When performance is near random guessing, ICL may not distinguish between default and random labels

### Mechanism 2
- Claim: ICL behaves like a time-series model, prioritizing recent examples over earlier ones
- Mechanism: Token positions create an implicit temporal dimension where later examples have stronger influence
- Core assumption: The model's attention mechanism weights recent context more heavily than distant context
- Evidence anchors:
  - [abstract] "ICL behaves more like a time series model, prioritizing recent examples over earlier ones"
  - [section 9] "accuracies decrease steadily when label relations are flipped" and "ICL begins to switch to that new label relationship"
  - [corpus] Moderate - some neighbor papers discuss ordering effects but not time-series interpretation
- Break condition: When context is very small or examples are very similar, temporal effects may be negligible

### Mechanism 3
- Claim: Pre-training preferences create a persistent inductive bias that in-context examples cannot fully overcome
- Mechanism: Pre-trained weights encode label relationships that continue influencing predictions even with contradictory in-context examples
- Core assumption: Pre-training creates stable parameter configurations that resist modification through inference-time context
- Evidence anchors:
  - [abstract] "ICL struggles to fully overcome prediction preferences acquired from pre-training data"
  - [section 7] "label relationships inferred from pre-training have a permanent effect that cannot be overcome through in-context observations"
  - [corpus] Strong - multiple neighbor papers discuss pre-training influence on ICL behavior
- Break condition: With extremely large numbers of contradictory examples, pre-training preference may eventually be overwhelmed

## Foundational Learning

- Concept: Probabilistic prediction and uncertainty quantification
  - Why needed here: The paper uses entropy and log-likelihood metrics to study ICL behavior beyond simple accuracy
  - Quick check question: Can you explain the difference between aleatoric and epistemic uncertainty in the context of ICL?

- Concept: Time-series modeling and temporal attention
  - Why needed here: Understanding how ICL prioritizes recent examples requires knowledge of temporal dependencies in sequence models
  - Quick check question: How would you design an experiment to test whether ICL treats all examples equally regardless of position?

- Concept: Inductive bias and prior knowledge in learning systems
  - Why needed here: The interaction between pre-training preferences and in-context learning is fundamentally about how prior knowledge influences new learning
  - Quick check question: What would you expect to happen to ICL performance if you could somehow "reset" the pre-training preferences?

## Architecture Onboarding

- Component map: Input tokenizer → Context concatenation → Multi-head attention → Feed-forward layers → Output probability distribution
- Critical path: Tokenization → Context formation → Forward pass through transformer → Probability extraction for labels
- Design tradeoffs: Using single forward pass for efficiency vs. iterative generation for traditional ICL; half-precision vs. full precision for large models
- Failure signatures: Performance plateaus below random guessing baseline; accuracy remains high but log-likelihood and entropy show degraded confidence
- First 3 experiments:
  1. Replicate the randomized label experiment to verify ICL's dependence on label distributions
  2. Test pre-training vs. in-context label preference by flipping labels halfway through context
  3. Evaluate the time-series behavior by measuring accuracy changes as new examples arrive

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does in-context learning treat different sources of in-context information equally, or does it prioritize recent information like a time series model?
- Basis in paper: [explicit] The authors directly test null hypothesis 3 (NH3) which states "ICL considers all information given in-context equally" and find evidence rejecting this hypothesis
- Why unresolved: The paper only tests this with label relationship changes halfway through in-context examples. It's unclear if this time-based prioritization extends to other types of information sources or different task formats
- What evidence would resolve it: Experiments testing ICL behavior when different types of information (not just labels) are introduced at different positions in the context, or when multiple simultaneous information sources are presented

### Open Question 2
- Question: Can prompts be designed that fully overcome pre-training preferences when learning new label relationships?
- Basis in paper: [explicit] The authors test multiple prompts (instruct, ignore, invert) to help ICL learn flipped label relationships but find they only provide initial boosts that don't persist at larger context sizes
- Why unresolved: The paper only tests a limited set of prompts and doesn't explore more sophisticated prompting strategies or prompt tuning methods
- What evidence would resolve it: Demonstrating a prompt that allows ICL to learn flipped label relationships as effectively as default ones across all context sizes, or proving no such prompt exists for the tested models

### Open Question 3
- Question: How does the size and architecture of language models affect their ability to overcome pre-training preferences in in-context learning?
- Basis in paper: [inferred] The authors observe that larger models (LLaMa-65B vs LLaMa-7B) show different behaviors when learning non-default label relationships, but don't systematically study this relationship
- Why unresolved: The paper only tests three model sizes and doesn't explore intermediate sizes or different architectural choices (attention mechanisms, etc.)
- What evidence would resolve it: Systematic testing across a broader range of model sizes and architectures to determine if there's a threshold effect or gradual improvement in ICL's ability to override pre-training preferences

## Limitations
- The claim that pre-training preferences are "permanent" may overstate the case, as experiments only tested limited context sizes
- Potential confounding factors like tokenization differences between model families weren't fully addressed
- The study doesn't explore whether extremely large numbers of contradictory examples could eventually override pre-training preferences

## Confidence

- **Medium confidence**: ICL depends on in-context label distributions - supported by entropy and log-likelihood metrics, though interpretation as "learning" remains debatable
- **Medium confidence**: ICL behaves like a time-series model - ordering effects are well-documented but attribution to true prioritization rather than positional encoding isn't fully established
- **Low confidence**: Pre-training preferences are permanent and cannot be overcome - extrapolates from limited experimental data without testing extreme scenarios

## Next Checks

1. **Extended context testing**: Run experiments with dramatically increased numbers of in-context examples (10x or 100x current maximum) to test whether pre-training preferences can eventually be overwhelmed, directly validating the "permanent" claim.

2. **Positional encoding ablation**: Design experiments that systematically vary token positions while keeping temporal order constant (e.g., shuffling examples within fixed windows) to distinguish between true temporal prioritization and positional encoding effects.

3. **Cross-task generalization test**: Evaluate whether ICL's time-series behavior and pre-training preference effects generalize across fundamentally different task types (e.g., numerical reasoning vs. sentiment analysis) to determine if these are universal properties or task-specific phenomena.