---
ver: rpa2
title: Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight
  Matrices
arxiv_id: '2311.00452'
source_url: https://arxiv.org/abs/2311.00452
tags:
- network
- hessian
- layer
- eigenvalues
- eigenvectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the dynamics of trained deep neural networks
  by analyzing their weight matrices and Hessian matrices. It identifies the "drift
  mode" as the primary direction of continued training in neural networks, which can
  be explained by a quadratic potential model of the loss function.
---

# Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight Matrices

## Quick Facts
- arXiv ID: 2311.00452
- Source URL: https://arxiv.org/abs/2311.00452
- Reference count: 0
- This study analyzes trained deep neural networks through PCA, SVD, and Hessian eigenvalue analysis to understand dynamics and develop regularization methods for catastrophic forgetting.

## Executive Summary
This study investigates the dynamics of trained deep neural networks by analyzing their weight matrices and Hessian matrices. The research identifies the "drift mode" as the primary direction of continued training in neural networks and demonstrates that principal component analysis can effectively approximate the Hessian matrix. A novel regularization approach using singular values is proposed to address catastrophic forgetting, showing comparable performance to traditional Hessian-based methods while being computationally more efficient.

## Method Summary
The study employs a multi-faceted approach to analyze trained neural networks. Networks are trained on CIFAR-10 and MNIST datasets using SGD with momentum. Weight matrices are then analyzed through principal component analysis (PCA) and singular value decomposition (SVD). The Hessian matrix is computed to examine the loss landscape curvature, and eigenvectors are analyzed for their correlation with network weights. A novel singular value regularization method is proposed and compared against traditional Hessian eigenvalue regularization for catastrophic forgetting mitigation.

## Key Results
- The drift mode captures the primary long-term training trajectory in deep neural networks
- Hessian eigenvectors and singular value decomposition are strongly correlated in trained networks
- PCA effectively approximates Hessian eigenvectors, with velocity-based PCA superior to weight-based PCA
- The proposed singular value regularization method performs comparably to Hessian-based approaches for catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The drift mode captures the primary long-term training trajectory in deep neural networks.
- Mechanism: Networks evolve predominantly along a single principal component direction (the drift mode), which can be modeled as a linear function of time with variance proportional to η²T², where η is the learning rate and T is the measurement period.
- Core assumption: The network's weight updates remain correlated over time, and the exploration phase exhibits linear drift behavior.
- Evidence anchors:
  - [abstract] "Trained networks predominantly continue training in a single direction, known as the drift mode."
  - [section 6.3] "In the direction of the first principal component, a clear linear behavior emerges" and "the dynamics of unregularized networks reveal that |θ₁(t)| experiences growth over time."
  - [corpus] Weak evidence - no direct corpus support for drift mode as primary trajectory.
- Break condition: If momentum or other SGD variants cause significant deviation from linear drift, or if the network enters a different training regime.

### Mechanism 2
- Claim: Hessian eigenvectors and singular value decomposition (SVD) are strongly correlated in trained networks.
- Mechanism: The largest singular values (outside the RMT bulk) align with the smallest-indexed Hessian eigenvectors, capturing the most important directions for network performance.
- Core assumption: The weight matrix structure after training reflects the loss landscape curvature captured by the Hessian.
- Evidence anchors:
  - [abstract] "We unveil a correlation between Hessian eigenvectors and network weights."
  - [section 7.3] "singular values lying outside the RMT bulk are predominantly covered by Hessian eigenvectors corresponding to the smallest indices."
  - [corpus] Moderate evidence - corpus mentions "Nonlinear spiked covariance matrices and signal propagation in deep neural networks" which relates to covariance structure.
- Break condition: If network initialization or training dynamics produce weight matrices that don't reflect the underlying loss landscape structure.

### Mechanism 3
- Claim: Principal component analysis (PCA) can effectively approximate Hessian eigenvectors for practical applications.
- Mechanism: The covariance matrix of weight updates (velocities) diagonalizes more sharply in the Hessian eigenbasis than the covariance of weights, making velocity-based PCA a superior approximation method.
- Core assumption: The velocity eigenbasis captures the training dynamics more accurately than the weight eigenbasis.
- Evidence anchors:
  - [abstract] "our examination showcases the applicability of principal component analysis in approximating the Hessian, with update parameters emerging as a superior choice over weights for this purpose."
  - [section 7.1] "velocity covariance matrix exhibits a sharper diagonal in the Hessian eigenbasis than for the eigenvectors of the weight covariance matrix."
  - [corpus] Weak evidence - no direct corpus support for velocity-based PCA superiority.
- Break condition: If training dynamics change significantly or if momentum effects dominate the velocity structure.

## Foundational Learning

- Concept: Random Matrix Theory (RMT) and Marchenko-Pastur distribution
  - Why needed here: Understanding the statistical properties of weight matrices and distinguishing signal from noise in singular values
  - Quick check question: What distribution describes the bulk of singular values in random matrices, and how do outliers relate to network information?

- Concept: Hessian matrix and second-order optimization
  - Why needed here: The Hessian captures the curvature of the loss landscape, determining the importance of different weight directions
  - Quick check question: How does the eigenvalue magnitude of the Hessian relate to the flatness of the loss minima along corresponding eigenvector directions?

- Concept: Principal Component Analysis and covariance matrices
  - Why needed here: PCA provides a computationally efficient way to approximate important directions in the weight space
  - Quick check question: What is the relationship between the variance of principal components and their corresponding eigenvalues?

## Architecture Onboarding

- Component map: Network Training -> Weight Matrix Extraction -> PCA and SVD Analysis -> Hessian Computation -> Correlation Analysis -> Regularization Application
- Critical path: Train network → Extract weight matrices → Compute PCA and SVD → Calculate Hessian matrix → Analyze correlations between methods → Apply insights to regularization or forgetting mitigation
- Design tradeoffs: PCA offers computational efficiency but may miss some Hessian structure; SVD provides insight into weight matrix properties but requires careful interpretation; Hessian analysis is most accurate but computationally expensive
- Failure signatures: If singular values don't separate from RMT bulk, if PCA doesn't align with Hessian eigenvectors, or if regularization doesn't improve forgetting performance
- First 3 experiments:
  1. Train a small MLP on MNIST and verify that the first principal component captures most variance in weights
  2. Compute SVD of weight matrices and check for singular values outside the RMT bulk
  3. Calculate the Hessian for a single layer and verify that the largest eigenvalues align with SVD results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of data augmentation during training impact the Hessian eigenspectrum and its relationship with network weights?
- Basis in paper: [explicit] The paper acknowledges that incorporating data augmentation into Hessian matrix evaluation poses challenges due to the vast number of possible realizations of training samples.
- Why unresolved: The paper does not explore the impact of data augmentation on the Hessian eigenspectrum, leaving the question of how this common training technique affects the network's critical directions and learning dynamics unanswered.
- What evidence would resolve it: Analyzing the Hessian eigenspectrum of networks trained with and without data augmentation, comparing the distribution of eigenvalues and their alignment with weight vectors, would provide insights into how data augmentation influences the network's learning dynamics.

### Open Question 2
- Question: How do different optimization algorithms, such as Adam and Lion, compare to SGD in terms of their impact on the Hessian eigenspectrum and network performance?
- Basis in paper: [inferred] The paper focuses on SGD and acknowledges that the dynamics of update steps for other SGD variants like Adam and Lion remain unexplored.
- Why unresolved: The paper does not investigate the impact of alternative optimization algorithms on the Hessian eigenspectrum and network performance, leaving the question of how these methods influence the network's learning dynamics and critical directions unanswered.
- What evidence would resolve it: Comparing the Hessian eigenspectrum and network performance of networks trained with different optimization algorithms, such as SGD, Adam, and Lion, would provide insights into how these methods influence the network's learning dynamics and critical directions.

### Open Question 3
- Question: How does the proposed singular value regularization method for catastrophic forgetting compare to other regularization techniques in terms of effectiveness and computational efficiency?
- Basis in paper: [explicit] The paper proposes a singular value regularization method for catastrophic forgetting and compares its performance to the Hessian eigenvalue method.
- Why unresolved: The paper does not compare the proposed singular value regularization method to other regularization techniques, such as elastic weight consolidation or learning without forgetting, leaving the question of its relative effectiveness and computational efficiency unanswered.
- What evidence would resolve it: Evaluating the proposed singular value regularization method against other regularization techniques in terms of catastrophic forgetting performance and computational efficiency would provide insights into its relative strengths and weaknesses.

## Limitations

- The study relies on specific network architectures and datasets (CIFAR-10, MNIST), limiting generalizability
- Hessian analysis remains computationally prohibitive for very large networks
- The analysis assumes stable training dynamics over time, which may not hold for complex training schedules

## Confidence

- High confidence: The correlation between PCA and Hessian eigenvectors for capturing network dynamics
- Medium confidence: The superiority of velocity-based PCA over weight-based PCA
- Medium confidence: The effectiveness of singular value regularization for catastrophic forgetting

## Next Checks

1. Cross-architecture validation: Apply the PCA-Hessian correlation analysis to transformer-based architectures to verify if the findings extend beyond convolutional and fully-connected networks
2. Training regime robustness: Test the drift mode behavior under different learning rate schedules, optimizers (Adam, RMSprop), and batch sizes to determine the stability of the observed linear drift pattern
3. Catastrophic forgetting benchmarks: Compare the proposed singular value regularization against more recent continual learning methods on established benchmarks like Split CIFAR and CORe50 to establish relative effectiveness