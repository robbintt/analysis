---
ver: rpa2
title: Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial
  Attack
arxiv_id: '2306.16050'
source_url: https://arxiv.org/abs/2306.16050
tags:
- adversarial
- denoising
- image
- noise
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of deep image denoising
  models against adversarial attacks. The authors propose denoising-PGD, an adversarial
  attack method that can successfully fool various deep denoising models while keeping
  the noise distribution almost unchanged.
---

# Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack

## Quick Facts
- arXiv ID: 2306.16050
- Source URL: https://arxiv.org/abs/2306.16050
- Reference count: 40
- Key outcome: Deep image denoising models share similar adversarial sample spaces across non-blind, blind, plug-and-play, and unfolding architectures

## Executive Summary
This paper investigates the robustness of deep image denoising models against adversarial attacks, revealing surprising similarities across different denoising architectures. The authors propose denoising-PGD, an adversarial attack method that successfully fools various denoising models while maintaining Gaussian noise distribution. They discover that current mainstream denoising models share nearly identical adversarial sample sets, indicating high architectural similitude. To quantify this similarity, they introduce a robustness similitude metric. The study also demonstrates that adversarial training can improve model robustness without degrading performance on clean images, while classical model-driven methods like BM3D show inherent resistance to such attacks.

## Method Summary
The authors develop denoising-PGD, a PGD-based adversarial attack method that generates adversarial samples for image denoising models by adding perturbations to Gaussian noise images rather than clean images, preserving the original noise distribution. They test transferability across non-blind models (DnCNN, FFDNet, ECNDNet, BRDNet), blind models (DnCNN-B, Noise2Noise, RDDCNN-B, FAN), plug-and-play models (DPIR, CurvPnP), and unfolding models (DeamNet). The robustness similitude metric measures overlap in adversarial sample regions between models. Adversarial training is implemented by mixing adversarial and Gaussian noise images in 1:1 ratio during training to improve robustness.

## Key Results
- Current mainstream non-blind, blind, plug-and-play, and unfolding denoising models almost share the same adversarial sample set
- Non-blind denoising models show high robustness similitude across each other
- Adversarial training significantly improves robustness without degrading performance on clean images
- Classical model-driven denoising method BM3D shows some resistance to adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep image denoising models share similar adversarial sample spaces due to high similitude in their local decision boundaries
- Mechanism: When adversarial samples generated for one denoising model successfully transfer to other models, it indicates that the models have similar local behaviors near test samples. This similitude is measured by the "robustness similitude" metric proposed in the paper.
- Core assumption: Transferability of adversarial samples across models is a reliable indicator of model similitude
- Evidence anchors:
  - [abstract] "the current mainstream non-blind denoising models... almost share the same adversarial sample set"
  - [section III.C] "we find that for all the images in Set12, the regions of adversarial samples are almost the same"
  - [corpus] Weak evidence - the corpus papers focus on adversarial attacks for classification tasks rather than denoising tasks
- Break condition: If adversarial samples generated for one model do not transfer well to other models, or if the adversarial regions differ significantly across models

### Mechanism 2
- Claim: Denoising-PGD generates adversarial samples that maintain Gaussian noise distribution while effectively fooling denoising models
- Mechanism: By adding adversarial perturbations to Gaussian noise images rather than clean images, the generated adversarial samples maintain the original noise distribution. This makes the adversarial attack more covert and effective.
- Core assumption: Adversarial perturbations applied to noise images are more effective than those applied to clean images
- Evidence anchors:
  - [abstract] "denoising-PGD which can successfully attack all the current deep denoising models while keep the noise distribution almost unchanged"
  - [section III.A] "the adversarial noise is visually random and independent without obvious structural information"
  - [section V.A] "generating adversarial attack on the clean image shows little adversarial effect with tiny PSNR drop in the denoising result"
- Break condition: If adversarial perturbations applied to clean images prove more effective, or if maintaining the noise distribution is not necessary for successful attacks

### Mechanism 3
- Claim: Adversarial training can significantly improve the robustness of deep image denoising models without degrading performance on clean images
- Mechanism: By including adversarial samples in the training data, the model learns to handle both clean and adversarial examples effectively. This improves robustness against adversarial attacks.
- Core assumption: Adversarial training improves model robustness without significant performance degradation
- Evidence anchors:
  - [abstract] "We use adversarial training to complement the vulnerability to adversarial attacks"
  - [section IV.D] "the adversarial trained denoising model doesn't show any performance decreasing on original Gaussian noise images"
  - [corpus] Weak evidence - the corpus papers focus on adversarial attacks for classification tasks rather than denoising tasks
- Break condition: If adversarial training significantly degrades performance on clean images, or if it fails to improve robustness against adversarial attacks

## Foundational Learning

- Concept: Gaussian noise distribution and its properties
  - Why needed here: The paper's adversarial attack method maintains Gaussian noise distribution while adding adversarial perturbations
  - Quick check question: What are the key properties of Gaussian noise that make it suitable for this adversarial attack method?

- Concept: Transferability of adversarial samples
  - Why needed here: The paper's main finding is that adversarial samples generated for one denoising model transfer to other models, indicating high similitude
  - Quick check question: What factors contribute to the transferability of adversarial samples across different models?

- Concept: Robustness similitude metric
  - Why needed here: The paper proposes a new metric to measure the similarity between denoising models based on their robustness to adversarial attacks
  - Quick check question: How does the robustness similitude metric differ from traditional similarity metrics used in machine learning?

## Architecture Onboarding

- Component map:
  - Denoising-PGD attack method -> L2-denoising-PGD constrained attack method -> Adversarial training component -> Robustness similitude calculation module -> Evaluation metrics (PSNR, SSIM, MAE)

- Critical path:
  1. Generate adversarial samples using denoising-PGD
  2. Test transferability of adversarial samples across different denoising models
  3. Calculate robustness similitude between models
  4. Apply adversarial training to improve model robustness
  5. Evaluate performance on both clean and adversarial samples

- Design tradeoffs:
  - Tradeoff between attack effectiveness and maintaining noise distribution
  - Tradeoff between model complexity and robustness to adversarial attacks
  - Tradeoff between transferability of adversarial samples and model specificity

- Failure signatures:
  - Low transferability of adversarial samples across models
  - Degradation in performance on clean images after adversarial training
  - Inability to maintain Gaussian noise distribution while adding adversarial perturbations

- First 3 experiments:
  1. Generate adversarial samples for DnCNN using denoising-PGD and test their transferability to other denoising models
  2. Apply adversarial training to DnCNN and evaluate its performance on both clean and adversarial samples
  3. Calculate robustness similitude between DnCNN and other denoising models using their adversarial sample spaces

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do deep image denoising models exhibit adversarial vulnerabilities in the frequency domain, and how does this compare to spatial domain attacks?
- Basis in paper: [explicit] The paper introduces FAN, a blind denoising model that combines frequency domain analysis with attention mechanism, suggesting potential differences in adversarial vulnerabilities between frequency and spatial domains.
- Why unresolved: The paper does not explicitly investigate adversarial attacks in the frequency domain or compare the robustness of models to attacks in different domains.
- What evidence would resolve it: Conducting adversarial attacks on denoising models in both frequency and spatial domains and comparing the effectiveness and transferability of attacks in each domain.

### Open Question 2
- Question: How do the architectural differences between non-blind, blind, plug-and-play, and unfolding denoising models contribute to their varying levels of adversarial robustness?
- Basis in paper: [explicit] The paper finds that non-blind models are generally more robust than blind models, and that hybrid-driven models (plug-and-play and unfolding) have different adversarial vulnerabilities compared to pure data-driven models.
- Why unresolved: While the paper identifies differences in robustness among model types, it does not delve into the specific architectural features that contribute to these differences.
- What evidence would resolve it: Analyzing the architectural components of different denoising models and correlating these features with their adversarial robustness through ablation studies or controlled experiments.

### Open Question 3
- Question: Can adversarial training be optimized to improve the denoising performance of deep image denoising models on clean images, beyond just improving robustness to adversarial attacks?
- Basis in paper: [explicit] The paper observes that adversarial training not only improves robustness but also enhances the denoising effect on Gaussian noise images, reducing artifacts.
- Why unresolved: The paper notes this improvement but does not explore methods to further optimize adversarial training for better performance on clean images.
- What evidence would resolve it: Experimenting with different adversarial training strategies, such as varying the ratio of adversarial to clean samples or incorporating additional regularization techniques, and measuring their impact on denoising performance.

## Limitations

- The analysis focuses primarily on Gaussian noise removal, potentially limiting generalizability to other noise types or real-world conditions
- The claim that all current mainstream denoising models share the same adversarial sample set generalizes from tested models to "all mainstream" models without comprehensive validation
- The study doesn't establish whether similitude reflects architectural similarity or merely shared training data characteristics

## Confidence

- **High Confidence**: The denoising-PGD method effectively generates transferable adversarial samples that maintain noise distribution (validated through PSNR/SSIM metrics)
- **Medium Confidence**: The robustness similitude metric meaningfully captures model similarity (metric is novel but intuitively reasonable)
- **Low Confidence**: The claim that all current mainstream denoising models share the same adversarial sample set (generalization from tested models to "all mainstream" models)

## Next Checks

1. Test adversarial sample transferability across different noise distributions (non-Gaussian, real-world noise) to verify generalizability
2. Conduct ablation studies on robustness similitude by varying model architectures while controlling for training data to isolate architectural factors
3. Evaluate adversarial training's effectiveness on previously untested denoising model families (e.g., transformer-based models)