---
ver: rpa2
title: How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for
  Image-Controlled Autonomy
arxiv_id: '2308.12252'
source_url: https://arxiv.org/abs/2308.12252
tags:
- safety
- predictors
- prediction
- learning
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of safety prediction for image-controlled
  autonomous systems without access to low-dimensional states. The authors propose
  a family of learning pipelines based on generative world models that offer modularity
  (monolithic vs.
---

# How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy

## Quick Facts
- arXiv ID: 2308.12252
- Source URL: https://arxiv.org/abs/2308.12252
- Reference count: 24
- Key outcome: Calibrated safety chance predictors outperform safety label predictors over longer horizons using conformal prediction with statistical guarantees.

## Executive Summary
This paper addresses the challenge of predicting safety chances for image-controlled autonomous systems without access to low-dimensional states. The authors propose a family of learning pipelines based on generative world models that offer modularity (monolithic vs. composite) and controller-specificity (controller-specific vs. controller-independent). They use robust vision features, data augmentation, and safety-informed latent representations to handle distribution shift and missing safety labels. The conformal prediction framework provides statistical calibration guarantees, and experiments on two OpenAI Gym benchmarks show that calibrated safety chance predictions outperform safety label predictions over longer horizons, with the best-performing predictors using latent representations.

## Method Summary
The method involves a family of learning pipelines with two key dimensions: modularity (monolithic vs. composite) and controller-specificity (controller-specific vs. controller-independent). Composite predictors use world models consisting of a VAE encoder, an LSTM-based forecaster, and an evaluator (CNN or latent space classifier). Conformal calibration is applied with post-hoc techniques like temperature scaling, isotonic regression, ENIR, and BBQ to provide statistical guarantees on safety chance predictions. The predictors are trained on observation sequences with safety labels or future images, and evaluated using F1 score, FPR, ECE, and MCE metrics.

## Key Results
- Calibrated safety chance predictors outperform safety label predictors over longer horizons
- Latent space predictors with safety-informed representations achieve the best performance
- Conformal calibration ensures reliable safety chance predictions with statistical bounds
- Controller-independent predictors show promising generalizability across different environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Conformal prediction provides distribution-free statistical calibration bounds on safety chance predictions.
- **Mechanism**: Conformal prediction ranks non-conformity scores across resampled bins to compute per-bin error upper bounds, ensuring that the predicted safety chance interval contains the true probability in at least 1-α cases.
- **Core assumption**: The resampled bins adequately represent the validation distribution and the safety chance errors are exchangeable within each bin.
- **Evidence anchors**:
  - [abstract] "statistical calibration guarantees on their safety chance predictions based on conformal prediction"
  - [section] "we apply conformal prediction (Lei et al. 2018) in Alg. 1... This algorithm guarantees Eq. 4"
  - [corpus] Weak evidence - no direct mention of conformal prediction guarantees in neighbors
- **Break condition**: If the underlying distribution shifts between validation and test data, or if the resampled bins are not representative, the coverage guarantee may fail.

### Mechanism 2
- **Claim**: Safety-informed latent representations improve prediction accuracy by preserving safety-relevant information during compression.
- **Mechanism**: The VAE is trained with a safety loss that encourages the latent space to encode safety information, so the latent forecaster can learn dynamics more efficiently than from raw images.
- **Core assumption**: Safety information can be preserved in a low-dimensional latent space and is useful for forecasting future safety.
- **Evidence anchors**:
  - [abstract] "investigate using safety labels to regularize latent representations learned by the world models"
  - [section] "To preserve the information about safety in latent state representations, we add an optional safety loss Lsaf ety"
  - [corpus] Weak evidence - no direct mention of safety-informed latent representations in neighbors
- **Break condition**: If the safety information is too complex to compress into the latent space, or if the safety loss term is poorly weighted, the latent representation may lose critical safety information.

### Mechanism 3
- **Claim**: Modular composite predictors can isolate and specialize components (forecasting vs evaluation) for better long-horizon performance.
- **Mechanism**: Composite predictors separate future observation forecasting from safety evaluation, allowing each component to be optimized independently and potentially improving robustness to distribution shift.
- **Core assumption**: Forecasting and evaluation are separable tasks that can be learned independently, and the distribution shift between forecasted and real images can be mitigated.
- **Evidence anchors**:
  - [abstract] "a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states"
  - [section] "composite predictors... consists of multiple learning models... A forecaster model constructs the likely future observations... Another binary classifier — an evaluator — judges whether a forecasted observation is safe"
  - [corpus] Weak evidence - no direct mention of modular composite predictors in neighbors
- **Break condition**: If the forecaster produces images that are too distribution-shifted from real images, the evaluator may not generalize, leading to degraded performance.

## Foundational Learning

- **Concept**: Variational Autoencoders (VAEs) for latent space learning
  - **Why needed here**: VAEs compress high-dimensional image observations into a lower-dimensional latent space that can be used for efficient forecasting and safety evaluation.
  - **Quick check question**: What is the role of the reconstruction loss and the KL divergence loss in VAE training?

- **Concept**: Conformal prediction for uncertainty quantification
  - **Why needed here**: Conformal prediction provides statistical guarantees on the accuracy of safety chance predictions without making distributional assumptions.
  - **Quick check question**: How does conformal prediction compute the confidence bounds on safety chance predictions?

- **Concept**: Convolutional LSTM for sequence modeling
  - **Why needed here**: Convolutional LSTM networks can model spatial-temporal dependencies in image sequences, enabling forecasting of future observations.
  - **Quick check question**: What is the advantage of using convolutional LSTM over regular LSTM for image-based forecasting?

## Architecture Onboarding

- **Component map**: Input observation sequence -> VAE encoder -> latent forecaster -> VAE decoder -> evaluator -> calibrated softmax -> conformal bounds
- **Critical path**: For safety chance prediction: observation sequence → VAE encoder → latent forecaster → VAE decoder → evaluator → calibrated softmax → conformal bounds
- **Design tradeoffs**:
  - Monolithic vs composite: Simplicity vs specialized components
  - Image-based vs latent predictors: Direct vs compressed representation
  - Controller-specific vs controller-independent: Performance vs generalizability
  - Calibration method: Post-hoc vs intrinsic calibration
- **Failure signatures**:
  - Poor calibration error: Calibration model not well-tuned or data not representative
  - High FPR on long horizons: Forecaster distribution shift or evaluator overfitting
  - Latent space loses safety info: Safety loss term too weak or latent space too small
- **First 3 experiments**:
  1. Train VAE with and without safety loss to measure impact on latent representation quality
  2. Compare monolithic vs composite predictors on short vs long horizons
  3. Test different calibrators (isotonic regression, ENIR, BBQ) on validation set to find best performer

## Open Questions the Paper Calls Out

- **Open Question 1**: How do latent state vectors fail to preserve safety information in latent space evaluators?
  - **Basis in paper**: [explicit] The paper states that latent space evaluators have shown particularly poor performance, indicating that the latent state vectors failed to preserve safety information.
  - **Why unresolved**: The paper does not provide a detailed analysis of why the latent state vectors fail to preserve safety information, leaving the underlying reasons unclear.
  - **What evidence would resolve it**: Experimental results comparing the performance of latent space evaluators with and without additional safety regularization, or ablation studies on the latent representation architecture, would provide insights into the cause of this failure.

- **Open Question 2**: What is the impact of adding physical constraints to latent states using Neural ODEs on the performance of safety predictors?
  - **Basis in paper**: [inferred] The paper mentions that future work includes adding physical constraints to latent states similar to Neural ODEs, suggesting that this is an unexplored area with potential impact on performance.
  - **Why unresolved**: This is a future work item that has not been investigated yet, and the paper does not provide any evidence or hypotheses about the potential impact.
  - **What evidence would resolve it**: Experimental results comparing the performance of safety predictors using latent states with and without physical constraints enforced by Neural ODEs would demonstrate the impact of this approach.

- **Open Question 3**: How effective is jointly learning forecasters and evaluators in overcoming distribution shift compared to using separate components?
  - **Basis in paper**: [inferred] The paper suggests jointly learning forecasters and evaluators as a potential future work item to overcome distribution shift, implying that this approach is not yet evaluated.
  - **Why unresolved**: The paper does not provide any experimental results or comparisons between jointly learned models and separate components in terms of handling distribution shift.
  - **What evidence would resolve it**: Experimental results comparing the performance of safety predictors using jointly learned forecasters and evaluators versus separate components, particularly in scenarios with significant distribution shift, would demonstrate the effectiveness of this approach.

## Limitations
- Composite predictors' reliance on image forecasting introduces potential failure modes when forecasted images diverge from real observations
- Controller-independent predictors may underperform controller-specific models in specialized scenarios
- Validation of conformal calibration bounds relies on assumptions about data exchangeability and bin representativeness that may not hold under significant distribution shift

## Confidence
- **High confidence** in conformal calibration methodology and its theoretical guarantees
- **Medium confidence** in composite predictor architecture benefits, given limited ablation studies
- **Medium confidence** in controller-independent predictor performance across different environments
- **Low confidence** in the specific choice of hyperparameters (latent size, safety loss weighting) due to lack of sensitivity analysis

## Next Checks
1. Conduct ablation studies comparing conformal calibration performance with and without safety-informed latent representations
2. Test controller-independent predictors on a third, previously unseen environment to validate generalizability claims
3. Analyze the impact of different adaptive binning strategies on conformal coverage guarantees across varying horizon lengths