---
ver: rpa2
title: Rollout Heuristics for Online Stochastic Contingent Planning
arxiv_id: '2310.02345'
source_url: https://arxiv.org/abs/2310.02345
tags:
- planning
- state
- action
- rollout
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using domain-independent heuristics for rollout
  policies in POMCP, an online POMDP solver. POMCP uses Monte-Carlo tree search and
  relies on rollout policies to estimate values of leaf nodes.
---

# Rollout Heuristics for Online Stochastic Contingent Planning

## Quick Facts
- arXiv ID: 2310.02345
- Source URL: https://arxiv.org/abs/2310.02345
- Reference count: 35
- Key outcome: The proposed h_add heuristic in belief space significantly improves POMCP performance, especially in domains requiring long planning horizons and information-gathering.

## Executive Summary
This paper introduces domain-independent heuristics for rollout policies in POMCP, an online POMDP solver. By leveraging the h_add heuristic from classical planning and extending it to belief space, the authors demonstrate significant improvements in solution quality for stochastic contingent planning domains. The method maintains a set of possible states and prunes inconsistent ones during sensing actions, capturing the value of information. Experimental results on benchmark domains show that the proposed heuristics outperform random rollouts, particularly in domains requiring long planning horizons and information-gathering.

## Method Summary
The paper proposes using the h_add heuristic from classical planning for POMCP rollout policies. The method extends h_add to belief space by maintaining a set of possible states and pruning inconsistent ones during sensing actions. The authors implement POMCP with three rollout policies: random, single-state h_add, and multi-state h_add. They evaluate these policies on stochastic versions of benchmark contingent planning domains, recording success rate, average cost to goal, and computation time per step.

## Key Results
- The h_add heuristic in belief space outperforms random rollouts in all tested domains.
- Multi-state h_add is particularly effective in domains requiring information-gathering, such as Wumpus.
- The proposed heuristics significantly improve performance in domains requiring long planning horizons.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-independent heuristics can guide rollout policies in POMCP to reduce the expected number of steps to goal.
- Mechanism: The method replaces uniform random action selection during rollout with informed choices based on delete-relaxation heuristics (h_add), which estimate minimal steps to reach the goal in a relaxed planning space.
- Core assumption: The delete-relaxation heuristic in belief space approximates the true minimal cost in stochastic contingent planning.
- Evidence anchors: [abstract] "we suggest two heuristics, the first is based on the well-known h_add heuristic from classical planning"
- Break condition: If preconditions of actions are not properly regressed through history, the heuristic may recommend inapplicable actions, leading to failed rollouts.

### Mechanism 2
- Claim: Multi-state belief-space heuristics capture value of information by maintaining a set of possible states and pruning inconsistent ones during sensing actions.
- Mechanism: Instead of a single state, the rollout maintains a belief set B. After each sensing action, states incompatible with the observation are removed from B, refining the belief.
- Core assumption: Deterministic observations can prune inconsistent states from B, and this pruning is computationally feasible.
- Evidence anchors: [abstract] "the second is computed in belief space, taking the value of information into account"
- Break condition: If belief set B grows too large, state-space explosion may make rollback or belief update intractable.

### Mechanism 3
- Claim: Using classical planning heuristics within POMCP exploits structured problem representation to scale to larger domains.
- Mechanism: By representing POMDPs as stochastic contingent planning problems, the method leverages factored state representations and preconditions/effects.
- Core assumption: Factored representation is available and preconditions are specified.
- Evidence anchors: [abstract] "we model POMDPs as stochastic contingent planning problems. This allows us to leverage domain-independent heuristics"
- Break condition: If domain is specified only in flat state space without factored structure, this method cannot be applied.

## Foundational Learning

- Concept: POMDP and Monte Carlo Tree Search
  - Why needed here: POMCP is the core algorithm being enhanced; understanding its search tree, belief representation, and rollout phase is essential.
  - Quick check question: What is the role of the rollout policy in POMCP, and how does it affect leaf node value estimates?

- Concept: Delete relaxation heuristics (h_add, h_max)
  - Why needed here: The proposed heuristics are based on delete relaxation; engineers must know how to construct action/fact layers and compute heuristic estimates.
  - Quick check question: In a delete-relaxation graph, how do you determine when to stop expanding layers?

- Concept: Belief-space regression and SAT queries
- Why needed here: The method maintains beliefs via regression through action-observation history instead of explicit belief updates; SAT queries check goal or precondition satisfaction.
  - Quick check question: How does regression of a formula through a sensing action and observation work?

## Architecture Onboarding

- Component map:
  - POMCP core: tree search, UCT selection, particle filter
  - Heuristic module: single-state and multi-state h_add computation
  - Regression engine: maintains initial belief and history, supports SAT queries
  - Stochastic contingent planner interface: encodes domain with preconditions, effects, observations

- Critical path:
  1. POMCP starts search at current belief node.
  2. Simulate samples state from initial belief via regression.
  3. At leaf, rollout policy selects actions using heuristic.
  4. Heuristic computes action/fact layers (single or multi-state).
  5. Sensing actions prune belief set if multi-state.
  6. Value estimate returned to POMCP update.

- Design tradeoffs:
  - Single-state vs. multi-state heuristics: speed vs. information value.
  - Depth limit vs. timeout: computational cost vs. solution quality.
  - Factored vs. flat representation: expressiveness vs. implementation complexity.

- Failure signatures:
  - Heuristic suggests inapplicable actions: regression fails to detect precondition satisfaction.
  - Belief set B grows too large: memory/time blowup in multi-state mode.
  - Rollout never reaches goal: heuristic misled by delete relaxation relaxation.

- First 3 experiments:
  1. Run POMCP with random rollout on a small contingent planning domain (e.g., doors) and record average steps to goal.
  2. Replace random rollout with single-state h_add and compare steps and runtime.
  3. Replace with multi-state h_add and measure belief pruning effectiveness and impact on steps.

## Open Questions the Paper Calls Out
- Question: How do the proposed domain-independent heuristics compare to other domain-specific heuristics in POMCP rollouts?
- Question: Can the proposed heuristics be extended to handle continuous state, action, and observation spaces as in POMCPOW?
- Question: How does the performance of the proposed heuristics scale with increasing problem size and complexity?
- Question: Can the proposed heuristics be integrated into other POMDP solvers beyond POMCP, such as DESPOT or RTDP-BEL?

## Limitations
- The method relies on factored domain representations, which may not be available in all POMDP applications.
- Scalability to larger domains with complex action histories remains an open challenge.
- The evaluation is limited to specific benchmark domains without extensive ablation studies on heuristic parameters.

## Confidence
- Claim: h_add heuristic in belief space significantly improves POMCP performance → High
- Claim: Multi-state h_add is particularly effective in domains requiring information-gathering → High
- Claim: Computational feasibility in larger domains → Medium

## Next Checks
1. Test the impact of different timeout limits and belief set sizes on performance across all domains to understand scalability boundaries.
2. Apply the proposed heuristics to a non-contingent planning domain (e.g., flat-state POMDP) to evaluate generalizability limitations.
3. Compare h_add with other delete-relaxation heuristics (e.g., h_max) and domain-specific heuristics to quantify the specific contribution of the proposed approach.