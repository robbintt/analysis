---
ver: rpa2
title: 'RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems'
arxiv_id: '2306.03091'
source_url: https://arxiv.org/abs/2306.03091
tags:
- code
- context
- cross-file
- retrieval
- completion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RepoBench is a new benchmark suite designed to evaluate repository-level
  code auto-completion systems, addressing the gap in current benchmarks that focus
  primarily on single-file tasks. It consists of three interconnected tasks: RepoBench-R
  (Retrieval), which tests the ability to retrieve relevant code snippets from other
  files; RepoBench-C (Code Completion), which predicts the next line of code using
  cross-file and in-file context; and RepoBench-P (Pipeline), which combines both
  retrieval and completion to simulate real-world auto-completion systems.'
---

# RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems

## Quick Facts
- arXiv ID: 2306.03091
- Source URL: https://arxiv.org/abs/2306.03091
- Reference count: 40
- Key outcome: RepoBench benchmark shows cross-file context significantly improves code completion performance across multiple models and tasks

## Executive Summary
RepoBench addresses the gap in code auto-completion benchmarks by introducing a comprehensive evaluation suite for repository-level systems. The benchmark consists of three interconnected tasks: retrieval of relevant code snippets from other files, code completion using cross-file and in-file context, and a pipeline combining both. Experiments demonstrate that incorporating cross-file contexts significantly improves performance, with models like Codex (175B) achieving high exact match and edit similarity scores. The benchmark supports Python and Java and provides a framework for advancing repository-level code completion systems.

## Method Summary
RepoBench evaluates repository-level code auto-completion through three tasks: RepoBench-R (Retrieval) tests snippet retrieval from other files using various strategies including Jaccard similarity, edit similarity, and semantic models like UniXcoder; RepoBench-C (Code Completion) predicts next lines using cross-file and in-file context with models like Codex, InCoder, and CodeGen; RepoBench-P (Pipeline) combines both retrieval and completion to simulate real-world systems. The benchmark uses Python and Java code from GitHub repositories (32-128 files each), focusing on cross-file dependencies and imports. Evaluation employs Exact Match and Edit Similarity metrics for completion, and Accuracy@k for retrieval.

## Key Results
- Cross-file contexts significantly improve code completion performance, even when retrieved snippets don't directly contain the target code
- Codex (175B) achieves high exact match and edit similarity scores, while smaller models benefit from fine-tuning on repository-level data
- UniXcoder's multi-modal approach consistently outperforms traditional retrieval methods across evaluation settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-file contexts significantly improve code completion performance
- Mechanism: Including relevant snippets from other files (parsed via import statements) provides additional semantic context that helps models predict the next line more accurately, even when those snippets don't directly contain the target code
- Core assumption: Cross-file dependencies in real codebases contain relevant contextual information that can improve prediction accuracy
- Evidence anchors:
  - [abstract]: "Experiments show that incorporating cross-file contexts significantly improves performance"
  - [section]: "including more cross-file contexts, regardless of the quality of retrieval (even when chosen at random), can significantly enhance the final performance"
  - [corpus]: Weak - corpus only mentions related work without quantitative validation of this specific mechanism
- Break condition: If cross-file contexts are irrelevant noise or if the model cannot effectively process longer contexts due to architectural limitations

### Mechanism 2
- Claim: Fine-tuning smaller models on cross-file tasks improves their transfer learning capability
- Mechanism: Smaller models (350M-2B parameters) lack the inherent ability to leverage cross-file contexts, but targeted fine-tuning on repository-level data enables them to learn relevant patterns and dependencies
- Core assumption: Smaller models can learn to effectively utilize cross-file context through exposure to repository-level training data
- Evidence anchors:
  - [abstract]: "Smaller models benefit from fine-tuning to handle cross-file tasks effectively"
  - [section]: "The limited transfer learning capability displayed by smaller models. This highlights the critical role of fine-tuning when adapting models to handle tasks involving cross-file contexts"
  - [corpus]: Weak - corpus mentions fine-tuning in related work but doesn't provide specific evidence for this mechanism
- Break condition: If fine-tuning doesn't generalize beyond the training distribution or if the model capacity is insufficient regardless of fine-tuning

### Mechanism 3
- Claim: UniXcoder's multi-modal representation learning approach outperforms traditional retrieval methods
- Mechanism: UniXcoder uses multi-modal contrastive learning and cross-modal generation tasks to capture semantic relationships between code fragments, leading to better retrieval of relevant snippets
- Core assumption: Multi-modal learning approaches can capture semantic relationships in code that lexical or single-modal semantic methods miss
- Evidence anchors:
  - [abstract]: "UniXcoder [18], with its unique approach of multi-modal data representation learning and combined use of multi-modal contrastive learning (MCL) [17] and cross-modal generation tasks (CMG), consistently outperforms other methods"
  - [section]: "This demonstrates its advanced ability in capturing the semantic meaning of diverse code fragments, solidifying its standing as a robust model for such tasks"
  - [corpus]: Strong - corpus explicitly identifies UniXcoder as outperforming other methods in retrieval tasks
- Break condition: If the multi-modal approach doesn't scale to larger codebases or if simpler methods become competitive with more training data

## Foundational Learning

- Concept: Cross-file dependencies and import statements
  - Why needed here: Understanding how code modules reference each other across files is fundamental to constructing the cross-file context for retrieval and completion tasks
  - Quick check question: What information do import statements provide that's critical for cross-file code completion?

- Concept: Token limits and context windows in transformer models
  - Why needed here: Models have architectural constraints on how much context they can process, which affects how prompts are constructed and which retrieval strategies are viable
  - Quick check question: Why does the paper limit in-file context to 30 lines and cross-file context to 6,144 tokens?

- Concept: Evaluation metrics for code generation (Exact Match, Edit Similarity)
  - Why needed here: These metrics determine how we measure success in predicting the next line of code, with different sensitivities to exact vs. semantically similar outputs
  - Quick check question: How does Edit Similarity differ from Exact Match in evaluating code completion?

## Architecture Onboarding

- Component map: Import parsing → Retrieval → Context builder → Model prediction → Evaluation
- Critical path: Import parsing → Retrieval → Context construction → Model prediction → Evaluation
- Design tradeoffs: Longer context windows enable better cross-file completion but require more compute and may exceed model limits; simpler retrieval methods are faster but less accurate than semantic approaches
- Failure signatures: Poor performance on XF-F setting indicates inability to handle first-use cases without prior hints; similar XF-R and IF performance suggests cross-file context isn't being effectively utilized
- First 3 experiments:
  1. Test retrieval accuracy with different numbers of kept lines (3, 5, 10) to find optimal context for retrieval
  2. Compare baseline performance (no cross-file context) against Oracle-Filled to measure impact of cross-file information
  3. Evaluate fine-tuning impact by comparing CodeGen-350M before and after training on repository-level data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different cross-file context retrieval strategies impact the performance of code completion models in real-world programming scenarios?
- Basis in paper: [explicit] The paper discusses various retrieval strategies (Oracle-Only, Oracle-Filled, Jaccard, UniXcoder, Random, Baseline) and their impact on code completion performance in RepoBench-P.
- Why unresolved: While the paper compares these strategies, it does not provide a comprehensive evaluation of their effectiveness in real-world programming scenarios, which may involve more complex and diverse codebases.
- What evidence would resolve it: A large-scale empirical study comparing the performance of different retrieval strategies on a diverse set of real-world codebases, measuring their impact on code completion accuracy and developer productivity.

### Open Question 2
- Question: How does the integration of cross-file contexts affect the performance of code completion models for different programming languages and code styles?
- Basis in paper: [explicit] The paper mentions that Python tasks typically display higher accuracy metrics than Java across all retrieval methods, suggesting language-specific differences in the effectiveness of cross-file contexts.
- Why unresolved: The paper does not explore the impact of cross-file contexts on code completion performance for different programming languages and code styles in depth, leaving questions about the generalizability of the findings.
- What evidence would resolve it: A comprehensive evaluation of the performance of code completion models with cross-file contexts on a diverse set of programming languages and code styles, analyzing the factors that contribute to differences in effectiveness.

### Open Question 3
- Question: How can code completion models be further improved to handle long-range contexts and complex multi-file dependencies more effectively?
- Basis in paper: [inferred] The paper emphasizes the importance of efficient models that can manage longer, more complex contexts, and the limitations of current models in handling such scenarios.
- Why unresolved: While the paper highlights the need for models capable of handling long-range contexts, it does not provide specific insights into how these models can be further improved or what architectural innovations are needed to address this challenge.
- What evidence would resolve it: Research into novel model architectures and training techniques that can effectively capture long-range dependencies and complex multi-file interactions, demonstrating their effectiveness through rigorous empirical evaluation.

## Limitations

- Evaluation relies heavily on proprietary models like Codex and InCoder, limiting reproducibility
- Dataset focuses on repositories with 32-128 files, potentially missing complexity of larger codebases
- Retrieval task uses artificially constructed examples with guaranteed relevant code, potentially overestimating real-world performance

## Confidence

**High Confidence**: The fundamental premise that cross-file context improves code completion performance is well-supported by controlled experiments comparing IF (in-file only) against XF-C (cross-file completion) settings. The statistical significance of performance differences across multiple models and evaluation metrics provides robust evidence for this core claim.

**Medium Confidence**: The superiority of UniXcoder's multi-modal approach over traditional retrieval methods is demonstrated within the paper's controlled setting, but the generalizability to other domains or larger codebases remains uncertain. The comparison is limited to specific model variants and may not capture the full landscape of retrieval techniques.

**Low Confidence**: The claim that fine-tuning smaller models effectively bridges the gap with larger models for cross-file tasks is supported by limited experimental evidence. The paper shows improvement after fine-tuning but doesn't explore whether this generalizes across different fine-tuning strategies, amounts of training data, or whether the improvement persists with more challenging examples.

## Next Checks

1. **Cross-model validation**: Replicate the core experiments using only open-source models (e.g., CodeGen variants) to verify that the performance improvements from cross-file context are consistent across different model families and not specific to proprietary implementations.

2. **Robustness testing**: Evaluate the retrieval and completion performance on repositories with varying characteristics (file count, programming language, dependency complexity) to determine whether the observed improvements generalize beyond the current dataset distribution.

3. **Zero-shot retrieval evaluation**: Test the retrieval component on examples where relevant code snippets are not guaranteed to exist in the corpus, measuring false positive rates and precision-recall tradeoffs to assess real-world applicability beyond the controlled oracle setting.