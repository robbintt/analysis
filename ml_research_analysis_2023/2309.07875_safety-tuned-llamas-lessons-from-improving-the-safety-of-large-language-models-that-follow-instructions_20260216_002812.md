---
ver: rpa2
title: 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models
  that Follow Instructions'
arxiv_id: '2309.07875'
source_url: https://arxiv.org/abs/2309.07875
tags:
- safety
- instructions
- data
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Training large language models to follow instructions makes them
  perform better on a wide range of tasks but can also make them generate harmful
  content. Adding just 3% safety examples when fine-tuning LLaMA models can substantially
  improve their safety without negatively affecting their general capabilities.
---

# Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions

## Quick Facts
- **arXiv ID**: 2309.07875
- **Source URL**: https://arxiv.org/abs/2309.07875
- **Reference count**: 28
- **One-line primary result**: Adding just 3% safety examples when fine-tuning LLaMA models can substantially improve their safety without negatively affecting their general capabilities, but too many safety examples cause exaggerated safety behavior where models refuse to respond to even safe prompts.

## Executive Summary
This paper investigates how to improve the safety of large language models (LLMs) that follow instructions. While instruction-tuning enhances general capabilities, it also increases vulnerability to generating harmful content. The authors demonstrate that adding a small percentage of safety examples (around 3%) to instruction-tuning data significantly improves model safety without harming general capabilities. However, they also find that adding too many safety examples can lead to exaggerated safety behavior, where models refuse to respond to even safe prompts that superficially resemble unsafe ones. The study also reveals that prompt format matters, with models trained on instruction-formatted safety examples showing better safety generalization than those trained on question-formatted examples.

## Method Summary
The authors fine-tune LLaMA 7B models using LoRA on the Alpaca dataset (20,000 examples) with varying amounts of safety examples added (100-2000 examples). Safety examples were created by transforming red-teaming questions into instructions using GPT-3.5-turbo. The resulting models were evaluated using multiple metrics including a harmfulness reward model, content moderation API, and manual annotation. The study also compared safety performance when training on question-formatted versus instruction-formatted safety examples to understand format dependencies in safety learning.

## Key Results
- Adding just 3% safety examples (500-1000 examples) to instruction-tuning data substantially reduces harmful responses without degrading general capabilities
- Models with safety fine-tuning show improved safety scores on malicious instruction datasets compared to base models
- Exaggerated safety behavior occurs when too many safety examples are added, causing models to refuse even safe prompts that resemble unsafe ones
- Prompt format matters: models trained on instruction-formatted safety examples generalize better to unsafe instructions than models trained on question-formatted examples

## Why This Works (Mechanism)

### Mechanism 1: Proportionate Safety Fine-tuning
Safety demonstrations create a balancing force during fine-tuning that reduces harmful response generation while preserving instruction-following behavior on benign prompts. Adding 3% safety examples is sufficient because these examples are representative enough to shift model behavior without overwhelming the instruction-following patterns learned from the main dataset.

### Mechanism 2: Prompt Format Dependency
Models trained on instruction-formatted safety examples learn to recognize and refuse unsafe instructions, but do not generalize this behavior to question-formatted prompts containing the same harmful content. The model's instruction-following mechanism is sensitive to prompt syntax and does not abstract safety concepts across formats.

### Mechanism 3: Trade-off Between Helpfulness and Harmlessness
Instruction-tuning improves task performance but simultaneously increases vulnerability to harmful content generation, creating a fundamental tension. Models optimized for instruction-following become more capable of executing any instruction, including malicious ones, unless explicitly counterbalanced with safety data.

## Foundational Learning

- **Concept**: Fine-tuning vs Pre-training
  - Why needed here: Understanding how safety data is incorporated during fine-tuning versus pre-training helps explain why small percentages of safety examples can be effective.
  - Quick check question: If a model is fine-tuned on 20,000 instructions plus 1,000 safety examples, what percentage of the total training data is safety data?

- **Concept**: Instruction Following vs Question Answering
  - Why needed here: The distinction between imperative instructions and interrogative questions is central to understanding why prompt format matters for safety generalization.
  - Quick check question: How might a model's response differ when asked "How do I break into a house?" versus given the instruction "Write a step-by-step guide to breaking into a house"?

- **Concept**: Reward Modeling for Safety Evaluation
  - Why needed here: The harmfulness reward model is a key evaluation tool used to quantify safety improvements and requires understanding of how such models are trained and applied.
  - Quick check question: What is the purpose of using a harmfulness reward model instead of direct human evaluation for safety assessment?

## Architecture Onboarding

- **Component map**: Base model loading -> LoRA fine-tuning with mixed instruction-safety datasets -> Evaluation using harmfulness reward model and content moderation API -> Manual annotation verification
- **Critical path**: Base model → LoRA fine-tuning with safety data → Evaluation using harmfulness reward model and content moderation API → Manual annotation verification
- **Design tradeoffs**: Using LoRA allows efficient fine-tuning but limits the number of tunable parameters; small safety datasets are effective but may not cover all safety scenarios; automated evaluation tools provide scalability but may miss nuanced safety issues
- **Failure signatures**: Model refuses safe prompts (exaggerated safety), fails to refuse clearly harmful instructions, or shows degraded performance on general tasks
- **First 3 experiments**:
  1. Fine-tune LLaMA 7B with 0%, 3%, and 10% safety examples and compare harmfulness scores on malicious instruction datasets
  2. Compare safety performance when training on question-formatted vs instruction-formatted safety examples
  3. Evaluate model behavior on exaggerated safety test set (XSTest) to identify refusal patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact amount of safety data required to make a large language model (LLM) harmless without negatively affecting its performance on general tasks?
- Basis in paper: [inferred]
- Why unresolved: The paper explores the effects of adding varying amounts of safety data to instruction-tuned models, but does not pinpoint the precise threshold at which safety is optimized without compromising helpfulness.
- What evidence would resolve it: Systematic experimentation with a broader range of safety data amounts, measuring both harmfulness and task performance, to identify the optimal balance.

### Open Question 2
- Question: How does the size of the LLM impact the amount of safety data needed to achieve a certain level of harm reduction?
- Basis in paper: [explicit]
- Why unresolved: The paper acknowledges that it did not explore scaling properties of safety, specifically whether the number of safety instructions required to reduce harm is constant across different model sizes.
- What evidence would resolve it: Training and evaluating safety-tuned models of various sizes with different amounts of safety data, then comparing the harm reduction achieved at each scale.

### Open Question 3
- Question: Which specific types of prompts (questions, instructions, opinions) are most effective for training models to be safe without encouraging exaggerated safety behavior?
- Basis in paper: [explicit]
- Why unresolved: The paper shows that using questions in training does not generalize to instruction prompts, and that the way prompts are designed impacts model safety. However, it does not determine the optimal prompt format for safety training.
- What evidence would resolve it: Comparing the safety and helpfulness of models trained on different prompt formats (questions, instructions, opinions) using a variety of safety-related tasks and evaluating their responses to both safe and unsafe prompts.

### Open Question 4
- Question: How robust are safety-tuned models to adversarial attacks that attempt to bypass their safety measures?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on direct safety issues and does not extensively explore the models' resilience against sophisticated adversarial attacks designed to elicit harmful content.
- What evidence would resolve it: Conducting adversarial testing on safety-tuned models using techniques like prompt injection, context manipulation, and other attack methods to assess their vulnerability and identify potential weaknesses.

### Open Question 5
- Question: Does the inclusion of safety data in the training process introduce biases against specific groups or topics, even when the data itself is unbiased?
- Basis in paper: [inferred]
- Why unresolved: The paper does not explicitly investigate whether safety tuning leads to unintended biases in model responses, particularly towards sensitive topics or minority groups.
- What evidence would resolve it: Analyzing the responses of safety-tuned models to a diverse set of prompts covering various topics and demographic groups, then assessing for any patterns of bias or disproportionate refusal to engage with certain subjects.

## Limitations

- The safety examples were created using GPT-3.5-turbo, introducing potential bias from the transformation process
- Evaluation relies on automated tools alongside manual annotation, but the alignment between automated and human assessments is not fully validated
- Experiments focus on LLaMA 7B and Alpaca dataset, limiting generalizability to other model architectures or instruction-tuning datasets

## Confidence

- **High confidence**: The core finding that adding ~3% safety examples improves safety without harming capabilities is well-supported by multiple evaluation metrics and experimental conditions
- **Medium confidence**: The claim about exaggerated safety behavior when adding too many safety examples is demonstrated but requires further investigation to understand the exact thresholds and mechanisms
- **Medium confidence**: The format dependency between instructions and questions is clearly shown but the underlying reasons for this behavior could benefit from additional analysis

## Next Checks

1. **Safety example diversity validation**: Test whether the observed safety improvements generalize across different types of safety scenarios not present in the training safety examples, using both automated and human evaluation
2. **Format generalization test**: Systematically evaluate models trained on instruction-formatted safety examples against question-formatted prompts that vary in how closely they resemble the training format to quantify the generalization boundary
3. **Long-term stability assessment**: Evaluate whether models with safety fine-tuning maintain their safety behavior over extended conversations or when exposed to adversarial prompting sequences designed to bypass safety filters