---
ver: rpa2
title: 'Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width
  Guarantees and Benefits of Complex Eigenvalues'
arxiv_id: '2307.11888'
source_url: https://arxiv.org/abs/2307.11888
tags:
- linear
- rmin
- reconstruction
- sequence
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that combining MLPs with linear diagonal recurrences
  leads to arbitrarily precise approximation of regular causal sequence-to-sequence
  maps. The key insight is that linear RNNs provide a lossless encoding of the input
  sequence, which MLPs can then process to achieve the desired map.
---

# Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues

## Quick Facts
- arXiv ID: 2307.11888
- Source URL: https://arxiv.org/abs/2307.11888
- Reference count: 40
- Primary result: Linear diagonal RNNs interleaved with MLPs can approximate any regular causal sequence-to-sequence map

## Executive Summary
This paper establishes that combining linear diagonal recurrent neural networks with multi-layer perceptrons yields universal approximation for causal sequence-to-sequence mappings. The key insight is that linear RNNs provide a lossless encoding of input sequences through Vandermonde matrix inversion, which MLPs can then process to achieve arbitrary precision approximation. The authors show that while real diagonal recurrences suffice for universality, using complex eigenvalues near the unit circle dramatically improves the RNN's ability to store information, connecting this to the vanishing gradient problem. Experiments demonstrate effectiveness on MNIST digit reconstruction and controlled Lotka-Volterra systems.

## Method Summary
The architecture consists of a linear diagonal RNN followed by position-wise MLPs. The RNN computes hidden states xk = Λxk-1 + Buk where Λ is a diagonal matrix of eigenvalues and B is the input projection. The MLP processes each hidden state position-wise to produce outputs. The key theoretical insight is that the unrolled recurrence creates a Vandermonde matrix whose invertibility (when eigenvalues are distinct) enables lossless reconstruction of input sequences. The MLP then approximates the desired sequence-to-sequence mapping on these encoded representations. The method relies on sparsity assumptions about input sequences and sufficient MLP width for universal approximation.

## Key Results
- Linear diagonal RNNs interleaved with MLPs achieve universal approximation for causal sequence-to-sequence maps
- Complex eigenvalues near the unit circle dramatically improve information retention and conditioning compared to real eigenvalues
- MNIST digit reconstruction experiments demonstrate practical effectiveness of the approach
- The architecture connects to vanishing gradient issues through eigenvalue placement near the unit circle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear diagonal RNNs act as lossless compressors for finite-length sequences under certain eigenvalue distributions
- Mechanism: Unrolling the recurrence xk = Λxk-1 + Buk produces a Vandermonde matrix V_k whose columns form a basis for reconstructing all past inputs if N ≥ k and eigenvalues are sufficiently distinct
- Core assumption: The input sequence can be represented sparsely in some basis, and the Vandermonde matrix V_k has full column rank
- Evidence anchors:
  - [abstract] "linear RNN provides a lossless encoding of the input sequence"
  - [section] "xk = Pk-1 j=0 ΛjBuk-j" and "V_k is invertible since det(V_N) = ∏1≤i<j≤N(λi − λj) ≠ 0"
  - [corpus] Weak - no corpus papers directly address Vandermonde invertibility in this setting
- Break condition: If eigenvalues are real and poorly conditioned, V_k becomes ill-conditioned and reconstruction fails

### Mechanism 2
- Claim: Complex eigenvalues near the unit circle dramatically improve the RNN's ability to store information
- Mechanism: The condition number of the Vandermonde matrix grows exponentially with N for real eigenvalues, but remains stable (even exactly 1) when eigenvalues are chosen as Nth roots of unity or sampled uniformly from a ring near the unit circle
- Core assumption: Initialization of Λ close to the unit disk ensures stable reconstruction
- Evidence anchors:
  - [section] "using complex eigenvalues near unit disk... greatly helps the RNN in storing information"
  - [section] "in the complex case it is possible to make the condition number of Vk exactly 1 by e.g. choosing the Nth-roots of unity"
  - [corpus] Weak - no corpus papers discuss complex eigenvalue initialization for linear RNNs
- Break condition: If rmin is too small (e.g., rmin = 0), the Vandermonde matrix becomes ill-conditioned and reconstruction is only partial

### Mechanism 3
- Claim: MLPs with sufficient width can approximate any continuous sequence-to-sequence map on the compressed representations
- Mechanism: Once the RNN provides a lossless encoding, the MLP acts as a universal approximator on the compact set of encoded sequences, implementing any desired non-linear map Tk
- Core assumption: The MLP is wide enough to approximate the mapping from encoded states to outputs
- Evidence anchors:
  - [abstract] "MLP performs non-linear processing on this encoding"
  - [section] "as the MLP size grows, it can approximate arbitrary non-linear map on compact sets"
  - [corpus] Weak - no corpus papers provide empirical evidence for MLP universality on RNN-encoded sequences
- Break condition: If the MLP is too narrow, it cannot approximate the required mapping, breaking the universality guarantee

## Foundational Learning

- Concept: Vandermonde matrix invertibility and conditioning
  - Why needed here: The core reconstruction mechanism relies on inverting V_k to recover past inputs from the hidden state
  - Quick check question: If N = k and eigenvalues are distinct, is V_k invertible? (Yes, because det(V_k) = ∏1≤i<j≤k(λi − λj) ≠ 0)

- Concept: Sparse representation and basis decomposition
  - Why needed here: Assumption A requires the input to be compressible in some basis to reduce the effective dimensionality
  - Quick check question: If u = Ψα with P ≪ L, can we recover u from a compressed representation? (Yes, if the mapping preserves the sparse coefficients)

- Concept: Universal approximation theorem for MLPs
  - Why needed here: The final step relies on MLPs approximating any continuous mapping on the encoded sequences
  - Quick check question: Can a one-hidden-layer MLP with ReLU activations approximate any continuous function on a compact set as width → ∞? (Yes, by Pinkus' theorem)

## Architecture Onboarding

- Component map: Input encoder (optional) → Linear diagonal RNN (Λ, B) → Position-wise MLP (ϕ) → Output
- Critical path:
  1. Generate hidden state xk = Λxk-1 + Buk for each timestep
  2. Optionally encode timestamp in first feature of xk
  3. Apply position-wise MLP to produce output yk = ϕ(xk)

- Design tradeoffs:
  - N vs. L: Need N ≥ L for lossless reconstruction without sparsity assumptions; sparsity allows N ≪ L
  - rmin vs. conditioning: Larger rmin (closer to unit circle) improves conditioning but may reduce expressivity
  - MLP width vs. approximation quality: Wider MLPs can better approximate complex mappings

- Failure signatures:
  - Poor reconstruction: Check Vandermonde condition number and eigenvalue distribution
  - Vanishing gradients: Monitor eigenvalue magnitudes and initialization scheme
  - Underfitting outputs: Increase MLP width or depth

- First 3 experiments:
  1. Test reconstruction from final hidden state on MNIST with varying N and rmin
  2. Evaluate condition number of V_k V_k^T for different eigenvalue distributions
  3. Train MLP to reconstruct Lotka-Volterra outputs from encoded states with varying MLP widths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the expressive power of linear RNNs with MLPs fundamentally change when using higher-order recurrences or non-diagonal matrices?
- Basis in paper: [explicit] The paper only considers diagonal linear recurrences, noting that complex eigenvalues near the unit disk are empirically beneficial but real eigenvalues suffice for universality
- Why unresolved: The paper establishes universality for diagonal linear recurrences, but does not explore whether the universality result extends to more general linear recurrences (e.g., dense matrices or higher-order recurrences) or if there are architectural choices beyond eigenvalue placement that impact expressivity
- What evidence would resolve it: Experimental and theoretical analysis comparing universality and approximation capabilities of diagonal vs. dense linear recurrences with MLPs, potentially using synthetic datasets with known sequence-to-sequence properties

### Open Question 2
- Question: How does the choice of basis functions (Ψ) affect the approximation quality and conditioning of the system, and can this be optimized?
- Basis in paper: [explicit] The paper assumes input sequences are sparse in some basis (Assumption A) and shows that this sparsity improves conditioning and reduces the required hidden state size. However, it does not provide a method for learning or selecting the optimal basis
- Why unresolved: While the paper demonstrates that sparsity in a fixed basis improves reconstruction, it does not address how to choose or learn the basis functions that minimize reconstruction error or improve conditioning in practice
- What evidence would resolve it: Empirical comparison of reconstruction error and conditioning for different basis functions (e.g., wavelets, Fourier, learned bases) on standard sequence-to-sequence tasks, along with a method for optimizing the basis for a given dataset

### Open Question 3
- Question: What are the limitations of the universality result in terms of sequence length, input complexity, and output regularity?
- Basis in paper: [inferred] The paper assumes finite-length sequences, regular enough sequence-to-sequence maps (Assumption B), and input sparsity (Assumption A). It does not explore how these assumptions limit the applicability of the universality result or what happens when they are violated
- Why unresolved: The universality result relies on assumptions about input sparsity and output regularity, but the paper does not provide concrete bounds on sequence length, input complexity, or output regularity that ensure the result holds. It also does not explore how the result breaks down when these assumptions are violated
- What evidence would resolve it: Experimental analysis of approximation error as a function of sequence length, input sparsity, and output regularity, along with theoretical bounds on the required hidden state size and MLP width to achieve a desired level of approximation

## Limitations
- Reconstruction quality critically depends on eigenvalue distribution and conditioning of the Vandermonde matrix
- Sparsity assumption (Assumption A) is necessary but not verified in experiments
- No explicit width bounds provided for MLP to achieve universal approximation
- Limited empirical validation beyond MNIST and Lotka-Volterra systems

## Confidence

- Mechanism 1 (Lossless reconstruction): Medium - mathematically rigorous but sensitive to eigenvalue conditioning
- Mechanism 2 (Complex eigenvalue benefits): Medium-High - theoretically sound with strong numerical evidence but limited empirical validation
- Mechanism 3 (MLP universality): Medium - relies on standard universal approximation theory but untested on RNN-encoded sequences specifically

## Next Checks

1. **Eigenvalue sensitivity test**: Systematically vary eigenvalue initialization (real vs. complex, different rmin values) and measure Vandermonde condition numbers and reconstruction accuracy on synthetic data
2. **Sparsity impact analysis**: Quantify how the compression ratio P/L affects the required hidden state dimension N for lossless reconstruction across different input distributions
3. **MLP width scaling**: Empirically determine the relationship between MLP width and approximation quality for various target sequence-to-sequence maps, establishing practical width bounds