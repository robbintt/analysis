---
ver: rpa2
title: 'Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking
  Oracles'
arxiv_id: '2303.03751'
source_url: https://arxiv.org/abs/2303.03751
tags:
- ranking
- algorithm
- human
- optimization
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ZO-RankSGD, a zeroth-order optimization algorithm
  that optimizes black-box functions using only ranking oracles, a scenario common
  in human feedback applications. The algorithm employs a rank-based stochastic estimator
  for descent direction and provides theoretical convergence guarantees.
---

# Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles

## Quick Facts
- arXiv ID: 2303.03751
- Source URL: https://arxiv.org/abs/2303.03751
- Reference count: 40
- Key outcome: Introduces ZO-RankSGD, a zeroth-order optimization algorithm that uses ranking oracles for provable convergence in human feedback applications

## Executive Summary
This paper introduces ZO-RankSGD, a novel zeroth-order optimization algorithm that optimizes black-box functions using only ranking oracles. The algorithm is particularly relevant for applications involving human feedback, such as reinforcement learning and image enhancement. By employing a rank-based stochastic estimator for descent direction, ZO-RankSGD achieves theoretical convergence guarantees while requiring fewer function evaluations compared to existing methods. The approach is demonstrated to be effective in synthetic function optimization, reinforcement learning with ranking oracles, and enhancing image quality in Stable Diffusion via human feedback.

## Method Summary
ZO-RankSGD is a zeroth-order optimization algorithm designed to work with ranking oracles, which provide only ordinal feedback about function values. The method constructs a directed acyclic graph (DAG) from ranking information and computes gradient estimates by aggregating multiple pairwise comparisons. Unlike traditional zeroth-order methods that rely on pairwise comparisons, ZO-RankSGD uses a rank-based stochastic estimator that reduces variance and improves convergence. The algorithm is proven to converge to a stationary point under appropriate conditions. A modified version is also proposed for applications like Stable Diffusion image enhancement, incorporating gradient memory and line search strategies using ranking oracles.

## Key Results
- Demonstrates convergence to stationary points on synthetic functions with fewer queries than baseline methods
- Achieves competitive performance in reinforcement learning tasks using only ranking feedback
- Successfully enhances image quality in Stable Diffusion when optimized via human ranking feedback

## Why This Works (Mechanism)

### Mechanism 1
The rank-based stochastic estimator reduces variance compared to pairwise comparison by aggregating multiple pairwise comparisons into a single gradient estimate. Instead of using a single pairwise comparison, the algorithm constructs a DAG from ranking information and computes the gradient as an average over all edges in the DAG, weighted by edge differences. This aggregation effectively combines multiple pairwise comparisons, reducing the variance of the gradient estimate.

### Mechanism 2
The algorithm achieves convergence to a stationary point by combining the rank-based estimator with appropriate step size and smoothing parameter choices. The theoretical analysis shows that the expected gradient norm decreases with each iteration when using the rank-based estimator, and the convergence rate depends on the variance of this estimator. The algorithm maintains this convergence property by selecting step sizes and smoothing parameters that balance the bias and variance of the estimator.

### Mechanism 3
The algorithm can be applied to reinforcement learning settings where only ranking oracles of episode rewards are available. By treating the policy optimization problem as a black-box optimization problem where the objective function (episode reward) can only be evaluated through a ranking oracle, the algorithm can directly optimize the policy parameters using the rank-based gradient estimator without requiring access to the actual reward values.

## Foundational Learning

- Concept: Zeroth-order optimization
  - Why needed here: The algorithm operates without gradient information, requiring methods to estimate descent directions from function evaluations only.
  - Quick check question: What is the main difference between zeroth-order optimization and first-order optimization?

- Concept: Stochastic gradient estimation
  - Why needed here: The algorithm uses random perturbations to estimate gradients, requiring understanding of how to construct unbiased gradient estimators with controlled variance.
  - Quick check question: How does the variance of a gradient estimator affect the convergence rate of stochastic optimization algorithms?

- Concept: Ranking oracles and their properties
  - Why needed here: The algorithm relies on ranking information rather than absolute function values, requiring understanding of how to extract useful information from ordinal feedback.
  - Quick check question: What information can be extracted from a ranking oracle that cannot be obtained from a comparison oracle?

## Architecture Onboarding

- Component map:
  - Ranking oracle interface: Handles queries to the ranking oracle and returns sorted indices
  - DAG construction module: Converts ranking information into a directed acyclic graph representation
  - Gradient estimator: Computes the rank-based gradient estimate from the DAG
  - Line search module: Implements the line search strategy using ranking oracles
  - Optimization loop: Coordinates the overall optimization process

- Critical path: Ranking oracle query → DAG construction → Gradient estimation → Line search → Parameter update

- Design tradeoffs:
  - Query efficiency vs. accuracy: Using more queries per iteration (larger m) can reduce variance but increases computational cost
  - Ranking granularity vs. information extraction: Using full rankings (k=m) provides more information than partial rankings but may be more difficult for humans to provide
  - Step size vs. convergence stability: Larger step sizes can accelerate convergence but may lead to instability

- Failure signatures:
  - If the algorithm converges very slowly or oscillates, the variance of the gradient estimator may be too high
  - If the algorithm fails to improve the objective function, the ranking oracle may not provide sufficient information
  - If the algorithm produces poor quality images in the Stable Diffusion application, the line search may not be finding good step sizes

- First 3 experiments:
  1. Implement the algorithm on a simple quadratic function and verify that it converges to the global minimum
  2. Test the algorithm with different values of m and k to observe the impact on convergence speed
  3. Apply the algorithm to the Stable Diffusion image enhancement task with synthetic ranking feedback to verify the overall pipeline works before collecting human feedback

## Open Questions the Paper Calls Out

### Open Question 1
How does the convergence rate of ZO-RankSGD scale with the dimensionality of the optimization problem?
- Basis in paper: The paper provides theoretical bounds on the convergence rate, but these bounds depend on the dimension d. Specifically, the convergence rate is O(sqrt(d/T)) in Corollary 1.
- Why unresolved: While the paper provides theoretical bounds, it does not provide empirical evidence of how the convergence rate scales with dimensionality in practice.
- What evidence would resolve it: Experiments on optimization problems with varying dimensions, comparing the convergence rates of ZO-RankSGD to other zeroth-order optimization algorithms.

### Open Question 2
How sensitive is ZO-RankSGD to the choice of the ranking oracle parameters m and k?
- Basis in paper: The paper discusses how the convergence rate depends on m and k, and provides an analysis of the variance of the gradient estimator in terms of these parameters. It also presents experiments showing the impact of different m and k values on performance.
- Why unresolved: While the paper provides theoretical analysis and some experimental evidence, it does not provide a comprehensive study of the sensitivity of ZO-RankSGD to the choice of m and k.
- What evidence would resolve it: A systematic study of the performance of ZO-RankSGD with different m and k values on a wide range of objective functions and ranking oracles, identifying patterns and guidelines for parameter selection.

### Open Question 3
How does the performance of ZO-RankSGD compare to other zeroth-order optimization algorithms that use ranking oracles?
- Basis in paper: The paper compares ZO-RankSGD to other zeroth-order optimization algorithms on synthetic functions and reinforcement learning tasks, but it does not compare it to other algorithms that use ranking oracles.
- Why unresolved: While the paper demonstrates the effectiveness of ZO-RankSGD compared to some other algorithms, it is unclear how it compares to other algorithms that are specifically designed to use ranking oracles.
- What evidence would resolve it: Experiments comparing the performance of ZO-RankSGD to other zeroth-order optimization algorithms that use ranking oracles on a variety of optimization problems.

### Open Question 4
How can ZO-RankSGD be extended to handle optimization problems with multiple ranking oracles?
- Basis in paper: The paper focuses on the case of a single ranking oracle, but in many real-world applications, there may be multiple ranking oracles that provide different types of feedback.
- Why unresolved: The paper does not discuss how to extend ZO-RankSGD to handle multiple ranking oracles, and it is unclear how to combine the information from different oracles in a principled way.
- What evidence would resolve it: Theoretical analysis and experimental results on the performance of ZO-RankSGD with multiple ranking oracles, including different methods for combining the information from different oracles.

## Limitations

- The convergence guarantees rely on idealized assumptions about ranking oracle consistency that may not hold with noisy human feedback
- The variance reduction mechanism is claimed but not rigorously proven in the paper
- Limited experimental validation of the RL application, with no concrete evidence for how ranking feedback translates to policy improvement

## Confidence

- High confidence: The algorithm's basic framework and its applicability to Stable Diffusion optimization
- Medium confidence: The theoretical convergence guarantees and the rank-based estimator's variance properties
- Low confidence: The claim about direct application to reinforcement learning settings

## Next Checks

1. **Empirical variance analysis**: Run controlled experiments on synthetic functions where the true gradient is known, measuring the variance of the rank-based estimator across multiple runs and comparing it to pairwise comparison methods. This would validate Mechanism 1's variance reduction claim.

2. **Ranking oracle robustness testing**: Introduce controlled noise into the ranking oracle (e.g., randomly flipping pairwise preferences) and measure how this affects convergence speed and final performance. This would test the sensitivity of the algorithm to inconsistent ranking feedback.

3. **Cross-validation of RL application**: Implement the RL application on a simple environment (like CartPole) with synthetic ranking feedback, comparing the learned policy against one trained with actual reward values. This would provide concrete evidence for the RL claims and help identify any gaps in the methodology.