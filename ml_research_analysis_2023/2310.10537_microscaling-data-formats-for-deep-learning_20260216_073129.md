---
ver: rpa2
title: Microscaling Data Formats for Deep Learning
arxiv_id: '2310.10537'
source_url: https://arxiv.org/abs/2310.10537
tags:
- formats
- training
- data
- inference
- fp32
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Microscaling (MX) data formats combine a per-block scaling factor
  with narrow floating-point and integer types for individual elements. The MX standard
  balances hardware efficiency, model accuracy, and user friction for deep learning
  training and inference.
---

# Microscaling Data Formats for Deep Learning

## Quick Facts
- arXiv ID: 2310.10537
- Source URL: https://arxiv.org/abs/2310.10537
- Reference count: 18
- One-line primary result: MX formats enable sub-8-bit training and inference with minimal accuracy loss compared to FP32

## Executive Summary
Microscaling (MX) data formats introduce per-block scaling factors combined with narrow floating-point and integer types for individual elements. The standard balances hardware efficiency, model accuracy, and user friction for deep learning training and inference. Experiments demonstrate that 8-bit MX formats can perform inference directly on FP32 pretrained models with minimal accuracy loss, while 6-bit MX formats achieve close-to-parity with FP32 after quantization-aware fine-tuning or using post-training quantization methods. The work shows the first instance of training large transformer models with sub-8-bit weights, activations, and gradients to accuracy matching FP32 without modifications to the training recipe.

## Method Summary
The paper proposes MX formats that use a per-block scaling factor with narrow floating-point and integer types for individual elements. The method employs a custom CUDA library for MX format emulation and evaluates four settings: direct-cast inference, error diffusion post-training quantization (PTQ), finetuned inference, and training from scratch with MX formats. Experiments cover over two dozen benchmarks including language translation, text encoding, image classification, speech recognition, and recommendation models, with generative tasks including GPT3-175B and LLaMA-7B models.

## Key Results
- 8-bit MX formats achieve inference with minimal accuracy loss directly on FP32 pretrained models without calibration or fine-tuning
- 6-bit MX formats reach close-to-parity with FP32 after quantization-aware fine-tuning or post-training quantization
- 6-bit MX training achieves FP32 accuracy matching without recipe modifications for large transformer models
- 4-bit MX format training incurs only minor accuracy drops while maintaining reasonable performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MX formats reduce bit-width storage while preserving dynamic range via per-block shared scaling.
- Mechanism: Each MX block uses one shared exponent across k elements, freeing exponent bits for each element. This increases effective mantissa precision without expanding overall bit-width.
- Core assumption: Tensor values within a block share similar magnitudes so a single shared exponent suffices.
- Evidence anchors:
  - [abstract] "Microscaling (MX) data formats combine a per-block scaling factor with narrow floating-point and integer types for individual elements."
  - [section] "A basic unit of data in an MX format represents a vector of k numbers and consists of a single shared scale X and k scalar elements {Pi}k i=1"
  - [corpus] Weak: Corpus does not contain evidence about per-block scaling; inferred from paper description.
- Break condition: If tensor elements span multiple orders of magnitude within a block, quantization error increases dramatically.

### Mechanism 2
- Claim: MX formats maintain training accuracy matching FP32 without modifying the training recipe.
- Mechanism: By using MX formats in both forward and backward passes, gradients remain sufficiently precise to support stable optimization; shared exponents preserve relative magnitudes.
- Core assumption: Training dynamics tolerate slightly noisier intermediate values if gradient updates remain accurate enough.
- Evidence anchors:
  - [abstract] "Using 6-bit MX formats, this work shows the first instance of training large transformer models with sub-8-bit weights, activations, and gradients to an accuracy matching FP32 without modifications to the training recipe."
  - [section] "The same hyperparameters were reused for the MX format runs with no changes."
  - [corpus] Weak: Corpus lacks training stability evidence; based on paper's experimental claims.
- Break condition: If gradient quantization noise exceeds the threshold for stable convergence, training accuracy degrades.

### Mechanism 3
- Claim: Direct-cast inference with MXINT8 achieves minimal accuracy loss compared to FP32.
- Mechanism: MXINT8's 8-bit integer elements combined with an 8-bit shared exponent provide sufficient dynamic range and precision for many inference tasks without fine-tuning.
- Core assumption: Inference tasks are less sensitive to small quantization errors than training, especially when models are already trained in higher precision.
- Evidence anchors:
  - [abstract] "8-bit MX formats can perform inference directly on FP32 pretrained models with minimal accuracy loss and without the need for calibration or finetuning."
  - [section] "Our results corroborate the effectiveness of MX formats in balancing the competing demands of hardware efficiency, model accuracy, and user friction."
  - [corpus] Weak: No corpus evidence for inference accuracy; relies on paper's benchmark tables.
- Break condition: If model outputs depend critically on fine-grained value differences, direct-cast accuracy will degrade.

## Foundational Learning

- Concept: Per-block scaling and shared exponents
  - Why needed here: Understanding how MX blocks reduce exponent redundancy while maintaining dynamic range.
  - Quick check question: What is the purpose of the shared scale X in an MX block?

- Concept: Quantization-aware training vs. direct-cast inference
  - Why needed here: Distinguishing when fine-tuning is required to recover accuracy versus when direct-cast suffices.
  - Quick check question: When is quantization-aware finetuning necessary for MX formats?

- Concept: Mixed-precision training flows
  - Why needed here: Recognizing how different formats (e.g., MXFP4 weights, MXFP6 activations) can be combined without breaking training stability.
  - Quick check question: Why must weights and their transposes be stored separately in MX training?

## Architecture Onboarding

- Component map: MX block → shared exponent + k element values; quantization module → scalar float → MX block conversion; inference/training engine → uses MX-aware matmul kernels.
- Critical path: Forward pass → quantize inputs → MX matmul → dequantize outputs → activations; Backward pass → quantize activations/weights → MX matmul → gradients → weight update (FP32 master copy).
- Design tradeoffs: Narrower elements increase quantization noise but save memory/compute; larger block sizes reduce exponent overhead but risk scaling factor mismatch across elements.
- Failure signatures: Accuracy collapse during training (gradient quantization too coarse); inference instability (per-block scaling mismatch across layers).
- First 3 experiments:
  1. Run direct-cast inference on a small CNN with MXINT8; compare accuracy to FP32 baseline.
  2. Perform MXFP6 quantization-aware finetuning on a transformer; measure convergence speed.
  3. Train a small GPT-like model with MXFP4 weights and MXFP6 activations; verify loss curve matches FP32 within tolerance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MX formats scale when applied to extremely large models beyond the 1.5B parameter range tested in this paper?
- Basis in paper: [inferred] The paper demonstrates MXFP6 effectiveness up to 1.5B parameters but doesn't explore larger models
- Why unresolved: The paper's experimental scope was limited to models up to 1.5B parameters, leaving the scalability question open
- What evidence would resolve it: Empirical results showing training/inference performance of MX formats on models significantly larger than 1.5B parameters

### Open Question 2
- Question: What is the impact of MX formats on training convergence speed compared to FP32, beyond just final accuracy?
- Basis in paper: [explicit] The paper focuses on matching FP32 accuracy but doesn't report on training speed or convergence dynamics
- Why unresolved: The experiments only report final model quality, not training dynamics or convergence behavior
- What evidence would resolve it: Detailed training curves showing step count, wall-clock time, and convergence rate comparisons between MX and FP32 formats

### Open Question 3
- Question: How sensitive are MX format results to the choice of rounding mode (e.g., round-half-to-nearest-even vs round-half-away-from-zero)?
- Basis in paper: [explicit] The paper uses different rounding modes in different experiments without systematic comparison
- Why unresolved: The paper applies different rounding modes across experiments but doesn't analyze the impact of this choice
- What evidence would resolve it: Controlled experiments comparing MX performance under different rounding modes while holding all other variables constant

### Open Question 4
- Question: What are the hardware implementation costs and energy efficiency gains of MX formats compared to existing formats like FP8 or INT8?
- Basis in paper: [inferred] The paper emphasizes hardware efficiency as a goal but doesn't provide hardware implementation analysis
- Why unresolved: The paper focuses on software emulation results without addressing actual hardware implementation considerations
- What evidence would resolve it: Hardware synthesis results, power consumption measurements, and area estimates for MX format implementations versus existing formats

## Limitations

- The paper lacks detailed error analysis and ablation studies for training stability claims
- Missing hardware implementation analysis despite emphasizing hardware efficiency as a goal
- Claims about training extremely large models (GPT3-175B, LLaMA-7B) lack supporting experimental evidence

## Confidence

**High confidence**: The basic MX format specification and conversion algorithm are clearly defined and theoretically sound. The block-level exponent sharing mechanism is mathematically valid.

**Medium confidence**: The experimental results showing FP32 parity for 6-bit MX training and minimal accuracy loss for 8-bit MX inference are internally consistent within the paper, but lack independent verification and detailed error analysis.

**Low confidence**: Claims about training large language models (GPT3-175B, LLaMA-7B) with sub-8-bit formats appear in the abstract but lack supporting experimental evidence in the main text. The statement about "first instance" of sub-8-bit training is difficult to verify without broader community validation.

## Next Checks

1. **Distribution Analysis**: Analyze activation and weight distributions across multiple layers in pretrained models to quantify the validity of the per-block scaling assumption. Measure the percentage of blocks where element magnitudes vary by more than 2x, 10x, and 100x.

2. **Training Stability Audit**: Implement MX format training on a small transformer (2-4 layers) and monitor gradient norms, loss curves, and weight evolution at each step. Compare quantization noise levels against theoretical thresholds for stable optimization.

3. **Inference Robustness Testing**: Test direct-cast MXINT8 inference across domain-shifted inputs (e.g., different image datasets for computer vision models, paraphrased text for NLP models) to identify failure modes and quantify accuracy degradation under distribution shift.