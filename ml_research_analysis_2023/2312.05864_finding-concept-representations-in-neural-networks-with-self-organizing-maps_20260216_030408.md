---
ver: rpa2
title: Finding Concept Representations in Neural Networks with Self-Organizing Maps
arxiv_id: '2312.05864'
source_url: https://arxiv.org/abs/2312.05864
tags:
- concepts
- concept
- network
- neural
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for finding concept representations
  in neural networks using self-organizing maps (SOMs). The key idea is to compare
  SOM activation frequency matrices for concept-specific data subsets against those
  for the entire dataset, using relative entropy as the primary metric.
---

# Finding Concept Representations in Neural Networks with Self-Organizing Maps

## Quick Facts
- arXiv ID: 2312.05864
- Source URL: https://arxiv.org/abs/2312.05864
- Reference count: 13
- One-line primary result: Method identifies concept representations in neural networks using SOM activation frequency matrices and relative entropy

## Executive Summary
This paper introduces a method for finding concept representations in neural networks using self-organizing maps (SOMs). The approach compares SOM activation frequency matrices for concept-specific data subsets against those for the entire dataset, using relative entropy as the primary metric. Applied to both text classification and image regression tasks, the method successfully identifies how different concepts are represented across network layers, revealing which concepts are most strongly represented in each layer and providing insights into the network's decision-making process.

## Method Summary
The method involves building SOMs from activation vectors extracted from each layer of a trained neural network, then computing activation frequency matrices for both the entire dataset and concept-specific subsets. Relative entropy (Kullback-Leibler divergence) is used to quantify the difference between these frequency matrices, with higher values indicating stronger concept representation. The approach is validated on two use cases: text classification for predicting whether painters have works in major museums, and image regression for predicting age from photos. Activation vectors are aggregated appropriately for different layer types (mean for LSTM, flattening for CNN) before SOM construction.

## Key Results
- Successfully identified "Italian painters" and "painters from Florence" as more strongly represented concepts in text classification network's final layer
- Revealed ethnicity concepts were generally more strongly represented than gender concepts across layers in image regression task
- Demonstrated consistent relative entropy behavior across layers, increasing from embedding to final fully connected layers
- Showed concept representations evolve across network layers, with higher layers capturing more abstract representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative entropy between concept and base SOM frequency matrices identifies concept representation strength.
- Mechanism: Relative entropy quantifies how much the activation distribution for a concept diverges from the overall data distribution. Higher divergence indicates the concept has a unique activation pattern distinct from general data.
- Core assumption: A well-represented concept will activate a specific subset of SOM units more frequently than the general case, creating a measurable divergence.
- Evidence anchors:
  - [abstract] "relative entropy of the activation map for a concept compared to the map for the whole data is a suitable candidate"
  - [section] "Relative entropy, on the other hand, is consistent and clearly increases from the embedding layer to the FC layer"
  - [corpus] Weak - corpus papers focus on related interpretability methods but not specifically on SOM-based relative entropy for concept representation
- Break condition: If concepts activate similar units as general data (low divergence) or if SOM training fails to create meaningful clusters, relative entropy becomes uninformative.

### Mechanism 2
- Claim: SOM activation frequency matrices reveal spatial clustering of concept-specific activations.
- Mechanism: SOMs group similar activation vectors into neighboring units on a 2D grid. When concept-specific data activates certain SOM regions more than others, this creates hotspots visible as darker regions in frequency heatmaps.
- Core assumption: Similar activation vectors (representing similar concepts) will map to nearby SOM units due to competitive learning.
- Evidence anchors:
  - [abstract] "self-organizing maps as a way to both visually and computationally inspect how activation vectors of whole layers of neural networks correspond to neural representations of abstract concepts"
  - [section] "SOMs tend to group together in a given area of the map examples (in our cases, activation vectors) that are similar"
  - [corpus] Weak - corpus papers discuss SOM visualization but not specifically for neural network concept analysis
- Break condition: If activation vectors are too dispersed or SOM parameters poorly tuned, spatial clustering fails to emerge.

### Mechanism 3
- Claim: Layer-wise SOM analysis reveals concept abstraction progression.
- Mechanism: Lower layers capture basic features while higher layers capture abstract concepts. By analyzing SOMs at each layer, we can track how concept representations evolve from specific to abstract.
- Core assumption: Neural networks learn hierarchical representations where earlier layers capture raw features and later layers capture semantic abstractions.
- Evidence anchors:
  - [abstract] "understand its importance in solving the prediction task at hand" and "where the representation of those concepts might be located in the network"
  - [section] "This hypothesis is based on the idea that the different layers of the network have a role to progressively abstract the examples"
  - [corpus] Weak - corpus papers discuss neural network layers but not specifically SOM-based concept abstraction analysis
- Break condition: If network architecture doesn't support meaningful abstraction (e.g., shallow networks) or if training fails to learn hierarchical features.

## Foundational Learning

- Concept: Self-Organizing Maps and competitive learning
  - Why needed here: SOMs are the core visualization and analysis tool that converts high-dimensional activation vectors into interpretable 2D maps
  - Quick check question: How does the "winning neuron" selection in SOMs differ from typical neural network activation?

- Concept: Relative entropy (Kullback-Leibler divergence)
  - Why needed here: This is the primary quantitative measure for comparing concept-specific activation patterns against baseline patterns
  - Quick check question: What does it mean if relative entropy between concept and base SOMs is zero?

- Concept: Activation vector aggregation across dimensions
  - Why needed here: Different layer types (CNN, RNN, FC) produce activation tensors of varying shapes that must be flattened or aggregated for SOM input
  - Quick check question: Why would you use mean aggregation for LSTM layers but flattening for convolutional layers?

## Architecture Onboarding

- Component map:
  - Data preprocessing → Model inference → Activation extraction → SOM construction → Frequency matrix computation → Metric calculation → Visualization
  - Key components: SOM training (minisom), activation aggregation functions, relative entropy computation

- Critical path:
  1. Extract activation vectors from all layers for entire dataset
  2. Build base SOMs using these vectors
  3. Extract activation vectors for concept-specific subsets
  4. Populate base SOMs with concept activations
  5. Compute frequency matrices and relative entropy
  6. Analyze layer-wise patterns

- Design tradeoffs:
  - SOM grid size (15x15 fixed here) vs computational cost and resolution
  - Aggregation method choice affects concept discrimination
  - Relative entropy vs other metrics for concept strength assessment

- Failure signatures:
  - Flat frequency matrices indicate poor SOM training or homogeneous activations
  - Inconsistent metric behavior across layers suggests concept extraction issues
  - High computational cost for large networks/datasets

- First 3 experiments:
  1. Test SOM training on random activation vectors to verify clustering behavior
  2. Compare relative entropy vs simple distance metrics on toy concept separation
  3. Validate layer-wise abstraction hypothesis using known hierarchical concepts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the method be extended to automatically identify unknown concepts in neural networks without prior knowledge of what concepts to look for?
- Basis in paper: [explicit] The paper mentions this as an exciting future direction: "However, a key question is whether, in training to make a particular prediction, the network could have identified particular properties of the considered objects that group them into meaningful but not yet identified concepts."
- Why unresolved: The paper only demonstrates the method for known, pre-labeled concepts. There's no established methodology for discovering previously unknown concepts that the network may have implicitly learned.
- What evidence would resolve it: Development of an unsupervised or semi-supervised approach that can cluster activation patterns and identify coherent concept representations without requiring pre-labeled concept data.

### Open Question 2
- Question: How do the identified concept representations vary when applying the same method to networks trained on different datasets or with different architectures?
- Basis in paper: [inferred] The paper tests the method on two different models (text classification and image regression) but doesn't compare how concept representations differ across architectures or datasets systematically.
- Why unresolved: The paper demonstrates the method works across different architectures but doesn't explore the variability or generalizability of concept representations across different training regimes.
- What evidence would resolve it: A systematic study comparing concept representation patterns across multiple architectures, datasets, and training conditions to identify invariant versus context-dependent representations.

### Open Question 3
- Question: What is the relationship between the strength of concept representation in intermediate layers and the network's generalization performance on unseen data?
- Basis in paper: [inferred] The paper discusses how concept representation evolves through layers but doesn't examine whether stronger representations in certain layers correlate with better predictive performance or generalization.
- Why unresolved: The paper focuses on interpretability and identifying concept representations but doesn't establish a connection between these representations and model performance metrics.
- What evidence would resolve it: Correlation studies between relative entropy values in different layers and model performance metrics (accuracy, generalization gap, etc.) across multiple trained networks.

## Limitations
- The method relies heavily on relative entropy as the sole quantitative metric, which may not capture all aspects of concept representation strength
- Effectiveness across different network architectures and tasks beyond the two tested use cases remains uncertain
- Layer-wise abstraction hypothesis, while theoretically grounded, lacks extensive empirical validation across diverse model architectures

## Confidence
- High Confidence: The SOM-based visualization approach for concept representation is well-established and the methodology for building and analyzing SOMs is sound.
- Medium Confidence: The use of relative entropy as a quantitative measure shows consistent results in the presented experiments, but its general effectiveness across diverse scenarios requires further validation.
- Low Confidence: The layer-wise abstraction hypothesis, while theoretically grounded, lacks extensive empirical validation across different model architectures and tasks.

## Next Checks
1. Apply the method to transformer-based models and compare concept representation patterns with traditional CNN/LSTM architectures
2. Evaluate alternative divergence measures (Jensen-Shannon, total variation) against relative entropy to assess metric sensitivity and robustness
3. Design experiments with known hierarchical concepts to systematically verify progressive abstraction patterns across network layers