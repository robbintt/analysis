---
ver: rpa2
title: 'MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition,
  Adaptability, Rationality and Collaboration'
arxiv_id: '2311.08562'
source_url: https://arxiv.org/abs/2311.08562
tags:
- player
- clue
- gpt-4
- undercover
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive benchmark framework to evaluate
  LLMs in multi-agent settings, focusing on judgment, reasoning, deception, self-awareness,
  cooperation, coordination, and rationality. The framework employs two social deduction
  games and three game theory scenarios, enhanced by a probabilistic graphical modeling
  (PGM) method.
---

# MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration

## Quick Facts
- arXiv ID: 2311.08562
- Source URL: https://arxiv.org/abs/2311.08562
- Reference count: 11
- Key outcome: PGM augmentation boosts LLM multi-agent capabilities by 50%, with GPT-4 achieving 63.5% win rate

## Executive Summary
This paper introduces a comprehensive benchmark framework to evaluate LLMs in multi-agent settings, focusing on judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. The framework employs two social deduction games and three game theory scenarios, enhanced by a probabilistic graphical modeling (PGM) method. The study evaluates seven LLMs, revealing a significant capability gap, with GPT-4+PGM outperforming others by achieving a 63.5% win rate and a 50% average boost in multi-agent capabilities.

## Method Summary
The evaluation framework tests seven LLMs (GPT-4, GPT-4-turbo, GPT-3.5-turbo, PaLM 2, Claude 2, Cohere, Llama-2-70B) across five game scenarios: two social deduction games (Chameleon, Undercover) and three game theory scenarios (Cost Sharing, Prisoner's Dilemma, Public Good). A PGM module generates belief state graphs from conversation history to enhance reasoning. The framework records win rates, judgment accuracy, reasoning correctness, deception success, self-awareness accuracy, collaboration success, coordination efficiency, and rationality metrics. Competitions are run with 20-21 configurations per scenario, with 200 simulations for certain games to balance role biases.

## Key Results
- GPT-4+PGM achieves a 63.5% win rate across all scenarios
- PGM augmentation boosts LLM multi-agent capabilities by 50% on average
- GPT-4 outperforms Llama-2-70B by over three times in overall multi-agent capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PGM augmentation boosts multi-agent reasoning and coordination.
- Mechanism: PGM embeds probabilistic graphical model memory into agent context, enabling two-hop understanding (own perspective + others' perspectives) before decision-making.
- Core assumption: LLM-generated PGM memory captures relevant belief states and can be used in downstream prompts without drift.
- Evidence anchors:
  - [abstract] "Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex social and cognitive dimensions."
  - [section] "We propose a two-hop understanding mechanism in our PGM memory design. The agent should analyze from its own perspective and perspective when it stands in other players’ shoes."
  - [corpus] Weak—no explicit PGM results in corpus, but related multi-agent reasoning works exist.
- Break condition: If PGM memory generation fails or diverges from actual context, decisions degrade or become incoherent.

### Mechanism 2
- Claim: Social deduction games (Chameleon, Undercover) elicit deception and self-awareness capabilities.
- Mechanism: Players are assigned roles without knowledge (chameleon/undercover), forcing inference, bluffing, and self-monitoring under partial information.
- Core assumption: LLMs can model hidden role states and generate plausible misinformation when prompted to "blend in."
- Evidence anchors:
  - [abstract] "We utilize two social deduction games alongside three game-theory scenarios to create diverse environments."
  - [section] "In the Chameleon and Undercover scenarios, we’ve noticed a consistent bias in competition outcomes…to rectify this imbalance…we carried out 200 game simulations."
  - [corpus] No direct corpus support; inference based on benchmark design.
- Break condition: If LLMs overfit to one role or fail to simulate deceptive reasoning, deception scores collapse.

### Mechanism 3
- Claim: Game theory scenarios (Cost Sharing, Prisoner's Dilemma, Public Good) quantify cooperation and rationality.
- Mechanism: Structured payoff matrices and negotiation rounds expose agents' willingness to cooperate vs. defect, with win rate tracking success.
- Core assumption: Agents treat the games as optimization problems and adjust strategies over rounds.
- Evidence anchors:
  - [abstract] "We utilize two social deduction games alongside three game-theory scenarios like Cost Sharing, Multi-player Prisoner’s Dilemma, and Public Good…"
  - [section] "In the scenarios, Cost-Sharing, Prisoner’s Dilemma, and Public Good, except for the final win rate, we have also recorded some details in all competitions."
  - [corpus] Weak—corpus contains no detailed game-theory agent results.
- Break condition: If agents ignore opponent strategies or ignore payoff structure, rationality metrics become meaningless.

## Foundational Learning

- Concept: Probabilistic graphical models (PGM)
  - Why needed here: PGM enables structured belief tracking across multiple agents, essential for multi-agent reasoning.
  - Quick check question: What is the difference between a Bayesian network and a Markov random field in representing dependencies?

- Concept: Social deduction mechanics
  - Why needed here: Understanding role concealment and inference is core to measuring deception and self-awareness.
  - Quick check question: In a game with one chameleon and two non-chameleons, what is the minimal clue set that can guarantee identification of the chameleon?

- Concept: Nash equilibrium in repeated games
  - Why needed here: Benchmarks use iterative games where equilibrium behavior informs rationality scores.
  - Quick check question: In a 3-player Prisoner's Dilemma with payoffs (5,3,0) for cooperate-defect combos, what is the symmetric Nash equilibrium strategy?

## Architecture Onboarding

- Component map:
  - Game engine -> LLM wrapper -> PGM module -> Evaluation harness
- Critical path: Topic → Role → Prompt → LLM call → Parse → Store context → Next round
- Design tradeoffs:
  - Use GPT-4 vs smaller models: Accuracy vs. cost
  - Fixed prompt templates vs. adaptive: Simplicity vs. flexibility
  - Synchronous vs. asynchronous agent turns: Determinism vs. realism
- Failure signatures:
  - Inconsistent PGM memory → incoherent agent actions
  - Overfitting to role bias → low deception scores
  - Score drift over rounds → rationality metrics invalid
- First 3 experiments:
  1. Run Chameleon with GPT-4 and GPT-3.5-turbo, compare judgment scores
  2. Enable PGM module on GPT-4, compare win rate in Undercover
  3. Vary payoff matrices in Prisoner's Dilemma, observe rationality metric changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of PGM on the generalization of LLMs across different multi-agent scenarios?
- Basis in paper: [explicit] The paper states that PGM enhancement boosts the abilities of all selected models by an average of 50% on multi-agent capabilities.
- Why unresolved: The paper does not provide a detailed analysis of how PGM affects the performance of LLMs in each specific scenario, such as Chameleon, Undercover, Cost Sharing, Prisoner's Dilemma, and Public Good.
- What evidence would resolve it: A detailed breakdown of the performance of LLMs with and without PGM enhancement in each scenario, along with an analysis of the consistency of the improvement across scenarios.

### Open Question 2
- Question: How does the arithmetic awareness of LLMs affect their performance in Public Good scenarios?
- Basis in paper: [explicit] The paper notes that except for GPT-4, other LLMs often invest more than the total available points in Public Good scenarios, indicating a lack of arithmetic awareness.
- Why unresolved: The paper does not explore the underlying reasons for this arithmetic awareness gap or its implications on the overall performance of LLMs in multi-agent settings.
- What evidence would resolve it: An investigation into the arithmetic capabilities of LLMs and their correlation with performance in scenarios requiring precise calculations, such as Public Good.

### Open Question 3
- Question: What are the long-term effects of PGM enhancement on the strategic decision-making abilities of LLMs in competitive environments?
- Basis in paper: [explicit] The paper mentions that PGM-aware agents use sophisticated strategies, such as giving deceptive clues in Undercover, to improve their performance.
- Why unresolved: The paper does not discuss the sustainability or potential drawbacks of relying on PGM for strategic decision-making over extended periods or in more complex environments.
- What evidence would resolve it: A longitudinal study comparing the performance of PGM-enhanced and non-enhanced LLMs in a series of increasingly complex and competitive scenarios, assessing their adaptability and strategic evolution over time.

## Limitations
- PGM method implementation details are not fully specified, limiting reproducibility
- LLM performance may vary significantly across different topic settings due to context sensitivity
- Arithmetic awareness gaps in smaller LLMs affect performance in resource allocation games

## Confidence
- **High Confidence**: GPT-4 outperforming smaller models in multi-agent tasks, as evidenced by the 63.5% win rate and three-fold advantage over Llama-2-70B
- **Medium Confidence**: PGM enhancement contributing to improved reasoning and coordination, based on the described mechanism but lacking direct implementation details
- **Low Confidence**: Generalizability of results across different topic settings and role assignments, due to potential LLM sensitivity to context variations

## Next Checks
1. Reproduce PGM Memory Generation: Implement a simplified version of the PGM method described and test its impact on LLM performance in one social deduction game (e.g., Chameleon)
2. Topic Sensitivity Analysis: Run multiple iterations of the benchmark with varied topics and role assignments to quantify the stability of LLM performance across different contexts
3. Mechanism Isolation Test: Compare LLM performance with and without PGM enhancement in a controlled Prisoner's Dilemma scenario to isolate the specific contribution of the PGM method to rationality scores