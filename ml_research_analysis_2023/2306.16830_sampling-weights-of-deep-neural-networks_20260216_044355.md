---
ver: rpa2
title: Sampling weights of deep neural networks
arxiv_id: '2306.16830'
source_url: https://arxiv.org/abs/2306.16830
tags:
- sampling
- networks
- layer
- sampled
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SWIM, a sampling-based method for training
  fully connected neural networks. Instead of iterative optimization, SWIM constructs
  weights and biases by sampling pairs of data points, using their differences and
  function values.
---

# Sampling weights of deep neural networks

## Quick Facts
- arXiv ID: 2306.16830
- Source URL: https://arxiv.org/abs/2306.16830
- Authors: 
- Reference count: 40
- Primary result: Sampling-based method achieves comparable accuracy to gradient-based training but orders of magnitude faster

## Executive Summary
This paper introduces SWIM, a sampling-based method for training fully connected neural networks that constructs weights and biases by sampling pairs of data points. Instead of iterative optimization, SWIM uses data-driven weight construction and solves a least squares problem for the final layer. The method achieves comparable accuracy to gradient-based methods but is significantly faster, while being invariant to rigid body transformations and scaling of input data. Theoretical analysis proves SWIM networks are universal approximators with O(m^(-1/2)) L2-approximation errors for Barron functions.

## Method Summary
SWIM constructs fully connected neural networks by sampling pairs of data points from the training set and using their differences to create weights and biases for each neuron. The sampling distribution prioritizes pairs that are close in input space but have large differences in output space. For each neuron, a weight vector is constructed as the normalized difference between two data points, and the bias is set as the inner product of this weight with one of the points. The final layer weights and biases are determined by solving a least squares optimization problem. This approach eliminates the need for iterative gradient descent and achieves invariance to rigid body transformations and scaling of input data.

## Key Results
- SWIM achieves comparable accuracy to gradient-based methods but is orders of magnitude faster
- The method is invariant to rigid body transformations and scaling of input data
- Theoretical analysis proves SWIM networks are universal approximators with O(m^(-1/2)) L2-approximation errors for Barron functions
- Experiments demonstrate effectiveness on classification benchmarks, neural operators for PDEs, and transfer learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The sampling method constructs weights and biases by using differences between data point pairs and their function values, directly targeting regions of high gradient.
- **Mechanism**: For each neuron, a weight vector is constructed as the normalized difference between two data points, and the bias is set as the inner product of this weight with one of the points. This places activation functions precisely where they can contribute most to approximating the target function.
- **Core assumption**: The target function has regions of significant gradient that can be captured by appropriately positioned activation functions.
- **Evidence anchors**:
  - [abstract]: "we use both the input and the output training data to sample both shallow and deep networks"
  - [section]: "we employ and analyze a recently introduced weight construction technique [22] that uses the direction between pairs of data points to construct individual weights and biases"
  - [corpus]: Weak evidence; related work focuses on initialization or Bayesian sampling, not data-driven weight construction.
- **Break condition**: If the target function is extremely flat across the input space, the gradient-based sampling would not have meaningful directions to exploit, reducing the advantage over random sampling.

### Mechanism 2
- **Claim**: The sampling distribution prioritizes point pairs that are close in input space but have large differences in output space, making efficient use of network capacity.
- **Mechanism**: The probability distribution over point pairs is proportional to the output difference divided by the input distance (Equation 2), ensuring weights are sampled where they matter most for approximation accuracy.
- **Core assumption**: The target function's complexity is concentrated in regions where small input changes produce large output changes.
- **Evidence anchors**:
  - [abstract]: "Our sampling scheme is invariant to rigid body transformations and scaling of the input data"
  - [section]: "The distribution also leads to invariance to orthogonal transformations and scaling of the input data"
  - [corpus]: Weak evidence; no direct comparison to alternative sampling distributions in related work.
- **Break condition**: If the target function is uniformly complex across all input regions, this adaptive sampling would not provide an advantage over uniform sampling.

### Mechanism 3
- **Claim**: The method maintains universal approximation capability despite the constrained weight construction, because the biases can be adjusted to compensate for the directional constraints on weights.
- **Mechanism**: The theoretical proofs (Theorem 1 and Theorem 4) show that even with weights constrained to be differences between data points, the network can still approximate any continuous function arbitrarily well by adjusting biases and using sufficient neurons.
- **Core assumption**: The input space contains enough diverse point pairs to construct the necessary weight directions for approximation.
- **Evidence anchors**:
  - [section]: "We prove that the sampled networks we construct are universal approximators"
  - [section]: "For Barron functions, we show that the L2-approximation error of sampled shallow networks decreases with the square root of the number of neurons"
  - [corpus]: Weak evidence; related work on random features doesn't establish universal approximation for their methods.
- **Break condition**: If the input data lacks sufficient diversity (e.g., all points lie on a low-dimensional manifold), the weight construction may not span the necessary directions for approximation.

## Foundational Learning

- **Concept**: Universal approximation theorem for neural networks
  - Why needed here: The paper builds on and extends this foundational result to the constrained weight construction of sampled networks
  - Quick check question: Can a single hidden layer network with sigmoid activation approximate any continuous function on a compact domain? (Answer: Yes, by Cybenko's theorem)

- **Concept**: Barron space and approximation rates
  - Why needed here: The paper establishes L2 approximation error bounds for sampled networks on Barron functions, connecting to this theoretical framework
  - Quick check question: What is the convergence rate of the L2 approximation error for Barron functions using standard neural networks with respect to the number of neurons? (Answer: O(1/√m))

- **Concept**: Random feature models and their limitations
  - Why needed here: The paper contrasts its data-driven sampling approach with random feature models that use data-agnostic distributions
  - Quick check question: Why do random feature models often require many more neurons than gradient-based trained networks to achieve similar accuracy? (Answer: Because they don't target important regions of the input space)

## Architecture Onboarding

- **Component map**: Data preprocessing -> Weight sampling -> Linear optimization -> Activation functions
- **Critical path**:
  1. Sample point pairs according to the distribution (Equation 2)
  2. Construct weights as normalized differences and biases as inner products
  3. Pass data through hidden layers to form the design matrix
  4. Solve least squares problem for final layer
  5. Evaluate accuracy and optionally fine-tune

- **Design tradeoffs**:
  - Speed vs accuracy: More neurons improve accuracy but increase computation time quadratically
  - ReLU vs tanh: ReLU is simpler and theoretically cleaner; tanh may work better for classification
  - Depth vs width: Deeper networks can capture more complex functions but require more computation per layer

- **Failure signatures**:
  - Poor accuracy despite many neurons: Likely indicates the target function doesn't have exploitable gradient structure
  - High variance in results: May indicate insufficient data diversity for meaningful point pair sampling
  - Memory issues: Large hidden layers create large design matrices for the least squares problem

- **First 3 experiments**:
  1. Replicate Barron function approximation (Figure 3) with sine activation to verify O(1/√m) convergence
  2. Test classification on a simple dataset (e.g., Iris) to verify invariance properties eliminate need for preprocessing
  3. Compare sampling vs Adam training on a small CNN transfer learning task to verify speed advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical guarantees for the sampling distribution used in SWIM to be optimal or near-optimal for various types of target functions beyond Barron functions?
- Basis in paper: [explicit] The paper proves that for Barron functions, the L2 approximation error of sampled shallow networks decreases with the square root of the number of neurons. However, the paper does not provide theoretical guarantees for the optimality of the sampling distribution for other types of functions.
- Why unresolved: The theoretical analysis is limited to Barron functions, and extending the analysis to other function classes would require new mathematical techniques and insights.
- What evidence would resolve it: Proving bounds on the approximation error for other function classes, such as Sobolev or Hölder spaces, or providing empirical evidence that the sampling distribution is near-optimal for a wide range of functions.

### Open Question 2
- Question: How does the performance of SWIM scale with the dimensionality of the input space, and are there any dimensionality-dependent bottlenecks or limitations?
- Basis in paper: [inferred] The paper shows that SWIM can achieve comparable accuracy to iterative training methods, but the experiments are limited to relatively low-dimensional problems. The theoretical analysis does not provide insights into how the method scales with dimensionality.
- Why unresolved: The paper does not investigate the behavior of SWIM in high-dimensional settings, and it is unclear whether the method suffers from the curse of dimensionality or if there are any inherent limitations in high-dimensional spaces.
- What evidence would resolve it: Conducting experiments with high-dimensional datasets and comparing the performance of SWIM to iterative methods. Analyzing the computational complexity of SWIM in high dimensions and identifying potential bottlenecks.

### Open Question 3
- Question: Can SWIM be extended to more complex neural network architectures, such as convolutional neural networks (CNNs) or transformers, and what are the challenges in doing so?
- Basis in paper: [explicit] The paper focuses on fully connected neural networks and mentions that extending SWIM to other architectures, such as CNNs or transformers, is an open issue.
- Why unresolved: The sampling approach used in SWIM relies on constructing weights and biases from pairs of data points, which may not be directly applicable to architectures with more complex structures, such as CNNs or transformers.
- What evidence would resolve it: Developing and testing SWIM extensions for CNNs or transformers on benchmark datasets. Analyzing the theoretical properties of the sampling approach in the context of these architectures and identifying any limitations or challenges.

## Limitations

- Theoretical analysis is limited to Barron functions and doesn't cover highly non-smooth or discontinuous functions
- Quadratic scaling with respect to the number of neurons in the least squares problem could become prohibitive for very large networks
- Empirical validation is limited to specific datasets and PDE operators, not covering all potential applications

## Confidence

- **High confidence**: Universal approximation capability and L2 error bounds for Barron functions (supported by theoretical proofs)
- **Medium confidence**: Speed advantage over gradient-based methods (supported by experiments but limited to specific cases)
- **Medium confidence**: Invariance properties eliminating need for preprocessing (theoretically proven but empirical validation could be more extensive)

## Next Checks

1. Test SWIM on a discontinuous function (e.g., Heaviside step function) to verify the universal approximation claim extends beyond smooth functions
2. Compare the quadratic scaling behavior empirically across different network sizes and datasets to identify practical limits
3. Evaluate SWIM's performance on a benchmark of non-Barron functions to assess the tightness of the theoretical error bounds