---
ver: rpa2
title: 'Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft
  and Hard Label Prediction'
arxiv_id: '2303.02468'
source_url: https://arxiv.org/abs/2303.02468
tags:
- soft
- activation
- labels
- function
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of activation functions on soft
  and hard label prediction in subjective tasks with annotator disagreement. The authors
  propose a sinusoidal activation function (SSF) for output layers in BERT-based models,
  comparing it to sigmoid and post-training step functions.
---

# Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction

## Quick Facts
- arXiv ID: 2303.02468
- Source URL: https://arxiv.org/abs/2303.02468
- Reference count: 9
- Primary result: SSF activation achieves 5.52% and 1.57% higher F1-scores than sigmoid for hard label prediction on ArMIS and MD-Agreement datasets respectively

## Executive Summary
This paper investigates the impact of activation functions on soft and hard label prediction in subjective tasks with annotator disagreement. The authors propose a sinusoidal activation function (SSF) for output layers in BERT-based models, comparing it to sigmoid and post-training step functions. Their approach trains only on soft labels, then rounds predictions to infer hard labels. Experiments across four datasets show sigmoid performs best for soft label prediction, while SSF achieves the highest F1-scores for hard label prediction on two datasets (ArMIS and MD-Agreement), improving results by 5.52% and 1.57% respectively over sigmoid.

## Method Summary
The authors implement a BERT-based model with dense layers and dropout for soft and hard label prediction in subjective tasks. They compare three activation functions at the output layer: sigmoid, sinusoidal step function (SSF), and post-training step function. The models are trained for 100 epochs on soft labels using cross-entropy loss, then rounded to infer hard labels. The approach is tested on four datasets (HS-Brexit, ConvAbuse, MD-Agreement, ArMIS) with varying numbers of annotators per instance.

## Key Results
- Sigmoid activation performs best for soft label prediction across most datasets
- SSF activation achieves 5.52% higher F1-score than sigmoid on ArMIS dataset for hard label prediction
- SSF activation achieves 1.57% higher F1-score than sigmoid on MD-Agreement dataset for hard label prediction
- Post-training step function performs poorly for both soft and hard label prediction

## Why This Works (Mechanism)

### Mechanism 1
The sinusoidal activation function (SSF) improves hard label prediction by creating steeper gradients near valid soft label values. SSF is designed with varying slopes that decrease near the discrete valid soft label values (e.g., 0, 0.33, 0.66, 1.0 for 3 annotators) and increase between these values. This encourages the model to converge predictions toward these discrete values rather than intermediate values.

### Mechanism 2
Sigmoid activation performs better for soft label prediction because its continuous output matches the continuous nature of cross-entropy loss. Sigmoid produces smooth, continuous outputs between 0 and 1 that align with the assumptions of cross-entropy loss, allowing for more precise gradient updates during training on soft labels.

### Mechanism 3
Training only on soft labels and rounding for hard labels preserves model performance without explicit hard label supervision. The model learns to predict the underlying distribution of annotator agreement through soft labels, and rounding captures the majority vote without requiring direct binary supervision during training.

## Foundational Learning

- Concept: Soft labels vs hard labels
  - Why needed here: Understanding the distinction between continuous soft labels (representing annotator disagreement) and discrete hard labels (majority vote) is fundamental to the task.
  - Quick check question: What are the valid soft label values when you have 4 annotators and each can vote 0 or 1?

- Concept: Activation function properties
  - Why needed here: Different activation functions have different gradient properties that affect how well models learn to predict discrete vs continuous targets.
  - Quick check question: Why might a step function be problematic as an activation function during training?

- Concept: Cross-entropy loss and probability distributions
  - Why needed here: The choice of loss function interacts with activation function properties and affects model training outcomes.
  - Quick check question: What property of sigmoid makes it well-suited for cross-entropy loss?

## Architecture Onboarding

- Component map: BERT preprocessor -> BERT encoder -> Dense(20, ReLU) -> Dense(1, activation)
- Critical path: Input text preprocessing using BERT preprocessor → BERT encoding and pooling → Dense layers with ReLU activation → Output layer with chosen activation function → Soft label prediction (used for training) → Hard label prediction (rounded output)
- Design tradeoffs:
  - Sigmoid: Better for soft label prediction (continuous outputs), but may not concentrate predictions at valid discrete values
  - SSF: Better for hard label prediction (encourages discrete values), but only works when annotator count is constant
  - Post-training step: Simple but not optimized during training, leading to poor performance
- Failure signatures: High cross-entropy loss on soft labels indicates poor calibration; large gap between soft and hard label performance suggests rounding is losing information; SSF performs poorly on datasets with variable annotator counts
- First 3 experiments: 1) Compare sigmoid vs SSF on a dataset with constant annotator count, measuring both soft loss and hard F1; 2) Test sigmoid performance on the same dataset to establish baseline for soft label prediction; 3) Apply the post-training step function to sigmoid outputs to see if rounding alone can match SSF performance

## Open Questions the Paper Calls Out

### Open Question 1
Does the Sinusoidal Step Function (SSF) activation outperform sigmoid in other subjective tasks beyond those studied in this paper? The paper only tested SSF on four specific datasets across three subjective tasks, leaving open whether the performance gains are generalizable to other domains.

### Open Question 2
What is the optimal slope parameter θ for the SSF activation function across different datasets and tasks? The paper fixed θ = 0.05 for all experiments without exploring how varying this parameter affects performance across different datasets with varying numbers of annotators.

### Open Question 3
How does SSF activation perform when the number of annotators varies across instances within a dataset? The paper explicitly notes that SSF is only applicable to datasets where the number of annotators is constant, and it was not tested on ConvAbuse which has variable annotator counts.

## Limitations
- The study compares only three activation functions despite the vast space of possible alternatives
- Results are based on four specific datasets with varying properties, limiting generalizability
- Exact implementation details of the Sinusoidal Step Function are not fully specified

## Confidence
- High Confidence: Sigmoid activation performs best for soft label prediction across datasets
- Medium Confidence: SSF activation improves hard label prediction by 5.52% on ArMIS and 1.57% on MD-Agreement compared to sigmoid
- Medium Confidence: Training only on soft labels and rounding for hard labels preserves model performance

## Next Checks
1. Conduct an ablation study on SSF parameters (a, b, c) to determine optimal configurations for different datasets
2. Test cross-dataset transferability by training on one dataset and evaluating on another
3. Implement and compare additional activation functions (Swish, ELU, tanh) to establish whether SSF's advantages are unique