---
ver: rpa2
title: 'Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational
  Agents'
arxiv_id: '2311.09513'
source_url: https://arxiv.org/abs/2311.09513
tags:
- passage
- passages
- text
- user
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper describes a TREC iKAT 2023 submission that outperforms
  median runs by large margins (e.g., 280-433% higher success rates, 40-100% higher
  NDCG scores). The authors propose a hybrid method combining LLMs, retrieval (BM25),
  and classification (logistic regression) in a "Generate-Retrieve-Generate" pipeline:
  first using LLMs to generate an initial answer, then retrieving supporting passages,
  filtering them, and finally using LLMs again to synthesize a grounded response.'
---

# Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents

## Quick Facts
- **arXiv ID**: 2311.09513
- **Source URL**: https://arxiv.org/abs/2311.09513
- **Reference count**: 6
- **Primary result**: Outperforms median TREC iKAT 2023 runs by 280-433% higher success rates and 40-100% higher NDCG scores

## Executive Summary
This paper presents a TREC iKAT 2023 submission that introduces a "Generate-Retrieve-Generate" (GRG) pipeline, achieving state-of-the-art performance by reversing the traditional retrieve-then-generate order. The approach uses LLMs to generate initial responses, retrieves supporting passages using BM25, filters them with logistic regression, and synthesizes a final grounded answer. Experiments show that sequencing LLMs before retrieval yields significantly better results than the reverse order, with a one-shot variant without filtering performing best. The method demonstrates the importance of answer generation preceding retrieval in conversational agents.

## Method Summary
The Generate-Retrieve-Generate pipeline combines conversational and retrieval technologies in a novel sequence: first using LLMs to generate an initial answer from user context, then retrieving supporting passages from ClueWeb-22B using BM25, filtering them with logistic regression to remove low-quality content, optimizing sentence relevance with Sentence-BERT, summarizing with FastChat-T5, and finally synthesizing a grounded response with Llama. The approach challenges the conventional retrieve-then-generate paradigm by demonstrating that LLM-generated responses provide richer semantic context that improves downstream retrieval effectiveness.

## Key Results
- Outperforms median TREC iKAT 2023 runs by 280-433% higher success rates and 40-100% higher NDCG scores
- One-shot GRG variant (without logistic regression filtering) performs best among tested configurations
- Sequencing LLMs before retrieval yields significantly better results than retrieve-then-generate approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate high-context initial responses that increase relevance of downstream retrieval
- Mechanism: Generating an LLM answer first creates a detailed, user-aligned context that BM25 can better match against than a short user utterance
- Core assumption: LLM-generated responses contain richer semantic content that improves retrieval recall
- Evidence anchors:
  - [abstract] "we've found to greatly outpace Retrieve-Then-Generate approaches"
  - [section] "We encourage the LLM to generate a lengthy response, in order to get more information that can be used to find grounding materials"
  - [corpus] Weak - related papers focus on conversational retrieval but don't directly test generate-then-retrieve ordering
- Break condition: If LLM hallucinates extensively, retrieval quality degrades despite higher context richness

### Mechanism 2
- Claim: Filtering passages with logistic regression removes low-quality or spam content before summarization
- Mechanism: TF-IDF + logistic regression classifier removes irrelevant or malicious passages, improving downstream summarization quality
- Core assumption: ClueWeb-22B contains noisy content that degrades LLM performance if unfiltered
- Evidence anchors:
  - [section] "We trained a Logistic Regression text classification model using TF-IDF Vectorization... to weed out bad quality passages"
  - [section] "we found that the top 5 BM25 passages would almost always be from reputable sources"
  - [corpus] Weak - no direct evaluation of filtering impact on final scores in related work
- Break condition: If logistic regression has high false positive rate, relevant passages are incorrectly discarded

### Mechanism 3
- Claim: Sentence-BERT ranking improves passage summarization by prioritizing relevant sentences
- Mechanism: Re-ranking sentences by semantic similarity to user utterance before summarization focuses the T5 model on relevant content
- Core assumption: Original passage ordering doesn't align with user information needs
- Evidence anchors:
  - [section] "we optimized the top 5 passages by ranking the sentences in order of relevance to write another prompt, again using Sentence-BERT"
  - [section] "Sentence-Ranking has been used before to increase the quality and relevance of text extraction"
  - [corpus] Weak - related papers discuss retrieval but not sentence-level re-ranking for summarization
- Break condition: If sentence re-ranking introduces context fragmentation, summarization coherence suffers

## Foundational Learning

- **Concept**: BM25 scoring and term frequency-inverse document frequency
  - Why needed here: Understanding how BM25 weights query terms is critical for predicting retrieval effectiveness after LLM generation
  - Quick check question: What happens to BM25 scores when a query contains terms that appear in many documents versus rare terms?

- **Concept**: TF-IDF vectorization for text classification
  - Why needed here: The logistic regression filter relies on TF-IDF features to distinguish reliable from unreliable passages
  - Quick check question: How does TF-IDF handle common stopwords versus domain-specific terms in spam detection?

- **Concept**: Sentence transformer embeddings and cosine similarity
  - Why needed here: Sentence-BERT ranking requires understanding how semantic similarity scores map to sentence relevance
  - Quick check question: What range of cosine similarity values typically indicate semantically similar sentences in QA contexts?

## Architecture Onboarding

- **Component map**: User utterance → Sentence-BERT PTKB ranking → Llama generation → BM25 retrieval → Logistic regression filtering → Sentence-BERT passage optimization → FastChat-T5 summarization → Llama final answer synthesis
- **Critical path**: Generate → Retrieve → Generate pipeline where each LLM step depends on the previous retrieval output
- **Design tradeoffs**: Multiple LLM calls increase conversationality but add latency; filtering improves quality but risks removing relevant content
- **Failure signatures**: Gibberish outputs indicate token limit exceeded; irrelevant passages suggest BM25 mismatch; poor summaries indicate FastChat context overflow
- **First 3 experiments**:
  1. Compare single-shot vs two-shot generation cycles to quantify self-correction benefit
  2. Test logistic regression filtering on vs off to measure impact on final scores
  3. Validate sentence-BERT ranking by comparing summaries with and without sentence re-ordering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Generate-Retrieve-Generate (GRG) approach compare to Retrieve-Then-Generate (RTG) in terms of user satisfaction and conversational quality, beyond objective metrics like nDCG and Success_1?
- Basis in paper: [explicit] The paper explicitly compares GRG to RTG, finding GRG outperforms RTG by significant margins (280-433% higher success rates, 40-100% higher NDCG scores).
- Why unresolved: The paper focuses on objective metrics but doesn't explore subjective measures of user satisfaction or conversational quality, which are crucial for conversational agents.
- What evidence would resolve it: User studies or surveys measuring perceived helpfulness, fluency, and overall satisfaction with responses generated by GRG versus RTG approaches.

### Open Question 2
- Question: What is the optimal number of retrieval-generate cycles (/u1D44B) for the GRG approach, and how does it vary depending on the complexity of the user query and the size of the knowledge base?
- Basis in paper: [explicit] The paper mentions testing /u1D44B= 1 and /u1D44B= 2, finding that one-shot methods (Run 2 & 3) beat two-shot methods (Run 1), but doesn't explore other values or provide a theoretical framework for determining the optimal number of cycles.
- Why unresolved: The optimal number of cycles likely depends on various factors, and the paper only provides limited empirical evidence for two specific values.
- What evidence would resolve it: Extensive experiments testing a range of cycle numbers (e.g., /u1D44B= 0, 1, 2, 3, 4) across diverse query types and knowledge base sizes, coupled with theoretical analysis of the trade-offs involved.

### Open Question 3
- Question: How can the GRG approach be adapted to handle more complex conversational scenarios, such as multi-turn dialogues, follow-up questions, and implicit user intents?
- Basis in paper: [inferred] The paper focuses on single-turn responses and doesn't address the challenges of maintaining conversational context and handling more nuanced user interactions.
- Why unresolved: Conversational agents need to handle complex dialogues, and the current GRG approach may not be sufficient for these scenarios without modifications.
- What evidence would resolve it: Experiments demonstrating the GRG approach's effectiveness in multi-turn dialogues, including metrics for tracking conversational coherence, context awareness, and the ability to handle follow-up questions and implicit intents.

## Limitations
- Limited ablation studies prevent definitive attribution of success to specific pipeline components
- Logistic regression filtering lacks direct quantitative evidence of its impact on final evaluation metrics
- No user studies measuring subjective conversational quality or satisfaction

## Confidence
- **High confidence** in overall pipeline effectiveness and TREC iKAT 2023 results
- **Medium confidence** in the specific advantage of LLM-first sequencing over retrieval-first approaches
- **Low confidence** in the individual contribution of the logistic regression filtering step without direct ablation evidence

## Next Checks
1. **Ablation study of sequencing order**: Run controlled experiments comparing Generate-Retrieve-Generate against Retrieve-Then-Generate using identical model configurations and evaluation metrics to isolate the sequencing effect.

2. **Logistic regression filter impact analysis**: Conduct a quantitative study measuring success_1 and nDCG scores with filtering enabled versus disabled, and analyze the precision/recall trade-offs of the filtering threshold.

3. **Token limit boundary testing**: Systematically test Llama generation performance at various input token thresholds (1024, 1536, 2048, 2560) to identify the exact point where gibberish generation begins and optimize prompt length accordingly.