---
ver: rpa2
title: Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed
  multi-armed bandits
arxiv_id: '2305.06743'
source_url: https://arxiv.org/abs/2305.06743
tags:
- stochastic
- heavy-tailed
- algorithm
- clipping
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-armed bandits (MAB)
  in the presence of heavy-tailed reward distributions, where traditional algorithms
  relying on bounded rewards or sub-Gaussian assumptions fail. The authors propose
  a new algorithm, Implicitly Normalized Forecaster with Clipping (INF-clip), which
  adapts the Implicitly Normalized Forecaster (INF) algorithm for adversarial MAB
  to the heavy-tailed stochastic setting.
---

# Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits

## Quick Facts
- arXiv ID: 2305.06743
- Source URL: https://arxiv.org/abs/2305.06743
- Authors: 
- Reference count: 40
- Primary result: Proposed INF-clip algorithm achieves optimal regret bounds for heavy-tailed stochastic multi-armed bandits with clipping-based gradient normalization

## Executive Summary
This paper addresses the challenge of multi-armed bandits with heavy-tailed reward distributions, where traditional algorithms fail due to unbounded moments. The authors propose INF-clip, which adapts the Implicitly Normalized Forecaster (INF) algorithm using a clipping strategy instead of truncation to handle large gradients. This approach retains more information from extreme values while controlling variance, leading to improved performance especially for heavier tails. The algorithm achieves optimal regret bounds of O(Mn^α/(1+α)T^1/(1+α)) for both linear and non-linear bandit settings under mild moment conditions.

## Method Summary
INF-clip is an online learning algorithm that combines online mirror descent with Tsallis entropy and clipped gradient estimates. Instead of truncating large gradients as in previous heavy-tailed approaches, INF-clip replaces them with clipped versions, preserving more information while bounding the impact of extreme values. The algorithm sets the clipping threshold λ = T^(1/(1+α))·(2α/(1-α))^(2/(1+α))·(8n)^(1/(1+α))·M and learning rate µ = √(2√T λ^(1-α)M^(1+α)), where M characterizes the heavy-tail behavior and α ∈ (0,1] determines tail heaviness. For non-linear settings, the algorithm uses smoothed loss estimates via uniform sampling on a sphere before applying clipping and OMD updates.

## Key Results
- INF-clip achieves optimal regret O(Mn^α/(1+α)T^1/(1+α)) for linear heavy-tailed stochastic MABs
- The algorithm outperforms best-of-both-worlds methods, particularly when tail heaviness parameter α is small
- Extends to non-linear bandit settings with gradient-free feedback and adversarial noise
- Clipping strategy provides better data utilization than truncation while maintaining theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: INF-clip replaces truncation with clipping, retaining large gradient samples rather than discarding them
- Mechanism: Each gradient component is clipped to threshold λ before OMD update, bounding variance while allowing clipped values to contribute information
- Core assumption: E[||gt||1+α] ≤ M1+α ensures clipping bias is controlled
- Evidence anchors: Abstract mentions clipping strategy; Lemma 3.3 shows bias E[|gt,i - clip(gt,i, λ)] ≤ 2α+1Mα+1/λα

### Mechanism 2
- Claim: Achieves optimal regret through careful tuning of λ and µ
- Mechanism: Threshold λ = T1/(1+α)·(2α/(1-α))2/(1+α)·(8n)1/(1+α)·M balances bias-variance tradeoff
- Core assumption: Heavy-tail parameter α is known for optimal parameter setting
- Evidence anchors: Abstract states O(Mn^α/(1+α)T^1/(1+α)) bound; Theorem 3.1 derives exact constants matching lower bounds

### Mechanism 3
- Claim: Extends to non-linear bandits with gradient-free feedback and adversarial noise
- Mechanism: Uses smoothed loss estimates via uniform sphere sampling, then clips gradients before OMD with 1-strongly convex prox function
- Core assumption: Loss functions are Lipschitz or smooth with bounded adversarial noise
- Evidence anchors: Abstract mentions nonlinear extension; Theorem 4.1 provides regret bounds for Lipschitz and smooth cases

## Foundational Learning

- Concept: Multi-armed bandit problem formulation and regret definition
  - Why needed here: Understanding the online learning setup is prerequisite to following algorithm and analysis
  - Quick check question: In a two-arm bandit with means 0.7 and 0.6, what is optimal cumulative reward over T rounds?

- Concept: Online Mirror Descent (OMD) with Tsallis entropy
  - Why needed here: INF-clip uses OMD with Tsallis entropy; understanding this divergence is crucial
  - Quick check question: For Tsallis entropy ψq(x) = (1/(1-q))(1 - ∑xi^q), what is Bregman divergence Bψq(x,y)?

- Concept: Heavy-tailed distributions and moment conditions
  - Why needed here: Algorithm guarantees depend on 1+α moment condition; understanding this is key to interpreting results
  - Quick check question: If X ~ Pareto(α), what is E[|X|1+α] for α ∈ (0,1]?

## Architecture Onboarding

- Component map: Bandit feedback -> Clipped gradient estimator -> OMD with Tsallis entropy -> Probability distribution over arms
- Critical path: 1) Sample arm a_t from x_t, 2) Observe loss l_t(a_t), 3) Construct clipped importance-weighted gradient ĝ_t, 4) Update via OMD with Tsallis entropy and ĝ_t, 5) Repeat
- Design tradeoffs:
  - Clipping threshold λ: Larger reduces bias but increases variance; smaller does opposite
  - Learning rate µ: Balances convergence speed and stability; tuned as function of λ and problem parameters
  - Tsallis entropy parameter q: Affects update geometry; q close to 1 recovers INF
- Failure signatures: λ too small → high bias/poor performance; µ too large → instability; q far from 1 → degraded bounds
- First 3 experiments: 1) Verify clipped gradient estimator unbiasedness, 2) Measure bias-variance tradeoff as λ varies on synthetic data, 3) Compare regret to HTINF and UCB on two-arm heavy-tailed bandit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does INF-clip performance scale with dimensionality of arm space?
- Basis in paper: [inferred] Paper discusses linear/nonlinear heavy-tailed MABs but doesn't analyze scaling with arm space dimensionality
- Why unresolved: Focuses on regret bounds rather than dimensional analysis
- What evidence would resolve it: Empirical results or theoretical analysis showing scaling behavior with arm space dimensionality

### Open Question 2
- Question: Can INF-clip be extended to contextual bandits with heavy-tailed rewards?
- Basis in paper: [inferred] Focuses on standard MAB, not contextual settings
- Why unresolved: No theoretical analysis or empirical results on contextual extensions
- What evidence would resolve it: Analysis or results demonstrating effectiveness in contextual bandit settings

### Open Question 3
- Question: How does choice of clipping parameter λ affect performance in practice?
- Basis in paper: [explicit] Mentions λ = T^(1/(1+α))·(2α/(1-α))^(2/(1+α))·(8n)^(1/(1+α))·M but doesn't analyze impact
- Why unresolved: No empirical results or theoretical analysis on sensitivity to λ
- What evidence would resolve it: Results showing impact of different λ choices on performance

## Limitations
- Requires known α parameter for optimal clipping threshold and learning rate setting
- Assumes 1+α moment exists, which may not hold for extremely heavy tails
- Experimental validation limited to specific heavy-tailed distributions

## Confidence
- Theorem 3.1 (linear regret bound): High - follows standard techniques with careful bias-variance analysis
- Theorem 4.1 (nonlinear extension): Medium - additional complexity from gradient-free feedback and Lipschitz/smoothness assumptions
- Experimental results: Medium - shown but not extensively validated across diverse distributions

## Next Checks
1. Implement two-phase algorithm that first estimates α from data then runs INF-clip with estimated parameter, measuring regret degradation
2. Test INF-clip on distributions with infinite 1+α moments (α > 1) to empirically verify claimed limitations
3. Compare bias-variance tradeoff of clipping versus truncation (HTINF) across range of tail parameters on synthetic data