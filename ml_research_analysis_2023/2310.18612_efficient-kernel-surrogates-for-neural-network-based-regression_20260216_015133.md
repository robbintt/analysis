---
ver: rpa2
title: Efficient kernel surrogates for neural network-based regression
arxiv_id: '2310.18612'
source_url: https://arxiv.org/abs/2310.18612
tags:
- training
- kernel
- function
- test
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the use of Conjugate Kernel (CK) approximations
  as efficient surrogates for Neural Tangent Kernel (NTK) approximations in neural
  network-based regression and classification tasks. The authors establish theoretical
  bounds for the relative test losses of CK and NTK approximations, demonstrating
  that CK performance is only marginally worse than NTK in certain cases.
---

# Efficient kernel surrogates for neural network-based regression

## Quick Facts
- arXiv ID: 2310.18612
- Source URL: https://arxiv.org/abs/2310.18612
- Reference count: 40
- Primary result: CK approximations provide computationally efficient alternatives to NTK with comparable accuracy in certain cases

## Executive Summary
This paper investigates Conjugate Kernel (CK) approximations as efficient surrogates for Neural Tangent Kernel (NTK) approximations in neural network-based regression and classification tasks. The authors establish theoretical bounds showing that CK performance is only marginally worse than NTK, and in some cases superior, particularly when using ReLU activations. Through numerical experiments on function regression and logistic regression problems, they demonstrate that CK approximations can significantly improve accuracy over trained neural networks while being computationally cheaper. The approach is also applied to improve the performance of GPT-2 for sentiment classification.

## Method Summary
The method involves using CK approximations as surrogates for NTK in kernel-based regression and classification tasks. For function regression, smooth target functions are approximated using CK and NTK, with performance compared to trained neural networks. In logistic regression, synthetic datasets are used to compare CK and NTK approximations against neural network baselines. The approach is extended to improve GPT-2's sentiment classification by applying linear probing and fine-tuning methods. The theoretical analysis establishes bounds for relative test losses of CK and NTK approximations, demonstrating their near-equivalence in certain cases.

## Key Results
- CK approximations achieve accuracy comparable to NTK while being computationally cheaper
- For ReLU activations, CK provides better generalization than NTK due to continuity properties
- CK improves GPT-2 sentiment classification performance while reducing overfitting risk
- Theoretical bounds show relative test loss between CK and NTK is bounded by network width and depth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CK approximates NTK because both depend only on the output of the last hidden layer for fully connected networks
- Mechanism: In the infinite width limit, NTK decomposes as NTK(z,ez) = CK(z,ez) + E(z,ez), where E represents contributions from all layers except the last. CK, using only the last layer's parameters, captures the dominant behavior of NTK.
- Core assumption: Contributions from earlier layers (E) are either small or similar across different networks
- Evidence anchors: Abstract states CK performance is only marginally worse than NTK; Section 3 describes CK as a "zeroth-order" approximation to NTK
- Break condition: If network architecture changes significantly (e.g., convolutional layers), last layer's contribution may no longer dominate

### Mechanism 2
- Claim: CK provides better generalization than NTK with ReLU activations due to continuity properties
- Mechanism: NTK feature map contains discontinuities from ReLU derivatives, leading to overfitting. CK feature map is continuous even with ReLU, avoiding this issue and providing more stable predictions.
- Core assumption: Continuity of feature map components is important for generalization, especially with ReLU activations
- Evidence anchors: Section 6 explains NTK performance may be inferior with ReLU due to discontinuities in feature map components
- Break condition: If dataset is extremely clean and large, continuity advantage may become negligible compared to NTK's increased expressivity

### Mechanism 3
- Claim: Training only the last layer of a neural network can achieve accuracy comparable to full network training
- Mechanism: Last hidden layer's activations already capture most relevant features for the task. By training only the final layer, we efficiently adjust the decision boundary without needing to relearn feature representations.
- Core assumption: Feature representations learned by the last hidden layer are sufficiently good for the task, requiring only minor adjustments to the final layer
- Evidence anchors: Section 7 demonstrates training only the last layer can achieve accuracy comparable to full network training
- Break condition: If task requires significant feature engineering or initial network was poorly trained, retraining only the last layer may not be sufficient

## Foundational Learning

- Concept: Kernel methods and representer theorem
  - Why needed here: Understanding how kernel machines approximate functions and the relationship between feature maps and kernels is crucial for grasping why CK and NTK work as surrogates
  - Quick check question: Can you explain why the representer theorem guarantees that kernel solutions can be expressed as linear combinations of kernel evaluations at training points?

- Concept: Neural Tangent Kernel (NTK) and its properties
  - Why needed here: The NTK framework explains why infinite-width neural networks behave like kernel machines, providing the theoretical foundation for using kernel approximations
  - Quick check question: What is the key property of the NTK in the infinite width limit that allows it to have a known closed form?

- Concept: Conjugate Kernel (CK) and its relationship to NTK
  - Why needed here: Understanding how CK approximates NTK and its computational advantages is central to the paper's contribution
  - Quick check question: How does the CK differ from the NTK in terms of which parameters it considers in the Jacobian?

## Architecture Onboarding

- Component map:
  - Neural Network (NN) -> Neural Tangent Kernel (NTK) -> Conjugate Kernel (CK) -> Kernel Approximation -> Linear combination of kernel evaluations at training points
  - Jacobian Computation -> Efficient NTK computation algorithm

- Critical path:
  1. Train or load pre-trained neural network
  2. Extract last hidden layer activations
  3. Compute CK or NTK
  4. Solve kernel regression/classification problem
  5. Evaluate on test data

- Design tradeoffs:
  - Accuracy vs. Computational Cost: NTK is more accurate but expensive; CK is cheaper with marginal accuracy loss
  - Expressivity vs. Generalization: NTK may overfit with ReLU; CK provides better generalization
  - Training Time vs. Inference Time: Full training is slow; last-layer training is fast but may require good initial features

- Failure signatures:
  - Poor generalization with ReLU activations: Indicates NTK overfitting due to discontinuities
  - High test error despite good training error: Suggests need for better regularization or feature learning
  - Extremely slow computation: Implies inefficient NTK computation algorithm

- First 3 experiments:
  1. Compare NN, CK, and NTK performance on a simple regression task with smooth target function
  2. Test CK vs. NTK with ReLU activations to observe generalization differences
  3. Implement efficient NTK computation algorithm and benchmark against naive approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Conjugate Kernel (CK) approximations compare to Neural Tangent Kernel (NTK) approximations for ReLU activation functions?
- Basis in paper: [explicit] The paper states that NTK approximations outperform CK approximations when ReLU activations are used, due to discontinuities in the feature map components of NTK
- Why unresolved: The paper provides numerical evidence for this claim but does not offer a theoretical explanation or proof
- What evidence would resolve it: A rigorous mathematical proof demonstrating why ReLU activations lead to inferior performance of CK approximations compared to NTK approximations

### Open Question 2
- Question: Can the proposed approach of using CK approximations to improve DNN accuracy be extended to other types of neural networks beyond fully connected networks (FCNs)?
- Basis in paper: [inferred] The paper focuses on FCNs and demonstrates the effectiveness of CK approximations for these networks. However, the potential applicability to other architectures is not explicitly explored
- Why unresolved: The paper does not investigate the performance of CK approximations on other types of neural networks, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)
- What evidence would resolve it: Numerical experiments comparing the performance of CK approximations on various neural network architectures, including CNNs and RNNs

### Open Question 3
- Question: What are the theoretical limits of using CK approximations to improve DNN accuracy, and under what conditions do they break down?
- Basis in paper: [explicit] The paper establishes bounds for the relative test losses of CK and NTK approximations, demonstrating that CK performance is only marginally worse than NTK in certain cases. However, it does not provide a comprehensive analysis of the limitations of CK approximations
- Why unresolved: The paper does not explore the theoretical limits of CK approximations or identify the conditions under which they fail to improve DNN accuracy
- What evidence would resolve it: A theoretical analysis identifying the conditions under which CK approximations fail to improve DNN accuracy, along with numerical experiments demonstrating these limitations

## Limitations

- Theoretical analysis primarily focuses on fully-connected networks with Tanh activations, while practical applications often use ReLU
- Experiments are limited to relatively simple function regression and logistic regression tasks, with limited testing on complex real-world datasets
- GPT-2 sentiment classification results rely on a small human-annotated subset (1500 samples) that may not be representative

## Confidence

- **High confidence**: Theoretical bounds for relative test losses (Theorem 3.1 and 3.2) are mathematically rigorous and well-established
- **Medium confidence**: Empirical results on function regression and logistic regression tasks, as they show consistent trends but are limited to specific test functions and synthetic datasets
- **Low confidence**: GPT-2 sentiment classification results due to small sample size and potential selection bias in the human-annotated subset

## Next Checks

1. Replicate the NTK vs CK comparison on larger, more diverse real-world datasets (e.g., CIFAR-10, ImageNet) to verify generalization claims
2. Conduct ablation studies varying network depth, width, and activation functions to test the robustness of CK approximations across different architectures
3. Implement and benchmark the proposed efficient NTK computation algorithm (Algorithm 1 or 2) to verify the claimed computational advantages in practice