---
ver: rpa2
title: 'Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained
  Devices'
arxiv_id: '2309.06612'
source_url: https://arxiv.org/abs/2309.06612
tags:
- fusion
- multimodal
- search
- unimodal
- harmonic-nas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Harmonic-NAS introduces a two-tier optimization framework to jointly
  design unimodal backbones and multimodal fusion networks for efficient deployment
  on resource-constrained devices. The first tier uses evolutionary search to explore
  optimal unimodal backbones for each modality, while the second tier employs differentiable
  NAS to find the best fusion architecture tailored to the selected backbones.
---

# Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices

## Quick Facts
- arXiv ID: 2309.06612
- Source URL: https://arxiv.org/abs/2309.06612
- Reference count: 7
- Key outcome: Up to 10.9% accuracy improvement, 1.91× latency reduction, and 2.14× energy efficiency gain on edge devices.

## Executive Summary
Harmonic-NAS introduces a two-tier optimization framework to jointly design unimodal backbones and multimodal fusion networks for efficient deployment on resource-constrained devices. The first tier uses evolutionary search to explore optimal unimodal backbones for each modality, while the second tier employs differentiable NAS to find the best fusion architecture tailored to the selected backbones. Hardware efficiency (latency and energy) is integrated into both stages via specialized loss functions. Evaluated across three multimodal datasets (AV-MNIST, MM-IMDB, and HARM-P) on Jetson AGX Xavier and TX2 devices, Harmonic-NAS achieves up to 10.9% accuracy improvement, 1.91× latency reduction, and 2.14× energy efficiency gain over state-of-the-art methods.

## Method Summary
Harmonic-NAS employs a two-tier optimization approach for efficient multimodal network design. The first tier uses evolutionary search to find optimal unimodal backbones for each modality by sampling from a once-for-all supernet trained with knowledge distillation. The second tier uses differentiable NAS (DARTS) to search for the best fusion architecture conditioned on the selected backbones, with hardware-aware loss functions incorporating latency and energy metrics from device-specific lookup tables. This framework enables fully searchable multimodal networks optimized for both accuracy and hardware efficiency on target edge devices.

## Key Results
- Achieves up to 10.9% accuracy improvement over state-of-the-art multimodal methods.
- Reduces latency by 1.91× through hardware-aware optimization.
- Improves energy efficiency by 2.14× on Jetson AGX Xavier and TX2 devices.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical two-tier search enables specialized unimodal backbones and fusion networks tailored to multimodal tasks and hardware constraints.
- Mechanism: Harmonic-NAS first uses evolutionary search to explore unimodal backbone architectures, then uses differentiable NAS to find optimal fusion networks conditioned on the selected backbones. This separation allows each stage to optimize for its own domain—feature extraction for each modality and cross-modality integration—while maintaining hardware awareness throughout.
- Core assumption: Optimal unimodal backbones and optimal fusion networks can be searched independently while still achieving globally optimal multimodal performance.
- Evidence anchors:
  - [abstract]: "Harmonic-NAS involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators."
  - [section 3]: Describes the evolutionary search stage for unimodal backbones and the differentiable DARTS stage for fusion networks, with hardware loss integrated into both.
  - [corpus]: Weak evidence; no direct citations of similar hierarchical search frameworks in multimodal NAS.
- Break condition: If the unimodal backbone performance heavily depends on fusion architecture choices, or if fusion effectiveness requires tightly coupled backbone parameters, the independence assumption fails.

### Mechanism 2
- Claim: Weight-sharing via once-for-all supernets dramatically reduces search cost while preserving backbone diversity.
- Mechanism: A single supernet is trained with shared weights for all candidate unimodal backbones. Subnets are sampled during training using knowledge distillation and the sandwich rule, ensuring coverage of the search space without retraining each candidate separately.
- Core assumption: Weight-sharing via a supernet preserves sufficient architectural diversity to find high-performing backbones for multimodal tasks.
- Evidence anchors:
  - [section 3.1.1]: Describes the supernet design and training using knowledge distillation and the sandwich rule.
  - [section 3.1.2]: Explains the sampling of subnets from the trained supernet for evolutionary search.
  - [corpus]: Weak evidence; related work mentions once-for-all frameworks but not specifically for multimodal backbone search.
- Break condition: If weight-sharing causes catastrophic interference between subnet architectures, the quality of backbones found may degrade significantly.

### Mechanism 3
- Claim: Hardware-aware loss functions in both search stages ensure final multimodal models meet latency and energy constraints on target devices.
- Mechanism: Hardware metrics (latency, energy) are measured via device-specific lookup tables and incorporated into the loss functions of both the evolutionary backbone search and the DARTS fusion search, guiding the search toward Pareto-optimal solutions.
- Core assumption: Hardware lookup tables accurately predict real device performance for arbitrary architectures sampled during search.
- Evidence anchors:
  - [abstract]: "Hardware efficiency (latency and energy) is integrated into both stages via specialized loss functions."
  - [section 3.1.2]: Mentions device-specific lookup tables for latency and energy in the evaluation function.
  - [section 3.2.2]: Details hardware loss computation using lookup tables for fusion operators.
  - [corpus]: No direct evidence; the assumption relies on lookup table accuracy.
- Break condition: If lookup tables do not generalize to novel architectures, hardware predictions will be inaccurate, leading to poor deployment performance.

## Foundational Learning

- Concept: Neural Architecture Search (NAS) fundamentals
  - Why needed here: Harmonic-NAS builds on both evolutionary and differentiable NAS; understanding their differences and how they can be combined is critical.
  - Quick check question: What is the key difference between evolutionary NAS and differentiable NAS in terms of search strategy and computational cost?

- Concept: Hardware-aware NAS and device-specific optimization
  - Why needed here: Hardware efficiency (latency, energy) is a primary optimization objective, requiring knowledge of how neural network operations map to specific device characteristics.
  - Quick check question: How do lookup tables help estimate hardware performance during NAS, and what are their limitations?

- Concept: Multimodal feature fusion strategies
  - Why needed here: The fusion network must effectively combine heterogeneous unimodal features; understanding fusion operators and their hardware costs is essential.
  - Quick check question: What are the trade-offs between different fusion operators (e.g., sum, attention, concatenation) in terms of expressiveness and hardware efficiency?

## Architecture Onboarding

- Component map: Supernet Training Module -> Evolutionary Backbone Search -> DARTS Fusion Search -> Evaluation and Selection
- Critical path:
  1. Train unimodal supernets (image, audio, text).
  2. Run evolutionary search to sample and evaluate unimodal backbones.
  3. Select top-performing backbones for fusion search.
  4. Execute DARTS-based fusion search with hardware-aware loss.
  5. Evaluate and select Pareto-optimal multimodal models.
- Design tradeoffs:
  - Backbone search independence vs. fusion coupling: Searching backbones first may miss synergies but enables modularity and reuse.
  - Search space granularity vs. computational cost: Finer-grained search spaces (e.g., more operators, larger backbones) increase accuracy potential but explode search time.
  - Hardware metric accuracy vs. generalization: Detailed lookup tables improve predictions but may not generalize to novel architectures.
- Failure signatures:
  - Poor unimodal backbone performance despite high search space coverage → weight-sharing or sampling issues.
  - Fusion search converges to trivial architectures → loss function weighting or operator set problems.
  - Hardware predictions mismatch real deployment → lookup table inaccuracies or unaccounted platform effects.
- First 3 experiments:
  1. Validate supernet training: Sample multiple subnets, measure accuracy and diversity, compare against independent training baseline.
  2. Test backbone search effectiveness: Run evolutionary search on a small dataset, analyze convergence and selected backbones' unimodal performance.
  3. Evaluate fusion search with fixed backbones: Fix unimodal backbones, run DARTS fusion search, measure improvement over fixed fusion baselines.

## Open Questions the Paper Calls Out

- How does the performance of Harmonic-NAS compare when using different backbone architectures (e.g., ResNet, EfficientNet) instead of the MobileNet-v3 based supernet?
- How does the hardware efficiency of Harmonic-NAS models vary across different edge devices with varying computational capacities (e.g., low-power microcontrollers, high-performance GPUs)?
- How does the fusion operator choice impact the performance of Harmonic-NAS models across different multimodal datasets and tasks?

## Limitations
- Hardware-aware NAS framework relies heavily on lookup tables for latency and energy predictions, which may not generalize to architectures beyond the training set.
- Independence assumption between unimodal backbone and fusion network optimization could limit global search effectiveness.
- Claimed improvements over state-of-the-art methods lack ablation studies to isolate contributions of the two-tier approach versus individual components.

## Confidence
- **High Confidence**: The two-tier optimization framework (evolutionary + differentiable NAS) is technically sound and well-motivated by existing literature.
- **Medium Confidence**: Hardware-aware loss integration and the reported efficiency gains are plausible but depend on lookup table accuracy and real device measurements not fully disclosed.
- **Low Confidence**: The claimed 10.9% accuracy improvement and 1.91× latency reduction are difficult to verify without access to the exact hardware lookup tables and full experimental details.

## Next Checks
1. Reconstruct the hardware lookup tables by profiling a representative subset of unimodal backbones and fusion operators on target devices, then validate prediction accuracy against real measurements.
2. Run ablation studies comparing: (a) joint backbone-fusion search vs. two-tier approach, (b) with and without hardware-aware loss, and (c) different fusion operator sets to isolate contribution margins.
3. Test the weight-sharing supernet's architectural diversity by sampling 100 random subnets, measuring unimodal performance distribution, and comparing against independently trained baselines.