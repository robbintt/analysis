---
ver: rpa2
title: Dissociating language and thought in large language models
arxiv_id: '2301.06627'
source_url: https://arxiv.org/abs/2301.06627
tags:
- language
- https
- linguistic
- linguistics
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that large language models (LLMs) have made\
  \ significant progress in mastering formal linguistic competence\u2014the rules\
  \ and patterns of language\u2014but still fall short in functional linguistic competence,\
  \ which requires real-world understanding and reasoning. Drawing on evidence from\
  \ cognitive neuroscience, the authors show that language processing and general\
  \ cognition rely on distinct brain mechanisms, supporting the distinction between\
  \ these two types of competence."
---

# Dissociating language and thought in large language models

## Quick Facts
- arXiv ID: 2301.06627
- Source URL: https://arxiv.org/abs/2301.06627
- Reference count: 40
- One-line primary result: LLMs excel at formal linguistic competence (rules and patterns) but struggle with functional linguistic competence (real-world understanding and reasoning).

## Executive Summary
This paper argues that large language models (LLMs) like GPT-3 have made significant progress in mastering formal linguistic competence—the rules and patterns of language—but still fall short in functional linguistic competence, which requires real-world understanding and reasoning. Drawing on evidence from cognitive neuroscience, the authors show that language processing and general cognition rely on distinct brain mechanisms, supporting the distinction between these two types of competence. The authors propose that future models should adopt a modular architecture and diverse training objectives to integrate language processing with non-linguistic cognitive abilities, and they call for separate benchmarks to assess progress in both formal and functional linguistic competence.

## Method Summary
The paper conducts a theoretical analysis of large language models' capabilities in formal versus functional linguistic competence. It reviews cognitive neuroscience evidence showing dissociable brain networks for language processing versus general cognition. The authors examine LLM performance on various benchmarks measuring formal competence (syntax, semantics, hierarchical structure) and functional competence (reasoning, world knowledge, pragmatics). They propose a modular architecture framework and call for development of separate evaluation metrics for each type of competence.

## Key Results
- LLMs like GPT-3 excel at generating grammatically correct text and demonstrate knowledge of hierarchical structure and abstract rules
- LLMs struggle with reasoning, world knowledge, and pragmatic understanding despite strong formal competence
- The human brain has specialized language networks that process linguistic rules independently from multiple-demand networks handling reasoning and world knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can achieve formal linguistic competence by learning statistical patterns in language data without needing real-world grounding.
- Mechanism: LLMs use next-word prediction on massive text corpora to internalize hierarchical structures and abstract grammatical rules, enabling fluent sentence generation that mirrors human linguistic patterns.
- Core assumption: Linguistic competence can be acquired purely from distributional patterns without external cognitive systems.
- Evidence anchors:
  - [abstract] "LLMs like GPT-3 excel at generating grammatically correct and syntactically coherent text, demonstrating knowledge of hierarchical structure and abstract rules"
  - [section] "LLMs achieve at least some degree of abstraction... These models have succeeded not just on tests of general language understanding developed by the NLP community"
  - [corpus] Weak - corpus lacks direct evidence for abstract rule learning; relies on inference from benchmark performance
- Break condition: When models encounter out-of-distribution reasoning tasks that require non-linguistic knowledge, performance degrades significantly.

### Mechanism 2
- Claim: Human language processing and general cognition rely on dissociable neural mechanisms, supporting the distinction between formal and functional competence.
- Mechanism: The human brain has a specialized language network that processes linguistic rules independently from multiple-demand networks that handle reasoning, world knowledge, and social cognition.
- Core assumption: Cognitive neuroscience findings about human brain organization apply to artificial language models as a conceptual framework.
- Evidence anchors:
  - [abstract] "Drawing on evidence from cognitive neuroscience, the authors show that language processing and general cognition rely on distinct brain mechanisms"
  - [section] "A wealth of evidence from cognitive science and neuroscience has established that language and thought in humans are robustly dissociable"
  - [corpus] Moderate - corpus provides supporting studies but limited direct evidence for LLM architecture implications
- Break condition: If future neuroscience reveals significant overlap between language and reasoning networks, the framework would need revision.

### Mechanism 3
- Claim: LLMs fail at functional linguistic competence because they lack the non-linguistic cognitive capacities required for real-world language use.
- Mechanism: Without integrated systems for world knowledge, formal reasoning, situation modeling, and social inference, LLMs can only simulate language competence through pattern matching.
- Core assumption: Functional competence requires cognitive modules beyond pure language processing.
- Evidence anchors:
  - [abstract] "LLMs... struggle with reasoning, world knowledge, and pragmatic understanding"
  - [section] "LLMs... fail on many tests requiring functional competence... they struggle when engaging in formal reasoning, fail to acquire comprehensive and consistent world knowledge"
  - [corpus] Moderate - corpus contains related work on LLM limitations but limited direct evidence for proposed cognitive architecture
- Break condition: If LLMs develop emergent reasoning capabilities through scale alone, the modular architecture assumption would be challenged.

## Foundational Learning

- Concept: Hierarchical linguistic structure
  - Why needed here: Understanding how words combine compositionally is fundamental to formal linguistic competence
  - Quick check question: Can you explain why "The keys to the cabinet are on the table" maintains subject-verb agreement despite intervening words?
- Concept: Statistical learning mechanisms
  - Why needed here: LLMs acquire linguistic knowledge through pattern recognition in text data
  - Quick check question: How might a model learn that "keys are" is more common than "keys is" without understanding grammar?
- Concept: Brain network specialization
  - Why needed here: The distinction between language and multiple-demand networks underpins the formal/functional competence framework
  - Quick check question: What evidence suggests that language processing occurs in brain regions separate from those used for reasoning?

## Architecture Onboarding

- Component map: Core language module (formal competence) + multiple non-linguistic modules (reasoning, world knowledge, situation modeling, social inference)
- Critical path: Data preprocessing → hierarchical structure learning → abstract rule acquisition → integration with non-linguistic modules
- Design tradeoffs: Pure language modeling vs. modular architectures that incorporate external cognitive systems
- Failure signatures: Grammatical fluency without semantic coherence, pattern matching without reasoning, inability to maintain context across long discourse
- First 3 experiments:
  1. Test hierarchical agreement in nested sentences with varying distances between subject and verb
  2. Evaluate world knowledge consistency across semantically equivalent prompts with different surface forms
  3. Measure context tracking ability across paragraph-length inputs to assess situation modeling capacity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural and training innovations would enable language models to develop functional linguistic competence while preserving their formal competence?
- Basis in paper: explicit
- Why unresolved: The paper identifies the need for modular architectures and diverse training objectives but does not specify which particular architectural designs or training approaches would be most effective.
- What evidence would resolve it: Empirical comparisons of different modular architectures and training paradigms on benchmarks measuring both formal and functional competence.

### Open Question 2
- Question: How much linguistic data do humans actually need to develop formal linguistic competence, and how does this compare to current language models?
- Basis in paper: explicit
- Why unresolved: The paper notes that current models require vastly more data than humans but doesn't provide a definitive answer on the minimal data requirements for human-like language learning.
- What evidence would resolve it: Detailed studies comparing human language acquisition with model performance across different training data sizes and distributions.

### Open Question 3
- Question: Can we develop benchmarks that effectively distinguish between statistical pattern matching and genuine reasoning in language models?
- Basis in paper: explicit
- Why unresolved: The paper identifies this as a key challenge but doesn't propose specific methodologies for creating such benchmarks.
- What evidence would resolve it: Development and validation of benchmark tasks that reliably differentiate between pattern-based and reasoning-based responses.

## Limitations

- The framework for dissociating formal and functional linguistic competence is largely conceptual with limited empirical validation
- The proposed modular architecture for improving functional competence remains speculative with no concrete implementation details or experimental results
- Assessment of functional competence relies on "hunch" evaluations and common-sense reasoning tests that may not capture the full complexity of real-world language use

## Confidence

- **High confidence**: The distinction between formal and functional linguistic competence as separate cognitive abilities (supported by extensive cognitive neuroscience literature)
- **Medium confidence**: LLMs demonstrate strong formal competence through statistical pattern learning (supported by benchmark performance but limited understanding of underlying mechanisms)
- **Medium confidence**: The modular architecture proposal for improving functional competence (conceptually sound but lacking empirical validation)
- **Low confidence**: Specific claims about which types of real-world knowledge and reasoning abilities are most critical for functional competence (based largely on intuition rather than systematic investigation)

## Next Checks

1. **Implement and test a modular architecture**: Design and train a transformer-based LLM with explicit modules for world knowledge, formal reasoning, and social inference, then evaluate whether this improves functional competence while maintaining formal competence. Compare performance to standard LLMs on both formal and functional benchmarks.

2. **Develop standardized functional competence benchmarks**: Create a comprehensive evaluation suite that systematically tests various aspects of functional competence (world knowledge, reasoning, pragmatics, situation modeling) using consistent metrics. Include both in-distribution and out-of-distribution test cases to identify brittleness.

3. **Conduct ablation studies on non-linguistic cognitive capacities**: Systematically remove or degrade specific types of world knowledge or reasoning abilities in LLMs and measure the impact on functional competence tasks. This would help identify which cognitive capacities are most critical for real-world language understanding.