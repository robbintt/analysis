---
ver: rpa2
title: Climate Change from Large Language Models
arxiv_id: '2312.11985'
source_url: https://arxiv.org/abs/2312.11985
tags:
- climate
- llms
- crisis
- knowledge
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating climate change
  knowledge within large language models (LLMs). The authors propose an automated
  evaluation framework that uses a hybrid approach to data acquisition, combining
  data synthesis and manual collection, to compile a diverse set of questions encompassing
  various aspects of climate change.
---

# Climate Change from Large Language Models

## Quick Facts
- arXiv ID: 2312.11985
- Source URL: https://arxiv.org/abs/2312.11985
- Reference count: 22
- Key outcome: This paper proposes an automated evaluation framework for assessing climate change knowledge in large language models, using a hybrid data acquisition approach and comprehensive metrics to evaluate both questions and answers across 10 distinct perspectives.

## Executive Summary
This paper addresses the challenge of evaluating climate change knowledge within large language models (LLMs) by proposing an automated evaluation framework. The authors develop a hybrid approach combining data synthesis and manual collection to create a diverse set of climate change questions. Using prompt engineering and multiple LLMs as evaluators, they assess knowledge quality across 10 distinct metrics spanning question and answer evaluation. The experimental results demonstrate that while state-of-the-art LLMs possess considerable climate-related knowledge, they exhibit shortcomings in timeliness, highlighting the need for continuous updating and refinement of climate-related content in these models.

## Method Summary
The authors employ a two-step hybrid data acquisition process: first generating questions using LLMs (primarily Llama2-70B), then manually curating and filtering the results to ensure quality and diversity. They design prompt templates to evaluate both questions and answers across 10 metrics (5 for each), including importance, clarity, relevance, difficulty, innovation for questions, and relevance, depth, readability, innovation, and timeliness for answers. Multiple LLMs (ChatGLM3-6B, Mistral-7B, Zephyr-7B, Baichuan2-13B, Yi-34B, Llama2-70B) serve as evaluators with temperature set to 0.5 and maximum length of 2048 tokens. Scores are aggregated by averaging across models and metrics to identify strengths and weaknesses in LLM climate knowledge.

## Key Results
- The evaluation framework successfully identified that LLMs possess substantial climate-related knowledge across multiple dimensions
- Timeliness emerged as a significant weakness, with LLMs struggling to provide up-to-date information reflecting current climate science developments
- The hybrid data acquisition approach produced a high-quality dataset of 19,241 questions and answers, with 95% auto-generated and 5% from Reddit
- Multiple LLM evaluators provided consistent scoring across most metrics, though individual models showed varying sensitivities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid data acquisition approach (combining data synthesis and manual collection) produces a high-quality, diverse set of climate change questions.
- Mechanism: Data synthesis generates a large volume of questions quickly, while manual collection ensures relevance and diversity. The two-stage process filters out irrelevant and duplicate questions, improving overall quality.
- Core assumption: Large language models can generate relevant climate change questions, and manual curation can effectively filter and refine these questions.
- Evidence anchors:
  - [abstract]: "We adopt a hybrid approach for data acquisition, combining data synthesis and manual collection, to compile a diverse set of questions encompassing various aspects of climate change."
  - [section]: "Our proposed method for acquiring a large number of questions about the climate crisis involves a two-step process: question generation and question selection."

### Mechanism 2
- Claim: The comprehensive set of metrics (5 for questions, 5 for answers) provides a nuanced and multifaceted evaluation of climate crisis knowledge in LLMs.
- Mechanism: Each metric targets a specific aspect of knowledge quality (e.g., importance, clarity, relevance, depth, timeliness). Multiple LLMs are used as evaluators to generate scores, reducing individual model bias.
- Core assumption: The proposed metrics accurately capture the essential aspects of climate crisis knowledge, and multiple LLMs can provide objective evaluations.
- Evidence anchors:
  - [abstract]: "We introduce a comprehensive set of metrics to assess climate-crisis knowledge, encompassing indicators from 10 distinct perspectives."
  - [section]: "We use several large language models for evaluation, with the temperature parameter set to 0.5 for all models, and a maximum length of 2048."

### Mechanism 3
- Claim: The automated evaluation framework can effectively identify shortcomings in LLMs' climate crisis knowledge, particularly in terms of timeliness.
- Mechanism: The framework evaluates both the quality of questions and answers, allowing for a comprehensive assessment of knowledge. The timeliness metric specifically addresses the need for up-to-date information in the context of climate change.
- Core assumption: LLMs' climate crisis knowledge can be effectively evaluated through their responses to questions, and timeliness is a critical aspect of this knowledge.
- Evidence anchors:
  - [abstract]: "We evaluate several state-of-the-art LLMs and find that their knowledge falls short in terms of timeliness, indicating a need for continuous updating and refinement of their climate-related content."
  - [section]: "To evaluate the quality of answers, we evaluate the following aspects: (5) Timeliness of the answer: Is the content of the answer up-to-date and able to adapt to changing circumstances and needs?"

## Foundational Learning

- Concept: Climate Change Knowledge
  - Why needed here: Understanding the key aspects of climate change (causes, impacts, mitigation, adaptation) is crucial for developing relevant questions and evaluating LLM responses.
  - Quick check question: What are the main causes of climate change, and what are some potential mitigation strategies?

- Concept: Large Language Models (LLMs)
  - Why needed here: Familiarity with LLMs' capabilities and limitations is essential for designing effective evaluation frameworks and interpreting results.
  - Quick check question: What are the key differences between autoregressive and autoencoding LLMs, and how might these differences affect their performance on climate change knowledge tasks?

- Concept: Prompt Engineering
  - Why needed here: Crafting effective prompts is crucial for eliciting relevant and high-quality responses from LLMs during the evaluation process.
  - Quick check question: How can prompt templates be designed to target specific aspects of climate change knowledge, such as the importance or timeliness of information?

## Architecture Onboarding

- Component map: Data Acquisition -> Prompt Engineering -> Evaluation -> Analysis
- Critical path: Data Acquisition → Prompt Engineering → Evaluation → Analysis
- Design tradeoffs:
  - Balancing the volume of synthesized questions with the quality ensured by manual curation
  - Choosing between a smaller number of highly specialized metrics versus a larger number of more general metrics
  - Selecting LLMs for evaluation based on their size, performance, and potential biases
- Failure signatures:
  - Low-quality questions or answers due to ineffective synthesis or curation
  - Inconsistent or biased evaluation results due to poorly designed metrics or biased LLMs
  - Inability to identify timeliness shortcomings due to inadequate question design or metric calibration
- First 3 experiments:
  1. Evaluate the quality of synthesized questions before and after manual curation to assess the effectiveness of the two-stage process.
  2. Test the proposed metrics on a small set of known climate change facts to ensure they accurately capture the intended aspects of knowledge.
  3. Compare the evaluation results of a single LLM versus multiple LLMs to assess the impact of using multiple evaluators on reducing bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed evaluation framework be further improved to address the limitations of LLMs in terms of timeliness of climate-related knowledge?
- Basis in paper: [explicit] The authors acknowledge that while LLMs possess considerable climate-related knowledge, there are shortcomings in terms of timeliness, indicating a need for continuous updating and refinement of their climate-related content.
- Why unresolved: The paper identifies the issue of timeliness but does not provide specific solutions for addressing this limitation in the evaluation framework.
- What evidence would resolve it: Development and testing of methods to enhance the framework's ability to evaluate the timeliness of LLM responses, such as incorporating real-time data sources or implementing continuous learning mechanisms.

### Open Question 2
- Question: How can the proposed framework be adapted to evaluate the knowledge of LLMs on other complex and rapidly evolving domains beyond climate change?
- Basis in paper: [inferred] The paper presents a methodology for evaluating climate crisis knowledge within LLMs, suggesting that the approach could potentially be applied to other domains.
- Why unresolved: The paper focuses specifically on climate change and does not explore the applicability of the framework to other domains.
- What evidence would resolve it: Demonstration of the framework's effectiveness in evaluating LLM knowledge on other complex and rapidly evolving domains, such as public health, technology, or social issues.

### Open Question 3
- Question: What are the potential biases and limitations of using LLMs as evaluators for assessing climate crisis knowledge, and how can these be mitigated?
- Basis in paper: [explicit] The authors mention that different LLMs may have different standards for evaluating climate crisis knowledge, suggesting potential biases in the evaluation process.
- Why unresolved: The paper does not delve into the specific biases and limitations of using LLMs as evaluators or provide strategies for mitigating these issues.
- What evidence would resolve it: Analysis of the biases and limitations of LLMs as evaluators, along with the development and testing of methods to reduce these biases and improve the reliability of the evaluation process.

## Limitations

- The hybrid data acquisition approach relies heavily on auto-generated questions (95%), raising concerns about potential bias in the dataset
- The evaluation metrics depend on LLM-based scoring rather than human judgment, creating uncertainty about whether they truly capture knowledge quality
- The timeliness metric is particularly vulnerable to evaluator bias, as it requires current knowledge of climate science developments

## Confidence

- High confidence: The general methodology of using multiple LLMs to evaluate climate knowledge is sound and well-executed
- Medium confidence: The effectiveness of the hybrid data acquisition approach in producing a high-quality, diverse question set
- Low confidence: The accuracy of the comprehensive metrics in capturing true climate knowledge quality, particularly the timeliness metric

## Next Checks

1. Conduct a small-scale human validation study of a random sample of questions and answers to compare against LLM-generated scores, particularly for the timeliness metric.

2. Test whether the observed timeliness shortcomings correlate with the publication dates of the training data for the evaluated LLMs, providing evidence for the mechanism behind this limitation.

3. Analyze the question dataset for systematic coverage gaps or biases (e.g., geographic, topical) that might affect the evaluation results, and test whether different synthesis approaches produce different bias patterns.