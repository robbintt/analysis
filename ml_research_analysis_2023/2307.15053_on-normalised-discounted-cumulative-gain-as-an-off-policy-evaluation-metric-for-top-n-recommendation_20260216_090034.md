---
ver: rpa2
title: On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric
  for Top-$n$ Recommendation
arxiv_id: '2307.15053'
source_url: https://arxiv.org/abs/2307.15053
tags:
- metric
- evaluation
- offline
- online
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work critically examines the use of (normalised) Discounted
  Cumulative Gain (nDCG) as an offline evaluation metric for top-n recommendation.
  It formally derives the assumptions under which DCG provides an unbiased estimate
  of online reward, contrasting this with the traditional IR use-case.
---

# On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top-$n$ Recommendation

## Quick Facts
- arXiv ID: 2307.15053
- Source URL: https://arxiv.org/abs/2307.15053
- Authors: 
- Reference count: 40
- Key outcome: This work critically examines the use of (normalised) Discounted Cumulative Gain (nDCG) as an offline evaluation metric for top-n recommendation. It formally derives the assumptions under which DCG provides an unbiased estimate of online reward, contrasting this with the traditional IR use-case. The paper proves that normalising DCG leads to inconsistencies, as nDCG can rank competing methods differently than DCG even when DCG is unbiased. Empirical results from a large-scale recommendation platform demonstrate that unbiased DCG strongly correlates with online metrics over time, whereas nDCG does not. Additionally, differences in online metrics directionally align with differences in both nDCG and DCG, but the latter exhibits improved sensitivity in detecting statistically significant online improvements.

## Executive Summary
This paper provides a formal analysis of Discounted Cumulative Gain (DCG) as an offline evaluation metric for top-n recommendation systems, distinguishing it from its traditional use in information retrieval. The authors derive the assumptions necessary for DCG to provide an unbiased estimate of online reward through importance sampling, and prove that normalising DCG (creating nDCG) can lead to inconsistent rankings even when DCG is unbiased. Through empirical validation on a large-scale recommendation platform, the paper demonstrates that unbiased DCG correlates more strongly with online metrics than nDCG, and that DCG provides better sensitivity in detecting significant improvements.

## Method Summary
The paper employs importance sampling to derive an unbiased estimator for DCG in recommendation systems, using logged data collected under a logging policy to estimate expected reward under a target policy. The method incorporates position-based models to account for position bias in user interactions, and can optionally de-bias interaction labels. The theoretical framework identifies five key assumptions: reward independence across trajectories, position-based model, reward independence across ranks, examination hypothesis, and full support of logging policy. The authors compare different variants of DCG (including nDCG) against online metrics from A/B tests to evaluate their effectiveness as offline evaluation metrics.

## Key Results
- Unbiased DCG provides stronger correlation with online metrics than nDCG over time
- Normalising DCG can cause inconsistent rankings between competing methods even when DCG is unbiased
- DCG shows improved sensitivity in detecting statistically significant online improvements compared to nDCG
- Learned position bias models and de-biased interaction labels improve correlation with online metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DCG provides an unbiased estimator of online reward when certain assumptions hold
- **Mechanism:** DCG is derived from first principles using importance sampling to reweight logged exposure under logging policies to estimate expected reward under target policies
- **Core assumption:** Full support of logging policy (every item with non-zero exposure under target has non-zero exposure under logging)
- **Evidence anchors:**
  - [abstract] "formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward"
  - [section 4] "Through Asm. 5 and Eq. 6, we formulate an importance sampling estimator for the reward under (G, R) given data collected under (G0, R0)"
  - [corpus] Weak - no direct evidence found
- **Break condition:** When logging policy has zero exposure for items that would be exposed under target policy

### Mechanism 2
- **Claim:** Normalising DCG (creating nDCG) can invert the relative ordering of competing methods compared to DCG
- **Mechanism:** Normalisation rescales metric values per sample, but when aggregated across multiple samples, this can lead to inconsistent orderings between DCG and nDCG
- **Core assumption:** Metric values from different samples should maintain consistent relative ordering when aggregated
- **Evidence anchors:**
  - [abstract] "normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order"
  - [section 5] "Lemma 5.2. The Discounted Cumulative Gain (DCG) and Normalised Discounted Cumulative Gain (nDCG) metrics yield inconsistent relative orders over a competing set of policies Ω"
  - [corpus] Weak - no direct evidence found
- **Break condition:** When metric values are unevenly distributed across samples or days

### Mechanism 3
- **Claim:** Using a learned position bias model and de-biased interaction labels improves correlation with online metrics compared to classical DCG formulation
- **Mechanism:** Learnt position bias models capture actual user behavior more accurately than theoretical logarithmic forms, and de-biasing interaction labels removes exposure bias
- **Core assumption:** Position bias model and de-biasing procedure accurately reflect true user behavior and item quality
- **Evidence anchors:**
  - [abstract] "empirical results from off- and on-line experiments on a large-scale recommendation platform show that: (a) the unbiased DCG metric strongly correlates with online metrics over time"
  - [section 6.1] "Results here align with what theory would suggest: the unbiased DCG variant that we have formally derived in Section 4 provides the strongest correlation with online reward"
  - [corpus] Weak - no direct evidence found
- **Break condition:** When position bias model is misspecified or de-biasing procedure introduces additional noise

## Foundational Learning

- **Concept: Importance Sampling**
  - Why needed here: Core technique used to derive unbiased DCG estimator by reweighting logged data
  - Quick check question: What is the key assumption that makes importance sampling unbiased?

- **Concept: Position-Based Models**
  - Why needed here: Models how user view probability depends on item rank, crucial for discount functions in DCG
  - Quick check question: How does the position-based model affect the calculation of exposure probabilities?

- **Concept: Counterfactual Evaluation**
  - Why needed here: Framework for evaluating policies using historical data collected under different policies
  - Quick check question: What is the difference between on-policy and off-policy evaluation?

## Architecture Onboarding

- **Component map:**
  - Logging policies (G0, R0) → Data collection → Offline evaluation (DCG/nDCG) → Correlation analysis with online metrics
  - Position bias models → Discount functions → Exposure probability calculations
  - De-biasing procedures → Quality estimation → Final reward estimation

- **Critical path:**
  1. Collect logged data under logging policies
  2. Estimate position bias model from logged data
  3. Calculate exposure probabilities for each item-context pair
  4. Compute DCG using importance sampling weights
  5. Compare with online metrics from A/B test

- **Design tradeoffs:**
  - Variance vs bias: Unbiased DCG has higher variance than nDCG due to importance weights
  - Model complexity: Learned position bias vs theoretical logarithmic discount
  - Data requirements: Need sufficient coverage for full support assumption

- **Failure signatures:**
  - Low correlation between offline and online metrics
  - High variance in DCG estimates
  - Inconsistent orderings between DCG and nDCG
  - Sensitivity to outliers in metric calculations

- **First 3 experiments:**
  1. Compare DCG correlation with online metrics using logarithmic vs learned position bias models
  2. Test effect of de-biasing interaction labels on offline-online correlation
  3. Vary clipping parameter for IPS weights and measure impact on sensitivity and variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does nDCG provide any practical value as an offline evaluation metric despite its theoretical inconsistencies with DCG?
- Basis in paper: [explicit] The paper shows nDCG has worse correlation with online metrics than DCG and lower sensitivity in detecting significant improvements, yet acknowledges it may still have some utility
- Why unresolved: The paper demonstrates nDCG's limitations but doesn't conclusively rule out scenarios where it might still be useful (e.g., when outliers are a major concern or when interpretability of bounded metrics is prioritized)
- What evidence would resolve it: Empirical studies comparing nDCG to DCG across diverse recommendation scenarios, particularly where outlier mitigation or bounded metrics are critical requirements

### Open Question 2
- Question: How can the assumptions underlying DCG as an unbiased estimator be relaxed or partially satisfied in real-world recommendation systems?
- Basis in paper: [explicit] The paper identifies five key assumptions (reward independence across trajectories, position-based model, reward independence across ranks, examination hypothesis, full support of logging policy) and discusses potential approaches to relax them
- Why unresolved: The paper outlines theoretical approaches to relax assumptions but doesn't provide comprehensive empirical validation of these relaxation methods across different recommendation contexts
- What evidence would resolve it: Extensive experimental studies evaluating the performance of DCG-based estimators under varying degrees of assumption violation, using different relaxation techniques and recommendation domains

### Open Question 3
- Question: What offline evaluation metrics would be most appropriate for reinforcement learning-based recommendation systems that violate DCG's assumptions?
- Basis in paper: [inferred] The paper notes that RL scenarios require different evaluation approaches and that DCG's assumptions (particularly reward independence across trajectories and ranks) are incompatible with RL
- Why unresolved: While the paper identifies the need for different metrics in RL contexts, it doesn't propose or evaluate specific alternatives that could handle the complex dependencies and state transitions in RL-based recommendation
- What evidence would resolve it: Development and validation of new offline evaluation metrics specifically designed for RL-based recommendation systems, with empirical comparisons showing their effectiveness relative to traditional IR metrics

### Open Question 4
- Question: How can the variance-bias trade-off in DCG estimators be optimally tuned for different recommendation scenarios?
- Basis in paper: [explicit] The paper shows that clipping IPS weights (reducing variance at the cost of bias) can improve sensitivity, but doesn't provide a systematic approach to determine optimal clipping parameters
- Why unresolved: The paper demonstrates the existence of a bias-variance trade-off and shows that clipping can help, but doesn't offer a principled method for selecting clipping parameters based on dataset characteristics or recommendation objectives
- What evidence would resolve it: A comprehensive framework for selecting optimal clipping parameters based on dataset properties, recommendation objectives, and desired sensitivity levels, validated across multiple recommendation scenarios

### Open Question 5
- Question: How do different user interaction models beyond the position-based model affect the validity and effectiveness of DCG as an offline evaluation metric?
- Basis in paper: [explicit] The paper uses the position-based model as a key assumption but acknowledges that other click models exist and that the examination hypothesis may not hold in all scenarios
- Why unresolved: While the paper derives DCG under the position-based model and shows it works well empirically, it doesn't explore how alternative user interaction models would affect DCG's theoretical properties or practical performance
- What evidence would resolve it: Empirical studies comparing DCG's performance across different user interaction models (e.g., cascade models, examination hypothesis variants), demonstrating how each model affects DCG's correlation with online metrics and its sensitivity in detecting improvements

## Limitations
- Theoretical analysis assumes stationary reward distributions and full support of logging policies, which may not hold in practice
- Empirical validation relies on a single platform's data, limiting generalizability across different recommendation domains
- The practical impact of nDCG's inconsistency on real-world recommendation system development is not thoroughly quantified

## Confidence
- **High confidence**: The theoretical derivation of unbiased DCG through importance sampling and the proof that normalisation can cause inconsistent rankings are mathematically sound and well-supported.
- **Medium confidence**: The empirical correlation results between offline and online metrics, while suggestive, are based on data from a single platform and limited A/B testing.
- **Low confidence**: The practical impact of nDCG's inconsistency on real-world recommendation system development is not thoroughly quantified.

## Next Checks
1. **Cross-platform validation**: Replicate the correlation analysis across multiple recommendation platforms (e.g., e-commerce, news, music streaming) to assess generalizability of the findings.
2. **Controlled simulation study**: Create synthetic recommendation scenarios with known ground truth to systematically test when and how severely nDCG inversions occur compared to DCG.
3. **Sensitivity analysis**: Quantify the practical impact of nDCG inconsistencies by measuring how often and by how much metric-based model selection decisions would differ when using DCG vs nDCG.