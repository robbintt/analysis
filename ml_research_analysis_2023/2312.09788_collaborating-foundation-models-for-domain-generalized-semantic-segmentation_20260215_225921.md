---
ver: rpa2
title: Collaborating Foundation Models for Domain Generalized Semantic Segmentation
arxiv_id: '2312.09788'
source_url: https://arxiv.org/abs/2312.09788
tags:
- domain
- segmentation
- dgss
- pages
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CLOUDS, a novel approach for Domain Generalized
  Semantic Segmentation (DGSS) that leverages multiple foundation models (FMs) in
  a collaborative manner. Existing DGSS methods rely on Domain Randomization (DR)
  to diversify style, but struggle with content diversity.
---

# Collaborating Foundation Models for Domain Generalized Semantic Segmentation

## Quick Facts
- arXiv ID: 2312.09788
- Source URL: https://arxiv.org/abs/2312.09788
- Reference count: 40
- Outperforms traditional DGSS methods by 5.6% and 6.7% mIoU on synthetic-to-real and weather condition transfers

## Executive Summary
This paper introduces CLOUDS, a novel approach for Domain Generalized Semantic Segmentation (DGSS) that leverages multiple foundation models in a collaborative manner. Existing DGSS methods rely on Domain Randomization (DR) to diversify style, but struggle with content diversity. CLOUDS integrates CLIP for robust feature extraction, generative models for content diversification, and Segment Anything Model (SAM) for refining pseudo labels in a self-training framework. The method demonstrates significant improvements over traditional approaches, achieving 5.6% and 6.7% mIoU gains on synthetic-to-real benchmarks and varying weather conditions respectively.

## Method Summary
CLOUDS combines CLIP as a frozen backbone for robust feature extraction, generative diffusion models conditioned on LLM-generated prompts for content diversification, and SAM for refining pseudo labels. The framework uses a self-training approach where synthetic images are generated, pseudo labels are created by a teacher network, refined by SAM, and used to train a student model. This collaborative approach addresses both style and content domain gaps, enabling better generalization to unseen target domains without requiring target domain data during training.

## Key Results
- Achieves 5.6% mIoU improvement over prior methods on adapting from synthetic to real benchmarks
- Demonstrates 6.7% mIoU gain on varying weather conditions transfer tasks
- Shows performance plateau at approximately 5K generated images, suggesting optimal dataset size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using CLIP as a frozen backbone provides robust feature representations that generalize across domains better than standard ImageNet-pretrained backbones.
- Mechanism: CLIP's image-text contrastive training aligns visual features with semantic concepts, making them less domain-specific and more transferable to unseen environments.
- Core assumption: The domain gap is primarily in style rather than semantic content, and CLIP's alignment captures semantic invariances.
- Evidence anchors:
  - [abstract] "CLIP backbone for its robust feature representation"
  - [section] "we use a CNN-based CLIP instead of a ViT-based one, as it has been demonstrated in [75] that it produces better semantic features"
  - [corpus] Weak evidence - no direct citations supporting this claim
- Break condition: If the domain shift involves novel semantic concepts not covered in CLIP's training, the frozen features may be insufficient.

### Mechanism 2
- Claim: Generative diffusion models conditioned on LLM-generated prompts create content-diverse synthetic data that fills gaps in the source domain distribution.
- Mechanism: LLM generates varied textual descriptions of urban scenes, which guide diffusion models to produce photorealistic images covering diverse content modes not present in the original dataset.
- Core assumption: Text descriptions can effectively capture and guide generation of diverse visual content relevant to target domains.
- Evidence anchors:
  - [abstract] "generative models to diversify the content, thereby covering various modes of the possible target distribution"
  - [section] "To introduce content diversification we generate synthetic images using a combination of Large Language Models (LLMs) [65] and text-to-image Diffusion Model (DM) [57]"
  - [corpus] Weak evidence - no direct citations showing LLM+diffusion effectiveness for DGSS
- Break condition: If the LLM-generated prompts fail to capture relevant visual diversity, the synthetic data won't improve generalization.

### Mechanism 3
- Claim: SAM refinement of pseudo labels improves self-training quality by providing more accurate class-agnostic masks.
- Mechanism: SAM generates precise masks from point prompts within teacher-predicted regions, filtering noisy connected components and reducing false positives in pseudo labels.
- Core assumption: SAM's zero-shot segmentation capability can improve upon noisy pseudo labels generated by the teacher model.
- Evidence anchors:
  - [abstract] "Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model"
  - [section] "we leverageSAM's excellent class-agnostic mask predictions to refine the pseudo labels"
  - [corpus] Moderate evidence - SAM is well-established, but its use for pseudo-label refinement in DGSS is novel
- Break condition: If SAM introduces too many false negatives by over-filtering, the refined pseudo labels may lose important supervision signals.

## Foundational Learning

- Concept: Domain Generalization (DG)
  - Why needed here: DGSS is a specific application of DG where the goal is semantic segmentation model generalization across domains without target domain data.
  - Quick check question: What distinguishes DG from domain adaptation in terms of available data during training?

- Concept: Self-training with pseudo labels
  - Why needed here: The generated synthetic images lack ground truth labels, requiring a self-training loop where the model generates and refines its own supervision.
  - Quick check question: Why might using noisy pseudo labels from a teacher model be problematic without refinement?

- Concept: Foundation Models (FMs) and their collaborative use
  - Why needed here: Different FMs (CLIP, diffusion models, SAM) provide complementary capabilities - feature extraction, content generation, and mask refinement respectively.
  - Quick check question: How does combining multiple FMs address limitations that individual FMs would have in isolation?

## Architecture Onboarding

- Component map: CLIP encoder (frozen) → MaskFormer decoder → teacher network (EMA) → pseudo label generation → SAM refinement → student training
  - LLM → text prompts → diffusion model → synthetic image generation
  - Synthetic data + source data → self-training loop

- Critical path: CLIP feature extraction → mask prediction → pseudo label generation → SAM refinement → student model training with both source and synthetic data

- Design tradeoffs:
  - Frozen CLIP vs fine-tuned: Preserves generalization but may miss domain-specific optimizations
  - Synthetic data quantity: More data improves diversity but increases computational cost and potential noise
  - SAM refinement aggressiveness: Stricter filtering improves label quality but may discard useful supervision

- Failure signatures:
  - Poor performance on target domains: Indicates feature representations aren't sufficiently domain-agnostic
  - No improvement from synthetic data: Suggests content diversity isn't addressing the right gaps
  - Student model collapse: May indicate overly aggressive pseudo label refinement removing too much signal

- First 3 experiments:
  1. Baseline: Train MaskFormer with CLIP backbone on source only, evaluate on target domains
  2. Add synthetic data without SAM refinement: Measure impact of content diversity alone
  3. Add SAM refinement to experiment 2: Quantify improvement from pseudo label quality enhancement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CLOUDS scale with the size of the generated dataset, and is there an optimal dataset size for balancing performance gains and computational efficiency?
- Basis in paper: [explicit] The paper discusses the effect of generated dataset size on mIoU in Figure 5, showing that performance improves up to approximately 5K images, after which a plateau is reached.
- Why unresolved: While the paper identifies a plateau at 5K images, it does not explore the performance implications of using much larger datasets, which could be relevant for different scenarios or more complex tasks.
- What evidence would resolve it: Systematic experiments varying the dataset size beyond 5K images, measuring mIoU and computational cost, would clarify the scalability and efficiency trade-offs of CLOUDS.

### Open Question 2
- Question: What is the impact of fine-tuning the CLIP encoder on the model's ability to generalize to unseen domains, and how does this compare to keeping the encoder frozen?
- Basis in paper: [explicit] Table 5 shows that keeping the backbone frozen during training retains the encoder's ability to generalize, while fine-tuning leads to poorer performance due to overfitting.
- Why unresolved: The paper does not explore intermediate approaches, such as partial fine-tuning or regularization techniques, which might balance the benefits of leveraging pre-trained features with the need for domain-specific adaptation.
- What evidence would resolve it: Experiments comparing different fine-tuning strategies (e.g., layer-wise fine-tuning, regularization) against the frozen encoder approach would reveal the optimal balance for generalization.

### Open Question 3
- Question: How does the choice of text prompts generated by the LLM affect the quality and diversity of the synthetic images, and what is the optimal prompt generation strategy for maximizing performance?
- Basis in paper: [inferred] The paper mentions using LLM to generate diverse text prompts for the diffusion model, but does not provide a detailed analysis of how different prompt strategies impact the quality of generated images and subsequent segmentation performance.
- Why unresolved: The relationship between prompt diversity, image quality, and segmentation accuracy is complex and likely task-dependent, requiring a systematic exploration of prompt generation strategies.
- What evidence would resolve it: Controlled experiments varying the LLM prompt generation strategy (e.g., prompt length, diversity, specificity) and measuring the impact on image quality and segmentation performance would elucidate the optimal approach.

## Limitations

- The paper's claims about CLIP backbone superiority and generative model benefits are supported by experimental results but lack strong theoretical justification.
- The approach requires substantial computational resources for generating synthetic data and running SAM refinement, which isn't fully quantified in terms of training overhead.
- The reliance on frozen CLIP features assumes semantic content remains consistent across domains, which may not hold for novel environments.

## Confidence

- CLIP feature generalization: Medium - supported by results but limited theoretical backing
- Generative content diversification: Medium - novel application with promising but untested assumptions
- SAM refinement effectiveness: Medium - leverages proven zero-shot segmentation but novel application context

## Next Checks

1. Ablation study isolating the contribution of each foundation model component to identify which is most critical for performance gains
2. Cross-dataset validation on domains with different semantic distributions to test CLIP's feature generalization limits
3. Computational overhead analysis comparing training time and resources against performance improvements