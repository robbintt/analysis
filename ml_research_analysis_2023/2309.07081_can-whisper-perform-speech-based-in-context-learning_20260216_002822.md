---
ver: rpa2
title: Can Whisper perform speech-based in-context learning?
arxiv_id: '2309.07081'
source_url: https://arxiv.org/abs/2309.07081
tags:
- speech
- dialect
- whisper
- sicl
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first study on speech-based in-context
  learning (SICL) for automatic speech recognition (ASR) using OpenAI's Whisper models.
  The proposed SICL method enables test-time adaptation of ASR models using a small
  number of labeled speech samples without gradient descent.
---

# Can Whisper perform speech-based in-context learning?

## Quick Facts
- arXiv ID: 2309.07081
- Source URL: https://arxiv.org/abs/2309.07081
- Reference count: 0
- Primary result: SICL achieves 32.3% average WER reduction on Chinese dialects

## Executive Summary
This paper introduces speech-based in-context learning (SICL) for automatic speech recognition using OpenAI's Whisper models. The method enables test-time adaptation without gradient descent by feeding paired speech inputs to the encoder and text labels to the decoder as in-context examples. Experiments on Chinese dialect ASR show consistent WER reductions, with k-nearest-neighbours-based example selection further improving performance to 36.4% relative WER reduction.

## Method Summary
The SICL method adapts Whisper ASR models at test time using a small number of labeled speech samples. It feeds paired speech inputs to the encoder and text labels to the decoder as in-context examples, mimicking text-based ICL patterns. The approach uses k-nearest neighbors for example selection based on audio embeddings, comparing to baselines including random selection and LoRA adaptation. The method is evaluated on Chinese dialect datasets including RASC863 and iFLYTEK private dialectal conversation data.

## Key Results
- Average relative WER reduction of 32.3% on two Chinese dialects
- kNN-based example selection increases WER reduction to 36.4%
- Language-level adaptation more effective than speaker-level adaptation
- SICL validated on speaker adaptation and continuous speech recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whisper's encoder-decoder architecture enables separate handling of speech inputs and text labels in SICL
- Mechanism: In-context examples are split into paired speech inputs (fed to encoder) and text labels (fed to decoder via prefix tokens)
- Core assumption: The encoder can effectively embed speech from dialects not seen during training
- Evidence anchors: [abstract] "feed paired speech inputs and text labels separately into the encoder and decoder"

### Mechanism 2
- Claim: kNN-based in-context example selection improves SICL by retrieving contextually relevant speech samples
- Mechanism: Audio embeddings are averaged and compared to test speech embeddings via Euclidean distance
- Core assumption: Mean audio embeddings preserve dialectal and speaker characteristics sufficiently for nearest neighbor retrieval
- Evidence anchors: [abstract] "k-nearest-neighbours-based in-context example selection technique can increase the average relative WER reduction to 36.4%"

### Mechanism 3
- Claim: Language-level adaptation is more effective than speaker-level adaptation in SICL for dialects
- Mechanism: When examples share dialect but differ in speaker, SICL adapts to dialect-specific phonology and lexicon
- Core assumption: Dialectal differences outweigh speaker differences in shaping the audio-text alignment needed for ASR accuracy
- Evidence anchors: [abstract] "insights into SICL's adaptability to phonological variances and dialect-specific lexical nuances"

## Foundational Learning

- Concept: Transformer encoder-decoder architecture
  - Why needed here: SICL relies on Whisper's encoder producing meaningful speech embeddings and the decoder generating aligned transcriptions
  - Quick check question: What role does the encoder play in SICL, and how does it differ from text-based ICL?

- Concept: In-context learning (ICL)
  - Why needed here: SICL extends ICL from text to speech by aligning paired modalities
  - Quick check question: How does the presentation of paired speech and text examples differ between SICL and text-based ICL?

- Concept: k-nearest neighbors (kNN) retrieval
  - Why needed here: kNN selects the most relevant in-context examples based on audio embedding similarity
  - Quick check question: What metric is used to measure similarity between test speech and candidate examples in kNN selection?

## Architecture Onboarding

- Component map: Whisper ASR model (encoder-decoder Transformer) -> kNN example retrieval model (Θ) -> Datastore of paired speech and text examples -> Prefix tokens -> Prompt tokens
- Critical path: 1. Encode test speech → get audio embedding. 2. Retrieve k nearest examples using Θ → build in-context set. 3. Concatenate retrieved speech + test speech → feed to encoder. 4. Feed corresponding text labels to decoder via prefix. 5. Decode transcription conditioned on both encoder output and decoder context.
- Design tradeoffs: Larger k improves adaptation but increases latency and memory usage; choice of Θ affects retrieval quality
- Failure signatures: High WER when in-context examples are from different dialect group; performance drops if kNN retrieval picks acoustically dissimilar samples
- First 3 experiments: 1. Baseline: Run Whisper without SICL (k=0) to measure baseline WER. 2. SICL with random example selection (k=4) to confirm improvement over baseline. 3. SICL with kNN selection (k=4, Θ=L) to validate retrieval benefit and measure WER reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific mechanism by which speech-based in-context learning (SICL) improves ASR performance?
- Basis in paper: [explicit] The paper states that SICL improves performance but does not provide a detailed explanation of the underlying mechanism
- Why unresolved: The paper focuses on demonstrating effectiveness rather than exploring reasons behind success
- What evidence would resolve it: Further analysis of internal workings of Whisper models during SICL, such as attention patterns or activation differences

### Open Question 2
- Question: How does SICL perform on other speech tasks beyond dialect adaptation?
- Basis in paper: [explicit] The paper mentions validation on speaker adaptation and continuous speech recognition but doesn't provide detailed results
- Why unresolved: The paper focuses on dialect adaptation as primary application and doesn't delve into other potential use cases
- What evidence would resolve it: Conducting experiments on wider range of speech tasks and comparing performance to other adaptation methods

### Open Question 3
- Question: How does the choice of in-context examples affect SICL performance?
- Basis in paper: [explicit] The paper discusses k-nearest neighbors for example selection but doesn't explore impact of different strategies
- Why unresolved: The paper focuses on demonstrating effectiveness with k-nearest neighbors but doesn't investigate factors contributing to successful example selection
- What evidence would resolve it: Conducting experiments with different example selection methods and analyzing properties of effective examples

## Limitations
- Limited generalizability to other languages and acoustic environments beyond Chinese dialects
- kNN-based example selection relies on audio embeddings from a retrieval model not fully characterized
- Experimental validation limited to WER as sole metric without considering latency, memory usage, or qualitative assessments

## Confidence
- High Confidence: Core finding that SICL can reduce WER compared to baseline Whisper performance
- Medium Confidence: Claim that kNN-based example selection improves SICL performance
- Low Confidence: Assertion that SICL can handle phonological variances and dialect-specific lexical nuances

## Next Checks
1. Validate SICL effectiveness on non-Chinese languages and dialects with different phonological systems, measuring both WER reduction and embedding space alignment
2. Systematically evaluate impact of different audio embedding models on kNN selection quality and downstream SICL performance
3. Measure computational overhead of SICL with varying k values and model sizes, and assess point of diminishing returns