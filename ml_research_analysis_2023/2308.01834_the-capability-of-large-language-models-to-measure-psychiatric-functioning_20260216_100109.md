---
ver: rpa2
title: The Capability of Large Language Models to Measure Psychiatric Functioning
arxiv_id: '2308.01834'
source_url: https://arxiv.org/abs/2308.01834
tags:
- page
- etal
- ptsd
- pcl-c
- phq-8
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tested Med-PaLM 2, a large language model (LLM) trained
  on general medical knowledge, for predicting psychiatric functioning without explicit
  training on psychiatric assessments. Using n=145 depression and n=115 PTSD clinical
  interview transcripts, plus n=46 DSM-5 case studies, the model estimated clinical
  scores and diagnoses via structured prompts.
---

# The Capability of Large Language Models to Measure Psychiatric Functioning

## Quick Facts
- arXiv ID: 2308.01834
- Source URL: https://arxiv.org/abs/2308.01834
- Reference count: 22
- Primary result: LLM predicted psychiatric scores from clinical text without explicit psychiatric training, matching human accuracy for depression

## Executive Summary
This study demonstrates that large language models trained on general medical knowledge can assess psychiatric functioning from clinical text without explicit psychiatric training. Using Med-PaLM 2, the model predicted depression and PTSD scores from clinical interviews and identified psychiatric diagnoses from case studies with accuracy comparable to human raters. The results suggest LLMs can serve as a flexible tool for psychiatric assessment, particularly for common conditions like depression, though performance varies across different psychiatric presentations.

## Method Summary
The study applied Med-PaLM 2 to 145 depression clinical interviews (PHQ-8), 115 PTSD clinical interviews (PCL-C), and 46 DSM-5 case studies without any model training. Instead, researchers used structured prompts to elicit the model's estimates of clinical scores, confidence levels, and diagnostic reasoning. The model generated predictions for psychiatric symptom severity and diagnostic categories, which were then compared to human clinician ratings. Word frequency analysis was used to evaluate the model's explanatory capabilities in generating diagnostic summaries.

## Key Results
- Depression (PHQ-8): Accuracy 0.80-0.84, matching human raters (p=0.23)
- PTSD (PCL-C): Accuracy 0.74, sensitivity 0.30, specificity 0.98
- Diagnostic accuracy: 92.5% correct diagnostic category, 77.5% correct specific diagnosis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Med-PaLM 2 can predict psychiatric functioning scores from unstructured clinical text without explicit psychiatric training.
- Mechanism: The model leverages self-supervised learning from a large general medical corpus to extract clinically relevant linguistic patterns that map to psychiatric symptom severity.
- Core assumption: General medical knowledge embeddings contain sufficient latent structure to approximate psychiatric assessment constructs.
- Evidence anchors:
  - [abstract] "without being trained to do so" and "general clinical language models to flexibly predict psychiatric risk based on free descriptions of functioning"
  - [section] "The capability to learn patterns in data without providing training examples, known as self-supervised learning, comes from training on large numbers of parameters and data sources to learn general rules and relationships"
- Break condition: If psychiatric symptoms are highly context-dependent or expressed in non-standard language not present in the medical training corpus.

### Mechanism 2
- Claim: Model performance on depression assessments matches human raters while PTSD performance is lower in sensitivity.
- Mechanism: Depression symptoms are more prevalent in general medical texts and have more standardized linguistic expression, leading to better capture by the model.
- Core assumption: Frequency and linguistic standardization of symptoms in training data correlates with model prediction accuracy.
- Evidence anchors:
  - [section] "Depression ratings from Med-PaLM 2 did not differ statistically from human raters while PTSD scores were shown to be statistically discrepant between human and automated scoring"
  - [section] "We hypothesize that Med-PaLM 2 performed best when assessing MDD because it is the most commonly occurring psychiatric condition and psychiatric comorbidity with physical illness"
- Break condition: If the training corpus underrepresents certain symptom presentations or uses highly variable terminology.

### Mechanism 3
- Claim: Med-PaLM 2 provides explainable summaries of diagnostic reasoning through frequency analysis of diagnostic terms.
- Mechanism: The model's attention mechanisms and training on medical language enable it to identify and summarize diagnostically relevant terms when generating explanations.
- Core assumption: The model's internal representations of psychiatric concepts can be surfaced through natural language generation.
- Evidence anchors:
  - [section] "When applied to case studies, Med-PaLM-2 demonstrated high accuracy in labeling DSM5 diagnoses without prior training"
  - [section] "Analyses of word frequencies show that Med-PaLM2 produces content-specific summarization"
- Break condition: If the model's explanations are post-hoc rationalizations rather than genuine reflections of its reasoning process.

## Foundational Learning

- Concept: Self-supervised learning in large language models
  - Why needed here: Understanding how LLMs can learn psychiatric assessment patterns without explicit training on psychiatric data
  - Quick check question: What is the difference between supervised and self-supervised learning in the context of medical language models?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: To understand how the model processes and extracts information from clinical text
  - Quick check question: How do attention mechanisms in transformers enable the model to focus on diagnostically relevant information in clinical interviews?

- Concept: Psychometric validation and diagnostic criteria
  - Why needed here: To interpret the model's performance metrics and understand psychiatric assessment standards
  - Quick check question: What are the key differences between sensitivity, specificity, and Kappa in evaluating diagnostic tools?

## Architecture Onboarding

- Component map: Clinical text input → Prompt processing → Score estimation → Confidence scoring → Diagnostic categorization → Explanation generation
- Critical path: Clinical text input → Prompt processing → Score estimation → Confidence scoring → Diagnostic categorization → Explanation generation
- Design tradeoffs: General medical training vs. specialized psychiatric fine-tuning; explanation generation vs. pure prediction accuracy; sensitivity vs. specificity in different disorders
- Failure signatures: Low sensitivity in PTSD detection indicates missed cases; inconsistent comorbidity detection suggests limitations in complex case handling; confidence score variability may indicate uncertainty in borderline cases
- First 3 experiments:
  1. Test the model on clinical text from different medical specialties to identify where psychiatric assessment capabilities transfer or fail
  2. Compare model performance with different prompt structures to optimize for sensitivity or specificity
  3. Evaluate explanation quality by having clinicians rate the relevance and accuracy of diagnostic terms in model-generated summaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would Med-PaLM 2 perform on psychiatric assessment tasks in languages other than English?
- Basis in paper: [inferred] The paper notes their results are limited to English using "demographically narrow data sources" and explicitly states "the current work is limited to English only."
- Why unresolved: The study only tested English-language clinical interviews and case studies, so performance on other languages remains unknown.
- What evidence would resolve it: Testing Med-PaLM 2 on clinical interviews and case studies in multiple languages to compare performance metrics.

### Open Question 2
- Question: Would fine-tuning Med-PaLM 2 on psychiatric-specific training data improve its accuracy in assessing comorbid conditions and diagnostic modifiers?
- Basis in paper: [explicit] The authors note that Med-PaLM 2 "performed inconsistently in identifying comorbidities and diagnostic modifiers" and suggest "additional training or prompt tuning may be required."
- Why unresolved: The study used the model without any psychiatric-specific training, only general medical knowledge.
- What evidence would resolve it: Comparing performance metrics between the base model and versions fine-tuned on psychiatric datasets.

### Open Question 3
- Question: How would Med-PaLM 2 perform on psychiatric assessment tasks using diverse demographic populations not represented in the current dataset?
- Basis in paper: [explicit] The authors state results are from "demographically narrow data sources" and additional diverse datasets are needed to "generalize and apply these results."
- Why unresolved: The clinical interview and case study datasets used likely do not represent the full diversity of psychiatric presentation across populations.
- What evidence would resolve it: Testing the model on clinical data from diverse demographic groups and comparing performance metrics across groups.

## Limitations

- Proprietary model prevents independent validation and verification of results
- Performance disparity between depression (high sensitivity) and PTSD (low sensitivity) suggests condition-specific limitations
- Clinical interview transcripts may not capture full complexity of real-world psychiatric presentations

## Confidence

*High confidence*: The depression assessment results showing accuracy comparable to human raters (0.80-0.84) with kappa of 0.55, and the model's ability to correctly identify diagnostic categories 92.5% of the time.

*Medium confidence*: The PTSD assessment results showing lower sensitivity (0.30) and kappa (0.33), and the generalizability of findings to diverse clinical populations not represented in the DAIC-WOZ dataset.

*Low confidence*: The explanatory capability claims based on frequency analysis, as the study does not validate whether these explanations reflect genuine reasoning or post-hoc rationalization.

## Next Checks

1. **External validity test**: Apply the same prompt-based assessment approach to a different clinical population (e.g., emergency department psychiatric evaluations) to determine if performance generalizes beyond the controlled interview setting.

2. **Independent model replication**: Test open-source medical LLMs (e.g., BioBERT, ClinicalBERT) using identical prompts and evaluation criteria to determine if the capability extends beyond proprietary models.

3. **Clinical impact assessment**: Conduct a prospective study where LLM assessments are used alongside clinical care to evaluate whether they improve diagnostic accuracy, reduce assessment time, or affect treatment outcomes compared to standard psychiatric evaluation.