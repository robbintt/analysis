---
ver: rpa2
title: Which AI Technique Is Better to Classify Requirements? An Experiment with SVM,
  LSTM, and ChatGPT
arxiv_id: '2311.11547'
source_url: https://arxiv.org/abs/2311.11547
tags:
- classi
- performance
- cation
- requirements
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares ChatGPT (gpt-3.5-turbo and gpt-4) with SVM
  and LSTM for requirements classification, using five datasets and four binary classification
  problems. ChatGPT outperformed LSTM in all tasks, and exceeded SVM in functional
  requirements classification but not non-functional ones.
---

# Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT

## Quick Facts
- arXiv ID: 2311.11547
- Source URL: https://arxiv.org/abs/2311.11547
- Authors: 
- Reference count: 17
- Key outcome: ChatGPT outperformed traditional ML methods (SVM, LSTM) in requirements classification, with GPT-3.5 excelling at functional requirements and SVM at non-functional ones.

## Executive Summary
This study compares three AI techniques—Support Vector Machine (SVM), Long Short-Term Memory (LSTM), and ChatGPT (gpt-3.5-turbo and gpt-4)—for classifying software requirements into functional and non-functional categories. Using five datasets and four binary classification problems, the researchers found that ChatGPT generally outperformed LSTM and sometimes exceeded SVM's performance. Zero-shot prompting proved more effective than few-shot prompting for most tasks, though SVM remained superior for non-functional requirements classification. The results suggest that large language models can play a significant role in future requirements classification tools.

## Method Summary
The study used five public datasets (PROMISE, Dronology, ReqView, Leeds Library, WASP) with requirements labeled for four binary classification problems. SVM and LSTM models were trained on the PROMISE dataset with 500-dimensional word-level features, while ChatGPT models were evaluated using zero-shot and few-shot prompting via the OpenAI API. Performance was measured using precision, recall, and F1-score across all datasets and classification tasks.

## Key Results
- ChatGPT outperformed LSTM in all classification tasks across all datasets
- GPT-3.5 (latest) achieved the best results for functional requirements classification using zero-shot prompting
- SVM excelled at non-functional requirements classification, particularly for quality-related categories
- Few-shot prompting often reduced ChatGPT's performance compared to zero-shot

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot prompting can outperform few-shot prompting for certain classification tasks.
- Mechanism: LLMs have extensive pre-training on diverse text data, allowing them to generalize from context without explicit examples. When tasks are clearly defined, the model's internal knowledge can suffice, while few-shot examples might introduce noise or conflicting patterns.
- Core assumption: The prompt provides sufficient clarity and structure to guide the model toward correct classification.
- Evidence anchors:
  - [abstract]: "Zero-shot prompting was generally more effective than few-shot."
  - [section]: "Interestingly, the few-shot setting has been found to be beneficial primarily in scenarios where zero-shot results are significantly low."
- Break condition: If the task requires nuanced, domain-specific distinctions not well-represented in pre-training, few-shot examples may become necessary.

### Mechanism 2
- Claim: Different classification methods excel at different types of requirements (functional vs. non-functional).
- Mechanism: Functional requirements often align with procedural or action-oriented language, which LLMs can parse effectively from context. Non-functional requirements tend to involve abstract quality attributes, where structured feature extraction (e.g., SVM with word-level features) may be more reliable than free-form language understanding.
- Core assumption: The classification task's nature (action-based vs. quality-based) determines the optimal model type.
- Evidence anchors:
  - [abstract]: "ChatGPT outperformed LSTM in all tasks, and exceeded SVM in functional requirements classification but not non-functional ones."
  - [section]: "SVM model stands out in the 'IsQuality' and 'OnlyQuality' categories, surpassing the best GPT setting."
- Break condition: If datasets are highly imbalanced or lack clear linguistic markers for the classification target, both methods may underperform.

### Mechanism 3
- Claim: Few-shot examples can degrade performance if they introduce ambiguity or bias.
- Mechanism: When few-shot examples are drawn from a limited or biased subset, they may not represent the full distribution of requirements. This can confuse the model, especially if the examples contradict the model's pre-trained understanding.
- Core assumption: The few-shot examples are representative and consistent with the model's pre-training.
- Evidence anchors:
  - [abstract]: "Few-shot prompting often reduced performance."
  - [section]: "the few-shot setting does not always lead to enhanced performance—in most instances, it was found to be suboptimal."
- Break condition: If the dataset is large and diverse, or if the task is straightforward, few-shot examples may be unnecessary or harmful.

## Foundational Learning

- Concept: Prompt engineering principles (persona adoption, clarity, and task specification)
  - Why needed here: Effective prompts guide the model toward correct classification without relying on training data.
  - Quick check question: Does the prompt clearly define the classification categories and adopt an expert persona?

- Concept: Evaluation metrics (precision, recall, F1-score)
  - Why needed here: These metrics balance the trade-offs between false positives and false negatives, critical for comparing model performance.
  - Quick check question: Can you explain why F1-score is preferred over accuracy in imbalanced datasets?

- Concept: Dataset tagging and feature engineering
  - Why needed here: High-dimensional word-level features are crucial for traditional models like SVM, while LLMs rely on contextual understanding.
  - Quick check question: How does the choice of features affect SVM's performance compared to LLM's context-based approach?

## Architecture Onboarding

- Component map: Data ingestion -> Preprocessing -> SVM/LSTM/ChatGPT models -> Evaluation -> Comparison
- Critical path: 1) Load and preprocess datasets 2) Train SVM and LSTM on PROMISE 3) Generate prompts for ChatGPT 4) Run evaluations on all datasets 5) Compare F1-scores across models
- Design tradeoffs: Zero-shot vs. few-shot (simplicity vs. task-specific guidance), Traditional vs. LLM (structured extraction vs. contextual understanding), Model choice (GPT-3.5 for functional, SVM for non-functional)
- Failure signatures: Low recall (misses relevant instances), Low precision (includes false positives), High variability (inconsistent performance)
- First 3 experiments: 1) Run SVM and LSTM on PROMISE, compare F1-scores 2) Test GPT-3.5 latest in zero-shot for 'IsFunctional' on all datasets 3) Compare GPT-3.5 latest zero-shot vs. few-shot for 'IsQuality'

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does few-shot prompting sometimes reduce ChatGPT's performance in requirements classification?
- Basis in paper: [explicit] "our results also show that contrary to our expectations, the few-shot setting does not always lead to enhanced performance—in most instances, it was found to be suboptimal"
- Why unresolved: The paper observes this phenomenon but does not investigate the underlying reasons. It's unclear whether this is due to prompt engineering issues, model architecture, or data characteristics.
- What evidence would resolve it: Comparative experiments varying prompt quality, number of examples, and example selection criteria while measuring impact on classification performance.

### Open Question 2
- Question: How do larger LLMs like GPT-4 perform compared to specialized RE models on requirements classification tasks?
- Basis in paper: [inferred] The paper compares ChatGPT models only against traditional ML methods (SVM, LSTM) but doesn't benchmark against other LLMs or specialized RE models
- Why unresolved: The study's scope was limited to ChatGPT models, leaving questions about whether these general-purpose models are optimal for RE tasks
- What evidence would resolve it: Direct comparison of ChatGPT, Llama-2, Mistral, and specialized RE models on identical datasets with identical evaluation metrics.

### Open Question 3
- Question: What are the specific linguistic features that make requirements easier or harder for LLMs to classify correctly?
- Basis in paper: [inferred] The paper mentions "high-dimensional (500) word level features" for SVM/LSTM but doesn't analyze what textual characteristics challenge LLMs
- Why unresolved: The paper evaluates model performance but doesn't examine which types of requirements (length, vocabulary, ambiguity, etc.) cause classification difficulties
- What evidence would resolve it: Detailed linguistic analysis of misclassified requirements, identifying patterns in ambiguity, terminology, or structure that correlate with classification errors.

## Limitations
- Small dataset sizes for Dronology, ReqView, Leeds Library, and WASP may limit generalizability
- No statistical significance testing between model comparisons
- Focus on binary classification may not capture real-world complexity of multi-class or hierarchical categorization
- Unclear representativeness of few-shot examples used in ChatGPT experiments

## Confidence

**High Confidence:**
- ChatGPT outperforming LSTM across all tasks is supported by consistent results across multiple datasets and classification problems.
- The superior performance of GPT-3.5 latest in zero-shot functional requirements classification is well-demonstrated with strong F1-scores across datasets.

**Medium Confidence:**
- The claim that zero-shot prompting generally outperforms few-shot prompting is supported by results but requires further investigation into the impact of example selection and task complexity.
- The observation that SVM excels at non-functional requirements classification is supported by data but may be dataset-dependent.

**Low Confidence:**
- The assertion that few-shot examples can introduce harmful bias is based on observed performance drops but lacks detailed analysis of why this occurs or when it might be beneficial.

## Next Checks

1. **Statistical Significance Testing**: Conduct paired t-tests or Wilcoxon signed-rank tests between model comparisons to determine if performance differences are statistically significant, particularly for close-margin comparisons like GPT-3.5 vs. SVM on non-functional requirements.

2. **Dataset Size Impact Analysis**: Systematically evaluate model performance as a function of training set size to determine whether the observed advantages of ChatGPT persist with larger datasets, or if traditional models like SVM catch up with more data.

3. **Prompt Variation Study**: Conduct a controlled experiment varying prompt structure, example selection, and number of few-shot examples to identify optimal prompting strategies and better understand when few-shot prompting helps versus hurts performance.