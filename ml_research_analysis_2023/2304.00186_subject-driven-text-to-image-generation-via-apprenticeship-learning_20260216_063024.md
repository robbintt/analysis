---
ver: rpa2
title: Subject-driven Text-to-Image Generation via Apprenticeship Learning
arxiv_id: '2304.00186'
source_url: https://arxiv.org/abs/2304.00186
tags:
- suti
- image
- generation
- images
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SuTI, a method for subject-driven text-to-image
  generation that performs instant customization without subject-specific fine-tuning.
  The core idea is apprenticeship learning: SuTI learns to imitate a large number
  of expert models, each fine-tuned for a specific visual subject.'
---

# Subject-driven Text-to-Image Generation via Apprenticeship Learning

## Quick Facts
- arXiv ID: 2304.00186
- Source URL: https://arxiv.org/abs/2304.00186
- Reference count: 40
- Key outcome: SuTI achieves 95% human evaluation score on overall quality while generating 20x faster than optimization-based methods

## Executive Summary
SuTI introduces an apprenticeship learning approach for instant subject-driven text-to-image generation without test-time optimization. The method trains a single apprentice model to imitate outputs from hundreds of expert models, each fine-tuned on specific visual subjects mined from internet image clusters. During inference, SuTI performs in-context learning using 3-5 demonstrations to generate novel subject-specific images. The approach achieves state-of-the-art performance on DreamBench benchmarks while offering significant speed advantages over optimization-based methods.

## Method Summary
SuTI employs apprenticeship learning where a base Imagen 64x64 diffusion model learns to imitate outputs from hundreds of expert models fine-tuned on diverse internet image clusters. The training pipeline involves mining millions of image clusters from the web, fine-tuning 400 parallel expert models (each trained for 500 steps on TPU cores), and generating high-quality pseudo-targets for unseen prompts. A delta CLIP score threshold (0.02) filters low-quality expert generations before training the 2.5B parameter apprentice model for 150K steps on 128 TPU v4 chips. During inference, SuTI uses 3-5 in-context demonstrations with a guidance weight of 15, generating each image in approximately 30 seconds.

## Key Results
- Achieves 95% human evaluation score on overall quality for DreamBench-v2
- Generates images 20x faster than optimization-based state-of-the-art methods
- Outperforms InstructPix2Pix, Textual Inversion, and Imagic on DreamBench and DreamBench-v2 benchmarks
- Matches DreamBooth quality while eliminating test-time optimization requirements

## Why This Works (Mechanism)

### Mechanism 1
The apprentice model learns generalizable subject representations by mimicking half a million expert models fine-tuned on diverse internet image clusters. Each expert specializes on a unique subject cluster, generating high-quality images for novel compositional prompts. The apprentice observes these outputs and learns to perform subject-driven generation without fine-tuning on the target subject. Core assumption: internet image clusters mined by URL grouping and similarity filtering capture sufficient subject diversity and visual fidelity to enable broad generalization.

### Mechanism 2
The delta CLIP score threshold (0.02) filters out low-quality expert generations, ensuring high-quality pseudo-targets for apprentice training. After an expert generates an image for an unseen prompt, the delta CLIP score measures the increase in CLIP similarity compared to demonstration images. Only images exceeding the threshold are kept. Core assumption: delta CLIP score correlates strongly with human judgments of subject fidelity and textual alignment in generated images.

### Mechanism 3
In-context learning with 3-5 demonstrations enables instant subject customization without test-time optimization. The apprentice model attends over the demonstrated image-text pairs and the target prompt, generating a novel rendition of the subject. This bypasses expensive fine-tuning per subject. Core assumption: the apprentice's attention layers over demonstrations encode sufficient subject-specific information to guide generation without further adaptation.

## Foundational Learning

- **Diffusion models as iterative denoising processes**
  - Why needed here: SuTI builds on diffusion model architecture and training; understanding the denoising objective is essential to grasp how experts and the apprentice are trained.
  - Quick check question: What is the mathematical form of the denoising loss used to train diffusion models?

- **Apprenticeship learning paradigm**
  - Why needed here: SuTI applies apprenticeship learning at massive scale—learning from many expert models rather than one. This differs from standard supervised learning.
  - Quick check question: How does apprenticeship learning differ from supervised learning in terms of data source and training objective?

- **CLIP embedding similarity for quality filtering**
  - Why needed here: Delta CLIP score is used to filter expert generations. Understanding CLIP embeddings and their use in measuring semantic similarity is critical.
  - Quick check question: What does a positive delta CLIP score indicate about the relationship between a generated image and the target prompt?

## Architecture Onboarding

- **Component map**: Web image clusters → Expert fine-tuning (400 models) → Pseudo-target generation → Delta CLIP filtering → Apprentice training (SuTI) → In-context inference
- **Critical path**: Mine image clusters → Fine-tune experts → Generate pseudo-targets → Filter by delta CLIP → Train apprentice → Inference with in-context demos
- **Design tradeoffs**:
  - Pros: No test-time optimization, fast inference, no per-subject storage
  - Cons: Requires massive upfront computation for expert training, depends on quality of mined clusters
- **Failure signatures**:
  - Low CLIP-I scores on DreamBench: likely poor subject preservation due to insufficient demonstration encoding
  - Low CLIP-T scores: likely misalignment between generated images and text prompts
  - High variance in generated images: possible overfitting to expert idiosyncrasies or insufficient filtering
- **First 3 experiments**:
  1. Train SuTI with delta CLIP threshold = 0.0 (no filtering) and evaluate on DreamBench-v2 to measure impact of quality filtering.
  2. Vary number of demonstrations (1, 3, 5, 10) during inference and measure human evaluation scores to identify optimal context size.
  3. Replace internet-mined clusters with synthetic or curated clusters and retrain experts to assess sensitivity to data source quality.

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of the expert-generated dataset impact SuTI's final performance? The paper states "We found that the Delta CLIP score is critical to ensure the quality of synthesized target images. Such a filtering mechanism is highly influential in terms of SuTI's final performance." While the paper shows that increasing the Delta CLIP threshold improves performance, it doesn't explore what the optimal threshold is or how much further improvements might be possible with even higher quality expert data.

### Open Question 2
Can SuTI's subject fidelity be improved to match or exceed DreamBooth's performance on complex manufactured subjects? The paper notes "we also found that SuTI is less faithful to the low-level visual details than DreamBooth [29], especially for more complex and often manufactured subjects such as 'robots' or 'rc cars'." The paper acknowledges this limitation but doesn't propose or test potential solutions to address it.

### Open Question 3
What is the impact of the number of demonstrations on SuTI's performance beyond 5 examples? The paper shows performance improves up to 5 demonstrations but doesn't test beyond that point. The paper only tests up to 5 demonstrations and shows performance plateaus, but doesn't test whether there might be further improvements with even more demonstrations.

## Limitations

- The scalability depends heavily on the quality and diversity of mined image clusters, but clustering methodology validation is lacking
- Delta CLIP score filtering effectiveness as a proxy for human judgment quality remains unverified
- 95% human evaluation score comes from limited subjects (10) and prompts (50), raising generalizability questions

## Confidence

- **High confidence**: The architectural approach of using apprenticeship learning to train a single subject-customization model is technically sound and well-documented
- **Medium confidence**: The claim of 20x faster inference compared to optimization-based methods is plausible given the elimination of test-time optimization
- **Low confidence**: The delta CLIP score's effectiveness as a proxy for human judgment quality remains unverified, and the clustering methodology's impact on generalization is not empirically tested

## Next Checks

1. Conduct human evaluation studies to establish correlation between delta CLIP scores and human quality judgments, varying the threshold parameter.
2. Perform ablation studies on the number of experts (100, 200, 400, 800) and their impact on SuTI performance to understand scalability limits.
3. Test SuTI on out-of-distribution subjects and complex compositional prompts beyond the DreamBench-v2 dataset to assess generalization bounds.