---
ver: rpa2
title: Utilizing Mood-Inducing Background Music in Human-Robot Interaction
arxiv_id: '2308.14269'
source_url: https://arxiv.org/abs/2308.14269
tags:
- music
- agent
- autonomous
- background
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors study how music affects human decision-making in the
  context of human-robot interaction. They conduct an experiment where participants
  drive a simulated car through an intersection while listening to either happy or
  sad music, with another autonomous vehicle also crossing the intersection.
---

# Utilizing Mood-Inducing Background Music in Human-Robot Interaction

## Quick Facts
- arXiv ID: 2308.14269
- Source URL: https://arxiv.org/abs/2308.14269
- Reference count: 27
- Key outcome: Music-aware autonomous agents complete intersection-crossing tasks faster than music-unaware agents without increasing crash rates, by modeling human mood from background music.

## Executive Summary
This paper investigates how background music affects human decision-making in human-robot interaction scenarios. Through a controlled experiment with 22 participants driving simulated cars through an intersection while listening to happy or sad music, the authors demonstrate that autonomous vehicles can improve performance by incorporating music as a state feature. The music-aware reinforcement learning agent learned to anticipate human risk tolerance based on the background music, resulting in faster completion times without compromising safety. This work suggests that modeling environmental cues like background music can enhance autonomous agents' ability to interact effectively with humans in mixed-autonomy settings.

## Method Summary
The experiment involved participants driving a simulated car through an intersection while listening to either happy or sad background music, with an autonomous vehicle also crossing the intersection. Two DQN (Deep Q-Network) models were trained: one that included the music condition as a binary state feature and one that did not. The state representation included car positions, speeds, time, and music condition (for the music-aware model). Three actions were available: FAST, SLOW, and BRAKE. The reward function was designed as negative completion time minus 100 times crash penalty. Models were trained using exploration data for the first 96 trials, then evaluated during exploitation to compare completion times and crash rates between the music-aware and music-unaware policies.

## Key Results
- Music-aware model achieved significantly faster completion times compared to music-unaware model
- No increase in crash rates was observed with the music-aware policy
- Music-aware model used fewer SLOW actions and more FAST actions overall, while increasing BRAKE frequency when necessary to maintain safety

## Why This Works (Mechanism)

### Mechanism 1
Background music provides affective state cues that modulate human decision-making, and these cues can be learned by an autonomous agent to improve prediction accuracy. Music induces mood (happy vs. sad), which influences how cautiously or aggressively a person drives. The autonomous agent encodes music as a state feature, allowing the RL model to condition its policy on the driver's likely emotional state. This improves task completion time without increasing crashes.

### Mechanism 2
Incorporating music as part of the agent's state representation allows it to learn a policy that anticipates human risk tolerance. The agent's RL model includes a binary music condition feature. During training, this feature correlates with observed human behaviors (e.g., slower driving with sad music). The learned Q-function uses this correlation to adjust its own speed and braking strategy to minimize completion time while maintaining safety.

### Mechanism 3
Modeling music allows the autonomous agent to balance speed and safety more effectively by adapting to the human's expected risk profile. With music-aware states, the agent learns to take more FAST actions overall but increases BRAKE frequency when necessary, especially in the intersection. This trade-off maintains safety while improving speed.

## Foundational Learning

- **Concept**: Reinforcement Learning (RL) with Q-learning and Deep Q-Networks (DQN)
  - Why needed here: The task is sequential decision-making where the agent must learn a policy from interaction data without explicit supervision. RL naturally models this setting.
  - Quick check question: In RL, what does the Q-function represent in the context of this intersection-crossing task?

- **Concept**: Markov Decision Process (MDP) formulation
  - Why needed here: The intersection task is episodic with clear states, actions, rewards, and terminal conditions, making it a textbook MDP problem.
  - Quick check question: Why is it important that the state representation includes the music condition in this MDP?

- **Concept**: State-action representation and feature engineering
  - Why needed here: The agent's performance depends on how well the state captures relevant information (vehicle positions, speeds, music condition). Poor features degrade learning.
  - Quick check question: How does adding the music condition as a binary feature change the dimensionality and potential expressiveness of the state space?

## Architecture Onboarding

- **Component map**: Simulation environment -> Data collection module -> RL training engine -> Evaluation module -> Music tagging system
- **Critical path**: Data collection → Model training (exploration phase) → Model evaluation (exploitation phase) → Performance comparison
- **Design tradeoffs**:
  - State dimensionality vs. learning speed: Adding music increases state size, potentially slowing convergence
  - Exploration vs. exploitation balance: Longer exploration may improve policy quality but reduce available trials for evaluation
  - Binary vs. continuous music representation: Binary is simpler but may lose nuance in mood intensity
- **Failure signatures**:
  - If crash rates increase with music-aware model, the model may be overconfident or overfitting
  - If completion times are similar between models, the music signal may be too weak or noisy
  - If training is unstable, the state representation or reward function may need adjustment
- **First 3 experiments**:
  1. Run the simulation with random exploration only to establish baseline completion times and crash rates
  2. Train and evaluate the music-unaware model in isolation to measure improvement over random
  3. Train and evaluate the music-aware model and compare both metrics (completion time and crash rate) to the music-unaware baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can knowledge of a person's background music lead to improved performance in autonomous agents across a wider range of tasks beyond intersection crossing? The paper discusses potential applications in stores, parks, stadiums, elevators, parking garages, and online shopping and gaming, but does not provide empirical evidence for improved performance in these other domains.

### Open Question 2
How does the complexity of the music representation impact the autonomous agent's performance? The paper uses a binary representation of music as happy or sad, but it's unclear if more nuanced representations of music would lead to better performance.

### Open Question 3
What are the ethical implications of using background information, such as music, to improve autonomous agent performance? The authors acknowledge ethical questions involving the legitimacy of using background information and potential for subtle forms of manipulation, but do not explore these concerns in depth.

## Limitations

- Individual music perception variability may weaken the mood-music-behavior correlation across participants
- Sample size of 22 participants may not be robust to individual differences in driving style and music perception
- Controlled intersection scenario may not capture the complexity of real-world mixed-autonomy traffic environments

## Confidence

- **High confidence**: The core finding that incorporating music into the autonomous agent's state representation improves task completion time without increasing crashes is well-supported by the experimental results
- **Medium confidence**: The claim that background music provides reliable mood cues that affect human driving behavior is supported by psychological literature but may not generalize perfectly to all individuals
- **Low confidence**: The scalability of these findings to complex, real-world autonomous vehicle scenarios with multiple interacting agents and dynamic environments

## Next Checks

1. Cross-validation with different music sets: Test whether the music-aware policy generalizes to different sets of happy/sad songs not used in training to ensure the model learns mood patterns rather than memorizing specific tracks
2. Individual differences analysis: Analyze whether the music-mood-driving correlation holds consistently across participants with different demographics, musical backgrounds, and baseline risk tolerance
3. Real-world simulation testing: Evaluate the music-aware policy in more complex driving simulators that include multiple vehicles, pedestrians, and varying traffic conditions to assess real-world applicability