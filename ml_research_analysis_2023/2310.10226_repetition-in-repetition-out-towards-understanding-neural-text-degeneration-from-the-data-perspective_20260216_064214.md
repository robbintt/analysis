---
ver: rpa2
title: 'Repetition In Repetition Out: Towards Understanding Neural Text Degeneration
  from the Data Perspective'
arxiv_id: '2310.10226'
source_url: https://arxiv.org/abs/2310.10226
tags:
- data
- degeneration
- repetitions
- words
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a fundamental explanation for neural text degeneration
  from the data perspective. It finds that repetitive patterns in training data are
  strongly correlated with degeneration and that selectively dropping out attention
  to such repetitions during training significantly reduces degeneration.
---

# Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective

## Quick Facts
- arXiv ID: 2310.10226
- Source URL: https://arxiv.org/abs/2310.10226
- Reference count: 40
- Key outcome: Repetitive patterns in training data are the primary cause of neural text degeneration, and selectively dropping out attention to such repetitions during training significantly reduces degeneration across multiple datasets and model scales.

## Executive Summary
This paper provides a fundamental explanation for neural text degeneration from the data perspective, demonstrating that repetitive patterns in training data directly cause repetitive and dull loops during generation. The authors propose a simple yet effective solution: repetition dropout, which selectively drops out attention to repetitive n-grams during training. This approach significantly reduces degeneration metrics (rep-2, rep-3, rep-4) while maintaining reasonable perplexity, and works effectively even on large language models and instruction-tuned models. The work also reveals that the success of various existing degeneration mitigation approaches (high-inflow word merging, likelihood modifications, self-reinforcement mitigation) can be attributed to their common effect of penalizing repetitions in training data.

## Method Summary
The core method involves modifying the attention mechanism during training to selectively drop out attention connections to repetitive n-grams. Specifically, the approach computes a masking vector that identifies repetitive patterns in the input context and applies dropout to the attention scores associated with these patterns. This forces the model to rely less on repetitive context and more on diverse information when predicting the next token. The method is implemented as a simple modification to the attention mechanism without requiring architectural changes, making it applicable to existing transformer-based language models. During inference, the model generates text using standard decoding methods (greedy or beam search), but the training-time exposure to repetitive patterns has been reduced.

## Key Results
- Repetition dropout significantly reduces repetition rates (rep-2, rep-3, rep-4) across multiple datasets including Wikitext-103, OpenWebText2, and domain-specific corpora like PubMed and ArXiv
- The method is effective on large language models including LLAMA 2-7B and GPT-XL, not just small GPT-2 models
- High-inflow word merging approaches work primarily because they merge repetitive words, not because of a unique property of high-inflow words themselves
- Standard likelihood training with repetition dropout can achieve better degeneration mitigation than specialized training objectives without dropout

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Repetitive patterns in training data directly cause neural text degeneration during generation.
- **Mechanism**: The model learns to predict words by attending to and reproducing repetitive patterns observed in training data. When generating text, this learned behavior manifests as generating similar repetitive sequences.
- **Core assumption**: The model treats repetitive patterns in training data as valid linguistic patterns to reproduce, not distinguishing between natural repetitions and degenerative loops.
- **Evidence anchors**: Strong correlation between repetition rates in training and generated text across multiple datasets; repetition dropout breaks this correlation.

### Mechanism 2
- **Claim**: High-inflow words contribute to degeneration primarily because they are often repetitive words.
- **Mechanism**: High-inflow words (words that follow many different preceding words) are disproportionately represented among repetitive words in training data. When these words are merged or penalized, the effect is primarily reducing repetitions rather than addressing a unique property of high-inflow words.
- **Core assumption**: The set of high-inflow words overlaps significantly with the set of repetitive words in natural text.
- **Evidence anchors**: 26% of high-inflow word pairs are repetitive in Wikitext-103; merging these pairs significantly reduces repetition rates.

### Mechanism 3
- **Claim**: Likelihood objective training inherently learns repetitive patterns when present in training data, and this is the root cause of degeneration rather than a flaw in the objective itself.
- **Mechanism**: Maximum likelihood estimation optimizes for predicting the next token given the context. When the context contains repetitive patterns, the model learns to reproduce these patterns to maximize likelihood, even if this leads to degenerative loops in generation.
- **Core assumption**: The likelihood objective is agnostic to whether repetitions are natural or degenerative - it simply optimizes for prediction accuracy.
- **Evidence anchors**: MLE + repetition dropout significantly reduces repetition issues; likelihood objective might not be the most important factor in the degeneration issue.

## Foundational Learning

- **Concept: Attention mechanism in transformers**
  - Why needed here: The paper's proposed solution (repetition dropout) operates directly on the attention mechanism by masking attention to repetitive n-grams.
  - Quick check question: How does the attention mechanism in transformers allow words to "attend to" other words in the context, and why would masking certain attention connections affect generation?

- **Concept: N-gram repetition metrics**
  - Why needed here: The paper uses rep-n scores (rep-2, rep-3, rep-4) as primary evaluation metrics for measuring degeneration.
  - Quick check question: What is the mathematical definition of rep-n score and how does it differ from other repetition metrics like rep-w and rep-r?

- **Concept: Maximum likelihood estimation in language modeling**
  - Why needed here: Understanding how language models are trained via likelihood objectives is crucial for interpreting why repetitive training data leads to repetitive generation.
  - Quick check question: How does maximum likelihood estimation work in language modeling, and why would it naturally learn to reproduce repetitive patterns present in training data?

## Architecture Onboarding

- **Component map**: Token embeddings -> Transformer blocks (multi-head self-attention + position-wise feed-forward) -> Output layer (linear + softmax) -> Cross-entropy loss
- **Critical path**: Training → Attention mechanism with repetition dropout → Generation with greedy/beam search → Evaluation with repetition metrics
- **Design tradeoffs**:
  - **Pro**: Simple to implement, requires minimal architectural changes
  - **Con**: May hurt perplexity on real data (as shown in Figure 3)
  - **Pro**: Effective even on large models and instruction-tuned models
  - **Con**: Introduces a new hyperparameter (dropout rate) that needs tuning
- **Failure signatures**:
  - If rep-n scores don't decrease after implementing repetition dropout: Check that the masking is being applied correctly to attention scores
  - If perplexity increases dramatically: The dropout rate may be too high, causing the model to lose important contextual information
  - If training becomes unstable: Ensure the masking is only applied during training, not inference
- **First 3 experiments**:
  1. Implement basic repetition dropout on a small GPT-2 model trained on Wikitext-103 and measure rep-2 score improvement
  2. Vary the repetition dropout rate (0.3, 0.6, 0.9) to find the optimal tradeoff between repetition reduction and perplexity
  3. Compare repetition dropout against random dropout baseline to verify that the effect is specific to repetitive n-grams rather than dropout in general

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does repetition dropout affect the quality and coherence of generated text beyond just reducing repetition rates?
- Basis in paper: The paper mentions that while repetition dropout significantly reduces repetition rates, it may hurt the perplexity of the language model and could potentially impact the overall quality and coherence of generated text.
- Why unresolved: The paper primarily focuses on repetition rates as the main evaluation metric and does not extensively explore the qualitative aspects of generated text.
- What evidence would resolve it: Comprehensive evaluation using human evaluation, automated metrics beyond repetition rates (e.g., coherence, informativeness, and fluency), and qualitative analysis of generated text.

### Open Question 2
- Question: How does the effectiveness of repetition dropout vary across different domains and languages?
- Basis in paper: The paper conducts experiments on five datasets from different domains but does not explore the effectiveness of repetition dropout in different languages or across a wider range of domains.
- Why unresolved: Experiments are limited to English text and a specific set of domains.
- What evidence would resolve it: Experiments on datasets from various languages and domains, including low-resource languages and specialized domains.

### Open Question 3
- Question: How does repetition dropout interact with other techniques for mitigating neural text degeneration, such as top-k sampling, nucleus sampling, or contrastive decoding?
- Basis in paper: The paper mentions that repetition dropout is a training-time technique that can be combined with different decoding methods but does not explore these interactions.
- Why unresolved: The paper focuses on the effectiveness of repetition dropout in isolation.
- What evidence would resolve it: Experiments combining repetition dropout with different decoding methods and comparing their effectiveness in reducing degeneration.

## Limitations
- The analysis of high-inflow words as primarily contributing to degeneration through repetition is dataset-specific and may not generalize to all languages or domains
- The paper focuses primarily on n-gram repetition metrics, which may not capture all forms of degeneration such as semantic repetition or topic drift
- The effectiveness of repetition dropout on extremely large models (beyond 7B parameters) and its impact on downstream task performance remains unexplored

## Confidence
- **High confidence**: Core finding that repetitive patterns in training data strongly correlate with degeneration, and that repetition dropout effectively mitigates this issue across multiple datasets and model scales
- **Medium confidence**: Claim that likelihood objective training is not the primary factor in degeneration, but rather the presence of repetitive patterns in data
- **Medium confidence**: Analysis of high-inflow words as primarily contributing to degeneration through repetition rather than their unique properties

## Next Checks
1. **Dataset Generalization Test**: Apply repetition dropout training to non-English corpora (e.g., Chinese, Japanese, or morphologically rich languages) to verify whether the correlation between training data repetitions and degeneration holds across languages with different structural properties.

2. **Architectural Ablation Study**: Implement repetition dropout on architectures beyond transformers (e.g., RNNs or newer architectures like Mamba) to determine whether the mechanism is specific to attention-based models or represents a more general principle of language model training.

3. **Semantic Repetition Analysis**: Extend the evaluation beyond n-gram metrics to include semantic similarity measures (e.g., using sentence embeddings) to determine whether repetition dropout also mitigates semantic degeneration and topic drift, not just surface-level repetition.