---
ver: rpa2
title: Hyperbolic Active Learning for Semantic Segmentation under Domain Shift
arxiv_id: '2306.11180'
source_url: https://arxiv.org/abs/2306.11180
tags:
- hyperbolic
- learning
- radius
- pixels
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HALO, a novel hyperbolic neural network approach
  for active learning in semantic segmentation under domain shift. HALO leverages
  the variations in hyperbolic radii of pixel embeddings as a new data acquisition
  strategy, effectively approximating epistemic uncertainty.
---

# Hyperbolic Active Learning for Semantic Segmentation under Domain Shift

## Quick Facts
- arXiv ID: 2306.11180
- Source URL: https://arxiv.org/abs/2306.11180
- Reference count: 40
- Primary result: HALO achieves state-of-the-art performance in active learning for semantic segmentation under domain shift, surpassing fully-supervised domain adaptation with only 1% labeled data

## Executive Summary
This paper introduces HALO, a novel hyperbolic neural network approach for active learning in semantic segmentation under domain shift. HALO leverages variations in hyperbolic radii of pixel embeddings as a new data acquisition strategy, effectively approximating epistemic uncertainty. The method is motivated by a geometric interpretation of the hyperbolic radius as an indicator of class complexity and label scarcity. Extensive experiments on GTA V→Cityscapes and SYNTHIA→Cityscapes benchmarks demonstrate that HALO sets a new state-of-the-art in active learning for semantic segmentation under domain shift, achieving superior performance with minimal labeled data.

## Method Summary
HALO is a hyperbolic active learning framework for semantic segmentation under domain shift. It uses a hyperbolic neural network to map pixel embeddings into the Poincaré ball, where classes are organized by complexity. The method computes an acquisition score combining hyperbolic boundaries (entropy of quantized radius variations) and prediction entropy to identify informative pixels for labeling. A Hyperbolic Feature Reweighting (HFR) module stabilizes training by preventing embeddings from approaching the Poincaré ball boundary where gradients vanish. The framework operates in rounds, selecting pixels based on the acquisition score and updating the model with newly labeled data.

## Key Results
- HALO surpasses state-of-the-art methods on GTA V→Cityscapes and SYNTHIA→Cityscapes benchmarks
- With only 1% labeled data, HALO outperforms fully-supervised domain adaptation methods
- The method achieves class compactness and identifies informative pixels both at class contours and within classes
- Negative correlation (ρ = -0.493) between per-class average radius and relative class accuracy validates the radius as class difficulty indicator

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The hyperbolic radius serves as a proxy for class difficulty and label scarcity, enabling better selection of informative pixels.
- **Mechanism**: In the hyperbolic manifold, classes are mapped to compact regions with similar intra-class radius variance. More complex and less known classes are placed closer to the Poincaré ball edge where the space is denser, resulting in larger average radii. Variations in these radii highlight both inter-class contours and intra-class details.
- **Core assumption**: The learned hyperbolic embedding naturally organizes classes by complexity without enforcing hierarchical constraints during training.
- **Evidence anchors**:
  - [abstract] "Analysis of the data statistics leads to a novel interpretation of the hyperbolic radius as an indicator of data scarcity."
  - [section] "Fig. 2a illustrates the correlation between the per-class average radius and the relative class SS accuracy. They correlate negatively with a significant ρ = −0.493."
  - [corpus] Weak - no direct neighbor evidence; the interpretation is novel.
- **Break condition**: If training enforces hierarchical labels, the radius would represent parent-child relationships instead, breaking this mechanism.

### Mechanism 2
- **Claim**: Hyperbolic boundaries computed from quantized radius variations provide a robust acquisition score complementary to prediction entropy.
- **Mechanism**: By discretizing the radius range into K values and computing the entropy of these quantized values within local regions, the method captures both the number and distribution of distinct radius values. This entropy score (hyperbolic boundaries) identifies regions with high class complexity and informative details.
- **Core assumption**: Entropy of quantized radius values within a region is a meaningful proxy for classification difficulty.
- **Evidence anchors**:
  - [abstract] "The hyperbolic radius, complemented by the widely-adopted prediction entropy, effectively approximates epistemic uncertainty."
  - [section] "We propose an entropic formulation of the radius quantized variations within the regions, to consider the number and distribution of discrete intervals."
  - [corpus] Weak - no direct neighbor evidence; this is a novel acquisition strategy.
- **Break condition**: If the radius quantization is too coarse or fine, the entropy measure may not capture meaningful variations, reducing acquisition effectiveness.

### Mechanism 3
- **Claim**: Hyperbolic Feature Reweighting (HFR) stabilizes training by addressing vanishing gradients near the Poincaré ball boundary.
- **Mechanism**: HFR reweights features before projecting them onto the Poincaré ball, preventing embeddings from approaching the boundary where gradients vanish. This stabilizes the training dynamics without harming performance.
- **Core assumption**: The instability issue is primarily due to feature magnitudes approaching the boundary, not curvature or optimization parameters.
- **Evidence anchors**:
  - [section] "During training, HNN can be susceptible to instability issues due to the inherent topology of the hyperbolic manifold. Specifically, when embeddings are close to the border, vanishing gradients can occur."
  - [section] "We introduce the Hyperbolic Feature Reweighting (HFR) module, designed to enhance training stability by reweighting features, prior to their projection onto the Poincaré ball."
  - [corpus] Weak - no direct neighbor evidence; HFR is a novel stabilization technique.
- **Break condition**: If the reweighting introduces bias or if the model architecture is inherently stable, HFR may provide minimal benefit or degrade performance.

## Foundational Learning

- **Concept**: Poincaré ball geometry and hyperbolic embeddings
  - Why needed here: The entire approach relies on mapping pixel embeddings into hyperbolic space where class organization and radius interpretation are meaningful.
  - Quick check question: What is the formula for the Poincaré distance between two points in the ball?

- **Concept**: Entropy and uncertainty estimation in active learning
  - Why needed here: The acquisition score combines hyperbolic boundaries with prediction entropy to approximate epistemic uncertainty.
  - Quick check question: How does the entropy of predicted class probabilities relate to model uncertainty?

- **Concept**: Domain adaptation and semantic segmentation
  - Why needed here: The method operates under domain shift, adapting a source-trained model to a target domain with minimal labels.
  - Quick check question: What are the key challenges in unsupervised domain adaptation for semantic segmentation?

## Architecture Onboarding

- **Component map**: Feature extractor → HFR → expmap → Hyper MLR → pseudo-labels → hyperbolic boundaries + entropy → acquisition score → label selection → loss computation
- **Critical path**: Feature extraction → HFR → expmap → Hyper MLR → acquisition score computation
- **Design tradeoffs**: Hyperbolic vs Euclidean space (performance vs. training stability), region size for boundary computation (coverage vs. precision), embedding dimension (representation power vs. computational cost)
- **Failure signatures**: Vanishing gradients during training (indicates need for HFR), poor class separation in radius map (indicates embedding issues), acquisition score dominated by one component (indicates imbalance)
- **First 3 experiments**:
  1. Verify Poincaré distance computation and embedding projection work correctly
  2. Test hyperbolic boundary computation on a simple synthetic dataset with known class distributions
  3. Validate HFR stabilizes training by comparing loss curves with and without HFR

## Open Questions the Paper Calls Out
- [No open questions explicitly called out in the paper]

## Limitations
- The effectiveness of hyperbolic radius as a proxy for class difficulty relies on the learned embedding naturally organizing classes by complexity without explicit hierarchical constraints, which may not generalize to datasets with different class relationships
- The HFR module's impact on training stability is demonstrated but the exact mechanism and optimal parameters are not fully explored
- The region size for hyperbolic boundary computation is fixed at 5x5 pixels, which may not be optimal for all datasets or feature resolutions

## Confidence

- **High**: The overall active learning framework combining hyperbolic embeddings with entropy-based acquisition works effectively
- **Medium**: The novel interpretation of hyperbolic radius as a class difficulty indicator and the HFR module's stabilization effect
- **Low**: The optimal quantization parameters for radius maps and region size for boundary computation

## Next Checks
1. Test HALO on additional domain adaptation scenarios (e.g., different source-target pairs) to verify generalizability of the hyperbolic radius interpretation
2. Conduct ablation studies varying the region size for hyperbolic boundary computation to find optimal parameters for different feature resolutions
3. Analyze the learned hyperbolic embeddings on synthetic datasets with known hierarchical structures to validate whether the radius truly reflects class complexity rather than enforced hierarchy