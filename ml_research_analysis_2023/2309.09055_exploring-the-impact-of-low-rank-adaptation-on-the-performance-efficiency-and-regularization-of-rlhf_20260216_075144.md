---
ver: rpa2
title: Exploring the impact of low-rank adaptation on the performance, efficiency,
  and regularization of RLHF
arxiv_id: '2309.09055'
source_url: https://arxiv.org/abs/2309.09055
tags:
- lora
- training
- alpacafarm
- divergence
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of low-rank adaptation (LoRA) to
  make reinforcement learning from human feedback (RLHF) more computationally efficient.
  By applying LoRA to the policy optimization stage of RLHF, the authors align the
  LLaMA 7B model on the Alpaca dataset using only two A100 GPUs instead of the typical
  eight.
---

# Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF

## Quick Facts
- arXiv ID: 2309.09055
- Source URL: https://arxiv.org/abs/2309.09055
- Reference count: 8
- Key outcome: LoRA-based RLHF achieves 47.5% win rate vs text-davinci-003 using only 2 A100 GPUs instead of 8, outperforming full fine-tuning baselines

## Executive Summary
This paper investigates the use of low-rank adaptation (LoRA) to make reinforcement learning from human feedback (RLHF) more computationally efficient. By applying LoRA to the policy optimization stage of RLHF, the authors align the LLaMA 7B model on the Alpaca dataset using only two A100 GPUs instead of the typical eight. Despite tuning only 0.2% of the model's parameters, their implementation achieves a win rate of 47.5% against text-davinci-003, outperforming the publicly-released AlpacaFarm checkpoint with full model fine-tuning (46.7% win rate). The authors also analyze different regularization schemes and find that removing the KL penalty term does not harm performance under LoRA, while using Jensen-Shannon divergence leads to improved performance. Additionally, they observe that PPO training with LoRA mitigates the negative impact on factuality compared to full model fine-tuning. The paper releases code and pretrained checkpoints to facilitate future research on more efficient RLHF.

## Method Summary
The authors implement LoRA-based RLHF by fine-tuning only the attention projection matrices of LLaMA 7B with rank=8 and α=64, while keeping the majority of the model frozen. They use the Alpaca-52K dataset with 10K samples for supervised fine-tuning, 10K for reward modeling, and 20K for PPO training. The training pipeline employs proximal policy optimization with optional KL regularization (β=0.02) and Jensen-Shannon divergence alternatives. The rollout batch size is 256 and gradient update batch size is 128. The method is evaluated on the AlpacaFarm benchmark, comparing win rates against text-davinci-003 and factuality scores using FActScore.

## Key Results
- LoRA-based RLHF achieves 47.5% win rate against text-davinci-003, outperforming the AlpacaFarm checkpoint with full fine-tuning (46.7%)
- Training requires only 2 A100 GPUs instead of 8, with memory consumption reduced by ~75%
- Removing KL regularization does not harm performance under LoRA setup
- Jensen-Shannon divergence regularization outperforms standard KL regularization
- PPO training with LoRA mitigates factuality degradation compared to full model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA enables efficient RLHF by fine-tuning only a small fraction of parameters while keeping the majority of the pre-trained model frozen.
- Mechanism: In LoRA, the weight matrices of the frozen base model are replaced with low-rank decomposition matrices A and B, which are optimized during training. This reduces memory consumption and computational cost while maintaining model performance.
- Core assumption: The pre-trained model has already learned general language representations, and only a small number of parameters need to be adjusted for alignment with human preferences.
- Evidence anchors:
  - [abstract] "Despite tuning only 0.2% of LLaMA 7B's parameters, our implementation achieves better performance than the publicly-released AlpacaFarm checkpoint with full model fine-tuning."
  - [section] "In LoRA, the pre-trained LLM is frozen while only low-rank decomposition of the weight matrices (commonly just the projection matrices in self-attention) are optimized."
- Break condition: If the pre-trained model does not have sufficient general language capabilities, LoRA may not be able to achieve good performance with such a small fraction of parameters.

### Mechanism 2
- Claim: Removing the KL regularization term in the PPO objective does not harm performance when using LoRA.
- Mechanism: LoRA itself acts as a strong regularizer by keeping most of the pre-trained model's parameters frozen. This prevents the policy from deviating too far from the reference model, reducing the need for additional KL regularization.
- Core assumption: LoRA's parameter efficiency provides sufficient regularization to prevent overfitting and reward hacking during RLHF.
- Evidence anchors:
  - [abstract] "We find that (1) removing this penalty term does not harm performance on the AlpacaFarm evaluation set under our LoRA setup"
  - [section] "We conjecture that LoRA provides implicit regularization by freezing most of the parameters (e.g., feed-forward layers, layernorm, and embeddings), which already discourages large deviations from the reference policy in parameter space."
- Break condition: If the pre-trained model is not sufficiently regularized by LoRA, removing the KL penalty may lead to reward hacking and poor alignment.

### Mechanism 3
- Claim: Alternative divergence estimators, such as Jensen-Shannon divergence, can outperform the standard KL estimator in LoRA-based RLHF.
- Mechanism: Different divergence estimators have varying effects on the policy's deviation from the reference model and the resulting reward. The Jensen-Shannon estimator may provide a better balance between staying close to the reference model and optimizing for the reward.
- Core assumption: The choice of divergence estimator impacts the policy's behavior and the final alignment performance.
- Evidence anchors:
  - [abstract] "we discover that implementing this penalty using different divergence estimators (e.g., Jensen-Shannon divergence) can lead to higher win rates on the AlpacaFarm evaluation set"
  - [section] "The Jensen-Shannon estimator consistently outperforms all other estimators after step 60 in our experiments"
- Break condition: If the Jensen-Shannon estimator does not provide a better balance between reference model proximity and reward optimization, its performance may not surpass the standard KL estimator.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the core technique used to align large language models with human preferences in this work.
  - Quick check question: What are the three main stages of RLHF, and how do they contribute to aligning a language model with human intents?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is the specific reinforcement learning algorithm used in the third stage of RLHF to optimize the policy model.
  - Quick check question: How does PPO's clipped surrogate objective function help stabilize training and prevent large policy updates?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is the parameter-efficient fine-tuning method used to reduce the computational cost of RLHF while maintaining performance.
  - Quick check question: How does LoRA decompose the weight matrices of a pre-trained model, and what are the benefits of this approach?

## Architecture Onboarding

- Component map: Pre-trained LLaMA 7B model -> LoRA adaptation layers (A and B matrices) -> PPO training pipeline -> KL regularization term (optional) -> Reward model -> Reference policy (SFT-10K checkpoint)

- Critical path:
  1. Load pre-trained LLaMA 7B model and freeze most parameters
  2. Add LoRA adaptation layers to the model
  3. Initialize PPO training pipeline with the adapted model
  4. Sample instructions and generate responses during rollout phase
  5. Calculate rewards using the reward model
  6. Optimize the LoRA matrices using the PPO objective (with or without KL regularization)
  7. Repeat rollout and optimization phases for a fixed number of steps

- Design tradeoffs:
  - Memory vs. performance: LoRA reduces memory consumption but may slightly impact performance compared to full fine-tuning.
  - Regularization vs. optimization: Removing KL regularization can speed up training but may increase the risk of reward hacking.
  - Divergence estimator choice: Different estimators (e.g., Jensen-Shannon) may lead to better performance but require additional experimentation.

- Failure signatures:
  - Poor performance: If the LoRA-adapted model does not achieve competitive results, it may indicate insufficient capacity or inappropriate divergence estimator choice.
  - Overfitting: If the model's performance degrades on the evaluation set, it may suggest a need for stronger regularization or early stopping.
  - Memory issues: If the training process runs out of memory, it may require adjusting the batch size, LoRA rank, or using gradient checkpointing.

- First 3 experiments:
  1. Implement LoRA adaptation for the LLaMA 7B model and verify that the memory consumption is significantly reduced compared to full fine-tuning.
  2. Train the LoRA-adapted model using PPO with the standard KL regularization term and evaluate its performance on the AlpacaFarm evaluation set.
  3. Remove the KL regularization term and compare the performance of the LoRA-adapted model to the version with KL regularization.

## Open Questions the Paper Calls Out

- Question: Does the effectiveness of LoRA in RLHF depend on the specific task domain or language?
  - Basis in paper: [inferred]
  - Why unresolved: The paper only experiments with the AlpacaFarm dataset, which contains English instructions. The authors explicitly state that their conclusions may not generalize to other domains, languages, or more complex instruction sets.
  - What evidence would resolve it: Conducting experiments with LoRA-based RLHF on datasets in different languages, domains, or with more complex instructions would provide evidence of its generalizability.

- Question: What is the optimal rank k for LoRA in RLHF, and how does it affect performance and efficiency?
  - Basis in paper: [explicit]
  - Why unresolved: The paper uses a rank of 8 for LoRA based on the original LoRA setup, but does not explore the impact of varying the rank on performance and efficiency.
  - What evidence would resolve it: Conducting experiments with different ranks for LoRA in RLHF and comparing the resulting performance and efficiency would provide insights into the optimal rank.

- Question: How does the choice of regularization estimator (e.g., KL, Jensen-Shannon, Bregman) affect the performance of RLHF with full model fine-tuning?
  - Basis in paper: [inferred]
  - Why unresolved: The paper only investigates the impact of regularization estimators when using LoRA, not full model fine-tuning. The authors state that they could not run RLHF on the LLaMA 7B checkpoint using full model fine-tuning due to limited compute resources.
  - What evidence would resolve it: Conducting experiments with full model fine-tuning and varying the regularization estimators would provide insights into their impact on RLHF performance.

## Limitations

- The comparison to full fine-tuning is limited to a single baseline (AlpacaFarm) with a narrow win-rate margin (47.5% vs 46.7%), making it difficult to assess the general effectiveness of LoRA across different model sizes and tasks.
- The ablation study on KL regularization and divergence estimators is conducted within a constrained experimental setup, and the findings may not generalize to other datasets or model architectures.
- The analysis of factuality improvements is based on FActScore, which may not capture all aspects of factuality degradation that can occur during RLHF.

## Confidence

- High confidence: The core claim that LoRA can reduce GPU memory requirements from 8 to 2 A100s while maintaining competitive performance is well-supported by the experimental results and aligns with the established properties of LoRA.
- Medium confidence: The finding that removing KL regularization does not harm performance under LoRA is based on experiments with a single dataset and model size, and may not generalize to other RLHF scenarios.
- Medium confidence: The observation that Jensen-Shannon divergence outperforms KL divergence in this setup is based on experiments at specific training steps, and the choice of divergence estimator likely interacts with other hyperparameters in complex ways.

## Next Checks

1. **Dataset generalization test**: Replicate the KL regularization ablation study on alternative alignment datasets (such as ShareGPT or open versions of human preference data) to determine if the findings hold across different data distributions and quality levels.

2. **Model scale validation**: Apply the same LoRA + RLHF pipeline to larger models (LLaMA 13B, 30B, 65B) to assess whether the efficiency gains and performance characteristics scale proportionally, and whether KL regularization becomes more or less important at larger scales.

3. **Factuality robustness evaluation**: Conduct a systematic analysis of factuality across multiple evaluation metrics (not just FActScore) to identify whether the apparent improvement is consistent across different types of factual errors and whether it persists under different prompting strategies.