---
ver: rpa2
title: LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination
arxiv_id: '2312.15224'
source_url: https://arxiv.org/abs/2312.15224
tags:
- soup
- human
- action
- mind
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We present a Hierarchical Language Agent for real-time human-AI
  coordination in the Overcooked game, where players can communicate with natural
  language and cooperate to serve orders. The agent consists of three modules: a proficient
  LLM for intention reasoning and language interaction, a lightweight LLM for generating
  macro actions, and a reactive policy for transforming macro actions into atomic
  actions.'
---

# LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination

## Quick Facts
- arXiv ID: 2312.15224
- Source URL: https://arxiv.org/abs/2312.15224
- Authors: 
- Reference count: 40
- Key outcome: HLA achieves approximately 50% higher game scores and highest human preference in Overcooked human-AI coordination

## Executive Summary
This paper presents a Hierarchical Language Agent (HLA) for real-time human-AI coordination in the Overcooked game, where players communicate via natural language to complete cooking tasks. The agent uses a three-tier architecture: a proficient LLM (Slow Mind) for intention reasoning and language interaction, a lightweight LLM (Fast Mind) for generating macro actions, and a reactive policy (Executor) for transforming macro actions into atomic actions. Human studies demonstrate that HLA outperforms baseline agents including slow-mind-only and fast-mind-only variants, achieving superior cooperation abilities, faster responses, and more consistent language communications with approximately 50% higher game scores.

## Method Summary
The method employs a hierarchical framework where Slow Mind (GPT-3.5) interprets human commands and reasons about intentions asynchronously with Fast Mind (Llama2-13B-chat), which generates macro actions with an action filtering mechanism. Executor, a script policy, converts these macro actions into atomic actions at high frequency. The design separates reasoning from real-time execution, allowing high-level command interpretation without blocking atomic action generation. Fast Mind uses conditional prompting that adapts to Slow Mind's progress, initially using raw commands until intention reasoning completes. The action filtering prioritizes task-relevant macro actions based on LLM probabilities and hardcoded value functions.

## Key Results
- HLA achieves approximately 50% higher game scores compared to baseline agents
- Superior human preference ratings compared to slow-mind-only, fast-mind-only, and no-executor agents
- Faster macro action and atomic action latencies enabling real-time coordination
- Stronger cooperation abilities and more consistent language communications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical design separates reasoning from real-time execution, allowing high-level command interpretation without blocking atomic action generation.
- Mechanism: Slow Mind interprets vague human commands and provides intentions to Fast Mind, while Fast Mind and Executor generate macro and atomic actions at medium and high frequencies respectively.
- Core assumption: Human commands and high-level planning can tolerate higher latency than atomic actions.
- Evidence anchors:
  - [abstract]: "HLA adopts a hierarchical framework that consists of three modules: a proficient LLM, referred to as Slow Mind, for intention reasoning and language interaction, a lightweight LLM, referred to as Fast Mind, for generating macro actions, and a reactive policy, referred to as Executor, for transforming macro actions into atomic actions."
  - [section 4.1]: "HLA combines both robust reasoning and interaction capabilities from a large model and real-time inference from a smaller model and a reactive policy."

### Mechanism 2
- Claim: Conditional prompting in Fast Mind allows it to work asynchronously with Slow Mind, maintaining responsiveness while awaiting intent clarification.
- Mechanism: Fast Mind uses raw commands until Slow Mind completes intention reasoning, then switches to inferred intentions, and finally uses chat messages when commands are satisfied.
- Core assumption: The lightweight LLM can generate reasonable macro actions from raw commands while awaiting refined intent.
- Evidence anchors:
  - [section 4.3.1]: "Fast Mind uses the raw human command as input before Slow Mind successfully infers the intention. This asynchronous execution nature results in the conditional prompt mechanism used by Fast Mind."
  - [section 4.3]: "To better align macro action generation with vague human commands, Fast Mind also uses the inferred intention from Slow Mind as input."

### Mechanism 3
- Claim: The action filtering mechanism in Fast Mind prioritizes macro actions based on task relevance, preventing suboptimal moves when no command is active.
- Mechanism: Selection probabilities combine LLM output probabilities with hardcoded task-relevant values, adjusted dynamically based on command satisfaction status.
- Core assumption: Task-relevant value functions can be hardcoded effectively for the Overcooked domain.
- Evidence anchors:
  - [section 4.3.2]: "We employ an action filter to filter out sub-optimal macro actions. The selection probabilities are calculated based on the probabilities output by the language model and the task-relevant value of each available macro action."
  - [section 4.4]: "The values for ð‘‰ (ð‘Ž|ð‘ ) are hardcoded, and their detailed information can be found in the Appendix B.2."

## Foundational Learning

- Concept: Large Language Model inference latency and its impact on real-time systems
  - Why needed here: Understanding why direct LLM action generation fails in fast-paced games
  - Quick check question: What is the typical latency range for GPT-3.5 API calls, and why does this matter for 2.5Hz action requirements?

- Concept: Hierarchical reasoning frameworks (System 1/System 2 thinking)
  - Why needed here: The architectural inspiration for separating fast reactive components from slow deliberative ones
  - Quick check question: How does the System 1/System 2 framework map to the Executor/Fast Mind/Slow Mind structure?

- Concept: Macro vs. atomic action distinction in sequential decision-making
  - Why needed here: Understanding why high-level planning can be slower than low-level execution
  - Quick check question: What are the trade-offs between planning at macro vs. atomic levels in terms of flexibility and latency?

## Architecture Onboarding

- Component map:
  - Slow Mind (GPT-3.5) -> Fast Mind (Llama2-13B-chat) -> Executor (script policy)

- Critical path:
  1. Human command arrives â†’ both Slow Mind and Fast Mind process
  2. Slow Mind reasons intent â†’ sends to Fast Mind
  3. Fast Mind generates macro actions â†’ Executor executes atomically
  4. Slow Mind periodically checks completion â†’ updates chat

- Design tradeoffs:
  - Using heavyweight LLM for reasoning vs. lightweight for action
  - Script policy vs. learned policy for Executor
  - Synchronous vs. asynchronous processing between modules

- Failure signatures:
  - High macro action latency â†’ check Slow Mind or network
  - Poor command interpretation â†’ verify Slow Mind reasoning
  - Suboptimal actions â†’ examine Fast Mind filtering or value functions
  - Executor failures â†’ check script policy or environment state

- First 3 experiments:
  1. Measure latency of each module independently to identify bottlenecks
  2. Test command interpretation with increasing complexity to find reasoning limits
  3. Compare Executor performance with scripted vs. learned policies on basic tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Executor be replaced with a learned goal-conditioned reinforcement learning policy to improve low-level execution performance?
- Basis in paper: [explicit] The paper mentions this as a topic for future studies in the conclusion section.
- Why unresolved: The current Executor uses hard-coded scripts, which may not be optimal for all scenarios and could benefit from learning.
- What evidence would resolve it: Experiments comparing the performance of the current Executor with a learned goal-conditioned reinforcement learning policy in terms of task completion, latency, and game scores.

### Open Question 2
- Question: How does the performance of HLA change when using different LLM architectures (e.g., GPT-4) in the Slow Mind module?
- Basis in paper: [inferred] The paper mentions substituting GPT-3.5 with GPT-4 as a potential improvement, but does not provide experimental results.
- Why unresolved: The paper only tests HLA with GPT-3.5 and does not explore the impact of using more advanced LLM architectures.
- What evidence would resolve it: Experiments comparing the performance of HLA with different LLM architectures in terms of command interpretation accuracy, completion time, and game scores.

### Open Question 3
- Question: How does the two-stage design of Slow Mind compare to a single-stage design in terms of intention reasoning and command completion assessment?
- Basis in paper: [explicit] The paper mentions an ablation study on the two-stage design, but does not provide detailed results or analysis.
- Why unresolved: The paper only briefly mentions the ablation study without providing a thorough comparison of the two designs.
- What evidence would resolve it: Detailed experimental results and analysis comparing the performance of HLA with the two-stage design and a single-stage design in terms of intention reasoning accuracy, completion time, and game scores.

## Limitations
- Single Environment Constraint: All experiments conducted exclusively in Overcooked game environment, limiting generalizability to other real-time coordination domains.
- Simulation-Based Validation: Complex command evaluation relies on simulated rather than actual human participants for performance measurement.
- Architecture Specificity: Effectiveness depends heavily on the assumption that high-level reasoning can tolerate higher latency than atomic actions.

## Confidence
- High Confidence: The latency measurements (Section 5.1) showing HLA's superior response times are straightforward empirical results that can be independently verified.
- Medium Confidence: The game score improvements (approximately 50% higher) and human preference ratings are convincing but may be influenced by the specific implementation details and Overcooked environment characteristics.
- Low Confidence: The complex command success rates and completion times (Section 5.4) require careful interpretation due to the simulated evaluation methodology and lack of detailed validation protocols.

## Next Checks
1. **Cross-Domain Transfer**: Implement HLA in a different real-time coordination environment (such as a collaborative navigation task) and measure whether the hierarchical design maintains its performance advantage without extensive domain-specific tuning.

2. **Human-In-The-Loop Verification**: Re-run the complex command experiments with actual human participants rather than simulation, measuring both objective metrics (success rates, completion times) and subjective experience (preference ratings, perceived coherence).

3. **Latency Sensitivity Analysis**: Systematically vary the relative latencies between Slow Mind, Fast Mind, and Executor to determine the minimum latency thresholds required for each component, testing the assumption that high-level reasoning can tolerate higher latency than atomic execution.