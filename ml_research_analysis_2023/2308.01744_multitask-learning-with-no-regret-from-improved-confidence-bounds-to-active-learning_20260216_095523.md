---
ver: rpa2
title: 'Multitask Learning with No Regret: from Improved Confidence Bounds to Active
  Learning'
arxiv_id: '2308.01744'
source_url: https://arxiv.org/abs/2308.01744
tags:
- learning
- tasks
- bound
- regret
- multitask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work provides new multitask confidence intervals for kernel-based\
  \ multitask regression. By refining the analysis of the multitask information gain,\
  \ the authors derive tighter confidence widths (by up to \u221AN) than previous\
  \ naive approaches."
---

# Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning

## Quick Facts
- arXiv ID: 2308.01744
- Source URL: https://arxiv.org/abs/2308.01744
- Reference count: 40
- This work provides new multitask confidence intervals for kernel-based multitask regression that can improve regret bounds by up to √N over naive approaches.

## Executive Summary
This paper develops improved confidence intervals for multitask kernel regression that exploit task similarity, leading to better online and active learning performance. The authors refine the analysis of multitask information gain to derive tighter confidence bounds that hold in adaptive settings without requiring i.i.d. data. These confidence intervals enable new regret guarantees that can significantly improve over treating tasks independently, and lead to a novel multitask active learning algorithm that selects which task to query at each round to minimize regret.

## Method Summary
The method centers on constructing multitask confidence intervals for kernel-based multitask regression. The authors develop a multitask kernel that encodes task similarity through a parameterized Gram matrix, then refine the analysis of multitask information gain to derive tighter confidence widths. These intervals are applied to both online learning (via MT-UCB algorithm) and active learning (via MT-AL algorithm), with regret bounds that depend on task similarity parameters. The approach is validated on synthetic data and drug discovery applications.

## Key Results
- Improved multitask confidence intervals that are up to √N tighter than naive approaches
- Online learning regret bounds that can significantly improve over independent task treatment depending on task similarity
- Novel multitask active learning algorithm that achieves sublinear regret by selecting the most informative task to query at each round

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multitask regression can significantly tighten confidence intervals by exploiting task similarity, leading to better online and active learning performance.
- Mechanism: The multitask kernel encodes task similarity through a parameterized Gram matrix. By refining the analysis of the multitask information gain and using the structured kernel matrix, the authors derive tighter confidence widths that are up to √N smaller than naive approaches.
- Core assumption: Tasks share a common structure that can be captured by the multitask kernel, and the task similarity parameter b can be chosen appropriately (or adapted online).
- Evidence anchors:
  - [abstract]: "Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently."
  - [section 2.2]: Provides explicit formulas showing how the confidence widths improve by up to √N over naive choices.
  - [corpus]: No direct mention of √N improvement, but related work (e.g., [9]) uses different setups and does not achieve this gain.
- Break condition: If task similarity is low (high ϵ), or the kernel parameter b is not chosen well, the improvement over independent learning vanishes.

### Mechanism 2
- Claim: The multitask confidence intervals do not require i.i.d. data and can be applied to adaptive settings like online and active learning.
- Mechanism: By leveraging the multitask kernel and information gain, the authors construct confidence intervals that hold in the adaptive setting (no i.i.d. assumption). These intervals are then used to guide exploration in online and active learning algorithms.
- Core assumption: The multitask kernel and regularization parameter λ can be set such that the confidence intervals are valid in the adaptive setting.
- Evidence anchors:
  - [abstract]: "The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning."
  - [section 3.1]: Shows how the multitask UCB algorithm uses these intervals to achieve sublinear regret in the online setting.
  - [corpus]: Related work (e.g., [6]) uses naive confidence intervals but does not discuss the adaptive setting as explicitly.
- Break condition: If the regularization parameter λ is not chosen appropriately, or if the multitask kernel does not capture the task structure well, the confidence intervals may not be valid.

### Mechanism 3
- Claim: The multitask active learning algorithm can achieve sublinear regret by selecting the most informative task to query at each round.
- Mechanism: The MT-AL algorithm uses the multitask confidence intervals to decide which task to query at each round. It selects the task for which the believed optimizer has the largest uncertainty, as measured by the product of the confidence width and the predictive variance.
- Core assumption: The multitask confidence intervals are valid and the task similarity can be exploited to reduce the number of queries needed.
- Evidence anchors:
  - [abstract]: "As a second key application of our results, we introduce a novel multitask active learning setup where several tasks must be simultaneously optimized, but only one of them can be queried for feedback by the learner at each round."
  - [section 4]: Provides the MT-AL algorithm and proves that it achieves sublinear active learning regret.
  - [corpus]: No direct mention of this specific active learning setup, but related work (e.g., [7, 16]) studies offline contextual Bayesian optimization with full feedback.
- Break condition: If the multitask confidence intervals are not valid, or if the task similarity is low, the active learning regret may not be sublinear.

## Foundational Learning

- Concept: Multitask kernel regression
  - Why needed here: The multitask kernel is the key tool for exploiting task similarity and deriving the improved confidence intervals.
  - Quick check question: How does the multitask kernel encode task similarity, and how does it differ from using independent kernels for each task?

- Concept: Information gain
  - Why needed here: The multitask information gain quantifies the reduction in uncertainty about the multitask function after observing a given set of data points. It is used to characterize the confidence intervals and regret bounds.
  - Quick check question: How does the multitask information gain differ from the single-task information gain, and how does it depend on the task similarity parameter b?

- Concept: Confidence intervals for kernel regression
  - Why needed here: The improved confidence intervals are the main technical contribution of the paper, and they are used to derive the new regret bounds and active learning algorithm.
  - Quick check question: How are the multitask confidence intervals constructed, and how do they improve over naive approaches?

## Architecture Onboarding

- Component map: Multitask kernel -> Improved confidence intervals -> MT-UCB algorithm -> Online learning regret bounds; Multitask kernel -> Improved confidence intervals -> MT-AL algorithm -> Active learning regret bounds
- Critical path:
  1. Construct the multitask kernel and derive the improved confidence intervals
  2. Use the confidence intervals to design the online learning algorithm (MT-UCB) and prove its regret bounds
  3. Use the confidence intervals to design the active learning algorithm (MT-AL) and prove its regret bounds
- Design tradeoffs:
  - Choosing the multitask kernel parameter b: Affects the tightness of the confidence intervals and the regret bounds. Should be chosen based on the task similarity (or adapted online)
  - Choosing the regularization parameter λ: Affects the validity of the confidence intervals in the adaptive setting. Should be chosen appropriately based on b and the multitask kernel
- Failure signatures:
  - If task similarity is low, the improvement over independent learning vanishes
  - If the multitask kernel does not capture the task structure well, the confidence intervals may not be valid
  - If the confidence intervals are not valid, the online and active learning algorithms may not achieve sublinear regret
- First 3 experiments:
  1. Generate synthetic data with varying task similarity (controlled by parameter δ) and compare the performance of MT-UCB with naive and improved confidence intervals
  2. Use the MHC-I drug discovery dataset and compare the performance of MT-AL with uniform sampling and other baselines
  3. Vary the number of tasks N and the task similarity parameter b, and study how the regret bounds change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the improved multitask confidence intervals be extended to more general kernel structures beyond the clique graph structure?
- Basis in paper: [explicit] The paper discusses extending the results to different graph structures and notes that the current analysis heavily exploits the structure of the task Gram matrix Ktask(b).
- Why unresolved: The paper does not provide a general framework for extending the confidence intervals to arbitrary kernel structures, and it is unclear how the specific properties of the clique graph structure could be generalized.
- What evidence would resolve it: Developing a general theory for multitask confidence intervals that applies to arbitrary kernel structures and demonstrating its effectiveness on various benchmark problems.

### Open Question 2
- Question: How can the multitask confidence intervals be utilized in safety-critical systems to assess uncertainty and ensure safe decision-making?
- Basis in paper: [explicit] The paper mentions the potential of the multitask confidence intervals in safety-critical systems and notes that they could be used to assess uncertainty and balance exploration-exploitation in multitask reinforcement learning.
- Why unresolved: The paper does not provide specific examples or algorithms for applying the confidence intervals to safety-critical systems, and it is unclear how they could be integrated into existing safety frameworks.
- What evidence would resolve it: Developing and testing algorithms that use the multitask confidence intervals for safe decision-making in safety-critical applications, such as autonomous driving or medical diagnosis.

### Open Question 3
- Question: How does the number of tasks and their similarity affect the performance of multitask learning compared to independent learning?
- Basis in paper: [explicit] The paper discusses how the regret bounds depend on the number of tasks N and the task similarity parameter ϵ, and shows that multitask learning can outperform independent learning when tasks are similar and the horizon is small.
- Why unresolved: The paper does not provide a comprehensive analysis of how the performance of multitask learning scales with N and ϵ, and it is unclear how these factors interact with other parameters like the kernel parameter b and the regularization parameter λ.
- What evidence would resolve it: Conducting a systematic experimental study that varies N, ϵ, b, and λ, and measuring the performance of multitask learning compared to independent learning on various benchmark problems.

## Limitations

- The theoretical improvements critically depend on the assumption that tasks share significant similarity; when tasks are largely independent, the improvements vanish
- The multitask kernel parameter b must be chosen appropriately based on task similarity, with sensitivity to initialization not thoroughly characterized
- Validation primarily uses synthetic data with idealized task generation, which may not reflect real-world task structures

## Confidence

- High confidence: The refined multitask information gain analysis and resulting confidence interval formulas are mathematically sound and follow from standard GP regression theory. The √N improvement is well-established in the multitask setting.
- Medium confidence: The online learning regret bounds and their dependence on task similarity (parameter b) are theoretically sound but may be conservative in practice. The bounds are sensitive to the choice of b and the assumed noise model.
- Medium confidence: The active learning regret bounds are derived from the confidence intervals, but the practical performance may depend heavily on the quality of the multitask kernel and the accuracy of the similarity parameter estimation.

## Next Checks

1. Apply the MT-AL algorithm to at least two additional real-world multitask datasets with varying degrees of task similarity to validate that improvements are not dataset-specific.

2. Systematically vary the kernel parameter b across a wide range and measure the impact on regret and active learning performance to quantify the algorithm's robustness to parameter misspecification.

3. Implement and compare against a simpler baseline that clusters similar tasks and applies independent learning within clusters, to determine if the kernel-based approach provides advantages over more interpretable methods.