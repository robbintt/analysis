---
ver: rpa2
title: 'ACES: Translation Accuracy Challenge Sets at WMT 2023'
arxiv_id: '2311.01153'
source_url: https://arxiv.org/abs/2311.01153
tags:
- metrics
- translation
- aces
- language
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ACES: Translation Accuracy Challenge Sets at WMT 2023 Benchmark
  of segment-level MT metrics submitted to WMT 2023 using ACES challenge set (36K
  examples, 146 language pairs, 68 linguistic phenomena). Core method: Evaluation
  of metric performance across ten top-level accuracy error categories using ACES-Score
  (weighted Kendall''s tau).'
---

# ACES: Translation Accuracy Challenge Sets at WMT 2023

## Quick Facts
- arXiv ID: 2311.01153
- Source URL: https://arxiv.org/abs/2311.01153
- Reference count: 11
- Primary result: Benchmark of segment-level MT metrics submitted to WMT 2023 using ACES challenge set (36K examples, 146 language pairs, 68 linguistic phenomena)

## Executive Summary
ACES evaluates segment-level MT metrics submitted to WMT 2023 using a comprehensive challenge set covering 68 linguistic phenomena across 146 language pairs. The benchmark uses ACES-Score (weighted Kendall's tau) to assess metric performance across ten top-level accuracy error categories. Results show no clear winner among metrics, with KG-BERTScore and COMETKiwi performing best overall. Reference-free metrics perform on par or better than reference-based metrics across most error categories, though performance varies significantly across error types with addition, undertranslation, real-world knowledge, and wrong language being most challenging.

## Method Summary
The ACES benchmark evaluates MT metrics using a challenge set of 36K examples covering 68 linguistic phenomena across 146 language pairs. Metrics are assessed using ACES-Score, which computes weighted Kendall's tau correlation between metric scores and human judgments on pairwise comparisons (good vs incorrect translation). The evaluation spans ten top-level accuracy error categories, comparing reference-based and reference-free metrics, and analyzing performance across different language pairs and error types. The study also compares 2023 metric versions against their 2022 counterparts to assess improvements.

## Key Results
- No single metric dominates across all error categories; KG-BERTScore and COMETKiwi perform best overall
- Reference-free metrics perform on par or better than reference-based metrics across most error categories
- Addition, undertranslation, real-world knowledge, and wrong language categories are most challenging for all metrics
- Performance varies significantly across error categories, with no metric excelling uniformly
- Multilingual embeddings may harm performance when translations are extremely poor quality

## Why This Works (Mechanism)

### Mechanism 1
Reference-free metrics avoid surface-level overlap pitfalls by focusing on source-target semantic similarity rather than source-reference alignment. This approach is more robust when translations have structural differences from references but maintain semantic meaning.

### Mechanism 2
Challenge sets reveal systematic metric weaknesses that general test sets obscure by isolating specific linguistic phenomena. This targeted approach exposes performance gaps and enables actionable improvements for metric developers.

### Mechanism 3
Multilingual embeddings can harm metric performance when translations are poor quality because language-agnostic representations may fail to distinguish between untranslated content and poorly translated content, leading to incorrect evaluations.

## Foundational Learning

- **Kendall's tau-like correlation for ranking evaluation**: Needed to measure metric performance on pairwise comparisons; Quick check: Why use Kendall's tau instead of Pearson correlation for this task?
- **Error category hierarchies in MT evaluation**: Needed to systematically evaluate metric performance across different error types; Quick check: How does hierarchical organization help identify metric strengths and weaknesses?
- **Reference-based vs reference-free metrics**: Needed to understand their relative strengths across error types; Quick check: What design differences lead to different performance patterns?

## Architecture Onboarding

- **Component map**: Challenge set construction → Metric evaluation → Performance analysis → Recommendations
- **Critical path**: Construct challenge sets with controlled errors → Run metrics on all examples → Compute Kendall's tau per category → Analyze performance patterns → Draw conclusions
- **Design tradeoffs**: Broad language coverage vs deep linguistic analysis; automatic vs manual example generation; reference-based vs reference-free evaluation
- **Failure signatures**: Metrics failing on specific error categories; inconsistent performance across language pairs; poor correlation on real-world knowledge tasks
- **First 3 experiments**:
  1. Run all metrics on a small subset of ACES (e.g., mistranslation categories) to verify implementation
  2. Compare baseline metrics (BLEU, chrF) against neural metrics on addition/omission categories
  3. Test multilingual embeddings impact by evaluating metrics on untranslated and wrong language categories

## Open Questions the Paper Calls Out

### Open Question 1
How does performance vary across different language pairs, particularly for low-resource languages? The paper covers 146 language pairs but doesn't provide detailed breakdowns by individual language or resource level. Evidence would include detailed correlation scores broken down by language pairs and comparisons between high-resource and low-resource settings.

### Open Question 2
What specific design strategies could improve LLM performance on translation evaluation tasks? The paper notes that LLM-based metrics struggle with the challenge set and suggests better design strategies are needed, but doesn't propose specific solutions. Evidence would include development and testing of new LLM-based metric designs that successfully outperform existing metrics.

### Open Question 3
How does training data size specifically impact metric performance across different ACES error categories? The paper analyzes COMETOID22 with different training data cutoffs but doesn't investigate how this affects other metrics or varies across error categories. Evidence would include systematic experiments varying training data size for multiple metrics across all ACES error categories.

## Limitations

- ACES focuses only on accuracy, excluding fluency-related errors that are crucial for real-world translation quality
- Challenge set construction based on expert knowledge rather than comprehensive error analysis may result in gaps or redundancies
- Limited generalizability across domains and language pairs, particularly for low-resource languages
- Some error categories (real-world knowledge, discourse) showed unexpectedly poor performance that may reflect limitations in both metric design and challenge set construction

## Confidence

**High Confidence**: No single metric dominates across all error categories; this is well-supported by comprehensive evaluation methodology and clear performance variations.

**Medium Confidence**: Specific ranking of top metrics (KG-BERTScore, COMETKiwi) and observations about multilingual embeddings' impact are plausible but depend on particular challenge set construction.

**Low Confidence**: Hypotheses about why certain error categories are more challenging require further investigation; explanations about surface-level overlap effects are reasonable but not definitively proven.

## Next Checks

1. **Cross-domain validation**: Test the same metrics on domain-specific challenge sets (medical, legal, technical) to assess whether performance patterns generalize beyond general-purpose ACES.

2. **Error category refinement**: Conduct error analysis on metrics' failures in most challenging categories (addition, undertranslation, real-world knowledge) to determine whether failures reflect genuine metric weaknesses or challenge set limitations.

3. **Multilingual embeddings ablation**: Systematically evaluate impact of removing multilingual components from metrics that use them, focusing on cases with untranslated or wrong language content to validate language-agnostic representation hypothesis.