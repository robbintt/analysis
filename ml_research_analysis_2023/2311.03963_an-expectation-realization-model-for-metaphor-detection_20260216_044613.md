---
ver: rpa2
title: An Expectation-Realization Model for Metaphor Detection
arxiv_id: '2311.03963'
source_url: https://arxiv.org/abs/2311.03963
tags:
- word
- metaphor
- used
- dataset
- melbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new metaphor detection architecture based
  on an expectation-realization model, where one component estimates the expected
  literal meaning given a context and another estimates the realized meaning of the
  actual word used in context. The idea is that metaphorical word usage leads to a
  violation of literal word expectations.
---

# An Expectation-Realization Model for Metaphor Detection

## Quick Facts
- arXiv ID: 2311.03963
- Source URL: https://arxiv.org/abs/2311.03963
- Reference count: 24
- This paper proposes a new metaphor detection architecture based on an expectation-realization model, where one component estimates the expected literal meaning given a context and another estimates the realized meaning of the actual word used in context.

## Executive Summary
This paper introduces an Expectation-Realization (ER) model for metaphor detection that explicitly models the contrast between expected literal meanings and realized word usage in context. The model consists of two components: an expectation module that predicts what word should appear given the context, and a realization module that encodes the actual word used. The key insight is that metaphorical usage creates a violation of literal word expectations. Experiments on three metaphor datasets (VUA-20, LCC, TroFi) demonstrate that the ER model outperforms strong baselines, particularly in out-of-distribution settings where novel metaphors or unseen words are involved. The approach also benefits from ensembling multiple ER models.

## Method Summary
The ER model uses a pre-trained language model (RoBERTa-base) with two processing streams: one that masks the target word to compute expected literal meaning, and another that keeps the word to compute realized meaning. The expectation and realization components produce embeddings that are fused at both word and sentence levels using MLP layers. The model is trained with cross-entropy loss for classification and a similarity loss that keeps the fine-tuned model's expectation embeddings close to the original pre-trained embeddings, helping preserve literal meanings. The architecture is evaluated on three metaphor detection datasets using within-distribution, out-of-distribution, and novel metaphor generalization settings.

## Key Results
- ER model outperforms strong baselines (including BERT, RoBERTa, and ERNIE) on VUA-20, LCC, and TroFi datasets
- ER model shows superior performance in out-of-distribution settings, particularly for novel metaphors and unseen words
- Ensembling 2-5 ER models provides additional accuracy improvements across all datasets
- The expectation component adds significant value over realization-only baselines, especially for metaphorical usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metaphor detection benefits from modeling the difference between literal expectations and actual word usage in context.
- Mechanism: The architecture explicitly computes two representations: one for expected literal meaning given context (E), and one for the realized meaning of the actual word (R). The contrast between these two signals metaphorical usage.
- Core assumption: Metaphorical usage violates literal expectations, creating a measurable divergence between E and R representations.
- Evidence anchors:
  - [abstract]: "one component estimates the expected literal meaning given a context and another estimates the realized meaning of the actual word used in context. The idea is that metaphorical word usage leads to a violation of literal word expectations."
  - [section 2]: "the use of a metaphorical word leads to a violation of literal word expectations, i.e. surprise."
  - [corpus]: Weak - no direct neighbor papers provide independent verification of this expectation-realization divergence hypothesis for metaphor detection.
- Break condition: If literal and metaphorical words have similar expected distributions in context, or if context provides insufficient signal for expectation computation.

### Mechanism 2
- Claim: Fine-tuning language models with similarity constraints helps maintain literal meaning expectations.
- Mechanism: The loss function includes similarity terms that encourage the fine-tuned model's expectation embeddings to stay close to original pre-trained embeddings, anchoring them to literal meanings.
- Core assumption: Most words are used literally most of the time, so pre-trained embeddings already encode good literal expectations.
- Evidence anchors:
  - [section 2]: "Given that most words in the vocabulary are used with their literal meaning most of the time, the similarity loss serves the purpose of anchoring the fine-tuned LM such that its expectation embeddings v reflect a literal meaning of words."
  - [section 2]: Loss formulation includes "similarity loss Li_Sim = α1 cos (uM,t, vM,t) + α2 cos (uM, vM ) where the embeddings u are obtained from the original pre-trained LM with fixed parameters, whereas the embeddings v are obtained from the fine-tuned LM."
  - [corpus]: Weak - no neighbor papers explicitly discuss similarity-constrained fine-tuning for metaphor detection.
- Break condition: If the similarity constraints are too strong, they may prevent the model from learning meaningful differences for metaphorical usage.

### Mechanism 3
- Claim: Ensembling multiple ER models improves generalization across different metaphor detection settings.
- Mechanism: Combining predictions from multiple independently trained ER models reduces variance and captures diverse patterns in metaphorical usage.
- Core assumption: Different random seeds and training dynamics lead to models that capture complementary aspects of metaphorical patterns.
- Evidence anchors:
  - [abstract]: "Further increases in metaphor detection accuracy are obtained through ensembling of ER models."
  - [section 3.3]: "ensembles ER-Ens of 2 or 5 ER models are shown to further improve the metaphor detection performance."
  - [corpus]: Weak - while ensembling is common in NLP, no neighbor papers specifically demonstrate its effectiveness for metaphor detection with ER models.
- Break condition: If models overfit similarly or if the ensemble size becomes too large relative to dataset size, performance gains may diminish.

## Foundational Learning

- Concept: Expectation-realization modeling
  - Why needed here: This is the core theoretical framework distinguishing the approach from traditional metaphor detection methods.
  - Quick check question: Can you explain why computing both expected literal meaning and realized meaning helps identify metaphors?

- Concept: Pre-trained language model fine-tuning with similarity constraints
  - Why needed here: The similarity loss prevents catastrophic forgetting of literal meanings during fine-tuning.
  - Quick check question: What would happen to the expectation component if we removed the similarity loss from training?

- Concept: Out-of-distribution generalization
  - Why needed here: The paper evaluates performance on unseen words and novel metaphors, requiring understanding of generalization beyond standard accuracy metrics.
  - Quick check question: Why is OOD performance typically lower than within-distribution performance for metaphor detection?

## Architecture Onboarding

- Component map:
  Transformer encoder (RoBERTa-base) -> Expectation module -> Expectation embeddings
  Transformer encoder (RoBERTa-base) -> Realization module -> Realization embeddings
  Expectation embeddings + Realization embeddings -> Fusion layers (f and g) -> Classifier -> Output

- Critical path: Input → Dual Transformer processing → Expectation and Realization embeddings → Fusion layers → Classifier → Output

- Design tradeoffs: 
  Uses dual processing of same input (masked and marked) - increases computation but enables direct comparison
  Similarity loss trades off some fine-tuning flexibility for better literal meaning preservation
  Ensembling improves accuracy but increases inference time

- Failure signatures:
  High accuracy on training but poor OOD performance suggests memorization of word-specific patterns
  Similar performance between E+R and R-only baselines indicates E component isn't learning useful expectations
  Very low performance on LCC dataset compared to others suggests dataset-specific challenges

- First 3 experiments:
  1. Implement R-only baseline (realization component only) and compare to full ER model on VUA-20 to verify E component adds value
  2. Train ER model without similarity loss to understand its impact on performance
  3. Test ER model on a held-out set of rare words not seen during training to evaluate OOD generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ER model handle idioms, which lack clear patterns and may require memorization?
- Basis in paper: The paper mentions that idioms are a challenge for the ER model, as they lack clear patterns and require memorization.
- Why unresolved: The paper does not provide specific details on how the ER model handles idioms or how it could be improved to better handle them.
- What evidence would resolve it: Further experiments and analysis showing how the ER model performs on idioms compared to other types of metaphors, and potential improvements to the model to better handle idioms.

### Open Question 2
- Question: How does the ER model compare to other state-of-the-art models in terms of computational efficiency and resource requirements?
- Basis in paper: The paper does not provide a detailed comparison of the ER model's computational efficiency and resource requirements with other state-of-the-art models.
- Why unresolved: The paper focuses on the performance of the ER model in terms of accuracy and generalization, but does not discuss its computational efficiency or resource requirements.
- What evidence would resolve it: A detailed comparison of the ER model's computational efficiency and resource requirements with other state-of-the-art models, including training and inference time, memory usage, and hardware requirements.

### Open Question 3
- Question: How does the ER model handle rare or unconventional metaphors, which may not have been seen during training?
- Basis in paper: The paper mentions that the ER model is evaluated on a subset of the LCC dataset containing rare or unconventional metaphors, but does not provide detailed analysis on how the model handles such metaphors.
- Why unresolved: The paper does not provide specific details on how the ER model handles rare or unconventional metaphors or how it could be improved to better handle them.
- What evidence would resolve it: Further experiments and analysis showing how the ER model performs on rare or unconventional metaphors compared to other types of metaphors, and potential improvements to the model to better handle such metaphors.

## Limitations
- The paper lacks direct empirical validation of the core hypothesis that metaphorical usage creates measurable divergence between expected literal and realized meanings
- The similarity-constrained fine-tuning mechanism is justified through intuition rather than ablation studies showing its necessity
- The performance gains from ensembling are demonstrated but not analyzed for diminishing returns or computational tradeoffs

## Confidence
- **High**: The architectural framework and experimental methodology are well-specified and reproducible
- **Medium**: The reported performance improvements over baselines are credible given the experimental design
- **Low**: The theoretical justification for why the ER model captures metaphoricity through expectation-realization divergence lacks direct evidence

## Next Checks
1. Conduct ablation studies comparing ER models with and without the similarity loss to quantify its contribution to performance
2. Measure the actual divergence between expectation and realization embeddings for literal vs. metaphorical usages to validate the core hypothesis
3. Analyze the learning curves and convergence behavior of ER models vs. R-only baselines to understand the complementary value of the expectation component