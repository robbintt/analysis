---
ver: rpa2
title: 'Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings concatenation?'
arxiv_id: '2312.07797'
source_url: https://arxiv.org/abs/2312.07797
tags:
- embeddings
- embedding
- learning
- which
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two sentiment analysis approaches for Moroccan
  shopping place reviews: fine-tuning BERT and concatenating pre-trained sentence
  embeddings fed into a stacked BiLSTM-BiGRU model. The BERT approach involved optimizing
  learning rate and achieved 75% accuracy on the "Jamaa El-Fena" dataset.'
---

# Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings concatenation?

## Quick Facts
- arXiv ID: 2312.07797
- Source URL: https://arxiv.org/abs/2312.07797
- Reference count: 27
- Primary result: BERT fine-tuning achieved 75% accuracy on Moroccan shopping reviews, while embedding concatenation showed promise but less stability

## Executive Summary
This paper compares two approaches for sentiment analysis of Moroccan shopping place reviews: fine-tuning BERT and concatenating pre-trained sentence embeddings fed into a stacked BiLSTM-BiGRU model. The BERT approach involved optimizing learning rate and achieved 75% accuracy on the "Jamaa El-Fena" dataset. For the concatenation method, four embedding pairs (e.g., GloVe + GoogleNews) were tested with multiple optimizers. SGD generally performed best across combinations, though results varied by embedding pair. The study highlights the effectiveness of BERT for this task while suggesting embedding concatenation as a viable alternative when BERT resources are limited.

## Method Summary
The study collected TripAdvisor reviews for "Jamaa El-Fena" shopping place (26,244 rows), converting star ratings to three categories: "bad" (0), "neutral" (1), "good" (2). The BERT approach used AdamW optimizer with learning rate search (10^-7 to 10^-1) and 90/10 train/test split. The concatenation approach tested six embedding pairs (GloVe + various embeddings) with a stacked BiLSTM-BiGRU architecture, evaluating multiple optimizers. Both methods used maximum sequence length of 60 tokens.

## Key Results
- BERT fine-tuning achieved 75% accuracy on the three-class sentiment classification task
- SGD optimizer performed best across most embedding pair combinations in the concatenation approach
- The concatenation method showed promise but exhibited less stability than BERT
- Four pre-trained embedding pairs were systematically tested with varying optimizer performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning BERT on a domain-specific dataset improves sentiment classification accuracy compared to generic embeddings.
- Mechanism: BERT's pre-training on large corpora provides rich contextual representations; fine-tuning adapts these representations to the target task with labeled data.
- Core assumption: The Moroccan shopping review domain shares enough linguistic patterns with BERT's pre-training data to benefit from transfer learning.
- Evidence anchors: BERT achieved 75% accuracy on the "Jamaa El-Fena" dataset; BERT's embedding extraction capabilities are well-established for semantic search and data retrieval.

### Mechanism 2
- Claim: Concatenating two pre-trained embeddings (e.g., GloVe + GoogleNews) can improve performance over single embeddings in low-resource scenarios.
- Mechanism: Each embedding captures different semantic or syntactic aspects; concatenation provides a richer representation space for the downstream model.
- Core assumption: The two embeddings are complementary rather than redundant for the task.
- Evidence anchors: The study tested four embedding pairs with a recurrent model; the algorithm used mean values to balance concatenation.

### Mechanism 3
- Claim: Using a stacked BiLSTM-BiGRU architecture with concatenated embeddings provides better sequence modeling than a single recurrent layer.
- Mechanism: BiLSTM captures long-term dependencies; BiGRU offers faster training and regularization; stacking allows hierarchical feature extraction.
- Core assumption: The sentiment signal in reviews requires both long-range context and fine-grained local patterns.
- Evidence anchors: The architecture included two regularization layers, BiLSTM, BiGRU, and GlobalMaxPooling1D layers before concatenation.

## Foundational Learning

- Concept: Learning rate optimization
  - Why needed here: Different optimizers and embedding combinations converge at different speeds; finding the optimal rate balances convergence speed and stability.
  - Quick check question: If loss decreases too slowly or oscillates, what hyperparameter should you adjust first?

- Concept: Embedding concatenation and balancing
  - Why needed here: Direct concatenation can lead to dimension imbalance; mean normalization ensures both embeddings contribute equally.
  - Quick check question: When concatenating a 300-dim and a 100-dim embedding, what preprocessing step prevents the larger from dominating?

- Concept: Recurrent neural network vanishing gradient
  - Why needed here: Stacked BiLSTM-BiGRU layers can suffer from vanishing gradients in long sequences; proper gating and initialization are critical.
  - Quick check question: Which RNN variant (LSTM or GRU) is more resistant to vanishing gradients and why?

## Architecture Onboarding

- Component map: TripAdvisor scrape → Filter "Jamaa El-Fena" → Tokenize → Split (90/10) → BERT fine-tune OR Load embeddings → Concatenate → Stacked RNN → Pool → SoftMax
- Critical path: 1) Tokenize and split data 2) For BERT: Load pre-trained model → Optimize LR → Fine-tune 3) For Concat: Load and balance embeddings → Build stacked RNN → Sweep optimizers 4) Compare accuracy and loss curves
- Design tradeoffs: BERT: High accuracy but resource-intensive; Concat: Lower resource usage but less stable; Stacked RNN: Captures complex patterns but risks overfitting on small datasets.
- Failure signatures: BERT: Training stalls if LR too high; Concat: Loss curves plateau suggest poor optimizer-embedding pairing; RNN: Vanishing gradients manifest as no learning after several epochs.
- First 3 experiments: 1) Fine-tune BERT with LR ∈ {1e-4, 2e-4, 5e-4} on 3 epochs; record accuracy. 2) Concatenate GloVe + GoogleNews; train with SGD (LR=2e-4) for 20 epochs; compare to BERT. 3) Test all four embedding pairs with SGD and Adadelta; record convergence speed and final loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would increasing the number of epochs beyond 20 affect the BERT model's accuracy and loss convergence on the Jamaa El-Fena dataset?
- Basis in paper: The paper mentions that the BERT model achieved 75% accuracy after 3 epochs and suggests that results could improve with more epochs.
- Why unresolved: The study only tested 3 epochs for BERT, so longer training effects are unknown.
- What evidence would resolve it: Running BERT with the same optimal learning rate (2×10⁻⁴) and AdamW optimizer for 50-100 epochs while monitoring accuracy and loss curves.

### Open Question 2
- Question: Which specific embedding concatenation combination (among the four tested) would provide the most stable and generalizable performance across different tourism datasets?
- Basis in paper: The paper tested four embedding pairs with SGD optimizer but showed varying results, with GoogleNews and Wiki-News combination showing promise.
- Why unresolved: Results varied significantly by embedding pair and dataset characteristics weren't fully explored.
- What evidence would resolve it: Cross-validating all four embedding combinations on multiple tourism datasets while measuring stability metrics.

### Open Question 3
- Question: What is the theoretical justification for the specific concatenation formula used in the sentence embedding approach, and how sensitive is performance to alternative weighting schemes?
- Basis in paper: The paper describes a specific formula for concatenating embeddings using mean values but doesn't provide theoretical justification.
- Why unresolved: The authors acknowledge the formula was designed for balance but don't explain why this particular approach was chosen over alternatives.
- What evidence would resolve it: Systematic comparison of different concatenation strategies on the same task to determine optimal approach.

## Limitations
- The dataset focus on a single Moroccan shopping location limits generalizability to broader tourism sentiment analysis
- 75% accuracy, while reasonable for a three-class problem, falls short of state-of-the-art benchmarks
- Embedding concatenation approach lacks rigorous statistical comparison against the BERT baseline

## Confidence
- High confidence: BERT's effectiveness for fine-tuning on domain-specific datasets is well-established in broader NLP literature
- Medium confidence: Specific performance metrics and optimizer recommendations are likely accurate for the described dataset but may not generalize
- Low confidence: The claim that embedding concatenation is "a viable alternative when BERT resources are limited" lacks sufficient comparative analysis

## Next Checks
1. Apply paired t-tests or McNemar's test to compare BERT and concatenation approach results across multiple random data splits
2. Test both approaches on reviews from other Moroccan shopping locations or different tourism domains to assess generalizability
3. Systematically test each embedding pair individually against their concatenated versions to quantify the actual contribution of the concatenation mechanism