---
ver: rpa2
title: 'MADS: Modulated Auto-Decoding SIREN for time series imputation'
arxiv_id: '2307.00868'
source_url: https://arxiv.org/abs/2307.00868
tags:
- time
- series
- data
- imputation
- siren
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MADS introduces a novel auto-decoding framework for time series
  imputation based on implicit neural representations. It combines SIREN networks
  for high-fidelity reconstruction with a hypernetwork architecture that learns a
  prior over time series.
---

# MADS: Modulated Auto-Decoding SIREN for time series imputation

## Quick Facts
- arXiv ID: 2307.00868
- Source URL: https://arxiv.org/abs/2307.00868
- Reference count: 6
- MADS introduces a novel auto-decoding framework for time series imputation based on implicit neural representations. It combines SIREN networks for high-fidelity reconstruction with a hypernetwork architecture that learns a prior over time series. A modulator network adjusts sine activation amplitudes to improve generalization.

## Executive Summary
MADS presents a novel approach to time series imputation using implicit neural representations. The method employs a SIREN-based architecture with a hypernetwork that learns a prior over time series, allowing flexible per-series signal reconstruction without requiring aligned inputs. A modulator network adjusts sine activation amplitudes to improve generalization by decoupling frequency mode learning from individual series representation. The auto-decoding setup avoids the need for an encoder while still allowing inference-time adaptation.

## Method Summary
MADS combines SIREN networks with a hypernetwork architecture to create a flexible auto-decoding framework for time series imputation. The hypernetwork maps learned latent codes to SIREN weights, creating unique implicit neural representations for each time series. A modulator network outputs amplitude scalars applied to SIREN sine activations, allowing the model to suppress irrelevant frequency modes per sample. The model is trained using MSE loss with L2 regularization on both latent codes and weights, optimized with Adam. The approach handles irregular sampling and varying lengths directly through SIREN's continuous, grid-free nature.

## Key Results
- On Human Activity dataset, MADS improved imputation performance by at least 40% compared to baseline methods
- On Air Quality dataset, MADS showed competitive performance across all metrics (MSE, Max, W2)
- On synthetic data, MADS achieved the best average rank across different dataset configurations compared to all baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hypernetwork + SIREN architecture allows flexible per-series signal reconstruction without requiring aligned inputs
- Mechanism: The hypernetwork maps a learned latent code to SIREN weights, creating a unique implicit neural representation for each time series. Since SIRENs are continuous and grid-free, they can handle irregular sampling and varying lengths directly.
- Core assumption: Latent codes sufficiently capture per-series dynamics, and SIRENs can represent the signal shape with enough fidelity given the weights.
- Evidence anchors:
  - [abstract] "... hypernetwork architecture which allows us to generalise by learning a prior over the space of time series."
  - [section 3.2] "The hypernet takes in a latent code corresponding to a given time series, and outputs a set of network weights. These weights map to those inside the SIREN, and thus a distinct INR is instantiated for each individual time series..."

### Mechanism 2
- Claim: Amplitude modulation via a modulator network improves generalization by decoupling frequency mode learning from individual series representation
- Mechanism: A separate modulator network outputs element-wise amplitude scalars applied to SIREN sine activations. This allows the model to suppress irrelevant frequency modes per sample without altering the SIREN weights, reducing overfitting.
- Core assumption: Frequency modes are shared across the dataset and can be learned independently from per-series latent codes.
- Evidence anchors:
  - [abstract] "... modulator network adjusts sine activation amplitudes to improve generalization."
  - [section 3.2] "Rather than utilise an encoding network to calculate the latent vectors, MADS follows the auto-decoding setup of DeepSDF... The third network is used for amplitude modulation..."

### Mechanism 3
- Claim: Auto-decoding setup avoids the need for an encoder while still allowing inference-time adaptation
- Mechanism: Latent codes are treated as trainable parameters and re-optimized at inference time for a new series, similar to DeepSDF. This avoids additional inference overhead and preserves flexibility.
- Core assumption: The optimization landscape is well-behaved and converges to meaningful codes for unseen data.
- Evidence anchors:
  - [abstract] "... auto-decoding framework for time series imputation, built upon implicit neural representations."
  - [section 3.2] "Rather than utilise an encoding network to calculate the latent vectors, MADS follows the auto-decoding setup of DeepSDF... the latent values are treated as variables during training (and so backpropagated), and then optimised again during inference..."

## Foundational Learning

- Concept: Implicit Neural Representations (INRs) and SIRENs
  - Why needed here: MADS relies on SIRENs as the core signal representation; understanding their periodic activation and spectral bias properties is essential.
  - Quick check question: Why do SIRENs with sine activations better handle high-frequency components compared to ReLU-based MLPs?

- Concept: Hypernetworks and weight prediction
  - Why needed here: The hypernetwork generates SIREN weights from a latent vector; knowing how this conditioning works and its trade-offs (expressiveness vs. generalization) is key.
  - Quick check question: How does conditioning SIREN weights via a hypernetwork differ from concatenating coordinates with a latent code?

- Concept: Auto-decoding vs. auto-encoding
  - Why needed here: MADS uses auto-decoding; understanding the implications (no encoder overhead, latent optimization at inference) is crucial for debugging and extending the method.
  - Quick check question: What is the main computational or architectural trade-off when choosing auto-decoding over auto-encoding in INR-based models?

## Architecture Onboarding

- Component map: Input (time coordinates + mask) -> Latent code (trainable per series) -> Hypernetwork (latent → SIREN weights) -> Modulator network (latent → amplitude scalars) -> SIREN (implicit function) -> Output (imputed values)

- Critical path: Forward pass: time → SIREN (via predicted weights and modulated activations) → imputed values; Backward pass: gradients flow through SIREN, hypernetwork, modulator, and latents

- Design tradeoffs:
  - Hypernetwork vs. direct conditioning: Hypernet allows more flexible weight shaping but increases parameter count.
  - Fixed vs. base modulator: Fixed variant shares modulation latents across dataset (more stable), base variant allows per-series adaptation (potentially more expressive).
  - SIREN depth/width: Deeper networks can capture more complex signals but risk overfitting and instability.

- Failure signatures:
  - Training instability: Watch for exploding/vanishing gradients in SIREN (use gradient clipping).
  - Poor imputation on high-frequency data: Likely due to insufficient modulator adaptation or hypernetwork capacity.
  - Overfitting on small datasets: Regularize latents and weights aggressively; consider fixed modulator.

- First 3 experiments:
  1. Train on synthetic low-frequency, low-dimensional dataset; evaluate imputation MSE vs. baselines.
  2. Swap base modulator for fixed modulator; compare performance and stability.
  3. Remove modulator entirely; assess impact on generalization and overfitting.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The exact SIREN frequency initialization scheme and modulator activation function details are unspecified, potentially affecting reproducibility.
- The hypernetwork architecture details (how many weights predicted, mapping to SIREN layers) are unclear.
- Only random missingness patterns were tested, not structured missingness patterns common in real-world scenarios.

## Confidence
- High confidence: The core mechanism of combining SIRENs with hypernetworks for time series imputation is sound and well-supported by the literature on implicit neural representations.
- Medium confidence: The specific design choices (fixed vs base modulator, exact regularization strengths) and their relative importance are partially supported by results but could benefit from ablation studies.
- Low confidence: The synthetic dataset generation process and beta distribution scaling are described but not fully specified, making exact reproduction difficult.

## Next Checks
1. Conduct an ablation study comparing fixed vs base modulator variants across all datasets to quantify their relative contribution to performance.
2. Test MADS on additional real-world datasets with varying characteristics (e.g., financial time series, sensor data) to assess generalizability beyond the current evaluation.
3. Perform sensitivity analysis on SIREN hyperparameters (depth, width, frequency initialization) to identify optimal configurations for different data types.