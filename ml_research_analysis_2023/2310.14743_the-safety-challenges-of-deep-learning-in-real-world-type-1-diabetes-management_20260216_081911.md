---
ver: rpa2
title: The Safety Challenges of Deep Learning in Real-World Type 1 Diabetes Management
arxiv_id: '2310.14743'
source_url: https://arxiv.org/abs/2310.14743
tags:
- glucose
- blood
- learning
- insulin
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep learning models for type 1 diabetes (T1D) blood glucose simulation
  show improved prediction accuracy compared to traditional mathematical simulators,
  but exhibit critical safety concerns due to their inability to distinguish fundamental
  insulin-carbohydrate relationships. When trained on real-world OpenAPS Data Commons
  data, these models achieve significantly lower residual prediction error but struggle
  in safety-critical scenarios like meals, low blood glucose events, and exercise.
---

# The Safety Challenges of Deep Learning in Real-World Type 1 Diabetes Management

## Quick Facts
- arXiv ID: 2310.14743
- Source URL: https://arxiv.org/abs/2310.14743
- Reference count: 40
- Key outcome: Deep learning models for T1D glucose simulation achieve lower prediction error but confuse insulin-carbohydrate dynamics, posing safety risks

## Executive Summary
Deep learning models for simulating blood glucose in type 1 diabetes show improved prediction accuracy over traditional mathematical approaches, but exhibit critical safety concerns when applied to real-world data. The models struggle to distinguish the fundamental physiological roles of insulin (which lowers blood glucose) and carbohydrates (which raise it), sometimes predicting that carbohydrates lower blood glucose levels. This confusion is particularly problematic in safety-critical scenarios like meals, low blood glucose events, and exercise. The study demonstrates that while deep learning can achieve superior prediction accuracy, standard machine learning metrics are insufficient for evaluating simulators intended for healthcare applications.

## Method Summary
The study trains a dilated recurrent neural network (RNN) on real-world OpenAPS Data Commons data to simulate T1D blood glucose dynamics, comparing its performance against the traditional Hovorka mathematical model and a hybrid neural network approach. The data is processed into 5-minute intervals with features including CGM readings, basal rates, total insulin, carbohydrate intake, activity indicators, and demographic information. The model is evaluated using residual prediction error (gMSE) across safety-critical scenarios and analyzed using SHAP values to interpret learned insulin-carbohydrate dynamics. Data augmentation techniques, including relabelling unreported carbohydrates, are explored to improve physiological correctness.

## Key Results
- Deep learning models achieve significantly lower median prediction error than traditional mathematical simulators
- SHAP analysis reveals models fundamentally confuse the physiological roles of insulin and carbohydrates
- Data augmentation techniques like relabelling unreported carbohydrates can improve learned dynamics but don't fully resolve the issue
- Models show poor performance in safety-critical scenarios including meals, low blood glucose events, and exercise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep learning models trained on real-world T1D data can achieve lower residual prediction error than traditional mathematical simulators.
- Mechanism: The neural network learns complex non-linear glucose dynamics from large volumes of patient data, adapting to patterns in basal rates, bolus dosing, and carbohydrate intake without being constrained by predefined physiological equations.
- Core assumption: The training data contains sufficient variation in insulin and carbohydrate events to allow the model to learn generalizable patterns of glucose response.
- Evidence anchors:
  - [abstract] "While deep learning prediction accuracy surpassed the widely-used mathematical simulator approach..."
  - [section] "The deep learning algorithm exceeds baseline simulator methods, achieving a lower median prediction error..."
- Break condition: If the dataset contains high levels of unreported carbohydrate consumption or correlated insulin-carbohydrate events, the model may learn incorrect relationships that reduce generalization.

### Mechanism 2
- Claim: SHAP value analysis can reveal when deep learning models have learned physiologically incorrect glucose dynamics.
- Mechanism: By calculating feature importance across thousands of predictions, SHAP values expose whether insulin and carbohydrate inputs have the expected directional effects on blood glucose (negative for insulin, positive for carbohydrates).
- Evidence anchors:
  - [abstract] "SHAP value analysis also indicated the model had fundamentally confused the roles of insulin and carbohydrates..."
  - [section] "Figure 2a and Figure 2b demonstrate the learned blood glucose dynamics of the deep learning model. From the figures, it is evident the simulator has drawn incorrect conclusions..."
- Break condition: If the SHAP analysis assumes feature independence when inputs are highly correlated, the interpretation may be misleading.

### Mechanism 3
- Claim: Dataset augmentations like relabelling unreported carbohydrates can improve learned glucose dynamics without constraining model complexity.
- Mechanism: By using the Hovorka model to identify and label unreported meals, the dataset better reflects true insulin-carbohydrate relationships, allowing the deep learning model to learn correct dynamics.
- Evidence anchors:
  - [section] "To correct for the conflation of insulin-carbohydrate relationships, relabelling and filtering were applied to the processed dataset... The relabelling approach appears to be the most successful, yielding a reduction in the dynamics error..."
- Break condition: If unreported meals are not the primary source of confusion, or if the relabelling algorithm incorrectly labels non-meal glucose changes, the augmentation may not improve or could degrade model performance.

## Foundational Learning

- Concept: Difference between prediction accuracy and physiological appropriateness
  - Why needed here: Low prediction error on test data doesn't guarantee the model has learned correct glucose dynamics; this distinction is critical for safe clinical deployment
  - Quick check question: If a model predicts blood glucose perfectly on test data but confuses insulin and carbohydrate effects, is it safe for clinical use?

- Concept: SHAP (Shapley Additive Explanations) values
  - Why needed here: Provides a way to interpret complex neural network predictions by attributing feature importance to each input, revealing whether learned dynamics align with known physiology
  - Quick check question: How would you interpret a SHAP value showing negative impact for carbohydrates on blood glucose 95 minutes after consumption?

- Concept: Confounding variables in real-world medical data
  - Why needed here: Unreported meals, correlated insulin-carbohydrate events, and estimation errors can mislead models into learning incorrect relationships
  - Quick check question: What real-world data characteristic could cause a model to incorrectly learn that carbohydrates lower blood glucose?

## Architecture Onboarding

- Component map:
  - Data processing pipeline: OpenAPS Data Commons extraction → 5-minute resampling → interpolation → event labeling
  - Feature engineering: CGM readings, basal rates, total insulin, carbohydrate intake, activity indicators, demographic features
  - Model architecture: Dilated RNN with gMSE loss function
  - Evaluation framework: Scenario-based testing + SHAP analysis + dataset augmentation

- Critical path:
  1. Process raw OpenAPS data into consistent 5-minute intervals
  2. Train baseline models (Hovorka, hybrid) for comparison
  3. Train deep learning model with tuned hyperparameters
  4. Evaluate across safety-critical scenarios
  5. Analyze learned dynamics using SHAP
  6. Apply dataset augmentations and re-evaluate

- Design tradeoffs:
  - Model complexity vs interpretability: Dilated RNN achieves better accuracy but requires SHAP for explanation
  - Data completeness vs real-world representativeness: Clinical trial data would be cleaner but less generalizable
  - Prediction horizon vs feature availability: Shorter horizons provide more information but may miss delayed effects

- Failure signatures:
  - High variance in safety-critical scenarios (meals, exercise, low glucose)
  - Reversed insulin/carbohydrate effects in SHAP analysis
  - Degradation when testing on scenarios with unreported events
  - Poor performance on individual patients despite good aggregate metrics

- First 3 experiments:
  1. Compare baseline model predictions across all test scenarios to identify where deep learning has largest advantage/disadvantage
  2. Generate SHAP values for a random sample of predictions to visualize learned insulin/carbohydrate dynamics
  3. Apply relabelling augmentation to identify unreported meals and retrain to measure improvement in learned dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deep learning models be developed that learn physiologically correct glucose-insulin dynamics without constraining model architecture?
- Basis in paper: [explicit] The paper explicitly states that deep learning models struggle with learning correct insulin-carbohydrate relationships, with SHAP analysis showing models fundamentally confused the roles of insulin and carbohydrates. The authors note this is "one of the most basic T1D management principles."
- Why unresolved: Current deep learning approaches achieve high prediction accuracy but learn incorrect physiological relationships, as evidenced by their inability to distinguish insulin and carbohydrate effects on blood glucose.
- What evidence would resolve it: A deep learning model that achieves both high prediction accuracy and correctly learned insulin-carbohydrate dynamics as verified by SHAP analysis and physiological experiments.

### Open Question 2
- Question: What data collection improvements are most effective at reducing uncertainty in deep learning models for T1D management?
- Basis in paper: [explicit] The paper identifies several sources of uncertainty including unreported carbohydrates, carbohydrate estimation errors, and correlation between insulin and carbohydrates. It suggests that "improved data collection methods could be employed to minimize error" but doesn't specify which would be most effective.
- Why unresolved: While the paper identifies various sources of uncertainty, it doesn't provide empirical evidence on which data collection improvements would most effectively reduce model uncertainty.
- What evidence would resolve it: Comparative studies measuring model uncertainty reduction when implementing different data collection improvements (e.g., passive accelerometer data, image recognition for carbohydrate counting, etc.).

### Open Question 3
- Question: What evaluation metrics beyond prediction error are most effective at ensuring deep learning models learn physiologically appropriate relationships in healthcare applications?
- Basis in paper: [explicit] The paper explicitly states that "Standard Machine Learning metrics are not sufficient for evaluating deep learning simulators" and recommends developing "evaluation metrics which consider the physiological appropriateness of predictions, in addition to test dataset prediction error."
- Why unresolved: Current evaluation focuses primarily on prediction error metrics, but the paper demonstrates that high prediction accuracy doesn't guarantee physiologically appropriate learned dynamics.
- What evidence would resolve it: Development and validation of novel evaluation metrics that successfully identify physiologically appropriate versus inappropriate learned dynamics, potentially using SHAP analysis or comparison to known physiological relationships.

## Limitations

- The study relies on real-world data that may contain unreported meals and estimation errors, which could affect model learning
- The SHAP analysis methodology for neural networks is not fully detailed, introducing uncertainty in the interpretation of learned dynamics
- The work focuses on prediction accuracy and physiological appropriateness but does not directly evaluate clinical safety outcomes or patient-level risks

## Confidence

- **High Confidence**: Deep learning models achieve lower prediction error than traditional simulators; SHAP analysis reveals learned dynamics; dataset augmentations can improve learned relationships.
- **Medium Confidence**: The specific mechanism of insulin-carbohydrate confusion is well-supported, but the exact prevalence and impact on clinical safety remain uncertain.
- **Low Confidence**: The effectiveness of data augmentation in fully resolving the physiological confusion is not conclusively demonstrated.

## Next Checks

1. Conduct a patient-level analysis to identify if individual cases exhibit extreme physiological confusion, and assess the potential clinical impact.
2. Validate the unreported meal identification algorithm by comparing augmented labels with patient-reported logs where available.
3. Perform ablation studies on the model architecture and training procedure to isolate whether the confusion is inherent to deep learning or specific to the implementation.