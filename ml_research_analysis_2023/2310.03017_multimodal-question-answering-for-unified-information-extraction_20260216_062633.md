---
ver: rpa2
title: Multimodal Question Answering for Unified Information Extraction
arxiv_id: '2310.03017'
source_url: https://arxiv.org/abs/2310.03017
tags:
- entity
- sentence
- image
- tasks
- span
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal question answering (MQA) framework
  to unify multimodal information extraction (MIE) tasks. MQA decomposes tasks into
  cascaded span extraction and multi-choice QA stages, reformulating them into formats
  LMMs are pre-trained on.
---

# Multimodal Question Answering for Unified Information Extraction

## Quick Facts
- **arXiv ID**: 2310.03017
- **Source URL**: https://arxiv.org/abs/2310.03017
- **Reference count**: 25
- **Key outcome**: MQA framework significantly improves LMM performance on MIE tasks through task decomposition and reformulation

## Executive Summary
This paper proposes a Multimodal Question Answering (MQA) framework that unifies multimodal information extraction (MIE) tasks by decomposing them into cascaded span extraction and multi-choice QA stages. The framework reformulates complex MIE tasks into formats that align with Large Multimodal Models' (LMMs) pre-training objectives, enabling zero-shot and few-shot performance improvements. Experiments across six LMMs and six datasets from three MIE tasks demonstrate that MQA consistently outperforms vanilla prompting approaches and achieves state-of-the-art results.

## Method Summary
The MQA framework decomposes MIE tasks into two stages: an optional span extraction stage for tasks like MNER and MTED, followed by a multi-choice QA classification stage. The approach reformulates extraction tasks into sequence labeling problems and classification tasks into multi-choice formats that LMMs are pre-trained on. This unified pipeline handles the output format diversity across various MIE tasks and reduces decoding complexity by constraining the output space to discrete options rather than free-form generation.

## Key Results
- MQA consistently and significantly improves performance compared to vanilla prompting across six LMMs
- Achieves state-of-the-art results in zero-shot and few-shot settings on six datasets from three MIE tasks
- Demonstrates robustness to instruction variants with low standard deviation across different prompt formulations
- 10B parameter LMMs enhanced with MQA framework become competitive with much larger models like ChatGPT and GPT-4

## Why This Works (Mechanism)

### Mechanism 1
Task decomposition into span extraction followed by multi-choice QA enables LMMs to leverage pre-trained VQA abilities. By breaking complex MIE tasks into simpler sub-tasks that align with LMMs' pre-training objectives (VQA for classification, sequence labeling for span extraction), the framework elicits capabilities acquired during pre-training. The core assumption is that LMMs have sufficient VQA capabilities that can be accessed through appropriate prompting. Break condition occurs if LMMs lack adequate VQA pre-training or if decomposition creates too many cascading errors.

### Mechanism 2
Multi-choice QA format reduces output space complexity compared to free-form generation. Instead of generating entity names or relation types directly, models select from discrete options, reducing decoding complexity and ambiguity. The core assumption is that discrete selection tasks are easier for LMMs than open-ended generation when options are provided. Break condition occurs if the option space becomes too large or if options are poorly aligned with the model's knowledge.

### Mechanism 3
Instruction-following robustness enables consistent performance across different prompt formulations. The MQA framework's modular design makes it less sensitive to specific wording of instructions, as each sub-task has a clear, constrained format. The core assumption is that models pre-trained with diverse instructions can generalize across semantically equivalent prompts. Break condition occurs if models encounter instructions far outside their pre-training distribution or if sub-task boundaries become unclear.

## Foundational Learning

- **Concept**: Task reformulation and decomposition
  - **Why needed here**: MIE tasks are complex and diverse; breaking them into simpler sub-tasks that match LMM pre-training objectives enables zero-shot performance
  - **Quick check question**: What are the two main sub-tasks MQA decomposes MIE tasks into?

- **Concept**: Prompt engineering for classification vs. generation
  - **Why needed here**: Different task types (classification vs. extraction) require different prompting strategies; MQA adapts accordingly
  - **Quick check question**: How does MQA handle the classification stage differently from vanilla prompting?

- **Concept**: Instruction tuning and robustness
  - **Why needed here**: Models need to follow instructions consistently; understanding robustness helps in designing reliable prompts
  - **Quick check question**: What experimental evidence shows MQA is more robust to instruction variations than vanilla methods?

## Architecture Onboarding

- **Component map**: Text/Image → Span Extraction (optional) → Candidate Spans → Multi-choice QA → Structured Output
- **Critical path**:
  1. Receive multimodal input
  2. Apply span extraction prompt (if applicable)
  3. Generate candidate spans
  4. For each span, apply multi-choice QA prompt
  5. Filter out NOTA responses
  6. Aggregate results

- **Design tradeoffs**:
  - Span extraction first vs. direct classification: Tradeoff between completeness and accuracy
  - Multi-choice vs. free generation: Simplicity vs. flexibility
  - Single-stage vs. cascaded approach: Efficiency vs. error accumulation

- **Failure signatures**:
  - High NOTA rate in classification stage suggests poor span extraction
  - Inconsistent results across instruction variants suggest robustness issues
  - Low performance on image-only tasks suggests vision-language alignment problems

- **First 3 experiments**:
  1. Compare vanilla prompting vs. MQA on a single MNER dataset with one LMM
  2. Test instruction robustness by running same task with 4 different prompt formulations
  3. Evaluate span extraction quality by comparing with ground truth spans in MQA setting

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed MQA framework handle the trade-off between allowing false positives in the span extraction stage and maintaining overall accuracy in the multi-choice QA stage?
- **Basis in paper**: The paper mentions that the span extraction stage is designed to allow a certain level of false positive entities in a strategic trade-off.
- **Why unresolved**: The paper does not provide specific details on how the false positives are managed or the impact of this trade-off on the overall accuracy.
- **What evidence would resolve it**: Detailed analysis of the false positive rate in the span extraction stage and its correlation with the accuracy in the multi-choice QA stage would help understand the trade-off better.

### Open Question 2
- **Question**: What is the impact of the MQA framework on models with different sizes, especially when comparing models with 10B parameters to those with significantly larger parameters like ChatGPT and GPT-4?
- **Basis in paper**: The paper states that LMMs with 10B parameters are enhanced to be competitive or outperform much larger language models such as ChatGPT and GPT-4.
- **Why unresolved**: The paper does not provide a detailed comparison of the performance improvements across different model sizes or explain the underlying reasons for the observed improvements.
- **What evidence would resolve it**: A comprehensive study comparing the performance of various model sizes with and without the MQA framework, along with an analysis of the reasons behind the improvements, would provide clarity.

### Open Question 3
- **Question**: How does the MQA framework perform in real-world scenarios with diverse task requirements and limited labeled data, beyond the controlled experimental settings?
- **Basis in paper**: The paper discusses the effectiveness of the MQA framework in zero-shot and few-shot settings, suggesting potential real-world applicability.
- **Why unresolved**: The paper does not provide evidence of the framework's performance in actual real-world scenarios or discuss the challenges faced in such environments.
- **What evidence would resolve it**: Empirical results from applying the MQA framework to real-world datasets with diverse task requirements and limited labeled data would demonstrate its practical effectiveness.

## Limitations

- The cascading nature of the approach could amplify errors at earlier stages, potentially limiting overall performance
- Evaluation of instruction robustness was limited to four variants, which may not capture the full spectrum of instruction variations
- The framework's performance on tasks requiring complex text-image relationship understanding beyond the tested MIE tasks remains unexplored

## Confidence

- **High confidence**: The core mechanism of decomposing MIE tasks into span extraction and multi-choice QA stages is well-supported by experimental results across multiple datasets and models
- **Medium confidence**: The instruction robustness findings are supported by empirical evidence but would benefit from a larger and more diverse set of instruction variants
- **Medium confidence**: The claim that MQA enables zero-shot and few-shot performance improvements is well-demonstrated, though the specific mechanisms enabling this across different LMMs warrant further investigation

## Next Checks

1. **Error Analysis**: Systematically analyze where and why errors occur in the cascaded MQA approach by examining cases where vanilla prompting outperforms MQA, particularly focusing on error propagation between stages

2. **Instruction Space Exploration**: Expand the instruction robustness evaluation to include a more diverse set of instruction variants (minimum 10) and employ automated methods to ensure semantic equivalence rather than manual selection

3. **Cross-Lingual Validation**: Test the MQA framework on non-English multimodal datasets to evaluate whether the task reformulation approach generalizes across languages and cultural contexts