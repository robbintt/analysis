---
ver: rpa2
title: IR-UWB Radar-Based Contactless Silent Speech Recognition of Vowels, Consonants,
  Words, and Phrases
arxiv_id: '2312.09572'
source_url: https://arxiv.org/abs/2312.09572
tags:
- radar
- speech
- recognition
- classification
- ir-uwb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a contactless silent speech recognition system
  using impulse radio ultra-wideband (IR-UWB) radar to recognize phonemes, words,
  and phrases without requiring physical contact with the user's articulators. The
  core method involves a novel speech feature extraction algorithm, FERASEC, designed
  specifically for IR-UWB radar-based silent speech recognition.
---

# IR-UWB Radar-Based Contactless Silent Speech Recognition of Vowels, Consonants, Words, and Phrases

## Quick Facts
- arXiv ID: 2312.09572
- Source URL: https://arxiv.org/abs/2312.09572
- Reference count: 40
- Key outcome: Achieves 86.47%, 81.59%, 88.95%, and 96.88% classification accuracy for vowels, consonants, words, and phrases respectively using FERASEC + DNN-HMM method with radar positioned in front of lips

## Executive Summary
This paper presents the first demonstration of phoneme-level silent speech recognition using contactless IR-UWB radar. The system captures subtle articulatory movements without physical contact, extracting features through a novel FERASEC algorithm and classifying using DNN-HMM. Experimental results show high accuracy across different speech units (vowels, consonants, words, phrases) when the radar is positioned in front of the lips. The study establishes IR-UWB radar as a viable technology for deploying silent speech recognition in space-constrained handheld devices for daily use.

## Method Summary
The method involves IR-UWB radar hardware capturing articulatory movements, followed by signal processing including clutter reduction using a loopback filter. FERASEC extracts features by concatenating frames from raw and clutter-reduced frame sets, applying envelope detection with a 400-point window, downsampling by 1024, and computing first and second derivatives. Two classification algorithms are compared: MD-DTW and DNN-HMM with five-state left-to-right HMM and three hidden layers of 256 units. The system uses Novelda NVA-R661 radar modules with different antenna configurations for upper (patch) and lower (sinuous with dielectric lens) positions.

## Key Results
- 86.47% average classification accuracy for vowels
- 81.59% average classification accuracy for consonants
- 88.95% average classification accuracy for words
- 96.88% average classification accuracy for phrases (FERASEC + DNN-HMM with upper radar)

## Why This Works (Mechanism)

### Mechanism 1
- IR-UWB radar provides high-range resolution, enabling capture of subtle articulatory movements for phoneme recognition through extremely short pulses with fractional bandwidth >25%
- Core assumption: Articulatory movements create detectable changes in radar signal reflections that can be differentiated between phonemes
- Break condition: If articulators are positioned outside radar's detection range or field of view, or if occluding materials block radar signals

### Mechanism 2
- FERASEC algorithm effectively extracts speech features by creating abbreviated envelope of concatenated frames from raw and clutter-reduced frame sets
- Core assumption: Abbreviated envelope of concatenated frames contains sufficient information to distinguish between different phonemes based on unique articulatory patterns
- Break condition: If frame set lacks enough articulatory movement information or downsampling factor removes critical temporal details

### Mechanism 3
- DNN-HMM classification algorithm outperforms MD-DTW for phoneme-level silent speech recognition using extracted features
- Core assumption: Extracted features contain sufficient discriminative information for DNN-HMM to learn phoneme-level distinctions
- Break condition: If extracted features are not discriminative enough or DNN-HMM architecture is not suitable for given feature space

## Foundational Learning

- Concept: IR-UWB radar signal processing
  - Why needed here: Understanding how IR-UWB radar captures and processes reflections from articulators is crucial for developing effective feature extraction algorithms
  - Quick check question: How does the fractional bandwidth of IR-UWB radar contribute to its high-range resolution compared to conventional radar systems?

- Concept: Speech production and articulatory phonetics
  - Why needed here: Knowledge of how different phonemes are produced through specific articulatory movements is essential for interpreting radar data and developing classification algorithms
  - Quick check question: What are the key differences in articulatory movements between vowels and consonants, and how might these differences be captured by IR-UWB radar?

- Concept: Machine learning for sequential data
  - Why needed here: Understanding strengths and limitations of different classification algorithms (MD-DTW, DNN-HMM) for handling sequential data with varying lengths is crucial for selecting appropriate method for phoneme recognition
  - Quick check question: How do MD-DTW and DNN-HMM differ in their approach to handling sequential data, and what are the implications for phoneme recognition tasks?

## Architecture Onboarding

- Component map: IR-UWB radar hardware -> Signal processing pipeline (clutter reduction, FERASEC) -> Classification algorithms (MD-DTW, DNN-HMM) -> Hardware testbed
- Critical path: IR-UWB radar data acquisition → Signal processing (clutter reduction, FERASEC) → Classification (DNN-HMM) → Phoneme recognition
- Design tradeoffs:
  - Radar position: Upper radar captures lip movements better, while lower radar captures tongue and chin movements; upper position generally yields higher accuracy
  - Feature extraction: FERASEC balances complexity and effectiveness; simpler methods may not capture sufficient articulatory information
  - Classification algorithm: DNN-HMM provides better phoneme-level recognition, while MD-DTW may be more suitable for phrase-level tasks
- Failure signatures: Low classification accuracy indicates insufficient articulatory movement information, inadequate feature extraction, or inappropriate classification algorithm; inconsistent results suggest variability in radar signal quality, participant pronunciation, or hardware alignment
- First 3 experiments:
  1. Test IR-UWB radar signal quality and range resolution using simple gestures or movements to verify basic functionality
  2. Apply FERASEC to raw radar data from a single participant pronouncing vowels and consonants, and visualize extracted features to ensure they capture relevant articulatory patterns
  3. Compare classification accuracy of DNN-HMM and MD-DTW using extracted features from a small dataset to validate choice of classification algorithm

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is FERASEC algorithm in extracting speech features from IR-UWB radar data compared to other feature extraction methods?
- Basis in paper: [explicit] Paper compares FERASEC with short-template-based CLEAN algorithm and demonstrates superior performance in classifying vowels, consonants, words, and phrases
- Why unresolved: While FERASEC outperforms CLEAN algorithm, direct comparison with other state-of-the-art feature extraction methods for radar-based silent speech recognition is not provided
- What evidence would resolve it: Conducting experiments comparing FERASEC with other feature extraction methods on same dataset would provide comprehensive evaluation of effectiveness

### Open Question 2
- Question: What are optimal radar parameters (e.g., frequency range, pulse width, sampling rate) for IR-UWB radar-based silent speech recognition?
- Basis in paper: [inferred] Paper uses specific IR-UWB radar with frequency range of 6 to 10.2 GHz and 4 mm distance resolution, but impact of different radar parameters on performance is not explored
- Why unresolved: Paper focuses on proposed feature extraction and classification algorithms rather than investigating effect of radar parameters on performance
- What evidence would resolve it: Conducting experiments with different radar configurations and analyzing impact on accuracy would provide insights into optimal radar parameters

### Open Question 3
- Question: How does performance of IR-UWB radar-based silent speech recognition compare to other non-acoustic biosignal-based methods (e.g., electromyography, ultrasound imaging) in terms of accuracy and usability?
- Basis in paper: [explicit] Paper mentions various non-acoustic biosignal-based methods for silent speech recognition but does not provide direct comparison with IR-UWB radar-based methods
- Why unresolved: Paper focuses on specific advantages and challenges of IR-UWB radar-based silent speech recognition without comparing performance to other methods
- What evidence would resolve it: Conducting comprehensive study comparing performance of IR-UWB radar-based silent speech recognition with other non-acoustic biosignal-based methods on same dataset would provide insights into relative strengths and weaknesses

## Limitations

- Study uses only 20 participants with limited demographic diversity, restricting generalizability
- Experimental protocol artificially constrains speaking rate to "approximately two syllables per second," not representative of natural speech
- FERASEC algorithm parameters appear tuned specifically for experimental conditions without demonstrating robustness to varying speech rates or environmental conditions
- Paper lacks ablation studies to isolate contribution of individual components to overall accuracy

## Confidence

**High Confidence**: Basic mechanism of IR-UWB radar capturing articulatory movements is well-established through signal processing description and hardware specifications, directly supported by equipment details

**Medium Confidence**: FERASEC feature extraction algorithm shows logical consistency with radar signal processing principles but lacks independent validation, making it difficult to assess whether chosen parameters represent optimal values

**Low Confidence**: Superiority of DNN-HMM over MD-DTW is demonstrated only within specific experimental conditions, without exploring whether advantage persists across different vocabulary sizes, speaking rates, or with participants who have speech impairments

## Next Checks

1. Test system in environments with varying levels of electromagnetic interference and acoustic noise to verify claimed high penetrability and contactless operation under realistic conditions

2. Systematically disable or replace each major component (clutter reduction, FERASEC feature extraction, DNN-HMM classifier) to quantify their individual contributions to overall accuracy, particularly the 10% gap between word (88.95%) and phrase (96.88%) recognition

3. Evaluate performance with participants who have speech disorders or accents substantially different from original cohort, and with vocabulary sizes exceeding 25 words to test scalability beyond proof-of-concept dataset