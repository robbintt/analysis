---
ver: rpa2
title: Model Sparsity Can Simplify Machine Unlearning
arxiv_id: '2304.04934'
source_url: https://arxiv.org/abs/2304.04934
tags:
- unlearning
- sparsity
- pruning
- data
- approximate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a model-based approach to machine unlearning
  (MU) by leveraging model sparsification via weight pruning. The authors demonstrate
  that model sparsity significantly improves the multi-criteria unlearning performance
  of approximate unlearning methods, closing the gap with exact unlearning (retraining).
---

# Model Sparsity Can Simplify Machine Unlearning

## Quick Facts
- **arXiv ID**: 2304.04934
- **Source URL**: https://arxiv.org/abs/2304.04934
- **Reference count**: 40
- **Key outcome**: Model sparsity significantly improves multi-criteria unlearning performance, closing the gap with exact unlearning while maintaining or improving other metrics.

## Executive Summary
This paper introduces a model-based approach to machine unlearning (MU) that leverages model sparsification via weight pruning. The authors demonstrate that sparse models substantially improve the effectiveness of approximate unlearning methods across multiple criteria, narrowing the performance gap with exact unlearning (retraining). They propose two novel sparsity-aware unlearning paradigms: "prune first, then unlearn" and "sparsity-aware unlearning" with ℓ1 regularization. Extensive experiments across various datasets and models show consistent improvements in unlearning accuracy (e.g., 77% gain for fine-tuning) while maintaining or improving fidelity, efficiency, and privacy preservation metrics.

## Method Summary
The paper proposes leveraging model sparsity to improve machine unlearning performance. The approach involves first training a dense model, then applying weight pruning methods (OMP, SynFlow, IMP) to create sparse models. These sparse models are then subjected to approximate unlearning methods (fine-tuning, gradient ascent, Fisher forgetting, influence unlearning). The authors also develop two new sparsity-aware unlearning paradigms: a "prune first, then unlearn" approach that applies unlearning to pre-pruned models, and a sparsity-aware unlearning method that integrates ℓ1-norm regularization into the unlearning objective. Theoretical analysis provides error bounds showing that unlearning error decreases with increased sparsity, and experiments validate these claims across multiple datasets and models.

## Key Results
- Sparse models achieve 77% higher unlearning accuracy for fine-tuning compared to dense models
- Sparsity-aware unlearning methods close the performance gap with exact unlearning (retraining)
- Model sparsity improves privacy preservation by reducing membership inference attack efficacy
- The approach benefits transfer learning and backdoor defense scenarios beyond core unlearning tasks

## Why This Works (Mechanism)

### Mechanism 1
Model sparsity reduces the unlearning error gap between approximate and exact methods by concentrating gradients and Hessian information in fewer dimensions. The unlearning error bound scales with the model's active parameter count and the largest Hessian singular value among unmasked parameters.

### Mechanism 2
Sparse models improve privacy preservation by containing less information about individual training examples, making membership inference attacks less effective. The MIA performance is inversely correlated with model sparsity.

### Mechanism 3
Integrating ℓ1-norm sparse regularization into the unlearning objective forces the process to naturally favor sparse solutions, achieving unlearning efficacy gains without pre-pruning overhead.

## Foundational Learning

- **Stochastic Gradient Descent (SGD) and convergence properties**: The theoretical analysis relies on unrolling SGD to bound unlearning error, so understanding SGD dynamics is essential. *Quick check: Does unlearning error bound depend on training iterations and learning rate?* (Yes)

- **Influence functions and their use in model unlearning**: IU uses influence functions to estimate parameter changes when data is removed; understanding this is key to grasping IU's effectiveness. *Quick check: Does IU require computing inverse Hessian?* (Yes, but approximated via WoodFisher)

- **Model pruning techniques**: Different pruning methods impact unlearning efficacy differently; selecting the right method is critical for the "prune first, then unlearn" paradigm. *Quick check: Which pruning method doesn't require training data?* (SynFlow)

## Architecture Onboarding

- **Component map**: CIFAR-10/CIFAR-100/SVHN/ImageNet datasets → ResNet-18/VGG-16 models → Unlearning methods (FT, GA, FF, IU, Retrain) → Pruning methods (OMP, SynFlow, IMP) → Evaluation metrics (UA, MIA-Efficacy, RA, TA, RTE)

- **Critical path**: Train dense model → Apply pruning → Run approximate unlearning → Evaluate metrics

- **Design tradeoffs**: Higher sparsity → better unlearning but potential TA drop; ℓ1 regularization → sparsity but possible fidelity loss; pruning method choice → affects dependence on forgetting data

- **Failure signatures**: Unlearning efficacy stalls despite increased sparsity; sharp RA/TA drops; MIA-Privacy doesn't improve with sparsity

- **First 3 experiments**: 1) Compare UA and RA for FT on dense vs. 95%-sparse ResNet-18 for class-wise forgetting; 2) Test MIA-Privacy for GA vs. IU on sparse models vs. dense; 3) Evaluate ℓ1-sparse MU vs. standard FT for UA and TA on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
How does pruning method choice (IMP vs. SynFlow vs. OMP) impact the unlearning efficacy-fidelity trade-off across different model architectures and datasets? While the paper provides some comparison, it doesn't comprehensively explore impact across diverse architectures and datasets.

### Open Question 2
What is the theoretical relationship between model sparsity, unlearning error, and the Hessian spectrum of the loss landscape? Proposition 2 provides a starting point, but deeper understanding of how sparsity affects Hessian spectrum and unlearning error is needed.

### Open Question 3
How do different unlearning scenarios affect the efficacy of sparsity-aware unlearning methods like ℓ1-sparse MU and AO-sparse MU? The paper lacks detailed analysis of how scenarios influence performance of these methods.

## Limitations
- Theoretical analysis relies on assumptions about Hessian concentration lacking direct MU-specific citations
- Privacy preservation claims depend on sparsity-MIA correlations that may not generalize across all attack types
- ℓ1-sparse regularization introduces additional hyperparameters without extensive optimal setting exploration

## Confidence
- Mechanism 1 (Sparsity reduces unlearning error): High confidence - Strong theoretical backing with empirical validation
- Mechanism 2 (Sparsity improves privacy): Medium confidence - Empirical results shown but mechanism not deeply analyzed
- Mechanism 3 (ℓ1 regularization integration): Medium confidence - Promising results but limited ablation studies on regularization strength

## Next Checks
1. Test unlearning efficacy on models with >95% sparsity to determine practical upper bound
2. Evaluate MIA performance against multiple attack types to verify general privacy improvements
3. Compare ℓ1-sparse MU against other sparse regularization techniques to isolate contribution of specific regularization choice