---
ver: rpa2
title: Adaptive trajectory-constrained exploration strategy for deep reinforcement
  learning
arxiv_id: '2312.16456'
source_url: https://arxiv.org/abs/2312.16456
tags:
- learning
- policy
- agent
- exploration
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the hard-exploration problem in deep reinforcement
  learning (DRL), where agents struggle to efficiently explore environments with sparse
  or deceptive rewards and large state spaces. The authors propose a trajectory-constrained
  exploration strategy (TACE) that leverages incomplete offline demonstrations as
  references to guide the agent's policy away from suboptimal solutions.
---

# Adaptive trajectory-constrained exploration strategy for deep reinforcement learning

## Quick Facts
- arXiv ID: 2312.16456
- Source URL: https://arxiv.org/abs/2312.16456
- Reference count: 40
- The proposed TACE method significantly outperforms baseline methods in hard-exploration tasks by using MMD-based constraints to avoid suboptimal demonstrations.

## Executive Summary
This paper addresses the hard-exploration problem in deep reinforcement learning, where agents struggle to efficiently explore environments with sparse or deceptive rewards and large state spaces. The authors propose a trajectory-constrained exploration strategy (TACE) that leverages incomplete offline demonstrations as references to guide the agent's policy away from suboptimal solutions. By formulating exploration as a constrained optimization task using MMD distance between current and past trajectories, TACE enables agents to systematically explore while avoiding known suboptimal regions. The method includes adaptive mechanisms for constraint boundary adjustment and Lagrange multiplier scaling to maintain stable training.

## Method Summary
The method introduces trajectory-constrained exploration using MMD distance to measure novelty between current trajectories and offline suboptimal demonstrations. The exploration problem is formulated as a constrained optimization where the agent maximizes environmental rewards while maintaining a minimum MMD distance from past trajectories. Adaptive constraint boundary adjustment normalizes MMD distances to keep thresholds effective across training. An adaptive scaling method dynamically adjusts the Lagrange multiplier based on MMD distance to balance exploration and exploitation. The approach is evaluated on grid world mazes and MuJoCo tasks, demonstrating superior performance compared to baseline exploration methods.

## Key Results
- TACE successfully avoids local optima in hard-exploration tasks, achieving higher average returns than PPO and other baselines
- The method demonstrates significant improvement in exploration efficiency, particularly in environments with deceptive rewards
- Adaptive constraint boundary adjustment enables stable training without manual hyperparameter tuning across different environments
- MMD-based constraints effectively guide agents away from suboptimal demonstrations while still allowing discovery of optimal solutions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The MMD distance metric enables the agent to measure novelty of trajectories by comparing current visitation distributions to past suboptimal demonstrations, preventing premature convergence.
- **Mechanism:** The MMD distance is computed between the state-action visitation distribution of the current trajectory and that of offline suboptimal demonstrations. When the MMD distance is below a threshold, it signals the agent is revisiting known suboptimal regions, and constraints are activated to push exploration outward.
- **Core assumption:** The state-action visitation distribution induced by each trajectory is sufficiently representative to capture the novelty of the region explored, and MMD can effectively differentiate between novel and redundant trajectories.
- **Evidence anchors:**
  - [abstract] "leverages incomplete offline demonstrations as references... maximum mean discrepancy (MMD) metric to measure the distance between the current and past trajectories"
  - [section] "we used the MMD metric... to measure the disparity between the different trajectories... the MMD distance is calculated between different state-action visitation distributions that belong to the offline demonstration trajectory"
  - [corpus] Weak evidence; no direct citations on MMD in DRL exploration, only mentions MMD in unrelated domains.
- **Break condition:** If the offline demonstrations are too close to optimal trajectories, the MMD constraints may overly restrict exploration and prevent reaching better solutions.

### Mechanism 2
- **Claim:** Adaptive constraint boundary adjustment using normalized MMD distances enables stable training by preventing the agent from getting stuck near suboptimal demonstrations.
- **Mechanism:** MMD distances are normalized by subtracting the mean and dividing by the standard deviation of distances over the current batch. This normalization allows the constraint threshold to remain effective even as the MMD values grow during training, keeping the agent in a feasible region away from local optima.
- **Core assumption:** The distribution of MMD distances over a batch is sufficiently stable and representative to normalize across epochs, and the normalized threshold can be kept constant (e.g., δ = 0.5) without manual tuning.
- **Evidence anchors:**
  - [section] "we propose an adaptive constraint distance normalization method... we compute the normalized distance... according to: ˆd(x, M) = d(x, M) − E[D(B)] / √Var[D(B)]"
  - [section] "Distance normalization enables us to adjust distance constraint boundaries dynamically... With distance normalization, the problem of determining parameter δ becomes relaxed and environmentally independent"
  - [corpus] No corpus evidence on normalization in RL; only standard references to batch normalization in supervised learning.
- **Break condition:** If the MMD distance distribution becomes too skewed or multimodal, normalization may fail to keep the constraint effective.

### Mechanism 3
- **Claim:** Adaptive scaling of the Lagrange multiplier σ based on MMD distance keeps the agent inside the feasible region and balances exploration with RL objective.
- **Mechanism:** The Lagrange multiplier σ is increased when trajectories fall within a small MMD distance threshold ϵ (encouraging more exploration), decreased when they exceed 2ϵ (reducing constraint influence), and increased if deceptive rewards are collected. This heuristic prevents the agent from lingering in suboptimal areas.
- **Core assumption:** The relationship between MMD distance and the need for exploration is monotonic and can be captured by simple multiplicative updates (e.g., σ = 1.05σ or σ = 0.98σ).
- **Evidence anchors:**
  - [section] "we propose an adaptive scaling method (ASM)... We associate σ with the MMD distance metric... σ = 1.05σ, if ∃υ ∈ B s.t. MMD(υ, M) ≤ ϵ, σ = 0.98σ, if ∀υ ∈ B s.t. MMD(υ, M) ≥ 2ϵ"
  - [section] "this method drastically increases the value of parameter σ to force the agent away from the local maximum"
  - [corpus] No corpus evidence on adaptive Lagrange multiplier scaling in DRL; only generic RL references.
- **Break condition:** If the threshold ϵ is poorly chosen or the heuristic fails to respond to complex reward landscapes, σ updates may destabilize training.

## Foundational Learning

- **Concept:** Maximum Mean Discrepancy (MMD) as a metric for distribution similarity
  - **Why needed here:** MMD provides a kernel-based, non-parametric way to measure how different the current trajectory's visitation distribution is from past suboptimal demonstrations, which is crucial for guiding exploration away from known bad regions.
  - **Quick check question:** Can you compute the empirical MMD² between two sets of samples using a Gaussian kernel, and explain why this is an unbiased estimator of the population MMD?

- **Concept:** Constrained optimization in RL (Lagrange multipliers)
  - **Why needed here:** Reformulating exploration as a constrained optimization problem allows the agent to maximize return while staying outside a region defined by past suboptimal demonstrations, ensuring systematic exploration.
  - **Quick check question:** How does the Lagrangian formulation transform a constrained optimization problem into an unconstrained one, and why is this useful for policy gradient methods?

- **Concept:** Policy gradient theorem and advantage estimation
  - **Why needed here:** The method derives gradients that combine environmental rewards with MMD-based intrinsic rewards, so understanding how to compute and use advantage functions is essential for implementing the algorithm.
  - **Quick check question:** Given a state-action visitation distribution and a Q-function, how do you derive the policy gradient, and how would you incorporate an intrinsic reward into this computation?

## Architecture Onboarding

- **Component map:** Policy network πθ -> On-policy buffer B -> MMD calculator -> Adaptive scaling module -> Optimizer -> Replay memory M
- **Critical path:** Generate trajectories → store in B → compute MMD distances → normalize → estimate gradients → update policy → update σ → repeat
- **Design tradeoffs:**
  - Using offline demonstrations as constraints avoids the need for complex novelty estimation models but requires careful selection of demonstration quality
  - Normalizing MMD distances simplifies hyperparameter tuning but assumes batch statistics are representative
  - Adaptive scaling of σ provides robustness but relies on heuristic thresholds that may not generalize
- **Failure signatures:**
  - If MMD distances are always below threshold, the agent may not explore sufficiently
  - If σ grows too large, the agent may ignore environmental rewards and wander aimlessly
  - If normalization fails (e.g., zero variance), the algorithm may crash or behave erratically
- **First 3 experiments:**
  1. **Sanity check:** Run TCPPO on a simple grid world with one deceptive reward; verify that the agent avoids the local optimum and reaches the global optimum more reliably than PPO
  2. **Ablation study:** Disable adaptive scaling or normalization and compare learning curves; confirm that both components are necessary for stable training
  3. **Robustness test:** Use noisy or irrelevant offline demonstrations; observe whether the MMD constraints still guide exploration effectively or if they mislead the agent

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed trajectory-constrained exploration strategy (TACE) perform in real-world applications beyond simulated environments?
- **Basis in paper:** [inferred] The paper evaluates TACE on various simulated tasks, but does not discuss its performance in real-world applications.
- **Why unresolved:** The paper does not provide any evidence or discussion on the performance of TACE in real-world scenarios.
- **What evidence would resolve it:** Experimental results demonstrating the effectiveness of TACE in real-world applications, such as robotics or autonomous systems.

### Open Question 2
- **Question:** How sensitive is TACE to the choice of hyperparameters, such as the initial value of the Lagrange multiplier σ and the distance threshold ϵ?
- **Basis in paper:** [explicit] The paper mentions that the values of σ and ϵ are selected empirically and that the performance of TACE may be sensitive to these hyperparameters.
- **Why unresolved:** The paper does not provide a systematic analysis of the sensitivity of TACE to these hyperparameters or discuss methods for automatically tuning them.
- **What evidence would resolve it:** Experimental results showing the impact of different hyperparameter values on the performance of TACE, along with guidelines for selecting appropriate values.

### Open Question 3
- **Question:** How does TACE compare to other exploration strategies in terms of sample efficiency and convergence speed in large-scale tasks?
- **Basis in paper:** [explicit] The paper compares TACE to several baseline methods in terms of exploration efficiency and average returns, but does not provide a comprehensive analysis of its sample efficiency and convergence speed in large-scale tasks.
- **Why unresolved:** The paper does not discuss the scalability of TACE or provide experimental results on large-scale tasks.
- **What evidence would resolve it:** Experimental results demonstrating the sample efficiency and convergence speed of TACE in large-scale tasks, along with a comparison to other exploration strategies.

## Limitations
- The method's performance depends on the quality and relevance of offline demonstrations, which may not always be available or representative
- Adaptive methods for constraint adjustment and Lagrange multiplier scaling rely on empirically chosen thresholds without theoretical guarantees
- Evaluation is limited to grid worlds and MuJoCo tasks, with unclear generalization to more complex or deceptive reward structures

## Confidence

- **High Confidence:** The MMD distance metric can effectively measure distributional differences between trajectories, and the constrained optimization framework is theoretically sound
- **Medium Confidence:** The adaptive constraint boundary adjustment via normalization will maintain effectiveness across different environments without manual tuning
- **Medium Confidence:** The adaptive scaling of the Lagrange multiplier based on simple heuristics will robustly balance exploration and exploitation in practice
- **Low Confidence:** The method will generalize effectively to tasks with highly complex or deceptive reward structures beyond the evaluated benchmarks

## Next Checks

1. **Robustness to Demonstration Quality:** Systematically vary the quality and relevance of offline demonstrations to test whether MMD constraints remain effective when demonstrations are noisy, irrelevant, or partially optimal.

2. **Scalability to High-Dimensional Tasks:** Evaluate TACE on continuous control tasks with significantly larger state and action spaces (e.g., humanoid locomotion) to assess whether MMD distance remains a meaningful exploration signal.

3. **Theoretical Guarantees on Constraint Satisfaction:** Provide formal analysis of the probability that the adaptive constraint boundary adjustment maintains the required MMD distance threshold over training, particularly under non-stationary policy updates.