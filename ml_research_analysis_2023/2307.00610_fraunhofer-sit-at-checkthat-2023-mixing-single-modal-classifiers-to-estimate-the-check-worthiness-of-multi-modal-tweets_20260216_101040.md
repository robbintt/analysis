---
ver: rpa2
title: 'Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate
  the Check-Worthiness of Multi-Modal Tweets'
arxiv_id: '2307.00610'
source_url: https://arxiv.org/abs/2307.00610
tags:
- data
- text
- tweets
- check-worthiness
- checkthat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of detecting check-worthy content
  in multi-modal tweets, where both text and images must be analyzed to determine
  if content requires manual fact-checking. The authors propose a novel method that
  combines two separate classifiers: one fine-tuned on the tweet text using a BERT
  model, and another using OCR-extracted text from images, also processed with BERT.'
---

# Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets

## Quick Facts
- arXiv ID: 2307.00610
- Source URL: https://arxiv.org/abs/2307.00610
- Reference count: 24
- F1 score: 0.7297 on private test set

## Executive Summary
This paper presents a multi-modal approach for detecting check-worthy content in tweets that combine text and images. The authors develop a system that uses two separate BERT-based classifiers - one for tweet text and another for OCR-extracted text from images - combined through weighted averaging based on validation losses. The approach achieved first place in the CheckThat! 2023 Task 1A competition with an F1 score of 0.7297. The key innovation is leveraging OCR extraction to transform image content into text that can be analyzed by text classifiers, which proved more effective than direct image classification approaches like Vision Transformers or CNNs.

## Method Summary
The method involves two parallel classification pipelines. The first fine-tunes a BERT model on preprocessed tweet text with emoji resolution and hashtag/URL normalization. The second extracts text from images using OCR, then fine-tunes a separate BERT model on this extracted text. Both classifiers are trained independently with class imbalance handling, and their predictions are combined using weighted averaging based on their respective validation losses. The approach uses Adam optimizer with learning rate 0.0004, batch sizes of 24 for text and 8 for OCR, and 5 training epochs.

## Key Results
- Achieved F1 score of 0.7297 on private test set, placing first in CheckThat! 2023 Task 1A
- OCR-extracted text analysis outperformed direct image classification (Vision Transformer, CNN) for check-worthiness detection
- Combining modality-specific classifiers through weighted averaging improved overall performance across all datasets
- Pre-processing tweets (emoji resolution, hashtag/URL conversion) significantly improved BERT classifier performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCR-extracted text from images performs better than direct image classification for check-worthiness detection.
- Mechanism: OCR extracts textual information from images that can be processed by text classifiers, which is more relevant for check-worthiness than visual features alone.
- Core assumption: Images contain textual claims relevant to check-worthiness that can be accurately extracted.
- Evidence anchors: Abstract states OCR performs best for image data; section 4.3 notes multilingual models like XLM could improve performance.
- Break condition: If images contain little text or OCR accuracy is poor, this approach would fail.

### Mechanism 2
- Claim: Weighted averaging of classifier predictions based on validation losses improves performance.
- Mechanism: Combines predictions from text and OCR classifiers using weights inversely proportional to their validation losses.
- Core assumption: Validation loss reliably indicates classifier performance and captures complementary information.
- Evidence anchors: Section 4.3 shows combined classifiers improved F1 scores across all datasets; abstract notes this led to first place.
- Break condition: If classifiers are highly correlated or one consistently outperforms the other, combination may not help.

### Mechanism 3
- Claim: Pre-processing tweets improves BERT performance.
- Mechanism: Transforms tweet-specific elements into generic tokens better understood by pre-trained BERT models.
- Core assumption: BERT understands text better when informal elements are converted to standard representations.
- Evidence anchors: Section 4.2 shows F1 increased from 0.5377 to 0.7172 with pre-processing.
- Break condition: If BERT is fine-tuned extensively on raw tweets, pre-processing benefits may diminish.

## Foundational Learning

- Multi-modal learning - combining information from different input types (text and images)
  - Why needed here: Task requires analyzing both tweet text and image content to determine check-worthiness
  - Quick check question: Why not use a single model that takes both text and image as input?

- Transfer learning with pre-trained language models (BERT)
  - Why needed here: Fine-tuning BERT on check-worthiness detection leverages pre-learned language representations
  - Quick check question: What is the advantage of fine-tuning BERT versus training from scratch?

- Class imbalance handling in classification
  - Why needed here: Dataset has 2:1 ratio of non-check-worthy to check-worthy tweets
  - Quick check question: How does class imbalance affect interpretation of accuracy versus F1 score?

## Architecture Onboarding

- Component map: Tweet text → Pre-processing → BERT fine-tuning → Text classifier; Image → OCR extraction → BERT fine-tuning → Image classifier; Predictions → Weighted averaging → Final classification
- Critical path: Tweet text → Pre-processing → BERT fine-tuning → Text classifier prediction; Image → OCR extraction → BERT fine-tuning → Image classifier prediction; Predictions → Weighted averaging → Final check-worthiness classification
- Design tradeoffs:
  - Separate modality-specific classifiers vs. joint multi-modal model: Separate allows specialized training but may miss cross-modal interactions
  - OCR-based image analysis vs. direct image classification: OCR captures textual content but fails when images contain no text
  - Weighted averaging vs. other fusion methods: Simple and interpretable but may miss complex relationships
- Failure signatures: Low OCR accuracy, class imbalance issues, poor text classifier adaptation to tweets
- First 3 experiments: 1) Compare BERT with/without pre-processing on development set, 2) Test OCR accuracy on sample images, 3) Evaluate individual classifier performance before ensemble

## Open Questions the Paper Calls Out

- How does the performance of XLM-based models compare to BERT for OCR-extracted text analysis in multi-modal tweets?
  - Basis in paper: Authors mention multilingual models like XLM could provide better performance for non-English images
  - Why unresolved: Only tested English BERT models despite noting multilingual benefits
  - What evidence would resolve it: Empirical comparison of XLM-based vs BERT-based models on the same dataset

- What specific visual features in images contribute most to check-worthiness, and can these be captured without relying on text extraction?
  - Basis in paper: Authors found image classifiers failed while OCR succeeded, suggesting untapped visual features
  - Why unresolved: Only tested general-purpose vision models without investigating specific visual patterns
  - What evidence would resolve it: Analysis of distinguishing visual features in successful vs failed check-worthy images

- How does the proposed multi-modal approach generalize to other languages and cultural contexts beyond English tweets?
  - Basis in paper: Authors note some images were not in English and suggest XLM could help
  - Why unresolved: Entire evaluation performed only on English data
  - What evidence would resolve it: Testing the same approach on multi-modal datasets from other languages

## Limitations

- Dependence on OCR accuracy for image-based check-worthiness detection
- Handling of class imbalance may still influence model behavior despite using F1 score
- May miss cross-modal interactions by using separate classifiers for text and image data

## Confidence

- High confidence: General effectiveness of combining modality-specific classifiers for multi-modal tasks
- Medium confidence: Specific superiority of OCR-based image analysis over direct image classification
- Medium confidence: Effectiveness of weighted averaging based on validation losses

## Next Checks

1. Evaluate OCR system's performance on diverse sample of images including non-English text and complex layouts
2. Implement and compare a joint multi-modal model using transformer-based cross-attention against current approach
3. Test system on recent sample of tweets from Twitter to evaluate performance on current discourse patterns and image types