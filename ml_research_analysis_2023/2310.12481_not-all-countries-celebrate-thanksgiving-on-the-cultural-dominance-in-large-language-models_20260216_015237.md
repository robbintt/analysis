---
ver: rpa2
title: 'Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large
  Language Models'
arxiv_id: '2310.12481'
source_url: https://arxiv.org/abs/2310.12481
tags:
- cultural
- year
- language
- llms
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a cultural dominance issue in large language
  models (LLMs) where responses are disproportionately influenced by English culture,
  even when queried in non-English languages. To evaluate this, the authors create
  a benchmark covering both concrete cultural objects (e.g., holidays, songs) and
  abstract cultural values (using established surveys).
---

# Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models

## Quick Facts
- arXiv ID: 2310.12481
- Source URL: https://arxiv.org/abs/2310.12481
- Reference count: 7
- Key outcome: Large language models (LLMs) exhibit strong English cultural dominance, with ChatGPT and GPT-4 showing the highest bias; pretraining on diverse multilingual data and culture-aware prompting can mitigate this issue.

## Executive Summary
This paper identifies a significant cultural dominance issue in large language models (LLMs), where responses are disproportionately influenced by English culture even when queried in non-English languages. The authors create a comprehensive benchmark covering both concrete cultural objects (holidays, songs, books, etc.) and abstract cultural values (using established surveys) to quantify this bias across 11 languages. Their findings reveal that GPT-4 is the most culturally dominant model, while pretraining on more diverse multilingual data and using culture-aware prompting can significantly reduce this bias. The study highlights the need for more inclusive and culturally sensitive LLM development practices.

## Method Summary
The authors developed a multilingual benchmark to evaluate cultural dominance in LLMs, testing three GPT models (text-davinci-003, ChatGPT, and GPT-4) across 11 languages. They measured cultural alignment using two approaches: In-Culture Scores for concrete objects (comparing model responses to Wikipedia references) and Euclidean distances from survey-based reference points for abstract cultural values. To mitigate cultural dominance, they tested two strategies: pretraining on more diverse multilingual data and culture-aware prompting. The evaluation covered 8 concrete cultural object categories and 2 abstract surveys (World Values Survey and Political Coordinates Test).

## Key Results
- GPT-4 exhibits the strongest English cultural dominance among all tested models
- Pretraining on more diverse multilingual data significantly reduces cultural bias (ERNI model shows marked improvement)
- Culture-aware prompting effectively improves performance on concrete cultural objects but shows limited effectiveness for abstract cultural values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on more diverse multilingual data directly reduces cultural dominance by shifting the model's learned distribution toward non-English cultural patterns.
- Mechanism: By including more data from non-English sources, the model's learned representations of cultural entities become more balanced, reducing the influence of English-centric norms.
- Core assumption: The pretraining data is representative of its source culture and does not itself contain strong English cultural bias.
- Evidence anchors:
  - [abstract] "We show that two straightforward methods in model development (i.e., pretraining on more diverse data) and deployment (e.g., culture-aware prompting) can significantly mitigate the cultural dominance issue in LLMs."
  - [section] "One fundamental solution to the cultural bias problem is to train the LLMs on more diverse data, which contains a larger portion of non-English data."
  - [corpus] Weak. Corpus contains no quantitative comparison of pretraining data composition effects.
- Break condition: If non-English pretraining data is already dominated by English-influenced content (e.g., translated English media), pretraining diversity will have limited effect.

### Mechanism 2
- Claim: Prompting that explicitly specifies the query language's cultural context steers the model to generate culturally aligned responses.
- Mechanism: The prompt provides an explicit context signal that overrides the default cultural bias in the model's output generation.
- Core assumption: The model has latent multilingual cultural knowledge that can be activated by explicit prompts.
- Evidence anchors:
  - [abstract] "We show that two straightforward methods in model development (i.e., pretraining on more diverse data) and deployment (e.g., culture-aware prompting) can significantly mitigate the cultural dominance issue in LLMs."
  - [section] "The prompting method can significantly improve the performance on concrete cultural objects, while is less effective on abstract objects that require more complex cultural knowledge for non-English languages."
  - [corpus] Weak. Corpus does not detail the linguistic or cultural knowledge retrieval process during prompting.
- Break condition: If the model lacks sufficient cultural knowledge for the target language, prompting cannot retrieve it.

### Mechanism 3
- Claim: Cultural dominance increases with later GPT models because RLHF fine-tuning amplifies existing English cultural bias in the training data.
- Mechanism: Human feedback is collected predominantly from English-speaking annotators, reinforcing English cultural norms during fine-tuning.
- Core assumption: The RLHF process is English-centric, both in data and annotator demographics.
- Evidence anchors:
  - [abstract] "For the GPT family, text-davinci-003 suffers least from the culture dominance issue, while GPT-4 suffers most from this problem."
  - [section] "One possible reason is that later GPT models is trained with more safety alignment in English."
  - [corpus] Weak. Corpus does not provide evidence of RLHF annotator demographics or language bias.
- Break condition: If RLHF uses diverse annotators or multilingual feedback, the cultural dominance pattern may not follow this trajectory.

## Foundational Learning

- Concept: Cultural dominance in LLMs
  - Why needed here: The paper's central claim hinges on identifying and measuring how English cultural norms disproportionately influence model outputs across languages.
  - Quick check question: How does the paper define and measure "cultural dominance"?

- Concept: Concrete vs. abstract cultural objects
  - Why needed here: The evaluation benchmark distinguishes between easily verifiable cultural facts and subjective cultural values, each requiring different measurement strategies.
  - Quick check question: What is the difference between concrete and abstract cultural objects in this study?

- Concept: Multilingual value surveys (WVS, PCT)
  - Why needed here: These surveys provide a standardized, cross-cultural reference for measuring abstract cultural opinions, allowing comparison between model outputs and real-world human beliefs.
  - Quick check question: Which two multilingual surveys are used to measure abstract cultural values?

## Architecture Onboarding

- Component map: Question Generator -> Model Query Interface -> Response Annotator -> Benchmark Evaluator (In-Culture Scores + Euclidean Distance) -> Mitigation Module (Pretraining/Prompting)
- Critical path: Generate multilingual queries → collect model responses → compute cultural alignment scores → apply mitigation (if needed) → compare before/after performance
- Design tradeoffs: Pretraining is more effective but expensive; prompting is cheaper but less effective for abstract objects
- Failure signatures: High in-culture scores for English queries but low scores for non-English queries indicate cultural dominance; high Euclidean distances from target culture values also indicate bias
- First 3 experiments:
  1. Replicate the in-culture score calculation for a single object (e.g., holidays) across all 11 languages
  2. Run the WVS and PCT surveys with a baseline model and compare Euclidean distances to English vs. target culture
  3. Test the effect of a simple prompt ("In the culture of [lang] language") on concrete objects and measure the change in in-culture scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the extent of cultural dominance vary across different types of concrete cultural objects (e.g., holidays vs. celebrities vs. mountains)?
- Basis in paper: [inferred] The paper shows that ChatGPT has varying in-culture scores across different concrete objects (e.g., higher for holidays, lower for songs and celebrities)
- Why unresolved: The paper provides aggregate results but doesn't analyze patterns or correlations between object types and cultural dominance
- What evidence would resolve it: A detailed breakdown showing cultural dominance scores for each object type across all languages, along with statistical analysis of correlations

### Open Question 2
- Question: Would pretraining on more diverse data maintain its effectiveness at mitigating cultural dominance when tested on abstract cultural objects?
- Basis in paper: [explicit] The paper shows pretraining on diverse data (ERNIE) significantly improves performance on concrete objects but doesn't test abstract objects
- Why unresolved: The study only tested pretraining's effect on concrete cultural objects, leaving open whether this approach works for abstract values and opinions
- What evidence would resolve it: Results comparing ERNIE's performance on abstract cultural objects (WVS and PCT) against GPT models

### Open Question 3
- Question: How does the effectiveness of culture-aware prompting vary with different prompt formulations beyond the two tested?
- Basis in paper: [explicit] The paper tested two simple prompts (P1 and P2) and found P1 significantly more effective than P2 for concrete objects
- Why unresolved: Only two prompt formulations were tested, leaving uncertainty about optimal prompting strategies
- What evidence would resolve it: Comparative results of multiple prompting strategies (varying in specificity, structure, and cultural cues) tested across both concrete and abstract objects

## Limitations
- Relies on static benchmarks that may not capture the dynamic nature of cultural knowledge
- Does not quantify the extent of English cultural bias in pretraining data
- Lacks evidence on RLHF annotator demographics, which is crucial for understanding cultural dominance mechanisms

## Confidence

- **High confidence**: GPT-4 shows highest cultural dominance; pretraining on diverse data effectively reduces bias
- **Medium confidence**: Prompting effective for concrete objects but not abstract values; mechanism not fully detailed
- **Low confidence**: RLHF with English-centric annotators is primary driver of increased cultural dominance; lacks direct evidence on annotator demographics

## Next Checks

1. **Quantify pretraining data bias**: Analyze the composition and cultural bias of the pretraining corpora used for each GPT model to determine if English cultural dominance is already present before RLHF.

2. **Experiment with multilingual RLHF**: Test whether using diverse, multilingual annotators for RLHF reduces cultural dominance in model outputs compared to English-only annotators.

3. **Probe latent cultural knowledge**: Conduct controlled experiments to assess whether culture-aware prompts successfully activate latent multilingual cultural knowledge, or if the model simply defaults to English norms when uncertain.