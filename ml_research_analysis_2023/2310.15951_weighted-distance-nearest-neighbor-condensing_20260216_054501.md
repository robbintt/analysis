---
ver: rpa2
title: Weighted Distance Nearest Neighbor Condensing
arxiv_id: '2310.15951'
source_url: https://arxiv.org/abs/2310.15951
tags:
- points
- nearest
- point
- rule
- condensing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Weighted distance nearest neighbor condensing introduces weights\
  \ to each point in the condensed set, modifying the distance function to consider\
  \ weighted distances (distance divided by weight). This generalization allows dramatically\
  \ better condensing than standard nearest neighbor condensing: there exist datasets\
  \ where WNN condensing yields a condensed set of size exactly 2, while standard\
  \ NN condensing requires \u0398(n) points."
---

# Weighted Distance Nearest Neighbor Condensing

## Quick Facts
- **arXiv ID**: 2310.15951
- **Source URL**: https://arxiv.org/abs/2310.15951
- **Reference count**: 9
- **Primary result**: Introduces weights to condensed set points, enabling Θ(n) to O(1) compression while maintaining Bayes consistency

## Executive Summary
Weighted Distance Nearest Neighbor Condensing generalizes standard nearest neighbor condensing by assigning weights to each point in the condensed set and modifying the distance function to consider weighted distances (distance divided by weight). This approach can dramatically improve condensing performance, achieving condensed sets of size 2 versus Θ(n) for standard methods on certain datasets, while maintaining nearly identical generalization bounds and Bayes consistency.

The paper proposes a greedy heuristic for WNN condensing that iteratively selects points to maximize coverage of uncovered points, assigning weights equal to the distance to the nearest enemy (opposite label). Empirical results demonstrate significantly better compression rates than popular NN condensing heuristics while maintaining comparable test error rates across multiple datasets including banana, circle, iris, and notMNIST.

## Method Summary
The greedy WNN condensing algorithm iteratively builds a weighted condensed set by selecting points that maximize coverage of uncovered points in the dataset. Each selected point receives a weight equal to its distance to the nearest enemy (the closest point with opposite label). The search module uses a navigating net structure augmented with heaviest-ancestor information to enable efficient approximate weighted nearest neighbor queries with time complexity independent of the number of distinct weights.

## Key Results
- WNN condensing achieves Θ(n) to O(1) size reduction on certain datasets while preserving Bayes consistency
- Greedy WNN heuristic maintains comparable test error rates to standard methods while achieving significantly better compression
- Weighted nearest neighbor search can be implemented in time independent of the number of distinct weights through navigating net structures
- Generalization bounds for WNN condensing are nearly identical to those of standard NN condensing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Weighted distances can reduce the condensed set size from Θ(n) to O(1) while preserving Bayes consistency.
- **Mechanism**: By assigning higher weights to points far from the decision boundary and close to the nearest enemy, the effective decision region expands, allowing few points to cover large areas.
- **Core assumption**: The distribution is atomless and the labeling function is piecewise continuous so small-radius balls have uniform label.
- **Evidence anchors**: [abstract] states optimal condensing under unweighted NN is Θ(n) while WNN yields size exactly 2; [section 3.1] proves Lemma 2 with explicit constructions.
- **Break condition**: If the distribution has atoms or the label function is discontinuous, the uniform label assumption fails and the bound doesn't hold.

### Mechanism 2
- **Claim**: The greedy WNN heuristic is Bayes consistent under mild assumptions.
- **Mechanism**: The greedy algorithm covers the largest uncovered set with a weighted point, and the permutation-invariant reconstruction ensures generalization bounds match unweighted NN up to constants.
- **Core assumption**: The space is separable metric, the distribution is atomless, and the label function is countably piecewise continuous.
- **Evidence anchors**: [section 4.2] proves Theorem 5 establishing Bayes consistency; [section 3.2] shows Theorem 4's generalization bound matches unweighted NN.
- **Break condition**: If the tail of the distribution decays too slowly, |S*| log |S*| may not be sub-linear, breaking consistency.

### Mechanism 3
- **Claim**: Weighted nearest-neighbor search can be implemented in time independent of the number of distinct weights.
- **Mechanism**: Using a navigating net structure with heaviest-ancestor augmentation, the algorithm checks O(1) nodes per tree level, making search time depend on doubling dimension rather than weight classes.
- **Core assumption**: The metric space has bounded doubling dimension and the weight function has finite distinct values.
- **Evidence anchors**: [section 5] provides Lemma 7 giving time bound ϵ^−O(ddim)·t for (1+ε)-approximate queries.
- **Break condition**: If doubling dimension is large or unbounded, the ϵ^−O(ddim) factor becomes prohibitive.

## Foundational Learning

- **Concept**: Metric spaces and doubling dimension
  - Why needed here: Condensing and search algorithms rely on metric properties (triangle inequality, packing/covering) and doubling dimension for efficient search.
  - Quick check question: In a metric space, does the triangle inequality guarantee that all balls of radius r can be covered by a bounded number of radius r/2 balls?

- **Concept**: Generalization bounds and sample compression
  - Why needed here: Theoretical guarantees show WNN rule doesn't overfit despite stronger condensing, relying on compression-based bounds.
  - Quick check question: If a learning rule can be described by a subset of the sample plus small auxiliary data, can we bound its generalization error using the compression coefficient?

- **Concept**: Bayes consistency
  - Why needed here: Theorems establish greedy WNN rule converges to Bayes-optimal classifier under appropriate conditions.
  - Quick check question: What must be true about the asymptotic size of the compressed set relative to sample size for Bayes consistency?

## Architecture Onboarding

- **Component map**: Preprocess -> Greedy condense -> Store (S, w) -> Query via search -> Label
- **Critical path**: Preprocess → Greedy condense → Store (S, w) → Query via search → Label
- **Design tradeoffs**:
  - Exact vs. approximate search: Exactness increases query time; approximation reduces to O(ϵ^−O(ddim)·t)
  - Compression vs. accuracy: Heavier weights increase coverage but may mislabel near boundaries
  - Permutation-invariant vs. non-invariant reconstruction: The former gives tighter bounds but may require more bookkeeping

- **Failure signatures**:
  - Condensing size ≈ n: Greedy heuristic failed to cover; likely due to poor weight choices or label discontinuities
  - Test error spikes: Over-compression or incorrect weight assignment; check nearest-enemy distances
  - Search times blow up: Doubling dimension is high; consider dimensionality reduction or alternative search structures

- **First 3 experiments**:
  1. Run the greedy WNN heuristic on the banana dataset; compare condensed set size and test error vs. MSS and RSS
  2. Implement the navigating net search on a small synthetic 2D dataset; verify (1+ε)-approximation by comparing to brute-force weighted NN
  3. Vary the weight function (e.g., use distance to nearest enemy vs. fixed weight) on the iris dataset; measure effect on compression ratio and generalization bound tightness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the greedy heuristic for weighted distance nearest neighbor (WNN) condensing be proven to be Bayes consistent under more general conditions?
- Basis in paper: The paper mentions Bayes consistency is established under mild assumptions but suggests further research could extend these conditions.
- Why unresolved: The current proof relies on specific assumptions about data distribution and labeling function.
- What evidence would resolve it: A formal proof showing Bayes consistency under broader conditions, such as more general distributions or different labeling functions.

### Open Question 2
- Question: How does the weighted distance function in WNN condensing perform compared to other distance functions in terms of generalization and compression?
- Basis in paper: The paper introduces the weighted distance function as a generalization of the standard NN rule and shows it can lead to better condensing, but doesn't compare to other distance functions.
- Why unresolved: The paper doesn't provide direct comparison to other distance functions commonly used in nearest neighbor algorithms.
- What evidence would resolve it: Empirical results comparing WNN condensing with other distance functions (Euclidean, Manhattan, Mahalanobis) in terms of generalization error and compression rate.

### Open Question 3
- Question: Can the weighted distance function in WNN condensing be extended to handle multi-class classification problems more effectively?
- Basis in paper: The paper focuses on binary classification problems, and it's unclear how the weighted distance function would perform in multi-class scenarios.
- Why unresolved: The current formulation is based on a binary distance function, and its extension to multi-class problems is not explicitly discussed.
- What evidence would resolve it: A study of WNN condensing performance in multi-class classification problems, comparing it to other multi-class nearest neighbor algorithms.

## Limitations
- Theoretical claims hinge on specific distributional assumptions (atomless, piecewise continuous labeling) that may not hold in practical datasets
- The Θ(n) to O(1) size reduction exists only for carefully constructed adversarial distributions, not typical real-world data
- Real-world data characteristics could significantly affect condensing performance compared to theoretical guarantees

## Confidence
- **High confidence**: The greedy heuristic implementation and its empirical comparison methodology (compression ratio, test error metrics)
- **Medium confidence**: Theoretical generalization bounds and Bayes consistency proofs, which depend on strong distributional assumptions
- **Medium confidence**: The weighted nearest neighbor search implementation, as doubling dimension properties vary significantly across datasets

## Next Checks
1. Test the greedy WNN heuristic on datasets with known distributional properties (e.g., uniform, Gaussian mixtures) to verify the Θ(n) vs O(1) size claims under controlled conditions.

2. Evaluate the condensing performance when distributional assumptions are violated (e.g., datasets with label noise, atoms, or discontinuous labeling functions) to identify break conditions.

3. Benchmark the weighted nearest neighbor search implementation across datasets with varying doubling dimensions to validate the claimed time complexity bounds in practice.