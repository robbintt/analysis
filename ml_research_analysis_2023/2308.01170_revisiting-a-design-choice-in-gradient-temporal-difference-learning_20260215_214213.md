---
ver: rpa2
title: Revisiting a Design Choice in Gradient Temporal Difference Learning
arxiv_id: '2308.01170'
source_url: https://arxiv.org/abs/2308.01170
tags:
- learning
- direct
- have
- proof
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper revisits the A\u22A4TD algorithm, an intermediate algorithm\
  \ in the derivation of Gradient Temporal Difference (GTD) learning, which was previously\
  \ deemed inferior to GTD. The authors propose a variant called A\u22A4tTD that solves\
  \ the double sampling issue in off-policy learning without requiring an extra weight\
  \ vector, unlike GTD."
---

# Revisiting a Design Choice in Gradient Temporal Difference Learning

## Quick Facts
- **arXiv ID**: 2308.01170
- **Source URL**: https://arxiv.org/abs/2308.01170
- **Reference count**: 6
- **Primary result**: A variant of ATD called A⊤tTD achieves asymptotic and finite-sample convergence rates comparable to on-policy TD while solving the double sampling issue without requiring an extra weight vector.

## Executive Summary
This paper revisits the A⊤TD algorithm, an intermediate algorithm in the derivation of Gradient Temporal Difference (GTD) learning that was previously deemed inferior. The authors propose a variant called A⊤tTD that solves the double sampling issue in off-policy learning without requiring an extra weight vector. The method uses two samples with an increasing gap to directly estimate the gradient, achieving computational efficiency similar to GTD. Asymptotic and finite-sample analyses are provided, with the latter showing a convergence rate comparable to on-policy TD learning up to logarithmic factors. Experiments on Baird's counterexample demonstrate that A⊤tTD converges faster and with less oscillation than GTD.

## Method Summary
The paper proposes Direct GTD, an off-policy temporal difference learning algorithm that avoids the double sampling issue by using two temporally separated samples with an increasing gap function f(t). The algorithm updates weights iteratively using the formula wt+1 = wt + αt ρt+f(t) (xt+f(t) - γxt+f(t)+1) x⊤t+f(t) ρt (Rt+1 + γx⊤t+1wt - x⊤twt) xt, where xt represents state features, ρt is the importance sampling ratio, and αt is the learning rate. The key innovation is that the gap function f(t) grows slowly with time (e.g., f(t) = ⌊ln²(t+1)⌋), allowing the algorithm to directly estimate the gradient without requiring an extra weight vector. The method also includes a finite-sample variant with projection to achieve O(f(t)ln(t)/t) convergence rate.

## Key Results
- Direct GTD achieves asymptotic convergence to the TD fixed point without requiring an extra weight vector
- The finite-sample variant achieves O(f(t)ln(t)/t) convergence rate, comparable to on-policy TD learning
- Experiments on Baird's counterexample show Direct GTD converges faster and with less oscillation than GTD
- The method requires O(f(t)) memory overhead, which grows sublinearly with time

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Increasing the time gap between samples reduces correlation that causes double sampling issue
- **Mechanism**: Uses two temporally separated samples At and At+f(t) where f(t) grows slowly with time. As gap increases, statistical dependence between samples diminishes, allowing A⊤(At+f(t))At to approximate E[A⊤]E[At] asymptotically
- **Core assumption**: Underlying Markov chain mixes geometrically fast, so correlation between samples decays exponentially with time gap
- **Evidence anchors**:
  - [abstract] "the correlation between ˆAt and ˆAt+τ diminishes to zero geometrically fast with the increase of τ under some mild conditions"
  - [section 4] "limt→∞ Pr[Yt = y, Yt+f(t) = y′] → dY(y)dY(y′)"
- **Break condition**: If mixing time of Markov chain is too long or gap grows too slowly, correlation will not decay sufficiently and bias in gradient estimate will persist

### Mechanism 2
- **Claim**: Refined discretization of limiting ODE compensates for non-stationary nature of increasing gap function
- **Mechanism**: Adapts interval lengths Tm based on both learning rates {αt} and gap function f(t), ensuring noise averages out while gap grows enough to reduce correlation
- **Core assumption**: Decreasing learning rates and increasing gap function can be balanced so each interval captures enough samples for averaging while gap increases appropriately
- **Evidence anchors**:
  - [section 4] "Key to our analysis is a novel refined discretization of limiting ODEs"
  - [section 4] Lemma 4 and Lemma 5 show how interval lengths relate to learning rates and gap
- **Break condition**: If gap grows too fast relative to learning rate decay, intervals become too short to average out noise; if too slow, correlation persists

### Mechanism 3
- **Claim**: Projection operator in finite-sample variant ensures bounded iterates and enables faster convergence rates
- **Mechanism**: Projects each update onto a ball of radius B, preventing parameter divergence and maintaining stability, allowing use of simpler learning rate schedule
- **Core assumption**: Projection radius B is chosen large enough to contain the TD fixed point w*
- **Evidence anchors**:
  - [abstract] "The convergence rate of this variant is on par with the canonical on-policy temporal difference learning"
  - [section 5] "Theorem 2... convergence rate... is on-par with the convergence rate of the canonical on-policy linear TD"
- **Break condition**: If B is too small, projection will clip updates excessively and prevent convergence to true solution; if too large, benefits of projection are reduced

## Foundational Learning

- **Concept**: Double sampling issue in off-policy TD
  - Why needed here: Understanding why E[XY] ≠ E[X]E[Y] in general is crucial to appreciating why standard TD methods diverge in off-policy settings
  - Quick check question: Why can't we use a single sample to estimate both expectations in A⊤A when A is estimated from data?

- **Concept**: Markov chain mixing and ergodicity
  - Why needed here: The convergence analysis relies on the chain mixing fast enough that correlation between temporally separated samples decays geometrically
  - Quick check question: What property of the Markov chain ensures that Pr[Yt = y, Yt+f(t) = y′] → dY(y)dY(y′) as t → ∞?

- **Concept**: Stochastic approximation with non-stationary noise
  - Why needed here: The increasing gap function creates time-varying noise structure that requires specialized ODE discretization techniques beyond standard stochastic approximation theory
  - Quick check question: How does the refined discretization with variable interval lengths {Tm} differ from standard fixed-interval ODE discretization?

## Architecture Onboarding

- **Component map**: State → Feature extraction → Importance sampling → Gap-based sample selection → Gradient computation → Parameter update (with optional projection)

- **Critical path**: State → Feature extraction → Importance sampling → Gap-based sample selection → Gradient computation → Parameter update (with optional projection)

- **Design tradeoffs**:
  - Gap function growth rate vs. memory overhead: Slower growth reduces memory but may increase bias
  - Projection radius B vs. convergence speed: Larger radius allows closer approach to true solution but may reduce stability
  - Learning rate schedule vs. convergence guarantees: Simpler schedules are easier to tune but may require more careful gap function design

- **Failure signatures**:
  - Divergence: Learning rate too large or gap function growing too slowly
  - Slow convergence: Gap function growing too fast (excessive memory use) or learning rate decaying too quickly
  - Oscillation: Gap function growth rate inappropriate for problem's mixing time

- **First 3 experiments**:
  1. Implement Direct GTD on Baird's counterexample with varying gap functions (f(t) = ⌊ln(t+1)⌋, ⌊ln²(t+1)⌋, ⌊ln³(t+1)⌋) and compare convergence to GTD
  2. Test memory usage as a function of gap function growth rate on a small MDP
  3. Verify the effect of projection radius B on convergence speed and final error on a benchmark task

## Open Questions the Paper Calls Out

- **Open Question 1**: How much faster does Direct GTD converge compared to GTD in practical applications?
  - Basis in paper: [explicit] The paper notes that Direct GTD "converges faster and with less oscillation than GTD" in Baird's counterexample
  - Why unresolved: The empirical study only tested one counterexample. The paper acknowledges the need for a "more ambitious and thorough empirical comparison in more benchmark tasks."
  - What evidence would resolve it: Extensive experiments on multiple benchmark RL tasks comparing convergence speed of Direct GTD vs GTD

- **Open Question 2**: What is the slowest possible growth rate for the gap function f(t) that still guarantees convergence?
  - Basis in paper: [explicit] The paper states that "none of the gap functions in (22) satisfies Assumption 4.4" but Direct GTD still converges, suggesting Assumption 4.4 is sufficient but not necessary
  - Why unresolved: The paper provides only sufficient conditions and conjectures about optimal gap function design
  - What evidence would resolve it: Mathematical proof of necessary and sufficient conditions for the gap function, or extensive empirical testing of various gap functions

- **Open Question 3**: Can the refined discretization approach be generalized to other stochastic approximation algorithms?
  - Basis in paper: [inferred] The authors state "we do believe the insights therein can be easily transferred to analyzing other stochastic approximation algorithms with a similar structure."
  - Why unresolved: The paper does not provide a general theorem or framework for applying the refined discretization technique
  - What evidence would resolve it: Successful application of refined discretization to analyze convergence of other stochastic approximation algorithms with increasing state spaces

## Limitations

- The theoretical analysis relies heavily on geometric decay of correlations, which may not hold for all Markov chains
- Practical benefits relative to existing approaches like GTD in complex environments are uncertain and require empirical validation
- The method's performance may be sensitive to hyperparameter choices (gap function, learning rate, projection radius)

## Confidence

- **High Confidence**: The core algorithmic insight of using temporally separated samples to avoid the double sampling issue is sound and well-motivated. The asymptotic convergence proof structure appears rigorous.
- **Medium Confidence**: The finite-sample convergence rate analysis is promising but relies on specific assumptions about the gap function and learning rate schedule. The projection-based variant's performance guarantees may be sensitive to the choice of projection radius.
- **Low Confidence**: The practical benefits of the method relative to existing approaches like GTD in more complex, non-contrived environments are uncertain and require empirical validation.

## Next Checks

1. **Gap Function Sensitivity**: Systematically test the performance of Direct GTD with various gap function growth rates (e.g., f(t) = ⌊ln(t+1)⌋, ⌊ln²(t+1)⌋, ⌊ln³(t+1)⌋) on both Baird's counterexample and a more complex benchmark task to determine the optimal growth rate and its dependence on problem characteristics.

2. **Memory Overhead Analysis**: Quantify the memory requirements of Direct GTD as a function of the gap function growth rate and compare it to the memory usage of GTD on problems of increasing state space size to assess the practical trade-off.

3. **Robustness to Hyperparameter Choices**: Conduct a comprehensive hyperparameter sweep over the learning rate, projection radius (if applicable), and initial conditions to determine the method's sensitivity and identify robust default choices.