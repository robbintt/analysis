---
ver: rpa2
title: Towards ethical multimodal systems
arxiv_id: '2304.13765'
source_url: https://arxiv.org/abs/2304.13765
tags:
- prompts
- ethical
- these
- more
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating the ethics of
  multimodal AI systems that process both text and images. The authors create a dataset
  of 789 multimodal ethical prompts by collecting human feedback through a gamified
  Discord-based interface.
---

# Towards ethical multimodal systems

## Quick Facts
- arXiv ID: 2304.13765
- Source URL: https://arxiv.org/abs/2304.13765
- Reference count: 11
- This paper creates a dataset of 789 multimodal ethical prompts through a gamified Discord-based interface and trains classifiers achieving 52-55% accuracy on ethical evaluation.

## Executive Summary
This paper addresses the challenge of evaluating the ethics of multimodal AI systems that process both text and images. The authors create a dataset of 789 multimodal ethical prompts by collecting human feedback through a gamified Discord-based interface. They then train two classifiers—a RoBERTa-large model and a custom multilayer perceptron—to automatically assess the ethicality of system responses. The results suggest that current methods struggle with multimodal ethical evaluation, indicating a need for improved models and techniques.

## Method Summary
The authors developed a pipeline for collecting human feedback on multimodal ethical prompts using a Discord-based gamified interface. Users rated image-text pairs with generated responses using thumbs up/down/shrug reactions. The team created two classifiers: a RoBERTa-large baseline and a custom multilayer perceptron that takes both text (GPT-2) and image (CLIP) embeddings as input. The MLP classifier achieved 55% accuracy on classifying responses as ethical/unethical/unclear.

## Key Results
- Dataset of 789 multimodal ethical prompts created through crowdsourced Discord-based evaluation
- RoBERTa-large classifier achieved 52% accuracy on ethical classification task
- Custom MLP classifier achieved 55% accuracy using both text and image embeddings
- Current methods show limited effectiveness, suggesting need for improved multimodal ethical evaluation approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human feedback aggregation through gamification on Discord improves dataset quality and scalability for multimodal ethical evaluation.
- Mechanism: The system collects human ratings on multimodal prompts (image + text + generated response) using Discord's reaction system, aggregating multiple votes to classify prompts as ethical/unethical/unclear.
- Core assumption: Crowd-sourced judgments from a controlled demographic provide reliable ethical classifications when aggregated across multiple raters.
- Evidence anchors:
  - [abstract] "The authors create a dataset of 789 multimodal ethical prompts by collecting human feedback through a gamified Discord-based interface."
  - [section 3.3] "The evaluation process was also simplified with only 3 possible reactions, being a thumbs up for 'ethical', a thumbs down for 'unethical', and a shrug for 'unclear'."
  - [corpus] Weak - related papers focus on emotional alignment and practical challenges but don't directly address crowd-sourcing mechanisms for ethics evaluation.
- Break condition: If demographic control is removed or user trust mechanisms fail, the aggregated classifications may become unreliable due to bias or manipulation.

### Mechanism 2
- Claim: Multimodal ethical classification requires both text and image embeddings to capture contextual information that text alone cannot provide.
- Mechanism: The system uses GPT-2 tokenizer for text and CLIP (ResNet large) embedder for images, concatenating these embeddings to feed into a classifier that can evaluate ethicality.
- Core assumption: The ethical content of responses depends on both the textual prompt and the visual context, making unimodal approaches insufficient.
- Evidence anchors:
  - [abstract] "This paper focuses on evaluating the ethics of multimodal AI systems involving both text and images - a relatively under-explored area, as most alignment work is currently focused on language models."
  - [section 4.2] "To improve our classification accuracy, we started looking into alternative techniques... we would use both: the embeddings of the prompt and of the image."
  - [corpus] Weak - related papers discuss emotional alignment of AI systems but don't provide specific evidence for multimodal embedding requirements in ethical evaluation.
- Break condition: If the multimodal model fails to properly integrate text and image information, the classifier performance will degrade to near-random guessing.

### Mechanism 3
- Claim: Pre-testing and post-testing users on known ethical scenarios provides a reliable method to filter out unreliable or biased evaluators.
- Mechanism: Users are given 5 pre-test and 5 post-test prompts with known ethical classifications, and their responses are used to establish trustworthiness before incorporating their other ratings.
- Core assumption: Consistent correct responses to controlled test prompts indicate that users understand the evaluation task and possess the ethical framework being measured.
- Evidence anchors:
  - [section 3.4] "The second safeguard that we introduced was to test our users. This came in the form of a pre-test and a post-test... These tests consist of 5 prompts at the start and 5 prompts at the end that were hand-picked by the team."
  - [section 3.4] "The comparison between the pre-test and post-test answers would also allow us to see if a user's behavior had changed during the evaluation process."
  - [corpus] Weak - related papers focus on practical challenges and ethics evaluation but don't specifically address user reliability testing methods.
- Break condition: If users can game the pre-test/post-test system or if the test prompts don't adequately represent the ethical spectrum being measured, the filtering mechanism becomes ineffective.

## Foundational Learning

- Concept: Multimodal AI systems that process both text and images
  - Why needed here: The paper's core contribution is evaluating ethicality in systems that take both image and text inputs, unlike most existing work that focuses only on text.
  - Quick check question: Can you explain why a text-only ethical evaluation approach would be insufficient for assessing multimodal AI systems?

- Concept: Crowdsourcing and data aggregation methods
  - Why needed here: The dataset creation relies on collecting human judgments through Discord, requiring understanding of how to design interfaces and aggregate noisy human feedback.
  - Quick check question: How does aggregating multiple human ratings improve the reliability of ethical classifications compared to single-rater approaches?

- Concept: Machine learning classification with multimodal inputs
  - Why needed here: The paper builds classifiers that take both text and image embeddings as input, requiring knowledge of how to combine different data modalities for prediction tasks.
  - Quick check question: What are the key challenges in designing a classifier that can effectively use both text and image embeddings for ethical evaluation?

## Architecture Onboarding

- Component map:
  - Discord bot interface for data collection -> Human evaluation aggregation system -> Multimodal embedding generation (GPT-2 for text, CLIP for images) -> Classification models (RoBERTa-large baseline, custom MLP) -> Dataset management and quality control system

- Critical path: Human evaluation → Data aggregation → Embedding generation → Classification → Performance evaluation

- Design tradeoffs:
  - Discord-based vs. traditional survey platforms: Discord enables gamification and community building but may introduce demographic bias
  - Text-only vs. multimodal classifiers: Text-only approaches are simpler but miss visual context; multimodal approaches are more complex but capture richer information
  - Fixed thresholds vs. probabilistic outputs: Fixed thresholds provide clear classifications but may oversimplify nuanced ethical judgments

- Failure signatures:
  - High proportion of "unclear" classifications may indicate poor prompt design or insufficient context
  - Low accuracy on test prompts suggests issues with classifier architecture or training data quality
  - Demographic skew in human evaluations may indicate sampling bias

- First 3 experiments:
  1. Run the RoBERTa-large classifier on the dataset and compare its accuracy on prompts where image context is clearly necessary vs. unnecessary
  2. Test the MLP classifier with varying numbers of hidden layers to find the optimal architecture for this task
  3. Conduct A/B testing on Discord interface design to measure impact on user engagement and evaluation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve multimodal ethical classifiers beyond the 55% accuracy achieved by the multilayer perceptron model?
- Basis in paper: [explicit] The paper states that the multilayer perceptron classifier achieved only 55% accuracy, which is only marginally better than a coin toss.
- Why unresolved: The paper mentions hardware limitations prevented testing with more or bigger hidden layers, and the authors suggest that further work on the model could lead to more impressive results.
- What evidence would resolve it: Demonstrating a multimodal ethics classifier with significantly higher accuracy (e.g., >75%) on the same dataset, along with analysis of which architectural changes contributed to the improvement.

### Open Question 2
- Question: What is the optimal threshold for classifying prompts as "unclear" based on user disagreement rates?
- Basis in paper: [explicit] The paper notes that when more than 25% of users think a prompt is unclear, there is much disagreement across other users, but they haven't collected enough data to accurately set this threshold.
- Why unresolved: The authors observed a pattern between unclear votes and disagreement but lack sufficient data to determine the precise cutoff point.
- What evidence would resolve it: Statistical analysis of user response patterns showing the exact percentage of unclear votes at which classification agreement significantly drops, validated across multiple datasets.

### Open Question 3
- Question: How does demographic diversity affect the reliability and consistency of ethical evaluations in multimodal AI systems?
- Basis in paper: [explicit] The authors controlled demographics during initial testing with computer science students but acknowledge this limited the scope of their ethical discussion and plan to widen demographics as more results accumulate.
- Why unresolved: The paper only tested with a homogeneous group of computer science students and recognizes the need to test with more diverse populations but hasn't done so yet.
- What evidence would resolve it: Comparative analysis of ethical classification consistency across different demographic groups (age, culture, education level) using the same dataset and methodology.

## Limitations
- Classifier accuracies of 52-55% are only marginally above random guessing for three-class problem
- Limited dataset size of 789 prompts and homogeneous demographic of Discord-based participants
- Doesn't address potential biases in MAGMA-generated responses that could confound ethical classification

## Confidence
- High confidence: The methodology for data collection through Discord and the basic framework for multimodal embedding generation
- Medium confidence: The reported classifier accuracies and their comparison between RoBERTa-large and MLP models
- Low confidence: The practical utility of the classifiers for real-world ethical evaluation given their performance is only slightly better than random

## Next Checks
1. **Performance ceiling analysis**: Test whether more sophisticated multimodal models (e.g., CLIP-based vision-language models) can achieve significantly better accuracy than the reported 52-55%
2. **Demographic bias audit**: Replicate the data collection with a more diverse participant pool and measure how classifier performance varies across demographic groups
3. **Human agreement study**: Measure inter-rater reliability among human evaluators on the same prompts to establish a baseline for how consistently humans agree on ethical classifications