---
ver: rpa2
title: Method for Generating Synthetic Data Combining Chest Radiography Images with
  Tabular Clinical Information Using Dual Generative Models
arxiv_id: '2308.07573'
source_url: https://arxiv.org/abs/2308.07573
tags:
- data
- tabular
- synthetic
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a method to generate synthetic hybrid medical\
  \ records combining chest X-ray images and structured tabular data using an auto-encoding\
  \ GAN (\u03B1GAN) and a conditional tabular GAN (CTGAN). The approach reduces the\
  \ dimensionality of images using a pretrained \u03B1GAN encoder, combines the resulting\
  \ latent vectors with tabular data, and trains a CTGAN to generate synthetic hybrid\
  \ records."
---

# Method for Generating Synthetic Data Combining Chest Radiography Images with Tabular Clinical Information Using Dual Generative Models

## Quick Facts
- arXiv ID: 2308.07573
- Source URL: https://arxiv.org/abs/2308.07573
- Reference count: 0
- Key outcome: Synthetic hybrid medical records combining chest X-rays and clinical data maintain correspondence between modalities while enabling privacy-preserving data sharing.

## Executive Summary
This paper presents a novel method for generating synthetic hybrid medical records that combine chest X-ray images with structured tabular clinical information. The approach uses a dual GAN architecture, employing αGAN for image dimensionality reduction and CTGAN for tabular data synthesis. By training on a large public chest X-ray dataset and a smaller private clinical database, the method successfully generates synthetic records that preserve the statistical properties and relationships between images and clinical variables. The synthetic dataset demonstrates comparable classification performance to the original data while maintaining privacy for secondary analysis.

## Method Summary
The method combines a pretrained αGAN encoder to reduce chest X-ray images to 128-dimensional latent vectors with a CTGAN trained on concatenated image features and tabular clinical data. The αGAN model is first trained on a large public chest X-ray dataset (29,684 images from RSNA Pneumonia Detection Challenge) to learn effective image compression. The pretrained encoder then processes the smaller private dataset (1,384 cases from Stony Brook University) to extract image features. These features are combined with 70 tabular variables (7 categorical, 6 numeric) and fed into the CTGAN, which learns the joint distribution. The synthetic records are generated by sampling from the CTGAN and reconstructing images using the αGAN decoder, maintaining correspondence between synthetic images and their associated clinical data.

## Key Results
- The synthetic database (sDB) captured the distribution of the original database (oDB) as evidenced by similar inter-record distance distributions and t-SNE visualizations
- Classification performance using sDB was comparable to oDB, with AUCs around 0.85-0.92 for models trained on both image features and tabular data
- The method successfully maintained correspondence between synthetic images and tabular data, unlike unmatched benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dimensionality reduction of high-resolution images using a pretrained αGAN encoder allows tabular GAN models to effectively learn the joint distribution of image features and clinical data.
- Mechanism: The αGAN encoder maps high-dimensional CXR images to a 128-dimensional latent vector, drastically reducing complexity while preserving essential visual features. This compressed representation can be concatenated with structured tabular variables and fed into a CTGAN, which is designed for tabular data synthesis.
- Core assumption: The latent space learned by αGAN preserves the relationship between image content and clinical context when combined with tabular variables.
- Evidence anchors:
  - [abstract] "dimensional reduction of images in a private dataset (pDS) using the pretrained encoder of the αGAN, followed by integration with the remaining non-image clinical data"
  - [section] "We adopted αGAN as a network to compress the image dimensions. αGAN is a structure that combines variational auto-encoders (VAE) with GAN"
- Break condition: If the encoder does not preserve discriminative features or if the latent space does not correlate with clinical outcomes, the synthetic records will lose critical information.

### Mechanism 2
- Claim: The synthetic dataset maintains correspondence between images and tabular data by training a conditional generator on joint feature-tabular representations.
- Mechanism: By training the CTGAN on concatenated image features and tabular data, the model learns conditional dependencies, ensuring that generated image features correspond to the clinical characteristics in the tabular output. The decoder of αGAN then reconstructs synthetic images from these features.
- Core assumption: The conditional GAN architecture can learn and enforce the correlation structure between latent image features and clinical variables.
- Evidence anchors:
  - [abstract] "successfully generated diverse synthetic records of hybrid CXR and tabular data, maintaining correspondence between them"
  - [section] "we prepared an unmatched database (uDB) as a benchmark for comparison with the proposed method"
- Break condition: If the CTGAN fails to capture the conditional relationships, synthetic records will show mismatched image features and clinical values.

### Mechanism 3
- Claim: Training on a large public image dataset enables effective pretraining of the αGAN encoder, allowing high-quality dimensionality reduction even with a small private dataset.
- Mechanism: The αGAN model is pretrained on the RSNA Pneumonia Detection Challenge dataset, which contains 29,684 CXRs. This pretrained encoder is then applied to the smaller private dataset (oDB) to extract image features without requiring extensive private data.
- Core assumption: Features learned on a large public dataset generalize well to the smaller private dataset of the same modality.
- Evidence anchors:
  - [abstract] "this method has the potential for the public release of synthetic datasets without compromising the secondary use of data"
  - [section] "Considering a large number of images to be necessary for the training of αGAN model with intermediate-level images, we used the RSNA Pneumonia Detection Challenge dataset"
- Break condition: If the domain shift between public and private datasets is large, the encoder will not extract meaningful features for the private data.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: GANs are used to generate synthetic data that mimics the distribution of real medical records, enabling privacy-preserving data sharing.
  - Quick check question: What are the two main components of a GAN and what roles do they play?
- Concept: Dimensionality reduction and latent space representations
  - Why needed here: High-dimensional image data must be compressed into a manageable form that can be combined with tabular data for joint synthesis.
  - Quick check question: How does reducing image dimensions to latent vectors facilitate the use of tabular GAN models?
- Concept: Conditional generation and correspondence enforcement
  - Why needed here: Ensures that synthetic images and tabular data remain aligned, preserving the clinical meaning of the generated records.
  - Quick check question: What mechanism ensures that generated image features correspond to the clinical variables in the synthetic records?

## Architecture Onboarding

- Component map:
  - Public Database (pDB) → αGAN Encoder → Latent Vectors
  - Private Database (oDB) → αGAN Encoder → Latent Vectors
  - oDB Image Features + Tabular Data → CTGAN → Synthetic Joint Records
  - Synthetic Image Features → αGAN Decoder → Synthetic Images
- Critical path: Pretrain αGAN on pDB → Encode oDB images → Train CTGAN on joint data → Generate synthetic records → Decode images
- Design tradeoffs:
  - Using a large public dataset for pretraining reduces the need for large private datasets but requires that public data match the modality and region of interest.
  - Dimensionality reduction simplifies the problem but risks losing fine-grained image details important for certain analyses.
  - Maintaining correspondence adds complexity but is necessary for realistic synthetic hybrid records.
- Failure signatures:
  - Broken or unrealistic images in the synthetic dataset suggest decoder issues or poor latent feature preservation.
  - Loss of correspondence between image features and tabular data indicates CTGAN training or architecture issues.
  - Poor classification performance on synthetic data may indicate mode collapse or insufficient feature representation.
- First 3 experiments:
  1. Train αGAN on pDB and evaluate image reconstruction quality on a held-out validation set.
  2. Encode oDB images, concatenate with tabular data, and train CTGAN; evaluate inter-record distances and t-SNE plots.
  3. Generate synthetic records and assess classification performance compared to oDB; check for correspondence between synthetic images and tabular data.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the text provided.

## Limitations
- The method relies on a large public dataset for αGAN pretraining, which may introduce domain shift concerns if public and private datasets differ in acquisition protocols or patient populations.
- The 128-dimensional latent space may not capture all clinically relevant image features, potentially limiting downstream utility for certain diagnostic tasks.
- The paper does not address potential privacy leakage through synthetic data, particularly for rare clinical cases that might be reconstructed.

## Confidence
- High confidence: The synthetic dataset successfully captures overall data distribution and maintains correspondence between image features and tabular data
- Medium confidence: The classification performance using synthetic data is comparable to original data, as the paper provides limited analysis of confidence intervals and variance across multiple runs
- Low confidence: Claims about clinical utility for secondary analysis, as the paper focuses on technical validation rather than real-world deployment outcomes

## Next Checks
1. Conduct privacy risk assessment on synthetic records to evaluate potential re-identification of rare cases
2. Perform ablation study with varying latent space dimensions to determine optimal feature preservation trade-offs
3. Test model generalization by applying the trained synthetic data generator to a completely independent chest X-ray dataset