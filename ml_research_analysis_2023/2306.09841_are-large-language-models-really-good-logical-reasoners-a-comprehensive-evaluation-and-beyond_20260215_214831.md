---
ver: rpa2
title: Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation
  and Beyond
arxiv_id: '2306.09841'
source_url: https://arxiv.org/abs/2306.09841
tags:
- reasoning
- answer
- explain
- llms
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on logical reasoning
  tasks, addressing the gap in comprehensive assessments of their reasoning abilities.
  The authors systematically evaluate 15 logical reasoning datasets across deductive,
  inductive, abductive, and mixed reasoning settings using three representative LLMs
  (text-davinci-003, ChatGPT, and BARD) under zero-shot, one-shot, and three-shot
  settings.
---

# Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond

## Quick Facts
- **arXiv ID**: 2306.09841
- **Source URL**: https://arxiv.org/abs/2306.09841
- **Reference count**: 29
- **Primary result**: Current LLMs struggle with logical reasoning, particularly in inductive settings, and often fail to provide rigorous explanations even when answers are correct

## Executive Summary
This paper systematically evaluates the logical reasoning capabilities of large language models (LLMs) across 15 datasets spanning deductive, inductive, abductive, and mixed reasoning types. Using three representative LLMs (text-davinci-003, ChatGPT, and BARD) under zero-shot, one-shot, and three-shot settings, the authors reveal that LLMs perform poorly on logical reasoning tasks, especially in inductive reasoning. Beyond simple accuracy metrics, they introduce fine-grained evaluation measures including answer correctness, explanation correctness, completeness, and redundancy, while categorizing errors into evidence selection and reasoning process types. The study introduces a new 3K-sample content-neutral dataset (NeuLR) to isolate reasoning capability from knowledge bias.

## Method Summary
The study evaluates 15 logical reasoning datasets across four reasoning types using three LLMs under three shot settings. The authors propose four fine-level evaluation metrics (answer correctness, explanation correctness, completeness, redundancy) and attribute errors to five types across two dimensions (evidence selection and reasoning process). To ensure fair evaluation, they introduce a content-neutral dataset called NeuLR. The evaluation methodology combines objective accuracy measures with subjective human annotation of explanation quality, providing a comprehensive assessment of both answer correctness and reasoning rigor.

## Key Results
- LLMs struggle with logical reasoning across all types, with particularly poor performance in inductive reasoning settings
- Many models achieve correct answers through flawed reasoning processes, as revealed by explanation quality analysis
- Hallucination errors are common, particularly in evidence selection processes
- Few-shot in-context learning provides inconsistent benefits for logical reasoning tasks
- The proposed NeuLR dataset successfully isolates reasoning capability from knowledge bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained evaluation metrics reveal logical reasoning flaws that simple accuracy metrics miss
- Mechanism: By decomposing reasoning evaluation into multiple dimensions, the evaluation captures both surface-level correctness and deeper reasoning quality
- Core assumption: Different dimensions of reasoning quality can be meaningfully separated and measured independently
- Evidence anchors:
  - [abstract]: "we propose fine-level evaluations in objective and subjective manners, covering both answers and explanations"
  - [section 3.3]: "To reflect the intermediate reasoning process of LLMs, we introduce four evaluation metrics"

### Mechanism 2
- Claim: Error attribution to evidence selection and reasoning process dimensions helps identify specific failure modes
- Mechanism: Categorizing errors into evidence selection and reasoning process creates a structured diagnostic framework
- Core assumption: Logical reasoning failures can be meaningfully decomposed into these two main dimensions
- Evidence anchors:
  - [abstract]: "To uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions"
  - [section 5.3]: "According to the previous statements, we define errors for the bad cases from two dimensions"

### Mechanism 3
- Claim: Content-neutral datasets help isolate logical reasoning capability from text understanding and knowledge bias
- Mechanism: By removing content-specific knowledge requirements, the evaluation focuses purely on the reasoning process
- Core assumption: Logical reasoning can be effectively evaluated without requiring domain-specific knowledge
- Evidence anchors:
  - [abstract]: "to avoid the influences of knowledge bias and concentrate purely on benchmarking the logical reasoning capability of LLMs, we propose a new dataset with neutral content"
  - [section 2]: "To avoid the influences of knowledge bias and purely focus on benchmarking the logical reasoning capability of LLMs, we propose a new dataset named NeuLR"

## Foundational Learning

- Concept: Deductive, inductive, and abductive reasoning types
  - Why needed here: The paper systematically evaluates LLMs across these three fundamental reasoning types
  - Quick check question: What distinguishes deductive reasoning from inductive reasoning in terms of conclusion certainty?

- Concept: Fine-grained evaluation metrics and their independence
  - Why needed here: Understanding how answer correctness, explanation correctness, completeness, and redundancy can be measured separately is crucial
  - Quick check question: Can a model have correct answers with incorrect explanations, and what does this reveal about reasoning quality?

- Concept: Error categorization frameworks
  - Why needed here: The ability to classify reasoning failures into evidence selection and reasoning process dimensions enables systematic analysis
  - Quick check question: How would you distinguish between an evidence selection error and a reasoning process error in a failed reasoning attempt?

## Architecture Onboarding

- Component map: Dataset collection → Evaluation metric implementation → Error categorization framework → Result visualization/analysis
- Critical path: Dataset → Evaluation metrics → Error attribution → Analysis and visualization
- Design tradeoffs: Content-neutral datasets provide fair evaluation but may lack real-world complexity; fine-grained metrics provide insight but increase evaluation complexity
- Failure signatures: Models showing correct answers but incorrect explanations indicate pattern matching rather than genuine reasoning
- First 3 experiments:
  1. Run zero-shot evaluation on a simple deductive reasoning dataset to establish baseline performance
  2. Evaluate the same model on content-neutral dataset to compare reasoning capability without knowledge bias
  3. Analyze error attribution on failed cases to identify predominant failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hallucinations manifest differently in deductive versus inductive reasoning tasks?
- Basis in paper: [explicit] The paper analyzes hallucination errors across different reasoning types and finds variations in frequency between deductive and inductive settings
- Why unresolved: The paper provides aggregate statistics on hallucinations but doesn't deeply explore the qualitative differences in how these errors manifest across reasoning types

### Open Question 2
- Question: What specific aspects of model architecture contribute to differences in logical reasoning performance between ChatGPT, text-davinci-003, and BARD?
- Basis in paper: [explicit] The paper compares three different LLMs with different sizes and training approaches, finding performance differences
- Why unresolved: While the paper identifies performance differences, it doesn't investigate the underlying architectural or training factors that cause these variations

### Open Question 3
- Question: How does the quality of explanations correlate with final answer correctness across different reasoning types?
- Basis in paper: [inferred] The paper introduces fine-grained evaluation metrics for both answer correctness and explanation quality but doesn't deeply analyze their relationship
- Why unresolved: The paper reports these metrics separately but doesn't explore whether good explanations consistently lead to correct answers or if they can be decoupled

## Limitations
- The evaluation's subjective metrics rely heavily on human annotation, introducing potential inter-annotator variability
- The proposed NeuLR dataset is described but not yet released, making it impossible to verify the content-neutral design claims
- The shot-based evaluation results may be influenced by prompt engineering quality rather than pure reasoning capability

## Confidence

- **High confidence**: Findings about overall LLM performance limitations in logical reasoning (Section 6.1)
- **Medium confidence**: Error attribution framework effectiveness - while the categorization appears systematic, the practical utility for improving models depends on further validation
- **Medium confidence**: Content-neutral dataset benefits - the concept is sound but the specific implementation and its impact require verification once NeuLR is released

## Next Checks

1. **Annotation reliability test**: Have multiple annotators independently evaluate explanation quality on a subset of cases to establish inter-annotator agreement scores and refine the rubrics

2. **Prompt engineering ablation**: Systematically vary the few-shot examples and prompt structure to determine how much performance differences are due to prompt quality versus reasoning capability

3. **NeuLR dataset validation**: Once released, conduct a detailed analysis of NeuLR's content neutrality by testing whether domain-specific knowledge still influences reasoning performance on this supposedly neutral dataset