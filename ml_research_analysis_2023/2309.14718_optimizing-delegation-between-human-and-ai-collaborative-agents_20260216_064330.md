---
ver: rpa2
title: Optimizing delegation between human and AI collaborative agents
arxiv_id: '2309.14718'
source_url: https://arxiv.org/abs/2309.14718
tags:
- agent
- agents
- manager
- actions
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work develops a framework for a manager agent to learn when\
  \ to delegate actions to a team of human and/or AI agents, even when the agents\
  \ operate under differing state transition models. The manager learns a delegation\
  \ policy without direct access to the agents\u2019 internal dynamics, instead observing\
  \ only team performance outcomes."
---

# Optimizing delegation between human and AI collaborative agents

## Quick Facts
- arXiv ID: 2309.14718
- Source URL: https://arxiv.org/abs/2309.14718
- Reference count: 24
- Primary result: Manager learns to optimally delegate between agents with different transition models without observing their internals, outperforming random selection in gridworld environments.

## Executive Summary
This work addresses the challenge of learning optimal delegation policies between human and AI agents when the agents have different state transition models. The proposed framework treats agents as options within a Semi-Markov Decision Process (SMDP), allowing the manager to learn when to delegate without direct access to agent internals. Through experimental validation in gridworld environments, the approach demonstrates that the manager can effectively learn to balance performance and cost by observing only team performance outcomes, achieving significantly better results than random selection across various team compositions and agent characteristics.

## Method Summary
The approach formulates delegation as an SMDP where the manager selects agents as options and observes outcomes after k-step actions. The manager learns a delegation policy through standard reinforcement learning techniques, specifically Q-learning adapted for SMDPs with γ^k discount factors. The reward function incorporates both task success (-1 per step, +100 for goal) and agent-specific costs, forcing the manager to optimize for both speed and efficiency. The key innovation is that the manager learns purely from observing state transitions and rewards without any knowledge of the agents' internal transition dynamics.

## Key Results
- Manager learned delegation policies significantly outperform random selection across various team compositions
- Framework successfully handles agents with different step sizes (1-3) and error likelihoods
- Manager adapts delegation policy to minimize total cost while maintaining performance
- Approach works effectively in both low and high cost scenarios for shorter-step agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The manager learns to delegate optimally without direct access to agents' internal transition models.
- Mechanism: The manager observes only state transitions and rewards after delegation, building a model of which agent performs best in which context.
- Core assumption: Agents operate in compatible MDPs where each state reachable by one agent is reachable by others, even if via different paths.
- Evidence anchors:
  - [abstract] "The manager learns a delegation policy without direct access to the agents’ internal dynamics, instead observing only team performance outcomes."
  - [section 4.2] "As is indicated, the likelihood of transitions relies directly on the agent policy πd and transitions Td for the delegated agent."
- Break condition: If agents operate in fundamentally incompatible state spaces where delegation would lead to unreachable states, the manager cannot learn valid policies.

### Mechanism 2
- Claim: The SMDP framework allows the manager to treat each agent as an option with multi-step actions.
- Mechanism: Agents are modeled as options where the manager selects an agent (option) and observes the outcome after k steps, updating value estimates with γ^k discount.
- Core assumption: Agent actions are temporally extended and can be treated as single options in the SMDP hierarchy.
- Evidence anchors:
  - [section 2.2] "In the case of a SMDP, the Markov assumption is relaxed... the transitions depend on more than just current state and the selected action."
  - [section 5] "Therefore, different agents can navigate an environment at different rates, and the manager observes that, when it delegates to a certain agent, that agent will operate for k single step actions in a time step without possibilities for further interventions."
- Break condition: If agents require frequent intervention or their actions are not temporally coherent, the SMDP option abstraction breaks down.

### Mechanism 3
- Claim: The manager learns to balance performance and cost by adjusting delegation policy based on agent-specific penalties.
- Mechanism: The reward function incorporates both task success (-1 per step, +100 for goal) and agent cost (-cd), forcing the manager to optimize for speed and efficiency.
- Core assumption: Agent costs are known and stable, allowing the manager to learn cost-aware policies.
- Evidence anchors:
  - [section 5] "R = {100 - cd if goal is reached, -1 - cd if goal not reached but valid step, -10 - cd if wall collision, -100 - cd if episode terminated}"
  - [section 6] "With this, the manager is given a reward function which counteracts some benefits of choosing agents with higher step sizes."
- Break condition: If agent costs change dynamically during operation, the learned policy may become suboptimal.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Both agents and manager operate within MDP frameworks, with agents having different state/action spaces but compatible reachability.
  - Quick check question: If two agents have different action sets but the same state space, what condition must hold for them to be "compatible" in this framework?

- Concept: Semi-Markov Decision Process (SMDP) and Options Framework
  - Why needed here: The manager treats each agent as an option that executes for multiple time steps, requiring SMDP theory for value estimation.
  - Quick check question: How does the value function in an SMDP differ from a standard MDP when dealing with temporally extended actions?

- Concept: Reinforcement Learning Policy Optimization
  - Why needed here: The manager learns a policy through standard RL techniques like Q-learning, despite not observing agent internals.
  - Quick check question: In the context of this paper, what specific modification is made to the standard Q-learning update to account for different agent step sizes?

## Architecture Onboarding

- Component map:
  - Environment: Gridworld with goal states and walls
  - Agent Layer: Multiple agents with different step sizes (1, 2, 3) and error probabilities
  - Manager Layer: RL agent that selects which underlying agent to delegate to
  - Interface: Manager observes state transitions and rewards, not agent actions
  - Reward: Combined performance and cost function

- Critical path:
  1. Initialize gridworld and agents
  2. Manager observes current state
  3. Manager selects agent (delegation decision)
  4. Selected agent executes k-step action
  5. Environment returns new state and reward
  6. Manager updates value estimates
  7. Repeat until goal or max steps

- Design tradeoffs:
  - Observation granularity: Manager only sees outcomes, not agent internals (reduces coupling but increases learning complexity)
  - Agent compatibility: Must ensure all agents can reach same states (limits agent diversity but enables delegation)
  - Cost modeling: Static costs simplify learning but may not capture dynamic resource usage

- Failure signatures:
  - Manager consistently selects suboptimal agents (likely learning rate or exploration issues)
  - Value estimates diverge or oscillate (potential discount factor or reward scaling problems)
  - Delegation policy ignores cost component (likely reward weighting imbalance)

- First 3 experiments:
  1. Test manager with two error-free agents of different step sizes in simple grid
  2. Introduce error probabilities and verify manager learns to prefer more reliable agent
  3. Vary agent costs and confirm manager adapts delegation policy to minimize total cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the manager perform in scenarios where agent transitions are non-deterministic or stochastic?
- Basis in paper: [explicit] The paper discusses agents with different transition functions but does not explore non-deterministic transitions.
- Why unresolved: The experiments only consider deterministic transitions, leaving the manager's adaptability to stochastic environments untested.
- What evidence would resolve it: Experiments with stochastic transitions and analysis of the manager's performance in such environments.

### Open Question 2
- Question: Can the manager effectively handle cases where agents have overlapping but not identical action spaces?
- Basis in paper: [explicit] The paper assumes agents operate in the same state space but with different actions, but does not address partial overlaps in action spaces.
- Why unresolved: The framework assumes distinct action spaces for agents, and the impact of overlapping actions is not explored.
- What evidence would resolve it: Testing the manager's performance with agents having partially overlapping action spaces.

### Open Question 3
- Question: How does the manager adapt when the cost structure for agents changes dynamically during an episode?
- Basis in paper: [inferred] The paper tests static cost scenarios but does not consider dynamic cost changes.
- Why unresolved: The experiments focus on static cost assignments, leaving the manager's adaptability to dynamic cost changes unexplored.
- What evidence would resolve it: Experiments with dynamic cost changes and analysis of the manager's adaptability in such scenarios.

## Limitations
- Assumption of compatible MDPs may not hold in heterogeneous human-AI teams with fundamentally different capabilities
- Static cost modeling may not capture dynamic resource usage patterns common in practical applications
- Scalability to more complex, real-world scenarios remains uncertain

## Confidence
- **High Confidence:** The SMDP framework correctly models temporally extended agent actions as options, and the value estimation methodology is sound given the assumptions.
- **Medium Confidence:** The learning algorithm converges to effective delegation policies in the tested gridworld environments, but generalization to larger or more complex state spaces requires further validation.
- **Low Confidence:** The assumption that agents operate in compatible MDPs with identical reachable state spaces may not hold in heterogeneous human-AI teams with fundamentally different capabilities.

## Next Checks
1. **State Space Compatibility Test:** Systematically vary the state space configurations to identify the boundary conditions where the manager's delegation policy breaks down, documenting the exact conditions under which agents become incompatible.
2. **Dynamic Cost Adaptation:** Implement a variant where agent costs change over time or based on agent state, and evaluate whether the manager can adapt its policy in real-time or requires retraining.
3. **Scalability Assessment:** Scale the gridworld complexity (larger grids, more complex navigation requirements) to evaluate whether the learning algorithm maintains performance and convergence speed as the state space grows exponentially.