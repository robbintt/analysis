---
ver: rpa2
title: 'Summaries as Captions: Generating Figure Captions for Scientific Documents
  with Automated Text Summarization'
arxiv_id: '2302.12324'
source_url: https://arxiv.org/abs/2302.12324
tags:
- captions
- figure
- caption
- text
- pegasus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that scientific figure captioning can be effectively
  treated as a text summarization task rather than a vision-to-language task. By extracting
  and summarizing paragraphs that mention each figure, the proposed method achieves
  better caption quality than prior vision-based approaches, as shown by higher ROUGE
  and MoverScore metrics.
---

# Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization

## Quick Facts
- arXiv ID: 2302.12324
- Source URL: https://arxiv.org/abs/2302.12324
- Reference count: 40
- Key outcome: Text summarization of figure-mentioning paragraphs outperforms vision-to-language approaches for scientific figure captioning

## Executive Summary
This paper reframes scientific figure captioning as a text summarization task rather than a vision-to-language problem. The authors demonstrate that by extracting and summarizing paragraphs that mention each figure, they can generate captions that are more effective than those produced by prior vision-based approaches. The method achieves higher ROUGE and MoverScore metrics and is preferred by human evaluators over original author captions 46.67% of the time, particularly for figures with originally unhelpful captions.

## Method Summary
The approach extracts sentences from scientific papers that reference specific figures, groups them into paragraphs, and applies an abstractive summarization model (PEGASUS) to condense this context into figure captions. The method uses multiple input variations including mentions only, full paragraphs, and OCR-extracted text from figures. A specialized model trained on longer captions (>30 tokens) shows improved performance. The study evaluates both automatic metrics (ROUGE, MoverScore) and human judgment by domain experts, revealing that automatic metrics can be misleading when training data contains many low-quality captions.

## Key Results
- Text summarization of figure-mentions achieves higher ROUGE and MoverScore metrics than vision-to-language approaches
- Human evaluation found generated captions were preferred over original captions 46.67% of the time
- Longer captions (>30 tokens) correlate with better reader comprehension and model performance
- Over 75% of figure caption tokens align with corresponding figure-mentioning paragraphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scientific figure captions can be effectively generated by summarizing the paragraphs that mention each figure.
- Mechanism: The method extracts sentences that explicitly reference a figure, groups them into paragraphs, and then applies an abstractive summarization model to condense these into a concise caption.
- Core assumption: The majority of caption content is already present in the surrounding text that describes the figure, making text summarization sufficient without requiring deep visual analysis.
- Evidence anchors:
  - [abstract] The paper states that over 75% of figure captions' tokens align with corresponding figure-mentioning paragraphs, motivating the use of language technology.
  - [section 3.2] BLEU-4 scores show high overlap between mentions and captions (e.g., First Mention vs. Whole Caption: 10.54 BLEU-4), supporting the token alignment claim.
  - [corpus] Weak. No direct corpus neighbor evidence for this alignment claim.
- Break condition: If the figure is not mentioned in the text, or mentions are too sparse or uninformative, the summarization approach will fail to generate useful captions.

### Mechanism 2
- Claim: Model performance improves when trained on higher-quality captions, specifically those longer than 30 tokens.
- Mechanism: By filtering the training data to include only captions exceeding a length threshold, the model learns to generate more detailed and informative captions, as longer captions are associated with better reader comprehension.
- Core assumption: Caption quality correlates with length, and human-written captions in scientific articles vary widely in usefulness, so training on a subset of better captions improves model outputs.
- Evidence anchors:
  - [abstract] The authors trained PegasusùëÉ +ùëÇ+ùêµ on captions with more than 30 tokens, based on literature suggesting longer captions improve comprehension.
  - [section 5.5] Human evaluation showed PegasusùëÉ +ùëÇ+ùêµ ranked significantly better than PegasusùëÉ +ùëÇ, indicating the quality control in training data matters.
  - [section 6.2] Correlation analysis found a moderate correlation between caption length and helpfulness ratings.
  - [corpus] Weak. No direct neighbor evidence for the length-quality relationship.
- Break condition: If the length threshold is set too high, the training set becomes too small, or if length does not actually correlate with quality in a given dataset, this approach may not help.

### Mechanism 3
- Claim: Human evaluation is necessary because automatic metrics like ROUGE can be misleading when training and test data contain many low-quality captions.
- Mechanism: The study conducted both automatic evaluations (ROUGE, MoverScore) and human evaluations by domain experts, revealing that models often perform better on unhelpful captions due to similarity to training data, not quality.
- Core assumption: Automatic metrics compare generated text to ground truth, but if the ground truth captions are often unhelpful, higher similarity does not mean better captions; human judgment is required to assess true quality.
- Evidence anchors:
  - [abstract] Human evaluation found the best model's captions were preferred over original captions 46.67% of the time, especially for figures with unhelpful original captions.
  - [section 5.5] MTurk pilot study excluded TrOCR due to poor human ratings, underscoring the