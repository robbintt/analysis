---
ver: rpa2
title: Biomedical Entity Linking with Triple-aware Pre-Training
arxiv_id: '2308.14429'
source_url: https://arxiv.org/abs/2308.14429
tags:
- entity
- biomedical
- linking
- information
- triples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of biomedical entity linking
  in text mining and question answering, where entities are scarcely distributed and
  semantic connections between them are not captured by current large language models
  (LLMs). To address this, the authors propose a novel framework for pre-training
  generative LLMs using a corpus synthesized from a knowledge graph (KG).
---

# Biomedical Entity Linking with Triple-aware Pre-Training

## Quick Facts
- arXiv ID: 2308.14429
- Source URL: https://arxiv.org/abs/2308.14429
- Reference count: 20
- One-line primary result: Triple-aware pre-training slightly improves biomedical entity linking performance, but benefits are marginal and not statistically significant.

## Executive Summary
This paper addresses the challenge of biomedical entity linking by proposing a novel framework for pre-training generative large language models (LLMs) using a corpus synthesized from a knowledge graph (KG). The approach linearizes triples and synonym information from the KG during pre-training to help the model learn semantic relationships between biomedical entities. However, experimental results show that including synonym, description, or relational information does not provide significant performance benefits, with only marginal improvements (0.2-0.5% in Recall@1) over the baseline BART model on BC5CDR and NCBI datasets. The authors conclude that more sophisticated methods are needed to effectively inject knowledge into LLMs for downstream tasks like biomedical entity linking.

## Method Summary
The proposed method involves pre-training a BART-large generative model on a synthesized corpus derived from the UMLS knowledge graph subset st21pv. The corpus includes linearized triples and synonym information, using two different linearization strategies (line-by-line and all-in-one). The pre-trained model is then fine-tuned on biomedical entity linking datasets (BC5CDR and NCBI) where the task is to generate entity identifiers for mentions in text. The evaluation metric is Recall@1, measuring the percentage of mentions correctly linked to the right entity.

## Key Results
- Pre-training on KG-derived corpus shows only marginal improvements (0.2-0.5% Recall@1) over baseline BART model
- Including synonym, description, or relational information during pre-training does not provide significant performance benefits
- Two linearization strategies (line-by-line vs all-in-one) show no clear performance differences
- The proposed triple-aware pre-training approach slightly outperforms baseline but with limited practical significance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on a synthesized corpus derived from a knowledge graph helps the model learn semantic relationships between biomedical entities.
- Mechanism: The model is exposed to triples during pre-training, which encode relational knowledge. By linearizing these triples into text format, the generative model learns how entities are connected, potentially improving its ability to disambiguate mentions during entity linking.
- Core assumption: The relational structure of the KG can be effectively captured by a generative model through pre-training on linearized triples.
- Evidence anchors:
  - [abstract] The paper states: "We propose a novel framework to pre-train the powerful generative LLM by a corpus synthesized from a KG."
  - [section] "We linearize the information from synonym and triples in the pre-training stage... We have tested 2 different settings for converting the triples, namely line-by-line and all-in-one."
- Break condition: If the linearization process loses essential graph structure, or if the generative model cannot effectively capture the relational semantics, the pre-training may not yield improvements.

### Mechanism 2
- Claim: Including synonym information during pre-training enhances the model's understanding of different textual representations of the same biomedical entity.
- Mechanism: The model learns to associate multiple synonyms with a single entity identifier, allowing it to recognize the same concept expressed in varied ways during fine-tuning.
- Core assumption: Synonyms in biomedical text are sufficiently diverse and frequent that exposure during pre-training will generalize to unseen contexts.
- Evidence anchors:
  - [abstract] The paper states: "Our proposed method linearizes triples and considers them during the pre-training step... synonym information, which involves using alternative names or terminologies for the same biomedical entity, has been suggested as a means to enhance entity linking when used during pre-training."
  - [section] "As an output the model has to generate: [BOS] sa_e is sb_e [EOS] This lets the model learn the connection between the different synonyms of the same entity."
- Break condition: If synonyms are too domain-specific or sparse, or if the model overfits to the pre-training synonym pairs, the benefit may not transfer to fine-tuning.

### Mechanism 3
- Claim: The combination of synonym and triple information during pre-training provides complementary signals that jointly improve biomedical entity linking performance.
- Mechanism: Synonyms help with lexical variation, while triples help with semantic relations. Together, they may cover both surface and structural aspects of entity representation.
- Core assumption: Both synonym and relational information are necessary and sufficient to capture the full semantics needed for entity linking.
- Evidence anchors:
  - [abstract] "Our study aims to build upon this existing knowledge by integrating both strategies and assessing their impact on performance."
  - [section] "We introduce an additional pre-training step to incorporate more semantic information by utilising triple information from the underlying knowledge graph."
- Break condition: If one source of information dominates or conflicts with the other, or if neither is sufficiently informative, the combination may not improve performance.

## Foundational Learning

- Concept: Generative sequence-to-sequence models (e.g., BART)
  - Why needed here: The task is framed as generating entity identifiers from text, which fits the generative paradigm.
  - Quick check question: What is the input-output format during fine-tuning for this generative entity linking approach?

- Concept: Knowledge graph linearization
  - Why needed here: To convert structured triple data into a format suitable for pre-training a text-based model.
  - Quick check question: How does the "all-in-one" linearization strategy differ from "line-by-line" in terms of input structure?

- Concept: Fine-tuning for downstream tasks
  - Why needed here: Pre-training alone is insufficient; the model must be adapted to the specific entity linking datasets.
  - Quick check question: What template is used to format mentions during fine-tuning, and how is the output mapped back to entities?

## Architecture Onboarding

- Component map:
  - Pre-training corpus generator: Synthesizes text from UMLS triples and synonyms
  - BART-based generative model: Core sequence-to-sequence architecture
  - Fine-tuning pipeline: Adapts the model to BC5CDR and NCBI datasets
  - Entity mapping layer: Translates generated identifiers back to KG entities

- Critical path:
  1. Extract triples and synonyms from UMLS
  2. Linearize into text corpus
  3. Pre-train BART on this corpus
  4. Fine-tune on biomedical entity linking datasets
  5. Evaluate Recall@1

- Design tradeoffs:
  - Using a generative model vs. a discriminative encoder-decoder: Generative models can memorize the KG but may lack precision; discriminative models need negatives but can be more accurate
  - Line-by-line vs. all-in-one triple linearization: Line-by-line may preserve more structure; all-in-one may be more efficient but lose granularity
  - Pre-training on biomedical vs. general corpus: Biomedical pre-training may help domain specificity but reduce general language understanding

- Failure signatures:
  - No improvement over baseline BART: Pre-training may not be capturing useful knowledge
  - Overfitting to pre-training data: Model performs well on synthetic data but poorly on real datasets
  - Slow convergence during fine-tuning: Pre-training may not be providing useful inductive biases

- First 3 experiments:
  1. Train a baseline BART model on the fine-tuning datasets without any pre-training; measure Recall@1
  2. Train the model with only synonym-based pre-training; compare performance to baseline
  3. Train the model with only triple-based pre-training; compare performance to baseline and synonym-only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific graph neural network architectures or techniques could be used to effectively incorporate KG information beyond linearized triples for biomedical entity linking?
- Basis in paper: [explicit] The authors suggest exploring more sophisticated methods to instruct LLMs to learn external knowledge by exploiting the graph-structure with Graph Neural Networks (GNNs).
- Why unresolved: The paper does not provide specific GNN architectures or techniques that could be used to incorporate KG information more effectively than the current linearized triple approach.
- What evidence would resolve it: Empirical studies comparing the performance of various GNN architectures (e.g., Graph Attention Networks, Graph Convolutional Networks) on biomedical entity linking tasks, demonstrating improved performance over the current linearized triple approach.

### Open Question 2
- Question: How can the pre-training process be optimized to better capture the semantic connections between biomedical entities and their synonyms, descriptions, and relational information?
- Basis in paper: [explicit] The authors state that more sophisticated methods are needed to effectively inject knowledge into LLMs for downstream tasks like biomedical entity linking, and that the current approach of including synonym, description, or relational information does not provide a significant benefit.
- Why unresolved: The paper does not provide insights into alternative pre-training strategies or techniques that could better capture the semantic connections between biomedical entities and their associated information.
- What evidence would resolve it: Comparative studies evaluating the performance of different pre-training strategies (e.g., contrastive learning, multi-task learning) on biomedical entity linking tasks, demonstrating improved performance over the current approach.

### Open Question 3
- Question: How can the generative model be fine-tuned to better handle the large number of synonyms and the sparse distribution of biomedical entities in text?
- Basis in paper: [explicit] The authors mention that biomedical entities are scarcely distributed in texts and have been rarely seen during training by the LLM, which poses a challenge for entity linking.
- Why unresolved: The paper does not provide specific fine-tuning techniques or strategies to address the challenges posed by the large number of synonyms and the sparse distribution of biomedical entities in text.
- What evidence would resolve it: Empirical studies comparing the performance of different fine-tuning techniques (e.g., data augmentation, curriculum learning) on biomedical entity linking tasks, demonstrating improved performance over the current approach.

## Limitations

- Pre-training on KG-derived corpus provides only marginal improvements (0.2-0.5% Recall@1) over baseline BART model
- Including synonym, description, or relational information during pre-training does not provide significant performance benefits
- Evaluation limited to only two biomedical entity linking datasets (BC5CDR and NCBI) restricts generalizability

## Confidence

- Effectiveness of triple-aware pre-training: Low confidence
- Claim: Pre-training on KG-derived corpus significantly improves biomedical entity linking performance
- Evidence: Only marginal improvements (0.2-0.5% Recall@1) observed, not statistically significant
- Alternative KG encoding methods: Low confidence
- Claim: Linearized triples are the most effective way to inject KG knowledge into generative models
- Evidence: Paper suggests exploring GNNs and other methods, indicating current approach may be suboptimal
- Generality of results: Medium confidence
- Claim: Results generalize to other biomedical entity linking tasks and datasets
- Evidence: Only two datasets evaluated; results may be dataset-specific

## Next Checks

1. **Ablation study with different KG encoding methods**: Systematically compare the proposed linearization approaches against alternative KG encoding strategies (e.g., graph neural networks, entity embeddings) to determine whether the lack of improvement stems from the linearization method itself or from fundamental limitations in how generative models can capture structured knowledge.

2. **Evaluation on additional biomedical entity linking datasets**: Test the pre-trained models on a broader range of entity linking datasets beyond BC5CDR and NCBI, including those with different entity types and annotation schemas, to assess whether the observed marginal improvements are dataset-specific or represent a general trend.

3. **Direct comparison with state-of-the-art discriminative models**: Benchmark the generative approach against recent discriminative entity linking models (such as BLINK or GenEL) on the same datasets to determine whether the generative framework itself is a limitation, or if the issue lies specifically with the triple-aware pre-training strategy.