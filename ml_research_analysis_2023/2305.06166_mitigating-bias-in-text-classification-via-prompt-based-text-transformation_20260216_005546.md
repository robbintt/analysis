---
ver: rpa2
title: Mitigating Bias in Text Classification via Prompt-Based Text Transformation
arxiv_id: '2305.06166'
source_url: https://arxiv.org/abs/2305.06166
tags:
- data
- simpli
- cation
- classi
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether ChatGPT can be used to mitigate
  demographic bias in text classification by simplifying the language used in reviews.
  The authors hypothesized that standardizing language through simplification would
  reduce cues that correlate with protected characteristics (location in this case)
  while preserving the original meaning.
---

# Mitigating Bias in Text Classification via Prompt-Based Text Transformation

## Quick Facts
- arXiv ID: 2305.06166
- Source URL: https://arxiv.org/abs/2305.06166
- Reference count: 20
- Primary result: ChatGPT-based text simplification reduces demographic bias in classification while preserving semantic meaning

## Executive Summary
This paper investigates whether ChatGPT can mitigate demographic bias in text classification by simplifying language in reviews. The authors hypothesized that standardizing language through simplification would reduce cues correlating with protected characteristics (location) while preserving original meaning. Using 300 TripAdvisor reviews of Disneyland Hong Kong, they found that classification accuracy for predicting reviewer location dropped by 12-17% after simplification, while sentiment analysis confirmed meaning preservation. McNemar's test confirmed the drop was statistically significant for Random Forest models.

## Method Summary
The authors collected 300 TripAdvisor reviews of Disneyland Hong Kong from US and UK reviewers, then manually prompted ChatGPT to simplify each review using the prompt "Simplify (Review)". They performed sentiment analysis on both original and simplified reviews to verify meaning preservation, then trained Naïve Bayes and Random Forest classifiers on both datasets. Classification accuracy and statistical significance were compared to measure bias reduction effectiveness.

## Key Results
- Location classification accuracy dropped 12% for Naïve Bayes and 17% for Random Forest after simplification
- Sentiment analysis showed minimal change, confirming meaning preservation
- McNemar's test confirmed statistical significance for Random Forest model reduction
- Results suggest prompt-based rewriting offers practical, generalizable approach for bias mitigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simplification removes subgroup-specific lexical cues that classifiers use to infer protected attributes
- Mechanism: ChatGPT rewrites text to use more general language, reducing reliance on terms that strongly correlate with the reviewer's country of origin (e.g., "queues" for British, "like" for American reviewers)
- Core assumption: Protected attribute classification accuracy will drop when distinctive subgroup language is neutralized while preserving overall meaning
- Evidence anchors:
  - [abstract] "Classification accuracy for predicting reviewer location dropped significantly—by 12% for Naïve Bayes and 17% for Random Forest models"
  - [section] "British people talk about queues more, possibly due to social norms in British culture, where American people use the word like more, which is part of their dialect"
- Break condition: If the simplified text still retains enough subgroup-specific markers for accurate classification, or if meaning is altered in a way that affects classification differently than intended

### Mechanism 2
- Claim: Text simplification preserves semantic content while reducing bias-related vocabulary
- Mechanism: ChatGPT's simplification process removes non-essential information and rephrases content using more neutral terms, maintaining sentiment and core meaning
- Core assumption: Sentiment analysis will show minimal change before and after simplification, indicating preserved meaning
- Evidence anchors:
  - [abstract] "sentiment analysis confirmed that the core meaning remained intact"
  - [section] "The sentiment analysis results, seen in Figure 5, show that the sentiment had little change before and after the simplification step"
- Break condition: If sentiment analysis shows significant polarity shifts, indicating meaning alteration, or if classification performance drops due to meaning loss rather than bias reduction

### Mechanism 3
- Claim: Prompt-based text transformation can be applied across different domains and text types
- Mechanism: The approach generalizes because ChatGPT can process various text inputs and rewrite them using simplification, neutralisation, localisation, and formalisation techniques
- Core assumption: Similar performance improvements (reduced bias classification accuracy with preserved meaning) can be achieved in other domains like job applications or social media posts
- Evidence anchors:
  - [abstract] "These results suggest that prompt-based rewriting offers a practical and generalisable approach for mitigating bias in text classification"
  - [section] "Each reviewer is talking about the same place, in an arguably formal manner. We could expect there to be more differences in subgroups in a more informal setting"
- Break condition: If different domains require fundamentally different prompting strategies that don't generalize, or if meaning preservation fails in less structured text types

## Foundational Learning

- Concept: Protected characteristics and demographic bias in machine learning
  - Why needed here: The paper addresses bias mitigation by targeting language that correlates with protected attributes (location/country)
  - Quick check question: What are protected characteristics, and why can't machine learning models use them directly for classification?

- Concept: Text classification metrics (accuracy, F1-score, precision, recall)
  - Why needed here: The paper evaluates model performance before and after text transformation using these metrics
  - Quick check question: How do you interpret a 17% drop in classification accuracy in the context of bias mitigation?

- Concept: Sentiment analysis and polarity preservation
  - Why needed here: The paper uses sentiment analysis to verify that meaning is preserved during text simplification
  - Quick check question: Why is it important to verify that sentiment polarity doesn't change when simplifying text for bias mitigation?

## Architecture Onboarding

- Component map:
  Data collection -> Preprocessing -> Text simplification (ChatGPT) -> Sentiment analysis -> Classification model training -> Performance evaluation
- Key components: TripAdvisor review dataset, ChatGPT API, sentiment analysis libraries (TextBlob, VADER), classification models (Naïve Bayes, Random Forest), McNemar's test for statistical significance

- Critical path:
  1. Extract relevant reviews and reviewer locations from dataset
  2. Manually input 300 review prompts into ChatGPT for simplification
  3. Run sentiment analysis on original and simplified reviews
  4. Train classification models on both original and simplified data
  5. Compare classification accuracy and perform statistical significance testing

- Design tradeoffs:
  - Manual vs automated ChatGPT prompting (300 manual inputs vs API automation)
  - Dataset size (300 samples vs full 42,000 reviews)
  - Simplification quality vs processing time
  - Generalizability across domains vs domain-specific optimization

- Failure signatures:
  - Sentiment polarity changes significantly after simplification
  - Classification accuracy drops but not due to reduced bias (e.g., due to meaning loss)
  - Statistical tests show no significant difference between original and simplified classifications
  - ChatGPT produces inconsistent or low-quality simplifications

- First 3 experiments:
  1. Replicate the study with a larger sample size (e.g., 1000 reviews) to validate results
  2. Test different ChatGPT prompts (e.g., "Summarize the main points" vs "Simplify the language") to find optimal bias reduction
  3. Apply the methodology to a different domain (e.g., job applications) to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the simplification effectiveness scale with dataset size beyond 300 samples?
- Basis in paper: [explicit] The authors state "Due to the lack of an automated way to give ChatGPT some prompts, we were only able to use 300 data points for this task. This would need to be scaled up to validate the hypothesis and results."
- Why unresolved: Manual prompting of ChatGPT limits scalability, and the authors acknowledge the need to test with larger datasets to confirm if bias reduction trends persist.
- What evidence would resolve it: Automated ChatGPT prompting or API integration to test bias reduction effectiveness across datasets of varying sizes (e.g., 1,000, 5,000, 10,000+ samples) while measuring classification accuracy drops.

### Open Question 2
- Question: Do different ChatGPT simplification prompts yield varying levels of bias mitigation?
- Basis in paper: [explicit] The authors suggest "Another investigation would be to trial a number of different prompts into ChatGPT such as 'Summarise the main points of this review' or 'Did this person enjoy their time at Disney?'"
- Why unresolved: Only one simplification prompt ("Simplify '(Review)'") was tested; alternative prompts could produce different text transformations and bias reduction outcomes.
- What evidence would resolve it: Systematic comparison of multiple ChatGPT prompts on the same dataset, measuring classification accuracy drops and semantic preservation across prompts.

### Open Question 3
- Question: Can prompt-based text simplification effectively reduce bias in more informal, dialect-rich text domains like social media?
- Basis in paper: [explicit] The authors note "In terms of the applications of this technique, any bias in this dataset of reviews would only have minor implications... We could expect there to be more differences in subgroups in a more informal setting, such as on Twitter where users often type out their own inflections."
- Why unresolved: The study used formal TripAdvisor reviews; informal text with more pronounced dialect differences (e.g., Twitter, Instagram) may show different simplification effectiveness.
- What evidence would resolve it: Applying the same simplification approach to informal text datasets (e.g., tweets, social media posts) and measuring classification accuracy changes for demographic attributes.

## Limitations

- Small sample size (n=300) may not be representative of broader review landscape
- Manual ChatGPT prompting introduces potential human bias in prompt formulation
- Only examines one domain (Disneyland Hong Kong reviews) and two protected characteristics (US/UK location)
- Relies on single large language model without exploring alternatives

## Confidence

- High Confidence: The core finding that text simplification reduces location-based classification accuracy is well-supported by statistical evidence (12-17% accuracy drop with McNemar's test significance for Random Forest)
- Medium Confidence: The generalizability of the approach across different domains and text types is suggested but not empirically validated beyond the Disneyland review context
- Low Confidence: The optimal prompting strategy for ChatGPT remains unclear beyond the single "Simplify (Review)" prompt used

## Next Checks

1. Replicate the study with a larger sample size (1000+ reviews) and multiple domains (e.g., restaurant reviews, product reviews) to test generalizability and establish effect size stability

2. Conduct ablation studies comparing different ChatGPT prompts (e.g., "Neutralize language," "Simplify while preserving meaning") to identify the most effective approach for bias reduction without semantic loss

3. Test the approach on different language models (e.g., GPT-4, Claude, LLaMA) to determine if the bias mitigation effect is model-dependent or a generalizable property of prompt-based text transformation