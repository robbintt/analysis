---
ver: rpa2
title: 'Tensor Networks Meet Neural Networks: A Survey and Future Perspectives'
arxiv_id: '2302.09019'
source_url: https://arxiv.org/abs/2302.09019
tags:
- tensor
- quantum
- decomposition
- networks
- tnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey introduces tensorial neural networks (TNNs), which
  combine tensor networks (TNs) and neural networks (NNs) through their shared multilinearity
  structure. TNNs offer three key advantages: (1) Network Compression - TNs compress
  NNs by decomposing weight tensors into smaller blocks, significantly reducing parameters
  while maintaining performance; (2) Information Fusion - TNs enhance NNs with their
  ability to model higher-order interactions among multimodal data sources; and (3)
  Quantum Circuit Simulation - TNs act as simulators to bridge classic NNs and quantum
  neural networks (QNNs), enabling exploration of quantum computing applications.'
---

# Tensor Networks Meet Neural Networks: A Survey and Future Perspectives

## Quick Facts
- arXiv ID: 2302.09019
- Source URL: https://arxiv.org/abs/2302.09019
- Reference count: 40
- One-line primary result: This survey introduces tensorial neural networks (TNNs) that combine tensor networks and neural networks, offering three key advantages: network compression, information fusion, and quantum circuit simulation.

## Executive Summary
This survey explores the emerging field of tensorial neural networks (TNNs), which leverage the shared multilinear mathematical structure of tensor networks and neural networks to create hybrid architectures with unique advantages. The paper systematically categorizes TNNs into five main areas: compact architectures for various neural network types, information fusion methods, quantum state embedding, training strategies, and toolboxes. TNNs offer three primary benefits: compressing neural networks by factorizing weight tensors into smaller blocks, enhancing information fusion through higher-order tensor operations for multimodal data, and bridging classical neural networks with quantum neural networks through tensor network simulation.

## Method Summary
The survey provides a comprehensive overview of TNNs through systematic categorization of existing approaches. It covers tensor decomposition formats (CP, Tucker, TT, TR, HT, PEPS) and their application to neural network architectures including CNNs, RNNs, Transformers, GNNs, and RBMs. For information fusion, it discusses tensor fusion layers and multimodal pooling techniques. Quantum state embedding methods are presented for classic data processing, quantum data handling, and ConvAC networks. Training strategies address stable training, rank selection, and hardware acceleration. The survey also provides an overview of toolboxes for tensor operations, network compression, and quantum circuit simulation.

## Key Results
- TNs compress NNs by decomposing weight tensors into smaller blocks, significantly reducing parameters while maintaining performance
- TNs enhance NNs by modeling higher-order interactions among multimodal data sources through tensor fusion operations
- TNs act as simulators to bridge classic NNs and quantum neural networks, enabling exploration of quantum computing applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor networks compress neural networks by decomposing weight tensors into smaller blocks, reducing parameters while maintaining performance
- Mechanism: TNs exploit the multilinear mathematical structure shared by both TNs and NNs. By factorizing high-dimensional weight tensors into low-rank tensor components (e.g., CP, Tucker, TT formats), the exponential growth in parameters is converted to polynomial complexity
- Core assumption: The decomposed low-rank TN representation sufficiently approximates the original weight tensor without significant loss in accuracy
- Evidence anchors: [abstract] "TNs compress NNs by decomposing weight tensors into smaller blocks, significantly reducing parameters while maintaining performance"
- Break condition: If the rank selection is too low or inappropriate, the approximation quality degrades, leading to significant performance loss

### Mechanism 2
- Claim: TNs enhance NNs by modeling higher-order interactions among multimodal data sources
- Mechanism: TNs naturally handle multiway arrays and can capture complex interactions across different modalities through tensor fusion operations
- Core assumption: The multimodal data can be effectively represented and processed as higher-order tensors, and tensor operations preserve meaningful relationships across modalities
- Evidence anchors: [abstract] "TNs enhance NNs with their ability to model the interactions among multimodal data sources"
- Break condition: If the tensor order or rank is insufficient to capture the complexity of interactions, the fusion becomes ineffective

### Mechanism 3
- Claim: TNs act as simulators to bridge classic NNs and quantum neural networks
- Mechanism: Quantum states can be represented as high-order tensors, and quantum circuits can be modeled as tensor networks
- Core assumption: The tensor representation accurately captures the quantum mechanical properties and that the classical simulation via TNs preserves the essential computational advantages of quantum circuits
- Evidence anchors: [abstract] "TNs act as simulators to bridge classic NNs and quantum neural networks (QNNs), enabling exploration of quantum computing applications"
- Break condition: If the tensor simulation cannot scale to large quantum systems or fails to capture entanglement effects accurately, the bridge to QNNs becomes impractical

## Foundational Learning

- Concept: Tensor decomposition formats (CP, Tucker, TT, TR, HT, PEPS)
  - Why needed here: Understanding these formats is essential because TNNs rely on them to compress and restructure neural network weights and data
  - Quick check question: What is the main difference between CP and Tucker decomposition in terms of core tensor structure?

- Concept: Tensor contraction and multilinear operations
  - Why needed here: TNNs use tensor contraction to combine tensor components; knowing how contraction works is key to understanding information flow in TNNs
  - Quick check question: How does tensor contraction reduce the number of tensor modes?

- Concept: Quantum state embedding and tensor network simulation
  - Why needed here: This concept underlies the use of TNs to simulate quantum circuits and process quantum data in TNNs
  - Quick check question: How is a classical image mapped to a quantum state using tensor networks?

## Architecture Onboarding

- Component map: Input data → Tensor decomposition/compression → Forward pass through TNN layers → Loss computation → Backward pass with mixed precision → Parameter update with rank-aware optimization
- Critical path: Data → Tensor decomposition/compression → Forward pass through TNN layers → Loss computation → Backward pass with mixed precision → Parameter update with rank-aware optimization
- Design tradeoffs: Lower tensor ranks yield higher compression but risk underfitting; higher ranks improve expressiveness but increase computation
- Failure signatures: Training instability due to numerical overflow in tensor contractions; poor performance from inappropriate rank selection; slow convergence from suboptimal contraction order
- First 3 experiments:
  1. Replace a dense layer in a small CNN with a TT-decomposed layer and compare parameter count and accuracy
  2. Implement a tensor fusion layer for a bimodal dataset (e.g., image + text) and evaluate fusion quality versus concatenation
  3. Simulate a simple quantum circuit using tensor networks and compare results to analytical quantum simulation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more efficient algorithms for rank selection in tensor networks used in neural networks?
- Basis in paper: [explicit] The paper explicitly states that rank selection is an NP-hard problem and discusses several heuristic approaches
- Why unresolved: Current methods rely on manual setting or computationally expensive searches, and no optimal solution exists for this NP-hard problem
- What evidence would resolve it: Development of a method that consistently finds near-optimal ranks with significantly lower computational cost than existing approaches

### Open Question 2
- Question: What hardware architectures would be most effective for accelerating tensor network operations in neural networks?
- Basis in paper: [explicit] The paper discusses current hardware acceleration efforts being "ad hoc designs for specific TD formats"
- Why unresolved: Existing approaches are format-specific and don't provide general acceleration for tensor operations across different TN formats
- What evidence would resolve it: A hardware design or software framework that demonstrates significant speedups across multiple TN formats in various neural network applications

### Open Question 3
- Question: How can we effectively implement and verify the performance of quantum neural networks using tensor networks as simulators?
- Basis in paper: [explicit] The paper identifies this as a "symbolic milestone" but notes that "strict mapping algorithms between simulated quantum TNNs and realistic physical systems still need to be explored"
- Why unresolved: Current quantum computers are not powerful enough for direct implementation, and the theoretical mappings between simulated and real quantum systems remain unclear
- What evidence would resolve it: Successful implementation of a TNN-based quantum simulation that produces verifiable results matching theoretical predictions

## Limitations
- Quantitative evidence for compression ratios and performance trade-offs across different tensor formats is limited
- Experimental results demonstrating multimodal fusion effectiveness compared to state-of-the-art methods are lacking
- Benchmarking of quantum circuit simulation accuracy against true quantum hardware or established simulators is not provided

## Confidence

- **High**: The mathematical foundations linking tensor networks and neural networks through multilinear structure are well-established
- **Medium**: The three claimed advantages (compression, fusion, quantum simulation) are theoretically sound but lack comprehensive experimental validation
- **Low**: Specific implementation details for quantum state embedding and processing are not fully specified

## Next Checks

1. Implement TT-format compression on a standard CNN (e.g., ResNet-18) and measure parameter reduction vs. accuracy drop across multiple datasets
2. Compare tensor fusion layer performance against concatenation baselines on a multimodal dataset (e.g., VQA) using identical feature extractors
3. Simulate a parameterized quantum circuit using tensor networks and verify results against Qiskit's statevector simulator for simple quantum algorithms