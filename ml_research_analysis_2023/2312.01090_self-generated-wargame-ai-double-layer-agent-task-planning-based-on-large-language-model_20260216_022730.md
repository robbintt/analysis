---
ver: rpa2
title: 'Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large
  Language Model'
arxiv_id: '2312.01090'
source_url: https://arxiv.org/abs/2312.01090
tags:
- agent
- language
- decision-making
- wargame
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies large language models to intelligent decision-making
  in wargame simulation, proposing a double-layer agent task planning framework centered
  on a large language model. The approach enables strategic agents to plan tasks using
  overall situational information and tactical agents to execute tasks based on individual
  observations, with natural language interaction between agents.
---

# Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large Language Model

## Quick Facts
- arXiv ID: 2312.01090
- Source URL: https://arxiv.org/abs/2312.01090
- Reference count: 8
- Key outcome: GW AE algorithm achieved 298±11 kills, 10504±64 survivals, and 4238±28 control points, outperforming reinforcement learning baselines.

## Executive Summary
This paper proposes a double-layer agent task planning framework for wargame simulation that leverages large language models. The approach centers on strategic agents using GPT-4 to generate macro-level task plans based on overall battlefield information, while tactical agents use GPT-3.5 to execute and potentially modify these tasks based on local observations. The framework enables natural language interaction between agents and incorporates memory and reflection streams to improve decision quality. Experimental results demonstrate superior performance compared to reinforcement learning and rule-based AI systems across multiple metrics.

## Method Summary
The framework implements a two-layer architecture where strategic agents (using GPT-4) synthesize all observations and expert knowledge to generate coordinated task assignments, and tactical agents (using GPT-3.5) execute and adjust these tasks based on local conditions. The system employs memory stream retrieval with recency, importance, and relevance scoring to provide context-rich prompts to the LLMs. A reflection stream periodically abstracts higher-level reasoning from observations and plans to enhance strategic foresight. The architecture relies on structured prompt engineering to translate natural language plans into simulation actions and enable iterative refinement between strategic and tactical levels.

## Key Results
- GW AE algorithm achieved 298±11 kills versus reinforcement learning baselines ranging from 745±9 to 1285±7 kills
- GW AE achieved 10504±64 survivals compared to RL baselines of 4526±15 to 6307±9
- GW AE achieved 4238±28 control points versus RL baselines of 0 to 357
- Expert knowledge input to strategic agents further improved performance over the base GW A algorithm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strategic agents in the double-layer framework generate macro-level task planning by synthesizing all agents' observations and expert knowledge via GPT-4.
- Mechanism: Strategic agents observe all battlefield states, encode them into natural language prompts (Summary-Observations-Planning format), and feed these prompts to GPT-4 to produce coordinated task assignments. Tactical agents then execute and optionally modify these tasks based on local observations using GPT-3.5.
- Core assumption: GPT-4 can reason about multi-agent coordination and generate coherent, actionable plans when given structured situational summaries and expert knowledge.
- Evidence anchors:
  - [abstract] The framework uses a large language model in the decision-making center to issue and execute decision commands through natural language interaction.
  - [section 3.2] Strategic agent combines current situation and expert knowledge into prompts to generate macro-level tactics planning; tactical agents modify tasks based on local states.
  - [corpus] Weak. WGSR-Bench and Open-Ended Wargames with LLMs show LLMs can reason in wargame contexts, but not the double-layer architecture specifically.

### Mechanism 2
- Claim: Memory stream retrieval with recency, importance, and relevance scoring improves decision quality by providing the LLM with context-rich prompts.
- Mechanism: Observations are stored in a memory list with timestamps. A retrieval function scores memories by recency, importance (rated 1-10), and relevance to current situation, then normalizes and combines scores to extract the most useful fragments for prompt construction.
- Core assumption: Higher-scoring memory fragments (recent, important, relevant) are more likely to inform better strategic and tactical decisions.
- Evidence anchors:
  - [section 3.2] The architecture uses a memory stream that stores observations and extracts relevant fragments via a retrieval function based on recency, importance, and relevance.
  - [section 3.2] Importance is rated by the LLM (1-10) and relevance is similarly scored; final scores combine weighted factors.
  - [corpus] Weak. No corpus evidence directly supports the memory retrieval scoring mechanism.

### Mechanism 3
- Claim: Reflection stream abstracts raw observations into higher-level reasoning semantics, enabling the LLM to generate more advanced decisions.
- Mechanism: When a memory's score exceeds a threshold, reflection is triggered. The agent combines observed semantics with planned semantics into reflective statements (e.g., threat assessment), producing abstract reasoning that augments the memory stream.
- Core assumption: Higher-level reflection enables the LLM to reason beyond immediate observations, improving strategic foresight.
- Evidence anchors:
  - [section 3.2] Reflection stream is described as higher-level reasoning semantics generated periodically, combining observed and planned semantics.
  - [section 3.2] Example: "The blue agent poses a significant threat..." is generated from observation and planning.
  - [corpus] Weak. No corpus evidence for reflection stream in wargame or multi-agent LLM systems.

## Foundational Learning

- Concept: Natural language to action translation in simulation environments
  - Why needed here: The framework relies on LLMs generating textual plans that must be converted into discrete actions (e.g., move to coordinate, attack target).
  - Quick check question: How does the system ensure that LLM-generated natural language commands are mapped correctly to valid simulation actions?

- Concept: Prompt engineering for multi-turn agent interaction
  - Why needed here: The double-layer design requires strategic and tactical agents to iteratively refine tasks via natural language, so prompts must be structured to support bidirectional feedback.
  - Quick check question: What prompt format enables tactical agents to provide actionable modification suggestions to strategic agents?

- Concept: Memory retrieval scoring and normalization
  - Why needed here: Efficient context selection is critical for LLM performance; this mechanism balances recency, importance, and relevance to avoid information overload.
  - Quick check question: How do the α weights for recency, importance, and relevance affect which memories are selected?

## Architecture Onboarding

- Component map:
  - Strategic Agent (GPT-4) -> generates macro plans from Summary-Observations-Planning prompts
  - Tactical Agent (GPT-3.5) -> executes/adjusts tasks from local observations
  - Memory Stream -> stores and retrieves observations with recency/importance/relevance scoring
  - Reflection Stream -> periodically abstracts higher-level reasoning from memory
  - Action Translator -> converts natural language commands into discrete simulation actions

- Critical path:
  1. Strategic agent observes all states -> encodes into prompt
  2. GPT-4 generates task plan -> sends to tactical agents
  3. Each tactical agent observes local state -> modifies task if needed
  4. Strategic agent integrates feedback -> re-plans if necessary
  5. Memory and reflection streams update for next turn

- Design tradeoffs:
  - Using GPT-4 for strategic agents ensures higher reasoning quality but increases compute cost.
  - Using GPT-3.5 for tactical agents reduces cost but may yield less nuanced adjustments.
  - The memory scoring system must balance between ignoring useful older memories and overfitting to recent noise.

- Failure signatures:
  - Plans become incoherent or repetitive -> memory retrieval or prompt quality issue.
  - Tactical agents ignore strategic tasks -> communication/alignment problem in natural language exchange.
  - System slows down -> memory stream or reflection generation overhead.

- First 3 experiments:
  1. Single-agent planning test: Give strategic agent a static scenario, verify it generates a coherent task plan without tactical feedback.
  2. Memory retrieval ablation: Disable memory stream, observe performance drop in strategic decision quality.
  3. Reflection threshold tuning: Vary the score threshold for triggering reflection, measure effect on plan coherence and runtime.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance differences between GW AE and GW A algorithms scale with increasing game complexity or map size?
- Basis in paper: [explicit] The paper compares GW AE (with expert knowledge input) and GW A algorithms, showing GW AE performs better in various metrics.
- Why unresolved: The experiments were conducted on a specific map with fixed parameters. The paper does not explore how algorithm performance changes with different map complexities or sizes.
- What evidence would resolve it: Comparative experiments testing both algorithms across maps of varying sizes and complexities, with performance metrics recorded for each scenario.

### Open Question 2
- Question: What specific aspects of the expert knowledge documents most significantly improve the GW AE algorithm's performance compared to GW A?
- Basis in paper: [explicit] The paper mentions that expert knowledge documents are input into GW AE, leading to improved performance over GW A.
- Why unresolved: The paper does not analyze which specific elements or types of expert knowledge contribute most to the performance improvement.
- What evidence would resolve it: Ablation studies where different components of the expert knowledge documents are selectively removed or modified to measure their individual impact on algorithm performance.

### Open Question 3
- Question: How does the double-layer agent task planning framework handle situations where tactical agents' local observations significantly contradict strategic agent's global assessment?
- Basis in paper: [inferred] The paper describes a framework where tactical agents can provide feedback to strategic agents, but does not detail how conflicts between local and global assessments are resolved.
- Why unresolved: The paper presents the architecture but does not provide concrete examples or mechanisms for resolving conflicts between tactical and strategic levels of decision-making.
- What evidence would resolve it: Case studies or simulations where conflicts arise between tactical and strategic assessments, demonstrating how the framework resolves such discrepancies and the impact on overall performance.

## Limitations
- The approach relies heavily on prompt engineering without fully specifying optimal prompt structures or validation methods.
- Memory retrieval mechanism lacks empirical justification for scoring weights and threshold parameters.
- The framework's generalizability to more complex wargame scenarios or different game mechanics is not demonstrated.

## Confidence

- **High Confidence**: The double-layer agent architecture design and its basic functionality are well-supported by the experimental results, showing clear performance improvements over RL baselines.
- **Medium Confidence**: The memory stream and reflection stream mechanisms are theoretically justified but lack rigorous ablation studies to confirm their individual contributions to performance gains.
- **Low Confidence**: The generalizability of the approach to other wargame scenarios or more complex strategic environments is not demonstrated.

## Next Checks
1. **Prompt Engineering Validation**: Systematically vary prompt structures and test their impact on strategic agent decision quality across multiple wargame scenarios to identify optimal prompt formats.
2. **Memory Retrieval Ablation**: Compare GW AE performance with and without memory stream retrieval, and with different scoring weight configurations (αrecency, αimportance, αrelevance) to quantify the memory mechanism's contribution.
3. **Cross-Scenario Generalization**: Test the GW AE algorithm on wargame scenarios with different numbers of agents, varying fog-of-war conditions, and alternative win conditions to assess scalability and adaptability.