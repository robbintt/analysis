---
ver: rpa2
title: 'OceanBench: The Sea Surface Height Edition'
arxiv_id: '2309.15599'
source_url: https://arxiv.org/abs/2309.15599
tags:
- data
- ocean
- which
- osse
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OceanBench is a framework that provides standardized processing
  steps for ocean satellite data, lowering the barrier to entry for machine learning
  researchers. It offers plug-and-play data and pre-configured pipelines for benchmarking
  models and a flexible framework for customization.
---

# OceanBench: The Sea Surface Height Edition

## Quick Facts
- arXiv ID: 2309.15599
- Source URL: https://arxiv.org/abs/2309.15599
- Reference count: 40
- Key outcome: Framework providing standardized processing steps for ocean satellite data to lower ML researcher entry barriers

## Executive Summary
OceanBench is a framework that provides standardized processing steps for ocean satellite data, lowering the barrier to entry for machine learning researchers. It offers plug-and-play data and pre-configured pipelines for benchmarking models and a flexible framework for customization. The first edition focuses on sea surface height (SSH) interpolation challenges, providing datasets and ML-ready pipelines for interpolating observations from simulated ocean satellite data, multi-modal and multi-sensor fusion issues, and transfer-learning to real ocean satellite observations. OceanBench aims to facilitate the uptake of ML schemes to address ocean observation challenges and bring new challenges to the ML community.

## Method Summary
OceanBench provides a framework for SSH interpolation using ocean satellite data through standardized processing steps, plug-and-play data, and pre-configured pipelines. The method uses coordinate-based and grid-based approaches with customizable YAML configurations for preprocessing, patch generation, model training, and evaluation. The framework integrates domain-specific metrics like normalized RMSE and isotropic PSD scores for assessing SSH reconstruction quality, with examples provided for baseline models.

## Key Results
- OceanBench successfully provides standardized processing pipelines for ocean satellite data, reducing entry barriers for ML researchers
- The framework enables consistent evaluation of SSH interpolation models using domain-specific metrics
- OSSE and OSE experimental setups allow testing of transfer learning from simulations to real observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OceanBench reduces entry barriers for ML researchers by providing standardized, domain-compliant preprocessing pipelines.
- Mechanism: The framework abstracts away complex geospatial preprocessing (coordinate transformations, regridding, validation) into configurable Hydra pipelines, allowing users to focus on model development rather than data wrangling.
- Core assumption: Domain-specific preprocessing steps are the primary barrier preventing ML researchers from engaging with ocean satellite data.
- Evidence anchors:
  - [abstract] "The processing steps from the raw observation data to a ML-ready state... require domain expertise, which can be a significant barrier to entry for ML researchers."
  - [section 3.1] "OceanBench aims to lower the barrier to entry cost for ML researchers to make meaningful progress in the field of state prediction."
  - [corpus] "Scale-aware neural calibration for wide swath altimetry observations" demonstrates that ML methods can work on altimetry data when preprocessing is handled properly.
- Break condition: If domain experts disagree on standardization, the "standardized" aspect could fragment, forcing users to re-implement pipelines.

### Mechanism 2
- Claim: OceanBench facilitates reproducibility and fair comparison by embedding evaluation metrics directly into the data pipeline.
- Mechanism: By integrating domain-specific metrics (e.g., isotropic power spectrum scores, resolved spatial scales) into the evaluation pipeline, OceanBench ensures that all models are assessed using consistent, physically meaningful criteria rather than generic ML losses.
- Core assumption: Reproducibility and consistent evaluation are critical for scientific adoption of ML methods in geoscience.
- Evidence anchors:
  - [section 3.2] "Regarding the evaluation framework, we include domain-relevant performance metrics beyond the standard ML loss and accuracy functions."
  - [section 4.5] "OceanBench also generated figure 4 which shows plots of the PSD and PSD scores of SSH for the different challenges."
  - [corpus] "Simulation-informed deep learning for enhanced SWOT observations of fine-scale ocean dynamics" shows that proper evaluation metrics are essential for assessing ML performance on oceanographic data.
- Break condition: If the embedded metrics do not capture the scientific goals of users, they may bypass OceanBench evaluation entirely.

### Mechanism 3
- Claim: OceanBench supports transfer learning from simulations to real observations, enabling practical ML deployment.
- Mechanism: By providing OSSE datasets with known ground truth alongside real OSE datasets, OceanBench allows users to pre-train models on simulations and fine-tune on real data, mimicking operational workflows.
- Core assumption: Simulated data can provide useful inductive biases for real-world ocean observation tasks.
- Evidence anchors:
  - [section 4.3] "We outline these two experimental setups... Observing System Simulation Experiments (OSSE)... and of real-world experiments, referred to as observing system experiments (OSE)."
  - [section 4.4] "Experiment IV ( OSE NADIR) addresses SSH interpolation for real NADIR altimetry data... one may also explore transfer learning or fine-tuning strategies from the available OSSE dataset."
  - [corpus] "Simulation-informed deep learning for enhanced SWOT observations of fine-scale ocean dynamics" directly addresses the simulation-to-real transfer challenge.
- Break condition: If the simulation-to-real gap is too large, transfer learning may fail, making OSSE experiments misleading.

## Foundational Learning

- Concept: Spatiotemporal data structures (xarray, coordinates, grids)
  - Why needed here: OceanBench works exclusively with xarray datasets and requires understanding of coordinate-aware operations for geospatial data.
  - Quick check question: What is the difference between a "DataArray" and a "Dataset" in xarray, and when would you use each for ocean satellite data?

- Concept: Fourier and spectral analysis for ocean dynamics
  - Why needed here: Power spectral density (PSD) scores are core evaluation metrics in OceanBench for assessing scale-resolved reconstruction quality.
  - Quick check question: How does the isotropic PSD differ from the along-track PSD in terms of what oceanographic features they capture?

- Concept: Geostrophic dynamics and SSH-derived quantities
  - Why needed here: OceanBench automatically derives physical variables (kinetic energy, vorticity, strain) from SSH, which are used for both evaluation and potential model inputs.
  - Quick check question: What is the physical interpretation of relative vorticity derived from SSH, and why is it important for assessing ocean dynamics reconstruction?

## Architecture Onboarding

- Component map: Data Registry (DVC-controlled) → Preprocessing Pipeline (Hydra + xarray) → XRPatcher (patch generation) → Model Training → Evaluation Pipeline (domain metrics)
- Critical path: 1. Load raw satellite data from registry 2. Apply preprocessing transformations (coordinate validation, regridding, subsetting) 3. Generate patches using XRPatcher for model input 4. Train model using custom PyTorch Dataset 5. Evaluate using domain-specific metrics (PSD, nRMSE, etc.)
- Design tradeoffs: Flexibility vs. complexity: High configurability requires learning Hydra and YAML syntax. Simulation vs. real data: OSSE experiments are easier to control but may not generalize to real observations. Global vs. local metrics: PSD scores capture global spectral properties but may miss local features.
- Failure signatures: Missing or malformed coordinates in input data → preprocessing pipeline fails. Incompatible patch sizes between training and evaluation → reconstruction errors. Domain metrics returning NaN or infinite values → likely issues with data normalization or coordinate transformations.
- First 3 experiments: 1. Run the basic interpolation task using OSSE NADIR data with default parameters to verify pipeline functionality. 2. Modify the spatial domain in the YAML config to test model performance in a different region. 3. Swap the baseline model (e.g., from 4DVarNet to MIOST) to compare performance using the same evaluation pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can OceanBench effectively handle the challenge of data serving, especially given the large size of ocean simulation datasets (terabytes to petabytes) and the need for accessibility across different institutions?
- Basis in paper: [explicit] The paper mentions that the original simulations are terabytes/petabytes of data, making them infeasible for most users, and suggests the need for closer collaboration with platforms like the Marine Data Store or Climate Data Store.
- Why unresolved: The current framework provides limited datasets and does not address the scalability of data serving for large-scale ocean simulations, which is crucial for broader adoption.
- What evidence would resolve it: Demonstrating successful integration with major data platforms or providing a scalable data distribution system that allows easy access to large datasets for users with varying computational resources.

### Open Question 2
- Question: What metrics can be developed to better assess the goodness of SSH reconstruction models, considering the limitations of current metrics in capturing important optimization criteria in scientific machine learning tasks?
- Basis in paper: [explicit] The paper acknowledges the lack of consensus within domain-specific communities about the perfect metric and suggests the need for a variety of scores from different perspectives.
- Why unresolved: Current metrics, such as RMSE and spectral scores, may not fully capture the nuances of SSH reconstruction quality, and there is a need for more specialized metrics that handle local scales and other aspects of ocean dynamics.
- What evidence would resolve it: Developing and validating new metrics that provide a more comprehensive assessment of SSH reconstruction quality, including those that handle local scales, uncertainty quantification, and physical consistency.

### Open Question 3
- Question: How can OceanBench be extended to support uncertainty quantification in SSH interpolation tasks, given the inherent noise in real data and the need for state/parameter estimation under uncertain conditions?
- Basis in paper: [explicit] The paper mentions the lack of uncertainty quantification in the SSH interpolation experiments and acknowledges the difficulty in quantifying uncertainty.
- Why unresolved: Uncertainty quantification is crucial for realistic SSH interpolation tasks, but the framework currently does not address this aspect, which is important for operational settings and downstream applications.
- What evidence would resolve it: Implementing methods for uncertainty quantification, such as ensemble predictions or Bayesian approaches, and evaluating their effectiveness in improving the robustness and reliability of SSH interpolation models.

## Limitations

- The effectiveness of transfer learning from OSSE to real data depends on the realism of simulations, which may not capture all real-world complexities
- Current evaluation metrics may not fully capture all aspects of SSH reconstruction quality, particularly local features and uncertainty
- Data serving and accessibility remain challenging due to the large size of ocean simulation datasets

## Confidence

- Claim: OceanBench significantly lowers barriers for ML researchers through standardized preprocessing - Medium
- Claim: Embedded domain metrics ensure reproducibility and fair comparison - Medium
- Claim: Transfer learning from OSSE to real data is effective - Low

## Next Checks

1. Conduct a user study with ML researchers unfamiliar with ocean data to measure time-to-first-result with and without OceanBench
2. Compare OceanBench's evaluation metrics against domain expert benchmarks on real satellite data to validate their physical meaningfulness
3. Test transfer learning performance across multiple simulation-to-real scenarios to quantify the simulation-real gap