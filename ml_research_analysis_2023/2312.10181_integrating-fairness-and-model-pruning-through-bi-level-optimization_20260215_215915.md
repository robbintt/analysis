---
ver: rpa2
title: Integrating Fairness and Model Pruning Through Bi-level Optimization
arxiv_id: '2312.10181'
source_url: https://arxiv.org/abs/2312.10181
tags:
- pruning
- fairness
- fair
- neural
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of algorithmic bias in neural network
  pruning, where traditional pruning methods can unintentionally exacerbate societal
  bias, leading to unequal prediction outcomes. The authors propose a novel concept
  of fair model pruning, which involves developing a sparse model that adheres to
  fairness criteria.
---

# Integrating Fairness and Model Pruning Through Bi-level Optimization

## Quick Facts
- arXiv ID: 2312.10181
- Source URL: https://arxiv.org/abs/2312.10181
- Reference count: 24
- One-line primary result: Proposes Bi-level Fair Pruning (BiFP) that integrates fairness constraints into neural network pruning via bi-level optimization, achieving better fairness preservation across sparsity levels while maintaining comparable or better accuracy.

## Executive Summary
This paper addresses the critical issue of algorithmic bias in neural network pruning, where traditional pruning methods can exacerbate societal bias leading to unequal prediction outcomes. The authors introduce a novel framework called Bi-level Fair Pruning (BiFP) that jointly optimizes both the pruning mask and weight updates with fairness constraints using bi-level optimization. This approach ensures that fairness is preserved throughout the pruning process rather than being an afterthought, which is crucial for applications where both model efficiency and fairness are paramount requirements.

## Method Summary
The paper proposes a bi-level optimization framework where the outer loop optimizes the pruning mask to minimize loss under fairness constraints, while the inner loop updates model weights to optimize accuracy while maintaining fairness. The framework uses convex relaxation of fairness constraints to enable gradient-based optimization. The method is evaluated on CelebA and LFW datasets using Gender as a sensitive attribute, comparing BiFP against mainstream pruning strategies across various sparsity levels.

## Key Results
- BiFP consistently ensures fairness at any sparsity level while maintaining comparable or better accuracy than mainstream pruning strategies
- The method effectively preserves fairness throughout the pruning process, unlike sequential approaches that can amplify bias
- BiFP demonstrates superior performance in maintaining model fairness, performance, and efficiency compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair pruning can be achieved by jointly optimizing masks and weights under fairness constraints via bi-level optimization
- Mechanism: The outer loop optimizes the pruning mask to minimize loss under a fairness constraint, while the inner loop updates model weights to optimize accuracy while maintaining fairness. This creates a feedback loop ensuring both sparsity and fairness are achieved simultaneously
- Core assumption: Fairness constraints can be relaxed to convex, differentiable functions that allow gradient-based optimization
- Evidence anchors:
  - [abstract] "propose a framework to jointly optimize the pruning mask and weight update processes with fairness constraints"
  - [section] "we formulate the fair pruning problem as a novel constrained bi-level optimization task"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If fairness constraints cannot be made differentiable or convex, the bi-level optimization framework becomes intractable

### Mechanism 2
- Claim: Incorporating fairness constraints during both mask and weight updates prevents bias amplification that occurs in sequential pruning-then-fairness approaches
- Mechanism: Traditional methods first prune then apply fairness corrections, allowing bias to manifest in the pruned structure. By enforcing fairness at both stages, the model structure itself is shaped to avoid discriminatory patterns
- Core assumption: Bias amplification in sequential methods is significant enough to justify the complexity of joint optimization
- Evidence anchors:
  - [section] "pruning inevitably introduces bias while intuitive methods cannot address this challenge"
  - [section] "Our proposed approach effectively tackles the challenge and endeavors to preserve fairness throughout bi-level optimization processes"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If bias amplification in sequential methods is negligible, the additional complexity of joint optimization may not be warranted

### Mechanism 3
- Claim: The convex relaxation of fairness constraints enables efficient gradient-based optimization while maintaining the integrity of fairness objectives
- Mechanism: The paper replaces indicator functions with differentiable surrogate functions and reformulates the fairness metric to be convex, allowing standard gradient descent to be applied effectively
- Core assumption: The surrogate functions adequately approximate the original fairness metrics without losing critical properties
- Evidence anchors:
  - [section] "To ensure smooth and differentiable, we reformulate Eq. 6 as follows" and subsequent derivation
  - [section] "Eq. 8 is convex and differentiable if the surrogate function u is convex and differentiable at zero"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the surrogate functions poorly approximate the original fairness metrics, the optimization may converge to unfair solutions

## Foundational Learning

- Concept: Bi-level optimization
  - Why needed here: The pruning mask and model weights must be optimized simultaneously but with different objectives - the mask needs to be sparse while weights need to maintain accuracy under fairness constraints
  - Quick check question: Can you explain the difference between the upper-level and lower-level problems in a bi-level optimization framework?

- Concept: Fairness metrics in machine learning
  - Why needed here: The method needs to quantify and constrain bias in model predictions across different demographic groups
  - Quick check question: What is the difference between statistical parity and equalized odds as fairness metrics?

- Concept: Neural network pruning techniques
  - Why needed here: Understanding how pruning works is essential to grasp how fairness constraints can be integrated into the pruning process
  - Quick check question: What is the key difference between structured and unstructured pruning methods?

## Architecture Onboarding

- Component map: Bi-level Optimizer -> Weight Update Module -> Mask Update Module -> Fairness Constraint Module -> Differentiable Relaxation Module

- Critical path: (1) Update weights with fairness constraints, (2) Update masks based on the updated weights and fairness considerations, (3) Repeat until convergence. Each iteration requires both forward and backward passes through the network.

- Design tradeoffs: The framework trades computational complexity for fairness preservation. Joint optimization is more computationally expensive than sequential methods but prevents bias amplification. The relaxation of fairness constraints enables gradient-based optimization but may introduce approximation errors.

- Failure signatures: Common failure modes include: (1) the fairness constraints are too restrictive causing accuracy to drop significantly, (2) the relaxation of fairness constraints leads to optimization getting stuck in local minima that don't satisfy the original fairness requirements, (3) the bi-level optimization fails to converge due to ill-conditioned gradients.

- First 3 experiments:
  1. Implement the basic bi-level optimization framework without fairness constraints to verify the optimization structure works
  2. Add simple fairness constraints to the weight update step only, measure the impact on accuracy and fairness
  3. Implement the full joint optimization with fairness constraints on both masks and weights, compare with baseline pruning methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fairness constraints be effectively incorporated into unstructured pruning methods while maintaining model performance?
- Basis in paper: [explicit] The paper mentions that the proposed BiFP method is capable of different pruning settings, including unstructured pruning (BiFP-uns).
- Why unresolved: The paper primarily focuses on structured pruning and does not provide detailed results or analysis of unstructured pruning with fairness constraints
- What evidence would resolve it: Experiments comparing the performance and fairness of unstructured pruning methods with and without fairness constraints

### Open Question 2
- Question: What is the impact of different fairness metrics on the pruning process and final model performance?
- Basis in paper: [explicit] The paper defines performance fairness and compressed model performance degradation fairness, but does not explore other fairness metrics
- Why unresolved: The choice of fairness metric can significantly affect the pruning process and final model, but the paper does not investigate the impact of different metrics
- What evidence would resolve it: Experiments comparing the performance and fairness of models pruned using different fairness metrics

### Open Question 3
- Question: How does the proposed BiFP method compare to other fairness-aware pruning methods in terms of computational efficiency and scalability?
- Basis in paper: [explicit] The paper mentions that BiFP outperforms the best baseline methods while using notably less training cost, but does not compare it to other fairness-aware pruning methods
- Why unresolved: The computational efficiency and scalability of BiFP compared to other fairness-aware pruning methods are not explored in the paper
- What evidence would resolve it: Comparative analysis of BiFP's computational efficiency and scalability against other fairness-aware pruning methods

## Limitations

- The framework relies heavily on the assumption that fairness constraints can be effectively relaxed to convex, differentiable functions without significant loss of fidelity
- The computational overhead of bi-level optimization is not thoroughly discussed, particularly for large-scale models
- The paper does not adequately address how the bi-level optimization framework scales to larger, more complex models beyond ResNet10

## Confidence

- **High Confidence**: The paper successfully demonstrates that traditional pruning methods can introduce bias into neural networks, and that this bias can be measured using standard fairness metrics across demographic groups
- **Medium Confidence**: The claim that joint optimization of masks and weights under fairness constraints produces better fairness outcomes than sequential approaches is supported by experimental results, but the theoretical justification for why this prevents bias amplification is not fully developed
- **Low Confidence**: The paper does not adequately address how the bi-level optimization framework scales to larger, more complex models beyond ResNet10

## Next Checks

1. **Validation of Surrogate Functions**: Conduct ablation studies that compare the fairness outcomes when using the exact (non-differentiable) fairness metrics versus the surrogate functions at various sparsity levels to quantify the approximation error introduced by the relaxation

2. **Scalability Assessment**: Apply the BiFP framework to larger architectures (e.g., ResNet50 or Vision Transformers) and measure both the computational overhead and fairness preservation across different model sizes to test the practical applicability of the method

3. **Hyperparameter Sensitivity Analysis**: Perform a systematic grid search over the key hyperparameters (learning rates, fairness constraint weights, regularization strengths) to identify the robustness of the framework and determine if the reported results are stable across different hyperparameter configurations