---
ver: rpa2
title: Federated Split Learning with Only Positive Labels for resource-constrained
  IoT environment
arxiv_id: '2307.13266'
source_url: https://arxiv.org/abs/2307.13266
tags:
- data
- sfpl
- client
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training deep learning models
  in distributed collaborative machine learning (DCML) settings where resource-constrained
  IoT devices have only access to positive labeled data. The proposed method, splitfed
  learning with positive labels (SFPL), introduces a global collector function that
  applies random shuffling to smashed data from clients before server-side training,
  along with local batch normalization for client-side models during inference.
---

# Federated Split Learning with Only Positive Labels for resource-constrained IoT environment

## Quick Facts
- arXiv ID: 2307.13266
- Source URL: https://arxiv.org/abs/2307.13266
- Reference count: 11
- Key outcome: SFPL achieves up to 51.54x accuracy improvement over SFLv2 on CIFAR-100 with ResNet-56

## Executive Summary
This paper addresses the challenge of training deep learning models in distributed collaborative machine learning settings where IoT devices have only access to positive labeled data. The proposed Splitfed Learning with Positive Labels (SFPL) framework introduces a global collector function that applies random shuffling to smashed data from clients before server-side training, along with local batch normalization for client-side models during inference. SFPL significantly outperforms splitfed learning v2 (SFLv2) on CIFAR-100 and CIFAR-10 datasets, achieving substantial accuracy improvements across various testing scenarios, particularly excelling when both training and testing use non-IID data distributions.

## Method Summary
The SFPL framework partitions neural networks into client-side and server-side portions, with the client-side performing initial forward passes on local data and transmitting "smashed" activations to a global collector function. This collector aggregates data from multiple clients, applies random shuffling to create artificial IID-like distributions, and sends the shuffled data to the server for training. A key innovation is the use of local batch normalization at the client-side during inference, preventing weight divergence during federated averaging when client data distributions differ significantly. The framework is evaluated on CIFAR-10 and CIFAR-100 datasets with extreme non-IID conditions where each client holds data from only one class.

## Key Results
- SFPL achieves 72.16% accuracy for ResNet-56 on CIFAR-100 compared to 1.40% for SFLv2 (51.54x improvement)
- For ResNet-32 on CIFAR-100, SFPL reaches 66.77% accuracy versus 2.05% for SFLv2 (32.57x improvement)
- On CIFAR-10 with ResNet-32, SFPL achieves 95.08% accuracy compared to 11.17% for SFLv2 (9.23x improvement)
- Superior precision, recall, and F1-score across various testing scenarios, particularly excelling with non-IID data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The global collector function's random shuffling of smashed data creates artificial IID-like distributions for server-side training, mitigating the effects of extreme non-IID client data.
- Mechanism: By aggregating smashed data from multiple clients before shuffling, the collector function simulates a more diverse data distribution that resembles IID conditions, which are better suited for batch normalization and gradient updates.
- Core assumption: The combination of positive-only labels from multiple clients provides sufficient class coverage when shuffled to approximate IID distribution for server-side training.
- Evidence anchors:
  - [abstract]: "SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server for model training"
  - [section IV-B]: "The global collector function collects activations and true labels from clients, shuffles and sends them to the server-side model function"
- Break condition: If the number of clients is too small or class coverage is insufficient, shuffling won't create adequate IID-like conditions.

### Mechanism 2
- Claim: Local batch normalization at the client-side prevents weight divergence during federated averaging by maintaining client-specific normalization statistics.
- Mechanism: Instead of aggregating batch normalization parameters globally, each client maintains its own mean and variance statistics, preventing the averaging process from introducing harmful discrepancies when client data distributions differ significantly.
- Core assumption: Client-side batch normalization parameters can be maintained independently without harming the overall model convergence when combined with the global collector's shuffled data.
- Evidence anchors:
  - [section IV-C]: "the batch normalization layer parameters, including the mean and variance, are typically aggregated as part of the client-side model portion"
  - [section VII]: "the CMSD setup yielded superior outcomes compared to the RMSD setup during the testing phase"
- Break condition: If client data distributions are too extreme, even local batch normalization may not prevent performance degradation.

### Mechanism 3
- Claim: The combination of shuffling and local batch normalization prevents catastrophic forgetting during sequential client training.
- Mechanism: Shuffling ensures that the server-side model doesn't overfit to the last client's class distribution, while local batch normalization maintains stable training dynamics across clients with different data distributions.
- Core assumption: The server-side model can learn effectively when exposed to shuffled, multi-class smashed data rather than sequential single-class batches.
- Evidence anchors:
  - [section V-A]: "SFLv2 suffers due to catastrophic forgetting when client-side models train on single-class data"
  - [section VII]: "SFPL framework significantly enhanced performance, achieving 72.16% and 66.77% accuracy for the R56 and R32 architectures respectively"
- Break condition: If shuffling frequency is too low or batch sizes are too small, catastrophic forgetting may still occur.

## Foundational Learning

- Concept: Batch Normalization
  - Why needed here: Understanding how batch normalization works and why it's typically aggregated across clients in federated learning is crucial to grasp why local batch normalization helps in this context.
  - Quick check question: What happens to batch normalization statistics when averaging models trained on non-IID data?

- Concept: Catastrophic Forgetting
  - Why needed here: The paper explicitly addresses catastrophic forgetting as a failure mode in SFLv2, so understanding this concept is essential.
  - Quick check question: Why does training sequentially on single-class data cause a model to forget previously learned classes?

- Concept: Federated Averaging
  - Why needed here: The paper uses federated averaging for client-side model aggregation, so understanding this mechanism is important.
  - Quick check question: How does federated averaging work when client data distributions are highly non-IID?

## Architecture Onboarding

- Component map:
  Client-side model -> smashed data transmission -> Global Collector Function (aggregation + shuffling) -> Server-side model -> gradient computation -> de-shuffling -> client backward pass -> Federated Server (aggregation excluding batch normalization)

- Critical path: Client forward pass → smashed data transmission → global collector aggregation → shuffling → server-side training → gradient computation → de-shuffling → client backward pass → federated averaging

- Design tradeoffs:
  - Memory vs. Privacy: Local batch normalization increases client memory usage but enhances privacy by not sharing normalization statistics
  - Communication vs. Performance: Shuffling requires buffering data but significantly improves convergence
  - Complexity vs. Accuracy: The additional global collector function adds complexity but provides substantial accuracy gains

- Failure signatures:
  - Low accuracy with non-IID data indicates insufficient shuffling or too few clients
  - Training instability suggests issues with local batch normalization configuration
  - Slow convergence may indicate suboptimal shuffling frequency or batch size

- First 3 experiments:
  1. Baseline SFLv2 with only positive labels on CIFAR-10 to confirm the failure mode
  2. SFPL with shuffling but without local batch normalization to isolate the shuffling effect
  3. SFPL with both shuffling and local batch normalization to verify the combined improvement

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Performance claims rely heavily on synthetic extreme non-IID conditions (one class per client) that may not represent real-world IoT scenarios
- The exact implementation details of the global collector function and how it handles label management during shuffling are not fully specified
- The comparison to SFLv2 assumes identical experimental conditions except for the proposed modifications, but implementation differences could contribute to performance gaps

## Confidence

- High confidence: The mechanism of using local batch normalization to prevent weight divergence during federated averaging is well-established in the literature
- Medium confidence: The claim that shuffling smashed data creates IID-like conditions is theoretically sound but depends on sufficient client diversity and class coverage
- Low confidence: The specific numerical performance improvements (51.54x, 32.57x, etc.) may be inflated due to the extreme synthetic setup and need validation on more realistic data distributions

## Next Checks

1. Cross-dataset validation: Test SFPL on real-world IoT datasets with naturally occurring non-IID distributions rather than synthetically partitioned ones to verify generalization
2. Ablation study: Systematically disable shuffling and local batch normalization independently to quantify their individual contributions to performance gains
3. Communication efficiency analysis: Measure actual communication overhead and memory usage on resource-constrained devices to validate the claimed efficiency benefits