---
ver: rpa2
title: 'Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A
  Critical Review and Assessment'
arxiv_id: '2312.12148'
source_url: https://arxiv.org/abs/2312.12148
tags:
- fine-tuning
- peft
- parameters
- methods
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive review and assessment of Parameter-Efficient
  Fine-Tuning (PEFT) methods for Pretrained Language Models (PLMs). PEFT methods address
  the challenge of adapting large PLMs, especially Large Language Models (LLMs), to
  specific downstream tasks by reducing the number of fine-tuning parameters and memory
  usage while maintaining comparable performance to full fine-tuning.
---

# Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment

## Quick Facts
- arXiv ID: 2312.12148
- Source URL: https://arxiv.org/abs/2312.12148
- Reference count: 40
- Key outcome: PEFT methods significantly improve parameter efficiency while maintaining or improving performance compared to full fine-tuning

## Executive Summary
This paper provides a comprehensive review and assessment of Parameter-Efficient Fine-Tuning (PEFT) methods for Pretrained Language Models (PLMs). PEFT methods address the challenge of adapting large PLMs, especially Large Language Models (LLMs), to specific downstream tasks by reducing the number of fine-tuning parameters and memory usage while maintaining comparable performance to full fine-tuning. The authors classify PEFT methods into five categories: additive, partial, reparameterized, hybrid, and unified fine-tuning. They conduct extensive experiments with 11 representative PEFT methods on various PLMs and tasks, including NLU, MT, and NLG. Results show that most PEFT methods significantly improve parameter efficiency and achieve comparable or even better performance compared to full fine-tuning.

## Method Summary
The paper systematically reviews and evaluates PEFT methods by classifying them into five categories and conducting extensive experiments on 11 representative methods. The evaluation covers three major PLM types (RoBERTa, T5, LLaMA) across three task domains (NLU, MT, NLG). The experiments measure both performance metrics (accuracy, BLEU scores) and efficiency metrics (trainable parameters, GPU memory usage) to compare PEFT methods against full fine-tuning baselines.

## Key Results
- PEFT methods significantly reduce trainable parameters (often to <5% of full fine-tuning) while maintaining comparable performance
- Memory usage is dramatically reduced, with QLoRA achieving substantial computational memory savings
- Some PEFT methods (like ProPELT) achieve better performance than full fine-tuning on certain tasks
- Different PEFT methods show varying effectiveness across different PLM architectures and task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEFT methods reduce the number of trainable parameters while maintaining comparable performance to full fine-tuning.
- Mechanism: By introducing small trainable modules (adapters, low-rank matrices, soft prompts) that are inserted into or combined with the pretrained model, PEFT methods selectively update only a subset of parameters during fine-tuning.
- Core assumption: The pretrained parameters capture general language knowledge that is transferable to downstream tasks, so only minimal adjustments are needed for task-specific adaptation.
- Evidence anchors:
  - [abstract] "PEFT offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning."
  - [section II] "Full fine-tuning...involves training the entire model...However, traditional fine-tuning...is time-consuming and computationally expensive."
  - [corpus] Weak. The corpus does not directly discuss the mechanism of maintaining performance with fewer parameters.
- Break condition: If the downstream task is highly dissimilar from the pretraining data, the pretrained parameters may not capture relevant knowledge, requiring more extensive parameter updates.

### Mechanism 2
- Claim: PEFT methods reduce memory usage during fine-tuning compared to full fine-tuning.
- Mechanism: By freezing most pretrained parameters and only updating a small subset, PEFT methods require less memory for gradient storage and parameter updates during training.
- Core assumption: The memory savings come from not needing to compute and store gradients for the vast majority of pretrained parameters.
- Evidence anchors:
  - [abstract] "reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning."
  - [section IV.C] "prompt-tuning, prefix-tuning, (IA)3, LoRA and AdaLoRA...significantly reduce the GPU memory footprint compared to full fine-tuning."
  - [corpus] Missing. The corpus does not discuss the memory efficiency mechanism of PEFT methods.
- Break condition: If the PEFT method itself introduces large auxiliary modules (e.g., some hybrid methods), the memory savings may be reduced or eliminated.

### Mechanism 3
- Claim: PEFT methods can achieve better performance than full fine-tuning on certain tasks.
- Mechanism: By introducing task-specific modules that are optimized for the target task, PEFT methods can learn task-relevant representations more efficiently than updating all pretrained parameters.
- Core assumption: The task-specific modules can capture task-relevant patterns that may be diluted or lost when updating all parameters in full fine-tuning.
- Evidence anchors:
  - [section IV.B.1] "ProPELTadapter...uses about 1.50% of the trainable parameters to fine-tune RoBERTbase and RoBERTa-large, but achieves optimal average performance on the GLUE benchmark, outperforming RoBERT-base (FT) by about 1.30% and RoBERT-large (FT) by about 1.65%."
  - [section IV.B.2] "both (IA)3 and LoRA significantly reduce the number of trainable parameters compared to full fine-tuning, while maintaining comparable performance."
  - [corpus] Assumption: The corpus does not provide a clear mechanism for why PEFT can outperform full fine-tuning.
- Break condition: If the task-specific modules are not well-designed or the target task is very similar to the pretraining data, full fine-tuning may still be superior.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: PEFT methods are applied to transformer-based PLMs, so understanding the transformer architecture is essential for understanding how PEFT methods modify or interact with the model.
  - Quick check question: What are the main components of a transformer layer and how do they process input sequences?

- Concept: Fine-tuning vs. PEFT
  - Why needed here: PEFT methods are an alternative to full fine-tuning, so understanding the differences between these approaches is crucial for grasping the motivation and benefits of PEFT.
  - Quick check question: How does full fine-tuning differ from PEFT in terms of the number of parameters updated and the memory requirements?

- Concept: Low-rank matrix decomposition
  - Why needed here: Many PEFT methods (e.g., LoRA, KronA) use low-rank matrix decomposition to reduce the number of trainable parameters, so understanding this concept is important for comprehending these methods.
  - Quick check question: How does low-rank matrix decomposition reduce the number of parameters in a matrix while preserving its essential information?

## Architecture Onboarding

- Component map: Pretrained PLM (frozen) + PEFT Module(s) -> Task-specific output
- Critical path: 1) Load pretrained PLM with frozen parameters 2) Insert PEFT module(s) 3) Fine-tune PEFT module(s) on task data 4) Use fine-tuned model for inference
- Design tradeoffs:
  - Parameter efficiency vs. performance: Some PEFT methods (e.g., prompt-tuning) achieve high parameter efficiency but may sacrifice some performance compared to full fine-tuning.
  - Memory efficiency vs. inference speed: Some PEFT methods (e.g., QLoRA) significantly reduce memory usage during fine-tuning but may require additional computations during inference.
  - Task specificity vs. generalization: Some PEFT methods (e.g., adapters) are highly task-specific, while others (e.g., LoRA) can be more easily adapted to multiple tasks.
- Failure signatures:
  - Underfitting: If the PEFT module is too small or not well-designed for the target task, the fine-tuned model may underperform compared to full fine-tuning.
  - Overfitting: If the task-specific data is limited, fine-tuning the PEFT module may lead to overfitting, especially if the module is relatively large.
  - Catastrophic forgetting: If the PEFT module is not properly initialized or trained, it may cause the model to forget important information from the pretraining data.
- First 3 experiments:
  1. Fine-tune RoBERTa-base on a GLUE task using prompt-tuning and compare the performance and memory usage to full fine-tuning.
  2. Fine-tune T5-base on the WMT16 En-Ro dataset using LoRA and evaluate the translation quality and parameter efficiency.
  3. Fine-tune LLaMA-7B on the MMLU benchmark using QLoRA and measure the memory savings and performance compared to full fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental mechanisms that make parameter-efficient fine-tuning (PEFT) methods effective, and how can these be theoretically explained?
- Basis in paper: [explicit] The paper discusses the need for further theoretical studies to understand the working mechanisms of PEFT methods, citing works that provide partial explanations but lack generalization.
- Why unresolved: Current theoretical analyses, such as those unifying PEFT under sparse fine-tuned models or exploring the express power of LoRA, offer insights but do not provide a comprehensive explanation applicable to all PEFT methods.
- What evidence would resolve it: Development of a unified theoretical framework that explains the effectiveness of various PEFT methods, potentially through rigorous mathematical proofs or extensive empirical validation across diverse tasks and model architectures.

### Open Question 2
- Question: How can hybrid PEFT methods be optimized to balance performance improvement with parameter efficiency?
- Basis in paper: [explicit] The paper highlights the potential of combining multiple PEFT methods but notes the challenge of increased parameter and memory usage. It suggests exploring methods to achieve performance gains while minimizing trainable parameters.
- Why unresolved: Existing hybrid approaches like MAM Adapter and ProPELT show improved performance but at the cost of higher parameter consumption. The trade-off between performance and efficiency remains unclear.
- What evidence would resolve it: Experimental studies comparing hybrid methods with varying combinations and configurations, demonstrating optimal balances between performance gains and parameter efficiency.

### Open Question 3
- Question: How can PEFT methods be effectively extended to computer vision and multimodal learning tasks?
- Basis in paper: [explicit] The paper discusses the potential of PEFT methods in computer vision and multimodal learning, noting that while initial explorations exist, there is significant room for further research, especially in cross-modality transfer.
- Why unresolved: Although PEFT methods have been adapted for vision tasks and multimodal learning, their effectiveness and optimal configurations in these domains are not fully understood. The paper suggests that cross-modality transfer through PEFT is promising but underexplored.
- What evidence would resolve it: Comprehensive experimental studies applying PEFT methods to a wide range of computer vision and multimodal tasks, with detailed analysis of performance improvements and parameter efficiency gains.

## Limitations

- The paper lacks theoretical grounding for why PEFT methods work, limiting understanding of when and why they are most effective.
- Results may be domain-dependent, as experiments focus on English NLP tasks and generalization to other languages or domains is unclear.
- Memory overhead from PEFT modules is not fully accounted for in memory efficiency claims.

## Confidence

**High Confidence**: The claim that PEFT methods can reduce the number of trainable parameters while maintaining comparable performance is well-supported by the extensive experimental results across multiple tasks and models.

**Medium Confidence**: The assertion that PEFT methods can outperform full fine-tuning on certain tasks is supported by specific examples but the mechanisms for this superiority are not well-explained.

**Low Confidence**: The explanation of why PEFT methods reduce memory usage during fine-tuning is incomplete, as it doesn't account for the memory overhead of PEFT modules themselves.

## Next Checks

1. **Cross-domain validation**: Test the same PEFT methods on non-English datasets (e.g., Chinese GLUE equivalents, multilingual machine translation) to verify if the parameter efficiency gains generalize across languages and domains.

2. **Ablation study on PEFT components**: Systematically remove or modify components of successful PEFT methods (like LoRA or adapters) to identify which specific architectural choices contribute most to their effectiveness, addressing the lack of theoretical understanding.

3. **Memory overhead quantification**: Conduct detailed profiling of GPU memory usage during both training and inference for each PEFT method, including all auxiliary computations, to verify that memory savings are not offset by PEFT module overhead.