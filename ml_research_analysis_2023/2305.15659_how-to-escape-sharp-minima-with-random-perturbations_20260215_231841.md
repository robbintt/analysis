---
ver: rpa2
title: How to escape sharp minima with random perturbations
arxiv_id: '2305.15659'
source_url: https://arxiv.org/abs/2305.15659
tags:
- minima
- lemma
- flat
- proof
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of finding flat minima, which are
  minima of a loss function that are both local minima and stationary points of the
  trace of the Hessian along the set of local minima. The authors formalize this notion
  and design efficient algorithms to find such minima.
---

# How to escape sharp minima with random perturbations

## Quick Facts
- **arXiv ID**: 2305.15659
- **Source URL**: https://arxiv.org/abs/2305.15659
- **Reference count**: 40
- **Primary result**: Algorithms find (ϵ, √ϵ)-flat minima within O(ϵ−3) iterations using only first-order gradients

## Executive Summary
This paper addresses the challenge of finding flat minima in non-convex optimization problems, which are associated with better generalization in deep learning. The authors formalize the notion of flat minima as local minima with low trace of the Hessian along the manifold of minima, and design efficient algorithms to find such minima. The key insight is that random perturbations can be used to estimate directions that decrease the trace of the Hessian using only first-order gradient information, avoiding the computational expense of directly minimizing the Hessian trace.

The paper proposes two algorithms: the randomly smoothed perturbation algorithm for general loss functions and the sharpness-aware perturbation algorithm for training losses over data. Both algorithms use a two-phase approach, switching between standard gradient descent and perturbation-based updates when near local minima. The theoretical analysis shows that these algorithms can efficiently find flat minima with convergence rates that depend polynomially on the desired accuracy, making them practical for large-scale optimization problems.

## Method Summary
The paper designs two gradient-based algorithms to find flat minima in non-convex optimization. The Randomly Smoothed Perturbation algorithm (Algorithm 1) perturbs iterates randomly and uses the resulting gradients to estimate directions that decrease the trace of the Hessian along the local minima manifold. The Sharpness-Aware Perturbation algorithm (Algorithm 2) extends this approach to training losses by perturbing along stochastic gradient directions, achieving faster convergence in high-dimensional settings. Both algorithms maintain a tolerance parameter ϵ₀ and switch between standard gradient descent and perturbation-based updates based on the gradient norm, requiring only first-order gradient information while achieving provable convergence to flat minima.

## Key Results
- The randomly smoothed perturbation algorithm finds an (ϵ, √ϵ)-flat minimum within O(ϵ−3) iterations
- The sharpness-aware perturbation algorithm achieves the same within O(d−1ϵ−2(1 ∨ 1/d3ϵ)) iterations for training losses
- Both algorithms use only first-order gradients, avoiding the computational expense of directly minimizing the trace of the Hessian
- The sharpness-aware algorithm provides a d-fold improvement in convergence speed for high-dimensional problems

## Why This Works (Mechanism)

### Mechanism 1: Random Perturbations Estimate Hessian Trace Gradients
Randomly smoothed perturbations can estimate directions that decrease the trace of the Hessian along the manifold of local minima using only first-order gradients. By computing gradients at randomly perturbed iterates (x + ρg where g is uniform on the sphere), the algorithm captures third-order derivative information. Projecting out the gradient at the current iterate isolates components that inform flatness. This works because the expectation of the difference quotient captures third-order terms that relate to Hessian trace changes.

### Mechanism 2: Sharpness-Aware Perturbations Leverage Data Structure
Sharpness-aware perturbation (SAM-inspired) decreases the trace of the Hessian faster than random smoothing in high-dimensional settings. Instead of random perturbations, the algorithm perturbs along stochastic gradient directions. This leverages the structure of training loss (sum over data points) to achieve a d-fold improvement in gradient estimation of the trace. The key insight is that data gradients provide a natural basis for exploring the parameter space in a way that efficiently reduces sharpness.

### Mechanism 3: Limit Map Provides Smooth Parameterization
The limit map Φ under gradient flow provides a smooth parameterization of the local minima manifold, enabling trace minimization along it. Φ(x) maps any point near local minima to the nearest minimum. The gradient of tr(Φ(x)) with respect to x can be computed via the chain rule, and this gradient points toward flatter minima. This works because the gradient flow converges to local minima under the PL inequality, creating a well-defined mapping that preserves smoothness properties.

## Foundational Learning

- **Concept**: Polyak-Łojasiewicz (PL) inequality
  - Why needed here: Ensures gradient descent converges to local minima from nearby points, allowing the algorithm to focus on flatness optimization rather than finding minima.
  - Quick check question: If a function satisfies PL with parameter α, what is the relationship between gradient norm and distance to the minimum?

- **Concept**: Randomized smoothing for gradient estimation
  - Why needed here: Provides a way to estimate higher-order derivatives (third-order) using only first-order information through expectation over random perturbations.
  - Quick check question: Given f(x + ρg) where g is uniform on the sphere, what is E[g g^T]?

- **Concept**: Sharpness-aware minimization (SAM)
  - Why needed here: Provides the conceptual foundation for perturbing along gradient directions rather than random directions, improving convergence speed.
  - Quick check question: How does SAM's perturbation differ from the standard gradient descent update?

## Architecture Onboarding

- **Component map**: Main loop with gradient norm check -> Perturbation computation (random direction + Proj⊥) -> Gradient evaluation at perturbed point -> Parameter update -> Store iterate -> Return random iterate

- **Critical path**: 
  1. Check gradient norm threshold
  2. If below threshold: compute perturbation direction, update with perturbed gradient
  3. If above threshold: standard gradient descent step
  4. Store iterate
  5. After T steps: return random iterate

- **Design tradeoffs**:
  - Perturbation radius ρ vs. convergence speed: Larger ρ captures more third-order information but may increase function value temporarily
  - Tolerance ϵ₀: Too small wastes iterations on flatness optimization; too large may miss flat minima
  - Full-batch vs. stochastic gradients: Full-batch gives exact trace information but is computationally expensive

- **Failure signatures**:
  - Iterates oscillate without decreasing trace: Perturbation radius too large relative to local curvature
  - Convergence to sharp minimum: Tolerance too high or insufficient iterations
  - Extremely slow convergence: Perturbation direction not capturing dominant trace-decreasing components

- **First 3 experiments**:
  1. Verify that Algorithm 1 decreases trace along minima manifold on a simple 2D function with known flat minima
  2. Compare convergence rates of random smoothing vs. sharpness-aware perturbation on a training loss with synthetic data
  3. Test robustness to perturbation radius by running Algorithm 1 with varying ρ on a benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the randomly smoothed perturbation algorithm compare to practical algorithms like SAM when both are designed to find flat minima?
- Basis in paper: The paper states that the randomly smoothed perturbation algorithm finds an (ϵ, √ϵ)-flat minimum within O(ϵ−3) iterations, while the sharpness-aware perturbation algorithm achieves the same within O(d−1ϵ−2(1 ∨ 1/d3ϵ)) iterations for training losses.
- Why unresolved: The paper does not provide empirical comparisons between the randomly smoothed perturbation algorithm and practical algorithms like SAM.
- What evidence would resolve it: Empirical studies comparing the performance of the randomly smoothed perturbation algorithm and practical algorithms like SAM on standard benchmark datasets and tasks.

### Open Question 2
- Question: What are the implications of using stochastic gradients versus full-batch gradients in the sharpness-aware perturbation algorithm for finding flat minima?
- Basis in paper: The paper discusses the role of stochastic gradients in the sharpness-aware perturbation algorithm and mentions that using stochastic gradients leads to faster decrease in the trace of the Hessian compared to full-batch gradients.
- Why unresolved: The paper does not provide a detailed analysis of the implications of using stochastic gradients versus full-batch gradients in the sharpness-aware perturbation algorithm.
- What evidence would resolve it: A theoretical and empirical study comparing the performance of the sharpness-aware perturbation algorithm using stochastic gradients versus full-batch gradients on various tasks and datasets.

### Open Question 3
- Question: How does the choice of the measure of flatness (e.g., trace of the Hessian) affect the efficiency and effectiveness of algorithms designed to find flat minima?
- Basis in paper: The paper adopts the trace of the Hessian as the measure of flatness and designs algorithms based on this choice.
- Why unresolved: The paper does not explore alternative measures of flatness or compare the performance of algorithms designed using different measures.
- What evidence would resolve it: A study comparing the performance of algorithms designed using different measures of flatness (e.g., trace of the Hessian, maximum eigenvalue of the Hessian) on various tasks and datasets.

### Open Question 4
- Question: What are the lower bounds for finding approximate flat minima, and how do they compare to the upper bounds established by the algorithms in the paper?
- Basis in paper: The paper does not provide lower bounds for finding approximate flat minima.
- Why unresolved: The paper focuses on designing efficient algorithms for finding flat minima but does not explore the theoretical limits of this problem.
- What evidence would resolve it: A study establishing lower bounds for finding approximate flat minima and comparing them to the upper bounds achieved by the algorithms in the paper.

## Limitations

- The theoretical analysis requires four-times continuous differentiability, which may not hold for all practical deep learning loss surfaces
- The assumption that gradient flow converges to local minima (PL inequality) needs verification for specific architectures and datasets
- The orthogonal model outputs assumption for the sharpness-aware algorithm appears strong and may not generalize beyond synthetic examples
- Limited empirical validation on real-world deep learning tasks makes practical effectiveness uncertain

## Confidence

- **High confidence**: The fundamental mechanism of using random perturbations to estimate third-order derivative information for flatness optimization is mathematically sound and well-supported by the analysis
- **Medium confidence**: The d-fold improvement claimed for the sharpness-aware perturbation algorithm depends on specific structural properties of training losses that may not hold universally
- **Low confidence**: The practical effectiveness of these algorithms in high-dimensional deep learning scenarios remains largely theoretical, with limited empirical validation shown

## Next Checks

1. **Empirical trace tracking**: Implement both algorithms on standard benchmark datasets (e.g., CIFAR-10 with simple CNN architectures) and measure the actual trace of the Hessian at convergence, comparing against theoretical predictions.

2. **Perturbation radius sensitivity**: Systematically vary the perturbation radius parameter ρ across several orders of magnitude and measure its impact on convergence speed and final flatness metrics.

3. **Comparison with SAM baselines**: Run the sharpness-aware perturbation algorithm alongside standard SAM implementations on identical tasks, measuring not just flatness but also generalization performance on held-out data.