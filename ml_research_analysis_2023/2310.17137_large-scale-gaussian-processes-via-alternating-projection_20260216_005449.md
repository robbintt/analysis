---
ver: rpa2
title: Large-Scale Gaussian Processes via Alternating Projection
arxiv_id: '2310.17137'
source_url: https://arxiv.org/abs/2310.17137
tags:
- alternating
- projection
- training
- iterations
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an alternating projection method for solving
  linear systems with kernel matrices in Gaussian process regression, achieving significant
  speedups over conjugate gradient (CG) methods. The method partitions the kernel
  matrix into subblocks and iteratively projects onto subspaces defined by these blocks,
  requiring only O(n) computation per iteration.
---

# Large-Scale Gaussian Processes via Alternating Projection

## Quick Facts
- arXiv ID: 2310.17137
- Source URL: https://arxiv.org/abs/2310.17137
- Reference count: 40
- This paper presents an alternating projection method for solving linear systems with kernel matrices in Gaussian process regression, achieving significant speedups over conjugate gradient (CG) methods.

## Executive Summary
This paper introduces an alternating projection method for solving kernel linear systems in Gaussian process regression, achieving O(n) per-iteration complexity. The method partitions the kernel matrix into subblocks and iteratively projects onto subspaces defined by these blocks, enabling scalable GP training on datasets up to 4 million points without inducing points. The approach demonstrates 2-27× speedups in training and 2-72× in test-time inference while maintaining comparable predictive performance to existing methods.

## Method Summary
The alternating projection method solves kernel linear systems by partitioning the kernel matrix into subblocks and iteratively updating weights and residuals through O(n) operations per iteration. Each iteration selects a block of data, computes the projection onto the corresponding subspace using cached Cholesky factors of the principal submatrix, and updates the solution estimate. The method requires only submatrices of the kernel matrix rather than the full matrix, making it memory-efficient and suitable for GPU acceleration. A pivoted Cholesky preconditioner is used for CG comparison, and hyperparameters are optimized using Adam.

## Key Results
- Achieves 2-27× speedup in training time compared to conjugate gradient on datasets up to 4 million points
- Demonstrates 2-72× speedup in test-time inference while maintaining comparable predictive performance
- Shows greater robustness to ill-conditioning than CG, with rapid early-stage convergence
- Largest GP model trained without inducing points demonstrated on 4-million-point dataset

## Why This Works (Mechanism)

### Mechanism 1
Alternating projection achieves linear convergence faster than conjugate gradient in early iterations for ill-conditioned GP kernel matrices. The method partitions the kernel matrix into subblocks and iteratively projects onto subspaces defined by these blocks, requiring only O(n) computation per iteration. This allows frequent updates that rapidly reduce the residual in the early stage, even if the asymptotic rate is slower than CG. The convergence advantage holds when the condition number of partitioned submatrices (κ') is significantly smaller than the full kernel matrix (κ).

### Mechanism 2
The algorithm implicitly works on better-conditioned matrices through block partitioning, providing robustness to ill-conditioning. By decomposing the projection into smaller subproblems with principal submatrices, each update operates on a matrix with a smaller condition number than the full kernel matrix. This robustness is particularly valuable when the full kernel matrix is ill-conditioned, as the partitioned approach maintains faster convergence rates even in challenging scenarios.

### Mechanism 3
The method requires only O(n) memory and computation per iteration, enabling mini-batching and GPU parallelism. Each iteration accesses only subblocks of the kernel matrix, updating weights and residuals through matrix operations that scale linearly with n. This memory-efficient design allows the method to handle massive datasets that would be infeasible with dense matrix operations, while still maintaining competitive convergence rates.

## Foundational Learning

- **Reproducing Kernel Hilbert Space (RKHS) projection**: Why needed - The alternating projection method frames the linear solve as a projection in RKHS, which allows decomposition into smaller subproblems. Quick check - Can you explain why proj_V[n](g) = b^T K^-1 k(X, ·) for any function g interpolating b?

- **Conjugate gradient convergence behavior**: Why needed - Understanding why CG struggles with ill-conditioning helps explain the alternating projection advantage. Quick check - What causes CG's residual to increase dramatically in early iterations for ill-conditioned matrices?

- **Matrix-free iterative methods**: Why needed - The method accesses the kernel matrix through matrix-vector multiplications rather than explicit storage, crucial for scalability. Quick check - How does the map-reduce approach using KeOps enable handling kernel matrices that don't fit in memory?

## Architecture Onboarding

- **Component map**: Data partitioning module -> Cholesky cache -> Block selection strategy -> Residual tracker -> Weight updater
- **Critical path**: MLL gradient computation -> linear solve (alternating projection) -> hyperparameter update -> convergence check
- **Design tradeoffs**: Batch size vs. convergence rate (larger batches reduce sequential updates but may increase condition number), Memory vs. speed (caching Cholesky factors trades memory for faster computation), Partition strategy (sequential vs. random affects convergence speed)
- **Failure signatures**: Slow convergence (poor block partitioning or large batch size), Memory overflow (Cholesky cache too large), Numerical instability (ill-conditioned submatrices causing failed decompositions)
- **First 3 experiments**: 1) Verify O(n) complexity by measuring runtime vs. dataset size for fixed batch size, 2) Compare convergence rates with different block selection strategies (random vs. Gauss-Southwell), 3) Test robustness to ill-conditioning by training on datasets with varying noise levels

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of alternating projection scale with increasing dimensionality (d) of the input features? The paper evaluates on datasets with varying dimensions (d = 3 to 90) but doesn't systematically analyze how performance changes with increasing d. This remains unresolved because the experimental section focuses on dataset size rather than dimensionality effects, and no theoretical analysis of dimensionality impact is provided.

### Open Question 2
What is the optimal block size (b) for alternating projection in terms of wall-clock time versus number of iterations? The paper doesn't provide a theoretical framework for choosing b optimally, and the empirical recommendation to use the largest possible b is based on practical constraints rather than systematic analysis. Figure 3 shows that smaller batch sizes converge faster within the same epochs but result in longer wall-clock time due to more sequential updates on GPU.

### Open Question 3
How does alternating projection perform on non-stationary or heteroscedastic data compared to CG? The paper only evaluates on stationary kernel functions (Matérn kernels) and doesn't investigate performance on data with varying noise levels or non-stationarity. All experiments use standard stationary kernels with homogeneous noise, and no theoretical analysis addresses non-stationary scenarios.

### Open Question 4
What is the relationship between the number of Lanczos iterations used for variance estimation and the NLL gap between alternating projection and SVGP methods? The paper doesn't systematically quantify how many Lanczos iterations are needed to match SVGP's NLL, or analyze the trade-off between computational cost and NLL accuracy. Appendix E.4 shows that increasing Lanczos iterations decreases NLL, and Appendix E notes that NLL gaps shrink as Lanczos rank increases.

## Limitations
- Performance guarantees rely on the assumption that partitioned submatrices have significantly smaller condition numbers than the full kernel matrix, which isn't empirically validated across diverse datasets
- The method's effectiveness may diminish when data distribution creates highly correlated blocks that lose the condition number advantage
- Comparison focuses on specific kernel types (Matérn) and doesn't explore performance with different kernel structures or non-stationary covariance functions

## Confidence

**High confidence**: The O(n) per-iteration time and space complexity claim is well-supported by the algorithm analysis showing O(b²) for weight updates and O(nb) for residual updates, where b is the block size.

**Medium confidence**: The 2-27× speedup in training and 2-72× in test-time inference claims are based on empirical results on UCI datasets, but these results may not generalize to all GP applications or different kernel structures.

**Low confidence**: The claim about being the "largest GP model trained without inducing points" lacks context about what constitutes a fair comparison, as different works may use different evaluation protocols or hardware constraints.

## Next Checks

1. **Condition number analysis**: Measure and compare κ and κ' across multiple datasets and kernel types to empirically validate the theoretical assumption that partitioned submatrices have significantly smaller condition numbers than the full kernel matrix.

2. **Block size sensitivity**: Systematically vary the block size parameter b across several orders of magnitude and measure its impact on convergence rate, runtime, and memory usage to establish optimal selection guidelines for different problem scales.

3. **Cross-kernel validation**: Test the alternating projection method on kernels beyond Matérn (e.g., RBF, periodic, spectral mixture) to verify that the performance advantages and O(n) complexity hold across different covariance structures commonly used in GP applications.