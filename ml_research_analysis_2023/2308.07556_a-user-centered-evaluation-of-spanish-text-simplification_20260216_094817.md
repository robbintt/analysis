---
ver: rpa2
title: A User-Centered Evaluation of Spanish Text Simplification
arxiv_id: '2308.07556'
source_url: https://arxiv.org/abs/2308.07556
tags:
- spanish
- simplification
- text
- language
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present an evaluation of text simplification (TS) in Spanish
  for a production system, by means of two corpora focused in both complex-sentence
  and complex-word identification. We compare the most prevalent Spanish-specific
  readability scores with neural networks, and show that the latter are consistently
  better at predicting user preferences regarding TS.
---

# A User-Centered Evaluation of Spanish Text Simplification

## Quick Facts
- arXiv ID: 2308.07556
- Source URL: https://arxiv.org/abs/2308.07556
- Reference count: 40
- Primary result: Neural networks outperform traditional readability scores for Spanish text simplification prediction

## Executive Summary
This paper presents a user-centered evaluation of text simplification in Spanish through two specialized corpora: BrevE for complex sentence identification and CLaro for complex word identification. The study compares traditional Spanish readability scores against modern neural networks, finding that neural models consistently outperform traditional approaches in predicting user preferences for simplification. The research reveals that Spanish-only models outperform their multilingual counterparts and that the two simplification tasks are genuinely disjoint, requiring separate models.

## Method Summary
The study creates two annotated corpora (BrevE and CLaro) from administrative and legal texts, using LLM-based pre-simplification followed by human annotation. Multiple neural architectures (BETO, DistilBETO, mBERT, DistilBERT, XLM-R) are trained and evaluated against traditional Spanish readability formulas (Fernández Huerta, Szigriszt, µ score) using F1 score as the primary metric. The evaluation includes cross-corpora studies to test task disjointness and error analysis to identify spurious feature reliance.

## Key Results
- Neural networks consistently outperform traditional Spanish readability scores in predicting user preferences for text simplification
- Spanish-only models significantly outperform equivalent multilingual models on both simplification tasks
- The complex word identification (PLI) and complex sentence identification (CSI) tasks are genuinely disjoint, with poor cross-corpora generalization (14.1-17.3% F1 drop)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks consistently outperform readability scores in predicting user preferences for Spanish text simplification.
- Mechanism: Neural networks capture complex linguistic features and patterns that traditional readability formulas cannot, such as contextual word usage and syntactic structure variations across Spanish dialects.
- Core assumption: The training data (BrevE and CLaro corpora) adequately represents the linguistic diversity and user preferences for Spanish text simplification.
- Evidence anchors:
  - [abstract]: "we compare the most prevalent Spanish-specific readability scores with neural networks, and show that the latter are consistently better at predicting user preferences"
  - [section 3.2]: "Overall we observed a gap between readability scores and neural networks"
  - [corpus]: The corpora contain 3,729 sentences for BrevE and 3,626 for CLaro, with human annotations providing ground truth for user preferences
- Break condition: If the training data does not adequately represent the target user population or if the neural network architecture is too shallow to capture the necessary linguistic features.

### Mechanism 2
- Claim: Spanish-only models outperform multilingual models for Spanish text simplification tasks.
- Mechanism: Spanish-only models have been trained on more Spanish-specific data and can better capture the nuances of Spanish morphology and syntax, which differ significantly from other languages.
- Core assumption: The additional Spanish-specific training data and fine-tuning provide sufficient advantage to overcome the potential benefits of transfer learning from other languages.
- Evidence anchors:
  - [abstract]: "we find that multilingual models underperform against equivalent Spanish-only models on the same task"
  - [section 3.2]: "There is a gap between monolingual (solid lines) and multilingual (dashed, with an asterisk) models, with the former generally performing better"
  - [corpus]: The corpora contain Spanish-specific linguistic features such as voseo and Nahuatl terms, which may not be well-represented in multilingual training data
- Break condition: If the multilingual models are significantly larger or have been trained on substantially more diverse data, potentially overcoming the language-specific advantage.

### Mechanism 3
- Claim: Text simplification in Spanish requires separate models for complex word identification (PLI) and complex sentence identification (CSI) due to task disjointness.
- Mechanism: PLI focuses on lexical simplification while CSI focuses on syntactic simplification, and the models trained on one task do not generalize well to the other.
- Core assumption: The linguistic features important for PLI and CSI are sufficiently different that joint training or transfer learning between tasks is ineffective.
- Evidence anchors:
  - [abstract]: "We concentrate on two subtasks of Spanish TS: complex word identification (CWI) and complex sentence identification (CSI)"
  - [section 3.3]: "The performance of the models in this scenario drops noticeably: on average, of 14.1% F1 for training in BrevE and testing in CLaro"
  - [corpus]: The frequency analysis in section 3.5 shows different lemma distributions for PLI and CSI tasks, supporting their distinct nature
- Break condition: If a model architecture is developed that can effectively learn and transfer features between PLI and CSI tasks.

## Foundational Learning

- Concept: Spanish morphology and syntax
  - Why needed here: Spanish has freer word order and more verb inflections than English, affecting how text simplification should be approached
  - Quick check question: How many simple verb inflections does Spanish have compared to English?
- Concept: Neural network architectures for NLP (BERT, DistilBERT, multilingual variants)
  - Why needed here: The paper compares different neural network architectures for text simplification tasks
  - Quick check question: What is the main difference between BERT and DistilBERT?
- Concept: Readability formulas and their limitations
  - Why needed here: The paper evaluates traditional readability scores against neural network approaches
  - Quick check question: What are the main components of the Fernández Huerta readability score?

## Architecture Onboarding

- Component map: Data collection pipeline → Annotation interface → Model training framework → Evaluation metrics → Result analysis
- Critical path: Corpus creation (BrevE and CLaro) → Model training and evaluation → Error analysis → Frequency analysis
- Design tradeoffs: Monolingual vs. multilingual models (accuracy vs. generalization), complex vs. simple models (performance vs. computational cost)
- Failure signatures: Over-reliance on sentence length as a feature, poor cross-corpus generalization, inability to capture dialectal variations
- First 3 experiments:
  1. Replicate the comparison between readability scores and neural networks using a subset of the data
  2. Train a baseline monolingual model and a multilingual model on the BrevE corpus and compare performance
  3. Perform cross-corpus evaluation by training on BrevE and testing on CLaro to confirm task disjointness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the difference in performance between monolingual and multilingual models in Spanish text simplification stem from inherent architectural differences or from the quality and size of the training data?
- Basis in paper: [inferred] The paper notes that multilingual models underperform against equivalent Spanish-only models in both complex word identification (PLI) and complex sentence identification (CSI), but doesn't definitively attribute this to model architecture versus data quality.
- Why unresolved: The study doesn't perform a controlled experiment isolating model architecture from training data effects, leaving the root cause unclear.
- What evidence would resolve it: A controlled experiment training identical architectures on matched Spanish and multilingual datasets would isolate the impact of architecture versus data quality on performance.

### Open Question 2
- Question: How do linguistic features beyond sentence length, such as syntactic complexity or semantic density, impact the performance of neural models in Spanish text simplification tasks?
- Basis in paper: [explicit] The paper's error analysis suggests that most models focus too much on spurious features like sentence length rather than correct grammatical features, but doesn't deeply explore which linguistic features are most predictive.
- Why unresolved: The study identifies a problem but doesn't systematically analyze which linguistic features would improve model performance if focused on.
- What evidence would resolve it: A feature importance analysis using linguistic annotations (e.g., dependency parsing, semantic role labeling) to identify which features most strongly correlate with user preferences in simplification.

### Open Question 3
- Question: Would incorporating dialectical variations and colloquial language into the training data improve the generalization of Spanish text simplification models across different Spanish-speaking regions?
- Basis in paper: [explicit] The paper acknowledges the challenge of Spanish dialect diversity and the annotators' familiarity with multiple variants, but the dataset construction aimed for general representativeness rather than dialect-specific coverage.
- Why unresolved: The study doesn't test whether dialect-specific or more diverse linguistic data would improve model performance on regional variations.
- What evidence would resolve it: Training and evaluating models on dialect-specific datasets (e.g., Mexican Spanish vs. Castilian Spanish) would reveal whether dialectical diversity in training data improves regional generalization.

## Limitations

- The corpora are relatively small (3,729 and 3,626 sentences) and focused on administrative/legal texts, limiting generalizability
- The reliance on LLM-generated pre-simplifications for human annotation introduces uncertainty about whether annotators evaluated simplification quality or compared generated variants
- The error analysis showing over-reliance on sentence length is concerning but not deeply explored or resolved

## Confidence

**High confidence**: Neural networks outperform traditional readability scores for Spanish text simplification prediction (supported by direct F1 comparisons and consistent across multiple model architectures).

**Medium confidence**: Spanish-only models significantly outperform multilingual models on these tasks (supported by empirical results but limited to current model sizes and training approaches; larger multilingual models might perform differently).

**Low confidence**: The tasks are genuinely disjoint and require separate models (the 14.1-17.3% F1 drop in cross-corpora evaluation could reflect domain shift rather than fundamental task differences; more rigorous ablation studies would be needed).

## Next Checks

1. **Feature importance validation**: Perform ablation studies removing sentence length from model inputs to determine whether neural networks truly capture linguistic simplification patterns or merely exploit this spurious correlation.

2. **Cross-lingual transfer validation**: Train multilingual models with Spanish-only fine-tuning versus full multilingual training to isolate whether the performance gap stems from insufficient Spanish data or inherent limitations of multilingual architectures.

3. **Task similarity validation**: Train joint models on both BrevE and CLaro with shared representations to test whether the claimed task disjointness is fundamental or could be overcome with better architecture design.