---
ver: rpa2
title: Steering Large Language Models for Machine Translation with Finetuning and
  In-Context Learning
arxiv_id: '2310.13448'
source_url: https://arxiv.org/abs/2310.13448
tags:
- few-shot
- zero-shot
- five-shot
- examples
- finetuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adapter-based LoRA finetuning on translation instructions significantly
  improves LLM translation quality over few-shot prompting while training 50x fewer
  parameters, and eliminates the need for post-processing or example selection. However,
  finetuning degrades few-shot performance, harming domain adaptation.
---

# Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning

## Quick Facts
- arXiv ID: 2310.13448
- Source URL: https://arxiv.org/abs/2310.13448
- Authors: 
- Reference count: 40
- Key outcome: Adapter-based LoRA finetuning on translation instructions significantly improves LLM translation quality over few-shot prompting while training 50x fewer parameters, and eliminates the need for post-processing or example selection. However, finetuning degrades few-shot performance, harming domain adaptation. Incorporating few-shot examples during LoRA finetuning recovers few-shot capabilities while retaining the benefits of finetuning, as shown on 10 language pairs.

## Executive Summary
This paper investigates how to effectively steer large language models (LLMs) for machine translation by combining finetuning and in-context learning. The authors demonstrate that adapter-based LoRA finetuning on translation instructions significantly improves translation quality over few-shot prompting while reducing the number of trainable parameters by 50x. However, they discover that finetuning generally degrades few-shot performance, hindering adaptation capabilities. To address this, they propose a simple approach that incorporates few-shot examples during LoRA finetuning, which recovers the original few-shot capabilities while keeping the added benefits of finetuning. Their experiments across 10 language pairs validate these findings.

## Method Summary
The authors use LLaMA 7B and 13B models, training with LoRA adapters to reduce parameters by 50x (134M vs 6.7B). They prepare OPUS general domain data, filter with Bicleaner (threshold 0.85) and COMETKiwi (score >0.8), then sample 250K records per language pair. Instruction templates are defined for zero-shot and few-shot settings. LoRA is trained with hyperparameters: batch size 8, learning rate 2e-4, warmup 500 steps, rank 256. Evaluation uses COMET, BLEU, chrF, and COMETKiwi metrics on Flores-200 and WMT22 test sets.

## Key Results
- LoRA adapter-based finetuning matches traditional finetuning while reducing trainable parameters by 50x
- Finetuning degrades few-shot performance, especially on specialized domains (Medical, Law, TICO, WMT Chat)
- Incorporating few-shot examples during LoRA finetuning recovers few-shot capabilities while retaining finetuning benefits
- Evaluated across 10 language pairs showing consistent improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA adapter-based finetuning reduces parameter count by 50x while maintaining translation performance.
- Mechanism: LoRA inserts small trainable low-rank matrices into each transformer layer, approximating full weight updates with far fewer parameters.
- Core assumption: Low-rank decomposition (rank r) is sufficient to capture the needed adaptation for translation tasks.
- Evidence anchors:
  - [abstract] "adapter-based finetuning with LoRA matches the performance of traditional finetuning while reducing the number of training parameters by a factor of 50"
  - [section 3.1] "LoRA requires only 134M trainable parameters, whereas traditional finetuning requires 6.7B"
  - [corpus] Weak - no corpus mentions of LoRA performance directly.
- Break condition: If rank r is too small to capture adaptation, performance degrades; if too large, parameter savings diminish.

### Mechanism 2
- Claim: Finetuning degrades few-shot performance by overspecializing the model to zero-shot instructions.
- Mechanism: Finetuning on zero-shot instruction data causes the model to overfit to that format, reducing its ability to extract useful information from few-shot examples during inference.
- Core assumption: The model's in-context learning capability is sensitive to the type of training data distribution.
- Evidence anchors:
  - [abstract] "finetuning generally degrades few-shot performance, hindering adaptation capabilities"
  - [section 3.2] "Few-shot performance degrades and is surpassed by zero-shot performance, suggesting that the finetuning procedure is hindering the in-context learning abilities"
  - [corpus] Weak - corpus does not provide evidence for this mechanism.
- Break condition: If training data includes few-shot examples, the degradation may be mitigated.

### Mechanism 3
- Claim: Mixing few-shot examples during finetuning recovers few-shot capabilities while retaining benefits of finetuning.
- Mechanism: Training on both zero-shot and few-shot instruction data teaches the model to handle both scenarios, preserving in-context learning while benefiting from specialized training.
- Core assumption: The model can learn to generalize across different instruction formats during training.
- Evidence anchors:
  - [abstract] "we propose a simple approach that incorporates few-shot examples during finetuning... recovers the original few-shot capabilities while keeping the added benefits of finetuning"
  - [section 4] "We uniformly sample between 0 and 5 few-shot examples for each training example... the models trained with in-context examples recover their few-shot capabilities"
  - [corpus] Weak - corpus does not discuss this approach.
- Break condition: If sampling ratio is unbalanced, either few-shot or zero-shot performance may suffer.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The paper compares few-shot prompting performance before and after finetuning; understanding ICL is key to interpreting results.
  - Quick check question: What distinguishes in-context learning from traditional finetuning?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: LoRA is the parameter-efficient method being evaluated; understanding its mechanics is essential for grasping the 50x parameter reduction claim.
  - Quick check question: How does LoRA approximate full weight updates with fewer parameters?

- Concept: Hallucination in translation
  - Why needed here: The paper analyzes hallucination rates when introducing few-shot examples, which affects evaluation of the finetuning approach.
  - Quick check question: What are the different categories of hallucinations mentioned in the analysis?

## Architecture Onboarding

- Component map:
  - LLaMA 7B/13B backbone -> LoRA adapters (rank r matrices) -> Training data pipeline (OPUS filtered data) -> Instruction templates (zero-shot/few-shot) -> Evaluation pipeline (COMET, BLEU, chrF, COMETKiwi)

- Critical path:
  1. Prepare and filter parallel data
  2. Define instruction templates
  3. Initialize LoRA adapters
  4. Train with mixed zero/few-shot examples
  5. Evaluate on zero-shot and few-shot scenarios

- Design tradeoffs:
  - LoRA rank r vs. parameter efficiency vs. performance
  - Zero-shot vs. few-shot instruction ratio in training data
  - Choice of evaluation metrics (lexical vs. semantic)

- Failure signatures:
  - High hallucination rates indicate degraded few-shot capability
  - Overgeneration suggests issues with instruction template
  - Performance gap between zero-shot and few-shot indicates overspecialization

- First 3 experiments:
  1. Compare LoRA vs. full finetuning on a single language pair with fixed hyperparameters
  2. Test different LoRA rank values (r=128, 256) on validation set
  3. Train with balanced vs. unbalanced few-shot example sampling to verify recovery of few-shot capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the findings on few-shot prompting degradation generalize to low-resource language pairs beyond English-centric high-resource pairs?
- Basis in paper: [explicit] The authors note their findings focus on English-centric high-resource pairs and state it remains an open question how these findings generalize for non-English language pairs or in low-resource settings.
- Why unresolved: The study only evaluated on high-resource language pairs involving English, so the effects on truly low-resource or non-English-centric pairs are untested.
- What evidence would resolve it: Experiments testing the same LoRA and few-shot integration approaches on low-resource and non-English-centric pairs, measuring degradation and recovery effects.

### Open Question 2
- Question: What is the underlying mechanism causing finetuning to degrade few-shot performance, and can it be prevented without adding few-shot examples during training?
- Basis in paper: [inferred] The paper observes that finetuned models perform worse on few-shot adaptation, especially in specialized domains, but does not explain why this occurs.
- Why unresolved: The paper shows the phenomenon but does not investigate or explain the cause of the degradation.
- What evidence would resolve it: Analysis of model internals (e.g., attention patterns, representations) comparing pretrained vs finetuned models on few-shot tasks, or ablation studies varying training objectives.

### Open Question 3
- Question: How severe are the hallucinations introduced by few-shot examples, and what are effective mitigation strategies?
- Basis in paper: [explicit] The paper identifies that few-shot examples can cause hallucinations (detached, oscillatory, off-target translations) and notes that this motivates further study on their influence.
- Why unresolved: While the problem is identified, the severity and solutions are not explored in depth.
- What evidence would resolve it: Human evaluation of hallucination types and severity, and experiments testing filtering, example selection, or model constraints to reduce hallucinations.

### Open Question 4
- Question: Does the order or diversity of few-shot examples during finetuning affect the recovery of few-shot capabilities?
- Basis in paper: [inferred] The paper uses random sampling of 0-5 examples per training instance but does not explore the impact of example order or diversity.
- Why unresolved: The study fixes the sampling strategy and does not test variations that might influence performance.
- What evidence would resolve it: Controlled experiments varying example diversity, ordering, or weighting during finetuning and measuring downstream few-shot performance.

## Limitations

- Instruction Template Dependence: The reported improvements hinge critically on specific instruction templates, which are not fully disclosed.
- Domain Adaptation Trade-off: The paper shows LoRA improves general translation but harms few-shot performance on specialized domains, with limited evidence that mixed training preserves few-shot capability in these domains.
- Limited Language Pair Analysis: Detailed analysis and visualizations are primarily shown for English→German, leaving uncertainty about generalization across all 10 language pairs.

## Confidence

**High Confidence**: The core claim that LoRA reduces trainable parameters by ~50x (134M vs 6.7B) while maintaining translation quality is well-supported by the provided quantitative comparisons and is consistent with established LoRA literature.

**Medium Confidence**: The claim that mixing few-shot examples during finetuning recovers few-shot capabilities while retaining finetuning benefits is supported by the presented data, but the mechanism explanation is speculative and the specialized domain performance remains unevaluated.

**Low Confidence**: The assertion that finetuning "eliminates the need for post-processing or example selection" is weakly supported - while the paper shows improved zero-shot performance, it doesn't provide systematic evidence that these steps are universally unnecessary across different translation scenarios.

## Next Checks

1. **Specialized Domain Evaluation**: Replicate the few-shot recovery experiments specifically on specialized domains (Medical, Law, TICO, WMT Chat) to verify whether the mixed training approach preserves few-shot capabilities in domain-specific translation tasks.

2. **Instruction Template Ablation**: Conduct controlled experiments varying instruction template formats (different prompt structures, different ways of presenting few-shot examples) to determine the sensitivity of the finetuning approach to template design and identify which template features are critical for maintaining few-shot capabilities.

3. **Language Pair Generalization**: Extend the detailed analysis of few-shot performance degradation and recovery to all 10 language pairs in the study, not just English→German, to confirm that the observed patterns hold across diverse language combinations with different linguistic distances from English.