---
ver: rpa2
title: 'SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning'
arxiv_id: '2310.04918'
source_url: https://arxiv.org/abs/2310.04918
tags:
- pruning
- sparsity
- loss
- noise
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sparse Entropic Wasserstein Regression (SWAP),
  a novel network pruning technique that addresses noisy gradients in computing the
  empirical Fisher Information Matrix. SWAP leverages Entropic Wasserstein regression
  to incorporate neighborhood interpolation across data points, effectively mitigating
  noise while preserving covariance information.
---

# SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning

## Quick Facts
- arXiv ID: 2310.04918
- Source URL: https://arxiv.org/abs/2310.04918
- Reference count: 40
- Key outcome: SWAP achieves comparable performance with state-of-the-art pruning algorithms, and outperforms them when network size or target sparsity is large, or when noisy gradients are present. Notably, SWAP achieves a 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.

## Executive Summary
This paper introduces Sparse Entropic Wasserstein Regression (SWAP), a novel network pruning technique that addresses noisy gradients in computing the empirical Fisher Information Matrix. SWAP leverages Entropic Wasserstein regression to incorporate neighborhood interpolation across data points, effectively mitigating noise while preserving covariance information. The method achieves comparable performance with state-of-the-art pruning algorithms and demonstrates superior performance under challenging conditions such as large networks, high sparsity targets, and noisy gradients.

## Method Summary
SWAP uses entropic Wasserstein regression to prune neural networks by computing a transportation plan that interpolates gradients across data points. The method formulates pruning as an optimization problem where weights are updated via stochastic gradient descent with iterative hard thresholding to enforce sparsity. The entropic regularization parameter ε controls the trade-off between noise reduction and covariance preservation. SWAP implicitly performs gradient averaging through optimal transport without explicitly computing convex combination coefficients.

## Key Results
- SWAP achieves 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of network parameters remaining
- Outperforms state-of-the-art pruning algorithms when network size or target sparsity is large
- Demonstrates superior robustness to noisy gradients compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SWAP reduces noisy gradient effects by averaging gradients across nearby data points via entropic Wasserstein regression.
- Mechanism: The OT plan Π creates neighborhood interpolation by assigning non-zero transport probabilities between data points that are geometrically close, effectively smoothing gradient noise.
- Core assumption: Noisy gradients are spatially distributed such that averaging over nearby gradients reduces noise while preserving covariance structure.
- Evidence anchors: [abstract] "This is analytically showcased to excel in noise mitigation by adopting neighborhood interpolation across data points." [section] "In practical terms, for each data point xi, only a subset of {yi}n i=1 will transport a substantial mass, rather than the entire set. This behavior of Π effectively defines n 'neighborhoods' for each data point xi within the empirical distribution of y."
- Break condition: If noisy gradients are uniformly distributed with no local structure, neighborhood averaging provides no noise reduction benefit.

### Mechanism 2
- Claim: The entropic regularization parameter ε controls the trade-off between noise reduction and covariance preservation.
- Mechanism: Larger ε values increase entropy in the transport plan, creating broader neighborhoods that average more gradients (more noise reduction) while smaller ε values create sparse transport plans that preserve individual gradient information (more covariance preservation).
- Core assumption: The optimal trade-off between noise reduction and covariance preservation depends on the specific noise characteristics and dataset size.
- Evidence anchors: [abstract] "The unique strength of the Wasserstein distance is its intrinsic ability to strike a balance between noise reduction and covariance information preservation." [section] "Specifically, increasing the value of ε amplifies the impact of the entropy term. This change broadens the neighborhoods, drawing more data points into the fold of the associated convex hulls."
- Break condition: If noise and signal have similar spatial distributions, increasing ε indiscriminately reduces both noise and useful signal.

### Mechanism 3
- Claim: SWAP implicitly performs gradient averaging without explicitly computing convex combination coefficients.
- Mechanism: By solving the OT problem with entropic regularization, SWAP automatically computes the optimal transport plan that weights nearby gradients, achieving the same effect as manual gradient averaging but more efficiently.
- Core assumption: The OT optimization naturally discovers the optimal gradient averaging weights without explicit density estimation or nearest-neighbor search.
- Evidence anchors: [abstract] "Notably, the sparse LR formulation is merely a specific instance of ours." [section] "Importantly, this seamless trade-off eludes the combination of the Euclidean distance with gradient averaging. The reason is, the original covariance information will inevitably be lost in the formulation equation 13, irrespective of the chosen averaging method."
- Break condition: If the computational overhead of solving OT becomes prohibitive for very large networks, explicit averaging methods might be preferred.

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: SWAP uses entropic Wasserstein regression to measure and minimize the distance between gradient distributions before and after pruning.
  - Quick check question: What is the difference between computing Euclidean distance between individual gradients versus computing Wasserstein distance between gradient distributions?

- Concept: Fisher Information Matrix and Empirical Estimation
  - Why needed here: The pruning relies on approximating the Hessian using the empirical Fisher Information Matrix computed from gradients.
  - Quick check question: How does the empirical Fisher Information Matrix relate to the true Fisher Information Matrix, and why is this approximation used in network pruning?

- Concept: Entropic Regularization in Optimization
  - Why needed here: The entropy term in the OT formulation controls the smoothness of the transport plan and enables efficient computation.
  - Quick check question: What role does the entropic regularization parameter ε play in balancing computational efficiency and solution quality in optimal transport problems?

## Architecture Onboarding

- Component map: Pre-trained model weights (w̄) -> Gradient computation module (produces G) -> OT solver (computes Π) -> Weight optimization loop (SGD + IHT) -> Sparsity controller
- Critical path: 1. Compute gradients G from batch 2. Project gradients to 1D (x = Gw, y = Gw̄) 3. Compute OT plan Π using Sinkhorn-Knopp or closed-form 4. Compute gradient ∇Q = G⊤(Π(Gw - Gw̄)) + λ(w - w̄) 5. Update weights with SGD 6. Apply IHT to enforce sparsity 7. Repeat until target sparsity reached
- Design tradeoffs: ε vs. noise reduction: Higher ε increases noise reduction but may over-smooth useful signal; Sample size vs. computation: Larger batches improve Fisher estimation but increase computation; Sparsity schedule: Gradual pruning preserves accuracy better than aggressive pruning
- Failure signatures: If accuracy drops dramatically: Check if ε is too high causing over-smoothing; If training becomes unstable: Verify that gradients are being computed correctly and that the OT solver is converging; If memory usage is excessive: Check batch size and gradient storage
- First 3 experiments: 1. Run SWAP with ε = 0 (should reduce to LR baseline) and verify it matches LR performance 2. Vary ε from 0.1 to 10 and plot accuracy vs. ε to find optimal value 3. Compare SWAP with standard magnitude pruning on a small network to establish baseline improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of ε in the entropic regularization term affect the robustness of SWAP to noisy gradients in real-world federated learning scenarios?
- Basis in paper: [explicit] The paper states that increasing ε amplifies the impact of the entropy term, broadening the neighborhoods and drawing more data points into the fold of the associated convex hulls. It also mentions that a larger dataset of high-quality training samples diminishes concerns over gradient noise.
- Why unresolved: The paper does not provide empirical evidence on how different ε values perform in real-world federated learning scenarios with noisy gradients. It only discusses the theoretical impact of ε on neighborhood size and gradient noise reduction.
- What evidence would resolve it: Experiments comparing SWAP's performance with different ε values in various federated learning scenarios with different levels and types of gradient noise.

### Open Question 2
- Question: What is the impact of SWAP's performance on extremely large neural networks (e.g., with trillions of parameters) compared to state-of-the-art pruning algorithms?
- Basis in paper: [inferred] The paper mentions that SWAP outperforms state-of-the-art algorithms when the network size is large, but it does not provide specific results for extremely large networks like GPT-4.
- Why unresolved: The experiments in the paper are conducted on relatively small networks (MLPNet, ResNet20, and MobileNetV1). The performance of SWAP on extremely large networks remains untested.
- What evidence would resolve it: Experiments applying SWAP to prune extremely large neural networks and comparing its performance with state-of-the-art algorithms in terms of accuracy, testing loss, and computational efficiency.

### Open Question 3
- Question: How does SWAP's neighborhood interpolation mechanism compare to other gradient noise reduction techniques in terms of preserving important covariance information?
- Basis in paper: [explicit] The paper claims that SWAP's neighborhood interpolation mechanism strikes a balance between gradient noise reduction and covariance capturing, but it does not provide a direct comparison with other gradient noise reduction techniques.
- Why unresolved: The paper does not compare SWAP's performance with other gradient noise reduction techniques that also aim to preserve covariance information.
- What evidence would resolve it: Experiments comparing SWAP's performance with other gradient noise reduction techniques on various datasets and network architectures, focusing on the trade-off between noise reduction and covariance preservation.

## Limitations

- The empirical Fisher Information Matrix estimation relies on gradient samples, which may introduce bias if the batch size is insufficient
- The entropic regularization parameter ε requires careful tuning, and the optimal value may vary significantly across different network architectures and sparsity levels
- The computational overhead of solving optimal transport problems may become prohibitive for very large-scale networks

## Confidence

- **High Confidence:** SWAP's theoretical framework for noise mitigation through entropic Wasserstein regression is well-established. The mathematical formulation and its connection to existing methods (like linear regression as a special case) are rigorously proven.
- **Medium Confidence:** The experimental results showing SWAP's superiority over state-of-the-art pruning methods are compelling, but they are limited to three specific network architectures and datasets. Generalization to other architectures remains to be tested.
- **Low Confidence:** The claim that SWAP "excels in noise mitigation" is supported by theoretical analysis but lacks direct empirical validation of noise reduction on noisy gradient datasets.

## Next Checks

1. **Noise Injection Experiment:** Systematically inject controlled noise into gradients and measure SWAP's noise reduction effectiveness compared to baseline methods.
2. **Architecture Generalization:** Apply SWAP to additional network architectures (e.g., Transformers, EfficientNet) and diverse datasets to validate broader applicability.
3. **Computational Scalability:** Evaluate SWAP's performance and computational overhead on large-scale models (e.g., ResNet50, BERT) to assess practical limitations.