---
ver: rpa2
title: Discovering Knowledge-Critical Subnetworks in Pretrained Language Models
arxiv_id: '2310.03084'
source_url: https://arxiv.org/abs/2310.03084
tags:
- knowledge
- subnetwork
- language
- target
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a method to discover sparse subnetworks\
  \ within pretrained language models that are responsible for expressing specific\
  \ relational knowledge. The authors propose a multi-objective differentiable weight\
  \ masking scheme that suppresses the expression of target knowledge triplets while\
  \ maintaining the model\u2019s general language modeling abilities and performance\
  \ on other relational knowledge."
---

# Discovering Knowledge-Critical Subnetworks in Pretrained Language Models

## Quick Facts
- arXiv ID: 2310.03084
- Source URL: https://arxiv.org/abs/2310.03084
- Reference count: 40
- Primary result: Identifies sparse subnetworks (98.6% sparsity) responsible for expressing specific relational knowledge in pretrained language models

## Executive Summary
This paper introduces a method to discover sparse subnetworks within pretrained language models that are responsible for expressing specific relational knowledge. The authors propose a multi-objective differentiable weight masking scheme that suppresses the expression of target knowledge triplets while maintaining the model's general language modeling abilities and performance on other relational knowledge. When these knowledge-critical subnetworks are removed, the remaining model struggles to express the suppressed knowledge but retains most of its original capacity, as evidenced by significant increases in perplexity on target knowledge (e.g., 257% for GPT2-small) and negligible changes on control datasets.

## Method Summary
The method uses a multi-objective differentiable weight masking scheme to identify knowledge-critical subnetworks in pretrained language models. It optimizes for three objectives: suppressing target knowledge expression, maintaining general language modeling ability, and maximizing sparsity. The approach uses a straight-through estimator to learn binary masks over model parameters, identifying which weights are critical for expressing specific knowledge triplets. The discovered subnetworks can then be removed to create a smaller model that has lost the ability to express the target knowledge while retaining most other capabilities.

## Key Results
- Knowledge-critical subnetworks achieve high sparsity (98.6% on average) while being responsible for expressing target knowledge
- Removing these subnetworks degrades performance on downstream tasks requiring the suppressed knowledge
- The method successfully disentangles knowledge-specific parameters from general language modeling capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-objective differentiable weight masking scheme can identify sparse subnetworks that are essential for expressing specific relational knowledge triplets.
- Mechanism: The mask learning process optimizes for suppression of target knowledge triplets while maintaining performance on other relational knowledge and general language modeling. This disentanglement identifies the knowledge-critical subnetwork as the masked portion of the original model.
- Core assumption: Knowledge in pretrained language models is localized to sparse computational subgraphs rather than being distributed throughout the entire network.

### Mechanism 2
- Claim: The discovered knowledge-critical subnetworks achieve high sparsity (98.6% on average) while still being responsible for expressing target knowledge.
- Mechanism: The sparsity regularization term in the objective function encourages the subnetwork to be as sparse as possible, identifying only the most critical parameters for knowledge expression.
- Core assumption: Only a small fraction of parameters are truly essential for expressing any specific piece of knowledge.

### Mechanism 3
- Claim: Removing knowledge-critical subnetworks degrades performance on downstream tasks that require the suppressed knowledge.
- Mechanism: The knowledge-critical subnetwork is essential for transferring the specific knowledge to downstream tasks. When removed, the model cannot access this knowledge during finetuning, leading to performance drops on tasks requiring that knowledge.
- Core assumption: Knowledge transfer to downstream tasks relies on the same parameters that express knowledge in the original model.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: The paper works with GPT2 models, which are transformer-based language models. Understanding how attention and feed-forward layers work is crucial for understanding how knowledge might be encoded.
  - Quick check question: What is the difference between the attention mechanism and the feed-forward network in a transformer layer?

- Concept: Knowledge graph representation and verbalization
  - Why needed here: The paper uses knowledge graphs (KGs) as the source of target knowledge, which must be verbalized into natural language for prompting the language model.
  - Quick check question: How would you verbalize the knowledge triplet (dog, IsA, animal) using the template "{article} {h} is {article} {t}"?

- Concept: Differentiable weight masking and straight-through estimators
  - Why needed here: The core methodology involves learning binary masks over model parameters using differentiable techniques.
  - Quick check question: What is the purpose of the straight-through estimator in the context of learning binary masks?

## Architecture Onboarding

- Component map: Knowledge graph sampler -> Verbalization module -> GPT2 model variants -> Differentiable mask learner -> Evaluation pipeline

- Critical path:
  1. Sample and verbalize knowledge triplets from KGs
  2. Initialize binary mask parameters with straight-through estimator
  3. Optimize mask using multi-objective loss function
  4. Evaluate discovered subnetworks on suppression and maintenance criteria
  5. Remove subnetworks and test impact on target knowledge and maintenance datasets
  6. Test downstream task transfer with finetuning

- Design tradeoffs:
  - Masking only top 50% of transformer layers vs. all layers (tradeoff between effectiveness and computational cost)
  - Number of maintenance objectives (tradeoff between robustness and optimization complexity)
  - Mask sparsity target (tradeoff between finding truly critical parameters and potentially missing some)

- Failure signatures:
  - High âˆ† PPL on CONTROL KG or CONTROL LM indicates the subnetwork is not knowledge-specific
  - Low sparsity in discovered subnetworks suggests the optimization is not finding truly critical parameters
  - No performance drop on downstream tasks after subnetwork removal indicates the knowledge may not be localized

- First 3 experiments:
  1. Run mask learning on GPT2-small with a small WordNet-derived TARGET KG and verify the basic suppression and maintenance criteria
  2. Test the randomly masked baseline at the same sparsity level to validate the effectiveness of the discovered subnetworks
  3. Evaluate the remaining model on CONTROL KG and CONTROL LM datasets to confirm maintenance of general abilities

## Open Questions the Paper Calls Out
- What is the precise mechanism by which knowledge-critical subnetworks encode and express specific relational knowledge?
- How does the model recover or adapt when knowledge-critical subnetworks are removed, especially in terms of long-term performance and adaptation?
- Are there alternative methods to identify knowledge-critical subnetworks that might be more efficient or yield different insights?
- How do knowledge-critical subnetworks vary across different model architectures or scales?
- What are the ethical implications of manipulating knowledge-critical subnetworks, especially in terms of bias and fairness?

## Limitations
- Generalizability to other knowledge types beyond relational knowledge from WordNet and ConceptNet
- Computational overhead of the mask learning process for larger models
- Limited exploration of alternative knowledge encoding hypotheses beyond localization

## Confidence
- High confidence in the technical implementation of the differentiable weight masking methodology
- Medium confidence in the knowledge localization claim due to potential alternative explanations
- Medium confidence in the 98.6% sparsity claim given the specific parameters and optimization schedules required

## Next Checks
1. Apply the methodology to procedural knowledge or numerical reasoning tasks to test cross-domain generalization
2. Create semantically equivalent but syntactically different expressions of knowledge triplets to evaluate adversarial robustness
3. Systematically remove multiple discovered subnetworks and test for recovery through finetuning to analyze redundancy