---
ver: rpa2
title: 'CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in
  On-Demand Food Delivery Services'
arxiv_id: '2308.08446'
source_url: https://arxiv.org/abs/2308.08446
tags:
- spatiotemporal
- gid00068
- cspm
- user
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a contrastive spatiotemporal preference model
  (CSPM) to improve click-through rate (CTR) prediction in on-demand food delivery
  (OFD) services. Unlike traditional e-commerce platforms, OFD services require modeling
  user behaviors that are highly sensitive to location and time due to delivery constraints
  and regional supply.
---

# CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services

## Quick Facts
- arXiv ID: 2308.08446
- Source URL: https://arxiv.org/abs/2308.08446
- Reference count: 20
- Primary result: CSPM achieves state-of-the-art CTR prediction performance on large-scale industrial OFD datasets with 0.88% CTR lift and 1.0% purchase rate increase per impression when deployed on Ele.me.

## Executive Summary
This paper addresses the challenge of click-through rate (CTR) prediction in on-demand food delivery (OFD) services, where user behaviors are highly sensitive to location and time due to delivery constraints. Traditional methods focus on historical behavior sequences but fail to capture complex spatiotemporal patterns in other features. The authors propose a Contrastive Spatiotemporal Preference Model (CSPM) that learns spatiotemporal activation representations through contrastive learning on search actions (query, location, time), then extracts explicit preferences from user behavior sequences and implicit preferences from context and item features through specialized modules. The model is evaluated on two large-scale industrial datasets and achieves state-of-the-art performance, with successful deployment on Alibaba's Ele.me platform delivering significant business impact.

## Method Summary
CSPM introduces a novel approach to CTR prediction in OFD services by learning spatiotemporal activation representations (SAR) through contrastive learning on search actions, then using these representations to extract explicit preferences from user behavior sequences (via SAR-gated attention) and implicit preferences from other features (via gating networks). The model jointly optimizes CTR prediction and contrastive learning objectives, balancing prediction accuracy with representation quality. Key components include CSRL for SAR generation, StPE for explicit spatiotemporal preference extraction using multi-head attention, and StIF for implicit preference capture through gating networks.

## Key Results
- CSPM achieves state-of-the-art AUC performance on two large-scale industrial datasets (Real-OFD Public Dataset and Ele.me logs)
- The model demonstrates significant business impact with 0.88% lift in CTR and 1.0% increase in purchase rate per impression on Ele.me
- CSPM outperforms multiple baselines including Wide & Deep, DCN, DeepFM, DIN, DIEN, AutoInt, StEN, BASM, and TRISAN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning of spatiotemporal activation representations (SAR) improves generalization by learning invariant patterns across semantically similar search states.
- Mechanism: CSRL module creates positive pairs from search actions with close geohashes and similar timestamps, and negative pairs from random samples. Triplet loss pushes representations of similar states closer while separating dissimilar ones.
- Core assumption: Search actions with similar queries, nearby locations, and close timestamps imply similar user needs and preferences.
- Evidence anchors: [abstract] "CSRL utilizes a contrastive learning framework to generate a spatiotemporal activation representation (SAR) for the search action." [section 2.2] "We introduce a CSRL module. This module assumes that representations of semantically similar search queries under close geohashes and within the same time period should be close together in the representation space, while dissimilar search states should be far apart."

### Mechanism 2
- Claim: SAR-gated attention in StPE extracts explicit spatiotemporal preferences from user behavior sequences by aligning current search context with historical actions.
- Mechanism: StPE uses multi-head attention where SAR serves as the query to match against user behavior sequence embeddings (keys and values), activating relevant historical preferences.
- Core assumption: Current search context (query, location, time) determines which historical behaviors are most relevant for predicting current CTR.
- Evidence anchors: [abstract] "StPE employs SAR to activate users' diverse preferences related to location and time from the historical behavior sequence field, using a multi-head attention mechanism." [section 2.3] "We propose the StPE module, which leverages the SAR of search state from CSRL to query the behavior sequence."

### Mechanism 3
- Claim: StIF captures implicit spatiotemporal effects in non-behavioral features by using SAR as gating signal to modulate feature importance.
- Mechanism: StIF applies a learnable gating network (GateNet) where SAR element-wise multiplies feature importance weights, selectively amplifying features with latent spatiotemporal effects.
- Core assumption: Seemingly unrelated features (e.g., subsidies) can exhibit spatiotemporal characteristics that affect CTR prediction.
- Evidence anchors: [abstract] "StIF incorporates SAR into a gating network to automatically capture important features with latent spatiotemporal effects." [section 2.4] "StIF module incorporates SAR into a gating network to automatically capture important features with latent spatiotemporal effects."

## Foundational Learning

- Concept: Contrastive learning and triplet loss
  - Why needed here: Enables learning of robust spatiotemporal representations by pulling together similar search states and pushing apart dissimilar ones, which is critical for capturing location/time-sensitive preferences.
  - Quick check question: How does triplet loss differ from standard classification loss in terms of what it optimizes?

- Concept: Multi-head attention mechanism
  - Why needed here: Allows parallel extraction of multiple types of spatiotemporal preferences from user behavior sequences, with SAR determining which historical behaviors are relevant for the current search context.
  - Quick check question: What is the role of Q, K, V matrices in multi-head attention and how do they relate to SAR in this architecture?

- Concept: Gating networks and feature importance weighting
  - Why needed here: Enables selective amplification of features with latent spatiotemporal effects that are not explicitly spatiotemporal but influence CTR prediction.
  - Quick check question: How does element-wise multiplication of SAR with feature importance weights differ from concatenation or addition in gating architectures?

## Architecture Onboarding

- Component map: Input features → Embedding layer → CSRL (SAR generation via contrastive loss) → StPE (attention-based preference extraction) → StIF (gating-based implicit preference capture) → Concatenation → MLP → CTR prediction → Cross-entropy + contrastive loss

- Critical path: Search state features → CSRL → SAR → StPE + StIF → Final prediction

- Design tradeoffs: Joint optimization of CTR prediction and contrastive loss balances prediction accuracy with representation quality; however, it introduces additional hyperparameters (α, margin m) that require tuning.

- Failure signatures: Poor SAR quality manifests as weak performance improvements from StPE/StIF; overfitting in StIF gating network shows as degraded generalization on holdout sets; contrastive loss dominating CTR loss shows as reduced prediction accuracy.

- First 3 experiments:
  1. Remove CSRL module and train with only StPE+StIF to measure contribution of SAR quality to overall performance.
  2. Vary α parameter in loss function (e.g., 0.1, 0.5, 0.9) to find optimal balance between CTR and contrastive objectives.
  3. Compare performance with different negative sampling strategies in CSRL (e.g., random vs. hardest negatives) to assess robustness of contrastive learning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CSPM compare to other state-of-the-art spatiotemporal models like StEN, BASM, and TRISAN on the Real-OFD Public Dataset?
- Basis in paper: [explicit] The paper mentions comparing CSPM with these models but does not provide detailed performance metrics for each model on the Real-OFD dataset.
- Why unresolved: The paper provides overall performance metrics but lacks a detailed breakdown of how CSPM compares to each individual model on the Real-OFD dataset.
- What evidence would resolve it: A table or detailed analysis showing the AUC scores of CSPM and each spatiotemporal model on the Real-OFD dataset would resolve this question.

### Open Question 2
- Question: What are the specific contributions of the CSRL, StPE, and StIF modules to the overall performance of CSPM, and how do they interact with each other?
- Basis in paper: [explicit] The paper mentions ablation studies but does not provide a detailed analysis of how each module contributes to the overall performance and their interactions.
- Why unresolved: The ablation studies show the effectiveness of each module, but the paper does not delve into the specific interactions and combined effects of the modules.
- What evidence would resolve it: A detailed analysis or visualization showing the individual and combined contributions of CSRL, StPE, and StIF to CSPM's performance would resolve this question.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the margin in the triplet loss and the weight for balancing CTR prediction and contrastive loss, affect the performance of CSPM?
- Basis in paper: [explicit] The paper mentions these hyperparameters but does not provide an analysis of their impact on model performance.
- Why unresolved: The paper does not explore the sensitivity of CSPM's performance to these hyperparameters, leaving their optimal values and impact unclear.
- What evidence would resolve it: A sensitivity analysis or ablation study varying these hyperparameters and their effects on model performance would resolve this question.

## Limitations

- Data representativeness: The model was validated primarily on Alibaba's Ele.me platform data, which may not generalize to other OFD services with different user behaviors or geographic distributions.
- Ablation depth: The ablation studies don't fully isolate the contribution of each component or precisely quantify the marginal contribution of SAR quality versus the attention and gating mechanisms.
- Contrastive learning assumptions: The effectiveness of CSRL depends heavily on the assumption that similar geohashes and timestamps imply similar user preferences, which may not hold in regions with heterogeneous demand patterns.

## Confidence

- High confidence: The core mechanism of using spatiotemporal representations to condition attention and gating operations is well-grounded in established deep learning principles. The reported AUC improvements over multiple baselines are statistically significant and the industrial deployment results provide strong validation.
- Medium confidence: The specific implementation details for positive/negative pair construction in CSRL and the exact gating network architecture in StIF are described but not fully specified, making exact reproduction challenging without additional information.
- Low confidence: The generalizability of the 0.88% CTR lift to other OFD platforms remains uncertain without cross-platform validation studies.

## Next Checks

1. Cross-platform generalization test: Evaluate CSPM on at least two additional OFD platforms with different geographic distributions and user demographics to assess robustness of the spatiotemporal modeling approach.

2. Temporal robustness analysis: Measure model performance during unusual periods (holidays, extreme weather, major promotions) to validate whether the contrastive learning assumptions about spatiotemporal similarity hold under stress conditions.

3. Component isolation study: Conduct a detailed ablation study varying each module independently while controlling for representation quality, to precisely quantify the marginal contribution of SAR quality versus the attention and gating mechanisms themselves.