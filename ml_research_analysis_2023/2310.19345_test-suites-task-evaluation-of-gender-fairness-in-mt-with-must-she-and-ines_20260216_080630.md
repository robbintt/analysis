---
ver: rpa2
title: 'Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES'
arxiv_id: '2310.19345'
source_url: https://arxiv.org/abs/2310.19345
tags:
- gender
- translation
- systems
- ines
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors evaluate gender bias and inclusivity in MT systems
  using two newly created test suites: MuST-SHE WMT23 for en-de and INES for de-en.
  MuST-SHE WMT23 assesses the ability to correctly translate feminine and masculine
  gender forms in naturalistic contexts, while INES measures the generation of inclusive
  English translations that avoid masculine generics.'
---

# Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES
## Quick Facts
- arXiv ID: 2310.19345
- Source URL: https://arxiv.org/abs/2310.19345
- Reference count: 24
- Primary result: MT systems show reasonable performance on unambiguous gender cases but struggle with ambiguous cases and inclusive language generation

## Executive Summary
This paper evaluates gender bias and inclusivity in machine translation systems using two test suites: MuST-SHE WMT23 for en-de and INES for de-en. The evaluation assesses how well systems translate feminine and masculine gender forms in naturalistic contexts and generate inclusive English translations that avoid masculine generics. Results show that while systems perform reasonably well on unambiguous gender cases, they struggle significantly with ambiguous gender scenarios and the generation of inclusive language forms.

The study introduces the Inclusivity Index metric for English to better capture the generation of alternative inclusive terms beyond those annotated in the test suite. Across 11-13 MT systems evaluated, the findings highlight the ongoing challenges in achieving gender fairness and inclusivity in machine translation, particularly for ambiguous cases and non-binary inclusive terms.

## Method Summary
The evaluation uses two newly created test suites: MuST-SHE WMT23 (200 segments) for en-de and INES (162 sentences) for de-en. MuST-SHE WMT23 assesses translation of feminine and masculine gender forms in naturalistic contexts, while INES measures generation of inclusive English translations avoiding masculine generics. The evaluation employs automatic metrics (Term Coverage, Gender Accuracy for MuST-SHE; Term Coverage, Inclusivity Accuracy, and Inclusivity Index for INES) combined with human validation for robustness. MT system translations are obtained from the WMT-2023 General Translation Task and compared against annotated reference translations with gender information.

## Key Results
- MT systems achieve reasonable and comparable performance in translating both feminine and masculine gender forms for unambiguous cases in MuST-SHE WMT23
- Systems default to masculine forms in ambiguous cases where input does not specify gender
- Generation of inclusive language forms remains challenging, with most systems producing over 40% non-inclusive translations in INES
- The Inclusivity Index metric better captures alternative inclusive translations compared to accuracy-based metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systems can correctly translate gender-marked words when provided with unambiguous cues in the input.
- Mechanism: When the source sentence includes gender-specific information (e.g., pronouns, gender-marked nouns), MT systems use this cue to select the correct masculine or feminine form in the target language.
- Core assumption: The model has learned to attend to and utilize explicit gender cues during translation.
- Evidence anchors:
  - [abstract] "Our results indicate that systems achieve reasonable and comparable performance in correctly translating both feminine and masculine gender forms for naturalistic gender phenomena."
  - [section 2.3] "Moving onto the overall accuracy scores (All-Acc), we can see that – while there is still room for improvement – all of the evaluated MT systems are reasonably good at translating gender"
- Break condition: Input lacks gender cues or provides conflicting gender information.

### Mechanism 2
- Claim: Systems default to masculine forms when gender information is ambiguous or absent in the source.
- Mechanism: In cases where the source sentence does not specify gender (e.g., first-person references without gender pronouns), systems default to masculine translations more frequently than feminine ones.
- Core assumption: The model has learned a masculine default bias from training data or linguistic patterns.
- Evidence anchors:
  - [abstract] "Instead, the generation of inclusive language forms in translation emerges as a challenging task for all the evaluated MT models"
  - [section 2.3] "However, for ambiguous cases where the input sentence does not inform about the gender form to be used in translation, we confirm a strong skew where all systems favour masculine generation almost by default"
- Break condition: Input provides explicit gender cues or systems are specifically trained/debiased to handle ambiguity.

### Mechanism 3
- Claim: The Inclusivity Index metric better captures gender-inclusive translation than accuracy-based metrics.
- Mechanism: The Inclusivity Index accounts for alternative valid inclusive translations that differ from the annotated terms, whereas accuracy metrics only consider exact matches to predefined inclusive terms.
- Core assumption: English has a restricted set of gender-marked words, so inclusive alternatives can take many forms.
- Evidence anchors:
  - [section 3.2] "English, a notional gender language, has a restricted repertoire of gender-marked – potentially N-IN – words, whereas most English nouns simply do not convey any gender distinctions"
  - [section 3.3] "the vast majority is represented by inclusive terms (e.g., <business person>/<businessman>→ entrepreneur)"
- Break condition: The metric is applied to languages with richer gender morphology or when evaluating systems with poor overall translation quality.

## Foundational Learning

- Concept: Binary gender bias in machine translation
  - Why needed here: Understanding how MT systems handle masculine vs. feminine translations is central to evaluating the test suites.
  - Quick check question: What evidence from the paper shows that systems struggle more with ambiguous gender cases than with unambiguous ones?

- Concept: Inclusive language and masculine generics
  - Why needed here: The INES test suite specifically evaluates the generation of inclusive English translations that avoid masculine generic forms.
  - Quick check question: According to the paper, what are some examples of masculine generic terms that should be replaced with inclusive alternatives?

- Concept: Evaluation metrics for gender fairness
  - Why needed here: The paper introduces specific metrics (Term Coverage, Gender Accuracy, Inclusivity Index) to measure gender translation performance and inclusivity.
  - Quick check question: How does the Inclusivity Index differ from the Inclusivity Accuracy metric, and why is it considered more suitable for evaluating English translations?

## Architecture Onboarding

- Component map: Test suites (MuST-SHE WMT23 and INES) -> Automatic evaluation scripts -> Human validation processes -> Final scores and rankings
- Critical path: 1) Create or obtain translations of test suite sentences from MT systems, 2) Run automatic evaluation scripts to compute coverage and accuracy metrics, 3) Perform human validation on out-of-coverage terms, 4) Calculate final scores and rankings
- Design tradeoffs: Using natural test data (TED talks) provides realistic evaluation but limits the number of test cases. Relying on human annotation ensures quality but increases creation time and cost.
- Failure signatures: Low term coverage indicates systems fail to generate gender-marked words. Accuracy below 50% shows systems produce wrong gender more often than correct. High out-of-coverage rates for INES suggest systems use alternative inclusive terms not annotated in the test suite.
- First 3 experiments:
  1. Run the evaluation script on a small subset of MuST-SHE WMT23 to verify metric calculations and understand the output format.
  2. Perform manual validation on out-of-coverage terms from INES to confirm the hypothesis about alternative inclusive translations.
  3. Compare the rankings produced by Inclusivity Accuracy vs. Inclusivity Index to verify which better correlates with human judgments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific neural architectures or training strategies could most effectively improve the generation of inclusive translations in English, particularly for non-binary and gender-neutral terms?
- Basis in paper: [inferred] The paper highlights the challenge of generating inclusive translations, especially for non-binary terms, and suggests that current systems struggle with this task.
- Why unresolved: The paper identifies the problem but does not propose specific solutions or architectures to address it.
- What evidence would resolve it: Empirical studies comparing different neural architectures or training strategies on benchmarks like INES, showing improved performance in generating inclusive translations.

### Open Question 2
- Question: How does the performance of machine translation systems on gender-inclusive tasks vary across different language pairs, and what linguistic features contribute to these differences?
- Basis in paper: [explicit] The paper evaluates gender bias and inclusivity for specific language pairs (en-de and de-en) but does not explore other pairs or linguistic factors.
- Why unresolved: The paper focuses on a limited set of language pairs and does not generalize findings to other languages or explore the impact of linguistic features.
- What evidence would resolve it: Comparative studies across multiple language pairs, analyzing how linguistic features like grammatical gender and gender-neutral terms affect inclusivity performance.

### Open Question 3
- Question: What are the long-term societal impacts of machine translation systems that consistently produce non-inclusive language, and how can these impacts be mitigated?
- Basis in paper: [explicit] The paper discusses the societal risks of biased and non-inclusive translation systems, such as reinforcing gender stereotypes and underrepresenting minorities.
- Why unresolved: While the paper identifies the risks, it does not explore the long-term societal impacts or propose mitigation strategies.
- What evidence would resolve it: Longitudinal studies on the societal effects of biased translations and the development of guidelines or tools to promote inclusive language in machine translation systems.

## Limitations
- Reliance on human validation introduces potential subjectivity and scalability issues
- Small test suite sizes (200 segments for MuST-SHE WMT23, 162 sentences for INES) may limit statistical significance
- Focus on only two language pairs constrains generalizability to other languages with different gender systems

## Confidence
- High confidence: Systems perform reasonably well on unambiguous gender cases in MuST-SHE WMT23
- Medium confidence: Systems default to masculine forms in ambiguous cases
- Medium confidence: Generating inclusive language remains challenging for current systems

## Next Checks
1. Calculate and report inter-annotator agreement scores for manual validation of out-of-coverage terms in INES
2. Perform significance tests on accuracy and inclusivity metrics across the 11-13 MT systems
3. Apply evaluation methodology to at least two additional language pairs with different gender systems