---
ver: rpa2
title: 'WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot
  Grounding on Wikipedia'
arxiv_id: '2305.14292'
source_url: https://arxiv.org/abs/2305.14292
tags:
- user
- wikichat
- response
- information
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents WikiChat, a few-shot LLM-based chatbot that
  is grounded with live information from Wikipedia to almost never hallucinate and
  achieve high conversationality and low latency. WikiChat generates a response from
  an LLM, retains only the grounded facts, and combines them with additional information
  it retrieves from the corpus to form factual and engaging responses.
---

# WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia

## Quick Facts
- arXiv ID: 2305.14292
- Source URL: https://arxiv.org/abs/2305.14292
- Authors: 
- Reference count: 27
- One-line primary result: Achieves 97.3% factual accuracy in simulated conversations and 97.9% in human conversations, significantly outperforming GPT-4 and retrieval-based baselines

## Executive Summary
WikiChat is a few-shot LLM-based chatbot that grounds responses in live Wikipedia information to eliminate hallucinations while maintaining high conversationality and low latency. The system combines retrieval-based and generation-based approaches, using Wikipedia passages to verify LLM-generated claims before producing responses. By distilling a GPT-4-based system into a 7B-parameter LLaMA model, WikiChat achieves significant improvements in cost, latency, and privacy while maintaining high quality. The authors demonstrate superior performance across head, tail, and recent knowledge categories compared to both retrieval-based and LLM-based baselines.

## Method Summary
WikiChat employs a 7-stage pipeline using few-shot prompts with an LLM. The system first generates queries from user input, retrieves relevant Wikipedia passages using ColBERT v2, and summarizes them into bullet points. It then generates a response, extracts claims, verifies them against retrieved evidence, and produces a draft response. A refinement stage improves conversationality metrics like relevance and naturalness. The system is evaluated using a novel hybrid methodology combining human and LLM assessments across 60 conversation topics spanning head, tail, and recent knowledge categories.

## Key Results
- Achieves 97.3% factual accuracy in simulated conversations, significantly outperforming GPT-4 and retrieval-based baselines
- Demonstrates 3.9%, 38.6%, and 51.0% improvements over GPT-4 on head, tail, and recent knowledge respectively
- Maintains 97.9% factual accuracy in human conversations while receiving significantly higher user ratings than GPT-4
- Successfully distills to a 7B-parameter LLaMA model with minimal quality loss, improving latency and cost

## Why This Works (Mechanism)

### Mechanism 1
Using both retrieval and LLM generation for knowledge curation provides broader coverage than either approach alone. Retrieval provides access to up-to-date and factual information from Wikipedia, while LLM generation can synthesize and connect information from its pre-training knowledge base. Core assumption: LLMs retain useful knowledge from pre-training that complements retrieved information. Evidence anchors: [abstract] "WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus" and [section 3.1] "We use both LLM generation and information retrieval. The output of this stage is a list of relevant bullet points."

### Mechanism 2
Fact-checking LLM-generated claims against retrieved evidence significantly reduces hallucination. Claims extracted from LLM responses are verified against Wikipedia passages using a verifier that classifies them as supported, refuted, or having insufficient information. Core assumption: Retrieved passages contain sufficient evidence to verify or refute most factual claims. Evidence anchors: [section 3.1.2] "The verification prompt (Table 14) assigns each claim to one of three classes: whether the retrieved evidence supports the claim, refutes the claim, or if there is not enough information" and [section 4.3] "We define factual accuracy (F A)of a dialogue set as the percentage of claims that are supported by Wikipedia."

### Mechanism 3
Two-stage response generation (draft then refine) improves conversationality compared to single-step generation. Initial draft response combines retrieved bullet points and verified claims, then refinement stage improves relevance, naturalness, non-repetitiveness, and temporal correctness. Core assumption: LLMs can effectively evaluate and improve their own outputs when given specific criteria. Evidence anchors: [section 3.2] "Our next step is to turn relevant retrieved paragraphs and verified claims to generate an appealing response. Our experiments show that writing the final response in one go... is challenging for LLMs" and [section 7.2] "The changes introduced in refinement make a significant improvement to the conversationality metrics, especially in reducing repetition."

## Foundational Learning

- Concept: Information retrieval systems and their evaluation metrics
  - Why needed here: WikiChat relies on ColBERT v2 for retrieving relevant Wikipedia passages, requiring understanding of retriever design and performance
  - Quick check question: What are the key differences between dense and sparse retrieval approaches, and when would each be preferable?

- Concept: Prompt engineering and few-shot learning techniques
  - Why needed here: All components of WikiChat are implemented using carefully designed few-shot prompts with an LLM
  - Quick check question: How does few-shot prompting differ from zero-shot prompting, and what are the key considerations when designing few-shot examples?

- Concept: Fact verification and evidence-based reasoning
  - Why needed here: WikiChat includes a verification step that checks LLM-generated claims against retrieved evidence
  - Quick check question: What are the main challenges in automated fact verification, and how do approaches like FEVER address them?

## Architecture Onboarding

- Component map: User input -> Query generation -> Information retrieval -> Summarization -> LLM generation -> Claim extraction -> Evidence retrieval -> Verification -> Draft response -> Refinement -> Final response
- Critical path: The verification step is critical as it filters out hallucinated claims before response generation
- Design tradeoffs: Using few-shot prompting provides flexibility but may be less consistent than fine-tuned models; retrieving 3 passages balances comprehensiveness with latency
- Failure signatures: Factual errors suggest verification failure; irrelevant responses suggest query generation or retrieval issues; repetitive responses suggest refinement failure
- First 3 experiments:
  1. Test query generation with various input types to validate time inference and relevance
  2. Evaluate verification accuracy on a sample of claims to establish baseline performance
  3. Compare draft vs. refined responses to quantify refinement impact on conversationality metrics

## Open Questions the Paper Calls Out

### Open Question 1
How does WikiChat's performance compare to other state-of-the-art retrieval-augmented language models (e.g., REALM, RARR) in terms of factuality and conversationality? Basis in paper: Inferred from the paper's focus on comparing WikiChat to existing baselines (Atlas, GPT-3.5) and the general discussion of retrieval-augmented language models. Why unresolved: The paper does not provide a direct comparison to other state-of-the-art retrieval-augmented language models like REALM or RARR. What evidence would resolve it: Conducting experiments to compare WikiChat's performance with these models on the same benchmark and evaluation metrics.

### Open Question 2
What is the impact of using different information retrieval systems (e.g., BM25, TF-IDF) on WikiChat's performance in terms of factuality and conversationality? Basis in paper: Inferred from the paper's use of ColBERT v2 as the information retrieval system and the general discussion of information retrieval in knowledge-grounded conversations. Why unresolved: The paper does not explore the impact of using different information retrieval systems on WikiChat's performance. What evidence would resolve it: Conducting experiments to compare WikiChat's performance using different information retrieval systems on the same benchmark and evaluation metrics.

### Open Question 3
How does WikiChat's performance scale with increasing conversation length and complexity? Basis in paper: Inferred from the paper's discussion of conversation length and the general challenge of maintaining factuality and conversationality in longer conversations. Why unresolved: The paper does not provide an in-depth analysis of WikiChat's performance in longer and more complex conversations. What evidence would resolve it: Conducting experiments to evaluate WikiChat's performance in conversations with varying lengths and complexities, and analyzing the impact on factuality and conversationality.

## Limitations

- The paper relies heavily on few-shot prompting without specifying exact prompt templates, making exact reproduction challenging
- Evaluation methodology uses both human and LLM assessors, but the reliability of LLM-as-a-judge for conversationality metrics remains uncertain
- System's performance on non-English Wikipedia content or other knowledge domains is unexplored

## Confidence

- **High confidence**: The architecture design and pipeline components are clearly specified and well-justified by experimental results showing improvements over baselines
- **Medium confidence**: Claims about conversationality improvements and latency reduction are supported by metrics but rely on LLM evaluation which may have biases
- **Medium confidence**: The distillation results showing minimal quality loss are plausible given similar work but lack detailed ablation studies

## Next Checks

1. **Verification robustness test**: Create a benchmark dataset of claims with known ground truth and measure the verifier's precision and recall, particularly for edge cases where evidence is ambiguous or contradictory

2. **Temporal knowledge coverage analysis**: Systematically evaluate WikiChat's performance on knowledge across different time periods (pre-training cutoff, recent years, very recent events) to identify degradation patterns

3. **Cost-benefit analysis of distillation**: Measure the actual latency, cost, and privacy improvements from distilling to 7B parameters across different deployment scenarios, including cold-start performance and memory usage