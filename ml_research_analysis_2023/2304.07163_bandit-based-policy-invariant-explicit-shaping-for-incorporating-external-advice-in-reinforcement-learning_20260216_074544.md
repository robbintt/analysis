---
ver: rpa2
title: Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice
  in Reinforcement Learning
arxiv_id: '2304.07163'
source_url: https://arxiv.org/abs/2304.07163
tags:
- agent
- advice
- policy
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Bandit-based Policy Invariant Explicit Shaping
  (Bandit-PIES), a method for incorporating external advice in reinforcement learning
  that formulates the problem as a multi-armed bandit called shaping-bandits. The
  key innovation is addressing the non-stationary nature of returns when an RL agent
  is still learning.
---

# Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2304.07163
- **Source URL**: https://arxiv.org/abs/2304.07163
- **Reference count**: 28
- **Primary result**: Proposes Bandit-PIES algorithms that achieve policy invariance while accelerating learning with good advice and avoiding bad advice

## Executive Summary
This paper addresses the challenge of incorporating external advice in reinforcement learning while maintaining policy invariance. The authors formulate this as a multi-armed bandit problem called shaping-bandits, where the agent must choose between following expert advice or its own default policy. Three algorithms are proposed: LPIES (elimination-based), UPIES (upper confidence bound), and RPIES (racing upper confidence bounds). These methods reason about the non-stationary nature of returns as the underlying RL agent learns, enabling them to avoid suboptimal advice while accelerating learning when advice is good. Experiments demonstrate that Bandit-PIES algorithms achieve policy invariance and outperform existing bandit and shaping methods across multiple environments.

## Method Summary
The method reformulates external advice incorporation as a 2-armed bandit problem where one arm follows the expert policy and the other follows the default RL policy. Three algorithms are proposed: LPIES eliminates the expert arm when its historical performance falls below the default policy, UPIES uses neural networks to estimate confidence bounds and selects arms based on upper confidence bounds, and RPIES races multiple upper confidence bounds. All methods reason about the non-stationary nature of returns as the RL agent learns. The approach is RL algorithm agnostic and simple to implement, requiring only wrapping the RL agent with the shaping bandit interface.

## Key Results
- Bandit-PIES algorithms achieve policy invariance across all tested environments
- UPIES and RPIES accelerate learning when expert advice is good
- LPIES successfully avoids bad or adversarial advice by eliminating the expert arm
- Existing bandit and shaping algorithms fail to achieve policy invariance or avoid bad advice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shaping bandit formulation reduces external advice integration to a multi-armed bandit problem where each arm corresponds to following either the expert advice or the default RL policy.
- Mechanism: At each episode, the agent selects between the expert policy (Φ arm) or the default RL policy (Q arm) based on expected returns. The reward for each arm is the cumulative return obtained by following that policy for the episode.
- Core assumption: The expected return from each arm is monotonically increasing as the underlying RL agent learns, even though the specific values are non-stationary.
- Evidence anchors:
  - [abstract]: "formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits"
  - [section 3.1]: "we formulate the problem of incorporating external advice as a 2-armed bandit that we call PIES-bandit or shaping bandit"
  - [corpus]: Weak - the corpus doesn't directly address this specific bandit formulation
- Break condition: If the underlying RL agent's expected return can decrease over time (e.g., due to catastrophic forgetting or exploration causing temporary performance drops), the monotonic assumption fails.

### Mechanism 2
- Claim: UPIES uses confidence bounds to reason about long-term consequences of following each policy, avoiding premature commitment to suboptimal advice.
- Mechanism: UPIES trains separate neural networks to estimate upper and lower confidence bounds on the expected cumulative return for each arm. It pulls the arm with the highest upper confidence bound, using Hoeffding's inequality to compute confidence intervals.
- Core assumption: The expected rewards associated with arms are monotone in the number of pulls (i.e., more experience leads to better or equal performance).
- Evidence anchors:
  - [section 3.4]: "RPIES uses separate deep neural networks to estimate the values of J_Q and J_Φ" and "RPIES applies this idea to PIES-bandits"
  - [section 3.5]: "We can now propose an upper confidence bound algorithm for PIES-Bandit, that at each round pulls the arm with the highest upper confidence bound"
  - [corpus]: Weak - the corpus doesn't discuss confidence bounds or neural network estimation for bandit problems
- Break condition: If the confidence intervals are too wide or the neural network estimation is poor, UPIES may make incorrect arm selections.

### Mechanism 3
- Claim: LPIES achieves policy invariance by eliminating the expert arm once its historical performance falls below the default RL arm, guaranteeing convergence to the optimal policy.
- Mechanism: LPIES starts by pulling both arms with equal probability. Once the average historical return from the expert arm falls below that of the default RL arm, it permanently eliminates the expert arm and continues pulling only the default RL arm.
- Core assumption: The default RL arm (Q arm) is optimal in the limit and will eventually outperform any suboptimal expert advice.
- Evidence anchors:
  - [section 3.2]: "LPIES follows a simple rule to pull between the Φ and Q arm: pull with equal probability each arm until the average of historical returns from Φ arm is less than the average of historical returns from arm Q, then eliminate the Φ arm"
  - [section 3.3]: "we assume that the expected rewards associated with the arms of PIES-bandits are monotone in the number of pulls"
  - [corpus]: Weak - the corpus doesn't specifically discuss elimination strategies in bandit problems
- Break condition: If the expert advice is actually optimal or nearly optimal, LPIES may prematurely eliminate it and miss opportunities for accelerated learning.

## Foundational Learning

- Concept: Multi-armed bandit problems
  - Why needed here: The entire shaping problem is reformulated as a 2-armed bandit where the agent must balance exploration (trying both policies) and exploitation (committing to the better policy)
  - Quick check question: What is the key difference between stationary and non-stationary bandit problems, and why does this distinction matter for shaping bandits?

- Concept: Policy invariance in reinforcement learning
  - Why needed here: The goal is to incorporate external advice without altering the optimal policy, which requires understanding what conditions preserve optimal behavior
  - Quick check question: What is the difference between potential-based reward shaping and explicit shaping in terms of policy invariance guarantees?

- Concept: Non-stationary reward distributions
  - Why needed here: The returns from each arm change over time as the underlying RL agent learns, making standard bandit algorithms inappropriate
  - Quick check question: How does the non-stationary nature of shaping bandits differ from typical non-stationary bandit problems in the literature?

## Architecture Onboarding

- Component map:
  - Shaping bandit interface -> Policy selector -> Episode execution -> Return tracker -> Confidence estimator (UPIES/RPIES) -> Elimination logic (LPIES)

- Critical path: RL agent interaction → Arm selection → Episode execution → Return observation → Policy update → Next episode

- Design tradeoffs:
  - Simplicity vs. performance: LPIES is simplest but may miss acceleration opportunities; UPIES is most complex but can better exploit good advice
  - Exploration vs. exploitation: Different algorithms balance this tradeoff differently
  - Computational overhead: UPIES requires training neural networks; LPIES has minimal overhead

- Failure signatures:
  - UPIES: Poor neural network training, overly conservative confidence intervals, or failure to adapt to changing reward distributions
  - RPIES: Similar to UPIES but with racing logic; may eliminate good arms too aggressively
  - LPIES: May eliminate good expert advice too early or fail to adapt to changing conditions

- First 3 experiments:
  1. Two-armed bandit with linearly increasing reward on one arm to verify bandit algorithms can distinguish between constant and increasing rewards
  2. Grid world with sub-optimal "friendly" advice to test whether algorithms can avoid getting stuck on easier but sub-optimal goals
  3. Cartpole with expert-trained DQN advice to verify performance with deep neural network function approximation and real expert advice

## Open Questions the Paper Calls Out
- How does the performance of Bandit-PIES methods change when incorporating multiple expert policies instead of just one?
- Can the monotone bandit assumption be relaxed or replaced with a more realistic model of RL agent learning curves?
- How does the performance of Bandit-PIES scale with state and action space complexity?

## Limitations
- Performance claims on complex environments like Pong have medium confidence due to limited empirical validation
- The monotonic assumption may not hold for all RL learning dynamics, particularly with catastrophic forgetting
- Limited testing on high-dimensional state spaces and continuous action spaces

## Confidence
- **High confidence**: The shaping bandit formulation as a 2-armed bandit problem, the general algorithmic framework
- **Medium confidence**: Policy invariance guarantees under the monotonicity assumption, the specific neural network architectures for confidence estimation
- **Low confidence**: Performance claims on complex environments like Pong, generalization to continuous action spaces

## Next Checks
1. Implement and compare against standard shaping methods (e.g., potential-based shaping) to quantify the benefits of the bandit approach
2. Evaluate performance when expert advice is adversarial or only partially correct to test the elimination mechanisms
3. Systematically vary key hyperparameters (learning rates, network architectures, confidence bounds) to identify robustness limits