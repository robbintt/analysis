---
ver: rpa2
title: 'RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation
  of Large Language Models'
arxiv_id: '2305.01146'
source_url: https://arxiv.org/abs/2305.01146
tags:
- tuning
- adaptation
- text
- lora
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic investigation of lightweight domain
  adaptation strategies for large language models (LLMs) in the task of radiology
  report summarization (RRS). The authors evaluate various combinations of pretrained
  models (T5, FLAN-T5, SCIFIVE, CLIN-T5-SCI, CLIN-T5) and adaptation methods (null
  prompting, prefixed prompting, in-context learning, prefix tuning, LoRA) on the
  MIMIC-III dataset.
---

# RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models

## Quick Facts
- arXiv ID: 2305.01146
- Source URL: https://arxiv.org/abs/2305.01146
- Reference count: 12
- Primary result: LoRA fine-tuning (0.32% parameter updates) of clinically-pretrained models achieves state-of-the-art radiology report summarization performance

## Executive Summary
This paper presents a systematic investigation of lightweight domain adaptation strategies for large language models in radiology report summarization. The authors evaluate five pretrained models (T5, FLAN-T5, SCIFIVE, CLIN-T5-SCI, CLIN-T5) with five adaptation methods (null prompting, prefixed prompting, in-context learning, prefix tuning, LoRA) on the MIMIC-III dataset. Their key finding is that models pretrained on clinical text and fine-tuned with LoRA achieve the best performance while requiring minimal computational resources. The study also demonstrates that increased domain adaptation through clinical pretraining and larger in-context examples improves performance, validated through a radiologist reader study.

## Method Summary
The authors systematically evaluate five T5-based language models with varying degrees of clinical domain adaptation, combined with five lightweight adaptation methods. They use the MIMIC-III dataset containing 79,790 radiology reports, split by imaging modality (CT, MR) and anatomy. The evaluation employs multiple metrics including BLEU, ROUGE-L, BERTScore, and F1-RadGraph, with qualitative validation through a radiologist reader study. The primary adaptation method investigated is LoRA (Low-Rank Adaptation), which fine-tunes only 0.32% of model parameters compared to full fine-tuning.

## Key Results
- LoRA fine-tuning of CLIN-T5 model achieves best performance across all metrics
- Clinical text pretraining (CLIN-T5) outperforms biomedical (SCIFIVE) and general (T5) pretraining
- In-context learning performance improves with more examples, except for models with minimal domain adaptation
- Increasing context leads to improved performance across almost all cases

## Why This Works (Mechanism)

### Mechanism 1
Pretraining on clinical text provides superior domain adaptation for radiology report summarization compared to biomedical or general text. Clinical text contains domain-specific terminology, report structures, and clinical context that directly align with the target task, enabling the model to better understand and generate radiology report impressions.

### Mechanism 2
Parameter-efficient fine-tuning (LoRA) achieves comparable performance to full fine-tuning while updating only 0.32% of parameters. LoRA approximates the weight updates needed for adaptation by learning low-rank decomposition matrices that are added to the frozen model weights, allowing task-specific adaptation without modifying the original parameters.

### Mechanism 3
Increasing the number of in-context examples improves performance for most models, with diminishing returns. In-context learning allows the model to condition its generation on examples of the desired output format and content, effectively providing a few-shot demonstration of the task.

## Foundational Learning

- **Domain adaptation via pretraining**: Why needed - Radiology reports have specialized terminology and structure that general language models may not capture effectively. Quick check - What is the key difference between pretraining on clinical text versus biomedical text for this task?
- **Parameter-efficient fine-tuning**: Why needed - Full fine-tuning of large language models is computationally expensive and requires large datasets. Quick check - How does LoRA achieve parameter efficiency while maintaining performance?
- **In-context learning**: Why needed - Provides a way to adapt the model to the task without any parameter updates, useful when computational resources are limited. Quick check - Why might providing too many in-context examples potentially hurt performance for a model with low domain adaptation?

## Architecture Onboarding

- **Component map**: Pretrained T5-based model (T5/FLAN-T5/SCIFIVE/CLIN-T5-SCI/CLIN-T5) -> Lightweight adaptation method (null/prefixed prompting/in-context learning/prefix tuning/LoRA) -> Evaluation metrics
- **Critical path**: Pretrain on relevant domain data → Apply lightweight adaptation method → Evaluate on radiology report summarization task
- **Design tradeoffs**: Full fine-tuning vs parameter-efficient methods (performance vs computational cost), different pretraining corpora (domain relevance vs data availability), number of in-context examples (performance vs prompt engineering complexity)
- **Failure signatures**: Poor performance on domain-specific terminology, inability to generate coherent impressions, excessive hallucination of medical information, failure to capture critical findings
- **First 3 experiments**:
  1. Evaluate all five pretrained models with null prompting on a small subset to establish baseline performance
  2. Apply prefix tuning with LoRA to the CLIN-T5 model and compare against best null prompting result
  3. Conduct ablation study on in-context examples (0, 1, 2, 4) using CLIN-T5 with prefix tuning

## Open Questions the Paper Calls Out

1. How does the performance of parameter-efficient fine-tuning methods scale with larger architecture sizes (1B+ parameters)?
2. What is the optimal number of in-context examples for in-context learning across different levels of domain adaptation?
3. How does out-of-distribution performance vary across different combinations of modalities and anatomies not represented in training data?
4. How does the quality of generated impressions vary when findings contain information from prior studies the model cannot access?
5. How do different methods for quantifying distribution distance compare in predicting OOD performance?

## Limitations
- Limited evaluation to single-institution datasets (MIMIC-III, MIMIC-CXR) without cross-institutional validation
- Radiologist reader study lacks detailed methodology and inter-rater reliability metrics
- Missing critical implementation details for faithful reproduction (preprocessing pipeline, k-NN algorithm specifics)

## Confidence

**High Confidence**: LoRA achieves comparable performance to full fine-tuning while updating only 0.32% of parameters, well-supported by systematic experimental results.

**Medium Confidence**: Clinical text pretraining provides superior domain adaptation, though comparison is limited to specific models evaluated without ablation studies.

**Low Confidence**: Value of prompt engineering and in-context learning shows positive trends but lacks sufficient empirical validation and systematic optimization.

## Next Checks

1. Test the best-performing CLIN-T5+LoRA model on radiology reports from a different healthcare system to assess domain generalization performance.

2. Conduct an ablation study comparing models pretrained on different corpora (clinical vs biomedical vs general) while keeping all other factors constant.

3. Systematically evaluate the impact of example quality, semantic similarity metrics, and number of in-context examples on performance to determine optimal prompt engineering strategies.