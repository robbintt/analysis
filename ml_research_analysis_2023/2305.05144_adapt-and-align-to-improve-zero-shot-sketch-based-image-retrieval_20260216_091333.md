---
ver: rpa2
title: Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval
arxiv_id: '2305.05144'
source_url: https://arxiv.org/abs/2305.05144
tags:
- image
- retrieval
- learning
- zero-shot
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an "Adapt and Align" approach to improve zero-shot
  sketch-based image retrieval (ZS-SBIR). It introduces lightweight domain adapters
  to better adapt the model to the sketch domain and align the learned image embedding
  with a more semantic text embedding from CLIP.
---

# Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval

## Quick Facts
- **arXiv ID**: 2305.05144
- **Source URL**: https://arxiv.org/abs/2305.05144
- **Reference count**: 40
- **Primary result**: Proposes "Adapt and Align" approach achieving state-of-the-art zero-shot sketch-based image retrieval performance across three benchmark datasets

## Executive Summary
This paper addresses the challenge of zero-shot sketch-based image retrieval (ZS-SBIR), where the goal is to retrieve photos from unseen classes based on sketch queries. The authors propose a two-pronged approach: lightweight domain adapters to better adapt pre-trained models to the sketch domain, and explicit vision-language alignment using CLIP text embeddings to transfer semantic knowledge from seen to unseen classes. The method demonstrates significant improvements over existing approaches across three benchmark datasets.

## Method Summary
The proposed "Adapt and Align" approach combines domain adapters inserted into pre-trained backbones (ResNet or ViT) with vision-language alignment using CLIP text embeddings. The adapters are bottleneck modules that allow the model to learn sketch-specific features while preserving pre-trained knowledge. For alignment, the method replaces the classifier with text features from CLIP, using dot product similarity to learn aligned visual-text embeddings. The model is trained end-to-end with both distillation loss (to preserve pre-training knowledge) and alignment loss.

## Key Results
- Achieves state-of-the-art performance on Sketchy, TU-Berlin, and QuickDraw datasets
- Demonstrates significant improvements in mAP@k and Prec@k metrics for zero-shot retrieval
- Shows that full fine-tuning outperforms adapter-only approaches
- Proves that vision-language alignment transfers knowledge effectively to unseen classes

## Why This Works (Mechanism)

### Mechanism 1: Domain Adapters
- Lightweight bottleneck modules inserted into pre-trained models learn sketch-specific abstract concepts
- Fine-tune entire model (not just adapters) for better domain gap balancing
- Evidence: Full fine-tuning shows significant improvement over parameter-limited tuning
- Break condition: May fail when domain gap is too large (e.g., QuickDraw's abstract sketches)

### Mechanism 2: Vision-Language Alignment
- CLIP text embeddings provide semantic richness to transfer knowledge across classes
- Dot product similarity with text features replaces traditional classifier
- Evidence: Alignment enables generalization from seen to unseen classes
- Break condition: Alignment fails if text encoder vocabulary doesn't cover sketch categories

### Mechanism 3: Full Fine-Tuning Strategy
- Fine-tuning entire model (not freezing pre-trained layers) better balances domain adaptation
- Combines adapter capacity with full model flexibility
- Evidence: Table 3 shows fully fine-tuned models significantly outperform partial tuning
- Break condition: May lead to overfitting if sketch domain is too different from pre-training

## Foundational Learning

- **Concept: Zero-shot learning (ZSL)**
  - Why needed: ZS-SBIR requires recognizing classes not seen during training
  - Quick check: What's the key difference between traditional SBIR and ZS-SBIR?

- **Concept: Cross-domain representation learning**
  - Why needed: Sketches and photos require a shared feature space for comparison
  - Quick check: Why is direct comparison of sketch and photo features challenging?

- **Concept: Vision-language pretraining (e.g., CLIP)**
  - Why needed: CLIP provides aligned visual-text embeddings for knowledge transfer
  - Quick check: How does CLIP's training objective benefit zero-shot scenarios?

## Architecture Onboarding

- **Component map**: Image → Backbone → Adapters → Vision encoder → CLIP text encoder → Dot product similarity → Classification
- **Critical path**: Image → Backbone → Adapters → Vision encoder → Alignment with CLIP text → Retrieval
- **Design tradeoffs**:
  - Adapter size vs. adaptation quality: Larger adapters capture more features but risk overfitting
  - Fine-tuning all vs. freezing layers: Full fine-tuning adapts better but may lose generalizable features
  - Text prompt choice: Complex prompts may help but add computational cost
- **Failure signatures**:
  - Poor unseen class retrieval: Likely adapter or alignment failure
  - Overfitting on seen classes: Possibly too much fine-tuning or complex prompts
  - High visual similarity confusion: Domain gap too large for adapters to bridge
- **First 3 experiments**:
  1. Baseline: Fine-tune backbone with classifier only (no adapters, no alignment)
  2. Add adapters only: Insert bottleneck layers, fine-tune entire model, keep classifier
  3. Add alignment only: Use CLIP text embeddings, replace classifier with similarity, no adapters

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations identified:

- How do domain adapters perform on different backbone architectures beyond ResNet and ViT?
- What is the impact of different prompt engineering techniques on vision-language alignment performance?
- How does the approach perform when unseen classes have significant visual differences from seen classes?

## Limitations

- Adapter architecture details are underspecified, making exact replication challenging
- Limited ablation studies on prompt engineering for vision-language alignment
- No comparison with concurrent vision-language models beyond CLIP
- Trade-off between adapter size and performance not thoroughly explored

## Confidence

- Retrieval performance claims: **High** (well-supported by multiple datasets)
- Adapter mechanism effectiveness: **Medium** (shown but with limited architectural details)
- Vision-language alignment benefits: **Medium** (demonstrated but could benefit from more prompt variations)

## Next Checks

1. Conduct ablation studies varying adapter bottleneck sizes (width/depth) to quantify the trade-off between parameter efficiency and retrieval performance
2. Test alternative vision-language models (e.g., BLIP, ALIGN) for alignment to assess CLIP-specific vs. general VL alignment benefits
3. Evaluate the approach on cross-dataset generalization - train on Sketchy, test on TU-Berlin/QuickDraw to measure true zero-shot transfer capability