---
ver: rpa2
title: 'Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation
  and Tail Risk'
arxiv_id: '2304.04341'
source_url: https://arxiv.org/abs/2304.04341
tags:
- regret
- tail
- optimal
- risk
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the optimal trade-off between regret expectation
  and tail risk in stochastic multi-armed bandits. The authors characterize how the
  order of expected regret exactly affects the decaying rate of regret tail probability,
  both in worst-case and instance-dependent scenarios.
---

# Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk

## Quick Facts
- arXiv ID: 2304.04341
- Source URL: https://arxiv.org/abs/2304.04341
- Authors: 
- Reference count: 40
- Key outcome: Characterizes optimal trade-off between regret expectation and tail risk in stochastic bandits, showing exponential tail decay rates with polynomial T terms, and discovers intrinsic gap between known/unknown time horizon cases.

## Executive Summary
This work establishes the fundamental trade-off between regret expectation and tail risk in stochastic multi-armed bandits. The authors prove that given any policy achieving O(T^α) worst-case regret and O(T^β) instance-dependent regret, the probability of incurring an Ω(T^δ) regret must decay exponentially at specific rates. They design new policies that achieve these optimal rates while providing explicit confidence bounds. A key finding is that knowing the time horizon T in advance creates an intrinsic gap in instance-dependent tail probabilities, while this gap disappears in the worst-case scenario.

## Method Summary
The authors develop new bandit policies using a novel bonus term structure that combines two components with different scaling regimes. The first component (η₁(T/K)^α/√n) dominates early to ensure worst-case optimality, while the second component (η₂√T^β/n) dominates later to achieve instance-dependent consistency. For the unknown T case, they use a time-dependent version that replaces T with t. The policies employ successive elimination and upper confidence bound frameworks, with extensions to linear bandits using covariance matrix-based confidence sets. The analysis relies on concentration inequalities and change of measure arguments to establish both upper and lower bounds on tail probabilities.

## Key Results
- Characterizes optimal tail probability decay rate as exp(-T^γ) for worst-case and exp(-x^γ) for instance-dependent scenarios
- Proves existence of intrinsic gap between known and unknown T cases in instance-dependent tail bounds
- Shows gap disappears in worst-case scenario but remains in instance-dependent one
- Extends results to general sub-exponential environments and stochastic linear bandits
- Connects findings to AlphaGo's Monte Carlo Tree Search methodology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The policy achieves optimal tail probability by combining two bonus terms with different scaling regimes, allowing it to handle both worst-case and instance-dependent regret simultaneously.
- Mechanism: The first bonus term scales as (T/K)^α/√n, dominating early when T is large, and the second term scales as √T^β/√n, dominating later. This creates a phase transition that ensures both worst-case optimality (O(T^α)) and instance-dependent consistency (O(T^β)) while maintaining light-tailed regret.
- Core assumption: The two bonus terms operate in complementary regimes without interfering with each other's guarantees.
- Evidence anchors:
  - [abstract]: "New policies are proposed to characterize the optimal regret tail probability for any regret threshold."
  - [section 4.1]: "The first component can be interpreted as controlling the worst-case tail risk, while the second one can be regarded as controlling the instance-dependent tail risk."
  - [corpus]: Weak evidence - neighboring papers focus on different bandit variants without addressing this specific bonus term design.
- Break condition: If the phase transition timing is mis-specified or if the two terms interfere with each other's concentration properties.

### Mechanism 2
- Claim: Knowing the time horizon T in advance significantly improves instance-dependent tail probability bounds, creating an intrinsic gap between known and unknown T scenarios.
- Mechanism: When T is known, the policy can optimize the bonus terms more precisely, leading to tighter concentration bounds. Without T, the policy must use a more conservative approach that degrades the tail probability bound from exp(-T^β) to exp(-x^β).
- Core assumption: The additional information from knowing T allows for more aggressive bonus term design without compromising safety.
- Evidence anchors:
  - [abstract]: "we discover an intrinsic gap of the optimal tail rate under the instance-dependent scenario between whether the time horizon T is known a priori or not."
  - [section 1.1]: "we diﬀerentiate between the situations where the policy knows the time horizon T in advance or not."
  - [corpus]: Weak evidence - neighboring papers don't address the T-known vs T-unknown distinction in tail probability bounds.
- Break condition: If the policy design cannot effectively utilize the T information or if the theoretical gap doesn't manifest in practice.

### Mechanism 3
- Claim: The linear bandit extension works by replacing the arm count K with dimension d and adapting the bonus terms to handle the covariance structure, maintaining the same trade-off properties.
- Mechanism: The policy uses a covariance matrix V_t that accumulates information about the action space, and the bonus terms are scaled by √(a^T V_t^-1 a) instead of 1/√n, preserving the phase transition behavior while handling the continuous action space.
- Core assumption: The linear bandit structure allows the same phase transition approach to work when properly adapted to the covariance matrix framework.
- Evidence anchors:
  - [section 5.2]: "The simple policy design that leads to optimal trade-off between optimality and light-tailed risk for the multi-armed bandit setting can be naturally extended to the linear bandit setting."
  - [section 5.2]: "We provide the Linear UCB policy (UCB-L, adapted from Abbasi-Yadkori et al. 2011 and Simchi-Levi et al. 2022)"
  - [corpus]: Weak evidence - neighboring papers focus on different linear bandit approaches without addressing this specific extension.
- Break condition: If the covariance matrix adaptation fails to preserve the concentration properties or if the dimension d introduces fundamental incompatibilities.

## Foundational Learning

- Concept: Concentration inequalities for sub-Gaussian random variables
  - Why needed here: The analysis relies heavily on bounding the probability that empirical means deviate from true means, which requires understanding sub-Gaussian concentration.
  - Quick check question: Given a σ-subGaussian random variable X with mean μ, what is the probability that |X - μ| exceeds x?

- Concept: Change of measure arguments in bandit analysis
  - Why needed here: The lower bound proofs use change of measure techniques to show that achieving both low expected regret and light tails is impossible without trade-offs.
  - Quick check question: In a two-armed bandit, how does changing the mean of one arm affect the likelihood ratio of the observed rewards?

- Concept: Confidence bounds and exploration-exploitation trade-offs
  - Why needed here: The policy design fundamentally relies on constructing appropriate confidence bounds that balance exploration (reducing uncertainty) with exploitation (maximizing rewards).
  - Quick check question: Why does using a confidence bound of the form σ√(ln t / n) ensure that the true mean is contained with high probability?

## Architecture Onboarding

- Component map:
  - Main policy modules: SE/SEwRP (Successive Elimination variants), UCB (Upper Confidence Bound), UCB-L (Linear UCB)
  - Key parameters: α (worst-case optimality), β (instance-dependent consistency), η1/η2 (bonus term scaling factors)
  - Supporting functions: Confidence bound calculation, elimination criteria, permutation sampling (for SEwRP)
  - Analysis tools: Concentration inequalities, change of measure arguments, regret decomposition

- Critical path:
  1. Initialize policy with parameters α, β, η1, η2
  2. For each time step, calculate confidence bounds using the current bonus terms
  3. Select action based on policy-specific rule (max UCB, random permutation, etc.)
  4. Update statistics and covariance matrix (for linear case)
  5. Check elimination criteria and update active set
  6. Repeat until horizon T reached

- Design tradeoffs:
  - Known vs unknown T: Known T allows more aggressive bonus terms but requires horizon specification
  - Worst-case vs instance-dependent focus: The two bonus terms represent a trade-off between these objectives
  - Exploration vs exploitation: Confidence bounds must be tight enough for exploitation but loose enough for exploration

- Failure signatures:
  - Linear regret with polynomial tail: Indicates bonus terms are too conservative
  - Suboptimal regret with heavy tails: Suggests bonus terms aren't properly phase-transitioning
  - Poor performance on specific instances: May indicate parameter mis-specification or algorithm instability

- First 3 experiments:
  1. Verify the phase transition by plotting bonus terms vs n for different T values and checking where the transition occurs
  2. Test known vs unknown T performance on a simple two-armed bandit with Gaussian rewards
  3. Validate linear bandit extension on a synthetic problem with known θ and compare against standard UCB-L

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of bonus term parameters (η1, η2) affect the constant terms in the regret bounds, and can these constants be optimized for specific problem instances?
- Basis in paper: Explicit - The paper mentions that η1 and η2 only affect the constant terms before the polynomial T term in the expected regret bounds, and that mis-specifying the variance parameter σ does not cost much with regard to the regret tail and expectation.
- Why unresolved: The paper focuses on the asymptotic behavior and does not provide specific guidance on optimizing these parameters for concrete problem instances.
- What evidence would resolve it: Empirical studies comparing the performance of the proposed policies with different parameter choices on various bandit instances, and theoretical analysis of the impact of these parameters on the constant terms.

### Open Question 2
- Question: Can the proposed policies be extended to handle more complex non-stationarity structures beyond those considered in the paper, such as adversarial rewards or contextual bandits with non-stationary contexts?
- Basis in paper: Explicit - The paper mentions that extending the results to more complex settings is a prospective future direction, and discusses the extension to stochastic linear bandits as a step in this direction.
- Why unresolved: The paper only considers extensions to stochastic bandits with structured non-stationarity, and does not address more complex scenarios like adversarial rewards or contextual bandits with non-stationary contexts.
- What evidence would resolve it: Theoretical analysis and empirical validation of the proposed policies in more complex non-stationary settings, such as adversarial bandits or contextual bandits with changing contexts.

### Open Question 3
- Question: How do the proposed policies perform in practice compared to other bandit algorithms, especially in terms of the trade-off between regret expectation and tail risk?
- Basis in paper: Explicit - The paper mentions that empirical studies would be interesting to see how the policy design works in various practical settings, but does not provide such empirical results.
- Why unresolved: The paper focuses on theoretical analysis and does not include empirical comparisons with other bandit algorithms.
- What evidence would resolve it: Empirical studies comparing the performance of the proposed policies with other bandit algorithms on various practical problems, focusing on both the expected regret and the tail risk.

### Open Question 4
- Question: Can the proposed policies be integrated into more complex decision-making frameworks, such as reinforcement learning, and what challenges might arise in such integrations?
- Basis in paper: Explicit - The paper mentions that integrating the policy design into more complex settings such as reinforcement learning is a prospective future direction.
- Why unresolved: The paper only considers the bandit setting and does not address the integration into reinforcement learning frameworks.
- What evidence would resolve it: Theoretical analysis and empirical validation of the proposed policies in reinforcement learning settings, along with identification and discussion of the challenges that arise in such integrations.

## Limitations
- The analysis assumes specific bonus term designs that may be sensitive to parameter mis-specification in practice
- The linear bandit extension's theoretical guarantees may not fully capture practical behavior across all covariance structures
- The intrinsic gap between known/unknown T scenarios requires empirical validation to confirm practical manifestation

## Confidence
- **High confidence**: Worst-case regret bounds and tail probability decay rates are well-established through concentration inequalities
- **Medium confidence**: Instance-dependent results and intrinsic gap are theoretically proven but may be sensitive to specific problem instances
- **Low confidence**: Linear bandit extension and general sub-exponential environment results require more empirical validation

## Next Checks
1. Parameter sensitivity analysis: Systematically vary α, β, η₁, and η₂ across a grid to identify regions where the phase transition breaks down or performance degrades significantly
2. Empirical gap verification: Design experiments comparing known vs unknown T policies on a range of bandit instances to verify the theoretical intrinsic gap manifests in practice
3. Linear bandit robustness test: Evaluate the linear bandit extension on problems with different covariance structures and dimensions to identify failure modes not captured by the theoretical analysis