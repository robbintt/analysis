---
ver: rpa2
title: 'LongStory: Coherent, Complete and Length Controlled Long story Generation'
arxiv_id: '2311.15208'
source_url: https://arxiv.org/abs/2311.15208
tags:
- story
- longstory
- generation
- arxiv
- coherence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of generating long, coherent,
  and complete stories of varying lengths using language models. It introduces LongStory,
  a novel approach that employs two key methodologies: the long and short-term contexts
  weight calibrator (CWC) and long story structural positions (LSP).'
---

# LongStory: Coherent, Complete and Length Controlled Long story Generation

## Quick Facts
- **arXiv ID:** 2311.15208
- **Source URL:** https://arxiv.org/abs/2311.15208
- **Reference count:** 40
- **Key outcome:** Introduces LongStory, a novel approach for generating long, coherent, and complete stories of varying lengths using language models, outperforming baselines in coherence, completeness, relevance, and repetitiveness.

## Executive Summary
LongStory addresses the challenge of generating long, coherent, and complete stories using language models. It introduces two key methodologies: the long and short-term contexts weight calibrator (CWC) and long story structural positions (LSP). The CWC dynamically weights long-term and short-term contexts using a BERT-tiny calibrator, while LSP uses discourse tokens to provide structural information. Trained on three datasets with varying story lengths, LongStory significantly improves coherence, completeness, relevance, and repetitiveness metrics compared to baseline models.

## Method Summary
LongStory employs a recursive paragraph generation approach to overcome context window limitations in language models. The model uses a BART-large base, fine-tuned with two novel components: CWC and LSP. CWC uses BERT-tiny to compute weights (β for long-term context, γ for short-term context) that adjust the influence of global story context versus recent paragraph context per paragraph. LSP adds discourse tokens (<intro>, <body>, <tail>, <front>, <middle>, <ending>, <next is ending>) to indicate paragraph order and structural role. The model recursively generates paragraphs while maintaining Memory (M_t) of long-term context and Cheating (C_t) of recent context, both updated and weighted via CWC.

## Key Results
- LongStory outperforms baselines (CTRL, PPLM, BART, GPT-2) on coherence, completeness, relevance, and repetitiveness metrics.
- The model demonstrates effectiveness in zero-shot tests, generating longer stories than seen during training.
- Ablation studies confirm the importance of both CWC and LSP components for overall performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LongStory model improves coherence and completeness by dynamically weighting long-term and short-term context.
- Mechanism: The model uses a BERT-tiny calibrator (CWC) to compute weights (β for long-term Memory, γ for short-term Cheating) that are applied during attention in the language model. This allows the model to adjust the influence of global story context versus recent paragraph context per paragraph.
- Core assumption: Long-term and short-term contexts contribute differently to coherence depending on the structural position in the story.
- Evidence anchors:
  - [abstract] The CWC adjusts weights for long-term context Memory and short-term context Cheating, acknowledging their distinct roles.
  - [section] The CWC uses BERT-tiny to determine the degree to which long-term and short-term contexts are employed, with outputs β and γ.
  - [corpus] Weak evidence - no direct citations to CWC in neighbors.
- Break condition: If the calibrator fails to differentiate context needs across story positions, or if BERT-tiny's small size limits accurate weighting.

### Mechanism 2
- Claim: Providing structural positional information improves long story generation quality.
- Mechanism: The LSP uses discourse tokens (<intro>, <body>, <tail>, <front>, <middle>, <ending>, <next is ending>) to indicate paragraph order and structural role, guiding the model's generation.
- Core assumption: A long story's structure is not uniform; early, middle, and ending sections require different contextual emphasis and narrative focus.
- Evidence anchors:
  - [abstract] The LSP employs discourse tokens to convey the structural positions of a long story.
  - [section] LSP adds four additional tokens to distinguish front, middle, and ending thirds of the story, plus a marker before the ending.
  - [corpus] Weak evidence - no direct citations to LSP or discourse tokens in neighbors.
- Break condition: If the model cannot effectively interpret the discourse tokens or if the token set is insufficient for nuanced structural guidance.

### Mechanism 3
- Claim: Recursive generation with context memory and cheating prevents coherence loss and repetition.
- Mechanism: The model recursively generates paragraphs, maintaining Memory (M_t) of long-term context and Cheating (C_t) of recent context (last few paragraphs), both updated and weighted via CWC.
- Core assumption: Recursive generation without memory mechanisms leads to forgetting earlier context and repeating content; these mechanisms counteract that.
- Evidence anchors:
  - [abstract] The model tackles coherence and completeness using CWC and LSP.
  - [section] M_t ensures sustained understanding of broader context, while C_t contributes to short-term continuity.
  - [corpus] Weak evidence - no direct citations to recursive generation with memory/cheating in neighbors.
- Break condition: If the Memory or Cheating updates fail to capture relevant context, or if the attention mechanism does not properly integrate these weighted contexts.

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: The model uses attention to combine input embeddings with weighted long-term and short-term contexts. Understanding attention is critical to grasp how CWC modulates context influence.
  - Quick check question: How does multi-head attention differ from single-head attention, and why is it beneficial in language models?

- Concept: Recursive text generation
  - Why needed here: LongStory generates stories paragraph by paragraph in a recursive loop, requiring each new paragraph to be coherent with all prior ones. This is central to the model's approach.
  - Quick check question: What are the main challenges of recursive generation, and how do Memory and Cheating help mitigate them?

- Concept: Discourse structure in narrative
  - Why needed here: The LSP relies on understanding story structure (intro, body, ending) to assign discourse tokens. This informs the model about narrative expectations at each position.
  - Quick check question: How does discourse structure differ between short stories and long-form narratives, and why does this matter for generation?

## Architecture Onboarding

- Component map:
  - Keywords K, discourse tokens Dt, [SEP] -> BERT-tiny (CWC) -> β, γ
  - Y_{t-1} -> Memory (M_t) and Cheating (C_t) computation
  - K, Dt, [SEP], Y_{t-1}, M_t, C_t -> LM (BART) -> Y_t

- Critical path:
  1. Input keywords and discourse tokens
  2. BERT-tiny computes β, γ
  3. M_t and C_t are computed/updated
  4. LM generates Y_t using weighted attention
  5. Y_t becomes input for next iteration

- Design tradeoffs:
  - Using BERT-tiny for calibration adds a small model but enables fine-grained context weighting; larger models might improve calibration but increase cost.
  - Adding many discourse tokens provides structural granularity but may complicate training; too few tokens might not capture story structure nuances.
  - Memory and Cheating mechanisms add complexity but are essential for coherence; simpler approaches might fail on long stories.

- Failure signatures:
  - Loss of coherence across paragraphs (CWC not working)
  - Repetitive content (Cheating not preventing repetition)
  - Stories not reaching proper endings (LSP tokens not guiding structure)
  - Poor relevance to keywords (LM or input processing failing)

- First 3 experiments:
  1. Test CWC calibration: Feed fixed Dt and Y_{t-1}, check if β and γ vary as expected across story positions.
  2. Test LSP token impact: Generate stories with and without LSP tokens, compare coherence and completeness.
  3. Test Memory/Cheating: Generate stories with only Memory, only Cheating, and both, compare coherence and repetition metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LongStory model's performance scale with even longer stories beyond 50 paragraphs?
- Basis in paper: [explicit] The paper tests the model up to 50 paragraphs but does not explore performance on longer narratives.
- Why unresolved: The experiments focus on a maximum of 50 paragraphs, leaving uncertainty about the model's effectiveness for very long stories.
- What evidence would resolve it: Testing the model on stories with 100+ paragraphs and evaluating coherence, completeness, and repetitiveness metrics.

### Open Question 2
- Question: What is the impact of using different base pretrained language models (e.g., GPT-4) on LongStory's performance?
- Basis in paper: [explicit] The authors mention that their algorithm is model-agnostic and could potentially be applied to larger language models for better performance.
- Why unresolved: The experiments use BART as the base model, but the potential benefits of using larger models like GPT-4 are not explored.
- What evidence would resolve it: Conducting experiments with various base models and comparing their performance in terms of coherence, completeness, and relevance.

### Open Question 3
- Question: How does the LongStory model handle stories with diverse themes or genres that may have different structural patterns?
- Basis in paper: [inferred] The paper mentions using diverse datasets but does not specifically address the model's ability to handle different genres or themes.
- Why unresolved: The experiments focus on general story generation without exploring the model's adaptability to various narrative styles or themes.
- What evidence would resolve it: Testing the model on genre-specific datasets (e.g., science fiction, romance) and evaluating its ability to maintain coherence and completeness across different themes.

## Limitations

- The paper relies heavily on automated metrics for evaluation, which may not fully capture nuanced coherence failures or narrative resolution quality.
- The model's performance on stories significantly longer than those in the Booksum dataset (6K+ tokens) remains untested.
- The discourse token system, while effective, is somewhat rigid and may not capture all narrative structural nuances.

## Confidence

**High Confidence:**
- The CWC component effectively calibrates context weights and improves coherence in generated stories.
- The LSP component improves structural awareness and story completeness.
- The overall approach outperforms baseline models on the specified metrics.

**Medium Confidence:**
- The model's ability to maintain coherence in extremely long stories (>10K tokens) is demonstrated but not thoroughly validated.
- The relative importance of CWC vs. LSP contributions is difficult to disentangle from ablation studies alone.
- The model's generalization to domains outside the three training datasets is suggested but not empirically validated.

**Low Confidence:**
- The specific numerical improvements (e.g., exact ROUGE score gains) may not generalize to different evaluation settings or datasets.

## Next Checks

1. **Human evaluation validation:** Conduct comprehensive human evaluation studies to validate automated metric results. Have human raters assess coherence, completeness, and relevance on a blind comparison between LongStory and baseline models across multiple story lengths and domains.

2. **Extreme length testing:** Generate and evaluate stories significantly longer than those in the Booksum dataset (e.g., 20K+ tokens) to test the model's true limits. Track coherence and completeness metrics across the full story length to identify failure points.

3. **Cross-domain transfer evaluation:** Test the model on story datasets from different domains (e.g., technical documentation, news articles, creative writing outside the training distributions) to assess generalization capabilities and identify domain-specific limitations.