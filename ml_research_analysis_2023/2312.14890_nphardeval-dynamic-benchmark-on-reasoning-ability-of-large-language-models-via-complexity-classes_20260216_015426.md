---
ver: rpa2
title: 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models
  via Complexity Classes'
arxiv_id: '2312.14890'
source_url: https://arxiv.org/abs/2312.14890
tags:
- reasoning
- llms
- complexity
- problem
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NPHardEval, a dynamic benchmark for evaluating
  the reasoning abilities of large language models (LLMs) across a spectrum of complexity
  classes up to NP-hard. The benchmark comprises 900 algorithmic questions, each categorized
  into 10 difficulty levels, and includes an automated update mechanism to mitigate
  overfitting risks.
---

# NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes

## Quick Facts
- **arXiv ID**: 2312.14890
- **Source URL**: https://arxiv.org/abs/2312.14890
- **Reference count**: 40
- **Key outcome**: Introduces NPHardEval, a dynamic benchmark evaluating LLMs' reasoning abilities across complexity classes up to NP-hard with 900 algorithmic questions

## Executive Summary
This paper introduces NPHardEval, a dynamic benchmark designed to evaluate the reasoning abilities of large language models (LLMs) across a spectrum of complexity classes up to NP-hard. The benchmark comprises 900 algorithmic questions, each categorized into 10 difficulty levels, and includes an automated update mechanism to mitigate overfitting risks. Through experiments, the study reveals that closed-source models like GPT-4 outperform open-source models, particularly on NP-hard problems, and highlights the importance of generalization in in-context learning. The benchmark aims to provide a rigorous and quantitative assessment of LLMs' reasoning capabilities, offering insights into their strengths and limitations.

## Method Summary
NPHardEval evaluates LLMs using 900 algorithmic questions across 9 tasks spanning complexity classes P, NP-complete, and NP-hard. Each task contains 100 questions across 10 difficulty levels. The benchmark employs zero-shot and few-shot in-context learning with examples at varying difficulty levels. A key innovation is the monthly dynamic update mechanism that regenerates problem instances to prevent overfitting. Performance is measured using weighted accuracy (weighted by difficulty level 1-10) and failure rate (proportion of failed attempts due to incorrect output format). The benchmark focuses on logical reasoning rather than mathematical skills, with tasks including sorted array search, edit distance, shortest path, traveling salesman, graph coloring, knapsack, and meeting scheduling problems.

## Key Results
- Closed-source models like GPT-4 significantly outperform open-source models, especially on NP-hard problems
- Weighted accuracy decreases and failure rates increase as task complexity rises, with marked performance decay at NP-hard complexity levels
- Closed-source models demonstrate minimal performance variation across different difficulty levels of few-shot examples, suggesting robust learning capabilities, while open-source models show varied adaptability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic updating of benchmark datapoints prevents model overfitting by ensuring models cannot memorize fixed problem instances.
- Mechanism: The benchmark regenerates problem instances monthly, creating fresh, algorithmically generated questions that models have not previously encountered. This forces models to rely on learned reasoning patterns rather than memorization of specific problem-answer pairs.
- Core assumption: LLMs' reasoning abilities can be meaningfully assessed through novel problem instances that require application of learned algorithmic skills rather than pattern matching.
- Evidence anchors:
  - [abstract] "This benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark."
  - [section] "This automated framework facilitates an effortless updating of datapoints within the benchmark. As a result, we design the benchmark to refresh its datapoints monthly, effectively reducing the likelihood of the model overfitting the dataset."
- Break condition: If models develop robust meta-learning capabilities that generalize across problem structures, or if the generation algorithm becomes predictable enough for models to reverse-engineer the pattern.

### Mechanism 2
- Claim: Categorizing problems by computational complexity classes provides a rigorous, quantitative measure of reasoning ability.
- Mechanism: By mapping problems to established complexity classes (P, NP-complete, NP-hard), the benchmark creates a structured hierarchy that correlates with reasoning difficulty. This allows for systematic evaluation of model performance across progressively harder problem types.
- Core assumption: The computational complexity hierarchy accurately reflects the cognitive complexity required for human-like reasoning.
- Evidence anchors:
  - [abstract] "This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class."
  - [section] "In our study, we employ the concept of complexity classes to categorize the reasoning tasks for LLMs. These classes are defined based on the computational resources, such as time or memory, required to solve the problems they contain."
- Break condition: If empirical performance gaps between complexity classes are not statistically significant, or if models perform well on NP-hard problems despite their theoretical intractability.

### Mechanism 3
- Claim: In-context learning performance differences between closed and open-source models reveal fundamental differences in learning mechanisms versus pattern matching.
- Mechanism: By varying the difficulty of few-shot examples relative to test questions, the benchmark distinguishes between genuine learning (consistent performance regardless of example difficulty) and pattern matching (performance dependent on example difficulty matching).
- Core assumption: Models that truly learn algorithmic skills will generalize across difficulty levels, while those that merely pattern match will be sensitive to difficulty alignment.
- Evidence anchors:
  - [abstract] "We differentiate between 'learning' and 'mimicking' by evaluating whether LLMs can generalize solutions to new problems of varying difficulty levels within the same task, after being exposed to examples."
  - [section] "If an LLM has truly learned the underlying algorithmic skill, it should be able to tackle problems across different difficulty levels within the same task. Conversely, if an LLM is merely mimicking, its performance may falter when faced with variations in problem difficulty."
- Break condition: If all models show consistent performance regardless of example difficulty, or if the task structure itself provides sufficient hints that difficulty becomes irrelevant.

## Foundational Learning

- Concept: Computational complexity classes (P, NP-complete, NP-hard)
  - Why needed here: These classes provide the theoretical foundation for categorizing problem difficulty and establishing a rigorous evaluation framework
  - Quick check question: Can you explain why NP-complete problems are considered "the hardest" problems in NP?

- Concept: Algorithmic problem-solving and complexity analysis
  - Why needed here: Understanding how to analyze algorithmic efficiency and problem structure is essential for both creating the benchmark and interpreting results
  - Quick check question: What is the time complexity of Dijkstra's algorithm for shortest path problems?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The benchmark's evaluation of learning versus mimicking relies on understanding how models generalize from examples in context
  - Quick check question: How does the number and quality of few-shot examples typically affect LLM performance?

## Architecture Onboarding

- Component map: Problem synthesis -> Benchmark evaluation -> Performance analysis -> Dynamic update scheduling
- Critical path: Problem generation → Benchmark evaluation → Performance analysis → Dynamic update scheduling
- Design tradeoffs: The system trades computational cost of frequent problem regeneration against the benefit of preventing overfitting; it also trades coverage of simpler mathematical problems against focus on logical reasoning
- Failure signatures: If weighted accuracy plateaus across all complexity classes, if failure rates increase disproportionately with difficulty, or if open-source models consistently fail to generalize from examples
- First 3 experiments:
  1. Run a baseline evaluation with zero-shot prompts across all 900 problems to establish initial performance metrics
  2. Conduct in-context learning experiments with varying example difficulties to test learning versus mimicking hypotheses
  3. Perform statistical analysis of performance differences between complexity classes to validate the theoretical difficulty ordering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the reasoning capabilities of LLMs differ when solving NP-hard problems compared to P or NP-complete problems?
- Basis in paper: [explicit] The paper presents results showing that all models show decreased accuracy and increased failure rates as task complexity rises, with a marked performance decay at NP-Hard complexity levels.
- Why unresolved: The paper identifies this trend but does not explore the underlying reasons for the performance difference between complexity classes or propose methods to improve LLMs' performance on NP-hard problems.
- What evidence would resolve it: Further research comparing the reasoning processes and solution strategies used by LLMs on problems of different complexity classes could shed light on the observed performance differences. Experiments testing the impact of different prompting strategies or model architectures on NP-hard problem solving could also provide insights.

### Open Question 2
- Question: How does the difficulty of few-shot examples affect the in-context learning capabilities of LLMs?
- Basis in paper: [explicit] The paper finds that closed-source models like GPT 4 Turbo and Claude 2 demonstrate minimal performance variation and high consistency across different difficulty levels of few-shot examples, suggesting a robust ability to learn algorithmic skills. In contrast, open-source models show varied adaptability, with some performing well on more challenging examples but struggling with simpler ones.
- Why unresolved: The paper observes this trend but does not provide a definitive explanation for the difference in few-shot learning capabilities between closed-source and open-source models or propose methods to improve open-source models' ability to learn from examples of varying difficulty.
- What evidence would resolve it: Further research comparing the internal representations and reasoning processes of closed-source and open-source models during few-shot learning could reveal the underlying reasons for their differing capabilities. Experiments testing the impact of different training strategies or model architectures on few-shot learning performance could also provide insights.

### Open Question 3
- Question: How can the NPHardEval benchmark be improved to provide a more accurate and comprehensive evaluation of LLMs' reasoning capabilities?
- Basis in paper: [inferred] The paper acknowledges several limitations of the current benchmark, including the scope of task selection, the definition of complexity, the impact of randomness, and the need for dynamic updates.
- Why unresolved: The paper identifies these limitations but does not propose specific solutions or outline a plan for addressing them in future versions of the benchmark.
- What evidence would resolve it: Further research exploring different approaches to task selection, complexity definition, and evaluation methodology could lead to improvements in the benchmark. Regular updates to the benchmark based on the latest developments in LLMs and feedback from the research community could also enhance its accuracy and relevance.

## Limitations
- Limited task diversity (only 9 tasks) may not capture the full spectrum of reasoning abilities
- The monthly update frequency may be insufficient to prevent sophisticated models from adapting to generation patterns
- Performance differences between closed and open models could be influenced by factors beyond reasoning ability, such as training data exposure

## Confidence
- High confidence: The benchmark architecture and dynamic updating mechanism are well-designed and technically sound
- Medium confidence: The theoretical foundation linking computational complexity to reasoning difficulty is valid but needs more empirical validation
- Low confidence: The effectiveness of the learning-versus-mimicking distinction across different model families and tasks

## Next Checks
1. Conduct a statistical power analysis to determine if the current sample size (900 questions) is sufficient to detect meaningful performance differences across complexity classes
2. Implement cross-validation testing where models are evaluated on problems generated by different algorithms to verify generalization beyond specific problem structures
3. Design a controlled experiment comparing model performance on dynamically updated versus static benchmarks to quantify the actual impact of the dynamic updating mechanism on preventing overfitting