---
ver: rpa2
title: 'Beyond Surprise: Improving Exploration Through Surprise Novelty'
arxiv_id: '2308.04836'
source_url: https://arxiv.org/abs/2308.04836
tags:
- surprise
- memory
- intrinsic
- reward
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel intrinsic motivation method called Surprise
  Memory (SM) to address the limitations of existing surprise-based exploration methods.
  SM measures the novelty of surprise rather than its norm, estimating it as retrieval
  errors from a memory network.
---

# Beyond Surprise: Improving Exploration Through Surprise Novelty

## Quick Facts
- arXiv ID: 2308.04836
- Source URL: https://arxiv.org/abs/2308.04836
- Reference count: 40
- Key outcome: Surprise Memory (SM) consistently improves exploration efficiency and final performance in sparse-reward environments by measuring surprise novelty via reconstruction error rather than raw surprise norm.

## Executive Summary
This paper addresses the limitations of surprise-based exploration methods in reinforcement learning by introducing Surprise Memory (SM), which measures the novelty of surprise rather than its magnitude. The method uses an autoencoder to reconstruct surprise vectors, with reconstruction error serving as the intrinsic reward signal. This approach maintains agent interest in genuinely novel experiences while reducing attraction to unpredictable or noisy observations, particularly addressing the "noisy-TV" problem. Experiments demonstrate that SM significantly boosts performance across various sparse-reward environments including Noisy-TV, navigation tasks, and challenging Atari games.

## Method Summary
The method combines a surprise generator (SG) with a memory-augmented autoencoder to compute surprise novelty as intrinsic reward. The SG produces surprise vectors from observations, which are stored in episodic memory and used as queries for the autoencoder. The autoencoder reconstructs these queries, and the reconstruction error serves as the novelty score. This novelty score is normalized and added to external rewards as intrinsic reward bonus. The approach operates on surprise space rather than raw observations, leveraging lower variance in surprise features to improve learning efficiency.

## Key Results
- SM consistently improves performance of three different surprise generators (RND, ICM, NGU) in sparse-reward tasks
- Significant performance gains demonstrated on Noisy-TV, MiniGrid navigation, and challenging Atari games
- SM reduces agent's attraction to noisy observations while maintaining interest in genuinely novel exploration opportunities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surprise novelty measured via reconstruction error provides a more robust intrinsic reward than raw surprise norm, reducing attraction to noisy or unpredictable observations.
- Mechanism: The Surprise Memory (SM) module uses an autoencoder to reconstruct the surprise vector; the reconstruction error reflects how novel or unique the current surprise is compared to past experiences. If the surprise is predictable or similar to past ones, the reconstruction error (and thus the intrinsic reward) is low.
- Core assumption: Surprises exhibit patterns that can be learned and reconstructed; random noise patterns are harder to reconstruct and thus yield higher novelty scores only initially.
- Evidence anchors:
  - [abstract] "The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises."
  - [section] "The surprise novelty is then determined as: ri t =||˜qt− qt|| which is the norm of the surprise residual ˜qt− qt. It will be normalized and added to the external reward as an intrinsic reward bonus."
  - [corpus] Weak evidence: no direct mentions of "reconstruction error" or "autoencoder" in neighbors; only general curiosity-based exploration themes.
- Break condition: If the autoencoder cannot learn to reconstruct common surprise patterns, all surprises will appear novel, inflating intrinsic rewards and causing inefficient exploration.

### Mechanism 2
- Claim: The episodic memory within SM enables rapid adaptation to surprise patterns that change within a single episode, preventing repeated attraction to the same surprising event.
- Mechanism: A slot-based episodic memory stores recent surprises; when the same or similar surprise recurs within an episode, the memory retrieves a context vector that, when combined with the current surprise, makes reconstruction easier, lowering the novelty score.
- Core assumption: Surprise patterns can repeat within an episode and such repetition should reduce intrinsic reward.
- Evidence anchors:
  - [abstract] "The memory stores and reconstructs surprises... maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations."
  - [section] "The episodic memory M stores representations of surprises that the agent encounters during an episode... To retrieve from M a read-out ue t that is close to ut, we perform content-based attention..."
  - [corpus] Weak evidence: no explicit discussion of episodic memory in neighbors; focus is on general curiosity or novelty measures.
- Break condition: If the episodic memory size is too small or retrieval is poor, repeated surprises within an episode may not be recognized, leading to persistent over-exploration of the same event.

### Mechanism 3
- Claim: Operating on the surprise space (rather than raw observations) reduces variance and makes learning the reconstruction easier, improving sample efficiency.
- Mechanism: By training the autoencoder on surprise vectors (which have lower variance per Proposition 1) instead of raw observations, the network can more easily learn to reconstruct patterns and distinguish novelty.
- Core assumption: The variance of surprise features is lower than that of observation features, making reconstruction learning easier.
- Evidence anchors:
  - [abstract] "We argue that using surprise novelty as an intrinsic reward is better than surprise norm."
  - [section] "Proposition 1. Let X and U be random variables representing the observation and surprise at the same timestep... the following inequality holds: ∀i : (σ X i ) 2 ≥ (σ U i ) 2 ..."
  - [corpus] Weak evidence: neighbors discuss novelty or curiosity but not variance reduction arguments.
- Break condition: If the surprise generator is too accurate (prediction error near zero), all surprises become identical, making novelty detection impossible.

## Foundational Learning

- Concept: Reinforcement learning with intrinsic motivation and sparse rewards
  - Why needed here: The paper addresses exploration in sparse-reward environments by augmenting external rewards with intrinsic rewards derived from surprise novelty.
  - Quick check question: In a sparse-reward setting, why is intrinsic motivation necessary, and how does it differ from random exploration?

- Concept: Surprise as prediction error and its role in curiosity-driven exploration
  - Why needed here: The paper builds on surprise-based exploration but modifies it to measure novelty of surprise rather than magnitude, avoiding pitfalls like the noisy-TV problem.
  - Quick check question: What is the difference between using surprise norm and surprise novelty as an intrinsic reward?

- Concept: Memory-augmented neural networks and associative memory
  - Why needed here: The Surprise Memory uses an autoencoder (associative memory) to reconstruct surprise patterns and an episodic memory for short-term context.
  - Quick check question: How does an autoencoder act as an associative memory in this context, and what is the role of the episodic memory?

## Architecture Onboarding

- Component map: RL Backbone -> Surprise Generator (SG) -> Episodic Memory (M) -> Autoencoder (W) -> Intrinsic Reward Normalization
- Critical path: 1) Observe state → SG produces surprise vector 2) Store surprise in episodic memory M 3) Retrieve context from M, form query with current surprise 4) Autoencoder W reconstructs query; reconstruction error = intrinsic reward 5) Normalize and add to external reward; train policy
- Design tradeoffs:
  - Larger episodic memory improves context but increases memory and computation
  - Autoencoder capacity vs. overfitting: too large may memorize noise; too small may fail to capture patterns
  - Operating on surprise space reduces variance but depends on quality of SG
- Failure signatures:
  - All states yield high intrinsic reward → episodic memory retrieval failing or autoencoder underfitting
  - No improvement in exploration efficiency → surprise generator producing uninformative surprises
  - High variance in intrinsic rewards → poor normalization or unstable surprise estimates
- First 3 experiments:
  1. Verify that surprise novelty is lower for repeated surprises within an episode using a simple gridworld
  2. Test that SM reduces attraction to noisy-TV compared to raw surprise norm baseline
  3. Compare performance of SM-augmented vs. vanilla SG in a sparse-reward navigation task

## Open Questions the Paper Calls Out
- Question: How does the performance of Surprise Memory (SM) scale with increasing memory capacity (N) and slot size (d) in environments with longer episodes or more complex surprise patterns?
- Question: What are the theoretical properties of surprise novelty beyond the second-order error mentioned, and how do higher-order errors relate to exploration efficiency?
- Question: How does the surprise novelty approach perform in environments where surprises do not have clear patterns or are inherently unpredictable?
- Question: What is the optimal balance between the reconstructive ability of the autoencoder and its ability to discriminate novel surprises?

## Limitations
- The variance reduction claim (Proposition 1) is theoretically stated but not empirically validated in the paper
- The effectiveness of SM critically depends on the quality of the underlying surprise generator, but the paper only tests with three specific generators
- The episodic memory mechanism's contribution is not isolated - experiments combine SM with surprise generators

## Confidence
- **High Confidence**: The core mechanism of using reconstruction error as novelty measure is technically sound and the mathematical formulation is correct
- **Medium Confidence**: The empirical results showing improved performance across multiple benchmarks, though sample efficiency comparisons could be more rigorous
- **Low Confidence**: Claims about SM's ability to distinguish "fake" surprises from genuine exploration opportunities lack quantitative validation

## Next Checks
1. Empirically measure and compare the variance of surprise vectors versus observation vectors across multiple environments to validate Proposition 1
2. Conduct ablation studies isolating the episodic memory component by comparing SM with and without memory in controlled environments
3. Test SM with deliberately degraded surprise generators (e.g., reduced network capacity) to establish robustness boundaries