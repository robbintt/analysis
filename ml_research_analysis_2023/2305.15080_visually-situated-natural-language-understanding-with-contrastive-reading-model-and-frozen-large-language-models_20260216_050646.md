---
ver: rpa2
title: Visually-Situated Natural Language Understanding with Contrastive Reading Model
  and Frozen Large Language Models
arxiv_id: '2305.15080'
source_url: https://arxiv.org/abs/2305.15080
tags:
- cream
- image
- question
- visual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cream, a Contrastive Reading Model designed
  to enhance the language-image understanding capability of large language models
  (LLMs) for text-rich visual tasks. Cream combines a vision encoder with auxiliary
  encoders (OCR, object detection) and employs a contrastive feature alignment technique
  to capture fine-grained visual details often missed by existing multimodal models.
---

# Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models

## Quick Facts
- arXiv ID: 2305.15080
- Source URL: https://arxiv.org/abs/2305.15080
- Reference count: 20
- Key outcome: Introduces Cream, a Contrastive Reading Model that achieves state-of-the-art results on Document Visual Question Answering (DocVQA), ChartQA, and InfographicVQA benchmarks by combining vision and auxiliary encoders with contrastive feature alignment.

## Executive Summary
This paper presents Cream, a novel Contrastive Reading Model designed to enhance the language-image understanding capability of large language models (LLMs) for text-rich visual tasks. Cream combines a vision encoder with auxiliary encoders (OCR, object detection) and employs a contrastive feature alignment technique to capture fine-grained visual details often missed by existing multimodal models. By aligning features from multiple encoders, Cream provides a powerful visual understanding module that can be integrated with frozen LLMs to answer questions on document images, charts, and infographics. Experiments show that Cream significantly outperforms other frozen LLM integrations on benchmark datasets while using a fixed-size visual prompt for efficiency.

## Method Summary
Cream is a Contrastive Reading Model that integrates a vision encoder with auxiliary encoders for OCR and object detection, using contrastive feature alignment to capture fine-grained visual details. The model is trained in two phases using multi-task learning with natural language prompts, and can be integrated with frozen LLMs through a learned queries mechanism that generates fixed-size visual prompts. Cream achieves state-of-the-art performance on text-rich visual tasks while maintaining efficiency through its compact prompt generation approach.

## Key Results
- Cream achieves 4.5% absolute improvement on DocVQA and 9.6% on ChartQA over existing frozen LLM integrations
- Outperforms state-of-the-art specialized visual document understanding models on standalone tasks
- Achieves competitive results with frozen LLM integration while using fixed-size visual prompts for efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive feature alignment improves cross-encoder embedding quality in visually-situated language tasks.
- **Mechanism**: The model defines positive pairs as feature evidence embeddings (OCR, objects) and their corresponding image patch embeddings, then optimizes a contrastive loss to align them in a shared space.
- **Core assumption**: Feature evidence and its image patch contain semantically similar information that can be captured through alignment.
- **Evidence anchors**:
  - [abstract]: "Cream employs auxiliary encoders... for text and object-specific feature extraction. Cream utilizes auxiliary encoders as well as a vision encoder to extract fine-grained features... without missing image details while understanding the visual context."
  - [section]: "We apply CL, defining a positive pair as relationship between the embedding of the feature evidence and the corresponding image patch, while all other relationships are considered negative pairs."
  - [corpus]: Weak - the corpus papers focus on related methods but not on this exact contrastive alignment approach.
- **Break condition**: If feature evidence and image patches do not semantically correspond (e.g., misaligned OCR bounding boxes), the contrastive alignment could harm rather than help.

### Mechanism 2
- **Claim**: Integrating Cream with frozen LLMs achieves strong performance without feeding all OCR tokens.
- **Mechanism**: Cream uses a decoder to generate a fixed-size soft visual prompt (192 vectors) that conditions the LLM on relevant visual context.
- **Core assumption**: A compact, task-relevant visual prompt can substitute for large OCR token inputs while maintaining comprehension.
- **Evidence anchors**:
  - [abstract]: "Cream achieves robust performance in text-rich visual tasks, which are challenging for existing LVLMs."
  - [section]: "Our proposed Cream integration exhibits significant improvement over other Frozen LLM integrations in benchmarks demanding advanced visual understanding capabilities."
  - [corpus]: Weak - related papers discuss OCR-LLM integration but don't demonstrate the efficiency gains of fixed-size prompts.
- **Break condition**: If the fixed prompt size is insufficient for complex scenes with many distinct text elements, performance may degrade.

### Mechanism 3
- **Claim**: Unified multitask training with natural language prompts enables seamless integration with downstream LLMs.
- **Mechanism**: Training with prompts like "Read all texts in the image" conditions the model to respond to similar prompts during inference, allowing direct LLM integration.
- **Core assumption**: Natural language prompts used during training generalize to inference prompts used by downstream systems.
- **Evidence anchors**:
  - [abstract]: "Cream trained with natural language-based prompts can be more seamlessly integrated into LLMs."
  - [section]: "Our prompt differs from other document understanding methods... which employ single task-specified prompt."
  - [corpus]: Weak - related works don't emphasize unified multitask training with generic natural language prompts.
- **Break condition**: If inference prompts differ significantly from training prompts, the model may fail to produce appropriate outputs.

## Foundational Learning

- **Contrastive learning**:
  - Why needed here: To align embeddings from vision and auxiliary encoders without requiring large paired datasets.
  - Quick check question: How does contrastive learning differ from traditional supervised alignment in multimodal models?

- **Vision transformers**:
  - Why needed here: To process images into patch embeddings that can be aligned with text embeddings from auxiliary encoders.
  - Quick check question: What advantage does a variable-resolution mechanism provide when processing document images?

- **Multitask learning**:
  - Why needed here: To train a single model that can handle text reading, question answering, captioning, and more without separate specialized models.
  - Quick check question: Why might mixing different task proportions during training phases improve overall performance?

## Architecture Onboarding

- **Component map**: Image → Vision encoder → Contrastive alignment → Decoder → LLM (for integrated mode)
- **Critical path**: Image → Vision encoder → Contrastive alignment → Decoder → LLM (for integrated mode)
- **Design tradeoffs**: Fixed prompt size vs. full OCR token input (efficiency vs. completeness); contrastive alignment vs. simple concatenation (alignment quality vs. simplicity)
- **Failure signatures**: Poor OCR-object correspondence leading to misaligned contrastive pairs; insufficient prompt size causing loss of relevant information; prompt engineering mismatch between training and inference
- **First 3 experiments**:
  1. Test standalone Cream on DocVQA with and without contrastive alignment to measure impact
  2. Integrate Cream with Vicuna using different prompt sizes (32, 96, 192) to find optimal balance
  3. Train with different task proportions in phase 1 to identify best mix for downstream QA performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Cream handle cases where the text in the image is handwritten or in a non-standard font?
- Basis in paper: [inferred] The paper discusses Cream's ability to extract fine-grained features from images, including text, but does not explicitly mention handling of handwritten or non-standard fonts.
- Why unresolved: The paper focuses on Cream's performance with structured text in document images and charts, but real-world applications may involve diverse text styles.
- What evidence would resolve it: Experiments evaluating Cream's performance on datasets with handwritten or non-standard font text would provide insights into its generalizability.

### Open Question 2
- Question: What is the impact of varying the number of learned queries (k) on the performance of Cream when integrated with LLMs?
- Basis in paper: [explicit] The paper mentions using k=192 learned queries in the main experiments but does not explore the impact of varying this hyperparameter.
- Why unresolved: The optimal number of learned queries may depend on the complexity of the visual input and the specific LLM used.
- What evidence would resolve it: Experiments varying the value of k and evaluating the performance of Cream-LLM integration would reveal the sensitivity of the model to this hyperparameter.

### Open Question 3
- Question: How does Cream's performance compare to other state-of-the-art models on tasks beyond VQA, such as image captioning or visual reasoning?
- Basis in paper: [explicit] The paper evaluates Cream on VQA tasks but does not explore its performance on other visual-language tasks.
- Why unresolved: While VQA is a common benchmark for visually-situated language understanding, Cream's capabilities may extend to other tasks.
- What evidence would resolve it: Experiments evaluating Cream on datasets for image captioning, visual reasoning, or other relevant tasks would provide a more comprehensive understanding of its strengths and limitations.

## Limitations

- Contrastive alignment effectiveness depends heavily on quality of feature correspondence between OCR/object detections and image patches
- Fixed-size visual prompt (192 vectors) may be insufficient for complex images with many distinct text elements
- Integration with frozen LLMs relies on specific prompt engineering that may not generalize well across different LLM architectures

## Confidence

**High Confidence Claims:**
- Cream architecture design combining vision and auxiliary encoders is sound and technically feasible
- Contrastive feature alignment approach is well-established in multimodal learning literature
- Multi-task training strategy with natural language prompts is a valid approach

**Medium Confidence Claims:**
- The specific performance improvements over baselines (4.5% on DocVQA, 9.6% on ChartQA) are likely reproducible given proper implementation
- The efficiency gains from using fixed-size prompts versus full OCR token input are plausible but may vary by application
- The seamless integration with frozen LLMs through learned queries is technically sound but may require careful tuning

**Low Confidence Claims:**
- Claims about Cream being "the first reading model with frozen LLMs" are difficult to verify given the rapid evolution of the field
- The assertion that contrastive alignment captures "fine-grained details" is somewhat vague and hard to measure precisely
- The generalizability of results to datasets not included in evaluation remains uncertain

## Next Checks

1. **Ablation Study on Contrastive Alignment**: Run experiments comparing Cream with and without the contrastive alignment component on DocVQA to isolate its contribution to performance improvements.

2. **Prompt Size Sensitivity Analysis**: Systematically test different visual prompt sizes (32, 64, 96, 128, 192, 256 vectors) when integrating Cream with frozen LLMs to identify optimal tradeoff between efficiency and accuracy.

3. **Cross-LLM Compatibility Testing**: Integrate Cream with multiple different frozen LLMs (Vicuna, Llama, GPT-4) to verify that the learned queries and prompt engineering generalize beyond the specific model used in the paper.