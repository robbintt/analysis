---
ver: rpa2
title: Stateful Large Language Model Serving with Pensieve
arxiv_id: '2312.05516'
source_url: https://arxiv.org/abs/2312.05516
tags:
- memory
- cache
- attention
- pensieve
- request
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of existing LLM serving systems
  when used for multi-turn conversations, where the conversation history is recomputed
  with each successive new request. The authors propose Pensieve, a stateful LLM serving
  system that maintains conversation state across requests by caching previously processed
  history to avoid duplicate processing.
---

# Stateful Large Language Model Serving with Pensieve

## Quick Facts
- arXiv ID: 2312.05516
- Source URL: https://arxiv.org/abs/2312.05516
- Reference count: 40
- This paper addresses the inefficiency of existing LLM serving systems when used for multi-turn conversations, where the conversation history is recomputed with each successive new request. The authors propose Pensieve, a stateful LLM serving system that maintains conversation state across requests by caching previously processed history to avoid duplicate processing. Experiments show that Pensieve can achieve 1.14-3.0x the throughput of vLLM and TensorRT-LLM, and significantly reduce latency by 60-75%.

## Executive Summary
This paper introduces Pensieve, a stateful LLM serving system designed to improve efficiency for multi-turn conversations by caching conversation history and avoiding redundant computation. Existing LLM serving systems recompute the entire conversation history for each new request, leading to significant computational overhead. Pensieve addresses this by maintaining a two-tier GPU-CPU memory cache for key-value (KV) tokens, using a pipelined CPU-GPU swapping strategy to overlap data movement with computation, and implementing a generalized multi-query attention kernel to handle non-contiguous memory layouts efficiently.

## Method Summary
Pensieve is a stateful LLM serving system that maintains conversation state across requests by caching previously processed history to avoid duplicate processing. It implements a unified batch scheduler that manages a two-tier caching system spanning GPU and CPU memory, and generalizes the PagedAttention kernel to support attention between multiple input tokens with a GPU cache spread over non-contiguous memory. The system is evaluated on two datasets, ShareGPT and LMSYS, containing user-shared ChatGPT conversations and AI-powered chatbot conversations respectively, and compared against vLLM and TensorRT-LLM in terms of throughput and latency.

## Key Results
- Pensieve achieves 1.14-3.0x the throughput of vLLM and TensorRT-LLM.
- Pensieve significantly reduces latency by 60-75%.
- The system improves throughput by 20-50% and latency by 10-30% compared to vLLM in multi-turn conversations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stateful caching reduces redundant computation of conversation history in multi-turn LLM serving.
- Mechanism: Pensieve caches KV tokens of previous conversation turns in a two-tier GPU-CPU memory hierarchy, avoiding recomputation of unchanged context.
- Core assumption: A large fraction of multi-turn conversations contain repeated or partially overlapping content, making cached context reusable.
- Evidence anchors:
  - [abstract] "maintains the conversation state across requests by caching previously processed history to avoid duplicate processing."
  - [section 3.1] "minimize redundant computation of the conversation history. This can be done by caching any previously processed embeddings at the serving system and re-use them across requests from the same conversation."
- Break condition: If conversation turns have very low overlap (e.g., completely unrelated topics), cached context offers minimal benefit.

### Mechanism 2
- Claim: Pipelined CPU-GPU swapping overlaps data movement with computation, hiding swap latency.
- Mechanism: Pensieve initiates CPU-to-GPU KV token transfers layer-by-layer while the GPU is still computing on earlier layers, using CUDA events to enforce dependencies.
- Core assumption: LLM layers can be processed in a pipeline fashion because each layer's attention only depends on the previous layer's outputs.
- Evidence anchors:
  - [section 4.3] "we follow the pipelined approach [2] to overlap computation with data transfer... we initiate the transfer layer by layer and start model computation at the same time."
- Break condition: If layer dependencies or GPU-CPU bandwidth become bottlenecks, pipelining may not fully hide latency.

### Mechanism 3
- Claim: A generalized multi-query attention kernel supports non-contiguous KV cache and multi-token prompts efficiently.
- Mechanism: Pensieve extends the PagedAttention kernel to handle multi-head attention between multiple new input tokens and non-contiguous cached KV tokens using matrix-matrix multiplication.
- Core assumption: Non-contiguous memory layout is unavoidable due to dynamic cache allocation and swapping; a kernel that handles it directly is necessary.
- Evidence anchors:
  - [section 4.4] "We develop a new GPU kernel to compute attention between multiple input tokens with a GPU cache spread over non-contiguous memory... generalizes the recent PagedAttention kernel."
  - [section 4.4.1] "single-query attention performed in the generation phase can be treated as a special case of the multi-query attention with query size equal to 1."
- Break condition: If all cached KV tokens fit in contiguous GPU memory, the more complex kernel may not be necessary.

## Foundational Learning

- Concept: Transformer attention mechanism and KV caching
  - Why needed here: Understanding how attention consumes KV tokens and why caching them avoids recomputation is central to Pensieve's design.
  - Quick check question: In a Transformer layer, which tensors are stored in the KV cache and reused across decoding steps?

- Concept: GPU memory hierarchy and data movement
  - Why needed here: Pensieve's two-tier caching and pipelined swapping rely on knowing GPU vs CPU memory characteristics and transfer latency.
  - Quick check question: What are the typical bandwidth and latency differences between GPU memory and CPU memory, and how does that affect cache eviction decisions?

- Concept: CUDA kernel fusion and tiling
  - Why needed here: The multi-query attention kernel uses matrix-matrix multiplication and tiling for efficiency; understanding these is key to grasping its performance advantage.
  - Quick check question: How does fusing the causal masking operation into the attention kernel reduce memory traffic compared to separate kernels?

## Architecture Onboarding

- Component map: Scheduler -> Worker -> GPU Cache -> CPU Cache
- Critical path:
  1. New request arrives → scheduler checks GPU cache availability.
  2. Scheduler generates cache plan → instructs worker to swap in missing KV tokens (pipelined).
  3. Worker batches request with others → executes fused multi-query attention.
  4. Results returned → scheduler updates cache state and evicts if needed.
- Design tradeoffs:
  - GPU vs CPU cache size: larger GPU cache reduces swapping but limits number of active conversations.
  - Eviction granularity: evicting half a conversation's KV tokens allows later merging, but risks more recomputation.
  - Kernel complexity: multi-query attention supports non-contiguous cache but is more complex than single-query.
- Failure signatures:
  - High latency spikes: likely due to cache misses and CPU-to-GPU swaps.
  - Low throughput: possible over-aggressive eviction or insufficient batch size.
  - Memory fragmentation: excessive small allocations in GPU cache.
- First 3 experiments:
  1. Measure baseline throughput/latency of vLLM vs Pensieve on ShareGPT dataset with 2K context.
  2. Vary CPU cache size to find the point where swap latency starts dominating.
  3. Test multi-query kernel with increasing numbers of input tokens to verify scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Pensieve's performance scale when serving multi-billion parameter models that require distributed execution across multiple GPUs?
- Basis in paper: [explicit] The paper mentions that distributed execution is not fully vetted and will be addressed in a future version, indicating this is an open area of investigation.
- Why unresolved: The paper focuses on single-GPU performance and does not provide concrete evaluation results for multi-GPU setups or models requiring model parallelism.
- What evidence would resolve it: Performance benchmarks comparing Pensieve's throughput and latency on distributed multi-GPU systems with different model sharding strategies, along with an analysis of how state management and cache coherence are maintained across GPUs.

### Open Question 2
- Question: What is the optimal cache eviction policy for Pensieve that balances the trade-off between reducing latency and maximizing throughput in different workload scenarios?
- Basis in paper: [inferred] The paper mentions that the scheduler uses a score-based eviction policy weighing the size of KV-tokens and inactivity time, but this is a simple heuristic. The paper does not explore alternative policies or provide an optimal solution.
- Why unresolved: Cache management is complex and highly dependent on workload characteristics. The paper presents a basic approach but does not explore more sophisticated policies or provide a rigorous evaluation of different strategies.
- What evidence would resolve it: Comparative analysis of various cache eviction policies (e.g., LRU, LFU, score-based, ML-based) under different workload patterns and system configurations, with clear metrics for both latency and throughput.

### Open Question 3
- Question: How does Pensieve's performance and effectiveness change when serving models with context lengths significantly larger than the evaluated 2048 tokens, such as those supporting 32K or 100K context windows?
- Basis in paper: [explicit] The paper briefly discusses experiments with maximum context size of 16384 tokens and notes that throughput gain decreases with longer contexts, but does not provide a comprehensive analysis.
- Why unresolved: The paper only evaluates up to 16K context length and does not explore the full range of modern LLM capabilities. The impact of very long contexts on cache management, memory pressure, and recomputation overhead is not fully characterized.
- What evidence would resolve it: Performance evaluation of Pensieve across a wide range of context lengths (2K to 100K+ tokens), including detailed analysis of cache hit rates, swap frequency, and the effectiveness of recomputation strategies at different context scales.

## Limitations

- The paper does not fully detail the specific implementation of the multi-query attention kernel and how it generalizes the PagedAttention kernel, which is a key component of the system.
- The performance evaluation is based on user-shared conversation datasets (ShareGPT and LMSYS), and the extent to which these datasets represent real-world multi-turn conversation complexity is uncertain.
- The system's scalability to very large numbers of concurrent conversations and potential bottlenecks in components beyond GPU-CPU memory hierarchy are not thoroughly explored.

## Confidence

- Performance Claims: Medium Confidence
- Mechanism Effectiveness: High Confidence
- Architectural Design: High Confidence

## Next Checks

1. **Controlled Eviction Policy Test:** Implement and test different KV cache eviction policies (e.g., LRU, FIFO, LFU) within Pensieve to quantify their impact on throughput and latency, particularly in scenarios with high conversation churn or overlapping contexts. Measure the frequency of recomputation due to eviction.

2. **Kernel Performance Profiling:** Profile the multi-query attention kernel under varying conditions (contiguous vs. non-contiguous cache, different numbers of input tokens, different cache hit rates) to identify performance bottlenecks and compare its efficiency against the original PagedAttention kernel and a baseline single-query kernel.

3. **Distributed Deployment Scalability Test:** Deploy Pensieve in a distributed environment with multiple GPU nodes and measure its performance and scalability as the number of concurrent conversations increases. Identify potential bottlenecks in network communication, scheduler load balancing, and cross-node cache coherence.