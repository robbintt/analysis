---
ver: rpa2
title: 'COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning'
arxiv_id: '2311.00886'
source_url: https://arxiv.org/abs/2311.00886
tags:
- learning
- outcome
- estimation
- treatment
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COSTAR, a self-supervised learning framework
  for estimating treatment outcomes over time from observational data, crucial when
  randomized controlled trials are impractical. COSTAR addresses challenges in modeling
  complex temporal dependencies and long-range dependencies by integrating temporal
  and feature-wise attention with a component-wise contrastive loss tailored for temporal
  treatment outcome observations.
---

# COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning

## Quick Facts
- arXiv ID: 2311.00886
- Source URL: https://arxiv.org/abs/2311.00886
- Reference count: 34
- Key outcome: COSTAR outperforms baselines by 6.2%, 22.5%, and 26.3% average reduction in RMSE across tumor growth, MIMIC-III, and M5 datasets respectively

## Executive Summary
This paper introduces COSTAR, a self-supervised learning framework for estimating treatment outcomes over time from observational data. The method addresses the challenge of modeling complex temporal dependencies and long-range dependencies in counterfactual estimation by integrating temporal and feature-wise attention with a component-wise contrastive loss. Empirical results demonstrate that COSTAR achieves superior performance in estimation accuracy and generalization to out-of-distribution data compared to existing models, reducing outcome estimation errors significantly across multiple datasets.

## Method Summary
COSTAR uses a Transformer-based encoder that alternates between temporal attention blocks (capturing temporal dependencies within each feature) and feature-wise attention blocks (modeling interactions among different features). The model is trained using self-supervised learning with component-wise contrastive losses on unlabeled data, then fine-tuned on labeled outcomes. A non-autoregressive decoder with 1D convolutions and MLPs predicts future outcomes. The framework is designed to learn transferable representations that are balanced toward treatments, addressing temporal confounding issues.

## Key Results
- Reduces outcome estimation errors by over 6.2% on average across different datasets
- Outperforms existing baselines in both synthetic and real-world datasets
- Demonstrates strong performance in zero-shot transfer and data-efficient transfer learning setups
- Shows improved generalization to out-of-distribution data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The component-wise contrastive loss improves transferability by treating counterfactual estimation as unsupervised domain adaptation.
- Mechanism: By constructing positive pairs from the same sample under different transformations and applying contrastive loss separately to covariates, treatments, and outcomes, the model learns representations that are both predictive and treatment-balanced. This aligns with UDA theory where labeled source data (same treatment) and unlabeled target data (different treatment) exist.
- Core assumption: The treatment values are discrete, allowing partitioning of data into labeled (same treatment) and unlabeled (different treatment) subsets.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: The alternating temporal and feature-wise attention architecture captures both temporal dependencies and feature interactions more effectively than single-attention approaches.
- Mechanism: Temporal attention blocks apply causal self-attention along time for each feature independently, preserving temporal order. Feature-wise attention then models full self-attention across all features at each time step, enabling interaction learning. This hierarchical structure better captures the complex dynamics in time series data.
- Core assumption: Temporal dependencies and feature interactions are distinct and benefit from separate modeling rather than joint attention.
- Evidence anchors: [section], [section], [corpus]

### Mechanism 3
- Claim: Self-supervised pretraining with component-wise contrastive loss enables better generalization to out-of-distribution data and cold-start scenarios.
- Mechanism: By learning representations from unlabeled data through contrastive objectives, the model develops a richer understanding of the data structure that transfers better to unseen domains. The component-wise design ensures each aspect (covariates, treatments, outcomes) is well-represented.
- Core assumption: The distribution shifts in features between source and target domains can be bridged by learning invariant representations through self-supervision.
- Evidence anchors: [abstract], [section], [corpus]

## Foundational Learning

- Concept: Temporal Causal Attention
  - Why needed here: Preserves the temporal order information which is critical for causal inference and prevents information leakage from future to past
  - Quick check question: In the temporal attention block, what prevents tokens from attending to future time steps?

- Concept: Contrastive Learning and InfoNCE Loss
  - Why needed here: Provides a way to learn meaningful representations without labeled outcome data, crucial for generalization and cold-start scenarios
  - Quick check question: In the component-wise contrastive loss, what forms the positive and negative pairs for the covariate component?

- Concept: Domain Adaptation Theory
  - Why needed here: Provides theoretical justification for why the component-wise contrastive loss improves counterfactual estimation performance
  - Quick check question: In the UDA view, what corresponds to the labeled source domain and unlabeled target domain in counterfactual estimation?

## Architecture Onboarding

- Component map:
  - Input projection layer -> Feature positional encoding -> Alternating encoder blocks -> Final representations -> Momentum encoder + MLP -> Contrastive loss computation -> Final representations + treatments -> 1D convolutions -> MLP -> Outcome predictions

- Critical path:
  1. Input features → Input projection → Feature positional encoding → Alternating attention blocks → Final representations
  2. Final representations → Momentum encoder + MLP → Contrastive loss computation
  3. Final representations + treatments → 1D convolutions → MLP → Outcome predictions

- Design tradeoffs:
  - Temporal vs. feature-wise attention: Captures both temporal dependencies and feature interactions but increases model complexity
  - Self-supervised vs. supervised learning: Better generalization and cold-start performance but requires careful loss balancing
  - Non-autoregressive vs. autoregressive decoding: Faster inference but may miss temporal dependencies in outcome prediction

- Failure signatures:
  - Training instability with high variance in results (observed in CRN baseline)
  - Poor performance on short horizons when using non-autoregressive decoder
  - Failure to converge when using fully temporal attention instead of alternating attention

- First 3 experiments:
  1. Ablation study: Replace alternating attention with vanilla transformer and measure performance drop
  2. Hyperparameter sweep: Test different temperature values in contrastive loss on validation set
  3. Transfer learning test: Train on source domain, fine-tune on small target domain, measure improvement over baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the component-wise contrastive loss in COSTAR specifically improve temporal treatment outcome estimation compared to using a standard InfoNCE loss alone?
- Basis in paper: [explicit] The paper introduces a component-wise contrastive loss tailored for temporal treatment outcome observations, in addition to the standard contrastive loss of the entire sequence, and claims it improves performance.
- Why unresolved: The paper mentions the component-wise contrastive loss but does not provide a detailed ablation study isolating its specific contribution compared to a standard contrastive loss. It is unclear whether the improvement comes from the component-wise design or other factors.
- What evidence would resolve it: An ablation study comparing COSTAR with and without the component-wise contrastive loss, while keeping other components constant, would clarify its specific contribution.

### Open Question 2
- Question: What is the impact of the feature positional encoding (FPE) on COSTAR's performance, and how does the tree-based FPE compare to an absolute variant?
- Basis in paper: [explicit] The paper introduces a tree-based feature positional encoding and compares it to an absolute variant, reporting performance gains in ablation studies.
- Why unresolved: The paper provides performance differences between the tree-based and absolute FPE variants but does not explain the underlying reasons for the observed differences. It is unclear whether the gains are due to the hierarchical structure or other factors.
- What evidence would resolve it: Further analysis of the learned representations and attention weights with different FPE variants would provide insights into the impact of FPE on COSTAR's performance.

### Open Question 3
- Question: How does COSTAR handle continuous treatments, and what modifications would be needed to extend it to continuous treatment spaces?
- Basis in paper: [inferred] The paper focuses on discrete treatments and assumes that continuous treatments can be mapped to discrete variables with proper encoding. However, it does not provide details on how this encoding would be done or how it would affect the model's performance.
- Why unresolved: The paper does not address the challenge of continuous treatments, which is a common scenario in many real-world applications. It is unclear how COSTAR would perform with continuous treatments and what modifications would be needed to handle them effectively.
- What evidence would resolve it: Experiments evaluating COSTAR's performance on datasets with continuous treatments, along with modifications to handle continuous treatment spaces, would provide insights into its capabilities and limitations.

## Limitations

- The theoretical framing relies heavily on discrete treatment values, which may not generalize to continuous treatment settings
- Tree-based positional encoding implementation details are not fully specified, potentially affecting reproducibility
- Training instability observed in baseline models suggests potential sensitivity to random seeds or hyperparameters

## Confidence

- **High confidence**: Temporal and feature-wise attention architecture improves modeling of complex dependencies
- **Medium confidence**: Component-wise contrastive loss provides UDA-style benefits
- **Medium confidence**: Self-supervised pretraining improves out-of-distribution generalization

## Next Checks

1. Reproduce ablation studies: Implement and compare vanilla transformer vs. alternating attention architecture on synthetic data to verify the claimed 6.2% average improvement
2. Test continuous treatment scenarios: Evaluate COSTAR's performance when treatment values are continuous rather than discrete to identify limitations of the UDA framing
3. Analyze seed sensitivity: Run COSTAR and baseline models across multiple random seeds to quantify stability differences and validate claims about reduced variance