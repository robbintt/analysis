---
ver: rpa2
title: Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection
arxiv_id: '2310.19247'
source_url: https://arxiv.org/abs/2310.19247
tags:
- learning
- uncertainty
- event
- representation
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the class imbalance problem in social event
  detection. The key idea is to use evidential uncertainty to guide representation
  learning, focusing on improving detection for uncertain event classes rather than
  just the rare ones.
---

# Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection

## Quick Facts
- arXiv ID: 2310.19247
- Source URL: https://arxiv.org/abs/2310.19247
- Authors: 
- Reference count: 40
- Key outcome: This paper tackles the class imbalance problem in social event detection. The key idea is to use evidential uncertainty to guide representation learning, focusing on improving detection for uncertain event classes rather than just the rare ones. The proposed framework (UCL-SCED) uses an uncertainty-guided contrastive loss to push apart representations of uncertain classes in the latent space, combined with evidential deep learning and Dempster-Shafer theory for robust uncertainty estimation and multi-view classification. Experiments on three imbalanced datasets (Events2012_100, Events2018_100, CrisisLexT_7) show that UCL-SCED achieves state-of-the-art accuracy and F1 scores, with significant improvements particularly for uncertain event classes compared to existing social event detection and long-tail recognition methods.

## Executive Summary
This paper addresses the challenge of detecting social events in imbalanced datasets, where rare event classes are often poorly recognized. The authors propose a novel framework that uses evidential uncertainty to guide representation learning, focusing on improving detection for uncertain event classes rather than just the rare ones. By combining evidential deep learning, Dempster-Shafer theory, and uncertainty-guided contrastive learning, the framework achieves state-of-the-art performance on three imbalanced social event detection datasets, with significant improvements particularly for uncertain event classes.

## Method Summary
The proposed framework consists of three temporal-aware GNN encoders for different views (co-hashtag, co-entity, co-user), view-specific evidential deep learning classifiers, an uncertainty calibration module, a Dempster-Shafer theory combiner, and an uncertainty-guided contrastive learning loss. The framework estimates evidential uncertainty for each event class and uses this to guide contrastive learning, increasing inter-class margins in the latent space for uncertain classes. Multi-view evidential classifiers are combined via Dempster-Shafer theory to obtain robust uncertainty estimates, while uncertainty calibration ensures the estimated uncertainty accurately reflects model confidence. The total optimization objective combines representation adjustment loss, multi-view classifier error loss, uncertainty calibration loss, and multi-view commonality loss.

## Key Results
- UCL-SCED achieves state-of-the-art accuracy and F1 scores on three imbalanced datasets (Events2012_100, Events2018_100, CrisisLexT_7)
- Significant improvements particularly for uncertain event classes compared to existing social event detection and long-tail recognition methods
- The framework shows robustness to severe class imbalance with imbalance ratios up to 155:1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed framework improves detection accuracy for uncertain event classes by dynamically adjusting class margins in the latent space based on evidential uncertainty.
- Mechanism: The framework estimates evidential uncertainty for each event class and uses this to guide contrastive learning. For uncertain classes (high uncertainty), it increases the inter-class margin in the latent space, making these classes more separable from others. This adaptive margin adjustment focuses on representation quality rather than just frequency.
- Core assumption: Evidential uncertainty is a better indicator of model generalization and class separability than class frequency.
- Evidence anchors:
  - [abstract]: "compared to the rarity of classes, the calibrated uncertainty estimated from well-trained evidential deep learning networks better reflects model performance"
  - [section]: "we propose a novel uncertainty-guided contrastive learning loss (UCL and UCL-EC) to manipulate distinguishable representation distribution for imbalanced data. During training, they force all classes, especially uncertain ones, to adaptively adjust a clear separable boundary in the feature space"
- Break condition: If evidential uncertainty does not correlate with model performance or class separability, the margin adjustment would be ineffective.

### Mechanism 2
- Claim: Multi-view evidential classifiers combined via Dempster-Shafer theory provide more robust uncertainty estimates than single-view approaches.
- Mechanism: The framework uses three view-specific classifiers (co-hashtag, co-entity, co-user) that each estimate evidential uncertainty. These are combined using Dempster's rule to obtain a final uncertainty estimate that emphasizes agreement across views. This multi-view combination increases robustness to noise in any single view.
- Core assumption: Different views capture complementary information about events, and combining them through Dempster-Shafer theory provides more reliable uncertainty estimates.
- Evidence anchors:
  - [abstract]: "to obtain more robust and accurate class uncertainty, we combine the results of multi-view evidential classifiers via the Dempster-Shafer theory"
  - [section]: "we use Dempster-Shafer theory to combine the results from the three single views... The combination rule, also known as Dempster's rule, strongly emphasizes the agreement between multiple views and extracts their common shared beliefs as the final judgment"
- Break condition: If views are highly conflicting or provide redundant information, the combination may not improve robustness.

### Mechanism 3
- Claim: Uncertainty calibration ensures the estimated uncertainty accurately reflects model confidence, enabling effective representation adjustment.
- Mechanism: An additional calibration loss is applied to the evidential classifiers to ensure that confident predictions are accurate and uncertain predictions are inaccurate. This calibration creates a reliable mapping between prediction accuracy and uncertainty, which the representation adjustment module can use to set appropriate margins.
- Core assumption: Without calibration, evidential uncertainty may not accurately reflect true model confidence, making it unsuitable for guiding representation learning.
- Evidence anchors:
  - [abstract]: "to obtain more robust and accurate class uncertainty, we combine the results of multi-view evidential classifiers via the Dempster-Shafer theory under the supervision of an additional calibration method"
  - [section]: "Though the uncertainty can be modelled directly with EDL and DST, it may not be well calibrated... We adopt an uncertainty calibration method to build the correct relationship between accuracy and uncertainty"
- Break condition: If the calibration method fails to establish a reliable accuracy-uncertainty relationship, the uncertainty-guided margin adjustment would be ineffective.

## Foundational Learning

- Concept: Evidential deep learning and Dempster-Shafer theory
  - Why needed here: These provide a principled way to estimate uncertainty and combine multi-view evidence, which is central to the framework's approach to handling class imbalance
  - Quick check question: How does evidential deep learning differ from traditional probabilistic deep learning in terms of uncertainty estimation?

- Concept: Contrastive learning with prototype-based loss
  - Why needed here: The framework builds on prototypical supervised contrastive loss and modifies it with uncertainty-guided margins to improve representation learning for imbalanced data
  - Quick check question: What is the key difference between standard supervised contrastive loss and prototypical supervised contrastive loss?

- Concept: Graph neural networks for heterogeneous information networks
  - Why needed here: The framework uses temporal-aware GNNs to learn representations from co-hashtag, co-entity, and co-user views, capturing complex relationships in social event data
  - Quick check question: How do temporal-aware GNNs incorporate temporal information into message representations?

## Architecture Onboarding

- Component map: GNN encoder → evidential classifier → uncertainty estimation → calibration → Dempster-Shafer combination → uncertainty-guided contrastive loss → representation adjustment → improved classification
- Critical path: The framework's critical path follows the component map from input to output, with the uncertainty-guided contrastive loss serving as the key innovation that distinguishes it from standard approaches.
- Design tradeoffs: The framework trades computational complexity (multiple views, uncertainty estimation, calibration) for improved accuracy on uncertain classes. It also requires careful hyperparameter tuning for the balance between uncertainty calibration, contrastive learning, and other losses.
- Failure signatures: Poor performance on uncertain classes may indicate miscalibrated uncertainty estimates or ineffective margin adjustment. Degraded overall performance may suggest overfitting to the uncertainty calibration or improper balance between representation and classification learning.
- First 3 experiments:
  1. Implement the temporal-aware GNN encoder and verify it produces reasonable representations for each view independently
  2. Add the evidential deep learning classifier and verify uncertainty estimates correlate with prediction accuracy
  3. Implement the uncertainty-guided contrastive loss (UCL) and verify it adjusts margins based on uncertainty estimates as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed UCL framework perform on extremely long-tail distributions (imbalance ratio > 1000) beyond the tested datasets?
- Basis in paper: [explicit] The authors note their framework focuses on severe imbalance but only tested up to 155:1 ratio
- Why unresolved: The paper only tested on datasets with imbalance ratios up to 155:1, leaving uncertainty about performance on more extreme distributions
- What evidence would resolve it: Testing on datasets with imbalance ratios >1000, particularly measuring performance degradation thresholds

### Open Question 2
- Question: Can the UCL framework effectively handle multi-modal class distributions where a single prototype is insufficient?
- Basis in paper: [explicit] The authors explicitly acknowledge this limitation in their conclusion, stating "Both UCLSED and UCL-ECSED learn only one single prototype for each class"
- Why unresolved: The paper acknowledges this limitation but does not provide solutions or empirical evidence of performance with multi-modal classes
- What evidence would resolve it: Experiments comparing single vs multiple prototypes per class on datasets with known multi-modal class distributions

### Open Question 3
- Question: How does the proposed framework compare to frequency-based approaches when dealing with semantically similar but rare classes?
- Basis in paper: [inferred] The authors claim uncertainty is a better indicator than frequency but don't directly compare performance on semantically similar rare classes
- Why unresolved: While the paper discusses the superiority of uncertainty over frequency, it doesn't isolate cases where semantically similar rare classes might be confused
- What evidence would resolve it: Controlled experiments with datasets containing both semantically similar and dissimilar rare classes, comparing uncertainty-guided vs frequency-guided methods

## Limitations

- The framework's performance heavily depends on the quality of evidential uncertainty estimation and calibration, which may be challenging to achieve in practice
- The assumption that evidential uncertainty better reflects model performance than class frequency may not hold for all datasets or domains
- The framework trades computational complexity for improved accuracy, potentially limiting its applicability to resource-constrained settings

## Confidence

- High confidence in the core mechanism: Using evidential uncertainty to guide representation learning for imbalanced classes
- Medium confidence in the multi-view Dempster-Shafer combination: While theoretically sound, effectiveness depends on view complementarity and conflict levels
- Medium confidence in the uncertainty calibration: The framework assumes calibrated uncertainty is crucial, but the specific calibration method's impact is not extensively validated

## Next Checks

1. Conduct ablation studies removing the uncertainty calibration component to quantify its contribution to performance improvements
2. Test the framework on additional imbalanced datasets from different domains (e.g., image classification, text classification) to assess generalizability
3. Compare evidential uncertainty estimates with alternative uncertainty quantification methods (e.g., Monte Carlo dropout, deep ensembles) to validate the choice of evidential deep learning