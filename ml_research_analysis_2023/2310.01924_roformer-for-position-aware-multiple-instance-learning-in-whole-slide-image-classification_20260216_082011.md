---
ver: rpa2
title: RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image
  Classification
arxiv_id: '2310.01924'
source_url: https://arxiv.org/abs/2310.01924
tags:
- patches
- encoding
- position
- layer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a position-aware patch encoding module, implemented
  as a RoFormer layer, to address the need for modeling dependencies and spatial relationships
  between patches in whole slide image classification. The RoFormer layer uses memory-efficient
  exact self-attention with relative positional encoding to capture patch correlations
  while remaining computationally tractable for large gigapixel images.
---

# RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification

## Quick Facts
- **arXiv ID:** 2310.01924
- **Source URL:** https://arxiv.org/abs/2310.01924
- **Reference count:** 25
- **Primary result:** RoFormer layer improves WSI classification performance across three datasets by capturing patch correlations and spatial relationships

## Executive Summary
This paper addresses the challenge of whole slide image (WSI) classification by introducing a position-aware patch encoding module using a RoFormer layer. The approach enables efficient exact self-attention with relative positional encoding to model dependencies between patches while remaining computationally tractable for gigapixel images. Applied to ABMIL and DSMIL models, the method consistently outperforms baseline MIL models and recent transformer-based approaches on three standard WSI datasets, demonstrating that performance gains stem from improved modeling capabilities rather than simply increasing model capacity.

## Method Summary
The method involves patching WSIs into 256x256 tiles at x20 magnification, extracting frozen ResNet50 features, and applying a single RoFormer encoder layer with masked self-attention and 2D rotary position encoding (RoPE) based on patch coordinates. This encoding captures spatial relationships and patch correlations before MIL aggregation. The approach is integrated into existing ABMIL and DSMIL frameworks and trained with Adam (lr=1e-4, batch size=4) for up to 50 epochs with early stopping, evaluated on 10-fold stratified splits.

## Key Results
- RoFormerMIL consistently outperforms baseline ABMIL and DSMIL models across TCGA-NSCLC, Camelyon16, and BRACS datasets
- Performance improvements are attributed to better modeling of patch dependencies rather than increased parameter count
- RoFormerMIL surpasses recent transformer-based method TransMIL in WSI classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RoFormer layer enables efficient exact self-attention on gigapixel WSIs
- Mechanism: Memory-efficient attention implementation allows full self-attention without approximation, avoiding O(nÂ²) memory/time bottlenecks
- Core assumption: Memory-efficient attention can handle tens of thousands of patches on consumer GPUs
- Evidence anchors:
  - [abstract] "relying on memory-efficient exact self-attention and relative positional encoding"
  - [section 3.2] "recent breakthrough implementations such as Flash attention[4] or Memory Efficient attention[14], considerably reduced the burden of those operations while still providing exact full self attention"
  - [corpus] Weak evidence - related works focus on MIL improvements but don't directly address self-attention efficiency
- Break condition: GPU memory limits are exceeded or attention becomes the computational bottleneck

### Mechanism 2
- Claim: Rotary Position Embedding (RoPE) enables relative position encoding for irregularly shaped WSIs
- Mechanism: RoPE modifies key-query inner product to encode 2D patch coordinates without discrete position buckets
- Core assumption: Tissue structure in WSIs can be captured through relative 2D positional encoding
- Evidence anchors:
  - [abstract] "relative positional encoding for large WSIs, with arbitrary and irregular shapes"
  - [section 3.2] "RoPE provides an elegant solution to this problem by modifying directly the key-query inner product of self-attention, with minimal computation overhead and handling any maximum size of relative position"
  - [corpus] Weak evidence - corpus neighbors don't directly address RoPE for WSIs
- Break condition: Positional encoding becomes less effective for very sparse or irregular tissue distributions

### Mechanism 3
- Claim: RoFormer encoding improves MIL performance beyond parameter scaling
- Mechanism: Direct modeling of patch dependencies and tissue structure provides better representations than permutation-invariant pooling
- Core assumption: Spatial relationships between patches contain discriminative information for classification
- Evidence anchors:
  - [section 4.1] "the improvement is not only due to model capacity, but to the modeling itself"
  - [section 2.1] "However, they do not model the dependencies between patches nor their relative positions"
  - [corpus] Moderate evidence - related works acknowledge limitations of permutation-invariant approaches
- Break condition: Additional positional encoding provides diminishing returns or introduces overfitting

## Foundational Learning

- Concept: Multiple Instance Learning (MIL) formulation
  - Why needed here: WSIs only have slide-level labels, not patch-level labels
  - Quick check question: How does MIL treat a WSI with multiple patches and a single label?

- Concept: Self-attention mechanism
  - Why needed here: Need to model dependencies between thousands of patches
  - Quick check question: What is the computational complexity of standard self-attention and why is it problematic for WSIs?

- Concept: Positional encoding in transformers
  - Why needed here: Need to capture tissue structure and spatial relationships
  - Quick check question: What's the difference between absolute and relative positional encoding?

## Architecture Onboarding

- Component map:
  WSI patching module -> RoFormer layer -> MIL aggregator (ABMIL/DSMIL)

- Critical path:
  1. Patch extraction with coordinate tracking
  2. Memory-efficient self-attention with RoPE
  3. MIL pooling for slide-level prediction

- Design tradeoffs:
  - RoPE vs absolute positional encoding: RoPE handles irregular shapes better
  - Memory-efficient vs full attention: Exact computation vs approximation
  - Number of RoFormer layers: Model capacity vs overfitting risk

- Failure signatures:
  - GPU OOM errors: Reduce patch size or number of patches
  - Degraded performance: Check coordinate encoding or attention masking
  - Overfitting: Reduce model capacity or apply regularization

- First 3 experiments:
  1. Baseline ABMIL without RoFormer layer
  2. RoFormer with absolute positional encoding (validation of RoPE benefit)
  3. RoFormer with different coordinate scaling factors for RoPE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of RoFormer layers to use in WSI classification without overfitting, given the typically small dataset sizes?
- Basis in paper: [inferred] The paper states "we only use 1-layer encoder and keep as perspective work to scale to bigger models" and mentions that "WSI datasets are usually small... and adding too many parameters could lead to overfitting."
- Why unresolved: The paper only tested with one RoFormer layer and did not explore deeper architectures, leaving the question of optimal depth unanswered.
- What evidence would resolve it: Systematic experiments varying the number of RoFormer layers (e.g., 1, 2, 3, 4) on the same datasets while monitoring validation performance and overfitting indicators.

### Open Question 2
- Question: How does the RoFormer-based MIL approach perform on WSI classification tasks beyond H&E stained images, such as IHC images or other modalities?
- Basis in paper: [explicit] The limitations section states "The datasets we used... are all tumor classification on H&E images... Applying these methods to other modalities, like IHC images... would be interesting."
- Why unresolved: The experiments were limited to H&E stained WSIs for tumor classification, with no exploration of other imaging modalities or pathology tasks.
- What evidence would resolve it: Applying the RoFormerMIL approach to IHC-stained WSIs and other pathology imaging modalities, comparing performance to current state-of-the-art methods.

### Open Question 3
- Question: What is the computational overhead trade-off between using RoFormer layers and the performance gains achieved in WSI classification?
- Basis in paper: [inferred] The paper mentions "the computation overhead may be an issue for more complex approaches" and discusses memory-efficient attention implementations, but does not provide detailed runtime or memory usage comparisons.
- Why unresolved: While the paper demonstrates performance improvements, it does not quantify the computational cost in terms of inference time, training time, or memory usage compared to baseline methods.
- What evidence would resolve it: Benchmarking studies comparing wall-clock training and inference times, memory consumption, and GPU utilization between RoFormerMIL and baseline MIL approaches across different hardware configurations.

## Limitations
- Limited ablation studies on the optimal number of RoFormer layers or hyperparameters for RoPE scaling
- Performance gains demonstrated only on three datasets; generalization to other WSI classification tasks remains untested
- RoFormer layer implementation details not fully specified, requiring adaptation from general transformer code

## Confidence
- **High Confidence:** Memory-efficient self-attention enables exact computation on large WSIs (supported by flash attention literature)
- **Medium Confidence:** RoPE provides superior positional encoding for irregular WSIs compared to absolute encoding (supported by theoretical advantages but limited ablation)
- **Medium Confidence:** Performance improvements are due to modeling capabilities rather than parameter scaling (supported by comparative results but limited hyperparameter exploration)

## Next Checks
1. Conduct systematic ablation studies comparing RoPE vs absolute positional encoding across different WSI tissue types and densities
2. Evaluate model performance with varying numbers of RoFormer layers to determine optimal depth for different dataset complexities
3. Test the approach on additional WSI classification tasks beyond cancer diagnosis (e.g., tumor grading, biomarker prediction) to assess generalizability