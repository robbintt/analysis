---
ver: rpa2
title: 'Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end
  relation extraction: experiments with the rare disease use-case'
arxiv_id: '2311.13729'
source_url: https://arxiv.org/abs/2311.13729
tags:
- relation
- entity
- entities
- disease
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors compare three paradigms for end-to-end relation extraction:
  pipeline models, sequence-to-sequence models, and generative pre-trained transformer
  models. They evaluate these on the RareDis dataset, which involves complex relation
  extraction tasks with discontinuous and nested entities.'
---

# Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case

## Quick Facts
- arXiv ID: 2311.13729
- Source URL: https://arxiv.org/abs/2311.13729
- Authors: 
- Reference count: 33
- Key outcome: Pipeline models achieve highest F1-score on RareDis dataset for end-to-end relation extraction, outperforming both sequence-to-sequence and GPT models.

## Executive Summary
This paper compares three paradigms for end-to-end relation extraction (E2ERE) on a complex rare disease dataset. The authors evaluate pipeline models (SODNER + PURE), sequence-to-sequence models (Seq2Rel), and generative pre-trained transformer models (BioMedLM) on tasks involving discontinuous and nested entities. Their experiments demonstrate that pipeline models achieve the best performance, followed closely by sequence-to-sequence models, while the much larger generative transformer model performs worst. The authors attribute these results to architectural differences, particularly the bidirectional context modeling in BERT-based encoders versus the autoregressive nature of GPT models.

## Method Summary
The study evaluates three E2ERE approaches on the RareDis dataset: (1) a pipeline architecture using SODNER for discontinuous entities followed by PURE for flat/nested entities and relation extraction, (2) Seq2Rel which uses a PubMedBERT encoder with LSTM decoder and copy mechanism, and (3) BioMedLM, a generative transformer fine-tuned on relation extraction with supervised training and copy instructions. All models use PubMedBERT as their base encoder. The evaluation uses strict matching criteria for both entity spans and relation types, with F1-score as the primary metric.

## Key Results
- Pipeline models achieve the highest F1-score among all three approaches
- Seq2Rel performs second-best, closely following pipeline models
- BioMedLM, despite being over eight times larger than the best pipeline model, performs worst on the task
- Error analysis reveals distinct failure patterns for each model type

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pipeline models outperform generative models due to bidirectional contextual understanding from BERT encoders.
- Mechanism: BERT-style models (PubMedBERTbase/large) learn bidirectional representations through masked language modeling, enabling better entity and relation understanding than unidirectional GPT decoders.
- Core assumption: Bidirectional context is more effective than unidirectional generation for relation extraction tasks.
- Evidence anchors:
  - [abstract]: "BioMedLM is over eight times larger than our best-performing pipeline model... its low performance compared to the pipeline is not surprising because GPT models are autoregressive and do not benefit from language understanding arising from the bidirectional masked language modeling objective used in BERT models."
  - [section 2.2.3]: "BioMedLM is over eight times larger than our best-performing pipeline model (considering it has three encoders based on the encoder PubMedBERTbase, which has 110M parameters). However, its low performance compared to the pipeline is not surprising because GPT models are autoregressive and do not benefit from language understanding arising from the bidirectional masked language modeling objective used in BERT models."
  - [corpus]: Weak evidence - corpus shows other E2ERE papers but doesn't directly compare bidirectional vs unidirectional approaches.
- Break condition: If the task requires open-ended generation rather than structured extraction, bidirectional context may be less critical.

### Mechanism 2
- Claim: Pipeline models handle discontinuous entities better through specialized components (SODNER + PURE).
- Mechanism: SODNER identifies discontinuous entities using dependency parses and GCN networks, while PURE handles flat and nested entities. This two-stage approach outperforms single models that struggle with all entity types.
- Core assumption: Specialized models for different entity types are more effective than unified approaches.
- Evidence anchors:
  - [section 2.2.1]: "One weakness of PURE is that it does not handle discontinuous entities in its NER component while it easily handles flat and nested entities. So we needed to adapt the PURE approach to the RareDis setting. Since PURE is pipeline-based, we could simply use a different NER model for identifying discontinuous entities and retain the PURE model to spot flat and nested entities."
  - [section 2.1.2]: "Discontinuous 528 136 103" shows significant proportion of discontinuous entities in dataset.
  - [corpus]: Weak evidence - corpus shows related E2ERE papers but doesn't directly compare specialized vs unified approaches.
- Break condition: If dataset contains minimal discontinuous entities, the specialized approach advantage diminishes.

### Mechanism 3
- Claim: Seq2Rel performs better than BioMedLM due to constrained decoding with copy mechanism.
- Mechanism: Seq2Rel uses a copy mechanism that restricts output to observed input tokens, while BioMedLM generates freely from full vocabulary, leading to more errors in structured extraction tasks.
- Core assumption: Constrained generation is more effective than unconstrained generation for structured output tasks.
- Evidence anchors:
  - [section 2.2.3]: "Seq2Rel and BioMedLM, however, produce sequences (based on the schemas and templates selected) that need to be interpreted back into the triple format... Since their outputs are sequences, the training objective is the well-known auto-regressive language model objective based on predicting the next token given previously predicted tokens."
  - [section 4]: "BioMedLM generations not in the input: In several cases we noticed spans that were not in the input but were nevertheless closely linked with the gold entity span's meaning."
  - [corpus]: Weak evidence - corpus doesn't explicitly compare copy mechanism vs unconstrained generation.
- Break condition: If task requires more open-ended, natural language output rather than structured triples, constrained generation may be limiting.

## Foundational Learning

- Concept: Masked language modeling
  - Why needed here: Understanding why BERT models outperform GPT models in this task
  - Quick check question: What is the key difference between BERT's training objective and GPT's training objective?

- Concept: Span-based entity recognition
  - Why needed here: Critical for understanding how SODNER and PURE models identify entity spans
  - Quick check question: How do span-based NER models differ from token-based NER models?

- Concept: Copy mechanism in sequence-to-sequence models
  - Why needed here: Understanding why Seq2Rel performs better than BioMedLM
  - Quick check question: What is the purpose of the copy mechanism in Seq2Rel?

## Architecture Onboarding

- Component map:
  - Input layer: Raw text documents
  - Pipeline variant: SODNER (discontinuous entities) → PURE (flat/nested entities + relations)
  - Seq2Rel variant: PubMedBERT encoder → LSTM decoder with copy mechanism
  - BioMedLM variant: PubMedBERT-based generative model with supervised fine-tuning

- Critical path:
  - For pipeline: Document → SODNER NER → Entity transformation → PURE NER → PURE RE → Relations
  - For Seq2Rel: Document → PubMedBERT encoder → LSTM decoder → Relations
  - For BioMedLM: Document + prompt → BioMedLM generation → Post-processing → Relations

- Design tradeoffs:
  - Pipeline: More components, better handling of discontinuous entities, but more complex pipeline
  - Seq2Rel: Unified model, constrained generation, but may struggle with very long entities
  - BioMedLM: Simpler architecture, larger model size, but unconstrained generation leads to more errors

- Failure signatures:
  - Pipeline: Partial matches in entity recognition, entity type mismatches, annotation issues
  - Seq2Rel: Long entity span errors, space issues around special characters
  - BioMedLM: Generations not in input, overly creative entity spans, copy instruction non-compliance

- First 3 experiments:
  1. Run all three models on a small subset of RareDis data and compare F1 scores
  2. Analyze error patterns by manually examining predictions for each model type
  3. Test different entity type classification strategies to reduce entity type mismatch errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do generative pre-trained transformer models perform worse than pipeline and sequence-to-sequence models for end-to-end relation extraction?
- Basis in paper: [explicit] The authors state that GPT models are worse than sequence-to-sequence models and lose to pipeline models by over 10 F1 points. They attribute this to their autoregressive nature and lack of language understanding.
- Why unresolved: While the authors provide a hypothesis for the lower performance of GPT models, they do not provide concrete evidence or further investigation to support this claim. More in-depth analysis is needed to fully understand the limitations of GPT models for this task.
- What evidence would resolve it: Further experiments and analysis comparing the performance of GPT models with other models on various datasets, as well as investigating the impact of autoregressive nature and language understanding on the performance of GPT models for end-to-end relation extraction.

### Open Question 2
- Question: How can we effectively leverage the power of generative language models for end-to-end relation extraction?
- Basis in paper: [explicit] The authors mention that GPT models may not be suitable for the type of strict evaluation imposed in the study, and suggest using vector similarity or edit-distance metrics to map phrases to closest matches of the input. They also suggest using inference-only proprietary large models such as GPT-4 to generate paraphrases for training instances to create larger augmented training datasets.
- Why unresolved: While the authors provide some suggestions, they do not provide concrete evidence or experiments to demonstrate the effectiveness of these approaches. Further research is needed to explore and validate these ideas.
- What evidence would resolve it: Experiments comparing the performance of GPT models using vector similarity or edit-distance metrics for evaluation, as well as experiments using GPT-4 to generate paraphrases for training instances and evaluating the impact on end-to-end relation extraction performance.

### Open Question 3
- Question: How can we improve the performance of sequence-to-sequence models for end-to-end relation extraction?
- Basis in paper: [explicit] The authors mention that sequence-to-sequence models are not far behind pipeline models in terms of performance, but do not provide further details or suggestions for improvement.
- Why unresolved: While the authors acknowledge the potential of sequence-to-sequence models, they do not provide concrete strategies or experiments to enhance their performance. Further research is needed to explore and validate approaches for improving sequence-to-sequence models for this task.
- What evidence would resolve it: Experiments comparing the performance of sequence-to-sequence models with different architectures, training strategies, and evaluation metrics, as well as investigating the impact of various factors such as data augmentation, pre-training, and fine-tuning on the performance of sequence-to-sequence models for end-to-end relation extraction.

## Limitations

- The study focuses on a single dataset with specific entity types (discontinuous, nested, and flat), limiting generalizability to other domains or simpler entity structures
- Strict matching criteria may penalize models disproportionately for minor annotation differences rather than substantive errors
- The study doesn't explore model ensembling or hybrid approaches that might combine strengths of different paradigms

## Confidence

- High confidence: Pipeline model superiority over BioMedLM (statistically significant F1-score differences, consistent across error analysis patterns)
- Medium confidence: Seq2Rel's intermediate performance (smaller performance gaps, potential dataset-specific effects)
- Low confidence: Attribution of performance differences solely to bidirectional vs unidirectional training objectives (correlation established but alternative explanations not fully explored)

## Next Checks

1. Test models on a dataset with predominantly flat entities to isolate the impact of discontinuous entity handling on performance differences
2. Conduct ablation studies removing the copy mechanism from Seq2Rel to quantify its contribution to performance gains
3. Evaluate models using relaxed matching criteria (partial span matches) to determine if strict evaluation unfairly penalizes generative approaches