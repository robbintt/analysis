---
ver: rpa2
title: Learning Geometric Representations of Objects via Interaction
arxiv_id: '2309.05346'
source_url: https://arxiv.org/abs/2309.05346
tags:
- agent
- object
- representation
- learning
- zext
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning structured representations
  of a scene involving an agent and an external object it interacts with. The core
  idea is to use actions performed by the agent as the only source of supervision
  to extract the geometric location of both the agent and object in physical space.
---

# Learning Geometric Representations of Objects via Interaction

## Quick Facts
- arXiv ID: 2309.05346
- Source URL: https://arxiv.org/abs/2309.05346
- Reference count: 40
- Primary result: Learns isometric representations of agent and object states using only action-based supervision, outperforming vision-based approaches on downstream control tasks.

## Executive Summary
This paper addresses the problem of learning structured representations of scenes involving an agent interacting with an external object, using only the agent's actions as supervision. The framework learns isometric representations that disentangle agent and object states by enforcing equivariance constraints and detecting contact-based interactions. The approach is theoretically grounded with guarantees that an ideal learner will recover the underlying state geometry, and empirically validated on various scenarios including both point-like and volumetric objects.

## Method Summary
The method splits the representation into agent (φint) and object (φext) components, learning from observation-action-observation transitions. It enforces equivariance constraints on the agent representation (z'int = zint + a), infers object location based on contact with the agent, and uses contrastive learning to classify interaction vs non-interaction transitions. The framework employs separate ResNet-18 encoders for agent states, object states, and contrastive space, trained with combined losses including equivariance, contact-based, and contrastive components.

## Key Results
- Outperforms state-of-the-art vision-based approaches on geometric representation learning
- Successfully learns isometric representations that preserve Euclidean geometry of states
- Enables efficient downstream control tasks via reinforcement learning
- Extends to volumetric objects with orientation information encoded in Gaussian covariance matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework learns isometric representations of both agent and object states using only action-based supervision.
- Mechanism: By enforcing equivariance (z'int = zint + a) and contact-based constraints, the representation φ must recover the underlying state geometry up to a constant translation.
- Core assumption: Actions induce deterministic transitions in observations (Assumption 1), and object displacement only occurs at contact points (Assumption 2).
- Evidence anchors:
  - [abstract] "the actions performed by the agent as the only source of supervision"
  - [section] "Theorem 4 states that if the conditions 1. − 3. are satisfied... then the representation recovers the inaccessible state up to a translation"
  - [corpus] Weak evidence - corpus neighbors focus on object manipulation and keypoint learning, not isometric recovery guarantees.
- Break condition: If the observation-emission map ω is not injective, the equivariance constraint cannot uniquely determine state geometry.

### Mechanism 2
- Claim: The contact-based loss L+ correctly distinguishes interacting from non-interacting transitions.
- Mechanism: When interaction occurs, zext must lie on the segment between zint and zint+a; otherwise zext remains constant. Otsu's algorithm partitions transitions based on contrastive distance dW(w,w').
- Core assumption: Interaction-free transitions produce smaller contrastive distances than interacting ones in the latent space W.
- Evidence anchors:
  - [section] "When an interaction occurs, zext should belong to the segment ⌊zint, zint + a⌋"
  - [section] "we propose to partition... the dataset in two disjoint classes... by applying a natural thresholding algorithm to the quantities dW(w, w')"
  - [corpus] Weak evidence - corpus focuses on object models and reconstruction, not contrastive transition classification.
- Break condition: If environmental noise causes overlapping contrastive distances between interaction classes, Otsu's algorithm cannot reliably separate them.

### Mechanism 3
- Claim: Volumetric objects are represented as Gaussian distributions whose covariance encodes the inertia ellipsoid.
- Mechanism: Stochastic outputs provide probabilistic state representations; the covariance matrix diagonalization yields orientation information via rotation matrices in SO(n).
- Core assumption: The object's shape can be approximated by an ellipsoid, and orientation is encoded in the covariance matrix structure.
- Evidence anchors:
  - [section] "the output of φext consists of (parameters of) a Gaussian distribution whose covariance matrix represents the inertia ellipsoid"
  - [section] "the orientation of the object can be extracted in the form of a rotation matrix in SO( n)"
  - [corpus] Weak evidence - corpus neighbors discuss 3D reconstruction but not probabilistic volumetric representation via Gaussian distributions.
- Break condition: If the object's shape significantly deviates from ellipsoidal symmetry, the Gaussian approximation fails to capture accurate orientation.

## Foundational Learning

- Concept: Equivariance under Lie group actions
  - Why needed here: Ensures the agent's internal state representation respects the geometry of its action space (translations in Rn)
  - Quick check question: If the agent performs action a followed by action b, what should be the relationship between zint, z'int, and z''int in an equivariant representation?

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Provides a self-supervised signal to enforce injectivity and distinguish interacting from non-interacting transitions
  - Quick check question: In the InfoNCE loss formulation, what happens to the gradient when two observations from the same transition pair are pulled too close together?

- Concept: Probabilistic modeling with Gaussian distributions
  - Why needed here: Enables representation of volumetric objects with orientation information encoded in covariance structure
  - Quick check question: Given a 2D Gaussian with covariance matrix [[a, b], [b, c]], what geometric property of the object does the ratio a/c represent?

## Architecture Onboarding

- Component map:
  - φint: ResNet-18 encoder for agent state representation (Zint = Rn)
  - φext: ResNet-18 encoder for object state representation (Zext = Rn, Gaussian parameters)
  - φcont: ResNet-18 encoder for contrastive space W (used for interaction detection)
  - Losses: Lint (equivariance), Lext (contact-based), Lcont (contrastive)

- Critical path: Collect (o,a,o') transitions → Compute representations → Apply Otsu's algorithm to dW distances → Select L+ or L- → Backpropagate combined loss

- Design tradeoffs:
  - Using ResNet-18 provides sufficient capacity while remaining computationally efficient
  - Separate encoders for Zint, Zext, and W allow specialized representations but increase parameter count
  - Otsu's algorithm is simple but may fail with noisy contrastive distances

- Failure signatures:
  - Ltest not converging to zero indicates poor geometric recovery
  - High variance in Ltest across runs suggests instability in contrastive classification
  - Transporter network outperforming our method on geometric tasks indicates representation collapse

- First 3 experiments:
  1. Implement the basic framework on Sprites dataset without dynamic background; verify Ltest < 0.1 after 50 epochs
  2. Add dynamic background to Sprites; confirm Otsu's algorithm correctly classifies >90% of transitions
  3. Implement volumetric object extension; test on anisotropic object dataset and verify orientation recovery via covariance eigenvalues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be extended to handle multiple interacting objects in the environment?
- Basis in paper: explicit - "Throughout the work we assumed that the agent interacts with a single object. An interesting line of future investigation is extending the framework to take multiple objects into account."
- Why unresolved: The paper only provides theoretical results and empirical validation for a single object scenario. Extending to multiple objects would require new mathematical formulations and potentially different loss functions.
- What evidence would resolve it: Experimental results demonstrating successful application of the framework to environments with multiple interactive objects, along with theoretical proofs showing that the extended framework still guarantees isometric representations.

### Open Question 2
- Question: How does the framework perform in scenarios with non-deterministic object dynamics beyond the uniform random displacement case?
- Basis in paper: inferred - The paper assumes deterministic transitions for simplicity but mentions that "the law governing the transition of the object is assumed to be unknown and can be arbitrarily complex and stochastic."
- Why unresolved: The experiments primarily use deterministic or uniformly random object dynamics. Performance with more complex stochastic dynamics is not explored.
- What evidence would resolve it: Experiments applying the framework to environments with non-uniform or state-dependent stochastic object dynamics, comparing performance against baseline methods.

### Open Question 3
- Question: What is the impact of observation noise or partial observability on the framework's ability to learn isometric representations?
- Basis in paper: inferred - The paper assumes "that ω is injective so that actions induce deterministic transitions of observations" which implies full observability.
- Why unresolved: The theoretical framework and experiments assume perfect observations without noise. Real-world scenarios often involve noisy or partially observable data.
- What evidence would resolve it: Experiments testing the framework's performance with varying levels of observation noise or partial observability, including analysis of how noise affects the learned representations and downstream task performance.

## Limitations
- The theoretical guarantee of isometric representation recovery critically depends on the observation-emission map ω being injective, which is never verified empirically
- The contrastive transition classification via Otsu's algorithm assumes clean separation of interaction classes, but no quantitative analysis of failure rates is provided
- The extension to volumetric objects relies on Gaussian approximations that may poorly capture non-ellipsoidal shapes

## Confidence
- **High**: The basic framework architecture (separate encoders, equivariance constraints) is well-specified and reproducible
- **Medium**: The contact-based interaction detection mechanism works in controlled scenarios but may fail with realistic noise
- **Low**: The volumetric object extension and orientation recovery claims lack sufficient empirical validation

## Next Checks
1. **Verify Injective Observation Mapping**: Systematically test whether the observation-emission map ω preserves distinguishability by measuring classification accuracy of unique observation pairs under various noise conditions.

2. **Stress-Test Otsu's Classification**: Create datasets with varying levels of contrastive distance overlap between interaction classes and measure false positive/negative rates to establish robustness bounds.

3. **Evaluate Gaussian Approximation Limits**: Test the volumetric object representation on objects with non-ellipsoidal geometries (e.g., asymmetric polyhedra) and quantify orientation estimation errors compared to ground truth.