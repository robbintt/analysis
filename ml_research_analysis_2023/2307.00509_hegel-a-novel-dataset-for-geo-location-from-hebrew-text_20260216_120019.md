---
ver: rpa2
title: 'HeGeL: A Novel Dataset for Geo-Location from Hebrew Text'
arxiv_id: '2307.00509'
source_url: https://arxiv.org/abs/2307.00509
tags:
- place
- task
- descriptions
- hegel
- hebrew
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present HeGeL, the first-ever dataset for geolocation from Hebrew
  text. We collect 5,649 place descriptions for various place types in three cities
  in Israel, where the text uses geospatial reasoning and requires full environmental
  representation.
---

# HeGeL: A Novel Dataset for Geo-Location from Hebrew Text

## Quick Facts
- arXiv ID: 2307.00509
- Source URL: https://arxiv.org/abs/2307.00509
- Reference count: 10
- We present HeGeL, the first-ever dataset for geolocation from Hebrew text, requiring geospatial reasoning and full environmental representation.

## Executive Summary
This paper introduces HeGeL, the first dataset specifically designed for geolocation from Hebrew text descriptions. The dataset contains 5,649 place descriptions across three Israeli cities, requiring geospatial reasoning beyond simple named entity recognition. The authors demonstrate that standard NER-based approaches fail on this task, highlighting the need for models that can perform full environmental spatial reasoning. The work establishes a new benchmark for Hebrew geolocation and provides insights into the challenges of processing morphologically rich languages for spatial tasks.

## Method Summary
The authors propose a dual-encoder model using AlephBERT for text encoding and S2Cells from OpenStreetMap for environment representation. The model learns to map text descriptions to environmental embeddings through cosine similarity and cross-entropy loss. For reproduction, one would need to download the HeGeL dataset, construct OSM-based environment graphs with S2Cells at level 13, and train the dual-encoder architecture with AlephBERT as the text encoder.

## Key Results
- Named entity recognition and gazetteer lookups alone are insufficient for accurate geolocation in HeGeL
- Cross-city evaluation reveals significant out-of-vocabulary challenges due to city-specific place names
- Few-shot experiments demonstrate the importance of using in-domain data for fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hebrew text geolocation requires full-environment spatial reasoning beyond local route-based descriptions.
- Mechanism: HeGeL descriptions contain multiple physical entities and geospatial relations requiring reasoning over a broad region rather than just agent-centric navigation.
- Core assumption: The descriptions reflect survey-level spatial knowledge rather than route-level knowledge, demanding a map-like environmental model.
- Evidence anchors:
  - "The frequent use of cardinal directions, as well as the use of survey knowledge, suggests that any NLP model built to deal with the HeGeL task should not only represent a local view of the goal, or possible routes, but also take into consideration the full region, and mimic people’s map-like view of the environment."
  - Limited: corpus includes 6,663 unique lemmas and 9,207 tokens, but no explicit analysis of entity density per description.
- Break condition: If descriptions were route-centric with few environmental references, local route-based models would suffice.

### Mechanism 2
- Claim: Named entity recognition and gazetteer lookups alone are insufficient for accurate geolocation in HeGeL.
- Mechanism: Descriptions often use spatial terms, cardinal directions, and relative positioning that cannot be resolved without geospatial reasoning.
- Core assumption: Proper names of the target and nearby landmarks are excluded from descriptions to force reliance on spatial reasoning.
- Evidence anchors:
  - "The task of textual geolocation — retrieving the coordinates of a place based on a free-form language description — calls for not only grounding but also natural language understanding and geospatial reasoning."
  - "The Google API model’s low performance suggests that NER and the Gazetteer-based methods in and of themselves are insufficient to handle the HeGeL task successfully, and that geospatial reasoning is necessary."
  - Confirmed: participants were instructed not to use proper names, and analysis shows low frequency of named entities (avg 0.55 per description).
- Break condition: If descriptions contained explicit proper names for both target and landmarks, NER-based approaches would perform adequately.

### Mechanism 3
- Claim: Cross-city evaluation reveals significant out-of-vocabulary challenges due to city-specific place names.
- Mechanism: Training on one city and testing on another exposes models to place names and spatial terms not seen during training.
- Core assumption: The three cities (Tel Aviv, Haifa, Jerusalem) have distinct vocabularies with minimal overlap, creating realistic OOV conditions.
- Evidence anchors:
  - "From the Venn diagram we also conclude that almost half of the lemmas of the three vocabularies, corresponding to the three cities, contain city-specific lemmas: 48.6%, 40.65%, and 49.3% for Tel Aviv, Haifa, and Jerusalem, respectively."
  - Supported: the corpus shows 6,663 unique lemmas total, with limited intersection between city vocabularies.
- Break condition: If city vocabularies had high overlap, OOV would not be a significant challenge.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: Initial baseline models attempt geolocation by recognizing named entities, highlighting the need to understand why this approach fails.
  - Quick check question: What is the average number of named entities per HeGeL description, and how does this impact geolocation accuracy?

- Concept: Geospatial Reasoning
  - Why needed here: HeGeL descriptions use spatial terms and cardinal directions requiring models to reason about relative positions and environmental layout.
  - Quick check question: How do spatial terms like "east of" or "two buildings away" differ from explicit named entity references in terms of geolocation complexity?

- Concept: Morphological Richness in Hebrew
  - Why needed here: Hebrew's morphological complexity affects parsing and understanding of place descriptions, influencing model design.
  - Quick check question: What challenges does Hebrew's morphological richness pose for NLP tasks like part-of-speech tagging and named entity recognition?

## Architecture Onboarding

- Component map:
  Text description → AlephBERT (768-dim) → text vector
  Environment graph → S2Cells (level 13) → random-walk encoding (64-dim → 768-dim) → environment vector
  Cosine similarity between text and environment vectors → cross-entropy loss

- Critical path:
  1. Text description → AlephBERT → 768-dim vector
  2. Environment graph → S2Cells → random-walk encoding → 768-dim vector
  3. Cosine similarity between vectors
  4. Cross-entropy loss → parameter updates

- Design tradeoffs:
  - AlephBERT vs. multilingual models: Hebrew-specific pre-training vs. broader coverage
  - S2Cell granularity: Level 13 chosen (vs. 15 or 17) balancing detail and computational cost
  - Dual-encoder vs. end-to-end: Separate encoding allows different pre-training but requires alignment learning

- Failure signatures:
  - High mean retrieval error (>2000m) indicates model fails to align text and environment representations
  - Low few-shot improvement suggests difficulty generalizing to new city vocabularies
  - Standard deviation across random initializations indicates instability in training

- First 3 experiments:
  1. Evaluate baseline AlephBERT text encoder alone on zero-shot city split to establish text-only performance ceiling
  2. Test dual-encoder with random environment encoding (no S2Cell structure) to isolate impact of spatial graph representation
  3. Implement data augmentation by generating synthetic descriptions for target city using templates from training city

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HeGeL dataset perform on other morphologically rich languages like Arabic or Russian?
- Basis in paper: [inferred] The paper discusses the lack of datasets for low-resource languages, including morphologically rich languages like Hebrew, and presents HeGeL as a solution. However, it does not provide results for other languages.
- Why unresolved: The paper focuses solely on Hebrew and does not compare the performance of HeGeL on other morphologically rich languages.
- What evidence would resolve it: Results from experiments using HeGeL on Arabic or Russian would provide insights into its generalizability to other morphologically rich languages.

### Open Question 2
- Question: What is the impact of using a larger dataset for fine-tuning the dual-encoder model?
- Basis in paper: [explicit] The paper mentions that the few-shot experiments demonstrate the importance of using in-domain data for fine-tuning, and the dual-encoder model shows improvement with 20% and 80% of the samples in the test-region.
- Why unresolved: The paper does not explore the effect of using a larger dataset for fine-tuning the dual-encoder model.
- What evidence would resolve it: Results from experiments using a larger dataset for fine-tuning the dual-encoder model would provide insights into its potential performance improvement.

### Open Question 3
- Question: How does the performance of the HeGeL dataset compare to other geolocation datasets in terms of retrieval error and task completion accuracy?
- Basis in paper: [explicit] The paper presents the HeGeL dataset and compares its performance to other baseline models, such as Google Maps API and Oracle NER, but does not provide a direct comparison to other geolocation datasets.
- Why unresolved: The paper does not provide a direct comparison of HeGeL's performance to other geolocation datasets in terms of retrieval error and task completion accuracy.
- What evidence would resolve it: Results from experiments comparing HeGeL's performance to other geolocation datasets in terms of retrieval error and task completion accuracy would provide insights into its relative performance.

## Limitations
- Dataset size of 5,649 descriptions across three cities is relatively small compared to established geolocation datasets in other languages
- Qualitative analysis relies heavily on linguistic intuition about Hebrew spatial expressions without systematic empirical validation
- Zero-shot and few-shot experimental results are based on limited hyperparameter tuning

## Confidence
- Mechanism 1: Medium - supported by qualitative analysis but lacks quantitative validation of entity density and spatial complexity
- Mechanism 2: Medium - empirical results show low performance but alternative NLP approaches were not thoroughly explored
- Mechanism 3: Medium - well-supported by vocabulary overlap analysis but practical impact on model performance could be more thoroughly characterized

## Next Checks
1. Conduct a quantitative analysis of spatial relation density and environmental entity counts per description to empirically validate the claim about full-environment reasoning requirements.
2. Implement and evaluate additional baseline models beyond NER and gazetteer approaches, including transformer-based models fine-tuned on the full HeGeL dataset, to establish a more comprehensive performance baseline.
3. Perform ablation studies systematically varying S2-cell resolution and text encoding strategies to identify the most critical components for geolocation performance.