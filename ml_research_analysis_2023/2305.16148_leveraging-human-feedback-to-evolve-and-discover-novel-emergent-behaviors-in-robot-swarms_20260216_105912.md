---
ver: rpa2
title: Leveraging Human Feedback to Evolve and Discover Novel Emergent Behaviors in
  Robot Swarms
arxiv_id: '2305.16148'
source_url: https://arxiv.org/abs/2305.16148
tags:
- behaviors
- behavior
- space
- swarm
- emergent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a human-in-the-loop method for discovering emergent
  swarm behaviors using self-supervised learning and active similarity queries to
  replace hand-engineered behavior representations. The approach learns a latent behavior
  space via contrastive learning and human feedback, combined with heuristic filtering
  to focus search on interesting controllers.
---

# Leveraging Human Feedback to Evolve and Discover Novel Emergent Behaviors in Robot Swarms

## Quick Facts
- **arXiv ID**: 2305.16148
- **Source URL**: https://arxiv.org/abs/2305.16148
- **Reference count**: 40
- **Primary result**: Discovers all six known emergent behaviors for single-sensor swarms and two novel behaviors (nested cycles and concave paths) for two-sensor swarms with 19% improvement over prior work.

## Executive Summary
This work presents a human-in-the-loop method for discovering emergent swarm behaviors using self-supervised learning and active similarity queries to replace hand-engineered behavior representations. The approach learns a latent behavior space via contrastive learning and human feedback, combined with heuristic filtering to focus search on interesting controllers. Tested on single- and two-sensor computation-free robots, the method discovers all six known emergent behaviors for single-sensor swarms and outperforms prior work by 19% on the two-sensor model, uncovering two novel behaviors (nested cycles and concave paths). The learned embedding achieves up to 88% accuracy in distinguishing behaviors and consistently finds richer behavior sets than random sampling or hand-crafted metrics.

## Method Summary
The approach adapts to user preferences by learning a similarity space over swarm collective behaviors using self-supervised learning and human-in-the-loop queries. It begins with self-supervised pretraining using triplet loss on randomly sampled controller trajectories, then refines the embedding through human similarity queries. Heuristic filters identify and eliminate controllers likely to produce uninteresting behaviors before evolution. Novelty search with the learned embedding discovers diverse behaviors, which are clustered using k-medoids to identify distinct emergent patterns. The method is evaluated on both single- and two-sensor computation-free robot models, demonstrating superior performance in discovering known and novel behaviors compared to baseline approaches.

## Key Results
- Discovers all six known emergent behaviors for single-sensor swarms
- Achieves 88% accuracy in behavior distinction with learned embedding
- Outperforms prior work by 19% on two-sensor model and discovers two novel behaviors (nested cycles and concave paths)
- Human-in-the-loop approach consistently finds richer behavior sets than random sampling or hand-crafted metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The self-supervised pretraining step creates a behavior embedding space that already captures meaningful distinctions between emergent behaviors without any human labels.
- **Mechanism**: The triplet loss learns to map similar behaviors (anchor and augmented positive) close together and dissimilar behaviors (anchor and negative) far apart. This forces the network to discover intrinsic structure in the behavior space.
- **Core assumption**: The random sampling of controllers during pretraining captures a diverse enough set of behaviors for the contrastive learning to learn useful features.
- **Evidence anchors**: [abstract], [section], [corpus]

### Mechanism 2
- **Claim**: The human-in-the-loop active learning stage refines the embedding space to better align with human perception of behavior differences.
- **Mechanism**: Human similarity queries provide class labels that are converted into triplets for fine-tuning the pretrained network, shifting the embedding to better separate behaviors that humans find distinct.
- **Core assumption**: Humans can reliably distinguish between emergent behaviors even without knowing what behaviors are possible.
- **Evidence anchors**: [abstract], [section], [corpus]

### Mechanism 3
- **Claim**: The heuristic filtering removes controllers that will not lead to interesting emergent behaviors, making the search more efficient without losing diversity.
- **Mechanism**: Five heuristics (slow swarms, passive swarms, neglectful swarms) identify controllers likely to produce uninteresting behaviors based on their velocity patterns before simulation, reducing wasted computation.
- **Core assumption**: Controllers identified as slow, passive, or neglectful by the heuristics will not produce interesting emergent behaviors within the simulation time horizon.
- **Evidence anchors**: [abstract], [section], [corpus]

## Foundational Learning

- **Concept**: Contrastive learning with triplet loss
  - **Why needed here**: Creates a meaningful low-dimensional embedding of high-dimensional swarm behavior trajectories without requiring labeled data
  - **Quick check question**: How does the triplet loss ensure that similar behaviors are mapped close together in the embedding space?

- **Concept**: Novelty search
  - **Why needed here**: Evolves controllers to explore diverse emergent behaviors rather than optimizing for a specific task
  - **Quick check question**: What is the fitness function used in novelty search and how does it encourage diversity?

- **Concept**: Convolutional neural networks for image processing
  - **Why needed here**: Maps 50x50 grayscale images of swarm trajectories to low-dimensional behavior vectors
  - **Quick check question**: Why are convolutional layers particularly suited for processing the trajectory images?

## Architecture Onboarding

- **Component map**: Simulation environment (Pygame-based) -> Data generation pipeline (random controller sampling and trajectory rendering) -> Neural network (CNN with 3 conv layers, 3 FC layers, triplet loss) -> Training loop (self-supervised pretraining, then active learning with human feedback) -> Heuristic filter (5 criteria for identifying uninteresting controllers) -> Evolution engine (novelty search with k-medoids clustering) -> Evaluation pipeline (accuracy measurement on validation set)

- **Critical path**: Simulation → Data generation → Pretraining → Active learning → Evolution → Clustering → Behavior discovery

- **Design tradeoffs**:
  - Pretraining vs. human-in-the-loop: More pretraining reduces human effort but may miss human-relevant distinctions
  - Filter aggressiveness: Stricter filtering saves computation but risks removing interesting behaviors
  - Embedding dimensionality: Higher dimensions capture more nuance but increase computational cost

- **Failure signatures**:
  - Low accuracy on validation set: Pretraining data insufficient or network architecture inadequate
  - No novel behaviors discovered: Evolution parameters too conservative or embedding space poorly aligned with behavior space
  - High variance in results: Insufficient training data or unstable network training

- **First 3 experiments**:
  1. Test the accuracy of the randomly initialized network on the validation set to establish baseline performance
  2. Run pretraining with different learning rates and batch sizes to optimize contrastive learning
  3. Evaluate the impact of different filter thresholds on the number of behaviors discovered vs. computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the minimum number of similarity queries required for the human-in-the-loop approach to achieve performance comparable to hand-crafted behavior mappings?
- **Basis in paper**: [explicit] The paper states that self-supervised training followed by supervised training with only 1% of truth labels can significantly improve network performance, and that adding a human to the training loop will result in a more accurate learned similarity metric.
- **Why unresolved**: The paper does not provide specific data on the minimum number of queries needed or conduct an ablation study to determine the optimal number of human queries for best performance.
- **What evidence would resolve it**: Conducting experiments with varying numbers of human similarity queries (e.g., 0%, 0.1%, 1%, 5%, 10%) and measuring the accuracy of the learned behavior embedding would provide insight into the minimum required for comparable performance.

### Open Question 2
- **Question**: How does the learned latent behavior space generalize to different swarm sizes or environmental conditions beyond the tested 24 agents and 500x500 unit environment?
- **Basis in paper**: [inferred] The paper mentions that the approach can be extended to other forms of workspaces as long as swarm trajectories can be represented visually, but does not test this generalization.
- **Why unresolved**: The experiments are limited to a specific swarm size and environment, so it's unclear if the learned embedding would perform similarly with different swarm sizes or environmental complexities.
- **What evidence would resolve it**: Testing the approach with varying swarm sizes (e.g., 10, 50, 100 agents) and different environmental configurations (e.g., obstacles, varying sizes) would demonstrate the robustness and generalization of the learned behavior space.

### Open Question 3
- **Question**: What is the computational overhead of the heuristic filtering approach compared to the potential gains in search efficiency?
- **Basis in paper**: [explicit] The paper introduces heuristic filtering to improve the efficiency of novelty search by predicting ahead of time whether a controller will result in an interesting emergent behavior, but does not quantify the computational cost versus benefit.
- **Why unresolved**: While the paper shows that filtering reduces the number of uninteresting behaviors, it does not provide data on the computational time saved or the trade-off between filtering computation and evolutionary search efficiency.
- **What evidence would resolve it**: Measuring the total computation time (including filtering and evolutionary search) with and without the heuristic filter, and comparing the number of emergent behaviors discovered, would quantify the efficiency gains.

## Limitations
- The effectiveness depends on random controller sampling capturing sufficient behavioral diversity for pretraining
- Human-in-the-loop refinement assumes reliable behavior distinction without domain expertise
- Heuristic filtering may eliminate potentially interesting behaviors if thresholds are too aggressive

## Confidence
- **High confidence**: 19% performance improvement over prior work and discovery of two novel behaviors are well-supported by quantitative results
- **Medium confidence**: 88% embedding accuracy claim appears robust but depends on the quality and representativeness of the validation dataset
- **Low confidence**: Generalizability of human-in-the-loop refinement across different user groups and long-term stability of discovered behaviors under environmental variations

## Next Checks
1. **Replication of embedding accuracy**: Independently implement the CNN architecture and training procedure to verify the 88% accuracy claim on the same validation set
2. **Human labeler consistency study**: Conduct a multi-participant study to measure inter-rater agreement on behavior similarity judgments and quantify the impact of labeling noise on embedding quality
3. **Filter threshold sensitivity analysis**: Systematically vary the heuristic filter thresholds and measure the trade-off between computational efficiency and behavioral diversity in the discovered behaviors