---
ver: rpa2
title: Temporal-Distributed Backdoor Attack Against Video Based Action Recognition
arxiv_id: '2308.11070'
source_url: https://arxiv.org/abs/2308.11070
tags:
- backdoor
- attack
- video
- attacks
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a temporal-distributed backdoor attack against
  video-based action recognition systems. The core idea is to embed imperceptible,
  temporally distributed triggers across video frames by perturbing components in
  transformed domains like DFT or DCT.
---

# Temporal-Distributed Backdoor Attack Against Video Based Action Recognition

## Quick Facts
- arXiv ID: 2308.11070
- Source URL: https://arxiv.org/abs/2308.11070
- Reference count: 32
- Primary result: Proposes a temporally-distributed backdoor attack on video action recognition that achieves >95% ASR while maintaining high clean accuracy and evading existing defenses.

## Executive Summary
This paper introduces a novel backdoor attack targeting video-based action recognition systems by embedding imperceptible triggers across video frames in the frequency domain. Unlike existing methods that independently embed triggers within frames, this approach leverages the temporal dimension to distribute perturbations, making detection by frame-wise defenses more difficult. The attack transforms video frames into frequency domains like DFT or DCT, adds small perturbations to selected components, then inverts back to the spatial domain. Extensive experiments demonstrate high attack success rates (>95%) across multiple datasets (UCF101, HMDB51, GSL) and model architectures (SlowFast, Res(2+1)D, S3D, I3D) while maintaining clean accuracy above 90%.

## Method Summary
The attack embeds backdoor triggers by perturbing components in transformed domains (DFT/DCT) of video frames, then inverting back to the spatial domain. Triggers are temporally distributed across frames to evade frame-wise detection methods. The method poisons training data by injecting mislabelled samples with embedded triggers, then trains victim models on this poisoned dataset. The attack achieves high effectiveness while maintaining imperceptibility through careful selection of perturbation magnitude and frequency components.

## Key Results
- Achieves >95% attack success rate across UCF101, HMDB51, and GSL datasets
- Maintains clean accuracy above 90% on unpoisoned data
- Successfully evades multiple state-of-the-art backdoor detection and mitigation methods
- Demonstrates resilience through "collateral damage" phenomenon where unintended frequency perturbations can also trigger the attack

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack works by embedding imperceptible, temporally distributed triggers in the frequency domain of video frames, exploiting the temporal dimension to evade frame-wise detection methods.
- Mechanism: The attack transforms video frames into the frequency domain (e.g., DFT), adds small perturbations to selected frequency components, then inverts back to the spatial domain. The temporal spread of perturbations across frames creates a trigger pattern that is not easily detectable by existing frame-wise backdoor defenses.
- Core assumption: The frequency-domain perturbations, when inverted, remain imperceptible in the spatial domain and distribute temporally across frames in a way that evades existing defenses.
- Evidence anchors:
  - [abstract] "Our proposed attack, adding perturbations in a transformed domain, plants an imperceptible, temporally distributed trigger across the video frames..."
  - [section 3.3] "The trigger is embedded by appropriately altering certain components of a representation of a video, thus the trigger is imperceptible to human observers in the video space."
  - [corpus] Weak - related papers focus on backdoor attacks in general but don't discuss the specific frequency-domain temporal distribution mechanism in detail.
- Break condition: If the perturbations become perceptible in the spatial domain, or if a defense method can analyze temporal correlations across frames effectively, the attack's stealth would be compromised.

### Mechanism 2
- Claim: The attack's effectiveness is due to its ability to exploit the temporal dimension of videos, which existing backdoor defenses for images do not account for.
- Mechanism: By embedding triggers across multiple frames in a temporally distributed manner, the attack creates a backdoor mapping that is not detectable by defenses designed for static images, which typically analyze each frame independently.
- Core assumption: Existing backdoor defenses are not designed to detect triggers that are distributed across the temporal dimension of videos.
- Evidence anchors:
  - [abstract] "Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are independently embedded within the frames, which tend to be detectable by existing defenses."
  - [section 3.2] "Videos incorporate an additional dimension: time. This provides the possibility of a higher level of stealthiness against the current backdoor defense strategies."
  - [corpus] Moderate - related papers discuss backdoor attacks on video data but don't specifically address the temporal distribution aspect as a defense evasion technique.
- Break condition: If defenses are developed that can analyze temporal patterns and correlations across video frames, the attack's stealth advantage would be diminished.

### Mechanism 3
- Claim: The attack leverages the properties of frequency domain transforms (e.g., DFT, DCT) to embed triggers that are imperceptible to human observers while still being effective for the model.
- Mechanism: The attack uses transforms like DFT or DCT to represent video frames in the frequency domain, where small perturbations to certain frequency components can have minimal impact on the spatial domain (imperceptible to humans) but significant impact on the model's classification.
- Core assumption: The chosen transform basis and the specific frequency components selected for perturbation allow for a balance between imperceptibility and attack effectiveness.
- Evidence anchors:
  - [section 3.3] "We then embed the backdoor trigger in the transformed space by perturbing certain components in the representation Rv."
  - [section 4.4] "Triggers from BadNet, Blend, SIG, and WaNet are relatively obvious to the human eye, while those from the proposed attack and FT-trojan are more imperceptible."
  - [corpus] Moderate - related papers discuss frequency-domain attacks but don't specifically address the balance between imperceptibility and attack effectiveness in the context of video backdoor attacks.
- Break condition: If the perturbations in the frequency domain become too large and perceptible in the spatial domain, or if the selected frequency components do not effectively influence the model's classification, the attack's effectiveness would be compromised.

## Foundational Learning

- Concept: Discrete Fourier Transform (DFT)
  - Why needed here: DFT is used to transform video frames into the frequency domain, where the attack embeds imperceptible perturbations.
  - Quick check question: What is the main advantage of using DFT for embedding backdoor triggers in video data?

- Concept: Imperceptibility metrics (PSNR, SSIM)
  - Why needed here: These metrics are used to evaluate the imperceptibility of the backdoor triggers to human observers.
  - Quick check question: How do PSNR and SSIM differ in measuring the quality of an image or video?

- Concept: Backdoor attacks
  - Why needed here: Understanding the basics of backdoor attacks is essential to comprehend the proposed attack's mechanism and objectives.
  - Quick check question: What is the main difference between a backdoor attack and a traditional adversarial attack?

## Architecture Onboarding

- Component map: Video frames (spatial domain) -> Transform (DFT, DCT, etc.) -> Frequency domain representation -> Perturbation embedding -> Inverse transform -> Backdoored video frames
- Critical path: Transform video frames → Embed perturbations in frequency domain → Inverse transform to spatial domain → Train model on backdoored data → Test model on triggered instances
- Design tradeoffs:
  - Imperceptibility vs. attack effectiveness: Larger perturbations are more effective but less imperceptible.
  - Temporal distribution vs. stealth: More distributed triggers are more stealthy but may require more complex perturbations.
  - Computational cost vs. attack sophistication: More complex transforms and perturbation strategies may be more effective but also more computationally expensive.
- Failure signatures:
  - High PSNR/SSIM values but low attack success rate: The perturbations are imperceptible but not effective.
  - Low PSNR/SSIM values but high attack success rate: The perturbations are effective but perceptible to humans.
  - High attack success rate on clean data: The model is not learning the backdoor mapping correctly.
- First 3 experiments:
  1. Implement the DFT-based attack on a simple video dataset (e.g., UCF-101) and evaluate its effectiveness and imperceptibility.
  2. Compare the DFT-based attack with other transform-based attacks (e.g., DCT, DWT) in terms of effectiveness and imperceptibility.
  3. Test the attack's resilience against existing backdoor defenses (e.g., NC, TABOR, STRIP) and analyze the results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does collateral damage impact the stealthiness of the proposed attack under backdoor defenses?
- Basis in paper: [inferred] The paper mentions collateral damage where perturbations in the transformed domain unintentionally activate the attack, but does not investigate how this affects stealthiness under defenses.
- Why unresolved: The paper only analyzes collateral damage in terms of ASR changes across frequencies, not in the context of defense detection.
- What evidence would resolve it: Experiments showing whether collateral damage triggers are detected by existing backdoor defenses like NC, TABOR, etc.

### Open Question 2
- Question: Is collateral damage model-dependent and if so, what architectural factors contribute to this difference?
- Basis in paper: [explicit] The paper observes different collateral damage effects across SlowFast, Res(2+1)D, S3D, and I3D models but does not analyze why.
- Why unresolved: The paper identifies the phenomenon but doesn't explore architectural differences that might explain varying susceptibility to unintended perturbations.
- What evidence would resolve it: Comparative analysis of model architectures (kernel sizes, attention mechanisms, etc.) correlated with collateral damage patterns.

### Open Question 3
- Question: Can the proposed attack be extended to many-to-one or one-to-one backdoor scenarios effectively?
- Basis in paper: [explicit] The paper acknowledges limitations of all-to-one attacks and mentions many-to-one attacks would be more practical for applications like sign language translation.
- Why unresolved: The paper focuses on all-to-one attacks due to training difficulties and doesn't explore more complex attack scenarios.
- What evidence would resolve it: Experiments demonstrating successful many-to-one attacks with reasonable poisoning ratios and ASRs on video datasets.

## Limitations
- The specific random seed and frequency component selection details are not fully specified, potentially affecting reproducibility.
- Evaluation focuses on specific datasets and model architectures, limiting generalizability to other video action recognition systems.
- The "collateral damage" phenomenon lacks a thorough explanation of its underlying causes and implications for attack design.

## Confidence
- Attack effectiveness: High - consistent high ASR and clean accuracy across multiple datasets and models
- Imperceptibility claims: Medium - PSNR/SSIM metrics show good values but human visual assessment is limited
- Evasion of existing defenses: Medium - shows resilience against tested defenses but defensive landscape is rapidly evolving

## Next Checks
1. Implement ablation studies varying the perturbation magnitude δ and frequency component selection to determine the minimum effective values that maintain attack success.
2. Test the attack against more recent defense mechanisms specifically designed for temporal backdoor detection in videos.
3. Analyze the sensitivity of the attack to different video compression schemes and quality levels to assess real-world applicability.