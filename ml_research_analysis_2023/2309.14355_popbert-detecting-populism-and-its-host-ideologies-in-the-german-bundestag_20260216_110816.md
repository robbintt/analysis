---
ver: rpa2
title: PopBERT. Detecting populism and its host ideologies in the German Bundestag
arxiv_id: '2309.14355'
source_url: https://arxiv.org/abs/2309.14355
tags:
- populism
- populist
- nicht
- language
- people
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PopBERT, a transformer-based model for detecting\
  \ populist language in German parliamentary speeches. The model identifies two core\
  \ dimensions of populism\u2014anti-elitism and people-centrism\u2014and distinguishes\
  \ between left-wing and right-wing host ideologies."
---

# PopBERT. Detecting populism and its host ideologies in the German Bundestag

## Quick Facts
- arXiv ID: 2309.14355
- Source URL: https://arxiv.org/abs/2309.14355
- Reference count: 40
- Primary result: PopBERT achieves F1-scores of 0.847 for anti-elitism, 0.696 for people-centrism, 0.713 for left-wing ideology, and 0.674 for right-wing ideology in detecting populist language in German parliamentary speeches

## Executive Summary
This paper introduces PopBERT, a transformer-based model for detecting populist language in German parliamentary speeches. The model identifies two core dimensions of populism—anti-elitism and people-centrism—and distinguishes between left-wing and right-wing host ideologies. Using a dataset of 8,795 annotated sentences from the German Bundestag (2013–2021), PopBERT achieves strong performance on anti-elitism detection while showing lower but still meaningful results for the more nuanced dimensions. The tool enables dynamic analysis of populist rhetoric and can be applied to other German-language datasets.

## Method Summary
The study fine-tunes GBERTLarge (335M parameters) as a multilabel classifier on 8,795 annotated sentences from German Bundestag plenary debates. The model treats each dimension (anti-elitism, people-centrism, left-wing ideology, right-wing ideology) as an independent binary classification task using binary cross-entropy loss. Training employed batch size 16, learning rate 4e-6, AdamW optimizer, and cosine annealing scheduler for 13 epochs with early stopping. An active learning strategy focused on edge cases and underrepresented categories to maximize performance while minimizing annotation effort.

## Key Results
- PopBERT achieves F1-score of 0.847 for anti-elitism detection
- People-centrism detection yields F1-score of 0.696
- Left-wing ideology classification reaches F1-score of 0.713
- Right-wing ideology classification achieves F1-score of 0.674

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The transformer-based BERT model captures contextual meaning of populist language better than dictionary approaches.
- Mechanism: BERT uses self-attention to represent words in context, allowing it to distinguish between literal and moralizing uses of language (e.g., "good" as positive adjective vs. in economics).
- Core assumption: The training data contains sufficient examples of moralizing vs. non-moralizing uses of populist terms to learn this distinction.
- Evidence anchors:
  - [abstract]: "Deep learning representations of language can capture words' contextual information and long-distance dependencies"
  - [section 2]: "dictionaries are simple frequency counts based on manually curated lists of phrases, they can, for instance, neither consider the context in which a word is used nor identify more abstract meanings"
  - [corpus]: Weak - no explicit validation of contextual understanding in the corpus

### Mechanism 2
- Claim: The multilabel architecture allows simultaneous detection of anti-elitism, people-centrism, and host ideology dimensions.
- Mechanism: By treating each dimension as an independent binary classification task, the model can learn each dimension separately without forcing mutually exclusive categories.
- Core assumption: The dimensions are truly independent enough to be learned separately rather than requiring a single joint model.
- Evidence anchors:
  - [section 4.3]: "Since both dimensions may appear separately, annotators assess both labels independently and assign multiple annotations per sentence if necessary"
  - [section 4.3]: "we treat our classification problem as a multilabel problem, meaning each sentence may contain multiple dimensions simultaneously"
  - [corpus]: Weak - no explicit validation that separate learning is better than joint learning

### Mechanism 3
- Claim: Active learning strategy focused on edge cases improves model performance on the most challenging examples.
- Mechanism: By iteratively selecting ambiguous cases and underrepresented categories for annotation, the model receives the most informative training examples.
- Core assumption: Edge cases provide more learning value than straightforward examples for this complex task.
- Evidence anchors:
  - [section 4.3]: "we utilized active learning to derive samples that maximize the classifiers' performance. Active learning is a common method in ML to reduce the labeling effort by selecting cases from which the supervised models profit most"
  - [section 4.3]: "In our case, we focused on ambiguous cases (i.e., edge cases) and cases from underrepresented categories"
  - [corpus]: Weak - no explicit validation that this strategy improved results

## Foundational Learning

- Concept: Populism as an ideational concept with anti-elitism, people-centrism, and moralizing frame as necessary conditions
  - Why needed here: The model's architecture and training data depend on understanding these theoretical foundations to correctly label and learn from the data
  - Quick check question: Can you explain why a sentence saying "the government made a mistake" would not be considered populist, but "the Merkel dictatorship made a mistake" would be?

- Concept: Multilabel classification vs. multiclass classification
  - Why needed here: The model needs to detect multiple dimensions that can co-occur, requiring a different approach than traditional single-label classification
  - Quick check question: What's the difference between treating anti-elitism and people-centrism as separate binary tasks versus creating combined categories like "left-wing anti-elite"?

- Concept: Transformer self-attention mechanism
  - Why needed here: Understanding how BERT represents context is crucial for interpreting why it might perform better than dictionary approaches
  - Quick check question: How does BERT's ability to consider context from both directions help it distinguish between "good" as positive vs. in economics?

## Architecture Onboarding

- Component map: GBERTLarge (pre-trained German BERT) → Linear layer + tanh activation → Fully connected layer + sigmoid activation → Binary cross-entropy loss
- Critical path: Text input → Tokenization → BERT embedding → Pooled CLS output → Linear transformation → Sigmoid activation → Binary prediction for each dimension
- Design tradeoffs: Sentence-level granularity allows flexibility in aggregation but may miss longer contextual cues; multilabel approach allows detection of co-occurring dimensions but requires more training data
- Failure signatures: Poor performance on moralizing vs. non-moralizing distinctions suggests insufficient contextual training examples; imbalance between precision and recall indicates threshold tuning issues
- First 3 experiments:
  1. Test model on out-of-sample populist statements from literature to verify detection capability
  2. Compare performance on sentences with clear vs. ambiguous populist content to validate edge case handling
  3. Aggregate predictions to party level and compare with expert survey rankings to validate construct validity

## Open Questions the Paper Calls Out

1. How does PopBERT's performance vary when applied to different types of German-language text beyond parliamentary speeches, such as social media posts or newspaper articles?
   - Basis in paper: [inferred] The paper mentions potential applications of PopBERT to other German-language datasets, including social media posts or newspaper articles, but does not provide empirical evidence for its performance in these contexts.
   - Why unresolved: The paper focuses on validating PopBERT's performance using parliamentary speeches and does not explore its generalizability to other text sources.
   - What evidence would resolve it: Empirical evaluation of PopBERT's performance on a diverse set of German-language text sources, including social media posts and newspaper articles, with appropriate validation checks.

2. How do the disagreements among annotators impact the performance and interpretability of PopBERT?
   - Basis in paper: [explicit] The paper acknowledges the presence of disagreements among annotators, particularly regarding the condition of moralistic language, and discusses the implications of these disagreements for the model's performance and interpretability.
   - Why unresolved: The paper does not provide a detailed analysis of how these disagreements affect the model's predictions or offer strategies for mitigating their impact.
   - What evidence would resolve it: A comprehensive analysis of the relationship between annotator disagreements and PopBERT's predictions, including an exploration of potential strategies for addressing these disagreements, such as using soft labels or incorporating annotator-specific information into the model.

3. How does the length of text snippets impact the performance of PopBERT?
   - Basis in paper: [explicit] The paper discusses the challenges of determining the appropriate length of text snippets for annotation and mentions that the chosen sentence-level approach may not be optimal for all contexts.
   - Why unresolved: The paper does not provide empirical evidence for how the performance of PopBERT varies with different text snippet lengths.
   - What evidence would resolve it: Empirical evaluation of PopBERT's performance using text snippets of varying lengths, such as paragraphs or speeches, with appropriate validation checks to assess the impact of snippet length on the model's accuracy and interpretability.

## Limitations

- Performance on people-centrism (F1=0.696) and right-wing ideology (F1=0.674) is substantially lower than anti-elitism detection, indicating difficulty with more nuanced dimensions
- The model's generalizability beyond the German parliamentary context remains uncertain due to training on a specific institutional setting
- The effectiveness of the active learning strategy is assumed but not empirically validated against alternative sampling approaches

## Confidence

- **High confidence**: Anti-elitism detection (F1=0.847) and basic model architecture choices
- **Medium confidence**: People-centrism detection and left/right ideology classification
- **Low confidence**: Generalizability to other German political contexts and out-of-domain applications

## Next Checks

1. Cross-context validation: Test PopBERT on populist rhetoric from other German political institutions (e.g., state parliaments, party manifestos) to assess domain transfer performance.

2. Human validation study: Have political scientists independently code a subset of Bundestag speeches and compare their judgments with PopBERT's predictions to quantify agreement and disagreement patterns.

3. Temporal stability analysis: Apply PopBERT to Bundestag speeches from before 2013 and after 2021 to verify that detected populist trends align with documented historical shifts in German politics.