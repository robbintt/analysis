---
ver: rpa2
title: 'PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments'
arxiv_id: '2311.09819'
source_url: https://arxiv.org/abs/2311.09819
tags:
- segmentation
- dataset
- bounding
- loss
- surgical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PWISeg addresses weakly-supervised surgical instrument instance
  segmentation by combining supervised point-to-box training with unsupervised point-to-mask
  learning. The method uses an FCN-based architecture with a projection loss to relate
  predicted masks to bounding boxes, and key pixel association/distribution losses
  to refine mask accuracy.
---

# PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments

## Quick Facts
- arXiv ID: 2311.09819
- Source URL: https://arxiv.org/abs/2311.09819
- Reference count: 0
- One-line primary result: PWISeg achieves 23.9% mAP on a new surgical instrument dataset and 30.6% mAP on HOSPI-Tools using only bounding box and keypoint supervision

## Executive Summary
PWISeg addresses the challenge of weakly-supervised instance segmentation for surgical instruments by combining supervised point-to-box training with unsupervised point-to-mask learning. The method uses an FCN-based architecture with a projection loss to relate predicted masks to bounding boxes, and key pixel association/distribution losses to refine mask accuracy. A novel dataset with manual keypoint and bounding box annotations is introduced to benchmark the task. PWISeg achieves 23.9% mAP on the new dataset and 30.6% mAP on the HOSPI-Tools dataset, outperforming existing weakly-supervised methods in both detection and segmentation.

## Method Summary
PWISeg uses a ResNet-50 backbone with FPN to extract multi-scale features, feeding both a point-to-box branch and a point-to-mask branch. The detection branch is trained with standard Focal Loss and IoU Loss using bounding box supervision. The segmentation branch is trained with three unsupervised losses: projection loss (Dice overlap between projected box boundaries and predicted masks), key pixel association loss (propagating keypoint labels via pixel affinity), and key pixel distribution loss (aligning mask probabilities with keypoint heatmaps). The method introduces a new surgical instrument dataset with 1788 training images annotated with bounding boxes and keypoints, enabling evaluation of weakly-supervised segmentation performance.

## Key Results
- Achieves 23.9% mAP on the proposed surgical instrument dataset with bounding box and keypoint supervision
- Achieves 30.6% mAP on the HOSPI-Tools dataset, outperforming existing weakly-supervised methods
- Demonstrates effectiveness of combining supervised detection with unsupervised segmentation losses

## Why This Works (Mechanism)

### Mechanism 1
The unsupervised projection loss bridges weak bounding box supervision to mask prediction by leveraging spatial overlap between projected box boundaries and predicted masks. During training, the model uses the bounding box as a coarse spatial guide, projecting the maximum probability along each axis to estimate boundaries, then measuring Dice overlap between these projected boundaries and the ground-truth box. This encourages the mask to respect the box extent without needing per-pixel labels. The core assumption is that bounding boxes tightly enclose objects and their axis projections are valid boundary proxies. Break condition: if boxes are loose or misaligned, the projection may be a poor boundary proxy, leading to misaligned mask predictions.

### Mechanism 2
Key pixel association propagates sparse point annotations into dense pseudo-labels via pixel affinity. A few manually annotated key pixels inside each instrument serve as seeds, and the model computes affinity between each seed and neighboring pixels using the product of foreground probabilities and their complements. Pixels exceeding a threshold are labeled as belonging to the same object, and labels are propagated outward until all pixels in the box are labeled. This dense pseudo-label set then supervises mask prediction. The core assumption is that pixels belonging to the same instrument have similar feature representations and can be linked through learned affinity. Break condition: if the affinity metric fails to capture instrument-specific appearance or if key pixels are poorly placed, propagation may leak into background or miss parts of the instrument.

### Mechanism 3
Key pixel distribution loss ensures predicted masks align spatially with ground-truth key pixel locations. After generating a heatmap from key pixels using a Gaussian kernel, the model computes the L1 distance between this heatmap and the predicted probability map within the bounding box. Minimizing this loss pulls the mask probability toward the known key pixel locations, improving localization. The core assumption is that the Gaussian-smoothed key pixel heatmap is a reasonable target for where mask probability should be high. Break condition: if key pixels are not centrally located or if instruments have multiple disconnected parts, the single heatmap may mislead the model.

## Foundational Learning

- **FCN Architecture**
  - Why needed here: Provides dense pixel-level predictions from shared feature maps, essential for both detection and segmentation in a single pass
  - Quick check question: How does an FCN differ from a standard CNN in terms of output resolution and spatial awareness?

- **Feature Pyramid Network (FPN)**
  - Why needed here: Supplies multi-scale feature maps that help detect and segment instruments of varying sizes within surgical scenes
  - Quick check question: Why is it beneficial to have shared FPN features across both detection and segmentation branches?

- **Dice Loss / IoU Loss**
  - Why needed here: Measures overlap quality between predicted masks and ground-truth boxes, directly optimizing for segmentation accuracy
  - Quick check question: How does Dice loss behave when predicted and target masks have little or no overlap?

## Architecture Onboarding

- **Component map**: Image → Backbone (ResNet-50) → FPN → Point-to-Box Branch (classification + regression) + Point-to-Mask Branch (mask prediction) → Shared heads on FPN outputs for both tasks
- **Critical path**: 1) Image → Backbone → FPN 2) Shared feature maps feed both branches 3) Detection branch outputs boxes 4) Segmentation branch outputs masks, supervised by combined weak losses 5) Backpropagation updates shared weights
- **Design tradeoffs**: Using bounding boxes instead of masks reduces annotation cost but weakens supervision; hence the need for additional key pixel and projection losses. Single-stage FCN avoids ROI pooling overhead but requires careful balancing of multi-task losses
- **Failure signatures**: Low mAP in detection but decent segmentation suggests detection branch training is insufficient. High detection mAP but poor segmentation mAP suggests segmentation branch lacks strong guidance from weak annotations. Over-smoothed masks suggest key pixel association propagation is too aggressive
- **First 3 experiments**: 1) Train only with bounding box supervision (no key pixels, no projection loss) to establish baseline performance 2) Add key pixel association loss to see impact of dense pseudo-label generation 3) Introduce projection loss and key pixel distribution loss together to evaluate joint effect on segmentation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of PWISeg change when using different backbones (e.g., ResNet-101, EfficientNet) instead of ResNet-50? The paper reports results using a ResNet-50 backbone and suggests that different backbones could be explored, but does not provide comparisons with other backbone architectures. What evidence would resolve it: Experiments comparing the performance of PWISeg with various backbones on the proposed dataset and HOSPI-Tools dataset.

### Open Question 2
What is the impact of varying the number of key pixels annotated per instrument on the segmentation accuracy? The paper introduces the use of key pixels and proposes key pixel association and distribution losses, but does not explore the effect of varying the number of key pixels. What evidence would resolve it: Experiments with different numbers of key pixels per instrument to determine the optimal number for segmentation accuracy.

### Open Question 3
How does PWISeg perform in real-time surgical scenarios with varying lighting conditions and occlusions? The paper mentions the challenges of dense stacking and occlusion of instruments in real operation rooms but does not test the model in such scenarios. What evidence would resolve it: Testing PWISeg in a simulated or real operating room environment with different lighting conditions and levels of instrument occlusion to assess its robustness and real-time performance.

## Limitations

- The unsupervised projection loss assumes bounding boxes tightly enclose instruments and align well with object boundaries, which may fail in surgical scenes with occlusions or partial views
- Key pixel association depends heavily on keypoint placement and pixel affinity quality, where poorly chosen keypoints could propagate incorrect labels
- The proposed dataset size (1788 training images) is relatively small, which may limit generalization and increase overfitting risk despite the weak supervision approach

## Confidence

- **High Confidence**: The core architectural design combining supervised detection with unsupervised segmentation losses is sound and well-justified by the methodology description
- **Medium Confidence**: The specific loss formulations (projection loss, key pixel association/distribution) are described but lack implementation details that could affect reproducibility
- **Medium Confidence**: Performance metrics on both proposed and HOSPI-Tools datasets suggest the method works, but the absolute mAP values indicate there's significant room for improvement compared to fully supervised approaches

## Next Checks

1. **Ablation Study**: Train PWISeg without key pixel supervision to quantify the exact contribution of key pixel association and distribution losses to overall performance
2. **Robustness Testing**: Evaluate PWISeg on surgical images with varying degrees of occlusion and clutter to assess how well the projection loss handles imperfect bounding box supervision
3. **Annotation Efficiency Analysis**: Compare annotation time and model performance between bounding box + key points versus full mask annotations to validate the claimed efficiency benefit