---
ver: rpa2
title: Evaluation of African American Language Bias in Natural Language Generation
arxiv_id: '2305.14291'
source_url: https://arxiv.org/abs/2305.14291
tags:
- language
- text
- counterpart
- generated
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) for bias in understanding
  African American Language (AAL) compared to White Mainstream English (WME). The
  authors create a novel dataset of AAL texts from social media, hip-hop lyrics, focus
  groups, and linguistic interviews, each paired with human-annotated WME counterparts.
---

# Evaluation of African American Language Bias in Natural Language Generation

## Quick Facts
- arXiv ID: 2305.14291
- Source URL: https://arxiv.org/abs/2305.14291
- Reference count: 35
- Primary result: LLMs perform significantly worse on African American Language tasks compared to White Mainstream English across all evaluation metrics

## Executive Summary
This paper evaluates six large language models for bias against African American Language (AAL) compared to White Mainstream English (WME). The authors create a novel dataset of 346 AAL-WME text pairs from social media, hip-hop lyrics, and linguistic interviews. Using counterpart generation and masked span prediction tasks, they find that all models perform significantly worse on AAL than WME across automatic metrics (Rouge, BERTScore, perplexity) and human judgments on linguistic match, meaning preservation, and tone. The results demonstrate significant bias against AAL in LLMs with implications for deployment in high-stakes contexts.

## Method Summary
The authors created a novel dataset of AAL texts from TwitterAAE corpus, CORAAL interviews, r/BlackPeopleTwitter, hip-hop lyrics, and focus groups, each paired with human-annotated WME counterparts. They evaluated six pre-trained LLMs (GPT-3, ChatGPT, GPT-4, Flan-T5, T5, BART) on two tasks: counterpart generation (translating between AAL and WME) and masked span prediction (filling in missing words). Automatic metrics (Rouge-1/2/L, BERTScore, perplexity) were computed alongside human evaluations by native AAL speakers and WME speakers on linguistic match, meaning preservation, and tone preservation using 5-point Likert scales.

## Key Results
- Models achieve significantly higher Rouge-1 scores on WME (0.83-0.92) than AAL (0.70-0.85) for WME→AAL generation
- Human evaluators consistently rate model-generated WME as more human-like and dialectally accurate than model-generated AAL
- Perplexity scores are substantially higher for AAL masked span prediction (4.94-13.19) than WME (2.11-7.11) across models
- Models struggle with specific AAL features like habitual be, aspectual verbs, and remote past been

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models perform worse on AAL due to insufficient training data representation
- Mechanism: The training corpora for LLMs are primarily drawn from internet sources with severe underrepresentation of AAL. Common Crawl, Wikipedia, and other sources reflect WME more than AAL, leading to learned biases
- Core assumption: Underrepresentation in training data directly causes performance gaps in language understanding tasks
- Evidence anchors:
  - [abstract] "Training corpora for LLMs are typically drawn from internet sources... which can severely underrepresent the language of African Americans"
  - [section 1] "only 0.07% of documents reflect AAL, while 97.8% are composed of WME"
  - [corpus] Weak evidence - only mentions neighbor papers on AAL bias without direct data on training corpus composition
- Break condition: If AAL were equally represented in training data or if models were explicitly fine-tuned on balanced datasets, the performance gap would diminish or disappear

### Mechanism 2
- Claim: Models struggle with specific AAL morphosyntactic features like aspectual verbs and habitual be
- Mechanism: AAL contains grammatical features (habitual be, remote past been, aspectual verbs) that lack direct semantic equivalents in WME. Models trained primarily on WME data fail to learn these distinctions
- Core assumption: Grammatical features without WME equivalents create unique challenges for language models
- Evidence anchors:
  - [abstract] "models struggle with... aspectual verbs and certain lexical items"
  - [section 6] "Models seem to correctly interpret specific features of AAL... They struggle, however, with many other features" including habitual be and remote past been
  - [corpus] No direct evidence about specific morphosyntactic features in training data
- Break condition: If models were trained with explicit linguistic annotations for AAL features or if these features had WME equivalents, the performance gap would decrease

### Mechanism 3
- Claim: Toxicity detection systems amplify bias against AAL by misclassifying dialect features as toxic
- Mechanism: Models are trained to avoid toxic outputs, leading them to neutralize AAL features when generating WME. This neutralization removes culturally specific linguistic elements and alters meaning
- Core assumption: Toxicity avoidance mechanisms inadvertently penalize legitimate dialect features
- Evidence anchors:
  - [abstract] "models tend to remove toxic words when generating both AAL and WME"
  - [section 5.1] "Models are developed to avoid generating toxic or offensive language... a significantly higher proportion of toxic language is removed when generating WME from AAL"
  - [corpus] No direct evidence about toxicity detection system training data composition
- Break condition: If toxicity detection systems were trained on balanced dialect representation or if toxicity classification were decoupled from dialect features, the neutralization effect would be reduced

## Foundational Learning

- Concept: Dialectal variation in English (AAL vs WME)
  - Why needed here: Understanding the specific linguistic features that distinguish AAL from WME is essential for interpreting why models perform differently on each variety
  - Quick check question: What are the key grammatical features of AAL that don't have direct WME equivalents?

- Concept: Automatic evaluation metrics limitations
  - Why needed here: Knowing the weaknesses of Rouge, BERTScore, and perplexity helps interpret why these metrics show performance gaps
  - Quick check question: Why might n-gram overlap metrics unfairly disadvantage AAL compared to WME?

- Concept: Bias in NLP systems
  - Why needed here: Understanding how training data biases propagate to model behavior is crucial for contextualizing the findings
  - Quick check question: How do demographic disparities in internet access affect language model training data composition?

## Architecture Onboarding

- Component map: Data collection (AAL corpus) -> Model inference (6 LLMs) -> Evaluation (automatic metrics + human judgments)
- Critical path: Data annotation → Model inference → Metric computation → Human evaluation → Analysis of performance gaps
- Design tradeoffs: Using human annotators who are native AAL speakers ensures linguistic accuracy but limits scalability. Automatic metrics provide efficiency but may not capture dialectal nuances. The choice of six diverse models provides breadth but increases computational costs
- Failure signatures: Large performance gaps between AAL and WME indicate bias. Low human-likeness scores for AAL generations suggest models struggle with dialectal features. High perplexity for AAL masked spans indicates difficulty with grammatical structures
- First 3 experiments:
  1. Run counterpart generation task with all six models and compare Rouge-1 scores between AAL and WME directions
  2. Conduct human evaluation on a subset of model generations using the provided Likert scales
  3. Perform masked span prediction task with GPT-3, BART, and T5 to measure perplexity differences between dialects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features of AAL cause the largest performance gaps in LLM understanding, and how do these features vary across different AAL dialects and contexts?
- Basis in paper: [explicit] The paper identifies specific AAL features like aspectual verbs, existential "it," and N-words that models struggle with, and notes that performance gaps vary across different AAL data sources
- Why unresolved: The paper provides initial identification of problematic features but doesn't systematically map which features cause the largest gaps or how these patterns vary across different AAL varieties
- What evidence would resolve it: A comprehensive analysis mapping specific AAL features to model performance gaps across multiple AAL dialects, with controlled experiments isolating individual features

### Open Question 2
- Question: How do different fine-tuning strategies affect LLM bias against AAL, and what is the optimal balance between task performance and bias mitigation?
- Basis in paper: [explicit] The paper mentions that Flan-T5 performs better when fine-tuned for the task, and notes the trade-offs between task performance and bias mitigation mentioned in related work
- Why unresolved: The paper only evaluates pre-trained models without exploring how different fine-tuning approaches might reduce AAL bias while maintaining performance
- What evidence would resolve it: Systematic experiments comparing various fine-tuning strategies (instruction tuning, domain adaptation, etc.) on their effects on AAL understanding while tracking performance on standard benchmarks

### Open Question 3
- Question: What are the long-term societal impacts of deploying LLMs that struggle with AAL, particularly in high-stakes contexts like healthcare and legal systems?
- Basis in paper: [inferred] The paper discusses potential harms including "misunderstandings" and notes that biased models could "exacerbate existing societal inequalities" in contexts like mental health counseling and medical healthcare
- Why unresolved: The paper identifies potential risks but doesn't empirically study the actual consequences of AAL bias in deployed systems or trace how these biases manifest in real-world applications
- What evidence would resolve it: Longitudinal studies tracking the effects of AAL-biased LLM deployments in specific high-stakes domains, measuring both direct outcomes and secondary societal impacts

## Limitations

- The study uses a relatively small dataset of 346 AAL-WME text pairs, which may not fully capture the diversity of AAL across different regions and social contexts
- Human evaluation relies on a small pool of annotators (15 AAL speakers and 11 WME speakers), raising questions about inter-annotator agreement and potential biases in subjective judgments
- The paper does not provide detailed information about the specific AAL features present in the test samples, making it difficult to determine which linguistic elements most challenge the models

## Confidence

- **High Confidence:** The core finding that LLMs perform significantly worse on AAL than WME tasks is well-supported by multiple metrics (Rouge, BERTScore, perplexity, human judgments)
- **Medium Confidence:** The specific AAL features that models struggle with (habitual be, aspectual verbs, remote past been) are identified but not comprehensively validated across the entire dataset
- **Low Confidence:** Claims about demographic disparities in internet access affecting training data composition are asserted but not empirically demonstrated

## Next Checks

1. Conduct inter-annotator agreement analysis on the human evaluations to establish reliability of subjective judgments and identify potential biases in the annotation process

2. Perform error analysis on model outputs to categorize specific AAL features that cause failures (habitual be, aspectual verbs, lexical items) and quantify their frequency across the dataset

3. Test the models on additional AAL datasets from different regions and social contexts to determine if performance gaps persist across dialectal variations and assess generalizability of the findings