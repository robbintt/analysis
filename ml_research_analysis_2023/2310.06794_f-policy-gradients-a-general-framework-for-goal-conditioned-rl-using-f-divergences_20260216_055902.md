---
ver: rpa2
title: '$f$-Policy Gradients: A General Framework for Goal Conditioned RL using $f$-Divergences'
arxiv_id: '2310.06794'
source_url: https://arxiv.org/abs/2310.06794
tags:
- policy
- goal
- learning
- distribution
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces f-Policy Gradients (f-PG), a novel framework
  for goal-conditioned reinforcement learning that minimizes the f-divergence between
  the agent's state visitation distribution and the goal distribution. This approach
  provides dense learning signals for exploration in sparse reward settings, unlike
  traditional reward augmentation methods that can lead to sub-optimal policies.
---

# $f$-Policy Gradients: A General Framework for Goal Conditioned RL using $f$-Divergences

## Quick Facts
- arXiv ID: 2310.06794
- Source URL: https://arxiv.org/abs/2310.06794
- Reference count: 29
- Key outcome: f-PG achieves 85% success rate on PointMazeMedium and 80% on PointMazeLarge, significantly outperforming standard policy gradient methods in exploration-heavy tasks

## Executive Summary
This paper introduces f-Policy Gradients (f-PG), a novel framework for goal-conditioned reinforcement learning that minimizes the f-divergence between the agent's state visitation distribution and the goal distribution. The approach provides dense learning signals for exploration in sparse reward settings, addressing a key limitation of traditional reward augmentation methods. By deriving gradients for various f-divergences and showing connections to metric-based shaping rewards, the authors demonstrate that f-PG can recover optimal policies under certain conditions and provides a new perspective on state entropy maximization in RL.

## Method Summary
f-PG extends policy gradient methods by using f-divergence gradients instead of reward gradients to learn goal-conditioned policies. The method estimates the agent's state visitation distribution and the goal distribution, then computes the gradient of the f-divergence between them to update the policy. The approach includes a PPO-style clipped objective for stability and can recover optimal policies when the chosen f-divergence has a defined f'(∞). A special case called state-MaxEnt RL emerges when using forward KL divergence, maximizing both reward and state visitation entropy.

## Key Results
- f-PG achieves 85% success rate on PointMazeMedium and 80% on PointMazeLarge
- Outperforms standard policy gradient methods in exploration-heavy tasks
- Provides dense learning signals for exploration in sparse reward settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing f-divergence between state visitation and goal distribution provides dense learning signals for exploration in sparse reward settings.
- **Mechanism:** When the agent's state visitation probability is low in regions where the goal distribution has high probability, the f-divergence gradient term f'(pθ(s)/pg(s)) becomes large, providing strong gradient signals to explore those regions.
- **Core assumption:** The goal distribution pg(s) is concentrated at the goal location (Dirac distribution).
- **Evidence anchors:**
  - [abstract] "Our learning paradigm provides dense learning signals for exploration in sparse reward settings."
  - [section] "Theorem 4.2 provides another perspective for f-Policy Gradient – ητ(g) is equivalent to the expected return for a goal-based sparse reward, hence optimizing the true goal-conditioned RL objective."
  - [corpus] "Weak evidence - corpus focuses on subgoal temporal ordering and value functions, not directly on f-divergence exploration signals."
- **Break condition:** If pg(s) is uniform or if pθ(s) ≈ pg(s) everywhere, the divergence gradient becomes flat and exploration signals vanish.

### Mechanism 2
- **Claim:** f-PG recovers optimal policies when f-divergence uses divergences with defined f'(∞).
- **Mechanism:** For divergences where f'(∞) is defined, minimizing Df(pθ||pg) pushes pθ(g) to be maximal, which Theorem 5.1 proves yields the optimal policy.
- **Core assumption:** f'(∞) is defined for the chosen f-divergence.
- **Evidence anchors:**
  - [section] "Theorem 5.1. The policy that minimizes Df(pπ||pg) for a convex function f with f(1) = 0 and f'(∞) being defined, is the optimal policy."
  - [corpus] "Weak evidence - corpus neighbors focus on goal-conditioned RL but don't specifically address f-divergence optimality conditions."
- **Break condition:** If f'(∞) is undefined (e.g., forward KL), the policy maximizes entropy of state visitation instead of achieving optimality.

### Mechanism 3
- **Claim:** State-MaxEnt RL emerges as a special case of f-PG, maximizing both reward and state visitation entropy.
- **Mechanism:** For forward KL divergence (f(u)=u log u), minimizing Df(pθ||pg) is equivalent to maximizing both log pg(s) reward and entropy H(pθ), providing balanced exploration-exploitation.
- **Core assumption:** The goal distribution pg(s) can be expressed as pg(s)=exp(f(s;g)) for some metric f.
- **Evidence anchors:**
  - [section] "Lemma 5.1. f kl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of the state-visitation distribution."
  - [corpus] "Weak evidence - corpus neighbors discuss temporal distance-aware representations but not the entropy-maximization property of f-PG."
- **Break condition:** If pg(s) cannot be factorized as exp(f(s;g)), the entropy maximization interpretation breaks down.

## Foundational Learning

- **Concept:** f-divergences as a family of distance measures between probability distributions.
  - **Why needed here:** The paper uses f-divergences to quantify mismatch between agent's state visitation and goal distributions, which is central to the entire framework.
  - **Quick check question:** What is the key property of f-divergences that makes them suitable for comparing distributions where one (pg) is a Dirac delta?

- **Concept:** Policy gradient methods and their variance reduction techniques.
  - **Why needed here:** f-PG extends policy gradients by using f-divergence gradients instead of reward gradients, requiring understanding of importance sampling and clipping techniques.
  - **Quick check question:** How does the f-PG gradient resemble standard policy gradient, and what additional term appears?

- **Concept:** Entropy regularization in reinforcement learning.
  - **Why needed here:** The paper shows connections between certain f-divergences and entropy maximization, particularly the distinction between π-MaxEnt (policy entropy) and s-MaxEnt (state visitation entropy).
  - **Quick check question:** Why does maximizing policy entropy not guarantee maximum state coverage, and how does s-MaxEnt RL address this?

## Architecture Onboarding

- **Component map:**
  Policy network πθ(at|st) -> Density estimators for pθ(s) and pg(s) -> f-divergence gradient computation -> PPO-style clipping -> Buffer

- **Critical path:**
  1. Collect trajectories with current policy
  2. Estimate pθ(s) and pg(s) densities using KDE
  3. Compute f'(pθ(s)/pg(s)) for each visited state
  4. Calculate PPO-style clipped gradients using these f' values
  5. Update policy parameters
  6. Repeat until convergence

- **Design tradeoffs:**
  - On-policy vs off-policy: Current f-PG is on-policy for theoretical guarantees but sample inefficient
  - Density estimation method: KDE is simple but may struggle in high dimensions; discriminators could be used but add adversarial instability
  - Choice of f-divergence: Forward KL gives s-MaxEnt properties but no optimality guarantee; RKL and TV have optimality guarantees but different exploration properties

- **Failure signatures:**
  - Policy collapse to narrow state regions: Check if f'(pθ(s)/pg(s)) values are not providing sufficient exploration signal
  - High variance in updates: Verify density estimation quality and consider variance reduction techniques
  - Poor performance despite exploration: Check if chosen f-divergence has undefined f'(∞) leading to suboptimal policies

- **First 3 experiments:**
  1. Gridworld navigation with wall separating start and goal - verify dense exploration signals work when no reward signal exists
  2. Reacher arm reaching fixed goal - visualize how f'(pθ(s)/pg(s)) evolves during learning and drives exploration
  3. PointMaze with overlapping vs non-overlapping initial/goal distributions - test coverage assumptions and discriminator-free operation

## Open Questions the Paper Calls Out
- The paper acknowledges that it doesn't tackle goal distributions with several modes and suggests it as a direction for future work.

## Limitations
- Scalability to high-dimensional continuous control remains untested
- Density estimation using KDE may face computational challenges and accuracy issues in higher dimensions
- Theoretical guarantees rely on specific conditions (like f'(∞) being defined) that may not hold for all practical choices of f-divergences

## Confidence
- **High Confidence:** The mechanism connecting f-divergence minimization to dense exploration signals (Mechanism 1) is well-supported by both theoretical derivation and empirical results across multiple environments.
- **Medium Confidence:** The optimality claims for divergences with defined f'(∞) (Mechanism 2) are theoretically sound but require more extensive empirical validation across diverse task distributions.
- **Medium Confidence:** The state-MaxEnt RL connection (Mechanism 3) is mathematically rigorous but its practical benefits over standard entropy regularization need further investigation.

## Next Checks
1. **Scalability Test:** Evaluate f-PG on high-dimensional environments (e.g., robotic manipulation with 20+ dimensions) to assess density estimation performance and computational feasibility.

2. **Ablation Study:** Systematically vary the choice of f-divergence (KL, reverse KL, total variation, etc.) across multiple tasks to quantify the impact on exploration efficiency and final performance.

3. **Theoretical Extension:** Investigate the behavior when f'(∞) is undefined (e.g., forward KL) in infinite-horizon settings, particularly whether the state-visitation distribution converges to a stationary distribution that satisfies the optimality conditions.