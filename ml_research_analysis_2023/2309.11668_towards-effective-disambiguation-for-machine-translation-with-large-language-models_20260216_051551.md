---
ver: rpa2
title: Towards Effective Disambiguation for Machine Translation with Large Language
  Models
arxiv_id: '2309.11668'
source_url: https://arxiv.org/abs/2309.11668
tags:
- translation
- llms
- shot
- prompting
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors compare the performance of Large Language Models (LLMs)
  against Neural Machine Translation (NMT) systems in the challenging task of translating
  ambiguous sentences. They select seven well-known open-source LLMs and evaluate
  their performance on the DiBiMT test set, which covers five language directions.
---

# Towards Effective Disambiguation for Machine Translation with Large Language Models

## Quick Facts
- arXiv ID: 2309.11668
- Source URL: https://arxiv.org/abs/2309.11668
- Reference count: 11
- Key outcome: LLMs, particularly BLOOMZ and LLaMA, match or outperform state-of-the-art NMT systems on ambiguous translation tasks

## Executive Summary
This paper addresses the challenge of translating ambiguous sentences by evaluating Large Language Models (LLMs) against traditional Neural Machine Translation (NMT) systems. The authors conduct comprehensive experiments on the DiBiMT test set across five language directions, demonstrating that LLMs can match or outperform systems like DeepL and NLLB. They propose two techniques to improve LLM performance: in-context learning with similar ambiguous contexts and fine-tuning on curated ambiguous corpora, achieving up to 15-point improvements in accuracy.

## Method Summary
The authors evaluate seven open-source LLMs on ambiguous translation tasks using k-shot prompting with random examples and similar context retrieval. They implement LoRA fine-tuning on curated corpora containing highly polysemous words with rare senses. The evaluation framework includes WSD disambiguation using ESCHER, similar context retrieval from Europarl corpus, and comprehensive metrics including DiBiMT accuracy, spBLEU, chrF++, and COMET22. The methods are validated across five language directions: English to Spanish, Italian, German, Russian, and Chinese.

## Key Results
- LLMs match or outperform state-of-the-art NMT systems in four out of five language directions
- In-context learning with similar contexts improves performance by up to 15 points over random demonstrations
- Fine-tuning on curated ambiguous corpora further enhances disambiguation accuracy
- BLOOMZ and LLaMA achieve the highest performance among tested LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can resolve semantic ambiguity better than traditional NMT systems because they are pretrained on diverse monolingual corpora that expose them to rare word senses
- Core assumption: The pretraining corpus contains sufficient examples of rare word senses and their contextual usage
- Evidence anchors: Abstract states LLMs demonstrate comparable performance to NMT while introducing new paradigms for controlling outputs; section notes biases against rare and polysemous word senses during pretraining
- Break condition: If pretraining corpus lacks sufficient coverage of rare senses, or if model capacity is insufficient to retain this knowledge

### Mechanism 2
- Claim: In-context learning with similar ambiguous contexts significantly improves LLM disambiguation performance
- Core assumption: The development corpus contains sufficient examples of the target word sense for effective retrieval
- Evidence anchors: Abstract mentions methods can match or outperform state-of-the-art systems; section describes randomly sampling k source-target pairs with similar senses as demonstrations
- Break condition: If similar context retrieval fails or if LLM capacity to learn from demonstrations is limited

### Mechanism 3
- Claim: Fine-tuning on curated ambiguous corpora improves LLM disambiguation by explicitly addressing both polysemy degree and sense frequency biases
- Core assumption: The curated corpus contains a balanced mix of polysemy and sense frequency challenges
- Evidence anchors: Abstract states methods can match or outperform state-of-the-art systems; section describes taking top N/2 sentences from each set to create final fine-tuning corpus
- Break condition: If fine-tuning data is insufficient, or if the model overfits to specific examples rather than learning general disambiguation patterns

## Foundational Learning

- Concept: Word Sense Disambiguation (WSD)
  - Why needed here: Understanding how ambiguous words map to specific senses is fundamental to evaluating and improving ambiguous translation
  - Quick check question: What is the difference between polysemy degree and sense frequency in the context of ambiguous words?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The paper relies heavily on demonstrating LLM capabilities through k-shot prompting and improving this with relevant examples
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and why is it particularly useful for LLMs?

- Concept: Low-rank adaptation (LoRA) for efficient fine-tuning
  - Why needed here: The paper uses LoRA to efficiently fine-tune large LLMs on ambiguous translation tasks without full parameter updates
  - Quick check question: What is the key advantage of LoRA over full fine-tuning when working with large language models?

## Architecture Onboarding

- Component map: Input sentence → WSD disambiguation → Retrieval of similar contexts → LLM prompting with demonstrations → Translation output
- Critical path: For inference: input → WSD → retrieval → LLM prompting → output; For fine-tuning: corpus selection → LoRA injection → training loop → evaluation
- Design tradeoffs: 8-bit quantization reduces memory usage but may impact performance; similar context retrieval adds complexity but improves accuracy; LoRA offers parameter efficiency at the cost of some performance compared to full fine-tuning
- Failure signatures: Poor disambiguation accuracy suggests WSD system issues, insufficient similar contexts, or inadequate demonstrations; fine-tuning failures may indicate data quality issues or improper LoRA configuration
- First 3 experiments:
  1. Compare 1-shot, 3-shot, and 5-shot prompting with random examples across different LLMs
  2. Implement and test similar context retrieval for 3-shot prompting to measure improvement over random examples
  3. Conduct LoRA fine-tuning with 36K-63K sentences and evaluate on DiBiMT to find optimal training corpus size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in translating ambiguous sentences correlate with their performance on non-ambiguous sentences?
- Basis in paper: The authors evaluate LLMs on FLORES200 test sets and find that trends in disambiguation accuracy scores correlate strongly with overall MT quality
- Why unresolved: The authors only provide correlation analysis between disambiguation accuracy and standard MT metrics, not exploring the relationship between ambiguous and non-ambiguous sentence performance specifically
- What evidence would resolve it: Detailed analysis comparing LLM performance on ambiguous and non-ambiguous sentences within the same test sets

### Open Question 2
- Question: What is the optimal size of the fine-tuning corpus for LLMs to achieve the best disambiguation performance?
- Basis in paper: The authors experiment with different sizes and find accuracy increases non-monotonically, peaking between 36K-63K training samples
- Why unresolved: While a range is identified, the exact optimal size is not determined, and testing was limited to three LLMs
- What evidence would resolve it: More extensive experiments with larger variety of LLMs and finer granularity of corpus sizes

### Open Question 3
- Question: How do the proposed techniques for improving LLM disambiguation performance compare to other potential methods?
- Basis in paper: The authors propose and evaluate two techniques (in-context learning with similar contexts and fine-tuning on ambiguous corpora) but only compare to naive few-shot prompting
- Why unresolved: The authors do not explore other potential techniques for improving LLM disambiguation performance
- What evidence would resolve it: Comprehensive comparison of proposed methods against a wide range of alternative techniques

## Limitations

- Evaluation limited to five language directions, preventing conclusions about cross-lingual generalization
- Curated corpus size (36K-63K sentences) may not capture the full range of ambiguous translation phenomena
- Study does not address domain-specific ambiguity or low-resource language pairs where challenges may be more pronounced

## Confidence

**High Confidence** (Supported by direct experimental evidence):
- LLMs can match or outperform traditional NMT systems on the DiBiMT test set for ambiguous translation
- In-context learning with similar contexts provides measurable improvements over random demonstrations
- LoRA fine-tuning on curated ambiguous corpora improves disambiguation performance

**Medium Confidence** (Inferred from experimental design but requires additional validation):
- Superiority of LLMs is primarily due to pretraining on diverse monolingual corpora containing rare word senses
- The 15-point improvement ceiling for in-context learning represents maximum achievable benefit
- Fine-tuning on highly polysemous words with rare senses addresses core LLM limitations for disambiguation

**Low Confidence** (Based on limited experimental coverage):
- Performance gap between BLOOMZ/LLaMA and other LLMs generalizes to other ambiguous translation tasks
- Optimal corpus size for fine-tuning would remain consistent across different language pairs
- 1.9 spBLEU improvement on FLORES200 indicates robust generalization beyond DiBiMT domain

## Next Checks

1. **Cross-lingual Generalization**: Evaluate the same LLM prompting and fine-tuning techniques on additional language pairs beyond the five tested to assess whether performance advantages extend to typologically diverse languages.

2. **Long-tail Sense Coverage**: Analyze the distribution of word senses in the pretraining corpora of different LLMs to empirically verify whether models with broader sense coverage consistently outperform those with narrower coverage on rare sense disambiguation tasks.

3. **Scaling Analysis**: Conduct experiments with varying fine-tuning corpus sizes (from 5K to 200K sentences) to determine whether observed improvements continue to scale with data size or whether diminishing returns set in.