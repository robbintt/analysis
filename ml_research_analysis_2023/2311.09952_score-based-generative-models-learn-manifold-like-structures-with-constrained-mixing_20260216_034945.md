---
ver: rpa2
title: Score-based generative models learn manifold-like structures with constrained
  mixing
arxiv_id: '2311.09952'
source_url: https://arxiv.org/abs/2311.09952
tags:
- score
- singular
- local
- vectors
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the structure of the vector fields learned
  by score-based generative models (SBMs) through their local linear approximations.
  The key finding is that SBMs learn a vector field that is non-conservative within
  the manifold, enabling flexible mixing between samples, while maintaining a manifold-like
  structure through energy-like off-manifold directions.
---

# Score-based generative models learn manifold-like structures with constrained mixing

## Quick Facts
- arXiv ID: 2311.09952
- Source URL: https://arxiv.org/abs/2311.09952
- Reference count: 10
- This paper investigates the structure of vector fields learned by score-based generative models through local linear approximations.

## Executive Summary
This paper analyzes the learned vector fields of score-based generative models (SBMs) by examining local linear approximations via singular value decomposition (SVD). The authors discover that SBMs learn a vector field that mixes samples through non-conservative dynamics within the data manifold while maintaining energy-like behavior in off-manifold directions. As noise levels decrease during diffusion, the local dimensionality of the data manifold increases and becomes more varied across samples. The study reveals that the subspaces spanned by the score model's local features overlap with those of an effective energy-based density function, suggesting SBMs can effectively model complex data distributions on low-dimensional manifolds while maintaining generative diversity.

## Method Summary
The authors analyze pre-trained score-based generative models by computing local linear approximations of the score function around generated samples. They perform SVD on the score Jacobian to extract singular vectors and values, and compare these with eigenvectors from the symmetrized Jacobian. The analysis is conducted at five different noise levels (0.01, 0.03, 0.07, 0.15, 0.3) using 128 random samples from each noise level. Local dimensionality is estimated from the singular value spectrum, and subspace overlap is quantified using a reconstruction criterion. The study uses the MNIST dataset with a U-Net architecture similar to Song et al. [8].

## Key Results
- SBMs learn non-conservative vector fields within the manifold that enable mixing between samples while maintaining energy-like behavior in off-manifold directions
- As noise decreases during diffusion, local dimensionality increases and becomes more varied between different sample sequences
- Subspaces spanned by local features of the score model overlap with those of an effective energy-based density function

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SBMs learn a non-conservative vector field within the manifold that enables mixing between samples while maintaining a manifold-like structure through energy-like off-manifold directions.
- Mechanism: The score Jacobian's singular value decomposition (SVD) reveals misaligned singular vectors at lower ranks, allowing mixing between different data components. At higher ranks, the singular vectors align with opposite signs, consistent with off-manifold directions.
- Core assumption: The data distribution is supported on a low-dimensional manifold in the ambient space.
- Evidence anchors:
  - [abstract]: "We find that the learned vector field mixes samples by a non-conservative field within the manifold, although it denoises with normal projections as if there is an energy function in off-manifold directions."
  - [section]: "We find that the singular vectors are misaligned at lower ranks, allowing for mixing between different data components."
  - [corpus]: Weak - corpus papers discuss score-based models but do not provide specific evidence for this mechanism.

### Mechanism 2
- Claim: As the noise level decreases during diffusion, the local dimensionality of the data increases and becomes more varied between different sample sequences.
- Mechanism: The singular values of the score Jacobian provide a measure of the local length scales along principal directions of the distribution. As the noise decreases, more singular values become significant, indicating higher local dimensionality.
- Core assumption: The score Jacobian's singular values are related to the local length scales of the data distribution.
- Evidence anchors:
  - [abstract]: "During diffusion as the noise decreases, the local dimensionality increases and becomes more varied between different sample sequences."
  - [section]: "The noise variance σ2t lower bounds the inverse singular values, because the smoothing effect of the Gaussian noise determines the smallest length scale."
  - [corpus]: Weak - corpus papers discuss dimensionality but do not provide specific evidence for this mechanism.

### Mechanism 3
- Claim: The subspaces spanned by the local features of the score model overlap with those of an effective energy-based density function.
- Mechanism: The singular vectors of the score Jacobian and the eigenvectors of the symmetrized score Jacobian span overlapping subspaces, indicating that the learned singular subspaces conform to the data subspace of the effective density.
- Core assumption: The score model approximates the score of an effective energy-based density function.
- Evidence anchors:
  - [abstract]: "At each noise level, the subspace spanned by the local features overlap with an effective density function."
  - [section]: "The singular subspaces of ∇xgσ(x) also overlap well with the eigen-subspace of the symmetrized ∇xˆgσ(x), suggesting that the learned singular subspaces conform to the data subspace of the effective density."
  - [corpus]: Weak - corpus papers discuss density functions but do not provide specific evidence for this mechanism.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is used to analyze the local linear approximations of the score function, revealing the principal directions of the data distribution.
  - Quick check question: What does the SVD of a matrix tell us about its structure?

- Concept: Manifold Hypothesis
  - Why needed here: The paper assumes that the data distribution is supported on a low-dimensional manifold in the ambient space, which is the basis for the observed phenomena.
  - Quick check question: What is the manifold hypothesis, and why is it important for understanding the structure of data distributions?

- Concept: Score-based Generative Models (SBMs)
  - Why needed here: The paper investigates the structure of the vector fields learned by SBMs, which are a class of generative models that estimate the score function of the data distribution.
  - Quick check question: How do score-based generative models differ from other generative models like VAEs and GANs?

## Architecture Onboarding

- Component map:
  - Score Model -> Diffusion Process -> Sample Generation

- Critical path:
  1. Train the score model on the data distribution.
  2. Apply the diffusion process to generate samples from the learned distribution.
  3. Analyze the structure of the learned vector field using SVD.

- Design tradeoffs:
  - Tradeoff between model capacity and training stability: A more expressive score model may capture more complex data distributions but may be harder to train.
  - Tradeoff between noise level and sample quality: A lower noise level may lead to higher-quality samples but may also increase the risk of mode collapse.

- Failure signatures:
  - Mode collapse: The generated samples fail to cover the full diversity of the data distribution.
  - High-frequency artifacts: The generated samples contain unrealistic high-frequency details.

- First 3 experiments:
  1. Train the score model on a simple synthetic dataset with a known manifold structure and analyze the learned vector field using SVD.
  2. Vary the noise level during the diffusion process and observe how the local dimensionality of the data changes.
  3. Compare the subspaces spanned by the singular vectors of the score Jacobian with those of an effective energy-based density function.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the non-conservative mixing behavior within the manifold impact the quality and diversity of generated samples?
- Basis in paper: [explicit] The paper states that "the learned vector field mixes samples by a non-conservative field within the manifold, although it denoises with normal projections as if there is an energy function in off-manifold directions."
- Why unresolved: While the paper identifies this non-conservative mixing behavior, it doesn't explore its effects on sample quality or diversity.
- What evidence would resolve it: Experiments comparing sample quality and diversity between SBMs with and without non-conservative mixing behavior.

### Open Question 2
- Question: What is the relationship between the local dimensionality of the data manifold and the complexity of the underlying data distribution?
- Basis in paper: [explicit] The paper finds that "during diffusion as the noise decreases, the local dimensionality increases and becomes more varied between different sample sequences."
- Why unresolved: The paper observes this relationship but doesn't explore its implications for understanding the complexity of the data distribution.
- What evidence would resolve it: Further analysis of how local dimensionality relates to data distribution complexity across different datasets.

### Open Question 3
- Question: How does the misalignment of singular vectors at lower ranks facilitate mixing between different data components?
- Basis in paper: [explicit] The paper speculates that "the misalignment of singular vectors mixes data across different components, and we speculate that it could facilitate mixing between high-level image classes at high noise levels."
- Why unresolved: The paper provides an example but doesn't provide a rigorous explanation for how this misalignment enables mixing.
- What evidence would resolve it: Mathematical analysis of how the misalignment affects the flow of samples through the score model.

## Limitations
- The analysis relies on local linear approximations through SVD, which may not capture global nonlinear structure in complex data manifolds
- The manifold hypothesis assumption may not hold for all datasets, limiting generalizability
- The study uses MNIST, which may not represent the complexity of real-world high-dimensional data

## Confidence
- **High**: The mechanism of non-conservative mixing within manifolds (Mechanism 1) is well-supported by the observed misalignment of singular vectors at lower ranks
- **Medium**: The claim about increasing local dimensionality during diffusion (Mechanism 2) is plausible but depends heavily on the noise schedule and score model training
- **Medium**: The overlap between score model subspaces and effective energy-based density subspaces (Mechanism 3) is observed but may not generalize to all data distributions

## Next Checks
1. Test on synthetic manifolds with known curvature: Generate synthetic datasets with varying curvature and dimensionality to verify that the observed SVD patterns and mixing behavior scale appropriately with manifold complexity.

2. Evaluate robustness across architectures: Repeat the analysis using different score model architectures (e.g., transformer-based vs U-Net) to determine whether the observed manifold-like structures are architecture-dependent or universal properties of SBMs.

3. Analyze mode coverage in high-dimensional settings: Apply the methodology to higher-dimensional datasets (e.g., CIFAR-10) to verify that the mixing behavior and subspace overlap persist when the ambient space dimensionality increases significantly.