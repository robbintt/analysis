---
ver: rpa2
title: A Neural Scaling Law from Lottery Ticket Ensembling
arxiv_id: '2310.02258'
source_url: https://arxiv.org/abs/2310.02258
tags:
- loss
- lottery
- networks
- tickets
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new mechanism for neural scaling laws (NSL)
  called lottery ticket ensembling. While existing theories attribute NSL to function
  approximation or task hierarchy, the authors find that a simple 1D regression task
  exhibits a scaling law inconsistent with these theories.
---

# A Neural Scaling Law from Lottery Ticket Ensembling

## Quick Facts
- arXiv ID: 2310.02258
- Source URL: https://arxiv.org/abs/2310.02258
- Reference count: 40
- Key outcome: New mechanism for neural scaling laws based on ensembling multiple "lottery tickets" within wide networks, leading to N^-1 variance reduction

## Executive Summary
This paper proposes a new mechanism for neural scaling laws based on lottery ticket ensembling. While existing theories attribute scaling to function approximation or task hierarchy, the authors find that a simple 1D regression task exhibits N^-1 scaling inconsistent with these theories. Through analysis of trained networks, they discover that wider networks contain multiple subnetworks ("lottery tickets") that ensemble together to reduce variance. This effect is strongest in narrow networks and diminishes as width increases, providing a new perspective on why larger networks generalize better.

## Method Summary
The authors train two-layer neural networks to fit the squared function y = x^2. They systematically vary the width N and analyze the resulting mean squared error loss across different random seeds. The training uses Adam optimizer with learning rate 0.01, reduced by ×0.2 every 10000 steps, for 50000 total steps. They analyze weight and bias distributions in trained networks to identify "lottery tickets" - subnetworks that perform well in isolation. The study compares ReLU and SiLU activation functions and examines scaling behavior across different network widths.

## Key Results
- Wider networks exhibit N^-1 scaling law through ensembling of multiple lottery tickets
- Early in training, narrow networks show synergy beyond simple ensembling, but this effect fades as width increases
- Lottery tickets contain symmetric neurons that enable effective approximation of even functions
- SiLU networks show consistent N^-1 scaling while ReLU networks transition from N^-4 to N^-1 at larger widths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural scaling laws arise from ensembling multiple lottery tickets within a single network
- Mechanism: Wider networks contain multiple subnetworks that individually perform well, and ensembling them reduces variance as N^-1 via central limit theorem
- Core assumption: Lottery tickets are approximately unbiased estimators with uncorrelated errors
- Evidence anchors: Section 4 attributes N^-1 scaling to variance reduction as in central limit theorem; abstract mentions wider networks have more lottery tickets that are ensembled

### Mechanism 2
- Claim: Early training shows synergy beyond simple ensembling that fades with increasing width
- Mechanism: Narrow networks find coordinated parameter configurations that work better than independent subnetworks
- Core assumption: Parameter space contains regions where subnetwork interactions produce emergent improvements
- Evidence anchors: Section 4 shows N=2 networks perform much better than simple ensembling of N/2=1 networks, but N=20/40 networks match simple ensembling

### Mechanism 3
- Claim: Symmetric neurons in narrow networks enable effective approximation of even functions
- Mechanism: Pairing neurons with weights (w,b) and (-w,b) creates even functions matching target functions
- Core assumption: Optimization can discover and exploit symmetric configurations
- Evidence anchors: Section 3 finds lottery tickets have symmetric neurons that guarantee even functions and are effective at approximating squared functions

## Foundational Learning

- Concept: Central Limit Theorem
  - Why needed here: Explains why averaging multiple independent estimators reduces variance as 1/sqrt(n)
  - Quick check question: If you average 100 independent unbiased estimators, by what factor does the variance decrease?

- Concept: Lottery Ticket Hypothesis
  - Why needed here: Provides framework that sparse subnetworks can train in isolation to good performance
  - Quick check question: What defines a "winning lottery ticket" in the context of neural network pruning?

- Concept: Taylor Series Expansion
  - Why needed here: Shows how symmetric neuron pairs naturally generate even-powered terms matching target functions
  - Quick check question: What is the Taylor expansion of sin(x) around x=0?

## Architecture Onboarding

- Component map: Input x ∈ [-2, 2] -> Hidden layer with N neurons (ReLU/SiLU) -> Output linear neuron -> MSE loss

- Critical path:
  1. Initialize weights and biases randomly
  2. Forward pass through network
  3. Compute MSE loss
  4. Backpropagate gradients using Adam optimizer
  5. Update parameters
  6. Repeat until convergence

- Design tradeoffs:
  - Width N vs training stability: Wider networks show more consistent performance but may have optimization challenges
  - Activation function choice: ReLU shows N^-4 then N^-1 scaling; SiLU shows consistent N^-1 scaling
  - Training steps: 50,000 steps used; may need adjustment for different scales

- Failure signatures:
  - Loss plateaus above expected value: Indicates bias in lottery tickets
  - Multi-modal loss histograms persist at large N: Suggests correlation between lottery tickets
  - N^-4 scaling continues indefinitely for ReLU: Suggests lack of transition to ensemble regime

- First 3 experiments:
  1. Train N=1,2,3 networks with different random seeds and plot loss histograms to observe lottery ticket emergence
  2. Train N=10,20,30 networks and verify loss follows N^-1 scaling
  3. Pair neurons in wide networks and measure R^2 correlation with target function to identify lottery tickets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the lottery ticket ensembling mechanism be quantified and separated from other neural scaling law mechanisms in practical deep learning tasks?
- Basis in paper: The paper discusses how lottery ticket ensembling leads to N^-1 scaling but acknowledges it's unclear how to disentangle this from approximation theory and quanta theory mechanisms
- Why unresolved: The paper provides theoretical analysis for a simple toy example but doesn't extend this to complex real-world tasks where multiple mechanisms likely interact
- What evidence would resolve it: Controlled experiments on real-world tasks that systematically vary width while measuring contributions of ensembling vs. other mechanisms

### Open Question 2
- Question: What determines the number and quality of lottery tickets in a network, and how does this relate to architecture choices like activation functions and depth?
- Basis in paper: The paper shows SiLU networks have different scaling behavior than ReLU networks, and discusses how symmetric neurons in lottery tickets depend on the activation function
- Why unresolved: The analysis focuses on a specific 1D regression task with one hidden layer, leaving open how lottery ticket properties generalize to deeper networks
- What evidence would resolve it: Systematic study of lottery ticket emergence across different architectures, activation functions, and tasks

### Open Question 3
- Question: Can the redundancy among lottery tickets be exploited for network compression and pruning without sacrificing performance?
- Basis in paper: The paper observes high correlation among lottery tickets in wide networks and mentions this suggests "unnecessarily redundant" representations
- Why unresolved: While the paper identifies correlated lottery tickets, it doesn't explore practical methods to exploit this redundancy for model compression
- What evidence would resolve it: Development and validation of pruning algorithms that specifically target correlated lottery tickets

## Limitations

- Empirical scope limited to simple 1D regression task (x²) without testing on complex real-world problems
- Theoretical framework connecting lottery ticket discovery to scaling exponents lacks rigorous mathematical characterization
- Results may be sensitive to optimization details including initialization schemes and training hyperparameters

## Confidence

- High Confidence: Empirical observation that wider networks exhibit N^-1 scaling through ensembling of subnetworks
- Medium Confidence: Interpretation that variance reduction via central limit theorem explains N^-1 scaling
- Low Confidence: Claim about early training synergy producing super-ensembling effects

## Next Checks

1. Compute pairwise correlations between lottery tickets in networks of varying widths to quantify the assumption of weak correlation and test whether correlation coefficients increase with width

2. Apply the same analysis framework to higher-dimensional regression tasks and classification benchmarks to verify whether N^-1 scaling persists and the mechanism generalizes

3. Systematically vary weight initialization schemes and measure their impact on lottery ticket emergence, correlation structure, and resulting scaling laws to test robustness to initialization choices