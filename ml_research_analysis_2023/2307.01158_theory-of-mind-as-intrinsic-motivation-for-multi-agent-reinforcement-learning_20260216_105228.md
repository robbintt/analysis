---
ver: rpa2
title: Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning
arxiv_id: '2307.01158'
source_url: https://arxiv.org/abs/2307.01158
tags:
- learning
- agents
- beliefs
- belief
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to ground semantically meaningful,
  human-interpretable beliefs within policies modeled by deep networks, and proposes
  the use of 2nd-order belief prediction as an intrinsic reward signal for multi-agent
  reinforcement learning. The authors develop an information-theoretic residual variant
  of the concept bottleneck learning paradigm based on mutual information minimization
  to model beliefs and a residual representation that is decorrelated with the beliefs.
---

# Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.01158
- Source URL: https://arxiv.org/abs/2307.01158
- Reference count: 12
- Primary result: Agents trained with 2nd-order belief prediction as intrinsic reward show improved performance and more complex strategies in mixed cooperative-competitive environments

## Executive Summary
This paper proposes grounding semantically meaningful beliefs within deep network policies and using second-order belief prediction as an intrinsic reward signal for multi-agent reinforcement learning. The authors develop a concept bottleneck learning approach with mutual information minimization to create interpretable belief representations while maintaining a decorrelated residual component. Their method is demonstrated on a mixed cooperative-competitive environment where agents trained with the second-order belief intrinsic reward exhibit more complex strategies and improved performance compared to baselines.

## Method Summary
The approach uses concept bottleneck learning with mutual information minimization to create interpretable belief representations while maintaining a decorrelated residual component. Agents predict other agents' beliefs about the environment, and the accuracy of these second-order predictions serves as an intrinsic reward signal. The method is implemented using MAPPO with centralized training and decentralized execution, trained through alternating optimization between cooperative agents and adversaries.

## Key Results
- Agents trained with 2nd-order belief intrinsic reward outperform baselines in mixed cooperative-competitive environments
- Belief prediction accuracy improves during training, demonstrating effective grounding of interpretable concepts
- Agents exhibit more complex strategic behaviors when using second-order belief prediction as intrinsic motivation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Second-order belief prediction acts as intrinsic motivation by rewarding agents for making other agents' beliefs predictable.
- Mechanism: The intrinsic reward signal is derived from the negative loss of predicting other agents' belief states. When an agent's actions lead to more predictable beliefs in other agents, the prediction loss decreases, increasing the reward.
- Core assumption: The mutual information between an agent's actions and other agents' belief predictability is positively correlated with task performance.

### Mechanism 2
- Claim: The residual network ensures interpretable policies by maintaining statistical independence from the belief concepts.
- Mechanism: Mutual information minimization between the residual representation and belief vector ensures that the residual contains no information about the beliefs, forcing the policy to rely on the interpretable belief concepts rather than a black-box residual.
- Core assumption: Decorrelation between beliefs and residual is sufficient to ensure policy interpretability.

### Mechanism 3
- Claim: First-order beliefs alone are insufficient for effective multi-agent coordination, requiring second-order predictions for optimal performance.
- Mechanism: By predicting other agents' beliefs about the environment, an agent gains strategic advantages in both cooperative and competitive scenarios, enabling more sophisticated behaviors like deception or targeted coordination.
- Core assumption: Multi-agent environments contain sufficient complexity that first-order beliefs cannot capture all relevant strategic information.

## Foundational Learning

- Concept: Theory of Mind (ToM) in AI
  - Why needed here: The entire approach is predicated on modeling agents' beliefs about other agents' mental states, which is the definition of ToM in AI contexts.
  - Quick check question: What is the difference between first-order and second-order belief prediction in the context of ToM?

- Concept: Intrinsic motivation in reinforcement learning
  - Why needed here: The paper proposes using belief prediction accuracy as an intrinsic reward signal rather than relying solely on environment-provided task rewards.
  - Quick check question: How does intrinsic motivation differ from extrinsic motivation in the context of RL reward signals?

- Concept: Mutual information and statistical independence
  - Why needed here: The approach uses mutual information minimization to ensure the residual representation is statistically independent from the belief concepts.
  - Quick check question: What is the relationship between mutual information and statistical independence?

## Architecture Onboarding

- Component map: State → Belief Predictor → Residual Network → Concatenation → Policy → Action → Belief Update → Intrinsic Reward Calculation
- Critical path: State → Belief Predictor → Residual Network → Concatenation → Policy → Action → Belief Update → Intrinsic Reward Calculation
- Design tradeoffs:
  - Interpretability vs performance: More structured belief representations improve interpretability but may limit policy expressiveness
  - Computational cost vs benefit: Second-order belief prediction adds significant computation but provides strategic advantages
  - Mutual information regularization vs reconstruction quality: Stronger decorrelation may reduce residual's ability to capture task-relevant information
- Failure signatures:
  - Policy ignoring beliefs and relying entirely on residual: Indicates insufficient mutual information regularization
  - Second-order predictions not improving over random guessing: Suggests the belief prediction network architecture is inadequate
  - Performance degradation with belief networks: Could indicate the belief concepts are poorly chosen or the environment doesn't benefit from ToM reasoning
- First 3 experiments:
  1. Ablation study: Train with only beliefs, only residual, and combined beliefs+residual to verify mutual information regularization is necessary
  2. First-order vs second-order: Train agents with only first-order beliefs and compare to those with second-order intrinsic rewards
  3. Belief concept selection: Test different belief concept sets (e.g., object locations, agent intentions, resource ownership) to determine which are most valuable for different environment types

## Open Questions the Paper Calls Out

- How does the inclusion of second-order belief prediction as an intrinsic reward affect the complexity and diversity of strategies learned by agents in more complex multi-agent environments?
- How does the concept-residual approach for modeling beliefs compare to standard concept whitening or bottleneck approaches in terms of interpretability and performance?
- How effective is the proposed approach in communication-heavy scenarios, both cooperative and competitive?

## Limitations

- The approach's effectiveness in more complex environments with varied social dynamics remains untested
- The computational overhead of second-order belief prediction is substantial but not thoroughly analyzed
- The choice of belief concepts appears tailored to the specific environment and may not generalize well

## Confidence

- Medium: Claims about second-order beliefs improving strategic behavior are supported by experimental results but lack ablation studies isolating the specific contributions of second-order predictions versus other architectural choices.
- Medium: The mutual information regularization approach for ensuring interpretable policies is theoretically sound but the practical effectiveness of the variational approximation remains unverified.
- Low: Claims about generalizability to other multi-agent environments are speculative given the single-environment evaluation.

## Next Checks

1. Implement an information-theoretic analysis (e.g., MINE-based estimation) to empirically verify that the residual network is truly decorrelated from the belief representations after training.

2. Test the approach on at least two additional multi-agent environments with different characteristics (e.g., one requiring cooperation and one requiring competition) to assess generalizability beyond the physical deception task.

3. Conduct a detailed comparison of wall-clock training time and sample efficiency between agents with and without second-order belief prediction to quantify the practical costs of the approach.