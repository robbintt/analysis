---
ver: rpa2
title: 'Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven
  Jailbreak'
arxiv_id: '2312.04127'
source_url: https://arxiv.org/abs/2312.04127
tags:
- llms
- arxiv
- instructions
- response
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel jailbreak attack method (RADIAL) that
  exploits the inherent response tendencies of large language models (LLMs). The key
  idea is to analyze the LLM's tendency to generate affirmation or rejection responses
  to real-world instructions, then strategically embed malicious instructions into
  affirmation-tendency instructions to bypass safety mechanisms.
---

# Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak

## Quick Facts
- arXiv ID: 2312.04127
- Source URL: https://arxiv.org/abs/2312.04127
- Authors: 
- Reference count: 11
- Key outcome: Novel jailbreak method achieving 75-87% attack success rates by exploiting LLMs' inherent response tendencies to real-world instructions

## Executive Summary
This paper introduces RADIAL, a novel jailbreak attack method that exploits large language models' inherent response tendencies to real-world instructions. The key insight is that LLMs exhibit consistent cognitive biases in their responses to different types of instructions, which can be systematically analyzed and leveraged to bypass safety mechanisms. By embedding malicious instructions within affirmation-tendency contexts, the method achieves significantly higher attack success rates than baseline approaches while producing more semantically coherent prompts that are harder to detect.

## Method Summary
The method involves two main steps: first, analyzing the LLM's inherent response tendencies to real-world instructions by calculating scores based on manually constructed affirmation and rejection responses; second, generating jailbreak prompts by embedding malicious instructions within high-scoring affirmation-tendency instructions. The approach tests different numbers of spliced instructions (2 or 4) and positions of malicious content (front, middle, or end) to optimize attack success. Experiments are conducted on three open-source LLMs using both English and Chinese malicious instructions.

## Key Results
- Attack success rates of 83-87% for English malicious instructions and 75-87% for Chinese malicious instructions
- RADIAL outperforms strong baseline methods in jailbreak effectiveness
- Follow-up questions can induce more detailed harmful responses in over 80% of successful attacks
- The method saves manpower compared to manual jailbreak approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit inherent cognitive biases in their response tendencies to real-world instructions, which can be systematically analyzed and exploited.
- Mechanism: The method identifies "affirmation-tendency instructions" - real-world instructions that naturally induce the LLM to generate affirmative responses. By embedding malicious instructions within these affirmation-tendency contexts, the LLM's natural response pattern is leveraged to bypass safety mechanisms.
- Core assumption: LLMs develop consistent response patterns to specific types of instructions during training that persist despite safety alignment efforts.
- Evidence anchors:
  - [abstract] "The key idea is to analyze the LLM's tendency to generate affirmation or rejection responses to real-world instructions"
  - [section] "When faced with certain instructions, the LLM inherently tends to produce an affirmation response, which we refer to as an 'affirmation-tendency instruction'"
- Break condition: If the LLM's response tendencies are not stable across different contexts or if the safety mechanism can detect the instruction embedding pattern regardless of response tendency.

### Mechanism 2
- Claim: The sequential embedding of multiple real-world instructions with a malicious instruction at the end creates a response context that favors harmful output generation.
- Mechanism: By strategically splicing 2-4 real-world instructions (particularly affirmation-tendency ones) before the malicious instruction, the LLM's attention and response pattern are influenced to favor affirmation responses when processing the malicious content.
- Core assumption: LLMs can effectively process multiple instructions in parallel, and the order/context of instructions significantly influences the final response.
- Evidence anchors:
  - [section] "We need to consider the LLM's ability to respond to multiple instructions in parallel. If there are too many spliced instructions, the LLM's response will be influenced by the in-context"
  - [section] "embedding the malicious instruction at the end of the prompt yields optimal performance for jailbreak attacks"
- Break condition: If the LLM's in-context learning capability is limited or if the model can detect and reject prompts with suspicious instruction patterns.

### Mechanism 3
- Claim: Follow-up questions can induce LLMs to provide more detailed harmful responses after a successful jailbreak attack.
- Mechanism: After the initial jailbreak succeeds but produces a brief harmful response, asking follow-up questions in subsequent dialogue rounds can elicit more detailed harmful content by maintaining the compromised safety context.
- Core assumption: Once the safety mechanism is bypassed in one turn, the compromised state persists into subsequent turns, making the LLM more susceptible to generating detailed harmful content.
- Evidence anchors:
  - [abstract] "The paper also finds that follow-up questions can induce LLMs to generate more detailed harmful responses after successful jailbreak attacks"
  - [section] "our exploration indicates that when our method effectively bypasses security measures in the first dialogue round, it can pave the way for the subsequent round to provide a more detailed harmful response"
- Break condition: If the LLM's safety mechanisms are session-based and reset between dialogue turns, or if the follow-up questions trigger additional safety checks.

## Foundational Learning

- Concept: Instruction Response Tendency Analysis
  - Why needed here: This is the core analytical step that identifies which real-world instructions the LLM is most likely to respond to affirmatively, forming the basis for the jailbreak strategy
  - Quick check question: How would you calculate the response tendency score for a given instruction, and what would a score >1.0 indicate?

- Concept: In-Context Learning and Instruction Processing
  - Why needed here: Understanding how LLMs process multiple instructions in sequence and how instruction ordering affects responses is crucial for designing effective jailbreak prompts
  - Quick check question: What happens to an LLM's response when you increase the number of spliced instructions from 2 to 6, and why might this affect jailbreak success?

- Concept: Safety Mechanism Bypass Strategies
  - Why needed here: The method relies on understanding how safety mechanisms work and how they can be circumvented through context manipulation rather than direct instruction manipulation
  - Quick check question: Why might embedding a malicious instruction within affirmation-tendency contexts be more effective than direct malicious instructions?

## Architecture Onboarding

- Component map: Response Tendency Analysis Engine -> Jailbreak Prompt Generator -> LLM Evaluation Module
- Critical path: Collect real-world instructions → Calculate response tendencies → Select affirmation-tendency instructions → Generate spliced prompts with malicious instructions → Evaluate attack success
- Design tradeoffs: The method trades off between prompt coherence (more natural prompts are harder to detect) and attack effectiveness (more spliced instructions may reduce coherence but increase success rate)
- Failure signatures: Low attack success rates may indicate that the response tendency analysis is inaccurate, the spliced instructions are too numerous causing confusion, or the malicious instructions are being detected by safety mechanisms
- First 3 experiments:
  1. Test the response tendency calculation on a small set of known affirmation/rejection instructions to validate the scoring algorithm
  2. Evaluate the effect of instruction order by testing malicious instructions at different positions within the prompt
  3. Measure the impact of the number of spliced instructions (2 vs 4) on attack success rate while maintaining prompt coherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying sources of cognitive biases in LLMs that lead to inherent response tendencies?
- Basis in paper: [explicit] The paper discusses the presence of cognitive bias in LLMs, attributing it to either biased training data distribution or inherent mechanisms within the RLHF strategy.
- Why unresolved: The paper acknowledges the existence of cognitive biases but does not delve into identifying the specific sources or mechanisms responsible for these biases.
- What evidence would resolve it: Further research to analyze the training data distribution and the RLHF strategy's impact on LLMs' cognitive biases, potentially through controlled experiments or in-depth analysis of the training process.

### Open Question 2
- Question: How does the number of spliced instructions affect the overall attack success rate, and is there an optimal number?
- Basis in paper: [explicit] The paper discusses the impact of splicing two or four instructions on the attack success rate, but it does not provide a definitive answer on the optimal number of spliced instructions.
- Why unresolved: The paper only presents preliminary findings on the effect of splicing different numbers of instructions, without conducting a comprehensive analysis to determine the optimal number.
- What evidence would resolve it: Conducting extensive experiments with varying numbers of spliced instructions to identify the point at which the attack success rate plateaus or decreases due to the LLM's limited capacity to handle multiple instructions.

### Open Question 3
- Question: How effective is the follow-up question strategy in inducing more detailed harmful responses from LLMs, and are there any limitations to this approach?
- Basis in paper: [explicit] The paper demonstrates that asking follow-up questions can induce LLMs to generate more detailed harmful responses in over 80% of cases, but it does not explore the limitations or potential drawbacks of this strategy.
- Why unresolved: The paper provides initial evidence of the strategy's effectiveness but does not investigate its limitations, such as the potential for LLMs to detect and resist follow-up questions or the possibility of generating irrelevant or nonsensical responses.
- What evidence would resolve it: Conducting experiments to test the strategy's effectiveness across different types of malicious instructions, analyzing the quality and relevance of the generated responses, and exploring potential countermeasures that LLMs might employ to resist follow-up questions.

## Limitations

- Dataset Specificity Uncertainty: The method relies on manually constructed affirmation and rejection responses for scoring, but the exact content and diversity of these responses are not specified.
- Manual Translation Dependency: The Chinese attack experiments depend on manual translation of English malicious instructions, introducing potential inconsistencies and scalability issues.
- Generalization Gap: Results may not generalize to other model architectures or larger commercial models beyond the three tested open-source LLMs.

## Confidence

**High Confidence:** The core observation that LLMs exhibit measurable response tendencies to different types of real-world instructions is well-supported by the experimental data. The attack success rates (75-87%) are quantitatively demonstrated across multiple models and instruction types.

**Medium Confidence:** The mechanism explanation for why embedding malicious instructions within affirmation-tendency contexts bypasses safety mechanisms is logically sound but could benefit from deeper analysis of the LLM's internal processing. The follow-up question effectiveness is observed but not thoroughly explained mechanistically.

**Low Confidence:** The claim about RADIAL saving manpower compared to manual methods is qualitative rather than quantitative. The paper mentions this advantage but doesn't provide metrics or comparisons to establish the degree of efficiency improvement.

## Next Checks

**Check 1: Response Tendency Stability Test** - Evaluate whether the calculated response tendencies for real-world instructions remain consistent when tested across different prompt phrasings, temperatures, or across different versions of the same model family.

**Check 2: Safety Mechanism Adaptation Test** - Test whether safety mechanisms can be adapted to detect the instruction embedding pattern itself, rather than just the malicious content, by training a classifier on the spliced instruction structure.

**Check 3: Cross-Domain Instruction Effectiveness** - Validate whether the method works when using real-world instructions from different domains (technical, creative, educational) rather than just the alpaca repository subset used in the original study.