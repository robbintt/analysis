---
ver: rpa2
title: 'Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning
  for Hierarchical Multi-Label Text Classification'
arxiv_id: '2310.05128'
source_url: https://arxiv.org/abs/2310.05128
tags:
- contrastive
- learning
- label
- labels
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of hierarchical multi-label
  text classification (HMTC), where documents can be assigned multiple labels organized
  in a hierarchy. Existing HMTC methods often struggle with leveraging label correlations
  across samples due to the complexity of structured labels.
---

# Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification

## Quick Facts
- **arXiv ID**: 2310.05128
- **Source URL**: https://arxiv.org/abs/2310.05128
- **Reference count**: 38
- **Primary result**: HJCL achieves up to 2.73% and 2.06% improvements in Macro-F1 on BGC and NYT datasets respectively

## Executive Summary
This paper addresses the challenge of hierarchical multi-label text classification (HMTC), where documents can be assigned multiple labels organized in a hierarchy. Existing HMTC methods often struggle with leveraging label correlations across samples due to the complexity of structured labels. To address this, the authors propose HJCL, a novel method that combines instance-wise and label-wise contrastive learning techniques. HJCL uses label-aware embeddings extracted via multi-head attention and carefully constructed batches to fulfill the contrastive learning objective. Extensive experiments on four multi-path HMTC datasets show that HJCL significantly outperforms state-of-the-art methods, achieving up to 2.73% and 2.06% improvements in Macro-F1 on BGC and NYT datasets, respectively. The ablation study confirms the effectiveness of the dual loss approach and the hierarchy-aware label contrastive loss.

## Method Summary
HJCL combines instance-wise and label-wise contrastive learning with hierarchy-aware weighting to address hierarchical multi-label text classification. The method uses BERT to encode input text, then applies multi-head attention to extract label-aware embeddings that capture the semantic relationship between each label and the input tokens. These label-aware embeddings are combined with label text embeddings and passed through a Graph Attention Network (GAT) to inject hierarchical information. The model then optimizes three losses: a Zero-bounded Logsum-exp & Pairwise Rank-based (ZLPR) loss for classification, an instance-wise contrastive loss that pulls together documents with similar label sets while pushing apart dissimilar ones (with deeper-level matches weighted more heavily), and a hierarchy-aware label contrastive loss (HiLeCon) that adjusts similarity weights based on hierarchical distance between labels in their source documents. The combined approach is trained with AdamW optimizer, learning rate 3e-5, batch size 80, and early stopping based on validation Macro-F1.

## Key Results
- HJCL achieves up to 2.73% improvement in Macro-F1 on BGC dataset compared to state-of-the-art methods
- HJCL achieves up to 2.06% improvement in Macro-F1 on NYT dataset
- Ablation study confirms the effectiveness of both dual contrastive losses, with HiLeCon showing particular importance for leveraging label correlations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instance-level contrastive learning forces similar documents with overlapping label sets to have closer embeddings, while dissimilar documents are pushed apart, with deeper-level label matches weighted more heavily.
- Mechanism: The method computes distance inequalities (distposℓ1 < distposℓ2 < distneg) where ℓ1 and ℓ2 are levels in the hierarchy. Pairs at deeper levels are penalized with exp(1/|L| - ℓ) to enforce tighter clustering.
- Core assumption: Label similarity correlates with document semantic similarity and hierarchy depth reflects semantic granularity.
- Evidence anchors:
  - [abstract]: "the representations of intra-class should obtain higher similarity scores than inter-class pairs, meanwhile intra-class pairs at deeper levels obtain more weight than pairs at higher levels."
  - [section 4.2]: Equation defining Llevel and LInst. shows explicit distance-based weighting by hierarchy depth.
  - [corpus]: Weak evidence - corpus neighbors do not directly support this specific hierarchy-weighted contrastive learning mechanism.
- Break condition: If label overlap does not reflect semantic similarity (e.g., co-occurring but unrelated labels), this weighting could mislead the model.

### Mechanism 2
- Claim: Label-aware embeddings enable effective label-wise contrastive learning by mapping each label independently while preserving semantic similarity between labels based on their source documents.
- Mechanism: Multi-head attention between label embeddings and token embeddings generates label-specific representations gi, which are then fused with label text embeddings to form contrastive anchors.
- Core assumption: Label embeddings derived from text attention capture the semantic context relevant to each label.
- Evidence anchors:
  - [section 4.1]: Detailed description of label-aware embedding extraction using multi-head attention between labels and input tokens.
  - [section 4.2]: Equation showing how label-aware embeddings are used as contrastive anchors in LHiLeCon.
  - [corpus]: No direct evidence; corpus lacks discussion of attention-based label embedding.
- Break condition: If attention weights are noisy or if label descriptions are ambiguous, embeddings may misrepresent label semantics.

### Mechanism 3
- Claim: The hierarchy-aware label contrastive loss (HiLeCon) adjusts similarity weights based on the hierarchical distance between labels in their source documents, preventing pushing apart labels that are semantically related.
- Mechanism: Uses a custom distance metric ρ(Yi, Yj) that weights label pairs by their depth in the hierarchy, and applies these weights to the contrastive loss.
- Core assumption: Hierarchical depth difference reflects semantic relatedness between labels.
- Evidence anchors:
  - [section 4.2]: Equation 3 defines ρ(Yi, Yj) with depth-based weighting.
  - [section 4.2]: HiLeCon formulation in Equation 4 shows weighted contrastive loss.
  - [corpus]: No corpus evidence linking hierarchy depth to label semantic similarity.
- Break condition: If labels at different depths are not semantically related (e.g., unrelated subcategories), this weighting could harm performance.

## Foundational Learning

- **Concept**: Multi-head attention
  - Why needed here: Enables label-aware embeddings by capturing different semantic aspects between labels and input tokens.
  - Quick check question: How does multi-head attention differ from single-head attention in this context?

- **Concept**: Contrastive learning (instance-wise and label-wise)
  - Why needed here: Addresses the challenge of learning semantic relationships between documents and labels in a hierarchical, multi-label setting.
  - Quick check question: What distinguishes instance-wise from label-wise contrastive learning in HJCL?

- **Concept**: Graph attention networks for hierarchy propagation
  - Why needed here: Propagates hierarchical information among label embeddings before contrastive learning.
  - Quick check question: Why is hierarchy propagation necessary before applying contrastive learning?

## Architecture Onboarding

- **Component map**: BERT encoder → label-aware embedding (multi-head attention) → GAT hierarchy propagation → contrastive loss heads (instance-wise + label-wise) → classification head (ZLPR loss)
- **Critical path**: Input text → BERT → label-aware embeddings → contrastive objectives → classification
- **Design tradeoffs**: Label-aware embeddings scale with label count; simpler GAT vs. stronger GNNs for hierarchy; two contrastive losses vs. single loss
- **Failure signatures**: Degraded Macro-F1 when removing HiLeCon (shows label correlation importance); drop in Micro-F1 when removing instance contrastive (shows global hierarchy importance)
- **First 3 experiments**:
  1. Ablation: Remove HiLeCon to confirm label correlation impact
  2. Ablation: Remove instance contrastive to test hierarchy learning
  3. Hyperparameter sweep: λ1 and λ2 to balance two contrastive losses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed HJCL method scale with the number of labels in the taxonomy, and what are the potential performance implications for datasets with a large number of labels?
- Basis in paper: [inferred] The paper mentions that the use of label-aware embeddings scales according to the number of labels in the taxonomy, and it may not be applicable for other HMTC datasets with a large number of labels.
- Why unresolved: The paper does not provide empirical evidence or experiments to support this claim. It also does not discuss potential strategies to address this limitation.
- What evidence would resolve it: Conducting experiments on HMTC datasets with a large number of labels and comparing the performance of HJCL with other methods would provide evidence to support or refute this claim.

### Open Question 2
- Question: How does the proposed HJCL method handle label correlation and hierarchy information in the presence of label noise or label uncertainty?
- Basis in paper: [inferred] The paper discusses the effectiveness of HJCL in leveraging label correlation and hierarchy information, but it does not explicitly address how the method handles label noise or uncertainty.
- Why unresolved: The paper does not provide any experiments or analysis on the robustness of HJCL to label noise or uncertainty.
- What evidence would resolve it: Conducting experiments on datasets with varying levels of label noise or uncertainty and evaluating the performance of HJCL compared to other methods would provide evidence to support or refute the method's robustness.

### Open Question 3
- Question: How does the proposed HJCL method perform on datasets with non-tree structured hierarchies or multi-parent relationships?
- Basis in paper: [inferred] The paper focuses on tree-structured hierarchies and does not explicitly discuss the applicability of HJCL to datasets with non-tree structured hierarchies or multi-parent relationships.
- Why unresolved: The paper does not provide any experiments or analysis on the performance of HJCL on datasets with non-tree structured hierarchies or multi-parent relationships.
- What evidence would resolve it: Conducting experiments on datasets with non-tree structured hierarchies or multi-parent relationships and comparing the performance of HJCL with other methods would provide evidence to support or refute the method's applicability.

## Limitations
- The method's scalability with large label taxonomies is not empirically validated and may be limited by the label-aware embedding approach
- The assumption that hierarchical depth directly reflects semantic relatedness between labels is not empirically validated in the corpus
- The choice of GAT for hierarchy propagation may not capture complex hierarchical relationships as effectively as stronger GNN variants

## Confidence
- **High confidence**: The general architecture combining instance-wise and label-wise contrastive learning is sound and addresses a real challenge in HMTC. The experimental results showing performance improvements are verifiable through the reported metrics.
- **Medium confidence**: The specific implementation details of the ZLPR loss function and the exact label text descriptions used for initialization are not fully specified, creating potential reproducibility gaps.
- **Low confidence**: The claim that hierarchy depth directly reflects semantic relatedness between labels is not empirically validated, and the corpus lacks evidence supporting this assumption.

## Next Checks
1. **Validate the hierarchy depth assumption**: Conduct a controlled experiment to test whether labels at different hierarchical depths are indeed semantically related. For example, measure the semantic similarity between label pairs at the same depth versus different depths using a pre-trained language model.

2. **Test the robustness of label-aware embeddings**: Perform an ablation study where label descriptions are replaced with random noise or removed entirely to assess the impact on model performance. This will validate whether the multi-head attention mechanism is capturing meaningful semantic information.

3. **Compare GAT to stronger GNNs**: Replace the GAT with a more powerful GNN variant (e.g., GraphSAGE or GATv2) and measure the impact on performance. This will determine whether the choice of GAT is a limitation or a deliberate design decision.