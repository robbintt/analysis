---
ver: rpa2
title: Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic
  Reasoning
arxiv_id: '2312.05571'
source_url: https://arxiv.org/abs/2312.05571
tags:
- reasoning
- language
- arithmetic
- number
- python
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SYRELM, a frugal large language model (LM)
  system that achieves parameter-efficient arithmetic reasoning by mapping natural
  language arithmetic problems to formal language (FL) expressions and using a symbolic
  solver to evaluate them. Unlike massive LLMs, SYRELM uses a small frozen LM equipped
  with a low-rank adapter and policy-gradient reinforcement learning to translate
  arithmetic questions into FL and generate interpretable step-by-step reasoning.
---

# Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning

## Quick Facts
- arXiv ID: 2312.05571
- Source URL: https://arxiv.org/abs/2312.05571
- Reference count: 20
- Key outcome: Parameter-efficient arithmetic reasoning through symbolic solver integration outperforms base LMs and tool-augmented approaches

## Executive Summary
SYRELM introduces a frugal large language model system that achieves parameter-efficient arithmetic reasoning by mapping natural language arithmetic problems to formal language expressions and using a symbolic solver to evaluate them. Unlike massive LLMs, SYRELM employs a small frozen LM with low-rank adapter and policy-gradient reinforcement learning to translate arithmetic questions into formal language and generate interpretable step-by-step reasoning. Experiments on datasets like SVAMP, GSM8K, and MultiArith show SYRELM significantly outperforms base LMs and other tool-augmented approaches, with GPT-J 6B improving accuracy by up to +31.6 points.

## Method Summary
SYRELM trains a frozen LM with low-rank adapter (LoRA) using Proximal Policy Optimization (PPO) with reasoning-specific rewards to generate formal language expressions from natural language arithmetic problems. The symbolic solver then evaluates these expressions to obtain answers. The approach modularizes language understanding, symbolic manipulation, and arithmetic solving, making reasoning more interpretable and efficient while requiring modest computational resources.

## Key Results
- SYRELM achieves +19.4 accuracy improvement on SVAMP dataset
- GPT-J 6B model improves by +31.6 points on arithmetic reasoning tasks
- Performance generalizes to longer reasoning chains rarely seen during training
- LoRA-based approach maintains parameter efficiency while matching or exceeding full fine-tuning baselines

## Why This Works (Mechanism)

### Mechanism 1
- Low-rank adapters with symbolic solvers enable frugal LMs to achieve performance competitive with massive LLMs on arithmetic reasoning tasks by offloading computation to deterministic symbolic evaluation.

### Mechanism 2
- Policy-gradient reinforcement learning with reasoning-specific rewards aligns the adapter-augmented LM with symbolic reasoning tasks through non-differentiable reward signals from the symbolic solver.

### Mechanism 3
- Modularizing language understanding, symbolic manipulation, and arithmetic solving improves interpretability and efficiency by allowing each component to specialize in its domain.

## Foundational Learning

- Concept: Formal language expression generation
  - Why needed here: The LM must translate natural language arithmetic problems into a structured format that symbolic solvers can process
  - Quick check question: Can the LM generate syntactically correct Python or pseudocode expressions from natural language descriptions?

- Concept: Reinforcement learning with non-differentiable rewards
  - Why needed here: Standard supervised learning cannot handle the symbolic solver's non-differentiable outputs, requiring policy-gradient methods
  - Quick check question: Does the reward structure capture both syntactic correctness and semantic accuracy of generated expressions?

- Concept: Low-rank adaptation techniques
  - Why needed here: Full fine-tuning of large LMs is computationally expensive, while LoRA provides parameter-efficient adaptation
  - Quick check question: Can the adapter generate meaningful improvements while keeping the base LM frozen?

## Architecture Onboarding

- Component map: Natural language problem + instruction tokens -> Adapter-augmented LM -> FL expression -> Symbolic solver -> Answer -> Reward -> PPO update

- Critical path: Natural language → Adapter-augmented LM → FL expression → Symbolic solver → Answer → Reward → PPO update

- Design tradeoffs:
  - LoRA rank parameter vs. adaptation quality: Higher rank provides better adaptation but increases parameters
  - Reward weighting vs. learning stability: Different reward components may need different weights for effective learning
  - FL complexity vs. LM capability: More complex FL allows better expression of reasoning but may exceed LM's generation capacity

- Failure signatures:
  - Compilation errors in generated FL expressions indicate syntax generation problems
  - Low variable coverage rewards suggest the LM misses relevant quantities
  - Poor operator matching indicates incorrect reasoning steps
  - Large answer differences suggest fundamental misunderstanding of the problem

- First 3 experiments:
  1. Test adapter-augmented LM on simple arithmetic problems to verify basic FL generation capability
  2. Evaluate symbolic solver integration by running generated expressions and checking compilation success
  3. Run PPO training with simplified reward structure to verify reinforcement learning pipeline works before adding complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SYRELM's performance scale with increasing problem complexity and reasoning step requirements?
- Basis in paper: Figure 4 shows error rate increases with more reasoning steps, but SYRELM still generalizes to longer chains rarely seen in training
- Why unresolved: The paper only shows this trend on SV AMP dataset - need to verify if similar patterns hold for GSM8K and MultiArith
- What evidence would resolve it: Detailed error rate analysis across all three datasets as reasoning steps increase, with specific focus on the longest problems

### Open Question 2
- Question: What are the limitations of using pseudocode vs Python as the formal language for SYRELM?
- Basis in paper: The authors note pseudocode struggles with GSM8K's complex problems and cannot handle percentages/interest calculations
- Why unresolved: The paper only briefly mentions these limitations without systematic comparison of when each formal language succeeds/fails
- What evidence would resolve it: Comprehensive error analysis showing specific problem types where pseudocode fails but Python succeeds, and vice versa

### Open Question 3
- Question: How would SYRELM's performance change if trained on more diverse arithmetic reasoning datasets?
- Basis in paper: The authors note SYRELM is trained on limited datasets (ASDiv, MAWPS, Math23k) and show varying performance across SV AMP, GSM8K, MultiArith
- Why unresolved: The paper doesn't explore how performance scales with training data diversity or quantity
- What evidence would resolve it: Training SYRELM on progressively larger and more diverse datasets, then measuring performance improvements on all three test sets

## Limitations
- Limited evaluation to arithmetic reasoning tasks only
- Reliance on frozen base LM may cap performance potential
- Unclear generalizability to non-arithmetic mathematical reasoning domains

## Confidence
- High Confidence: Core mechanism of using LoRA adapters with symbolic solvers is well-supported by experimental results
- Medium Confidence: PPO reinforcement learning framework effectiveness, but individual reward component contributions need further isolation
- Low Confidence: Generalization claims beyond arithmetic reasoning are weakly supported with limited empirical evidence

## Next Checks
1. Conduct ablation study on reward components to quantify individual contributions to final performance
2. Evaluate SYRELM performance across different base LM sizes to determine efficiency advantages at various scales
3. Test the SYRELM framework on non-arithmetic reasoning tasks such as logical inference or commonsense reasoning