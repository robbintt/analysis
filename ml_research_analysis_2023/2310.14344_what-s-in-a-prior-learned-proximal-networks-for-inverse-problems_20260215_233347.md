---
ver: rpa2
title: What's in a Prior? Learned Proximal Networks for Inverse Problems
arxiv_id: '2310.14344'
source_url: https://arxiv.org/abs/2310.14344
tags:
- proximal
- function
- inverse
- prior
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces learned proximal networks (LPN), a class
  of neural networks that guarantee to parameterize exact proximal operators for a
  learned data-driven nonconvex regularizer. Unlike existing plug-and-play methods,
  LPNs provide provable proximal operators, enabling convergence guarantees for iterative
  algorithms.
---

# What's in a Prior? Learned Proximal Networks for Inverse Problems

## Quick Facts
- arXiv ID: 2310.14344
- Source URL: https://arxiv.org/abs/2310.14344
- Reference count: 40
- Primary result: Introduces LPNs that parameterize exact proximal operators for learned nonconvex regularizers, achieving state-of-the-art performance on deblurring, CT reconstruction, and compressed sensing tasks.

## Executive Summary
This paper introduces learned proximal networks (LPNs), a novel class of neural networks that guarantee to parameterize exact proximal operators for learned nonconvex regularizers. Unlike existing plug-and-play methods, LPNs provide provable proximal operators, enabling convergence guarantees for iterative algorithms. The authors propose a new training strategy, proximal matching, which provably promotes recovery of the log-prior of the true data distribution. Experiments on increasingly complex datasets demonstrate that LPNs learn meaningful priors and achieve state-of-the-art performance while providing explicit characterization of the learned priors.

## Method Summary
LPNs are constructed by differentiating input convex neural networks (ICNN) with softplus activations and non-negative weights. The proximal matching loss is used for training, which approximates the Dirac delta to maximize posterior probability and recover the true proximal operator of the log-prior. LPNs are then used as denoisers in plug-and-play algorithms like PnP-ADMM for solving inverse problems. The method is evaluated on deblurring, CT reconstruction, and compressed sensing tasks using datasets ranging from 1D Laplacian to high-resolution CelebA and Mayo-CT images.

## Key Results
- LPNs learn exact proximal operators for nonconvex regularizers, providing provable convergence guarantees for PnP algorithms.
- Proximal matching loss provably recovers the log-prior of the true data distribution.
- LPNs achieve state-of-the-art performance on deblurring, CT reconstruction, and compressed sensing tasks across increasingly complex datasets.
- The learned priors can be explicitly characterized and evaluated via log-likelihood values.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LPNs parameterize exact proximal operators for a learned nonconvex regularizer.
- Mechanism: By construction, LPNs are defined as gradients of convex functions (ICNN), and according to Proposition 1, any continuous gradient of a convex function is the proximal operator of some function. This guarantees that the network output is an exact proximal operator, not an approximation.
- Core assumption: The activation function is convex, non-decreasing, and twice differentiable (e.g., softplus), and all network weights are non-negative.
- Evidence anchors:
  - [abstract]: "provide a framework to develop learned proximal networks (LPN), prove that they provide exact proximal operators for a data-driven nonconvex regularizer"
  - [section]: "Proposition 2 (Learned Proximal Networks)... Then, there exists a function ϕθ : Rn → R ∪ {+∞} such that fθ(y) = proxϕθ(y), ∀ y ∈ Rn."
  - [corpus]: Weak - related works discuss denoiser-based PnP but not exact proximal parameterization.
- Break condition: If the network violates the non-negativity constraint on weights or uses non-convex/non-decreasing activations, the guarantee no longer holds.

### Mechanism 2
- Claim: Proximal matching loss recovers the true log-prior of the data distribution.
- Mechanism: The proximal operator is the MAP denoiser for Gaussian noise. The proposed proximal matching loss approximates the Dirac delta, so minimizing it maximizes the posterior probability, yielding the true proximal operator of the log-prior.
- Core assumption: The data distribution has a continuous density and the signal is bounded.
- Evidence anchors:
  - [abstract]: "show how a new training strategy, dubbed proximal matching, provably promotes the recovery of the log-prior of the true data distribution"
  - [section]: "Theorem 3.1 (Learning via Proximal Matching)... f ∗(y) = argmaxc px|y(c) ≜ prox−σ2 log px(y)"
  - [corpus]: Weak - related works use ℓ2 or ℓ1 losses for denoising, which do not recover the MAP estimator.
- Break condition: If the noise model deviates significantly from Gaussian or the data distribution lacks sufficient regularity, the theoretical guarantee may not hold.

### Mechanism 3
- Claim: LPNs enable convergence guarantees for PnP algorithms without restrictive assumptions.
- Mechanism: Since LPNs are guaranteed to be proximal operators, classical convergence results for proximal algorithms apply directly, requiring only basic conditions like step size bounds.
- Core assumption: The LPN uses softplus activations and is trained with 0 < α < 1 (strong convexity weight).
- Evidence anchors:
  - [abstract]: "Such LPN provide general, unsupervised, expressive proximal operators that can be used for general inverse problems with convergence guarantees"
  - [section]: "Theorem 4.1... the iterates xk converge to a fixed point x∗ of Algorithm 1"
  - [corpus]: Weak - related works require Lipschitz or nonexpansivity constraints on denoisers for convergence, which are difficult to verify.
- Break condition: If the measurement operator is not definable in an o-minimal structure or the data fidelity term lacks Lipschitz gradient, the convergence proof may not apply.

## Foundational Learning

- Concept: Proximal operators and their role in inverse problems
  - Why needed here: LPNs are defined as proximal operators, and understanding their properties is essential for both the theoretical framework and practical implementation.
  - Quick check question: What is the defining equation for a proximal operator, and how does it relate to denoising?

- Concept: Input convex neural networks (ICNN)
  - Why needed here: LPNs are constructed by differentiating ICNNs, so understanding the architecture and constraints of ICNNs is crucial.
  - Quick check question: What are the key constraints on ICNN weights and activations to ensure convexity?

- Concept: Maximum a posteriori (MAP) estimation and denoising
  - Why needed here: The proximal operator is the MAP denoiser for Gaussian noise, and the proximal matching loss is designed to recover this property.
  - Quick check question: How does the MAP estimator differ from the minimum mean squared error (MMSE) estimator in denoising?

## Architecture Onboarding

- Component map: Data → ICNN → LPN (proximal) → Inverse problem solver (PnP) → Solution
- Critical path: Data → ICNN → LPN (proximal) → Inverse problem solver (PnP) → Solution
- Design tradeoffs:
  - Expressivity vs. constraints: Non-negative weights and convex activations limit expressivity but guarantee exact proximal parameterization.
  - Training stability vs. performance: Strong convexity weight α > 0 ensures invertibility but may reduce expressivity.
  - Computational cost: Prior estimation requires solving a convex optimization problem for each query.
- Failure signatures:
  - Poor denoising performance: May indicate insufficient network capacity or inappropriate loss function.
  - Non-convergence in PnP: Could be due to step size issues, measurement operator properties, or LPN training.
  - Inaccurate prior estimation: Might result from numerical instability in the inversion step or insufficient optimization iterations.
- First 3 experiments:
  1. Train LPN on 1D Laplacian data with proximal matching loss and verify learned proximal matches soft-thresholding.
  2. Train LPN on MNIST with ℓ1 loss, then switch to proximal matching loss, and compare prior estimation quality.
  3. Use trained LPN in PnP-ADMM for deblurring on CelebA and compare PSNR/SSIM with PnP-BM3D and PnP-DnCNN.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the expressivity of LPNs compare to other methods like ICGN [73] in terms of learning complex priors?
- Basis in paper: [explicit] The paper discusses ICGN in Appendix F.1, noting that ICGN has limited representation capacity due to its single-layer constraint.
- Why unresolved: The paper does not provide a direct comparison of LPNs and ICGN on complex datasets.
- What evidence would resolve it: Experimental results comparing LPN and ICGN performance on diverse datasets would be needed.

### Open Question 2
- Question: What are the limitations of using LPNs for high-dimensional data beyond the CelebA and Mayo-CT datasets?
- Basis in paper: [inferred] The paper demonstrates LPN performance on increasingly complex datasets, suggesting potential limitations for even higher dimensions.
- Why unresolved: The paper does not explore LPN performance on data with higher dimensions than 512x512.
- What evidence would resolve it: Experiments applying LPN to ultra-high resolution images or 3D medical data would be needed.

### Open Question 3
- Question: How sensitive is the performance of LPNs to the choice of the strong convexity weight α?
- Basis in paper: [explicit] The paper uses different α values (0.01, 1e-6, 0) in experiments but does not explore the sensitivity to this parameter.
- Why unresolved: The paper does not provide a systematic study of α's impact on performance.
- What evidence would resolve it: A parameter sweep study varying α across a wide range for different tasks would be needed.

## Limitations
- The theoretical framework requires strict constraints on network architecture (non-negative weights, convex non-decreasing activations) which may limit expressivity.
- The proximal matching loss requires knowledge of the noise level σ, and convergence guarantees depend on specific conditions like the measurement operator being definable in an o-minimal structure.
- The prior estimation step adds computational overhead and numerical challenges.

## Confidence

- Exact proximal parameterization guarantee: High
- Proximal matching loss recovery of true log-prior: Medium
- Convergence guarantees for PnP algorithms: Medium

## Next Checks

1. Verify that the learned proximal operator on 1D Laplacian data matches the theoretical soft-thresholding operator within numerical tolerance.
2. Conduct ablation studies on different convex activation functions (softplus variants) to assess impact on denoising performance.
3. Test LPN performance on measurement operators not definable in o-minimal structures to evaluate robustness of convergence guarantees.